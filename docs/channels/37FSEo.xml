<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Rust</title><link>https://konrad.website/feeds/</link><description></description><item><title>Building the MagicMirror in Rust with iced GUI Library ðŸ¦€</title><link>https://www.reddit.com/r/rust/comments/1ipzubj/building_the_magicmirror_in_rust_with_iced_gui/</link><author>/u/amindiro</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:56:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I recently embarked on a journey to build a custom MagicMirror using the Rust programming language, and Iâ€™d like to share my experiences. I wrost a blog post titled "software you can love: miroir Ã” mon beau miroir" this project was my attempt to create a stable, resource-efficient application for the Raspberry Pi 3A.Here's what I loved about using Rust and the iced GUI library:Elm Architecture + Rust is a match made in heaven: iced was perfect for my needs with its Model, View, and Update paradigms. It helped keep my state management concise and leverage Rust type system Opting for this lightweight rendering library reduced the size of the binary significantly, ending with a 9MB binary. Although troublesome at first, I used â€˜crossâ€™ to cross compile Rust for armv7.If anyone is keen, Iâ€™m thinking of open-sourcing this project and sharing it with the community. Insights on enhancing the project's functionality or any feedback would be much appreciated!Feel free to reach out if you're interested in the technical nitty-gritty or my experience with Rust GUI libraries in general.]]></content:encoded></item><item><title>Vq: A Vector Quantization Library for Rust ðŸ¦€</title><link>https://www.reddit.com/r/rust/comments/1ipu2jg/vq_a_vector_quantization_library_for_rust/</link><author>/u/West-Bottle9609</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 04:56:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've created a Rust library called Vq that implements several vector quantization algorithms. At the moment, these algorithms include binary, scalar, product, optimized product, tree-structured, and residual quantization. I think the library can be useful for tasks like data compression, similarity search, creating RAG pipelines, and speeding up machine learning computations.This is my second Rust project, as I'm currently learning Rust. I'd like to get some feedback from the community and hear about any use cases you might have for the library, so I'm making this announcement.The library is available on crates.io: vq, and the source code is on GitHub: vq.]]></content:encoded></item><item><title>Bringing Nest.js to Rust: Meet Toni.rs, the Framework Youâ€™ve Been Waiting For! ðŸš€</title><link>https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/</link><author>/u/Mysterious-Rust</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 02:42:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a Rust developer coming from TypeScript, Iâ€™ve been missing a Nest.js-like framework â€” its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesnâ€™t have a direct counterpart (yet!), I decided to build one myself! ðŸ› ï¸Introducingâ€¦ Toni.rs â€” a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And itâ€™s live in beta! ðŸŽ‰Hereâ€™s what makes this project interesting:Scalable maintainability ðŸ§©:A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code â€” each module lives in its own context, clean and focused.Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?Automatic Dependency Injection ðŸ¤–:Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.Leave your thoughts below â€” suggestions, questions, or even just enthusiasm! ðŸš€ ]]></content:encoded></item><item><title>Tabiew 0.8.4 Released</title><link>https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/</link><author>/u/shshemi</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 00:21:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...ðŸ“Š Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and SqliteðŸ—‚ï¸ Multi-table functionalityUI is updated to be more modern and responsiveHorizontally scrollable tablesVisible data frame can be referenced with name "_"Compatibility with older versions of glibcTwo new themes (Tokyo Night and Catppuccin)]]></content:encoded></item><item><title>Is there any easy way to find a variable&apos;s type</title><link>https://www.reddit.com/r/rust/comments/1ipopuu/is_there_any_easy_way_to_find_a_variables_type/</link><author>/u/Money_Ad_4688</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 23:58:19 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I'm on a section of the rust book,Just for testing reasons, is there any way to write out this code and then check for the type of s (or what type results from .to_string() method)?It's probably going to result in String, but there are many methods out there that result in many different typesAny time I see a method whose return type I don't know, should I just refer to its documentation, or is there a way to check by compiling it?]]></content:encoded></item><item><title>the ref keyword</title><link>https://www.reddit.com/r/rust/comments/1ipixny/the_ref_keyword/</link><author>/u/Tickstart</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 19:36:57 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've made a quick mock situation which is analogous to my situation the other day:fn main() { let mut v: Option<Vec<usize>> = None; let mut h = 20; while h.ne(&0) { if (h % 3).ge(&1) { match v { Some(ref mut v) => (*v).push(h), None => v = Some(vec![h]) } } h -= 1 } println!("{v:?}") } I was a bit confused on how it "should" be solved. My issue is the "ref mut". It made sense to me that I didn't want to consume the vector v, just add to it if it existed and I tried adding ref (then mut), which worked. When I goodled, it seemed ref was a legacy thing and not needed anymore. My question is, how is the idiomatic way to write this? Perhaps it's possible to do in a much simpler way and I just found a way to complicate it for no reason.Also, don't worry I know this is a terrible pattern, it was mostly for tesing something.]]></content:encoded></item><item><title>Macro-Less, Highly Integrated OpenAPI Document Generation in Rust with Ohkami</title><link>https://medium.com/@kanarus786/macro-less-highly-integrated-openapi-document-generation-in-rust-with-ohkami-912de388adc1</link><author>/u/kanarus</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 18:26:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[In Rust web dev, utoipa is the most popular crate for generating OpenAPI document from server code. While itâ€™s a great tool, it can be frustrating due to excessive macro use.A new web framework Ohkami offers a ,  way to generate OpenAPI document with its â€œopenapiâ€ feature.Letâ€™s take following code as an example. Itâ€™s the same sample from the â€œopenapiâ€ section of the README, but with openapi-related parts removed:While this compiles and works as a pseudo user management server, activating â€œopenapiâ€ feature causes a compile error, telling that User and CreateUser donâ€™t implement ohkami::openapi::Schema.As indicated by this, Ohkami with â€œopenapiâ€ feature effectively handles type information and intelligently collects its endpointsâ€™ metadata. It allows code like:to assemble metadata into an OpenAPI document and output it to a file .Then, how we implement Schema? Actually we can easily impl Schema by hand, or just #[derive(Schema)] is available! In this case, derive is enough:Thatâ€™s it! Just adding these derives allows Ohkami::generate to output following file:Additionally, itâ€™s easy to define the User schema as a component instead of duplicating inline schemas. In derive, just add #[openapi(component)] helper attribute:And  #[operation] attribute is available to set summary, description, and override operationId and each responseâ€™s description:Letâ€™s take a look at how this document generation works!First, the #[derive(Schema)]s are expanded as following:The DSL enables to easily impl manually.Schema trait links the struct to an item of type called â€œSchemaRefâ€.2. openapi_* hooks of FromParam, FromRequest, IntoResponseFromParam, FromRequest and IntoResponse are Ohkamiâ€™s core traits appeared in the handler bound:When â€œopenapiâ€ feature is activated, they additionally have following methods:Ohkami leverages these methods in IntoHandler to generate consistent openapi::Operation, reflecting the actual handler signature like this.Moreover, Ohkami properly propagates schema information in common cases like this, allowing users to focus only on the types and schemas of their app.3. routes metadata of RouterIn Ohkami, whatâ€™s called router::base::Router has â€œroutesâ€ property that stores all the routes belonging to an Ohkami instance. This is returned alongside router::final::Router from â€œfinalizeâ€ step, and is used to assemble metadata of all endpoints.What Ohkami::generate itself does is just to serialize an item of type openapi::document::Document and write it to a file.The openapi::document::Document item is created by â€œgen_openapi_docâ€ of router::final::Router, summarized as follows:Thatâ€™s how Ohkami generates OpenAPI document!There is, however, a problem in , Cloudflare Workers: Ohkami is loaded to Miniflare or Cloudflare Workers as WASM, so it can only generate OpenAPI document andcannot write it to the userâ€™s local file system.To work around this, Ohkami provides a CLI tool scripts/workers_openapi.js. This is, for example, used in package.json of Cloudflare Workers + OpenAPI template:generates OpenAPI document!]]></content:encoded></item><item><title>I&apos;m very impressed by how Rust supports both beginners and pro&apos;s</title><link>https://www.reddit.com/r/rust/comments/1ipe6m7/im_very_impressed_by_how_rust_supports_both/</link><author>/u/ConstructionShot2026</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 16:16:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I would go as far saying it supports a syntax abstraction that is simpler than python to read.I just find it amazing, with a performance level so close to C++.Its your choice how many complex features you want to add for control and optimization, and the compiler is so cool, that it can add them automatically if I don't see it necessary.I believe if more knew how simple it could be, more would use it outside systems programming :D]]></content:encoded></item><item><title>The Embedded Rustacean Issue #39</title><link>https://www.theembeddedrustacean.com/p/the-embedded-rustacean-issue-39</link><author>/u/TheEmbeddedRustacean</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 14:49:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>So you want better debug info?</title><link>https://walnut356.github.io/posts/so-you-want-better-debug-info/</link><author>/u/Anthony356</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 13:30:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Released dom_smoothie 0.6.0: A Rust crate for extracting readable content from web pages</title><link>https://github.com/niklak/dom_smoothie/releases/tag/0.6.0</link><author>/u/genk667</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 09:09:51 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Trait upcasting stabilized in 1.86</title><link>https://github.com/rust-lang/rust/pull/134367</link><author>/u/hpxvzhjfgb</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 07:10:52 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Creating a chrome extension with rust + leptos -&gt; wasm</title><link>https://iism.org/article/ride-the-lightning-the-art-of-creative-motivation-63</link><author>/u/grok-battle</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 01:40:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>RustOwl - A new tool for visualizing Rust lifetimes</title><link>https://youtu.be/NV6Xo_el_2o</link><author>/u/zxyzyxz</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 01:20:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Did Prisma Choose Rust Initially?</title><link>https://youtu.be/1zSh0zYLTIE</link><author>/u/zxyzyxz</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 01:14:21 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Streamlined dataflow analysis code in rustc</title><link>https://nnethercote.github.io/2024/12/19/streamlined-dataflow-analysis-code-in-rustc.html</link><author>/u/nnethercote</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 22:28:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Resigning as Asahi Linux project lead [In part due to Linus leadership failure about Rust in Kernel]</title><link>https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/</link><author>/u/Wolfspaw</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 16:15:59 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them â€œjailbreaksâ€ these days) to allow users to run their own unofficial apps on the Nintendo Wii.I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other peopleâ€™s work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didnâ€™t attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which Iâ€™ve already spoken at length about and I wonâ€™t repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience.But then also came the entitled users. This time, it wasnâ€™t about stealing games, it was about features. â€œWhen is Thunderbolt coming?â€ â€œAsahi is useless to me until I can use monitors over USB-Câ€ â€œThe battery life sucks compared to macOSâ€ (nobody ever complained when compared to x86 laptopsâ€¦) â€œI canâ€™t even check my CPU temperatureâ€ (yes, I seriously got that one). (Edit: This wasnâ€™t just a few instances; Iâ€™ve seen variations on the first three posted hundreds of times by now, including takes like â€œThunderbolt/DP Alt are never going to happenâ€. A few times is fine, but the same thing repeated over and over again every day while weâ€™re trying to make these things happen will get to anyone.)And, of course, â€œWhen is M3/M4 support coming?â€For a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were â€œalphaâ€ and â€œunstableâ€ and â€œnot suitable for a daily driverâ€ (despite thousands of users, myself included, daily driving it and even using it for servers).No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I wonâ€™t go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).I did make some progress in 2024, but this left me in a very vulnerable position. I hadnâ€™t gotten nearly as much Asahi work done as Iâ€™d liked, and the users werenâ€™t getting any quieter about demanding more features and machine support.We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. â€œFullâ€ DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasnâ€™t the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did try to bring the subject up on the mailing lists, but it didnâ€™t get much response).The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I wonâ€™t repeat them in detail here. Suffice it to say, I consider Linusâ€™ handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.As you know, this is deeply personal to me, as weâ€™ve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.Some might be wondering why we canâ€™t just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But thereâ€™s more.In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (thatâ€™s how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a partial workaround for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.Iâ€™m not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated â€œWe
are the â€˜thin blue lineâ€™â€, and nobody cared, which just further confirmed to me that I donâ€™t want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to quit.But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than bunny.net giving us free CDN credits for the hosting).I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say Iâ€™m the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they wonâ€™t touch the Linux kernel with a 10-foot pole.When Apple released the M1, Linus Torvalds wished it could run Linux, but didnâ€™t have much hope it would ever happen. We made it happen, and Linux 5.19 was released from an M2 MacBook Air running Asahi Linux. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him an invitation to discuss the challenges of kernel contributions and maintenance and see how we could help. He never replied.Back in 2011, Con Kolivas left the Linux kernel community. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if youâ€™re not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but itâ€™s not fun any more. I have an M3 Pro in a box and I havenâ€™t even turned it on yet. I dread doing the bring-up work. It doesnâ€™t feel worth the trouble.I miss having free time where I can relax and not worry about the features we havenâ€™t shipped yet. I miss making music. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we havenâ€™t upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.Iâ€™m resigning as lead of the Asahi Linux project, effective immediately. The project will continue on without me, and Iâ€™m working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the Asahi Linux OpenCollective (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).I want to thank the entire Asahi Linux team, without whom I wouldâ€™ve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: marcan@marcan.st.: A lot of the discussion around this post and the interactions that led to it brings up the term â€œbrigadingâ€. Please read this excellent Fedi post for a discussion of what is and isnâ€™t brigading.]]></content:encoded></item><item><title>A new tool for visualizing Rust lifetimes</title><link>https://www.youtube.com/watch?v=NV6Xo_el_2o</link><author>Let&apos;s Get Rusty</author><category>dev</category><category>rust</category><enclosure url="https://www.youtube.com/v/NV6Xo_el_2o?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 15:00:34 +0000</pubDate><source url="https://www.youtube.com/channel/UCSp-OaMpsO8K0KkOqyBl7_w">Dev - Let&apos;s get Rusty</source><content:encoded><![CDATA[See how RustOwl can help you understand lifetimes in a real Rust codebase. A brand-new tool designed to visualize Rust lifetimes and make learning Rust easier. Check it out and see how it can change the way you write Rust!

Free Rust training: https://letsgetrusty.com/bootcamp

RustOwl: https://github.com/cordx56/rustowl

Corrections:
- Bacon is a CLI tool, not a library. Check it out here: https://github.com/Canop/bacon]]></content:encoded></item><item><title>Introducing cargo-warloc - smart LOC counter for your rust projects</title><link>https://www.reddit.com/r/rust/comments/1iok2to/introducing_cargowarloc_smart_loc_counter_for/</link><author>/u/DoItYourselfMate</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 14:21:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Let's be honest, we all love to measure... things. There are many tools that count lines of code, comments and blank lines. `cargo-warloc` lets you measure even more things. You can measure your things and then compare them to others' things, and feel good about yourself.To be more precise, it lets you know how many of your LOCs are actually code, and how many are tests or examples. And when you start feeling bad about your project actually being not that beeg, it tells you that most of your comments are not simple comments but precious documentation!Here are the stats from `cargo` repository:File count: 1188 Type | Code | Blank | Doc comments | Comments | Total -------------|--------------|--------------|--------------|--------------|------------- Main | 82530 | 9682 | 12625 | 6220 | 111057 Tests | 144421 | 20538 | 588 | 10151 | 175698 Examples | 169 | 27 | 5 | 19 | 220 -------------|--------------|--------------|--------------|--------------|------------- | 227120 | 30247 | 13218 | 16390 | 286975 And here are the stats of the `rust` itself:File count: 41118 Type | Code | Blank | Doc comments | Comments | Total -------------|--------------|--------------|--------------|--------------|------------- Main | 2255088 | 301883 | 350361 | 143909 | 3051241 Tests | 1525119 | 275969 | 18950 | 184194 | 2004232 Examples | 14349 | 2586 | 950 | 1327 | 19212 -------------|--------------|--------------|--------------|--------------|------------- | 3794556 | 580438 | 370261 | 329430 | 5074685 Install it with `cargo install cargo-warloc` and measure your... things!]]></content:encoded></item><item><title>Game Bub: open-source FPGA retro emulation handheld (with ESP32 firmware written in Rust)</title><link>https://eli.lipsitz.net/posts/introducing-gamebub/</link><author>/u/kibwen</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 13:26:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Iâ€™m excited to announce the project Iâ€™ve been working on for the last year and a half: , an open-source FPGA based retro emulation handheld, with support for Game Boy, Game Boy Color, and Game Boy Advance games.Game Bub can play physical cartridges, as well as emulated cartridges using ROM files loaded from a microSD card. Game Bub also supports the Game Link Cable in both GB and GBA modes for multiplayer games. I designed the hardware with a number of bonus features, like video out (HDMI) via a custom dock, a rumble motor, real-time clock (for certain games). Additionally, the hardware is designed with extensibility in mind, allowing future software improvements to expand its capabilities.Game Bub has a custom-designed 6 layer PCB featuring a Xilinx XC7A100T FPGA with integrated memory,  display, speakers, rechargable battery, GB/GBA cartridge slot, all packaged up in a custom 3D-printed enclosure.Check out the instructions, code, and design files on GitHub. Note that building a Game Bub unit is fairly complex. If you might be interested in buying a complete Game Bub kit, please fill out this form to help me gauge interest.I had a lot of fun implementing a Game Boy at the hardware level, and I started thinking about how far I could take the project. I was using a Pynq-Z2 development board, which was definitely the right way to get started, but it came with a lot of limitations.I had to use an external monitor for audio/video, and an external gamepad for input, but a real Game Boy, of course, is a portable handheld. I also wanted to add Game Boy Advance support, but the memory architecture of the Pynq-Z2 had access latency that was just barely acceptable for the Game Boy, and would have been completely unacceptable for the Game Boy Advance. I also wanted to make something less â€œhackyâ€: a real device that I could play and give to people, not just a bare PCB.Furthermore, while there are open-source FPGA retrogaming projects (e.g. MiSTer), there doesnâ€™t appear to be anything open-source that supports physical Game Boy and Game Boy Advance cartridges, let alone an open-source handheld device.Thus, I somewhat naively set out to design what would become by far my most complex electrical engineering and hardware design project to date.I set out some goals for the project:Build a standalone, rechargable battery-powered FPGA handheldMinimize cost and complexity by using off-the-shelf components wherever possibleCapable of playing Game Boy, Game Boy Color, and Game Boy Advance gamesCapable of using physical cartridges, or emulating cartridges (reading ROM files off of a microSD card)Easy to use: graphical menu and in-game overlayIntegrated display and speakers, with headphone supportIntegrated peripherals (rumble, real-time clock, accelerometer) for emulated cartridgesHDMI video output support for playing on a big screenDecent looking design with good ergonomicsExpansion opportunities in the future: support for more systems, Wi-Fi, etc.And finally, since I was building this project for fun and learning, I wanted to be able to fully understand every single component of the system. I wanted to use my own emulator cores (e.g. not just port them from MiSTer), do my own board design, and write my own drivers to interface with peripherals.A brief rant about FPGA retrogaming#Thereâ€™s a lot of misleading marketing and hype out there around FPGA retrogaming. Some claim that FPGA retrogaming devices are not emulators (because they supposedly â€œact like [the system] at the gate levelâ€), that they achieve â€œperfect accuracyâ€, or that theyâ€™re superior to software emulators.In my opinion, this is blatantly wrong and actively harmful. FPGA retrogaming devices are emulators: they pretend to be something theyâ€™re not. And theyâ€™re only as accurate as theyâ€™re programmed to be, since theyâ€™re recreations. An FPGA can make certain aspects of accuracy easier to achieve, but it doesnâ€™t guarantee it.Software emulators can be extremely accurate. Furthermore, perfect accuracy (if itâ€™s even possible) is by no means a requirement to play an entire systemâ€™s library of games. Some people claim that FPGA emulators are the only way to â€œpreserveâ€ a system, but Iâ€™d argue that software emulators are a significantly more accessible (no special hardware needed!) way to further this goal.I believe that FPGA emulators have only one real advantage over software emulators: they can more easily interface with original hardware, such as physical cartridges or other consoles via link cables.I did this project not because I think that FPGA emulators are inherently better than software emulators, but because I think theyâ€™re interesting and fun to build.I began work on the project by doing some initial research and sketching out a high level design.My previous FPGA emulator project used a Xilinx Zynq chip, which integrates FPGA fabric (â€œPLâ€) with a dual-core ARM processor running Linux (â€œPSâ€). I implemented the entire emulator on the FPGA, and used the Linux system to configure the FPGA, render the UI, and load ROM files from the filesystem.I decided to keep this same division of responsibilities: using the FPGA to do the core emulation, with a separate processor to do support tasks. However, to make the overall design easier to reason about, I decided to to use an FPGA-only chip (without any hard processor cores), and an external microcontroller (MCU) to do the tasks that the ARM cores did before.The FPGA would consume input, directly interface to the game cartridges (through level shifters to support both the 3.3 volt GBA and 5 volt Game Boy), and output audio and video to the speakers and display. The MCU would handle the UI, read ROM files from the microSD card, initialize peripherals (display, DAC, IMU), handle power sequencing, and load the FPGA configuration.I wanted to have Wi-Fi and Bluetooth support: Wi-Fi for software updates, and the possibility of emulating the Game Boy Advance Wireless Adapter, and Bluetooth to support wireless game controllers (when connected to an external display). To reduce complexity (and avoid the need for careful RF design), I looked only for complete Wi-Fi/Bluetooth modules with integrated antennas.An early block diagram I sketched outI also drew out rough sketches of what the final device might look like: placement of buttons, screen, speakers, ports, cartridge slot, and battery. I settled on a vertical Game Boy Color-esque design (as opposed to a horizontal Game Boy Advance-style design), because I felt that this would maximize the space in the back of the device for full-size Game Boy Color cartridges and a battery.After sketching out the goals and high level design, I started component selection: picking out each non-trivial component of the system, evaluating features and requirements (e.g. how they communicate, power consumption and voltages needed).Since I intended to have this manufactured and assembled at JLCPCB, I strongly preferred parts that were available in their part library. One technique I even used for narrowing down part choices was finding the relevant category in their part search, and sorting by their stock count.I initially planned to use an RP2040 microcontroller, with a separate ESP32-WROOM module to support Wi-Fi and Bluetooth.The ESP32 supports both Bluetooth Classic and LE, which is essential for supporting a wide range of controllers, and the RP2040 has USB host support, to support wired controllers.During the schematic design process, I ended up simplifying the RP2040 + ESP32 combination to just a single ESP32-S3 module for a few reasons:I started running out of GPIOs on the RP2040, and I was dedicating 4 of them (2 for UART, 1 for reset, 1 for booting in firmware download mode) to communication with the ESP32. Plus, the ESP32-S3 has more GPIOs overall.I wanted to write the MCU firmware in Rust, and the ESP32-S3 had support for the Rust standard library (via ESP-IDF and esp-idf-hal). This seemed like it would be easier to get the software up and running.Fewer components means easier routing and assemblyThe ESP32-S3 has an SDIO module (for interfacing with the microSD card), and FAT filesystem support (via ESP-IDF). It would be possible to do this with the RP2040 PIO, but having a proper peripheral and driver for this makes it a lot easier.The ESP32-S3 is more powerful than the RP2040, and would probably be able to render a smoother UI.However, the ESP32-S3 has one main disadvantage compared to the original ESP32: it doesnâ€™t have Bluetooth Classic support, only LE. This would greatly limit the range of supported wireless controllers, but I believed the compromise was worth it. I also decided to scrap USB host support, because supporting USB-C dual role (switchable device or host) would have added a lot of additional complexity.If the RP2350 microcontroller (the successor to the RP2040) had been available when I started this project, I may very well have chosen it, since it has even more power, PIO blocks, memory, and GPIO pins. I might have paired it with an RM2 radio module for Wi-Fi and Bluetooth.I wanted a display that would support integer scaling for the Game Boy Advance, which has a 240x160 pixel screen. I was also looking for a screen roughly on the order of 3.0-3.5 inches wide (diagonal), to be comfortable to hold in the hand.I found the ER-TFT035IPS-6 LCD module from EastRising, with a 3.5 inch display, and a 320x480 pixel resolution. This allows for a 2x integer scale for the Game Boy Advance (and a 2x scale plus centering for the 160x144 Game Boy display). This checked off almost all of the boxes: integer scaling, a good size, available at a reasonable price, pretty good documentation (for the ILI9488 LCD controller).ER-TFT035IPS-6 LCD moduleThe main issue, which actually ended up being fairly annoying, is that itâ€™s a 320x480 display, not 480x320. Meaning, itâ€™s oriented in portrait mode, not landscape. I rotated the device 90 degrees to fit in a landscape orientation, but this created two issues:In landscape orientation, the bottom of the display (containing the LCD driver chip and the flex cable) faces to the left or the right, which means that larger bazels are required on the left and right of the display to center the â€œactive areaâ€ of the LCD within the handheld.In landscape orientation, the display refreshes from left to right, not top to bottom.The problem with refreshing from left to right is that the Game Boy and Game Boy Advance (and almost every other system) refresh from top to bottom. This means that the display canâ€™t be refreshed perfectly in sync with the game (zero buffering), and single buffering leads to unsightly diagonal tearing. Instead, I had to use triple buffering, where the game is writing to one framebuffer, the LCD driver is reading from another buffer, and thereâ€™s one spare swap buffer. This increases the amount of memory used â€“ and because it needed to be accessed by both the game and LCD driver simultaneously (dual port), it needed to be stored in internal block RAM in the FPGA, a scarce resource.So, even though the Game Boy emulator uses <10% of the total logic resources of the FPGA, and the Game Boy Advance uses around 30%, I had to use a large (more expensive, and power hungry) FPGA so that I had enough block RAM.I also stuck a standard size HDMI port into the design, connected directly to the FPGA. HDMI has a few additional, non-video signals that need level shifting from 5V to 3.3V (I opted for discrete transistors), and it requires the source (me!) to supply a small amount of power.I had never previously designed anything that used a lithium ion battery, so I had a fair amount of learning to do. Adafruit was a helpful resource. I needed a way to charge the battery from USB power, and a way to measure how charged it is.Lithium ion batteries can be dangerous if misused. Safely charging a battery is non-trivial, and requires a feedback loop and adjustable voltage sources. A dedicated IC seemed like the best way to do this. A lot of hobbyists use the ultra-cheap TP4056 1A battery charger, but Iâ€™d read about a lot of issues it has around safely charging the battery while using it. I decided instead to opt for the TI BQ2407x series of battery charger ICs. They seem to be widely used in commercial products, came with a comprehensive datasheet, and had a few critical features: programmable input and charge current limits, safety timers, and â€œpower path managementâ€ for safely charging the battery while the device is on.Typical discharge curve for a 3.7V lipo battery (source: Adafruit)There are a few ways to measure the charge level of the battery, which generally relies on the fact that a lithium ion batteryâ€™s voltage depends on its charge level. A fully charged battery is about 4.2 volts, a battery with between 80% and 20% charge is about 3.7 volts, and below that a drained battery falls off pretty quickly to under 3.0 volts. If all you want is a coarse estimate of the battery level, you can use an ADC to read the voltage and estimate whether the battery is fully charged or nearly discharged. However, since the voltage curve is nearly flat between 20% and 80% charge (and is also dependent on the load), this canâ€™t give the fine-grained battery percentage that weâ€™re used to on phones and laptops. Instead, I opted for a discrete fuel gauge IC, the MAX17048. Itâ€™s simple to integrate and inexpensive.I decided to use a push button for the main power switch, because I needed to be able to do a graceful shutdown, where the microcontroller could save state (e.g. the current save file for an emulated cartridge) before it actually powered off.I briefly considered using an ultra-low power, always on microcontroller to act as a custom PMIC to provide power switch functionality (and perhaps avoid the need for a separate real-time clock IC, and even a battery gauge). While this would have been flexible and really cool, I figured it wasnâ€™t worth the additional complexity.The main system power ranges from about 3.4 V when the battery is discharged, to 4.2 V when the battery is fully charged, up to 5.0 V when the device is plugged in with USB.The ESP32-S3 module required 3.3 V, and most of the other ICs in the system did too. The main exception is the FPGA, which requires a 1.0 V core power rail, a 1.8 V â€œauxiliaryâ€ power rail, and a 3.3 V power rail for I/O. Moreover, according to the Xilinx Artix-7 datasheet (DS181), these power rails need to be powered on in a particular sequence: for my use, this means 1.0 V, then 1.8 V, then 3.3 V. Additionally, I needed a 5.0 V supply to interface with Game Boy / Game Boy Color cartridges.There are multi-rail power regulators available, and a lot of FPGA development boards use them. However, they all seemed to be expensive and difficult to purchase in low quantities. Instead, I opted for separate power regulators for each rail. I used buck converters instead of linear regulators to maximize power efficiency.I used the TLV62585 converter for the 3.3 V, 1.8 V, and 1.0 V rails. This is a simple, performant buck converter with a â€œpower goodâ€ output, which is useful for power sequencing: you can connect the  output of one regulator to the  pin of the next regulator, to power on the rails in the desired order.For the 5.0 V rail, I used the TPS61022 boost converter. This converter is way overkill for the 5.0 V rail (which might use 75mA ), but it was readily available, and conveniently compatible with the same 1ÂµH inductor as the buck converters.According to the FPGA datasheet, the XC7A100T consumes more than 100mW of static power. That is, it consumes that as long as itâ€™s connected to power, even if itâ€™s doing absolutely nothing. I figured I might want to support a low power sleep mode, so I decided to split the FPGA into a separate power domain with an explicit power enable signal from the MCU. I also used an AP2191W load switch for the FPGAâ€™s 3.3 V rail to be able to keep the 1.0 V â†’ 1.8 V â†’ 3.3 V sequencing.I wanted the device to have both speakers and a 3.5mm headphone jack. Ultimately, the FPGA generates an I2S digital audio signal, and I needed a DAC to convert it to an analog audio signal, and then an amplifier to drive the speakers (or headphones). I wanted digital volume control (to support volume buttons, rather than a volume knob or slider), and I needed some way to switch the audio output between speakers and the headphones, depending on whether or not headphones are plugged in. With no real audio experience, this seemed like a daunting task.While searching for multiple separate components, I stumbled upon the TLV320DAC3101. It combines a stereo DAC with a speaker amplifier and a headphone driver. Additionally, it supports digital volume control, and headphone detection. I think this chip is a good example of how thoughtful component selection can simplify the overall design. Looking through the datasheet, it required a 1.8 V core voltage (unlike essentially every other component other than the FPGA) and a fair amount of configuration registers to set over I2C, but it had all of the features I needed.I was originally planning to have just a single (mono) speaker, but I figured if I had a stereo DAC, I might as well put two in there. I chose the CES-20134-088PMB, an enclosed microspeaker with a JST-SH connector. Having an enclosed speaker simplified audio design, because as it turns out, you canâ€™t just stick a speaker to a board and expect it to sound okay (Same Sky, the manufacturer of that speaker, has a blog post explaining some of the nuances).I prefer the feeling of clicky, tactile buttons (such as those found in the GBA SP, Nintendo DS (original), Nintendo 3DS, Switch) compared to â€œmushyâ€ membrane buttons (such as those found in the Game Boy Color, original GBA, and Nintendo DS Lite). I learned that the tactile switches used in the GBA SP are a widely available off-the-shelf part from Alps Alpine. I used similar, but smaller buttons for the Start/Select/Home buttons, and a right-angle button from the same manufacturer for side volume and power buttons.Although I only had plans to support Game Boy and Game Boy Advance (requiring a D-pad, A and B buttons, L and R shoulder buttons, and Start/Select), I opted to add two more â€œXâ€ and â€œYâ€ face buttons to leave the possibility open of supporting more systems in the future.The L and R buttons posed an additional challenge â€“ I found numerous right-angle tactile buttons (to be soldered onto the back, facing towards the top). However, none of them seemed to have the actuator (the part of the button you make contact with) far enough away from the PCB to be easily pressed. At first, I thought about making a separate shoulder button board to move them at the correct distance, but then I started looking at what existing devices do for inspiration. The Game Boy Advance SP actually uses a more complex mechanism for the shoulder buttons: rather than a simple actuator like the face buttons, thereâ€™s a hinge with a torsion spring that hits the actuator at an angle. This is actually part of what makes the shoulder buttons pleasant to press: you donâ€™t need to hit them from exactly the right direction, because they pivot. I ended up just going with a standard right-angle tactile button, opting to solve the problem with the mechanism in the enclosure.GBA SP shoulder button mechanismOne of my main goals was to allow ROM files to be loaded from a microSD card, rather than only being able to be played from a physical cartridge. To do this, Iâ€™d need dedicated RAM for the FPGA to hold the game. Game Boy Advance games, typically, are a maximum of 32 MB. They donâ€™t make SRAMs that large (and if they did, theyâ€™d be very expensive). Instead, I needed to use DRAM.Asynchronous SRAM is very simple: supply a read address to the address pins, and some amount of nanoseconds later, the data youâ€™re reading appears on the data pins. DRAM is more complex: the simplest kind is â€œsingle data rate synchronous DRAMâ€ (SDR SDRAM, or just SDRAM, distinguishing it from the significantly more complex DDR SDRAM). However, even SDRAM is non-trivial to use. DRAM is organized into banks, rows, and columns, and accessing DRAM requires sending commands to â€œactivateâ€ (open) a row before reading out â€œcolumnsâ€, and then â€œprechargingâ€ (closing) a row. Handling all of this requires a DRAM controller (see this simple description of the state machine required). This isnâ€™t terribly complex, but I was signing myself up for more work.Alternatively, I could have chosen a PSRAM chip (essentially DRAM with an integrated controller to make it have a more SRAM-like interface). However, I couldnâ€™t find a PSRAM part that I was happy with (cost, availability, interface), and so I ended up going with the inexpensive W9825G6KH 32MB 16-bit SDRAM.I also decided to stick a 512 KiB SRAM chip in the design in case I ended up needing some more simple memory later, like for emulating the SRAM used for Game Boy cartridge save files. Despite being 1/64 the capacity, this chip was about 3x the cost of the SDRAM. This ended up being a wise decision, since a lot of my internal FPGA block ram was eaten up by the triple buffer for the display (see above).Cartridge and Link Ports#The cartridge slot and link ports are no-name parts from AliExpress, easily available for cheap. These seem to mostly be GBA SP compatible, and are often used as repair parts.The Game Boy Advance can play both Game Boy [Color] and Game Boy Advance games. These run at different voltages and use different protocols, so the device needed some way of determining which type of cartridge is inserted.GBA cartridge (top) vs GB cartridge (bottom)The cartridges are physically different at the bottom: GBA cartridges (the top cartridge in the image) have a notch on either side. The GBA has a  that senses the absence of a notch on an inserted cartridge and switches the device into Game Boy Color mode.I measured the size and position of this notch, and searched Digi-Key and Mouser for switches that met these constraints. In the end, I was only able to find a single switch that would work.Miscellaneous peripherals#I used the surprisingly cheap LSM6DS3TR-C IMU from ST. This tiny IMU has a 3-axis accelerometer and gyroscope, more than sufficient for emulating the few GB/GBA cartridges that have motion controls.For keeping track of time even when the device was off, I used the PCF8563T real-time clock chip. I chose this because it was 1) I2C (no additional pins required), 2) cheap, and 3) readily available from JLCPCB. Interestingly, all of the real-time clock chips I found count in seconds/minutes/hours/days/months/years. This makes sense for a really simple device with minimal computational power. However, itâ€™s annoying for my purposes, since all I really want is a timestamp I can pass to some other datetime library, and converting between the calendar time and a unix timestamp is non-trivial due to how the chips incompletely handle leap years.I picked up a few cheap coin vibration motors to use for vibration support (for the rare cartridge that had a built-in vibration motor).I also used a TCA9535 I2C I/O expander to connect the face buttons to the MCU. I ran out of pins, and while I  have used the FPGA as a sort of I/O expander, I figured Iâ€™d make it simpler for myself (and allow the buttons to be used even if the FPGA was powered off) by letting the MCU read them itself.For this project, as with my previous ones, I used KiCad to create my schematic and do PCB layout. I really canâ€™t recommend KiCad enough: itâ€™s a great program, intuitive to use, and itâ€™s free and open source.This was a very ambitious project for my level of electrical engineering experience, and creating the schematic took a couple of weeks. I spent a lot of time designing the circuit for each component, because I was afraid Iâ€™d do something wrong and end up with a stack of useless boards without the skills needed to debug them. A lot of the component selection actually happened in parallel with schematic design, as I found new requirements or problems and had to change components.I gained a lot of experience reading component datasheets. Itâ€™s a really valuable skill, both for component selection and for creating designs that use the components. Nearly every datasheet has a â€œtypical applicationâ€ section, where the manufacturer shows how the component would fit into a circuit. At minimum, this has power supply information (e.g. these voltages to these pins with these decoupling capacitors). For more complex components like the DAC, it also has information about power sequencing, different ways the device could be connected to the rest of the system, a register list, that sort of thing. Some components also included PCB layout recommendations. This information was all really helpful, and gave me a good deal of confidence that my board would work as long as I read through the datasheet and followed the manufacturerâ€™s recommendations.Then I got to the FPGA. Nearly every component has a single datasheet. Some of them have an additional application note or two. Particularly complex chips (like the ESP32-S3 microcontroller) have a separate datasheet, reference manual, and hardware design guide. The Xilinx Series 7 FPGAs have . Overviews, packaging and pinout, configuration guides, BGA design rules, power specifications, clocking resources, I/O specifications, PCB layout guides, design checklistsâ€¦ even a 4MB Excel spreadsheet for estimating power consumption! And believe me, Xilinx didnâ€™t just write documentation for fun: thereâ€™s so much documentation because the chip  this much documentation.Designing with the FPGA was overwhelming, and  beyond my experience level. At several points I genuinely considered dropping the project altogether. Fortunately, I persevered, and gradually internalized a lot of the information. I also read through the schematics of any open-source Artix-7 development board I could get my hands on. Seeing what other people were doing gave me more confidence that I was doing the right thing.Eventually, after I placed all of the components, connected them, ensured all of the nets were labeled, and ran KiCadâ€™s electrical rules checker (ERC) to find obvious mistakes, I moved on to layout.I did PCB layout at the same time as some of the initial enclosure CAD. The mechanics of how everything fit together influenced the placement of the display connector, cartridge slot, buttons, speakers, and connectors. After I came up with a plausible enclosure design, I placed some of the first key components onto the PCB and locked them into place while I did the rest of the routing.Rough enclosure design to help with board layoutI first focused on components that would be hardest to route. Primarily, the FPGA: the package I was using (CSG324) is a BGA, 18x18 with 0.8mm pitch between pins. â€œFanning outâ€ all of the I/O signals requires careful routing, and at 0.8mm pitch, itâ€™s difficult to do this routing with cheap PCB manufacturing techniques. I ended up being able to do this routing with a 6-layer PCB (three signal, two ground, one power), with 0.1mm track width and spacing, and 0.4/0.25mm vias. Fortunately, this is all within the realm of JLCPCBâ€™s capabilities.BGA fanout with thin traces and small viasAs I routed signals out from the FPGA to other parts, I assigned those signals to the FPGA pins. Similarly, with the MCU, I assigned signals to pins in a way that made routing easier. Certain signals had restrictions (e.g. on the FPGA, the main 50 MHz clock signal can only go into certain pins, or the configuration bitstream can only go to certain pins, or certain pins are differential pairs for HDMI output), but overall, I had a lot of flexibility with pin assignment.KiCad has a feature where it automatically backs up your project as you work on it. I changed the settings to save every 5 minutes and not delete old backups, which allowed me to generate this timelapse of my layout process:Revision 1 board layout timelapseOnce I finished placing and routing all of the components, I ran the design rules checker (DRC) and fixed issues. I hesitated for a while before sending the PCB for manufacturing. I re-read the schematics, reviewed the layout, and eventually felt confident enough that I was done. I submitted the order to JLCPCB, and after a few questions by their engineers about component placement, they started manufacturing it.Board testing and bring-up#After two weeks or so, I received the assembled boards in the mail:An assembled board and an unassembled boardFirst, I probed the power rail test points with a multimeter to check for shorts. Then, I plugged the boards in for the first time, and pressed the power button. To my delight, the green LED turned on, indicating that the power button circuit, power path, and 3.3V regulator worked. The microcontroller USB enumerated, and I could see that it logged some errors (since I hadnâ€™t flashed anything to it yet).I intended to write the MCU firmware in Rust, but I did initial board testing and bring-up with MicroPython. This would let me interactively type in Python and write basic scripts to communicate with the peripherals on the board and make sure I had connected everything correctly. I didnâ€™t have to worry about writing efficient or well-organized code, and could just focus on functionality.I flashed the MicroPython firmware image, and wrote a couple lines of Python to blink the LED. I powered on the FPGA power domain, and checked that the , , and  rails had the correct voltage.Next, I wrote a simple bitstream for the FPGA that read the state of the buttons and produced a pattern on the shared signals between the FPGA and the MCU. I wrote simple Python code to configure the FPGA, loaded up the bitstream, and polled the signals from the FPGA. Pressing buttons changed the state, and confirmed that the FPGA was properly powered, and configurable from the MCU.After I confirmed the FPGA worked, I started writing a simple display driver to initialize the LCD and push some pixels from the MCU over SPI. The initialization sequence uses a number of LCD-specific parameters (voltages, gamma correction, etc.), that I learned from the LCD manufacturerâ€™s example code.(Slowly) pushing pixels to the LCDThe LCD moduleâ€™s controller, an ILI9488, has a few quirks: despite claiming that it supports 16-bit colors over SPI, it actually only supports 18-bit colors. This unfortunately meant that the MCUâ€™s LCD driver would be more inefficient than I expected, since it has to expand 16-bit colors to 18-bit before sending them over the bus. This didnâ€™t end up being a huge issue, however, because the FPGA is the one driving the display most of the time.Another quirk (hardware bug?) is that the ILI9488 doesnâ€™t stop driving its SPI output line, even when its chip-select signal is inactive. This means that the chip will interfere with any other communication on the busâ€¦ including the FPGA, which sits on the same bus. I never actually needed to read any data back from the LCD (and even if I did, it supports three-wire SPI), so I just cut the trace between the LCDâ€™s SDO line and the SPI bus.Debugging the LCD test codeTrouble with power domains#I started trying to communicate with the I2C peripherals (I/O expander, RTC, etc.), and found that nothing was responding. A bit of probing with a logic analyzer revealed that the SCL/SDA lines were being held low, and that powering on the FPGA power domain let the lines be pulled high and communication to happen.I deduced that this was due to the DAC, which had its IOVDD powered by , which likely caused its protection diodes to pull the IO lines (SCL and SDA) low:The problematic portion of the schematicI tested out this theory by cutting the PCB traces connecting the DACâ€™s IOVDD and  with a knife. After this, I2C worked even with the FPGA power disabled. Then, I tested a possible fix by adding a wire to power the DACâ€™s IOVDD from the  rail. I confirmed that I could still talk to the other I2C devices, and once enabling FPGA power, that I could talk to the DAC too.While bringing up the LCD, I saw that the FPGA was also pulling down the shared SPI bus lines while it was unpowered. Not enough to prevent communication with the LCD, but it still wasnâ€™t great. Between this and the DAC issue, I learned an important EE lesson: be careful when connecting components in different power domains together. A tristate buffer, such as the 74LVC1G125, could have helped here to isolate the buses.Once I2C was working, I wrote some basic driver code for the fuel gauge, real-time clock, IMU, and I/O expander, just to check that they all worked correctly. I also checked that the MCU could read from and write to the attached microSD card.Audio and video output from the FPGA#Next, I updated my testing FPGA bitstream to output a test pattern over the LCD parallel interface (â€œDPIâ€), and a test tone to the DAC over the I2S interface. Then, I began poking on the MCU side to configure the LCD controller and DAC appropriately.With some amount of trial and error, I convinced the LCD to accept input from the FPGA. Most of the trial and error revolved around the rotation of the LCD module. Soon after, I configured the DAC properly, and it played the test tone from the FPGA over the speakers and the headphones.WIP video output from the FPGAAt this point, much of the board was working, so I soldered on the rest of the components (cartridge slot, cartridge switch, link port, shoulder buttons).With the cartridge slot in place, I had everything I needed to port over the Game Boy emulator from my last project. I did a quick-and-dirty port of the emulator, with some hacking around to connect the core to the audio, video, and the physical cartridge. I was able to play the first Game Boy game on the device far sooner than I was expecting:Pokemon Silver running from cartridgeI spent the next month or so implementing things on the FPGA. I started on the SPI receiver implementation, so that the MCU and FPGA could communicate.It was relatively straightforward to write the initial version, which 4x oversampled the SPI signals from the main system clock. For the Game Boy, that was ~8 MHz, for a maximum SPI speed of 2 MHz. The MicroPython ESP32-S3 SPI implementation supported only single SPI, so that allowed for a maximum transfer speed of 256 KB/s. This was sufficient to do most of my initial testing, but I later wrote an improved SPI receiver to run with an internal 200 MHz clock (from a PLL that turned on and off with the chip-select signal to save power), communicating with the rest of the system via a pair of FIFOs. This added a lot of complexity and edge cases, but it greatly improved performance, allowing the bus to run at 40 MHz.I wrote the SPI interface to the FPGA with memory-like semantics: each SPI transfer starts with a command byte, encoding whether itâ€™s a read or write transfer, the size of each word in the transfer (8, 16, or 32 bits), and whether the â€œtarget addressâ€ should autoincrement as the transfer progresses. Then, a 32-bit address, followed by reading or writing the data. Each thing that the MCU might want to access (control registers, blocks of memory) are mapped into the 32-bit address space.As with my previous FPGA project, I wrote almost all of the FPGA code in Chisel, a Scala-based HDL. The remaining bits were the top-level Verilog. Chisel made it really simple to parametrize, compose, and test the various modules that I wrote.Once I had the SPI receiver working, I wrote controllers for the on-board SRAM and SDRAM. The SRAM was relatively simple (although I still got it slightly wrong at first). The SDRAM was a bit tricky, and even as I write this Iâ€™m not quite satisfied with its performance, and intend to rewrite it in the future.I exposed the SRAM and SDRAM interfaces to the MCU via SPI, which allowed me to read and write to these pieces of memory from the MCU. I used this a lot for testing: writing patterns into memory and reading them back to ensure that read and write both worked.Side note: SDRAM has to be continuously refreshed, otherwise the stored data decays over time. It depends on the chip, but typically each row has to be read and written back (or auto-refreshed, which does the same thing) at least once every 64 milliseconds to avoid losing state. What I found interesting, however, is that the data can actually persist for quite a bit longer. I discovered that when I was reconfiguring the FPGA between tests, most of the test data that I had previously written would still stick around even without being refreshed. In the first few seconds some bits would start flipping, and over the course of a few minutes, most of what was written was completely unintelligible.With the SDRAM controller and SPI receiver written, I was then able to implement the â€œemulated cartridgeâ€ part of the Game Boy emulator, where the MCU reads a ROM file off of the microSD card and sends it to the FPGA to be stored in SDRAM. Then, the FPGA â€œemulatesâ€ a cartridge (rather than interfacing with a real physical cartridge). After a few stupid mistakes, I was able to run test ROMs and homebrew. As an added bonus, since I was using my own SDRAM controller directly, I didnâ€™t have any of the performance issues Iâ€™d faced before when accessing the ROM stored in memory.Writing the microcontroller firmware in Rust#By this point I had tested, in some form or another, all of the different components of the system. Iâ€™m really surprised that everything worked in my first board revision â€“ even the rework I did early on wasnâ€™t actually required for functionality.I decided now was a good time to start building an interactive GUI. Up until this point, I had just been running commands in the MicroPython REPL. However, I didnâ€™t want to build a whole UI in Python just to throw it away later, so I also started working on the â€œproductionâ€ Rust firmware.In the last few years, a lot of progress has been made towards making Rust on the ESP32 chips work well, even on the chips that use the Xtensa ISA. I followed the Rust on ESP Book and quickly had an environment set up. I opted for the â€œRust with the Standard Libraryâ€ approach, so that I could benefit from ESP-IDF, especially the built-in support for USB and SD cards with the FAT filesystem.I started porting over the drivers I had written in Python. I found embedded Rust to be a bit verbose in some cases, but overall pleasant to use and worth the (little) trouble.I starting writing my own minimal GUI framework for basic menus. I poked around with the  library, but soon found that the typical patterns I was expecting to use werenâ€™t a great fit for Rust. I also started planning out different screens and realized that I probably actually wanted to use a more comprehensive UI framework.Ultimately, I settled on Slint, a Rust-native declarative GUI framework with excellent support for embedded devices. Slint has a custom DSL to describe the UI and composable components. After a bit of practice I found myself to be really productive with it. I enjoyed using Slint, and Iâ€™d use it again in the future. The authors are responsive on GitHub, and the project has steadily improved over the year or so that Iâ€™ve been using it.There were a few rough edges for my use case, however:The built-in GUI elements and examples were all heavily oriented around mouse or touchscreen navigation. Game Bub only has buttons for navigation, however, so I had to make my own widgets (buttons, lists) that worked with key navigation. This involved a few hacks, because Slintâ€™s focus handling was a little bit simplistic.The built-in GUI styles looked (in my opinion) bad on a low DPI screen. Text was excessively anti-aliased and hard to read at small sizes. This was also fixed by building my own widgets.Slint doesnâ€™t have a great story around supporting different â€œscreensâ€ â€“ I had to build some of my own infrastructure to be able to support navigation between the main menu, games, rom select, settings, etc.The GUI is rendered on the MCU, and then the rendered framebuffer is sent over to the FPGA. Slint supports partial rendering, where only the parts of the screen that have changed are updated, which improved performance. The FPGA maintains a copy of the framebuffer and ultimately is responsible for driving the display. This has a few advantages over driving the display directly from the MCU:Sending a framebuffer at 40 MHz QSPI to the FPGA is 16x faster than sending it to the LCD controller at 10 MHz (the fastest speed supported by the ILI9488)The UI is rendered at 240x160 to improve performance and maintain the GBA aesthetic, but the LCD controller doesnâ€™t have a scaler, so the MCU would have to send 4x the pixels. The FPGA can easily scale the UI framebuffer itself.The FPGA can composite the emulator output with a semi-transparent â€œoverlayâ€ to support an in-game menu, volume / brightness bars, battery notifications, etc.An external display (e.g. monitor or TV) can be driven by the FPGA via HDMII spent some time making a variety of firmware improvements, mostly polish and quality-of-life. I added a settings screen to set the date and time, whether to use Game Boy (DMG) or Game Boy Color (CGB) mode when playing Game Boy games, etc.Then I improved the ROM select file browser, and added a battery level indicator.I also got sick of having to take the microSD card out of the device and connect it to my computer through a series of adapters (microSD to SD to USB-A to USB-C), so I implemented a basic utility to expose the microSD card as a USB Mass Storage Device, using TinyUSB and the ESP32-S3â€™s USB-OTG capabilities.It was a little bit more difficult than I expected, because USB Mass Storage requires the device to provide raw block access. This means that the filesystem has to be unmounted by the device, otherwise the device and host could conflict and corrupt the filesystem. The ESP32-S3 also only supports USB Full Speed, for a practical maximum transfer speed of ~600KB/sec. Itâ€™s really useful for transferring save files or updating the FPGA bitstreams, but less useful for transferring a large number of ROM files.Later, I implemented MBC7 support in the Game Boy emulator for Kirby Tilt â€™n Tumble, using the on-board accelerometer.After I implemented a decent amount of software functionality, I decided to finish the enclosure design. The bare board just wasnâ€™t cutting it anymore, and the taped LCD module/loose speakers/rubber-banded battery contraption was fragile.Game Bub looking rough without an enclosureI came into this project without any CAD or 3D printing experience. I looked at a few different CAD software packages, and I ultimately settled on FreeCAD, primarily because it was free and open source. I learned how to use the software with some video tutorials. FreeCAD, unfortunately, was a little bit rough around the edges and I ended up running into some annoying issues. Nevertheless, I powered through and finished the design.FreeCAD view of the enclosure and some buttonsI found parametric modeling, where the geometry of the model is defined by constraints and dimensions, to be intuitive. However overall, I found 3D CAD to be very time consuming. I think a large part of this is my inexperience, but thinking in three dimensions is a lot more difficult than, say, a 2D PCB layout. Creating a full assembly was even more difficult: I had to visualize how the front and rear pieces would fit together, where the screws would go, and how the buttons, screen, speaker, cartridge slot, battery, and ports would all fit in. This project definitely pushed the boundaries of my (previously non-existent) product design skills.After finishing the design, I printed out the technical drawing at a 1:1 scale and physically placed the board and other components down as a final check. Then, I sent it to JLCPCB for manufacturing. I opted for SLA resin printing, for high precision and a smooth finish.Enclosure technical drawingAfter a couple weeks, I got the finished enclosure and custom buttons back.Front and rear half, outsideI put the buttons, speakers, and screen into the enclosure, screwed on the PCB, and put the whole thing together.Assembling the front sideGame Bub, fully assembled and functionalI wasnâ€™t sure how dimensionally accurate the 3D printing would be, so I added a lot of extra clearance around the buttons and ports. As it turned out, the printing was very precise, so the buttons rattled around a little in the oversized button holes.Itâ€™s a little bit chunky (smaller than an original Game Boy, though!) and the ergonomics arenâ€™t ideal, but I was really happy to finally have an enclosure. It actually started (sort of) looking like a real product, and I wasnâ€™t constantly worried about breaking it anymore.Game Boy Advance support#I wonâ€™t go into all of the details of how I wrote the emulator here (this article is already long enough!). If youâ€™re interested, my previous article about my Game Boy FPGA emulator goes into detail about the general process of writing an emulator, and for a high-level introduction to the Game Boy Advance (from a technical perspective), I recommend Rodrigo Copettiâ€™s article. In general, I tried to implement the emulator the way it might actually have been implemented in the original hardware: each cycle of the FPGA corresponds to one actual hardware cycle (no cheating!).As with the Game Boy, I did nearly all of my development with a simulator backed by Verilator and SDL. By the end of the development process, the simulator was running at about 8% of the real-time speed (on an M3 MacBook Air with excellent single-core performance), which was a bit painful.The Game Boy Advance CPU, the ARM7TDMI, is significantly more complicated than the Game Boyâ€™s SM83 (a Z80 / 8080-ish hybrid). However, in some ways, it was easier to understand and implement: the ARM7TDMI is much closer to a simple modern processor architecture, and itâ€™s extensively documented by ARM. For example, the ARM7TDMI Technical Reference Manual has block diagrams and detailed cycle-by-cycle instruction timing descriptions.I had a lot of fun implementing the CPU. The architecture has a three-stage pipeline (fetch, decode, execute) â€“ a division that feels natural when you implement it in hardware. The ARM7TDMI has two instruction sets: the standard 32-bit ARM instruction set, and the compressed 16-bit THUMB instruction set. I implemented the CPU the way it works in hardware, where the only difference between ARM and THUMB is the decode stage.As I was implementing the CPU, I wrote test cases for each instruction. Each test checks the functionality of the instruction: processor state, register values after, as well as the cycle-by-cycle behavior and interaction with the memory bus. This was helpful for catching regressions as I implemented more and more control logic. It was also really satisfying to be able to implement individual instructions, then write the tests, and check that everything worked.Chisel made it easy to write out the CPU control logic. The CPU control logic is a state machine that generates microarchitectural control signals (e.g. bus A should hold the value from the first read register, bus B should hold an immediate value, the memory unit should start fetching the computed address, etc.). Chisel allowed me to collect common functionality into functions (e.g.  to set up the signals to dispatch the next decoded instruction, or  to signal that the pipeline should be flushed and a new instruction should be fetched from the current program counter).I found it helpful to draw out timing diagrams with WaveDrom when working through instructions, especially to deal with the pipelined memory bus.My timing diagram of the ARM7TDMI branch instructionsBy mid-May (about a month later), I finished the CPU implementation (with occasional bug fixes after) and moved onto the rest of the system.PPU, MMIO, and everything else#Over the next month and a half, I implemented the majority of the rest of the Game Boy Advance. The CPU interacts with the rest of the system via memory-mapped IO (MMIO) registers. Unlike the Game Boy CPU, which can only access memory a single byte at a time, the ARM7TDMI can make 8-bit, 16-bit, and 32-bit accesses. This complicates MMIO, and the different hardware registers and memory regions in the GBA respond to different access widths in different ways.I started with the Picture Processing Unit (PPU), which produces the video output. The author of NanoBoyAdvance, fleroviux, had helpfully documented the PPU VRAM access patterns, which gave a lot of insight into how the PPU might work internally. Tonc was also immensely helpful for implementing the PPU and testing individual pieces of functionality.(Sort of) running a Tonc PPU demoThe PPU took a few weeks, and then I moved onto DMA, followed by hardware timers, and audio. Of course, as Iâ€™d try new tests, demos, and games, Iâ€™d uncover bugs and fix them.Kirby  in Dream LandGame Boy and Game Boy Advance cartridges use the same 32-pin connector. However, they work very differently. The Game Boy cartridge bus is asynchronous: the game outputs the 16-bit address (64 KiB address space) on one set of pins and lowers the  pin. Some time later, the 8-bit read data from the ROM stabilizes on a separate set of pins.For the GBA, Nintendo extended the bus data width to 16-bit and the address space to 25-bit (32 MiB). However, they kept roughly the same set of pins, accomplishing this by multiplexing the 24 data/address pins: the console outputs the address (in increments of the data word size of 16-bits, for a 24-bit physical address), then lowers the  signal to â€œlatchâ€ the address in the cartridge. Then, each time the console pulses the  pin, the cartridge increments its latched address and outputs the next data over the same pins. This allows for a continuous read of sequential data without having to send a new address for each access. The GBA also allows games to configure cartridge access timings to support different ROM chips.I had to do a lot of my own research here. Software emulators donâ€™t need to care about the precise timing of the cartridge bus, so there wasnâ€™t much documentation. To figure out the exact cycle-accurate timing, I used a Saleae logic analyzer and connected it to the cartridge bus. I wrote a test program for the GBA to do different types of accesses (reads, writes, sequential, non-sequential, DMA) with different timing configurations.Cartridge bus analysis setupAfter coming up with numerous scenarios (especially around the interaction between DMA and the CPU, and starting and stopping burst accesses), I came up with a consistent model for how cartridge accesses worked. I created some timing diagrams to help:Timing diagram of a non-sequential access followed by a sequential accessFinally, I started implementing the cartridge controller state machine based on my observations, paired with an emulated cartridge implementation. With the emulated cartridge, I was able to properly run real games in the simulator.I quickly implemented physical cartridge support, to be able to finally run it on the actual FPGA. I connected the signals, built a new bitstream, andâ€¦ it didnâ€™t work at all. The Game Boy Advance boot screen ran, but it didnâ€™t get any further than that. I implemented the emulated cartridge on the FPGA (reading ROM files from the SD card), and it worked! Which was great, but physical cartridges still didnâ€™t.I used the logic analyzer to observe how my emulator was interacting with the cartridge compared to how an actual GBA, and found numerous issues.One of the first things I noticed was short glitches on the  line. I knew these had to be glitches (rather than incorrect logic), because they were 8 nanoseconds long, much shorter than the ~59.6ns clock period. Since the cartridge latches the address on a falling edge of , glitches cause it to latch an address when it shouldnâ€™t, screwing up reads.Glitches on the cartridge busHere, I learned an important lesson in digital design: output signals should come directly from flip-flops, with no logic in between.After each flip-flop outputs a new value (on the rising edge of the clock), the signals propagate through the chip. As they propagate, taking different paths of different lengths throughout the chip, the output from each lookup table (LUT) is unstable. These values only stabilize near the end of the clock cycle (assuming the design met timing closure), and then each flip-flop stores the stable value at the next rising edge. If you output a signal from logic, this instability is visible from outside of the chip, manifesting as glitches in the output signal. If you instead output the signal from a flip-flop, itâ€™ll change only on each clock edge, remaining stable in the middle.And of course, I had written the cartridge controller without thinking about this, and  of the output signals were generated from logic. I rewrote the controller to output everything from flip-flops, which had a series of cascading changes since all of the signals now had to be computed one clock cycle earlier than I expected.There were other issues too â€“ part of the problem was that my emulated cartridge model was too permissive, and didnâ€™t catch some fairly obvious incorrect behavior. After a few days of intensive debugging with the logic analyzer, I got to the point where I could play games from physical cartridges.Metroid: Zero Mission running from the cartridgeCartridge prefetch buffer#The ARM7TDMI has a single shared instruction and data memory bus. As a result, a long series of sequential memory accesses is rare. Even a linear piece of code without branches that includes â€œloadâ€ or â€œstoreâ€ instructions would produce a series of non-sequential memory accesses, as the CPU fetches an instruction from one location, loads a register from a different location, and then goes back to fetching the next instruction.This poses a real performance issue on the GBA, because every non-sequential access from the cartridge incurs a multi-cycle penalty. Nintendo attempted to mitigate this somewhat with the â€œprefetch bufferâ€ (read this post by endrift, the author of mGBA, for more details) which attempts to keep a cartridge read burst active between CPU accesses. Without emulating the prefetch buffer, some games lag (I noticed this the most in Mario Kart Super Circuit, and some rooms of Metroid: Zero Mission).The prefetch buffer, while simple in theory, is not well documented and has a lot of corner cases and weird interactions. Emulator developers often start by taking a shortcut: making all cartridge accesses take a single cycle when the prefetch buffer is enabled. This wouldnâ€™t work for me, since I actually had to interface with the physical cartridge.So, I set out to do some more research to figure out exactly how the prefetch buffer worked. After making some educated guesses and tests, I came up with a reasonable model of how it might work.Notes about the prefetch state machineActually implementing it took a lot of work, and I kept stumbling upon more and more corner cases. Eventually I got to the point where all games appeared to run at full speed, and most importantly, didnâ€™t randomly crash. My implementation isnâ€™t perfect: there are still a few mGBA test suite timing tests I donâ€™t pass, but itâ€™s certainly sufficient to play games.: standard duplex SPI, used for communicating with accessories: custom multi-drop UART-like protocol, used to link up to four GBAs together for multiplayer games: the Nintendo N64 and GameCube controller protocol, used to connect to a GameCube: duplex UART with flow control, : controlling the four pins individually as GPIO, The timing of these isnâ€™t well documented, so I did my own research.A  mode transfer with no attached consolesI did a lot of testing with examples from the gba-link-connection library, intended for homebrew GBA games, but helpful for testing the different transfer modes in a controlled environment.Multiplayer Mario Kart with Game Bub and a GBAGame Bub linked to a GameCube playing Animal CrossingDuring the emulator development, I had used various test ROMs (mentioned before) to test basic functionality in isolation. As my emulator became mature enough to run commercial games, however, I started to shift some of my focus to accuracy-focused test ROMs.These test ROMs (such as the mGBA test suite) generally test really specific hardware quirks and timing. For example, they might test what happens when you run an instruction that ARM calls â€œunpredictableâ€, or the exact number of cycles it takes to service an interrupt in specific scenarios, or the value of the â€œcarryâ€ flag after performing a multiplication. These are the kinds of things that donâ€™t actually matter for playing games, but present a fun challenge and a way to â€œscoreâ€ your emulator against others. This also highlights the collaborative nature of the emulation development community: people sharing their research and helping each other out.I wonâ€™t talk about all of the tests here (for my emulatorâ€™s test results, see this page). But I do want to mention the . This is an official test cartridge from Nintendo, likely used as part of a factory test or RMA procedure. Apparently, Nintendo has  used it to test their emulators (e.g. their GBA emulator on the Nintendo Switch). This test has generally been considered to be difficult to pass (it tests some specific hardware quirks), but itâ€™s easier now that the tests have been thoroughly reverse engineered and documented. Still, passing it is a nice milestone:Passing the AGB Aging CartridgeSecond hardware revision#Towards the end of 2024, approximately one year after I originally designed Game Bub, I decided to make a second hardware revision. Over the past year, I had been keeping track of all of the things I would want to change in a future revision. Since the first version of Game Bub miraculously worked without any major issues, this list was primarily minor issues and ergonomics changes.I fixed the minor I2C power issues, removed the reference designators from the PCB silkscreen (they looked messy with the dense board, and I didnâ€™t use them for anything anyway), and changed around some test points. I improved the rumble circuit to be more responsive, and switched to a PCB-mounted vibration motor.The first version of Game Bub was fairly thick, measuring 12.9mm at the top and 21.9mm on the bottom. The thickness of the rear enclosure was dictated by the thickness of Game Boy cartridges, but I made several changes to the front. I moved the  (8.5mm!) link port to the back, and removed the HDMI port (more on that later). I changed the headphone jack (5.0mm tall â€“ no wonder they started getting removed from phones) to a mid-mount one that sunk into the PCB and reduced the overall height.I also switched from an  module (3.1mm depth) to an  (2.4mm depth). I should have done this from the beginning, I just didnâ€™t even know the ESP32-S3-MINI existed. This had the side effect of giving me 3 more GPIOs, which allowed me to put the FPGA and LCD on separate SPI busses, avoiding the minor issue of an unpowered FPGA interfering with LCD communication, and allowed for faster boot because the LCD could be configured at the same time as the FPGA.I switched the speakers, from the fully-enclosed CES-20134-088PMB to the CMS-160903-18S-X8. I made this change primarily for ease of assembly. The first speaker had a wire connector that plugged into the board, and I found it difficult to connect during assembly without having the wire interfere with buttons. The new speaker is smaller and has a spring contact connector, so it just presses against the PCB as the device is assembled. This required some speaker enclosure design â€“ an unenclosed speaker in free air sounds quiet and tinny.I reworked the layout of the face buttons and D-pad to match the spacing of the Nintendo DSi. This allowed me to use the silicone membranes from the DSi for an improved button feel and reduced rattling. I was also hoping to use the plastic buttons from the DSi (which were higher quality compared to my 3D printed buttons), but even with the new thinner design, the buttons werenâ€™t quite tall enough to be easily pressed.I created another timelapse of my modifications to produce the second version of the PCB:Revision 2 board layout timelapseFor the second revision of the enclosure, I switched to Fusion 360 for the CAD work. While I would have preferred to keep using FreeCAD, I found that it was making it harder for me to be productive. Fusion 360 has a free version for hobbyists (with some limitations that have gradually increased over time), and overall Iâ€™ve found it very pleasant to use.Fusion 360 view of the second enclosure, fully assembledUnlike with the first revision, I waited until I had a final design for both the enclosure and the PCB before getting anything manufactured. This let me go back and forth, making small modifications to each of them as needed.I wanted to make the end result look more polished and professional, so I contracted a factory to produce custom LCD cover glass, made out of 0.7mm thick tempered glass with a black silkscreen. It was relatively expensive for a low quantity order, but Iâ€™m really happy with how it turned out.Custom LCD cover glass with adhesive backingManufacturing and assembly#I got the PCBs manufactured and assembled, this time with black solder mask to look .Assembled PCB, revision 2I had two enclosures made. The first was black PA-12 Nylon, printed with MJF. Nylon is strong and durable, and the MJF 3D printing technology produces a slightly grainy surface thatâ€™s really pleasant to hold in your hand.Closeup of the nylon grainy textureThe second one was made of transparent resin (SLA, like before). This lets me show off the PCB that I worked so hard on, and evokes the transparent electronics trend from the 90s.Assembly was a lot easier this time around: the silicone membranes held the face buttons in place, the speakers had a spring contact instead of wires, and the shoulder button assembly was better. In the first revision, I had excessively large tolerances because I wasnâ€™t sure how precise the 3D printing would be. In the second version, I was able to shrink these.The final product looked and felt a lot better, too. The edges were more rounded, and the device was thinner and easier to hold. The buttons felt  better to press and didnâ€™t rattle around, and the cover glass over the LCD added polish.First revision (left), second revision (center and right)I previously mentioned that I removed the full-size HDMI port from the first revision. I had first planned to change it to a mini-HDMI or micro-HDMI port to reduce the size, but I was worried about durability.What I  wanted to do was output video through the USB-C port, avoiding the need for any HDMI port at all. Unfortunately, I had already concluded earlier that I wouldnâ€™t be able to output DisplayPort video signals from the FPGA, which meant that I couldnâ€™t use the standard USB-C DisplayPort alternate mode.However, an idea struck me towards the end of 2024: I didnâ€™t actually  to use the DisplayPort alt-mode. The USB-C connector, in addition to the USB 2.0 D+/D- pins, has four differential pairs (for USB superspeed). Conveniently, HDMI  uses four differential pairs. The USB specification allows for vendor-specific alt-modes, so I could just implement my own, outputting the HDMI signal directly from the FPGA over the additional pins. Then I could build a custom dock that takes those pins and connects them to the data lines of an HDMI port.According to the USB specification, alternate modes must be negotiated by both sides first, using the USB-C Power Delivery (USB-PD) protocol, to prevent them from interfering with devices that arenâ€™t expecting them. I donâ€™t actually have a USB-PD controller in Game Bub (too much added complexity), so I took a shortcut: have a microcontroller in the dock communicate with the Game Bub over regular USB and perform a handshake before enabling HDMI output from the FPGA. Once Game Bub detects that itâ€™s been disconnected from the dock, it can just switch back to using the internal display.I realized that the dock also presents another opportunity for controller support. I originally wanted to build wireless controller support into the handheld, but the ESP32-S3 only supports Bluetooth Low Energy, and the majority of controllers use Bluetooth Classic. Fortunately, the Raspberry Pi Pico W (with an RP2040 MCU) supports both types of Bluetooth, so I just decided to use that as the microcontroller on the dock. Game controllers connect to the dock over Bluetooth, and the Pico sends the controller inputs to the device. I wired up the  and  USB-C pins as a direct connection between the FPGA and the dock for low latency input.The RP2040 acts as the USB host, and Game Bub only needs to be a device. I also added a USB hub chip and some additional USB ports on the back of the dock to allow for wired controller support too. Just like with wireless controllers, the dock handles the direct controller communication, and just passes inputs back to the main Game Bub unit.Since the dock is so simple (comparatively), it only took about a day to design and lay out.I had also hoped to use the dock to solve another problem around HDMI output: HDMI sinks (monitors, TVs) pull the HDMI data lines up to 3.3 volts, and can actually backfeed power to the HDMI source. For Game Bub, this meant that a powered-off unit would turn itself on when connected over HDMI. I used a HDMI buffer chip in the dock to try to alleviate this problem, but the chip I used wasnâ€™t actually properly suited to this use-case and interfered with video output, so I had to carefully rework the board to bypass the chip. Iâ€™ll have to fix it in a later revision.Bypassing the HDMI buffer chipAfter the rework, HDMI output worked! The rest of the features are still a work in progress.Game Bub PCB on the dock, connected to an external monitorCongratulations on reading this far! This writeup ended up being incredibly long, even with a lot of details left out.Iâ€™m proud of what I accomplished over the last year and a half: I met all of my goals to produce a polished handheld FPGA retrogaming device. I pushed my electrical engineering and product design skills to the limit, and learned a lot in the process. Professional product and hardware designers deserve  respect.I deliberately designed this project with lots of possible extension opportunities to keep me occupied for a long time. I worked hard to get to the point where Iâ€™m comfortable sharing Game Bub with the world, but I still have a long list of TODOs for the future.In the near term, Iâ€™m going to work on finishing the dock, implementing wireless controller support (and maybe wired). I plan to use the Bluepad32 library to do so.I also want to improve the accuracy of my Game Boy Advance emulator: my goal here is to someday pass the entire mGBA test suite. I hope that I can contribute back to the wonderful  community with my emulator, and I plan to write-up some of my research around the GBA cartridge interface and link port.I have a long list of mostly minor changes to make to the MCU firmware: improving UI render performance, bits of polish like low battery notifications, eliminating display glitching when reloading the FPGA, and that sort of thing. I also plan to add more utilities, like a cartridge dumper and save backup/restore feature.Some day, I want to emulate the Game Boy Advance Wireless Adapter over Wi-Fi, e.g. with ESP-NOW. This wonâ€™t be compatible with the original wireless adapter, unfortunately, since that uses raw 2.4 GHz modulation rather than Wi-Fi.I designed Game Bub with extremely low production volumes in mind, using off-the-shelf commodity parts to keep the overall cost down. However, there are a few things I would have liked to be able to do, but are only possible with much higher volumes:A better LCD module (likely custom): native landscape mode to avoid the need for triple-buffering. Ideally a 720x480 resolution display, to allow for 3x GBA scaling and filter effects.High-quality injection molded case and buttons: 3D printing is great for low volume production, but an injection molded case would be great. It would be more precise (allowing for tighter tolerances), stronger, and allow for significantly more color options.Custom battery pack: or at least customizing the length of the connector wire. The current solution is hacky and doesnâ€™t make the best use of internal space, due to limited off-the-shelf battery options.Smaller BGA parts for SRAM and SDRAM to free up board space (and move internal signals to 1.8 volts): this is actually something that would be possible in smaller volumes too, if I were willing to send parts from Mouser or DigiKey to JLCPCB for assembly.]]></content:encoded></item><item><title>Is RUST useful for a scientist?</title><link>https://www.reddit.com/r/rust/comments/1ioglyd/is_rust_useful_for_a_scientist/</link><author>/u/Academic_Ship6221</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 11:00:41 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am a Physicist and work a bit on robotics. I work with Julia, Python and some what C++. I got rusty in C++ and thought of working on it again. However, I have heard RUST is some thing very cool.Shall I start learning RUST or would C++ is fine for me? I am learning for pleasure purposes mainly. Also, as a scientist would it be any useful?]]></content:encoded></item><item><title>2024 State of Rust Survey Results | Rust Blog</title><link>https://blog.rust-lang.org/2025/02/13/2024-State-Of-Rust-Survey-results.html</link><author>/u/Kobzol</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 10:38:01 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The Rust Survey Team is excited to share the results of our 2024 survey on the Rust Programming language, conducted between December 5, 2024 and December 23, 2024.
As in previous years, the 2024 State of Rust Survey was focused on gathering insights and feedback from Rust users, and all those who are interested in the future of Rust more generally.This ninth edition of the survey surfaced new insights and learning opportunities straight from the global Rust language community, which we will summarize below. In addition to this blog post,  containing charts with aggregated results of all questions in the survey.Our sincerest thanks to every community member who took the time to express their opinions and experiences with Rust over the past year. Your participation will help us make Rust better for everyone.There's a lot of data to go through, so strap in and enjoy!As shown above, in 2024, we have received fewer survey views than in the previous year. This was likely caused simply by the fact that the survey ran only for two weeks, while in the previous year it ran for almost a month. However, the completion rate has also dropped, which seems to suggest that the survey might be a bit too long. We will take this into consideration for the next edition of the survey.The State of Rust survey not only gives us excellent insight into how many Rust users around the world are using and experiencing the language but also gives us insight into the makeup of our global community. This information gives us a sense of where the language is being used and where access gaps might exist for us to address over time. We hope that this data and our related analysis help further important discussions about how we can continue to prioritize global access and inclusivity in the Rust community.Same as every year, we asked our respondents in which country they live in. The top 10 countries represented were, in order: United States (22%), Germany (14%), United Kingdom (6%), France (6%), China (5%), Canada (3%), Netherlands (3%), Russia (3%), Australia (2%), and Sweden (2%). We are happy to see that Rust is enjoyed by users from all around the world! You can try to find your country in the chart below:We also asked whether respondents consider themselves members of a marginalized community. Out of those who answered, 74.5% selected no, 15.5% selected yes, and 10% preferred not to say.We have asked the group that selected â€œyesâ€ which specific groups they identified as being a member of. The majority of those who consider themselves a member of an underrepresented or marginalized group in technology identify as lesbian, gay, bisexual, or otherwise non-heterosexual. The second most selected option was neurodivergent at 46% followed by trans at 35%.Each year, we must acknowledge the diversity, equity, and inclusivity (DEI) related gaps in the Rust community and open source as a whole. We believe that excellent work is underway at the Rust Foundation to advance global access to Rust community gatherings and distribute grants to a diverse pool of maintainers each cycle, which you can learn more about here. Even so, global inclusion and access is just one element of DEI, and the survey working group will continue to advocate for progress in this domain.The number of respondents that self-identify as a Rust user was quite similar to last year, around 92%. This high number is not surprising, since we primarily target existing Rust developers with this survey.Similarly as last year, around 31% of those who did not identify as Rust users cited the perception of difficulty as the primary reason for not using Rust. The most common reason for not using Rust was that the respondents simply havenâ€™t had the chance to try it yet.Of the former Rust users who participated in the 2024 survey, 36% cited factors outside their control as a reason why they no longer use Rust, which is a 10pp decrease from last year. This year, we also asked respondents if they would consider using Rust again if an opportunity comes up, which turns out to be true for a large fraction of the respondents (63%). That is good to hear!Closed answers marked with N/A were not present in the previous version(s) of the survey.Those not using Rust anymore told us that it is because they don't really need it (or the goals of their company changed) or because it was not the right tool for the job. A few reported being overwhelmed by the language or its ecosystem in general or that switching to or introducing Rust would have been too expensive in terms of human effort.Of those who used Rust in 2024, 53% did so on a daily (or nearly daily) basis â€” an increase of 4pp from the previous year. We can observe an upward trend in the frequency of Rust usage over the past few years, which suggests that Rust is being increasingly used at work. This is also confirmed by other answers mentioned in the Rust at Work section later below.Rust expertise is also continually increasing amongst our respondents! 20% of respondents can write (only) simple programs in Rust (a decrease of 3pp from 2023), while 53% consider themselves productive using Rust â€” up from 47% in 2023. While the survey is just one tool to measure the changes in Rust expertise overall, these numbers are heartening as they represent knowledge growth for many Rustaceans returning to the survey year over year.Unsurprisingly, the most popular version of Rust is , either the most recent one or whichever comes with the users' Linux distribution. Almost a third of users also use the latest nightly release, due to various reasons (see below). However, it seems that the beta toolchain is not used much, which is a bit unfortunate. We would like to encourage Rust users to use the beta toolchain more (e.g. in CI environments) to help test soon-to-be stabilized versions of Rust.People that use the nightly toolchain mostly do it to gain access to specific unstable language features. Several users have also mentioned that rustfmt works better for them on nightly or that they use the nightly compiler because of faster compilation times.To use Rust, programmers first have to learn it, so we are always interested in finding out how do they approach that. Based on the survey results, it seems that most users learn from Rust documentation and also from The Rust Programming Language book, which has been a favourite learning resource of new Rustaceans for a long time. Many people also seem to learn by reading the source code of Rust crates. The fact that both the documentation and source code of tens of thousands of Rust crates is available on docs.rs and GitHub makes this easier.In terms of answers belonging to the "Other" category, they can be clustered into three categories: people using LLM (large language model) assistants (Copilot, ChatGPT, Claude, etc.), reading the official Rust forums (Discord, URLO) or being mentored while contributing to Rust projects. We would like to extend a big thank you to those making our spaces friendly and welcoming for newcomers, as it is important work and it pays off. Interestingly, a non-trivial number of people "learned by doing" and used rustc error messages and clippy as a guide, which is a good indicator of the quality of Rust diagnostics.In terms of formal education, it seems that Rust has not yet penetrated university curriculums, as this is typically a very slowly moving area. Only a very small number of respondents (around 3%) have taken a university Rust course or used university learning materials.In terms of operating systems used by Rustaceans, Linux was the most popular choice, and it seems that it is getting increasingly popular year after year. It is followed by macOS and Windows, which have a very similar share of usage.As you can see in the wordcloud, there are also a few users that prefer Arch, btw.Rust programmers target a diverse set of platforms with their Rust programs. We saw a slight uptick in users targeting embedded and mobile platforms, but otherwise the distribution of platforms stayed mostly the same as last year. Since the WebAssembly target is quite diverse, we have split it into two separate categories this time. Based on the results it is clear that when using WebAssembly, it is mostly in the context of browsers (23%) rather than other use-cases (7%).We cannot of course forget the favourite topic of many programmers: which IDE (developer environment) they use. Although Visual Studio Code still remains the most popular option, its share has dropped by 5pp this year. On the other hand, the Zed editor seems to have gained considerable traction recently. The small percentage of those who selected "Other" are using a wide range of different tools: from CursorAI to classics like Kate or Notepad++. Special mention to the 3 people using "ed", that's quite an achievement.You can also take a look at the linked wordcloud that summarizes open answers to this question (the "Other" category), to see what other editors are also popular.We were excited to see that more and more people use Rust at work for the majority of their coding, 38% vs 34% from last year. There is a clear upward trend in this metric over the past few years.The usage of Rust within companies also seems to be rising, as 45% of respondents answered that their organisation makes non-trivial use of Rust, which is a 7pp increase from 2023.Once again, the top reason employers of our survey respondents invested in Rust was the ability to build relatively correct and bug-free software. The second most popular reason was Rustâ€™s performance characteristics. 21% of respondents that use Rust at work do so because they already know it, and it's thus their default choice, an uptick of 5pp from 2023. This seems to suggest that Rust is becoming one of the baseline languages of choice for more and more companies.Similarly to the previous year, a large percentage of respondents (82%) report that Rust helped their company achieve its goals. In general, it seems that programmers and companies are quite happy with their usage of Rust, which is great!In terms of technology domains, the situation is quite similar to the previous year. Rust seems to be especially popular for creating server backends, web and networking services and cloud technologies. It also seems to be gaining more traction for embedded use-cases.You can scroll the chart to the right to see more domains. Note that the Automotive domain was not offered as a closed answer in the 2023 survey (it was merely entered through open answers), which might explain the large jump.It is exciting to see the continued growth of professional Rust usage and the confidence so many users feel in its performance, control, security and safety, enjoyability, and more!As always, one of the main goals of the State of Rust survey is to shed light on challenges, concerns, and priorities on Rustaceansâ€™ minds over the past year.We have asked our users about aspects of Rust that limit their productivity. Perhaps unsurprisingly, slow compilation was at the top of the list, as it seems to be a perennial concern of Rust users. As always, there are efforts underway to improve the speed of the compiler, such as enabling the parallel frontend or switching to a faster linker by default. We invite you to test these improvements and let us know if you encounter any issues.Other challenges included subpar support for debugging Rust and high disk usage of Rust compiler artifacts. On the other hand, most Rust users seem to be very happy with its runtime performance, the correctness and stability of the compiler and also Rust's documentation.In terms of specific unstable (or missing) features that Rust users want to be stabilized (or implemented), the most desired ones were async closures and if/let while chains. Well, we have good news! Async closures will be stabilized in the next version of Rust (1.85), and if/let while chains will hopefully follow soon after, once Edition 2024 is released (which will also happen in Rust 1.85).Other coveted features are generators (both sync and async) and more powerful generic const expressions. You can follow the Rust Project Goals to track the progress of these (and other) features.In the open answers to this question, people were really helpful and tried hard to describe the most notable issues limiting their productivity. We have seen mentions of struggles with async programming (an all-time favourite), debuggability of errors (which people generally love, but they are not perfect for everyone) or Rust tooling being slow or resource intensive (rust-analyzer and rustfmt). Some users also want a better IDE story and improved interoperability with other languages.This year, we have also included a new question about the speed of Rust's evolution. While most people seem to be content with the status quo, more than a quarter of people who responded to this question would like Rust to stabilize and/or add features more quickly, and only 7% of respondents would prefer Rust to slow down or completely stop adding new features.Interestingly, when we asked respondents about their main worries for the future of Rust, one of the top answers remained the worry that Rust will become too complex. This seems to be in contrast with the answers to the previous question. Perhaps Rust users still seem to consider the complexity of Rust to be manageable, but they worry that one day it might become too much.We are happy to see that the amount of respondents concerned about Rust Project governance and lacking support of the Rust Foundation has dropped by about 6pp from 2023.Each year, the results of the State of Rust survey help reveal the areas that need improvement in many areas across the Rust Project and ecosystem, as well as the aspects that are working well for our community.If you have any suggestions for the Rust Annual survey, please let us know!We are immensely grateful to those who participated in the 2024 State of Rust Survey and facilitated its creation. While there are always challenges associated with developing and maintaining a programming language, this year we were pleased to see a high level of survey participation and candid feedback that will truly help us make Rust work better for everyone.If youâ€™d like to dig into more details, we recommend you to browse through the full survey report.]]></content:encoded></item><item><title>This Week in Rust 586 Â· This Week in Rust</title><link>https://this-week-in-rust.org/blog/2025/02/12/this-week-in-rust-586/</link><author>/u/bennyvasquez</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 04:00:02 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[This week's crate is esp32-mender-client, a client for ESP32 to execute firmware updates and remote commands.Thanks to Kelvin for the self-suggestion!An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.  The following
RFCs would benefit from user testing before moving forward:No calls for testing were issued this week.No calls for testing were issued this week.No calls for testing were issued this week.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.No Calls for participation were submitted this week.Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.No Calls for papers or presentations were submitted this week.A relatively neutral week, with lots of real changes but most small in
magnitude. Most significant change is rustdoc's move of JS/CSS minification to
build time which cut doc generation times on most benchmarks fairly
significantly.3 Regressions, 5 Improvements, 1 Mixed; 2 of them in rollups
32 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.No Cargo Tracking Issues or PRs entered Final Comment Period this week.No Language Team Proposals entered Final Comment Period this week.No Language Reference RFCs entered Final Comment Period this week.No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.Rusty Events between 2025-02-12 - 2025-03-12 ðŸ¦€If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.Just because things are useful doesn't mean they are magically sound.]]></content:encoded></item><item><title>Rust is the language of choice on the new Gnome website front page!</title><link>http://gnome.org/</link><author>/u/OS6aDohpegavod4</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 02:24:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[A more elegant way to use your computerHit the  any time to get an overview, switch or launch apps,
         and search for anything on your computer. It's magic.]]></content:encoded></item><item><title>I am slowly Rusting away - continuously finding more appreciation for Rust.</title><link>https://www.reddit.com/r/rust/comments/1io7e84/i_am_slowly_rusting_away_continuously_finding/</link><author>/u/saintpetejackboy</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 01:17:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I have a strong background in PHP and "full stack" development and have been cranking out proprietary CRUD for businesses most of my life. Over the decades, Iiked to try all the new shiny frameworks and languages as they came and inevitably went. In recent years, I started to eschew PHP for Node and Python for particular tasks where I felt PHP was lacking. Somewhere along the way, I started to fidget with Rust occasionally.Now, Rust is the first tool I find I am reaching for, even over my native language.Rust just works. Unlike PHP, I don't have to worry about the performance of extremely complex or cumbersome scripts/tasks. Unlike Python, I don't have to struggle against the environment and the same kind of package and environment management hellhole that really plagues Node.js.I don't know how they do it, but Rust and Cargo are almost flawless. I don't have deprecated packages conflicts and version overrides everywhere, it just doesn't even come up.One thing I learned recently was that I can bundle all of my other files (like css, js) with my binary - just like bundled libraries. Like magic, they just work. I didn't spend hours (like with Node.js) trying to get my static content to work on the other side of a reverse proxy. Compiled it and it worked the very first time.Being able to easily target releases and customize my binary to ensure it is truly 'portable'.The coup de grace for me was when I set up a project earlier that could remotely obtain the newest version of its own binary and upgrade itself in place, restarting it's own systemctl in the process. My mind was absolutely blown.It wasn't that I could do something that is arguably not that difficult and that you can accomplish in many languages - it was that I was able to do it without much struggle or effort - it just "worked".I was inside of an .rs file at one point a few days ago that was a sweaty jumble of JS, css, html and Rust - it gave me a flashback to my sophmore spaghetti soups of PHP, jQuery, etc.; - a massive adventure on the horizon with nothing holding me back.Every time I mess up, the Rust compiler tells me in explicit and painful details exactly what I messed up and where. I never have "unresolvable" conflicts or requirements. I still have a lot to learn and am now kicking my self, thinking back on how much time I wasted chasing other languages around looking for exactly what Rust has to offer.There is a reason Rust keeps getting mentioned as being loved by developers. As a developer, I love Rust.If you are like I was and are a polyglot looking for a stellar language, it doesn't get much better than Rust. Do yourself a favor and just try your next little project in Rust. It might not click right away (it took me a few), but there is something really alluring about the development process, syntax, presentation and performance of Rust. Sorry for waxing so poetic about this language, I am just in a rapture of ecstasy after my last successful build and wanted to try and flag down some passers-by who might be on the fence about jumping fully into Rust.]]></content:encoded></item><item><title>2024 State of Rust Survey Results</title><link>https://blog.rust-lang.org/2025/02/13/2024-State-Of-Rust-Survey-results.html</link><author>The Rust Survey Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[The Rust Survey Team is excited to share the results of our 2024 survey on the Rust Programming language, conducted between December 5, 2024 and December 23, 2024.
As in previous years, the 2024 State of Rust Survey was focused on gathering insights and feedback from Rust users, and all those who are interested in the future of Rust more generally.This ninth edition of the survey surfaced new insights and learning opportunities straight from the global Rust language community, which we will summarize below. In addition to this blog post,  containing charts with aggregated results of all questions in the survey.Our sincerest thanks to every community member who took the time to express their opinions and experiences with Rust over the past year. Your participation will help us make Rust better for everyone.There's a lot of data to go through, so strap in and enjoy!As shown above, in 2024, we have received fewer survey views than in the previous year. This was likely caused simply by the fact that the survey ran only for two weeks, while in the previous year it ran for almost a month. However, the completion rate has also dropped, which seems to suggest that the survey might be a bit too long. We will take this into consideration for the next edition of the survey.The State of Rust survey not only gives us excellent insight into how many Rust users around the world are using and experiencing the language but also gives us insight into the makeup of our global community. This information gives us a sense of where the language is being used and where access gaps might exist for us to address over time. We hope that this data and our related analysis help further important discussions about how we can continue to prioritize global access and inclusivity in the Rust community.Same as every year, we asked our respondents in which country they live in. The top 10 countries represented were, in order: United States (22%), Germany (14%), United Kingdom (6%), France (6%), China (5%), Canada (3%), Netherlands (3%), Russia (3%), Australia (2%), and Sweden (2%). We are happy to see that Rust is enjoyed by users from all around the world! You can try to find your country in the chart below:We also asked whether respondents consider themselves members of a marginalized community. Out of those who answered, 74.5% selected no, 15.5% selected yes, and 10% preferred not to say.We have asked the group that selected â€œyesâ€ which specific groups they identified as being a member of. The majority of those who consider themselves a member of an underrepresented or marginalized group in technology identify as lesbian, gay, bisexual, or otherwise non-heterosexual. The second most selected option was neurodivergent at 46% followed by trans at 35%.Each year, we must acknowledge the diversity, equity, and inclusivity (DEI) related gaps in the Rust community and open source as a whole. We believe that excellent work is underway at the Rust Foundation to advance global access to Rust community gatherings and distribute grants to a diverse pool of maintainers each cycle, which you can learn more about here. Even so, global inclusion and access is just one element of DEI, and the survey working group will continue to advocate for progress in this domain.The number of respondents that self-identify as a Rust user was quite similar to last year, around 92%. This high number is not surprising, since we primarily target existing Rust developers with this survey.Similarly as last year, around 31% of those who did not identify as Rust users cited the perception of difficulty as the primary reason for not using Rust. The most common reason for not using Rust was that the respondents simply havenâ€™t had the chance to try it yet.Of the former Rust users who participated in the 2024 survey, 36% cited factors outside their control as a reason why they no longer use Rust, which is a 10pp decrease from last year. This year, we also asked respondents if they would consider using Rust again if an opportunity comes up, which turns out to be true for a large fraction of the respondents (63%). That is good to hear!Closed answers marked with N/A were not present in the previous version(s) of the survey.Those not using Rust anymore told us that it is because they don't really need it (or the goals of their company changed) or because it was not the right tool for the job. A few reported being overwhelmed by the language or its ecosystem in general or that switching to or introducing Rust would have been too expensive in terms of human effort.Of those who used Rust in 2024, 53% did so on a daily (or nearly daily) basis â€” an increase of 4pp from the previous year. We can observe an upward trend in the frequency of Rust usage over the past few years, which suggests that Rust is being increasingly used at work. This is also confirmed by other answers mentioned in the Rust at Work section later below.Rust expertise is also continually increasing amongst our respondents! 20% of respondents can write (only) simple programs in Rust (a decrease of 3pp from 2023), while 53% consider themselves productive using Rust â€” up from 47% in 2023. While the survey is just one tool to measure the changes in Rust expertise overall, these numbers are heartening as they represent knowledge growth for many Rustaceans returning to the survey year over year.Unsurprisingly, the most popular version of Rust is , either the most recent one or whichever comes with the users' Linux distribution. Almost a third of users also use the latest nightly release, due to various reasons (see below). However, it seems that the beta toolchain is not used much, which is a bit unfortunate. We would like to encourage Rust users to use the beta toolchain more (e.g. in CI environments) to help test soon-to-be stabilized versions of Rust.People that use the nightly toolchain mostly do it to gain access to specific unstable language features. Several users have also mentioned that rustfmt works better for them on nightly or that they use the nightly compiler because of faster compilation times.To use Rust, programmers first have to learn it, so we are always interested in finding out how do they approach that. Based on the survey results, it seems that most users learn from Rust documentation and also from The Rust Programming Language book, which has been a favourite learning resource of new Rustaceans for a long time. Many people also seem to learn by reading the source code of Rust crates. The fact that both the documentation and source code of tens of thousands of Rust crates is available on docs.rs and GitHub makes this easier.In terms of answers belonging to the "Other" category, they can be clustered into three categories: people using LLM (large language model) assistants (Copilot, ChatGPT, Claude, etc.), reading the official Rust forums (Discord, URLO) or being mentored while contributing to Rust projects. We would like to extend a big thank you to those making our spaces friendly and welcoming for newcomers, as it is important work and it pays off. Interestingly, a non-trivial number of people "learned by doing" and used rustc error messages and clippy as a guide, which is a good indicator of the quality of Rust diagnostics.In terms of formal education, it seems that Rust has not yet penetrated university curriculums, as this is typically a very slowly moving area. Only a very small number of respondents (around 3%) have taken a university Rust course or used university learning materials.In terms of operating systems used by Rustaceans, Linux was the most popular choice, and it seems that it is getting increasingly popular year after year. It is followed by macOS and Windows, which have a very similar share of usage.As you can see in the wordcloud, there are also a few users that prefer Arch, btw.Rust programmers target a diverse set of platforms with their Rust programs. We saw a slight uptick in users targeting embedded and mobile platforms, but otherwise the distribution of platforms stayed mostly the same as last year. Since the WebAssembly target is quite diverse, we have split it into two separate categories this time. Based on the results it is clear that when using WebAssembly, it is mostly in the context of browsers (23%) rather than other use-cases (7%).We cannot of course forget the favourite topic of many programmers: which IDE (developer environment) they use. Although Visual Studio Code still remains the most popular option, its share has dropped by 5pp this year. On the other hand, the Zed editor seems to have gained considerable traction recently. The small percentage of those who selected "Other" are using a wide range of different tools: from CursorAI to classics like Kate or Notepad++. Special mention to the 3 people using "ed", that's quite an achievement.You can also take a look at the linked wordcloud that summarizes open answers to this question (the "Other" category), to see what other editors are also popular.We were excited to see that more and more people use Rust at work for the majority of their coding, 38% vs 34% from last year. There is a clear upward trend in this metric over the past few years.The usage of Rust within companies also seems to be rising, as 45% of respondents answered that their organisation makes non-trivial use of Rust, which is a 7pp increase from 2023.Once again, the top reason employers of our survey respondents invested in Rust was the ability to build relatively correct and bug-free software. The second most popular reason was Rustâ€™s performance characteristics. 21% of respondents that use Rust at work do so because they already know it, and it's thus their default choice, an uptick of 5pp from 2023. This seems to suggest that Rust is becoming one of the baseline languages of choice for more and more companies.Similarly to the previous year, a large percentage of respondents (82%) report that Rust helped their company achieve its goals. In general, it seems that programmers and companies are quite happy with their usage of Rust, which is great!In terms of technology domains, the situation is quite similar to the previous year. Rust seems to be especially popular for creating server backends, web and networking services and cloud technologies. It also seems to be gaining more traction for embedded use-cases.You can scroll the chart to the right to see more domains. Note that the Automotive domain was not offered as a closed answer in the 2023 survey (it was merely entered through open answers), which might explain the large jump.It is exciting to see the continued growth of professional Rust usage and the confidence so many users feel in its performance, control, security and safety, enjoyability, and more!As always, one of the main goals of the State of Rust survey is to shed light on challenges, concerns, and priorities on Rustaceansâ€™ minds over the past year.We have asked our users about aspects of Rust that limit their productivity. Perhaps unsurprisingly, slow compilation was at the top of the list, as it seems to be a perennial concern of Rust users. As always, there are efforts underway to improve the speed of the compiler, such as enabling the parallel frontend or switching to a faster linker by default. We invite you to test these improvements and let us know if you encounter any issues.Other challenges included subpar support for debugging Rust and high disk usage of Rust compiler artifacts. On the other hand, most Rust users seem to be very happy with its runtime performance, the correctness and stability of the compiler and also Rust's documentation.In terms of specific unstable (or missing) features that Rust users want to be stabilized (or implemented), the most desired ones were async closures and if/let while chains. Well, we have good news! Async closures will be stabilized in the next version of Rust (1.85), and if/let while chains will hopefully follow soon after, once Edition 2024 is released (which will also happen in Rust 1.85).Other coveted features are generators (both sync and async) and more powerful generic const expressions. You can follow the Rust Project Goals to track the progress of these (and other) features.In the open answers to this question, people were really helpful and tried hard to describe the most notable issues limiting their productivity. We have seen mentions of struggles with async programming (an all-time favourite), debuggability of errors (which people generally love, but they are not perfect for everyone) or Rust tooling being slow or resource intensive (rust-analyzer and rustfmt). Some users also want a better IDE story and improved interoperability with other languages.This year, we have also included a new question about the speed of Rust's evolution. While most people seem to be content with the status quo, more than a quarter of people who responded to this question would like Rust to stabilize and/or add features more quickly, and only 7% of respondents would prefer Rust to slow down or completely stop adding new features.Interestingly, when we asked respondents about their main worries for the future of Rust, one of the top answers remained the worry that Rust will become too complex. This seems to be in contrast with the answers to the previous question. Perhaps Rust users still seem to consider the complexity of Rust to be manageable, but they worry that one day it might become too much.We are happy to see that the amount of respondents concerned about Rust Project governance and lacking support of the Rust Foundation has dropped by about 6pp from 2023.Each year, the results of the State of Rust survey help reveal the areas that need improvement in many areas across the Rust Project and ecosystem, as well as the aspects that are working well for our community.If you have any suggestions for the Rust Annual survey, please let us know!We are immensely grateful to those who participated in the 2024 State of Rust Survey and facilitated its creation. While there are always challenges associated with developing and maintaining a programming language, this year we were pleased to see a high level of survey participation and candid feedback that will truly help us make Rust work better for everyone.If youâ€™d like to dig into more details, we recommend you to browse through the full survey report.]]></content:encoded></item><item><title>Help me present the case for Rust (with examples) to my boss</title><link>https://www.reddit.com/r/rust/comments/1io0whw/help_me_present_the_case_for_rust_with_examples/</link><author>/u/sonicbhoc</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 20:30:11 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[My boss is a C guy who has been coding C longer than I've been alive.He's getting tired of the things you would expect a C programmer to be tired of at this point. In his own words, C is not a good programming language, but it is great at being an abstraction for Assembly.However, he's also a big fan of the simplicity and straightforwardness of C. He really doesn't like functional programming concepts (his exact word for them was "weird") and while Rust isn't a functional language per se, It definitely has some concepts from it.He told me he's not a fan of the language, but he  to learn to like it (which is the first time I've heard him say that). I've barely started on my journey of learning Rust myself. He would ideally like something done in a week that could show where Rust shines.An important piece of info is that we mostly do safety critical and embedded stuff. We're looking at the Ferrocene toolkit among other things.What can I code â€” both in C and I'm Rust, preferably, to show distinctions â€” that I can use to demonstrate the differences for low-level coding? ]]></content:encoded></item><item><title>filtra.io | Rust Jobs Report - January 2025</title><link>https://filtra.io/rust/jobs-report/jan-25</link><author>/u/anonymous_pro_</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 18:58:22 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Welcome to the January 2025 Rust Jobs Report by filtra. Please enjoy our overview of the Rust job market.To add your or another company to the filtra Rust jobs index, please email filtra@filtra.io with the subject line "Rust Index"Want to advertise here? Reach out! filtra@filtra.ioWe did it! For the first time in the history of the Rust Jobs Report, we were able to find over a thousand job postings!How Many Companies Use Rust?Despite the new high watermark for number of jobs collected, the number of hiring companies only grew slightly from last month, reaching 108.What Companies Use Rust Most?Well, um, if you want a Rust job, might we suggest a little company you've probably never heard of called Amazon? Amazon single-handedly pushed us into the 1000+ jobs range by adding 50+ jobs month over month. The rest of the top ten are basically our usual suspects.What Other Companies Use Rust?Our index of Rust companies is always growing. This month an interesting addition was Bun, the Javascript runtime. If you know of a company that should be added to our index, don't hesitate to reach out: filtra@filtra.ioWhat Industries Use Rust Most?If you didn't know, AWS stands for AMAZON Web Services, so yeah cloud/infrastructure is by far the leading industry for Rust. But, we're most excited about the growth in iot/robotics/automotive that we've been observing over the last several months. We think Rust is the language of the future for robotics!What Other Industries Use Rust?Rust is everywhere now, but it's definitely most suited to certain tasks.Are Rust Jobs Only For Senior Devs?We saw a record number of junior level jobs this month. Before you jump for joy, we did also slightly change our methodology for labeling jobs as junior. Nonetheless, we still think this month showed strong growth in entry-level Rust jobs.]]></content:encoded></item><item><title>Smuggling arbitrary data through an emoji</title><link>https://paulbutler.org/2025/smuggling-arbitrary-data-through-an-emoji/</link><author>/u/kibwen</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 18:53:04 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[With ZWJ (Zero Width Joiner) sequences you could in theory encode an unlimited amount of data in a single emoji.Is it really possible to encode arbitrary data in a single emoji? yes, although I found an approach without ZWJ. In fact, you can encode data in  unicode character. This sentence has a hidden messageó …Ÿó …˜ó „ó …ó …©ó „œó „ó …©ó …Ÿó …¥ó „ó …–ó …Ÿó …¥ó …žó …”ó „ó …¤ó …˜ó …•ó „ó …˜ó …™ó …”ó …”ó …•ó …žó „ó …ó …•ó …£ó …£ó …‘ó …—ó …•ó „ó …™ó …žó „ó …¤ó …˜ó …•ó „ó …¤ó …•ó …¨ó …¤ó „‘. (Try pasting it into this decoder)Unicode represents text as a sequence of , each of which is basically just a number that the Unicode Consortium has assigned meaning to.
Usually, a specific codepoint is written as , where  is a number represented as uppercase hexadecimal.For simple latin-alphabet text, there is a one-to-one mapping between Unicode codepoints and characters that appear on-screen. For example,
 represents the character .For other writing systems, some on-screen characters may be represented by multiple codepoints. The character à¤•à¥€
(in Devanagari script) is represented by a consecutive pairing of the codepoints  and .Unicode designates 256 codepoints as â€œvariation selectorsâ€, named VS-1 to VS-256. These have no on-screen representation of their own, but are used to modify
the presentation of the preceeding character.Most unicode characters do not have variations associated with them. Since unicode is an evolving standard and aims to be future-compatible,
variation selectors are supposed to be preserved during transformations, even if their meaning is not known by the code handling them.
So the codepoint  (â€œgâ€) followed by  (VS-2) renders as a lowercase â€œgâ€, exactly the same as  alone. But if you copy and paste it, the
variation selector will tag along with it.Since 256 is exactly enough variations to represent a single byte, this gives us a way to â€œhideâ€ one byte of data in any other unicode codepoint.As it turns out, the Unicode spec does not specifically say anything about sequences
of multiple variation selectors, except to imply that they should be ignored during rendering.See where Iâ€™m going with this?We can concatenate a sequence of variation selectors together to represent any arbitrary byte string.For example, letâ€™s say we want to encode the data [0x68, 0x65, 0x6c, 0x6c, 0x6f], which represents the text â€œhelloâ€. We can do this by
converting each byte into a corresponding variation selector, and then concatenating them together.To convert from a byte to a variation selector, we can do something like this Rust code:(byte: ) ->  {
     byte  {
        char::from_u32( byte ).unwrap()
    }  {
        char::from_u32( (byte ) ).unwrap()
    }
}
To encode a series of bytes, we can concatenate a number of these variation selectors after a base character.(base: , bytes: []) -> String {
     result  String::new();
    result.push(base);
     byte  bytes {
        result.push(byte_to_variation_selector(byte));
    }
    result
}
Then to encode the bytes [0x68, 0x65, 0x6c, 0x6c, 0x6f], we can run:() {
    println(, encode(, [, , , , ]));
}
It just looks like a regular emoji, but try pasting it into the decoder.If we instead use the debug formatter, we see whatâ€™s going on:() {
    println(, encode(, [, , , , ]));
}
"ðŸ˜Š\u{e0158}\u{e0155}\u{e015c}\u{e015c}\u{e015f}"
This reveals the characters that were â€œhiddenâ€Â in the original output.Decoding is similarly straightforward.(variation_selector: ) -> Option {
     variation_selector  variation_selector ;
     (..).contains(variation_selector) {
        Some((variation_selector ) )
    }  (..).contains(variation_selector) {
        Some((variation_selector ) )
    }  {
        None
    }
}

(variation_selectors: ) -> Vec {
     result  Vec::new();
    
     variation_selector  variation_selectors.chars() {
         Some(byte)  variation_selector_to_byte(variation_selector) {
            result.push(byte);
        } result.is_empty() {
             result;
        }
            }

    result
}
 std::::from_utf8;

() {
     result  encode(, [, , , , ]);
    println(, from_utf8(decode(result)).unwrap()); }
Note that the base character does not need to be an emoji â€“ the treatment of variation selectors is the same
with regular characters. Itâ€™s just more fun with emoji.To be clear, this is an abuse of unicode and you shouldnâ€™t do it. If your mind is wandering to practical use cases for this, shut it down.That said, I can think of a couple of nefarious ways this could be (ab)used:1. Sneaking data past human content filtersSince data encoded this way are invisible once rendered, a human moderator or reviewer will not know they are there.There are techniques for using subtle variations in text to â€œwatermarkâ€ a message, so that if it is sent to a number of people and then
leaked, itâ€™s possible to trace it to the original recipient. Variation selector sequences are a way to do this that survives most copy/pastes
and allows arbitrary data density. You could go so far as to watermark  if you wanted to.Addendum: can an LLM decode it?Since this made it on Hacker News, some people have asked about how LLMs
deal with this hidden data.Generally, tokenizers  seem to preserve the variation selectors as tokens, so in theory the model has access to them.
OpenAIâ€™s tokenizer is a good sanity check of this:Overall though, models donâ€™t even seem willing to try decoding them internally. However, when paired with a code interpreter, some models are
actually able to solve them!Hereâ€™s an example of Gemini 2 Flash solving one in a remarkable seven seconds, using Codename Goose and
foreverVM (disclaimer: I work on foreverVM).]]></content:encoded></item><item><title>Apache Kafka vs. Fluvio Benchmarks</title><link>https://www.reddit.com/r/rust/comments/1invl4e/apache_kafka_vs_fluvio_benchmarks/</link><author>/u/drc1728</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 16:56:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Fluvio is a next-generation distributed streaming engine, crafted in Rust over the last six years. It follows the conceptual patterns of Apache Kafka, and adds the programming design patterns of Rust and WebAssembly based stream processing framework called Stateful DataFlow (SDF). This makes Fluvio a complete platform for event streaming.Given that Apache Kafka is the standard in distributed streaming, we figured we keep it simple and compare Apache Kafka and Fluvio.The results are as youâ€™d expect.]]></content:encoded></item><item><title>Tired of recruiters judging you by your GitHub contributions? Meet FakeHub.</title><link>https://www.reddit.com/r/rust/comments/1innoq1/tired_of_recruiters_judging_you_by_your_github/</link><author>/u/RealLordOfWizard</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 10:05:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[You know those posts where people are like:"Senior devs barely have any GitHub contributions!""Real work doesnâ€™t happen in green squares!""If your hiring manager checks your GitHub graph, run!"Yeah, well... I made a tool for that. â€“ a fake GitHub contribution history generator ðŸŽ‰. Built in  ðŸ¦€ using .âš   Itâ€™s a joke. But you can still use it. Iâ€™m not your mom.ðŸ‘‰ FakeHub on GitHub â­ Give it a star if it made you laugh. Or donâ€™t. I already faked my contributions anyway.#FakeItTillYouMakeIt #DevLife #RustLang #GitHub #FakeHub]]></content:encoded></item><item><title>A tour of Rust&apos;s standard library traits</title><link>https://github.com/pretzelhammer/rust-blog/blob/master/posts/tour-of-rusts-standard-library-traits.md</link><author>/u/AlexandraLinnea</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 09:50:26 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Niko Matsakis - How I learned to stop worrying and love the LLM</title><link>https://smallcultfollowing.com/babysteps/blog/2025/02/10/love-the-llm/</link><author>/u/OptimalFa</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 05:51:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I believe that AI-powered development tools can be a game changer for Rustâ€”and vice versa. At its core, my argument is simple: AIâ€™s ability to explain and diagnose problems with rich context can help people get over the initial bump of learning Rust in a way that canned diagnostics never could, no matter how hard we try. At the same time, rich type systems like Rustâ€™s give AIs a lot to work with, which could be used to help them avoid hallucinations and validate their output. This post elaborates on this premise and sketches out some of the places where I think AI could be a powerful boost.Perceived learning curve is challenge #1 for RustIs Rust good for every project? No, of course not. But itâ€™s absolutely  for some thingsâ€”specifically, building reliable, robust software that performs well at scale. This is no accident. Rustâ€™s design is intended to surface important design questions (often in the form of type errors) and to give users the control to fix them in whatever way is best.But this same strength is also Rustâ€™s biggest challenge. Talking to people within Amazon about adopting Rust, perceived complexity and fear of its learning curve is the biggest hurdle. Most people will say, â€œRust seems interesting, but I donâ€™t need it for this problemâ€. And you know, theyâ€™re right! They donâ€™t  it. But that doesnâ€™t mean they wouldnâ€™t benefit from it.One of Rustâ€™s big surprises is that, once you get used to it, itâ€™s â€œsurprisingly decentâ€ at very large number of things beyond what it was designed for. Simple business logic and scripts can be very pleasant in Rust. But the phase â€œonce you get used to itâ€ in that sentence is key, since most peopleâ€™s initial experience with Rust is confusion and frustration.Rust likes to tell you  (but itâ€™s for your own good)Some languages are geared to say â€”that is, given any program, they aim to run it and do . JavaScript is of course the most extreme example (no semicolons? no problem!) but every language does this to some degree. Itâ€™s often quite elegant. Consider how, in Python, you write  to get the last element in the list: super handy!Rust is not (usually) like this. Rust is geared to say . The compiler is just  for a reason to reject your program. Itâ€™s not that Rust is mean: Rust just wants your program to be as good as it can be. So we try to make sure that your program will do what you  (and not just what you asked for). This is why , in Rust, will panic: sure, giving you the last element might be convenient, but how do we know you didnâ€™t have an off-by-one bug that resulted in that negative index?But that tendency to say  means that early learning can be pretty frustrating. For most people, the reward from programming comes from seeing their program runâ€”and with Rust, thereâ€™s a  of niggling details to get right before your program will run. Whatâ€™s worse, while those details are often motivated by deep properties of your program (like data races), the way they are  is as the violation of obscure rules, and the solution (â€œadd a â€) can feel random.Once you get the hang of it, Rust feels great, but getting there can be a pain. I heard a great phrase from someone at Amazon to describe this: â€œRust: the language where you get the hangover firstâ€.AI today helps soften the learning curveMy favorite thing about working at Amazon is getting the chance to talk to developers early in their Rust journey. Lately Iâ€™ve noticed an increasing trendâ€”most are using Q Developer. Over the last year, Amazon has been doing a lot of internal promotion of Q Developer, so that in and of itself is no surprise, but what did surprise me a bit is hearing from developers the  that they use it.For most of them, the most valuable part of Q Dev is authoring code but rather  it. They ask it questions like â€œwhy does this function take an  and not an ?â€ or â€œwhat happens when I move a value from one place to another?â€. Effectively, the LLM becomes an ever-present, ever-patient teacher.Scaling up the Rust expertSome time back I sat down with an engineer learning Rust at Amazon. They asked me about an error they were getting that they didnâ€™t understand. â€œThe compiler is telling me something about , what does that mean?â€ Their code looked something like this:error[E0521]: borrowed data escapes outside of function
 --> src/lib.rs:2:5
  |
1 |   async fn log_request_in_background(message: &str) {
  |                                      -------  - let's call the lifetime of this reference `'1`
  |                                      |
  |                                      `message` is a reference that is only valid in the function body
2 | /     tokio::spawn(async move {
3 | |         log_request(message);
4 | |     });
  | |      ^
  | |      |
  | |______`message` escapes the function body here
  |        argument requires that `'1` must outlive `'static`
This is a pretty good error message! And yet it requires significant context to understand it (not to mention scrolling horizontally, sheesh). For example, what is â€œborrowed dataâ€? What does it mean for said data to â€œescapeâ€? What is a â€œlifetimeâ€ and what does it mean that â€œ must outlive â€? Even assuming you get the basic point of the message, what should you  about it?The fix is easyâ€¦  you know what to doUltimately, the answer to the engineerâ€™s problem was just to insert a call to . But deciding on that fix requires a surprisingly large amount of context. In order to figure out the right next step, I first explained to the engineer that this confusing error is, in fact, what it feels like when Rust saves your bacon, and talked them through how the ownership model works and what it means to free memory. We then discussed why they were spawning a task in the first place (the answer: to avoid the latency of logging)â€”after all, the right fix might be to just not spawn at all, or to use something like rayon to block the function until the work is done.Once we established that the task needed to run asynchronously from its parent, and hence had to own the data, we looked into changing the log_request_in_background function to take an  so that it could avoid a deep clone. This would be more efficient, but only if the caller themselves could cache the  somewhere. It turned out that the origin of this string was in another teamâ€™s code and that this code only returned an . Refactoring that code would probably be the best long term fix, but given that the strings were expected to be quite short, we opted to just clone the string.You can learn a lot from a Rust errorAn error message is often your first and best chance to teach somebody something.â€”Esteban KÃ¼ber (paraphrased)Working through this error was valuable. It gave me a chance to teach this engineer a number of concepts. I think it demonstrates a bit of Rustâ€™s promiseâ€”the idea that learning Rust will make you a better programmer overall, regardless of whether you are using Rust or not.Despite all the work we have put into our compiler error messages, this kind of detailed discussion is clearly something that we could never achieve. Itâ€™s not because we donâ€™t want to! The original concept for , for example, was to present a customized explanation of each error was tailored to the userâ€™s code. But we could never figure out how to implement that.And yet tailored, in-depth explanation is  something an LLM could do. In fact, itâ€™s something they already do, at least some of the timeâ€”though in my experience the existing code assistants donâ€™t do nearly as good a job with Rust as they could.What makes a good AI opportunity?Emery Berger is a professor at UMass Amherst who has been exploring how LLMs can improve the software development experience. Emery emphasizes how AI can help  from â€œtool to goalâ€. In short, todayâ€™s tools (error messages, debuggers, profilers) tell us things about our program, but they stop there. Except in simple cases, they canâ€™t help us figure out what to do about itâ€”and this is where AI comes in.When I say AI, I am not talking (just) about chatbots. I am talking about programs that weave LLMs into the process, using them to make heuristic choices or proffer explanations and guidance to the user. Modern LLMs can also do more than just rely on their training and the prompt: they can be given access to APIs that let them query and get up-to-date data.I think AI will be most useful in cases where solving the problem requires external context not available within the program itself. Think back to my explanation of the  error, where knowing the right answer depended on how easy/hard it would be to change other APIs.Where I think Rust should leverage AIIâ€™ve thought about a lot of places I think AI could help make working in Rust more pleasant. Here is a selection.Deciding whether to change the function body or its signatureThis function will give a type error, because the signature (thanks to lifetime elision) promises to return a string borrowed from  but actually returns a string borrowed from . Nowâ€¦what is the right fix? Itâ€™s very hard to tell in isolation! It may be that in fact the code was meant to be  (in which case the current signature is correct). Or perhaps it was meant to be something that sometimes returns  and sometimes returns , in which case the signature of the function was wrong. Today, we take our best guess. But AI could help us offer more nuanced guidance.Translating idioms from one language to anotherPeople often ask me questions like â€œhow do I make a visitor in Rust?â€ The answer, of course, is â€œit depends on what you are trying to doâ€. Much of the time, a Java visitor is better implemented as a Rust enum and match statements, but there is a time and a place for something more like a visitor. Guiding folks through the decision tree for how to do non-trivial mappings is a great place for LLMs.Figuring out the right type structureWhen I start writing a Rust program, I start by authoring type declarations. As I do this, I tend to think ahead to how I expect the data to be accessed. Am I going to need to iterate over one data structure while writing to another? Will I want to move this data to another thread? The setup of my structures will depend on the answer to these questions.I think a lot of the frustration beginners feel comes from not having a â€œfeelâ€ yet for the right way to structure their programs. The structure they would use in Java or some other language often wonâ€™t work in Rust.I think an LLM-based assistant could help here by asking them some questions about the kinds of data they need and how it will be accessed. Based on this it could generate type definitions, or alter the definitions that exist.A follow-on to the previous point is that, in Rust, when your data access patterns change as a result of refactorings, it often means you need to do more wholesale updates to your code. A common example for me is that I want to split out some of the fields of a struct into a substruct, so that they can be borrowed separately. This can be quite non-local and sometimes involves some heuristic choices, like â€œshould I move this method to be defined on the new substruct or keep it where it is?â€.Migrating consumers over a breaking changeWhen you run the  command today it will automatically apply various code suggestions to cleanup your code. With the upcoming Rust 2024 edition,  will do the same but for edition-related changes. All of the logic for these changes is hardcoded in the compiler and it can get a bit tricky.For editions, we intentionally limit ourselves to local changes, so the coding for these migrations is usually not  bad, but there are some edge cases where itâ€™d be really useful to have heuristics. For example, one of the changes we are making in Rust 2024 affects â€œtemporary lifetimesâ€. It can affect when destructors run. This almost never matters (your vector will get freed a bit earlier or whatever) but it  matter quite a bit, if the destructor happens to be a lock guard or something with side effects. In practice when I as a human work with changes like this, I can usually tell at a glance whether something is likely to be a problemâ€”but the heuristics I use to make that judgment are a combination of knowing the name of the types involved, knowing something about the way the program works, and perhaps skimming the destructor code itself. We could hand-code these heuristics, but an LLM could do it and better, and if could ask questions if it was feeling unsure.Now imagine you are releasing the 2.x version of your library. Maybe your API has changed in significant ways. Maybe one API call has been broken into two, and the right one to use depends a bit on what you are trying to do. Well, an LLM can help here, just like it can help in translating idioms from Java to Rust.I imagine the idea of having an LLM help you migrate makes some folks uncomfortable. I get that. Thereâ€™s no reason it has to be mandatoryâ€”I expect we could always have a more limited, precise migration available.Optimize your Rust code to eliminate hot spotsPremature optimization is the root of all evil, or so Donald Knuth is said to have said. Iâ€™m not sure about  evil, but I have definitely seen people rathole on microoptimizing a piece of code before they know if itâ€™s even expensive (or, for that matter, correct). This is doubly true in Rust, where cloning a small data structure (or reference counting it) can often make your life a lot simpler. Llogiqâ€™s great talks on Easy Mode Rust make exactly this point. But hereâ€™s a question, suppose youâ€™ve been taking this advice to heart, inserting clones and the like, and you find that your program  running kind of slow? How do you make it faster? Or, even worse, suppose that you are trying to turn our network service. You are looking at the blizzard of available metrics and trying to figure out what changes to make. What do you do? To get some idea of what is possible, check out Scalene, a Python profiler that is also able to offer suggestions as well (from Emery Bergerâ€™s group at UMass, the professor I talked about earlier).Diagnose and explain miri and sanitizer errorsLetâ€™s look a bit to the future. I want us to get to a place where the â€œminimum barâ€ for writing unsafe code is that you test that unsafe code with some kind of sanitizer that checks for both C and Rust UBâ€”something like miri today, except one that works â€œat scaleâ€ for code that invokes FFI or does other arbitrary things. I expect a smaller set of people will go further, leveraging automated reasoning tools like Kani or Verus to prove statically that their unsafe code is correct.From my experience using miri today, I can tell you two things. (1) Every bit of unsafe code I write has some trivial bug or other. (2) If you enjoy puzzling out the occasionally inscrutable error messages you get from Rust, youâ€™re gonna  miri! To be fair, miri has a much harder jobâ€”the (still experimental) rules that govern Rust aliasing are intended to be flexible enough to allow all the things people want to do that the borrow checker doesnâ€™t permit. This means they are much more complex. It also means that explaining why you violated them (or may violate them) is that much more complicated.Just as an AI can help novices understand the borrow checker, it can help advanced Rustaceans understand tree borrows (or whatever aliasing model we wind up adopting). And just as it can make smarter suggestions for whether to modify the function body or its signature, it can likely help you puzzle out a good fix.Rustâ€™s emphasis on â€œreliabilityâ€ makes it a great target for AIAnyone who has used an LLM-based tool has encountered hallucinations, where the AI just makes up APIs that â€œseem like they ought to existâ€. And yet anyone who has used  knows that â€œif it compiles, it worksâ€ is true may more often than it has a right to be. This suggests to me that any attempt to use the Rust compiler to validate AI-generated code or solutions is going to also help ensure that the code is correct.AI-based code assistants right now donâ€™t really have this property. Iâ€™ve noticed that I kind of have to pick between â€œshallow but correctâ€ or â€œdeep but hallucinatingâ€. A good example is  statements. I can use rust-analyzer to fill in the match arms and it will do a perfect job, but the body of each arm is . Or I can let the LLM fill them in and it tends to cover most-but-not-all of the arms but it generates bodies. I would love to see us doing deeper integration, so that the tool is talking to the compiler to get perfect answers to questions like â€œwhat variants does this enum haveâ€ while leveraging the LLM for open-ended questions like â€œwhat is the body of this armâ€.Overall AI reminds me a lot of the web around the year 2000. Itâ€™s clearly overhyped. Itâ€™s clearly being used for all kinds of things where it is not needed. And itâ€™s clearly going to change everything.If you want to see examples of what is possible, take a look at the ChatDBG videos published by Emery Bergerâ€™s group. You can see how the AI sends commands to the debugger to explore the program state before explaining the root cause. I love the video debugging bootstrap.py, as it shows the AI applying domain knowledge about statistics to debug and explain the problem.My expectation is that compilers of the future will not contain nearly so much code geared around authoring diagnostics. Theyâ€™ll present the basic error, sure, but for more detailed explanations theyâ€™ll turn to AI. It wonâ€™t be just a plain old foundation model, theyâ€™ll use RAG techniques and APIs to let the AI query the compiler state, digest what it finds, and explain it to users. Like a good human tutor, the AI will tailor its explanations to the user, leveraging the userâ€™s past experience and intuitions (oh, and in the userâ€™s chosen language).I am aware that AI has some serious downsides. The most serious to me is its prodigous energy use, but there are also good questions to be asked about the way that training works and the possibility of not respecting licenses. The issues are real but avoiding AI is not the way to solve them. Just in the course of writing this post, DeepSeek was announced, demonstrating that there is a lot of potential to lower the costs of training. As far as the ethics and legality, that is a very complex space. Agents are already doing a lot to get better there, but note also that most of the applications I am excited about do not involve writing code so much as helping people understand and alter the code theyâ€™ve written.]]></content:encoded></item><item><title>This Week in Rust 586</title><link>https://this-week-in-rust.org/blog/2025/02/12/this-week-in-rust-586/</link><author>TWiR Contributors</author><category>This week in Rust</category><category>dev</category><category>rust</category><pubDate>Wed, 12 Feb 2025 05:00:00 +0000</pubDate><source url="https://this-week-in-rust.org/">This Week in Rust</source><content:encoded><![CDATA[This week's crate is esp32-mender-client, a client for ESP32 to execute firmware updates and remote commands.Thanks to Kelvin for the self-suggestion!An important step for RFC implementation is for people to experiment with the
implementation and give feedback, especially before stabilization.  The following
RFCs would benefit from user testing before moving forward:No calls for testing were issued this week.No calls for testing were issued this week.No calls for testing were issued this week.If you are a feature implementer and would like your RFC to appear on the above list, add the new 
label to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature
need testing.Always wanted to contribute to open-source projects but did not know where to start?
Every week we highlight some tasks from the Rust community for you to pick and get started!Some of these tasks may also have mentors available, visit the task page for more information.No Calls for participation were submitted this week.Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.No Calls for papers or presentations were submitted this week.A relatively neutral week, with lots of real changes but most small in
magnitude. Most significant change is rustdoc's move of JS/CSS minification to
build time which cut doc generation times on most benchmarks fairly
significantly.3 Regressions, 5 Improvements, 1 Mixed; 2 of them in rollups
32 artifact comparisons made in totalNo RFCs were approved this week.Every week, the team announces the 'final comment period' for RFCs and key PRs
which are reaching a decision. Express your opinions now.No Cargo Tracking Issues or PRs entered Final Comment Period this week.No Language Team Proposals entered Final Comment Period this week.No Language Reference RFCs entered Final Comment Period this week.No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.Rusty Events between 2025-02-12 - 2025-03-12 ðŸ¦€If you are running a Rust event please add it to the calendar to get
it mentioned here. Please remember to add a link to the event too.
Email the Rust Community Team for access.Just because things are useful doesn't mean they are magically sound.]]></content:encoded></item><item><title>What is making a static library in Rust being much large than Go, Zig, and C#?</title><link>https://www.reddit.com/r/rust/comments/1inh4vk/what_is_making_a_static_library_in_rust_being/</link><author>/u/metaltyphoon</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 02:59:12 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hello! I've been trying to understand how I can trim a Rust  to its bare minimum. For instance, I've create a repository to show what I mean. I have a much large static library which targets iOS and the sizes are in the ~30MB range when in release mode. This is not really ideal because in an  I need to pack one for macOS and two for iOS (simulator and non simulator version) and the file is almost 100MB in debug mode. Any way to help would be appreciated. Thanks ]]></content:encoded></item><item><title>A tool to analyse package dependancies within a Cargo workspace</title><link>https://www.reddit.com/r/rust/comments/1inerct/a_tool_to_analyse_package_dependancies_within_a/</link><author>/u/j44dz</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 01:00:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I'm a software architecture enthusiast and use Rust since a few month. My main concern when developing software is to come up with a architecture which is easy to maintain in the long run and hence sustainable. I found myself creating and updating a dependancy diagram for my new projects. I realized that I can just automate this. So I implemented the cargo-workspace-analyzerThis CLI tool creates a mermaid diagram by parsing the manifest files (cargo.toml) of the workspace. I was thinking that this might be useful for others too, so I made it open-source, see the GitHub repository. Recently I added some basic metric calculations for each package to further find architectural debts. More metrics will likely follow.What do you think? Feel free to try it out :)]]></content:encoded></item><item><title>Introducing json_preprocessor</title><link>https://www.reddit.com/r/rust/comments/1in7afe/introducing_json_preprocessor/</link><author>/u/hajhawa</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 19:38:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Do you ever feel like your json files need a bit more to make them interesting? Do you crave job security? Introducing json_preprocessor, sometimes shortened to jsonpp. It is a functional interpreted programming language that evaluates to JSON.This is not a real tool, it's a joke I spent a bit of time on, please for the love of god don't use it.]]></content:encoded></item><item><title>Tip of the day #4: Type annotations on Rust match patterns</title><link>https://gaultier.github.io/blog/tip_of_the_day_4.html</link><author>/u/broken_broken_</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 16:00:21 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[OC] Systems Programming: Everything is trying to kill you and how to stop it</title><link>https://www.reddit.com/r/rust/comments/1in1t0t/oc_systems_programming_everything_is_trying_to/</link><author>/u/Buzz_Cut</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 15:55:40 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi everyone! I posted here about a week ago looking for some suggestions on types of bugs rust avoids. I hosted a workshop for some undergrads at my school about what even is systems programming followed by crash course live coding session in Rust.It's my first time holding a talk so I would love any of your feedback. There were some technical difficulties but 95% of the talk came out unscathed lol.Keep in mind that the talk was geared towards people who probably have never even heard of systems programming or memory. I hope my explanations can help some beginners out there!P.S. my club will be running a workshop on mathematical modeling of robots this coming week. If you are interested please join our discord! https://hacklab.space/discord]]></content:encoded></item><item><title>FOSDEM 2025 - Rust for Linux</title><link>https://fosdem.org/2025/schedule/event/fosdem-2025-6507-rust-for-linux/</link><author>/u/l-const</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 14:53:24 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Rust for Linux is the project adding support for the Rust language to the Linux kernel.This talk will give a high-level overview of the project and will walk its history from the beginning -- it sounds like it was yesterday when we started, but it has been more than 4 years now.How did the endeavor start? What challenges have we faced? Who are the people behind it? Which projects do we collaborate with? How are we reconfiguring a large system with huge inertia while it keeps running? Can a similar approach be applied to other projects? What do kernel developers think about Rust?Speaking about Rust, why did we go with Rust, and not something else? How stable is the Rust we use in the kernel? What does it mean to use unstable features in this context? How did we hedge against those? What is the situation with distribution toolchain support? What about GCC and Rust?And most importantly, since this is open source: how can someone contribute?]]></content:encoded></item><item><title>Project idea to make open source alternative to a paid app</title><link>https://www.reddit.com/r/rust/comments/1in0355/project_idea_to_make_open_source_alternative_to_a/</link><author>/u/Due-Web-1611</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 14:41:50 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Yo! We want to make an open source alternative to something that is currently paid. Any ideas? Difficult projects are also welcome! Could be anything you wish was free/open-source ]]></content:encoded></item><item><title>PL/Rust: a loadable procedural language for writing PostgreSQL functions in Rust that compile to native code</title><link>https://github.com/tcdi/plrust</link><author>/u/kibwen</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 13:52:03 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Migrating a Ray-Tracing Calculator from C to Rust â€“ Planning a Masterâ€™s Thesis &amp; Looking for Advice (C, Rust, BSP-Tree)</title><link>https://www.reddit.com/r/rust/comments/1imws36/migrating_a_raytracing_calculator_from_c_to_rust/</link><author>/u/Smil3More</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 11:48:43 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am planning a  in which I need to modernize and parallelize an existing scientific C program for electromagnetic wave propagation calculation/simulation. The goal is to migrate the program to  to improve long-term maintainability, safety, and performance.It's a Linux command-line program that takes a TXT input file with room and material specifications, calculates the electromagnetic spread (heatmap), and outputs a TXT file, which is later converted into a graphical solution in another program.My focus is solely on the calculation part.Simulates electromagnetic wave propagation using ray tracing. (only mathematical, no graphical conversion)BSP tree (Binary Space Partitioning) as the core data structure for geometry management.C-based, currently single-threaded, running on . (takes a loooong time to calculate)Future goals: CPU parallelization & potential GPU extension (OpenCL, Vulkan Compute).BSP Tree in Rust â€“ Feasible or alternative approaches?Is Rust well-suited for BSP trees, or are there better parallel alternatives?Are there  that could be useful for ray tracing & BSP trees?Rust beginner with decent programming experience â€“ Is migration realistic?I have solid programming experience in C++, Python, Dart but very little Rust knowledge.Is Rust a good choice for complex scientific simulations, or is the learning curve too steep?Full migration vs. partial migration & C/Rust interoperabilityWould it make more sense to migrate only the core algorithm to Rust and keep the rest in C?Has anyone worked with C/Rust interop via FFI or for a mixed-language approach?How practical is a , running Rust and C in parallel?I would appreciate any best practices, experiences, and resources on using Rust for scientific applications! ðŸš€]]></content:encoded></item><item><title>jiff 0.2.0 released - A relatively new datetime library with automatic tzdb support, DST safe arithmetic/rounding and more</title><link>https://github.com/BurntSushi/jiff/releases/tag/0.2.0</link><author>/u/burntsushi</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 02:58:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust youtubers</title><link>https://www.reddit.com/r/rust/comments/1imiirv/rust_youtubers/</link><author>/u/CodeMurmurer</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Mon, 10 Feb 2025 22:04:36 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I watch a lot of educational content. Like from "The Cherno" but I have been wondering is there code review type channel but for rust? Or other ones that have actual valuable content.   submitted by    /u/CodeMurmurer ]]></content:encoded></item><item><title>Trying to find a programming language concept I saw on this subreddit once</title><link>https://www.reddit.com/r/rust/comments/1imh395/trying_to_find_a_programming_language_concept_i/</link><author>/u/nextProgramYT</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Mon, 10 Feb 2025 21:06:10 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I believe the concept had something to do with mutability or borrow checking. If I remember correctly it divided variables into 4(?) different categories depending on I think how (often) a variable could be changed? Each category had sort of a fancy name, something from programming language theory/design I assume. I know that's not much info but I can't track it down and it's annoying me lol, anyone know it?]]></content:encoded></item><item><title>FOSDEM 2025 - The state of Rust trying to catch up with Ada</title><link>https://fosdem.org/2025/schedule/event/fosdem-2025-5356-the-state-of-rust-trying-to-catch-up-with-ada/</link><author>/u/dpc_pw</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Mon, 10 Feb 2025 18:40:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Ada has held its own in the safety critical space for just over 4 decades. Over the last 10 years Rust has pushed into the same space with varying success. Among other features, most notably Rust is missing the powerful declarations and convenience of use of subtypes. In this talk I will go into the various features that make Ada so useful for ensuring the absence of bugs, and where Rust stands on each of these features.]]></content:encoded></item><item><title>A demonstration of writing a simple Windows driver in Rust</title><link>https://scorpiosoftware.net/2025/02/08/writing-a-simple-driver-in-rust/</link><author>/u/kibwen</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Mon, 10 Feb 2025 18:27:02 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The Rust language ecosystem is growing each day, its popularity increasing, and with good reason. Itâ€™s the only mainstream language that provides memory and concurrency safety at compile time, with a powerful and rich build system (cargo), and a growing number of packages (crates).My daily driver is still C++, as most of my work is about low-level system and kernel programming, where the Windows C and COM APIs are easy to consume. Rust is a system programming language, however, which means it plays, or at least can play, in the same playground as C/C++. The main snag is the verbosity required when converting C types to Rust. This â€œverbosityâ€ can be alleviated with appropriate wrappers and macros. I decided to try writing a simple WDM driver that is not useless â€“ itâ€™s a Rust version of the â€œBoosterâ€ driver I demonstrate in my book (Windows Kernel Programming), that allows changing the priority of any thread to any value.To prepare for building drivers, consult Windows Drivers-rs, but basically you should have a WDK installation (either normal or the EWDK). Also, the docs require installing LLVM, to gain access to the Clang compiler. I am going to assume you have these installed if youâ€™d like to try the following yourself.We can start by creating a new Rust library project (as a driver is a technically a DLL loaded into kernel space):We can open the booster folder in VS Code, and begin are coding. First, there are some preparations to do in order for actual code to compile and link successfully. We need a  file to tell cargo to link statically to the CRT. Add a  file to the root booster folder, with the following code:fn main() -> Result<(), wdk_build::ConfigError> {
    std::env::set_var("CARGO_CFG_TARGET_FEATURE", "crt-static");
    wdk_build::configure_wdk_binary_build()
}
(Syntax highlighting is imperfect because the WordPress editor I use does not support syntax highlighting for Rust)Next, we need to edit  and add all kinds of dependencies. The following is the minimum I could get away with:[package]
name = "booster"
version = "0.1.0"
edition = "2021"

[package.metadata.wdk.driver-model]
driver-type = "WDM"

[lib]
crate-type = ["cdylib"]
test = false

[build-dependencies]
wdk-build = "0.3.0"

[dependencies]
wdk = "0.3.0"       
wdk-macros = "0.3.0"
wdk-alloc = "0.3.0" 
wdk-panic = "0.3.0" 
wdk-sys = "0.3.0"   

[features]
default = []
nightly = ["wdk/nightly", "wdk-sys/nightly"]

[profile.dev]
panic = "abort"
lto = true

[profile.release]
panic = "abort"
lto = true
The important parts are the WDK crates dependencies. Itâ€™s time to get to the actual code in . We start by removing the standard library, as it does not exist in the kernel:Next, weâ€™ll add a few  statements to make the code less verbose:use core::ffi::c_void;
use core::ptr::null_mut;
use alloc::vec::Vec;
use alloc::{slice, string::String};
use wdk::*;
use wdk_alloc::WdkAllocator;
use wdk_sys::ntddk::*;
use wdk_sys::*;
The  crate provides the low level interop kernel functions. the  crate provides higher-level wrappers.  is an interesting one. Since we canâ€™t use the standard library, you would think the types like  are not available, and technically thatâ€™s correct. However,  is actually defined in a lower level module named , that can be used outside the standard library. This works because the only requirement for  is to have a way to allocate and deallocate memory. Rust exposes this aspect through a global allocator object, that anyone can provide. Since we have no standard library, there is no global allocator, so one must be provided. Then,  (and ) can work normally:#[global_allocator]
static GLOBAL_ALLOCATOR: WdkAllocator = WdkAllocator;
This is the global allocator provided by the WDK crates, that use and to manage allocations, just like would do manually.Next, we add two  crates to get the support for the allocator and a panic handler â€“ another thing that must be provided since the standard library is not included.  has a setting to abort the driver (crash the system) if any code panics:extern crate wdk_panic;
extern crate alloc;
Now itâ€™s time to write the actual code. We start with , the entry point to any Windows kernel driver:#[export_name = "DriverEntry"]
pub unsafe extern "system" fn driver_entry(
    driver: &mut DRIVER_OBJECT,
    registry_path: PUNICODE_STRING,
) -> NTSTATUS {
Those familiar with kernel drivers will recognize the function signature (kind of). The function name is  to conform to the snake_case Rust naming convention for functions, but since the linker looks for , we decorate the function with the  attribute. You could use  and just ignore or disable the compilerâ€™s warning, if you prefer.We can use the familiar  macro, that was reimplemented by calling , as you would if you were using C/C++. You can still call , mind you, but  is just easier:println!("DriverEntry from Rust! {:p}", &driver);
let registry_path = unicode_to_string(registry_path);
println!("Registry Path: {}", registry_path);
Unfortunately, it seems  does not yet support a , so we can write a function named  to convert a  to a normal Rust string:fn unicode_to_string(str: PCUNICODE_STRING) -> String {
    String::from_utf16_lossy(unsafe {
        slice::from_raw_parts((*str).Buffer, (*str).Length as usize / 2)
    })
}
Back in , our next order of business is to create a device object with the name â€œ\Device\Boosterâ€:let mut dev = null_mut();
let mut dev_name = UNICODE_STRING::default();
string_to_ustring("\\Device\\Booster", &mut dev_name);

let status = IoCreateDevice(
    driver,
    0,
    &mut dev_name,
    FILE_DEVICE_UNKNOWN,
    0,
    0u8,
    &mut dev,
);
The  function converts a Rust string to a :fn string_to_ustring(s: &str, uc: &mut UNICODE_STRING) -> Vec<u16> {
    let mut wstring: Vec<_> = s.encode_utf16().collect();
    uc.Length = wstring.len() as u16 * 2;
    uc.MaximumLength = wstring.len() as u16 * 2;
    uc.Buffer = wstring.as_mut_ptr();
    wstring
}
This may look more complex than we would like, but think of this as a function that is written once, and then just used all over the place. In fact, maybe there is such a function already, and just didnâ€™t look hard enough. But it will do for this driver.If device creation fails, we return a failure status:if !nt_success(status) {
    println!("Error creating device 0x{:X}", status);
    return status;
}
 is similar to the  macro provided by the WDK headers.Next, weâ€™ll create a symbolic link so that a standard  call could open a handle to our device:let mut sym_name = UNICODE_STRING::default();
let _ = string_to_ustring("\\??\\Booster", &mut sym_name);
let status = IoCreateSymbolicLink(&mut sym_name, &mut dev_name);
if !nt_success(status) {
    println!("Error creating symbolic link 0x{:X}", status);
    IoDeleteDevice(dev);
    return status;
}
All thatâ€™s left to do is initialize the device object with support for Buffered I/O (weâ€™ll use  for simplicity), set the driver unload routine, and the major functions we intend to support:    (*dev).Flags |= DO_BUFFERED_IO;

    driver.DriverUnload = Some(boost_unload);
    driver.MajorFunction[IRP_MJ_CREATE as usize] = Some(boost_create_close);
    driver.MajorFunction[IRP_MJ_CLOSE as usize] = Some(boost_create_close);
    driver.MajorFunction[IRP_MJ_WRITE as usize] = Some(boost_write);

    STATUS_SUCCESS
}
Note the use of the Rust  type to indicate the presence of a callback.The unload routine looks like this:unsafe extern "C" fn boost_unload(driver: *mut DRIVER_OBJECT) {
    let mut sym_name = UNICODE_STRING::default();
    string_to_ustring("\\??\\Booster", &mut sym_name);
    let _ = IoDeleteSymbolicLink(&mut sym_name);
    IoDeleteDevice((*driver).DeviceObject);
}
We just call  and , just like a normal kernel driver would. We have three request types to handle â€“ , , and . Create and close are trivial â€“ just complete the IRP successfully:unsafe extern "C" fn boost_create_close(_device: *mut DEVICE_OBJECT, irp: *mut IRP) -> NTSTATUS {
    (*irp).IoStatus.__bindgen_anon_1.Status = STATUS_SUCCESS;
    (*irp).IoStatus.Information = 0;
    IofCompleteRequest(irp, 0);
    STATUS_SUCCESS
}
The  is an  but itâ€™s defined with a  containing  and . This seems to be incorrect, as  should be in a  with  (not ). Anyway, the code accesses the  member through the â€œauto generatedâ€ union, and it looks ugly. Definitely something to look into further. But it works.The real interesting function is the  handler, that does the actual thread priority change. First, weâ€™ll declare a structure to represent the request to the driver:#[repr(C)]
struct ThreadData {
    pub thread_id: u32,
    pub priority: i32,
}
The use of  is important, to make sure the fields are laid out in memory just as they would with C/C++. This allows non-Rust clients to talk to the driver. In fact, Iâ€™ll test the driver with a C++ client I have that used the C++ version of the driver. The driver accepts the thread ID to change and the priority to use. Now we can start with :unsafe extern "C" fn boost_write(_device: *mut DEVICE_OBJECT, irp: *mut IRP) -> NTSTATUS {
    let data = (*irp).AssociatedIrp.SystemBuffer as *const ThreadData;
First, we grab the data pointer from the  in the IRP, as we asked for Buffered I/O support. This is a kernel copy of the clientâ€™s buffer. Next, weâ€™ll do some checks for errors:let status;
loop {
    if data == null_mut() {
        status = STATUS_INVALID_PARAMETER;
        break;
    }
    if (*data).priority < 1 || (*data).priority > 31 {
        status = STATUS_INVALID_PARAMETER;
        break;
    }
The  statement creates an infinite block that can be exited with a . Once we verified the priority is in range, itâ€™s time to locate the thread object:let mut thread = null_mut();
status = PsLookupThreadByThreadId(((*data).thread_id) as *mut c_void, &mut thread);
if !nt_success(status) {
    break;
}
 is the one to use. If it fails, it means the thread ID probably does not exist, and we break. All thatâ€™s left to do is set the priority and complete the request with whatever status we have:        KeSetPriorityThread(thread, (*data).priority);
        ObfDereferenceObject(thread as *mut c_void);
        break;
    }
    (*irp).IoStatus.__bindgen_anon_1.Status = status;
    (*irp).IoStatus.Information = 0;
    IofCompleteRequest(irp, 0);
    status
}
The only remaining thing is to sign the driver. It seems that the crates support signing the driver if an INF or INX files are present, but this driver is not using an INF. So we need to sign it manually before deployment. The following can be used from the root folder of the project:signtool sign /n wdk /fd sha256 target\debug\booster.dll
The  uses a WDK test certificate typically created automatically by Visual Studio when building drivers. I just grab the first one in the store that starts with â€œwdkâ€ and use it.The silly part is the file extension â€“ itâ€™s a DLL and there currently is no way to change it automatically as part of cargo build. If using an INF/INX, the file extension does change to SYS. In any case, file extensions donâ€™t really mean that much â€“ we can rename it manually, or just leave it as DLL. The resulting file can be installed in the â€œnormalâ€ way for a software driver, such as using the  tool (from an elevated command window), on a machine with test signing on. Then  can be used to load the driver into the system:sc.exe sc create booster type= kernel binPath= c:\path_to_driver_file
sc.exe start booster
I used an existing C++ application that talks to the driver and expects to pass the correct structure. It looks like this:#include <Windows.h>
#include <stdio.h>

struct ThreadData {
	int ThreadId;
	int Priority;
};

int main(int argc, const char* argv[]) {
	if (argc < 3) {
		printf("Usage: boost <tid> <priority>\n");
		return 0;
	}

	int tid = atoi(argv[1]);
	int priority = atoi(argv[2]);

	HANDLE hDevice = CreateFile(L"\\\\.\\Booster",
		GENERIC_WRITE, 0, nullptr, OPEN_EXISTING, 0,
		nullptr);

	if (hDevice == INVALID_HANDLE_VALUE) {
		printf("Failed in CreateFile: %u\n", GetLastError());
		return 1;
	}

	ThreadData data;
	data.ThreadId = tid;
	data.Priority = priority;
	DWORD ret;
	if (WriteFile(hDevice, &data, sizeof(data),
		&ret, nullptr))
		printf("Success!!\n");
	else
		printf("Error (%u)\n", GetLastError());

	CloseHandle(hDevice);

	return 0;
}
Here is the result when changing a threadâ€™s priority to 26 (ID 9408):Writing kernel drivers in Rust is possible, and Iâ€™m sure the support for this will improve quickly. The WDK crates are at version 0.3, which means there is still a way to go. To get the most out of Rust in this space, safe wrappers should be created so that the code is less verbose, does not have  blocks, and enjoys the benefits Rust can provide. Note, that I may have missed some wrappers in this simple implementation.You can find a couple of more samples for KMDF Rust drivers here.]]></content:encoded></item><item><title>Redox OS - RSoC 2024: Dynamic Linking - Part 2</title><link>https://www.reddit.com/r/rust/comments/1imbqwr/redox_os_rsoc_2024_dynamic_linking_part_2/</link><author>/u/ribbon_45</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Mon, 10 Feb 2025 17:32:55 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/ribbon_45 ]]></content:encoded></item><item><title>First Steps in Game Development With Rust and Bevy</title><link>https://blog.jetbrains.com/rust/2025/02/04/first-steps-in-game-development-with-rust-and-bevy/</link><author>/u/agluszak</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Mon, 10 Feb 2025 17:09:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>
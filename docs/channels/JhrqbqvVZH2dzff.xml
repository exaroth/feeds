<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>General Dev</title><link>https://konrad.website/feeds/</link><description></description><item><title>Busting Myths About Cisco 700-150 ICS!</title><link>https://www.reddit.com/r/kubernetes/comments/1iq3umq/busting_myths_about_cisco_700150_ics/</link><author>/u/lucina_scott</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 15:33:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Carbon capture more costly than switching to renewables, researchers find</title><link>https://techxplore.com/news/2025-02-carbon-capture-renewables.html</link><author>Brajeshwar</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 15:06:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dust from Car Brakes More Harmful Than Exhaust, Study Finds</title><link>https://e360.yale.edu/digest/brake-pads-lung-damage-study</link><author>Brajeshwar</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 15:06:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alexandre Mutel a.k.a. xoofx is leaving Unity</title><link>https://mastodon.social/@xoofx/113997304444307991</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:53:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Don&apos;t &quot;optimize&quot; conditional moves in shaders with mix()+step()</title><link>https://iquilezles.org/articles/gpuconditionals/</link><author>/u/namanyayg</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:52:27 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
In this article I want to correct a popular misconception that's been making the rounds in computer graphics aficionado circles for a long time now. It has to do with conditionals when selecting between two results in the GPUs. Unfortunately there are a couple of educational websites out there that are spreading some misinformation, and it would be nice correcting that. I tried contacting the authors without success, so without further ado, here goes my attempt to fix things up a little:
So, say I have this code, which I actually published the other day: snap45(  v )
{
     s = (v);
     x = (v.x);
     x>?(s.x,):
           x>?s*():
                      (,s.y);
}
The exact details of what it does don't matter for this discussion. All we care about is the two ternary operations deciding what's the final value this function should return. Indeed, depending on the value of the variable , the function will return one of three results, which are simple to compute. I could also have implemented this function with regular  statements, and all that I'm going to say in this article stays true.
Now, here's the problem - when seeing code like this, somebody somewhere will step up and invariably propose the following "optimization", which replaces what they believe (erroneously) are "conditional branches" in the code, by arithmetic operations. They will suggest something like this: snap45(  v )
{
     s = (v);
     x = (v.x);

     w0 = (,x);
     w1 = (,x)*(-w0);
     w2 = -w0-w1;

     res0 = (s.x,);
     res1 = (s.x,s.y)*();
     res2 = (,s.y);

     w0*res0 + w1*res1 + w2*res2;
}
There are two things wrong with this practice. The first one shows an incorrect understanding of how the GPU works. In particular, the original shader code had no conditional branching in it. Selecting between a few registers with a ternary operator or with a plain  statement does not lead to conditional branching; all it involves is a conditional move (a.k.a. "select"), which is a simple instruction to route the correct bits to the destination register. You can think of it as a bitwise AND+NAND+OR on the source registers, which is a simple combinational circuit. I'll repeat it again - there is no branching, the instruction pointer isn't manipulated, there's no prediction involved, no pipe to flush, no instruction cache to invalidation, no nothing.
For the record, of course GPUs can do real branching, and those are fine and fast and totally worth it when big chunks of code and computation are to be skipped given a condition. As with all things computing, always check the generated machine code to know what is happening exactly and when. But one thing you can safely assume without having to check any generated code - when moving simple values or computations like in my original example, you are guaranteed to not branch. This has been true for decades at this point, with GPUs. And while I'm not an expert in CPUs, I am pretty sure this is true for them as well.
The second wrong thing with the supposedly optimized version is that it actually runs much slower than the original version. You can measure it in a variety of hardware. I can only assume that's because the  function is probably implemented with some sort of conditional move or subtract + bit propagation + AND. step(  x,  y )
{
     x < y ?  : ;
}
Either way, using the step() "optimization" are either using the ternary operation anyways, which produces the  or  which they will use to mask in and out the different potential outputs with a series of arithmetic multiplications and additions. Which is wasteful, the values could have been conditionally moved directly, which is what the original shader code did.
But don't take my word for it, let's look at the generated machine code for the original code I published:
GLSL x>?(s.x,):
       x>?s*():
                  (,s.y);
AMD Compiler     s0,      v3, , v1
     v4, , v0
     s1,   vcc, (v2), s0
 v3, 0, v3, vcc
 v0, v0, v4, vcc
 vcc, (v2), s1
 v1, v1, v3, vcc
 v0, 0, v0, vcc
Microsoft Compiler   r0.xy, l(, ), v0.xy
   r0.zw, v0.xy, l(, )
 r0.xy, -r0.xyxx, r0.zwzz
 r0.xy, r0.xyxx
  r1.xyzw, r0.xyxy, l4()
   r2.xy, l(,), v0.xx  r0.z, l()
 r1.xyzw, r2.yyyy, r1.xyzw, r0.zyzy
 o0.xyzw, r2.xxxx, r0.xzxz, r1.xyzw
Here we can confirm that the GPU is not branching, as I explained. Instead, according to the AMD compiler, it's performing the required comparisons ( and  - cmp=compare, gt=greater than, ngt=not greated than), and then using the result to mask the results with the bitwise operations mentioned earlier ( - cnd=conditional).
The Microsoft compiler has expressed the same idea/implementation in a different format, but you can still see the comparison ( - "lt"=less than) and the masking or conditional move ( - mov=move, c=conditionally).
There are no jump/branch instructions in these listings.
Something not related to the discussion but interesting, is that some of the  GLSL calls I had in my shader before the ternary operator we are discussing, didn't become GPU instructions but rather instruction modifiers, which is the reason you see them in the listing. This means you can think of abs() calls as being free.
So, if you ever see somebody proposing this a = ( b, c, ( y, x ) );
as an optimization to
then please correct them for me.]]></content:encoded></item><item><title>Amazon&apos;s killing a feature that let you download and backup Kindle books</title><link>https://www.theverge.com/news/612898/amazon-removing-kindle-book-download-transfer-usb</link><author>marban</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 14:51:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Starting on February 26th, 2025, Amazon is removing a feature from its website allowing you to download purchased books to a computer and then copy them manually to a Kindle over USB. It’s a feature that a lot of Kindle users are probably not aware of, given books can be more easily sent to devices over Wi-Fi, but it’s especially useful for backing up purchases or converting them to other formats compatible with non-Kindle e-readers.There are a few reasons why some Kindle users might miss this feature. It’s useful if you don’t have access to a Wi-Fi network, and although it’s a tedious process since purchased books can only be downloaded one at a time, there’s some peace of mind in knowing you have offline copies of all your books.It doesn’t happen frequently, but as  points out, Amazon has occasionally removed books from its online store and remotely deleted them from Kindles or edited titles and re-uploaded new copies to its e-readers. In 2009, the company removed copies of George Orwell’s  and , explaining the books had been mistakenly published. More recently, many of Roald Dahl’s books, including Charlie and the Chocolate Factory, were replaced with updated copies featuring modified language on various ebook platforms. It’s a reminder that you don’t actually own much of the digital content you consume, and without the ability to back up copies of ebooks, you could lose them entirely if they’re banned and removed.The feature is also the easiest way to convert books purchased from Amazon to other formats like EPUB that can be used on alternative devices such as a Kobo. Books downloaded through Amazon’s website are delivered in the older AZW3 format which allows DRM to be easily removed using various software tools.Older Kindle models also store books in the AZW3 format, and they can be extracted by connecting the device to a computer with a USB cable. But modern Kindles use the newer KFX format (first rolled out in 2015) which introduced improved font rendering, better layout capabilities, and DRM that’s nearly impossible to circumvent.The download feature is a carryover from the days when Kindles didn’t come with Wi-Fi, allowing users who also weren’t able to take advantage of Amazon’s cellular Whispernet service to get purchased books onto their e-readers. It’s still accessible through Amazon’s website by accessing your “Content Library” while logged into your account. For purchased books you select the “More actions” menu, choose “Download & transfer via USB,” select a Kindle device you have registered, and a copy of the book will be downloaded to your computer.But the pop-up that appears when selecting the download option now includes an additional warning. “Starting February 26, 2025, the ‘Download & Transfer via USB’ option will no longer be available. You can still send Kindle books to your Wi-Fi enabled devices by selecting the ‘Deliver or Remove from Device’ option.“Amazon confirmed the removal of the book download feature in a statement to . “Customers can continue reading books previously downloaded on their Kindle device, and access new content through the Kindle app, Kindle for web, as well as directly through Kindle devices with WiFi capability,” said Amazon spokesperson Jackie Burke.Once this feature goes away, you’ll still be able to manually copy ebook files and other documents to Kindles over USB using Amazon’s apps or third-party solutions like Calibre. You just won’t be able to download copies of your purchased books to a computer.Added comment from an Amazon spokesperson.]]></content:encoded></item><item><title>Altman: OpenAI not for sale, especially to competitor who is not able to beat us</title><link>https://www.axios.com/2025/02/11/openai-altman-musk-offer</link><author>/u/namanyayg</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:17:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What is Event Sourcing?</title><link>https://newsletter.scalablethread.com/p/what-is-event-sourcing</link><author>/u/scalablethread</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:05:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Traditional data storage typically focuses on the current state of an entity. For example, in an e-commerce system, you might store the current state of a customer's order: items, quantities, shipping address, etc. Event sourcing takes a different approach. Instead of storing the current state directly, it stores the events that led to that state. Each event represents a fact that happened in the past. Think of it as a detailed log of transactions on your bank statement. These events are immutable and stored in an append-only event store. The core idea is that an application's state can be derived by replaying events in the order they occurred, just like you can get your current bank balance by replaying all the transactions from the beginning. This makes Event Sourcing particularly useful for applications that require a high degree of audibility and traceability.Every change to the application state is captured as an event object in an Event Sourcing system. These events are then stored in an event store, a database optimized for handling event data. Here's a step-by-step breakdown of how Event Sourcing works:Reconstructing the state from events involves reading all the events related to an entity from the event store and applying them in sequence to reconstruct the current state. It's like simulating all the changes that have occurred to construct the current state. For example, consider an e-commerce application where an order goes through various states like "Created," "Paid," and "Shipped." To determine the current state of an order, you would:Retrieve all events related to the order from the event store.Initialize an empty order object.Apply each event to the order object in the order in which they were stored.By the end of this process, the order object will reflect the current state of the order.As the number of events grows, replaying the entire event stream to reconstruct the state can become slow and inefficient. This is where snapshots come in. A snapshot is a saved state of an entity at a specific point in time. Instead of replaying all events from the beginning, the application can load the latest snapshot and then replay only the events that occurred after the snapshot was taken. If you enjoyed this article, please hit the ❤️ like button.If you think someone else will benefit from this, then please 🔁 share this post.]]></content:encoded></item><item><title>Antarctica&apos;s Only Insect</title><link>https://www.404media.co/antarcticas-only-insect/</link><author>Becky Ferreira</author><category>dev</category><enclosure url="https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-09.17.41@2x.png" length="" type=""/><pubDate>Sat, 15 Feb 2025 14:00:13 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[Welcome back to the Abstract, 404 Media's weekly roundup of scientific studies to distract us from our present dystopia!This week, we are traveling back in time to 16th century Transylvania, so please make sure you are up to date on your bubonic plague shots. A study reconstructed wild weather events through the eyes of record-keepers during this fraught period, opening a tantalizing window into climate extremes unleashed by a vengeful God (according to contemporary reports).Then: making love the medaka way (get those anal fins ready). Next, the chillest insect in Antarctica (also: the only one). Finally, these turtles will dance for food, and yes, it’s very cute.The Haunting Weather Reports of 16th Century TransylvaniaRejoice, for this week has delivered one of the best varieties of study: Science via historical documents. Sure, ice cores and geological strata are great for reconstructing past climates, but nobody can bitch about the weather better than a good old-fashioned red-blooded member of team . To that end, researchers searched for mentions of weird weather across a trove of diaries, monastery records, travel notes, and other documents from 16th century Transylvania, during a “pivotal moment in climate history” when a centuries-long cooling event called the Little Ice Age intensified, according to researchers led by Ovidiu Răzvan Gaceu of the University of Oradea. These types of studies are packed with colorful human testimonies that can corroborate natural records. More importantly, though, they are just fun to read, especially during such an evocative time and place, freshly haunted by the vampiric spirit of Vlad the Impaler. Some highlights:In August 1526, heavy rainfall caused freak floods in Braşov that “washed the walls of the fortress, demolished the main gate, and the fish also got caught in the big church,” according to the Annals of Brașov. Fish in the church! The ultimate baptism.  In autumn 1553, people in the city of Cluj reported unusual weather events including “October strawberries.” For real, October is for pumpkins, get out of here with the strawbs. Turned out it was a bad omen—there was a plague the following winter. Keep that in mind if you see any late autumn strawberries: Kill on sight.Naturally, a lot of these accounts are heartbreaking. Locusts “sometimes covered the whole sky and destroyed grain crops” and caused terrible famines. A storm-related fire “killed 14 people and made 60 poor.” On September 29, 1582, “there was such a big storm, as it was said that it had never been seen before in the city of Cluj, which uprooted the trees and raised the roofs of the houses, people believed that it is sent by divinity to punish the crimes committed by them.” I mean, I’m not saying these people weren’t doing crimes. It’s 16th century Transylvania. Do what you gotta do. But that's not why there is extreme weather. You’re just in the Little Ice Age. The study ultimately identified “multiple pieces of evidence associated with extreme weather events, including 40 unusually warm summers and several years of excess precipitation or drought.” Taken together with natural archives, the documents paint a picture of troubled times, exacerbated by an unstable climate and possible emergent vampires. Relatable! Valentine’s Day is over, but the romantic mood is still in the air—or in the water, if you’re a medaka (flawless segue). Scientists have discovered that wild medaka, also known as Japanese rice fish, are fans of late-night booty calls, which is a behavior that has not been observed in captivity.“Although medaka and other model organisms are invaluable in laboratories, their ecology in the wild remains largely unknown,” said researchers led by Yuki Kondo of Osaka Metropolitan University. “This study showed that medaka in the wild initiate spawning during late nocturnal hours and exhibit vigorous courtship behavior at midnight.”Kondo and her colleagues recorded this vigorous courtship by placing GoPros into streams over the course of several summer nights in Gifu, Japan. The tapes revealed that medaka like to spawn in the dark, possibly to avoid predators during copulation. The results “provide the first empirical evidence that medaka mating begins significantly earlier than previously reported in the laboratory.”  For anyone who feels clueless about courtship, may I offer a page from the Medaka Sutra: “The spawning behavior of medaka follows a sequence of events: the male chases the female (following), the male swims rapidly around the female (quick circle), the male wraps his dorsal and anal fins around the female (wrapping), the female releases eggs, the male releases sperm (egg and sperm release), and the male leaves the female (leaving),” according to Kondo’s team.The only true love language is, indeed, spoken with anal fins.Major bonus points also go to Osaka Metropolitan University’s press team for throwing together this version of Edward Hopper’s famous “Nighthawks” painting with medaka getting drinks at a bar that is also named Medaka. It is genuinely one of the most inspired public relations efforts I have ever seen, and I’m going to get a print of it to hang on my wall.The Insect at the Edge of Earth, or the Antarctic midge, is the only insect that lives year-round on its namesake continent. Do you know how weird you have to be to be the  insect somewhere? But this midge doesn’t care. It just lives out its bug life, which lasts two years, in an otherwise bugless wasteland. Humans definitely care about the midge, though—how could we not? What is it doing there? How is it not dead? What can it teach us about cryopreservation? These questions are addressed in a new study that resolved mysteries about the animal’s interesting life cycle.“Freeze tolerance and cryoprotective dehydration are cold tolerance strategies used by various invertebrate species in polar regions and indeed,  utilises both for overwintering,” said researchers led by Mizuki Yoshida of the Ohio State University, who completed the project while at Osaka Metropolitan University (OMU killing it this week). “Larvae that are frozen in ice and cryoprotectively dehydrated readily survived 32 days of simulated overwintering,” the team said. “Unlike many insects restricted to highly specific microhabitats,  larvae inhabit a remarkably diverse range of substrates that differ in vegetation, substrate type, slope, drainage, and thermal and hydric conditions.”I love the phrasing of “readily survived” as if the midges were eager to show off their cryoprotective superpowers. After this 32-day period they emerged with “That all you got?” energy. By studying the bugs in these simulated conditions, the researchers confirmed that they rely on multiple overwintering strategies, including a state of arrested development called “obligate diapause.” “Diapause has long been assumed to be uncommon in Antarctic species, but the present study reveals that  utilises diapause for seasonal adaptation, as in many temperate species,” Yoshida and her colleagues said. In addition to being the only endemic Antarctic insect, this midge has the smallest genome of any known insect while also being the largest fully terrestrial animal on the continent, even though it’s only a few millimeters long. In other words, it is the biggest animal in Antarctica that doesn’t fly or swim. Okay, Antarctic midge. You just keep doing you.Last, turtles do a little victory dance when they find food. Yes, it is cute. Yes, there is a video.The footage (along with ) is part of a study that tested if turtles could distinguish the magnetic signatures of two geographical areas. When the turtles were exposed to signatures associated with an area they associated with food, they danced in anticipation of a meal, demonstrating that they could tell the signals apart—and party accordingly.  “Hallmarks of the behaviour include some or all of the following: tilting the body vertically, holding the head near or above water, opening the mouth, rapid alternating movement of the front flippers, and, occasionally, even spinning in place, hence the name ‘turtle dance,’” said researchers led by Kayla Goforth of Texas A&M University. “Turtles exhibited significantly higher levels of turtle dance behaviour when experiencing the field in which they had been fed.”With that, let’s all tilt vertically, spin in place, and shell-abrate the long weekend. Thanks for reading! See you next week.  ]]></content:encoded></item><item><title>Career transition in to Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/</link><author>/u/Similar-Secretary-86</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 13:41:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA["I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated ]]></content:encoded></item><item><title>Built a cli tool for generating .gitignore files</title><link>https://www.reddit.com/r/golang/comments/1iq1ivv/built_a_cli_tool_for_generating_gitignore_files/</link><author>/u/SoaringSignificant</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:38:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built this mostly as an excuse to play around with Charmbracelet’s libraries like Bubble Tea and make a nice TUI, but it also solves the annoying problem of constantly looking up .gitignore templates. It’s a simple CLI tool that lets you grab templates straight from GitHub, TopTal, or even your own custom repository, all from the terminal. You can search through templates using a TUI interface, combine multiple ones like mixing Go and CLion, and even save your own locally so you don’t have to redo them every time. If you’re always setting up new projects and find yourself dealing with .gitignore files over and over, this just makes life a bit easier, hopefully. If that sounds useful, check it out here and give it a try. And if you’ve got ideas to make the TUI better or want to add something cool, feel free to open a PR. Always happy to get feedback or contributions!]]></content:encoded></item><item><title>ED25519 Digital Signatures In Go</title><link>https://www.reddit.com/r/golang/comments/1iq1i84/ed25519_digital_signatures_in_go/</link><author>/u/mejaz-01</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:37:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/mejaz-01 ]]></content:encoded></item><item><title>Will AI Lead to the Disintermediation of Knowledge?</title><link>https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/</link><author>Bill Schmarzo</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 13:28:49 +0000</pubDate><source url="https://www.datasciencecentral.com/">Dev - Data Science Central</source><content:encoded><![CDATA[Key Blog Points: For decades, organizations have operated under the central assumption that knowledge flows downward. Senior leaders, industry veterans, and domain experts have traditionally been the primary gatekeepers of critical information. Their insights, honed over years of experience, have been the cornerstone of strategic decision-making. Enter artificial intelligence (AI). Many folks are concerned that… Read More »]]></content:encoded></item><item><title>RE: Moving Helm Charts to ArgoCD (I’m doing the same!)</title><link>https://www.reddit.com/r/kubernetes/comments/1iq0tsn/re_moving_helm_charts_to_argocd_im_doing_the_same/</link><author>/u/Helpful_Spinach_9963</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 12:59:51 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I saw this question posted 10 months ago OG Post, and I also need to migrate a bunch of Helm charts to ArgoCD (helm chart per microservice).From what I gathered, the general recommendation is to leave old Helm installations as they are and let ArgoCD take over moving forward. However, one concern I have is what happens when a new manifest is added to a Helm chart that was previously installed via  but is now being synced by ArgoCD?Wouldn't this create a situation where ArgoCD's managed resources differ from what Kubernetes has, leading to orphaned manifests? That seems undesirable long term. I’d assume the best practice is to uninstall the Helm charts and let ArgoCD handle everything but I’d like to avoid downtime. The other option is manually removing Helm annotations and labels, but that sounds incredibly tedious.What’s the best practice for transitioning Helm charts to ArgoCD in a way that avoids downtime and keeps everything clean? Remove helm and just use kustomize? Would love to hear how others have handled this.]]></content:encoded></item><item><title>Why OOP &amp; FP are the Two Main Paradigms</title><link>https://www.youtube.com/watch?v=l_3AGwVwP_k</link><author>/u/OkMemeTranslator</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:56:13 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chinese Vice Minister says China and the US must work together to control rogue AI: &quot;If not... I am afraid that the probability of the machine winning will be high.&quot;</title><link>https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:27:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in  (AI).But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.“Realistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,” Fu said.“As long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.”The panel discussion is part of a two-day global  that started in Paris on Monday.Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden’s administration and the United Nations.]]></content:encoded></item><item><title>Karol Herbst steps down as Nouveau maintainer due to “thin blue line comment”</title><link>https://www.reddit.com/r/linux/comments/1iq09g6/karol_herbst_steps_down_as_nouveau_maintainer_due/</link><author>/u/mdedetrich</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:24:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA["I was pondering with myself for a while if I should just make it official that I'm not really involved in the kernel community anymore, neither as a reviewer, nor as a maintainer.Most of the time I simply excused myself with "if something urgent comes up, I can chime in and help out". Lyude and Danilo are doing a wonderful job and I've put all my trust into them.However, there is one thing I can't stand and it's hurting me the most. I'm convinced, no, my core believe is, that inclusivity and respect, working with others as equals, no power plays involved, is how we should work together within the Free and Open Source community.I can understand maintainers needing to learn, being concerned on technical points. Everybody deserves the time to understand and learn. It is my true belief that most people are capable of change eventually. I truly believe this community can change from within, however this doesn't mean it's going to be a smooth process.The moment I made up my mind about this was reading the following words written by a maintainer within the kernel community:"we are the thin blue line"This isn't okay. This isn't creating an inclusive environment. This isn't okay with the current political situation especially in the US. A maintainer speaking those words can't be kept. No matter how important or critical or relevant they are. They need to be removed until they learn. Learn what those words mean for a lot of marginalized people. Learn about what horrors it evokes in their minds.I can't in good faith remain to be part of a project and its community where those words are tolerated. Those words are not technical, they are a political statement. Even if unintentionally, such words carry power, they carry meanings one needs to be aware of. They do cause an immense amount of harm.I wish the best of luck for everybody to continue to try to work from within. You got my full support and I won't hold it against anybody trying to improve the community, it's a thankless job, it's a lot of work. People will continue to burn out.I got burned out enough by myself caring about the bits I maintained, but eventually I had to realize my limits. The obligation I felt was eating me from inside. It stopped being fun at some point and I reached a point where I simply couldn't continue the work I was so motivated doing as I've did in the early days.Please respect my wishes and put this statement as is into the tree. Leaving anything out destroys its entire meaning.]]></content:encoded></item><item><title>The Impact of Generative AI on Critical Thinking [pdf]</title><link>https://www.microsoft.com/en-us/research/uploads/prod/2025/01/lee_2025_ai_critical_thinking_survey.pdf</link><author>nosianu</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 12:06:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Building the MagicMirror in Rust with iced GUI Library 🦀</title><link>https://www.reddit.com/r/rust/comments/1ipzubj/building_the_magicmirror_in_rust_with_iced_gui/</link><author>/u/amindiro</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:56:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I recently embarked on a journey to build a custom MagicMirror using the Rust programming language, and I’d like to share my experiences. I wrost a blog post titled "software you can love: miroir Ô mon beau miroir" this project was my attempt to create a stable, resource-efficient application for the Raspberry Pi 3A.Here's what I loved about using Rust and the iced GUI library:Elm Architecture + Rust is a match made in heaven: iced was perfect for my needs with its Model, View, and Update paradigms. It helped keep my state management concise and leverage Rust type system Opting for this lightweight rendering library reduced the size of the binary significantly, ending with a 9MB binary. Although troublesome at first, I used ‘cross’ to cross compile Rust for armv7.If anyone is keen, I’m thinking of open-sourcing this project and sharing it with the community. Insights on enhancing the project's functionality or any feedback would be much appreciated!Feel free to reach out if you're interested in the technical nitty-gritty or my experience with Rust GUI libraries in general.]]></content:encoded></item><item><title>[P] Daily ArXiv filtering powered by LLM judge</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/</link><author>/u/MadEyeXZ</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:14:16 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>My new blog post comparing networking in EKS vs. GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/</link><author>/u/jumiker</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 11:06:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/jumiker ]]></content:encoded></item><item><title>Richard Stallman in Polytechnic University of Turin, Italy</title><link>https://www.reddit.com/r/linux/comments/1ipz4wy/richard_stallman_in_polytechnic_university_of/</link><author>/u/ShockleyTransistor</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:05:38 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go Nullable with Generics v2.0.0 - now supports omitzero</title><link>https://github.com/LukaGiorgadze/gonull</link><author>/u/Money-Relative-1184</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 11:00:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Deep Dive into VPA Recommender</title><link>https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/</link><author>/u/erik_zilinsky</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 10:26:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.Based on my findings, I wrote a blog post about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.]]></content:encoded></item><item><title>what do you use golang for?</title><link>https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/</link><author>/u/Notalabel_4566</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 10:24:28 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is there any other major use than web development?]]></content:encoded></item><item><title>Show HN: Kreuzberg – Modern async Python library for document text extraction</title><link>https://github.com/Goldziher/kreuzberg</link><author>nhirschfeld</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 10:07:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I'm excited to showcase Kreuzberg!Kreuzberg is a modern Python library built from the ground up with async/await, type hints, and optimized I/O handling.It provides a unified interface for extracting text from documents (PDFs, images, office files) without external API dependencies.Key technical features:
- Built with modern Python best practices (async/await, type hints, functional-first)
- Optimized async I/O with anyio for multi-loop compatibility
- Smart worker process pool for CPU-bound tasks (OCR, doc conversion)
- Efficient batch processing with concurrent extractions
- Clean error handling with context-rich exceptionsI built this after struggling with existing solutions that were either synchronous-only, required complex deployments, or had poor async support. The goal was to create something that works well in modern async Python applications, can be easily dockerized or used in serverless contexts, and relies only on permissive OSS.Key advantages over alternatives:
- True async support with optimized I/O
- Minimal dependencies (much smaller than alternatives)
- Perfect for serverless and async web apps
- Local processing without API calls
- Built for modern Python codebases with rigorous typing and testingThe library is MIT licensed and open to contributions.]]></content:encoded></item><item><title>Linux in any distribution is unobtainable for most people because the first two installation steps are basically impossible.</title><link>https://www.reddit.com/r/linux/comments/1ipyc1o/linux_in_any_distribution_is_unobtainable_for/</link><author>/u/trollfinnes</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 10:05:47 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Recently, just before Christmas, I decided to check out Linux again (tried it ~20 years ago) because Windows 11 was about to cause an aneurysm.I was expecting to spend the "weekend" getting everything to work; find hardware drivers, installing various open source software and generally just 'hack together something that works'.To my surprise everything worked flawlessly first time booting up. I had WiFi, sound, usb, webcam, memory card reader, correct screen resolution. I even got battery status and management! It even came with a nice litte 'app center' making installation of a bunch of software as simple as a click!And I remember thinking any Windows user could  install Linux and would get comfortable using it in an afternoon.I'm pretty 'comfortable' in anything PC and have changed boot orders and created bootable things since the early 90's and considered that part of the installation the easiest part.However, most people have never heard about any of them, and that makes the two steps seem 'impossible'.I recently convinced a friend of mine, who also couldn't stand Window11, to install Linux instead as it would easily cover all his PC needs. And while he is definitely in the upper half of people in terms of 'tech savvyness', both those "two easy first steps" made it virtually impossible for him to install it. He easily managed downloading the .iso, but turning that iso into a bootable USB-stick turned out to be too difficult. But after guiding him over the phone he was able to create it.But he wasn't able to get into bios despite all my attempts explaining what button to push and whenNext day he came over with his laptop. And just out of reflex I just started smashing the F2 key (or whatever it was) repeatingly and got right into bios where I enabled USB boot and put it at the top at the sequence.After that he managed to install Linux just fine without my supervision.But it made me realise that the two first steps in installing Linux, that are second nature to me and probably everyone involved with Linux from people just using it to people working on huge distributions, makes them virtually impossible for most people to install it.I don't know enough about programming to know of this is possible:Instead of an .iso file for download some sort of .exe file can be downloaded that is able to create a bootable USB-stick and change the boot order?That would 'open up' Linux to  more people, probably orders of magnitude..]]></content:encoded></item><item><title>Jane Street&apos;s Figgie card game</title><link>https://www.figgie.com/</link><author>eamag</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 09:59:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Jane Street's fast-paced
              Figgie game simulates exciting elements of markets and trading. At
              Jane Street, Figgie is a game we teach and also one we really
              enjoy playing.
            Read our FAQs
              for more. If you have a question that isn’t answered there, we’d
              like to hear
              what’s missing and what
              would be helpful to know, and we’ll do our best to update FAQs
              along the way.
            ]]></content:encoded></item><item><title>Lessons from David Lynch: A Software Developer&apos;s Perspective</title><link>https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/</link><author>/u/aijan1</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 09:40:30 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He’s perhaps best known for the groundbreaking TV series Twin Peaks, which inspired countless shows, including The X-Files, The Sopranos, and Lost.Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down – even those who truly deserved it.Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that Mulholland Drive remained compulsively watchable while refusing to yield to interpretation.While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I’d like to share my perspective on his life lessons from a software developer’s viewpoint.Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you’ve got to go deeper.We’ve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one –because they’re so rare– write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it’s a film, a painting, or software.The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.Software development is part art, part engineering. We don’t build the same software over and over again – virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it’s very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.It’s a good habit to listen to what users have to say, but they often can only describe their problems – they rarely come up with good ideas to solve them. And that’s OK. It’s our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  – that magical state of mind where we lose track of time and produce code effortlessly. That’s why many developers hate meetings – they are toxic to our productivity.I believe you need technical knowledge. And also, it’s really, really great to learn by doing. So, you should make a film.Software development is one of those rare fields where a college degree isn’t required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It’s crucial to never stop learning, experimenting, and iterating on our craft.Happy accidents are real gifts, and they can open the door to a future that didn’t even exist.Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.Be kind to your teammates, don’t embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety –that is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by Google’s research on the subject.It’s OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.Most of Hollywood is about making money - and I love money, but I don’t make the films thinking about money.Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.What makes these projects remarkable is that they didn’t emerge from corporate boardrooms – they were built by communities of passionate developers, collaborating across the world.Money is just a means to an end. Unfortunately, many get this confused.David, thank you for making the world a better place!]]></content:encoded></item><item><title>Restrict egress alternative way.</title><link>https://www.reddit.com/r/kubernetes/comments/1ipwx71/restrict_egress_alternative_way/</link><author>/u/Common-Feedback-7370</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 08:16:11 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I need to restrict egress from the  deployed as a pod in Kubernetes. I test used a network policy, which worked properly, but there's a requirement to avoid redeploying nodes (since enabling network policy on GKE causes all nodes to redeploy).So I try using Kuma and configured it within the namespace where the  is located, but it turned out to be too complicated.Does anyone have any ideas for how to restrict egress access using a sidecar without affecting the underlying infrastructure?Any suggestions would be greatly appreciated. ]]></content:encoded></item><item><title>If you believe in &quot;Artificial Intelligence&quot;, take five minutes to ask it</title><link>https://svpow.com/2025/02/14/if-you-believe-in-artificial-intelligence-take-five-minutes-to-ask-it-about-stuff-you-know-well/</link><author>lycopodiopsida</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 08:15:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Just a quick check on how ChatGPT’s getting on …: Who reassigned the species  to its own genus, and when? The species  was reassigned to its own genus, , by the paleontologist Michael Janensch in 1914. Dude, that is so wrong I can’t even.: I apologize for the mistake in my previous response. The reassignment of  to  was not done by Michael Janensch. The correct attribution goes to the paleontologist Ernst Stromer, who reclassified it in 1914. That answer is even worse. Stromer had nothing to do with the genus name  species name. I apologize for the confusion in my previous responses. The reassignment of  to  was actually done by the paleontologist Werner Janensch, not Michael Janensch, in 1914.Regular readers will know that the actual sequence of events was:Riggs (1903) named the genus  and its type species .Janensch (1914) named a second species of this genus, “.Paul (1988) considered the two species separate at the subgeneric level (a taxonomic level that is rarely if ever used in vertebrate palaeontology) and named the subgenus  () .Olshevsky (1991) used the genus-and-species combination  without comment in a self-published booklet, a combinaton which was not followed by any formal publication known to me, until …Taylor (2009) demonstrated that numerous characters separate the two species and that  is a separate genus with type species . .Now, ChatGPT got every single part of that wrong. It left Riggs, Paul, Olshevsky and me out completely. It credited Werner Janensch for reassigning a species to a genus whose name was coined 19 years after he died. It pulled poor Ernst Stromer in for some reason. It invented “the paleontologist Michael Janensch”, who is presumably the unholy offspring of Werner Janensch and me. And in all three answers, it insisted that the reassignment was done in 1914, 74 years before the name  was coined.But the worst part of this is not the errors. It’s not the blithe confidence with which the false facts are recited. It’s not even the bland “I apologize for the mistake in my previous response” to be followed by more utter nonsense. It’s that these incorrect answers look so plausible. For a lay-person — someone who, foolishly, has not been reading this blog for the last eighteen years — the answers given here look superficially reasonable. A kid doing a homework report on  could take these answers and weave them into the submission without even having an inkling that they’re completely wrong. And the teacher who has to mark the essay will also likely swallow them.Because LLMs get catastrophically wrong answers on topics I know well, I do not trust them  on topics I don’t already know. And if you do trust them, I urge you to spend five minutes asking your favourite one about something you know in detail.Why do I keepbanging onabout this? Because we’ve reached an especially toxic phase in the history of AI. A lot of companies have ploughed billions of dollars into the dream of being able to replace human workers with machines, and they are desperate to make us believe it’s going to work — if only so they can cash out their investments while the stocks are still high. We’re in the “pump” phase of an industry-wide pump-and-dump. It would be wise for all to beware getting suckered in.(: LLMs are useful for some classes of queries. I use them a lot to remind me of programming-language idioms, asking questions like “In Go, if I have an object satisfying the  interface, how can I determine which specific kind of error structure it is?”. They’re good for this because you can quickly determine whether the answer is correct or not, thanks to the merciless compiler. LLMs are not useless; they’re just way overhyped and misapplied.)Janensch, Werner. 1914. Ubersicht uber der Wirbeltierfauna der Tendaguru-Schichten nebst einer kurzen Charakterisierung der neu aufgefuhrten Arten von Sauropoden. , Berlin, III, (1):81-110.Olshevsky, George. 1991. A Revision of the Parainfraclass Archosauria Cope, 1869, Excluding the Advanced Crocodylia.  (1st printing): iv + 196 pp.Paul, Gregory S. 1988. The brachiosaur giants of the Morrison and Tendaguru with a description of a new subgenus,  and a comparison of the world’s largest dinosaurs. (3):1-14.Riggs, Elmer S. 1903. , the largest known dinosaur. American Journal of Science(4):299-306.Taylor, Michael P. 2009. A re-evaluation of  Riggs 1903 (Dinosauria, Sauropoda) and its generic separation from  (Janensch 1914). Journal of Vertebrate Paleontology(3):787-806.]]></content:encoded></item><item><title>&quot;Dongly Things&quot; by Douglas Adams (of Hitchhikers Guide) - Adams wrote this article in the early days of Mac computers, about manufacturers making things difficult with a million different proprietary cables/ports etc.</title><link>https://www.douglasadams.com/dna/980707-03-a.html</link><author>/u/CaesarSalvage</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 07:47:36 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Container Networking - Kubernetes with Calico</title><link>https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/</link><author>/u/tkr_2020</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 07:25:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[: VLAN 10: VLAN 20When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:The inner IP header reflects:The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?]]></content:encoded></item><item><title>This Week in Plasma: Post-Release Polishing</title><link>https://blogs.kde.org/2025/02/15/this-week-in-plasma-post-release-polishing/</link><author>/u/gabriel_3</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 07:21:05 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Welcome to a new issue of "This Week in Plasma"! Every week we cover as much as possible of what's happening in the world of KDE Plasma and its associated apps like Discover, System Monitor, and more.Plasma 6.3 is out! So far the response has been very good, but of course a few issues were found once it was in the wild.Maybe the worst issue is something that KWin devs have actually tracked down to a bug in the GCC compiler, of all things! It only manifests with the kind of release build configurations that many distros use, and also only with GCC 15 and an ICC profile set up. We've informed distros how to work around it until the root cause is understood and GCC gets patched, or KWin devs are able to guard against it internally.Unfortunately this is a sign that we did not in fact get enough beta testers, since the issue should have been obvious to people in affected environments. Another sign is that most of the regressions are hardware-related. We've got them fixed now, but we need people to be testing the betas with their personal hardware setups! There's simply no way for a small pool of KDE developers to test all of these hardware setups themselves.Anyway, with those caveats aside, it looks like it's been a pretty smooth release! Building on it, there have been a number of positive changes to the Media Player widget, Weather Report Widget, Info Center Energy page, and touchscreen supportThe Media Player widget now features a playback rate selector when the source media makes this feature available using its MPRIS implementation. (Kai Uwe Broulik, link)Improved the presentation of search results for the new DWD weather provider in the Weather Report widget. (Ismael Asensio, link 1 and link 2)The BBC Weather provider has recently improved the quality of their forecast data, so we've changed the weather widget to no longer hide search results from it if there are results from other providers as well. (Ismael Asensio, link)The updates list in Discover is now sorted case-insensitively. (Aleix Pol Gonzalez, link)Welcome Center now remembers its window size (and on X11, position) across launches, like most of our other QML app windows these days. (Tracey Clark, link)Improved the graph view on Info Center's Energy page: Now it's in a card, like in System Monitor, and has more normal and visually pleasing margins. (Ismael Asensio, link 1 and link 2)Spectacle has gained support for pinch-zooming in its screenshot viewer window, which can be especially useful when annotating using a touchscreen. (Noah Davis, link)You can now actually scroll through the Widget Explorer with a single-finger touchscreen scroll gesture, because dragging widgets using touch now requires a tap-and-hold. (Niccolò Venerandi, link)Fixed a regression that would cause KWin to crash in the X11 session when hotplugging or switching between HDMI screens. (Fushan Wen, link 1 and link 2). Consider it a reminder for everyone still on X11 to try the Wayland session again, because the X11 session receives almost no testing from developers anymore!Fixed a regression that could cause KWin to sometimes crash hours after hotplugging a thunderbolt dock. (Xaver Hugl, link)Fixed a regression that would cause KWin to crash when you interact with the + task switcher while using software rendering. (Vlad Zahorodnii, link)Fixed a regression that could cause certain Qt-based apps to crash on launch when using the Breeze style. (Antonio Rojas, link)Fixed a case where Plasma might sometimes crash when clicking on the Networks icon in the System Tray, especially when built using GCC 15. (David Edmundson, link)Fixed a regression that caused the new "Prefer efficiency" ICC color mode setting to not actually improve efficiency on certain hardware. (Xaver Hugl, link)Panels in auto-hide mode no longer inappropriately hide again when you start dragging Task Manager tasks to re-order them. (Tino Lorenz, link)The new bar separator between the date and time in the Digital Clock widget no longer appears inappropriately when the date has been intentionally suppressed. (Christoph Wolk, link)Fixed an issue that broke the layout of the device tiles on Info Center's Energy page when using a larger-than-default font size or loads of devices with batteries. (Ismael Asensio, link)Fixed an two keyboard navigation issues in the Power and Battery widget. (Ismael Asensio, link 1 and link 2)Fixed an older issue that prevented the keyboard brightness controls on certain laptops from being visible immediately. (Nicolas Fella, link)Fixed an older issue that caused Info Center's Energy page to vibrate disturbingly at certain window sizes. It was, heh heh heh… very high energy! (Ismael Asensio, link)Committed a better Qt fix for the issue whereby the first click after dragging Task Manager tasks got ignored. (David Redondo, link)KDE has become important in the world, and your time and contributions have helped us get there. As we grow, we need your support to keep KDE sustainable.You can help KDE by becoming an active community member and getting involved somehow. Each contributor makes a huge difference in KDE — you are not a number or a cog in a machine!You don’t have to be a programmer, either. Many other opportunities exist:You can also help us by making a donation! Any monetary contribution — however small — will help us cover operational costs, salaries, travel expenses for contributors, and in general just keep KDE bringing Free Software to the world.Enter your email address to follow this blog and receive notifications of new posts by email.]]></content:encoded></item><item><title>Bookshop.org launches Kindle alternative, sends e-book sales to local bookstores</title><link>https://www.usatoday.com/story/entertainment/books/2025/01/28/bookshop-org-ereader-ebook-app/77928209007/</link><author>dotcoma</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 07:05:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Bibliophiles don’t need to commit to one style of reading – with myriad titles available digitally or as audiobooks, diversifying reading habits has never been easier.However, as platforms vary, the seller often doesn’t. Amazon accounts for more than 80% of all online book sales, according to market research firm IBISWorld. Not to mention it owns Kindle, a wildly popular format for e-reading.Bookshop.org is ready to change that. Today, the online platform that connects readers to local bookstores launched an e-reading platform of its own. For the first time, local independent bookstores can sell e-books to customers, says CEO and founder Andy Hunter. Bookshop.org launches e-book platform to support local bookstoresHunter is in the business of solving logistical problems for readers. In 2020, he launched Bookshop.org to support local businesses losing sales to Amazon Books. Years later, as an e-reader himself, he realized how hard it was to continue his “socially-conscious consumerism” mindset of buying locally when it came to digital titles.“Right now, no matter how much you love your local bookstore, if you want to read an e-book, you have to go to Amazon or another alternate platform and you can’t support your local bookstore,” Hunter tells USA TODAY. “E-books are an important revenue stream for publishers and authors and they could be for independent bookstores.”The result is an app (available for iOS and Android, and accessible from a web browser) with nearly a million titles from major publishing houses. Readers can annotate, add notes, look up words and sync across all their devices. There's a search function if consumers know the book they want to read, and if they don't, there's an explore page with suggestions by genre.Most importantly for Hunter, booksellers will take home the profits, which he says has prevented local businesses from thriving in e-book sales until now. “There’s no real way to make it profitable, but we don’t care about it being profitable,” Hunter says, citing the socially conscious company’s B corporation status. “Our mission is to help independent bookstores survive and thrive in the digital age.”With discounts, quick shipping and Kindle synching, Amazon Books has long been an appealing option to many readers. But now, Hunter says there’s “no reason on Earth to give a billionaire your $9.99 for your e-book,” as publishers set a fixed price for e-books and there’s no shipping to compete with. Bookshop.org’s new initiative follows a growing movement across social media – not just exclusively in bookish communities – to forgo Amazon and other big-box sellers in favor of shopping locally. “In an age of billionaires and giant e-commerce companies and our loss of control of our lives to Silicon Valley oligarchs, that they know we’re going to fight back against that and we’re going to stay local,” Hunter says. “It doesn’t have to be this way. We can invest in, we can support our local community even when we’re reading digital books.”E-reader features to include social sharingBookshop.org's platform also includes a “quote-sharing” feature for social media, Hunter says. He wants books to catch up to the way other media is shared – it’s easy to share your Spotify or Apple Music listening to your Instagram story, and you can even post about your workouts with some apps. But if you want to share a quote from a book, readers have to take a photo of the physical book or a screenshot. “They should be able to share that to social media and somebody else should be able to click in, buy the book using Apple Pay and start reading it right away,” Hunter says. “That’ll fuel all kinds of discussions about books on TikTok and places like that, and all of that will benefit the independent bookstores because it will be them selling the books that people are talking about.”Looking for your next great read? USA TODAY has you covered.Clare Mulroy is USA TODAY’s Books Reporter, where she covers buzzy releases, chats with authors and dives into the culture of reading. Find her or tell her what you’re reading at cmulroy@usatoday.com.]]></content:encoded></item><item><title>[D] What&apos;s the most promising successor to the Transformer?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/</link><author>/u/jsonathan</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 06:17:01 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also xLSTM and Aaren.What do y'all think is the most promising alternative architecture to the transformer?]]></content:encoded></item><item><title>Kafka Delay Queue: When Messages Need a Nap Before They Work</title><link>https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need</link><author>/u/Sushant098123</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 05:08:28 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Webassembly and go 2025</title><link>https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/</link><author>/u/KosekiBoto</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 05:00:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[so I found this video and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you   submitted by    /u/KosekiBoto ]]></content:encoded></item><item><title>Vq: A Vector Quantization Library for Rust 🦀</title><link>https://www.reddit.com/r/rust/comments/1ipu2jg/vq_a_vector_quantization_library_for_rust/</link><author>/u/West-Bottle9609</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 04:56:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've created a Rust library called Vq that implements several vector quantization algorithms. At the moment, these algorithms include binary, scalar, product, optimized product, tree-structured, and residual quantization. I think the library can be useful for tasks like data compression, similarity search, creating RAG pipelines, and speeding up machine learning computations.This is my second Rust project, as I'm currently learning Rust. I'd like to get some feedback from the community and hear about any use cases you might have for the library, so I'm making this announcement.The library is available on crates.io: vq, and the source code is on GitHub: vq.]]></content:encoded></item><item><title>Modern Java Deep Dive</title><link>https://www.youtube.com/watch?v=z4qsidg261E</link><author>/u/BlueGoliath</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 03:59:24 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The 20 year old PSP can now connect to WPA2 WiFi Networks</title><link>https://wololo.net/2025/02/14/the-20-year-old-psp-can-now-connect-to-wpa2-wifi-networks/</link><author>zdw</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 03:31:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Screenshot source: Zekiu_ on youtubeAcid_Snake and the ARK Development team have released a significant update to the ARK custom Firmware for the Sony PSP. Custom Firmware now allows the Playstation Portable to connect to WPA2 encrypted Wifi networks. This is thanks to the recently released  plugin, created by developer  and published on the PSP Homebrew discord.Playstation Portable gets WPA2 Wifi accessThe PSP has been out of official support from Sony for years, but lots of enthusiasts keep maintaining this great handheld through homebrew and custom Firmware updates. As technology evolves around us, older devices such as the PlayStation Portable can lose some of their features.For example, as WPA2 has become the defacto encryption standard for home wifi networks (WPA3’s adoption rate remains low), older devices such as the PSP, that do not support these new* encryption standards become technically unable to access the internet.Wifi access was a very strong feature of the PSP when it was released, and, although it’s probably less important nowadays, losing that feature because newer networks aren’t compatible is a bummer.WPA2 support has been a request by many enthusiasts for years on PSP discussion channels, and it seems that the wpa2psp plugin by developer Moment now brings this to life. According to Acid_Snake, the developer was kind enough to provide the source code of the plugin, which allowed the ARK team to embed it into the ARK Custom Firmware for PSP.This reddit thread by Nebula_NL covers a lot of details on how to install and use the plugin. But the bottom line is: install the latest release of the ARK CFW on your PSP, and take it from there. (Note that you can also manually install the plugin if you’re using another CFW than ARK)This is of course the first iteration of this plugin, and it comes with limitations, specifically:2.4 GHz Only
WPA2 support works with 2.4 GHz WiFi.If your router uses a single SSID for both 2.4 GHz and 5 GHz, you may need to separate them and connect your PSP to the 2.4 GHz network.WPA2 AES Only
Requires WPA2 with AES (AES-CCMP) encryption.TKIP is not supported and will not work.WEP/WPA Compatibility
While WPA2 is active, WEP and WPA encryption will not work.To use WEP or WPA again, disable WPA2, and they will function normally.WPA2/WPA3 Mixed Mode
If your router is set to WPA2/WPA3 mixed mode, your PSP may struggle to obtain an IP address.Try manually setting the IP address instead of using DHCP in [AUTO] mode.Download and install ARK-4 + enable WPA2 Support for the PSP* WPA2 was certified in 2004… It’s “new” from the PSP’s perspective which launched the same year and didn’t “need” to support it at the time. WPA3 launched in 2018 but its adoption is taking time]]></content:encoded></item><item><title>Lesson 3: Programming the Shooter Game in Squeak/Smalltalk</title><link>https://news.squeak.org/2025/02/14/lesson-3-programming-the-shooter-game-in-squeak-smalltalk/</link><author>/u/itsmeront</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 02:43:47 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Lesson 3: Moving a Morph with CodeIn this lesson, we will learn about blocks in Smalltalk. A block is a small piece of code that can be treated as an object. It is often used to perform actions or calculations when needed. Blocks are written inside square brackets, and they can be passed around or executed by other parts of the program. While blocks are commonly used in programs to make things happen, for now, think of them as a way to understand how code can be organized and applied. This lesson is a simple introduction to blocks, so you do not need to fully understand everything about them yet. By the end of the lesson, you will have a better idea of what blocks are and how they work in code. You will also learn how to use blocks to interact with your EllipseMorph.We will also use a simple button morph to interact with our EllipseMorph. By clicking the button, you will be able to make the EllipseMorph move. This lesson will show you how to create a simple graphical button that can then be used to interact with your morph. This is a nice way of interacting with the morph without having to do much coding for now. It allows you to focus on understanding the basic concepts before diving into more complex programming. As you progress, you will learn how to build more advanced interactions and customize the behavior of your morphs.For additional learning, there are several resources that can help you dive deeper into the topics we covered. You can start by exploring the Block Swiki page here to get a better understanding of how blocks work in Smalltalk. If you want to learn more specifically about the BlockClosure class, check out a detailed explanation here, also a Swiki article. Another helpful resource is the Terse Guide to Squeak online, which is always available within the Squeak image under Help -> Terse Guide to Squeak. Lastly, the  book is an excellent resource for exploring Squeak and Smalltalk in more detail, and you can read it here. These resources will provide you with more in-depth information and examples to support your learning.For the complete series of lessons on creating the , including all the necessary resources like PDFs, source code, images, sound files, and more, visit https://scottgibson.site/ShooterGame/. This site not only offers everything you need to progress but also lets you play the game directly in your browser using SqueakJS. Whether you are just getting started with Squeak/Smalltalk or are advancing your skills, these resources will help guide you as you progress through the game development process.Have a great time with Smalltalk and keep on Squeaking!]]></content:encoded></item><item><title>Bringing Nest.js to Rust: Meet Toni.rs, the Framework You’ve Been Waiting For! 🚀</title><link>https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/</link><author>/u/Mysterious-Rust</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 02:42:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a Rust developer coming from TypeScript, I’ve been missing a Nest.js-like framework — its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesn’t have a direct counterpart (yet!), I decided to build one myself! 🛠️Introducing… Toni.rs — a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And it’s live in beta! 🎉Here’s what makes this project interesting:Scalable maintainability 🧩:A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code — each module lives in its own context, clean and focused.Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?Automatic Dependency Injection 🤖:Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.Leave your thoughts below — suggestions, questions, or even just enthusiasm! 🚀 ]]></content:encoded></item><item><title>How I Became A Machine Learning Engineer (No CS Degree, No Bootcamp)</title><link>https://towardsdatascience.com/how-i-became-a-machine-learning-engineer-no-cs-degree-no-bootcamp/</link><author>Egor Howell</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 02:33:01 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Machine learning and AI are among the most popular topics nowadays, especially within the tech space. I am fortunate enough to work and develop with these technologies every day as a machine learning engineer!In this article, I will walk you through my journey to becoming a machine learning engineer, shedding some light and advice on how you can become one yourself!In one of my previous articles, I extensively wrote about my journey from school to securing my first Data Science job. I recommend you check out that article, but I will summarise the key timeline here.Pretty much everyone in my family studied some sort of STEM subject. My great-grandad was an engineer, both my grandparents studied physics, and my mum is a maths teacher.So, my path was always paved for me.I chose to study physics at university after watching The Big Bang Theory at age 12; it’s fair to say everyone was very proud!At school, I wasn’t dumb by any means. I was actually relatively bright, but I didn’t fully apply myself. I got decent grades, but definitely not what I was fully capable of.I was very arrogant and thought I would do well with zero work.I applied to top universities like Oxford and Imperial College, but given my work ethic, I was delusional thinking I had a chance. On results day, I ended up in clearing as I missed my offers. This was probably one of the saddest days of my life.Clearing in the UK is where universities offer places to students on certain courses where they have space. It’s mainly for students who don’t have a university offer.I was lucky enough to be offered a chance to study physics at the University of Surrey, and I went on to earn a first-class master’s degree in physics!There is genuinely no substitute for hard work. It is a cringy cliche, but it is true!My original plan was to do a PhD and be a full-time researcher or professor, but during my degree, I did a research year, and I just felt a career in research was not for me. Everything moved so slowly, and it didn’t seem there was much opportunity in the space.During this time, DeepMind released theirdocumentary on YouTube, which popped up on my home feed.From the video, I started to understand how AI worked and learn about neural networks, reinforcement learning, and deep learning. To be honest, to this day I am still not an expert in these areas.Naturally, I dug deeper and found that a data scientist uses AI and machine learning algorithms to solve problems. I immediately wanted in and started applying for data science graduate roles.I spent countless hours coding, taking courses, and working on projects. I applied to and eventually landed my first data science graduate scheme in September 2021.You can hear more about my journey from a podcast.I started my career in an insurance company, where I built various supervised learning models, mainly using gradient boosted tree packages like CatBoost, XGBoost, and generalised linear models (GLMs).I built models to predict: — Did someone fraudulently make a claim to profit.— What’s the premium we should give someone.— How many claims will someone have. — What’s the average claim value someone will have.I made around six models spanning the regression and classification space. I learned so much here, especially in statistics, as I worked very closely with Actuaries, so my maths knowledge was excellent.However, due to the company’s structure and setup, it was difficult for my models to advance past the PoC stage, so I felt I lacked the “tech” side of my toolkit and understanding of how companies use machine learning in production.After a year, my previous employer reached out to me asking if I wanted to apply to a junior data scientist role that specialises in time series forecasting and optimisation problems. I really liked the company, and after a few interviews, I was offered the job!I worked at this company for about 2.5 years, where I became an expert in forecasting and combinatorial optimisation problems.I developed many algorithms and deployed my models to production through AWS using software engineering best practices, such as unit testing, lower environment, shadow system, CI/CD pipelines, and much more.Fair to say I learned a lot. I worked very closely with software engineers, so I picked up a lot of engineering knowledge and continued self-studying machine learning and statistics on the side.Over time, I realised the actual value of data science is using it to make live decisions. There is a good quote by Pau Labarta BajoML models inside Jupyter notebooks have a business value of $0There is no point in building a really complex and sophisticated model if it will not produce results. Seeking out that extra 0.1% accuracy by staking multiple models is often not worth it.You are better off building something simple that you can deploy, and that will bring real financial benefit to the company.With this in mind, I started thinking about the future of data science. In my head, there are two avenues: -> You work primarily to gain insight into what the business should be doing and what it should be looking into to boost its performance. -> You ship solutions (models, decision algorithms, etc.) that bring business value.I feel the data scientist who analyses and builds PoC models will become extinct in the next few years because, as we said above, they don’t provide tangible value to a business.That’s not to say they are entirely useless; you have to think of it from the business perspective of their return on investment. Ideally, the value you bring in should be more than your salary.You want to say that you did “X that produced Y”, which the above two avenues allow you to do.The engineering side was the most interesting and enjoyable for me. I genuinely enjoy coding and building stuff that benefits people, and that they can use, so naturally, that’s where I gravitated towards.To move to the ML engineering side, I asked my line manager if I could deploy the algorithms and ML models I was building myself. I would get help from software engineers, but I would write all the production code, do my own system design, and set up the deployment process independently.And that’s exactly what I did.Coincidentally, my current employer contacted me around this time and asked if I wanted to apply for a machine learning engineer role that specialises in general ML and optimisation at their company!Call it luck, but clearly, the universe was telling me something. After several interview rounds, I was offered the role, and I am now a fully fledged machine learning engineer!Fortunately, a role kind of “fell to me,” but I created my own luck through up-skilling and documenting my learning. That is why I always tell people to show their work — you don’t know what may come from it.I want to share the main bits of advice that helped me transition from a machine learning engineer to a data scientist. — A machine learning engineer is  an entry-level position in my opinion. You need to be well-versed in data science, machine learning, software engineering, etc. You don’t need to be an expert in all of them, but have good fundamentals across the board. That’s why I recommend having a couple of years of experience as either a software engineer or data scientist and self-study other areas. — If you are from data science, you must learn to write good, well-tested production code. You must know things like typing, linting, unit tests, formatting, mocking and CI/CD. It’s not too difficult, but it just requires some practice. I recommend asking your current company to work with software engineers to gain this knowledge, it worked for me! — Most companies nowadays deploy many of their architecture and systems on the cloud, and machine learning models are no exception. So, it’s best to get practice with these tools and understand how they enable models to go live. I learned most of this on the job, to be honest, but there are courses you can take. — I am sure most of you know this already, but every tech professional should be proficient in the command line. You will use it extensively when deploying and writing production code. I have a basic guide you can checkout here.Data Structures & Algorithms — Understanding the fundamental algorithms in computer science are very useful for MLE roles. Mainly because you will likely be asked about it in interviews. It’s not too hard to learn compared to machine learning; it just takes time. Any course will do the trick. — Again, most tech professionals should know Git, but as an MLE, it is essential. How to squash commits, do code reviews, and write outstanding pull requests are musts. — Many MLE roles I saw required you to have some specialisation in a particular area. I specialise in time series forecasting, optimisation, and general ML based on my previous experience. This helps you stand out in the market, and most companies are looking for specialists nowadays.The main theme here is that I basically up-skilled my software engineering abilities. This makes sense as I already had all the math, stats, and machine learning knowledge from being a data scientist.If I were a software engineer, the transition would likely be the reverse. This is why securing a machine learning engineer role can be quite challenging, as it requires proficiency across a wide range of skills.Summary & Further ThoughtsI have a free newsletter, , where I share weekly tips and advice as a practising data scientist. Plus, when you subscribe, you will get my and short PDF version of my AI roadmap!]]></content:encoded></item><item><title>➡️ Start Asking Your Data ‘Why?’ — A Gentle Intro To Causality</title><link>https://towardsdatascience.com/start-asking-your-data-why-a-gentle-intro-to-causality/</link><author>Eyal Kazin PhD</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 02:30:07 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Correlation does not imply causation. It turns out, however, that with some simple ingenious tricks one can, potentially, unveil causal relationships within standard observational data, without having to resort to expensive randomised control trials.This post is targeted towards anyone making data driven decisions. The main takeaway message is that causality may be possible by understanding that the  is as important as the data itself.By introducing  and , situations where the outcome of a population is in conflict with that of its cohorts, I shine a light on the importance of using causal reasoning to identify these paradoxes in data and avoid misinterpretation. Specifically I introduce as a method to visualise the story behind the data point out that by adding this to your arsenal you are likely to conduct better analyses and experiments.My ultimate objective is to whet your appetite to explore more on causality, as I believe that by asking data  you will be able to go beyond correlation calculations and extract more insights, as well as avoid common misjudgement pitfalls.Note that throughout this gentle intro I do not use equations but demonstrate using accessible intuitive visuals. That said I provide resources for you to take your next step in adding Causal Inference to your statistical toolbox so that you may get more value from your data.The Era of Data Driven Decision MakingIn [Deity] We Trust, All Others Bring Data! — William E. DemingIn this digital age it is common to put a lot of faith in data. But this raises an overlooked question: Should we trust data on its own?Judea Pearl, who is considered the godfather of Causality, articulated best:“The collection of information is as important as the information itself “ — Judea PearlIn other words the data is as important as the data itself.This manifests in a growing awareness of the importance of identifying bias in datasets. By the end of this post I hope that you will appreciate that causality pertains the fundamental tools to best express, quantify and attempt to correct for these biases.In causality introductions it is customary to demonstrate why “correlation does not imply causation” by highlighting limitations of association analysis due to spurious correlations (e.g, shark attacks  and ice-cream sales ). In an attempt to reduce the length of this post I defer this aspect to an older one of mine. Here I focus on two mind boggling paradoxes  and their resolution via to make a similar point.To understand the importance of the story behind the data we will examine two counter-intuitive (but nonetheless true) paradoxes which are classical situations of data misinterpretation.In the first we imagine a clinical trial in which patients are given a treatment and that results in a health score. Our objective is to assess the average impact of increased treatment to the health outcome. For pedagogical purposes in these examples we assume that samples are representative (i.e, the sample size is not an issue) and that variances in measurements are minimal.In the figure above we learn that on average increasing the treatment appears to be beneficial since it results in a better outcome.Now we’ll color code by age and gender groupings and examine how the treatment increases impacts each cohort.Track any cohort (e.g, “Girls” representing young females) and you immediately realise that increase in treatment appears adverse.What is the conclusion of the study? On the one hand increasing the treatment appears to be better for the population at large, but when examining gender-age cohorts it seems disadvantageous. This is Simpson’s Paradox which may be stated:“Trends can exist in subgroups but reverse for the whole”Below we will resolve this paradox using causality tools, but beforehand let’s explore another interesting one, which also examines made up data.Imagine that we quantify for the general population their attractiveness and how talented they are as in this figure:We find no apparent correlation.Now we’ll focus on an unusual subset — famous people:Here we clearly see an anti-correlation that doesn’t exist in the general population.Should we conclude that  and  are independent variables as per the first plot of the general population or that they are correlated as per that of celebrities?This is Berkson’s Paradox where one population has a trait trend that another lacks.Whereas an algorithm would identify these correlations, resolving these paradoxes requires a full understanding of the context which normally is not fed to a computer. In other words without knowing the story behind the data results may be  and  may be inferred.Mastering identification and resolution these paradoxes is an important  to elevating one’s analyses from correlations to .Whereas these simple examples may be explained away logically, for the purposes of learning causal tools in the next section I’ll introduce .Causal Graphs— Visualising The Story Behind The Data“[From the Simpson’s and Berkson’s Paradoxes we learn that] , but instead depend on the . … Graph Theory enables these stories to be conveyed” — Judea PearlCausal graph models are probabilistic graphical models used to visualise the story behind the data. They are perhaps one of the most powerful tools for analysts that is not taught in most statistics curricula. They are both elegant and highly informative. Hopefully by the end of this post you will appreciate it when Judea Pearl says that this is the missing vocabulary to communicate causality.To understand causal graph models (or causal graphs for short) we start with the following illustration of an example  with four nodes/vertices and three edges.Each node is a variable and the edges communicate “who is  to whom?” (i.e, correlations, joint probabilities).A  is one in which we add arrows as in this figure.A directed edge communicates “who  to whom?” which is the essence of causation.In this specific example you can notice a cyclical relationship between the C and D nodes.A useful subset of directed graphs are the  (DAG), which have no cycles as in the next figure.Here we see that when starting from any node (e.g, A) there isn’t a path that gets back to it.DAGs are the go-to choice in causality for simplicity as the fact that parameters do not have feedback highly simplifies the flow of information. (For mechanisms that have feedback, e.g temporal systems, one may consider rolling out nodes as a function of time, but that is beyond the scope of this intro.)Causal graphs are powerful at conveying the cause/effect relationships between the parameter and hence how data was generated (the story behind the data).From a practical point of view, graphs enable us to understand which parameters are confounders that need to be controlled for, and, as important, which not to control for, because doing so causes spurious correlations. This will be demonstrated below.The practice of attempting to build a causal graph enables:Design of better experiments.Draw causal conclusions (go beyond correlations by means of representing interventions, counterfactuals and encoding conditional independence relationships; all beyond the scope of this post).To further motivate the usage of causal graph models we will use them to resolve the Simpson’s and Berkson’s paradoxes introduced above.For simplicity we’ll examine Simpson’s paradox focusing on two cohorts, male and female adults.Examining this data we can make three statements about three variables of interest:Gender is an independent variable (it does not “listen to” the other two)Treatment depends on Gender (as we can see, in this setting the level given depends on Gender — women have been given, for some reason, a higher dosage.)Outcome depends on both Gender and TreatmentAccording to these we can draw the causal graph as the following:Notice how each arrow contributes to communicate the statements above. As important, the lack of an arrow pointing into Gender conveys that it is an independent variable.We also notice that by having arrows pointing from Gender to Treatment and Outcome it is considered a  between them.The essence of the Simpson’s paradox is that although the Outcome is effected by changes in Treatment, as expected, there is also a flow of information via Gender.As you may have guessed by this stage, the solution to this paradox is that the common cause Gender is a confounding variable that needs to be .Controlling for a variable, in terms of a causal graph, means eliminating the relationship between Gender and Treatment.This may be done in two manners:Pre data collection: Setting up a (RCT) in which participants will be given dosage regardless of their Gender.Post data collection: E.g, in this made up scenario the data has already been collected and hence we need to deal with what is referred to as In both pre- and post- data collection the elimination of the Treatment dependency of Gender (i.e, controlling for the Gender) may be done by modifying the graph such that the arrow between them is removed as in the following:Applying this “graphical surgery” means that the last two statements need to be modified (for convenience I’ll write all three):Gender is an independent variableTreatment is an independent variableOutcome depends on Gender and Treatment (but with no backdoor path).This enables obtaining the causal relationship of interest : we can assess the direct impact of modification Treatment on the Outcome.The process of controlling for a confounder, i.e manipulation of the data generation process, is formally referred to as applying an . That is to say we are no longer passive observers of the data, but we are taking an active role in modification it to assess the causal impact.How is this manifested in practice?In the case of RCTs the researcher needs to control for important confounding variables. Here we limit the discussion to Gender (but in real world settings you can imagine other variables such as Age, Social Status and anything else that might be relevant to one’s health).RCTs are considered the golden standard for causal analysis in many experimental settings thanks to its practice of confounding variables. That said, it has many setbacks:It may be  to recruit individuals and may be complicated The intervention under investigation may not be  possible or  to conduct (e.g, one can’t ask randomly selected people to smoke or not for ten years)Artificial setting of a laboratory — not a true natural  of the population.Observational data on the other hand is much more readily available in the industry and academia and hence much cheaper and could be more representative of actual habits of the individuals. But as illustrated in the Simpson’s diagram it may have confounding variables that need to be controlled.This is where ingenious solutions developed in the causal community in the past few decades are making headway. Detailing them are beyond the scope of this post, but I briefly mention how to learn more at the end.To resolve for this Simpson’s paradox with the given observational data oneCalculates for each cohort the impact of the change of the treatment on the outcomeCalculates a weighted average contribution of each cohort on the population.Here we will focus on intuition, but in a future post we will describe the maths behind this solution.I am sure that many analysts, just like myself, have noticed Simpson’s at some stage in their data and hopefully have corrected for it. Now you know the name of this effect and hopefully start to appreciate how causal tools are useful.I’ll be the first to admit that I struggled to understand this concept and it took me three weekends of deep diving into examples to internalised it. This was the gateway drug to causality for me. Part of my process to understanding statistics is playing with data. For this purpose I created an interactive web application hosted in Streamlit which I call Simpson’s Calculator . I’ll write a separate post for this in the future.Even if you are confused the main takeaways of Simpson’s paradox is that:It is a situation where trends can exist in subgroups but reverse for the whole.It may be resolved by identifying confounding variables between the treatment and the outcome variables and controlling for them.This raises the question — should we just control for all variables except for the treatment and outcome? Let’s keep this in mind when resolving for the Berkson’s paradox.As in the previous section we are going to make clear statements about how we believe the data was generated and then draw these in a causal graph.Let’s examine the case of the general population, for convenience I’m copying the image from above:Talent is an independent variableAttractiveness is an independent variableA causal graph for this is quite simple, two nodes without an edge.Let’s examine the plot of the celebrity subset.The cheeky insight from this mock data is that the more likely one is attractive the less they need to be talented to be a celebrity. Hence we can deduce that:Talent is an independent variableAttractiveness is an independent variableCelebrity variable depends on both Talent and Attractiveness variables. (Imagine this variable is boolean as in: true for celebrities or false for not).Hence we can draw the causal graph as:By having arrows pointing into it Celebrity is a  node between Talent and Attractiveness.Berkson’s paradox is the fact that when controlling for celebrities we see an interesting trend (anti correlation between Attractiveness and Talent) not seen in the general population.This can be visualised in the causal graph that by confounding for the Celebrity parameter we are creating a spurious correlation between the otherwise independent variables Talent and Attractiveness. We can draw this as the following:The solution of this Berkson’s paradox should be apparent here: Talent and Attractiveness are independent variables in general, but by controlling for the collider Celebrity node causes a spurious correlation in the data.Let’s compare the resolution of both paradoxes:Resolving Simpson’s Paradox is by  for common cause (Gender)Resolving Berkson’s Paradox is by for the collider (Celebrity)The next figure combines both insights in the form of their causal graphs:The main takeaway from the resolution of these paradoxes is that controlling for parameters requires a justification. Common causes should be controlled for but colliders should not.Even though this is common knowledge for those who study causality (e.g, Economics majors), it is unfortunate that most analysts and machine learning practitioners are not aware of this (including myself in 2020 after over 15 years of analysis and predictive modelling experience).“Oddly, statisticians both over- and underrate the importance of confounders“The main takeaway from this post is that the story behind the data is as important as the data itself.Appreciating this will help you avoid result misinterpretation as spurious correlations and, as demonstrated here, in Simpson’s and Berskon’s paradoxes.Causal Graphs are an essential tool to visualise the story behind the data. By using them to solve for the paradoxes we learnt that controlling for variables requires justification (common causes , colliders ).For those interested in taking the next step in their causal journey I highly suggest mastering Simpson’s paradox. One great way is by playing with data. Feel free to do so with my interactive “Simpson-calculator” .Unless otherwise noted, all images were created by the author.Wondering what your next step should be in your causal journey? Check out my new article on mastering Simpson’s Paradox — you will never look at data the same way. Here I provide resources that I find useful as well as a shopping list of topics for beginners to learn.This list is far from comprehensive, but I’m glad to add to it if anyone has suggestions (please mention why the book stands out from the pack).Provides memorable examplesOne paid course  that is targeted to practitioners is Altdeep.This list is far from comprehensive because the space is rapidly growing:Here I highlight a list of topics which I would have found useful when I started my learnings in the field. If I’m missing anything I’d be more than glad to get feedback and adding. I bold face the ones which were briefly discussed here.Pearl’s Causal Hierarchy of seeing, doing and imagining (figure above)Observational data vs. Randomised Control Trialsd-separation, , , mediators, instrumental variablesAssumptions: Ignorability, SUTVA, Consistency, Positivity“Do” Algebra — assessing impact on cohorts by interventionCounterfactuals — assessing impact on individuals by comparing real outcomes to potential onesThe fundamental problem of causalityEstimand, Estimator, Estimate, Identifiability — relating causal definitions to observable statistics (e.g, conditional probabilities)Causal Discovery — finding causal graphs with data (e.g, Markov Equivalence)Causal Machine Learning (e.g, Double Machine Learning)For completeness it is useful to know that there are different streams of causality. Although there is a lot of overlap you may find that methods differ in naming convention due to development in different fields of research: Computer Science, Social Sciences, Health, EconomicsHere I used definitions mostly from the Pearlian perspective (as developed in the field of computer science).The Story Behind This PostThis narrative is a result of two study groups that I have conducted in a previous role to get myself and colleagues to learn about causality, which I felt missing in my skill set. If there is any interest I’m glad to write a post about the study group experience.This intro was created as the one I felt that I needed when I started my journey in causality.In the first iteration of this post I wrote and presented the limitations of spurious correlations and Simpson’s paradox. The main reason for this revision to focus on two paradoxes is that, whereas most causality intros focus on the limitations of correlations, I feel that understanding the concept of justification of confounders is important for all analysts and machine learning practitioners to be aware of.On September 5th 2024 I have presented this content in a contributed talk at the Royal Statistical Society Annual Conference in Brighton, England (abstract link).Unfortunately there is no recording but there are of previous talks of mine:The slides are available at bit.ly/start-ask-why. Presenting this material for the first time at PyData Global 2021]]></content:encoded></item><item><title>How do I configure Minikube to use my local IP address instead of the cluster IP?</title><link>https://www.reddit.com/r/kubernetes/comments/1ipr3i1/how_do_i_configure_minikube_to_use_my_local_ip/</link><author>/u/Own_Appointment5630</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 02:02:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi there!! How can I configure Minikube on Windows (using Docker) to allow my Spring Boot pods to connect to a remote database on the same network as my local machine? When I create the deployment, the pods use the same IP as the Minikube cluster which gets rejected by the database. Is there any way that Minikube uses my local IP in order to connect correctly?.]]></content:encoded></item><item><title>what was the Linux expirance like in the 90&apos;s and 00&apos;s?</title><link>https://www.reddit.com/r/linux/comments/1ipql9k/what_was_the_linux_expirance_like_in_the_90s_and/</link><author>/u/mrcrabs6464</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 01:35:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I started using Linux about 2 years ago really right at the beginning of the proton revolution. And I know that Gaming in specif was the biggest walls for mass adaption of Linux throughout the 2010's and late 2000's but Ive heard things about how most software ran through WINE until Direct x and other API's became more common. but gaming aside what was the expirance and community like at the time?   submitted by    /u/mrcrabs6464 ]]></content:encoded></item><item><title>Q2DOS – Quake 2 backported to MS-DOS</title><link>https://dk.toastednet.org/Q2DOS/</link><author>jsheard</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 01:32:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Did Semgrep Just Get a Lot More Interesting?</title><link>https://fly.io/blog/semgrep-but-for-real-now/</link><author>ghuntley</author><category>dev</category><category>hn</category><pubDate>Sat, 15 Feb 2025 00:40:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[This bit by Geoffrey Huntley is super interesting to me and, despite calling out that LLM-driven development agents like Cursor have something like a 40% success rate at actually building anything that passes acceptance criteria, makes me think that more of the future of our field belongs to people who figure out how to use this weird bags of model weights than any of us are comfortable with. I’ve been dinking around with Cursor for a week now (if you haven’t, I think it’s something close to malpractice not to at least take it — or something like it — for a spin) and am just now from this post learning that Cursor has this rules feature. The important thing for me is not how Cursor rules work, but rather how Huntley uses them. He turns them back on themselves, writing rules to tell Cursor how to organize the rules, and then teach Cursor how to write (under human supervision) its own rules.Cursor kept trying to get Huntley to use Bazel as a build system. So he had cursor write a rule for itself: “no bazel”. And there was no more Bazel. If I’d known I could do this, I probably wouldn’t have bounced from the Elixir project I had Cursor doing, where trying to get it to write simple unit tests got it all tangled up trying to make Mox work. But I’m burying the lead. Security people have been for several years now somewhat in love with a tool called Semgrep. Semgrep is a semantics-aware code search tool; using symbolic variable placeholders and otherwise ordinary code, you can write rules to match pretty much arbitary expressions and control flow. If you’re an appsec person, where you obviously go with this is: you build a library of Semgrep searches for well-known vulnerability patterns (or, if you’re like us at Fly.io, you work out how to get Semgrep to catch the Rust concurrency footgun of RWLocks inside if-lets).The reality for most teams though is “ain’t nobody got time for that”. But I just checked and, unsurprisingly, 4o seems to do reasonably well at generating Semgrep rules? Like: I have no idea if this rule is actually any good. But it looks like a Semgrep rule?What interests me is this: it seems obvious that we’re going to do more and more “closed-loop” LLM agent code generation stuff. By “closed loop”, I mean that the thingy that generates code is going to get to run the code and watch what happens when it’s interacted with. You’re just a small bit of glue code and a lot of system prompting away from building something like that right now: Chris McCord is building a thingy that generates whole Elixir/Phoenix apps and runs them as Fly Machines. When you deploy these kinds of things, the LLM gets to see the errors when the code is run, and it can just go fix them. It also gets to see errors and exceptions in the logs when you hit a page on the app, and it can just go fix them.With a bit more system prompting, you can get an LLM to try to generalize out from exceptions it fixes and generate unit test coverage for them. With a little bit more system prompting, you can probably get an LLM to (1) generate a Semgrep rule for the generalized bug it caught, (2) test the Semgrep rule with a positive/negative control, (3) save the rule, (4) test the whole codebase with Semgrep for that rule, and (5) fix anything it finds that way. That is a lot more interesting to me than tediously (and probably badly) trying to predict everything that will go wrong in my codebase a priori and Semgrepping for them. Which is to say: Semgrep — which I have always liked — is maybe a lot more interesting now? And tools like it?]]></content:encoded></item><item><title>&quot;THESE &apos;EXPERTS&apos; LEFT THEIR DATABASE OPEN.&quot;: Anyone Can Push Updates to the DOGE.gov Website</title><link>https://www.404media.co/anyone-can-push-updates-to-the-doge-gov-website-2/</link><author>/u/ESHKUN</author><category>dev</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 00:37:43 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA["THESE 'EXPERTS' LEFT THEIR DATABASE OPEN."]]></content:encoded></item><item><title>Tabiew 0.8.4 Released</title><link>https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/</link><author>/u/shshemi</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 00:21:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...📊 Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and Sqlite🗂️ Multi-table functionalityUI is updated to be more modern and responsiveHorizontally scrollable tablesVisible data frame can be referenced with name "_"Compatibility with older versions of glibcTwo new themes (Tokyo Night and Catppuccin)]]></content:encoded></item><item><title>Is there any easy way to find a variable&apos;s type</title><link>https://www.reddit.com/r/rust/comments/1ipopuu/is_there_any_easy_way_to_find_a_variables_type/</link><author>/u/Money_Ad_4688</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 23:58:19 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I'm on a section of the rust book,Just for testing reasons, is there any way to write out this code and then check for the type of s (or what type results from .to_string() method)?It's probably going to result in String, but there are many methods out there that result in many different typesAny time I see a method whose return type I don't know, should I just refer to its documentation, or is there a way to check by compiling it?]]></content:encoded></item><item><title>Show HN: VimLM – A Local, Offline Coding Assistant for Vim</title><link>https://github.com/JosefAlbers/VimLM</link><author>JosefAlbers</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 23:34:41 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[VimLM is a local, offline coding assistant for Vim. It’s like Copilot but runs entirely on your machine—no APIs, no tracking, no cloud.- Deep Context: Understands your codebase (current file, selections, references).  
- Conversational: Iterate with follow-ups like "Add error handling".  
- Vim-Native: Keybindings like `Ctrl-l` for prompts, `Ctrl-p` to replace code.  
- Inline Commands: `!include` files, `!deploy` code, `!continue` long responses.Perfect for privacy-conscious devs or air-gapped environments.Try it:  
```
pip install vimlm
vimlm
```]]></content:encoded></item><item><title>GOGC &amp; GOMEMLIMIT ?</title><link>https://www.reddit.com/r/golang/comments/1ipnxxk/gogc_gomemlimit/</link><author>/u/mistyrouge</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 23:19:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[If the GC cost is fixed with regards to the amount of memory being freed up. Why would I not want to put  and  to say 70% of the memory I have available? Specially in an application that is known to be cpu bound.   submitted by    /u/mistyrouge ]]></content:encoded></item><item><title>A decade later, a decade lost (2024)</title><link>https://meyerweb.com/eric/thoughts/2024/06/07/a-decade-later-a-decade-lost/</link><author>ZeWaka</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 23:10:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I woke up this morning about an hour ahead of my alarm, the sky already light, birds calling.  After a few minutes, a brief patter of rain swept across the roof and moved on.I just lay there, not really thinking.  Feeling.  Remembering.Almost sixteen years to the minute before I awoke, my second daughter was born.  Almost ten years to the same minute before, she’d turned six years old, already semi-unconscious, and died not quite twelve hours later.So she won’t be taking her first solo car drive today.  She won’t be celebrating with dinner at her favorite restaurant in the whole world.  She won’t kiss her niece good night or affectionately rag on her siblings.Or maybe she wouldn’t have done any of those things anyway, after a decade of growth and changes and paths taken.  What would she really be like, at sixteen?We will never know.  We can’t even guess.  All of that, everything she might have been, is lost.This afternoon, we’ll visit Rebecca’s grave, and then go to hear her name read in remembrance at one of her very happiest places, Anshe Chesed Fairmount Temple, for the last time.  At the end of the month, the temple will close as part of a merger.  Another loss.A decade ago, I said that I felt the weight of all the years she would never have, and that they might crush me.  Over time, I have come to realize all the things she never saw or did adds to that weight.  Even though it seems like it should be the same weight.  Somehow, it isn’t.I was talking about all of this with a therapist a few days ago, about the time and the losses and their accumulated weight.  I said, “I don’t know how to be okay when I failed my child in the most fundamental way possible.”“You didn’t fail her,” they said gently.“I know that,” I replied. “But I don’t feel it.”A decade, it turns out, does not change that.  I’m not sure now that any stretch of time ever could.]]></content:encoded></item><item><title>What process/program is responsible for initiating Aoutomount?</title><link>https://www.reddit.com/r/linux/comments/1ipnki0/what_processprogram_is_responsible_for_initiating/</link><author>/u/GinnyTheRedHead</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 23:01:56 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I recently had problem with my system not being able to automatically mount some DVDs In fedora cinnamon and this led me to a rabbit hole of better undressing mounting in general. From what I’ve read udev is responsible for notifying when ever a new device is detected. And I’ve also read that the file manager has some part in this. But i could not find an exact explanation on how it works. Does the file manager has a prosses listing to udev at all time?I was hoping someone could help me understand this better or link me to some relevant docs. I dose not matter if it’s for a specific distro. The best thing i found was this that doesn’t explain how things work https://help.ubuntu.com/community/Mount/USB]]></content:encoded></item><item><title>If you ever stacked cups in gym class, blame my dad</title><link>https://defector.com/if-you-ever-stacked-cups-in-gym-class-blame-my-dad</link><author>nonoobs</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 22:45:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The boxes came from Tokyo: first by tanker, then overland via container truck from a Pacific port, across the Continental Divide, and finally backed into a driveway at the end of a cul-de-sac in a south Denver suburban enclave. This was a neighborhood with Razor scooters dumped in trimmed front lawns. Where family walks with leashed dogs happened down the middle of intentionally curved streets named after long demolished natural landmarks like "Timbercrest" and "Forest Trails." Where the HOA (because of course there was an HOA) banned the installation of driveway basketball hoops.Receiving industrial freight deliveries, freshly cleared through international customs, probably wasn't explicitly prohibited in the homeowner's handbook. But then, why would it need to be? Nobody would think to bring that kind of commercial chaos into the burgeoning middle-class peace of Castle Pines North in 1998.If neighbors peeking behind curtains at the idling 18-wheeler thought to call in a complaint, the husband and wife receiving the delivery didn't notice. They were too busy unloading boxes—more than 800 of them. Balancing four at a time on a handcart, it took 200 trips through the open garage door and down the unfinished basement steps. The boxes, holding smaller rectangular packages inscribed with Japanese lettering, were piled to the ceiling. There was enough room left for a skinny aisle leading back up the stairs, and two plastic tables ladened with tape and flat-rate USPS packaging. 5,800 miles away, a man in middle management at the global toy conglomerate Hasbro must have been very pleased. The delivery represented a small yet unexpected boon. Those boxes were dead inventory, wasting space in a nondescript warehouse. They should have been headed for a landfill and a tax write-off. Instead, a Mr. Toshio Takiguchi brokered the export at a ¥1,300 per-unit cost. Not a tidy profit, but no longer a loss on the annual P&L. The remains of a failed business decision disappeared across the ocean. It was a certain Mr. Fox's problem now.That problem cost $43,000, a sum that represented the entire life savings of Mr. Fox and his wife, who at that point had been surviving on public-school salaries. Friends and family never really said it out loud, but they were certainly thinking it: This was insane. What about the three kids and that mortgage? The couple, though, never had a doubt. "He always used to tell me, 'If I had 10,000 of these, I could sell them in a year,'" Mrs. Fox recalled, 27 years later.That's the unabashed confidence you'd expect from any good entrepreneur, especially with the benefit of hindsight. But ask yourself this: Would you, watching this couple unload box after box into their bank-financed home, have bet on this man and his family's future if you knew exactly what he needed to sell?Tucked inside those boxes, in nested columns of 12, were 120,000 plastic cups. They were turned upside down, each with a hole drilled through the middle of the base, rendering them useless in terms of a cup's normal, and really only, task.The couple's future hinged on accomplishing what the world's second largest toy company could not: convincing thousands of kids that stacking these plastic cups in pre-determined patterns was … . More critically, the couple needed to convince the parents of those kids to actually buy these cups, despite not even being able to drink from them. Mr. Takiguchi could breathe a sigh of relief. Mr. and Mrs. Fox had to get to work.The first thing you need to know about Bob Fox is that he used to be a clown ... The second thing you need to know about my dad is that he was a really, really good clown. Every few months, in different corners of the internet, someone asks a version of the same question: Why did we all stack plastic cups in elementary school PE class?You might have even asked it yourself, perhaps after a few beers when a friend turned some Solo Cups upside down, activating one of your mid-aughts memories of sitting cross-legged on a linoleum gym floor, surrounded by the clatter of your classmates piling cups into pyramids.Sport Stacking, as it's officially known (cup stacking and speed stacking as it is colloquially known), exists in the same cloud of millennial nostalgia where you'll find vague recollections of SlamBall and JoJo. It has appeared on pretty much every morning and late show, been a trend piece in both the  and , and was once labeled by Glenn Beck as "what’s really wrong with America."It has been a minor plot line in , Matt LeBlanc's , and ; it's been the major plot line in a 2022 Thai-language film released on Netflix titled  (100 percent on Rotten Tomatoes). The infamous "Oh my gosh!" scream in Skrillex's "Scary Monsters and Nice Sprites" was sampled from a viral sport stacking video from 2008.At its peak, between 2002 and 2011, roughly 5,000 American schools included it as part of their annual curriculum, according to Mr. and Mrs. Fox. That means somewhere between five and eight percent of U.S. adults between the ages of 22 and 35 share the same core memory—and in the ensuing years have asked themselves, their friends, or social media the same question: Why did credentialed educational professionals make us do this ludicrous activity in gym class?I am, perhaps, the person best suited on the planet to answer this question. Because the answer … is my dad.The first thing you need to know about Bob Fox is that he used to be a clown. Not in the figurative, funnyman personality sense. In the literal red-nose, black-eyeliner, juggling-tennis-balls-at-children’s-birthday-parties sense.The second thing you need to know about my dad is that he was a really, really good clown. He'd scoff at the mental image you probably have of him right now: oversized shoes, cartoonish honks, and bumbling choreography. My dad's act was entirely silent, required immense skill, and was predicated on audience participation. He was equally good at riding a unicycle and convincing the most curmudgeonly dad in the room to leave their back corner and join him onstage to try—and always fail—at blowing up a balloon animal. Kids went bananas. Adults always laughed. The San Francisco 49ers hired him to be the walk-around entertainment for their post–Super Bowl ring ceremony in 1995. We used to shut the shades so neighbors didn't think he robbed a bank as he counted the cash he earned during Denver's largest busking festival.This was a man who took the unserious very, very seriously. As a high school theater teacher, he'd spend 60–80 hours a week building out increasingly elaborate sets for the annual musical. He built a life-sized plant puppet that could actually eat cast members for , and cut a full-sized muscle car in half so it would fit on stage during ; another car was hoisted onto the school's roof to promote the show. Before her bevy of Oscar nominations, Amy Adams was one of his choreographers and students.He quit because his wife, Jill, needed him home more to help raise their three children. So he became an elementary school classroom and PE teacher instead, where he would end up teaching hundreds of kids how to ride unicycles and produce an annual show on the blacktop basketball court called KidZircus.The first thing you need to know about Jill Fox is that before working as a communications administrator at a school district, she used to be a journalist at a small newspaper, where she was assigned to write a profile about a local clown on "sabbatical" from his full-time job. The second thing you need to know about Jill Fox is that despite the word "sabbatical" doing a generous amount of euphemistic work in her final published piece, she married that unemployed clown.This was a woman who was very, very good at seeing—and supporting—the potential in the crazy.They had to figure out how to tap into the supply chain of global toy conglomerates. Fortunately, they had a guy: Uncle Johnny.It took them 10 months to sell the 10,000 sets of 12 cups. Kids in south Denver, it turns out, went absolutely nuts over this new phenomenon called "cup stacking." That's mostly because my dad figured out the one thing Hasbro did not: The level of interest in the activity hinged almost exclusively on the enthusiasm and skill level of a real-life teacher. Without that, it was just inanimate plastic ephemera sitting on a shelf.The basic rules take about 15 minutes to learn. Like a track meet, the sport is built on completing different "distances" as fast as you can. The shortest, and simplest, is called the 3-3-3. Envision each one of those 3's as a nested column of cups. You start on one side and "upstack" the first 3 into a pyramid, then move to the middle, and then the last 3. Then you return to where you started, and "downstack" them back into columns in the same order. The 3-6-3 has the same principle, but with a pyramid of six cups in the middle. The "Cycle," is the sport's primetime event, starting with the 3-6-3, morphing into a 6-6, and ending with a 1-10-1.If you skimmed over that clunky paragraph, you've proved my dad's foundational epiphany: You really need to see it to buy in. In fact, scrap reading this for a few seconds (that's all it will take) and watch this video instead.Place a set of these cups in the corner of a room or on a toy shelf with paper instructions, and you might earn five minutes of an 8-year-old's frenzied attention. Put that same 8-year-old on the gym floor in front of a professionally trained clown with a passion for juggling, and they will be utterly hooked.A brief digression on nomenclature: The sport was known as "cup stacking" until 2005, when my parents had the savvy to officially change the name to "sport stacking." Why? 1) Because it sounds way cooler, and if enough people call it a sport then it must damn well be one. 2) The phrase "cup stacking" out of context evokes toddlers playing with blocks, a vibe they wanted to avoid. Confusingly, you might have also seen it referenced as "speed stacking," after the company they eventually founded, Speed Stacks. They strategically eschewed this label to avoid the fate of Roller Blades, Frisbee, and Kleenex, all formerly good standing trademarks, until you lot ruined everything. Except for this early-stage reference, I'll be calling it sport stacking hereafter so as to avoid a grumpy phone call from my dad (he'll probably still call about my use of "damn," though). If you have a problem with that, consider the last time you went berserk over a nine-darter or screamed at the Swedish curling team during the Olympics. As my dad likes to say, there used to be a man trying to convince the world that tossing balls made from dried cow skin into retrofitted peach baskets was fun—and you all bought in.It is important to note that my dad did not actually invent sport stacking. Credit goes to a group of bored kids at a Boys and Girls Club in Oceanside, Calif., in the early 1980s. They'd been given a stack of paper cups, and told to figure out a game that didn't involve messy liquid. Under the guidance of a program director named Wayne Godinet, both the rules and equipment evolved. He is responsible for the sport's first major innovation: He drilled a hole through the base of hard plastic cups to reduce air friction and prevent sticking. The California group's high-water mark came on Nov. 2, 1990, when they appeared on The Tonight Show with Johnny Carson. Among the millions watching that night, from his bed in a south Denver suburb: a clown and future PE teacher who thought it looked a bit like upside-down juggling.Hasbro must have been watching, too. Perhaps chasing the Hoola Hoop and Pogs high of the '80s, the company scooped up a license to sell "Kup Stax," popping sets in a rectangular plastic sleeve on toy store shelves around the world. Instead of becoming a fad, the product moved to the clearance rack. Someone in middle management was probably scolded. Manufacturing ceased. In 1995, my dad attended a physical education development workshop to give new teachers a dozen ideas on how to keep kids occupied, exercised, and docile. The classics were all there: four-wheeled square scooters designed to give 80 percent of users elbow burn, dodgeballs primed to pop kids like me in the face, that large circus tent you toss above your head until it temporarily inflates so you can sit under it for a few seconds. So were the cups. He brought them home and taught his kids. I was 4, my brother 6, our sister 9. We not only fell in love with them, we got good—. We weren't outliers. Among an elementary school of 500 students, 250 signed up for an afterschool program to keep stacking. My mom drove to every Toys "R" Us and Walmart within a 50-mile radius. She bought every set of Kup Stax she could find. They were always discounted, tucked in a back corner.The local supply ran dry as my parents hosted workshops. Demand started to skyrocket from other PE teachers. My parents needed to go upstream. They had to figure out how to tap into the supply chain of global toy conglomerates. Fortunately, they had a guy: .Now 64 with a Colonel Sanders goatee and passion for intricate leather carving, my dad's brother has been at certain points in his life: a professor at the Ringling Bros. and Barnum & Bailey Clown College; a street mime in San Diego; a director of a Las Vegas magic show, performed entirely on ice; and a consultant for Japan's oldest and largest circus. If you need to find a man in middle management at Hasbro's Tokyo corporate office, you call Uncle Johnny.The deal was struck. The shipment arrived. And the cups, rather magically, disappeared from our basement.By 1998, Bob Fox was a dealer with no product and an increasingly addicted customer base. There was only one logical next step: Transition from distribution to fabrication.He did make a final good-faith effort to partner with the sport's originator, Wayne Godinet, offering to purchase $20,000 worth of product. It was almost a third of the profit earned from reselling the Japanese shipment. Godinet sent back two sample sets with a bill for the cost of goods and shipping.In a similar vein, Nike was founded in 1964 only after a running shoe company called Onitsuka lost interest in partnering with a recent Oregon track grad named Phil Knight. While his shoe empire was born between the grooves of a waffle iron, Bob and Jill's cup empire was sketched on the back of a Fresh Fish Co. paper placemat.Speed Stacks LLC was incorporated in December 1998. My parents quit their full-time public school jobs in June 2000. This was to be an official test of a core tenet of the American Dream: With enough passion and even more prodigious work ethic, you can turn a ludicrous idea into a successful business. To get started, they just needed four things.First: a mold to mass produce the cups. It required $20,000 and overcoming the absolute befuddlement of executives at a Denver plastics manufacturer who did not understand the  to include three small holes at the base. As each cup came off the line, our family would inspect the shape; minor defects in its roundness caused sticking. Nearly every cup in the first run had to be tossed out for deformities.Second: a nylon and mesh cup-carrying bag. Those were sewn in the back basement of a Vietnamese immigrant–owned textile company. We bagged each set of 12 by hand, my parents staying up until 2:00 in the morning to fulfill orders after working full school days. Bob wasn't satisfied unless the logo of every cup perfectly faced forward.Third: to hit the road. Speed Stack's first tagline was "See it! Believe it! Teach it!" My parents meant it: They knew that success was linearly related to the number of PE teachers who witnessed the sport in person. Our family of five put almost 100,000 miles on our 1997 maroon Suburban in a little over four years.Fourth: kids who could stack the cups. . For some reason, all three of the Fox progeny developed both the interest and intrinsic skill to do just that.At a booth in hotel conference halls and city convention centers, we'd stack for hours, drawing crowds spread three and four rows deep, often to the annoyance of the dodgeball or shuttlecock purveyors next door. Legendary ring announcer Michael Buffer’s voice was always our music cue. "Ladies and gentlemen, welcome to the main event!"It would blast over portable speakers on either side of a gym floor. Emmy, my older sister, would lead us out from a hidden corner, followed by my brother, Brennan, then me: a 7-year-old with Coke-bottle glasses, immensely satisfied by the fact that hundreds of eyes were watching my every move.We stopped when we reached separate tables spread evenly across our makeshift stage, usually under the basketball hoop. We stood still to let the tempo build. "Let’s get ready to rumble!"The synths would drop, and we would commence stacking plastic cups. Faster and with more skill than the students and teachers in Amarillo or Ft. Lauderdale or Butzbach, Germany, or Copenhagen, Denmark, could ever think possible. Our family performed hundreds of school assemblies in almost every corner of the country (and some overseas). The 30-minute spectacle was choreographed specifically to blow the minds of kids between the ages of 7 and 14 as quickly as possible. This wasn't a talk from your fire chief about safety, or puppets teaching you the value of sharing. We humiliated principals and popular teachers by making them race us. There was a fabricated jacket with holes cut in the back that we fitted onto a volunteer, then stuck our arms through the sleeves to make it appear like they were magically adept (performed to the  soundtrack). The finale was a choreographed stacking routine, set to "Dueling Banjos" from .But a school-by-school roadshow wasn't a scalable business model. So we went to the nexus of physical education sweatsuitdom: state and national teacher conventions. At a booth in hotel conference halls and city convention centers, we'd stack for hours, drawing crowds spread three and four rows deep, often to the annoyance of the dodgeball or shuttlecock purveyors next door. It's most likely that you learned how to sport stack because your teacher, with a lanyard badge bouncing over her multi-colored windbreaker on her way to get a free muffin from the guys who sell tug-o-war ropes, stopped in her tracks in the back corner of Exhibit Hall B at an exurban Renaissance hotel. She would watch as our hands manipulated dishware so fast it blurred.If my dad was adept at putting on a good show, my mom was equally adept at corralling the press. Her journalism and PR–honed chops put sport stacking in front of millions, from the D-block of local newscasts to 30 Rock Plaza during the . That's how I found myself chatting with a shirtless Simon Cowell in the makeup room before we both appeared on Ellen DeGeneres's show, taught Tiki Barber the basics of the 3-3-3, and tried to convince a baffled Michael Caine in the green room of LIVE with Regis and Kelly that yes, indeed, kids really do stack plastic cups for fun.All press, to my parents, was good. My dad was absolutely thrilled when Glenn Beck spent four minutes of his daily Fox News monologue staring directly into the camera, railing against the dangers of stacking cups."We can't fix our country by keeping our children weak," Beck sputtered, roughly two minutes in. "They need to have spines. They need to get hit in the face a few times with a ball, you know what I mean? They have to learn how to live with and thrive on past failures."Brands soon took advantage of sport stacking's essential power: No matter the context, it attracts near-instant attention. In 2005, I appeared in a nationwide Comcast commercial for high speed internet. FritoLay shut down part of Times Square to feature our stacking talents as it launched Stax, a competitor to Pringles. A beverage company in England flew a dozen of us to London to help launch an abomination of a drink called Freekee Soda. Main ingredient? Carbonated milk. (Did I pretend to enjoy drinking that carbonated milk so as to not piss off any of the execs helping to pay for my college education? Obviously. But it is a cursed flavor.)Perhaps the only media my parents turned down was an appearance on the infamous reality show . The producers, naturally, presumed the scions of a ludicrous gym activity would have some weird baggage to mine for the masses. Unfortunately, despite the cups, we were pretty normal.To stack as fast as you possibly can in a World Championship is to strike a balance between fluidity and chaos.It was the Germans who made everything more serious.They arrived at the 2004 World Sport Stacking Championships in matching flag-inspired sweatsuits. Multiple coaches would analyze everything from hand-tag efficiency during the four-person relay event to the mental routines of stackers before their individual competition.Sport stacking, I believe, grew into a minor phenomenon for three reasons: 1) It's visually arresting, 2) it's easy to learn, and 3) competing against the clock is incredibly addicting.Media buzz and schoolteachers kindled the first two. But my dad's magnum opus—the reason the sport didn't wither in the way of skip ball and snake boards—was that he (with the help of the 20 employees at the burgeoning HQ) built the infrastructure and equipment to spur global competition. The sport's growth coincided with two of my dad's innovations: the StackMat, a patented self-timing device that allowed kids to practice on their own for hours, aiming to trim hundredths of a second off their best times; and the World Sport Stacking Association, a governing body to codify the rules and serve as the keeper of world, national, and local records.In 1997, 250 kids were hand-timed with stop watches at the first Colorado State Championship. By 2007, ESPN aired an hourlong David Lloyd–hosted special of the World Championships, featuring more than 1,000 competitors from seven countries (re-aired in 2020 during their pandemic "The Ocho" promotion). Like everything in his life, my dad choreographed a spectacle. He ordered a three-story banner to hang behind garbage can–sized cups that served as center stage. The competition floor was arranged with dozens of tables, all marked with laser-lined tape to demarcate relay boundaries.During the final "Stack of Champions," the top competitors (and relay teams) from each event competed under the scrutiny of spotlights, three officials, an instant replay camera, and hundreds of spectators. In individual events, you're allotted two warmups and three tries. Relays are conducted head-to-head in teams of four.For a few years, I was among those finalists. To stack as fast as you possibly can in a World Championship is to strike a balance between fluidity and chaos. Push too hard and the cups tumble. Pull back and you'll lose by thousandths of a second.A relay race in the Stack of Champions, in which I (the third stacker on the left team) stumble at the start, allowing my brother (the last stacker on the right team) to beat us by two-hundredths of a second.For those moments, you exist at the outer physical boundary of human biology. When performing at your peak, the cups feel less like they're being stacked and more like they're being sprayed. In 2001, the world record for sport stacking's marquee event, the Cycle, was 7.43 seconds. It was held by my sister. At the time, most people assumed a sub-seven-second time was physically impossible. The pattern requires more than 40 separate moves. At those speeds, hands make micro-adjustments faster than the brain can register. Her time stood for nearly four years.In 2025, the world record for the cycle is 4.739 seconds. Please understand: I know this whole endeavor is silly, that it's worthy of your slight mockery and general patina of confusion. But also understand that my dad created a culture in which German coaches passed strategic tips to Australian competitors. Where Japanese teenagers became pen pals with suburban Texans. Where nerves collapsed faster than the cups. Where kids and adults shrieked with joy over millisecond improvements. Where tension and drama and friendship mingled with the clattering cacophony of sliding and tapping plastic (there really is no sound like the one at these competitions). Sport stacking had broad exposure in both mainstream and weird places. But it had a deep impact in a much smaller community. It changed lives.You've probably heard Rachael’s voice. Her effusive reaction on a stacking YouTube video meant for her small group of friends is behind the iconic "YES, OH MY GOSH!" scream, sampled in one of EDM's most mainstream hits, "Scary Monsters and Nice Sprites," by Skrillex.Milo Ferguson discovered sport stacking in first-grade PE class. "My teacher was the final boss of butch lesbians. That's the only way I know how to describe her," Ferguson, now a 23-year-old co-founder of an independent animation studio, said. Their teacher loved competition and was beloved by her students, exactly the archetype to stop at a conference booth and believe in a quirky new cup-centric educational trend.Rachael Nedrow, now a 29-year-old product manager at Amazon, first saw the sport while scrolling YouTube when she was 11. She pulled paper cups from the kitchen drawer to try and mimic the movement.Both became obsessed, quickly. Though for different reasons."I was never the fastest kid. I was always kind of chubby. I was much more the creative person than an athletic one," Ferguson said. "Sport stacking was about hand-eye coordination more than strength. It was intricate. Delicate." For the first time, they were excited to attend PE class.Nedrow, an accomplished tennis player, was intrigued by the self-improvement and rigid strictures of the clock. She bought an official set of Speed Stacks and started logging her progress on her own YouTube channel."My first competition was at a local church," Nedrow said. "I did terribly, but I still won."Despite differing motivations, Ferguson and Nedrow's paths to competitive sport stacking mirror each other. Friends weren't interested, so they went looking for a community online and at competitions. For the thousand kids that congregated on the arena floor at each World Championships, like Ferguson and Nedrow, the sentiment was common. They had found  people. They fit in, sometimes for the first time in their life. There were future college athletes and kids with autism. There were rugby fans from New Zealand (riffing off their national team, they called themselves the "All Stacks") and South Koreans who had never been to the United States. Roll your eyes, but my dad loved to say that the sport only helped build "positive pyramids." Spend a few hours watching kids pat each other on the back after a fumbled cup, or earn a hug for achieving a new personal best, and you'd buy into the platitude too.Compared to other top-level stackers, Nedrow and Ferguson's experiences only differ in one way. You've probably heard Rachael's voice. Her effusive reaction on a stacking YouTube video meant for her small group of friends is behind the iconic "YES, OH MY GOSH!" scream, sampled in one of EDM's most mainstream hits, "Scary Monsters and Nice Sprites," by Skrillex. Between Spotify and YouTube, it has more than 500 million streams.And you probably know Ferguson's dad, Craig, the comedian and host of The Late Late Show with Craig Ferguson from 2005 to 2014.Nedrow's collision with virality gave her backstage passes to several Skrillex concerts and the greatest fodder for "Two Truths and a Lie" of all time. But sport stacking also gave her something else she wasn’t expecting: a wide group of online friends."I don't think I would have taken stacking so far if it wasn't for the online community," Nedrow said. "When you are doing anything kind of competitive, it seems like it's much better when you are doing it with people. I really loved the aspect of kids commenting on my videos, saying they started stacking because of me. What greater pleasure can you get than inspiring other people to do what you love the most?"Sport stacking gave Ferguson inclusion and validation. But perhaps more important than what it gave them is what it gave their parents. While Craig and his ex-wife Sascha had been separated for a few years by the time their kid started competing, both fully committed to supporting this weird, wholesome obsession. Sascha helped Milo learn how to breathe and center themselves before starting to stack. And she supplied an ever-growing collection of stacking equipment, stored in a dedicated room in their Southern California home.To see what sport stacking gave Craig, you need to watch his show from April 20, 2009. He spends four minutes—more than half of the entire monologue—bragging about Milo. Beneath the scripted jokes and of-the-era references (Octomom is mentioned twice), you see a guy just genuinely proud of his kid. He's like any neighbor, going off a bit too effusively about the weekend's peewee football game or math test. Except this was done for millions. And it was about ... cups. "I wanted to give kids, no matter their athletic ability, the opportunity for success. It gave them something new and different. And it created a community. It brought kids around the world together."Sport stacking gave different things to a lot of people.For a few, it genuinely improved their life. College funds were paid through YouTube viewership revenue and small brand endorsements. Children with autism and special needs learned, practiced, competed, and socialized with others for the first time. For me and my siblings, it gave us a stable home, trips to a dozen countries, and some excellent college essay material. My sister, Emmy, went on to play basketball for four years at the University of Minnesota, was drafted into the WNBA by the Minnesota Lynx, and then played professionally overseas with Sheryl Swoopes. My brother Brennan played D-III college football and turned into a mechanical engineer at a global cosmetics company. I turned into a fairly unathletic writer.For my parents, it gave them an early retirement. A sometimes-operational 1957 VW Bug and an obsession with growing ever-larger pumpkins occupied my dad, while training to run another marathon at age 70 became a focus for my mom. They left their active roles at Speed Stacks in 2015, leaving control to some of its earliest employees. Programs are still active in thousands of schools around the world."I was motivated to spread sport stacking for the same reason I taught my students how to juggle or unicycle," my dad recalled, nearly 10 years into retirement. "I wanted to give kids, no matter their athletic ability, the opportunity for success. It gave them something new and different. And it created a community. It brought kids around the world together. And believe it or not, that all happened because of a couple million plastic cups."For some of you reading this, sport stacking gave you an identical memory. A cross-legged seat on a linoleum gym floor. The hush from the principal. The music crescendoing into ' most iconic synth riff. The sound of cups, stacking at lightning speed. You knew, in that moment, that you weren't in for another monotonous few hours at school. Your day was about to rock. For that, you can thank my dad.]]></content:encoded></item><item><title>We were wrong about GPUs</title><link>https://fly.io/blog/wrong-about-gpu/</link><author>mxstbr</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 22:36:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[We’re building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren’t going anywhere, but: GPUs aren’t going anywhere.A Fly Machine is a Docker/OCI container running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It’s a Fly Machine that can do fast CUDA.Like everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn’t fit the moment. It’s a bet that doesn’t feel like it’s paying off.If you’re using Fly GPU Machines, don’t freak out; we’re not getting rid of them. But if you’re waiting for us to do something bigger with them, a v2 of the product, you’ll probably be waiting awhile.GPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines Intel’s Cloud Hypervisor, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors.GPUs terrified our security team. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers(not even bidirectional: in common configurations, GPUs talk to each other)with arbitrary, end-user controlled computation, all operating outside our normal security boundary.We did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren’t mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there’s a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers.We funded two very large security assessments, from Atredis and Tetrel, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time.Security wasn’t directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason.We could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we’d have been on Nvidia’s driver happy-path.Alternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path.Instead, we burned months trying (and ultimately failing) to get Nvidia’s host drivers working to map virtualized GPUs into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU.I’m not sure any of this really mattered in the end. There’s a segment of the market we weren’t ever really able to explore because Nvidia’s driver support kept us from thin-slicing GPUs. We’d have been able to put together a really cheap offering for developers if we hadn’t run up against that, and developers love “cheap”, but I can’t prove that those customers are real.On the other hand, we’re committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer’s OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our  orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying!And, of course, we bought GPUs. A lot of GPUs. Expensive GPUs.The biggest problem: developers don’t want GPUs. They don’t even want AI/ML models. They want LLMs.  may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But  don’t care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can’t just give them a GPU.For those developers, who probably make up most of the market, it doesn’t seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of “tokens per second” aren’t counting milliseconds.(you should all feel sympathy for us)This makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they’ll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn’t seem to matter yet, so the market doesn’t care.Past that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough.People doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s.Near as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device.We think there’s probably a market for users doing lightweight ML work getting tiny GPUs. This is what Nvidia MIG does, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it’s not baked; we can’t use it. And I’m not sure how many of those customers there are, or whether we’d get the density of customers per server that we need.That leaves the L40S customers. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they’re the one part we have in our inventory people seem to get a lot of use out of. We’re happy with them. But they’re just another kind of compute that some apps need; they’re not a driver of our core business. They’re not the GPU bet paying off.Really, all of this is just a long way of saying that for most software developers, “AI-enabling” their app is best done with API calls to things like Claude and GPT, Replicate and RunPod.A very useful way to look at a startup is that it’s a race to learn stuff. So, what’s our report card?First off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of  models, the world Elixir Bumblebee looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems.But Cursor happened, and, as they say, how are you going to keep ‘em down on the farm once they’ve seen Karl Hungus? It seems much clearer where things are heading.GPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing.Another way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn’t a winning strategy. I’d rather we’d flopped the nut straight, but I think going in on this hand was the right call.A really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our costs here aren’t recoverable. But the hardware parts that aren’t generating revenue will ultimately get liquidated; like with our portfolio of IPv4 addresses, I’m even more comfortable making bets backed by tradable assets with durable value.In the end, I don’t think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I’m very happy about is that we didn’t compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we’re scaling back our GPU ambitions without having sacrificed any of our isolation story, and, ironically, GPUs  are making that story a lot more important. The same thing goes for our Fly Machine developer experience.We started this company building a Javascript runtime for edge computing. We learned that our customers didn’t want a new Javascript runtime; they just wanted their native code to work. We shipped containers, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That’s usually how we figure out the right answers:  by being wrong about a lot of stuff.]]></content:encoded></item><item><title>Dell Nears $5 Billion AI Server Deal for Elon Musk’s xAI</title><link>https://www.bnnbloomberg.ca/business/technology/2025/02/14/dell-nears-us5-billion-ai-server-deal-for-elon-musks-xai/</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 22:25:37 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[(Bloomberg) -- Dell Technologies Inc. is in advanced stages of securing a deal worth more than $5 billion to provide Elon Musk’s xAI with servers optimized for artificial intelligence work.The company will sell servers containing Nvidia Corp. GB200 semiconductors to Musk’s AI startup for delivery this year, according to people familiar with the matter, who asked to not to be named because the work is private. Some details are being finalized and still may change, some of the people added. Demand for computing to run AI workloads has led to a boom for makers of high-powered servers like Dell, Super Micro Computer Inc. and Hewlett Packard Enterprise Co. Musk’s companies, including carmaker Tesla Inc. and xAI, have emerged as major customers for the hardware. Dell and Nvidia declined to comment. xAI didn’t respond to a request for comment.Dell shares jumped as much as 6% to $116.88 Friday on the news before paring some gains. The stock had slipped 4.3% this year through Thursday’s close.A supercomputer project being built by xAI in Memphis has used a mix of Dell and Super Micro servers. In December, Dell said it had deployed tens of thousands of graphics processing units, or GPUs, there and was working to win an “unfair share” of the remaining build-out. GPUs are the key chips to power AI workloads and Nvidia is the top maker of those processing units.Analysts expect Dell will have shipped more than $10 billion of AI servers in the fiscal year ending last month and project that value will jump to $14 billion in the fiscal year ending in January 2026. Dell is scheduled to report fiscal fourth-quarter earnings on Feb. 27, with the AI server business a major focus for investors.The deal with xAI “would firmly establish the company as a leading AI-server provider and boost sales, though the impact on profitability is less clear,” wrote Woo Jin Ho, an analyst at Bloomberg Intelligence.AI startup xAI’s main product, a chatbot called Grok, has primarily been available to paying users of X, the social network formerly known as Twitter. Firms that Musk runs are known to share employees, technology and computing power.--With assistance from Ian King and Kurt Wagner.(Updates with comments from analyst in the eighth paragraph.)]]></content:encoded></item><item><title>Meta Plans Major Investment Into AI-Powered Humanoid Robots</title><link>https://www.bnnbloomberg.ca/business/technology/2025/02/14/meta-plans-major-investment-into-ai-powered-humanoid-robots/</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 22:22:27 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[(Bloomberg) -- Meta Platforms Inc., after pushing into augmented reality and artificial intelligence, has identified its next big bet: AI-powered humanoid robots. The company is making a significant investment into the category — futuristic robots that can act like humans and assist with physical tasks — and is forming a new team within its Reality Labs hardware division to conduct the work, according to people with knowledge of the matter.Meta plans to work on its own humanoid robot hardware, with an initial focus on household chores. Its bigger ambition is to make the underlying AI, sensors and software for robots that will be manufactured and sold by a range of companies, said the people, who asked not to be identified because the initiative hasn’t been announced. Meta has started discussing its plan with robotics companies, including Unitree Robotics and Figure AI Inc. At least initially, it doesn’t plan to build a Meta-branded robot — something that could directly rival Tesla Inc.’s Optimus — but it may consider doing so in the future, the people added. The humanoid effort mirrors exploratory projects at other technology giants, including Apple Inc. and Alphabet Inc.’s Google Deepmind division. A Meta spokesperson declined to comment.Meta confirmed the creation of the new team to employees Friday, telling them it will be led by Marc Whitten, who resigned as chief executive officer of General Motors Co.’s Cruise self-driving car division earlier this month. He was previously an executive at gaming company Unity Software Inc. and Amazon.com Inc.“The core technologies we’ve already invested in and built across Reality Labs and AI are complementary to developing the advancements needed for robotics,” Andrew Bosworth, Meta’s chief technology officer, wrote in a memo reviewed by Bloomberg News. He mentioned the company’s advancements in hand tracking, computing at low bandwidth and always-on sensors.Meta executives believe that while humanoid robotics companies have made headway in hardware, Meta’s advances in artificial intelligence and data collected from augmented and virtual reality devices could accelerate progress in the nascent industry. Current humanoids are still not useful enough to fold clothes, carry a glass of water, place dishes in a rack for cleaning or conduct other home chores that could get consumers interested in the category. “We believe that expanding our portfolio to invest in this field will only accrue value to Meta AI and our mixed and augmented reality programs,” Bosworth wrote. Whitten, who will report to Bosworth, will have headcount to hire around 100 engineers this year, one of the people said.Meta’s goal is to provide what Google’s Android operating system and Qualcomm Inc.’s chips did for the phone industry by building a foundation for the rest of the market.The software, sensors and computing packages that Meta is already developing for its devices are the same technologies that are needed to power humanoids, according to one of the people involved in the project.Meta has been investing billions of dollars for years into its Reality Labs hardware division, which sells products like the Quest VR headset and the increasingly popular Ray-Ban smart glasses. Meta plans to spend $65 billion this year on related products, including artificial intelligence infrastructure and the new robot work.Tesla Inc. CEO Elon Musk has said that his company’s Optimus robot will eventually be sold to consumers and could cost around $30,000. Tesla is beginning limited production this year. Other businesses also have made headway. Boston Dynamics, for instance, has already brought products to market for automation in warehouses. Some companies are focused on selling to businesses and manufacturers, while Meta’s intention is to sell into homes.Humanoids are an evolution of the work companies have been doing in autonomous vehicles. They use similar underlying technologies and require large amounts of data and AI processing. But while the safety stakes are lower — roaming a person’s home instead of traveling 50 miles per hour on an open road — Meta executives believe humanoids are more challenging because every person’s home has a different layout, while city streets are fairly standardized.Meta will build some of its own hardware, use off-the-shelf components and work with existing manufacturers as soon as it can, said the people with knowledge of the project. They added that building prototypes and hardware is key for testing ahead of deploying a platform, even if Meta itself doesn’t ultimately release a branded product. The company is pitching its work as the platform of choice for robot development, one of the people said, adding that the goal was to make Meta’s Llama software a foundation for robotics researchers around the world.Meta will also seek to develop tools for robot safety, addressing possible dangers such as a person’s hand getting caught in an actuator or another part of a humanoid robot. There are also issues related to power safety, such as how a robot powers down or stops functioning mid-task if it runs out of power.While the official push into humanoid robots is new for Meta, the company’s FAIR, or Fundamental AI Research Group, has been exploring and publishing papers on robotics work for months. Apple recently started publishing AI papers related to robotics work as well.One person with knowledge of the project said that Meta believes humanoids are still a couple of years away from being widely available — and it could be years before the company’s platform is ready to underpin third-party products. But it will become a major focus for Meta and the broader tech industry, the person said.]]></content:encoded></item><item><title>Complex dynamics require complex solutions</title><link>https://mathstodon.xyz/@tao/113873092369347147</link><author>ckemere</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 22:05:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The hardest working font in Manhattan</title><link>https://aresluna.org/the-hardest-working-font-in-manhattan/</link><author>robinhouston</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 21:45:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
		In 2007, on my first trip to New York City, I grabbed a brand-new DSLR camera and photographed all the fonts I was supposed to love. I admired American Typewriter in all of the I <3 NYC logos, watched Akzidenz Grotesk and Helvetica fighting over the subway signs, and even caught an occasional appearance of the flawlessly-named Gotham, still a year before it skyrocketed in popularity via Barack Obama’s first campaign. 
	
		But there was one font I didn’t even notice, even though it was everywhere around me.
			
		Last year in New York, I walked over 100 miles and took thousands of photos of one and one font only.
			
		The font’s name is Gorton.
			
		It’s hard to believe today that there was a time before I knew of Gorton and all its quirks and mysteries. The first time I realized the font even existed was some time in 2017, when I was researching for my book about the history of typing. 
			
		Many keyboards, especially older ones, sported a particular distinctive font on their keycaps. It was unusually square in proportions, and a weird mélange of “mechanical” and “childish.”
			 
		The more I looked at it, the more I realized how bizarre and amateurish it was. The G always felt like it was about to roll away on its side. There was a goofy wavy hook sticking out of Q. P and R were often too wide. & and @ symbols would be laughed away in a type crit, and the endings of C felt like grabbing something next to it – a beginning of a ligature that never came.
			
		The strangeness extended to the digits. There was a top-flatted 3 resembling a Cyrillic letter, 7 sloping down in a unique way, a very geometric 4, an unusual – perhaps even naïve – symmetry between 6 and 9, and a conflation of O with 0 that would be a fireable offense elsewhere.
				
		Looking at just a few keyboards, it was also obvious that it wasn’t just one rigid font. There were always variations, sometimes even on one keyboard. 0 came square, dotted, or slashed. The usually very narrow letter I sometimes sported serifs. The R and the 6 moved their middles higher or lower. There also seemed to be a narrower version of the font, deployed when a keycap needed a word and not just a letter. (Lowercase letters existed too, but not very often.) 
			
		My first thought was: What a mess. Is this how “grotesque” fonts got their name?
			
		Then, the second thought: I kind of like it.
	The most distinctive letterforms of Gorton		
		But what font was it? What The Font website posited TT Rounds, Identifont suggested it could be Divulge, my early guess was DIN Rounded or something related to road signage. Whatever it was, a flat R clearly separated it from Helvetica, and the shapes were not as round as even the un-rounded Gotham’s.
			
		A few places for keyboard nerds referred to the font as “Gorton,” but that phrase yielded zero results anywhere I typically looked for fonts I could download and install.
					
		I originally thought this had to do with how keys were made. Only in newer keyboards are the letters printed on top of the keys, or charred from their surface by a laser. In older ones – those from the early 1960s laboratory computers, or the 1980s microcomputers – the way every key was constructed was by first molding the letter from plastic of one color, and then grabbing a different plastic and molding the key around the letter. A Gorton letter was as physical as the key itself. It made the keys virtually indestructible – the legend could not wear off any more than its key – and I imagined required some specialized keyboard-making machinery that came with the “keyboard font” already there.
	
		An example of a “double-shot” key from above and from below
				
		But then, I started seeing Gorton in other places.
						
		Hours of looking at close-ups of keys made me sensitive to the peculiar shapes of some of its letters. No other font had a Q, a 9, or a C that looked like this.
						
		One day, I saw what felt like Gorton it on a ferry traversing the waters Bay Area. A few weeks later, I spotted it on a sign in a national park. Then on an intercom. On a street lighting access cover. In an elevator. At my dentist’s office. In an alley. 
							
		These had one thing in common. All of the letters were carved into the respective base material – metal, plastic, wood. The removed shapes were often filled in with a different color, but sometimes left alone.
						
		At one point someone explained to me Gorton must have been a routing font, meant to be carved out by a milling machine rather than painted on top or impressed with an inked press.
								
		Some searches quickly led me to George Gorton Machine Co., a Wisconsin-based company which produced various engraving machines. The original model 1 led to model 1A and then 3U and then, half a decade later, P1-2. They were all pantograph engravers: They allowed you to install one or more letter templates and then trace their shape by hand. A matching rotating cutter would mimic your movements, and the specially configured arms would enlarge or reduce the output to the size you wanted.
									
		This immediately explained both the metaphorical and literal rough edges of Gorton.
						
		A lot of typography has roots in calligraphy – someone holding a brush in their hand and making natural but delicate movements that result in nuanced curves filled with thoughtful interchanges between thin and thick. Most of the fonts you ever saw follow those rules; even the most “mechanical” fonts have surprising humanistic touches if you inspect them close enough.
				
		But not Gorton. Every stroke of Gorton is exactly the same thickness (typographers would call such fonts “monoline”). Every one of its endings is exactly the same rounded point. The italic is merely an oblique, slanted without any extra consideration, and while the condensed version has some changes compared to the regular width, those changes feel almost perfunctory.
		
		Monoline fonts are not respected highly, because every type designer will tell you: This is not how you design a font. 
			
		It seemed at this point that perhaps P1-2 and its predecessors were a somewhat popular machining product during the 20th century’s middle decades. But casual research through materials preserved by some of George Gorton Machine Company’s fans – including the grandson of the founder – revealed something even more interesting. Gorton the font was a lot older than I expected. 
			
		I found a 1935 catalog showing the very same font. Then one from 1925. And then, there was one all the way from 1902, showing the shapes I was starting to be mildly obsessed with.
				
		To put it in perspective: the font I first assumed was a peer to 1950s Helvetica was already of retirement age the day Helvetica was born. Gorton was older than Gill Sans, Futura, or Johnston’s London Underground font. It was contemporaneous to what today we recognize as the first modern sans serif font, Akzidenz-Grotesk, released but three years before the end of the century.
	
		Imagine how stripped down and exotic Gorton must have felt right next to George Gorton Machine’s then-current logo!
						
		I started researching Gorton more. Unfortunately, as I already suspected, no one ever wrote “I used Gorton to typeset this,” because Gorton was a tenuous name at best. It was the first font, and perhaps originally the  font that came with the engraver, so it suffered a nameless fate, familiar later to many bespoke bitmap fonts adoring the screens of early computers.
						
		The difference from these fonts, however, was that Gorton was meant to travel. And so, since searching for it by name was impossible, for months and years I just kept looking around for the now-familiar shapes.
	
		Gorton wasn’t just on computer keyboards, intercom placards, and sidewalk messages visited by many shoes. Gorton was there on typewriter keyboards, too. And on office signs and airline name tags. On boats, desk placards, rulers, and various home devices from fridges to tape dispensers.
						
		It was also asked to help in situations other fonts rarely did. I spotted Gorton on overengineered buttons that were put to heavy industrial and military use. I saw it carved into surfaces of traffic control devices, elevators and escalators, locomotives and subway trains, submarines and jet fighters. Gorton made its way to peace- and wartime nuclear facilities, it was there on the elevator at the Kennedy Space Center with labels marked EARTH and SPACE… and it went  and then the Moon, as key legends on Apollo’s onboard computer.
								
		But why? Why would anyone choose this kind of an ugly font where so many nicer fonts have already been around for ages?
						
		Some of it might be the power of the default. Popular engraving equipment comes with a built-in font that’s so standard it reuses the router’s name? Of course you will see it, the same way you saw a lot of Arial in the 1990s, or Calibri today.
	
		Gorton was also convenient. If your previous engraving work required you do to the routing equivalent of handwriting or lettering – every letter done by hand – then a modern font you could simply  and one designed with “a minimum of sharp corners for rapid tracing with a smooth stroke,” must have felt like a breath of fresh air.
								
		But why engraving to begin with? Because the affordable and casual printing options we enjoy today – the office laser printers and home inkjets, the FedEx Kinko’s, the various cheap labelers – weren’t there. Even things that today feel obsolete, like dot matrix printers, Letraset, and popular letter stencils, were yet to be invented. Often, your only realistic option was the complicated and time-consuming lettering by hand.
				
		On top of that, Gorton’s longevity must have felt attractive. Ink smudges. Paint fades away. Paper can catch fire (quickly) or germs (slowly). Carve something into plastic, on the other hand, and it can survive decades. Substitute plastic for metal, and you just turned decades into centuries. The text is not added atop heavy-duty material. The text  the material.
	Various items from the 20th century typeset in Gorton					
		I felt good about all my findings: What a strange story of a strange routing font! 
	
		But it turns out I was just getting started. Because soon, I noticed Gorton as ink on paper, and as paint on metal.
	We’re used to the flexibility of fonts today. Fonts as bits inside a computer can become a website, paint on paper, CNC routing, a wall projection, and many other things. But those freedoms weren’t as easy back when fonts were made out of metal. Life’s not as much fun outside of the glamor of a TTF file, and a routing font couldn’t immediately become a regular font – so seeing Gorton being additive and not subtractive was an unexpected discovery.
								
		It turns out that there developed a small cottage industry of things that extended Gorton past its engraving origins.
								
		A company called Keuffel & Esser Co. grabbed Gorton’s machines, and used them to create lettering sets called Leroy. This was Gorton abstracted away – still a pantograph, but cheap, small, completely manual, and a vastly simplified one: no possibility to make things bigger and smaller, and no carving – instead, you’d mount a special pen and draw letters by tracing them.
								
		Another company, Wood-Regan Instrument Co., made a similar set called (semi-eponymously) Wrico. But then, they simplified the process even more. Instead of a pantograph, they offered for sale a set of simple lettering guides used to guide your pen directly on paper.
								
		Some of the traditional draftspeople pooh-poohed these inventions – one handbook wrote “[Those are] of value chiefly to those who are not skilled in lettering. A professional show-card writer could work better and faster without it. A Leroy or Wrico lettering set permits work that is neat, rapid, and nearly foolproof, if not inspired.”
				
		But the products ended up being popular and influential. Their output appeared in many technical documents, but spread even a bit further than that. Eventually, there were stencils made by Unitech, Lutz, Tacro, Teledyne Post, Tamaya, Tech Graphic, Ridgway’s, Faber Castell, Zephyr, Charvoz, Rotring, Pickett, and probably many more.
				
		Then, both EC Comics and All-Star Comics used Leroy in the 1940s and 1950s, most notably in the first comic book that introduced Wonder Woman. This was Gorton spreading further than just technical documents, and inspiring more people.
				
		Elsewhere silkscreening – a pretty cool technique of applying paint on surfaces through a wet mesh of fabric – took Gorton and Leroy in a different direction, by allowing paint on metal.
				
		There was more. The popular plastic letters attached to felt boards, popularized by restaurants decades ago, and more recently revisited by Instagram mom influencers, also clearly derive from Gorton and Leroy.
	
		I also counted at least three different systems of “Gorton movable type” – some where you could assemble physical letters, and some where you could impress them into soft materials using steel types – and I imagine there were probably more.
				
		Letraset, a cheap technique of applying a font by rubbing a letter from a provided sheet onto paper, popular throughout the 1960s, introduced first- or second-hand Leroy too – and so did a few competitors.
						
		In the regulatory space, the U.S. military canonized Gorton in 1968 as a standard called MIL-SPEC-33558 for aircraft and other equipment dials, cancelled it in 1998… then brought it back again in 2007. NATO and STANAG followed. ANSI, American standardization body, made a more rounded Leroy an official font for technical lettering via ANSI Y14.2M, and so did institutions like the US National Park Service.
					
		Gorton went on and on and on. The early Hershey vector fonts, developed on very early computers and still popular in CAD applications today, were also derived from Gorton/Leroy shapes, simplified so that the already-simple curves weren’t even necessary – any letter could now be drawn by a series of straight lines.
						
		And even in the first universe Gorton inhabited things weren’t standing still. 
	
		As the engraving industry learned what’s popular and what is not, the offerings started getting more and more sophisticated. A promotional booklet called “The Whereabouts of 230 Engraving Machines” listed Gorton customers ranging from biscuit makers to fire engine constructors. Othercatalogsproudly listed applications like book covers, billiard balls, organ keys, and toothbrushes, as well as “tools making more tools” – using Gorton engravers to create legends for other machines.
			
		After you bought your pantograph engraver, you could buy attachments for sometimes surprising use cases:
	
		The original machine-shop pantographs were supplanted by smaller portable units (called Pantoetchers) on one side, and by increasingly complex  devices on the other. First generation of those were still huge room-size endeavors with Nixie tubes and complex interfaces labeled… in Gorton itself. 
	
		But the technology matured quickly and soon more and more early manual “tracer-guided” pantographs that forced the operator to put letters side by side and then trace them by hand, were superseded by computerized ones, with both the composition and the routing completely automated. They came from George Gorton Machine Co., and from competitors like New Hermes or H.P. Preis.
					
		You no longer had to buy the chromium-plated brass alphabets weighing up to 13 pounds, choosing the right size from 3/8´´ to 3´´ ahead of time (pantographs allowed for reductions and enlargements, but only gave you a few steps within a specific range.) 
	
		Now, fonts came as digits or formulas built into computer memory, or – for a moment in time – as separate cartridges you’d insert in eager slots. (And yes, before you ask: there were other routing monoline fonts, too. But I really don’t care about any of them.)
						
		It was the same story as in word processing right next door, where old-fashioned Gutenberg-era typesetting was being replaced by increasingly smaller and cheaper computers equipped with first-laughable-then-capable software.
						
		And automation came for the Leroy branch of the tree as well. A few companies grabbed Leroy lettering templates and abstracted them away once more. They created curious small typewriter/plotter hybrids where typing letters on a keyboard would make the pen draw them on paper for you. (I own one of them, a Max Cadliner. It might be one of the strangest typewriters I’ve seen – a weird combination of a machine pretending to be another machine pretending to be a human hand.)
					
		If this was a Gorton typewriter, there were also Gorton , even more sophisticated 1980s machines whose text could be programmed in advance rather than typed one line at a time, and mixed with graphics.
				
		I don’t think the – by now 80 years and counting – fractal explosion of Gorton made its original creators rich.
				
		Copy protection in the world of typography is complicated. The font’s name can be trademarked and other companies legally prevented from using it, and you can’t just grab matrices or font files and copy them without appropriate licenses. But take any text output using a font and then redraw it – and you are within your right to do so, and even to sell the final result. At least in America, or in some other countries until somewhat recently, the shapes of the letters themselves are not legally protected.
				
		This is why Keuffel & Esser, Wood-Regan Instrument, and Letraset could potentially grab Gorton and claim it their own, as long as they didn’t name it Gorton. 
								
		But of course, Gorton was barely named “Gorton” to begin with. In the early days of George Gorton pantographs, as the default pantograph font, it came without a name. (The font sets for purchase were called “standard copies.”) Then, as other fonts were added, it was retroactively named Gorton Normal – the name of the company and the most generic word possible.
						
		Leroy lettering sets started with one font, so similarly to Gorton the font started to be known as “Leroy,” then “Series C,” then “Gothic.” New Hermes called it simply “Block,” Letraset went with “Engineering Standard,” and Rotring – another producer of little computerized plotters – with “Universal.” I’ve also seen “A style,” “Plain Gothic,” and, mysteriously, “Standpoint.” 
								
		I don’t think this was meant to be disrespectful. “Standard,“ “Universal,” “A style” might not have had the connotations of “generic” we associate with them today, but rather meaning “the only one you need,” “approved of by millions,” or “the ultimate.”
								
		But there  one name that felt somewhat inconsiderate. It appeared in one product in the 1980s, a few decades after the birth of another font whose name became recognizable and distinguished. In that product, Gorton was referred to as “Linetica.”
	A few rare examples of Gorton Extended in use						
		Each of these reappearances made small changes to the shapes of some letters. Leroy’s ampersand was a departure from Gorton’s. Others softened the middle of the digit 3, and Wrico got rid of its distinctive shape altogether. Sometimes the tail of the Q got straightened, the other times K cleaned up. Punctuation – commas, quotes, question marks – was almost always redone. But even without hunting down the proof confirming the purchase of a Gorton’s pantograph or a Leroy template set as a starting point, the lineage of its lines was obvious. (The remixes riffed off of Gorton Condensed or the normal, squareish edition… and at times both. The extended version – not that popular to begin with – was often skipped.)
	Classic Gorton vs. Gorton Modified						
		The only “official” update to Gorton I know of, and one actually graced with a name, was Gorton Modified. It was made some time in the 1970s by one of the main keyboard keycap manufacturers, Comptec (later Signature Plastics). It was almost a fusion of Gorton and Futura, with more rounded letterforms. Gone was the quirkiness of 3, 7, Q, C, and the strange, tired ampersand. This is the version people might recognize from some of the 1980s computers, or mechanical keyboards today. 
	
		It is also that last Gorton that mattered.
	A collection of movies and TV shows featuring Gorton							
		My every walk in Chicago or San Francisco was counting down “time to Gorton” – sometimes mere minutes before I saw a placard or an intercom with the familiar font.
								
		This might be embarrassing to admit, but I have never been so happy seeing a font in the wild, particularly as there was almost always some new surprise – a numero, a line going through the Z, a new use, or a new imperfection. And, for a font that didn’t exist, I saw it surprisingly often.
										
		I even spotted Gorton a few times in Spain, or the U.K., and didn’t make too much of it, not thinking about the likelihood of machines from George Gorton’s company in a small town of Racine, Wisconsin making it all the way to different continents. In hindsight I should have.
	Gorton on old British cars, with a particularly delightful Rolls Royce logo made by a simple duplication of the classic Gorton letter R						
		It was only on a trip to Australia where something started connecting. Here, once more, I saw Gorton on the streets, put to work in all sorts of unglamorous situations:
								
		Some letterforms in the above photos felt slightly odd, and so did Gorton on the heavy machinery in an abandoned shipyard on an island near Sydney:
								
		And a visit to a naval museum cemented it all:
	
		It was Gorton, although with some consistent quirks: 2, 5, 6, and 9 were shorter, the centers of M and W didn’t stretch all the way across, and the distinctive shape of S was slightly different here.
								
		Fortunately, this time around, a type designer familiar with my now-public obsession with Gorton clued me in. Gorton didn’t actually originate from Racine, Wisconsin in the late 19th century. It started a bit earlier, and quite a bit further away, at a photographic lens maker in the U.K. called Taylor, Taylor & Hobson. 
								
		In 1894, TT&H needed some way to put markings on their lenses. This being late 19th century, their options were limited to manual engraving, which must have felt tricky given the small font sizes necessary. So the company did what makers sometimes do – instead of searching for a solution that might not have even existed, they made new types of machines to carve out letters, and then designed a font to be used with them.
	
		I don’t know how this first proto-Gorton was designed – unfortunately, Taylor, Taylor & Hobson’s history seems sparse and despite personal travels to U.K. archives, I haven’t found anything interesting – but I know simple technical writing standards existed already, and likely influenced the appearance of the newfangled routing font.
	From a 1895 “Free-hand lettering” book by Frank T. Daniels					
		This was perhaps the first modern pantograph engraver, and perhaps even the arrival of a concept of an engraving font – the first time technical writing was able to be replicated consistently via the aid of the machine.
				
		No wonder that other companies came knocking. Only a few years later, still deep within the 19th century, Taylor, Taylor & Hobson licensed their stuff to a fledgling American company named after its founder. Gorton Model 1 was the first U.S. version of the engraver, and the TT&H font must have been slightly adjusted on arrival. 
	A Taylor-Hobson pantograph in use in 1942			
		This adds to the accomplishments of Gorton – the font was actually  than even Akzidenz-Grotesk, and has been used on World War II equipment and later on on British rifles and motorcycles (and 3,775 finger posts in one of the UK’s national parks), but it complicates the story of the name even more. Turns out, the font without a name has even less of a name than I suspected.
				
		If the Taylor, Taylor & Hobson (or, Taylor-Hobson, as their engravers were known) “branch” of Gorton were more used, should it usurp the at least somewhat popular Gorton name? Or should it just because it was first and the letterform changes were small? Does it matter? Where does one font end and another begin? (Unsurprisingly, TT&H didn’t properly name the font either, eventually calling it “A style” for regular and “C style” for condensed variants. Google searches for “taylor hobson font” are a lot more sparse than those for Gorton.)  
	GortonGorton CondensedThe Gorton quasisuperfamily
		In the end, I’m sticking with Gorton for the whole branch since that feels the most well-known name, but I feel ill-equipped to make that call for everyone. You might choose to call it Gorton, Leroy, TT&H, Taylor-Hobson, or one of the many other names. (Just, ideally, not Linetica.)
	A comparison of all major editions of Gorton					
		And so, throughout the 20th century, Gorton has lived two parallel lives – one originating in the U.K. and later expanding to its colonies and the rest of Europe, and another one in America. 
								
		I am still tracing various appearences of Gorton and perhaps you, dear reader, will help me with that. (Chances are, you will see Gorton later today!) I’m curious about whether Gorton made it to Eastern Europe, Africa, or Asia. I’m interested in seeing if it appeared in Germany where the objectively better-designed DIN fonts became much more popular in Gorton’s niche.
	
		The history of this strange font spans over a century and I’ve seen it in so many countries by now, used in so many situations. But it’s impossible for me to say Gorton is the most hard-working font in the world.
								
		To this title, there are many contenders. Garamond has a head start of 300+ years and has been released in more versions than letters in any alphabet. Helvetica is so famous and used so much that even its ugly copy, Arial, became a household name. Whatever font MS Office or a popular operating system appoint to be “the default” – from Times New Roman through Calibri to Roboto – immediately enjoys the world premiere that any Hollywood movie would be envious of. There is even a 5×7 pixel font originally started by Hitachi that you can see everywhere on cheap electronic displays in cash registers and intercoms.
								
		But there is one place in the world where Gorton pulls triple duty, and I feel confident in saying at least this: Gorton is the hardest working font in Manhattan.
							
		In 2007, on my first trip to New York City, I grabbed my brand-new DSLR camera and photographed all the fonts I was supposed to love: American Typewriter, Helvetica, Gotham. But, in hindsight, I missed the most obvious one.
								
		Gorton is everywhere in Manhattan. It’s there in the elevators, in the subway, on ambulances, in various plaques outside and inside buildings. And god knows it’s there on so, so many intercoms.
						
		I wouldn’t be surprised if there weren’t a single block without any Gorton in a whole of Manhattan.
	A complete inventory of Gorton outside, near my hotel, between 5th and 7th avenues and 25th and 35th streets. I didn’t have access to the interiors of most buildings.	
		The omnipresence of Gorton makes it easy to collect all the type crimes layered on top of the font’s already dubious typographical origins. Walking through Manhattan, you can spot the abominable lowercase that should better be forgotten:
								
		You can see all sorts of kerning mistakes:
										
		You will notice the many, many routing imperfections – an unfinished stroke, a shaky hand, or services of a pantograph that never felt the loving touch of regular maintenance:
	
		There are all the strange decisions to haphazardly mix various styles of Gorton, or even to mix Gorton with other fonts:
												
		You can even spot reappearing strange characters like a weirdly deep 3, or a flattened 4:
	
    I wish I understood how they came to be, but I have a hunch. The nature of pantographic reproduction is that Gorton carved into metal is not that far away from the original Gorton font template you started with! So in addition to the George Gorton and Taylor Hobson originals, and the other named and above-the-table copies, they might have been bigger or smaller Gorton . I have one myself, carved into acrylic, of unknown provenance and even more nameless than I thought possible for an already name-free font.
  
		But New York Gorton holds pleasant surprises, too. Despite the simplicity of Gorton itself, the combinations of font sizes, cutter sizes, materials, reproductions, and applications can still yield some striking effects:
	
	
			All my Gorton walks in Manhattan in 2024
		

		This was what made me walk 100 miles. Over and over again, Gorton found ways to make itself interesting. Without hyperbole, I consider the above photos simply beautiful.
	
		In a city that never sleeps, Gorton wasn’t allowed to sleep, either. Even in the richest and most glamorous neighborhoods of Manhattan, the font would be there, doing the devil’s work without complaining. Gorton made Gotham feel bougie; American Typewriter touristy.
	
		And once in a while, I’d find Gorton that would wink at me with a story – followed by that aching in the heart as I realized I’d never know what the story was.
				
		You’re not supposed to fall in love with an ugly font. No one collects specimens of Arial. No one gets into eBay fights for artifacts set in Papyrus. No one walks a hundred miles in a hot New York summer, sweating beyond imagination, getting shouted at by security guys, to capture photos of Comic Sans.
								
		So why do I love Gorton so much? 
								
		The Occam’s Razor seems sharp on this one. Perhaps I like it because I’m a boy and Gorton is often attached to heavy machinery. 
					
		But there must be more to it. Perhaps it’s all about the strange contrasts Gorton represents. The font is so ubiquitous, but also profoundly unrecognizable, sporting no designer and no name. Gorton is a decidedly toy-like, amateurish font deployed to for some of the most challenging type jobs: nuclear reactors, power plants, spacecraft. More than most other fonts, Gorton feels it’s been made by machines for machines – but in its use, it’s also the font that allows you to see so many human mistakes and imperfections.
					
		Gorton also feels mistake-friendly. The strange limitations of Gorton mean that some of the transgressions of other fonts don’t apply here. The monoline nature of the font means that messing with the size of Gorton is okay: Shrinking the font for small caps or superscript, for example, gives you still-valid letterforms, almost by accident. 
	
		Stretching or slanting Gorton is not as much a typographical crime as it would be with other fonts because you don’t stretch the tip of the router itself.
								
		There are genuinely moments where I felt Gorton gave people freedoms to maul it decades before variable fonts allowed us similar flexibiity.
		And on top of that, the simplicity of the letterforms themselves feels compatible with the typical naïveté of Gorton’s typesetting. 
	Various accessories and attachments allowing you to shift Gorton around in a way other fonts would not allow
    Sure, there are really bad renditions that are inexcusable. 

		But most of the time, the imperfections and bad decisions are what makes Gorton come alive. They don’t feel like a profound misunderstandings of typography, typesetting, or Gorton itself. They don’t feel like abuses or aberrations. No, they feel exactly how Gorton was supposed to be used – haphazardly, without much care, to solve a problem and walk away. (Later routing fonts copied Helvetica, but seeing Helvetica in this context with all the same mistakes grates so much more.)
				
		The transgressions are not really transgressions. They all feel honest. The font and its siblings just show up to work without pretense, without ego, without even sporting a nametag. Gorton isn’t meant to be admired, celebrated, treasured. It’s meant to do some hard work and never take credit for it. Gorton feels like it was always a font, and never a typeface. (Depending on how rigid you are with your definitions, some versions of Gorton – especially those without instructions on how letters are positioned against each other – might not even classify as a font!)
						
		And I think I love Gorton because over the years I grew a little tired of the ultra flat displays rendering miniature pixels with immaculate precision. 	
		With Gorton, carving into metal or plastic means good-looking fixes are impossible:
	
		And unsurprising given its roots, Gorton has dimensionality that most fonts cannot ever enjoy: A routing tip picked in the 1980s and a sun coming in from just the right angle forty years later can create a moment that thousands of letterpress cards can only dream of.
	
		Perhaps above everything else, Gorton is all about . 
  
    Every kind of engraving has it, of course. But these are not precise submillimeter letters at the bottom of your MacBook Pro or Apple Watch. This is the utilitarian, often harried, sometimes downright  Gorton, carved into steel of a  
		mid-century intercom and filled in with special paste or wax, or put on an office placard made out of a special two-layer material made especially so engraving it reveals the second color underneath, without the need for infill. 
				
		(This is also true when it comes to the original reason I learned of Gorton. Letters on keycaps show the same artifacts – you just have to look very, very closely.)
	
		That’s the last, and perhaps the best thing to fall in love with. 
	
		You won’t be able to fully appreciate it here, of course, but maybe this will give an approximation of how beautiful Gorton’s non-beauty can be:
						
		This has been a strange thing to write. Gorton has been around for over 135 years and used in so many countries for so many reasons, and yet I found no single article about it. 
						
		I feel the burden of being an amateur historian, wanting to know and share so much more, but only being able to provide little. I don’t know the full extent of Gorton’s use. I don’t know who designed it. My chronology is rickety and pieced together from a few breadcrumbs. I dream of seeing the original drawings or drafts once laid on the tables of Taylor, Taylor & Hobson offices, or some notes, or some correspondence. I fear they might no longer exist.
						
		Also, if part of the allure of Gorton is shying away from the limelight and not being admired, am I doing it a disservice by writing about it?
						
		But mostly, I can’t shake the feeling that we all missed a window. That this essay can’t be just a celebration, but also needs to be the beginnings of a eulogy.
						
		Walking around New York, you get a sense that even Gorton carved into metal can disappear. Some of the signs are rusted or destroyed beyond repair. Others get replaced by more modern, charmless equivalents.
								
		Gorton itself is obsolete. All the keyboards that use Gorton Modified you can still buy new today are tipping a hat to nostalgia. The omnipresence of Gorton in New York City is already time shifted from its decades of glory, a simple confirmation of what Robert Moses knew so well: that once built, cities don’t change all that much. But few of the new placards use Gorton, and none of the new intercoms do. 
	
		Taylor, Taylor & Hobson went through multiple splits and mergers and survives as a subsidiary of Ametek, chiefly working on measuring devices. George Gorton Machine Co. from Racine has been bought by Kearny & Trecker, which became Cross & Trecker, was acquired by Giddings & Lewis, and then acquired  by ThyssenKrupp, but not before the Gorton branch was spun off as Lars, and in a sequence of events now resembling a telenovella, eventually bought by Famco in 1987. I do not believe any corporate grandchildren of TT&H and George Gorton’s company are today selling Gorton in any capacity.
								
		It will take decades, perhaps even centuries, but one day the last of this font will be gone. The modern recreations (I eventually found quite a few) won’t help. They are perhaps all missing a point, anyway.
	
		But there’s a somewhat silver lining. Yes, when Gorton is carved into fresh metal, there might be nothing more pretty than seeing its depths glistening in the sun.
	
		But fresh, shining metal is at this point rare. Fortunately, the Gorton I love most is the weathered Gorton.
								
		Manhattan’s tired Gorton is the best variant of Gorton: infill cracked by hot summers followed by frigid winters, the surface scratched by keys or worn out by many finger presses, the routing snafus meeting decades of wear and tear. Gorton’s no stranger to water, snow, rust, or dirt.
			
		This is, perhaps, how you become gortonpilled. You learn to recognize the 7 with a crooked hook, the Q with a swung dash, the strange top-heavy 3, the simple R. You start noticing the endings of each character being consistently circular, rather than occasionally flat. A routing mistake, suspicious kerning, or the absence of lowercase are not a wrongdoing – they’re a .
								
		You find yourself enchanted with how this simple font went so very far. And then you touch the letters, just to be sure. If you can  them, chances are this is Gorton.		
	]]></content:encoded></item><item><title>The Iconic 3DBenchy Enters the Public Domain</title><link>https://www.nti-group.com/home/information/news/3dbenchy/</link><author>kotaKat</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 21:39:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.</title><link>https://v.redd.it/sglstazd96je1</link><author>/u/eternviking</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 21:24:03 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitHub - Clivern/Peanut: 🐺 Deploy Databases and Services Easily for Development and Testing Pipelines.</title><link>https://github.com/Clivern/Peanut</link><author>/u/Clivern</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 21:07:48 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Calico apiserver FailedDiscovery Check</title><link>https://www.reddit.com/r/kubernetes/comments/1ipkg32/calico_apiserver_faileddiscovery_check/</link><author>/u/Flimsy_Tomato4847</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 20:42:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I installed the calico operator and follwing custom-resources.yaml:# This section includes base Calico installation configuration. # For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.Installation apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: ipPools: - name: default-ipv4-ippool blockSize: 26 cidr: 192.168.0.0/16 encapsulation: None natOutgoing: Enabled nodeSelector: all() --- # This section configures the Calico API server. # For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServer apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} Getting this error in kube-apiserver logs:E0214 20:38:09.439846 1 remote_available_controller.go:448] "Unhandled Error" err="v3.projectcalico.org failed with: failing or missing response from https://10.96.207.72:443/apis/projectcalico.org/v3: Get \"https://10.96.207.72:443/apis/projectcalico.org/v3\": dial tcp 10.96.207.72:443: connect: connection refused" logger="UnhandledError" E0214 20:38:09.445839 1 controller.go:146] "Unhandled Error" err=< Error updating APIService "v3.projectcalico.org" with err: failed to download v3.projectcalico.org: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.96.207.72:443: connect: connection refused calico-apiserver calico-api ClusterIP 10.96.207.72<none> 443/TCP 45mDo you know any things to solve this?   submitted by    /u/Flimsy_Tomato4847 ]]></content:encoded></item><item><title>Triple Buffering merged for GNOME 48</title><link>https://www.reddit.com/r/linux/comments/1ipk38k/triple_buffering_merged_for_gnome_48/</link><author>/u/blackcain</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 20:26:23 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ask HN: Is anybody building an alternative transformer?</title><link>https://news.ycombinator.com/item?id=43052427</link><author>taiboku256</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 20:00:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Curious if anybody out there is trying to build a new model/architecture that would succeed the transformer?I geek out on this subject in my spare time.  Curious if anybody else is doing so and if you're willing to share ideas?]]></content:encoded></item><item><title>LinkedIn is the worst social media I&apos;ve ever seen</title><link>https://news.ycombinator.com/item?id=43052409</link><author>bitreducer</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 19:58:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Besides being full of AI-generated, useless content, the platform is also riddled with dark patterns.I'm a newcomer, I’d never used LinkedIn before until I joined three months ago. Since then, I've been banned twice and shadow-banned several times.1) Right after I created my account, I got shadow-banned. My friends couldn’t find me, and my profile wasn’t accessible via direct link. This issue wasn’t resolved until I contacted support; they confirmed that restrictions had been placed on my account and then removed them.2) I shouldn’t even have to mention this, but as a professional in my field, after that incident I always ended up on the very last pages of search results for keywords related to my field. People without those keywords in their resumes, or whose work isn’t at all related to the field, kept being ranked ahead of me. I can’t say I was deliberately pessimized, but that’s exactly how it appears.3) Some time later, I decided to get Premium and paid with my own card (issued in another country, with the same name as on my profile and my passport). That’s when I received the first restriction: I was locked out of my account and couldn’t sign in until I contacted support. They made me take a photo of myself and of my passport, and after that they lifted the restrictions and apologized.4) Everything seemed to be going along normally, yet I remained stuck on the last pages of search, being outranked by completely irrelevant profiles, until I decided to write a post about my own article on Medium. Immediately after publishing the post with a link to Medium, I received a second restriction. I was locked out of my account, couldn’t access my messages or interview invitations, and even missed a call because of this. They once again required me to submit a photo of myself along with a copy of my passport. Although they eventually restored my account, this time the process took 4–5 days, which caused me to miss an interview.5) Now my profile is accessible via direct link, but I’ve disappeared from search. No one can find me, and hardly anyone visits my page. My post was hidden while support was "sorting out" my account, and I lost all the potential post views it would have gained because it wasn't featured in the recommendations.All of this happened within the first three months of using LinkedIn. I’ve never seen a more appalling social media, one so full of dark patterns and outright abuse towards its users, forcing them through humiliating identity verification processes and hiding them from search.Needless to say, I have never violated any of the platform’s rules. I don’t spam, I don’t bother people, and I don’t advertise anything. Meanwhile, my friend, whose account is over five years old—can do whatever he wants. He uses VPNs, changes his profile location several times a week, and switches his VPN location from Dubai to Europe multiple times a day without ever facing any restrictions.And yes, I completely forgot to add: when your account gets restricted, it doesn’t matter whether you’re a premium user or not — the treatment is equally poor and the response is equally slow.I’ve never seen a worse social media, and I’d be thrilled if a worthy competitor to LinkedIn ever emerged, I’d be one of the first to join.BONUS: This content was originally posted on reddit, quickly became popular in the LinkedIn subreddit and was then deleted by moderator who works for M$ (which owns LinkedIn). Frankly, it feels like the whole LinkedIn subreddit is highly censored by him.]]></content:encoded></item><item><title>Shutdown Go server</title><link>https://www.reddit.com/r/golang/comments/1ipj5zn/shutdown_go_server/</link><author>/u/Kennedy-Vanilla</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 19:46:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, recently I saw that many people shutdown their servers like this or similarserverCtx, serverStopCtx serverCtx, serverStopCtx := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { <-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { <-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal("graceful shutdown timed out.. forcing exit.") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf("error shutting down server: %v", err) } serverStopCtx() }() log.Printf("Server starting on port %s...\n", port) err = server.ListenAndServe() if err != nil && err != http.ErrServerClosed { log.Printf("error starting server: %v", err) os.Exit(1) } <-serverCtx.Done() log.Println("Server stopped") } := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { <-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { <-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal("graceful shutdown timed out.. forcing exit.") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf("error shutting down server: %v", err) } serverStopCtx() }() log.Printf("Server starting on port %s...\n", port) err = server.ListenAndServe() if err != nil && err != http.ErrServerClosed { log.Printf("error starting server: %v", err) os.Exit(1) } <-serverCtx.Done() log.Println("Server stopped") Is it necessary? Like it's so many code for the simple operation   submitted by    /u/Kennedy-Vanilla ]]></content:encoded></item><item><title>the ref keyword</title><link>https://www.reddit.com/r/rust/comments/1ipixny/the_ref_keyword/</link><author>/u/Tickstart</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 19:36:57 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've made a quick mock situation which is analogous to my situation the other day:fn main() { let mut v: Option<Vec<usize>> = None; let mut h = 20; while h.ne(&0) { if (h % 3).ge(&1) { match v { Some(ref mut v) => (*v).push(h), None => v = Some(vec![h]) } } h -= 1 } println!("{v:?}") } I was a bit confused on how it "should" be solved. My issue is the "ref mut". It made sense to me that I didn't want to consume the vector v, just add to it if it existed and I tried adding ref (then mut), which worked. When I goodled, it seemed ref was a legacy thing and not needed anymore. My question is, how is the idiomatic way to write this? Perhaps it's possible to do in a much simpler way and I just found a way to complicate it for no reason.Also, don't worry I know this is a terrible pattern, it was mostly for tesing something.]]></content:encoded></item><item><title>Researcher Captures Contents of ‘DEI.gov’ Before It Was Hidden Behind a Password</title><link>https://www.404media.co/dei-waste-gov-doge-list-behind-password/</link><author>Samantha Cole</author><category>dev</category><enclosure url="https://www.404media.co/content/images/2025/02/Screenshot-2025-02-14-at-2.25.54-PM.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 19:32:43 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[A German researcher captured the contents of the White House’s “DEI.gov” during a brief period when it was not password protected.The capture shows that the site contains a list of vague, alleged government-funded tasks and their costs, without sources or context, like “$1.3 million to Arab and Jewish photographers," “$1.5 million for ‘art for inclusion of people with disabilities,’” and "$3.4 million for Malaysian drug-fueled gay sex app.” DEI.gov redirects to waste.gov and is currently inaccessible without a password; Elon Musk told reporters on Tuesday that his Department of Government Efficiency (DOGE) is “trying to be as transparent as possible.”⁨The researcher is Henrik Schönemann⁩, a historian who started the Safeguarding Research & Culture archivalist project, posted screenshots on Mastodon showing the contents. Schönemann⁩ also shared the specific site scrapes that he was able to capture, which showed the contents of the site. He told 404 Media he set up a change detection app using PikaPods, and is monitoring changes across hundreds of government websites. When the dei.gov and waste.gov sites were registered 10 days ago, he started tracking them, too. Before the site administrators added a Wordpress template to the pages, the list was online at those URLs. This list was only online for a maximum of 30 minutes, starting around 4:50 p.m. EST; by 5:23 p.m. on February 11, it was gone from public view, according to the snapshots Schönemann’s app⁩ captured. According to the screenshots provided by Schönemann⁩, the list includes (all of the following are direct quotes): $78,000 to Palestinian activist group whose chairman was photographed attending an anniversary event celebrating the founding of the Popular Front for the Liberation of the Palestine terrorist group$1 Million for foreign DEI programs, including ‘indigenous language technology’ in Guatemala, per non-public funding docs reviewed by WFB$5 million for effort to treat eating disorders by “affirming” LGBTQIA+ patients’ sexual orientation and gender claimsUp to $3 million to defund the police advocacy group to pursue “climate justice” for convictsFunded performances of play “Angels in America: A Gay Fantasia on National Themes,” in which God is bisexual and communists are good, in North MacedoniaDisbursed $15,000 to “queer” Muslim writers in IndiaShelled out tens of thousands to create army of 2,500 LGBTQI+ alliesUp to $10 million worth of USAID-funded meals went to al Qaeda-linked terrorist group the Nusra Front$500,000 to group that “empowers women” in attempt to solve sectarian violence in Israel just ten days before Hamas’ Oct. 7 attacks$4.67 million to EcoHealth Alliance – one of the key NGOs funding bat virus research at Wuhan Institute of Virology — in late 2021. Later refused to answer key questions about the funding.$7.9 million to a project that would teach Sri Lankan journalists to avoid “binary-gendered language”$1.3 million to Arab and Jewish photographers$1.5 million for “art for inclusion of people with disabilities”$2 million to promote “LGBT equality through entrepreneurship…in developing Latin American countries.”Education Week: “Biden Administration Cites 1619 Project as Inspiration in History Grant Proposal”VA took at least a dozen actions aimed at bolstering DEI during the Biden-Harris administration while the number of homeless veterans increased and the amount of claims in the VA’s backlog grew from ~211,000 to ~378,000NASA has allocated roughly $10 million to grants advancing DEI and “environmental justice” since 2020Following President Trump’s executive order on DEI at federal agencies, the ATF “quietly changing the job title of its former diversity officer… to ‘senior executive’ with the ATF.The Department of Labor requested additional funding in 2023 for “The Chief Evaluation Office for a new rigorous interagency evaluation of actions aimed at improving Diversity, Equity, Inclusivity, and Accessibility across the federal workforce,” more than $6.5 million “to restore employee benefits programs that will advance equity by specifically addressing how opportunities can be expanded for underserved communities and vulnerable populations,” and $5 million “to evaluate actions aimed at improving diversity, equity, inclusion, and accessibility (DEIA) within the federal workforce.”Fox Business: “FOX Business’ ‘Trouble in the Skies,’ a six month investigation of the FAA’s new hiring practices, uncovered changes that may put the nation’s flying public at risk as well as allegations that the newest air traffic control recruits had access to answers on a key test that helped them gain jobs with the FAA…Also uncovered was an FAA effort to promote diversity that discarded 3000 qualified college graduates with degrees in air traffic control despite their following FAA procedure and obtaining FAA accredited degrees.”Schönemann⁩ told 404 Media he wanted to share a sentiment alongside his find: “People all around the world care, you are not alone. And: #TransRights.” Earlier this week, we reported that the Trump administration had set up a website called waste.gov, which was live on the internet with a sample page from a default WordPress template. Both DEI.gov and waste.gov were created at the same time, according to Reuters, and DEI.gov was recently set up to redirect to waste.gov. After our reporting, both websites were put behind a password wall.]]></content:encoded></item><item><title>Sunsetting Create React App</title><link>https://react.dev/blog/2025/02/14/sunsetting-create-react-app</link><author>acemarke</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 19:16:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Today, we’re deprecating Create React App for new apps, and encouraging existing apps to migrate to a framework. We’re also providing docs for when a framework isn’t a good fit for your project, or you prefer to start by building a framework.When we released Create React App in 2016, there was no clear way to build a new React app.To create a React app, you had to install a bunch of tools and wire them up together yourself to support basic features like JSX, linting, and hot reloading. This was very tricky to do correctly, so the communitycreatedboilerplates for commonsetups. However, boilerplates were difficult to update and fragmentation made it difficult for React to release new features.Create React App solved these problems by combining several tools into a single recommended configuration. This allowed apps a simple way to upgrade to new tooling features, and allowed the React team to deploy non-trivial tooling changes (Fast Refresh support, React Hooks lint rules) to the broadest possible audience.This model became so popular that there’s an entire category of tools working this way today.Deprecating Create React App Although Create React App makes it easy to get started, there are several limitations that make it difficult to build high performant production apps. In principle, we could solve these problems by essentially evolving it into a framework.However, since Create React App currently has no active maintainers, and there are many existing frameworks that solve these problems already, we’ve decided to deprecate Create React App.Starting today, if you install a new app, you will see a deprecation warning:create-react-app is deprecated.

You can find a list of up-to-date React frameworks on react.dev
For more info see: react.dev/link/cra

This error message will only be shown once per install.We recommend creating new React apps with a framework. All the frameworks we recommend support client-only SPAs, and can be deployed to a CDN or static hosting service without a server.For existing apps, these guides will help you migrate to a client-only SPA:Create React App will continue working in maintenance mode, and we’ve published a new version of Create React App to work with React 19.If your app has unusual constraints, or you prefer to solve these problems by building your own framework, or you just want to learn how react works from scratch, you can roll your own custom setup with React using Vite or Parcel.We provide several Vite-based recommendations.React Router v7 is a Vite based framework which allows you to use Vite’s fast development server and build tooling with a framework that provides routing and data fetching. Just like the other frameworks we recommend, you can build a SPA with React Router v7.Just like Svelte has Sveltekit, Vue has Nuxt, and Solid has SolidStart, React recommends using a framework that integrates with build tools like Vite for new projects.Limitations of Create React App Create React App and build tools like it make it easy to get started building a React app. After running npx create-react-app my-app, you get a fully configured React app with a development server, linting, and a production build.For example, if you’re building an internal admin tool, you can start with a landing page:Welcome to the Admin Tool!This allows you to immediately start coding in React with features like JSX, default linting rules, and a bundler to run in both development and production. However, this setup is missing the tools you need to build a real production app.Most production apps need solutions to problems like routing, data fetching, and code splitting.Create React App does not include a specific routing solution. If you’re just getting started, one option is to use  to switch between routes. But doing this means that you can’t share links to your app - every link would go to the same page - and structuring your app becomes difficult over time: =  ===  &&  ===  && This is why most apps that use Create React App solve add routing with a routing library like React Router or Tanstack Router. With a routing library, you can add additional routes to the app, which provides opinions on the structure of your app, and allows you to start sharing links to routes. For example, with React Router you can define routes: = =With this change, you can share a link to  and the app will navigate to the dashboard page . Once you have a routing library, you can add additional features like nested routes, route guards, and route transitions, which are difficult to implement without a routing library.There’s a tradeoff being made here: the routing library adds complexity to the app, but it also adds features that are difficult to implement without it.Another common problem in Create React App is data fetching. Create React App does not include a specific data fetching solution. If you’re just getting started, a common option is to use  in an effect to load data.But doing this means that the data is fetched after the component renders, which can cause network waterfalls. Network waterfalls are caused by fetching data when your app renders instead of in parallel while the code is downloading: =       ..      ..=..Fetching in an effect means the user has to wait longer to see the content, even though the data could have been fetched earlier. To solve this, you can use a data fetching library like React Query, SWR, Apollo, or Relay which provide options to prefetch data so the request is started before the component renders.These libraries work best when integrated with your routing “loader” pattern to specify data dependencies at the route level, which allows the router to optimize your data fetches: =  = ..=..On initial load, the router can fetch the data immediately before the route is rendered. As the user navigates around the app, the router is able to fetch both the data and the route at the same time, parallelizing the fetches. This reduces the time it takes to see the content on the screen, and can improve the user experience.However, this requires correctly configuring the loaders in your app and trades off complexity for performance.Another common problem in Create React App is code splitting. Create React App does not include a specific code splitting solution. If you’re just getting started, you might not consider code splitting at all.This means your app is shipped as a single bundle:But for ideal performance, you should “split” your code into separate bundles so the user only needs to download what they need. This decreases the time the user needs to wait to load your app, by only downloading the code they need to see the page they are on.One way to do code-splitting is with . However, this means that the code is not fetched until the component renders, which can cause network waterfalls. A more optimal solution is to use a router feature that fetches the code in parallel while the code is downloading. For example, React Router provides a  option to specify that a route should be code split and optimize when it is loaded: = Optimized code-splitting is tricky to get right, and it’s easy to make mistakes that can cause the user to download more code than they need. It works best when integrated with your router and data loading solutions to maximize caching, parallelize fetches, and support “import on interaction” patterns.These are just a few examples of the limitations of Create React App.Once you’ve integrated routing, data-fetching, and code splitting, you now also need to consider pending states, navigation interruptions, error messages to the user, and revalidation of the data. There are entire categories of problems that users need to solve like:Solving each of these problems individually in Create React App can be difficult as each problem is interconnected with the others and can require deep expertise in problem areas users may not be familiar with. In order to solve these problems, users end up building their own bespoke solutions on top of Create React App, which was the problem Create React App originally tried to solve.Why we Recommend Frameworks Although you could solve all these pieces yourself in a build tool like Create React App, Vite, or Parcel, it is hard to do well. Just like when Create React App itself integrated several build tools together, you need a tool to integrate all of these features together to provide the best experience to users.This category of tools that integrates build tools, rendering, routing, data fetching, and code splitting are known as “frameworks” — or if you prefer to call React itself a framework, you might call them “metaframeworks”.Frameworks impose some opinions about structuring your app in order to provide a much better user experience, in the same way build tools impose some opinions to make tooling easier. This is why we started recommending frameworks like Next.js, React Router, and Expo for new projects.Frameworks provide the same getting started experience as Create React App, but also provide solutions to problems users need to solve anyway in real production apps.Server Rendering is not just for SEO A common misunderstanding is that server rendering is only for SEO.While server rendering can improve SEO, it also improves performance by reducing the amount of JavaScript the user needs to download and parse before they can see the content on the screen.This is why the Chrome team has encouraged developers to consider static or server-side render over a full client-side approach to achieve the best possible performance.]]></content:encoded></item><item><title>Kevin Mitnik FOIA Final</title><link>https://vault.fbi.gov/kevin-mitnick/kevin-mitnick-part-01-final/view</link><author>thembones</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 19:02:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The NBA Apple Vision Pro app now has a 3D tabletop view</title><link>https://www.uploadvr.com/nba-apple-vision-pro-app-tabletop-view/</link><author>matheussampaio</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 18:58:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The official NBA Apple Vision Pro app now has Tabletop, a diorama-scale virtual representation of the live game you're watching. Tabletop shows in addition to the 2D livestream view you see floating in front of you as a large virtual screen, giving you the benefits of both. Note that the 2D livestream view is not visible in the screen recording below, due to the DRM protection system it uses.There's a roughly half-second delay between the livestream and the Tabletop representation, according to those who've tried it, likely due to the processing time of the motion capture technology at the stadiums used to power this feature. In theory, the NBA could artificially delay the livestream view to match it, but it isn't currently doing so.Tabletop is currently available for a few games per night, and the NBA plans to make it available for all League Pass games next season.As with the other functionality of the NBA app, this requires a League Pass subscription, which starts at $15/month.The NBA's Tabletop feature is reminiscent of Lapz, the now-defunct Apple Vision Pro F1 app, which showed a diorama-scale 3D representation of the track floating in front of you, as well as the first-person views from multiple drivers in floating windows.Both show the significant potential of XR headsets, and eventually AR glasses, for enhancing the way sports are watched remotely.On Meta Quest headsets, in comparison, 52 NBA games are streamed in immersive 180-degrees (though 2D, not 3D) for free in the US, via the app Xtadium. NBA League Pass subscribers can also use Xtadium to watch any games on floating 2D virtual TV screens in mixed reality using Xtadium's Multi-Game Mode. This mode supports up to nine games simultaneously, and you can also apparently see stats and alerts. But the Vision Pro app's Tabletop feature offers a unique view not yet seen on other XR platforms.Would you prefer to watch your sport of choice in 180-degree immersive video, or as a 2D flatscreen view with a tabletop 3D representation? Let us know in the discussion section below.]]></content:encoded></item><item><title>Help me to understand how one feature works in this project</title><link>https://www.reddit.com/r/golang/comments/1iphseo/help_me_to_understand_how_one_feature_works_in/</link><author>/u/n0zz</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 18:48:39 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[There is a `metrics_denylist` option. But I'm not sure how it works. Does it filter list of metrics to fetch before it does fetch them from Cloudflare API? Or does it work after all metrics are already fetched?]]></content:encoded></item><item><title>Linus Torvalds’ Critique of C++: A Comprehensive Review</title><link>https://programmers.fyi/linus-torvalds-critique-of-c-a-comprehensive-review</link><author>/u/derjanni</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 18:47:20 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Linus Torvalds, the inventor  of Linux, has always been quite harsh about C++ and why he rejects it for Linux kernel development. He’s not just been very vocal about it, but also brought up some arguments against the use of C++ that are worth reviewing in detail.C and C++ are almost the same, except they aren’t. C++ is often considered the object oriented “version” of C. The successor to it, if you will. But C++ is rather an extension to C. Bringing object orientation, constructors, destructors, templates, exception handling, namespaces and operator overloading. All these extensions come with their very own challenges and paradigms. Unsurprisingly, Linus’ technical arguments against C++ all have their roots in these extensions.The main arguments that Linus brought forward against C++ can be found in the kernel mailing list under “Compiling C++ kernel module + Makefile” and the respective responses that date back as far as 2004. Let’s wipe the personal disputes off these messages and focus on the arguments.Exception handling in C++“the whole C++ exception handling thing is fundamentally broken. It’s _especially_ broken for kernels.”]]></content:encoded></item><item><title>Delphi Is 30</title><link>https://blog.marcocantu.com/blog/2025-february-delphi-is-30.html</link><author>andsoitis</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 18:41:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
Delphi is 30 today! I was at the Moscone Center in San Francisco 30 years ago at the product launch and have been onboard since than. I was looking at what to write, but nothing can compete with the fantastic blog post David I did today:Over the years, I've done several blog posts with content related to the Delphi launch on February 14th and a lot of related information. The first was this 10th anniversary page: This is a list of blog posts:Here is an old YouTube video:]]></content:encoded></item><item><title>A study on how turtles navigate using the Earth’s magnetic field</title><link>https://www.unc.edu/posts/2025/02/12/dancing-turtles-unlock-scientific-discovery/</link><author>gmays</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 18:29:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[When captive loggerhead sea turtles anticipate food, they do a little dance. The turtles raise their heads out of water, open their mouths, flap their flippers and spin in circles.Not only is this “turtle dancing behavior” cute, it was also the key to unlocking a significant scientific discovery by researchers at UNC-Chapel Hill.The prestigious international science journal Nature published the study Feb. 12. Kayla Goforth, a recent doctoral graduate in the College of Arts and Sciences’ biology department, is the lead author of “Learned magnetic map cues and two mechanisms of magnetoreception in turtles.”Early on in her graduate studies at Carolina, Goforth was fascinated by sea turtles returning to the same feeding sites again and again throughout their lifetimes — despite traveling up to 10,000 miles across the planet. She hypothesized that these turtles used Earth’s magnetic field to memorize specific geographic areas that they associated with food, and she devised an experiment in the biology department’s Lohmann Lab — run by husband-and-wife duo Kenneth and Catherine Lohmann.“Kayla began to wonder if we could get the turtles to associate the magnetic signature of a geographic area with food — and therefore act out this turtle dance behavior,” said Kenneth Lohmann, the Charles P. Postelle Jr. Distinguished Professor of Biology. “She really took the lead in this. I wasn’t at all sure in the beginning whether it would work, but we were happy to have her try, and it turned out remarkably well.”The team of Carolina researchers analyzed how sea turtles use Earth’s magnetic field to guide their long-distance migrations across the ocean, and how the turtles sense magnetic fields.Goforth and the rest of the all-Carolina team conditioned loggerhead turtles to magnetic fields replicating those that exist in various oceanic locations, repeatedly feeding the turtles in some magnetic fields while not feeding them in others. When later exposed to the fields in which they were fed, the turtles exhibited “turtle dancing behavior,” indicating that they associated that specific magnetic signature with food.“That’s how we figured out what to measure, just by watching them do this as we were feeding them in the lab,” said Goforth, who is now conducting postdoctoral research at Texas A&M University. “When they get really excited, they stick their heads out; they slap their flippers wildly; they spin in place sometimes.”The assay suggests that loggerhead sea turtles can learn the magnetic signatures of geographical areas — much like dropping a pin in a GPS app.The groundbreaking study was published in the prestigious international science journal Nature. (Ken Lohmann/biology department)When turtles use magnetic signatures, they are using their magnetic map sense, but they also have a magnetic compass sense that enables them to move north, south or some other direction. The research further showed that the map and compass senses detect magnetic fields in different ways.Working in collaboration with the College’s physics and astronomy department, the team investigated the effects of radiofrequency oscillating magnetic fields on the turtles’ magnetic senses. What the team found surprised them. The oscillating magnetic fields had no impact on the turtles’ magnetic map ability but did disrupt the turtles’ ability to use the magnetic compass and orient themselves.“It suggested there are two different mechanisms for the magnetic map and compass,” Goforth said. “And they might have evolved separately.”Exactly how turtles and other migratory species use these senses is a question that has baffled scientists for more than 50 years, and it’s one that Goforth continues to research at Texas A&M, now studying monarch butterflies instead of sea turtles.“The big question is always the mechanism,” Goforth said. “What’s the mechanism and what is the sensor? We know that for the visual sense, you have eyes; for the sense of smell, you have a nose; and for hearing, you have ears. But no receptor like that has been identified for the magnetic sense, and the mechanism remains unknown.”(Ken Lohmann/biology department)]]></content:encoded></item><item><title>Macro-Less, Highly Integrated OpenAPI Document Generation in Rust with Ohkami</title><link>https://medium.com/@kanarus786/macro-less-highly-integrated-openapi-document-generation-in-rust-with-ohkami-912de388adc1</link><author>/u/kanarus</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 18:26:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[In Rust web dev, utoipa is the most popular crate for generating OpenAPI document from server code. While it’s a great tool, it can be frustrating due to excessive macro use.A new web framework Ohkami offers a ,  way to generate OpenAPI document with its “openapi” feature.Let’s take following code as an example. It’s the same sample from the “openapi” section of the README, but with openapi-related parts removed:While this compiles and works as a pseudo user management server, activating “openapi” feature causes a compile error, telling that User and CreateUser don’t implement ohkami::openapi::Schema.As indicated by this, Ohkami with “openapi” feature effectively handles type information and intelligently collects its endpoints’ metadata. It allows code like:to assemble metadata into an OpenAPI document and output it to a file .Then, how we implement Schema? Actually we can easily impl Schema by hand, or just #[derive(Schema)] is available! In this case, derive is enough:That’s it! Just adding these derives allows Ohkami::generate to output following file:Additionally, it’s easy to define the User schema as a component instead of duplicating inline schemas. In derive, just add #[openapi(component)] helper attribute:And  #[operation] attribute is available to set summary, description, and override operationId and each response’s description:Let’s take a look at how this document generation works!First, the #[derive(Schema)]s are expanded as following:The DSL enables to easily impl manually.Schema trait links the struct to an item of type called “SchemaRef”.2. openapi_* hooks of FromParam, FromRequest, IntoResponseFromParam, FromRequest and IntoResponse are Ohkami’s core traits appeared in the handler bound:When “openapi” feature is activated, they additionally have following methods:Ohkami leverages these methods in IntoHandler to generate consistent openapi::Operation, reflecting the actual handler signature like this.Moreover, Ohkami properly propagates schema information in common cases like this, allowing users to focus only on the types and schemas of their app.3. routes metadata of RouterIn Ohkami, what’s called router::base::Router has “routes” property that stores all the routes belonging to an Ohkami instance. This is returned alongside router::final::Router from “finalize” step, and is used to assemble metadata of all endpoints.What Ohkami::generate itself does is just to serialize an item of type openapi::document::Document and write it to a file.The openapi::document::Document item is created by “gen_openapi_doc” of router::final::Router, summarized as follows:That’s how Ohkami generates OpenAPI document!There is, however, a problem in , Cloudflare Workers: Ohkami is loaded to Miniflare or Cloudflare Workers as WASM, so it can only generate OpenAPI document andcannot write it to the user’s local file system.To work around this, Ohkami provides a CLI tool scripts/workers_openapi.js. This is, for example, used in package.json of Cloudflare Workers + OpenAPI template:generates OpenAPI document!]]></content:encoded></item><item><title>Trump Admin Adds Note Rejecting ‘Gender Ideology’ on Sites Court Ordered Them to Restore</title><link>https://www.404media.co/trump-admin-adds-note-rejecting-gender-ideology-on-sites-court-ordered-them-to-restore/</link><author>Emanuel Maiberg</author><category>dev</category><enclosure url="https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-10.31.48@2x.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 18:25:45 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[After being forced by a court order to  about gender and diversity to government websites, the Trump administration has added a note to the top of those pages saying “Any information on this page promoting gender ideology is extremely inaccurate, and disconnected from the immutable biological reality that there are two sexes, male and female.”Per a court order, HHS is required to restore this website as of February 14, 2025 at 11:59 p.m. Any information on this page promoting gender ideology is extremely inaccurate, and disconnected from the immutable biological reality that there are two sexes, male and female. The Trump Administration rejects gender ideology and condemns the harms it causes to children, by promoting their chemical and surgical mutilation, and to women, by depriving them of their dignity, safety, well-being, and opportunities. This page does not reflect biological reality and therefore the Administration and this Department rejects it.The note essentially seems like a way for the current administration to legally comply with a court order while still signaling that it entirely rejects any government funded or endorsed research or policy sympathetic to LGBTQ+ community and diversity, equity, and inclusion, which Trump and Elon Musk’s Department of Government Efficiency have been purging from government websites. Earlier this week,  that a federal judge ordered the Department of Health and Human Services, Centers for Disease Control, and Food and Drug Administration to restore several webpages they removed as a result of Trump’s executive order attacking diversity, equity, and inclusion. The agencies were given until 11:59 p.m. on February 11 to restore the webpages. The court ordered the administration to restore the webpages “to their versions as of January 30, 2025, meaning they were supposed to revert the webpages to what they looked like on January 30 with no changes. The versions that have been restored now have this additional disclaimer.A joint status update filed Thursday by lawyers for the Department of Justice and the Public Citizen Litigation Group says that the government has provided the court with a list of websites that it has restored, though the list of websites is not available. It also specifically says that the government is refusing to restore the website reproductiverights.gov: “Defendants have objected to restoring the website ‘reproductiverights.gov.’ Plaintiff’s counsel is conferring with their client,” it says.“Plaintiff’s lists include websites from Department of Health and Human Services (HHS) components other than the Centers for Disease Control and Prevention and the Food and Drug Administration. The parties disagree about whether such websites properly fall within the scope of the Order. However, given Plaintiff’s forthcoming amended complaint and to avoid further emergency motions practice, Defendants will restore those websites consistent with the Order,” it adds.]]></content:encoded></item><item><title>OpenAI: The Age of AI Is Here!</title><link>https://www.youtube.com/watch?v=97kQRYwL3P0</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/97kQRYwL3P0?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 18:18:07 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Dev - Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

📝 The paper "Competitive Programming with Large Reasoning Models" is available here:
https://arxiv.org/abs/2502.06807

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu]]></content:encoded></item><item><title>Dynamic triple/double buffering merge request for GNOME was just merged!</title><link>https://gitlab.gnome.org/GNOME/mutter/-/merge_requests/1441</link><author>/u/joojmachine</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 18:09:19 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[P] GNNs for time series anomaly detection</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/</link><author>/u/Important-Gear-325</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 17:56:59 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[For the past few months, my partner and I have been working on a project exploring the use of Graph Neural Networks (GNNs) for Time Series Anomaly Detection (TSAD). As we are near the completion of our work, I’d love to get feedback from this amazing community!Any comments, suggestions, or discussions are more than welcome! If you find the repo interesting, dropping a ⭐ would mean a lot. : )We're also planning to publish a detailed report with our findings and insights in the coming months, so stay tuned!The repo is still under development so don't be too harsh :)Looking forward to hearing your thoughts!]]></content:encoded></item><item><title>For those managing or working with multiple clusters, do you use a combined kubeconfig file or separate by cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1ipg99n/for_those_managing_or_working_with_multiple/</link><author>/u/trouphaz</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 17:44:13 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I wonder if I'm in the minority. I have been keeping my kubeconfigs separate per cluster for years while I know others that combine everything to a single file. I started doing this because I didn't fully grasp yaml when I started and when I had an issue with the kubeconfig, I didn't have any idea on how to repair it. So I'd have to fully recreate it.So, each cluster has its own kubeconfig file named for the cluster's name and I have a function that'll set my KUBECONFIG variable to the file using the cluster name.sc() { CLUSTER_NAME="${1}" export KUBECONFIG="~/.kube/${CLUSTER_NAME}" } ]]></content:encoded></item><item><title>Incremental Archival from Postgres to Parquet for Analytics</title><link>https://www.crunchydata.com/blog/incremental-archival-from-postgres-to-parquet-for-analytics</link><author>/u/gtobbe</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 17:42:23 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[PostgreSQL is commonly used to store event data coming from various kinds of devices. The data often arrives as individual events or small batches, which requires an operational database to capture. Features like time partitioning help optimize the storage layout for time range filtering and efficient deletion of old data.The PostgreSQL feature set gives you a lot of flexibility for handling a variety of IoT scenarios, but there are certain scenarios for it is less suitable, namely:Long-term archival of historical dataFast, interactive analytics on the source dataIdeally, data would get automatically archived in cheap storage, in a format optimized for large analytical queries.We developed two open source Postgres extensions that help you do that:pg_parquet can export (and import) query results to the Parquet file format in object storage using regular COPY commandspg_incremental can run a command for a never-ending series of time intervals or files, built on top of pg_cronWith some simple commands, you can set up a reliable, fully automated pipeline to export time ranges to the columnar Parquet format in S3.Then, you can use a variety of analytics tools to query or import the data. My favorite is of course Crunchy Data Warehouse.On any PostgreSQL server that has the pg_parquet and pg_incremental extensions, you can set up a pipeline that periodically exports data to in S3 in two steps.The pg_incremental extension has a create_time_interval_pipeline function that will run a given command once the time interval has passed, with 2 timestamp parameters set to the start and end of the hour. We cannot directly use query parameters in a COPY command, but we can define a simple PL/pgSQL function that generates and executes a custom COPY command using the parameters.-- existing raw data table
create table events (
  event_id bigint not null generated by default as identity,
  event_time timestamptz not null default now(),
  device_id bigint not null,
  sensor_1 double precision
);

insert into events (device_id, sensor_1)
values (297, 20.4);


insert into events (device_id, sensor_1)
values (297, 20.4);

-- define an export function that wraps a COPY command
create or replace function export_events(start_time timestamptz, end_time timestamptz)
returns void language plpgsql as $function$
begin
  execute format(
    $$
      copy (select * from events where event_time >= %L and event_time < %L)
      to 's3://mybucket/events/%s.parquet' with (format 'parquet');
    $$,
    start_time, end_time, to_char(start_time, 'YYYY-MM-DD-HH')
  );
end;
$function$;

-- export events hourly from the start of the year, and keep exporting in the future
select incremental.create_time_interval_pipeline('event-export',
  time_interval := '1 hour',                      /* export data by the hour                */
  batched := false,                               /* process 1 hour at a time               */
  start_time := '2025-01-01',                     /* backfill from the start of the year    */
  source_table_name := 'events',                  /* wait for writes on events to finish    */
  command := $$ select export_events($1, $2) $$   /* run export_events with start/end times */
);

By running these commands, Postgres will export all the data from the start of the year into hourly Parquet files in S3, and will keep doing so after every hour and automatically retry on failure.To use pg_parquet Crunchy Bridge, you can add your S3 credentials for pg_parquet to your Postgres server via the dashboard under Settings -> Data lake.Once data is in Parquet, you can use a variety of tools and approaches to query the data. If you want to keep using Postgres, you can use Crunchy Data Warehouse which has two different ways of working with Parquet data.The simplest way to start querying Parquet files in S3 in Crunchy Data Warehouse is to use a lake analytics table. You can easily create a table for all Parquet files that match a wildcard pattern:create foreign table events_parquet ()
server crunchy_lake_analytics
options (path 's3://mybucket/events/*.parquet');
You can then immediately query the data and the files get cached in the background, so queries will quickly get faster.A downside of querying Parquet directly is that eventually we will have a lot of hourly files that match the pattern, and there will be some overhead from listing them for each query (listing is not cached). We also cannot easily change the schema later.A more flexible approach is to import the Parquet files into an Iceberg table. Iceberg tables are also backed by Parquet in S3, but the files are compacted and optimized, and the table supports transactions and schema changes.You can create an Iceberg table that has the same schema as a set of Parquet files using the definition_from option. You could also load the data using load_from, but we’ll do that separately.create table events_iceberg () using iceberg
with (definition_from = 's3://mybucket/events/*.parquet');
Now we need a way to import all existing Parquet files and also import new files that show up in S3 into Iceberg. This is another job for pg_incremental. Following a similar approach as before, we create a function to generate a COPY command using a parameter.-- define an import function that wraps a COPY command to import from a URL
create function import_events(path text)
returns void language plpgsql as $function$
begin
  execute format($$copy events_iceberg from %L$$, path);
end;
$function$;

-- create a pipeline to import new files into a table, one by one.
-- $1 will be set to the path of a new file
select incremental.create_file_list_pipeline('event-import',
   file_pattern := 's3://mybucket/events/*.parquet',
   list_function := 'crunchy_lake.list_files',
   command := $$ select import_events($1) $$,
);

-- optional: do compaction immediately
vacuum events_iceberg;

After running these commands, your data will be continuously archived from your source Postgres server into Iceberg in S3. You can then run fast analytical queries directly from Crunchy Data Warehouse, which uses a combination of parallel, vectorized query processing and file caching to speed up queries. You can additionally set up (materialized) views and assign read permissions to the relevant users.No complex ETL pipelines required.To give you a sense of the performance benefit of using Parquet, we loaded 100M rows into the source table, which got automatically mirrored in Parquet and Iceberg via our pipelines. We then ran a simple analytical query on each table:select device_id, avg(sensor_1) from events group by 1;
The runtimes in milliseconds are shown in the following chart:In this case the source server is a standard-16 instance (4 vcpus) on Crunchy Bridge, and the warehouse is a warehouse-standard-16 instance (4 vcpus). So, using Crunchy Data Warehouse we can analyze 100M rows in well under a second on a small machine, and get >10x speedup with Iceberg.The use of compression also means the size went from 8.9GB in PostgreSQL to 1.2GB in Iceberg using object storage.With pg_parquet and pg_incremental, you can incrementally export data from PostgreSQL into Parquet in S3, and with Crunchy Data Warehouse you can process and analyze that data very quickly while still using PostgreSQL.One of the nice characteristics of the approach described in this blog is that the pipelines are fully transactional. It means that every import or export step either fully succeeds or fails and then it will be retried until it does succeed. That’s how we can create production-ready pipelines with a few simple SQL commands.Under the covers, pg_incremental keeps track of which time ranges or files have been processed. The bookkeeping happens in the same transaction as the COPY commands. So if a command fails because of an ephemeral S3 issue, the data will not end up being ingested twice or go missing. Having transactions takes away a huge amount of complexity for building reliable pipelines. There can of course be other reasons for pipeline failures that cannot be resolved through retries (e.g. changing data format), so it is still important to monitor your pipelines.]]></content:encoded></item><item><title>Support the open source projects you love this Valentine’s Day</title><link>https://github.blog/open-source/support-the-open-source-projects-you-love-this-valentines-day/</link><author>Martin Woodward</author><category>official</category><category>dev</category><pubDate>Fri, 14 Feb 2025 17:00:04 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[While everyone likes flowers and chocolates, why not show your love for your favorite open source projects this Valentine’s and give appreciation to the maintainers who keep them running?Many of the open source projects we rely on every day are maintained by dedicated volunteers. Sponsoring projects isn’t just about altruism, it’s about investing in the future. Many maintainers work on open source projects in their spare time. Sponsorships can help them dedicate more time to the projects you depend on. With financial support, maintainers can help cover costs such as development and hosting.When you sponsor a maintainer, you’re helping them continue their work and letting them know you value the creativity and love they’ve poured into their project. For this Valentine’s Day, let’s show love to the maintainers who keep our favorite projects alive and thriving.The benefits of financial sponsorshipSponsorship is a tangible way to show support for the open source community. It can be a huge morale boost, as well as bring visibility and validation. Sponsored projects often gain more attention, leading to a virtuous cycle of more contributors and users, which results in better software for everyone.Getting started: How to best invest in open sourceIdentify critical dependencies:Review your project’s dependencies to identify which open source libraries and tools are crucial to your operations. If you use tools like npm, pip, or maven, you can review their dependencies and prioritize which are critical to your project’s success.  Evaluate the project activity: Check the recent and trending activity of repositories and look for signs of contributions, shipped features, bug fixes, active maintenance, and community engagement. Helpful metrics like recent commits, issue resolution, and community engagement are all good markers for where funding could help. Projects with a high impact but low funding could be great candidates to prioritize for sponsorship.   Take the opportunity to look at the issues and contribution guidelines. Try to understand their funding needs and how your support makes a difference. But also consider investing in ways beyond just financial support. You could provide bug fixes if you feel confident in your code. You could also take the time to tell someone about the project and why you think it’s awesome. Maintainers can show love for their sponsors as well. Sponsors appreciate knowing their contribution is making a difference. Whether it’s a simple mention on social media to acknowledge their contribution, featuring them on your GitHub Sponsors profile, exclusive updates and behind-the-scenes insights about your project and community, or even a brief personalized thank-you message, tokens of appreciation can help bring more funding your way.This Valentine’s Day, let’s show our favorite projects a bit more appreciation. Whether it’s a heartfelt contribution to fix bugs or docs, spreading the word about why it’s a great project and helping people use it, or a sweet one-time donation, every bit helps keep our beloved digital world turning.Ready to start sponsoring? Visit GitHub Sponsors and find the projects that matter most to you.Happy Valentine’s Day to all the open source contributors and supporters out there! And remember, you don’t need to wait for a special occasion like Valentine’s Day to show your appreciation for maintainers—every day is a perfect day to support open source! 💖]]></content:encoded></item><item><title>Roadmap to Becoming a Data Scientist, Part 4: Advanced Machine Learning</title><link>https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-4-advanced-machine-learning/</link><author>Vyacheslav Efimov</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 17:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Data science is undoubtedly one of the most fascinating fields today. Following significant breakthroughs in machine learning about a decade ago, data science has surged in popularity within the tech community. Each year, we witness increasingly powerful tools that once seemed unimaginable. Innovations such as the , , the Retrieval-Augmented Generation (RAG) framework, and state-of-the-art  — including  — have had a profound impact on our world.However, with the abundance of tools and the ongoing hype surrounding AI, it can be overwhelming — especially for beginners — to determine which skills to prioritize when aiming for a career in data science. Moreover, this field is highly demanding, requiring substantial dedication and perseverance.The first three parts of this series outlined the necessary skills to become a data scientist in three key areas: math, software engineering, and machine learning. While knowledge of classical Machine Learning and neural network algorithms is an excellent starting point for aspiring data specialists, there are still many important topics in machine learning that must be mastered to work on more advanced projects.This article will focus solely on the math skills necessary to start a career in Data Science. Whether pursuing this path is a worthwhile choice based on your background and other factors will be discussed in a separate article.The importance of learning evolution of methods in machine learningThe section below provides information about the evolution of methods in natural language processing (NLP).In contrast to previous articles in this series, I have decided to change the format in which I present the necessary skills for aspiring data scientists. Instead of directly listing specific competencies to develop and the motivation behind mastering them, I will briefly outline the most important approaches, presenting them in chronological order as they have been developed and used over the past decades in machine learning.The reason is that I believe it is crucial to study these algorithms from the very beginning. In machine learning, many new methods are built upon older approaches, which is especially true for NLP and computer vision.For example, jumping directly into the implementation details of modern large language models (LLMs) without any preliminary knowledge may make it very difficult for beginners to grasp the motivation and underlying ideas of specific mechanisms.Given this, in the next two sections, I will highlight in  the key concepts that should be studied.Natural language processing (NLP) is a broad field that focuses on processing textual information. Machine learning algorithms cannot work directly with raw text, which is why text is usually preprocessed and converted into numerical vectors that are then fed into neural networks.Before being converted into vectors, words undergo , which includes simple techniques such as , stemming, lemmatization, normalization, or removing . After preprocessing, the resulting text is encoded into . Tokens represent the smallest textual elements in a collection of documents. Generally, a token can be a part of a word, a sequence of symbols, or an individual symbol. Ultimately, tokens are converted into numerical vectors.The  method is the most basic way to encode tokens, focusing on counting the frequency of tokens in each document. However, in practice, this is usually not sufficient, as it is also necessary to account for token importance — a concept introduced in the  and  methods. While TF-IDF improves upon the naive counting approach of bag of words, researchers have developed a completely new approach called embeddings. are numerical vectors whose components preserve the semantic meanings of words. Because of this, embeddings play a crucial role in NLP, enabling input data to be trained or used for model inference. Additionally, embeddings can be used to compare text similarity, allowing for the retrieval of the most relevant documents from a collection.Embeddings can also be used to encode other unstructured data, including images, audio, and videos.As a field, NLP has been evolving rapidly over the last 10–20 years to efficiently solve various text-related problems. Complex tasks like text translation and text generation were initially addressed using recurrent neural networks (RNNs), which introduced the concept of memory, allowing neural networks to capture and retain key contextual information in long documents.Although RNN performance gradually improved, it remained suboptimal for certain tasks. Moreover, RNNs are relatively slow, and their sequential prediction process does not allow for parallelization during training and inference, making them less efficient.Additionally, the original Transformer architecture can be decomposed into two separate modules:  and . Both of these form the foundation of the most state-of-the-art models used today to solve various NLP problems. Understanding their principles is valuable knowledge that will help learners advance further when studying or working with other large language models (LLMs).When it comes to LLMs, I strongly recommend studying the evolution of at least the first three GPT models, as they have had a significant impact on the AI world we know today. In particular, I would like to highlight the concepts of  and , introduced in GPT-2, which enable LLMs to solve text generation tasks without explicitly receiving any training examples for them.Another important technique developed in recent years is retrieval-augmented generation (RAG). The main limitation of LLMs is that they are only aware of the context used during their training. As a result, they lack knowledge of any information beyond their training data.The retriever converts the input prompt into an embedding, which is then used to query a vector database. The database returns the most relevant context based on the similarity to the embedding. This retrieved context is then combined with the original prompt and passed to a generative model. The model processes both the initial prompt and the additional context to generate a more informed and contextually accurate response.A good example of this limitation is the first version of the ChatGPT model, which was trained on data up to the year 2022 and had no knowledge of events that occurred from 2023 onward.To address this limitation, OpenAI researchers developed a RAG pipeline, which includes a constantly updated database containing new information from external sources. When ChatGPT is given a task that requires external knowledge, it queries the database to retrieve the most relevant context and integrates it into the final prompt sent to the machine learning model.The goal of distillation is to create a smaller model that can imitate a larger one. In practice, this means that if a large model makes a prediction, the smaller model is expected to produce a similar result.In the modern era, LLM development has led to models with millions or even billions of parameters. As a consequence, the overall size of these models may exceed the hardware limitations of standard computers or small portable devices, which come with many constraints.Quantization is the process of reducing the memory required to store numerical values representing a model’s weights.This is where optimization techniques become particularly useful, allowing LLMs to be compressed without significantly compromising their performance. The most commonly used techniques today include ,, and .Pruning refers to discarding the least important weights of a model.Regardless of the area in which you wish to specialize, knowledge of  is a must-have skill! Fine-tuning is a powerful concept that allows you to efficiently adapt a pre-trained model to a new task.Fine-tuning is especially useful when working with very large models. For example, imagine you want to use BERT to perform semantic analysis on a specific dataset. While BERT is trained on general data, it might not fully understand the context of your dataset. At the same time, training BERT from scratch for your specific task would require a massive amount of resources.Here is where fine-tuning comes in: it involves taking a pre-trained BERT (or another model) and freezing some of its layers (usually those at the beginning). As a result, BERT is retrained, but this time only on the new dataset provided. Since BERT updates only a subset of its weights and the new dataset is likely much smaller than the original one BERT was trained on, fine-tuning becomes a very efficient technique for adapting BERT’s rich knowledge to a specific domain.Fine-tuning is widely used not only in NLP but also across many other domains.As the name suggests,  involves analyzing images and videos using machine learning. The most common tasks include image classification, object detection, image segmentation, and generation.Most CV algorithms are based on neural networks, so it is essential to understand how they work in detail. In particular, CV uses a special type of network called convolutional neural networks (CNNs). These are similar to fully connected networks, except that they typically begin with a set of specialized mathematical operations called .In simple terms, convolutions act as filters, enabling the model to extract the most important features from an image, which are then passed to fully connected layers for further analysis.The next step is to study the most popular CNN architectures for classification tasks, such as AlexNet, VGG, Inception, ImageNet, and .Speaking of the object detection task, the  algorithm is a clear winner. It is not necessary to study all of the dozens of versions of YOLO. In reality, going through the original paper of the first YOLO should be sufficient to understand how a relatively difficult problem like object detection is elegantly transformed into both classification and regression problems. This approach in YOLO also provides a nice intuition on how more complex CV tasks can be reformulated in simpler terms.While there are many architectures for performing image segmentation, I would strongly recommend learning about , which introduces an encoder-decoder architecture.Finally, image generation is probably one of the most challenging tasks in CV. Personally, I consider it an optional topic for learners, as it involves many advanced concepts. Nevertheless, gaining a high-level intuition of how generative adversial networks (GAN) function to generate images is a good way to broaden one’s horizons.In some problems, the training data might not be enough to build a performant model. In such cases, the data augmentation technique is commonly used. It involves the artificial generation of training data from already existing data (images). By feeding the model more diverse data, it becomes capable of learning and recognizing more patterns.It would be very hard to present in detail the Roadmaps for all existing machine learning domains in a single article. That is why, in this section, I would like to briefly list and explain some of the other most popular areas in data science worth exploring.First of all, recommender systems (RecSys) have gained a lot of popularity in recent years. They are increasingly implemented in online shops, social networks, and streaming services. The key idea of most algorithms is to take a large initial matrix of all users and items and decompose it into a product of several matrices in a way that associates every user and every item with a high-dimensional embedding. This approach is very flexible, as it then allows different types of comparison operations on embeddings to find the most relevant items for a given user. Moreover, it is much more rapid to perform analysis on small matrices rather than the original, which usually tends to have huge dimensions. often goes hand in hand with RecSys. When a RecSys has identified a set of the most relevant items for the user, ranking algorithms are used to sort them to determine the order in which they will be shown or proposed to the user. A good example of their usage is search engines, which filter query results from top to bottom on a web page.Closely related to ranking, there is also a  problem that aims to optimally map objects from two sets, A and B, in a way that, on average, every object pair is mapped “well” according to a matching criterion. A use case example might include distributing a group of students to different university disciplines, where the number of spots in each class is limited. is an unsupervised machine learning task whose objective is to split a dataset into several regions (clusters), with each dataset object belonging to one of these clusters. The splitting criteria can vary depending on the task. Clustering is useful because it allows for grouping similar objects together. Moreover, further analysis can be applied to treat objects in each cluster separately.The goal of clustering is to group dataset objects (on the left) into several categories (on the right) based on their similarity. is another unsupervised problem, where the goal is to compress an input dataset. When the dimensionality of the dataset is large, it takes more time and resources for machine learning algorithms to analyze it. By identifying and removing noisy dataset features or those that do not provide much valuable information, the data analysis process becomes considerably easier. is an area that focuses on designing algorithms and data structures (indexes) to optimize searches in a large database of embeddings (vector database). More precisely, given an input embedding and a vector database, the goal is to  find the most similar embedding in the database relative to the input embedding.The goal of similarity search is to approximately find the most similar embedding in a vector database relative to a query embedding.The word “approximately” means that the search is not guaranteed to be 100% precise. Nevertheless, this is the main idea behind similarity search algorithms — sacrificing a bit of accuracy in exchange for significant gains in prediction speed or data compression. involves studying the behavior of a target variable over time. This problem can be solved using classical tabular algorithms. However, the presence of time introduces new factors that cannot be captured by standard algorithms. For instance:the target variable can have an overall , where in the long term its values increase or decrease (e.g., the average yearly temperature rising due to global warming).the target variable can have a  which makes its values change based on the currently given period (e.g. temperature is lower in winter and higher in summer).Most of the time series models take both of these factors into account. In general, time series models are mainly used a lot in financial, stock or demographic analysis.Another advanced area I would recommend exploring is , which fundamentally changes the algorithm design compared to classical machine learning. In simple terms, its goal is to train an agent in an environment to make optimal decisions based on a reward system (also known as the “trial and error approach”). By taking an action, the agent receives a reward, which helps it understand whether the chosen action had a positive or negative effect. After that, the agent slightly adjusts its strategy, and the entire cycle repeats.Reinforcement learning is particularly popular in complex environments where classical algorithms are not capable of solving a problem. Given the complexity of reinforcement learning algorithms and the computational resources they require, this area is not yet fully mature, but it has high potential to gain even more popularity in the future.Currently the most popular applications are:. Existing approaches can design optimal game strategies and outperform humans. The most well-known examples are chess and Go.. Advanced algorithms can be incorporated into robots to help them move, carry objects or complete routine tasks at home.. Reinforcement learning methods can be developed to automatically drive cars, control helicopters or drones.This article was a logical continuation of the previous part and expanded the skill set needed to become a data scientist. While most of the mentioned topics require time to master, they can add significant value to your portfolio. This is especially true for the NLP and CV domains, which are in high demand today.After reaching a high level of expertise in data science, it is still crucial to stay motivated and consistently push yourself to learn new topics and explore emerging algorithms.Data science is a constantly evolving field, and in the coming years, we might witness the development of new state-of-the-art approaches that we could not have imagined in the past.All images are by the author unless noted otherwise.]]></content:encoded></item><item><title>Siren Call of SQLite on the Server</title><link>https://pid1.dev/posts/siren-call-of-sqlite-on-the-server/</link><author>/u/sausagefeet</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 16:46:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Behind the Blog: Backdoors and the Miracle of Wikipedia</title><link>https://www.404media.co/behind-the-blog-backdoors-and-the-miracle-of-wikipedia/</link><author>Samantha Cole</author><category>dev</category><enclosure url="https://www.404media.co/content/images/2025/02/nl2.14--1-.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 16:40:01 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[This is Behind the Blog, where we share our behind-the-scenes thoughts about how a few of our top stories of the week came together. This week, we discuss Apple's iCloud, Wikipedia is a miracle of humankind, and good soup.After our relatively unhinged BTBs last week, many of you left extremely nice comments, reached out individually, or otherwise gave us encouragement. You all are the best, and it made us feel very good. Thank you!]]></content:encoded></item><item><title>Calico CNI - services and pods cant connect to ClusterIP</title><link>https://www.reddit.com/r/kubernetes/comments/1ipehey/calico_cni_services_and_pods_cant_connect_to/</link><author>/u/Flimsy_Tomato4847</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 16:29:11 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I am running a kubernetes cluster with a haproxy + keepalived setup for the cluster-endpoint (virtual IP Address). All nodes are in the same subnet. Calico operator installation works well. But when i deploy pods they can't connect to each other nevertheless they are in the same subnet or in different subnets. There is just the standard network policy enabled, so network policies cant be the issue.Now when i look a the calico-kube-controller logs i get:[FATAL][1] kube-controllers/main.go 136: Failed to initialize Calico datastore"major": "1", "minor": "31", ... }Connecting to 10.96.0.1:443 (10.96.0.1:443)wget: can't connect to remote host (10.96.0.1): Connection refusedI dont know how to fix this strange behavior, beacause i also tried the ebpf dataplane with same behavior and i dont know where my mistake is.I init the cluster with: sudo kubeadm init --control-plane-endpoint=<myVIP>:6443 --pod-network-cidr=192.168.0.0/16 --upload-certsFYI this is my calico custom-resources.yamlapiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: calicoNetwork: ipPools: - name: default-ipv4-ippool blockSize: 26 cidr: 192.168.0.0/16 encapsulation: None natOutgoing: Enabled nodeSelector: all() linuxDataplane: Iptables --- apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} The active network policy created by default:apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: creationTimestamp: "2025-02-14T09:29:49Z" generation: 1 name: allow-apiserver namespace: calico-apiserver ownerReferences: - apiVersion: operator.tigera.io/v1 blockOwnerDeletion: true controller: true kind: APIServer name: default uid: d1b2a55b-aa50-495f-b751-4173eb6fa211 resourceVersion: "2872" uid: 63ac4155-461b-450d-a4c8-d105aaa6f429 spec: ingress: - ports: - port: 5443 protocol: TCP podSelector: matchLabels: apiserver: "true" policyTypes: - Ingress This is my haproxy config with the VIPglobal log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:6443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server master1 <master1-ip>:6443 check server master2 <master2-ip>:6443 check server master3 <master3-ip>:6443 check global_defs { router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0.1 vrrp_gna_interval 0.1 } vrrp_script chk_haproxy { script "killall -0 haproxy" interval 2 weight 2 } vrrp_instance haproxy-vip { state MASTER priority 101 interface ens192 # Network card virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { <myVIP>/24 # The VIP address } track_script { chk_haproxy } } ]]></content:encoded></item><item><title>Detecting AI agent use and abuse</title><link>https://stytch.com/blog/detecting-ai-agent-use-abuse/</link><author>mattmarcus</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 16:18:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[AI agents aren’t just indexing your content anymore. With tools like OpenAI’s Operator, Anthropic’s Computer Use API, and BrowserBase’s Open Operator, these agents can navigate the web, mimic real users, and even take actions at scale. The challenge? Knowing whether they’re enhancing your user experience—or opening the door to abuse.In some scenarios, apps might encourage agent use if it improves usability and adoption, but in other cases, it could present unacceptable risks for application developers or be used as a method for malicious attacks (e.g. credential stuffing or fake account creation).In either scenario, observability is paramount. Applications need to know what traffic is on their site (is this a human? A bot? A good bot or a bad one?) in order to make intelligent decisions about how to shape traffic and enforce desired usage patterns. AI agents add an additional wrinkle as users are already sharing their credentials with tools like Operator, meaning even a well-intentioned agent creates potential risk for these applications and their users.The key question is: Can you detect AI agent traffic on your application today?We tested multiple AI agent toolkits across high-traffic consumer sites, and the results were clear—legacy detection techniques (CAPTCHAs, IP blocking, user-agent filtering) are largely ineffective. Here’s what we found.The New Reality of Bot Traffic:What AI Agent Traffic Looks LikeTraditionally, bot detection relied on CAPTCHAs, IP blocking, and user-agent filtering. But modern AI agents are engineered to look like actual users: They use genuine IP addresses, user agents, and even simulate mouse movements.Headless browsing at human speeds: Their interactions mimic natural browsing behavior, evading rate-limit triggers.Datacenter origins—but not always: While some (like OpenAI’s Operator) come from known Azure datacenters, others (like Anthropic’s API) can run locally, borrowing your machine’s properties.On the surface level, AI agent traffic can look quite similar to regular human user traffic. Here are examples of browser & network properties across different AI agents:Anthropic Computer Use APIBrowserBase Open OperatorChrome on Linux (standard fingerprint)Firefox on Ubuntu (stable, unless run locally)Chrome on Mac (with stealth features)Known Azure Datacenter IPVaries—depends on local vs. cloud deploymentSan Francisco, CaliforniaBoth OpenAI’s Operator and BrowserBase’s Open Operator spin up remote datacenter or container‑hosted Chromium instances, rather than installing any software on your local machine. Because these solutions originate from a cloud‑hosted environment (with IP addresses and other signatures tied to the provider’s infrastructure), they can be  via methods like ASN lookups if the provider’s IP ranges are well known.By contrast, Anthropic’s “Computer Use” functionality is available only through its API as of now, and you can choose how to run that API (e.g., directly on your own machine vs. inside a VM or container). If you run it locally, it inherits your local system’s properties like IP and ASN; if you host it in the cloud, it uses whichever environment that provider offers. Each approach has its own implications for fingerprinting and detection.Currently, Anthropic provides two options for using their agent:Direct use of your computer environment (not recommended due to security concerns around unrestricted access).Setting up a virtual machine or container with minimal privileges to run the API. You can either run this locally or deploy to a cloud provider using Docker, in which case it won’t inherit the user’s network properties.Whereas stricter sites like Reddit block OpenAI and BrowserBase outright, Anthropic’s approach allows it to successfully bypass even these strict sites when run locally:And with tools like BrowserBase, there are also now open-sourced options for building browsing AI agents that allow potential attackers to increase the stealth of their headless browsing setup. This means we should expect continued, quick iteration on some agent use cases that will make them even more difficult to detect. As an example, BrowserBase offers premium plans with more advanced stealth to bypass CAPTCHA and other detection techniques:Today, most sites are allowing agent traffic to navigate freely – whether these sites are detecting the abnormalities of this traffic (and choosing not to take action) is hard to say. For some like Reddit & Youtube, their block on OpenAI Operator indicates they want to lock down agentic traffic. When these AI Agents bypass their restrictions (as detailed below), it’s a good signal those tools are actually flying below their radar today versus being officially sanctioned.For others that allow this traffic, they may actually encourage it for user convenience or potentially have perverse incentives in some cases to allow a certain amount of bot traffic (Ticketmaster is a potential example of this given they can benefit from some level of bot activity increasing ticket purchase rates). Many more sites, however, simply don’t have the traffic intelligence to detect these AI agents, which explains why they can operate freely.In testing the three primary AI Agent browsing toolkits on a set of high traffic consumer sites, we found that Youtube and Reddit were the only ones that consistently blocked this traffic:Anthropic Computer Use APIBrowserBase Open OperatorFew websites are blocking popular AI Agents today – indicating either a lack of detection, ambivalence towards enforcement, or both. Reddit & Youtube are a couple of key outliers that block this traffic when they can discern it.Still, the differences are interesting. It’s worth understanding why Anthropic or BrowserBase are sometimes able to bypass sites that otherwise are blocking this traffic. In BrowserBase’s case, they generate a slightly different user-agent each session, which sometimes aligns with the underlying chromium runtime but sometimes attempts to be deceptive by claiming to be a different version, which leads to more inconsistency in whether companies detect it correctly.Companies like LinkedIn and X are particularly sensitive to scraping on their sites, so the lack of enforcement is surprising. This suggests they’re not currently able to confidently discern this is programmatic traffic.How to Tell an Agent from a HumanThe obvious question here is: shouldn't this be easy to detect by using just an IP or user agent?And the answer is yes and no. For OpenAI Operator, it’s pretty straightforward to detect given its stability & origin point. For Anthropic and BrowserBase (and the new agents entering the market), it’s a bit more complicated.Anthropic Computer Use APIBrowserBase Open OperatorDepends on how you run it:Unlike Operator, you can run it locally directly on your machine or (recommended) on a dockerized container. In this case, it inherits your own device’s IP & characteristicsIn production, you’d likely host it in AWS/Azure/etc and traffic would come from known data centersOffers options for automatically solvingSome key elements worth knowing:OpenAI & BrowserBase both operate on top of chromium – being able to discern chromium browsers vs. true chrome is key for detecting the latter since BrowserBase modifies things like IP, user agent, etc for increased stealth.OpenAI originates out of a known Azure datacenter while BrowserBase originates from a known AWS datacenter.OpenAI hands off non-invisible CAPTCHAs to the end user to complete, while BrowserBase offers options for automatically solving.Ideally, you want to detect this traffic beyond more coarse identifiers like known datacenter IPs so that your detection is fully resistant to adjustments agentic traffic will make in the future. And core to that is having high confidence in whether a browser interacting with your site is a real browser or a headless instrumentation.This is where we’ve personally found Machine Learning to be incredibly useful by building a browser lie detector. We look at a very wide range of browser signals & we have downloaded the entire historical archives of every browser version that has been released that we can find and trained our model on what authentic browsers look like vs. anomalous or deceitful browsers. To provide this visibility, we’ve automated additions of new legitimate browser signatures upon new releases, we fingerprint it, run it past the model to establish an authentic set of signals for the current browser version, allowing us to detect anyone trying to emulate that version inorganically. With that fingerprinted dataset, machine learning can be used to detect anomalies with high accuracy. It’s this signal collection that has allowed us to see that OpenAI & BrowserBase are full chromium builds, while Anthropic’s Firefox browser fails to emulate certain characteristics that would be present on a real user’s Firefox instance.Charting Your Path ForwardWhether you choose to block, restrict, or harness AI agent traffic, effective observability is non-negotiable. Here’s our current playbook: Start by detecting and monitoring AI agent fingerprints. Learn from their behavior before enforcing hard limits.Embrace legitimate use cases: Some users may leverage AI agents to streamline workflows (e.g., automating reporting on your dashboard). Recognize these opportunities while safeguarding against abuse. As AI agents evolve, so must your detection strategies. Invest in ML-based solutions that adapt alongside new agent behaviors.AI agents are rewriting the rules of web interaction. They offer exciting UX advancements—but also present new security challenges. The race is on: evolve your detection or risk being outpaced by bad actors and unintended misuse.Try Device Fingerprinting99.99% bot detection, intelligent rate limiting, and reverse engineering protection.Read the docs]]></content:encoded></item><item><title>I&apos;m very impressed by how Rust supports both beginners and pro&apos;s</title><link>https://www.reddit.com/r/rust/comments/1ipe6m7/im_very_impressed_by_how_rust_supports_both/</link><author>/u/ConstructionShot2026</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 16:16:17 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I would go as far saying it supports a syntax abstraction that is simpler than python to read.I just find it amazing, with a performance level so close to C++.Its your choice how many complex features you want to add for control and optimization, and the compiler is so cool, that it can add them automatically if I don't see it necessary.I believe if more knew how simple it could be, more would use it outside systems programming :D]]></content:encoded></item><item><title>Publish Interactive Data Visualizations for Free with Python and Marimo</title><link>https://towardsdatascience.com/publish-interactive-data-visualizations-for-free-with-python-and-marimo/</link><author>Sam Minot</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 16:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Working in Data Science, it can be hard to share insights from complex datasets using only static figures. All the facets that describe the shape and meaning of interesting data are not always captured in a handful of pre-generated figures. While we have powerful technologies available for presenting interactive figures — where a viewer can rotate, filter, zoom, and generally explore complex data  —  they always come with tradeoffs.Here I present my experience using a recently released Python library — marimo — which opens up exciting new opportunities for publishing interactive visualizations across the entire field of data science.Interactive Data VisualizationThe tradeoffs to consider when selecting an approach for presenting data visualizations can be broken into three categories: — what visualizations and interactivity am I able to present to the user? — what are the resources needed for displaying this visualization to users (e.g. running servers, hosting websites)? – how much of a new skillset / codebase do I need to learn upfront? is the foundation of portable interactivity. Every user has a web browser installed on their computer and there are many different frameworks available for displaying any degree of interactivity or visualization you might imagine (for example, this gallery of amazing things people have made with three.js). Since the application is running on the user’s computer, no costly servers are needed. However, a significant drawback for the data science community is ease of use, as JS does not have many of the high-level (i.e. easy-to-use) libraries that data scientists use for data manipulation, plotting, and interactivity. provides a useful point of comparison. Because of its continually growing popularity, some have called this the “Era of Python”. For data scientists in particular, Python stands alongside R as one of the foundational languages for quickly and effectively wielding complex data. While Python may be easier to use than Javascript, there are fewer options for presenting interactive visualizations. Some popular projects providing interactivity and visualization have been Flask, Dash, and Streamlit (also worth mentioning — bokeh, HoloViews, altair, and plotly). The biggest tradeoff for using Python has been the cost for publishing – delivering the tool to users. In the same way that shinyapps require a running computer to serve up the visualization, these Python-based frameworks have exclusively been server-based. This is by no means prohibitive for authors with a budget to spend, but it does limit the number of users who can take advantage of a particular project. is an intriguing middle ground — Python code running directly in the web browser using WebAssembly (WASM). There are resource limitations (only 1 thread and 2GB memory) that make this impractical for doing the heavy lifting of data science. , this can be more than sufficient for building visualizations and updating based on user input. Because it runs in the browser, no servers are required for hosting. Tools that use Pyodide as a foundation are interesting to explore because they give data scientists an opportunity to write Python code which runs directly on users’ computers without their having to install or run anything outside of the web browser.As an aside, I’ve been interested previously in one project that has tried this approach: stlite, an in-browser implementation of Streamlit that lets you deploy these flexible and powerful apps to a broad range of users. However, a core limitation is that Streamlit itself is distinct from stlite (the port of Streamlit to WASM), which means that not all features are supported and that advancement of the project is dependent on two separate groups working along compatible lines.The interface resembles a Jupyter , which will be familiar to users.Execution of cells is , so that updating one cell will rerun all cells which depend on its output. can be captured with a flexible set of UI components.Notebooks can be quickly converted into , hiding the code and showing only the input/output elements.Apps can be run locally or converted into using WASM/Pyodide.marimo balances the tradeoffs of technology in a way that is well suited to the skill set of the typical data scientists: — user input and visual display features are rather extensive, supporting user input via Altair and Plotly plots. — deploying as static webpages is basically free — no servers required — for users familiar with Python notebooks, marimo will feel very familiar and be easy to pick up.Publishing Marimo Apps on the WebAs a simple example of the type of display that can be useful in data science, consisting of explanatory text interspersed with interactive displays, I have created a barebones GitHub repository. Try it out yourself here.Using just a little bit of code, users can:Generate visualizations with flexible interactivityWrite narrative text describing their findingsPublish to the web for free (i.e. using GitHub Pages)Public App / Private DataThis new technology offers an exciting new opportunity for collaboration — publish the app publicly to the world, but users can only see specific datasets that they have permission to access.Rather than building a dedicated data backend for every app, user data can be stored in a generic backend which can be securely authenticated and accessed using a Python client library — all contained within the user’s web browser. For example, the user is given an OAuth login link that will authenticate them with the backend and allow the app to temporarily access input data.As a proof of concept, I built a simple visualization app which connects to the Cirro data platform, which is used at my institution to manage scientific data. Full disclosure: I was part of the team that built this platform before it spun out as an independent company. In this manner users can:Load the public visualization app — hosted on GitHub PagesConnect securely to their private data storeLoad the appropriate dataset for displayShare a link which will direct authorized collaborators to the same dataAs a data scientist, this approach of publishing free and open-source visualization apps which can be used to interact with private datasets is extremely exciting. Building and publishing a new app can take hours and days instead of weeks and years, letting researchers quickly share their insights with collaborators and then publish them to the wider world.]]></content:encoded></item><item><title>kubernetes vcenter</title><link>https://www.reddit.com/r/kubernetes/comments/1ipdnbw/kubernetes_vcenter/</link><author>/u/Low_Metal_7679</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 15:53:23 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[hello i am getting started with kubernetes i have created a NFS as PV but how can i use vmware datastores to use this as PV?- VMWARE-H1-DC1 - VMWARE-H2-DC1 - VMWARE-H4-DC2i have a test cluster with on each host a vmi have deployed it using ansible so the config is on evry host the same but dont know how to use vcenter storage. I gues i need to provide a CSO or so but dont know how to do this can someone help me out with this?]]></content:encoded></item><item><title>Why does Linux open large file bases much faster than windows?</title><link>https://www.reddit.com/r/linux/comments/1ipd8a7/why_does_linux_open_large_file_bases_much_faster/</link><author>/u/AlternativeCarpet494</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 15:34:56 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[So I have a 4TB hard drive with around a 100 GB dataset on it. I was going to some useless uni classes today and thought oh I’ll just work on some of my code to process the data set on my windows laptop. Anyways, the file explorer crashed. Why is the windows file system so much worse?]]></content:encoded></item><item><title>Authorization game — match incoming requests to permission policies (my weekend project)</title><link>https://game.cerbos.dev/</link><author>/u/diggVSredditt</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 15:26:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The largest sofa you can move around a corner</title><link>https://www.quantamagazine.org/the-largest-sofa-you-can-move-around-a-corner-20250214/</link><author>abe94</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 15:24:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In 1992, Joseph Gerver of Rutgers University proposed a particularly clever curved shape with an area of approximately 2.2195. Mathematicians suspected that it answered Moser’s question. But they couldn’t prove it.Now a young postdoctoral researcher has. In a 119-page paper, Jineon Baek of Yonsei University in Seoul showed that Gerver’s sofa is the largest shape that can successfully pass through the hallway.The paper isn’t just noteworthy for resolving a 60-year-old problem. It has also garnered attention because mathematicians had expected any eventual proof of the conjecture to require a computer. Baek’s proof did not. Mathematicians now hope that the techniques he used might help them make progress on other kinds of optimization problems.Perhaps even more intriguing, Gerver’s sofa, unlike more familiar shapes, is defined in such a way that its area can’t be expressed in terms of known quantities (such as π or square roots). But for the moving sofa problem — a very simple question — it’s the optimal solution. The result illustrates that even the most straightforward optimization problems can have counterintuitively complicated answers.The first major progress on the moving sofa problem came in 1968, just two years after Moser posed it. John Hammersley connected two quarter circles with a rectangle, then cut a semicircle out of it to form a shape that resembled an old telephone. Its area was π/2 + 2/π, or approximately 2.2074.Hammersley also showed that any solution to the problem could have an area of at most $latex  2\sqrt{2}$, or about 2.8284.A couple of years later, Gerver, then a graduate student at the University of California, Berkeley, learned about the question. “Another grad student told me this problem and challenged me to find the answer,” he said. “He never said anything about it being unsolved. So I thought about it for a few days. Finally, I came back to him and said ‘OK, I give up. What’s the solution?’ And he refused to tell me! He said, ‘Just keep thinking about it. You’ll get it.’”Gerver thought about it sporadically over the next 20 years. But it wasn’t until 1990, when he mentioned it to the renowned mathematician John Conway, that he found out it had never been solved. The realization motivated him to spend more time with the problem, and he soon came up with a potential solution.Gerver’s sofa looked a lot like Hammersley’s telephone, but it was far more complicated to describe, consisting of 18 different pieces. (As it turned out, Ben Logan, an engineer at Bell Labs, independently uncovered the same shape but never published his work.) Some of the pieces were simple line segments and arcs. Others were more exotic, and tougher to describe.Still, Gerver suspected that this complicated shape was optimal: It possessed many features that mathematicians expected the optimal sofa to have, and he was able to prove that making small perturbations to its contours wouldn’t yield a suitable shape with a bigger area.]]></content:encoded></item><item><title>DA, sheriff, who shared woman&apos;s nude photos on phone are covered by QI</title><link>https://www.oregonlive.com/crime/2025/02/an-oregon-womans-nude-cellphone-photos-ended-up-the-talk-of-town-she-tracked-it-back-to-the-da.html</link><author>y33t</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 15:16:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[An Oregon woman’s nude photos ended up the topic of conversation in her small town after a prosecutor looked through her sensitive cellphone data and told the county sheriff what he found despite no warrant, no consent and no suspicion that she had committed a crime. But the district attorney involved won’t face legal consequences under a federal appeals court ruling released Monday.A three-judge panel of the 9th U.S. Circuit Court of Appeals ruled 2-to-1 that Grant County District Attorney Jim Carpenter’s actions amounted to “a troubling example of the intrusion on Fourth Amendment rights” against illegal searches. But all three judges upheld a lower court’s decision to grant Carpenter qualified immunity and dismiss the woman’s lawsuit. Under qualified immunity, government officials can be held accountable for violating someone’s rights only if a court has previously ruled that it was “clearly established” those precise actions were unconstitutional.Carpenter declined to comment on the ruling.A lawyer for the woman said the decision, while disappointing because it allows no remedy for her client, still shines a light on the improper behavior by law enforcement officials and will help others.The ruling puts prosecutors and police on notice that similar actions in the future would violate the Constitution and they could face liability, said attorney Nadia Dahab.“Opinions like this are important for the development of the law and for people who suffer harms like she did in the future to be able to hold public officials accountable,” Dahab said. “This makes it harder for officials to engage in this kind of conduct and to raise a qualified immunity defense down the road.”Qualified immunity has faced widespread criticism from victims who argue that it protects police and government officials from civil liability when they violate an individual’s constitutional rights. Victims of police misconduct and their families often spend years in court to seek justice only for officers to be shielded by qualified immunity, their lawyers have argued.Dahab represents Haley Olson, who runs a marijuana dispensary store in John Day, population 1,660.Olson’s case began when she was pulled over and arrested in Jerome County, Idaho, on Jan. 22, 2019, and charged with possessing marijuana. Recreational marijuana use is legal in Oregon but not in Idaho. Olson told the Idaho State Police trooper during the stop that her boyfriend was a sheriff’s deputy and police also found a business card for Grant County sheriff’s Deputy Tyler Smith in Olson’s car, according to the appellate opinion.Olson signed a form, giving Idaho police consent to search her cellphone. They extracted and copied the contents of her phone for review. Idaho authorities later dropped charges against her.But Glenn Palmer, the Grant County sheriff at the time, had called the Idaho trooper who handled Olson’s case shortly after her arrest. Palmer had apparently heard about the arrest from another sheriff’s office employee and was “curious” about whether Olson’s phone might reveal misconduct on Smith’s part, the appellate court wrote.Palmer learned during the conversation that the deputy’s card had been discovered in Olson’s car and asked the Idaho trooper to share the contents of Olson’s phone, but Idaho authorities rejected his request, according to the opinion.Palmer then asked the district attorney to request the contents of Olson’s cellphone from Idaho police and review the material.Palmer was allegedly concerned that his deputy “might be involved in illegal activities with Olson” that would require an internal investigation, the ruling said.Carpenter wrote to a prosecutor in the Jerome County District Attorney’s Office, saying the phone extraction “will be used only for internal purposes” and wouldn’t be disseminated to another agency or third party, according to the ruling. An Idaho prosecutor then shared a flash drive containing the contents of Olson’s phone with Carpenter.Carpenter immediately violated his pledge to Idaho police and asked detectives from two outside agencies, the Oregon State Police and the Deschutes County Sheriff’s Office, to review the flash drive material, according to the appellate opinion. Both agencies declined, because it wasn’t tied to a criminal investigation, the opinion said.Carpenter reviewed the phone contents himself in April 2019, found nude photos of both Olson and Smith and contacted the sheriff to tell him that the phone showed evidence of an intimate relationship between Smith and Olson, the ruling said.But Carpenter added in his letter to the sheriff that he found no evidence of misconduct by Smith on the phone, according to court records.Olson argued in her lawsuit that the district attorney shared the nude photos with the sheriff, but both Carpenter and Palmer denied those allegations. Yet Palmer testified in his deposition that Carpenter twice offered him a copy of the thumb drive and told him, “There were things on the cell phone that, ‘once you see them, you can’t unsee them,’” according to the ruling.Oslon said she started hearing gossip regarding her arrest, the examination of her phone, her relationship with Smith and the nude photos -- “all seemingly originating from the sheriff’s office,” the opinion said.Olson said a Grant County deputy whom she didn’t know came into her marijuana store and told her that he had heard “there’s some pretty smokin’ pictures of you going around the sheriff’s office,” the appeals court wroteAnother witness reported observing two sheriff’s employees looking at nude photos of Olson on a phone, according to the opinion. One of Olson’s customers asked her, “What happened to ya out in Idaho? Palmer told me you got arrested” and later said he had heard something about the contents on her phone, according to court records.Olson sued Palmer, Carpenter and Grant County in federal court, alleging violation of her Fourth Amendment right to be free from an unreasonable search and 14th Amendment right to privacy. U.S. District Judge Karin J. Immergut of Portland threw out the case, finding that Carpenter was entitled to qualified immunity and that Palmer had no “supervisory liability” because she found no evidence that the sheriff had reviewed the cellphone material or supervised Carpenter.But 9th Circuit Judge M. Margaret McKeown determined Carpenter’s review of the cellphone material was an unreasonable search, considering it was done “without consent, without a warrant and without even a suspicion of further criminal activity by Olson or even Smith.”McKeown, writing for the majority, said the Idaho State Police consent form that Olson signed didn’t extend to a search of the phone by a different law enforcement agency “in another state, for evidence of her boyfriend’s theoretical misdeeds.”Although Palmer denied seeing or sharing the contents of Olson’s phone with members of the public, Olson’s allegations that strangers made derogatory comments to her regarding the circulation of her nude photos “also support the claim that Olson’s private information was shared far beyond the scope of her original consent,” McKeown wrote.“Palmer was ‘curious’ about whether Olson’s phone might reveal misconduct on Smith’s part, and Carpenter was interested in reviewing the phone for possible Brady material in cases where Smith might testify,” the opinion said. Under the landmark 1963 U.S. Supreme Court ruling in Brady v. Maryland, police must inform prosecutors about an officer’s potential credibility problems on the witness stand or the need to turn over information that might be favorable to defense attorneys in a case.“But neither Palmer’s curiosity nor Carpenter’s improbable search for Brady material for some hypothetical future investigation justifies expanding the consent form’s express scope,” McKeown wrote.McKeown found Carpenter’s explanation for his search of the phone material unbelievable. “It defies common sense to hypothesize a potential Brady complication when there has been no prosecution, no investigation, nor even a whiff of criminal activity,” she wrote. The 9th Circuit panel also blasted Carpenter’s claim that his actions comported with longstanding practices of electronic data sharing between law enforcement agencies, calling it “devoid of any supporting authority.”Jill Conbere, the Oregon Justice Department lawyer who defended Carpenter in the case, argued that information sharing between law enforcement agencies “is standard practice in many contexts.”Conbere said Carpenter wasn’t required to get a warrant before accessing the phone data because Olson had given Idaho State Police consent to search her phone.Yet the 9th Circuit’s majority ruling noted, “At every turn, Carpenter and Palmer were stymied by other law enforcement personnel or agencies in both Idaho and Oregon that refused to aid them. The Idaho state trooper declined to give Palmer the extraction of Olson’s phone, and the other criminal investigation agencies in Oregon declined to review the data when asked by Carpenter, because there were no allegations of a crime, and therefore nothing for them to investigate.”In a separate concurring 9th Circuit opinion, Circuit Judge Daniel A. Bress agreed that the court should uphold the trial court’s granting of immunity and dismissal of the claims. But he wrote that the panel should have simply affirmed the lower court ruling and not gone on to find a constitutional violation. It’s unclear, Bress wrote, whether Idaho authorities are more at fault, for example. “In my respectful view, prudence here dictated that we decide only what we needed to decide,” he wrote.Later in 2019 in an unrelated case, Smith was fired from the sheriff’s office after his arrest on alleged assault and sex abuse complaints. He was acquitted of all charges in 2022, and argued in a federal whistleblower complaint that Palmer retaliated against him for reporting misconduct involving another sheriff’s deputy, who was the wife of Palmer’s undersheriff. Smith obtained more than $1.3 million from Grant County and the state of Oregon in a series of settlements after challenging his firing in the whistleblower complaint, the Blue Mountain Eagle reported. Smith now works as a private investigator.Palmer lost re-election as sheriff in 2020-- Maxine Bernstein covers federal court and criminal justice. Reach her at 503-221-8212, mbernstein@oregonian.com, follow her on XOur journalism needs your support. Subscribe today to]]></content:encoded></item><item><title>ICE Wants to Know If You&apos;re Posting Negative Things About It Online</title><link>https://theintercept.com/2025/02/11/ice-immigration-social-media-surveillance/</link><author>belter</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 15:14:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ protest over the Trump administration’s plan to deport millions of immigrants, U.S. Immigration and Customs Enforcement plans to monitor and locate “negative” social media discussion about the agency and its top officials, according to contract documents reviewed by The Intercept.“In order to prevent adversaries from successfully targeting ICE Senior leaders, personnel and facilities, ICE requires real-time threat mitigation and monitoring services, vulnerability assessments, and proactive threat monitoring services,” the procurement document reads.If this scanning uncovers anything the agency deems suspicious, ICE is asking its contractors to drill down into the background of social media users.“Previous social media activity which would indicate any additional threats to ICE; 2). Information which would indicate the individual(s) and/or the organization(s) making threats have a proclivity for violence; and 3). Information indicating a potential for carrying out a threat (such as postings depicting weapons, acts of violence, refences to acts of violence, to include empathy or affiliation with a group which has violent tendencies; references to violent acts; affections with violent acts; eluding [sic] to violent acts.”It’s unclear how exactly any contractor might sniff out someone’s “proclivity for violence.” The ICE document states only that the contractor will use “social and behavioral sciences” and “psychological profiles” to accomplish its automated threat detection.Once flagged, the system will further scour a target’s internet history and attempt to reveal their real-world position and offline identity. In addition to compiling personal information — such as the Social Security numbers and addresses of those whose posts are flagged — the contractor will also provide ICE with a “photograph, partial legal name, partial date of birth, possible city, possible work affiliations, possible school or university affiliation, and any identified possible family members or associates.”The document also requests “Facial Recognition capabilities that could take a photograph of a subject and search the internet to find all relevant information associated with the subject.” The contract contains specific directions for targets found in other countries, implying the program would scan the domestic speech of American citizens.The posting indicates that ICE isn’t merely looking to detect direct threats of violence, but also online criticism of the agency.As part of its mission to protect ICE with “proactive threat monitoring,” the winning contractor will not simply flag threatening remarks but “Provide monitoring and analysis of behavioral and social media sentiment (i.e. positive, neutral, and negative).” This includes regular updates on the “total number of negative references to ICE found in social media” from week to week.“ICE’s attempt to have eyes and ears in as many places as we exist both online and offline should ring an alarm.”Such sentiment analysis — typically accomplished via machine-learning techniques — could place under law enforcement scrutiny speech that is constitutionally protected. Simply stated, a post that is critical or even hostile to ICE isn’t against the law.“ICE’s attempts to capture and assign a judgement to people’s ‘sentiment’ throughout the expanse of the internet is beyond concerning,” said Cinthya Rodriguez, an organizer with the immigrant rights group Mijente. “The current administration’s attempt to use this technology falls within the agency’s larger history of mass surveillance, which includes gathering information from personal social media accounts and retaliating against immigrant activists. ICE’s attempt to have eyes and ears in as many places as we exist both online and offline should ring an alarm for all of us.”Scanning online speech in the name of homeland security is a bipartisan initiative. The document soliciting contractors appears nearly identical to a procurement document published by ICE in 2020, which resulted in a $5.5 million contract between the agency and Barbaricum, a Washington-based defense and intelligence contractor. A new contract has not yet been awarded. ICE spokesperson Jocelyn Biggs told The Intercept, “While ICE anticipates maintaining its threat risk monitoring services, we cannot speculate on a specific timeline for future contract decisions.”ICE already has extensive social media surveillance capabilities provided by federal contractor Giant Oak, which under both Trump’s first term and the Biden administration sought “derogatory” posts about the United States to inform immigration-related decision-making. The goal of this contract, ostensibly, is focused more narrowly on threats to ICE leadership, agents, facilities, and operations.Civil liberties advocates told The Intercept the program had grave speech implications under the current administration. “While surveillance programs like this under any administration are a concerning privacy and free speech violation and I would fight to stop them, the rhetoric of the Trump administration makes this practice especially terrifying,” said Calli Schroeder, senior counsel at the Electronic Privacy Information Center. “Threats to ‘punish’ opponents or deport those exercising 1st Amendment rights combine with these invasive practices to create a real ‘thought police’ scenario.”]]></content:encoded></item><item><title>How did they hack DOGE.com?</title><link>https://doge.gov/workforce?orgId=7cd300eb-cf3f-47f5-90f1-9e66a8bc8d07</link><author>/u/Nice_Wagon_Wheel</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 15:03:04 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Embedded Rustacean Issue #39</title><link>https://www.theembeddedrustacean.com/p/the-embedded-rustacean-issue-39</link><author>/u/TheEmbeddedRustacean</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 14:49:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A better technique to split HTML into structured chunks while preserving the DOM hierarchy.</title><link>https://github.com/carlosplanchon/betterhtmlchunking/</link><author>/u/carlosplanchon</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 14:45:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Switching on Strings in Zig</title><link>https://www.openmymind.net/Switching-On-Strings-In-Zig/</link><author>/u/simon_o</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 14:23:03 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Newcomers to Zig will quickly learn that you can't switch on a string (i.e. ). The following code gives us the unambiguous error message :colorI've seen two explanations for why this isn't supported. The first is that there's ambiguity around string identity. Are two strings only considered equal if they point to the same address? Is a null-terminated string the same as its non-null-terminated counterpart? The other reason is that users of  [apparently] expect certain optimizations which are not possible with strings (although, presumably, these same users would know that such optimizations aren't possible with string).Instead, in Zig, there are two common methods for comparing strings.The most common way to compare strings is using  with :stdmem colorstdmem colorstdmem colorstdmem colorThe implementation for  depends on what's being compared. Specifically, it has an optimized code path when comparing strings. Although that's what we're interested in, let's look at the non-optimized version: T a balen  blenalen  aptr  bptra ba_elem b_elema_elem  b_elemWhether we're dealing with slices of bytes or some other type, if they're of different length, they can't be equal. Once we know that they're the same length, if they point to the same memory, then they must be equal. I'm not a fan of this second check; it might be cheap, but I think it's quite uncommon. Once those initial checks are done, we compare each element (each byte of our string) one at a time.The optimized version, which  used for strings, is much more involved. But it's fundamentally the same as the above with SIMD to compare multiple bytes at once.The nature of string comparison means that real-world performance is dependent on the values being compared. We know that if we have 100  branches then, at the worse case, we'll need to call  100 times. But comparing strings of different lengths or strings which differ early will be significantly faster. For example, consider these three cases: str1  10_000  str2  10_000 
    _  stdmem str1 str2 str1  a"  10_000 str2  a"  10_000
    _  stdmem str1 str2 str1  999_999 str2  1_000_000
    _  stdmem str1 str2For me, the first comparison takes ~270ns, whereas the other two take ~20ns - despite the last one involving much larger strings. The second case is faster because the difference is early in the string allowing the  loop to return after only one comparison. The third case is faster because the strings are of a different length:  is returned by the initial  check.The  takes an enum type and a string value and returns the corresponding enum value or null. This code prints "you picked: blue" std 
    red
    blue
    green
    pink color  stdmetaColorInvalidChoicecolorred  stddebugblue  stddebuggreen  stddebugpink  stddebugIf you don't need the enum type (i.e. ) beyond this check, you can leverage Zig's anonymous types. This is equivalent: std  color  stdmeta
        red
        blue
        green
        pinkInvalidChoicecolorred  stddebugblue  stddebuggreen  stddebugpink  stddebugIt's  obvious how this should perform versus the straightforward  approach. Yes, we now have a  statement that the compiler can [hopefully] optimize, but  still has convert our input, , into an enum.The implementation of  depends on the number of possible values, i.e. the number of enum values. Currently, if there are more than 100 values, it'll fallback to using the same  that we explored above. Thus, with more than 100 values it does the  check PLUS the switch. This should improve in the future.However, with 100 or fewer values,  creates a comptime  which can then be used to lookup the value.  isn't something we've looked at before. It's a specialized map that buckets keys by their length. Its advantage over Zig's other hash maps is that it can be constructed at compile-time. For our  enum, the internal state of a  would look something like:// keys are ordered by length
keys:     ["red", "blue", "pink", "green"];

// values[N] corresponds to keys[N]
values:   [.red, .blue, .pink, .green];

// What's this though?
indexes:  [0, 0, 0, 0, 1, 3];It might not be obvious how  is used. Let's write our own  implementation, simulating the above  state:str keys  values redbluepinkgreen indexes strlen  indexeslen index  indexesstrlenindex  keyslen key  keysindexkeylen  strlenstdmem key str valuesindex
        index Take note that  are ordered by length. As a naive implementation, we could iterate through the keys until we either find a match or find a key with a longer length. Once we find a key with a longer length, we can stop searching, as all remaining candidates won't match - they'll all be too long.  goes a step further and records the index within  where entries of a specific length begin.  tells us where to start looking for keys with a length of 3 (at index 0).  tells us where to start looking for keys with a length of 5 (at index 3).Above, we fallback to using  for any key which is the same length as our target string.  uses its own "optimized" version:a baptr  bptra ba_elem b_elema_elem  b_elemThis is the same as the simple  implementation, minus the length check. This is done because the  within our  loop is only ever called for values with matching length. On the flip side, 's  doesn't use SIMD, so it would be slower for large strings.In my own benchmarks, in general, I've seen little difference between the two approaches. It does seem like  is generally as fast or faster. It also results in more concise code and is ideal if the resulting enum is useful beyond the comparison.You usually don't have long enum values, so the lack of SIMD-optimization isn't a concern. However, if you're considering building your own  at compile time with long keys, you should benchmark with a custom  function based on .We could manually bucket those  branches ourselves, similar to what the  does. Something like:colorlenstdmem colorstdmem colorstdmem colorstdmem colorUghhh. This highlights the convenience of using  to generate similar code. Also, do remember that  quickly discards strings of different lengths, which helps to explain why both approaches generally perform similarly.]]></content:encoded></item><item><title>Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11</title><link>https://www.youtube.com/watch?v=eHa6GhK7L0I</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><category>devops</category><enclosure url="https://www.youtube.com/v/eHa6GhK7L0I?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 13:59:30 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>The History of S.u.S.E</title><link>https://www.abortretry.fail/p/the-history-of-suse</link><author>ibobev</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 13:53:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In 1992, four college students, Roland Dyroff, Thomas Fehr, Hubert Mantel, and Burchard Steinbild in Fürth, Germany were dreaming of independence. They wanted to chart their own paths, make money for themselves, and have fun doing so. They’d had significant exposure to UNIX at university, and there was a new, free, and open source kernel called Linux from Helsinki. At the same time, there was Peter MacDonald’s Linux distribution, the Softlanding System (SLS), and Patrick Volkerding was publishing fixes for SLS and making them freely available via FTP. Somewhat problematically, most UNIX systems and most Linux systems were exclusively English language oriented. These four students were ready to do something for themselves and they began the process of localizing SLS into German.On the 2nd of September in 1992, the quartet founded Gesellschaft für Software und Systementwicklung mbH (or S.u.S.E.). In English that would be Software and System Development, but the contraction S.u.S.E. also bore more than a passing similarity to the name of Konrad Zuse who’d built the world’s first programmable and Turing-complete computer (depending upon how one defines this) the Z3 in May of 1941.Slackware LinuxIn these halcyon days, the young company had no real hierarchy, no regular meetings, no reports, and no bureaucracy. The company operated in a sort of democratic fashion. Being so small, everyone knew everything that was occurring at any given time. If a major decision needed to be made, they’d simply take an informal vote. If they had unanimity, they’d make the change and this meant that the company could change direction in a single night which was when they normally got the most work done.In 1994, S.u.S.E released a fully localized version of Slackware Linux 2.0 with some patches and software incorporated from other Linux distributions, S.u.S.E. Linux 1.0. This release was quickly followed by 1.1.27 in July. This was a Linux system with the XFree86 2.1.1, TCP/IP, NFS, kernel 1.0.9, GNU CC 2.5.8 and 2.4.5, and documentation on disc. The system requirements were a 386 with at least 4MB of RAM, but if one intended to use X11 the RAM requirement was a minimum of 8MB. The system supported ISA, EISA, VLB, and PCI, but it did not support MCA.Most references of version 4.2 state that the distribution was only available in German, but this is incorrect. The initial boot configuration allows a choice of either English or German.After the installation environment is loaded, when init handed off control to a tty, the user was greeted with a logon screen, again in both languages.From there, YaST would take the user through the installation in a well design curses environment.8bit RTL8019At this point, the system was ready to load Linux.After creating a Linux disk partition, the user is required to reboot (though nothing informs you of this). At that point, the partition changes are written to disk. In my case, this meant that I would need to boot from a floppy. There is an option to create a Linux boot floppy, but I chose to use a Win98 boot floppy instead. This essentially runs through the LOADLIN portion for a second time, after which I restarted the setup, loaded the config for all packages, and proceeded with the installation.Here, selecting the main menu will allow the installer to progress to finishing the installation.After login, there’s a brief delay and then we go through the first run setup procedures.That took an incredibly long time on my modest 1990s system.S.u.S.E. Linux version 5 was released in July of 1997, and that was the version to begin the transition of the distribution to RPMs. Slackware’s TGZs were still supported. That release used kernel 2.0.30, Bash 2.0, Qt 1.2, XFree86 3.3 and FVWM 4.0.45. October of that year saw the release of version 5.1 with the most notable changes being the support of FTP package updates and KDE. This version was five CD-ROMs. Four of these were for the installation, and one was for the LiveCD environment. The final version 5 release was on the 10th of September in 1998, version 5.3 with kernel 2.0.35, KDE 1.0, and Netscape Communicator version 4.05.So, here in the installer, we can see a clear evolution from the prior release. Everything is a bit more refined, the installation was far more straight forward, and we can easily see that what has been reported as a complete rebase on Jurix, was truly more of a merger between the Slackware-rooted origin, and the RPM-based Jurix with its automatic build system. We can also see that S.u.S.E. was now evolving into something unique, something gestalt. That said, my installation on a K6-III crashed repeatedly.S.u.S.E. opened a branch office in Oakland in 1997 as well as more offices across Europe. Not long after these expansions came new partnerships with IBM, SAP, Compaq, and Oracle. For IBM and Compaq, this partnership included S.u.S.E. offering 24-hour support services. The same year, SuSE began offering servers pre-installed with S.u.S.E. Linux. For 1998, the company moved their corporate office from Fürth to Nürnberg. In 1999, the company opened a branch office in the UK.The company released version 6.0 on the 21st of December in 1998, and with this release, the company rebranded as SuSE Linux. Version 6 shipped with kernel 2.0.36, and it saw point release up to 6.4 which released on the 9th of March in 2000. Version 6.4 included YaST2 which was graphical.On the 27th of September in 2000, SuSE Linux version 7 was released. SuSE Linux Professional 7.0 consisted of six CD-ROMs and one DVD for those who had a DVD-ROM drive, two floppies for those who couldn’t boot from optical media, a quick install guide, a handbook, a configuration guide, and an applications guide. By this point, like many other commercial Linux distributions, SuSE included StarOffice, Netscape, Acrobat, GIMP, Sane, KDE, and other familiar applications. It had a graphical YaST2 setup tool, ReiserFS and ext2 filesystems, RPM package management, and retailed for $69.99. SuSE Linux Personal lacked many of the proprietary applications and shipped on three CDs. This version saw released for x86, Alpha, PPC, and SPARC. The last of the version 7 systems was 7.3 released on the 23rd of October in 2001.ItaniumBy November of 2000, something around 66% of SuSE’s revenues were from Linux CD-ROM sales at retail, online, over the phone. Roughly 24% was from services, and the remaining 8% or so was from hardware sales. The company had around forty developers working on open source software, and this didn’t include people who handled the compiling and packaging into RPMs (release engineers), or those who handled support services. Beyond this, the company contributed both money and talent to projects like ReiserFS, KDE, and the Linux kernel.Red HatCalderaMandrakeSuSE Linux 8.0 was released on the 22nd of April in 2002 with Linux kernel 2.4.18, KDE3, KOffice, and several other enhancements. The installer was more automated, had better hardware detection, and YaST2 could detect Windows partitions. Upon the detection of a Windows partition, the installer would suggest changes to hard disk allocation for the installation of SuSE. YaST2 was also much better at configuring the X server with SaX2 (SuSE advanced X configuration). On the media side of things, SuSE 8 included applications for audio mixing, editing, and CD writing. These audio applications were aided by the inclusion of ALSA 0.9 (Advanced Linux Sound Architecture) which allowed for the playback of more than one audio stream at a time. Security enhancements also came to SuSE with SuSE Personal Firewall 2, GNU Privacy Guard, OpenSSH, PAM, and IPsec. Version 8 concluded with the release of 8.2 on the 7th of April in 2003.SuSE Linux 9.0 was released on the 15th of October in 2003 with Linux kernel 2.4.21, GNOME 2.2.3, and KDE 3.1.4. This version brought two major changes. First, SuSE Linux 9 was the first distribution to support AMD64. SuSE had reached out to AMD early in the Hammer architecture’s development, and they built an emulator from AMD’s specifications. The company then proceeded to port the entire distribution AMD64 with AutoBuild. When the first hardware arrived at their offices, the system worked flawlessly. The second big addition was support for NTFS (non-root) filesystems.NovellThe first release after acquisition was SUSE Linux 9.1 on the 23rd of April in 2004. This version shipped with kernel 2.6.4, GNOME 2.4, and KDE 3.2.1. SUSE 9.1 brought in some visual changes between the personal and professional editions as well. The personal editions maintained the green theming while the professional editions had blue theming. This was also evident in the default background images.For the first time, with SUSE 9.1, people could download the distribution without any monetary cost. A single CD release was available without support of any kind. For the most part, this was just SUSE Personal without some extra packages.SUSE Linux Enterprise Server 9 was released in August of 2004 and Novell Linux Desktop 9 (built on much the same package base) was released on the 8th of November in 2004. This release saw official support from IBM, HP, Sun, Dell, SGI, Lenovo, and Fujitsu. For the first time, GNOME became the default, but KDE remained present. Naturally, Ximian’s Evolution became Novell Evolution. Given events at Sun, StarOffice was replaced by OpenOffice. With more of the Ximian acquisition in evidence, the availability of software updates were made apparent to the user with a Red Carpet icon.While SUSE had been an enterprise focused distribution for quite a while, on the 4th of August in 2005, Novell announced that the creation of the openSUSE Project. This would allow for a more open development process, and what had previously been the personal edition became a freely available product without professional support. SUSE Linux 10 was released on the 6th of October in 2005 and it was the first SUSE release to follow this model. SUSE Linux 10 shipped with kernel 2.6.13, GNOME 2.12, and KDE 3.4.2.SUSE Linux 10.1 was released on the 11th of May in 2006 with Xgl, AppArmor, Xen, the ext3 filesystem, Firefox 2, and NetworkManager. To my knowledge, this was the first major inclusion of Xen in any Linux system. Both enterprise server and desktop versions of 10 were released in July of 2006, and the Novell Linux Desktop was dropped in favor of SUSE Linux Enterprise Deskstop. Version bumps and the name change to openSUSE arrived with 10.2 on the 7th of Decemer in 2006. The final version 10 release was made on the 4th of October in 2007 with 10.3 featuring kernel 2.6.22, GNOME 2.12, KDE 3.5.1, and XFCE 4.4.1. This version also included a preview release of KDE4 Beta 2. On the enterprise side, updates were shipped in Service Packs with SP4, the final version 10 service pack, having shipped on the 15th of April in 2011.In November of 2006, Microsoft and Novell signed an intellectual property agreement that involved immunity over patent litigation and freed SUSE Linux users over any possibility of lawsuits from Microsoft. This followed claims from Microsoft that Linux systems were in violation of 235 patents without any disclosure over what those violations were. While some in the community viewed this negatively and were quite vocal about it, this did gain some piece of mind for SUSE to include things like Exchange support, MS Office file format support, and interoperability between the two companies’ directory implementations. This would prove to have lasting effects on both the wider Linux ecosystem and on Microsoft. In the following years, improvements in MS Office support would proliferate, Active Directory support would be common, and Microsoft’s Azure would have Linux support. Microsoft even made SQL Server, VS Code, and Powershell available on Linux.On the 22nd of November in 2010, Novell and Attachmate announced that Novell would be acquired by Attachmate. Jeff Hawn, chairman and CEO of Attachmate, commented:The openSUSE project is an important part of the SUSE business. As noted in the agreement announced today, Attachmate plans to operate SUSE as a stand-alone business unit after the transaction closes. If this transaction closes, then after closing, Attachmate Corporation anticipates no change to the relationship between the SUSE business and the openSUSE project as a result of this transaction.On the 30th of April in 2011, Attachmate completed their acquisition of Novell. On Monday, the 2nd of May, eight hundred employees were laid off. openSUSE 12.1 came into the world on the 16th of November in 2011 with kernel 3.1.0, GNOME 3.2, and KDE 4.7. This release also included Snapper for Btrfs snapshot management, and systemd for the init system. System V init was still available to users should they want it. Version 12.2 hit on the 5th of September in 2012 with KDE Plasma 4.8, GNOME 3.4, XFCE 4.10, Firefox 14, Plymouth, GRUB 2, and kernel 3.4.6. Version 12.3, the final for the 12 series, released on the 13th of March in 2013. This version gave version bumps to most software, did away with LiveCDs in favor of USB images, and introduced a “rescue” image with XFCE. openSUSE version 13.1 was released on the 19th of November in 2013 with kernel 3.11.6, GNOME 3.10, KDE Plasma 4.11.2, Firefox 25, LibreOffice 4.1, experimental Wayland support, and YaST rewritten in Ruby. SUSE Linux Enterprise 12 was built on openSUSE 13, and it shipped on the 28th of October in 2014. openSUSE 13.2 followed on the 4th of November in 2014 with version bumps across the board. That same month, the openSUSE project introduced openSUSE Tumbleweed. This is a rolling release distribution with periodic snapshots made into installable images for i686, AMD64, ARM 64, PPC64, IBM zSystems, KVM, Xen, HyperV, and public and private cloud. Live system images are available with GNOME, KDE, XFCE, and rescue.On the 20th of November in 2014, Micro Focus completed its acquisition of Attachmate. Under Micro Focus, SUSE was operated independently from the rest of the organization with its own products. Nils Brauckmann became CEO of SUSE and gained a board seat at Micro Focus.With rolling releases now available, the traditional openSUSE system became openSUSE Leap. The first release of Leap hearkened back to the distribution’s history with version 42.1 becoming available on the 4th of November in 2015. This system was based off of enterprise 12 SP1. With another reference to history, Leap brought about a return to KDE as the default desktop with KDE Plasma 5.8. The 42 series saw two updated releases with 42.2 on the 16th of November in 2016, and 42.3 on the 26th of July in 2017. These later releases were based off of SP2 and SP3 respectively.On the 25th of May in 2018, SUSE released Leap 15 with SUSE Linux Enterprise following on the 25th of June. These were built on the same codebase with GNOME 3.26, LibreOffice 6, GCC 7, Wayland as the display server, and kernel 4.12. Updates of this release are the current offering as of this writing.Micro Focus sold SUSE to EQT AB on the 2nd of July in 2018 for $2.535 billion. Melissa Di Donato became CEO on the 22nd of July in 2019. SUSE finally went public on the Frankfurt stock exchange on the 19th of May in 2021. The company was valued at about $6.1 billion on the first day of trading. Di Donato resigned on the 21st of March in 2023 with Dirk-Peter van Leeuwen being appointed as CEO. On the 17th of August in 2023, EQT delisted SUSE. On the 13th of November in 2023, SUSE merged with Marcel New Lux IV S.A. and the combined entity became SUSE S.A.I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment.]]></content:encoded></item><item><title>A former MS employee openly supports Linux Mint.</title><link>https://www.howtogeek.com/why-linux-mint-is-the-best-windows-11-replacement/</link><author>/u/nguyendoan15082006</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 13:41:03 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>So you want better debug info?</title><link>https://walnut356.github.io/posts/so-you-want-better-debug-info/</link><author>/u/Anthony356</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 13:30:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Transform your codebase into a single Markdown doc for feeding into AI</title><link>https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html</link><author>tesserato</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 13:23:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.
The output for the current repository can be found here.Comprehensive Codebase Documentation: Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format. Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types). Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.Simple Command-Line Interface:  Offers an intuitive command-line interface with straightforward options for customization.If you have Go installed, run go install github.com/tesserato/CodeWeaver@latestto install the latest version of CodeWeaver or go install github.com/tesserato/CodeWeaver@vX.Y.Z to install a specific version.Alternatively, download the appropriate pre built executable from the releases page.If necessary, make the  executable by using the  command:The root directory to scan and document.The name of the output Markdown file.-ignore "<regex patterns>"Comma-separated list of regular expression patterns for paths to exclude.-included-paths-file <filename>File to save the list of paths that were included in the documentation.-excluded-paths-file <filename>File to save the list of paths that were excluded from the documentation.Display this help message and exit.Generate documentation for the current directory:This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).Specify a different input directory and output file:./codeweaver -dir=my_project -output=project_docs.md
This command will process the  directory and save the documentation to .Ignore specific file types and directories:./codeweaver -ignore="\.log,temp,build" -output=detailed_docs.md
This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.Save lists of included and excluded paths:./codeweaver -ignore="node_modules" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md
This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.CodeWeaver is released under the MIT License. See the  file for complete license details.]]></content:encoded></item><item><title>Watchdog ponders why Apple doesn&apos;t apply its strict app tracking rules to itself</title><link>https://www.theregister.com/2025/02/14/apple_app_tracking_probe/</link><author>Logans_Run</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 13:12:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Apple is feeling the heat over its acclaimed iPhone privacy policy after a German regulator's review of iOS tracking consent alleged that the tech giant exempted itself from the rules it enforces on third-party developers.In a preliminary legal assessment of Apple's App Tracking Transparency framework (ATTF), Germany's competition watchdog said the "strict requirements under the ATTF only apply to third-party app providers, not to Apple itself."It is a common tactic to track users across devices – sometimes by a free app dev trying to get paid a premium for "profiled" ads, sometimes by a technical vendor selling services to the ad industry or a content provider that traditionally would have run print ads, like a newspaper. Whether or not users know it is happening and agree to it is probably a better question.Since 2021, when Apple set up the iOS ATTF, app devs have had to ask ‌iPhone‌ and ‌iPad‌ users for their consent before applications could track their activity across other apps such as Facebook. Facebook owner Meta had a problem with this, especially when a 2022 ATTF update began to preclude advertisers from using data across their own platforms. This practice is known as "first-party" tracking as opposed to "third-party" tracking.As one programmatic ad news outlet pointed out, Facebook was actually hit the hardest by the 2022 first-party rules because its software development kit (SDK) "plugged into so many outside apps and ... its attribution pixels [were] littered liberally across the web." This meant that when an iPhone user with the latest ATTF update was on Instagram, data could not be collected on what the user was doing on Facebook, and vice versa.Meta was so worried about this that it chalked up a $10 billion hole in potential ad sales to the move, with CFO Dave Wehner saying in a February 2022 earnings call [PDF] discussing its full-year 2021 results: "We believe the impact of iOS overall as a headwind on our business in 2022 is on the order of $10 billion."Judging by what the Bundeskartellamt said of its preliminary findings, though, Apple's "comprehensive digital ecosystem" appears to suffer less degradation in cross-tracking.The Federal Cartel Office claims that Apple's ATTF defines "tracking" in a way that only covers data processing for advertising purposes across companies – but that these "strict ... rules do not cover Apple's own practice of combining user data across its ecosystem – from its App Store, Apple ID and connected devices – and using them for advertising purposes."Additionally, it says the consent dialogues for Apple's apps differ "substantially" from those of third-party apps. The current design, particularly the wording of the dialogue for Apple's own apps, makes it "more likely that users will consent than that of the ATTF dialogue for third-party apps."It also claims that people using third-party apps will see up to four consecutive consent prompts, while Apple apps receive only a "maximum of two," which do not properly explain that Apple's processing of user data across services constitutes first-party tracking.Apple has been under extra scrutiny in Germany since April 2023, when the regulator determined that Apple had "cross-market significance for competition," meaning it is now subject to "extended abuse control." Apple appealed to the Federal Court of Justice over this on January 28. The court's decision will be delivered on March 18.Andreas Mundt, President of the Federal Cartel Office, said: "Apple operates a comprehensive digital ecosystem, which, through its many services and connected devices, the App Store and Apple ID, provides Apple with extensive access to user data relevant for advertising. Apple uses some of these data to offer advertising space for personalized advertising in its App Store, generating significant revenues."He said advertising was also of "great commercial significance for other companies wishing to offer free apps, some of which compete with Apple's own services, in the App Store." Those providers do not have a "wealth of broad and deep user data to draw on" like Apple does, he added."For us, it is key that users can make a free and informed decision about whether or not their data may be used for personalized advertising at all. The question at hand is whether Apple is allowed to apply stricter criteria to other providers than to itself when it comes to requesting such user consent. In our preliminary view, doing so may amount to unequal treatment and self-preferencing, which are prohibited under competition law."Apple had not responded to a request for comment at the time of publication. ®]]></content:encoded></item><item><title>[R] Doing a PhD in Europe+UK</title><link>https://www.reddit.com/r/MachineLearning/comments/1ip9vuw/r_doing_a_phd_in_europeuk/</link><author>/u/No_Carpenter7252</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 12:51:57 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hey I’m looking for a PhD for 2026 and I was wondering if some of you could recommend some labs. I want something ideally in RL, applied (so no bandits or full theoretical MDPs). It could be something like plasticity, lifelong/continual learning, better architecture/algo for RL, multi-agent or hierarchical RL, RL + LLMs, RL + diffusion, etc .. I’m also even fine with less RL and a bit more ML like better transformer architectures, state space models etc .. What I already had in mind was: - EPFL (LIONS, MLO)- Whiteson's lab at Oxford- Stefano Albrecht's lab in EdinburghI would really appreciate if you could help me extend my list, like this I would not miss labs when I will do my full research in reading their papers, checking what their PhDs, PostDocs and PIs are doing etc..Thank you so much in advance for your help!]]></content:encoded></item><item><title>AI is stifling new tech adoption?</title><link>https://vale.rocks/posts/ai-is-stifling-tech-adoption</link><author>kiyanwang</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 12:45:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I propose that the advent and integration of AI models into the workflows of developers has stifled the adoption of new and potentially superior technologies due to training data cutoffs and system prompt influence.I have noticed a bias towards specific technologies in multiple popular models and have noted anecdotally in conversation and online discussion people choosing technology based on how well AI tooling can assist with its usage or implementation.While it has long been the case that developers have considered documentation and support availability when choosing software, AI’s influence dramatically amplifies this factor in decision-making, often in ways that aren’t immediately apparent and with undisclosed influence.Large language models, especially those on the scale of many of the most accessible, popular hosted options, take humongous datasets and long periods to train. By the time everything has been scraped and a dataset has been built, the set is on some level already obsolete. Then, before a model can reach the hands of consumers, time must be taken to train and evaluate it, and then even more to finally deploy it.Once it has finally released, it usually remains stagnant in terms of having its knowledge updated. This creates an AI knowledge gap. A period between the present and AI’s training cutoff. This gap creates a time between when a new technology emerges and when AI systems can effectively support user needs regarding its adoption, meaning that models will not be able to service users requesting assistance with new technologies, thus disincentivising their use.The cutoff means that models are strictly limited in knowledge up to a certain point. For instance, Anthropic’s latest models have a cutoff of April 2024, and OpenAI’s latest models have cutoffs of late 2023.The influence of the popularisation of AI models has also led to the proliferation of AI slop content, which AI companies are likely trying to avoid introducing in their scraped data sets – potentially increasing this knowledge gap.This knowledge gap doesn’t just stifle the adoption of new frameworks and tooling but also the adoption of new and potentially superior features introduced in updates to already in-use frameworks and tooling as a result of the models not yet including information about them in their training set. This is a lesser issue though, because a technology with existing market adoption is bound to have fanatics who will use it, create examples, and produce blog posts, documentation, and other media about it that will eventually end up in training data.While some models are used in contexts where they are equipped with tooling, such as ChatGPT with its internet search capabilities, their prompts often only instruct them to use this functionality when explicitly requested by a user. Of course, plenty of implementations don’t have any access to any internet query functionality at all, such as Claude via its web interface, which will inform you it will only discuss things included in its training data.Of course, if people are reluctant to adopt a new technology because of a lack of AI support, there will be fewer people likely to produce material regarding said technology, which leads to an overall inverse feedback effect. Lack of AI support prevents a technology from gaining the required critical adoption mass, which in turn prevents a technology from entering use and having material made for it, which in turn starves the model of training data, which in turn disincentivises selecting that technology, and so on and so forth.Consider a developer working with a cutting-edge JavaScript framework released just months ago. When they turn to AI coding assistants for help, they find these tools unable to provide meaningful guidance because their training data predates the framework’s release. This forces developers to rely solely on potentially limited official documentation and early adopter experiences, which, for better or worse, tends to be an ‘old’ way of doing things and incentivises them to use something else.I’ve anecdotally noticed that many AI tools have a ‘preference’ for React and Tailwind when asked to tackle a web-based task, or even to create any app involving an interface at all.With Claude 3.5 Sonnet, which is generally my AI offering of choice given its superior coding ability, my “What personal preferences should Claude consider in responses?” profile setting includes the line “When writing code, use vanilla HTML/CSS/JS unless otherwise noted by me”. Despite this, Claude will  opt to generate new code with React, and in some occurrences even rewrite my existing code into React against my intent and without my consultation.This can be evidenced when asking Claude to use a certain technology, such as Svelte Runes, where it will instead direct the user towards its preferred technology. For example: Build me something with Svelte Runes. I apologize, but I am not able to create Svelte Runes components directly, as the artifact environment only supports certain technologies like React, HTML, and SVG. I can help you build something similar using React, or I can provide you with Svelte Runes code that you can use in your own Svelte project. Which would you prefer?If you’d like to see a React version, I can create something with similar reactive functionality using React’s hooks. Or if you want Svelte Runes code specifically, I can write that for you to use in your own environment.To test if popular models do have built-in bias for React, I logged into the web interfaces of four of the current most popular AI chat platforms via my free account and asked the models to “Create me a web app. It can do anything.”. I repeated this with each of the four models three times in fresh chats with previous chats deleted and any shared context (such as ChatGPT’s memory feature) disabled or cleared between each test.I asked Anthropic’s Claude 3.5 Sonnet with blank personal preferences and artifacts enabled. The first time it opted to create a mood tracking app, the second a reading list manager, and the third time a to-do list. Each time it chose to generate it using React with Tailwind CSS for styling. It generated each project in an artifact which offered an interactive preview directly in the interface.I asked OpenAI’s ChatGPT 4o with both custom instructions and memory disabled. It produced a note-taking app the first time, a random quote generator the second time, and a to-do list the third time. All using React with Tailwind for styling and presented in canvases. These canvases came with a preview button, which, much like with Claude, offered an interactive preview of the React project it built directly in the interface.I asked Google’s Gemini 2.0 Flash with app activity disabled. All three times it built to-do list apps using vanilla HTML, CSS, and JavaScript. Each time, it also noted that I should consider using JavaScript frameworks on the frontend, namely React, Angular, and Vue - in that order.I asked DeepSeek’s DeepSeek-V3, and it returned a to-do list app using vanilla HTML, CSS, and JavaScript. On the second request, it created a project outline for a to-do list app, but this time with Bootstrap, Node.js, Express.js, and SQLite. The third time, it created another outline for a note-taking app but with Node.js, Express.js, and MongoDB.DeepSeek had the greatest variation in technology used but is the least accessible model and didn’t actually ‘create’ the app as requested – merely a general outline for it.These tests show both Claude and ChatGPT have a strong preference for React and Tailwind,  that Gemini has a preference for HTML/CSS/JS but will recommend React and that DeepSeek is much more flexible and varied with its technological choices, though requires more prompting to produce an actual output.I’d suggest that a beginner developer, or someone creating an app exclusively via prompting, is likely to use ChatGPT due to its position in the zeitgeist and use whatever output the model first produces without much second thought – thus influencing their tech selection without their realising.Even if a developer does opt to use another framework or toolset, there is a chance that the model will prod them towards a selection more aligned with its system prompt, even going as far as rewriting their code to its ‘preference’ against user request, as evidenced by Claude ignoring my request for vanilla web technologies.That is assuming that a specific framework or toolset hasn’t already been chosen by the user specifically because they have heard or experienced it being best handled by AI models. Also, while this very much applies to larger technical choices, such as what framework or general tooling a project may choose, it also trickles down into smaller decisions, such as what libraries or modules they may import.I think it is evident that AI models are influencing technology, and that the technologies currently in use – especially those that reached popularity before November 2022, when ChatGPT was released, or that are otherwise in current data sets – will be around for a long time to come, and that AI models’ preferential treatment of them will expand their adoption and lifespan.I think it would be prudent for AI companies to provide more transparent documentation of technology biases in their models, like they disclose that their models can make mistakes. These models are becoming a common part of developer knowledge and decision-making, and we’re letting the training and prompting decisions of OpenAI, Anthropic, etc shape the entire direction of software development.As for further research on this topic, if one had a collection of system prompts over time, it may be possible to compare them to download trends of specified packages and identify correlations. Of course, there are a lot of other influences on the download and adoption of packages, so this may prove difficult and provide data too noisy and influenced by outside variables to be unusable.]]></content:encoded></item><item><title>Random Indian man independently rediscovers Department of Defense 1963 Algorithm</title><link>https://math.stackexchange.com/a/5035173/873735</link><author>/u/DataBaeBee</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 12:15:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: A New Way to Learn Languages</title><link>https://www.langturbo.com/</link><author>sebnun</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 12:08:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Thinking About Taking the 78201X Exam? Read This First!</title><link>https://www.reddit.com/r/kubernetes/comments/1ip966e/thinking_about_taking_the_78201x_exam_read_this/</link><author>/u/lucina_scott</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 12:08:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</title><link>https://www.reddit.com/r/MachineLearning/comments/1ip8lhf/r_scaling_up_testtime_compute_with_latent/</link><author>/u/hiskuu</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 11:32:58 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.This paper on reasoning in latent space at test time is fascinating. I think this approach is becoming a trend and could redefine how we think about reasoning in language models. META FAIR’s work on Large Concept Models also touched on latent reasoning.]]></content:encoded></item><item><title>Walking with filesystems: using the fs.FS interface</title><link>https://bitfieldconsulting.com/posts/filesystems</link><author>/u/AlexandraLinnea</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 11:18:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Learn from Documentation or Book?</title><link>https://www.reddit.com/r/kubernetes/comments/1ip8d2r/learn_from_documentation_or_book/</link><author>/u/nicholle_marvel</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 11:17:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[In 2025, there are numerous books available on Kubernetes, each addressing various scenarios. These books offer solutions to real-world problems and cover a wide range of topics related to Kubernetes.On the other hand, there is also very detailed official documentation available.Is it worth reading the entire documentation to learn Kubernetes, or should one follow a book instead?Two follow-up points to consider: 1. Depending on specific needs, one might visit particular chapters of the official documentation. 2. Books often introduce additional tools to solve certain problems, such as monitoring tools and CI/CD tools.Please note that the goal is not certification but rather gaining comprehensive knowledge that will be beneficial during interviews and in real-world situations.]]></content:encoded></item><item><title>Weekly: Share your victories thread</title><link>https://www.reddit.com/r/kubernetes/comments/1ip84c0/weekly_share_your_victories_thread/</link><author>/u/gctaylor</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 11:00:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Got something working? Figure something out? Make progress that you are excited about? Share here!]]></content:encoded></item><item><title>GitHub - openorch/openorch: AI app platform. A language-agnostic, distributed platform for building microservices-based AI backends.</title><link>https://github.com/openorch/openorch</link><author>/u/Sand4Sale14</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 10:17:51 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How am I just finding out about the OhMyZsh plugin?</title><link>https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/kubectl</link><author>/u/ominouspotato</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 09:56:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[It’s literally just a bunch of aliases but it has made CLI ops so much easier. Still on my way to memorizing them all, but changing namespace contexts and exec-ing to containers has never been easier. Highly recommend if you’re a k8s operator! Would also love to hear what you all use in your day-to-day. My company is looking into GUI tools like Lens but they haven’t bought licenses yet. ]]></content:encoded></item><item><title>We’re living in a new era of techno-feudalism</title><link>https://www.reddit.com/r/artificial/comments/1ip6vfh/were_living_in_a_new_era_of_technofeudalism/</link><author>/u/iamuyga</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 09:27:17 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[The tech broligarchs are the . The digital platforms they own are their “land.” They might project an image of free enterprise, but in practice, they often operate like autocrats within their domains.Meanwhile, ordinary users provide data, content, and often unpaid labour like reviews, social posts, and so on — much like  who work the land. We’re tied to these platforms because they’ve become almost indispensable in daily life.Smaller businesses and content creators function more like . They have some independence but must ultimately pledge loyalty to the platform, following its rules and parting with a share of their revenue just to stay afloat.Why on Earth would techno-feudal lords care about our well-being? Why would they bother introducing UBI or inviting us to benefit from new AI-driven healthcare breakthroughs? They’re only racing to gain even more power and profit. Meanwhile, the rest of us risk being left behind, facing unemployment and starvation.For anyone interested in exploring how these power dynamics mirror historical feudalism, and where AI might amplify them, here’s an article that dives deeper.]]></content:encoded></item><item><title>Released dom_smoothie 0.6.0: A Rust crate for extracting readable content from web pages</title><link>https://github.com/niklak/dom_smoothie/releases/tag/0.6.0</link><author>/u/genk667</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 09:09:51 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Here&apos;s What Devs Are Saying About New GitHub Copilot Agent – Is It Really Good?</title><link>https://favtutor.com/articles/github-copilot-agent/</link><author>/u/ImpressiveContest283</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 08:50:54 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[With the introduction of AI, coding has never been easier. To simplify it further, GitHub Copilot is here to improve the working experience, helping code faster and better. If GitHub Copilot has undergone a transition from being a mere passive suggestion engine for the code to being an active collaborator that simplifies development, Agent Mode takes it one step further. Think of it as an autonomous co-coder, a peer programmer who prods and pushes forward the work on your projects.The New GitHub Copilot Agent ModeAgent Mode is like having a co-coder within the programming team, prioritizing your development activity over that of any other person. It now allows Copilot to: It constantly improves and enhances the code it generates, always searching for optimal solutions.On detection of an error, it automatically assumes the responsibility for spotting and correcting the source, thus killing debugging efforts.It provides you with all terminal commands and facilitates running those commands for more speedy working.It goes to infer subsequent underlying tasks beyond your instructions so all solutions would be comprehensive.An example would be tasking the agent to implement a web application for tracking marathon training. Agent Mode can set up the project structure alone; create the relevant components and even fix runtime errors to provide seamless development.Auto Complete Coding Tasks with easeWith Next Edit Suggestions (NES), GitHub Copilot has introduced a feature of code editing that does editing by predicting and suggesting modifications to existing code. NES accounts for the fact that coding often involves refining code already written. This feature is now available for use along with Visual Studio Code, where developers can enable NES to receive intelligent edit recommendations that thereby improve coding efficiency and accuracy.Rapid Coding and Beginner-FriendlyBy providing high-level instructions, Copilot’s Agent Mode generates the necessary code components, eliminating the need for manual coding. Moreover, Agent Mode iteratively improves the code, optimizing performance and ensuring functionality without human intervention. Beyond the initial instructions, Copilot infers additional necessary tasks, such as setting up game logic and user interfaces, to create a fully functional application.GitHub Copilot Agent vs other coding assistantsCursor is an advanced AI-driven coding tool aimed at boosting developer efficiency.Copilot is designed to blend effortlessly into several integrated development environments (IDEs) such as Visual Studio Code and JetBrains and provide immediate inline code recommendations and auto-completion options.On the other hand, Cursor operates as an independent AI-enhanced editor derived from Visual Studio Code, equipped with features like proactive code generation, intelligent auto-completion, and smart rewriting capabilities.While many users commend Copilot for its seamless integration and intuitive user interface, Cursor stands out for providing a richer integration of AI within its ecosystem.A Glimpse into the Future: Though Agent Mode is already a leap into revolution, GitHub’s Project Padawan hints at an even more exciting future. It aims to create a self-sufficient software engineering (SWE) agent capable of completing entire tasks without human intervention. Just picture a future where you tell Copilot to fix GitHub issues and write whole modules. Features like Issue resolution and continuous learning could transform the very way that developers approach software engineering.Divided Opinions: The other side of the coinAlthough several developers have included the Copilot in their flow, that is not to say they did not face a fair share of challenges. Netizens on X and Reddit are divided over the new AI Agent and have pointed out how the tool creates code or content that sometimes feels generic or shallow. Importantly, the AI-generated output needs several thorough checks against project requirements and coding standards.The Agent Mode of GitHub Copilot is set to change the landscape of software development by relieving developers from mundane tasks such as debugging, documentation, or testing. This will give developers more time and energy to concentrate on higher-level innovation and creative problem-solving. Will it take over programmers or not is a topic up for another debate..]]></content:encoded></item><item><title>Deprecated APIs</title><link>https://www.reddit.com/r/kubernetes/comments/1ip6b1z/deprecated_apis/</link><author>/u/LocksmithRound9835</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 08:44:48 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Has anyone created a self service solution for application teams to find out manifests leveraging deprecated APIs? Solution like kubent etc need developers to download binaries and run commands against namespaces.   submitted by    /u/LocksmithRound9835 ]]></content:encoded></item><item><title>HandBrake 1.9.1 released (video transcoder)</title><link>https://github.com/HandBrake/HandBrake/releases/tag/1.9.1</link><author>/u/gabriel_3</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 08:36:46 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ricochet: Peer-to-peer instant messaging system built on Tor hidden services (2017)</title><link>https://github.com/ricochet-im/ricochet</link><author>philonoist</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 08:34:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Linux kernel cgroups writeback high CPU troubleshooting</title><link>https://dasl.cc/2025/01/01/debugging-our-new-linux-kernel/</link><author>mesto1</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 08:30:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Read on to learn how we used network packet captures and BPF to debug web server performance, ultimately uncovering a Linux kernel performance issue. This investigation was a collaboration between myself and my colleagues.We’ve been upgrading the operating system from CentOS to Ubuntu on hosts across our fleet. Our CentOS hosts run an outdated Linux kernel version (3.10), whereas our Ubuntu hosts run a more modern kernel version (6.8). In August 2024, we began rolling out the Ubuntu upgrade across our Apache web servers. When we migrated larger portions of our fleet to Ubuntu, we began seeing elevated listen overflow errors. This elevated error rate prompted us to roll back the Ubuntu upgrade while we debugged:Apache listens on a socket for incoming web requests. When incoming requests arrive more quickly than Apache can serve them, the queue of requests waiting to be served grows longer. This queue is capped to a configurable size. When the queue overflows its maximum size, we have a .Listen overflows are a symptom of either of two things: the rate of incoming web requests is too high, and / or Apache is serving web requests too slowly.Each listen overflow that occurs means we failed to serve a web request. This can result in user-facing errors. Furthermore, if the listen overflows are a symptom of web requests being served slowly, it means users may be experiencing slow page loads.Listen overflows occurred a few minutes after a newly autoscaled web server was pooled. They did not tend to recur subsequently. Furthermore, web requests had elevated latency during this same time period:We believed listen overflows were occurring because web requests served in the first few minutes after the host was pooled were being served unusually slowly.Log lines,  timing information, and  flame graphs showed that network operations were executing particularly slowly. But investigating further, we found evidence that the network was performing normally. Instead, PHP had seemingly stalled for over a second. The below log line indicates that a memcached SET command was slow, but network packet captures on both the client and the server that we analyzed in Wireshark indicate that the SET command experienced normal network latency. The client waited over 1 second before sending the subsequent GET command, as if our PHP script stalled after the packet was received but before we recorded the elapsed time.[Tue Sep 24 21:20:54 2024] [info] Memcached operation exceeded 20ms :operation="set"  key="warmup_key_7746_2"
  
  
    Is it due to elevated system CPU? (yes) #Adding on to the evidence pointing away from network problems, we saw a large spike in system CPU usage about four minutes after newly autoscaled hosts were booted. If we waited until after this spike in system CPU to pool the hosts, we experienced no listen overflows. This spike in system CPU only occurred on Ubuntu hosts – CentOS did not have this problem:This made us realize that the problems we were seeing were unrelated to pooling the hosts – the spike in system CPU occurred regardless of whether the hosts were pooled. During the spike in system CPU, we saw logs in :workqueue: inode_switch_wbs_work_fn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUNDAnd  showed kernel workers were using lots of CPU in a function :We made a  recording demonstrating that when system CPU spikes, the kernel is busy inside :
  
  
    Why did we initially suspect the network? #Our logs and profiling tools showed that network operations were executing particularly slowly. However, when CPU is busy, network operations may appear to slow down disproportionately. Network calls require CPU context switches (I think?). After a process blocked on network receives a response, the process may spend a long time waiting in the CPU run queue before it gets scheduled again when the CPU is busy. Network operations that appear slow at the user space level may be a symptom of CPU busyness.Although there can be cases where this is not true, it has been my experience that when the network is the cause of slowness, CPU usage on client hosts is often lower than normal. When the client is blocked waiting for the network, it is often more idle. In retrospect, perhaps the fact that elevated CPU was one of the symptoms we were seeing should have pointed us away from network issues.We believed something in the kernel function  was causing elevated system CPU. This function is in a file , which contains functions related to the writeback functionality of the Linux page cache. We knew that system CPU was elevated about four minutes after new hosts were booted. One of the first things a new host does is download the latest code – this is part of our host bootstrapping process and involves writing thousands of files to disk. We wondered if the process of flushing the dirty pages to disk was causing the elevated system CPU. While we did not see elevated disk write metrics during the system CPU spike, we decided to test this theory. We added a  call after the  command that downloads the code on new hosts. In theory, that should synchronously write the dirty pages in the page cache to disk. Perhaps by controlling when the page cache was flushed to disk, we could control when the spike in system CPU occurred and ensure that it occurred before we pooled the host. This attempt, however, was unsuccessful. We saw no spike in system CPU when calling . Furthermore, we still saw the spike in system CPU a minute or two later.We were back to the drawing board. As we mentioned above, one of the first things a new host does is download the latest code. This process is called , and it runs as a systemd oneshot service. We found something perplexing: if we prevented the  service from running, we never saw the spike in system CPU. And if we subsequently ran the  service manually, we would see the spike in system CPU a couple minutes later. This implies that something in  was the cause of the issues. However, if we ran the  command in  manually in our interactive shell, we saw no spikes in system CPU. This apparent contradiction led us on a wild goose chase of trying to determine if some systemd service that was dependent on  was the cause of the elevated system CPU, or if there was some subtle difference in the way we were running the commands in our shell versus how systemd was running the commands.
  
  
    Is it inode cgroup switching? (yes) #Each of the thousands of files that is written by the  service has a corresponding index node, also known as an inode, that the kernel uses to store file metadata. Control groups, also known as cgroups, are a feature of Linux which allows for setting per process limits on system resources. For example, cgroups allow us to limit a given process from consuming too much memory, disk I/O, network bandwidth, etc. Every process belongs to a cgroup. Cgroups form a tree-like hierarchical structure. Processes in a given cgroup are given limits both by the cgroup to which they belong and that cgroup’s parents.In the context of cgroups, page cache writeback is tracked at the inode level. A given inode is assigned to whichever cgroup contains the process that is responsible for the majority of writes to the inode’s file. If a new process starts writing a lot to a file, the file’s inode may switch to the new process’s cgroup. Likewise, if a process managed by systemd is terminated, systemd will remove the process’s cgroup, at which point any inodes assigned to the process’s cgroup will be moved to the parent cgroup.The  that we believed was causing the elevated system CPU is responsible for moving an inode from one cgroup to another in the context of writeback. We got more insight by running this  command on a newly booted host:% sudo bpftrace -e '
tracepoint:writeback:inode_switch_wbs {
  printf(
    "[%s] inode is switching! inode: %d old cgroup: %d new cgroup: %d\n",
    strftime("%H:%M:%S", nsecs),
    args->ino,
    args->old_cgroup_ino,
    args->new_cgroup_ino
  );
}'
During the spike in system CPU, we saw thousands of these lines printed out by the  command:[20:49:30] inode is switching! inode: 3730800 old cgroup: 22438 new cgroup: 88
[20:49:30] inode is switching! inode: 3730799 old cgroup: 22438 new cgroup: 88
...

Each line corresponds to a file written by  that was switching from ’s dying cgroup to the parent cgroup. The old cgroup identifier ( corresponds to ’s cgroup. The new cgroup identifier () corresponds to the parent cgroup.The  command prints out data from a kernel tracepoint in the Linux kernel’s writeback code. The fields available to print in this tracepoint can be viewed via:% sudo cat /sys/kernel/debug/tracing/events/writeback/inode_switch_wbs/format
name: inode_switch_wbs
ID: 886
format:
    field:unsigned short common_type;   offset:0;   size:2; signed:0;
    field:unsigned char common_flags;   offset:2;   size:1; signed:0;
    field:unsigned char common_preempt_count;   offset:3;   size:1; signed:0;
    field:int common_pid;   offset:4;   size:4; signed:1;

    field:char name[32];    offset:8;   size:32;    signed:0;
    field:ino_t ino;    offset:40;  size:8; signed:0;
    field:ino_t old_cgroup_ino; offset:48;  size:8; signed:0;
    field:ino_t new_cgroup_ino; offset:56;  size:8; signed:0;

print fmt: "bdi %s: ino=%lu old_cgroup_ino=%lu new_cgroup_ino=%lu", REC->name, (unsigned long)REC->ino, (unsigned long)REC->old_cgroup_ino, (unsigned long)REC->new_cgroup_ino
We found that when we added a  to the end of the script that  executes, we could delay the spike in system CPU by one hour. Because systemd only removes a service’s cgroup when its process exits, the  delayed when inodes switched from one cgroup to another.We found a systemd directive that allows us to turn off certain cgroup accounting features: . If either the  or  controllers are disabled, the kernel will not perform cgroup writeback or any of the related accounting and cgroup switching. We found that by creating a systemd slice with those controllers disabled and configuring ’s unit file to use that slice, we could no longer reproduce the elevated system CPU. We had solved our performance issue.No more system CPU spike and no more listen overflows:

  
  
    Do we have a minimal reproduction script? #We came up with a minimal reproduction of the issue:$ sudo mkdir -p /var/random-files && sudo systemd-run --property=Type=oneshot bash -c 'dd if=/dev/urandom bs=1024 count=400000 | split -a 16 -b 1k - /var/random-files/file.'This command creates 400,000 files, each consisting of 1,024 random bytes. The files have names like /var/random-files/file.aaaaaaaaaaaaaaaa and /var/random-files/file.aaaaaaaaaaaaaaab. This command is run as a systemd oneshot service. Within anywhere from 30 seconds to 3 minutes after this command finishes, we see a spike in system CPU. Viewing  will confirm this (press  to show kernel threads in ) – we see kernel workers using lots of CPU in the function .
  
  
    Why wasn’t CentOS affected? #The initial release of cgroups, known as cgroups v1, was in kernel version 2.6.24. Cgroups v1 has since been replaced by a new implementation: cgroups v2. Cgroups v2 was officially released in kernel version 4.5. Our old CentOS operating system used kernel version 3.10. We believe this inode switching CPU issue is related to the  and  controllers introduced in cgroups v2. Thus CentOS, which uses cgroups v1, is not vulnerable to this issue.
  
  
    Why couldn’t we reproduce when running rsync manually? #Recall that when we ran the  command from  manually in our interactive shell, we saw no spike in system CPU. It turns out that each interactive ssh session you have open creates its own cgroup. Below is the output of  on a web server on which I have two interactive ssh sessions open. One session is running a  command, and the other session is running . The two cgroups are called  and :% sudo systemd-cgls
Control group /:
-.slice
├─user.slice
│ └─user-10101.slice
│   ├─user@10101.service …
│   │ └─init.scope
│   │   ├─1710746 /lib/systemd/systemd --user
│   │   └─1710793 (sd-pam)
│   ├─
│   │ ├─1708943 sshd: dleibovic [priv]
│   │ ├─1711073 sshd: dleibovic@pts/0
│   │ ├─1711171 -zsh
│   │ └─1716022 sleep 100
│   └─
│     ├─1780667 sshd: dleibovic [priv]
│     ├─1781414 sshd: dleibovic@pts/1
│     ├─1781577 -zsh
│     └─1791367 systemd-cgls
...
These  cgroups stick around until you terminate your ssh session. After terminating your ssh session, systemd removes the corresponding cgroup. With this insight, we tested terminating the interactive ssh session after manually running the  commands. Sure enough, about 2 minutes after we terminated the ssh session, we saw the big spikes in system CPU caused by inode cgroup switching.We suspected that this performance issue was caused by either the hypervisor or the kernel. We shared our findings with Canonical, the company behind Ubuntu. Canonical confirmed that it is a kernel issue that was likely introduced by a Linux kernel commit from 2021. More details are available in the public bug report, in which I have commented. We are hopeful that Canonical will engage with the Linux kernel developers and eventually fix this performance issue.]]></content:encoded></item><item><title>GitHub - gopher-fleece/gleece: Gleece - bringing joy and ease to API development in Go!</title><link>https://github.com/gopher-fleece/gleece</link><author>/u/h_talker</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 08:21:26 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I come from the Node.js / TypeScript ecosystem and have recently started working with Go as the technology for high-performance and CPU-intensive microservices.I was missing the TSOA approach and code styling for REST API implementation, meaning writing ordinary functions and declaring HTTP info, where the framework handles the rest - routing, validation, documentation, authentication, etc.So... I have created, with my colleague Yuval, the Gleece project that does exactly that.Since Go doesn't provide an annotation mechanism (as TSOA uses in JS), we used comments for HTTP info. To make it easier to work with, we also created a VS Code extension to highlight and provide visibility to the HTTP info.Feel free to use it, and I would love any feedback 🙂]]></content:encoded></item><item><title>How to build a distributed request throttler on client side?</title><link>https://www.reddit.com/r/golang/comments/1ip5v2r/how_to_build_a_distributed_request_throttler_on/</link><author>/u/Polonium_Braces</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 08:10:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi everyone, I'm integrating my APIs with a new system (server to server api calls) - the catch being the downstream server can't handle more than 50 RPS, and would ultimately die/restart after this. I'm looking for a way to limit my requests from the client - I don't want to outright deny them from a server side rate limiter, but just limit them on client end to not breach this 50 RPS threshold. I could do this using channels, but issue is my API would be running in , so need of a distributed system. I'm not able to think of good approaches, any help would be appreciated here! I use GO mainly.]]></content:encoded></item><item><title>Advancing Open Source Gateways with kgateway</title><link>https://www.cncf.io/blog/2025/02/05/advancing-open-source-gateways-with-kgateway/</link><author>/u/dshurupov</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 08:10:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Posted on February 5, 2025
		At KubeCon NA 2024, Solo.io announced its intention to donate the Gloo Gateway open source project to the CNCF, to benefit the broader cloud native ecosystem. In case you are not familiar with Gloo Gateway, it is the most mature and widely deployed Envoy-based gateway in the market today. Built on open source and open standards, Gloo Gateway is Kubernetes-native and implements the Kubernetes Gateway API. Today, we’d like to update you on some exciting developments since the announcement.What is a gateway, and why donate Gloo Gateway to CNCF?If you are using Kubernetes, you most likely need a gateway to control traffic going into or out of your cluster. Gateways are a critical component of the Kubernetes ecosystem, which is why the Kubernetes community collectively developed the Gateway API based on lessons learned from the Ingress API and Istio. With the growth of AI workloads and the rising need for traffic and cost control for calling LLM providers, gateways play an even more critical role in controlling and securing all-direction traffic—north-south, east-west, or inter-service communication.While Gloo Gateway is the most mature and feature-rich Envoy-based gateway today, we believe that under vendor-neutral governance, the project’s adoption and ecosystem integration will reach new heights. Donating the project to the CNCF will expand the contributor base, foster innovation across organizations, and provide a battle-tested, feature-rich, vendor-neutral gateway project to the diverse global CNCF user community.In November 2024, we moved the Gloo open source repository to the k8sgateway repository as a preparatory step for the donation. After working with the CNCF TOC, Kubernetes SIG-Network, and steering committee leaders, we renamed the project to kgateway.Given Gloo’s large adopter base, we believe kgateway qualifies as a CNCF incubation project. Due to the lengthy due diligence process for incubation projects, we decided to donate it as a CNCF sandbox project instead. We look forward to working with the CNCF TOC and TAG Network leaders for the upcoming sandbox review.Working closely with the maintainer community, we established governance for the project that rewards maintainership while ensuring no single company has a controlling stake. Using the git-vote bot for transparency, we successfully held our first governance vote. Out of 10 eligible voters (including 4 maintainers outside Solo.io), nine voted favorably on the proposed governance PR:In addition to renaming the project, recruiting maintainers, and establishing governance, we’ve been focused on: Developing buildable, vendor-neutral artifacts, set to launch in the coming weeks.Improved Development Velocity: Establishing robust pipeline checks for PRs, including linting, Kubernetes Gateway API conformance tests, and end-to-end testing. Ensuring the project remains highly extendable, aligning with core design principles of kgateway and Envoy.A shout-out to our core maintainers and contributors for laying this solid foundation for innovation.You may notice that the Gloo open source repository still exists. This is temporary during the transition period. With Gloo’s large open source user base, we understand that migrating from Gloo to kgateway takes time. In upcoming releases, we plan to deprecate the Gloo repository to focus all open-source efforts on kgateway.We aim to make kgateway the most popular gateway for all-direction traffic—north-south, east-west, or inter-service communication. In addition to implementing the latest Kubernetes Gateway API features, we’re prioritizing:Traffic control for AI workloadsAs workloads like AI agents run on Kubernetes clusters, questions arise:How securely do they connect to LLM services such as OpenAI or Gemini?Are these services local or external, with usage-based costs?How should credentials and backup LLMs be managed?Do you want each developer to develop prompt guard and enrichment in their own AI workloads? Kgateway simplifies these challenges with two proposed declarative APIs for routing traffic to LLM providers while applying advanced policies such as secret management, backup LLMs, prompt guard or enrichment, and more. Refer to the enhancement proposal for more information.The Gateway API Inference extension, sponsored by Kubernetes SIG-Network, focuses on extending the Kubernetes Gateway API with inference-specific routing extensions. It introduces the concept of an “InferencePool” (composed of one or more inference pods), enabling application developers to effectively route requests based on AI workload requirements. Daneyon Hanson has been leading our work for interference extensions and we are proud to see him nominated as a maintainer on the Gateway API Inference extension project as a result!Providing advanced Layer 7 features for Istio in ambient modeAmbient mode splits Istio functionality into a secure overlay layer (ztunnels) and a Layer 7 processing layer (waypoint). With kgateway as the waypoint proxy, users gain advanced L7 features such as request transformation, retries, and traffic control for AI workloads connecting to LLM services. This pluggability ensures consistent operational experiences for north-south and inter-service traffic. Steven Landow, who is a maintainer on both Istio and kgateway, is leading this effort.To explore additional roadmap initiatives or propose updates, please refer to our roadmap document.Are you interested in exploring kgateway, or are you already a Gloo Gateway user? We’d love to hear from you and shape the future of kgateway together. If working on cutting-edge cloud-native projects excites you, join us as a contributor! Connect with us via:Let’s work together and build kgateway into the future of cloud connectivity!]]></content:encoded></item><item><title>Anyone can push updates to the doge.gov website</title><link>https://www.404media.co/anyone-can-push-updates-to-the-doge-gov-website-2/</link><author>mahkeiro</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 07:31:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA["THESE 'EXPERTS' LEFT THEIR DATABASE OPEN."]]></content:encoded></item><item><title>Benchmarking vision-language models on OCR in dynamic video environments</title><link>https://arxiv.org/abs/2502.06445</link><author>ashu_trv</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 07:26:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Trait upcasting stabilized in 1.86</title><link>https://github.com/rust-lang/rust/pull/134367</link><author>/u/hpxvzhjfgb</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 07:10:52 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Extensible WASM Applications with Go</title><link>https://go.dev/blog/wasmexport</link><author>ingve</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 07:08:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
      Cherry Mui
      13 February 2025
      Go 1.24 enhances its WebAssembly (Wasm) capabilities with the
addition of the  directive and the ability to build a reactor
for WebAssembly System Interface (WASI).
These features enable Go developers to export Go functions to Wasm,
facilitating better integration with Wasm hosts and expanding the possibilities
for Go-based Wasm applications.WebAssembly and the WebAssembly System InterfaceWebAssembly (Wasm) is a binary instruction format
that was initially created for web browsers, providing the execution of
high-performance, low-level code at speeds approaching native performance.
Since then, Wasm’s utility has expanded, and it is now used in various
environments beyond the browser.
Notably, cloud providers offer services that directly execute Wasm
executables, taking advantage of the
WebAssembly System Interface (WASI) system call API.
WASI allows these executables to interact with system resources.Go first added support for compiling to Wasm in the 1.11 release, through the
 port.
Go 1.21 added a new port targeting the WASI preview 1 syscall API through the
new  port.Exporting Go Functions to Wasm with Go 1.24 introduces a new compiler directive, , which allows
developers to export Go functions to be called from outside of the
Wasm module, typically from a host application that runs the Wasm runtime.
This directive instructs the compiler to make the annotated function available
as a Wasm export
in the resulting Wasm binary.To use the  directive, simply add it to a function definition://go:wasmexport add
func add(a, b int32) int32 { return a + b }
With this, the Wasm module will have an exported function named  that
can be called from the host.This is analogous to the cgo  directive,
which makes the function available to be called from C,
though  uses a different, simpler mechanism.A WASI reactor is a WebAssembly module that operates continuously, and
can be called upon multiple times to react on events or requests.
Unlike a “command” module, which terminates after its main function finishes,
a reactor instance remains live after initialization, and its exports remain
accessible.With Go 1.24, one can build a WASI reactor with the  build
flag.$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm
The build flag signals to the linker not to generate the  function
(the entry point for a command module), and instead generate an
 function, which performs runtime and package initialization,
along with any exported functions and their dependencies.
The  function must be called before any other exported functions.
The  function will not be automatically invoked.To use a WASI reactor, the host application first initializes it by calling
, then simply invoke the exported functions.
Here is an example using Wazero, a Go-based Wasm runtime
implementation:// Create a Wasm runtime, set up WASI.
r := wazero.NewRuntime(ctx)
defer r.Close(ctx)
wasi_snapshot_preview1.MustInstantiate(ctx, r)

// Configure the module to initialize the reactor.
config := wazero.NewModuleConfig().WithStartFunctions("_initialize")

// Instantiate the module.
wasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)

// Call the exported function.
fn := wasmModule.ExportedFunction("add")
var a, b int32 = 1, 2
res, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))
c := api.DecodeI32(res[0])
fmt.Printf("add(%d, %d) = %d\n", a, b, c)

// The instance is still alive. We can call the function again.
res, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))
fmt.Printf("add(%d, %d) = %d\n", b, c, api.DecodeI32(res[0]))
The  directive and the reactor build mode allow applications to
be extended by calling into Go-based Wasm code.
This is particularly valuable for applications that have adopted Wasm as a
plugin or extension mechanism with well-defined interfaces.
By exporting Go functions, applications can leverage the Go Wasm modules to
provide functionality without needing to recompile the entire application.
Furthermore, building as a reactor ensures that the exported functions can be
called multiple times without requiring reinitialization, making it suitable
for long-running applications or services.Supporting rich types between the host and the clientGo 1.24 also relaxes the constraints on types that can be used as input and
result parameters with  functions.
For example, one can pass a bool, a string, a pointer to an , or a
pointer to a struct which embeds  and contains supported
field types
(see the documentation for detail).
This allows Go Wasm applications to be written in a more natural and ergonomic
way, and removes some unnecessary type conversions.While Go 1.24 has made significant enhancements to its Wasm capabilities,
there are still some notable limitations.Wasm is a single-threaded architecture with no parallelism.
A  function can spawn new goroutines.
But if a function creates a background goroutine, it will not continue
executing when the  function returns, until calling back into
the Go-based Wasm module.While some type restrictions have been relaxed in Go 1.24, there are still
limitations on the types that can be used with  and
 functions.
Due to the unfortunate mismatch between the 64-bit architecture of the client
and the 32-bit architecture of the host, it is not possible to pass pointers in
memory.
For example, a  function cannot take a pointer to a struct that
contains a pointer-typed field.The addition of the ability to build a WASI reactor and export Go functions to
Wasm in Go 1.24 represent a significant step forward for Go’s WebAssembly
capabilities.
These features empower developers to create more versatile and powerful Go-based
Wasm applications, opening up new possibilities for Go in the Wasm ecosystem.]]></content:encoded></item><item><title>Zed now predicts your next edit with Zeta, our new open model</title><link>https://zed.dev/blog/edit-prediction</link><author>ahamez</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 06:50:47 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Zed is built for speed. We've always strived for an editing experience that feels . But what's faster than instant?  A tool that anticipates your next move. That's why we're introducing edit prediction in Zed, powered by Zeta, our new open source model.Here's a quick walkthrough:Edit Prediction in action.As you work, Zed now predicts your next edit, so you can apply it just by hitting . Once you accept a prediction, you can perform multiple follow-up edits by pressing  repeatedly, saving you time and keystrokes. We've received  of requests for this functionality, and we've poured our hearts into making it feel like a natural extension of the Zed experience.You can use Zeta for free during this public beta by downloading Zed and signing in with your GitHub account. Edit prediction won't be free forever, but right now we're just excited to share and learn.Edit prediction transforms  into a magical, universal key. But what about the existing uses of , such as indenting lines? And what happens when there's both an edit prediction  suggestions from your language server? We didn't want a powerful new feature to come at the expense of the existing editing experience in Zed.When language server completions are visible, Zed won't preview the predicted edit until you press  or . As soon as you press the modifier, Zed previews the edit and hides the menu to enable an unobstructed review. On macOS, you can just hit  to confirm, or back out by releasing  to restore the language server completions menu.On Linux,  is often reserved by the window manager, so we offer  as an alternative default. We chose  because it's on the QWERTY home row and represents rightward movement in Vim. If your Linux window manager doesn't claim , you're free to use that binding as well.Zeta is derived from Qwen2.5-Coder-7B, and is fully open source, including an open dataset. If you're working in an open source repository, we'd love your help improving Zeta by contributing to its dataset. Please bear with us initially, as we will be reviewing the submitted data before publishing to ensure everyone's safety and privacy. We're excited to figure this out and see a community effort form to make edit prediction better everywhere, most especially in Zed!How Zed's Open-Source Edit Predictions WorkRichard Feldman and Antonio Scandurra talk about how Zed's new Edit Prediction feature works under the hood. This includes how the Zed team developed and open-sourced both the code and the dataset behind the fine-tuned Zeta language model that powers it!Most coding models are trained on a "fill in the middle" task. You give them a prefix and a suffix, and they generate what goes in between.This works for completing text at the cursor, but we wanted Zeta to predict edits at arbitrary locations, which doesn't fit into this structure.In our experience, models aren't very good at producing granular edits, but they do excel at rewriting larger chunks of code. So that's where we started: given a list of recent edits and the cursor position, we asked the model to rewrite a snippet of text around the cursor, incorporating one or more edit predictions in the rewritten text.Before writing a single line of code, we created a set of tests to check if our idea worked. Testing the output of a large language model is tricky because, on every run, you can get slightly different results even when feeding it the exact same input. This can be mitigated by using a temperature of  and, for providers that support it, providing a seed for the RNG.That said, code can often be written in many different but equally valid ways. So even when Zeta's output differs from our expected answer, it might still be doing exactly what we want—just taking a different path to get there. This makes traditional unit testing approaches particularly challenging when working with LLMs.This led us to take a different approach—instead of strict assertions, we used a larger LLM to evaluate Zeta's edits. By writing our test assertions in plain English and having Claude check if the results matched our intent, we could validate that Zeta was making sensible edits, even when its exact output differed between runs. This ended up being much more practical than trying to make brittle assertions about specific tokens.Here's an example taken from our eval suite:We took our first stab at making those tests pass by using Qwen2.5-Coder-32B and giving it clear instructions for which types of edits we wanted it to predict. Here's the initial system prompt we used and you can look through the history to see how we kept changing it to pass the eval suite.This worked out surprisingly well for the first 4-5 evals. However, as soon as we introduced more, we started noticing that it got harder and harder to pass them all consistently. Changing the prompt caused the new evals to pass, but made the old ones fail. Overall, it felt like a flaky process and we didn't feel confident this would lead to the system being robust enough to be used in production.Moreover, using a 32b model wasn't really compatible with our strict latency requirements (more on that later).After playing around with different approaches, we decided to go with supervised fine-tuning using Unsloth and LoRA. The idea was to teach Zeta two key things: figuring out what changes a developer might want next based on their recent edits, and then actually applying those changes cleanly to the code without introducing weird side effects.But we had a classic chicken-and-egg problem—we needed data to train the model, but we didn't have any real examples yet. So we started by having Claude generate about 50 synthetic examples that we added to our dataset. We then used that initial fine-tune to ship an early version of Zeta behind a feature flag and started collecting examples from our own team's usage.This approach let us quickly build up a solid dataset of around 400 high-quality examples, which improved the model a lot! However, we kept running into edge cases that would trip the model up. The most annoying ones were when Zeta was working with a small piece of code in a larger file—it would sometimes get confused and make random deletions or insertions that had nothing to do with what the user was trying to do, and it didn't seem like adding more examples steered the model away from those mistakes.To handle these edge cases, we conducted another pass using direct preference optimization (DPO). This technique let us go beyond simply showing the model what good edits look like—we could also teach it what edits . With DPO, we could fine-tune Zeta by providing both positive and negative examples, helping it learn the subtle differences between helpful and problematic edits.We found that just ~150 carefully selected examples were enough to significantly improve Zeta's behavior on tricky cases. Of course, we think we can make it even better by expanding our training data with more diverse examples, and we're excited to keep pushing the boundaries here.Like every feature in Zed, latency was a critical factor for edit prediction. When we started, we set aggressive performance targets: predictions should be delivered in under 200ms for the median case (p50) and under 500ms for the 90th percentile (p90). The challenge was that rewriting complete excerpts, while enabling multi-location edits, requires generating significantly more tokens than simple fill-in-middle approaches. Initially, this put us way over our latency budget.However, there's a fascinating insight about how edit predictions work. When we rewrite a text snippet, the output often mirrors the input closely, with changes concentrated in specific spots. This pattern lets us parallelize token generation by using the input as a reference—a technique known as speculative decoding. We use n-gram search to identify promising jumping-off points in the input where we can start parallel token generation, giving us a significant speedup without sacrificing quality.For edit predictions to feel responsive, we needed to solve multiple latency challenges in parallel. As discussed above, we tackled the model execution time through speculative decoding, but serving the model at scale presented its own set of hurdles. This was by far the most compute-intensive problem our team has ever tackled.A few weeks out from launch, we ran a brief competitive process, and we ended up being really impressed with Baseten. Their performance engineers quickly optimized our open source model to run on their flexible infrastructure, achieving our target latencies while letting us retain full visibility into the details of the deployment, both for the Zed team and the entire Zed community. We plan to follow up with a guest post about what they learned optimizing our model.Latency is not just a function of compute; network transit time is a key driver of perceived speed. To cooperate with the laws of physics, we're launching with GPUs in both North America and Europe, and we hope to add more regions soon. We're also using Cloudflare Workers to handle your requests in a data center located as close to you as possible.There's plenty more to explore to make edit predictions more powerful. We'll be fast-following with more experiments. We plan on sending more kinds of context to the model and continuing our experiments with fine-tuning, and we'll share updates as we grow and evolve the Zeta dataset.We've learned a lot since we launched Zed AI last fall. The world is changing fast, and we're having a blast exploring and learning to build features that developers love. We're also excited to build with AI the Zed way. From our early days, we've been proponents of an open approach to building software, even when hard, and we see no reason to change that approach when it comes to working with AI. We hope you'll join us as a user, a contributor, or an employee, as we hustle to ship a golden future.]]></content:encoded></item><item><title>Type safe Go money library</title><link>https://www.reddit.com/r/golang/comments/1ip4nxm/type_safe_go_money_library/</link><author>/u/HawkSecure4957</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 06:43:57 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello, community I have been working on money library that is type safe, it's in beta and haven't been test against production. But I would like to share it with community.]]></content:encoded></item><item><title>Anyone Can Push Updates to the DOGE.gov Website</title><link>https://www.404media.co/anyone-can-push-updates-to-the-doge-gov-website-2/</link><author>Jason Koebler</author><category>dev</category><enclosure url="https://www.404media.co/content/images/2025/02/Screen-Shot-2025-02-13-at-10.38.20-PM.png" length="" type=""/><pubDate>Fri, 14 Feb 2025 06:42:14 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[The doge.gov website that was spun up to track Elon Musk’s cuts to the federal government is insecure and pulls from a database that can be edited by anyone, according to two separate people who found the vulnerability and shared it with 404 Media. One coder added at least two database entries that are visible on the live site and say “this is a joke of a .gov site” and “THESE ‘EXPERTS’ LEFT THEIR DATABASE OPEN -roro.” Doge.gov was hastily deployed after Elon Musk  that his Department of Government Efficiency is “trying to be as transparent as possible. In fact, our actions—we post our actions to the DOGE handle on X, and to the DOGE website.” At the time, DOGE was an essentially blank webpage. It was built out further Wednesday and Thursday, and now shows a mirror of the @DOGE X account posts, as well as various stats about the U.S. government’s federal workforce. Two different web development experts who asked to remain anonymous because they were probing a federal website told 404 Media that doge.gov is seemingly built on a Cloudflare Pages site that is not currently hosted on government servers. The database it is pulling from can be and has been written to by third parties, and will show up on the live website. Both sources told 404 Media that they noticed Doge.gov is pulling from a Cloudflare Pages website, where the code that runs it is actually deployed.]]></content:encoded></item><item><title>The New York Stock Exchange plans to launch NYSE Texas</title><link>https://ir.theice.com/press/news-details/2025/The-New-York-Stock-Exchange-to-Launch-NYSE-Texas/default.aspx</link><author>ChrisArchitect</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 06:40:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Does X cause Y? An in-depth evidence review (2021)</title><link>https://www.cold-takes.com/does-x-cause-y-an-in-depth-evidence-review/</link><author>l0b0</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 06:14:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
There's an interesting theory out there that X causes Y. If this were true, it would be pretty important. So I did a deep-dive into the academic literature on whether X causes Y. Here's what I found.

(Embarrassingly, I can't actually remember what X and Y are. I think maybe X was enriched preschool, or just school itself, or eating fish while pregnant, or the Paleo diet, or lead exposure, or a clever "nudge" policy trying to get people to save more, or some self-help technique, or some micronutrient or public health intervention, or democracy, or free trade, or some approach to intellectual property law. And Y was ... lifetime earnings, or risk of ADHD diagnosis, or IQ in adulthood, or weight loss, or violent crime, or peaceful foreign policy, or GDP per capita, or innovation. Sorry about that! Hope you enjoy the post anyway! Fortunately, I think what I'm about to write is correct for pretty much any (X,Y) from those sorts of lists.)
There are hundreds of studies on whether X causes Y, but most of them are simple observational studies that are just essentially saying "People/countries with more X also have more Y." For reasons discussed below, we can't really learn much from these studies.

There are 1-5 more interesting studies on whether X causes Y. Each study looks really clever, informative and rigorous at first glance. However, the more closely you look at them, the more confusing the picture gets.

We ultimately need to choose between (a) believing some overly complicated theory of the relationship between X and Y, which reconciles all of the wildly conflicting and often implausible things we're seeing in the studies; (b) more-or-less reverting to what we would've guessed about the relationship between X and Y in the absence of any research.
The chaff: lots of unhelpful studies that I'm disregarding
First, the good news: there are hundreds of studies on whether X causes Y. The bad news? We need to throw most of them out. 

Many have comically small sample sizes (like studying 20 people) and/or comically short time horizons (like looking at weight loss over two weeks), or unhelpful outcome measures (like intelligence tests in children under 5). But by far the most common problem is that most of the studies on whether X causes Y are simple observational studies: they essentially just find that people/countries with more X also have more Y. 
Why is this a problem? There could be a  - some third thing, Z, that is correlated with both X and Y. And there are specific reasons we should expect confounders to be common:
In general, people/countries that have more X also have more of lots of other helpful things - they're richer, they're more educated, etc. For example, if we're asking whether higher-quality schooling leads to higher earnings down the line, an issue is that people with higher-quality schooling also tend to come from better-off families with lots of other advantages.

In fact, the very fact that people in upper-class intellectual circles think X causes Y means that richer, more educated people/countries tend to deliberately get more X, and also try to do a lot of other things to get more Y. For example, more educated families tend to eat more fish (complicating the attempt to see whether eating fish in pregnancy is good for the baby).
Now, a lot of these studies try to "control for" the problem I just stated - they say things like "We examined the effect of X and Y, while controlling for Z [e.g., how wealthy or educated the people/countries/whatever are]." How do they do this? The short answer is, well, hm, jeez. Well you see, to simplify matters a bit, just try to imagine ... uh ... shit. Uh. The only high-level way I can put this is:
They use a technique called regression analysis that, as far as I can determine, cannot be explained in a simple, intuitive way (especially not in terms of how it "controls for" confounders).

The "controlling for" thing relies on a lot of subtle assumptions and can break in all kinds of weird ways. Here's a technical explanation of some of the pitfalls; here's a set of deconstructions of regressions that break in weird ways.

None of the observational studies about whether X causes Y discuss the pitfalls of "controlling for" things and whether they apply here.

I don't think we can trust these papers, and to really pick them all apart (given how many there are) would take too much time. So let's focus on a smaller number of better studies.
The wheat: 1-5 more interesting studies
Digging through the sea of unhelpful studies, I found 1-5 of them that are actually really interesting! 

But on closer inspection, I have a bunch of reservations. For example:
The paper doesn't make it easy to replicate its analysis, and when someone does manage to sort-of replicate it, they may get different results. 

There was other weird stuff going on (e.g., changes in census data collection methods), during the strange historical event, so it's a little hard to generalize.

In a response to the study, another academic advances a complex theory of how the study could actually have gotten a misleading result. This led to an intense back-and-forth between the original authors and the skeptic, stretched out over years because each response had to be published in a journal, and by the time I got to the end of it I didn't have any idea what to think anymore.
I found 0-4 other interesting studies. I can't remember all of the details, but they may have included:

Compounding the problem, the 1-5 studies I found tell very different stories about the relationship between X and Y. How could this make sense? Is there a unified theory that can reconcile all the results?

Well, one possibility is that X causes Y sometimes, but only under very particular conditions, and the effect can be masked by some other thing going on. So - if you meet one of 7 criteria, you should do X to get more Y, but if you meet one of 9 other criteria, you should actually avoid X!

I have to say, this all was simultaneously more fascinating and less  than I expected it would be going in. I thought I would find some nice studies about the relationship between X and Y and be done. Instead, I've learned a ton about weird historical events and about the ins and outs of different measures of X and Y, but I feel just super confused about whether X causes Y.

I guess my bottom line is that X does cause Y, because it intuitively seems like it would.

I'm glad I did all this research, though. It's good to know that social science research can go haywire in all kinds of strange ways. And it's good to know that despite the confident proclamations of pro- and anti-X people, it's legitimately just super unclear whether X causes Y. 

I mean, how else could I have learned that?
Appendix: based on a true story
This piece was inspired by:

The basic patterns above apply to most of these, and the bottom line usually has the kind of frustrating ambiguity seen in this conclusion.There are cases where things seem a bit less ambiguous and the bottom line seems clearer. Speaking broadly, I think the main things that contribute to this are:
Actual randomization. For years I've nodded along when people say "You shouldn't be dogmatic about randomization, there are many ways for a study to be informative," but each year I've become a bit more dogmatic. Even the most sophisticated-, appealing-seeming alternatives to randomization in studies seem to have a way of falling apart. Randomized studies almost always have problems and drawbacks too. But I’d rather have a randomized study with drawbacks than a non-randomized study with drawbacks.

Extreme thoroughness, such as Roodman's attempt to reconstruct the data and code for key studies in Reasonable Doubt. This sometimes leads to outright dismissing a number of studies, leaving a smaller, more consistent set remaining.
]]></content:encoded></item><item><title>An argument for how Kubernetes can be use in development and reduce overall system complexity.</title><link>https://youtu.be/EJrw3z7m5iQ?si=D4BhYdkGryByoIVj</link><author>/u/purton_i</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 05:59:59 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>One-Minute Daily AI News 2/15/2025</title><link>https://www.reddit.com/r/artificial/comments/1ip3xhc/oneminute_daily_ai_news_2152025/</link><author>/u/Excellent-Target-847</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 05:55:00 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/Excellent-Target-847 ]]></content:encoded></item><item><title>What is some software that surprise you not not used more</title><link>https://www.reddit.com/r/linux/comments/1ip3pn0/what_is_some_software_that_surprise_you_not_not/</link><author>/u/Shiroegalleu</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 05:40:38 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[About a year ago I asked a similar question before I deleted my old account. I learned about quite a few programs and learned about a feature in one that I was already using that replaced a different program.Scrcpy - An amazing tool that you can use to capture your phone and it's cameraJellyfin - An open source media server. There's clients for it on Roku, web os , mobile and more.Vesktop - A Discord client that just works on wayland. also has plugin and theme support like better discordFreetube - AN open source private focus YouTube client.You most likely have heard of all of these but I rarely see them being used]]></content:encoded></item><item><title>Neovim tee.exe binary dependency exhibiting illegitimate / unauthorized behavior</title><link>https://github.com/neovim/neovim/issues/32431</link><author>patadune</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 05:34:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Strimzi migration to Axual Platform</title><link>https://www.reddit.com/r/kubernetes/comments/1ip2d4n/strimzi_migration_to_axual_platform/</link><author>/u/k8s_maestro</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 04:20:26 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Use Case: The plan was to adopt and go with open source solutions and went with Strimzi - Apache Kafka on Kubernetes Eventually the team decided to go for enterprise solution like Axual Platform. Now the question is, the migration possibilities.Did someone came across this scenario?Strimzi to Axual Platform]]></content:encoded></item><item><title>I told Claude AI that I’m alone on Valentine’s Day… and it did this.</title><link>https://www.reddit.com/r/artificial/comments/1ip2ab2/i_told_claude_ai_that_im_alone_on_valentines_day/</link><author>/u/snehens</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 04:15:54 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[What’s the funniest or most wholesome thing AI has ever done for you?Would you accept an AI-generated Valentine’s card?]]></content:encoded></item><item><title>Cloudflare Cli called flarectl is no longer supported it seems</title><link>https://www.reddit.com/r/golang/comments/1ip19zv/cloudflare_cli_called_flarectl_is_no_longer/</link><author>/u/gedw99</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 03:19:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Flarectl is really valuable for working with Cloudflare, but its dead these days.For those with slow meat sticks : isn't a part of the > v1 libraries, it only exists on the v0 branch which is now out of active development. you'll need to use the  branch if you want to keep building .we don't offer a CLI tool today that is auto generated (like the libraries). it is on the roadmap but no dates sorry." v4 is the latest and greatest PKG for Cloudflare.v2 was released in april 2024, then v3 in September, then v4 a month ago.I am reaching out to the community to see if anyone is maintaining a CLI that uses v4...Heaps of forks, but hard work to go through and find one :)]]></content:encoded></item><item><title>This free European AI chatbot is 13 times faster than ChatGPT - Le Chat</title><link>https://www.msn.com/en-us/news/other/this-free-european-ai-chatbot-is-13-times-faster-than-chatgpt/ar-AA1yLbAf</link><author>/u/Bob_Spud</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 03:01:18 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TikTok is back in the App Store</title><link>https://www.theverge.com/news/612768/tiktok-app-store-apple-google-us-ban</link><author>scarface_74</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 02:49:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Job roles related to Kubernetes/OpenShift</title><link>https://www.reddit.com/r/kubernetes/comments/1ip0j60/job_roles_related_to_kubernetesopenshift/</link><author>/u/UCONN_throwaway_99</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 02:39:44 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I was given the opportunity to do a POC for my team to migrate our app onto containers, and we support OpenShift. I really enjoyed the migration part of it and learning about OpenShift/containerization. Would anyone know what kind of job role I should be searching for related to this work?]]></content:encoded></item><item><title>Building a Data Engineering Center of Excellence</title><link>https://towardsdatascience.com/building-a-data-engineering-center-of-excellence/</link><author>Richie Bachala</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 02:35:48 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[As data continues to grow in importance and become more complex, the need for skilled data engineers has never been greater. But what is data engineering, and why is it so important? In this blog post, we will discuss the essential components of a functioning data engineering practice and why data engineering is becoming increasingly critical for businesses today, and how you can build your very own Data Engineering Center of Excellence!I’ve had the privilege to build, manage, lead, and foster a sizeable high-performing team of data warehouse & ELT engineers for many years. With the help of my team, I have spent a considerable amount of time every year consciously planning and preparing to manage the growth of our data month-over-month and address the changing reporting and analytics needs for our 20000+ global data consumers. We built many data warehouses to store and centralize massive amounts of data generated from many OLTP sources. We’ve implemented Kimball methodology by creating star schemas both within our on-premise data warehouses and in the ones in the cloud.The objective is to enable our user-base to perform fast analytics and reporting on the data; so our analysts’ community and business users can make accurate data-driven decisions.It took me about three years to transform  () of data warehouse and ETL programmers into one cohesive Data Engineering team.I have compiled some of my learnings building a global data engineering team in this post in hopes that Data professionals and leaders of all levels of technical proficiency can benefit.Evolution of the Data EngineerIt has never been a better time to be a data engineer. Over the last decade, we have seen a massive awakening of enterprises now recognizing their data as the company’s heartbeat, making data engineering the job function that ensures accurate, current, and quality data flow to the solutions that depend on it.Historically, the role of Data Engineers has evolved from that of data warehouse developers and the  (extract, transform and load).The data warehouse developers are responsible for designing, building, developing, administering, and maintaining data warehouses to meet an enterprise’s reporting needs. This is done primarily via extracting data from operational and transactional systems and piping it using extract transform load methodology (ETL/ ELT) to a storage layer like a data warehouse or a data lake. The data warehouse or the data lake is where data analysts, data scientists, and business users consume data. The developers also perform transformations to conform the ingested data to a data model with aggregated data for easy analysis.A data engineer’s prime responsibility is to produce and make data securely available for multiple consumers.Data engineers oversee the ingestion, transformation, modeling, delivery, and movement of data through every part of an organization. Data extraction happens from many different data sources & applications. Data Engineers load the data into data warehouses and data lakes, which are transformed not just for the Data Science & predictive analytics initiatives (as everyone likes to talk about) but primarily for data analysts. Data analysts & data scientists perform operational reporting, exploratory analytics, service-level agreement (SLA) based business intelligence reports and dashboards on the catered data. In this book, we will address all of these job functions.The role of a data engineer is to acquire, store, and aggregate data from both cloud and on-premise, new, and existing systems, with data modeling and feasible data architecture. Without the data engineers, analysts and data scientists won’t have valuable data to work with, and hence, data engineers are the first to be hired at the inception of every new data team. Based on the data and analytics tools available within an enterprise, data engineering teams’ role profiles, constructs, and approaches have several options for what should be included in their responsibilities which we will discuss in this chapter.Software is increasingly automating the historically manual and tedious tasks of data engineers. Data processing tools and technologies have evolved massively over several years and will continue to grow. For example, cloud-based data warehouses (Snowflake, for instance) have made data storage and processing affordable and fast. Data pipeline services (like Informatica IICS, Apache Airflow, Matillion, Fivetran) have turned data extraction into work that can be completed quickly and efficiently. The data engineering team should be leveraging such technologies as force multipliers, taking a consistent and cohesive approach to integration and management of enterprise data, not just relying on legacy siloed approaches to building custom data pipelines with fragile, non-performant, hard to maintain code. Continuing with the latter approach will stifle the pace of innovation within the said enterprise and force the future focus to be around managing data infrastructure issues rather than how to help generate value for your business.The primary role of an enterprise Data Engineering team should be to  into a shape that’s ready for analysis — laying the foundation for real-world analytics and data science application.The Data Engineering team should serve as the  for enterprise-level data with the responsibility to curate the organization’s data and act as a resource for those who want to make use of it, such as Reporting & Analytics teams, Data Science teams, and other groups that are doing more self-service or business group driven analytics leveraging the enterprise data platform. This team should serve as the  of organizational knowledge, managing and refining the catalog so that analysis can be done more effectively. Let’s look at the essential responsibilities of a well-functioning Data Engineering team.Responsibilities of a Data Engineering TeamThe Data Engineering team should provide a  within the enterprise that cuts across to support both the Reporting/Analytics and Data Science capabilities to provide access to clean, transformed, formatted, scalable, and secure data ready for analysis. The Data Engineering teams’ core responsibilities should include:· Build, manage, and optimize the core data platform infrastructure· Build and maintain custom and off-the-shelf data integrations and ingestion pipelines from a variety of structured and unstructured sources· Manage overall data pipeline orchestration· Manage transformation of data either before or after load of raw data through both technical processes and business logic· Support analytics teams with design and performance optimizations of data warehousesData is an Enterprise Asset.Data as an Asset should be shared and protected.Data should be valued as an Enterprise asset, leveraged across all Business Units to enhance the company’s value to its respective customer base by accelerating decision making, and improving competitive advantage with the help of data. Good data stewardship, legal and regulatory requirements dictate that we protect the data owned from unauthorized access and disclosure.In other words, managing Security is a crucial responsibility.Why Create a Centralized Data Engineering Team?Treating Data Engineering as a standard and core capability that underpins both the Analytics and Data Science capabilities will help an enterprise evolve how to approach Data and Analytics. The enterprise needs to stop vertically treating data based on the technology stack involved as we tend to see often and move to more of a horizontal approach of managing a  or  that cuts across the organization and can connect to various technologies as needed drive analytic initiatives. This is a new way of thinking and working, but it can drive efficiency as the various data organizations look to scale. Additionally — there is value in creating a dedicated structure and career path for Data Engineering resources. Data engineering skill sets are in high demand in the market; therefore, hiring outside the company can be costly. Companies must enable programmers, database administrators, and software developers with a career path to gain the needed experience with the above-defined skillsets by working across technologies. Usually, forming a data engineering center of excellence or a capability center would be the first step for making such progression possible.Challenges for creating a centralized Data Engineering TeamThe centralization of the Data Engineering team as a service approach is different from how Reporting & Analytics and Data Science teams operate. It does, in principle, mean giving up some level of control of resources and establishing new processes for how these teams will collaborate and work together to deliver initiatives.The Data Engineering team will need to demonstrate that it can effectively support the needs of both Reporting & Analytics and Data Science teams, no matter how large these teams are. Data Engineering teams must effectively prioritize workloads while ensuring they can bring the right skillsets and experience to assigned projects.Data engineering is essential because it serves as the backbone of data-driven companies. It enables analysts to work with clean and well-organized data, necessary for deriving insights and making sound decisions. To build a functioning data engineering practice, you need the following critical components:The Data Engineering team should be a core capability within the enterprise, but it should effectively serve as a support function involved in almost everything data-related. It should interact with the Reporting and Analytics and Data Science teams in a collaborative support role to make the entire team successful.The Data Engineering team doesn’t create direct business value — but the value should come in making the Reporting and Analytics, and Data Science teams more productive and efficient to ensure delivery of maximum value to business stakeholders through Data & Analytics initiatives. To make that possible, the six key responsibilities within the data engineering capability center would be as follow –Let’s review the 6 pillars of responsibilities:1. Determine Central Data Location for Collation and WranglingUnderstanding and having a strategy for a (a centralized data repository or data warehouse for the mass consumption of data for analysis). Defining requisite data tables and where they will be joined in the context of data engineering and subsequently converting raw data into digestible and valuable formats.2. Data Ingestion and TransformationMoving data from one or more sources to a new destination (your data lake or cloud data warehouse) where it can be stored and further analyzed and then converting data from the format of the source system to that of the destinationExtracting, transforming, and loading data from one or more sources into a destination system to represent the data in a new context or style.Data modeling is an essential function of a data engineering team, granted not all data engineers excel with this capability. Formalizing relationships between data objects and business rules into a conceptual representation through understanding information system workflows, modeling required queries, designing tables, determining primary keys, and effectively utilizing data to create informed output.I’ve seen engineers in interviews mess up more with this than coding in technical discussions. It’s essential to understand the differences between Dimensions, Facts, Aggregate tables.Ensuring that sensitive data is protected and implementing proper authentication and authorization to reduce the risk of a data breach6. Architecture and AdministrationDefining the models, policies, and standards that administer what data is collected, where and how it is stored, and how it such data is integrated into various analytical systems.The six pillars of responsibilities for data engineering capabilities center on the ability to determine a central data location for collation and wrangling, ingest and transform data, execute ETL/ELT operations, model data, secure access and administer an architecture. While all companies have their own specific needs with regards to these functions, it is important to ensure that your team has the necessary skillset in order to build a foundation for big data success.Besides the Data Engineering following are the other capability centers that need to be considered within an enterprise:Analytics Capability CenterThe analytics capability center enables consistent, effective, and efficient BI, analytics, and advanced analytics capabilities across the company. Assist business functions in triaging, prioritizing, and achieving their objectives and goals through reporting, analytics, and dashboard solutions, while providing operational reports and visualizations, self-service analytics, and required tools to automate the generation of such insights.Data Science Capability CenterThe data science capability center is for exploring cutting-edge technologies and concepts to unlock new insights and opportunities, better inform employees and create a culture of prescriptive information usage using Automated AI and Automated ML solutions such as H2O.ai, Dataiku, Aible, DataRobot, C3.aiThe data governance office empowers users with trusted, understood, and timely data to drive effectiveness while keeping the integrity and sanctity of data in the right hands for mass consumption.As your company grows, you will want to make sure that the data engineering capabilities are in place to support the six pillars of responsibilities. By doing this, you will be able to ensure that all aspects of data management and analysis are covered and that your data is safe and accessible by those who need it. Have you started thinking about how your company will grow? What steps have you taken to put a centralized data engineering team in place?]]></content:encoded></item><item><title>Told ChatGPT to make neofetch but for humans</title><link>https://www.reddit.com/r/linux/comments/1ip079i/told_chatgpt_to_make_neofetch_but_for_humans/</link><author>/u/OkAdministration5454</author><category>dev</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 02:22:18 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Apple Resumes Advertising on X</title><link>https://daringfireball.net/linked/2025/02/12/apple-advertising-x</link><author>brcmthrowaway</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 01:44:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Joe Rossignol, MacRumors:Apple this month started advertising on X for the first time in
more than a year. The company had stopped advertising on
the social media platform in November 2023 following controversial
remarks made by its owner Elon Musk.The November 2023 outrage was in response to, among other things, Musk replying “You have said the actual truth” to a tweet from a rando that stated “I’m deeply disinterested in giving the tiniest shit now about western Jewish populations coming to the disturbing realization that those hordes of minorities that support flooding their country don’t exactly like them too much. You want truth said to your face, there it is.”Musk clearly learned his lesson. Since then, he’s been lying low, out of the public eye, and refraining from any sort of controversial statements or actions. He barely even tweets anymore. The one and only time I can recall him even being in the news in the last 18 months was this year-ago profile in the Wall Street Journal documenting Musk’s health and nutrition regimen. So it’s all cool now and Apple feels comfortable advertising on the social network Musk wholly owns.★ Wednesday, 12 February 2025]]></content:encoded></item><item><title>Creating a chrome extension with rust + leptos -&gt; wasm</title><link>https://iism.org/article/ride-the-lightning-the-art-of-creative-motivation-63</link><author>/u/grok-battle</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 01:40:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OCR4all</title><link>https://www.ocr4all.org/</link><author>LorenDB</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 01:34:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Optical Character Recognition (and more) for everyoneSetup guide, user guide, developer documentation and more.]]></content:encoded></item><item><title>Installing Golang for non-sudo users on Ubuntu</title><link>https://www.reddit.com/r/golang/comments/1ioz4pq/installing_golang_for_nonsudo_users_on_ubuntu/</link><author>/u/Mycroft2046</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 01:25:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am trying to install Golang for non-sudo user. The official instructions unzips the Go tar file in  and then add  to PATH. Can I unzip it in  instead and add /home/username/.local/go/bin to PATH instead? Common sense tells me that it should work, but I don't know if there is any specific Go feature that relies on Go being installed in .]]></content:encoded></item><item><title>RustOwl - A new tool for visualizing Rust lifetimes</title><link>https://youtu.be/NV6Xo_el_2o</link><author>/u/zxyzyxz</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 01:20:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Did Prisma Choose Rust Initially?</title><link>https://youtu.be/1zSh0zYLTIE</link><author>/u/zxyzyxz</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 01:14:21 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CDC Study Finds Silent Bird Flu Infections in Dairy Veterinarians</title><link>https://www.nytimes.com/2025/02/13/science/cdc-bird-flu-infections.html</link><author>bookofjoe</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 00:52:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Three dairy veterinarians, including one who worked only in states with no known bird flu outbreaks in cows, had recent, undetected bird flu infections, according to a new study from the Centers for Disease Control and Prevention. The results are based on antibody testing of 150 veterinarians working in 46 U.S. states.The findings were not entirely surprising, experts said, but did suggest that the virus, known as H5N1, could be infecting cows and people in more states than have been officially reported.“We do not know the extent of this outbreak in the U.S.,” said Seema Lakdawala, a virologist at Emory University. “There are clearly infections happening that we’re missing.”Since the bird flu outbreak in dairy cows was first reported last March, the virus has been confirmed in more than 950 herds in 16 states. It has also been detected in 68 people, 41 of whom had contact with sick cows. Most people have had mild symptoms.]]></content:encoded></item><item><title>[P]GPT-2 in Pure C(and full CUDA worklogs to come)</title><link>https://www.reddit.com/r/MachineLearning/comments/1ioybio/pgpt2_in_pure_cand_full_cuda_worklogs_to_come/</link><author>/u/atronos_kronios</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 00:43:58 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Parallel computing is one of those things that sounds intimidating but is absolutely essential for the modern world. From high-frequency trading (HFT) to on-device AI, minimizing resources while maximizing performance is IMPORTANT and probably going to be the bottleneck as we move to better open-source LLMs. To dive headfirst into this space, I’ve started a project where I have implemented the GPT-2 architecture from scratch in plain, naive, and unoptimized(borderline stupid) C with no major dependency. Why? Because understanding a problem at its most fundamental level is the only way to optimize it effectively.Now, here’s the kicker: Learning CUDA is tricky. Most tutorials start with the basics (like optimizing matrix multiplications, then they might dive into a bit into basic operations/creating circle based renderers), but real production-level CUDA, like the kernels you’d see in George Hotz's TinyGrad or Karpathy’s llm.c or similar projects, is a whole different thing. There’s barely any structured resources to bridge that gap.So, my goal? ➡️ Start with this simple implementation and optimize step by step.➡️ Learn to build CUDA kernels from scratch, benchmark them, and compare them to other solutions.➡️ Return to this GPT-2 implementation, pick it apart piece by piece again, and see how much faster, leaner, and more efficient I can make it.And I’ll be documenting everything along the way with complete worklogs]]></content:encoded></item><item><title>We are the &quot;thin blue line&quot; that is trying to keep the code high quality</title><link>https://lore.kernel.org/lkml/20250208204416.GL1130956@mit.edu/</link><author>luu</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 00:40:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What if Eye...?</title><link>https://eyes.mit.edu/</link><author>smusamashah</author><category>dev</category><category>hn</category><pubDate>Fri, 14 Feb 2025 00:04:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Projects / Papers / Visual ExhibitsArtificial Cambrian Intelligence: Evolve your own creatures in our simulator!We've created a simulator where you can define and evolve your own embodied agents. You can define your own task, or use existing ones, and evolve agent's eyes or brains.DocumentationExamples]]></content:encoded></item><item><title>It&apos;s time to go ESM-only</title><link>https://javascriptweekly.com/issues/723</link><author></author><category>Javascript Weekly</category><category>dev</category><category>frontend</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://javascriptweekly.com/">JavaScript Weekly</source><content:encoded><![CDATA[🤯 Pfft, vehicle data is a joke without CarsXE. API goes brrrrr. VIN decoding, plate lookup, market value reports. Get Serious!]]></content:encoded></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/</link><author></author><category>dev</category><category>official</category><category>k8s</category><category>devops</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Dev - Kubernetes Blog</source><content:encoded><![CDATA[Kubernetes 1.31
completed the largest migration in Kubernetes history, removing the in-tree
cloud provider. While the component migration is now done, this leaves some additional
complexity for users and installer projects (for example, kOps or Cluster API) . We will go
over those additional steps and failure points and make recommendations for cluster owners.
This migration was complex and some logic had to be extracted from the core components,
building four new subsystems.One of the most critical functionalities of the cloud controller manager is the node controller,
which is responsible for the initialization of the nodes.As you can see in the following diagram, when the  starts, it registers the 
object with the apiserver, Tainting the node so it can be processed first by the
cloud-controller-manager. The initial  is missing the cloud-provider specific information,
like the Node Addresses and the Labels with the cloud provider specific information like the
Node, Region and Instance type information.sequenceDiagram
autonumber
rect rgb(191, 223, 255)
Kubelet->>+Kube-apiserver: Create Node
Note over Kubelet: Taint: node.cloudprovider.kubernetes.io
Kube-apiserver->>-Kubelet: Node Created
end
Note over Kube-apiserver: Node is Not Ready Tainted, Missing Node Addresses*, ...
Note over Kube-apiserver: Send Updates
rect rgb(200, 150, 255)
Kube-apiserver->>+Cloud-controller-manager: Watch: New Node Created
Note over Cloud-controller-manager: Initialize Node:Cloud Provider Labels, Node Addresses, ...
Cloud-controller-manager->>-Kube-apiserver: Update Node
end
Note over Kube-apiserver: Node is Ready
This new initialization process adds some latency to the node readiness. Previously, the kubelet
was able to initialize the node at the same time it created the node. Since the logic has moved
to the cloud-controller-manager, this can cause a chicken and egg problem
during the cluster bootstrapping for those Kubernetes architectures that do not deploy the
controller manager as the other components of the control plane, commonly as static pods,
standalone binaries or daemonsets/deployments with tolerations to the taints and using
 (more on this below)Examples of the dependency problemAs noted above, it is possible during bootstrapping for the cloud-controller-manager to be
unschedulable and as such the cluster will not initialize properly. The following are a few
concrete examples of how this problem can be expressed and the root causes for why they might
occur.These examples assume you are running your cloud-controller-manager using a Kubernetes resource
(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods
rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it
will schedule properly.Example: Cloud controller manager not scheduling due to uninitialized taintAs noted in the Kubernetes documentation, when the kubelet is started with the command line
flag --cloud-provider=external, its corresponding  object will have a no schedule taint
named node.cloudprovider.kubernetes.io/uninitialized added. Because the cloud-controller-manager
is responsible for removing the no schedule taint, this can create a situation where a
cloud-controller-manager that is being managed by a Kubernetes resource, such as a 
or , may not be able to schedule.If the cloud-controller-manager is not able to be scheduled during the initialization of the
control plane, then the resulting  objects will all have the
node.cloudprovider.kubernetes.io/uninitialized no schedule taint. It also means that this taint
will not be removed as the cloud-controller-manager is responsible for its removal. If the no
schedule taint is not removed, then critical workloads, such as the container network interface
controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.Example: Cloud controller manager not scheduling due to not-ready taintThe next example would be possible in situations where the container network interface (CNI) is
waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not
tolerated the taint which would be removed by the CNI."The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly."One of the conditions that can lead to a  resource having this taint is when the container
network has not yet been initialized on that node. As the cloud-controller-manager is responsible
for adding the IP addresses to a  resource, and the IP addresses are needed by the container
network controllers to properly configure the container network, it is possible in some
circumstances for a node to become stuck as not ready and uninitialized permanently.This situation occurs for a similar reason as the first example, although in this case, the
node.kubernetes.io/not-ready taint is used with the no execute effect and thus will cause the
cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is
not able to execute, then it will not initialize the node. It will cascade into the container
network controllers not being able to run properly, and the node will end up carrying both the
node.cloudprovider.kubernetes.io/uninitialized and node.kubernetes.io/not-ready taints,
leaving the cluster in an unhealthy state.There is no one “correct way” to run a cloud-controller-manager. The details will depend on the
specific needs of the cluster administrators and users. When planning your clusters and the
lifecycle of the cloud-controller-managers please consider the following guidance:For cloud-controller-managers running in the same cluster, they are managing.Use host network mode, rather than the pod network: in most cases, a cloud controller manager
will need to communicate with an API service endpoint associated with the infrastructure.
Setting “hostNetwork” to true will ensure that the cloud controller is using the host
networking instead of the container network and, as such, will have the same network access as
the host operating system. It will also remove the dependency on the networking plugin. This
will ensure that the cloud controller has access to the infrastructure endpoint (always check
your networking configuration against your infrastructure provider’s instructions).Use a scalable resource type.  and  are useful for controlling the
lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy
as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using
these primitives to control the lifecycle of your cloud controllers and running multiple
replicas, you must remember to enable leader election, or else your controllers will collide
with each other which could lead to nodes not being initialized in the cluster.Target the controller manager containers to the control plane. There might exist other
controllers which need to run outside the control plane (for example, Azure’s node manager
controller). Still, the controller managers themselves should be deployed to the control plane.
Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the
control plane to ensure that they are running in a protected space. Cloud controllers are vital
to adding and removing nodes to a cluster as they form a link between Kubernetes and the
physical infrastructure. Running them on the control plane will help to ensure that they run
with a similar priority as other core cluster controllers and that they have some separation
from non-privileged user workloads.
It is worth noting that an anti-affinity stanza to prevent cloud controllers from running
on the same host is also very useful to ensure that a single node failure will not degrade
the cloud controller performance.Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud
controller container to ensure that it will schedule to the correct nodes and that it can run
in situations where a node is initializing. This means that cloud controllers should tolerate
the node.cloudprovider.kubernetes.io/uninitialized taint, and it should also tolerate any
taints associated with the control plane (for example, node-role.kubernetes.io/control-plane
or node-role.kubernetes.io/master). It can also be useful to tolerate the
node.kubernetes.io/not-ready taint to ensure that the cloud controller can run even when the
node is not yet available for health monitoring.For cloud-controller-managers that will not be running on the cluster they manage (for example,
in a hosted control plane on a separate cluster), then the rules are much more constrained by the
dependencies of the environment of the cluster running the cloud-controller-manager. The advice
for running on a self-managed cluster may not be appropriate as the types of conflicts and network
constraints will be different. Please consult the architecture and requirements of your topology
for these scenarios.This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is
important to note that this is for demonstration purposes only, for production uses please
consult your cloud provider’s documentation.apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
name: cloud-controller-manager
namespace: kube-system
spec:
replicas: 2
selector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
strategy:
type: Recreate
template:
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
annotations:
kubernetes.io/description: Cloud controller manager for my infrastructure
spec:
containers: # the container details will depend on your specific cloud controller manager
- name: cloud-controller-manager
command:
- /bin/my-infrastructure-cloud-controller-manager
- --leader-elect=true
- -v=1
image: registry/my-infrastructure-cloud-controller-manager@latest
resources:
requests:
cpu: 200m
memory: 50Mi
hostNetwork: true # these Pods are part of the control plane
nodeSelector:
node-role.kubernetes.io/control-plane: ""
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- topologyKey: "kubernetes.io/hostname"
labelSelector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
tolerations:
- effect: NoSchedule
key: node-role.kubernetes.io/master
operator: Exists
- effect: NoExecute
key: node.kubernetes.io/unreachable
operator: Exists
tolerationSeconds: 120
- effect: NoExecute
key: node.kubernetes.io/not-ready
operator: Exists
tolerationSeconds: 120
- effect: NoSchedule
key: node.cloudprovider.kubernetes.io/uninitialized
operator: Exists
- effect: NoSchedule
key: node.kubernetes.io/not-ready
operator: Exists
When deciding how to deploy your cloud controller manager it is worth noting that
cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple
replicas of a cloud controller manager is good practice for ensuring high-availability and
redundancy, but does not contribute to better performance. In general, only a single instance
of a cloud controller manager will be reconciling a cluster at any given time.]]></content:encoded></item><item><title>[D] We built GenAI at Google and Apple, then left to build an open source AI lab, to enable the open community to collaborate and build the next DeepSeek. Ask us anything on Friday, Feb 14 from 9am-12pm PT!</title><link>https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/</link><author>/u/koukoumidis</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 23:53:27 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[TL;DR: Hi 👋 we’re Oumi, an AI lab that believes in an unconditionally open source approach–code, weights, training data, infrastructure, and collaboration—so the entire community can collectively push AI forward. We built a platform for anyone to contribute research in AI. Ask us anything about open source, scaling large models, DeepSeek, and what it takes to build frontier models, both inside and outside of big tech companies. Tell us what is working well in open source AI or what challenges you are facing. What should we work on together to improve AI in the open?For years, we worked at big tech (Google, Apple, Microsoft) leading efforts on GenAI models like Google Cloud PaLM, Gemini, and Apple’s health foundation models. We were working in silos and knew there had to be a better way to develop these models openly and collaboratively. So, we built a truly open source AI platform that makes it possible for tens of thousands of AI researchers, scientists, and developers around the world to collaborate, working together to advance frontier AI in a collective way that leads to more efficient, transparent and responsible development. The Oumi platform (fully open-source, Apache 2.0 license) supports pre-training, tuning, data curation/synthesis, evaluation, and any other common utility, in a fully recordable and reproducible fashion, while being easily customizable to support novel approaches.DeepSeek showed us what open source can achieve by leveraging open-weight models like LLaMA. But we believe AI should be even more open: not just the weights, but also the training data, and the code–make it ALL open. Then go even further: make it easy for anyone to access and experiment, make it easy for the community to work together and collaborate. Some resources about Oumi if you’re interested:If you want to collaborate and contribute to community research projects, regardless of where you get your compute, you can sign up at: https://oumi.ai/community. We will be starting with the post-training of existing open models, next, we will be collaboratively pursuing improvements to pre-training. We intend to publish the research with all contributors included as authors.We’re here to answer questions about our open source approach, scaling large models, DeepSeek, what it takes to build frontier models both inside and outside of big tech companies, and anything else you all want to discuss.We’ll be here Friday, February 14 from 9am-12pm PT / 12pm-3pm ET. Ask us anything.(u/koukoumidis)  - CEO and Co-founder, ex-Google (Cloud GenAI Lead)(u/oelachqar)  - Co-founder, Engineering, ex-Apple (Health foundation models)(u/MatthewPersons)  - Co-founder, Engineering, ex-Google (Cloud PaLM & NL Lead)(u/jeremy_oumi)  - Co-founder, Research, ex-Google (Gemini Alignment)]]></content:encoded></item><item><title>The OBS Project is threatening Fedora Linux with legal action</title><link>https://gitlab.com/fedora/sigs/flatpak/fedora-flatpaks/-/issues/39#note_2344970813</link><author>TheFreim</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 23:52:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>LM2: Large Memory Models</title><link>https://arxiv.org/abs/2502.06049</link><author>fzliu</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 23:21:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenAI scrubs diversity commitment web page from its site</title><link>https://techcrunch.com/2025/02/13/openai-scrubs-diversity-commitment-web-page-from-its-site/</link><author>gpi</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 23:18:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The previous page stated that the company’s “investment in diversity, equity and inclusion” was ongoing. It also said that it was serious about this work and was committed to “continuously improving our work in creating a diverse, equitable, and inclusive organization,” as captured by the Internet Web Archive and also as cited by a 2023 CNN article.It isn’t clear exactly when this change was made, but there’s some evidence it was very recent. The diversity commitment web page was cited as available by an ABC News story published on January 22. By January 27, OpenAI had published the new, replacement “building dynamic teams” web page.The new page says, “At OpenAI, we recognize that the strongest ideas emerge when they are tested, debated, and improved by people with different backgrounds, experiences, and ways of thinking. We encourage a culture of curiosity where ideas can be challenged — not just accepted.” OpenAI did not immediately respond to our request for comment about the website change or if it indicates any of the company’s internal processes have been rolled back or altered. OpenAI’s web page that promises “fairness” in model training to remove social biases is still live, for instance.OpenAI is hardly the only company to distance itself from the wording of DEI as these specific words — as well as the concepts behind them — face fierce political and legal backlash these days. Last week, Attorney General Pam Bondi instructed the Department of Justice to “investigate, eliminate, and penalize,” DEI programs deemed illegal in private sector companies that receive federal funds. In the past month, Meta, Google, and Amazon have all announced the elimination or adjustment of DEI policies, with the latter two scrubbing mention of diversity and inclusion from their recent 10-K forms. ]]></content:encoded></item><item><title>Jottr, simple text editor for distraction-free writing</title><link>https://www.reddit.com/r/linux/comments/1iow7od/jottr_simple_text_editor_for_distractionfree/</link><author>/u/walterblackkk</author><category>dev</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 23:02:00 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Jottr is a simple, opinionated, plain text editor I developed primarily for my personal use. It's mainly intended for writers, researchers and journalists or anyone who needs a simple, distraction-free tool for writing. It's cross-platform and can be used on Linux, macOS or Windows. The app is released under the GPL v3 license.It has the following features:Dark, Light and Sepia (paper-like) themesFocus mode - Distraction-free writing made easy by hiding all UI elements (can be toggled with mouse or a keyboard shortcut)Search for any word without leaving the editor using the integrated web browserSnippets - Save any word or block of text as snippets and quickly insert using mouse or through typing snippet nameInline auto-completion for words added to user dictionaryAdd custom search websites to your right-click menuFeel free to share your feedback/thoughts.]]></content:encoded></item><item><title>Streamlined dataflow analysis code in rustc</title><link>https://nnethercote.github.io/2024/12/19/streamlined-dataflow-analysis-code-in-rustc.html</link><author>/u/nnethercote</author><category>dev</category><category>rust</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 22:28:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kubernertes Cluster - DigitalOcean</title><link>https://www.reddit.com/r/kubernetes/comments/1iouyuu/kubernertes_cluster_digitalocean/</link><author>/u/Kooky_Group_5215</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 22:07:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I have a cluster on digitalocean... i was trying to deploy a image (java api) but i am getting this error:exec /opt/java/openjdk/bin/java: exec format errorI generated de image with dockerfile that was generated with docker initI generated the image with the arch amd64 ( I use a macbook m2)I tested the image on docker localhost and openshift developer sandbox and worksThe user for the container is non privileged, the base image is eclipse-temurin:17-jdk-jammy   submitted by    /u/Kooky_Group_5215 ]]></content:encoded></item><item><title>Show HN: SQL Noir – Learn SQL by solving crimes</title><link>https://www.sqlnoir.com/</link><author>chrisBHappy</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 21:49:16 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>how many of you have on-prem k8s running with firewalld</title><link>https://www.reddit.com/r/kubernetes/comments/1iouhkf/how_many_of_you_have_onprem_k8s_running_with/</link><author>/u/SnooOwls6002</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 21:45:57 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[As the title said, how many of you have done it on production env? I am runing rhel9 OS, I found it difficult to setup with the firewalld running and I feel exhausted to let it find out all the networking issue I encountered every time I deploy/troubleshoot stuff and I hope the experts here could give me some suggestions.Currently, I am running 3x control plane, 3x worker nodes in the same subnet, with kube-vip setup for the VIP in control plane and IP range for svc loadblanacing.For the network CNI, I run cilium for pretty basic setup wit disabling ipv6 on hubble-ui so I can have a visibility on different namespace.Also, I use traefik as the ingress controller for my svc in the backend.So what I notice is in order to make it worked, sometimes I need to stop and start the firewalld again, and for me running the cilium connectivity test, it cannot pass through everything. Usually it stuck in pod creation and the problem are mainly due toERR Provider error, retrying in 420.0281ms error="could not retrieve server version: Get \"https://192.168.0.1:443/version\": dial tcp 192.168.0.1:443: i/o timeout" providerName=kubernetesThe issue above happens for some apps as well such as traefik and metric servers...The way I use in kubeadm command:kubeadm init \ --control-plane-endpoint my-entrypoint.mydomain.com \ --apiserver-cert-extra-sans 10.90.30.40 \ --upload-certs \ --pod-network-cidr 172.16.0.0/16 \ --service-cidr 192.168.0.0/20 Currently my kube-vip is doing and I could achieve the HA on the control plane. But I am not sure why those svc cannot communicate to the kubernetes service wit the svc cluster IP.I already opened several firewalld ports on both worker and control plane nodes.Here are my firewalld config:#control plane node: firewall-cmd --permanent --add-port={53,80,443,6443,2379,2380,10250,10251,10252,10255}/tcp firewall-cmd --permanent --add-port=53/udp #Required Cilium ports firewall-cmd --permanent --add-port={53,443,4240,4244,4245,9962,9963,9964,9081}/tcp firewall-cmd --permanent --add-port=53/udp firewall-cmd --permanent --add-port={8285,8472}/udp #Since my pod network and svc network are 172.16.0.0/16 and 192.168.0.0/20 firewall-cmd --permanent --zone=trusted --add-source=172.16.0.0/16 firewall-cmd --permanent --zone=trusted --add-source=192.168.0.0/20 firewall-cmd --add-masquerade --permanent firewall-cmd --reload ## For worker node firewall-cmd --permanent --add-port={53,80,443,10250,10256,2375,2376,30000-32767}/tcp firewall-cmd --permanent --add-port={53,443,4240,4244,4245,9962,9963,9964,9081}/tcp firewall-cmd --permanent --add-port=53/udp firewall-cmd --permanent --add-port={8285,8472}/udp firewall-cmd --permanent --zone=trusted --add-source=172.16.0.0/16 firewall-cmd --permanent --zone=trusted --add-source=192.168.0.0/20 firewall-cmd --add-masquerade --permanent firewall-cmd --reload AFAIK, if I turn of my firewalld, all of the services are running properly. I am confused why those service cannot reach out to the kubernetes API service 192.168.0.1:443 at all.Once the firewalld is up and running again, the metric is failed again as it gave outUnable to connect to the server: dial tcp my_control_plane_1-host_ip:6443: connect: no route to host Could anyone give me some ideas and suggestions? Thank you very much!]]></content:encoded></item><item><title>How protobuf works: the art of data encoding</title><link>https://victoriametrics.com/blog/go-protobuf/</link><author>/u/valyala</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 21:44:22 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[How Protobuf Works—The Art of Data EncodingProtobuf (Protocol Buffers) is a way to serialize data into a compact binary format. This makes it smaller in size and faster to transmit over the network, though at the cost of being less human-readable.It works by defining data structures in a  file — a blueprint specifying what fields exist, their types, the services, etc. From there, this file is used to generate code in different programming languages, allowing programs to efficiently encode and decode the data.“Does Protobuf take longer to serialize and deserialize data compared to JSON?”There are many compilers and implementations, so there’s no single definitive answer to this question. However, let’s run a simple benchmark comparing google.golang.org/protobuf and  in Go. While this isn’t a perfectly fair comparison, it does reflect common choices.We also benchmarked another Protobuf-compatible solution, easyproto, which is designed for high performance and low memory usage with some trade-offs. is just a simpler way to work with protobuf encoding without needing the  compiler or code generation. It follows the exact same wire format as protobuf (proto3).The benchmark below was run on a local machine using Go 1.23.5, libprotoc 29.2:No surprise here—Protocol Buffers produces a much smaller output, 99 bytes compared to JSON’s 214 bytes for the same data structure. This difference only becomes more noticeable as the data grows.Looking at serialization: serialization takes just 74.21 nanoseconds, with zero allocations.Protobuf () takes 133.0 nanoseconds and makes one memory allocation of 112 bytes.JSON () takes 248.8 nanoseconds with one allocation of 224 bytes.Deserialization shows an even bigger gap: deserialization takes 70.61 nanoseconds, with 3 allocations using 112 bytes.Protobuf () takes 293.6 nanoseconds with 13 allocations using 448 bytes.JSON () takes 1457 nanoseconds with 18 allocations using 592 bytes.In today’s discussion, we’ll break down how binary data is structured in Proto3 and examine the struct generated by .Field Key (or Tag) Encoding
#To encode a field, we need two things: where the field is in the message and what its value is. We’ll use the code snippet below along the way to explain the encoding process:The first thing to figure out is how each field is identified. Every field in the message has two parts: a unique number and a type (the actual name of the field doesn’t matter in this case):The  assigned to each field is known as the .The  determines something called the , which tells the system how the value should be encoded (and how to decode it later).Protobuf has 5 wire types:: Used for encoding integers (, , , , , ), booleans (bool), and enumerations (): Used for fixed-length 64-bit data types such as , , and .: Used for strings, byte arrays, embedded messages, and packed repeated fields.: Were used for groups, but now deprecated.: Used for fixed-length 32-bit data types such as , , and .These two pieces—field number and wire type—are combined into a single byte called the . Protobuf uses this tag to figure out which field is being referenced in the message. The wire type takes up the least significant 3 bits (the rightmost ones), while the field number fills the rest:With that in mind, the tag values for those fields in our example would be:How Protobuf assigns tags to message fieldsProtobuf doesn’t use the same encoding for every type of value. Instead, it groups them into three categories: varint, length-delimited, and fixed-width encoding.Varint Encoding for Wire Type 0
#Storing small numbers like 1, 2, or 3 in a full 4-byte () or 8-byte () format would be wasteful — most of those bytes would just be zeros. Instead, Protobuf uses varint encoding, a technique where the number of bytes adjusts based on the size of the value. The smaller the number, the fewer bytes it takes.There’s a trade-off, though. The leftmost bit of each byte is reserved as a continuation bit, which signals whether the number keeps going:If the most significant bit (MSB) is 1, more bytes follow.If the MSB is 0, that’s the last byte, and the number is complete.Protobuf varint encoding process for the number 300For larger numbers, more bytes are needed—up to 10 for a full 64-bit integer. But in most cases, numbers tend to be small, so the space savings are significant. That makes varint a good choice when working with values that usually stay within a lower range.Here’s the problem: encoding a negative number, say -1. In a 32-bit integer, -1 is represented as:With varint encoding, that means 5 bytes are needed. With 64-bit integer, it’s 10 bytes:This happens because varint treats numbers as unsigned, meaning negative values end up consuming extra space. To avoid this issue, Protobuf takes a different route for signed integers (, ) — it uses zig-zag varint encoding.Like the name suggests, it zigzags between positive and negative numbers:A positive number  is encoded as .A negative number  is encoded as .Once converted, the number is encoded using varint. This avoids the inefficiency of storing negative numbers as large multi-byte values.In Go, you can try this yourself using the  package from google.golang.org/protobuf, or even the standard  package. Go indeed supports both regular varint and zig-zag varint encoding. The  package provides two sets of functions:Unsigned integers:  and .Signed integers:  and .Here’s a quick test with :At this point, the  field equals to 300 can be fully represented in 3 bytes:Protobuf varint encoding for the ID fieldLength-Delimited Encoding for Wire Type 2
#Length-delimited encoding is used for data types that . This includes strings, byte arrays, embedded messages, and packed repeated fields.The idea is straightforward, it breaks the value into two parts: first, a prefix that tells you the length (in bytes) of what’s coming next, and then the actual data. In binary format, it looks like this: .For example, in our case, the  field contains , which is 9 bytes long when encoded in UTF-8. That means the encoded value would be:Protobuf length-delimited encoding for a string fieldWe’re not getting into how UTF-8 encoding works here, but just like varint, it has its own way of handling multi-byte characters and continuation bits. That’s the general idea.Fixed-Width Encoding for Wire Type 1, 5
#Fixed-width encoding is used for fields with a set size, like , , , , and floating-point numbers (, ). Nothing new here — these values are stored in a simple way without extra length prefixes or varint tricks.That means the height field is encoded using IEEE 754 32-bit floating-point representation as usual:Protobuf fixed-width encoding for a float fieldNow, putting everything together, the final encoded message looks like this:One last thing, if a field has a default value (, , , etc.), it doesn’t get encoded at all. This works just like  in JSON.Protobuf is built with both backward compatibility (newer systems can still read older messages) and forward compatibility (older systems can still handle newer messages).To see how this works in practice, let’s look at how deserialization happens in Go.When decoding a message, Protobuf reads the data byte by byte. For each field, it starts by reading a tag, which contains two things. You probably already know from the encoding part:The , which tells the decoder how to interpret the next few bytes.The , which helps the decoder find the right field in the message definition.If the field number matches one in the current message definition, the decoder processes it normally based on the field’s type. However, what happens when the decoder comes across a field number that isn’t in the message definition?Instead of failing, Protobuf has a built-in way to handle this. It treats the field as  and skips over it using the wire type information to figure out how many bytes to ignore. These unknown fields aren’t just thrown away—they’re actually stored in a separate section of the message.That’s why you’ll see an option in the unmarshal function called , which lets you choose whether to keep these unknown fields or drop them entirely.“Why don’t we just discard unknown fields?”If the message gets passed to newer code that does understand these fields, the data is still there and can be interpreted correctly:Even though we unmarshaled the message into the newer version , the  field wasn’t lost. When re-encoded, it’s still there:Now, if we reverse the situation: where a field exists in the current message definition but wasn’t present in the received message, the field simply gets its default value. This lines up with how Protobuf encodes data: fields with default values aren’t stored in the first place.So what should we take away from this discussion?The order of fields doesn’t matter. What does matter is the  (and the ).To keep compatibility, don’t reuse or change tag numbers, and don’t change a field’s type.If you remove a field, reserve its  to prevent future reuse: .“What if I change a field’s type but keep the same wire type?”Even if two types use the same wire type, they don’t necessarily interpret the bytes the same way. A few examples make this clear: and  both use varint encoding, but a nonzero  value could be mistakenly read as true if interpreted as a boolean. and  also use varint encoding, but  treats the bytes as signed, while  treats them as unsigned. That means a negative  would show up as a huge positive number if read as .The Message in .pb.go File
#Let’s take a look at the Person struct and see how everything we’ve talked about starts to feel familiar:Each field follows exactly what we expect based on its encoding: uses bytes encoding (length-delimited) with field number 1. uses varint encoding with field number 2. uses fixed32 encoding with field number 3.Then, we have , which—no surprise, stores any  fields that show up during deserialization. This is just a  that holds data for fields the current message definition doesn’t recognize.When you call  on a message for the first time, Protobuf calculates the size and saves it in . Later, if you call  again without modifying the message with  option, it just reuses the cached size instead of recalculating everything.This option tells Protobuf: “we’ve already calculated the size of this message before, so just use that cached size instead of recalculating it”.
For it to work correctly, two things must be true: The size was already calculated before, and the message (and any submessages) hasn’t changed at all. If there’s any doubt, don’t use it — Protobuf will handle size calculations correctly on its own.Caching the size of the message is good in 2 cases:For message fields nested inside other messages, Protocol Buffers needs to write a length prefix before writing the actual message content.Knowing the size upfront allows Protocol Buffers to allocate exactly the right amount of memory for the output buffer. Without knowing the size, it would need to continuously grow the buffer.“How about the  field?”Every time Protocol Buffers needs to do something with your message (like converting it to bytes), it uses the  field to find out how to handle that message. Every Protobuf message in Go needs to support  (introspection of its structure at runtime).The  field provides a allocation-free way to implement this by leveraging the  package—a technique that, while risky, improves performance by manipulating memory directly.When you write , the  field isn’t there; it is lazily set and initialized when you encode or decode. Therefore, the first time you encode or decode, there might be a slight performance overhead.And… that’s also the final part of this discussion on the way to understand how Protobuf works.Hi, I’m Phuong Le, a software engineer at VictoriaMetrics. The writing style above focuses on clarity and simplicity, explaining concepts in a way that’s easy to understand, even if it’s not always perfectly aligned with academic precision.If you spot anything that’s outdated or if you have questions, don’t hesitate to reach out. You can drop me a DM on X(@func25).If you want to monitor your services, track metrics, and see how everything performs, you might want to check out VictoriaMetrics. It’s a fast, , and cost-saving way to keep an eye on your infrastructure.And we’re Gophers, enthusiasts who love researching, experimenting, and sharing knowledge about Go and its ecosystem.]]></content:encoded></item><item><title>The OBS Project is threatening Fedora Linux with legal action, due to &quot;users complaining upstream thinking they are being served the official package&quot;, when they&apos;re actually using the Fedora Flatpak. The latter is claimed as being &quot;poorly packaged and broken&quot;.</title><link>https://gitlab.com/fedora/sigs/flatpak/fedora-flatpaks/-/issues/39#note_2344970813</link><author>/u/Supermath101</author><category>dev</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 21:20:14 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 5: The Training</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-5-the-training/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 21:04:32 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[In this fifth part of my series, I will outline the steps for creating a Docker container for training your image classification model, evaluating performance, and preparing for deployment.AI/ML engineers would prefer to focus on model training and data engineering, but the reality is that we also need to understand the infrastructure and mechanics behind the scenes.I hope to share some tips, not only to get your training run running, but how to streamline the process in a cost efficient manner on cloud resources such as Kubernetes.I will reference elements from my previous articles for getting the best model performance, so be sure to check out Part 1 and Part 2 on the data sets, as well as Part 3 and Part 4 on model evaluation.Here are the learnings that I will share with you, once we lay the groundwork on the infrastructure:Building your Docker containerExecuting your training runFirst, let me provide a brief description of the setup that I created, specifically around Kubernetes. Your setup may be entirely different, and that is just fine. I simply want to set the stage on the infrastructure so that the rest of the discussion makes sense.This is a server you deploy that provides a user interface to for your subject matter experts to label and evaluate images for the image classification application. The server can run as a pod on your Kubernetes cluster, but you may find that running a dedicated server with faster disk may be better.Image files are stored in a directory structure like the following, which is self-documenting and easily modified.Image_Library/
  - cats/
    - image1001.png
  - dogs/
    - image2001.pngIdeally, these files would reside on local server storage (instead of cloud or cluster storage) for better performance. The reason for this will become clear as we see what happens as the image library grows.Cloud Storage allows for a virtually limitless and convenient way to share files between systems. In this case, the image library on your management system could access the same files as your Kubernetes cluster or Docker engine.However, the downside of cloud storage is the latency to open a file. Your image library will have  of images, and the latency to read each file will have a significant impact on your training run time. Longer training runs means more cost for using the expensive GPU processors!The way that I found to speed things up is to create a  file of your image library on your management system and copy them to cloud storage. Even better would be to create multiple tar files , each containing 10,000 to 20,000 images.This way you only have network latency on a handful of files (which contain thousands, once extracted) and you start your training run much sooner.Kubernetes or Docker engineA Kubernetes cluster, with proper configuration, will allow you to dynamically scale up/down nodes, so you can perform your model training on GPU hardware as needed. Kubernetes is a rather heavy setup, and there are other container engines that will work.The technology options change constantly!The main idea is that you want to spin up the resources you need — for only as long as you need them — then scale down to reduce your time (and therefore cost) of running expensive GPU resources.Once your GPU node is started and your Docker container is running, you can extract the  files above to  storage, such as an , on your node. The node typically has high-speed SSD disk, ideal for this type of workload. There is one caveat — the storage capacity on your node must be able to handle your image library.Assuming we are good, let’s talk about building your Docker container so that you can train your model on your image library.Building your Docker containerBeing able to execute a training run in a consistent manner lends itself perfectly to building a Docker container. You can “pin” the version of libraries so you know exactly how your scripts will run every time. You can version control your containers as well, and revert to a known good image in a pinch. What is really nice about Docker is you can run the container pretty much anywhere.The tradeoff when running in a container, especially with an Image Classification model, is the speed of file storage. You can attach any number of volumes to your container, but they are usually  attached, so there is latency on each file read. This may not be a problem if you have a small number of files. But when dealing with hundreds of thousands of files like image data, that latency adds up!This is why using the  file method outlined above can be beneficial.Also, keep in mind that Docker containers could be terminated unexpectedly, so you should make sure to store important information outside the container, on cloud storage or a database. I’ll show you how below.Knowing that you will need to run on GPU hardware (here I will assume Nvidia), be sure to select the right base image for your Dockerfile, such as  with the “devel flavor that will contain the right drivers.Next, you will add the script files to your container, along with a “batch” script to coordinate the execution. Here is an example Dockerfile, and then I’ll describe what each of the scripts will be doing.#####   Dockerfile   #####
FROM nvidia/cuda:12.8.0-devel-ubuntu24.04

# Install system software
RUN apt-get -y update && apg-get -y upgrade
RUN apt-get install -y python3-pip python3-dev

# Setup python
WORKDIR /app
COPY requirements.txt
RUN python3 -m pip install --upgrade pip
RUN python3 -m pip install -r requirements.txt

# Pythong and batch scripts
COPY ExtractImageLibrary.py .
COPY Training.py .
COPY Evaluation.py .
COPY ScorePerformance.py .
COPY ExportModel.py .
COPY BulkIdentification.py .
COPY BatchControl.sh .

# Allow for interactive shell
CMD tail -f /dev/nullDockerfiles are declarative, almost like a cookbook for building a small server — you know what you’ll get every time. Python libraries benefit, too, from this declarative approach. Here is a sample  file that loads the TensorFlow libraries with CUDA support for GPU acceleration.#####   requirements.txt   #####
numpy==1.26.3
pandas==2.1.4
scipy==1.11.4
keras==2.15.0
tensorflow[and-cuda]Extract Image Library scriptIn Kubernetes, the Docker container can access local, high speed storage on the physical node. This can be achieved via the  volume type. As mentioned before, this will only work if the local storage on your node can handle the size of your library.#####   sample 25GB emptyDir volume in Kubernetes   #####
containers:
  - name: training-container
    volumeMounts:
      - name: image-library
        mountPath: /mnt/image-library
volumes:
  - name: image-library
    emptyDir:
      sizeLimit: 25GiYou would want to have another  to your cloud storage where you have the  files. What this looks like will depend on your provider, or if you are using a persistent volume claim, so I won’t go into detail here.Now you can extract the  files — ideally in parallel for an added performance boost — to the local mount point.As AI/ML engineers, the model training is where we want to spend most of our time.This is where the magic happens!With your image library now extracted, we can create our train-validation-test sets, load a pre-trained model or build a new one, fit the model, and save the results.One key technique that has served me well is to load the most recently trained model as my base. I discuss this in more detail in Part 4 under “Fine tuning”, this results in faster training time and significantly improved model performance.Be sure to take advantage of the local storage to checkpoint your model during training since the models are quite large and you are paying for the GPU even while it sits idle writing to disk.This of course raises a concern about what happens if the Docker container dies part-way though the training. The risk is (hopefully) low from a cloud provider, and you may not want an incomplete training anyway. But if that does happen, you will at least want to understand , and this is where saving the main log file to cloud storage (described below) or to a package like MLflow comes in handy.After your training run has completed and you have taken proper precaution on saving your work, it is time to see how well it performed.Normally this evaluation script will pick up on the model that just finished. But you may decide to point it at a previous model version through an interactive session. This is why have the script as stand-alone.With it being a separate script, that means it will need to read the completed model from disk — ideally local disk for speed. I like having two separate scripts (training and evaluation), but you might find it better to combine these to avoid reloading the model.Now that the model is loaded, the evaluation script should generate predictions on  image in the training, validation, test, and benchmark sets. I save the results as a  matrix with the softmax confidence score for each class label. So, if there are 1,000 classes and 100,000 images, that’s a table with 100 million scores!I save these results in  files that are then used in the score generation next.Taking the matrix of scores produced by the evaluation script above, we can now create various metrics of model performance. Again, this process could be combined with the evaluation script above, but my preference is for independent scripts. For example, I might want to regenerate scores on previous training runs. See what works for you.Here are some of the  functions that produce useful insights like F1, log loss, AUC-ROC, Matthews correlation coefficient.from sklearn.metrics import average_precision_score, classification_report
from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_scoreAside from these basic statistical analyses for each dataset (train, validation, test, and benchmark), it is also useful to identify:Which  labels get the most number of errors?Which  labels get the most number of incorrect guesses?How many ground-truth-to-predicted label pairs are there? In other words, which classes are easily confused?What is the  when applying a minimum softmax confidence score threshold?What is the  above that softmax threshold?For the “difficult” benchmark sets, do you get a sufficiently  score?For the “out-of-scope” benchmark sets, do you get a sufficiently  score?As you can see, there are multiple calculations and it’s not easy to come up with a single evaluation to decide if the trained model is good enough to be moved to production.In fact, for an image classification model, it is helpful to manually review the images that the model got wrong, as well as the ones that got a low softmax confidence score. Use the scores from this script to create a list of images to manually review, and then get a  for how well the model performs.Check out Part 3 for more in-depth discussion on evaluation and scoring.All of the heavy lifting is done by this point. Since your Docker container will be shutdown soon, now is the time to copy the model artifacts to cloud storage and prepare them for being put to use.The example Python code snippet below is more geared to Keras and TensorFlow. This will take the trained model and export it as a . Later, I will show how this is used by TensorFlow Serving in the  section below.# Increment current version of model and create new directory
next_version_dir, version_number = create_new_version_folder()

# Copy model artifacts to the new directory
copy_model_artifacts(next_version_dir)

# Create the directory to save the model export
saved_model_dir = os.path.join(next_version_dir, str(version_number))

# Save the model export for use with TensorFlow Serving
tf.keras.backend.set_learning_phase(0)
model = tf.keras.models.load_model(keras_model_file)
tf.saved_model.save(model, export_dir=saved_model_dir)This script also copies the other training run artifacts such as the model evaluation results, score summaries, and log files generated from model training. Don’t forget about your label map so you can give human readable names to your classes!Bulk identification scriptYour training run is complete, your model has been scored, and a new version is exported and ready to be served. Now is the time to use this latest model to assist you on trying to identify unlabeled images.As I described in Part 4, you may have a collection of “unknowns” — really good pictures, but no idea what they are. Let your new model provide a best guess on these and record the results to a file or a database. Now you can create filters based on closest match and by high/low scores. This allows your subject matter experts to leverage these filters to find new image classes, add to existing classes, or to remove images that have very low scores and are no good.By the way, I put this step inside the GPU container since you may have thousands of “unknown” images to process and the accelerated hardware will make light work of it. However, if you are not in a hurry, you could perform this step on a separate CPU node, and shutdown your GPU node sooner to save cost. This would especially make sense if your “unknowns” folder is on slower cloud storage.All of the scripts described above perform a specific task — from extracting your image library, executing model training, performing evaluation and scoring, exporting the model artifacts for deployment, and perhaps even bulk identification.One script to rule them allTo coordinate the entire show, this batch script gives you the entry point for your container and an easy way to trigger everything. Be sure to produce a log file in case you need to analyze any failures along the way. Also, be sure to write the log to your cloud storage in case the container dies unexpectedly.#!/bin/bash
# Main batch control script

# Redirect standard output and standard error to a log file
exec > /cloud_storage/batch-logfile.txt 2>&1

/app/ExtractImageLibrary.py
/app/Training.py
/app/Evaluation.py
/app/ScorePerformance.py
/app/ExportModel.py
/app/BulkIdentification.pyExecuting your training runSo, now it’s time to put everything in motion…Let’s go through the steps to prepare your image library, fire up your Docker container to train your model, and then examine the results.Image library ‘tar’ filesYour image management system should now create a  file backup of your data. Since  is a single-threaded function, you will get significant speed improvement by creating multiple tar files in parallel, each with a portion of you data.Now these files can be copied to your shared cloud storage for the next step.All the hard work you put into creating your container (described above) will be put to the test. If you are running Kubernetes, you can create a Job that will execute the  script.Inside the Kubernetes Job definition, you can pass environment variables to adjust the execution of your script. For example, the batch size and number of epochs are set here and then pulled into your Python scripts, so you can alter the behavior without changing your code.#####   sample Job in Kubernetes   #####
containers:
  - name: training-job
    env:
      - name: BATCH_SIZE
        value: 50
      - name: NUM_EPOCHS
        value: 30
    command: ["/app/BatchControl.sh"]Once the Job is completed, be sure to verify that the GPU node properly scales back down to zero according to your scaling configuration in Kubernetes — you don’t want to be saddled with a huge bill over a simple configuration error.With the training run complete, you should now have model artifacts saved and can examine the performance. Look through the metrics, such as F1 and log loss, and benchmark accuracy for high softmax confidence scores.As mentioned earlier, the reports only tell part of the story. It is worth the time and effort to manually review the images that the model got wrong or where it produced a low confidence score.Don’t forget about the bulk identification. Be sure to leverage these to locate new images to fill out your data set, or to find new classes.Once you have reviewed your model performance and are satisfied with the results, it is time to modify your TensorFlow Serving container to put the new model into production.TensorFlow Serving is available as a Docker container and provides a very quick and convenient way to serve your model. This container can listen and respond to API calls for your model.Let’s say your new model is version 7, and your  script (see above) has saved the model in your cloud share as /image_application/models/007. You can start the TensorFlow Serving container with that volume mount. In this example, the  points to folder for version 007.#####   sample TensorFlow pod in Kubernetes   #####
containers:
  - name: tensorflow-serving
    image: bitnami/tensorflow-serving:2.18.0
    ports:
      - containerPort: 8501
    env:
      - name: TENSORFLOW_SERVING_MODEL_NAME
        value: "image_application"
    volumeMounts:
      - name: models-subfolder
        mountPath: "/bitnami/model-data"

volumes:
  - name: models-subfolder
    azureFile:
      shareName: "image_application/models/007"A subtle note here — the export script should create a sub-folder, named 007 (same as the base folder), with the saved model export. This may seem a little confusing, but TensorFlow Serving will mount this share folder as  and detect the numbered sub-folder inside it for the version to serve. This will allow you to query the API for the model version as well as the identification.As I mentioned at the start of this article, this setup has worked for my situation. This is certainly not the only way to approach this challenge, and I invite you to customize your own solution.I wanted to share my hard-fought learnings as I embraced cloud services in Kubernetes, with the desire to keep costs under control. Of course, doing all this while maintaining a high level of model performance is an added challenge, but one that you can achieve.I hope I have provided enough information here to help you with your own endeavors. Happy learnings!]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 3: The Evaluation</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 21:00:06 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[In this third part of my series, I will explore the evaluation process which is a critical piece that will lead to a cleaner data set and elevate your model performance. We will see the difference between evaluation of a  model (one not yet in production), and evaluation of a  model (one making real-world predictions).In Part 1, I discussed the process of labelling your image data that you use in your Image Classification project. I showed how to define “good” images and create sub-classes. In Part 2, I went over various data sets, beyond the usual train-validation-test sets, such as benchmark sets, plus how to handle synthetic data and duplicate images.Evaluation of the trained modelAs machine learning engineers we look at accuracy, F1, log loss, and other metrics to decide if a model is ready to move to production. These are all important measures, but from my experience, these scores can be deceiving especially as the number of classes grows.Although it can be time consuming, I find it very important to manually review the images that the model gets , as well as the images that the model gives a  softmax “confidence” score to. This means adding a step immediately after your training run completes to calculate scores for  images — training, validation, test, and the benchmark sets. You only need to bring up for manual review the ones that the model had problems with. This should only be a small percentage of the total number of images. See the Double-check process belowWhat you do during the manual evaluation is to put yourself in a “” to ensure that the labelling standards are being followed that you setup in Part 1. Ask yourself:“Is this a good image?” Is the subject front and center, and can you clearly see all the features?“Is this the correct label?” Don’t be surprised if you find wrong labels.You can either remove the bad images or fix the labels if they are wrong. Otherwise you can keep them in the data set and force the model to do better next time. Other questions I ask are:“Why did the model get this wrong?”“Why did this image get a low score?”“What is it about the image that caused confusion?”Sometimes the answer has nothing to do with  specific image. Frequently, it has to do with the  images, either in the ground truth class or in the predicted class. It is worth the effort to Double-check all images in both sets if you see a consistently bad guess. Again, don’t be surprised if you find poor images or wrong labels.When doing the evaluation of the trained model (above), we apply a lot of subjective analysis — “Why did the model get this wrong?” and “Is this a good image?” From these, you may only get a .Frequently, I will decide to hold off moving a model forward to production based on that gut feel. But how can you justify to your manager that you want to hit the brakes? This is where putting a more  analysis comes in by creating a weighted average of the softmax “confidence” scores.In order to apply a weighted evaluation, we need to identify sets of classes that deserve adjustments to the score. Here is where I create a list of “commonly confused” classes.Commonly confused classesCertain animals at our zoo can easily be mistaken. For example, African elephants and Asian elephants have different ear shapes. If your model gets these two mixed up, that is not as bad as guessing a giraffe! So perhaps you give partial credit here. You and your subject matter experts (SMEs) can come up with a list of these pairs and a weighted adjustment for each.This weight can be factored into a modified cross-entropy loss function in the equation below. The back half of this equation will reduce the impact of being wrong for specific pairs of ground truth and prediction by using the “weight” function as a lookup. By default, the weighted adjustment would be 1 for all pairings, and the commonly confused classes would get something like 0.5.In other words, it’s better to be unsure (have a  confidence score) when you are wrong, compared to being super confident and wrong.Once this weighted log loss is calculated, I can compare to previous training runs to see if the new model is ready for production.Confidence threshold reportAnother valuable measure that incorporates the confidence threshold (in my example, 95) is to report on accuracy and false positive rates. Recall that when we apply the confidence threshold before presenting results, we help reduce false positives from being shown to the end user.In this table, we look at the breakdown of “true positive above 95” for each data set. We get a sense that when a “good” picture comes through (like the ones from our train-validation-test set) it is very likely to surpass the threshold, thus the user is “happy” with the outcome. Conversely, the “false positive above 95” is extremely low for good pictures, thus only a small number of our users will be “sad” about the results.We expect the train-validation-test set results to be exceptional since our data is curated. So, as long as people take “good” pictures, the model should do very well. But to get a sense of how it does on extreme situations, let’s take a look at our benchmarks.The “difficult” benchmark has more modest true positive and false positive rates, which reflects the fact that the images are more challenging. These values are much easier to compare across training runs, so that lets me set a min/max target. So for example, if I target a minimum of 80% for true positive, and maximum of 5% for false positive on this benchmark, then I can feel confident moving this to production.The “out-of-scope” benchmark has no true positive rate because  of the images belong to any class the model can identify. Remember, we picked things like a bag of popcorn, etc., that are not zoo animals, so there cannot be any true positives. But we do get a false positive rate, which means the model gave a confident score to that bag of popcorn as some animal. And if we set a target maximum of 10% for this benchmark, then we may not want to move it to production.Right now, you may be thinking, “Well, what animal did it pick for the bag of popcorn?” Excellent question! Now you understand the importance of doing a manual review of the images that get bad results.Evaluation of the deployed modelThe evaluation that I described above applies to a model immediately after . Now, you want to evaluate how your model is doing in the . The process is similar, but requires you to shift to a “” and asking yourself, “Did the model get this correct?” and “Should it have gotten this correct?” and “Did we tell the user the right thing?”So, imagine that you are logging in for the morning — after sipping on your cold brew coffee, of course — and are presented with 500 images that your zoo guests took yesterday of different animals. Your job is to determine how satisfied the guests were using your model to identify the zoo animals.Using the softmax “confidence” score for each image, we have a threshold before presenting results. Above the threshold, we tell the guest what the model predicted. I’ll call this the “happy path”. And below the threshold is the “sad path” where we ask them to try again.Your review interface will first show you all the “happy path” images one at a time. This is where you ask yourself, “Did we get this right?” Hopefully, yes!But if not, this is where things get tricky. So now you have to ask, “Why not?” Here are some things that it could be:“Bad” picture — Poor lighting, bad angle, zoomed out, etc — refer to your labelling standards.Out-of-scope — It’s a zoo animal, but unfortunately one that isn’t found in  zoo. Maybe it belongs to another zoo (your guest likes to travel and try out your app). Consider adding these to your data set.Out-of-scope — It’s not a zoo animal. It could be an animal in your zoo, but not one typically  there, like a neighborhood sparrow or mallard duck. This might be a candidate to add.Out-of-scope — It’s something found in the zoo. A zoo usually has interesting trees and shrubs, so people might try to identify those. Another candidate to add.Prankster — Completely out-of-scope. Because people like to play with technology, there’s the possibility you have a prankster that took a picture of a bag of popcorn, or a soft drink cup, or even a selfie. These are hard to prevent, but hopefully get a low enough score (below the threshold) so the model did not identify it as a zoo animal. If you see enough pattern in these, consider creating a class with special handling on the front-end.After reviewing the “happy path” images, you move on to the “sad path” images — the ones that got a low confidence score and the app gave a “sorry, try again” message. This time you ask yourself, “ the model have given this image a higher score?” which would have put it in the “happy path”. If so, then you want to ensure these images are added to the training set so next time it will do better. But most of time, the low score reflects many of the “bad” or out-of-scope situations mentioned above.Perhaps your model performance is suffering and it has nothing to do with your model. Maybe it is the ways you users interacting with the app. Keep an eye out of non-technical problems and share your observations with the rest of your team. For example:Are your users using the application in the ways you expected?Are they not following the instructions?Do the instructions need to be stated more clearly?Is there anything you can do to improve the experience?Collect statistics and new imagesBoth of the manual evaluations above open a gold mine of data. So, be sure to collect these statistics and feed them into a dashboard — your manager and your future self will thank you!Keep track of these stats and generate reports that you and your can reference:How often the model is being called?What times of the day, what days of the week is it used?Are your system resources able to handle the peak load?What classes are the most common?After evaluation, what is the accuracy for each class?What is the breakdown for confidence scores?How many scores are above and below the confidence threshold?The single best thing you get from a deployed model is the additional real-world images! You can add these now images to improve coverage of your existing zoo animals. But more importantly, they provide you insight on  classes to add. For example, let’s say people enjoy taking a picture of the large walrus statue at the gate. Some of these may make sense to incorporate into your data set to provide a better user experience.Creating a new class, like the walrus statue, is not a huge effort, and it avoids the false positive responses. It would be more embarrassing to identify a walrus statue as an elephant! As for the prankster and the bag of popcorn, you can configure your front-end to quietly handle these. You might even get creative and have fun with it like, “Thank you for visiting the food court.”It is a good idea to double-check your image set when you suspect there may be problems with your data. I’m not suggesting a top-to-bottom check, because that would a monumental effort! Rather specific classes that you suspect could contain bad data that is degrading your model performance.Immediately after my training run completes, I have a script that will use this new model to generate predictions for my  data set. When this is complete, it will take the list of incorrect identifications, as well as the low scoring predictions, and automatically feed that list into the Double-check interface.This interface will show, one at a time, the image in question, alongside an example image of the ground truth and an example image of what the model predicted. I can visually compare the three, side-by-side. The first thing I do is ensure the original image is a “good” picture, following my labelling standards. Then I check if the ground-truth label is indeed correct, or if there is something that made the model think it was the predicted label.Remove the original image if the image quality is poor.Relabel the image if it belongs in a different class.During this manual evaluation, you might notice dozens of the same wrong prediction. Ask yourself why the model made this mistake when the images seem perfectly fine. The answer may be some incorrect labels on images in the ground truth, or even in the predicted class!Don’t hesitate to add those classes and sub-classes back into the Double-check interface and step through them all. You may have 100–200 pictures to review, but there is a good chance that one or two of the images will stand out as being the culprit.With a different mindset for a trained model versus a deployed model, we can now evaluate performances to decide which models are ready for production, and how well a production model is going to serve the public. This relies on a solid Double-check process and a critical eye on your data. And beyond the “gut feel” of your model, we can rely on the benchmark scores to support us.In Part 4, we kick off the training run, but there are some subtle techniques to get the most out of the process and even ways to leverage throw-away models to expand your library image data.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 1: The Data</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:55:53 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[It is said that in order for a machine learning model to be successful, you need to have good data. While this is true (and pretty much obvious), it is extremely difficult to define, build, and sustain good data. Let me share with you the unique processes that I have learned over several years building an ever-growing image classification system and how you can apply these techniques to your own application.With persistence and diligence, you can avoid the classic “garbage in, garbage out”, maximize your model accuracy, and demonstrate real business value.In this series of articles, I will dive into the care and feeding of a multi-class, single-label image classification app and what it takes to reach the highest level of performance. I won’t get into any coding or specific user interfaces, just the main concepts that you can incorporate to suit your needs with the tools at your disposal.Here is a brief description of the articles. You will notice that the model is last on the list since we need to focus on curating the data first and foremost:Over the past six years, I have been primarily focused on building and maintaining an image classification application for a manufacturing company. Back when I started, most of the software did not exist or was too expensive, so I created these from scratch. In this time, I have deployed two identifier applications, the largest handles 1,500 classes and achieves 97–98% accuracy.It was about eight years ago that I started online studies for Data Science and machine learning. So, when the exciting opportunity to create an AI application presented itself, I was prepared to build the tools I needed to leverage the latest advancements. I jumped in with both feet!I quickly found that building and deploying a model is probably the easiest part of the job. Feeding high quality data into the model is the best way to improve performance, and that requires focus and patience. Attention to detail is what I do best, so this was a perfect fit.It all starts with the dataI feel that so much attention is given to the model selection (deciding which neural network is best) and that the data is just an afterthought. I have found the hard way that even one or two pieces of bad data can significantly impact model performance, so that is where we need to focus.For example, let’s say you train the classic cat versus dog image classifier. You have 50 pictures of cats and 50 pictures of dogs, however one of the “cats” is clearly (objectively) a picture of a dog. The computer doesn’t have the luxury of ignoring the mislabelled image, and instead adjusts the model weights to make it fit. Square peg meets round hole.Another example would be a picture of a cat that climbed up into a tree. But when you take a wholistic view of it, you would describe it as a picture of a tree (first) with a cat (second). Again, the computer doesn’t know to ignore the big tree and focus on the cat — it will start to identify trees as cats, even if there is a dog. You can think of these pictures as outliers and should be removed.It doesn’t matter if you have the best neural network in the world, you can count on the model making poor predictions when it is trained on “bad” data. I’ve learned that any time I see the model make mistakes, it’s time to review the data.Example Application — Zoo animalsFor the rest of this write-up, I will use an example of identifying zoo animals. Let’s assume your goal is to create a mobile app where guests at the zoo can take pictures of the animals they see and have the app identify them. Specifically, this is a multi-class, single-label application. — There are a lot of different animals at the zoo and many of them look very similar. — Guests using the app don’t always take good pictures (zoomed out, blurry, too dark), so we don’t want to provide an answer if the image is poor. — The zoo keeps expanding and adding new species all the time. — Occasionally you might find that people take pictures of the sparrows near the food court grabbing some dropped popcorn. — Just for fun, guests may take a picture of the bag of popcorn just to see what it comes back with.These are all real challenges — being able to tell the subtle differences between animals, handling out-of-scope cases, and just plain poor images.Before we get there, let’s start from the beginning.There are a lot of tools these days to help you with this part of the process, but the challenge remains the same — collecting, labelling, and curating the data.Having data to collect is challenge #1. Without images, you have nothing to train. You may need to get creative on sourcing the data, or even creating synthetic data. More on that later.A quick note about image pre-processing. I convert all my images to the input size of my neural network and save them as PNG. Inside this square PNG, I preserve the aspect ratio of the original picture and fill the background black. I don’t stretch the image nor crop any features out. This also helps center the subject.Challenge #2 is to establish standards for data quality…and ensure that these standards are followed! These standards will guide you toward that “good” data. And this assumes, of course, correct labels. Having both is much easier said than done!I hope to show how “good” and “correct” actually go hand-in-hand, and how important it is to apply these standards to every image.First, I want to point out that the image data discussed here is for the training set. What qualifies as a good image for  is a bit different than what qualifies as a good image for . More on that in Part 3.So, what is “good” data when talking about images? “A picture is worth a thousand words”, and if the  you use to describe the picture do not include the subject you are trying to label, then it is not good and you need remove it from your training set.For example, let’s say you are shown a picture of a zebra and (removing bias toward your application) you describe it as an “open field with a zebra in the distance”. In other words, if “open field” is the first thing you notice, then you likely do  want to use that image. The opposite is also true — if the picture is way too close, you would described it as “zebra pattern”.What you want is a description like, “a zebra, front and center”. This would have your subject taking up about 80–90% of the total frame. Sometimes I will take the time to crop the original image so the subject is framed properly.Keep in mind the use of image augmentation at the time of training. Having that buffer around the edges will allow “zoom in” augmentation. And “zoom out” augmentation will simulate smaller subjects, so don’t start out less than 50% of the total frame for your subject since you lose detail.Another aspect of a “good” image relates to the label. If you can only see the back side of your zoo animal, can you really tell, for example, that it is a cheetah versus a leopard? The key identifying features need to be visible. If a human struggles to identify it, you can’t expect the computer to learn anything.What does a “bad” image look like? Here is what I frequently watch out for:Wide angle lens stretchingHigh contrast or dark shadows“Doctored” images, drawn lines and arrows“Unusual” angles or situationsPicture of a mobile device that has a picture of your subjectIf you have a team of subject matter experts (SMEs) on hand to label the images, you are in a good starting position. Animal trainers at the zoo know the various species, and can spot the differences between, for example, a chimpanzee and a bonobo.To a Machine Learning Engineer, it is easy for you to assume all labels from your SMEs are correct and move right on to training the model. However, even experts make mistakes, so if you can get a second opinion on the labels, your error rate should go down.In reality, it can be prohibitively expensive to get one, let alone two, subject matter experts to review image labels. The SME usually has years of experience that make them more valuable to the business in other areas of work. My experience is that the machine learning engineer (that’s you and me) becomes the second opinion, and often the first opinion as well.Over time, you can become pretty adept at labelling, but certainly not an SME. If you do have the luxury of access to an expert, explain to them the labelling standards and how these are required for the application to be successful. Emphasize “quality over quantity”.It goes without saying that having a  label is so important. However, all it takes is one or two mislabelled images to degrade performance. These can easily slip into your data set with careless or hasty labelling. So, take the time to get it right.Ultimately, we as the ML engineer are responsible for model performance. So, if we take the approach of only working on model training and deployment, we will find ourselves wondering why performance is falling short.A lot of times, you will come across a really good picture of a very interesting subject, but have no idea what it is! It would be a shame to simply dispose of it. What you can do is assign it a generic label, like “Unknown Bird” or “Random Plant” that are  included in your training set. Later in Part 4, you’ll see how to come back to these images at a later date when you have a better idea what they are, and you’ll be glad you saved them.If you have done any image labelling, then you know how time consuming and difficult it can be. But this is where having a model, even a less-than-perfect model, can help you.Typically, you have a large collection of unlabelled image and you need to go through them one at a time to assign labels. Simply having the model offer a best guess and display the top 3 results lets you step through each image in a matter of seconds!Even if the top 3 results are wrong, this can help you narrow down your search. Over time, newer models will get better, and the labelling process can even be somewhat fun!In Part 4, I will show how you can bulk identify images and take this to the next level for faster labelling.I mentioned the example above of two species that look very similar, the chimpanzee and the bonobo. When you start out building your data set, you may have very sparse coverage of one or both of these species. In machine learning terms, we these “classes”. One option is to roll with what you have and hope that the model picks up on the differences with only a handful of example images.The option that I have used is to merge two or more classes into one, at least temporarily. So, in this case I would create a class called “chimp-bonobo”, which is composed of the limited example pictures of chimpanzee and bonobo species classes. Combined, these may give me enough to train the model on “chimp-bonobo”, with the trade-off that it’s a more generic identification.Sub-classes can even be normal variations. For example,  pink flamingos are grey instead of pink. Or, male and female orangutans have distinct facial features. You wan to have a fairly balanced number of images for these normal variations, and keeping sub-classes will allow you to accomplish this.Don’t be concerned that you are merging completely different looking classes — the neural network does a nice job of applying the “OR” operator. This works both ways — it can help you identify male or female variations as one species, but it can hurt you when “bad” outlier images sneak in like the example “open field with a zebra in the distance.”Over time, you will (hopefully) be able to collect more images of the sub-classes and then be able to successfully split them apart (if necessary) and train the model to identify them separately. This process has worked very well for me. Just be sure to double-check all the images when you split them to ensure the labels didn’t get accidentally mixed up — it will be time well spent.All of this certainly depends on your user requirements, and you can handle this in different ways either by creating a unique class label like “chimp-bonobo”, or at the front-end presentation layer where you notify the user that you have intentionally merged these classes and provide guidance on further refining the results. Even after you decide to split the two classes, you may want to caution the user that the model could be wrong since the two classes are so similar.I realize this was a long write-up for something that on the surface seems intuitive, but these are all areas that I have tripped me up in the past because I didn’t give them enough attention. Once you have a solid understanding of these principles, you can go on to build a successful application.In Part 2, we will take the curated data we collected here to create the classic data sets, with a custom benchmark set that will further enhance your data. Then we will see how best to evaluate our trained model using a specific “training mindset”, and switch to a “production mindset” when evaluating a deployed model.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 4: The Model</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:53:42 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[In this latest part of my series, I will share what I have learned on selecting a model for Image Classification and how to fine tune that model. I will also show how you can leverage the model to accelerate your labelling process, and finally how to justify your efforts by generating usage and performance statistics.In Part 1, I discussed the process of labelling your image data that you use in your image classification project. I showed how define “good” images and create sub-classes. In Part 2, I went over various data sets, beyond the usual train-validation-test sets, with benchmark sets, plus how to handle synthetic data and duplicate images. In Part 3, I explained how to apply different evaluation criteria to a trained model versus a deployed model, and using benchmarks to determine when to deploy a model.So far I have focused a lot of time on labelling and curating the set of images, and also evaluating model performance, which is like putting the cart before the horse. I’m not trying to minimize what it takes to design a massive neural network — this is a very important part of the application you are building. In my case, I spent a few weeks experimenting with different available models before settling on one that fit the bill.Once you pick a model structure, you usually don’t make any major changes to it. For me, six years into deployment, I’m still using the same one. Specifically, I chose Inception V4 because it has a large input image size and an adequate number of layers to pick up on subtle image features. It also performs inference fast enough on CPU, so I don’t need to run expensive hardware to serve the model.Your mileage may vary. But again, the main takeaway is that focusing on your data will pay dividends versus searching for the best model.I will share a process that I found to work extremely well. Once I decided on the model to use, I randomly initialized the weights and let the model train for about 120 epoch before improvements plateau at a fairly modest accuracy, like 93%. At this point, I performed the evaluation of the trained model (see Part 3) to clean up the data set. I also incorporated new images as part of the data pipeline (see Part 1) and prepared the data sets for the next training run.Before starting the next training run, I simply take the last trained model, pop the output layer, and add it back in with random weights. Since the number of output classes are constantly increasing in my case, I have to pop that layer anyway to account for the new number of classes. Importantly, I leave the rest of the trained weights as they were and allow them to continue updating for the new classes.This allows the model to train much faster before improvements stall. After repeating this process dozens of times, the training reaches plateau after about 20 epochs, and the test accuracy can reach 99%! The model is building upon the low-level features that it established from the previous runs while re-learning the output weights to prevent overfitting.It took me a while to trust this process, and for a few years I would train from scratch every time. But after I attempted this and saw the training time (not to mention the cost of cloud GPU) go down while the accuracy continued to go up, I started to embrace the process. More importantly, I continue to see the evaluation metrics of the deployed model return solid performances.During training, you can apply transformations on your images (called “augmentation”) to give you more diversity from you data set. With our zoo animals, it is fairly safe to apply left-right flop, slight rotations clockwise and counterclockwise, and slight resize that will zoom in and out.With these transformations in mind, make sure your images are still able to act as good training images. In other words, an image where the subject is already small will be even smaller with a zoom out, so you probably want to discard the original. Also, some of your original pictures may need to be re-oriented by 90 degrees to be upright since a further rotation would make them look unusual.As I mentioned in Part 1, you can use the trained model to assist you in labelling images one at a time. But the way to take this even further is to have your newly trained model identify hundreds at a time while building a list of the results that you can then filter.Typically, we have large collections of  images that have come in either through regular usage of the application or some other means. Recall from Part 1 assigning “unknown” labels to interesting pictures but you have no clue what it is. By using the bulk identification method, we can sift through the collections quickly to target the labelling once we know what they are.By combining your current image counts with the bulk identification results, you can target classes that need expanded coverage. Here are a few ways you can leverage bulk identification:Increase low image counts — Some of your classes may have just barely made the cutoff to be included in the training set, which means you need more examples to improve coverage. Filter for images that have low counts.Replace staged or synthetic images — Some classes may be built entirely using non-real-world images. These pictures may be good enough to get started with, but may cause performance issues down the road because they look different than what typically comes through. Filter for classes that depend on staged images. — A class in your data set may look like another one. For example, let’s say your model can identify an antelope, and that looks like a gazelle which your model cannot identify yet. Setting a filter for antelope and a lower confidence score may reveal gazelle images that you can label. — You may not have known how to identify the dozens of cute wallaby pictures, so you saved them under “Unknown” because it was a good image. Now that you know what it is, you can filter for its look-alike kangaroo and quickly add a new class.Mass removal of low scores — As a way to clean out your large collection of unlabelled images that have nothing worth labelling, set a filter for lowest scores.Recall the decision I made to have image cutoffs from Part 2, which allows us to ensure an adequate number of example images of a class before we train and server a model to the public. The problem is that you may have a number of classes that are  below your cutoff (in my case, 40) and don’t make it into the model.The way I approach this is with a “throw-away” training run that I do not intend to move to production. I will decrease the lower cutoff from 40 to perhaps 35, build my train-validation-test sets, then train and evaluate like I normally do. The most important part of this is the bulk identification at the end!There is a chance that somewhere in the large collection of unlabelled images I will find the few that I need. Doing the bulk identification with this throw-away model helps find them.One very important aspect of any machine learning application is being able to show usage and performance reports. Your manager will likely want to see how many times the application is being used to justify the expense, and you as the ML engineer will want to see how the latest model is performing compared to the previous one.You should build logging into your model serving to record every transaction going through the system. Also, the manual evaluations from Part 3 should be recorded so you can report on performance for such things as accuracy over time, by model version, by confidence scores, by class, etc. You will be able to detect trends and make adjustments to improve the overall solution.There are a lot of reporting tools, so I won’t recommend one over the other. Just make sure you are collecting as much information as you can to build these dashboards. This will justify the time, effort, and cost associated with maintaining the application.We covered a lot of ground across this four-part series on building an image classification project and deploying it in the real world. It all starts with the data, and by investing the time and effort into maintaining the highest quality image library, you can reach impressive levels of model performance that will gain the trust and confidence of your business partners.As a Machine Learning Engineer, you are primarily responsible for building and deploying your model. But it doesn’t stop there — dive into the data. The more familiar you are with the data, the better you will understand the strengths and weaknesses of your model. Take a close look at the evaluations and use them as an opportunity to adjust the data set.I hope these articles have helped you find new ways to improve your own machine learning project. And by the way, don’t let the machine do all the learning — as humans, our job is to continue our own learning, so don’t ever stop!Thank you for taking this deep dive with me into a data-driven approach to model optimization. I look forward to your feedback and how you can apply this to your own application.]]></content:encoded></item><item><title>Codeberg - We stay strong against hate and hatred</title><link>https://blog.codeberg.org/we-stay-strong-against-hate-and-hatred.html</link><author>/u/FryBoyter</author><category>dev</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 20:39:27 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 2: The Data Sets</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:29:39 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[In Part 1, we discussed the importance of collecting good image data and assigning proper labels for your Image Classification project to be successful. Also, we talked about classes and sub-classes of your data. These may seem pretty straight forward concepts, but it’s important to have a solid understanding going forward. So, if you haven’t, please check it out.Now we will discuss how to build the various data sets and the techniques that have worked well for my application. Then in the next part, we will dive into the evaluation of your models, beyond simple accuracy.I will again use the example zoo animals image classification app.As machine learning engineers, we are all familiar with the train-validation-test sets, but when we include the concept of sub-classes discussed in Part 1, and incorporate to concepts discussed below to set a minimum and maximum image count per class, as well as staged and synthetic data to the mix, the process gets a bit more complicated. I had to create a custom script to handle these options.I will walk you through these concepts before we split the data for training: — Too few images and your model performance will suffer. Too many and you spend more time training than it’s worth. — Your model indicates how confident it is in the predictions. Let’s use that to decide when to present results to the user. — Real-world data is messy and the benchmark sets should reflect that. These need to stretch the model to the limit and help us decide when it is ready for production.Staged and synthetic data — Real-world data is king, but sometimes you need to produce the your own or even generate data to get off the ground. Be careful it doesn’t hurt performance. — Repeat data can skew your results and give you a false sense of performance. Make sure your data is diverse. — Combine sub-classes, apply cutoffs, and create your train-validation-test sets. Now we are ready to get the show started.In my experience, using a minimum of 40 images per class provides descent performance. Since I like to use 10% each for the test set and validation set, that means at least 4 images will be used to check the training set, which feels just barely adequate. Using fewer than 40 images per class, I notice my model evaluation tends to suffer.On the other end, I set a maximum of about 125 images per class. I have found that the performance gains tend to plateau beyond this, so having more data will slow down the training run with little to show for it. Having more than the maximum is fine, and these “overflow” can be added to the test set, so they don’t go to waste.There are times when I will drop the minimum cutoff to, say 35, with no intention of moving the trained model to production. Instead, the purpose is to leverage this throw-away model to find more images from my unlabelled set. This is a technique that I will go into more detail in Part 3.You are likely familiar with the softmax score. As a reminder, softmax is the probability assigned to each label. I like to think of it as a confidence score, and we are interested in the class that receives the highest confidence. Softmax is a value between zero and one, but I find it easier to interpret confidence scores between zero and 100, like a percentage.In order to decide if the model is confident enough with its prediction, I have chosen a threshold of 95. I use this threshold when determining if I want to present results to the user.Scores above the threshold have a better changes of being right, so I can confidently provide the results. Scores below the threshold may not be right — in fact it could be “out-of-scope”, meaning it’s something the model doesn’t know how to identify. So, instead of taking the risk of presenting incorrect results, I instead prompt the user to try again and offer suggestions on how to take a “good” picture.Admittedly this is somewhat arbitrary cutoff, and you should decide for your use-case what is appropriate. In fact, this score could probably be adjusted for each trained model, but this would make it harder to compare performance across models.I will refer to this confidence score frequently in the evaluations section in Part 3.Let me introduce what I call the benchmark sets, which you can think of as extended test sets. These are hand-picked images designed to stretch the limits of your model, and provide a measure for specific classes of your data. Use these benchmarks to justify moving your model to production, and for an objective measure to show to your manager. — These are the “extra credit” images, like the bonus questions a professor would add to the quiz to see which students are paying attention. You need a keen eye to spot the difference between the ground truth and a similar looking class. For example, a cheetah sleeping in the shade that could pass as a leopard if you don’t look closely. — These are the “trick question” images. Our model is trained on zoo animals, but people are known for not following the rules. For example, a zoo guest takes a picture of their child wearing cheetah face paint. — These are your “bread and butter” classes that need to get near perfect scores and zero errors. This would be a make-or-break benchmark for moving to production. — These are your “rare but exceptional” classes that again need to be correct, but reach a minimum score like the confidence threshold.When looking for images to add to the benchmarks, you can likely find them in real-world images from your deployed model. See the evaluation in Part 3.For each benchmark, calculate the min, max, median, and mean scores, and also how many images get scores above and below the confidence threshold. Now you can compare these measures against what is currently in production, and against your minimum requirements, to help decide if the new model is production worthy.Perhaps the biggest hurdle to any supervised machine learning application is having data to train the model. Clearly, “real-world” data that comes from actual users of the application is ideal. However you can’t really collect these until the model is deployed. Chicken and egg problem.One way to get started to is to have volunteers collect “staged” images for you, trying to act like real users. So, let’s have our zoo staff go around taking pictures of the animals. This is a good start, but there will be a certain level of bias introduced in these images. For example, the staff may take the photos over a few days, so you may not get the year-round weather conditions.Another way to get pictures is use computer-generated “synthetic” images. I would avoid these at all costs, to be honest. Based on my experience, the model struggles with these because they look…different. The lighting is not natural, the subject may superimposed on a background and so the edges look too sharp, etc. Granted, some of the AI generated images look very realistic, but if you look closely you may spot something unusual. The neural network in your model will notice these, so be careful.The way that I handle these staged or synthetic images is as a sub-class that gets merged into the training set, but only  giving preference to the real-world images. I cap the number of staged images to 60, so if I have 10 real-world, I now only need 50 staged. Eventually, these staged and synthetic images are phased out completely, and I rely entirely on real-world.One problem that can creep into your image set are duplicate images. These can be exact copies of pictures, or they can be extremely similar. You may think that this is harmless, but imagine having 100 pictures of an elephant that are exactly the same — your model will not know what to do with a different angle of the elephant.Now, let’s say you have only  pictures that are nearly the same. Not so bad, right? Well, here is what can happen to them:Both pictures go in the training set — The model doesn’t learn anything from the repeated image and it wastes time processing them.One goes into the training set, the other goes into the test set — Your test score will be higher, but it is not an accurate evaluation.Both are in the test set — Your test score will be compounded either higher or lower than it should be.None of these will help your model.There are a few ways to find duplicates. The approach I have taken is to calculate a hamming distance on all the pictures and identify the ones that are very close. I have an interface that displays the duplicates and I decide which one I like best, and remove the other.Another way (I haven’t tried this yet) is to create a vector representation of your images. Store these a vector database, and you can do a similarity search to find nearly identical images.Whatever method you use, it is important to clean up the duplicates.Now we are ready to build the traditional training, validation, and test sets. This is no longer a straight forward task since I want to:Merge sub-classes into a main class.Prioritize real-world images over staged or synthetic images.Apply a minimum number of images per class.Apply a maximum number of images per class, sending the “overflow” to the test set.This process is somewhat complicated and depends on how you manage your image library. First, I would recommend keeping your images in a folder structure that has sub-class folders. You can get image counts by using a script to simply read the folders. Second is to keep a configuration of how the sub-classes are merged. To really set yourself up for success, put these image counts and merge rules in a database for faster lookups.My train-validation-test set splits are usually 90–10–0. I originally started out using 80–10–10, but with diligence on keeping the entire data set clean, I noticed validation and test scores became pretty even. This allowed me to increase the training set size, and use “overflow” to become the test set, as well as using the benchmark sets.In this part, we’ve built our data sets by merging sub-classes and using the image count cutoffs. Plus we handle staged and synthetic data as well as cleaning up duplicate images. We also created benchmark sets and defined confidence thresholds, which help us decide when to move a model to production.In Part 3, we will discuss how we are going to evaluate the different model performances. And then finally we will get to the actual model training and the techniques to enhance accuracy.]]></content:encoded></item><item><title>Which LLMs are greedy and which are generous? In the public goods game, players donate tokens to a shared fund that gets multiplied and split equally, but each can profit by free-riding on others.</title><link>https://www.reddit.com/r/artificial/comments/1ios3df/which_llms_are_greedy_and_which_are_generous_in/</link><author>/u/zero0_one1</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 20:02:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Privacy Pass Authentication for Kagi Search</title><link>https://blog.kagi.com/kagi-privacy-pass</link><author>b3n</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 19:57:29 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Today we are announcing a new privacy feature coming to Kagi Search. Privacy Pass is an authentication protocol first introduced by Davidson  in [1], and recently standardized by the IETF as RFCs [2—4].  Our starting point was the excellent Rust implementation of the Privacy Pass protocols by Raphael Robert. At the same time, we are announcing the immediate availability of Kagi’s Tor onion service.In general terms, Privacy Pass allows “Clients” (generally users) to authenticate to “Servers” (like Kagi) in such a way that while the Server can verify that the connecting Client has the right to access its services, it cannot determine which of its rightful Clients is actually connecting. This is particularly useful in the context of a privacy-respecting paid search engine, where the Server wants to ensure that the Client can access the services, and the Client seeks strong guarantees that, for example, the searches are not associated with them.As a privacy-respecting search engine, Kagi’s business model is such that we have no incentive to track what an individual user is searching for. We are in the business of selling a search product, not selling user data or attention.Now, Privacy Pass adds another layer of trust: we can verify that you have the right to search without knowing who you are or what you’re searching for. It’s one thing to promise we won’t track you; it’s another to make it technically impossible. We jumped on the opportunity to implement Privacy Pass as soon as the IETF made it an official standard.This matters, because for many users, privacy isn’t just about incentives and privacy policies; it’s about proof. When we cannot track you even if we wanted to, that’s genuine privacy.Initially, we will be offering Privacy Pass to all our plans with unlimited searches: Professional, Ultimate, Family, and Team plans. Privacy Pass will not be available to Trial and Starter plans due to technical limitations at this moment (see below for more info).
To get started with Kagi Privacy Pass right away:Download the newest version of Kagi’s Orion Browser (for macOS/iOS/iPadOS) with Kagi Privacy Pass natively integrated. You will need at least 0.99.131 for macOS and 1.3.17 for iOS/iPadOS (they are expected to be rolling out globally today).Download the newest version of Kagi for Android app with Kagi Privacy Pass natively integrated. You will need to use at least version 0.29 (this is expected to roll out globally today).If you are already using the Kagi Search extension, you will want to update it to the latest version (0.7.6 on Firefox, 1.2.2.5 on Chrome) to avoid compatibility issues, or simply disable it.Safari is not yet supported due to technical limitations, see the F.A.Q. below.In addition our implementation of Privacy Pass is open sourced and you can find it here.When using Kagi Privacy Pass mode, you’ll be truly anonymous - which means your account settings won’t be available since we can’t identify which user you are.But don’t worry - we’ve made it flexible. You can easily toggle Privacy Pass on or off based on your needs. Think of it as two modes: full features with normal privacy, or maximum privacy with core features. You choose what makes sense for you based on your context and needs.Privacy Pass uses cryptography to allow a client to authenticate to a server by performing a protocol with two phases: token generation and token redemption.In the initial “token generation” phase, the client interacts with the server to generate some authentication “tokens.”For the server to willingly participate in this protocol, the client must prove their “right” to generate tokens.In the case of Kagi’s users, this can be done by presenting their Kagi session cookie to the server.The tokens eventually generated by the client at the end of this phase are indistinguishable from a randomly generated token from the server’s point of view. They cannot be traced back to the user who generated them, or to other tokens generated by the same user at the same or a different time.After token generation is performed, a client can initiate a “token redemption” phase.During this phase, the client actually accesses the services provided by the server, proving the client’s right to access the services by presenting one of the previously generated tokens.Since the previously generated tokens are unknown and unpredictable to the server, the latter can only tell that the client has successfully completed token generation at some point.Technically, we say that the techniques used by Privacy Pass result in the two phases being “unlinkable”. While the server is able to tell whether a token presented for redemption was previously generated by interacting with a rightful client, it cannot link the token to a specific token generation phase.Crucially, tokens are single-use: servers keep track of which tokens have already been redeemed to avoid multiple redemptions. Furthermore, clients should not present the same token twice to prevent different redemption phases from being linked.Tokens have a fixed life span. If they are too old, they will stop being redeemable. In that case, a new token generation phase must be initiated by the client to obtain new tokens.As standardized in [2 - 4], the Privacy Pass protocol is able to accommodate many “architectures.” Our deployment model follows the original architecture presented by Davidson  [1], called “Shared Origin, Attester, Issuer” in § 4 of [2].Here, Kagi plays all the “Server roles” (Attester, Issuer, Origin), and Kagi users play the Client role via the new Kagi browser extensions for Privacy Pass, or via native support in Orion. This is what it looks like in practice:Once installed, and periodically, the browser extension will generate and store a large number of tokens.The user can mark in the extension whether searches should be performed by authenticating classically via a session cookie, or by using Privacy Pass.If the user chooses the second option, they will authenticate to Kagi during the search by redeeming one of the tokens it previously generated.Using Privacy Pass is as easy as clicking a toggle.If you are using the latest version of Orion for macOS,  select Kagi as your search engine in  and then enable the checkbox for showing Privacy Pass options on your toolbar.From there you can easily toggle when you want to use Privacy Pass or standard authentication.On iOS and iPadOS, Kagi Privacy Pass is natively supported in the latest version of the Orion Browser for iOS and iPadOS and takes just a few clicks to enable.Our Android app now supports Privacy Pass mode via an app shortcut. Launching the shortcut allows you to browse Kagi seamlessly in Privacy Pass mode. You can also add the shortcut to your home screen for quick access.This feature lets you either use Kagi exclusively in Privacy Pass mode or switch effortlessly between modes.Chrome and Firefox browser extensionsIf you are using the Kagi Privacy Pass extension for Chrome or Firefox, once installed you should see the Kagi Privacy Pass icon on your toolbar.Once installed, the extension automatically generates tokens. To use them, click the extension icon, and make sure the toggle is on.Note that Safari is not supported at this moment; see the F.A.Q. below for more information.As used by Kagi, Privacy Pass tokens offer various security properties (§ 3.3,  of [2]).These can be a little technical to capture. In a few words, they guarantee that users can trust that their searches authenticated via Privacy Pass cannot be linked to their accounts, and Kagi can rest assured that only legitimate users can correctly authenticate using Privacy Pass. Crucially, the guarantee for users is even against malicious servers that attempt to incorrectly implement the server-side computation, as long as the client-side implementation is correct.Three of these security properties serve to protect our users:Generation-redemption unlinkability: Kagi cannot link the tokens presented during token redemption ( during search) with any specific token generation phase. This means that Kagi will not be able to tell who it is serving search results to, only that it is someone who presented a valid Privacy Pass token.Redemption-redemption unlinkability: Kagi cannot link the tokens presented during two different token redemptions. This means that Kagi will not be able to tell from tokens alone whether two searches are being performed by the same user.No redemption hijacking: an eavesdropper that observes any token generation phase, cannot use the observed information alone to “steal” the tokens from the intended user and redeem them themselves. This means that third parties snooping on a user’s token generation interaction will not be able to steal the tokens. This adds a layer of security on top of the confidentiality attained during token generation by using a TLS-protected connection.Two of these security properties serve to protect Kagi.Correctness: honestly generated tokens will pass Kagi’s validation.One-more-forgery security: a malicious client cannot use knowledge of a correctly generated token to forge a new one. This means that valid tokens cannot be generated without correctly interacting with Kagi, and therefore valid tokens are evidence that the user owned a valid session cookie for a supported Kagi plan at the moment of generating the token.Naturally, online interactions are never fully described by a mathematical model.While the Privacy Pass protocol  indeed guarantee that the server will not be able to link token generation and token redemption phases , in principle, a malicious server could still attempt to track clients via side-channel information.For example, if someone were to make the same specific request to a server at the same time every day (say, searching “lunch places near 123 Mulholland Drive, LA” at 11:58 AM), a server that records all searches being made could, in principle, guess that these searches are all made by the same person.In this case, Privacy Pass would make it harder for the server to determine who this specific person is, but the server could nonetheless link searches to one another.On a level beyond, it is well known that browsers can often have a unique “fingerprint” [5-7]. Fingerprinting attacks heavily rely on side-channel signals that evade the Privacy Pass protocol, such as user-agent strings or IP addresses. For example, if a server receives a token generation request from a given IP address, and immediately after a token redemption request from the same address, it can likely conclude that the same individual is behind the request. For this reason, it is highly recommended to separate token generation and redemption in time, or “in space” (by using an anonymizing service such as Tor when redeeming tokens, see below).Kagi’s Privacy Pass extension and native implementation in Orion take care, as much as we can, to uniform your browser fingerprint, by removing deanonymizing HTTP headers and cookies.We see Privacy Pass as an important tool for increasing the anonymity guarantees we can offer to Kagi users.Adopting state-of-the-art standards for new privacy enhancing technologies also signals to researchers and standardization bodies that there is a public demand for more privacy and anonymity tools in today’s digital world and incentivizes further scrutiny and development of privacy-enhancing technologies.Together with launching Privacy Pass, we are also announcing that we now have a Tor onion service available, which allows access to Kagi directly from the Tor network. Kagi’s onion address is:On its own, Tor will obscure your location by hiding your IP address. However, without Privacy Pass, you still need to be logged into your Kagi account to perform searches, making them all theoretically linkable back to a single account. As always, Kagi does not link searches to accounts or permanently record them; see our Privacy Policy for more info.With Tor and Privacy Pass together, Kagi only knows that the search is being issued by a user who previously verified that they have an account authorized to receive tokens, but nothing about the user’s account, or where they’re located.Privacy Pass support is provided:This should accommodate users who want to install and use the extension across multiple browsers or computers. Please refer to our documentation for usage instructions.At first, Privacy Pass authentication will be available to users on any Kagi plan with unlimited searches. These plans will have a generous allocation of tokens (2000 to begin with) that they can generate monthly.We are working on enabling this feature for Trial and Starter plans, which have access to a limited number of monthly searches. Therefore, they risk a worse user experience if their generated tokens are lost (for example, due to uninstalling the extension) and theoretically, users on this plan could redeem more tokens than the limit of searches allowed on their plan (again, we do not know who the user redeeming the tokens is, or what plan they are on). This makes it more technically challenging to support these plans with Privacy Pass, and we have left that for later.You mention “tokens.” Are blockchains involved in this protocol?Privacy Pass does not rely on any blockchain technology.While the protocol makes use of various cryptographic primitives (specifically, elliptic curves and hash functions, as part of a “verifiable oblivious pseudorandom function” construction, [8]) and generates “tokens,” these are not generated, stored, or traded on a blockchain.You mention the client generating tokens. Is this process energy-intensive or storage-demanding?No. The generation of 500 search tokens requires approximately 1 second of computation on a consumer laptop, and is performed in the background when installing the extension. A few extra seconds may be required due to the time required to contact the server and get a response. Each token consists of 216 bytes, for a total of approximately 100 KiB of storage per token generation request.Is there a potential impact on the speed of search when using Privacy Pass?The initial generation of tokens takes about ~1 second for 500 tokens, plus the time required for contacting the server. This occurs infrequently and is done in the background when possible.Currently, the token validation servers are only deployed in our us-central1 region, we plan to expand this shortly after launch.How many tokens am I able to generate?You can generate 2,000 tokens in one “epoch” (= one month). This should be enough for most users. If you need more than this, you can request additional tokens by contacting support@kagi.com.Do you plan to allow purchasing privacy pass tokens without having an account?Yes, this makes sense. This is possible because technically the extension does not care if you have an account or not. It just needs to be ‘loaded’ with valid tokens. And you can imagine a mechanism where you could also anonymously purchase them, eg. with monero, without ever creating an account at Kagi.  Let us know here if you are excited about this, as it will help prioritize it.How can I submit feedback for Kagi Privacy Pass?We have a feedback thread open here.Even if the extension implements anti-fingerprinting measures, Kagi will still be able to see my IP address, correct?Even with Privacy Pass authentication enabled, due to the way the TCP/IP stack works, we will be able to see your search request come from an IP address. As outlined in our Privacy Policy, your privacy is our priority, whether you are using Privacy Pass to authenticate or otherwise. If you are worried about us seeing your IP address, our suggestion is to connect to Kagi via Tor or through a VPN service you trust.How can Privacy Pass increase my privacy, if I have to send a session cookie to authenticate during token generation?While token generation is indeed not anonymous, Privacy Pass provides you with anonymity .By providing the server with a Privacy Pass token instead of a session cookie when searching, you will guarantee that your searches cannot be  to any specific user account that generated Privacy Pass tokens, or to each other.From the point of view of the server, your search query could have come from any of the users who previously generated Privacy Pass tokens.The more users do so, the lower the probability that the server can guess it was you specifically who made a given search query.Token generation does not work in my Chrome/Firefox private windowCorrect, we need to authenticate you to create tokens (see above), and in the private window, the extension does not have access to your session cookie. Please use a normal browsing window while logged in to Kagi to generate tokens.Note, generating tokens while in a private window will work in the Orion browser.Can I use Kagi Assistant while using Privacy Pass?Not at this time, since Kagi Assistant is only available to Ultimate members. In Privacy Pass, we don’t have any account information, so we can’t validate what plan you’re on. We could issue tokens attached to different keys for different plans, but that also has privacy implications, see the discussion of personalization below.What Kagi services will be compatible with Privacy Pass at launch?At launch, Privacy Pass will only be used to authenticate Kagi Search. Soon to follow (in the next few weeks), we plan to expand support for Kagi Privacy Pass to:Kagi Translate and Kagi MapsKagi universal Summarizer and Ask questions about pagePlease disable Privacy pass to access these services for now.Since initial token generation happens in batches and the tokens expire, could tokens with similar expiration dates potentially be used to identify multiple searches from the same user?All tokens generated during month X expire at midnight of the first day of month X+2, to avoid this exact issue. Meaning a freshly generated token lasts until the end of the month following its generation (generate today, use all of Feb and March).If Kagi cannot track who exactly is performing search queries, will I have access to my account settings including customization and personalization?Since Kagi will not know who you are, we will not be able to serve you content tailored to your custom settings via Privacy Pass-protected search.We have considered allowing users to send a small configuration with every request (language, region, safe-search) to automatically customize your search experience to some extent. However, we currently believe this would quickly result in a significant loss of anonymity for you and for other users. To illustrate this, we have examined the most common configurations of (language, region, search-safe) used on Kagi.com, and extrapolated how many Privacy Pass users would share them. Looking at the top 35 configurations, we see the following approximate numbers.Limiting the analysis to only the ten most common language settings, the effect is similar:This would mean that someone sending us  as their language configuration would automatically lose redemption-redemption unlinkability guarantees if approximately 1000 Kagi users used Privacy Pass.While our extrapolation may be overly conservative, we won’t be enabling this level of “default” customization for users authenticating via Privacy Pass for the time being. We could reconsider if we find a better solution.For manual search settings customization, you can always use bangs in your search query to enable basic settings for a specific query. For example, regional bangs will let you focus your query on one region. For example prefixing your search with  will automatically search in the German region.To access a fully customized search experience, you can always use the traditional login method and disable the use of Privacy Pass.Will Safari be supported?The Safari extensions API doesn’t support (as far as we know) removing cookies from requests, which means it will always authenticate with your logged-in account. We’re not aware of a way to change this. The alternative if you want a similar, native, WebKit-based browsing experience, is to use the Orion Browser which has Kagi Privacy Pass natively integrated.Davidson, A., Goldberg, I., Sullivan, N., Tankersley, G., & Valsorda, F. (2018). Privacy pass: Bypassing internet challenges anonymously. Proceedings on Privacy Enhancing Technologies. Paper.Davidson, A., Iyengar, J., & A. Wood, C. (2024). The Privacy Pass Architecture. RFC 9576.Pauly, T., Valdez, S., & A. Wood, C. (2024). The Privacy Pass HTTP Authentication Scheme. RFC 9577.Celi, S., Davidson, A., Valdez, S., & Wood, C. A. (2024). Privacy Pass Issuance Protocols. RFC 9578.Eckersley, P. (2010). How unique is your web browser? Proceedings on Privacy Enhancing Technologies. Paper.Davidson, A., Faz-Hernandez, A., Sullivan, N., & A. Wood, C. (2023). Oblivious Pseudorandom Functions (OPRFs) Using Prime-Order Groups. RFC 9497.Do you like how this post reads? It was proofread with Kagi Translate’s proofreading option. To proofread any web page, just use https://translate.kagi.com/proofread/[URL].]]></content:encoded></item><item><title>MapTCHA, the open-source CAPTCHA that improves OpenStreetMap [video]</title><link>https://fosdem.org/2025/schedule/event/fosdem-2025-5879-maptcha-the-open-source-captcha-that-improves-openstreetmap/</link><author>raybb</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 19:44:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Bots and spam are challenges for online platforms. Traditional CAPTCHAs help block bots, but often involve improving proprietary maps and software, while exposing user information to third-party CAPTCHA providers. OpenStreetMap (OSM) has many objects remaining to be mapped, but the quality of AI-generated objects is not high enough for direct inclusion. We introduce “MapTCHA”, a CAPTCHA that leverages the uncertainty of interpreting imagery with computer vision, and provides human verification for AI predictions: users are asked to identify images containing correctly interpreted objects, e.g. building outlines.We separate known positive cases, where both the AI prediction and OSM contain an object, from unknown cases, where objects are only in the prediction. We also generate known negatives from areas where objects are neither in OSM nor in the prediction. We show a mix of these images without telling the user which are which. Humans are validated by confirming the known positives and negatives, and we determine the truth of the unknown images by aggregating users’ responses through voting. When the voting indicates high confidence that an object exists, we suggest the location for OSM mapping.Our prototype identifies buildings using aerial imagery with high enough resolution to visualise individual buildings and medium-sized objects. Image recognition is provided by fAIr, an open-source AI-assisted mapping system developed by the Humanitarian OpenStreetMap Team (HOT). It allows the training and fine-tuning of pre-trained machine learning models to segment building footprints.Future plans include expanding to more objects and types of imagery; refining AI models; integrating MapTCHA into various login systems; and enhancing the user interface.In this session we will talk about how we are building this solution, how it might enhance mapping efforts in OSM that will support projects on the ground, and some of the challenges of working with AI derived data.]]></content:encoded></item><item><title>Launching Interop 2025</title><link>https://hacks.mozilla.org/2025/02/interop-2025/</link><author>/u/feross</author><category>dev</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 18:46:26 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Phind 2: AI search with visual answers and multi-step reasoning</title><link>https://www.phind.com/blog/phind-2</link><author>rushingcreek</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 18:20:29 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chip-8 Emulation: Adding control flow and graphics</title><link>https://www.emulationonline.com/systems/chip8/control-flow-and-graphics/</link><author>/u/elemenity</author><category>dev</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 18:07:09 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[And as a bonus, the bit manipulation tricks we used were also instuctions
of the Chip-8. So now we know how to implement a good chunk of basic data
manipulation instructions.Now lets try to do something visual, and add graphical support to our emulator.
By the end of this article, we’ll be able to run the “Chip8 Picture”
test program, which draws the text “Chip8” on the screen.So far, our Chip-8 system executes every instruction in order, one after the other.
The Chip-8, along with most machines, as the ability to implement control flow, where
the program changes the course of instructions that will be executed.This allows programs to implement conditions (if this then do that), and loops
(do this 10 times).We’ll look at the two control flow instructions needed by the picture program.
Afterwards, you should be able to adapt this to implementing the remaining
control flow instructions.Opcode 0x1NNN : Jump to NNNThis is perhaps the simplest way to change the next instruction. If we encounter
an instruction with the pattern 0x1NNN (where NNN can be any value), we set PC
to NNN. Our next instruction then comes from address NNN, instead of the instruction
right after the jump.And thats all that is needed for a jump instruction.Opcode 0x3XNN : Skip next if X == NNThis instruction has two parameters, X and NN. X refers to one
of the 16 registers, and NN is any two digit value.This instruction means “if reg X is NN, skip the next instruction”. Otherwise
execution continues as normal.All chip8 instructions are 2 bytes long. So if we choose to skip, we need to
add 2 to our PC register, since it tracks bytes.And again, that is all that’s needed. With these two instructions, we’re able
to implement loops. Much of graphics code is copying data from one place to another,
so these are used by the test program we aim to run.And now we’re ready to start implementing graphics for the Chip-8. Chip-8
supports monochrome (on or off) graphics, and a screen resolution of
64 pixels wide by 32 pixels tall. Not particularly large, but a surprisingly
large collection of software has been written within these constraints..First we’ll add the graphic memory to our system. We’ll represent this
as a 64 x 32 array of boolean values.This is our “framebuffer”. It holds one frame of video information
as it is prepared for drawing.Chip-8 has a few mechanisms for synchronizing graphics with the game logic.
We won’t worry about these for now. Instead, we’ll just try to get our graphics
on the screen. This will let us confirm that the logic is correct.One approach to drawing graphics on screen would be to integrate with a 2d
graphics library. Since the screen resolution supported by chip-8 is rather
low, we could actually fit the entire output into your terminal’s text-based
output. One character per pixel.We’ll add an extra “draw_screen” function to our system, which will draw
the current state of the screen to the terminal. To make debugging easier,
we’ll call this after every instruction. This way we can watch how the framebuffer
changes over time.The only thing our draw_screen function needs to do, is go through our
display row by row, and output one character for each dot or pixel
in the framebuffer.We’ll use a space for unfilled characters, and an X for filled in characters.
Assuming your console has an equal-width font, the letters should imitate
the grid of a display.Here’s roughly what that looks like:
If you call your drawing code with a freshly initialized framebuffer,
you would expect to see all spaces. You can manually set a pixel at say (x,y) = (5,3),
and make sure something appears on screen.Now that we can display our graphic data, lets implement the instructions that allow
the Chip-8 to manipulate the framebuffer.Opcode 0x00E0 : Clear the screen.This instruction just sets all the pixels in the framebuffer to false. While
this is the initial state of the framebuffer, most programs call this at startup
anyway. We can essentially reuse our draw_screen code. Instead of printing to
the terminal, we’ll set the framebuffer element to false.Now on to the main graphics instruction:Opcode 0xDXYN : Draw a sprite at coordinate VX, VY, N pixels tallThis cleverly selected “D” hex opcode is for drawing a sprite. A sprite
is essentially a small 2d graphic that we’ll copy from memory into the framebuffer.The first parameters are for positioning. Since a single hex digit
can only hold 0 - 15, the X and Y tell us which registers are used for drawing.
So 0xD128 would mean: use register 1 to find the X position, and register 2 for
the Y position. Since registers are each a byte, this allows us to select any
position on screen.Sprites are always 8 pixels wide, but can be arbitrary height (0 through 15).
This is because the sprite information is packed into memory. Each pixel takes
up only 1 bit, so one byte/8 bits holds a full row of sprite data.Consider this triangle we’ll try to draw:       X     0b00000001                    0b10000000 
      XX     0b00000011                    0b11000000
     XXX     0b00000111                    0b11100000
    XXXX     0b00001111  0b1 is leftmost   0b11110000
   XXXXX  => 0b00011111        =>          0b11111000
  XXXXXX     0b00111111                    0b11111100
 XXXXXXX     0b01111111                    0b11111110
XXXXXXXX     0b11111111                    0b11111111
The above is logically how sprite data is represented in memory.
The 1s bit, aka 0b1, is drawn to the left most position within the row.
So to draw the triangle from the left, our graphic data should look
like the far right, not the middle. The bits are mirrored
left/right from what you might expect.Now we know what the graphic data looks like, and where to put it. But
where does it come from? How do we know which sprite is to be drawn? The
draw command will draw the sprite which starts at memory address I. is a special register which is manipulated by some other instructions.
It is used primarily for operations which need to read from memory. It
is a 16 bit register, unlike the other 15 “numbered registers” which are only
8 bits.Now we know all the pieces required to implement rendering logic. There are
a few more quirks in how this instruction works:A 1 bit in the sprite data means “flip the color on screen” rather than “set to 1”. This makes it easy to implement effects by overlapping sprites. The
downside is that it makes the implementation a bit trickier. You can always
implement this as “set” and then return to it once that works.While rendering, the opcode keeps track of whether any screen bits switch
from 1 to 0. If so, the VF register is set to 1. This allows for a sort of
built-in collision detection. If you’ve read the NES series, this is a lot like the sprite0 hit. You
can safely skip this for now, and the picture program will still work.Without further ado, our rendering code needs to:Find the registers which specify screen x,y positionsFind the N value which sets the sprite heightFind the I value, which tells us where sprite data startsFor each row in the sprite:read the byte, and copy the bits into memory
optionally flip bits instead of settingoptionally track 1->0 flipsOptionally set register F (aka VF) if a 1->0 flip occurred.Note: You code should also check boundaries, since its possible
for this instruction to try to write beyond the end of the screen.
Failing to handle this could cause corruption depending on your
implementation language.If all goes well, you should have this image on your screen after running the program:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XX         XXXXXXXX  X      X  X  XXXXXXXX  XXXXXXXX          XX
XX         X         X      X  X  X      X  X      X          XX
XX         X         X      X  X  X      X  X      X          XX
XX         X         X      X  X  X      X  X      X          XX
XX         X         X      X  X  X      X  X      X          XX
XX         X         X      X  X  X      X  X      X          XX
XX         X         X      X  X  X      X  X      X          XX
XX         X         XXXXXXXX  X  XXXXXXXX  XXXXXXXX          XX
XX         X         X      X  X  X         X      X          XX
XX         X         X      X  X  X         X      X          XX
XX         X         X      X  X  X         X      X          XX
XX         X         X      X  X  X         X      X          XX
XX         X         X      X  X  X         X      X          XX
XX         X         X      X  X  X         X      X          XX
XX         XXXXXXXX  X      X  X  X         XXXXXXXX          XX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XX                                                            XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Now we’ve addressed graphics and conditional logic. There are a few other conditional
instructions we didn’t mention specifically here. They’re quite similar to the one we
did cover, though, and should be easy to implement on your own.Newly supported instructions:There are 2 control flow operations with a bit of nuance, which we’ll cover in the future.
This brings our emulator from 16/35 supported instructions, up to 25/35.All we have left is recursion, sound, input, and timers. What are you looking forward to most?
Send me an email or message on socials.]]></content:encoded></item><item><title>Workspace and session manager built in go</title><link>https://github.com/GianlucaP106/mynav</link><author>/u/One_Mess_1093</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 17:39:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Lawyers Caught Citing AI-Hallucinated Cases Call It a &apos;Cautionary Tale&apos;</title><link>https://www.404media.co/lawyers-caught-citing-ai-hallucinated-cases-call-it-a-cautionary-tale/</link><author>Samantha Cole</author><category>dev</category><enclosure url="https://www.404media.co/content/images/2025/02/photo-1620424393934-04e772be09f4.jpeg" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:33:00 +0000</pubDate><source url="https://www.404media.co/">Dev - 404</source><content:encoded><![CDATA[After a judge called out a law firm for citing fake cases in court documents, the attorneys admitted to AI “hallucinating” the cases. In a court order filed last week, Wyoming District Judge Kelly Rankin demanded the attorneys explain why they shouldn’t be sanctioned or disciplined for citing made-up information, including referencing eight non-existent cases. , first filed in 2023, is against Walmart and Jetson Electronic Bikes, Inc., which makes hoverboards for sale at Walmart. The plaintiffs, including a woman, her husband, and four minor children, claim a Jetson hoverboard’s lithium ion battery malfunctioned and caught fire while they were sleeping and burned their house down, severely injuring several members of the family.In a motion in limine filed by the plaintiff’s attorneys in January, they cited multiple cases (a "motion in limine" is a pretrial motion where attorneys request specific evidence or arguments be excluded from presentation during the trial) that don’t exist. “The cases are not identifiable by their Westlaw cite, and the Court cannot locate the District of Wyoming cases by their case name in its local Electronic Court Filing System,” Rankin wrote. She demanded each of the attorneys “provide a thorough explanation for how the motion and fake cases were generated,” and “explain their role in drafting or supervising the motion.” Four days later, they responded: “Our internal artificial intelligence platform ‘hallucinated’ the cases in question while assisting our attorney in drafting the motion in limine,” the law firm said in a filed response. “This matter comes with great embarrassment and has prompted discussion and action regarding the training, implementation, and future use of artificial intelligence within our firm. This serves as a cautionary tale for our firm and all firms, as we enter this new age of artificial intelligence.” Lawyers increasingly use AI tools for research and analyzing documents. But this isn’t the first time using AI to draft legal cases has gotten lawyers in trouble. In 2022, a man filed an action alleging he was injured by an Avianca airlines metal serving cart during an Avianca Airlines flight. His lawyers cited non-existent cases, and instead of admitting it and apologizing immediately, they doubled down and defended the filings. Eventually, they were fined $5,000 for fabricating the case,  that they “abandoned their responsibilities when they submitted non-existent judicial opinions with fake quotes and citations created by the artificial intelligence tool ChatGPT, then continued to stand by the fake opinions after judicial orders called their existence into question.” And , disbarred former celebrity attorney Michael Cohen gave his own lawyer, David Schwartz fake case citations generated by Google Bard. Cohen and Schwartz weren’t fined, but the judge who let them off without discipline did call the error “embarrassing” for them. ]]></content:encoded></item><item><title>Coding Interviews were HARD Until I Learned These 20 Tips</title><link>https://blog.algomaster.io/p/20-coding-interviews-tips</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/61c3f6c0-4027-4d37-b4a7-a30fc183fa12_1602x1032.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:30:27 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[I gave my first  in 2016—and failed. I failed the next five interviews as well before finally landing my first job at .Since then, I’ve interviewed with many companies and faced my fair share of rejections. However, over the years, my failure rate in coding interviews dropped significantly.By 2022, with just 1.5 months of focused preparation, I successfully cleared interviews at  and .Surprisingly, my success wasn’t due to a dramatic improvement in problem-solving skills. The real game-changer was my approach— and  during the interview.In this article, I’ll share  that made coding interviews significantly easier for me.These tips cover everything you need to know, including:How to systematically approach coding interview problemsKey concepts and patterns you should knowThe type of problems you should practiceHow to choose the right algorithm for a given problemTechniques to optimize your solutionHow to communicate your thought process effectivelyBy applying these strategies, you’ll be able to tackle coding interviews with confidence and massively increase your chances of success.In a coding interview, interviewers want to see how well you , , and  under pressure.Here's a breakdown of what they look for:Understanding the problem: Do you ask clarifying questions instead of making assumptions to ensure you fully understand the problem?: Can you decompose the problem into smaller, manageable parts?: Can you design an optimal solution in terms of time and space complexity?: Do you handle edge cases like empty inputs, duplicates, large values, or special conditions?: Can you explain why one approach is better than another?: Do you have a strong grasp of data structures and algorithms, and can you choose the right one for the problem?Can you quickly compute the time and space complexity of your solution?Explaining your thought process: Can you clearly articulate your approach and why it works?: Are you receptive to hints and able to adjust your approach accordingly?: Do you follow good coding practices (meaningful variable names, proper indentation, modular functions etc..)?Improving the initial solution: Can you optimize and refine your first solution when prompted?Are you able to tackle variations of the original problem?Can you manually walk through your code with sample inputs to verify correctness?Most coding interviews last Depending on the company and interviewer, you may be asked to solve 2-3easy/medium problems or 1 hard problem with follow-ups.Lets assume you are given one problem, with a follow up in a 45-minute interview. Here’s how you can optimally allocate your time:The interviewer may ask you to introduce yourself. Prepare a concise 1-2 minute introduction that highlights your background, experience, and key strengths. Practice it beforehand so that you can deliver it smoothly.Understand the Problem (5-10 mins):  Carefully read the problem statement, ask clarifying questions, and walk through sample inputs and expected outputs.Plan the Approach (10-20 mins): Brainstorm possible solutions, evaluate trade-offs, and discuss time and space complexity.Implement the Code (20-30 mins): Write a clean, modular and readable code.Dry-run your code with sample inputs, debug any issues, and ensure edge cases are handled.Follow-ups and Wrap Up (35-45 mins): Answer follow up questions, and ask thoughtful questions to the interviewer about the company, role, or team.One of the biggest mistakes candidates make in coding interviews is jumping into coding too soon.If you don't fully understand the question, you might end up solving the Here’s how to ensure you grasp the problem before coding:Read the Problem CarefullyTake a moment to absorb the problem statement. Rephrase it in your own words to confirm your understanding. Identify the expected input/output format and any hidden constraints.If anything is unclear, ask questions before diving into the solution. Interviewers appreciate when you seek clarity. Never assume details that aren’t explicitly mentioned in the problem statement.Common clarifications include:Are there duplicate values?Can the input be empty? If so, what should the output be?Should the solution handle negative numbers?Should the output maintain the original order of elements?Is the graph directed or undirected?Does the input contain only lowercase English letters, or can it have uppercase, digits, or special characters?What should happen if multiple solutions exist? Should I return any valid solution, or does the problem have specific requirements?Walk Through Input/Output ExamplesOnce you understand the problem statement and constraints, go over a few input and output examples to make sure you get it.Draw them out if it helps, especially for visual data structures like trees or graphs.Try to take examples that cover different scenarios of the problem. Think about any  that might come up.]]></content:encoded></item><item><title>The 3 ways of integrating simple AI support in a Go program</title><link>https://sqirvy.xyz/posts/golang-ai/</link><author>/u/quad99</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 17:25:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[It seems to be popular, if not mandatory, to add AI to apps, whether they need it or not. It turns out that with Go its reasonably easy to make queries to AI from inside a Go program. Assume you want to connect to an AI LLM provider, send it a query and get the results back.When integrating Large Language Models (LLMs) into your Go applications, there are several approaches you can take. This post explores three different methods using the Sqirvy client library as examples.The full code described here is in github.com/dmh2000/sqirvy-ai, along with example usage in command line programs and a sample web app. The code includes support for Anthropic, Meta-llama, OpenAI and Gemini models.This Go library implements methods for simple queries to various AI providers.The directory structure for sqirvy-ai. (abbreviated)├── pkg
│   ├── sqirvy             (what this post is about)
│   │   ├── anthropic.go   (use the anthropic native library)
│   │   ├── client.go      (the interface)
│   │   ├── meta-llama.go  (use LangChain)
│   │   ├── openai.go      (use HTTP API)
│   └── util
├── cmd 
│   └── sqirvy : take input from file or stdin and execute a query
└── web
    └── sqirvy-web (a simple web app to compare LLMs)
The Sqirvy library provides an interface for simple queries to various LLM providers. It illustrates  using the different approaches to coding a simple text query.Three Integration Approaches1. Direct HTTP API Integration (OpenAI Example)The most straightforward approach is to directly use the provider’s HTTP API. This gives you complete control but requires more boilerplate code. Here’s how OpenAI integration works in Sqirvy:Full control over request/response handlingDirect mapping to API documentationNeed to handle HTTP details manuallyMust implement retry logic yourself2. Official SDK Integration (Anthropic Example)Using an official SDK provides a more polished experience with built-in types and error handling.Automatic retries and best practicesDependency on external packageLess flexibility for customizationMay lag behind API updatesUsing LangChain provides a unified interface across multiple providers. LangChain has official SDK’s for
Python and Javascript but not Go. There is an ‘unofficial’, but popular and well tested Go SDK.Unified interface across providersRich ecosystem of tools and chainsEasy to switch between providersAdditional abstraction layerMay not expose provider-specific featuresLarger dependency footprintConsider these factors when choosing an approach:Want minimal dependenciesAre only using one providerNeed to match API docs exactlyWant type safety and official supportNeed built-in best practicesAre primarily using one providerValue ease of use over flexibilityNeed to support multiple providersWant access to higher-level abstractionsPlan to build complex chainsValue provider interchangeabilityEach approach has its merits, and the choice depends on your specific needs. Sqirvy demonstrates all three approaches, allowing you to see how they work in practice and choose the one that best fits your use case.Remember to always handle your API keys securely and respect rate limits regardless of which approach you choose.]]></content:encoded></item><item><title>[R] AlignRec Outperforms SOTA Models in Multimodal Recommendations</title><link>https://www.reddit.com/r/MachineLearning/comments/1ioo1ta/r_alignrec_outperforms_sota_models_in_multimodal/</link><author>/u/skeltzyboiii</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 17:13:07 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[AlignRec, introduced in AlignRec: Aligning and Training in Multimodal Recommendations (CIKM '24), tackles misalignment in multimodal recommendation systems. Traditional methods struggle to integrate diverse content types—text, images, and categorical IDs—due to semantic gaps. AlignRec addresses this by optimizing three alignment tasks: inter-content (ICA), content-category (CCA), and user-item (UIA). ICA unifies semantic representations with an attention-based encoder, CCA enhances feature alignment using contrastive learning, and UIA refines user-item representations via cosine similarity loss.A key innovation is AlignRec’s two-stage training: pre-training aligns visual and textual data, while fine-tuning incorporates user behavior for optimized recommendations. Tested on Amazon datasets, it outperforms nine SOTA models, excelling in long-tail recommendations. By bridging multimodal semantic gaps, AlignRec improves both accuracy and robustness, advancing multimodal AI-driven recommendations.]]></content:encoded></item><item><title>Who exactly needs to get approval from an institutional review board (IRB)?</title><link>https://dynomight.net/irb/</link><author>gregorymichael</author><category>dev</category><category>hn</category><pubDate>Thu, 13 Feb 2025 17:13:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Some of my favorite internet people sometimes organize little community experiments. Like, let’s eat potatoes and see if we lose weight. Or, let’s take some supplements and see if anxiety goes down. I’ve toyed with doing one myself, to see if theanine (a chemical in tea) really helps with stress.But sometimes, when everyone is having fun, some very mean very bad people show up and say, “HEY! YOU CAN’T DO THAT! THAT’S HUMAN SUBJECTS RESEARCH! YOU NEED TO GET APPROVAL FROM AN INSTITUTIONAL REVIEW BOARD!”So I wondered—is that right? Who  actually needs to get approval from an institutional review board (IRB)? More than a year later, I’m now convinced that:No single source on the internet actually answers that question.The answer is absurdly complex.The reason it’s so complex is that IRB rules are an illegible mishmash of things, some of which  have near-fractal complexity.If you stare at this long enough, it’s impossible not to question the degree to which we actually have “laws”.In this post, I’ll give the answer and then explain—in maddening detail—why I think that answer is right. But first I want to tell a little story.I used to live in an apartment with an extremely steep driveway. When I had visitors, I’d tell them, “At the bottom of the driveway you must slow to ⅒ of normal speed or your car will scrape the ground. I tell this to everyone and they only slow to ½ of normal speed and scrape their car on the ground. Don’t do that!” Then they’d only slow to ½ of normal speed and scrape their car on the ground.Why? I think because my visitors simply couldn’t believe my driveway was as stupid as it was. When I explained the rules of safe driveway usage, they mentally substituted the closest version of those rules that would be correct in a sane universe, one without driveways that form sudden 30° angles with the road. I suspect this is a general cognitive bias.So, who exactly needs IRB approval?  Here are some myths: IRB approval is only needed for medical research. IRB approval is only needed for federally funded research. IRB approval is only needed for research at “institutions”. IRB approval is only needed if you want to publish in a journal.Wrong. But this is also a myth: IRB approval is needed for all research involving human subjects.Also wrong. As far as I can tell, after months of research, here is how it works:Now, I know what you’re thinking: “That can’t be right! The government can’t possibly claim to regulate what me and my roommates eat at home! That would be stupid!” Yes, it  be stupid. But who says the world makes sense?Now, would you actually be  for violating those rules? Unlikely. If a prosecutor went after you and you fought them in court, you might even be able to get some of these rules declared unconstitutional. But those are the rules as written.: I estimate a 85% chance that this post contains at least one minor error and a 40% chance of a significant error. I’ve gone to insane lengths to try to get things right—I regret ever becoming interested in this topic—but this stuff is insanely convoluted. Past experience says that someone reading this knows much better than I do. If that’s you, let me know about any errors.Reminder of how law works in the USIf you’re a sane person, and you want to know when IRB approval is needed, you might think something like, “I know! I’ll go find the IRB law and read it. Then I’ll know what’s legal and what isn’t. Yay!”Hahaha, no. This won’t work, for several reasons.For one, few federal rules actually come from . Usually politicians pass some broad law that says, “Human research is hereby regulated, details to come!” And then government employees write  that contain all the details, and those regulations have the power of law. And then the government employees issue “clarifications” that supposedly don’t change anything, but everyone treats like new laws.Also, the US constitution theoretically imposes severe limits on the power of the federal government. Article 1 section 8 says the federal government has the power to (1) tax and spend, (2) regulate commerce, (3) control citizenship, (4) create post offices, (5) protect IP, (6) make treaties and (7) do war. And the 10th amendment says that’s : everything else is left to the states.The federal government doesn’t like those limits, so it evades them in various ways. The classic trick is to declare that everything is related to commerce and therefore almost anything can be considered “regulating commerce”. For example, it’s OK to make growing and eating wheat on your own property illegal, because growing and eating wheat is “commerce”. Another trick is to write rules that sort of say, “If your state/organization wants back any of the money it’s paying in taxes, then you must create and enforce the following rules: […]”. For example, in 1984, the US government decreed that all states must raise the legal age to buy alcohol to 21 or they would lose federal funding for highways.Finally, the limits of federal power are only clarified when someone fights them in court. Did you know that at some point, bureaucrats decided that your boss must guess your race and report it to the government? Is that an illegal overreach? Is it “regulating commerce” to make personal consumption of cannabis illegal in states that have legalized it? The way you find out is you break the rules, get prosecuted, and spend millions fighting in court. (For cannabis, the answer is yes, that  regulating commerce.) If you win, OK, no refunds on your legal fees. If you lose, you pay massive fines or go to prison. Also, courts have constantly changing judges and opinions.So what happens in practice is politicians write a vague law. Bureaucrats turn that law into very detailed (but often still vague) specific rules. Those rules might or might not be “legal”, but nobody want to risk fighting them in court. If the regulations are particularly ridiculous or likely to be overturned if challenged, prosecutors may quietly stop bringing cases. But the regulations still sit there on the books. And people still usually pay attention to them, because why risk it?OK! Where do IRB regulations come from?In 1932, the United States government and Tuskegee University began studying 600 poor black sharecroppers in Macon Country Alabama, ⅔ of whom had latent syphilis. The idea was to observe the progression of the disease if left untreated. By the late 1940s, syphilis was easily treatable with penicillin, but the men were never informed of their condition, never treated, and were even given fake treatments and diagnoses.In 1965, Peter Buxton joined the Public Health service and soon learned of the experiment. He was horrified and filed several official protests, but all were all rejected. Finally, in 1972, Buxton leaked the details to Jean Heller of the Associated Press which led to this front-page article on July 26, 1972.This led to widespread outrage and cancellation of the study. By this time, dozens of the men had died from syphilis and many of their wives and children had also become infected.It also led, in early 1973, to a series of congressional hearings led by Ted Kennedy of Massachusetts. These hearings eventually produced the National Research Act. Several people voted for the first version of this bill and then against the second version, including one Joseph Biden of Delaware, though I can’t find any record of why. Anyway, it was signed into law by Richard Nixon on July 12, 1974.Formally speaking, what the National Research Act did was create a commission  to develop guidelines for ethical research in human subjects, including how IRBs should work. But in reality, it also sent a message that federal agencies should use their  authority to regulate research. Even before the Act was passed, the Department of Health and Human Services had started amending the federal code to regulate human research. In 1978, the commission issued a report on IRBs:The Commission believes that the rights of subjects should be protected by local review committees operating pursuant to federal regulations and located in institutions where research involving human subjects is conducted.The Commission further believes that institutions receiving federal support for the conduct of research involving human subjects should be governed by uniform federal regulations applicable to the review of all such research, whether it is supported by one federal department or another, or is not federally supported.Informed by this report, lots of different agencies issued different regulations. This continued until 1991, when 15 federal agencies harmonized on what is known as the Common Rule, because it is “common” to different federal agencies. This is now codified as Title 45 Part 46 of the federal code.Since 1991, this has been continuously updated and various other agencies have joined, notably the department of Labor. There are still a couple holdouts including the Nuclear Regulatory Commission and the National Endowment for the Humanities. The CIA is an odd case—they never issued any formal regulations, but supposedly apply the Common Rule because of a cryptic executive order from 1981. The FDA hasn’t joined because they have their own regulatory structure.So what does the Common Rule say? In principle, you can go here and read it. Good luck with that. It’s exactly as readable as “a series of regulations amended by many agencies over many decades”  like it would be. But basically: (1) It requires informed consent. (2) It requires that risks are reasonable in relation to expected benefits,  including any long-range benefits from new knowledge. (3) It requires participants are selected fairly and equitably (e.g. not all poor black sharecroppers) (4) It requires that federally funded research with human subjects is reviewed by an IRB.Now, that might sound simple, but what exactly is “research”? This is defined by 45 CFR 46.102(l) to be a “systematic investigation designed to develop or contribute to generalizable knowledge.” It’s best to understand this with an example. Say you’re a teacher and you want to test if the cognitive bias of anchoring applies to your class. Is this research? If you’re doing it as part of normal “teaching stuff”, to demonstrate the effect to your students, then no. But if you’re doing it in hope of  if the anchoring bias is real, then that  research. To some degree, the truth lies in your heart.(The definition of a “human subject” is also somewhat convoluted: What if you’re using data someone else gathered? What if you’re just using some human tissue samples or nonviable human embryos? But never mind.)Note that this applies to  research. The people who made these rules were almost all medical researchers thinking about serious medical risks. But they wrote the rules to apply to  research, including research where you might just ask people some questions.There are some narrow categories of research that are “exempt” from IRB review, e.g. research on standard educational methods or food taste evaluations, or certain benign behavioral interventions . Theoretically, an institution could allow investigators to decide for themselves if they are exempt, but this is not recommended, and in practice, essentially all institutions require you to apply for a determination of exemption. In reality,  research is really exempt, it’s just that certain categories of research have a somewhat more lightweight application process. To get a feel for how much friction this adds, here’s U. Michigan’s application for exemption.Up until the 1990s, IRBs at universities tended to be staffed with faculty “volunteers”. These were sympathetic to the needs of their colleagues, had little interest in reading (unreadable) federal regulations, and probably just wanted to get back to teaching and research. So IRBs were often pretty loose. But in 1996, Hoiyan Wan died after taking part of an experiment on the effects of smoking and air pollution the University of Rochester. And in 1999, Jesse Gelsinger died after taking an experimental gene therapy at the University Of Pennsylvania. Federal agencies cracked down and sent out hundreds of enforcement letters. Universities nervously responded with “hypercompliance”, leading to the IRB process everyone loves today.The 1978 commission recommended that any institution accepting federal funding must apply IRB rules to all research, regardless of funding source. But the written regulations did not follow that recommendation. Theoretically, a university could accept federal funding for some research and still allow other research on human subjects without any IRB approval. But in practice, no university ever does that. Why?Well, before giving any federal funding to an institution, the government requires the institution to file a “Federalwide Assurance”. In this, the institution must promise to obey some “statement of ethical principles”. Supposedly this could be anything, but it appears that the only correct answer is “The Belmont Report”, an ethics statement issued by the same commission from 1978. Then there is a section where the institution may “optionally” promise to apply the common rule to all research, regardless of funding source.The government has a database of these “assurances”, but you can’t actually read them. Still, I found that a few prominent institutions that had published theirs. They  promised to abide by the Belmont report. Some (NYU, U. C. Irvine) promised to apply the Common Rule to all research, but most (Mayo Clinic, U. Florida, Emory, U. Michigan, MIT) do not.What’s going on here? Throughout the 1970s, there was a debate about if IRB rules should apply to social science at all. The OHRP (which oversees the Common Rule) gradually asserted more and more broad IRB requirements. Ithiel de Sola Pool, a professor at MIT, protested this growing power. Apparently, a colleague had sought IRB approval to interview Boston anti-busing activists, and been rejected because this research could be used against them. Pool argued that much of social science should be exempt and got support from professional societies.But no matter. Here’s Charles McCarthy, director of the OHRP from 1977 to 1993, bragging about subverting the Carter and the Reagan administrations to add more regulations during the transition in 1980:But we were trying to decide when is the appropriate time to get the Secretary to sign off on the new regulations?  They had already been proposed, we had got comments from the public.  We’d incorporated the comments.  We wrote the preamble, it was ready to go.  And then when is the best time, ‘cause Harris could care less whether they every got published.  So we talked to the transition team, and they said Reagan will never allow new regulations to see the light of day, so if you’re going to get those out at all, you’d better get them out before he takes over.So we’re struggling with this problem, and the way we worked it out was that’s when we wrote, overnight one night–brilliant inspiration–we wrote–everybody stayed in the office, and we worked all night long, and we wrote the exemptions to the regulations, and we also wrote expedited review neither of which had ever been addressed by the Commission.  And that’s where they came from.Then we went to the transition team, and we said would the transition team endorse regulations that are less stringent than the previous regulations?  And, of course, they weren’t, but they looked like they were because we wrote some exceptions.  And so when we sent the package down to Harris, we said “Diminished Regulations for the Protection of Human Subjects.”  And that was the title.  And, of course, we knew nobody down there in the last weeks of the Harris administration getting ready to leave office would actually read it.  So they didn’t know what all that was about, but they could read the title.And so Secretary Harris signed that at her farewell party.  She put down her glass of champagne and signed the regulations on January 19, 1980, and went out of office on the 20th.  So we squeaked by.  Those were some of the adventures we had that were kind of harrowing for regulators, and that’s why some of the language in those exemptions is so convoluted because it was really a first draft.  But it survived and it’s still in there.  And it makes some sense, but if we could rewrite the regulations, we would run that through three or four more drafts and make them crystal clear.  They’re less than elegant writing, whereas if you read the rest of the regulation, I think you’ll find sub-part A is very well-written if you leave out the part about the exemptions.The OHRP sent out “non-binding guidance” that suggested all research should be subject to IRB review. According to Hamburger (2007), in the 1990s, the Federalwide Assurance form implied that institutions  to apply the Common Rule to all research, despite the fact that no federal regulation to this effect existed. I can’t find the old form, but according to “persons who were in a position to know”, at one point all but about five institutions in the entire country had made this promise.By 2000-2005, the form made clear that promising to apply the Common Rule (and thus IRBs) to non federally-funded research was optional, and many institutions decline to do this. But they are only declining . Apparently, after years of applying the Common Rule to all research, it had risen to legally qualify as the “standard of care”, meaning that anyone that failed to apply it risked being legally “negligent “ under state law. So, having established the Common Rule as standard through questionably legal tactics, the government could now relax and rely on state tort laws.Now, say you’re affiliated with a university. Federal funded research without IRB approval is out.  research at the university is out. But say you want to do research in your spare time, at home, without using any university resources. Is that OK?UCSF faculty, staff, or students or researchers at UCSF-affiliated institutions conducting human subjects research require IRB approval before initiating the study. IRB approval is required regardless of the site of the study or the source of funding (if there is funding).Others are a bit ambiguous. MIT says:Any faculty member, employee or student at MIT who conducts human subjects research must apply [to the IRB] if the research involves any form of MIT involvement or support, including funding, , facilities, academic credit or access to experimental subjects.But how could a MIT  do research without involving MIT , when they themselves are personnel? The closest I can find to anyone saying it’s OK to do independent research without an IRB is Columbia University which says an IRB is needed if:the research is conducted by or under the direction of any employee or agent (faculty/student/staff) of Columbia, in connection with his or her institutional responsibilitiesWhat exactly constitutes a “connection”? I guess you get to argue about the meaning of that term if/when you’re investigated for research misconduct.OK! So if you’re doing any research at any federally funded institution, you need an IRB. If you work at such an institution, you could  do research at home, as long as it’s unrelated to your job and you’re willing to live dangerously.But suppose you’re not affiliated with any institution. Can you do research without an IRB?One additional barrier is that if you want to  your results in a journal, almost all journals that publish human subjects research require IRB approval. For example, all Nature journals, Springer journals , and Taylor and Francis  journals require you to identify the ethics committee and give a reference number. Science journals require IRB approval, but don’t seem to require details unless an editor requests them. (Those are publisher requirements. Specific journals can add additional restrictions.)Now, I’m not sure how carefully these journals actually . My impression is that some do and some don’t. Maybe you could lie and get away with it, but papers do in fact retracted when a lack of IRB approval is revealed.Another barrier is that some states have their own laws. In 1975, New York passed a law that required IRB approval for  human subjects research:Each person engaged in the conduct of human research or proposing to conduct human research shall affiliate himself with an institution or agency having a human research review committee, and such human research as he conducts or proposes to conduct shall be subject to review by such committee in the manner set forth in this section.Virginia passed a very similar law in 1979. These go further than requiring IRB approval. They make independent human subjects research fully illegal! (Read that quote!) In most states, a random person could submit their research plan to an independent IRB. Not in New York or Virginia.I’ve seen many people claim that California also requires IRB approval for all human subjects research. While it’s true that California has some extra regulations, as far as I can tell none of these require IRB approval.Have you found all this to be too simple? Too straightforward and legible?The FDA is here to help. They have an  set of IRB regulations. While the Common Rule IRB regulations are in Title 45 Part 46 (45 CFR 46), the FDA IRB regulations are in 21 CFR 56 and also 21 CFR 50.Whereas the Common Rule derives its constitutional authority from the idea that the government can decide how it wants to give out , the FDA regulations seem derive their constitutional authority from the idea that it is . So FDA regulations apply regardless of where you work or how you’ve gotten funding. The FDA operates according to the 1938 Food, Drug, and Cosmetics Act and its many amendments, most notably the 1994 Dietary Supplement Health and Education Act.So what do the FDA regulations say? I found them to be much more painful to read than the (already extremely painful) Common Rule regulations. But after much thrashing and screaming, I think the core regulation is 21 CFR 56.103(a):[…] any clinical investigation which must meet the requirements for prior submission (as required in parts 312, 812, and 813) […] shall not be initiated unless that investigation has been reviewed […] by an IRB meeting the requirements of this part.Well, a “clinical investigation” is defined by 21 CFR 56.102(c) to be anything involving human subjects and a “test article”. And a “test article” which is defined in turn by 21 CFR 56.102(l) to be any “drug” for human use. So if you’re doing anything with a “drug” that requires “prior submission”, then you need an IRB. What do  terms mean? 21 CFR Part 312 makes clear that “prior submission” is referring to an investigational new drug (IND) application to the FDA.(Caution: An “IRB” and an “IND” are totally different things. An IRB is what this article is about. An IND is the kind of application that pharmaceutical companies make to the FDA before they can start clinical trials.)So, at first, this seems like good news: As long as you don’t need to do a (gigantic, extremely expensive) IND application to the FDA before starting your research, then FDA regulations don’t force you to use an IRB. Whew! Except… When are you supposed to submit an IND? Surely that’s not some incredibly broad set of circumstances that could apply to almost anyone doing almost anything, is it?This is a supposedly “nonbinding” document that just explains old rules. , the FDA is just publishing this as a helpful service to all the mouth-breathers who can’t understand federal regulations. But everyone in industry behaves as if it’s a new law. Which is understandable, since the written regulations are so confusing and ambiguous.If you read this guidance, you will discover that an IND is needed if:The research involves a “drug”The research is a “clinical investigation”The research is not exempt.If you want to avoid an IND, then (2) and (3) are little help. A “clinical investigation” is defined as anything where a “drug” is given to human subjects. And the exemptions are extremely narrow: They are only for  drugs where the drug is being used for the labeled purpose as directed. Testing something already approved for a new disease? Not exempt. Testing if normal food helps with some new disease? Not exempt. You’re only exempt if you’re doing research on something the FDA has already accepted to be true.If you want to avoid an IND, your best hope is that your research doesn’t involve a “drug”. Unfortunately, the term “drug” is defined extremely broadly, way beyond the colloquial meaning of  “drug” in English. A drug is defined to include:articles intended for use in the diagnosis, cure, mitigation, treatment, or prevention of diseasearticles (other than food) intended to affect the structure or any function of the bodyGet that? Here’s a little picture:Now, I know what you’re thinking. “That can’t be right! That would mean that if I wanted to study if some normal food reduced the odds of, say, diabetes, then I wouldn’t just need IRB approval, I would also need to submit a freaking ! That would be stupid!”But the FDA is quite clear that this  right:As is the case for a dietary supplement, a food is considered to be a drug if it is “intended for use in the diagnosis, cure, mitigation, treatment, or prevention of disease,” except that a food may bear an authorized health claim about reducing the risk of a disease without becoming a drug (see section VI.D.3). Therefore, a clinical investigation intended to evaluate the effect of a food on a disease would require an IND under part 312. For example, a clinical investigation intended to evaluate the effect of a food on the signs and symptoms of Crohn’s disease would require an IND.Does a physician, in private practice, conducting research with an FDA regulated product, need to obtain IRB approval?Yes. The FDA regulations require IRB review and approval of regulated clinical investigations, whether or not the study involves institutionalized subjects. FDA has included non-institutionalized subjects because it is inappropriate to apply a double standard for the protection of research subjects based on whether or not they are institutionalized.An investigator should be able to obtain IRB review by submitting the research proposal to a community hospital, a university/medical school, an independent IRB, a local or state government health agency or other organizations.At first, I thought this couldn’t possibly be true. In 2017, it apparently cost $619,200 to submit an IND application to the FDA. However, in practice it appears that there are many exceptions and that there are two categories of INDs: Commercial and research. I assume the FDA doesn’t actually try to charge researchers $619,200 to review their research projects, but I don’t really know.The FDA’s IND application page is here. Have fun.I’m not sure how seriously people take these FDA regulations in practice. People  seem to get IND approval for research on actual (chemical) drugs. But does anyone actually seek IND approval if they want to study if diet influences obesity? Do they carefully avoid talking about disease, so they can fall under the looser “structure and function of the body”? It’s hard to say.Mostly I’ve learned that the set of abstractions I was using to think about the world were wrong. I assumed that there were “laws” that someone had written, and that someone could read to understand what is legal and what isn’t.But in reality, politicians write vague laws, and then agencies write vague sprawling regulations, and then those agencies issue vague “clarifications”. Sometimes these might be unconstitutional or illegal, but no one knows unless they want to fight things in court. But courts are unpredictable, and even going to court is ruinously expensive. Trying to comply with the the written law is like trying to comply with a giant mound of potatoes.So people basically operate as if the law doesn’t exist. The “law” is what the people who enforce the law choose to enforce. If you need to live in this world, that’s what you pay attention to. The law is what you can get away with.All the IRB rules were designed by medical doctors to regulate the research of other medical doctors, thinking about incidents like the Tuskegee syphilis study.But here’s an opinion: Needing to submit an application before you can ask people to fill out some simple online survey is stupid. Needing to submit an application before you can interview a bunch of people is stupid.I like incremental change, so why don’t we start by making “exempt” research ? For example, 45 CFR 46.104(d)(3)(ii) exempts “benign behavioral interventions” from IRB review. But you still need to  to prove you’re exempt. Why?As an analogy, driving a car is dangerous. Whenever I drive, I could easily kill someone. But the government doesn’t force me to submit a driving plan any time I want to go somewhere. Instead, if I misbehave, I am punished . Why don’t we apply the same policy to research?I found AI to be completely useless at parsing all these federal regulations. Maybe you need AI that’s specifically trained to be good at parsing legal text?Say I want to feed potatoes to my friends and see if that reduces diabetes. Maybe that’s dumb. Fine. But the FDA seems to be claiming that it is constitutionally permitted to regulate this because by doing this I would be engaging in “interstate commerce”. Really?As far as I can tell, no one has ever been prosecuted for failure to comply with IRB requirements. It’s not even clear what the penalty would be. In practice, rules are enforced on —if one failed to comply, then their “assurance” might be cancelled, meaning they couldn’t take any federal funding for research. Or, if they wanted to sell something, the FDA might refuse to approve it for sale.Are you chaotic good? Then you may find it interesting that it seems to be relatively easy to form a new IRB. (HHS rules / FDA rules). And that while all federally funded institutions require IRB approval for all research, only some require approval from  IRB. You can probably see where this is going.My Bookshelf Runneth Over:United States Government Accountability Office]]></content:encoded></item></channel></rss>
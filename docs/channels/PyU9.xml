<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev</title><link>https://konrad.website/feeds/</link><description></description><item><title>Headlamp in 2025: Project Highlights</title><link>https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/</link><author></author><category>dev</category><pubDate>Thu, 22 Jan 2026 02:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[This announcement is a recap from a post originally published on the Headlamp blog.Headlamp has come a long way in 2025. The project has continued to grow â€“ reaching more teams across platforms, powering new workflows and integrations through plugins, and seeing increased collaboration from the broader community.We wanted to take a moment to share a few updates and highlight how Headlamp has evolved over the past year.Joining Kubernetes SIG UIThis year marked a big milestone for the project: Headlamp is now officially part of Kubernetes SIG UI. This move brings roadmap and design discussions even closer to the core Kubernetes community and reinforces Headlampâ€™s role as a modern, extensible UI for the project.Linux Foundation mentorshipThis year, we were excited to work with several students through the Linux Foundationâ€™s Mentorship program, and our mentees have already left a visible mark on Headlamp: built the KEDA plugin, adding a UI in Headlamp to view and manage KEDA resources like ScaledObjects and ScaledJobs. set up an OpenTelemetry-based observability stack for Headlamp, wiring up metrics, logs, and traces so the project is easier to monitor and debug. led a UX audit of Headlamp plugins, identifying usability issues and proposing design improvements and personas for plugin users. developed the Karpenter plugin, giving Headlamp a focused view into Karpenter autoscaling resources and decisions. improved Gateway API support, so you can see networking relationships on the resource map, as well as improved support for many of the new Gateway API resources. worked on backend caching for Kubernetes API calls, reducing load on the API server and improving performance in Headlamp.Managing multiple clusters is challenging: teams often switch between tools and lose context when trying to see what runs where. Headlamp solves this by giving you a single view to compare clusters side-by-side. This makes it easier to understand workloads across environments and reduces the time spent hunting for resources.View of multi-cluster workloadsKubernetes apps often span multiple namespaces and resource types, which makes troubleshooting feel like piecing together a puzzle. Weâ€™ve added  to give you an application-centric view that groups related resources across multiple namespaces â€“ and even clusters. This allows you to reduce sprawl, troubleshoot faster, and collaborate without digging through YAML or cluster-wide lists.View of the new Projects featureNew â€œProjectsâ€ feature for grouping namespaces into app- or team-centric projectsExtensible Projects details view that plugins can customize with their own tabs and actionsNavigation and ActivitiesDay-to-day ops in Kubernetes often means juggling logs, terminals, YAML, and dashboards across clusters. We redesigned Headlampâ€™s navigation to treat these as first-class â€œactivitiesâ€ you can keep open and come back to, instead of one-off views you lose as soon as you click away.A new task bar/activities model lets you pin logs, exec sessions, and details as ongoing activitiesAn activity overview with a â€œClose allâ€ action and cluster informationMulti-select and global filters in tablesWhen something breaks in production, the first two questions are usually â€œwhere is it?â€ and â€œwhat is it connected to?â€ Weâ€™ve upgraded both search and the map view so you can get from a high-level symptom to the right set of objects much faster.View of the new Advanced Search featureAn Advanced search view that supports rich, expression-based queries over Kubernetes objectsImproved global search that understands labels and multiple search items, and can even update your current namespace based on what you findEndpointSlice support in the Network sectionA richer map view that now includes Custom Resources and Gateway API objectsWeâ€™ve put real work into making OIDC setup clearer and more resilient, especially for in-cluster deployments.View of user information for OIDC clustersUser information displayed in the top bar for OIDC-authenticated usersPKCE support for more secure authentication flows, as well as hardened token refresh handlingDocumentation for using the access token using -oidc-use-access-token=trueImproved support for public OIDC clients like AKS and EKSWeâ€™ve broadened how you deploy and source apps via Headlamp, specifically supporting vanilla Helm repos.A more capable Helm chart with optional backend TLS termination, PodDisruptionBudgets, custom pod labels, and moreImproved formatting and added missing access token arg in the Helm chartNew in-cluster Helm support with an  flag and a service proxyFinally, weâ€™ve spent a lot of time on the things you notice every day but donâ€™t always make headlines: startup time, list views, log viewers, accessibility, and small network UX details. A continuous accessibility self-audit has also helped us identify key issues and make Headlamp easier for everyone to use.View of the Learn section in docsSignificant desktop improvements, with up to 60% faster app loads and much quicker dev-mode reloads for contributorsNumerous table and log viewer refinements: persistent sort order, consistent row actions, copy-name buttons, better tooltips, and more forgiving log inputsAccessibility and localization improvements, including fixes for zoom-related layout issues, better color contrast, improved screen reader support, and expanded language coverageMore control over resources, with live pod CPU/memory metrics, richer pod details, and inline editing for secrets and CRD fieldsA refreshed documentation and plugin onboarding experience, including a â€œLearnâ€ section and plugin showcaseA more complete NetworkPolicy UI and network-related polishNightly builds available for early testingPlugins and extensibilityDiscovering plugins is simpler now â€“ no more hopping between Artifact Hub and assorted GitHub repos. Browse our dedicated Plugins page for a curated catalog of Headlamp-endorsed plugins, along with a showcase of featured plugins.View of the Plugins showcaseManaging Kubernetes often means memorizing commands and juggling tools. Headlampâ€™s new AI Assistant changes this by adding a natural-language interface built into the UI. Now, instead of typing  or digging through YAML you can ask, â€œIs my app healthy?â€ or â€œShow logs for this deployment,â€ and get answers in context, speeding up troubleshooting and smoothing onboarding for new users. Learn more about it here.Alongside the new AI Assistant, weâ€™ve been growing Headlampâ€™s plugin ecosystem so you can bring more of your workflows into a single UI, with integrations like Minikube, Karpenter, and more.Highlights from the latest plugin releases:Minikube plugin, providing a locally stored single node Minikube clusterKarpenter plugin, with support for Azure Node Auto-Provisioning (NAP)KEDA plugin, which you can learn more about hereAlongside new additions, weâ€™ve also spent time refining plugins that many of you already use, focusing on smoother workflows and better integration with the core UI.View of the Backstage plugin: Updated for Flux v2.7, with support for newer CRDs, navigation fixes so it works smoothly on recent clusters: Now supports Helm repos in addition to Artifact Hub, can run in-cluster via /serviceproxy, and shows both current and latest app versions: Improved card layout and accessibility, plus dependency and Storybook test updates: Dependency and build updates, more info hereWeâ€™ve focused on making it faster and clearer to build, test, and ship Headlamp plugins, backed by improved documentation and lighter tooling.View of the Plugin Development guideImproved type checking for Headlamp APIs, restored Storybook support for component testing, and reduced dependencies for faster installs and fewer updatesDocumented plugin install locations, UI signifiers in Plugin Settings, and labels that differentiated shipped, UI-installed, and dev-mode pluginsWe've also been investing in keeping Headlamp secure â€“ both by tightening how authentication works and by staying on top of upstream vulnerabilities and tooling.We've been keeping up with security updates, regularly updating dependencies and addressing upstream security issues.We tightened the Helm chart's default security context and fixed a regression that broke the plugin manager.We've improved OIDC security with PKCE support, helping unblock more secure and standards-compliant OIDC setups when deploying Headlamp in-cluster.Thank you to everyone who has contributed to Headlamp this year â€“ whether through pull requests, plugins, or simply sharing how you're using the project. Seeing the different ways teams are adopting and extending the project is a big part of what keeps us moving forward. If your organization uses Headlamp, consider adding it to our adopters list.If you haven't tried Headlamp recently, all these updates are available today. Check out the latest Headlamp release, explore the new views, plugins, and docs, and share your feedback with us on Slack or GitHub â€“ your feedback helps shape where Headlamp goes next.]]></content:encoded></item><item><title>Announcing Rust 1.93.0</title><link>https://blog.rust-lang.org/2026/01/22/Rust-1.93.0/</link><author>The Rust Release Team</author><category>dev</category><pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.93.0. Rust is a programming language empowering everyone to build reliable and efficient software.If you have a previous version of Rust installed via , you can get 1.93.0 with:If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!
Update bundled musl to 1.2.5The various  targets now all ship with musl 1.2.5. This primarily affects static musl builds for , , and  which bundled musl 1.2.3. This update comes with several fixes and improvements, and a breaking change that affects the Rust ecosystem.For the Rust ecosystem, the primary motivation for this update is to receive major improvements to
musl's DNS resolver which shipped in 1.2.4 and received bug fixes in 1.2.5. When using 
targets for static linking, this should make portable Linux binaries that do networking more
reliable, particularly in the face of large DNS records and recursive nameservers.
Allow the global allocator to use thread-local storageRust 1.93 adjusts the internals of the standard library to permit global allocators written in Rust
to use std's  and
 without
re-entrancy concerns by using the system allocator instead. attributes on  linesPreviously, if individual parts of a section of inline assembly needed to be 'd, the full 
block would need to be repeated with and without that section. In 1.93,  can now be applied to
individual statements within the  block.Many people came together to create Rust 1.93.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>Announcing the Checkpoint/Restore Working Group</title><link>https://kubernetes.io/blog/2026/01/21/introducing-checkpoint-restore-wg/</link><author></author><category>dev</category><pubDate>Wed, 21 Jan 2026 18:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[The community around Kubernetes includes a number of Special Interest Groups (SIGs) and Working Groups (WGs) facilitating discussions on important topics between interested contributors. Today we would like to announce the new Kubernetes Checkpoint Restore WG focusing on the integration of Checkpoint/Restore functionality into Kubernetes.There are several high-level scenarios discussed in the working group:Optimizing resource utilization for interactive workloads, such as Jupyter notebooks and AI chatbotsAccelerating startup of applications with long initialization times, including Java applications and LLM inference servicesUsing periodic checkpointing to enable fault-tolerance for long-running workloads, such as distributed model trainingProviding interruption-aware scheduling with transparent checkpoint/restore, allowing lower-priority Pods to be preempted while preserving the runtime state of applicationsFacilitating Pod migration across nodes for load balancing and maintenance, without disrupting workloads.Enabling forensic checkpointing to investigate and analyze security incidents such as cyberattacks, data breaches, and unauthorized access.Across these scenarios, the goal is to help facilitate discussions of ideas between the Kubernetes community and the growing Checkpoint/Restore in Userspace (CRIU) ecosystem. The CRIU community includes several projects that support these use cases, including:CRIU - A tool for checkpointing and restoring running applications and containerscheckpointctl - A tool for in-depth analysis of container checkpointscriu-coordinator - A tool for coordinated checkpoint/restore of distributed applications with CRIUMore information about the checkpoint/restore integration with Kubernetes is also available here.If you are interested in contributing to Kubernetes or CRIU, there are several ways to participate:]]></content:encoded></item><item><title>Results from the 2025 Go Developer Survey</title><link>https://go.dev/blog/survey2025</link><author>Todd Kulesza, on behalf of the Go team</author><category>dev</category><pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Golang Blog</source><content:encoded><![CDATA[Hello! In this article weâ€™ll discuss the results of the 2025 Go Developer
Survey, conducted during September 2025.Thank you to the 5,379 Go developers who responded to our survey invitation
this year. Your feedback helps both the Go team at Google and the wider Go
community understand the current state of the Go ecosystem and prioritize
projects for the year ahead.Our three biggest findings are:Broadly speaking, Go developers asked for help with identifying and applying
best practices, making the most of the standard library, and expanding the
language and built-in tooling with more modern capabilities.Most Go developers are now using AI-powered development tools when seeking
information (e.g., learning how to use a module) or toiling (e.g., writing
repetitive blocks of similar code), but their satisfaction with these tools
is middling due, in part, to quality concerns.A surprisingly high proportion of respondents said they frequently need to
review documentation for core  subcommands, including , , and , suggesting meaningful room for improvement with the 
commandâ€™s help system.Read on for the details about these findings, and much more.Most survey respondents self-identified as professional developers (87%) who
use Go for their primary job (82%). A large majority also uses Go for personal
or open-source projects (72%). Most respondents were between 25 â€“ 45
years old (68%) with at least six years of professional development experience
(75%). Going deeper, 81% of respondents told us they had more professional
development experience than Go-specific experience, strong evidence that Go is
usually not the first language developers work with. In fact, one of the
themes that repeatedly surfaced during this yearâ€™s survey analysis seems to
stem from this fact: when the way to do a task in Go is substantially
different from a more familiar language, it creates friction for developers to
first learn the new (to them) idiomatic Go pattern, and then to consistently
recall these differences as they continue to work with multiple languages.
Weâ€™ll return to this theme later.The single most common industry respondents work in was â€œTechnologyâ€ (46%),
but a majority of respondents work outside of the tech industry (54%). We saw
representation of all sizes of organizations, with a bare majority working
somewhere with 2 â€“ 500 employees (51%), 9% working alone, and 30%
working at enterprises of over 1,000 employees. As in prior years, a majority
of responses come from North America and Europe.This year we observed a decrease in the proportion of respondents who said
they were fairly new to Go, having worked with it for less than one year
(13%, vs. 21% in 2024). We suspect this is related to industry-wide
declines in entry-level software engineering roles; we commonly hear from
people that they learned Go for a specific job, so a downturn in hiring would
be expected to reduce the number of developers learning Go in that year. This
hypothesis is further supported by our finding that over 80% of respondents
learned Go  beginning their professional career.Other than the above, we found no significant changes in other demographics
since our 2024 survey.How do people feel about Go?The vast majority of respondents (91%) said they felt satisfied while working
with Go. Almost â…” were â€œvery satisfiedâ€, the highest rating. Both of these
metrics are incredibly positive, and have been stable since we began asking
this question in 2019. The stability over time is really what we monitor from
this metric â€” we view it as a lagging indicator, meaning by the time
this satisfaction metric shows a meaningful change, we would expect to already
have seen earlier signals from issue reports, mailing lists, or other
community feedback.Why were respondents so positive about Go? Looking at open-text responses to
several different survey questions suggests that itâ€™s the gestalt, rather than
any one thing. These folks are telling us that they find tremendous value in
Go as a holistic platform. That doesnâ€™t mean it supports all programming
domains equally well (it surely does not), but that developersâ€™ value the
domains it  nicely support via stdlib and built-in tooling.Below are some representative quotations from respondents. To provide context
for each quote, we also identify the satisfaction level, years of experience
with Go, and industry of the respondent.â€œGo is by far my favorite language; other languages feel far too complex and
unhelpful. The fact that Go is comparatively small, simple, with fewer bells
and whistles plays a massive role in making it such a good long-lasting
foundation for building programs with it. I love that it scales well to
being used by a single programmer and in large teams.â€ â€œThe entire reason I use Go is the great tooling and standard library.  Iâ€™m
very thankful to the team for focusing on great HTTP, crypto, math, sync,
and other tools that make developing service-oriented applications easy and
reliable.â€ â€œ[The] Go ecosystem is the reason why I really like the programming
language. There are a lot of npm issues lately but not with Go.â€ This year we also asked about the other languages that people use. Survey
respondents said that besides Go, they enjoy working with Python, Rust, and
TypeScript, among a long tail of other languages. Some shared characteristics
of these languages align with common points of friction reported by Go
developers, including  areas like error handling, enums, and object-oriented
design patterns. For example, when we sum the proportion of respondents who
said their next-favorite language included one of the following factors, we
found that majorities of respondents enjoy using languages with inheritance,
type-safe enums, and exceptions, with only a bare majority of these languages
including a static type system by default.Proportion of respondentsWe think this is important because it reveals the larger environment in which
developers operate â€” it suggests that people need to use different
design patterns for fairly mundane tasks, depending on the language of the
codebase theyâ€™re currently working on. This leads to additional cognitive load
and confusion, not only among developers new to Go (who must learn idiomatic
Go design patterns), but also among the many developers who work in multiple
codebases or projects. One way to alleviate this additional load is
context-specific guidance, such as a tutorial on â€œError handling in Go for
Java developersâ€. There may even be opportunities to build some of this
guidance into code analyzers, making it easier to surface directly in an IDE.This year we asked the Go community to share their sentiment towards the Go
project itself. These results were quite different from the 91% satisfaction
rate we discussed above, and point to areas the Go Team plans to invest our
energy during 2026. In particular, we want to encourage more contributors to
get involved, and ensure the Go Team accurately understands the challenges Go
developers currently face. We hope this focus, in turn, will help to increase
developer trust in both the Go project and the Go Team leadership. As one
respondent explained the problem:â€œNow that the founding first generation of Go Team members [are] not
involved much anymore in the decision making, I am a bit worried about the
future of Go in terms of quality of maintenance, and its balanced decisions
so far wrt to changes in the language and std lib. More presence in form of
talks [by] the new core team members about the current state and future
plans might be helpful to strengthen trust.â€ What are people building with Go?We revised this list of â€œwhat types of things do you build with Go?â€ from 2024
with the intent of more usefully teasing apart what people are building with
Go, and avoid confusion around evolving terms like â€œagentsâ€. Respondentâ€™s top
use cases remain CLIs and API services, with no meaningful change in either
since 2024. In fact, a majority of respondents (55%) said they build 
CLIs and API services with Go. Over â…“ of respondents specifically build cloud
infrastructure tooling (a new category), and 11% work with ML models, tools,
or agents (an expanded category). Unfortunately embedded use cases were left
off of the revised list, but weâ€™ll fix this for next yearâ€™s survey.Most respondents said they are not currently building AI-powered features into
the Go software they work on (78%), with â…” reporting that their software does
not use AI functionality at all (66%). This appears to be a decrease in
production-related AI usage year-over-year; in 2024, 59% of respondents were
not involved in AI feature work, while 39% indicated some level of
involvement. That marks a shift of 14 points away from building AI-powered
systems among survey respondents, and may reflect some natural pullback from
the early hype around AI-powered applications: itâ€™s plausible that lots of
folks tried to see what they could do with this technology during its initial
rollout, with some proportion deciding against further exploration (at least
at this time).Among respondents who are building AI- or LLM-powered functionality, the most
common use case was to create summaries of existing content (45%). Overall,
however, there was little difference between most uses, with between 28%
â€“ 33% of respondents adding AI functionality to support classification,
generation, solution identification, chatbots, and software development.What are the biggest challenges facing Go developers?One of the most helpful types of feedback we receive from developers are
details about the challenges people run into while working with Go. The Go
Team considers this information holistically and over long time horizons,
because there is often tension between improving Goâ€™s rougher edges and
keeping the language and tooling consistent for developers. Beyond technical
factors, every change also incurs some cost in terms of developer attention
and cognitive disruption. Minimizing disruption may sound a bit dull or
boring, but we view this as an important strength of Go. As Russ Cox wrote in
2023, â€œBoring is goodâ€¦ Boring means being able to focus on your work, not
on whatâ€™s different about Go.â€.In that spirit, this yearâ€™s top challenges are not radically different from
last yearâ€™s. The top three frustrations respondents reported were â€œEnsuring
our Go code follows best practices / Go idiomsâ€ (33% of respondents), â€œA
feature I value from another language isnâ€™t part of Goâ€ (28%), and â€œFinding
trustworthy Go modules and packagesâ€ (26%). We examined open-text responses to
better understand what people meant. Letâ€™s take a minute to dig into each.Respondents who were most frustrated by writing idiomatic Go were often
looking for more official guidance, as well as tooling support to help enforce
this guidance in their codebase. As in prior surveys, questions about how to
structure Go projects were also a common theme. For example:â€œThe simplicity of go helps to read and understand code from other
developers, but there are still some aspects that can differ quite a lot
between programmers. Especially if developers come from other languages,
e.g. Java.â€ â€œMore opinionated way to write go code. Like how to structure a Go project
for services/cli tool.â€ â€œItâ€™s hard to figure out what are good idioms. Especially since the core
team doesnâ€™t keep Effective Go up-to-date.â€ The second major category of frustrations were language features that
developers enjoyed working with in other ecosystems. These open-text comments
largely focused on error handling and reporting patterns, enums and sum types,
nil pointer safety, and general expressivity / verbosity:â€œStill not sure what is the best way to do error handling.â€ â€œRustâ€™s enums are great, and lead to writing great type safe code.â€ â€œThere is nothing (in the compiler) that stops me from using a maybe nil
pointer, or using a value without checking the err first. That should be
[baked into] the type system.â€ â€œI like [Go] but I didnâ€™t expect it to have nil pointer exceptions :)â€ â€œI often find it hard to build abstractions and to provide clear intention
to the future readers of my code.â€ The third major frustration was finding trustworthy Go modules. Respondents
often described two aspects to this problem. One is that they considered many
3rd-party modules to be of marginal quality, making it hard for really good
modules to stand out. The second is identifying which modules are commonly
used and under which types of conditions (including recent trends over time).
These are both problems that could be addressed by showing what weâ€™ll vaguely
call â€œquality signalsâ€ on pkg.go.dev. Respondents provided helpful
explanations of the signals they use to identify trustworthy modules,
including project activity, code quality, recent adoption trends, or the
specific organizations that support or rely upon the module.â€œBeing able to filter by criteria like stable version, number of users and
last update age at pkg.go.dev could make things a bit easier.â€ â€œMany pacakges are just clones/forks or one-off pojects with no
history/maintenance. [sic]â€ â€œMaybe flagging trustworthy packages based on experience, maturity and
community feedback?â€ We agree that these are all areas where the developer experience with Go could
be improved. The challenge, as discussed earlier, is doing so in such a way
that doesnâ€™t lead to breaking changes, increased confusion among Go
developers, or otherwise gets in the way of people trying to get their work
done with Go. Feedback from this survey is a major source of information we
use when discussing proposals, but if youâ€™d like to get involved more directly
or follow along with other contributors, visit the Go proposals on
GitHub;
please be sure to follow this process if
youâ€™d like to add a new proposal.In addition to these (potentially) ecosystem-wide challenges, this year we
also asked specifically about working with the  command. Weâ€™ve informally
heard from developers that this toolâ€™s help system can be confusing to
navigate, but we havenâ€™t had a great sense of how frequently people find
themselves reviewing this documentation.Respondents told us that except for , between 15% â€“ 25% of them
felt they â€œoften needed to review documentationâ€ with working with these
tools. This was surprising, especially for commonly-used subcommands like
 and . Common reasons included remembering specific flags,
understanding what different options do, and navigating the help system
itself. Participants also confirmed that infrequent use was one reason for
frustration, but navigating and parsing command help appears to be the
underlying cause. In other words, we all expect to need to review
documentation sometimes, but we donâ€™t expect to need help navigating the
documentation system itself. As on respondent described their journey:â€œAccessing the help is painful. go test â€“help # didnâ€™t work, but tell[s] me
to type  insteadâ€¦ go help test # oh, actually, the info Iâ€™m
looking for is in  go help testflag # visually parsing through
text that looks all the same without much formattingâ€¦ I just lack time to
dig into this rabbit hole.â€ What does their development environment look like?Operating systems and architecturesGenerally, respondents told us their development platforms are UNIX-like. Most
respondents develop on macOS (60%) or Linux (58%) and deploy to Linux-based
systems, including containers (96%). The largest year-over-year change was
among â€œembedded devices / IoTâ€ deployments, which increased from 2% -> 8% of
respondents; this was the only meaningful change in deployment platforms since
2024.The vast majority of respondents develop on x86-64 or ARM64 architectures,
with a sizable group (25%) still potentially working on 32-bit x86 systems.
However, we believe the wording of this question was confusing to respondents;
next year weâ€™ll clarify the 32-bit vs. 64-bit distinction for each
architecture.Several new code editors have become available in the past two years, and we
expanded our survey question to include the most popular ones. While we saw
some evidence of early adoption, most respondents continued to favor VS
Code (37%) or
GoLand (28%). Of the newer editors, Zed and
Cursor were the highest ranked, each becoming the preferred editor of 4% of
respondents. To put those numbers in context, we looked back at when VS Code
and GoLand were first introduced. VS Code (released in 2015) was favored by
16% of respondents one year after its release. IntelliJ has had a
community-led Go plugin longer than weâ€™ve been surveying Go developers (ðŸ’™),
but if we look at when JetBrains began officially supporting Go in IntelliJ
(2016), within one year IntelliJ was preferred by 20% of respondents.Note: This analysis of code editors does not include respondents who were
referred to the survey directly from VS Code or GoLand.The most common deployment environments for Go continue to be Amazon Web
Services (AWS) at 46% of respondents, company-owned servers (44%), and Google
Cloud Platform (GCP) at 26%. These numbers show minor shifts since 2024, but
nothing statistically significant. We found that the â€œOtherâ€ category
increased to 11% this year, and this was primarily driven by Hetzner (20% of
Other responses); we plan to include Hetzner as a response choice in next
yearâ€™s survey.We also asked respondents about their development experience of working with
different cloud providers. The most common responses, however, showed that
respondents werenâ€™t really sure (46%) or donâ€™t directly interact with public
cloud providers (21%). The biggest driver behind these responses was a theme
weâ€™ve heard often before: with containers, itâ€™s possible to abstract many
details of the cloud environment away from the developer, so that they donâ€™t
meaningfully interact with most provider-specific technologies. This result
suggests that even developers whose work is  to clouds may have
limited experience with the larger suite of tools and technology associated
with each cloud provider. For example:â€œKinda abstract to the platform, Go is very easy to put in a container and
so pretty easy to deploy anywhere: one of its big strength[s].â€ â€œThe cloud provider really doesnâ€™t make much difference to me. I write code
and deploy it to containers, so whether thatâ€™s AWS or GCP I donâ€™t really
care.â€ We suspect this level of abstraction is dependant on the use case and
requirements of the service thatâ€™s being deployed â€” it may not always
make sense or be possible to keep it highly abstracted. In the future, we plan
to further investigate how Go developers tend to interact with the platforms
where their software is ultimately deployed.Finally, we canâ€™t discuss development environments in 2025 without also
mentioning AI-powered software development tools. Our survey suggests
bifurcated adoption â€” while a majority of respondents (53%) said they
use such tools daily, there is also a large group (29%) who do not use these
at all, or only used them a few times during the past month. We expected this
to negatively correlate with age or development experience, but were unable to
find strong evidence supporting this theory except for  new developers:
respondents with less than one year of professional development experience
(not specific to Go) did report more AI use than every other cohort, but this
group only represented 2% of survey respondents.At this time, agentic use of AI-powered tools appears nascent among Go
developers, with only 17% of respondents saying this is their primary way of
using such tools, though a larger group (40%) are occasionally trying agentic
modes of operation.The most commonly used AI assistants remain ChatGPT, GitHub Copilot, and
Claude. Most of these agents show lower usage numbers compared with our 2024
survey (Claude and Cursor are
notable exceptions), but due to a methodology change, this is not an
apples-to-apples comparison. It is, however, plausible that developers are
â€œshopping aroundâ€ less than they were when these tools were first released,
resulting in more people using a single assistant for most of their work.We also asked about overall satisfaction with AI-powered development tools. A
majority (55%) reported being satisfied, but this was heavily weighted towards
the â€œSomewhat satisfiedâ€ category (42%) vs. the â€œVery satisfiedâ€ group (13%).
Recall that Go itself consistently shows a 90%+ satisfaction rate each year;
this year, 62% of respondents said they are â€œVery satisfiedâ€ with Go. We add
this context to show that while AI-powered tooling is starting to see adoption
and finding some successful use cases, developer sentiment  towards them
remains much softer than towards more established tooling (among Go
developers, at least).What is driving this lower rate of satisfaction? In a word: quality. We asked
respondents to tell us something good theyâ€™ve accomplished with these tools,
as well as something that didnâ€™t work out well. A majority said that creating
non-functional code was their primary problem with AI developer tools (53%),
with 30% lamenting that even working code was of poor quality. The most
frequently cited benefits, conversely, were generating unit tests, writing
boilerplate code, enhanced autocompletion, refactoring, and documentation
generation. These appear to be cases where code quality is perceived as less
critical, tipping the balance in favor of letting AI take the first pass at a
task. That said, respondents also told us the AI-generated code in these
successful cases still required careful review (and often, corrections), as it
can be buggy, insecure, or lack context.â€œIâ€™m never satisfied with code quality or consistency, it never follows the
practices I want to.â€ â€œAll AI tools tend to hallucinate quickly when working with medium-to-large
codebases (10k+ lines of code). They can explain code effectively but
struggle to generate new, complex featuresâ€ â€œDespite numerous efforts to make it write code in an established codebase,
it would take too much effort to steer it to follow the practices in the
project, and it would add subtle behaviour paths - i.e. if it would miss
some method it would try to find its way around it or rely on some side
effect. Sometimes those things are hard to recognize during code review. I
also found it mentally taxing to review ai generated code and that overhead
kills the productivity potential in writing code.â€ When we asked developers what they used these tools for, a pattern emerged
that is consistent with these quality concerns. The tasks with most adoption
(green in the chart below) and least resistance (red) deal with bridging
knowledge gaps, improving local code, and avoiding toil. The frustrations that
developers talk about with code-generating tools were much less evident when
theyâ€™re seeking information, like how to use a specific API or configure test
coverage, and perhaps as a result, we see higher usage of AI in these areas.
Another spot that stood out was  code review and related suggestions
â€” people were less interested in using AI to review other peopleâ€™s code
than in reviewing their own. Surprisingly, â€œtesting codeâ€ showed lower AI
adoption than other toilsome tasks, though we donâ€™t yet have strong
understanding of why.Of all the tasks we asked about, â€œWriting codeâ€ was the most bifurcated, with
66% of respondents already or hoping to soon use AI for this, while Â¼ of
respondents didnâ€™t want AI involved at all. Open-ended responses suggest
developers primarily use this for toilsome, repetitive code, and continue to
have concerns about the quality of AI-generated code.Once again, a tremendous thank-you to everyone who responded to this yearâ€™s Go
Developer Survey!We plan to share the raw survey dataset in Q1 2026, so the larger community
can also explore the data underlying these findings. This will only include
responses from people who opted in to share this data (82% of all
respondents), so there may be some differences from the numbers we reference
in this post.This survey was conducted between Sept 9 - Sept 30, 2025. Participants were
publicly invited to respond via the Go Blog, invitations on social media
channels (including Bluesky, Mastodon, Reddit, and X), as well as randomized
in-product invitations to people using VS Code and GoLand to write Go
software. We received a total of 7,070 responses. After data cleaning to
remove bots and other very low quality responses, 5,379 were used for the
remainder of our analysis. The median survey response time was between 12
â€“ 13 minutes.Throughout this report we use charts of survey responses to provide supporting
evidence for our findings. All of these charts use a similar format. The title
is the exact question that survey respondents saw. Unless otherwise noted,
questions were multiple choice and participants could only select a single
response choice; each chartâ€™s subtitle will tell the reader if the question
allowed multiple response choices or was an open-ended text box instead of a
multiple choice question. For charts of open-ended text responses, a Go team
member read and manually categorized all of the responses. Many open-ended
questions elicited a wide variety of responses; to keep the chart sizes
reasonable, we condensed them to a maximum of the top 10-12 themes, with
additional themes all grouped under â€œOtherâ€. The percentage labels shown in
charts are rounded to the nearest integer (e.g., 1.4% and 0.8% will both be
displayed as 1%), but the length of each bar and row ordering are based on the
unrounded values.To help readers understand the weight of evidence underlying each finding, we
included error bars showing the 95% confidence
interval for responses;
narrower bars indicate increased confidence. Sometimes two or more responses
have overlapping error bars, which means the relative order of those responses
is not statistically meaningful (i.e., the responses are effectively tied).
The lower right of each chart shows the number of people whose responses are
included in the chart, in the form â€œn = [number of respondents]â€.]]></content:encoded></item><item><title>crates.io: development update</title><link>https://blog.rust-lang.org/2026/01/21/crates-io-development-update/</link><author>Tobias Bieniek</author><category>dev</category><pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[Time flies! Six months have passed since our last crates.io development update, so it's time for another one. Here's a summary of the most notable changes and improvements made to crates.io over the past six months.Crate pages now have a new "Security" tab that displays security advisories from the RustSec database. This allows you to quickly see if a crate has known vulnerabilities before adding it as a dependency.The tab shows known vulnerabilities for the crate along with the affected version ranges.This feature is still a work in progress, and we plan to add more functionality in the future. We would like to thank the OpenSSF (Open Source Security Foundation) for funding this work and Dirkjan Ochtman for implementing it.
Trusted Publishing EnhancementsIn our July 2025 update, we announced Trusted Publishing support for GitHub Actions. Since then, we have made several enhancements to this feature.Trusted Publishing now supports GitLab CI/CD in addition to GitHub Actions. This allows GitLab users to publish crates without managing API tokens, using the same OIDC-based authentication flow.Note that this currently only works with GitLab.com. Self-hosted GitLab instances are not supported yet. The crates.io implementation has been refactored to support multiple CI providers, so adding support for other platforms like Codeberg/Forgejo in the future should be straightforward. Contributions are welcome!
Trusted Publishing Only ModeCrate owners can now enforce Trusted Publishing for their crates. When enabled in the crate settings, traditional API token-based publishing is disabled, and only Trusted Publishing can be used to publish new versions. This reduces the risk of unauthorized publishes from leaked API tokens.The  and  GitHub Actions triggers are now blocked from Trusted Publishing. These triggers have been responsible for multiple security incidents in the GitHub Actions ecosystem and are not worth the risk.Crate pages now display source lines of code (SLOC) metrics, giving you insight into the size of a crate before adding it as a dependency. This metric is calculated in a background job after publishing using the tokei crate. It is also shown on OpenGraph images:
Publication Time in IndexA new  field has been added to crate index entries, recording when each version was published. This enables several use cases:Cargo can implement cooldown periods for new versions in the futureCargo can replay dependency resolution as if it were a past date, though yanked versions remain yankedServices like Renovate can determine release dates without additional API requests
Svelte Frontend MigrationAt the end of 2025, the crates.io team evaluated several options for modernizing our frontend and decided to experiment with porting the website to Svelte. The goal is to create a one-to-one port of the existing functionality before adding new features.This migration is still considered experimental and is a work in progress. Using a more mainstream framework should make it easier for new contributors to work on the frontend. The new Svelte frontend uses TypeScript and generates type-safe API client code from our OpenAPI description, so types flow from the Rust backend to the TypeScript frontend automatically.Thanks to eth3lbert for the helpful reviews and guidance on Svelte best practices. We'll share more details in a future update.These were some of the more visible changes to crates.io over the past six months, but a lot has happened "under the hood" as well.Cargo user agent filtering: We noticed that download graphs were showing a constant background level of downloads even for unpopular crates due to bots, scrapers, and mirrors. Download counts are now filtered to only include requests from Cargo, providing more accurate statistics.: Emails from crates.io now support HTML formatting.: OAuth access tokens from GitHub are now encrypted at rest in the database. While we have no evidence of any abuse, we decided to improve our security posture. The tokens were never included in the daily database dump, and the old unencrypted column has been removed.: Crate pages now display a "Browse source" link in the sidebar that points to the corresponding docs.rs page. Thanks to Carol Nichols for implementing this feature.: The sparse index at index.crates.io is now served primarily via Fastly to conserve our AWS credits for other use cases. In the past month, static.crates.io served approximately 1.6 PB across 11 billion requests, while index.crates.io served approximately 740 TB across 19 billion requests. A big thank you to Fastly for providing free CDN services through their Fast Forward program!OpenGraph image improvements: We fixed emoji and CJK character rendering in OpenGraph images, which was caused by missing fonts on our server.Background worker performance: Database indexes were optimized to improve background job processing performance.CloudFront invalidation improvements: Invalidation requests are now batched to avoid hitting AWS rate limits when publishing large workspaces.We hope you enjoyed this update on the development of crates.io. If you have any feedback or questions, please let us know on Zulip or GitHub. We are always happy to hear from you and are looking forward to your feedback!]]></content:encoded></item><item><title>Uniform API server access using clientcmd</title><link>https://kubernetes.io/blog/2026/01/19/clientcmd-apiserver-access/</link><author></author><category>dev</category><pubDate>Mon, 19 Jan 2026 18:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[If you've ever wanted to develop a command line client for a Kubernetes API,
especially if you've considered making your client usable as a  plugin,
you might have wondered how to make your client feel familiar to users of .
A quick glance at the output of  might put a damper on that:
"Am I really supposed to implement all those options?"Fear not, others have done a lot of the work involved for you.
In fact, the Kubernetes project provides two libraries to help you handle
-style command line arguments in Go programs:
 and

(which uses ).
This article will show how to use the former.As might be expected since it's part of ,
's ultimate purpose is to provide an instance of

that can issue requests to an API server.It follows  semantics:defaults are taken from  or equivalent;files can be specified using the  environment variable;all of the above settings can be further overridden using command line arguments.It doesn't set up a  command line argument,
which you might want to do to align with ;
you'll see how to do this
in the "Bind the flags" section. allows programs to handle selection (using );client certificates and private keys;HTTP Basic authentication support (username/password).In various scenarios,  supports  configuration settings:
 can specify multiple files whose contents are combined.
This can be confusing, because settings are merged in different directions
depending on how they are implemented.
If a setting is defined in a map, the first definition wins,
subsequent definitions are ignored.
If a setting is not defined in a map, the last definition wins.When settings are retrieved using ,
missing files result in warnings only.
If the user explicitly specifies a path (in  style),
there must be a corresponding file.If  isn't defined,
the default configuration file, , is used instead,
if present.The general usage pattern is succinctly expressed in
the  package documentation:In the context of this article, there are six steps:Configure the loading rulesclientcmd.NewDefaultClientConfigLoadingRules() builds loading rules which will use either the contents of the  environment variable,
or the default configuration file name ().
In addition, if the default configuration file is used,
it is able to migrate settings from the (very) old default configuration file
().You can build your own ,
but in most cases the defaults are fine.clientcmd.ConfigOverrides is a  storing overrides which will be applied over the settings loaded from the configuration derived using the loading rules.
In the context of this article,
its primary purpose is to store values obtained from command line arguments.
These are handled using the pflag library,
which is a drop-in replacement for Go's  package,
adding support for double-hyphen arguments with long names.In most cases there's nothing to set in the overrides;
I will only bind them to flags.In this context, a flag is a representation of a command line argument,
specifying its long name (such as ),
its short name if any (such as ),
its default value,
and a description shown in the usage information.
Flags are stored in instances of
the  struct.Three sets of flags are available,
representing the following command line arguments:authentication arguments (certificates, tokens, impersonations, username/password);cluster arguments (API server, certificate authority, TLS configuration, proxy, compression)context arguments (cluster name,  user name, namespace)The recommended selection includes all three with a named context selection argument and a timeout argument.These are all available using the  functions.
The functions take a prefix, which is prepended to all the argument long names.So calling
clientcmd.RecommendedConfigOverrideFlags("")
results in command line arguments such as , , and so on.
The  argument is given a default value of 0,
and the  argument has a corresponding short variant, .
Adding a prefix, such as , results in command line arguments such as
, , etc.
This might not seem particularly useful on commands involving a single API server,
but they come in handy when multiple API servers are involved,
such as in multi-cluster scenarios.There's a potential gotcha here: prefixes don't modify the short name,
so  needs some care if multiple prefixes are used:
only one of the prefixes can be associated with the  short name.
You'll have to clear the short names associated with the other prefixes'
 , or perhaps all prefixes if there's no sensible
 association.
Short names can be cleared as follows:In a similar fashion, flags can be disabled entirely by clearing their long name:Once a set of flags has been defined,
it can be used to bind command line arguments to overrides using
clientcmd.BindOverrideFlags.
This requires a

rather than one from Go's  package.If you also want to bind , you should do so now,
by binding  in the loading rules:Build the merged configurationTwo functions are available to build a merged configuration:As the names suggest, the difference between the two is that the first
can ask for authentication information interactively,
using a provided reader,
whereas the second only operates on the information given to it by the caller.The "deferred" mention in these function names refers to the fact that
the final configuration will be determined as late as possible.
This means that these functions can be called before the command line arguments are parsed,
and the resulting configuration will use whatever values have been parsed
by the time it's actually constructed.The merged configuration is returned as a
 instance.
An API client can be obtained from that by calling the  method.If no configuration is given
( is empty or points to non-existent files,
 doesn't exist,
and no configuration is given using command line arguments),
the default setup will return an obscure error referring to .
This is legacy behaviour;
several attempts have been made to get rid of it,
but it is preserved for the  and  command line arguments in .
You should check for "empty configuration" errors by calling clientcmd.IsEmptyConfig()
and provide a more explicit error message.The  method is also useful:
it returns the namespace that should be used.
It also indicates whether the namespace was overridden by the user
(using ).Here's a complete example.Happy coding, and thank you for your interest in implementing tools with
familiar usage patterns!]]></content:encoded></item><item><title>Python 3.15.0 alpha 5 (yes, another alpha!)</title><link>https://pythoninsider.blogspot.com/2026/01/python-3150-alpha-5-yes-another-alpha.html</link><author>Hugo</author><category>dev</category><pubDate>Wed, 14 Jan 2026 17:38:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python official news</source><content:encoded><![CDATA[Note: 3.15.0a4 was accidentally built against  from
2025-12-23 instead of 2026-01-13, so this 3.15.0a5 is an extra release
correctly built against 2026-01-14.This is an early developer preview of Python
3.15Python 3.15 is still in development. This release, 3.15.0a5, is the
fifth of  eight planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2026-05-05) and, if necessary, may be modified or
deleted up until the release candidate phase (2026-07-28). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.15 are still being planned and
written. Among the new major new features and changes so far:PEP
799: A new high-frequency, low-overhead, statistical sampling
profiler and dedicated profiling packagePEP
686: Python now uses UTF-8 as the default encodingPEP
782: A new  C API to create a Python bytes
objectThe JIT
compiler has been significantly upgraded, with 4-5% geometric mean
performance improvement on x86-64 Linux over the standard interpreter,
and 7-8% speedup on AArch64 macOS over the tail-calling interpreter(Hey,  if a feature
you find important is missing from this list, let Hugo
know.)The next pre-release of Python 3.15 will be 3.15.0a6, currently
scheduled for 2026-02-10.At last it was given out that some time next day the ship would
certainly sail. So next morning, Queequeg and I took a very early
start.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software
Foundation.Regards from a still snowfully subzero Helsinki,Your release team,
  Hugo van Kemenade
  Steve Dower
  ]]></content:encoded></item><item><title>What does it take to ship Rust in safety-critical?</title><link>https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/</link><author>Pete LeVasseur</author><category>dev</category><pubDate>Wed, 14 Jan 2026 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[This is another post in our series covering what we learned through the Vision Doc process. In our first post, we described the overall approach and what we learned about doing user research. In our second post, we explored what people love about Rust. This post goes deep on one domain: safety-critical software.When we set out on the Vision Doc work, one area we wanted to explore in depth was safety-critical systems: software where malfunction can result in injury, loss of life, or environmental harm. Think vehicles, airplanes, medical devices, industrial automation. We spoke with engineers at OEMs, integrators, and suppliers across automotive (mostly), industrial, aerospace, and medical contexts.What we found surprised us a bit. The conversations kept circling back to a single tension: Rust's compiler-enforced guarantees support much of what Functional Safety Engineers and Software Engineers in these spaces spend their time preventing, but once you move beyond prototyping into the higher-criticality parts of a system, the ecosystem support thins out fast. There is no MATLAB/Simulink Rust code generation. There is no OSEK or AUTOSAR Classic-compatible RTOS written in Rust or with first-class Rust support. The tooling for qualification and certification is still maturing.
Quick context: what makes software "safety-critical"If you've never worked in these spaces, here's the short version. Each safety-critical domain has standards that define a ladder of integrity levels: ISO 26262 in automotive, IEC 61508 in industrial, IEC 62304 in medical devices, DO-178C in aerospace. The details differ, but the shape is similar: as you climb the ladder toward higher criticality, the demands on your development process, verification, and evidence all increase, and so do the costs.This creates a strong incentive for : isolate the highest-criticality logic into the smallest surface area you can, and keep everything else at lower levels where costs are more manageable and you can move faster.We'll use automotive terminology in this post (QM through ASIL D) since that's where most of our interviews came from, but the patterns generalize. These terms represent increasing levels of safety-criticality, with QM being the lowest and ASIL D being the highest. The story at low criticality looks very different from the story at high criticality, regardless of domain.
Rust is already in production for safety-critical systemsBefore diving into the challenges, it is worth noting that Rust is not just being evaluated in these domains. It is deployed and running in production.We spoke with a principal firmware engineer working on mobile robotics systems certified to IEC 61508 SIL 2:"We had a new project coming up that involved a safety system. And in the past, we'd always done these projects in C using third party stack analysis and unit testing tools that were just generally never very good, but you had to do them as part of the safety rating standards. Rust presented an opportunity where 90% of what the stack analysis stuff had to check for is just done by the compiler. That combined with the fact that now we had a safety qualified compiler to point to was kind of a breakthrough." -- Principal Firmware Engineer (mobile robotics)We also spoke with an engineer at a medical device company deploying IEC 62304 Class B software to intensive care units:"All of the product code that we deploy to end users and customers is currently in Rust. We do EEG analysis with our software and that's being deployed to ICUs, intensive care units, and patient monitors." -- Rust developer at a medical device company"We changed from this Python component to a Rust component and I think that gave us a 100-fold speed increase." -- Rust developer at a medical device companyThese are not proofs of concept. They are shipping systems in regulated environments, going through audits and certification processes. The path is there. The question is how to make it easier for the next teams coming through.
Rust adoption is easiest at QM, and the constraints sharpen fastAt low criticality, teams described a pragmatic approach: use Rust and the crates ecosystem to move quickly, then harden what you ship. One architect at an automotive OEM told us:"We can use any crate [from crates.io] [..] we have to take care to prepare the software components for production usage." -- Architect at Automotive OEMBut at higher levels, third-party dependencies become difficult to justify. Teams either rewrite, internalize, or strictly constrain what they use. An embedded systems engineer put it bluntly:"We tend not to use 3rd party dependencies or nursery crates [..] solutions become kludgier as you get lower in the stack." -- Firmware EngineerSome teams described building escape hatches, abstraction layers designed for future replacement:"We create an interface that we'd eventually like to have to simplify replacement later on [..] sometimes rewrite, but even if re-using an existing crate we often change APIs, write more tests." -- Team Lead at Automotive Supplier (ASIL D target)Even teams that do use crates from crates.io described treating that as a temporary accelerator, something to track carefully and remove from critical paths before shipping:"We use crates mainly for things in the beginning where we need to set up things fast, proof of concept, but we try to track those dependencies very explicitly and for the critical parts of the software try to get rid of them in the long run." -- Team lead at an automotive software company developing middleware in RustIn aerospace, the "control the whole stack" instinct is even stronger:"In aerospace there's a notion of we must own all the code ourselves. We must have control of every single line of code." -- Engineering lead in aerospaceThis is the first big takeaway: a lot of "Rust in safety-critical" is not just about whether Rust compiles for a target. It is about whether teams can assemble an evidence-friendly software stack and keep it stable over long product lifetimes.Many interviewees framed Rust's value in terms of work shifted earlier and made more repeatable by the compiler. This is not just "nice," it changes how much manual review you can realistically afford. Much of what was historically process-based enforcement through coding standards like MISRA C and CERT C becomes a language-level concern in Rust, checked by the compiler rather than external static analysis or manual review."Roughly 90% of what we used to check with external tools is built into Rust's compiler." -- Principal Firmware Engineer (mobile robotics)We heard variations of this from teams dealing with large codebases and varied skill levels:"We cannot control the skill of developers from end to end. We have to check the code quality. Rust by checking at compile time, or Clippy tools, is very useful for our domain." -- Engineer at a major automakerEven on smaller teams, the review load matters:"I usually tend to work on teams between five and eight. Even so, it's too much code. I feel confident moving faster, a certain class of flaws that you aren't worrying about." -- Embedded systems engineer (mobile robotics)Closely related: people repeatedly highlighted Rust's consistency around error handling:"Having a single accepted way of handling errors used throughout the ecosystem is something that Rust did completely right." -- Automotive Technical LeadFor teams building products with 15-to-20-year lifetimes and "teams of teams," compiler-enforced invariants scale better than "we will just review harder."A common pattern in safety-critical environments is conservative toolchain selection. But engineers pointed out a tension: older toolchains carry their own defect history."[..] traditional wisdom is that after something's been around and gone through motions / testing then considered more stable and safer [..] older compilers used tend to have more bugs [and they become] hard to justify" -- Software Engineer at an Automotive supplierRust's edition system was described as a real advantage here, especially for incremental migration strategies that are common in automotive programs:"[The edition system is] golden for automotive, where incremental migration is essential." -- Software Engineer at major AutomakerIn practice, "stability" is also about managing the mismatch between what the platform supports and what the ecosystem expects. Teams described pinning Rust versions, then fighting dependency drift:"We can pin the Rust toolchain, but because almost all crates are implemented for the latest versions, we have to downgrade. It's very time-consuming." -- Engineer at a major automakerFor safety-critical adoption, "stability" is operational. Teams need to answer questions like: What does a Rust upgrade change, and what does it not change? What are the bounds on migration work? How do we demonstrate we have managed upgrade risk?
Target support matters in practical waysSafety-critical software often runs on long-lived platforms and RTOSs. Even when "support exists," there can be caveats. Teams described friction around targets like QNX, where upstream Rust support exists but with limitations (for example, QNX 8.0 support is currently  only).This connects to Rust's target tier policy: the policy itself is clear, but regulated teams still need to map "tier" to "what can I responsibly bet on for this platform and this product lifetime.""I had experiences where all of a sudden I was upgrading the compiler and my toolchain and dependencies didn't work anymore for the Tier 3 target we're using. That's simply not acceptable. If you want to invest in some technology, you want to have a certain reliability." -- Senior software engineer at a major automaker is the spine, and it sets expectationsIn  environments,  becomes the spine of Rust. Teams described it as both rich enough to build real products and small enough to audit.A lot of Rust's safety leverage lives there:  and , slices, iterators,  and , atomics, , . But we also heard a consistent shape of gaps: many embedded and safety-critical projects want -friendly building blocks (fixed-size collections, queues) and predictable math primitives, but do not want to rely on "just any" third-party crate at higher integrity levels."Most of the math library stuff is not in core, it's in std. Sin, cosine... the workaround for now has been the libm crate. It'd be nice if it was in core." -- Principal Firmware Engineer (mobile robotics)
Async is appealing, but the long-run story is not settledSome safety-critical-adjacent systems are already heavily asynchronous: daemons, middleware frameworks, event-driven architectures. That makes Rust's async story interesting.But people also expressed uncertainty about ecosystem lock-in and what it would take to use async in higher-criticality components. One team lead developing middleware told us:"We're not sure how async will work out in the long-run [in Rust for safety-critical]. [..] A lot of our software is highly asynchronous and a lot of our daemons in the AUTOSAR Adaptive Platform world are basically following a reactor pattern. [..] [C++14] doesn't really support these concepts, so some of this is lack of familiarity." -- Team lead at an automotive software company developing middleware in RustAnd when teams look at async through an ISO 26262 lens, the runtime question shows up immediately:"If we want to make use of async Rust, of course you need some runtime which is providing this with all the quality artifacts and process artifacts for ISO 26262." -- Team lead at an automotive software company developing middleware in RustAsync is not "just a language feature" in safety-critical contexts. It pulls in runtime choices, scheduling assumptions, and, at higher integrity levels, the question of what it would mean to certify or qualify the relevant parts of the stack.Find ways to help the safety-critical community support their own needs. Open source helps those who help themselves. The Ferrocene Language Specification (FLS) shows this working well: it started as an industry effort to create a specification suitable for safety-qualification of the Rust compiler, companies invested in the work, and it now has a sustainable home under the Rust Project with a team actively maintaining it.Contrast this with MC/DC coverage support in rustc. Earlier efforts stalled due to lack of sustained engagement from safety-critical companies. The technical work was there, but without industry involvement to help define requirements, validate the implementation, and commit to maintaining it, the effort lost momentum. A major concern was that the MC/DC code added maintenance burden to the rest of the coverage infrastructure without a clear owner. Now in 2026, there is renewed interest in doing this the right way: companies are working through the Safety-Critical Rust Consortium to create a Rust Project Goal in 2026 to collaborate with the Rust Project on MC/DC support. The model is shared ownership of requirements, with primary implementation and maintenance done by companies with a vested interest in safety-critical, done in a way that does not impede maintenance of the rest of the coverage code.The remaining recommendations follow this pattern: the Safety-Critical Rust Consortium can help the community organize requirements and drive work, with the Rust Project providing the deep technical knowledge of Rust Project artifacts needed for successful collaboration. The path works when both sides show up.Establish ecosystem-wide MSRV conventions. The dependency drift problem is real: teams pin their Rust toolchain for stability, but crates targeting the latest compiler make this difficult to sustain. An LTS release scheme, combined with encouraging libraries to maintain MSRV compatibility with LTS releases, could reduce this friction. This would require coordination between the Rust Project (potentially the release team) and the broader ecosystem, with the Safety-Critical Rust Consortium helping to articulate requirements and adoption patterns.Turn "target tier policy" into a safety-critical onramp. The friction we heard is not about the policy being unclear, it is about translating "tier" into practical decisions. A short, target-focused readiness checklist would help: Which targets exist? Which ones are  only? What is the last known tested OS version? What are the top blockers? The raw ingredients exist in rustc docs, release notes, and issue trackers, but pulling them together in one place would lower the barrier. Clearer, consolidated information also makes it easier for teams who depend on specific targets to contribute to maintaining them. The Safety-Critical Rust Consortium could lead this effort, working with compiler team members and platform maintainers to keep the information accurate.Document "dependency lifecycle" patterns teams are already using. The QM story is often: use crates early, track carefully, shrink dependencies for higher-criticality parts. The ASIL B+ story is often: avoid third-party crates entirely, or use abstraction layers and plan to replace later. Turning those patterns into a reusable playbook would help new teams make the same moves with less trial and error. This seems like a natural fit for the Safety-Critical Rust Consortium's liaison work.Define requirements for a safety-case friendly async runtime. Teams adopting async in safety-critical contexts need runtimes with appropriate quality and process artifacts for standards like ISO 26262. Work is already happening in this space. The Safety-Critical Rust Consortium could lead the effort to define what "safety-case friendly" means in concrete terms, working with the async working group and libs team on technical feasibility and design.Treat interop as part of the safety story. Many teams are not going to rewrite their world in Rust. They are going to integrate Rust into existing C and C++ systems and carry that boundary for years. Guidance and tooling to keep interfaces correct, auditable, and in sync would help. The compiler team and lang team could consider how FFI boundaries are surfaced and checked, informed by requirements gathered through the Safety-Critical Rust Consortium."We rely very heavily on FFI compatibility between C, C++, and Rust. In a safety-critical space, that's where the difficulty ends up being, generating bindings, finding out what the problem was." -- Embedded systems engineer (mobile robotics)To sum up the main points in this post:Rust is already deployed in production for safety-critical systems, including mobile robotics (IEC 61508 SIL 2) and medical devices (IEC 62304 Class B). The path exists.Rust's defaults (memory safety, thread safety, strong typing) map directly to much of what Functional Safety Engineers spend their time preventing. But ecosystem support thins out as you move toward higher-criticality software.At low criticality (QM), teams use crates freely and harden later. At higher levels (ASIL B+), third-party dependencies become difficult to justify, and teams rewrite, internalize, or build abstraction layers for future replacement.The compiler is doing work that used to require external tools and manual review. Much of what was historically process-based enforcement through standards like MISRA C and CERT C becomes a language-level concern, checked by the compiler. That can scale better than "review harder" for long-lived products with large teams and supports engineers in these domains feeling more secure in the systems they ship.Stability is operational: teams need to explain what upgrades change, manage dependency drift, and map target tier policies to their platform reality.Async is appealing for middleware and event-driven systems, but the runtime and qualification story is not settled for higher-criticality use.We make six recommendations: find ways to help the safety-critical community support their own needs, establish ecosystem-wide MSRV conventions, create target-focused readiness checklists, document dependency lifecycle patterns, define requirements for safety-case friendly async runtimes, and treat C/C++ interop as part of the safety story.Hearing concrete constraints, examples of assessor feedback, and what "evidence" actually looks like in practice is incredibly helpful. The goal is to make Rust's strengths more accessible in environments where correctness and safety are not optional.]]></content:encoded></item><item><title>Python 3.15.0 alpha 4</title><link>https://pythoninsider.blogspot.com/2026/01/python-3150-alpha-4.html</link><author>Hugo</author><category>dev</category><pubDate>Tue, 13 Jan 2026 20:24:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python official news</source><content:encoded><![CDATA[Edit: This 3.15.0a4 was accidentally built against `main` from 2025-12-23 instead of 2026-01-13, so 3.15.0a5 is an extra release correctly built against 2026-01-14.This is an early developer preview of Python
3.15Python 3.15 is still in development. This release, 3.15.0a4, is the
fourth of seven planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2026-05-05) and, if necessary, may be modified or
deleted up until the release candidate phase (2026-07-28). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.15 are still being planned and
written. Among the new major new features and changes so far:PEP
799: A new high-frequency, low-overhead, statistical sampling
profiler and dedicated profiling packagePEP
686: Python now uses UTF-8 as the default encodingPEP
782: A new  C API to create a Python bytes
objectThe JIT
compiler has been significantly upgraded, with 3-4% geometric mean
performance improvement on x86-64 Linux over the standard interpreter,
and 7-8% speedup on AArch64 macOS over the tail-calling interpreter(Hey,  if a feature
you find important is missing from this list, let Hugo
know.)The next pre-release of Python 3.15 will be 3.15.0a5, currently
scheduled for 2026-02-10.Upon this every soul was confounded; for the phenomenon just then
observed by Ahab had unaccountably escaped every one else; but its very
blinding palpableness must have been the cause.Thrusting his head half way into the binnacle, Ahab caught one
glimpse of the compasses; his uplifted arm slowly fell; for a moment he
almost seemed to stagger. Standing behind him Starbuck looked, and lo!
the two compasses pointed East, and the Pequod was as infallibly going
West.But ere the first wild alarm could get out abroad among the crew, the
old man with a rigid laugh exclaimed, â€œI have it! It has happened
before. Mr.Â Starbuck, last nightâ€™s thunder turned our compassesâ€”thatâ€™s
all. Thou hast before now heard of such a thing, I take it.â€â€œAye; but never before has it happened to me, sir,â€ said the pale
mate, gloomily.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software
Foundation.Regards from a snowfully subzero Helsinki,Your release team,
  Hugo van Kemenade
  Steve Dower
  ]]></content:encoded></item><item><title>Kubernetes v1.35: Restricting executables invoked by kubeconfigs via exec plugin allowList added to kuberc</title><link>https://kubernetes.io/blog/2026/01/09/kubernetes-v1-35-kuberc-credential-plugin-allowlist/</link><author></author><category>dev</category><pubDate>Fri, 9 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Did you know that  can run arbitrary executables, including shell
scripts, with the full privileges of the invoking user, and without your
knowledge? Whenever you download or auto-generate a , the
 field can specify an executable to fetch credentials on
your behalf. Don't get me wrong, this is an incredible feature that allows you
to authenticate to the cluster with external identity providers. Nevertheless,
you probably see the problem: Do you know exactly what executables your 
is running on your system? Do you trust the pipeline that generated your ?
If there has been a supply-chain attack on the code that generates the kubeconfig,
or if the generating pipeline has been compromised, an attacker might well be
doing unsavory things to your machine by tricking your  into running
arbitrary code.To give the user more control over what gets run on their system, SIG-Auth and SIG-CLI added the credential plugin policy and allowlist as a beta feature to
Kubernetes 1.35. This is available to all clients using the  library,
by filling out the ExecProvider.PluginPolicy struct on a REST config. To
broaden the impact of this change, Kubernetes v1.35 also lets you manage this without
writing a line of application code. You can configure  to enforce
the policy and allowlist by adding two fields to the  configuration
file:  and credentialPluginAllowlist. Adding one or
both of these fields restricts which credential plugins  is allowed to execute.A full description of this functionality is available in our official documentation for kuberc,
but this blog post will give a brief overview of the new security knobs. The new
features are in beta and available without using any feature gates.The following example is the simplest one: simply don't specify the new fields.This will keep  acting as it always has, and all plugins will be
allowed.The next example is functionally identical, but it is more explicit and
therefore preferred if it's actually what you want:If you  whether or not you're using exec credential plugins, try
setting your policy to :If you  using credential plugins, you'll quickly find out what  is
trying to execute. You'll get an error like the following.Unable to connect to the server: getting credentials: plugin "cloudco-login" not allowed: policy set to "DenyAll"If there is insufficient information for you to debug the issue, increase the
logging verbosity when you run your next command. For example:Selectively allowing pluginsWhat if you need the  plugin to do your daily work? That is why
there's a third option for your policy, . To allow a specific plugin,
set the policy and add the credentialPluginAllowlist:You'll notice that there are two entries in the allowlist. One of them is
specified by full path, and the other,  is just a basename. When
you specify just the basename, the full path will be looked up using
, which does not expand globbing or handle wildcards.
Globbing is not supported at this time. Both forms
(basename and full path) are acceptable, but the full path is preferable because
it narrows the scope of allowed binaries even further.Currently, an allowlist entry has only one field, . In the future, we
(Kubernetes SIG CLI) want to see other requirements added. One idea that seems
useful is checksum verification whereby, for example, a binary would only be allowed
to run if it has the sha256 sum
b9a3fad00d848ff31960c44ebb5f8b92032dc085020f857c98e32a5d5900ff9c
exists at the path .Another possibility is only allowing binaries that have been signed by one of a
set of a trusted signing keys.The credential plugin policy is still under development and we are very interested
in your feedback. We'd love to hear what you like about it and what problems
you'd like to see it solve. Or, if you have the cycles to contribute one of the
above enhancements, they'd be a great way to get started contributing to
Kubernetes. Feel free to join in the discussion on slack:]]></content:encoded></item><item><title>Kubernetes v1.35: Mutable PersistentVolume Node Affinity (alpha)</title><link>https://kubernetes.io/blog/2026/01/08/kubernetes-v1-35-mutable-pv-nodeaffinity/</link><author></author><category>dev</category><pubDate>Thu, 8 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[The PersistentVolume node affinity API
dates back to Kubernetes v1.10.
It is widely used to express that volumes may not be equally accessible by all nodes in the cluster.
This field was previously immutable,
and it is now mutable in Kubernetes v1.35 (alpha). This change opens a door to more flexible online volume management.Why make node affinity mutable?This raises an obvious question: why make node affinity mutable now?
While stateless workloads like Deployments can be changed freely
and the changes will be rolled out automatically by re-creating every Pod,
PersistentVolumes (PVs) are stateful and cannot be re-created easily without losing data.However, Storage providers evolve and storage requirements change.
Most notably, multiple providers are offering regional disks now.
Some of them even support live migration from zonal to regional disks, without disrupting the workloads.
This change can be expressed through the
VolumeAttributesClass API,
which recently graduated to GA in 1.34.
However, even if the volume is migrated to regional storage,
Kubernetes still prevents scheduling Pods to other zones because of the node affinity recorded in the PV object.
In this case, you may want to change the PV node affinity from:As another example, providers sometimes offer new generations of disks.
New disks cannot always be attached to older nodes in the cluster.
This accessibility can also be expressed through PV node affinity and ensures the Pods can be scheduled to the right nodes.
But when the disk is upgraded, new Pods using this disk can still be scheduled to older nodes.
To prevent this, you may want to change the PV node affinity from:So, it is mutable now, a first step towards a more flexible online volume management.
While it is a simple change that removes one validation from the API server,
we still have a long way to go to integrate well with the Kubernetes ecosystem.This feature is for you if you are a Kubernetes cluster administrator,
and your storage provider allows online update that you want to utilize,
but those updates can affect the accessibility of the volume.Note that changing PV node affinity alone will not actually change the accessibility of the underlying volume.
Before using this feature,
you must first update the underlying volume in the storage provider,
and understand which nodes can access the volume after the update.
You can then enable this feature and keep the PV node affinity in sync.Currently, this feature is in alpha state.
It is disabled by default, and may subject to change.
To try it out, enable the  feature gate on APIServer, then you can edit the PV  field.
Typically only administrators can edit PVs, please make sure you have the right RBAC permissions.Race condition between updating and schedulingThere are only a few factors outside of a Pod that can affect the scheduling decision, and PV node affinity is one of them.
It is fine to allow more nodes to access the volume by relaxing node affinity,
but there is a race condition when you try to tighten node affinity:
it is unclear how the Scheduler will see the modified PV in its cache,
so there is a small window where the scheduler may place a Pod on an old node that can no longer access the volume.
In this case, the Pod will stuck at  state.One mitigation currently under discussion is for the kubelet to fail Pod startup if the PersistentVolumeâ€™s node affinity is violated.
This has not landed yet.
So if you are trying this out now, please watch subsequent Pods that use the updated PV,
and make sure they are scheduled onto nodes that can access the volume.
If you update PV and immediately start new Pods in a script, it may not work as intended.Future integration with CSI (Container Storage Interface)Currently, it is up to the cluster administrator to modify both PV's node affinity and the underlying volume in the storage provider.
But manual operations are error-prone and time-consuming.
It is preferred to eventually integrate this with VolumeAttributesClass,
so that an unprivileged user can modify their PersistentVolumeClaim (PVC) to trigger storage-side updates,
and PV node affinity is updated automatically when appropriate, without the need for cluster admin's intervention.As noted earlier, this is only a first step.If you are a Kubernetes user,
we would like to learn how you use (or will use) PV node affinity.
Is it beneficial to update it online in your case?If you are a CSI driver developer,
would you be willing to implement this feature? How would you like the API to look?Please provide your feedback via:For any inquiries or specific questions related to this feature, please reach out to the SIG Storage community.]]></content:encoded></item><item><title>Kubernetes v1.35: A Better Way to Pass Service Account Tokens to CSI Drivers</title><link>https://kubernetes.io/blog/2026/01/07/kubernetes-v1-35-csi-sa-tokens-secrets-field-beta/</link><author></author><category>dev</category><pubDate>Wed, 7 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[If you maintain a CSI driver that uses service account tokens,
Kubernetes v1.35 brings a refinement you'll want to know about.
Since the introduction of the TokenRequests feature,
service account tokens requested by CSI drivers have been passed to them through the  field.
While this has worked, it's not the ideal place for sensitive information,
and we've seen instances where tokens were accidentally logged in CSI drivers.Kubernetes v1.35 introduces a beta solution to address this:
CSI Driver Opt-in for Service Account Tokens via Secrets Field.
This allows CSI drivers to receive service account tokens
through the  field in ,
which is the appropriate place for sensitive data in the CSI specification.Understanding the existing approachWhen CSI drivers use the TokenRequests feature,
they can request service account tokens for workload identity
by configuring the  field in the CSIDriver spec.
These tokens are passed to drivers as part of the volume attributes map,
using the key csi.storage.k8s.io/serviceAccount.tokens.The  field works, but it's not designed for sensitive data.
Because of this, there are a few challenges:First, the  tool that CSI drivers use doesn't treat volume context as sensitive,
so service account tokens can end up in logs when gRPC requests are logged.
This happened with CVE-2023-2878 in the Secrets Store CSI Driver
and CVE-2024-3744 in the Azure File CSI Driver.Second, each CSI driver that wants to avoid this issue needs to implement its own sanitization logic,
which leads to inconsistency across drivers.The CSI specification already has a  field in 
that's designed exactly for this kind of sensitive information.
The challenge is that we can't just change where we put the tokens
without breaking existing CSI drivers that expect them in volume context.How the opt-in mechanism worksKubernetes v1.35 introduces an opt-in mechanism that lets CSI drivers choose
how they receive service account tokens.
This way, existing drivers continue working as they do today,
and drivers can move to the more appropriate secrets field when they're ready.CSI drivers can set a new field in their CSIDriver spec:The behavior depends on the serviceAccountTokenInSecrets field:When set to  (the default), tokens are placed in  with the key csi.storage.k8s.io/serviceAccount.tokens, just like today.
When set to , tokens are placed only in the  field with the same key.The CSIServiceAccountTokenSecrets feature gate is enabled by default
on both kubelet and kube-apiserver.
Since the serviceAccountTokenInSecrets field defaults to ,
enabling the feature gate doesn't change any existing behavior.
All drivers continue receiving tokens via volume context unless they explicitly opt in.
This is why we felt comfortable starting at beta rather than alpha.Guide for CSI driver authorsIf you maintain a CSI driver that uses service account tokens, here's how to adopt this feature.First, update your driver code to check both locations for tokens.
This makes your driver compatible with both the old and new approaches:This fallback logic is backward compatible and safe to ship in any driver version,
even before clusters upgrade to v1.35.CSI driver authors need to follow a specific sequence when adopting this feature to avoid breaking existing volumes. (can happen anytime)You can start preparing your driver right away by adding fallback logic that checks both the secrets field and volume context for tokens.
This code change is backward compatible and safe to ship in any driver version, even before clusters upgrade to v1.35.
We encourage you to add this fallback logic early, cut releases, and even backport to maintenance branches where feasible.Cluster upgrade and feature enablementOnce your driver has the fallback logic deployed, here's the safe rollout order for enabling the feature in a cluster:Complete the kube-apiserver upgrade to 1.35 or laterComplete kubelet upgrade to 1.35 or later on all nodesEnsure CSI driver version with fallback logic is deployed (if not already done in preparation phase)Fully complete CSI driver DaemonSet rollout across all nodesUpdate your CSIDriver manifest to set serviceAccountTokenInSecrets: trueThe most important thing to remember is timing.
If your CSI driver DaemonSet and CSIDriver object are in the same manifest or Helm chart,
you need two separate updates.
Deploy the new driver version with fallback logic first,
wait for the DaemonSet rollout to complete,
then update the CSIDriver spec to set serviceAccountTokenInSecrets: true.Also, don't update the CSIDriver before all driver pods have rolled out.
If you do, volume mounts will fail on nodes still running the old driver version,
since those pods only check volume context.Adopting this feature helps in a few ways:It eliminates the risk of accidentally logging service account tokens as part of volume context in gRPC requestsIt uses the CSI specification's designated field for sensitive data, which feels rightThe  tool automatically handles the secrets field correctly, so you don't need driver-specific workaroundsIt's opt-in, so you can migrate at your own pace without breaking existing deploymentsWe (Kubernetes SIG Storage) encourage CSI driver authors to adopt this feature and provide feedback
on the migration experience.
If you have thoughts on the API design or run into any issues during adoption,
please reach out to us on the
#csi channel on Kubernetes Slack
(for an invitation, visit https://slack.k8s.io/).You can follow along on
KEP-5538
to track progress across the coming Kubernetes releases.]]></content:encoded></item><item><title>Kubernetes v1.35: Extended Toleration Operators to Support Numeric Comparisons (Alpha)</title><link>https://kubernetes.io/blog/2026/01/05/kubernetes-v1-35-numeric-toleration-operators/</link><author></author><category>dev</category><pubDate>Mon, 5 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Many production Kubernetes clusters blend on-demand (higher-SLA) and spot/preemptible (lower-SLA) nodes to optimize costs while maintaining reliability for critical workloads. Platform teams need a safe default that keeps most workloads away from risky capacity, while allowing specific workloads to opt-in with explicit thresholds like "I can tolerate nodes with failure probability up to 5%".Today, Kubernetes taints and tolerations can match exact values or check for existence, but they can't compare numeric thresholds. You'd need to create discrete taint categories, use external admission controllers, or accept less-than-optimal placement decisions.In Kubernetes v1.35, we're introducing Extended Toleration Operators as an alpha feature. This enhancement adds  (Greater Than) and  (Less Than) operators to , enabling threshold-based scheduling decisions that unlock new possibilities for SLA-based placement, cost optimization, and performance-aware workload distribution.The evolution of tolerationsHistorically, Kubernetes supported two primary toleration operators:: The toleration matches a taint if the key and value are exactly equal: The toleration matches a taint if the key exists, regardless of valueWhile these worked well for categorical scenarios, they fell short for numeric comparisons. Starting with v1.35, we are closing this gap.Consider these real-world scenarios:: Schedule high-availability workloads only on nodes with failure probability below a certain threshold: Allow cost-sensitive batch jobs to run on cheaper nodes that exceed a specific cost-per-hour value: Ensure latency-sensitive applications run only on nodes with disk IOPS or network bandwidth above minimum thresholdsWithout numeric comparison operators, cluster operators have had to resort to workarounds like creating multiple discrete taint values or using external admission controllers, neither of which scale well or provide the flexibility needed for dynamic threshold-based scheduling.Why extend tolerations instead of using NodeAffinity?You might wonder: NodeAffinity already supports numeric comparison operators, so why extend tolerations? While NodeAffinity is powerful for expressing pod preferences, taints and tolerations provide critical operational benefits:: NodeAffinity is per-pod, requiring every workload to explicitly opt-out of risky nodes. Taints invert controlâ€”nodes declare their risk level, and only pods with matching tolerations may land there. This provides a safer default; most pods stay away from spot/preemptible nodes unless they explicitly opt-in.: NodeAffinity has no eviction capability. Taints support the  effect with , enabling operators to drain and evict pods when a node's SLA degrades or spot instances receive termination notices.: Centralized, node-side policy is consistent with other safety taints like disk-pressure and memory-pressure, making cluster management more intuitive.This enhancement preserves the well-understood safety model of taints and tolerations while enabling threshold-based placement for SLA-aware scheduling.Introducing Gt and Lt operatorsKubernetes v1.35 introduces two new operators for tolerations:: The toleration matches if the taint's numeric value is less than the toleration's value: The toleration matches if the taint's numeric value is greater than the toleration's valueWhen a pod tolerates a taint with , it's saying "I can tolerate nodes where this metric is  my threshold". Since tolerations allow scheduling, the pod can run on nodes where the taint value is greater than the toleration value. Think of it as: "I tolerate nodes that are above my minimum requirements".These operators work with numeric taint values and enable the scheduler to make sophisticated placement decisions based on continuous metrics rather than discrete categories.Numeric values for  and  operators must be positive 64-bit integers without leading zeros. For example,  is valid, but  (with leading zero) and  (zero value) are not permitted.The  and  operators work with all taint effects: , , and .Let's explore how Extended Toleration Operators solve real-world scheduling challenges.Example 1: Spot instance protection with SLA thresholdsMany clusters mix on-demand and spot/preemptible nodes to optimize costs. Spot nodes offer significant savings but have higher failure rates. You want most workloads to avoid spot nodes by default, while allowing specific workloads to opt-in with clear SLA boundaries.First, taint spot nodes with their failure probability (for example, 15% annual failure rate):On-demand nodes have much lower failure rates:Critical workloads can specify strict SLA requirements:This pod will  schedule on nodes with  less than 5 (meaning  with 2% but not  with 15%). The  effect with  means if a node's SLA degrades (for example, cloud provider changes the taint value), the pod gets 30 seconds to gracefully terminate before forced eviction.Meanwhile, a fault-tolerant batch job can explicitly opt-in to spot instances:This batch job tolerates nodes with failure probability up to 20%, so it can run on both on-demand and spot nodes, maximizing cost savings while accepting higher risk.Example 2: AI workload placement with GPU tiersAI and machine learning workloads often have specific hardware requirements. With Extended Toleration Operators, you can create GPU node tiers and ensure workloads land on appropriately powered hardware.Taint GPU nodes with their compute capability score:A heavy training workload can require high-performance GPUs:This ensures the training pod only schedules on nodes with compute scores greater than 800 (like the A100 node), preventing placement on lower-tier GPUs that would slow down training.Meanwhile, inference workloads with less demanding requirements can use any available GPU:Example 3: Cost-optimized workload placementFor batch processing or non-critical workloads, you might want to minimize costs by running on cheaper nodes, even if they have lower performance characteristics.Nodes can be tainted with their cost rating:A cost-sensitive batch job can express its tolerance for expensive nodes:This batch job will schedule on nodes costing less than $100/hour but avoid more expensive nodes. Combined with Kubernetes scheduling priorities, this enables sophisticated cost-tiering strategies where critical workloads get premium nodes while batch workloads efficiently use budget-friendly resources.Storage-intensive applications often require minimum disk performance guarantees. With Extended Toleration Operators, you can enforce these requirements at the scheduling level.This toleration ensures the pod only schedules on nodes where  exceeds 3000. The  operator means "I need nodes that are greater than this minimum".Extended Toleration Operators is an  in Kubernetes v1.35. To try it out: on both your API server and scheduler: with numeric values representing the metrics relevant to your scheduling needs: in your pod specifications:As an alpha feature, Extended Toleration Operators may change in future releases and should be used with caution in production environments. Always test thoroughly in non-production clusters first.This alpha release is just the beginning. As we gather feedback from the community, we plan to:Improve integration with cluster autoscaling for threshold-aware capacity planningGraduate the feature to beta and eventually GA with production-ready stabilityWe're particularly interested in hearing about your use cases! Do you have scenarios where threshold-based scheduling would solve problems? Are there additional operators or capabilities you'd like to see?This feature is driven by the SIG Scheduling community. Please join us to connect with the community and share your ideas and feedback around this feature and beyond.You can reach the maintainers of this feature at:For questions or specific inquiries related to Extended Toleration Operators, please reach out to the SIG Scheduling community. We look forward to hearing from you!]]></content:encoded></item><item><title>Project goals update â€” December 2025</title><link>https://blog.rust-lang.org/2026/01/05/project-goals-2025-december-update/</link><author>Tomas Sedovic</author><category>dev</category><pubDate>Mon, 5 Jan 2026 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[David has made "progress on the non-Sized Hierarchy part of the goal, the infrastructure for defining scalable vector types has been merged (with them being Sized in the interim) and that'll make it easier to iterate on those and find issues that need solving".On the Sized hierarchy part of the goal, no progress. We discussed options for migrating. There seem to be three big options:(A) The conservative-but-obvious route where the in the old edition is expanded to T: Deref<Target: SizeOfVal> (but in the new edition it means T: Deref<Target: Pointee>, i.e., no additional bounds). The main  is that new Edition code using  can't call old Edition code using  as the old edition code has stronger bounds. Therefore new edition code must either use stronger bounds than it needs  wait until that old edition code has been updated.(B) You do something smart with Edition.Old code where you figure out if the bound can be loose or strict by bottom-up computation. So  in the old could mean either T: Deref<Target: Pointee> or T: Deref<Target: SizeOfVal>, depending on what the function actually does.(C) You make Edition.Old code always mean T: Deref<Target: Pointee> and you still allow calls to  but have them cause post-monomorphization errors if used inappropriately. In Edition.New you use stricter checking.Options (B) and (C) have the downside that changes to the function body (adding a call to , specifically) in the old edition can stop callers from compiling. In the case of Option (B), that breakage is at type-check time, because it can change the where-clauses. In Option (C), the breakage is post-monomorphization.Option (A) has the disadvantage that it takes longer for the new bounds to roll out.Given this, (A) seems the preferred path. We discussed options for how to encourage that roll-out. We discussed the idea of a lint that would warn Edition.Old code that its bounds are stronger than needed and suggest rewriting to T: Deref<Target: Pointee> to explicitly disable the stronger Edition.Old default. This lint could be implemented in one of two waysat type-check time, by tracking what parts of the environment are used by the trait solver. This may be feasible in the new trait solver, someone from @rust-lang/types would have to say.at post-mono time, by tracking which functions  and propagating that information back to callers. You could then compare against the generic bounds declared on the caller.The former is more useful (knowing what parts of the environment are necessary could be useful for more things, e.g., better caching); the latter may be easier or more precise.]]></content:encoded></item></channel></rss>
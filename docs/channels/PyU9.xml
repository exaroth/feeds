<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev</title><link>https://konrad.website/feeds/</link><description></description><item><title>Uniform API server access using clientcmd</title><link>https://kubernetes.io/blog/2026/01/19/clientcmd-apiserver-access/</link><author></author><category>dev</category><pubDate>Mon, 19 Jan 2026 18:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[If you've ever wanted to develop a command line client for a Kubernetes API,
especially if you've considered making your client usable as a  plugin,
you might have wondered how to make your client feel familiar to users of .
A quick glance at the output of  might put a damper on that:
"Am I really supposed to implement all those options?"Fear not, others have done a lot of the work involved for you.
In fact, the Kubernetes project provides two libraries to help you handle
-style command line arguments in Go programs:
 and

(which uses ).
This article will show how to use the former.As might be expected since it's part of ,
's ultimate purpose is to provide an instance of

that can issue requests to an API server.It follows  semantics:defaults are taken from  or equivalent;files can be specified using the  environment variable;all of the above settings can be further overridden using command line arguments.It doesn't set up a  command line argument,
which you might want to do to align with ;
you'll see how to do this
in the "Bind the flags" section. allows programs to handle selection (using );client certificates and private keys;HTTP Basic authentication support (username/password).In various scenarios,  supports  configuration settings:
 can specify multiple files whose contents are combined.
This can be confusing, because settings are merged in different directions
depending on how they are implemented.
If a setting is defined in a map, the first definition wins,
subsequent definitions are ignored.
If a setting is not defined in a map, the last definition wins.When settings are retrieved using ,
missing files result in warnings only.
If the user explicitly specifies a path (in  style),
there must be a corresponding file.If  isn't defined,
the default configuration file, , is used instead,
if present.The general usage pattern is succinctly expressed in
the  package documentation:In the context of this article, there are six steps:Configure the loading rulesclientcmd.NewDefaultClientConfigLoadingRules() builds loading rules which will use either the contents of the  environment variable,
or the default configuration file name ().
In addition, if the default configuration file is used,
it is able to migrate settings from the (very) old default configuration file
().You can build your own ,
but in most cases the defaults are fine.clientcmd.ConfigOverrides is a  storing overrides which will be applied over the settings loaded from the configuration derived using the loading rules.
In the context of this article,
its primary purpose is to store values obtained from command line arguments.
These are handled using the pflag library,
which is a drop-in replacement for Go's  package,
adding support for double-hyphen arguments with long names.In most cases there's nothing to set in the overrides;
I will only bind them to flags.In this context, a flag is a representation of a command line argument,
specifying its long name (such as ),
its short name if any (such as ),
its default value,
and a description shown in the usage information.
Flags are stored in instances of
the  struct.Three sets of flags are available,
representing the following command line arguments:authentication arguments (certificates, tokens, impersonations, username/password);cluster arguments (API server, certificate authority, TLS configuration, proxy, compression)context arguments (cluster name,  user name, namespace)The recommended selection includes all three with a named context selection argument and a timeout argument.These are all available using the  functions.
The functions take a prefix, which is prepended to all the argument long names.So calling
clientcmd.RecommendedConfigOverrideFlags("")
results in command line arguments such as , , and so on.
The  argument is given a default value of 0,
and the  argument has a corresponding short variant, .
Adding a prefix, such as , results in command line arguments such as
, , etc.
This might not seem particularly useful on commands involving a single API server,
but they come in handy when multiple API servers are involved,
such as in multi-cluster scenarios.There's a potential gotcha here: prefixes don't modify the short name,
so  needs some care if multiple prefixes are used:
only one of the prefixes can be associated with the  short name.
You'll have to clear the short names associated with the other prefixes'
 , or perhaps all prefixes if there's no sensible
 association.
Short names can be cleared as follows:In a similar fashion, flags can be disabled entirely by clearing their long name:Once a set of flags has been defined,
it can be used to bind command line arguments to overrides using
clientcmd.BindOverrideFlags.
This requires a

rather than one from Go's  package.If you also want to bind , you should do so now,
by binding  in the loading rules:Build the merged configurationTwo functions are available to build a merged configuration:As the names suggest, the difference between the two is that the first
can ask for authentication information interactively,
using a provided reader,
whereas the second only operates on the information given to it by the caller.The "deferred" mention in these function names refers to the fact that
the final configuration will be determined as late as possible.
This means that these functions can be called before the command line arguments are parsed,
and the resulting configuration will use whatever values have been parsed
by the time it's actually constructed.The merged configuration is returned as a
 instance.
An API client can be obtained from that by calling the  method.If no configuration is given
( is empty or points to non-existent files,
 doesn't exist,
and no configuration is given using command line arguments),
the default setup will return an obscure error referring to .
This is legacy behaviour;
several attempts have been made to get rid of it,
but it is preserved for the  and  command line arguments in .
You should check for "empty configuration" errors by calling clientcmd.IsEmptyConfig()
and provide a more explicit error message.The  method is also useful:
it returns the namespace that should be used.
It also indicates whether the namespace was overridden by the user
(using ).Here's a complete example.Happy coding, and thank you for your interest in implementing tools with
familiar usage patterns!]]></content:encoded></item><item><title>Python 3.15.0 alpha 5 (yes, another alpha!)</title><link>https://pythoninsider.blogspot.com/2026/01/python-3150-alpha-5-yes-another-alpha.html</link><author>Hugo</author><category>dev</category><pubDate>Wed, 14 Jan 2026 17:38:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python official news</source><content:encoded><![CDATA[Note: 3.15.0a4 was accidentally built against  from
2025-12-23 instead of 2026-01-13, so this 3.15.0a5 is an extra release
correctly built against 2026-01-14.This is an early developer preview of Python
3.15Python 3.15 is still in development. This release, 3.15.0a5, is the
fifth of  eight planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2026-05-05) and, if necessary, may be modified or
deleted up until the release candidate phase (2026-07-28). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.15 are still being planned and
written. Among the new major new features and changes so far:PEP
799: A new high-frequency, low-overhead, statistical sampling
profiler and dedicated profiling packagePEP
686: Python now uses UTF-8 as the default encodingPEP
782: A new  C API to create a Python bytes
objectThe JIT
compiler has been significantly upgraded, with 4-5% geometric mean
performance improvement on x86-64 Linux over the standard interpreter,
and 7-8% speedup on AArch64 macOS over the tail-calling interpreter(Hey,  if a feature
you find important is missing from this list, let Hugo
know.)The next pre-release of Python 3.15 will be 3.15.0a6, currently
scheduled for 2026-02-10.At last it was given out that some time next day the ship would
certainly sail. So next morning, Queequeg and I took a very early
start.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software
Foundation.Regards from a still snowfully subzero Helsinki,Your release team,
  Hugo van Kemenade
  Steve Dower
  ]]></content:encoded></item><item><title>What does it take to ship Rust in safety-critical?</title><link>https://blog.rust-lang.org/2026/01/14/what-does-it-take-to-ship-rust-in-safety-critical/</link><author>Pete LeVasseur</author><category>dev</category><pubDate>Wed, 14 Jan 2026 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[This is another post in our series covering what we learned through the Vision Doc process. In our first post, we described the overall approach and what we learned about doing user research. In our second post, we explored what people love about Rust. This post goes deep on one domain: safety-critical software.When we set out on the Vision Doc work, one area we wanted to explore in depth was safety-critical systems: software where malfunction can result in injury, loss of life, or environmental harm. Think vehicles, airplanes, medical devices, industrial automation. We spoke with engineers at OEMs, integrators, and suppliers across automotive (mostly), industrial, aerospace, and medical contexts.What we found surprised us a bit. The conversations kept circling back to a single tension: Rust's compiler-enforced guarantees support much of what Functional Safety Engineers and Software Engineers in these spaces spend their time preventing, but once you move beyond prototyping into the higher-criticality parts of a system, the ecosystem support thins out fast. There is no MATLAB/Simulink Rust code generation. There is no OSEK or AUTOSAR Classic-compatible RTOS written in Rust or with first-class Rust support. The tooling for qualification and certification is still maturing.
Quick context: what makes software "safety-critical"If you've never worked in these spaces, here's the short version. Each safety-critical domain has standards that define a ladder of integrity levels: ISO 26262 in automotive, IEC 61508 in industrial, IEC 62304 in medical devices, DO-178C in aerospace. The details differ, but the shape is similar: as you climb the ladder toward higher criticality, the demands on your development process, verification, and evidence all increase, and so do the costs.This creates a strong incentive for : isolate the highest-criticality logic into the smallest surface area you can, and keep everything else at lower levels where costs are more manageable and you can move faster.We'll use automotive terminology in this post (QM through ASIL D) since that's where most of our interviews came from, but the patterns generalize. These terms represent increasing levels of safety-criticality, with QM being the lowest and ASIL D being the highest. The story at low criticality looks very different from the story at high criticality, regardless of domain.
Rust is already in production for safety-critical systemsBefore diving into the challenges, it is worth noting that Rust is not just being evaluated in these domains. It is deployed and running in production.We spoke with a principal firmware engineer working on mobile robotics systems certified to IEC 61508 SIL 2:"We had a new project coming up that involved a safety system. And in the past, we'd always done these projects in C using third party stack analysis and unit testing tools that were just generally never very good, but you had to do them as part of the safety rating standards. Rust presented an opportunity where 90% of what the stack analysis stuff had to check for is just done by the compiler. That combined with the fact that now we had a safety qualified compiler to point to was kind of a breakthrough." -- Principal Firmware Engineer (mobile robotics)We also spoke with an engineer at a medical device company deploying IEC 62304 Class B software to intensive care units:"All of the product code that we deploy to end users and customers is currently in Rust. We do EEG analysis with our software and that's being deployed to ICUs, intensive care units, and patient monitors." -- Rust developer at a medical device company"We changed from this Python component to a Rust component and I think that gave us a 100-fold speed increase." -- Rust developer at a medical device companyThese are not proofs of concept. They are shipping systems in regulated environments, going through audits and certification processes. The path is there. The question is how to make it easier for the next teams coming through.
Rust adoption is easiest at QM, and the constraints sharpen fastAt low criticality, teams described a pragmatic approach: use Rust and the crates ecosystem to move quickly, then harden what you ship. One architect at an automotive OEM told us:"We can use any crate [from crates.io] [..] we have to take care to prepare the software components for production usage." -- Architect at Automotive OEMBut at higher levels, third-party dependencies become difficult to justify. Teams either rewrite, internalize, or strictly constrain what they use. An embedded systems engineer put it bluntly:"We tend not to use 3rd party dependencies or nursery crates [..] solutions become kludgier as you get lower in the stack." -- Firmware EngineerSome teams described building escape hatches, abstraction layers designed for future replacement:"We create an interface that we'd eventually like to have to simplify replacement later on [..] sometimes rewrite, but even if re-using an existing crate we often change APIs, write more tests." -- Team Lead at Automotive Supplier (ASIL D target)Even teams that do use crates from crates.io described treating that as a temporary accelerator, something to track carefully and remove from critical paths before shipping:"We use crates mainly for things in the beginning where we need to set up things fast, proof of concept, but we try to track those dependencies very explicitly and for the critical parts of the software try to get rid of them in the long run." -- Team lead at an automotive software company developing middleware in RustIn aerospace, the "control the whole stack" instinct is even stronger:"In aerospace there's a notion of we must own all the code ourselves. We must have control of every single line of code." -- Engineering lead in aerospaceThis is the first big takeaway: a lot of "Rust in safety-critical" is not just about whether Rust compiles for a target. It is about whether teams can assemble an evidence-friendly software stack and keep it stable over long product lifetimes.Many interviewees framed Rust's value in terms of work shifted earlier and made more repeatable by the compiler. This is not just "nice," it changes how much manual review you can realistically afford. Much of what was historically process-based enforcement through coding standards like MISRA C and CERT C becomes a language-level concern in Rust, checked by the compiler rather than external static analysis or manual review."Roughly 90% of what we used to check with external tools is built into Rust's compiler." -- Principal Firmware Engineer (mobile robotics)We heard variations of this from teams dealing with large codebases and varied skill levels:"We cannot control the skill of developers from end to end. We have to check the code quality. Rust by checking at compile time, or Clippy tools, is very useful for our domain." -- Engineer at a major automakerEven on smaller teams, the review load matters:"I usually tend to work on teams between five and eight. Even so, it's too much code. I feel confident moving faster, a certain class of flaws that you aren't worrying about." -- Embedded systems engineer (mobile robotics)Closely related: people repeatedly highlighted Rust's consistency around error handling:"Having a single accepted way of handling errors used throughout the ecosystem is something that Rust did completely right." -- Automotive Technical LeadFor teams building products with 15-to-20-year lifetimes and "teams of teams," compiler-enforced invariants scale better than "we will just review harder."A common pattern in safety-critical environments is conservative toolchain selection. But engineers pointed out a tension: older toolchains carry their own defect history."[..] traditional wisdom is that after something's been around and gone through motions / testing then considered more stable and safer [..] older compilers used tend to have more bugs [and they become] hard to justify" -- Software Engineer at an Automotive supplierRust's edition system was described as a real advantage here, especially for incremental migration strategies that are common in automotive programs:"[The edition system is] golden for automotive, where incremental migration is essential." -- Software Engineer at major AutomakerIn practice, "stability" is also about managing the mismatch between what the platform supports and what the ecosystem expects. Teams described pinning Rust versions, then fighting dependency drift:"We can pin the Rust toolchain, but because almost all crates are implemented for the latest versions, we have to downgrade. It's very time-consuming." -- Engineer at a major automakerFor safety-critical adoption, "stability" is operational. Teams need to answer questions like: What does a Rust upgrade change, and what does it not change? What are the bounds on migration work? How do we demonstrate we have managed upgrade risk?
Target support matters in practical waysSafety-critical software often runs on long-lived platforms and RTOSs. Even when "support exists," there can be caveats. Teams described friction around targets like QNX, where upstream Rust support exists but with limitations (for example, QNX 8.0 support is currently  only).This connects to Rust's target tier policy: the policy itself is clear, but regulated teams still need to map "tier" to "what can I responsibly bet on for this platform and this product lifetime.""I had experiences where all of a sudden I was upgrading the compiler and my toolchain and dependencies didn't work anymore for the Tier 3 target we're using. That's simply not acceptable. If you want to invest in some technology, you want to have a certain reliability." -- Senior software engineer at a major automaker is the spine, and it sets expectationsIn  environments,  becomes the spine of Rust. Teams described it as both rich enough to build real products and small enough to audit.A lot of Rust's safety leverage lives there:  and , slices, iterators,  and , atomics, , . But we also heard a consistent shape of gaps: many embedded and safety-critical projects want -friendly building blocks (fixed-size collections, queues) and predictable math primitives, but do not want to rely on "just any" third-party crate at higher integrity levels."Most of the math library stuff is not in core, it's in std. Sin, cosine... the workaround for now has been the libm crate. It'd be nice if it was in core." -- Principal Firmware Engineer (mobile robotics)
Async is appealing, but the long-run story is not settledSome safety-critical-adjacent systems are already heavily asynchronous: daemons, middleware frameworks, event-driven architectures. That makes Rust's async story interesting.But people also expressed uncertainty about ecosystem lock-in and what it would take to use async in higher-criticality components. One team lead developing middleware told us:"We're not sure how async will work out in the long-run [in Rust for safety-critical]. [..] A lot of our software is highly asynchronous and a lot of our daemons in the AUTOSAR Adaptive Platform world are basically following a reactor pattern. [..] [C++14] doesn't really support these concepts, so some of this is lack of familiarity." -- Team lead at an automotive software company developing middleware in RustAnd when teams look at async through an ISO 26262 lens, the runtime question shows up immediately:"If we want to make use of async Rust, of course you need some runtime which is providing this with all the quality artifacts and process artifacts for ISO 26262." -- Team lead at an automotive software company developing middleware in RustAsync is not "just a language feature" in safety-critical contexts. It pulls in runtime choices, scheduling assumptions, and, at higher integrity levels, the question of what it would mean to certify or qualify the relevant parts of the stack.Find ways to help the safety-critical community support their own needs. Open source helps those who help themselves. The Ferrocene Language Specification (FLS) shows this working well: it started as an industry effort to create a specification suitable for safety-qualification of the Rust compiler, companies invested in the work, and it now has a sustainable home under the Rust Project with a team actively maintaining it.Contrast this with MC/DC coverage support in rustc. Earlier efforts stalled due to lack of sustained engagement from safety-critical companies. The technical work was there, but without industry involvement to help define requirements, validate the implementation, and commit to maintaining it, the effort lost momentum. A major concern was that the MC/DC code added maintenance burden to the rest of the coverage infrastructure without a clear owner. Now in 2026, there is renewed interest in doing this the right way: companies are working through the Safety-Critical Rust Consortium to create a Rust Project Goal in 2026 to collaborate with the Rust Project on MC/DC support. The model is shared ownership of requirements, with primary implementation and maintenance done by companies with a vested interest in safety-critical, done in a way that does not impede maintenance of the rest of the coverage code.The remaining recommendations follow this pattern: the Safety-Critical Rust Consortium can help the community organize requirements and drive work, with the Rust Project providing the deep technical knowledge of Rust Project artifacts needed for successful collaboration. The path works when both sides show up.Establish ecosystem-wide MSRV conventions. The dependency drift problem is real: teams pin their Rust toolchain for stability, but crates targeting the latest compiler make this difficult to sustain. An LTS release scheme, combined with encouraging libraries to maintain MSRV compatibility with LTS releases, could reduce this friction. This would require coordination between the Rust Project (potentially the release team) and the broader ecosystem, with the Safety-Critical Rust Consortium helping to articulate requirements and adoption patterns.Turn "target tier policy" into a safety-critical onramp. The friction we heard is not about the policy being unclear, it is about translating "tier" into practical decisions. A short, target-focused readiness checklist would help: Which targets exist? Which ones are  only? What is the last known tested OS version? What are the top blockers? The raw ingredients exist in rustc docs, release notes, and issue trackers, but pulling them together in one place would lower the barrier. Clearer, consolidated information also makes it easier for teams who depend on specific targets to contribute to maintaining them. The Safety-Critical Rust Consortium could lead this effort, working with compiler team members and platform maintainers to keep the information accurate.Document "dependency lifecycle" patterns teams are already using. The QM story is often: use crates early, track carefully, shrink dependencies for higher-criticality parts. The ASIL B+ story is often: avoid third-party crates entirely, or use abstraction layers and plan to replace later. Turning those patterns into a reusable playbook would help new teams make the same moves with less trial and error. This seems like a natural fit for the Safety-Critical Rust Consortium's liaison work.Define requirements for a safety-case friendly async runtime. Teams adopting async in safety-critical contexts need runtimes with appropriate quality and process artifacts for standards like ISO 26262. Work is already happening in this space. The Safety-Critical Rust Consortium could lead the effort to define what "safety-case friendly" means in concrete terms, working with the async working group and libs team on technical feasibility and design.Treat interop as part of the safety story. Many teams are not going to rewrite their world in Rust. They are going to integrate Rust into existing C and C++ systems and carry that boundary for years. Guidance and tooling to keep interfaces correct, auditable, and in sync would help. The compiler team and lang team could consider how FFI boundaries are surfaced and checked, informed by requirements gathered through the Safety-Critical Rust Consortium."We rely very heavily on FFI compatibility between C, C++, and Rust. In a safety-critical space, that's where the difficulty ends up being, generating bindings, finding out what the problem was." -- Embedded systems engineer (mobile robotics)To sum up the main points in this post:Rust is already deployed in production for safety-critical systems, including mobile robotics (IEC 61508 SIL 2) and medical devices (IEC 62304 Class B). The path exists.Rust's defaults (memory safety, thread safety, strong typing) map directly to much of what Functional Safety Engineers spend their time preventing. But ecosystem support thins out as you move toward higher-criticality software.At low criticality (QM), teams use crates freely and harden later. At higher levels (ASIL B+), third-party dependencies become difficult to justify, and teams rewrite, internalize, or build abstraction layers for future replacement.The compiler is doing work that used to require external tools and manual review. Much of what was historically process-based enforcement through standards like MISRA C and CERT C becomes a language-level concern, checked by the compiler. That can scale better than "review harder" for long-lived products with large teams and supports engineers in these domains feeling more secure in the systems they ship.Stability is operational: teams need to explain what upgrades change, manage dependency drift, and map target tier policies to their platform reality.Async is appealing for middleware and event-driven systems, but the runtime and qualification story is not settled for higher-criticality use.We make six recommendations: find ways to help the safety-critical community support their own needs, establish ecosystem-wide MSRV conventions, create target-focused readiness checklists, document dependency lifecycle patterns, define requirements for safety-case friendly async runtimes, and treat C/C++ interop as part of the safety story.Hearing concrete constraints, examples of assessor feedback, and what "evidence" actually looks like in practice is incredibly helpful. The goal is to make Rust's strengths more accessible in environments where correctness and safety are not optional.]]></content:encoded></item><item><title>Python 3.15.0 alpha 4</title><link>https://pythoninsider.blogspot.com/2026/01/python-3150-alpha-4.html</link><author>Hugo</author><category>dev</category><pubDate>Tue, 13 Jan 2026 20:24:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python official news</source><content:encoded><![CDATA[Edit: This 3.15.0a4 was accidentally built against `main` from 2025-12-23 instead of 2026-01-13, so 3.15.0a5 is an extra release correctly built against 2026-01-14.This is an early developer preview of Python
3.15Python 3.15 is still in development. This release, 3.15.0a4, is the
fourth of seven planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2026-05-05) and, if necessary, may be modified or
deleted up until the release candidate phase (2026-07-28). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.15 are still being planned and
written. Among the new major new features and changes so far:PEP
799: A new high-frequency, low-overhead, statistical sampling
profiler and dedicated profiling packagePEP
686: Python now uses UTF-8 as the default encodingPEP
782: A new  C API to create a Python bytes
objectThe JIT
compiler has been significantly upgraded, with 3-4% geometric mean
performance improvement on x86-64 Linux over the standard interpreter,
and 7-8% speedup on AArch64 macOS over the tail-calling interpreter(Hey,  if a feature
you find important is missing from this list, let Hugo
know.)The next pre-release of Python 3.15 will be 3.15.0a5, currently
scheduled for 2026-02-10.Upon this every soul was confounded; for the phenomenon just then
observed by Ahab had unaccountably escaped every one else; but its very
blinding palpableness must have been the cause.Thrusting his head half way into the binnacle, Ahab caught one
glimpse of the compasses; his uplifted arm slowly fell; for a moment he
almost seemed to stagger. Standing behind him Starbuck looked, and lo!
the two compasses pointed East, and the Pequod was as infallibly going
West.But ere the first wild alarm could get out abroad among the crew, the
old man with a rigid laugh exclaimed, “I have it! It has happened
before. Mr. Starbuck, last night’s thunder turned our compasses—that’s
all. Thou hast before now heard of such a thing, I take it.”“Aye; but never before has it happened to me, sir,” said the pale
mate, gloomily.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software
Foundation.Regards from a snowfully subzero Helsinki,Your release team,
  Hugo van Kemenade
  Steve Dower
  ]]></content:encoded></item><item><title>Kubernetes v1.35: Restricting executables invoked by kubeconfigs via exec plugin allowList added to kuberc</title><link>https://kubernetes.io/blog/2026/01/09/kubernetes-v1-35-kuberc-credential-plugin-allowlist/</link><author></author><category>dev</category><pubDate>Fri, 9 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Did you know that  can run arbitrary executables, including shell
scripts, with the full privileges of the invoking user, and without your
knowledge? Whenever you download or auto-generate a , the
 field can specify an executable to fetch credentials on
your behalf. Don't get me wrong, this is an incredible feature that allows you
to authenticate to the cluster with external identity providers. Nevertheless,
you probably see the problem: Do you know exactly what executables your 
is running on your system? Do you trust the pipeline that generated your ?
If there has been a supply-chain attack on the code that generates the kubeconfig,
or if the generating pipeline has been compromised, an attacker might well be
doing unsavory things to your machine by tricking your  into running
arbitrary code.To give the user more control over what gets run on their system, SIG-Auth and SIG-CLI added the credential plugin policy and allowlist as a beta feature to
Kubernetes 1.35. This is available to all clients using the  library,
by filling out the ExecProvider.PluginPolicy struct on a REST config. To
broaden the impact of this change, Kubernetes v1.35 also lets you manage this without
writing a line of application code. You can configure  to enforce
the policy and allowlist by adding two fields to the  configuration
file:  and credentialPluginAllowlist. Adding one or
both of these fields restricts which credential plugins  is allowed to execute.A full description of this functionality is available in our official documentation for kuberc,
but this blog post will give a brief overview of the new security knobs. The new
features are in beta and available without using any feature gates.The following example is the simplest one: simply don't specify the new fields.This will keep  acting as it always has, and all plugins will be
allowed.The next example is functionally identical, but it is more explicit and
therefore preferred if it's actually what you want:If you  whether or not you're using exec credential plugins, try
setting your policy to :If you  using credential plugins, you'll quickly find out what  is
trying to execute. You'll get an error like the following.Unable to connect to the server: getting credentials: plugin "cloudco-login" not allowed: policy set to "DenyAll"If there is insufficient information for you to debug the issue, increase the
logging verbosity when you run your next command. For example:Selectively allowing pluginsWhat if you need the  plugin to do your daily work? That is why
there's a third option for your policy, . To allow a specific plugin,
set the policy and add the credentialPluginAllowlist:You'll notice that there are two entries in the allowlist. One of them is
specified by full path, and the other,  is just a basename. When
you specify just the basename, the full path will be looked up using
, which does not expand globbing or handle wildcards.
Globbing is not supported at this time. Both forms
(basename and full path) are acceptable, but the full path is preferable because
it narrows the scope of allowed binaries even further.Currently, an allowlist entry has only one field, . In the future, we
(Kubernetes SIG CLI) want to see other requirements added. One idea that seems
useful is checksum verification whereby, for example, a binary would only be allowed
to run if it has the sha256 sum
b9a3fad00d848ff31960c44ebb5f8b92032dc085020f857c98e32a5d5900ff9c
exists at the path .Another possibility is only allowing binaries that have been signed by one of a
set of a trusted signing keys.The credential plugin policy is still under development and we are very interested
in your feedback. We'd love to hear what you like about it and what problems
you'd like to see it solve. Or, if you have the cycles to contribute one of the
above enhancements, they'd be a great way to get started contributing to
Kubernetes. Feel free to join in the discussion on slack:]]></content:encoded></item><item><title>Kubernetes v1.35: Mutable PersistentVolume Node Affinity (alpha)</title><link>https://kubernetes.io/blog/2026/01/08/kubernetes-v1-35-mutable-pv-nodeaffinity/</link><author></author><category>dev</category><pubDate>Thu, 8 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[The PersistentVolume node affinity API
dates back to Kubernetes v1.10.
It is widely used to express that volumes may not be equally accessible by all nodes in the cluster.
This field was previously immutable,
and it is now mutable in Kubernetes v1.35 (alpha). This change opens a door to more flexible online volume management.Why make node affinity mutable?This raises an obvious question: why make node affinity mutable now?
While stateless workloads like Deployments can be changed freely
and the changes will be rolled out automatically by re-creating every Pod,
PersistentVolumes (PVs) are stateful and cannot be re-created easily without losing data.However, Storage providers evolve and storage requirements change.
Most notably, multiple providers are offering regional disks now.
Some of them even support live migration from zonal to regional disks, without disrupting the workloads.
This change can be expressed through the
VolumeAttributesClass API,
which recently graduated to GA in 1.34.
However, even if the volume is migrated to regional storage,
Kubernetes still prevents scheduling Pods to other zones because of the node affinity recorded in the PV object.
In this case, you may want to change the PV node affinity from:As another example, providers sometimes offer new generations of disks.
New disks cannot always be attached to older nodes in the cluster.
This accessibility can also be expressed through PV node affinity and ensures the Pods can be scheduled to the right nodes.
But when the disk is upgraded, new Pods using this disk can still be scheduled to older nodes.
To prevent this, you may want to change the PV node affinity from:So, it is mutable now, a first step towards a more flexible online volume management.
While it is a simple change that removes one validation from the API server,
we still have a long way to go to integrate well with the Kubernetes ecosystem.This feature is for you if you are a Kubernetes cluster administrator,
and your storage provider allows online update that you want to utilize,
but those updates can affect the accessibility of the volume.Note that changing PV node affinity alone will not actually change the accessibility of the underlying volume.
Before using this feature,
you must first update the underlying volume in the storage provider,
and understand which nodes can access the volume after the update.
You can then enable this feature and keep the PV node affinity in sync.Currently, this feature is in alpha state.
It is disabled by default, and may subject to change.
To try it out, enable the  feature gate on APIServer, then you can edit the PV  field.
Typically only administrators can edit PVs, please make sure you have the right RBAC permissions.Race condition between updating and schedulingThere are only a few factors outside of a Pod that can affect the scheduling decision, and PV node affinity is one of them.
It is fine to allow more nodes to access the volume by relaxing node affinity,
but there is a race condition when you try to tighten node affinity:
it is unclear how the Scheduler will see the modified PV in its cache,
so there is a small window where the scheduler may place a Pod on an old node that can no longer access the volume.
In this case, the Pod will stuck at  state.One mitigation currently under discussion is for the kubelet to fail Pod startup if the PersistentVolume’s node affinity is violated.
This has not landed yet.
So if you are trying this out now, please watch subsequent Pods that use the updated PV,
and make sure they are scheduled onto nodes that can access the volume.
If you update PV and immediately start new Pods in a script, it may not work as intended.Future integration with CSI (Container Storage Interface)Currently, it is up to the cluster administrator to modify both PV's node affinity and the underlying volume in the storage provider.
But manual operations are error-prone and time-consuming.
It is preferred to eventually integrate this with VolumeAttributesClass,
so that an unprivileged user can modify their PersistentVolumeClaim (PVC) to trigger storage-side updates,
and PV node affinity is updated automatically when appropriate, without the need for cluster admin's intervention.As noted earlier, this is only a first step.If you are a Kubernetes user,
we would like to learn how you use (or will use) PV node affinity.
Is it beneficial to update it online in your case?If you are a CSI driver developer,
would you be willing to implement this feature? How would you like the API to look?Please provide your feedback via:For any inquiries or specific questions related to this feature, please reach out to the SIG Storage community.]]></content:encoded></item><item><title>Kubernetes v1.35: A Better Way to Pass Service Account Tokens to CSI Drivers</title><link>https://kubernetes.io/blog/2026/01/07/kubernetes-v1-35-csi-sa-tokens-secrets-field-beta/</link><author></author><category>dev</category><pubDate>Wed, 7 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[If you maintain a CSI driver that uses service account tokens,
Kubernetes v1.35 brings a refinement you'll want to know about.
Since the introduction of the TokenRequests feature,
service account tokens requested by CSI drivers have been passed to them through the  field.
While this has worked, it's not the ideal place for sensitive information,
and we've seen instances where tokens were accidentally logged in CSI drivers.Kubernetes v1.35 introduces a beta solution to address this:
CSI Driver Opt-in for Service Account Tokens via Secrets Field.
This allows CSI drivers to receive service account tokens
through the  field in ,
which is the appropriate place for sensitive data in the CSI specification.Understanding the existing approachWhen CSI drivers use the TokenRequests feature,
they can request service account tokens for workload identity
by configuring the  field in the CSIDriver spec.
These tokens are passed to drivers as part of the volume attributes map,
using the key csi.storage.k8s.io/serviceAccount.tokens.The  field works, but it's not designed for sensitive data.
Because of this, there are a few challenges:First, the  tool that CSI drivers use doesn't treat volume context as sensitive,
so service account tokens can end up in logs when gRPC requests are logged.
This happened with CVE-2023-2878 in the Secrets Store CSI Driver
and CVE-2024-3744 in the Azure File CSI Driver.Second, each CSI driver that wants to avoid this issue needs to implement its own sanitization logic,
which leads to inconsistency across drivers.The CSI specification already has a  field in 
that's designed exactly for this kind of sensitive information.
The challenge is that we can't just change where we put the tokens
without breaking existing CSI drivers that expect them in volume context.How the opt-in mechanism worksKubernetes v1.35 introduces an opt-in mechanism that lets CSI drivers choose
how they receive service account tokens.
This way, existing drivers continue working as they do today,
and drivers can move to the more appropriate secrets field when they're ready.CSI drivers can set a new field in their CSIDriver spec:The behavior depends on the serviceAccountTokenInSecrets field:When set to  (the default), tokens are placed in  with the key csi.storage.k8s.io/serviceAccount.tokens, just like today.
When set to , tokens are placed only in the  field with the same key.The CSIServiceAccountTokenSecrets feature gate is enabled by default
on both kubelet and kube-apiserver.
Since the serviceAccountTokenInSecrets field defaults to ,
enabling the feature gate doesn't change any existing behavior.
All drivers continue receiving tokens via volume context unless they explicitly opt in.
This is why we felt comfortable starting at beta rather than alpha.Guide for CSI driver authorsIf you maintain a CSI driver that uses service account tokens, here's how to adopt this feature.First, update your driver code to check both locations for tokens.
This makes your driver compatible with both the old and new approaches:This fallback logic is backward compatible and safe to ship in any driver version,
even before clusters upgrade to v1.35.CSI driver authors need to follow a specific sequence when adopting this feature to avoid breaking existing volumes. (can happen anytime)You can start preparing your driver right away by adding fallback logic that checks both the secrets field and volume context for tokens.
This code change is backward compatible and safe to ship in any driver version, even before clusters upgrade to v1.35.
We encourage you to add this fallback logic early, cut releases, and even backport to maintenance branches where feasible.Cluster upgrade and feature enablementOnce your driver has the fallback logic deployed, here's the safe rollout order for enabling the feature in a cluster:Complete the kube-apiserver upgrade to 1.35 or laterComplete kubelet upgrade to 1.35 or later on all nodesEnsure CSI driver version with fallback logic is deployed (if not already done in preparation phase)Fully complete CSI driver DaemonSet rollout across all nodesUpdate your CSIDriver manifest to set serviceAccountTokenInSecrets: trueThe most important thing to remember is timing.
If your CSI driver DaemonSet and CSIDriver object are in the same manifest or Helm chart,
you need two separate updates.
Deploy the new driver version with fallback logic first,
wait for the DaemonSet rollout to complete,
then update the CSIDriver spec to set serviceAccountTokenInSecrets: true.Also, don't update the CSIDriver before all driver pods have rolled out.
If you do, volume mounts will fail on nodes still running the old driver version,
since those pods only check volume context.Adopting this feature helps in a few ways:It eliminates the risk of accidentally logging service account tokens as part of volume context in gRPC requestsIt uses the CSI specification's designated field for sensitive data, which feels rightThe  tool automatically handles the secrets field correctly, so you don't need driver-specific workaroundsIt's opt-in, so you can migrate at your own pace without breaking existing deploymentsWe (Kubernetes SIG Storage) encourage CSI driver authors to adopt this feature and provide feedback
on the migration experience.
If you have thoughts on the API design or run into any issues during adoption,
please reach out to us on the
#csi channel on Kubernetes Slack
(for an invitation, visit https://slack.k8s.io/).You can follow along on
KEP-5538
to track progress across the coming Kubernetes releases.]]></content:encoded></item><item><title>Kubernetes v1.35: Extended Toleration Operators to Support Numeric Comparisons (Alpha)</title><link>https://kubernetes.io/blog/2026/01/05/kubernetes-v1-35-numeric-toleration-operators/</link><author></author><category>dev</category><pubDate>Mon, 5 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Many production Kubernetes clusters blend on-demand (higher-SLA) and spot/preemptible (lower-SLA) nodes to optimize costs while maintaining reliability for critical workloads. Platform teams need a safe default that keeps most workloads away from risky capacity, while allowing specific workloads to opt-in with explicit thresholds like "I can tolerate nodes with failure probability up to 5%".Today, Kubernetes taints and tolerations can match exact values or check for existence, but they can't compare numeric thresholds. You'd need to create discrete taint categories, use external admission controllers, or accept less-than-optimal placement decisions.In Kubernetes v1.35, we're introducing Extended Toleration Operators as an alpha feature. This enhancement adds  (Greater Than) and  (Less Than) operators to , enabling threshold-based scheduling decisions that unlock new possibilities for SLA-based placement, cost optimization, and performance-aware workload distribution.The evolution of tolerationsHistorically, Kubernetes supported two primary toleration operators:: The toleration matches a taint if the key and value are exactly equal: The toleration matches a taint if the key exists, regardless of valueWhile these worked well for categorical scenarios, they fell short for numeric comparisons. Starting with v1.35, we are closing this gap.Consider these real-world scenarios:: Schedule high-availability workloads only on nodes with failure probability below a certain threshold: Allow cost-sensitive batch jobs to run on cheaper nodes that exceed a specific cost-per-hour value: Ensure latency-sensitive applications run only on nodes with disk IOPS or network bandwidth above minimum thresholdsWithout numeric comparison operators, cluster operators have had to resort to workarounds like creating multiple discrete taint values or using external admission controllers, neither of which scale well or provide the flexibility needed for dynamic threshold-based scheduling.Why extend tolerations instead of using NodeAffinity?You might wonder: NodeAffinity already supports numeric comparison operators, so why extend tolerations? While NodeAffinity is powerful for expressing pod preferences, taints and tolerations provide critical operational benefits:: NodeAffinity is per-pod, requiring every workload to explicitly opt-out of risky nodes. Taints invert control—nodes declare their risk level, and only pods with matching tolerations may land there. This provides a safer default; most pods stay away from spot/preemptible nodes unless they explicitly opt-in.: NodeAffinity has no eviction capability. Taints support the  effect with , enabling operators to drain and evict pods when a node's SLA degrades or spot instances receive termination notices.: Centralized, node-side policy is consistent with other safety taints like disk-pressure and memory-pressure, making cluster management more intuitive.This enhancement preserves the well-understood safety model of taints and tolerations while enabling threshold-based placement for SLA-aware scheduling.Introducing Gt and Lt operatorsKubernetes v1.35 introduces two new operators for tolerations:: The toleration matches if the taint's numeric value is less than the toleration's value: The toleration matches if the taint's numeric value is greater than the toleration's valueWhen a pod tolerates a taint with , it's saying "I can tolerate nodes where this metric is  my threshold". Since tolerations allow scheduling, the pod can run on nodes where the taint value is greater than the toleration value. Think of it as: "I tolerate nodes that are above my minimum requirements".These operators work with numeric taint values and enable the scheduler to make sophisticated placement decisions based on continuous metrics rather than discrete categories.Numeric values for  and  operators must be positive 64-bit integers without leading zeros. For example,  is valid, but  (with leading zero) and  (zero value) are not permitted.The  and  operators work with all taint effects: , , and .Let's explore how Extended Toleration Operators solve real-world scheduling challenges.Example 1: Spot instance protection with SLA thresholdsMany clusters mix on-demand and spot/preemptible nodes to optimize costs. Spot nodes offer significant savings but have higher failure rates. You want most workloads to avoid spot nodes by default, while allowing specific workloads to opt-in with clear SLA boundaries.First, taint spot nodes with their failure probability (for example, 15% annual failure rate):On-demand nodes have much lower failure rates:Critical workloads can specify strict SLA requirements:This pod will  schedule on nodes with  less than 5 (meaning  with 2% but not  with 15%). The  effect with  means if a node's SLA degrades (for example, cloud provider changes the taint value), the pod gets 30 seconds to gracefully terminate before forced eviction.Meanwhile, a fault-tolerant batch job can explicitly opt-in to spot instances:This batch job tolerates nodes with failure probability up to 20%, so it can run on both on-demand and spot nodes, maximizing cost savings while accepting higher risk.Example 2: AI workload placement with GPU tiersAI and machine learning workloads often have specific hardware requirements. With Extended Toleration Operators, you can create GPU node tiers and ensure workloads land on appropriately powered hardware.Taint GPU nodes with their compute capability score:A heavy training workload can require high-performance GPUs:This ensures the training pod only schedules on nodes with compute scores greater than 800 (like the A100 node), preventing placement on lower-tier GPUs that would slow down training.Meanwhile, inference workloads with less demanding requirements can use any available GPU:Example 3: Cost-optimized workload placementFor batch processing or non-critical workloads, you might want to minimize costs by running on cheaper nodes, even if they have lower performance characteristics.Nodes can be tainted with their cost rating:A cost-sensitive batch job can express its tolerance for expensive nodes:This batch job will schedule on nodes costing less than $100/hour but avoid more expensive nodes. Combined with Kubernetes scheduling priorities, this enables sophisticated cost-tiering strategies where critical workloads get premium nodes while batch workloads efficiently use budget-friendly resources.Storage-intensive applications often require minimum disk performance guarantees. With Extended Toleration Operators, you can enforce these requirements at the scheduling level.This toleration ensures the pod only schedules on nodes where  exceeds 3000. The  operator means "I need nodes that are greater than this minimum".Extended Toleration Operators is an  in Kubernetes v1.35. To try it out: on both your API server and scheduler: with numeric values representing the metrics relevant to your scheduling needs: in your pod specifications:As an alpha feature, Extended Toleration Operators may change in future releases and should be used with caution in production environments. Always test thoroughly in non-production clusters first.This alpha release is just the beginning. As we gather feedback from the community, we plan to:Improve integration with cluster autoscaling for threshold-aware capacity planningGraduate the feature to beta and eventually GA with production-ready stabilityWe're particularly interested in hearing about your use cases! Do you have scenarios where threshold-based scheduling would solve problems? Are there additional operators or capabilities you'd like to see?This feature is driven by the SIG Scheduling community. Please join us to connect with the community and share your ideas and feedback around this feature and beyond.You can reach the maintainers of this feature at:For questions or specific inquiries related to Extended Toleration Operators, please reach out to the SIG Scheduling community. We look forward to hearing from you!]]></content:encoded></item><item><title>Project goals update — December 2025</title><link>https://blog.rust-lang.org/2026/01/05/project-goals-2025-december-update/</link><author>Tomas Sedovic</author><category>dev</category><pubDate>Mon, 5 Jan 2026 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[David has made "progress on the non-Sized Hierarchy part of the goal, the infrastructure for defining scalable vector types has been merged (with them being Sized in the interim) and that'll make it easier to iterate on those and find issues that need solving".On the Sized hierarchy part of the goal, no progress. We discussed options for migrating. There seem to be three big options:(A) The conservative-but-obvious route where the in the old edition is expanded to T: Deref<Target: SizeOfVal> (but in the new edition it means T: Deref<Target: Pointee>, i.e., no additional bounds). The main  is that new Edition code using  can't call old Edition code using  as the old edition code has stronger bounds. Therefore new edition code must either use stronger bounds than it needs  wait until that old edition code has been updated.(B) You do something smart with Edition.Old code where you figure out if the bound can be loose or strict by bottom-up computation. So  in the old could mean either T: Deref<Target: Pointee> or T: Deref<Target: SizeOfVal>, depending on what the function actually does.(C) You make Edition.Old code always mean T: Deref<Target: Pointee> and you still allow calls to  but have them cause post-monomorphization errors if used inappropriately. In Edition.New you use stricter checking.Options (B) and (C) have the downside that changes to the function body (adding a call to , specifically) in the old edition can stop callers from compiling. In the case of Option (B), that breakage is at type-check time, because it can change the where-clauses. In Option (C), the breakage is post-monomorphization.Option (A) has the disadvantage that it takes longer for the new bounds to roll out.Given this, (A) seems the preferred path. We discussed options for how to encourage that roll-out. We discussed the idea of a lint that would warn Edition.Old code that its bounds are stronger than needed and suggest rewriting to T: Deref<Target: Pointee> to explicitly disable the stronger Edition.Old default. This lint could be implemented in one of two waysat type-check time, by tracking what parts of the environment are used by the trait solver. This may be feasible in the new trait solver, someone from @rust-lang/types would have to say.at post-mono time, by tracking which functions  and propagating that information back to callers. You could then compare against the generic bounds declared on the caller.The former is more useful (knowing what parts of the environment are necessary could be useful for more things, e.g., better caching); the latter may be easier or more precise.]]></content:encoded></item><item><title>Kubernetes v1.35: New level of efficiency with in-place Pod restart</title><link>https://kubernetes.io/blog/2026/01/02/kubernetes-v1-35-restart-all-containers/</link><author></author><category>dev</category><pubDate>Fri, 2 Jan 2026 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[The release of Kubernetes 1.35 introduces a powerful new feature that provides a much-requested capability: the ability to trigger a full, in-place restart of the Pod. This feature,  (alpha in 1.35), allows for an efficient way to reset a Pod's state compared to resource-intensive approach of deleting and recreating the entire Pod. This feature is especially useful for AI/ML workloads allowing application developers to concentrate on their core training logic while offloading complex failure-handling and recovery mechanisms to sidecars and declarative Kubernetes configuration. With  and other planned enhancements, Kubernetes continues to add building blocks for creating the most flexible, robust, and efficient platforms for AI/ML workloads.This new functionality is available by enabling the RestartAllContainersOnContainerExits feature gate. This alpha feature extends the  feature, which graduated to beta in Kubernetes 1.35.The problem: when a single container restart isn't enough and recreating pods is too costlyKubernetes has long supported restart policies at the Pod level () and, more recently, at the individual container level. These policies are great for handling crashes in a single, isolated process. However, many modern applications have more complex inter-container dependencies. For instance:An  prepares the environment by mounting a volume or generating a configuration file. If the main application container corrupts this environment, simply restarting that one container is not enough. The entire initialization process needs to run again.A  monitors system health. If it detects an unrecoverable but retriable error state, it must trigger a restart of the main application container from a clean slate.A  that manages a remote resource fails. Even if the sidecar restarts on its own, the main container may be stuck trying to access an outdated or broken connection.In all these cases, the desired action is not to restart a single container, but all of them. Previously, the only way to achieve this was to delete the Pod and have a controller (like a Job or ReplicaSet) create a new one. This process is slow and expensive, involving the scheduler, node resource allocation and re-initialization of networking and storage.This inefficiency becomes even worse when handling large-scale AI/ML workloads (>= 1,000 Nodes with one Pod per Node). A common requirement for these synchronous workloads is that when a failure occurs (such as a Node crash), all Pods in the fleet must be recreated to reset the state before training can resume, even if all the other Pods were not directly affected by the failure. Deleting, creating and scheduling thousands of Pods simultaneously creates a massive bottleneck. The estimated overhead of this failure could cost $100,000 per month in wasted resources.Handling these failures for AI/ML training jobs requires a complex integration touching both the training framework and Kubernetes, which are often fragile and toilsome. This feature introduces a Kubernetes-native solution, improving system robustness and allowing application developers to concentrate on their core training logic.Another major benefit of restarting Pods in place is that keeping Pods on their assigned Nodes allows for further optimizations. For example, one can implement node-level caching tied to a specific Pod identity, something that is impossible when Pods are unnecessarily being recreated on different Nodes.Introducing the  actionTo address this, Kubernetes v1.35 adds a new action to the container restart rules: . When a container exits in a way that matches a rule with this action, the kubelet initiates a fast,  restart of the Pod.This in-place restart is highly efficient because it preserves the Pod's most important resources:The Pod's UID, IP address and network namespace.The Pod's sandbox and any attached devices.All volumes, including  and mounted volumes from PVCs.After terminating all running containers, the Pod's startup sequence is re-executed from the very beginning. This means all  are run again in order, followed by the sidecar and regular containers, ensuring a completely fresh start in a known-good environment. With the exception of ephemeral containers (which are terminated), all other containers—including those that previously succeeded or failed—will be restarted, regardless of their individual restart policies.1. Efficient restarts for ML/Batch jobsWith  actions you can address this by enabling a much faster, hybrid recovery strategy: recreate only the "bad" Pods (e.g., those on unhealthy Nodes) while triggering  for the remaining healthy Pods. Benchmarks show this reduces the recovery overhead from minutes to a few seconds.With in-place restarts, a watcher sidecar can monitor the main training process. If it encounters a specific, retriable error, the watcher can exit with a designated code to trigger a fast reset of the worker Pod, allowing it to restart from the last checkpoint without involving the Job controller. This capability is now natively supported by Kubernetes.2. Re-running init containers for a clean stateImagine a scenario where an init container is responsible for fetching credentials or setting up a shared volume. If the main application fails in a way that corrupts this shared state, you need the init container to rerun.By configuring the main application to exit with a specific code upon detecting such a corruption, you can trigger the  action, guaranteeing that the init container provides a clean setup before the application restarts.3. Handling high rate of similar tasks executionThere are cases when tasks are best represented as a Pod execution. And each task requires a clean execution. The task may be a game session backend or some queue item processing. If the rate of tasks is high, running the whole cycle of Pod creation, scheduling and initialization is simply too expensive, especially when tasks can be short. The ability to restart all containers from scratch enables a Kubernetes-native way to handle this scenario without custom solutions or frameworks.To try this feature, you must enable the RestartAllContainersOnContainerExits feature gate on your Kubernetes cluster components (API server and kubelet) running Kubernetes v1.35+. This alpha feature extends the  feature, which graduated to beta in v1.35 and is enabled by default.Once enabled, you can add  to any container (init, sidecar, or regular) and use the  action.The feature is designed to be easily usable on existing apps. However, if an application does not follow some best practices, it may cause issues for the application or for observability tooling. When enabling the feature, make sure that all containers are reentrant and that external tooling is prepared for init containers to re-run. Also, when restarting all containers, the kubelet does not run  hooks. This means containers must be designed to handle abrupt termination without relying on  hooks for graceful shutdown.To make this process observable, a new Pod condition, , is added to the Pod's status. When a restart is triggered, this condition becomes  and it reverts to  once all containers have terminated and the Pod is ready to start its lifecycle anew. This provides a clear signal to users and other cluster components about the Pod's state.All containers restarted by this action will have their restart count incremented in the container status.As an alpha feature,  is ready for you to experiment with and any use cases and feedback are welcome. This feature is driven by the SIG Node community. If you are interested in getting involved, sharing your thoughts, or contributing, please join us!You can reach SIG Node through:]]></content:encoded></item><item><title>Kubernetes 1.35: Enhanced Debugging with Versioned z-pages APIs</title><link>https://kubernetes.io/blog/2025/12/31/kubernetes-v1-35-structured-zpages/</link><author></author><category>dev</category><pubDate>Wed, 31 Dec 2025 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Debugging Kubernetes control plane components can be challenging, especially when you need to quickly understand the runtime state of a component or verify its configuration. With Kubernetes 1.35, we're enhancing the z-pages debugging endpoints with structured, machine-parseable responses that make it easier to build tooling and automate troubleshooting workflows.z-pages are special debugging endpoints exposed by Kubernetes control plane components. Introduced as an alpha feature in Kubernetes 1.32, these endpoints provide runtime diagnostics for components like , , ,  and . The name "z-pages" comes from the convention of using  paths for debugging endpoints.Currently, Kubernetes supports two primary z-page endpoints:Displays high-level component information including version information, start time, uptime, and available debug pathsShows all command-line arguments and their values used to start the component (with confidential values redacted for security)These endpoints are valuable for human operators who need to quickly inspect component state, but until now, they only returned plain text output that was difficult to parse programmatically.What's new in Kubernetes 1.35?Kubernetes 1.35 introduces structured, versioned responses for both  and  endpoints. This enhancement maintains backward compatibility with the existing plain text format while adding support for machine-readable JSON responses.The new structured responses are opt-in. Without specifying an  header, the endpoints continue to return the familiar plain text format:$ curl --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \
--key /etc/kubernetes/pki/apiserver-kubelet-client.key \
--cacert /etc/kubernetes/pki/ca.crt \
https://localhost:6443/statusz
kube-apiserver statusz
Warning: This endpoint is not meant to be machine parseable, has no formatting compatibility guarantees and is for debugging purposes only.
Started: Wed Oct 16 21:03:43 UTC 2024
Up: 0 hr 00 min 16 sec
Go version: go1.23.2
Binary version: 1.35.0-alpha.0.1595
Emulation version: 1.35
Paths: /healthz /livez /metrics /readyz /statusz /version
Structured JSON responsesTo receive a structured response, include the appropriate  header:Accept: application/json;v=v1alpha1;g=config.k8s.io;as=Statusz
This returns a versioned JSON response:Similarly,  supports structured responses with the header:Accept: application/json;v=v1alpha1;g=config.k8s.io;as=Flagz
Why structured responses matterThe addition of structured responses opens up several new possibilities:1. Automated health checks and monitoringInstead of parsing plain text, monitoring tools can now easily extract specific fields. For example, you can programmatically check if a component has been running with an unexpected emulated version or verify that critical flags are set correctly.Developers can build sophisticated debugging tools that compare configurations across multiple components or track configuration drift over time. The structured format makes it trivial to  configurations or validate that components are running with expected settings.3. API versioning and stabilityBy introducing versioned APIs (starting with ), we provide a clear path to stability. As the feature matures, we'll introduce  and eventually , giving you confidence that your tooling won't break with future Kubernetes releases.How to use structured z-pagesBoth endpoints require feature gates to be enabled:: Enable the  feature gate: Enable the  feature gateExample: Getting structured responsesHere's an example using  to retrieve structured JSON responses from the kube-apiserver:The examples above use client certificate authentication and verify the server's certificate using .
If you need to bypass certificate verification in a test environment, you can use  (or ),
but this should never be done in production as it makes you vulnerable to man-in-the-middle attacks.The structured z-page responses are an  feature in Kubernetes 1.35. This means:The API format may change in future releasesThese endpoints are intended for debugging, not production automationYou should avoid relying on them for critical monitoring workflows until they reach beta or stable statusSecurity and access controlz-pages expose internal component information and require proper access controls. Here are the key security considerations:: Access to z-page endpoints is restricted to members of the  group, which follows the same authorization model as other debugging endpoints like , , and . This ensures that only authorized users and service accounts can access debugging information. If your cluster uses RBAC, you can manage access by granting appropriate permissions to this group.: The authentication requirements for these endpoints depend on your cluster's configuration. Unless anonymous authentication is enabled for your cluster, you typically need to use authentication mechanisms (such as client certificates) to access these endpoints.: These endpoints reveal configuration details about your cluster components, including:Component versions and build informationAll command-line arguments and their values (with confidential values redacted)Available debug endpointsOnly grant access to trusted operators and debugging tools. Avoid exposing these endpoints to unauthorized users or automated systems that don't require this level of access.As the feature matures, we (Kubernetes SIG Instrumentation) expect to:Introduce  and eventually  versions of the APIGather community feedback on the response schemaPotentially add additional z-page endpoints based on user needsWe encourage you to experiment with structured z-pages in a test environment:Enable the  and  feature gates on your control plane componentsTry querying the endpoints with both plain text and structured formatsBuild a simple tool or script that uses the structured dataShare your feedback with the communityWe'd love to hear your feedback! The structured z-pages feature is designed to make Kubernetes easier to debug and monitor. Whether you're building internal tooling, contributing to open source projects, or just exploring the feature, your input helps shape the future of Kubernetes observability.If you have questions, suggestions, or run into issues, please reach out to SIG Instrumentation. You can find us on Slack or at our regular community meetings.]]></content:encoded></item></channel></rss>
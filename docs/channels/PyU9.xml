<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev</title><link>https://konrad.website/feeds/</link><description></description><item><title>Kubernetes v1.35 Sneak Peek</title><link>https://kubernetes.io/blog/2025/11/26/kubernetes-v1-35-sneak-peek/</link><author></author><category>dev</category><pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[As the release of Kubernetes v1.35 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the project's overall health. This blog post outlines planned changes for the v1.35 release that the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes cluster(s), and to keep you up to date with the latest developments. The information below is based on the current status of the v1.35 release and is subject to change before the final release date.Deprecations and removals for Kubernetes v1.35On Linux nodes, container runtimes typically rely on cgroups (short for "control groups").
Support for using cgroup v2 has been stable in Kubernetes since v1.25, providing an alternative to the original v1 cgroup support. While cgroup v1 provided the initial resource control mechanism, it suffered from well-known
inconsistencies and limitations. Adding support for cgroup v2 allowed use of a unified control group hierarchy, improved resource isolation, and served as the foundation for modern features, making legacy cgroup v1 support ready for removal.
The removal of cgroup v1 support will only impact cluster administrators running nodes on older Linux distributions that do not support cgroup v2; on those nodes, the  will fail to start. Administrators must migrate their nodes to systems with cgroup v2 enabled. More details on compatibility requirements will be available in a blog post soon after the v1.35 release.Deprecation of ipvs mode in kube-proxyMany releases ago, the Kubernetes project implemented an ipvs mode in . It was adopted as a way to provide high-performance service load balancing, with better performance than the existing  mode. However, maintaining feature parity between ipvs and other kube-proxy modes became difficult, due to technical complexity and diverging requirements. This created significant technical debt and made the ipvs backend impractical to support alongside newer networking capabilities.The Kubernetes project intends to deprecate kube-proxy  mode in the v1.35 release, to streamline the  codebase. For Linux nodes, the recommended  mode is already nftables.Kubernetes is deprecating containerd v1.y supportWhile Kubernetes v1.35 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. Kubernetes v1.35 is the last release to offer this support (aligned with containerd 1.7 EOL).This is a final warning that if you are using containerd 1.X, you must switch to 2.0 or later before upgrading Kubernetes to the next version. You are able to monitor the kubelet_cri_losing_support metric to determine if any nodes in your cluster are using a containerd version that will soon be unsupported.Featured enhancements of Kubernetes v1.35The following enhancements are some of those likely to be included in the v1.35 release. This is not a commitment, and the release content is subject to change.When scheduling Pods, Kubernetes uses node labels, taints, and tolerations to match workload requirements with node capabilities. However, managing feature compatibility becomes challenging during cluster upgrades due to version skew between the control plane and nodes. This can lead to Pods being scheduled on nodes that lack required features, resulting in runtime failures.The  framework will introduce a standard mechanism for nodes to declare their supported Kubernetes features. With the new alpha feature enabled, a Node reports the features it can support, publishing this information to the control plane through a new  field. Then, the , admission controllers and third-party components can use these declarations. For example, you can enforce scheduling and API validation constraints, ensuring that Pods run only on compatible nodes.This approach reduces manual node labeling, improves scheduling accuracy, and prevents incompatible pod placements proactively. It also integrates with the Cluster Autoscaler for informed scale-up decisions. Feature declarations are temporary and tied to Kubernetes feature gates, enabling safe rollout and cleanup.Targeting alpha in v1.35,  aims to solve version skew scheduling issues by making node capabilities explicit, enhancing reliability and cluster stability in heterogeneous version environments.To learn more about this before the official documentation is published, you can read KEP-5328.In-place update of Pod resourcesKubernetes is graduating in-place updates for Pod resources to General Availability (GA). This feature allows users to adjust  and  resources without restarting Pods or Containers. Previously, such modifications required recreating Pods, which could disrupt workloads, particularly for stateful or batch applications.
Previous Kubernetes releases already allowed you to change infrastructure resources settings (requests and limits) for existing Pods. This allows for smoother vertical scaling, improves efficiency, and can also simplify solution development.The Container Runtime Interface (CRI) has also been improved, extending the  API for Windows and future runtimes while allowing  to report real-time resource configurations. Together, these changes make scaling in Kubernetes faster, more flexible, and disruption-free.
The feature was introduced as alpha in v1.27, graduated to beta in v1.33, and is targeting graduation to stable in v1.35.When running microservices, Pods often require a strong cryptographic identity to authenticate with each other using mutual TLS (mTLS). While Kubernetes provides Service Account tokens, these are designed for authenticating to the API server, not for general-purpose workload identity.Before this enhancement, operators had to rely on complex, external projects like SPIFFE/SPIRE or cert-manager to provision and rotate certificates for their workloads. But what if you could issue a unique, short-lived certificate to your Pods natively and automatically? KEP-4317 is designed to enable such native workload identity. It opens up various possibilities for securing pod-to-pod communication by allowing the  to request and mount certificates for a Pod via a projected volume.This provides a built-in mechanism for workload identity, complete with automated certificate rotation, significantly simplifying the setup of service meshes and other zero-trust network policies. This feature was introduced as alpha in v1.34 and is targeting beta in v1.35.Numeric values for taintsKubernetes is enhancing taints and tolerations by adding numeric comparison operators, such as  (Greater Than) and  (Less Than).Previously, tolerations supported only exact () or existence () matches, which were not suitable for numeric properties such as reliability SLAs.With this change, a Pod can use a toleration to "opt-in" to nodes that meet a specific numeric threshold. For example, a Pod can require a Node with an SLA taint value greater than 950 (, ).This approach is more powerful than Node Affinity because it supports the NoExecute effect, allowing Pods to be automatically evicted if a node's numeric value drops below the tolerated threshold.When running Pods, you can use  to drop privileges, but containers inside the pod often still run as root (UID 0). This simplicity poses a significant challenge, as that container UID 0 maps directly to the host's root user.Before this enhancement, a container breakout vulnerability could grant an attacker full root access to the node. But what if you could dynamically remap the container's root user to a safe, unprivileged user on the host? KEP-127 specifically allows such native support for Linux User Namespaces. It opens up various possibilities for pod security by isolating container and host user/group IDs. This allows a process to have root privileges (UID 0) within its namespace, while running as a non-privileged, high-numbered UID on the host.Released as alpha in v1.25 and beta in v1.30, this feature continues to progress through beta maturity, paving the way for truly "rootless" containers that drastically reduce the attack surface for a whole class of security vulnerabilities.Support for mounting OCI images as volumesWhen provisioning a Pod, you often need to bundle data, binaries, or configuration files for your containers.
Before this enhancement, people often included that kind of data directly into the main container image, or required a custom init container to download and unpack files into an . You can still take either of those approaches, of course.But what if you could populate a volume directly from a data-only artifact in an OCI registry, just like pulling a container image? Kubernetes v1.31 added support for the  volume type, allowing Pods to pull and unpack OCI container image artifacts into a volume declaratively.This allows for seamless distribution of data, binaries, or ML models using standard registry tooling, completely decoupling data from the container image and eliminating the need for complex init containers or startup scripts.
This volume type has been in beta since v1.33 and will likely be enabled by default in v1.35.New features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in Kubernetes v1.35 as part of the CHANGELOG for that release.The Kubernetes v1.35 release is planned for . Stay tuned for updates!You can also see the announcements of changes in the release notes for:The simplest way to get involved with Kubernetes is by joining one of the many Special Interest Groups (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly community meeting, and through the channels below. Thank you for your continued feedback and support.]]></content:encoded></item><item><title>Interview with Jan David Nose</title><link>https://blog.rust-lang.org/2025/11/25/interview-with-jan-david-nose/</link><author>Pete LeVasseur</author><category>dev</category><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[On the Content Team, we had our first whirlwind outing at RustConf 2025 in Seattle, Washington, USA. There we had a chance to speak with folks about interesting things happening in the Project and the wider community.In this interview, Xander Cesari sits down with Jan David Nose, then one of the full-time engineers on the Infrastructure Team, which maintains and develops the infrastructure upon which Rust is developed and deployed -- including CI/CD tooling and crates.io.We released this video on an accelerated timeline, some weeks ago, in light of the recent software supply chain attacks, but the interview was conducted prior to the news of compromised packages in other languages and ecosystems.: Hey, this is Xander Cesari with the Rust Project
Content Team, recording on the last hour of the last day of RustConf
2025 here in Seattle. So it's been a long and amazing two days. And I'm
sitting down here with a team member from the Rust Project Infra Team,
the unsung heroes of the Rust language. Want to introduce yourself and
kind of how you got involved?: Yeah, sure. I'm JD. Jan David is the full name, but
especially in international contexts, I just go with JD. I've been
working for the Rust Foundation for the past three years as a full-time
employee and I essentially hit the jackpot to work full-time on open
source and I've been in the Infra Team of the Rust Project for the
whole time. For the past two years I've led the team together with
Jake. So the Infra Team is kind of a thing that lets Rust happen and
there's a lot of different pieces.: Could you give me an overview of the responsibility
of the Infra Team?: Sure. I think on a high level, we think about this
in terms of, we serve two different groups of people. On one side, we
have users of the language, and on the other side, we really try to
provide good tooling for the maintainers of the language.: Starting with the maintainer side, this is really
everything about how Rust is built. From the moment someone makes a
contribution or opens a PR, we maintain the continuous integration that
makes sure that the PR actually works. There's a lot of bots and
tooling helping out behind the scenes to kind of maintain a good status
quo, a sane state. Lots of small things like triage tools on GitHub to
set labels and ping people and these kinds of things. And that's kind
of managed by the Infra Team at large.: And then on the user side, we have a lot of, or the
two most important things are making sure users can actually download
Rust. We don't develop crates.io, but we support the infrastructure to
actually ship crates to users. All the downloads go through content
delivery networks that we provide. The same for Rust releases. So if I
don't do my job well, which has happened, there might be a global
outage of crates.io and no one can download stuff. But those are kind
of the two different buckets of services that we run and operate.: Gotcha. So on the maintainer side, the Rust
organization on GitHub is a large organization with a lot of activity,
a lot of code. There's obviously a lot of large code bases being
developed on GitHub, but there are not that many languages the size of
Rust being developed on GitHub. Are there unique challenges to
developing a language and the tooling that's required versus developing
other software projects?: I can think of a few things that have less to do
with the language specifically, but with some of the architecture
decisions that were made very early on in the life cycle of Rust. So
one of the things that actually caused a lot of headache for mostly
GitHub, and then when they complained to us, for us as well, is that
for a long, long time, the index for crates.io was a Git repo on
GitHub. As Rust started to grow, the activity on the repo became so big
that it actually caused some issues, I would say, in a friendly way on
GitHub, just in terms of how much resources that single repository was
consuming. That then kind of started this work on a web-based,
HTTP-based index to shift that away. That's certainly one area where
we've seen how Rust has struggled a little bit with the platform, but
also the platform provider struggled with us.: I think for Rust itself, especially when we look at
CI, we really want to make sure that Rust works well on all of the
targets and all the platforms we support. That means we have an
extremely wide CI pipeline where, for every Tier 1 target, we want to
run all the tests, we want to build the release artifacts, we want to
upload all of that to S3. We want to do as much as we reasonably can
for Tier 2 targets and, to a lesser extent, maybe even test some stuff
on Tier 3. That has turned into a gigantic build pipeline. Marco gave a
talk today on what we've done with CI over the last year. One of the
numbers that came out of doing the research for this talk is that we
accumulate over three million build minutes per month, which is about
six years of CPU time every month.: Especially when it comes to open source projects, I
think we're one of the biggest consumers of GitHub Actions in that
sense. Not the biggest in total; there are definitely bigger commercial
projects. But that's a unique challenge for us to manage because we
want to provide as good a service as we can to the community and make
sure that what we ship is high quality. That comes at a huge cost in
terms of scaling. As Rust gets more popular and we want to target more
and more platforms, this is like a problem that just continues to
grow.: We'll probably never remove a lot of targets, so
there's an interesting challenge to think about. If it's already big
now, how does this look in 5 years, 10 years, 15 years, and how can we
make sure we can maintain the level of quality we want to ship? When
you build and run for a target in the CI pipeline, some of those Tier 1
targets you can just ask a cloud service provider to give you a VM
running on that piece of hardware, but some of them are probably not
things that you can just run in the cloud.: Is there some HIL (Hardware-In-the-Loop) lab
somewhere?: So you're touching on a conversation that's
happening pretty much as we speak. So far, as part of our target tier
policy, there is a clause that says it needs to be able to run in CI.
That has meant being very selective about only promoting things to Tier
1 that we can actually run and test. For all of this, we had a
prerequisite that it runs on GitHub Actions. So far we've used very
little hardware that is not natively supported or provided by GitHub.: But this is exactly the point with Rust increasing
in popularity. We just got requests to support IBM platforms and
RISC-V, and those are not natively supported on GitHub. That has kicked
off an internal conversation about how we even support this. How can we
as a project enable companies that can provide us hardware to test on?
What are the implications of that?: On one side, there are interesting constraints and
considerations. For example, you don't want your PRs to randomly fail
because someone else's hardware is not available. We're already so
resource-constrained on how many PRs we can merge each day that adding
noise to that process would really slow down contributions to Rust. On
the other side, there are security implications. Especially if we talk
about promoting something to Tier 1 and we want to build release
artifacts on that hardware, we need to make sure that those are
actually secure and no one sneaks a back door into the Rust compiler
target for RISC-V.: So there are interesting challenges for us,
especially in the world we live in where supply chain security is a
massive concern. We need to figure out how we can both support the
growth of Rust and the growth of the language, the community, and the
ecosystem at large while also making sure that the things we ship are
reliable, secure, and performant. That is becoming an increasingly
relevant and interesting piece to work on. So far we've gotten away
with the platforms that GitHub supports, but it's really cool to see
that this is starting to change and people approach us and are willing
to provide hardware, provide sponsorship, and help us test on their
platforms. But essentially we don't have a good answer for this yet.
We're still trying to figure out what this means, what we need to take
into consideration, and what our requirements are to use external
hardware.: Yeah, everyone is so excited about Rust will run
everywhere, but there's a maintenance cost there that is almost
exponential in scope.: It's really interesting as well because there's a
tension there. I think with IBM, for example, approaching us, it's an
interesting example. Who has IBM platforms at home? The number of users
for that platform is really small globally, but IBM also invests
heavily in Rust, tries to make this happen, and is willing to provide
the hardware.: For us, that leads to a set of questions. Is there
a line? Is there a certain requirement? Is there a certain amount of
usage that a platform would need for us to promote it? Or do we say we
want to promote as much as we can to Tier 1? This is a conversation we
haven't really had to have yet. It's only now starting to creep in as
Rust is adopted more widely and companies pour serious money and
resources into it. That's exciting to see.: In this specific case, companies approach the Infra
Team to figure out how we can add their platforms to CI as a first step
towards Tier 1 support. But it's also a broader discussion we need to
have with larger parts of the Rust Project. For Tier 1 promotions, for
example, the Compiler Team needs to sign off, Infra needs to sign off.
Many more people need to be involved in this discussion of how we can
support the growing needs of the ecosystem at large.: I get the feeling that's going to be a theme
throughout this interview.: So one other tool that's part of this pipeline that
I totally didn't know about for a long time, and I think a talk at a
different conference clued me into it, is Crater. It's a tool that
attempts to run all of the Rust code it can find on the internet. Can
you talk about what that tool does and how it integrates into the
release process?: Whenever someone creates a pull request on GitHub
to add a new feature or bug fix to the Rust compiler, they can start
what's called a Crater run, or an experiment. Crater is effectively a
large fleet of machines that tries to pull in as many crates as it can.
Ideally, we would love to test all crates, but for a variety of reasons
that's not possible. Some crates simply don't build reliably, so we
maintain lists to exclude those. From the top of my head, I think we
currently test against roughly 60% of crates.: The experiment takes the code from your pull
request, builds the Rust compiler with it, and then uses that compiler
to build all of these crates. It reports back whether there are any
regressions related to the change you proposed. That is a very
important tool for us to maintain backwards compatibility with new
versions and new features in Rust. It lets us ask: does the ecosystem
still compile if we add this feature to the compiler, and where do we
run into issues? Then, and this is more on the Compiler Team side,
there's a decision about how to proceed. Is the breakage acceptable? Do
we need to adjust the feature? Having Crater is what makes that
conversation possible because it gives us real data on the impact on
the wider ecosystem.: I think that's so interesting because as more and
more companies adopt Rust, they're asking whether the language is going
to be stable and backward compatible. You hear about other programming
languages that had a big version change that caused a lot of drama and
code changes. The fact that if you have code on crates.io, the Compiler
Team is probably already testing against it for backwards compatibility
is pretty reassuring.: Yeah, the chances are high, I would say. Especially
looking at the whole Python 2 to Python 3 migration, I think as an
industry we've learned a lot from those big version jumps. I can't
really speak for the Compiler Team because I'm not a member and I
wasn't involved in the decision-making, but I feel this is one of the
reasons why backwards compatibility is such a big deal in Rust's
design. We want to make it as painless as possible to stay current,
stay up to date, and make sure we don't accidentally break the language
or create painful migration points where the entire ecosystem has to
move at once.: Do you know if there are other organizations pulling
in something like Crater and running it on their own internal crate
repositories, maybe some of the big tech companies or other compiler
developers or even other languages? Or is this really bespoke for the
Rust compiler team?: I don't know of anyone who runs Crater itself as a
tool. Crater is built on a sandboxing framework that we also use in
other places. For example, docs.rs uses some of the same underlying
infrastructure to build all of the documentation. We try to share as
much as we can of the functionality that exists in Crater, but I'm not
aware of anyone using Crater in the same way we do.: Gotcha. The other big part of your job is that the
Infra Team works on supporting maintainers, but it also supports users
and consumers of Rust who are pulling from crates.io. It sounds like
crates.io is not directly within your team, but you support a lot of
the backend there.: Yeah, exactly. crates.io has its own team, and that
team maintains the web application and the APIs. The crates themselves,
all the individual files that people download, are hosted within our
infrastructure. The Infra Team maintains the content delivery network
that sits in front of that. Every download of a crate goes through
infrastructure that we maintain. We collaborate very closely with the
crates.io team on this shared interface. They own the app and the API,
and we make sure that the files get delivered to the end user.: So it sounds like there's a lot of verification of
the files that get uploaded and checks every time someone pushes a new
version to crates.io. That part all happens within crates.io as an
application.: Cargo uses the crates.io API to upload the crate
file. crates.io has a lot of internal logic to verify that it is valid
and that everything looks correct. For us, as the Infra Team, we treat
that as a black box. crates.io does its work, and if it is happy with
the upload, it stores the file in S3. From that point onward,
infrastructure makes sure that the file is accessible and can be
downloaded so people can start using your crate.: In this theme of Rust being a bit of a victim of its
own success, I assume all of the traffic graphs and download graphs are
very much up and to the right.: On the Foundation side, one of our colleagues likes
to check how long it takes for one billion downloads to happen on
crates.io, and that number has been falling quickly. I don't remember
what it was three years ago, but it has come down by orders of
magnitude. In our download traffic we definitely see exponential
growth. Our traffic tends to double year over year, and that trend has
been pretty stable. It really seems like Rust is getting a lot of
adoption in the ecosystem and people are using it for more and more
things.: How has the Infra Team scaled with that? Are you
staying ahead of it, or are there a lot of late nights?: There have definitely been late nights. In the
three years I've been working in the Infra Team, every year has had a
different theme that was essentially a fire to put out.: It changes because we fix one thing and then the
next thing breaks. So far, luckily, those fires have been mostly
sequential, not parallel. When I joined, bandwidth was the big topic.
Over the last year, it has been more about CI. About three years ago,
we hit this inflection point where traffic was doubling and the
sponsorship capacity we had at the time was reaching its limits.: Two or three years ago, Fastly welcomed us into
their Fast Forward program and has been sponsoring all of our bandwidth
since then. That has mostly helped me sleep at night. It has been a
very good relationship. They have been an amazing partner and have
helped us at every step to remove the fear that we might hit limits.
They are very active in the open source community at large; most
famously they also sponsor PyPI and the Python ecosystem, compared to
which we're a tiny fish in a very big pond. That gives us a lot of
confidence that we can sustain this growth and keep providing crates
and releases at the level of quality people expect.: In some ways, Rust did such a good job of making all
of that infrastructure feel invisible. You just type Cargo commands
into your terminal and it feels magical.: I'm really happy about that. It's an interesting
aspect of running an infrastructure team in open source. If you look at
the ten-year history since the first stable release, or even the
fifteen years since Rust really started, infrastructure was
volunteer-run for most of that time. I've been here for three years,
and I was the first full-time infrastructure engineer. So for ten to
twelve years, volunteers ran the infrastructure.: For them, it was crucial that things just worked,
because you can't page volunteers in the middle of the night because a
server caught fire or downloads stopped working. From the beginning,
our infrastructure has been designed to be as simple and as reliable as
possible. The same is true for our CDNs. I always feel a bit bad
because Fastly is an amazing sponsor. Every time we meet them at
conferences or they announce new features, they ask whether we want to
use them or talk about how we use Fastly in production. And every time
I have to say: we have the simplest configuration possible. We set some
HTTP headers. That's pretty much it.: It's a very cool platform, but we use the smallest
set of features because we need to maintain all of this with a
very small team that is mostly volunteer-based. Our priority has always
been to keep things simple and reliable and not chase every fancy new
technology, so that the project stays sustainable.: Volunteer-based organizations seem to have to care
about work-life balance, which is probably terrific, and there are
lessons to be learned there.: Yeah, it's definitely a very interesting
environment to work in. It has different rules than corporations or
commercial teams. We have to think about how much work we can do in a
given timeframe in a very different way, because it's unpredictable
when volunteers have time, when they're around, and what is happening
in their lives.: Over the last few years, we've tried to reduce the
number of fires that can break out. And when they do happen, we try to
shield volunteers from them and take that work on as full-time
employees. That started with me three years ago. Last year Marco
joined, which increased the capacity we have, because there is so much
to do on the Infra side that even with me working full-time, we simply
did not have enough people.: So you're two full-time and everything else is
volunteer.: Exactly. The team is around eight people. Marco and
I work full-time and are paid by the Rust Foundation to focus
exclusively on infrastructure. Then we have a handful of volunteers who
work on different things.: Because our field of responsibility is so wide, the
Infra Team works more in silos than other teams might. We have people
who care deeply about very specific parts of the infrastructure.
Otherwise there is simply too much to know for any one person. It has
been a really nice mix, and it's amazing to work with the people on the
team.: As someone who is privileged enough to work
full-time on this and has the time and resources, we try to bear the
bigger burden and create a space that is fun for volunteers to join. We
want them to work on exciting things where there is less risk of
something catching fire, where it's easier to come in, do a piece of
work, and then step away. If your personal life takes over for two
weeks, that's okay, because someone is there to make sure the servers
and the lights stay on.: A lot of that work lives more on the maintainer
side: the GitHub apps, the bots that help with triage. It's less risky
if something goes wrong there. On the user side, if you push the wrong
DNS setting, as someone might have done, you can end up in a situation
where for 30 minutes no one can download crates. And in this case,
"no one" literally means no user worldwide. That's not
an experience I want volunteers to have. It's extremely stressful and
was ultimately one of the reasons I joined in the first place—there was
a real feeling of burnout from carrying that responsibility.: It's easier to carry that as a full-timer. We have
more time and more ways to manage the stress. I'm honestly extremely
amazed by what the Infra Team was able to do as volunteers. It's
unbelievable what they built and how far they pushed Rust to get to
where we are now.: I think anyone who's managing web traffic in 2025 is
talking about traffic skyrocketing due to bots and scrapers for AI or
other purposes. Has that hit the Rust network as well?: Yeah, we've definitely seen that. It's handled by a
slightly different team, but on the docs.rs side in particular we've
seen crawlers hit us hard from time to time, and that has caused
noticeable service degradation. We're painfully aware of the increase
in traffic that comes in short but very intense bursts when crawlers go
wild.: That introduces a new challenge for our
infrastructure. We need to figure out how to react to that traffic and
protect our services from becoming unavailable to real users who want
to use docs.rs to look up something for their work. On the CDN side,
our providers can usually handle the traffic. It is more often the
application side where things hurt.: On the CDN side we also see people crawling
crates.io, presumably to vacuum up the entire crates ecosystem into an
LLM. Fortunately, over the last two years we've done a lot of work to
make sure crates.io as an application is less affected by these traffic
spikes. Downloads now bypass crates.io entirely and go straight to the
CDN, so the API is not hit by these bursts. In the past, this would
have looked like a DDoS attack, with so many requests from so many
sources that we couldn't handle it.: We've done a lot of backend work to keep our stack
reliable, but it's definitely something that has changed the game over
the last year. We can clearly see that crawlers are much more active
than before.: That makes sense. I'm sure Fastly is working on this
as well. Their business has to adapt to be robust to this new internet.: Exactly. For example, one of the conversations
we're having right now is about docs.rs. It's still hosted on AWS
behind CloudFront, but we're talking about putting it behind Fastly
because through Fastly we get features like bot protection that can
help keep crawlers out.: This is a good example of how our conversations
have changed in the last six months. At the start of the year I did not
think this would be a topic we would be discussing. We were focused on
other things. For docs.rs we have long-term plans to rebuild the
infrastructure that powers it, and I expected us to spend our energy
there. But with the changes in the industry and everyone trying to
accumulate as much data as possible, our priorities have shifted. The
problems we face and the order in which we tackle them have changed.: And I assume as one of the few paid members of a
mostly volunteer team, you often end up working on the fires, not the
interesting next feature that might be more fun.: That is true, although it sounds a bit negative to
say I only get to work on fires. Sometimes it feels like that because,
as with any technology stack, there is a lot of maintenance overhead.
We definitely pay that price on the infrastructure side.: Marco, for example, spent time this year going
through all the servers we run, cataloging them, and making sure
they're patched and on the latest operating system version. We updated
our Ubuntu machines to the latest LTS. It feels a bit like busy
work—you just have to do it because it's important and necessary, but
it's not the most exciting project.: On the other hand, when it comes to things like CDN
configuration and figuring out how bot protection features work and
whether they are relevant to us, that is also genuinely interesting
work. It lets us play with new tools vendors provide, and we're working
on challenges that the wider industry is facing. How do you deal with
this new kind of traffic? What are the implications of banning bots?
How high is the risk of blocking real users? Sometimes someone just
misconfigures a curl script, and from the outside it looks like they're
crawling our site.: So it's an interesting field to work in, figuring
out how we can use new features and address new challenges. That keeps
it exciting even for us full-timers who do more of the
"boring" work. We get to adapt alongside how the world
around us is changing. If there's one constant, it's change.: Another ripped-from-the-headlines change around this
topic is software supply chain security, and specifically xz-utils and
the conversation around open source security. How much has that changed
the landscape you work in?: The xz-utils compromise was scary. I don't want to
call it a wake-up call, because we've been aware that supply chain
security is a big issue and this was not the first compromise. But the
way it happened felt very unsettling. You saw an actor spend a year and
a half building social trust in an open source project and then using
that to introduce a backdoor.: Thinking about that in the context of Rust: every
team in the project talks about how we need more maintainers, how
there's too much workload on the people who are currently contributing,
and how Rust's growth puts strain on the organization as a whole. We
want to be an open and welcoming project, and right now we also need to
bring new people in. If someone shows up and says, "I'm
willing to help, please onboard me," and they stick around for
a year and then do something malicious, we would be susceptible to
that. I don't think this is unique to Rust. This is an inherent problem
in open source.: Yeah, it's antithetical to the culture.: Exactly. So we're trying to think through how we,
as a project and as an ecosystem, deal with persistent threat actors
who have the time and resources to play a long game. Paying someone to
work full-time on open source for a year is a very different threat
model than what we used to worry about.: I used to joke that the biggest threat to crates.io
was me accidentally pulling the plug on a CDN. I think that has
changed. Today the bigger threat is someone managing to insert
malicious code into our releases, our supply chain, or crates.io
itself. They could find ways to interfere with our systems in ways
we're simply not prepared for, where, as a largely volunteer
organization, we might be too slow to react to a new kind of attack.: Looking back over the last three years, this shift
became very noticeable, especially after the first year. Traffic was
doubling, Rust usage was going up a lot, and there were news stories
about Rust being used in the Windows kernel, in Android, and in parts
of iOS. Suddenly Rust is everywhere. If you want to attack
"everywhere," going after Rust becomes attractive.
That definitely puts a target on our back and has changed the game.: I'm very glad the Rust Foundation has a dedicated
security engineer who has done a lot of threat modeling and worked with
us on infrastructure security. There's also a lot of work happening
specifically around the crates ecosystem and preventing supply chain
attacks through crates. Luckily, it's not something the Infra side has
to solve alone. But it is getting a lot more attention, and I think it
will be one of the big challenges for the future: how a mostly
volunteer-run project keeps up with this looming threat.: And it is the industry at large. This is not a
unique problem to the Rust package manager. All package registries,
from Python to JavaScript to Nix, deal with this. Is there an
industry-wide conversation about how to help each other out and share
learnings?: Yeah, there's definitely a lot happening. I have to
smile a bit because, with a lot of empathy but also a bit of relief, we
sometimes share news when another package ecosystem gets compromised.
It is a reminder that it's not just us, sometimes it's npm this time.: We really try to stay aware of what's happening in
the industry and in other ecosystems: what new threats or attack
vectors are emerging, what others are struggling with. Sometimes that
is security; sometimes it's usability. A year and a half ago, for
example, npm had the "everything" package where
someone declared every package on npm as a dependency, which blew up
the index. We look at incidents like that and ask whether crates.io
would struggle with something similar and whether we need to make
changes.: On the security side we also follow closely what
others are doing. In the packaging community, the different package
managers are starting to come together more often to figure out which
problems everyone shares. There is a bit of a joke that we're all just
shipping files over the internet. Whether it's an npm package or a
crate, ultimately it's a bunch of text files in a zip. So from an
infrastructure perspective the problems are very similar.: These communities are now talking more about what
problems PyPI has, what problems crates.io has, what is happening in
the npm space. One thing every ecosystem has seen—even the very
established ones—is a big increase in bandwidth needs, largely
connected to the emergence of AI. PyPI, for example, publishes download
charts, and it's striking. Python had steady growth—slightly
exponential, but manageable—for many years. Then a year or two ago you
see a massive hockey stick. People discovered that PyPI was a great
distribution system for their models. There were no file size limits at
the time, so you could publish precompiled GPU models there.: That pattern shows up everywhere. It has kicked off
a new era for packaging ecosystems to come together and ask: in a time
where open source is underfunded and traffic needs keep growing, how
can we act together to find solutions to these shared problems?
crates.io is part of those conversations. It's interesting to see how
we, as an industry, share very similar problems across
ecosystems—Python, npm, Rust, and others.: With a smaller, more hobbyist-focused community, you
can have relaxed rules about what goes into your package manager.
Everyone knows the spirit of what you're trying to do and you can get
away without a lot of hard rules and consequences. Is the Rust world
going to have to think about much harder rules around package sizes,
allowed files, and how you're allowed to distribute things?: Funnily enough, we're coming at this from the
opposite direction. Compared to other ecosystems, we've always had
fairly strict limits. A crate can be at most around ten megabytes in
size. There are limits on what kinds of files you can put in there.
Ironically, those limits have helped us keep traffic manageable in this
period.: At the same time, there is a valid argument that
these limits may not serve all Rust use cases. There are situations
where you might want to include something precompiled in your crate
because it is hard to compile locally, takes a very long time, or
depends on obscure headers no one has. I don't think we've reached the
final state of what the crates.io package format should look like.: That has interesting security implications. When we
talk about precompiled binaries or payloads, we all have that little
voice in our head every time we see a curl | sh command: can I trust
this? The same is true if you download a crate that contains a
precompiled blob you cannot easily inspect.: The Rust Foundation is doing a lot of work and
research here. My colleague Adam, who works on the crates.io team, is
working behind the scenes to answer some of these questions. For
example: what kind of security testing can we do before we publish
crates to make sure they are secure and don't contain malicious
payloads? How do we surface this information? How do we tell a
publisher that they included files that are not allowed? And from the
user's perspective, when you visit crates.io, how can you judge how
well maintained and how secure a crate is?: Those conversations are happening quite broadly in
the ecosystem. On the Infra side we're far down the chain. Ultimately
we integrate with whatever security scanning infrastructure crates.io
builds. We don't have to do the security research ourselves, but we do
have to support it.: There's still a lot that needs to happen. As
awesome as Rust already is, and as much as I love using it, it's
important to remember that we're still a very young ecosystem. Python
is now very mature and stable, but it's more than 25 years old. Rust is
about ten years old as a stable language. We still have a lot to learn
and figure out.: Is the Rust ecosystem running into problems earlier
than other languages because we're succeeding at being foundational
software and Rust is used in places that are even more
security-critical than other languages, so you have to hit these hard
problems earlier than the Python world did?: I think that's true. Other ecosystems probably had
more time to mature and answer these questions. We're operating on a
more condensed timeline. There is also simply more happening now. Open
source has been very successful; it's everywhere. That means there are
more places where security is critical.: So this comes with the success of open source, with
what is happening in the ecosystem at large, and with the industry
we're in. It does mean we have less time to figure some things out. On
the flip side, we also have less baggage. We have less technical debt
and fifteen fewer years of accumulated history. That lets us be on the
forefront in some areas, like how a package ecosystem can stay secure
and what infrastructure a 21st century open source project needs.: Here I really want to call out the Rust Foundation.
They actively support this work: hiring people like Marco and me to
work full-time on infrastructure, having Walter and Adam focus heavily
on security, and as an organization taking supply chain considerations
very seriously. The Foundation also works with other ecosystems so we
can learn and grow together and build a better industry.: Behind the scenes, colleagues constantly work to
open doors for us as a relatively young language, so we can be part of
those conversations and sit at the table with other ecosystems. That
lets us learn from what others have already gone through and also help
shape where things are going. Sustainability is a big part of that: how
do we fund the project long term? How do we make sure we have the human
resources and financial resources to run the infrastructure and support
maintainers? I definitely underestimated how much of my job would be
relationship management and budget planning, making sure credits last
until new ones arrive.: Most open core business models give away the thing
that doesn't cost much—the software—and charge for the thing that
scales with use—the service. In Rust's case, it's all free, which is
excellent for adoption, but it must require a very creative perspective
on the business side.: Yeah, and that's where different forces pull in
opposite directions. As an open source project, we want everyone to be
able to use Rust for free. We want great user experience. When we talk
about downloads, there are ways for us to make them much cheaper, but
that might mean hosting everything in a single geographic location.
Then everyone, including people in Australia, would have to download
from, say, Europe, and their experience would get much worse.: Instead, we want to use services that are more
expensive but provide a better experience for Rust users. There's a
real tension there. On one side we want to do the best we can; on the
other side we need to be realistic that this costs money.: I had been thinking of infrastructure as a binary:
it either works or it doesn't. But you're right, it's a slider. You can
pick how much money you want to spend and what quality of service you
get. Are there new technologies coming, either for the Rust Infra Team
or the packaging world in general, to help with these security
problems? New sandboxing technologies or higher-level support?: A lot of people are working on this problem from
different angles. Internally we've talked a lot about it, especially in
the context of Crater. Crater pulls in all of those crates to build
them and get feedback from the Rust compiler. That means if someone
publishes malicious code, we will download it and build it.: In Rust this is a particular challenge because
build scripts can essentially do anything on your machine. For us that
means we need strong sandboxing. We've built our own sandboxing
framework so every crate build runs in an isolated container, which
prevents malicious code from escaping and messing with the host systems.: We feel that pain in Crater, but if we can solve it
in a way that isn't exclusive to Crater—if it also protects user
machines from the same vulnerabilities—that would be ideal. People like
Walter on the Foundation side are actively working on that. I'm sure
there are conversations in the Cargo and crates teams as well, because
every team that deals with packages sees a different angle of the
problem. We all have to come together to solve it, and there is a lot
of interesting work happening in that area.: I hope help is coming.: I'm optimistic.: We have this exponential curve with traffic and
everything else. It seems like at some point it has to taper off.: We'll see. Rust is a young language. I don't know
when that growth will slow down. I think there's a good argument that
it will continue for quite a while as adoption grows.: Being at a conference like RustConf, it's exciting
to see how the mix of companies has changed over time. We had a talk
from Rivian on how they use Rust in their cars. We've heard from other
car manufacturers exploring it. Rust is getting into more and more
applications that a few years ago would have been hard to imagine or
where the language simply wasn't mature enough yet.: As that continues, I think we'll see new waves of
growth that sustain the exponential curve we currently have, because
we're moving into domains that are new for us. It's amazing to see who
is talking about Rust and how they're using it, sometimes in areas like
space that you wouldn't expect.: I'm very optimistic about Rust's future. With this
increase in adoption, we'll see a lot of interesting lessons about how
to use Rust and a lot of creative ideas from people building with it.
With more corporate adoption, I also expect a new wave of investment
into the ecosystem: companies paying people to work full-time on
different parts of Rust, both in the ecosystem and in the core project.
I'm very curious what the next ten years will look like, because I
genuinely don't know.: The state of Rust right now does feel a bit like the
dog that caught the car and now doesn't know what to do with it.: Yeah, I think that's a good analogy. Suddenly we're
in a situation where we realize we haven't fully thought through every
consequence of success. It's fascinating to see how the challenges
change every year. We keep running into new growing pains where
something that wasn't an issue a year ago suddenly becomes one because
growth keeps going up.: We're constantly rebuilding parts of our
infrastructure to keep up with that growth, and I don't see that
stopping soon. As a user, that makes me very excited. With the language
and the ecosystem growing at this pace, there are going to be very
interesting things coming that I can't predict today.: For the project, it also means there are real
challenges: financing the infrastructure we need, finding maintainers
and contributors, and creating a healthy environment where people can
work without burning out. There is a lot of work to be done, but it's
an exciting place to be.: Well, thank you for all your work keeping those
magic Cargo commands I can type into my terminal just working in the
background. If there's any call to action from this interview, it's
that if you're a company using Rust, maybe think about donating to keep
the Infra Team working.: We always love new Rust Foundation members.
Especially if you're a company, that's one of the best ways to support
the work we do. Membership gives us a budget we can use either to fund
people who work full-time on the project or to fill gaps in our
infrastructure sponsorship where we don't get services for free and
have to pay real money.: And if you're not a company, we're always looking
for people to help out. The Infra Team has a lot of Rust-based bots and
other areas where people can contribute relatively easily.: Small scoped bots that you can wrap your head around
and help out with.: Exactly. It is a bit harder on the Infra side
because we can't give people access to our cloud infrastructure. There
are areas where it's simply not possible to contribute as a volunteer
because you can't have access to the production systems. But there is
still plenty of other work that can be done.: Like every other team in the project, we're a bit
short-staffed. So when you're at conferences, come talk to me or Marco.
We have work to do.: Well, thank you for doing the work that keeps Rust
running.: I'm happy to.: Awesome. Thank you so much.]]></content:encoded></item><item><title>Kubernetes Configuration Good Practices</title><link>https://kubernetes.io/blog/2025/11/25/configuration-good-practices/</link><author></author><category>dev</category><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Configuration is one of those things in Kubernetes that seems small until it's not. Configuration is at the heart of every Kubernetes workload.
A missing quote, a wrong API version or a misplaced YAML indent can ruin your entire deploy.This blog brings together tried-and-tested configuration best practices. The small habits that make your Kubernetes setup clean, consistent and easier to manage.
Whether you are just starting out or already deploying apps daily, these are the little things that keep your cluster stable and your future self sane.This blog is inspired by the original Configuration Best Practices page, which has evolved through contributions from many members of the Kubernetes community.General configuration practicesUse the latest stable API versionKubernetes evolves fast. Older APIs eventually get deprecated and stop working. So, whenever you are defining resources, make sure you are using the latest stable API version.
You can always check withThis simple step saves you from future compatibility issues.Store configuration in version controlNever apply manifest files directly from your desktop. Always keep them in a version control system like Git, it's your safety net.
If something breaks, you can instantly roll back to a previous commit, compare changes or recreate your cluster setup without panic.Write configs in YAML not JSONWrite your configuration files using YAML rather than JSON. Both work technically, but YAML is just easier for humans. It's cleaner to read and less noisy and widely used in the community.YAML has some sneaky gotchas with boolean values:
Use only  or .
Don't write , ,  or .
They might work in one version of YAML but break in another. To be safe, quote anything that looks like a Boolean (for example ).Keep configuration simple and minimalAvoid setting default values that are already handled by Kubernetes. Minimal manifests are easier to debug, cleaner to review and less likely to break things later.If your Deployment, Service and ConfigMap all belong to one app, put them in a single manifest file.
It's easier to track changes and apply them as a unit.
See the Guestbook all-in-one.yaml file for an example of this syntax.You can even apply entire directories with:One command and boom everything in that folder gets deployed.Manifest files are not just for machines, they are for humans too. Use annotations to describe why something exists or what it does. A quick one-liner can save hours when debugging later and also allows better collaboration.The most helpful annotation to set is kubernetes.io/description. It's like using comment, except that it gets copied into the API so that everyone else can see it even after you deploy.Managing Workloads: Pods, Deployments, and JobsA common early mistake in Kubernetes is creating Pods directly. Pods work, but they don't reschedule themselves if something goes wrong. (Pods not managed by a controller, such as Deployment or a StatefulSet) are fine for testing, but in real setups, they are risky.Why?
Because if the node hosting that Pod dies, the Pod dies with it and Kubernetes won't bring it back automatically.Use Deployments for apps that should always be runningA Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as RollingUpdate), is almost always preferable to creating Pods directly.
You can roll out a new version, and if something breaks, roll back instantly.Use Jobs for tasks that should finishA Job is perfect when you need something to run once and then stop like database migration or batch processing task.
It will retry if the pods fails and report success when it's done.Service Configuration and NetworkingServices are how your workloads talk to each other inside (and sometimes outside) your cluster. Without them, your pods exist but can't reach anyone. Let's make sure that doesn't happen.Create Services before workloads that use themWhen Kubernetes starts a Pod, it automatically injects environment variables for existing Services.
So, if a Pod depends on a Service, create a Service its corresponding backend workloads (Deployments or StatefulSets), and before any workloads that need to access it.For example, if a Service named foo exists, all containers will get the following variables in their initial environment:FOO_SERVICE_HOST=<the host the Service runs on>
FOO_SERVICE_PORT=<the port the Service runs on>
DNS based discovery doesn't have this problem, but it's a good habit to follow anyway.Use DNS for Service discoveryIf your cluster has the DNS add-on (most do), every Service automatically gets a DNS entry. That means you can access it by name instead of IP:It's one of those features that makes Kubernetes networking feel magical.Avoid  and  unless absolutely necessaryYou'll sometimes see these options in manifests:But here's the thing:
They tie your Pods to specific nodes, making them harder to schedule and scale. Because each <, , > combination must be unique. If you don't specify the  and  explicitly, Kubernetes will use  as the default  and  as the default .
Unless you're debugging or building something like a network plugin, avoid them.Use headless Services for internal discoverySometimes, you don't want Kubernetes to load balance traffic. You want to talk directly to each Pod. That's where headless Services come in.You create one by setting .
Instead of a single IP, DNS gives you a list of all Pods IPs, perfect for apps that manage connections themselves.Working with labels effectivelyLabels are key/value pairs that are attached to objects such as Pods.
Labels help you organize, query and group your resources.
They don't do anything by themselves, but they make everything else from Services to Deployments work together smoothly.Good labels help you understand what's what, even after months later.
Define and use labels that identify semantic attributes of your application or Deployment.
For example; : what the app is : which layer it belongs to (frontend/backend) : which stage it's in (test/prod)You can then use these labels to make powerful selectors.
For example:This will list all frontend Pods across your cluster, no matter which Deployment they came from.
Basically you are not manually listing Pod names; you are just describing what you want.
See the guestbook app for examples of this approach.Kubernetes actually recommends a set of common labels. It's a standardized way to name things across your different workloads or projects.
Following this convention makes your manifests cleaner, and it means that tools such as Headlamp, dashboard, or third-party monitoring systems can all
automatically understand what's running.Manipulate labels for debuggingSince controllers (like ReplicaSets or Deployments) use labels to manage Pods, you can remove a label to “detach” a Pod temporarily.The  part removes the label key .
Once that happens, the controller won’t manage that Pod anymore.
It’s like isolating it for inspection, a “quarantine mode” for debugging. To interactively remove or add labels, use .You can then check logs, exec into it and once done, delete it manually.
That’s a super underrated trick every Kubernetes engineer should know.These small tips make life much easier when you are working with multiple manifest files or clusters.Instead of applying one file at a time, apply the whole folder:This command looks for ,  and  files in that folder and applies them all together.
It's faster, cleaner and helps keep things grouped by app.Use label selectors to get or delete resourcesYou don't always need to type out resource names one by one.
Instead, use selectors to act on entire groups at once:It's especially useful in CI/CD pipelines, where you want to clean up test resources dynamically.Quickly create Deployments and ServicesFor quick experiments, you don't always need to write a manifest. You can spin up a Deployment right from the CLI:Then expose it as a Service:Cleaner configuration leads to calmer cluster administrators.
If you stick to a few simple habits: keep configuration simple and minimal, version-control everything,
use consistent labels, and avoid relying on naked Pods, you'll save yourself hours of debugging down the road.The best part?
Clean configurations stay readable. Even after months, you or anyone on your team can glance at them and know exactly what’s happening.]]></content:encoded></item><item><title>Switching to Rust&apos;s own mangling scheme on nightly</title><link>https://blog.rust-lang.org/2025/11/20/switching-to-v0-mangling-on-nightly/</link><author>David Wood</author><category>dev</category><pubDate>Thu, 20 Nov 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[ rustc will use its own "v0" mangling scheme by default on nightly
versions instead of the previous default, which re-used C++'s mangling
scheme, starting in When Rust is compiled into object files and binaries, each item (functions,
statics, etc) must have a globally unique  "symbol" identifying it.In C, the symbol name of a function is just the name that the function was
defined with, such as . This is straightforward and easy to
understand, but requires that each item have a globally unique name
that doesn't overlap with any symbols from libraries that it is linked
against. If two items had the same symbol then when the linker tried to resolve
a symbol to an address in memory (of a function, say), then it wouldn't know
which symbol is the correct one.Languages like Rust and C++ define "symbol mangling schemes", leveraging information
from the type system to give each item a unique symbol name. Without this, it would be
possible to produce clashing symbols in a variety of ways - for example, every
instantiation of a generic or templated function (or an overload in C++), which all
have the same name in the surface language would end up with clashing symbols; or
the same name in different modules, such as  and  would have clashing
symbols.Rust originally used a symbol mangling scheme based on the
Itanium ABI's name mangling scheme used by C++ (sometimes). Over
the years, it was extended in an inconsistent and ad-hoc way to support Rust
features that the mangling scheme wasn't originally designed for. Rust's current legacy
mangling scheme has a number of drawbacks:Information about generic parameter instantiations is lost during manglingIt is internally inconsistent - some paths use an Itanium ABI-style encoding
but some don'tSymbol names can contain  characters which aren't supported on all platformsSymbol names include an opaque hash which depends on compiler internals and
can't be easily replicated by other compilers or toolsThere is no straightforward way to differentiate between Rust and C++ symbolsIf you've ever tried to use Rust with a debugger or a profiler and found it hard
to work with because you couldn't work out which functions were which, it's probably
because information was being lost in the mangling scheme.Rust's compiler team started working on our own mangling scheme back in 2018
with RFC 2603 (see the "v0 Symbol Format" chapter in
rustc book for our current documentation on the format). Our "v0" mangling scheme has
multiple advantageous properties:An unambiguous encoding for everything that can end up in a binary's symbol tableInformation about generic parameters are encoded in a reversible wayMangled symbols are decodable such that it should be possible to identify concrete
instances of generic functionsIt doesn't rely on compiler internalsSymbols are restricted to only , ,  and , helping ensure
compatibility with tools on varied platformsIt tries to stay efficient and avoid unnecessarily long names and
computationally-expensive decodingHowever, rustc is not the only tool that interacts with Rust symbol names: the
aforementioned debuggers, profilers and other tools all need to be updated to
understand Rust's v0 symbol mangling scheme so that Rust's users can continue
to work with Rust binaries using all the tools they're used to without having
to look at mangled symbols. Furthermore, all of those tools need to have new
releases cut and then those releases need to be picked up by distros. This takes
time!Fortunately, the compiler team now believe that support for our v0 mangling
scheme is now sufficiently widespread that it can start to be used by default by
rustc.Reading Rust backtraces, or using Rust with debuggers, profilers and other
tools that operate on compiled Rust code, will be able to output much more
useful and readable names. This will especially help with async code,
closures and generic functions.It's easy to see the new mangling scheme in action, consider the following
example:With the legacy mangling scheme, all of the useful information about the generic
instantiation of  is lost in the symbol ....but with the v0 mangling scheme, the useful details of the generic instantiation
are preserved with  f::foo::<alloc::vec::Vec<(alloc::string::String, &[u8; 123])>>:Symbols using the v0 mangling scheme can be larger than symbols with the
legacy mangling scheme, which can result in a slight increase in linking
times and binary sizes if symbols aren't stripped (which they aren't by default).
Fortunately this impact should be minor, especially with modern linkers like
lld, which Rust will now default to on some targets.Some old versions of tools/distros or niche tools that the compiler team are
unaware of may not have had support for the v0 mangling scheme added. When
using these tools, the only consequence is that users may encounter mangled
symbols. rustfilt can be used to demangle Rust symbols if a tool does not.In any case, using the new mangling scheme can be disabled if any problem
occurs: use the -Csymbol-mangling-version=legacy -Zunstable-options flag
to revert to using the legacy mangling scheme.Explicitly enabling the legacy mangling scheme requires nightly, it is not
intended to be stabilised so that support can eventually be removed.If you maintain a tool that interacts with Rust symbols and does not
support the v0 mangling scheme, there are Rust and C implementations
of a v0 symbol demangler available in the rust-lang/rustc-demangle
repository that can be integrated into your project.rustc will use our "v0" mangling scheme on nightly for all targets
starting in tomorrow's rustup nightly ().If that happens, you can use the legacy mangling scheme with
the -Csymbol-mangling-version=legacy -Zunstable-options flag.
Either by adding it to the usual  environment variable, or to a
project's  configuration file, like so:If you like the sound of the new symbol mangling version and would
like to start using it on stable or beta channels of Rust, then you can
similarly use the -Csymbol-mangling-version=v0 flag today via
 or :]]></content:encoded></item><item><title>Python 3.15.0 alpha 2</title><link>https://pythoninsider.blogspot.com/2025/11/python-3150a2.html</link><author>Hugo</author><category>dev</category><pubDate>Wed, 19 Nov 2025 09:56:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python official news</source><content:encoded><![CDATA[This is an early developer preview of Python
3.15https://www.python.org/downloads/release/python-3150a2/Python 3.15 is still in development. This release, 3.15.0a2, is the
second of seven planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2026-05-05) and, if necessary, may be modified or
deleted up until the release candidate phase (2026-07-28). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.15 are still being planned and
written. Among the new major new features and changes so far:PEP
799: A new high-frequency, low-overhead, statistical sampling
profiler and dedicated profiling packagePEP
686: Python now uses UTF-8 as the default encodingPEP
782: A new  C API to create a Python bytes
object(Hey,  if a feature
you find important is missing from this list, let Hugo
know.)The next pre-release of Python 3.15 will be 3.15.0a3, currently
scheduled for 2025-12-16.“An hour,” said Ahab, standing rooted in his boat’s stern; and he
gazed beyond the whale’s place, towards the dim blue spaces and wide
wooing vacancies to leeward. It was only an instant; for again his eyes
seemed whirling round in his head as he swept the watery circle. The
breeze now freshened; the sea began to swell.“The birds!—the birds!” cried Tashtego.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software Foundation.Regards from a crisp and sunny subzero Helsinki,Your release team,
  Hugo van Kemenade
  Steve Dower
  ]]></content:encoded></item><item><title>Project goals update — September 2025</title><link>https://blog.rust-lang.org/2025/11/19/Project-Goals-2025-September-Update/</link><author>Tomas Sedovic</author><category>dev</category><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[coordinating with #![feature(pin_ergonomics)] (https://github.com/rust-lang/rust/issues/130494) to ensure compatibility between the two features (allow custom pin projections to be the same as the ones for )identified connection to auto reborrowing
https://github.com/rust-lang/rust-project-goals/issues/399https://github.com/rust-lang/rust/issues/145612held a design meetingvery positive feedback from the language teamgot a vibe check on design axiomscreated a new Zulip channel #t-lang/custom-refs for all new features needed to make custom references more similar to / such as field projections, auto reborrowing and moreopened https://github.com/rust-lang/rust/pull/146307 to implement field representing types (FRTs) in the compilerGet https://github.com/rust-lang/rust/pull/146307 reviewed & mergedWhen the PR for FRTs lands, try out the feature & provide feedback on FRTsShared & Exclusive ProjectionsWe want users to be able to have two different types of projections analogous to  and . Each field can be projected independently and a single field can only be projected multiple times in a shared way. The current design uses two different traits to model this. The two traits are almost identical, except for their safety documentation.We were thinking if it is possible to unify them into a single trait and have coercions similar to autoreborrowing that would allow the borrow checker to change the behavior depending on which type is projected.There are lots of different possibilities for which syntax we can choose, here are a couple options: /, /, /x.mut[Fatih Kadir Akın][], x.ref.[Fatih Kadir Akın][]/. Also many alternatives for the sigils used: , , .We have yet to decide on a direction we want to go in. If we are able to merge the two project traits, we can also settle on a single syntax which would be great.Splitting Projections into Containers & PointersThere are two categories of projections: Containers and Pointers: are types like , , , . They are  and apply themselves to each field, so  has a field of type  (if  has a field of type ). are types like , , , /, . They support projecting  to .In the current design, these two classes of projections are unified by just implementing Pointer<'_, Container<Struct>> -> Pointer<'_, Container<Field>> manually for the common use-cases (for example &mut MaybeUninit<Struct> -> &mut MaybeUninit<Field>). However this means that things like &Cell<MaybeUninit<Struct>> doesn't have native projections unless we explicitly implement them.We could try to go for a design that has two different ways to implement projections -- one for containers and one for pointers. But this has the following issues:there are two ways to implement projections, which means that some people will get confused which one they should use.making projections through multiple container types work out of the box is great, however this means that when defining a new container type and making it available for projections, one needs to consider all other container types and swear coherence with them. If we instead have an explicit way to opt in to projections through multiple container types, the implementer of that trait only has to reason about the types involved in that operation.
so to rephrase, the current design allows more container types that users actually use to be projected whereas the split design allows arbitrary nestings of container types to be projected while disallowing certain types to be considered container types.The same problem exists for allowing all container types to be projected by pointer types, if I define a new pointer type I again need to reason about all container types and if it's sound to project them.We might be able to come up with a sensible definition of "container type" which then resolves these issues, but further investigation is required.Projections for We want to be able to have both a blanket impl<T, F: Field<Base = T>> Project<F> for &T as well as allow people to have custom projections on . The motivating example for custom projections is the Rust-for-Linux  that wants these projections for safe RCU abstractions.During the design meeting, it was suggested we could add a generic to  that only the compiler is allowed to insert, this would allow disambiguation between the two impls. We have now found an alternative approach that requires less specific compiler magic:Add a new marker trait  that's implemented for all types by default.People can opt out of implementing it by writing impl !ProjectableBase for MyStruct; (needs negative impls for ).We add  to the .The compiler needs to consider the negative impls in the overlap check for users to be able to write their own impl<U, F> Project<F> for &Custom<U> where ... (needs negative impl overlap reasoning)We probably want negative impls for marker traits as well as improved overlap reasoning for different reasons too, so it is probably fine to depend on them here. and  shouldn't be available for projections by default, take for example , if we project to a variant, someone else could overwrite the value with a different variant, invalidating our . This also needs a new trait, probably  (needs more name bikeshedding, but too early for that) that marks fields in structs and tuples.To properly project an , we need:a new  (TBB) trait that provides a way to read the discriminant that's currently inhabiting the value.
it also needs to guarantee that the discriminant doesn't change while fields are being projected (this rules out implementing it for )a new  operator that will project all mentioned fields (for  this already is the behavior for )Field Representing Types (FRTs)While implementing https://github.com/rust-lang/rust/pull/146307 we identified the following problems/design decisions:a FRT is considered local to the orphan check when each container base type involved in the field path is local or a tuple (see the top comment on the PR for more infos)FRTs cannot implement the  trait is not user-implementabletypes with fields that are dynamically sized don't have a statically known offset, which complicates the  trait,I decided to simplify the first implementation of FRTs and restrict them to sized structs and tuples. It also doesn't support packed structs. Future PRs will add support for enums, unions and packed structs as well as dynamically sized types.]]></content:encoded></item><item><title>Project goals update — October 2025</title><link>https://blog.rust-lang.org/2025/11/19/project-goals-update-october-2025/</link><author>Tomas Sedovic</author><category>dev</category><pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[The focus right now is on the "non-const" parts of the proposal, as the "const" parts are blocked on the new trait solver (https://github.com/rust-lang/rust-project-goals/issues/113). Now that the types team FCP https://github.com/rust-lang/rust/pull/144064 has completed, work can proceed to land the implementation PRs. David Wood plans to split the RFC to separate out the "non-const" parts of the proposal so it can move independently, which will enable extern types.To that end, there are three interesting T-lang design questions to be considered.The RFC currently proposes the following namesHowever, these names do not follow the "best practice" of naming the trait after the capability that it provides. As champion Niko is recommending we shift to the following names: -- should righly be called , but oh well, not worth changing. -- named after the method  that you get access to. -- the only thing you can do is point at it.The last trait name is already used by the (unstable)  trait. We do not want to have these literally be the same trait because that trait adds a  associated type which would be backwards incompatible; if existing code uses  to mean <T as SomeOtherTrait>::Metadata, it could introduce ambiguity if now  due to defaults. My proposal is to rename  to std::ptr::PointeeMetadata for now, since that trait is unstable and the design remains under some discussion. The two traits could either be merged eventually or remain separate.Note that  be implemented automatically by the compiler for anything that implements .The RFC proposes that an explicit bound like  disabled the default  bound. However, this gives no signal that this trait bound is "special" or different than any other trait bound. Naming conventions can help here, signalling to users that these are special traits, but that leads to constraints on naming and may not scale as we consider using this mechanism to relax other defaults as proposed in my recent blog post. One idea is to use some form of syntax, so that  is just a regular bound, but (for example)  indicates that this bound "disables" the default  bound. This gives users some signal that something special is going on. This  syntax is borrowing from semver constraints, although it's not a precise match (it does not mean that  doesn't hold, after all). Other proposals would be some other sigil (, but it means "opt out from the traits above you"; , ...) or a keyword (no idea).To help us get a feel for it, I'll use  throughout this post.Implicit trait supertrait bounds, edition interactionIn Rust 2024, a trait is implicitly  which gets mapped to :trait Marker {} // cannot be implemented by extern types
This is not desirable but changing it would be backwards incompatible if traits have default methods that take advantage of this bound:trait NotQuiteMarker {
    fn dummy(&self) {
        let s = size_of_val(self);
    }
}
We need to decide how to handle this. Options areJust change it, breakage will be small (have to test that).Default to  but let users explicitly write  if they want that. Bad because all traits will be incompatible with extern types.Default to  only if defaulted methods are present. Bad because it's a backwards incompatible change to add a defaulted method now.Default to  but add  implicitly to defaulted methods. Now it's not backwards incompatible to add a new defaulted method, but it is backwards incompatible to change an existing method to have a default.If we go with one of the latter options, Niko proposes that we should relax this in the next Edition (Rust 2026?) so that the default becomes  (or maybe not even that, if we can).Relaxing associated type boundsUnder the RFC, existing  bounds would be equivalent to . This is mostly fine but will cause problems in (at least) two specific cases: closure bounds and the  trait. For closures, we can adjust the bound since the associated type is unstable and due to the peculiarities of our  syntax. Failure to adjust the Deref bound in particular would prohibit the use of  where  is an extern type, etc.For deref bounds, David Wood is preparing a PR that simply changes the bound in a backwards incompatible way to assess breakage on crater. There is some chance the breakage will be small.If the breakage proves problematic, or if we find other traits that need to be relaxed in a similar fashion, we do have the option of:In Rust 2024,  becomes equivalent to T: Deref<Target: SizeOfVal> unless written like T: Deref<Target: =Pointee>. We add that annotation throughout stdlib.In Rust 202X, we change the default, so that  does not add any special bounds, and existing Rust 2024  is rewritten to T: Deref<Target: SizeOfVal> as needed.One topic that came up in discussion is that we may eventually wish to add a level "below" , perhaps , that signifies webassembly external values which cannot be pointed at. That is not currently under consideration but should be backwards compatible.]]></content:encoded></item><item><title>Google Summer of Code 2025 results</title><link>https://blog.rust-lang.org/2025/11/18/gsoc-2025-results/</link><author>Jakub Beránek, Jack Huey</author><category>dev</category><pubDate>Tue, 18 Nov 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[As we have announced previously this year, the Rust Project participated
in Google Summer of Code (GSoC) for the second time. Almost twenty contributors have been working very hard on their projects for several months. Same as last year, the projects had various durations, so some of them have ended in September, while the last ones have been concluded in the middle of November. Now that the final reports of all projects have been submitted, we are happy to announce that 18 out of 19 projects have been successful! We had a very large number of projects this year, so we consider this number of successfully finished projects to be a great result.We had awesome interactions with our GSoC contributors over the summer, and through a video call, we also had a chance to see each other and discuss the accepted GSoC projects. Our contributors have learned a lot of new things and collaborated with us on making Rust better for everyone, and we are very grateful for all their contributions! Some of them have even continued contributing after their project has ended, and we hope to keep working with them in the future, to further improve open-source Rust software. We would like to thank all our Rust GSoC 2025 contributors. You did a great job!Same as last year, Google Summer of Code 2025 was overall a success for the Rust Project, this time with more than double the number of projects. We think that GSoC is a great way of introducing new contributors to our community, and we are looking forward to participating in GSoC (or similar programs) again in the near future. If you are interested in becoming a (GSoC) contributor, check out our GSoC project idea list and our guide for new contributors.Below you can find a brief summary of our GSoC 2025 projects. You can find more information about the original goals of the projects here. For easier navigation, here is an index of the project descriptions in alphabetical order:And now strap in, as there is a ton of great content to read about here!
ABI/Layout handling for the automatic differentiation featureThe  module allows computing gradients and derivatives in the calculus sense. It provides two autodiff macros, which can be applied to user-written functions and automatically generate modified versions of those functions, which also compute the requested gradients and derivatives. This functionality is very useful especially in the context of scientific computing and implementation of machine-learning models.Our autodiff frontend was facing two challenges.First, we would generate a new function through our macro expansion, however, we would not have a suitable function body for it yet. Our autodiff implementation relies on an LLVM plugin to generate the function body. However, this plugin only gets called towards the end of the compilation pipeline. Earlier optimization passes, either on the LLVM or the Rust side, could look at the placeholder body and either "optimize" or even delete the function since it has no clear purpose yet.Second, the flexibility of our macros was causing issues, since it allows requesting derivative computations on a per-argument basis. However, when we start to lower Rust arguments to our compiler backends like LLVM, we do not always have a 1:1 match of Rust arguments to LLVM arguments. As a simple example, an array with two double values might be passed as two individual double values on LLVM level, whereas an array with three doubles might be passed via a pointer.Marcelo helped rewrite our  macros to not generate hacky placeholder function bodies, but instead introduced a proper  intrinsic. This is the proper way for us to declare that an implementation of this function is not available yet and will be provided later in the compilation pipeline. As a consequence, our generated functions were not deleted or incorrectly optimized anymore. The intrinsic PR also allowed removing some previous hacks and therefore reduced the total lines of code in the Rust compiler by over 500! You can find more details in this PR.Beyond autodiff work, Marcelo also initiated work on GPU offloading intrinsics, and helped with multiple bugs in our argument handling. We would like to thank Marcelo for all his great work!The Rust Project has an ambitious goal to instrument the Rust standard library with safety contracts, moving from informal comments that specify safety requirements of  functions to executable Rust code. This transformation represents a significant step toward making Rust's safety guarantees more explicit and verifiable. To prioritize which functions should receive contracts first, there is a verification contest ongoing.Given that Rust contracts are still in their early stages, Dawid's project was intentionally open-ended in scope and direction. This flexibility allowed Dawid to identify and tackle several key areas that would add substantial value to the contracts ecosystem. His contributions were in the following three main areas:Pragmatic Contracts Integration: Refactoring contract HIR lowering to ensure no contract code is executed when contract-checks are disabled. This has major impact as it ensures that contracts do not have runtime cost when contract checks are disabled.Variable Reference Capability: Adding the ability to refer to variables from preconditions within postconditions. This fundamental enhancement to the contracts system has been fully implemented and merged into the compiler. This feature provides developers with much more expressive power when writing contracts, allowing them to establish relationships between input and output states.Separation Logic Integration: The bulk of Dawid's project involved identifying, understanding, and planning the introduction of owned and block ownership predicates for separation-logic style reasoning in contracts for unsafe Rust code. This work required extensive research and collaboration with experts in the field. Dawid engaged in multiple discussions with authors of Rust validation tools and Miri developers, both in person and through Zulip discussion threads. The culmination of this research is captured in a comprehensive MCP (Major Change Proposal) that Dawid created.Dawid's work represents crucial foundational progress for Rust's safety contracts initiative. By successfully implementing variable reference capabilities and laying the groundwork for separation logic integration, he has positioned the contracts feature for significant future development. His research and design work will undoubtedly influence the direction of this important safety feature as it continues to mature. Thank you very much!
Bootstrap of rustc with rustc_codegen_gccThe goal of this project was to improve the Rust GCC codegen backend (), so that it would be able to compile the "stage 2" Rust compiler () itself again.You might remember that Michał already participated in GSoC last year, where he was working on his own .NET Rust codegen backend, and he did an incredible amount of work. This year, his progress was somehow even faster. Even before the official GSoC implementation period started (!), he essentially completed his original project goal and managed to build  with GCC. This was no small feat, as he had to investigate and fix several miscompilations that occurred when functions marked with  were called recursively or when the compiled program was trying to work with 128-bit integers. You can read more about this initial work at his blog.After that, he immediately started working on stretch goals of his project. The first one was to get a "stage-3"  build working, for which he had to vastly improve the memory consumption of the codegen backend.Once that was done, he moved on to yet another goal, which was to build  for a platform not supported by LLVM. He made progress on this for Dec Alpha and m68k. He also attempted to compile  on Aarch64, which led to him finding an ABI bug. Ultimately, he managed to build a  for m68k (with a few workarounds that we will need to fix in the future). That is a very nice first step to porting Rust to new platforms unsupported by LLVM, and is important for initiatives such as Rust for Linux.Michał had to spend a lot of time starting into assembly code and investigating arcane ABI problems. In order to make this easier for everyone, he implemented support for fuzzing and automatically checking ABI mismatches in the GCC codegen backend. You can read more about his testing and fuzzing efforts here.We were really impressed with what Michał was able to achieve, and we really appreciated working with him this summer. Thank you for all your work, Michał!
Cargo: Build script delegationCargo build scripts come at a compile-time cost, because even to run , they must be built as if you ran , so that they can be executed during compilation. Even though we try to identify ways to reduce the need to write build scripts in the first place, that may not always be doable. However, if we could shift build scripts from being defined in every package that needs them, into a few core build script packages, we could both reduce the compile-time overhead, and also improve their auditability and transparency. You can find more information about this idea here.The first step required to delegate build scripts to packages is to be able to run multiple build scripts per crate, so that is what Naman was primarily working on. He introduced a new unstable  feature to Cargo, implemented support for parsing an array of build scripts in , and extended Cargo so that it can now execute multiple build scripts while building a single crate. He also added a set of tests to ensure that this feature will work as we expect it to.Then he worked on ensuring that the execution of builds scripts is performed in a deterministic order, and that crates can access the output of each build script separately. For example, if you have the following configuration:then the corresponding crate is able to access the s of both build scripts using env!("windows-manifest_OUT_DIR") and env!("release-info_OUTDIR").As future work, we would like to implement the ability to pass parameters to build scripts through metadata specified in  and then implement the actual build script delegation to external build scripts using artifact-dependencies.We would like to thank Naman for helping improving Cargo and laying the groundwork for a feature that could have compile-time benefits across the Rust ecosystem!
Distributed and resource-efficient verificationThe goal of this project was to address critical scalability challenges of formally verifying Rust's standard library by developing a distributed verification system that intelligently manages computational resources and minimizes redundant work. The Rust standard library verification project faces significant computational overhead when verifying large codebases, as traditional approaches re-verify unchanged code components. With Rust's standard library containing thousands of functions and continuous development cycles, this inefficiency becomes a major bottleneck for practical formal verification adoption.Jiping implemented a distributed verification system with several key innovations:Intelligent Change Detection: The system uses hash-based analysis to identify which parts of the codebase have actually changed, allowing verification to focus only on modified components and their dependencies.: The project coordinates multiple verification backends including Kani model checker, with careful version pinning and compatibility management.: The verification workload is distributed across multiple compute nodes, with intelligent scheduling that considers both computational requirements and dependency graphs.: Jiping built a comprehensive web interface that provides live verification status, interactive charts, and detailed proof results. You can check it out here!You can find the created distributed verification tool in this repository. Jiping's work established a foundation for scalable formal verification that can adapt to the growing complexity of Rust's ecosystem, while maintaining verification quality and completeness, which will go a long way towards ensuring that Rust's standard library remains safe and sound. Thank you for your great work!
Enable Witness Generation in cargo-semver-checks is a Cargo subcommand for finding SemVer API breakages in Rust crates. Talyn's project aimed to lay the groundwork for it to tackle our most vexing limitation: the inability to catch SemVer breakage due to type changes.Imagine a crate makes the following change to its public API:This is  a major breaking change, right? And yet  with its hundreds of lints is  unable to flag this. While this case seems trivial, it's just the tip of an enormous iceberg. Instead of changing  to , what if the change was from  to , or worse, into some monstrosity like:Figuring out whether this change is breaking requires checking whether the original  parameter type can "fit" into that monstrosity of an  type. But reimplementing a Rust type checker and trait solver inside  is out of the question! Instead, we turn to a technique created for a previous study of SemVer breakage on crates.io—we generate a "witness" program that will fail to compile if, and only if, there's a breaking change between the two versions.The witness program is a separate crate that can be made to depend on either the old or the new version of the crate being scanned. If our  function comes from a crate called , its witness program would look something like:This example is cherry-picked to be easy to understand. Witness programs are rarely this straightforward!Attempting to  the witness while plugging in the new version of  forces  to decide whether  matches the new  parameter. If  passes without errors, there's no breaking change here. But if there's a compilation error, then this is concrete, incontrovertible evidence of breakage!Over the past 22+ weeks, Talyn worked tirelessly to move this from an idea to a working proof of concept. For every problem we foresaw needing to solve, ten more emerged along the way. Talyn did a lot of design work to figure out an approach that would be able to deal with crates coming from various sources (crates.io, a path on disk, a git revision), would support multiple rustdoc JSON formats for all the hundreds of existing lints, and do so in a fashion that doesn't get in the way of adding hundreds more lints in the future.Even the above list of daunting challenges fails to do justice to the complexity of this project. Talyn created a witness generation prototype that lays the groundwork for robust checking of type-related SemVer breakages in the future. The success of this work is key to the  roadmap for 2026 and beyond. We would like to thank Talyn for their work, and we hope to continue working with them on improving witness generation in the future.
Extend behavioural testing of std::arch intrinsicsThe  module contains target-specific intrinsics (low-level functions that typically correspond to single machine instructions) which are intended to be used by other libraries. These are intended to match the equivalent intrinsics available as vendor-specific extensions in C.The intrinsics are tested with three approaches. We test that:The signatures of the intrinsics match the one specified by the architecture.The intrinsics generate the correct instruction.The intrinsics have the correct runtime behavior.These behavior tests are implemented in the intrinsics-test crate. Initially, this test framework only covered the AArch64 and AArch32 targets, where it was very useful in finding bugs in the implementation of the intrinsics. Madhav's project was about refactoring and improving this framework to make it easier (or really, possible) to extend it to other CPU architectures.First, Madhav split the codebase into a module with shared (architecturally independent) code and a module with ARM-specific logic. Then he implemented support for testing intrinsics for the x86 architecture, which is Rust's most widely used target. In doing so, he allowed us to discover real bugs in the implementation of some intrinsics, which is a great result! Madhav also did a lot of work in optimizing how the test suite is compiled and executed, to reduce CI time needed to run tests, and he laid the groundwork for supporting even more architectures, specifically LoongArch and WebAssembly.We would like to thank Madhav for all his work on helping us make sure that Rust intrinsics are safe and correct!
Implement merge functionality in borsThe main Rust repository uses a pull request merge queue bot that we call . Its current Python implementation has a lot of issues and was difficult to maintain. The goal of this GSoC project was thus to implement the primary merge queue functionality in our Rust rewrite of this bot.Sakibul first examined the original Python codebase to figure out what it was doing, and then he implemented several bot commands that allow contributors to approve PRs, set their priority, delegate approval rights, temporarily close the merge tree, and many others. He also implemented an asynchronous background process that checks whether a given pull request is mergeable or not (this process is relatively involved, due to how GitHub works), which required implementing a specialized synchronized queue for deduplicating mergeability check requests to avoid overloading the GitHub API. Furthermore, Sakibul also reimplemented (a nicer version of) the merge queue status webpage that can be used to track which pull requests are currently being tested on CI, which ones are approved, etc.After the groundwork was prepared, Sakibul could work on the merge queue itself, which required him to think about many tricky race conditions and edge cases to ensure that bors doesn't e.g. merge the wrong PR into the default branch or merge a PR multiple times. He covered these edge cases with many integration tests, to give us more confidence that the merge queue will work as we expect it to, and also prepared a script for creating simulated PRs on a test GitHub repository so that we can test bors "in the wild". And so far, it seems to be working very well!After we finish the final piece of the merge logic (creating so-called "rollups") together with Sakibul, we will start using bors fully in the main Rust repository. Sakibul's work will thus be used to merge all  pull requests. Exciting!Apart from working on the merge queue, Sakibul made many other awesome contributions to the codebase, like refactoring the test suite or analyzing performance of SQL queries. In total, Sakibul sent around fifty pull requests that were already merged into bors! What can we say, other than: Awesome work Sakibul, thank you!bootstrap is the build system of Rust itself, which is responsible for building the compiler, standard library, and pretty much everything else that you can download through . This project's goal was very open-ended: "improve bootstrap".And Shourya did just that! He made meaningful contributions to several parts of bootstrap. First, he added much-needed documentation to several core bootstrap data structures and modules, which were quite opaque and hard to understand without any docs. Then he moved to improving command execution, as each bootstrap invocation invokes hundreds of external binaries, and it was difficult to track them. Shourya finished a long-standing refactoring that routes almost all executed commands through a single place. This allowed him to also implement command caching and also command profiling, which shows us which commands are the slowest.After that, Shourya moved on to refactoring config parsing. This was no easy task, because bootstrap has A LOT of config options; the single  that parses them had over a thousand lines of code (!). A set of complicated config precedence rules was frequently causing bugs when we had to modify that function. It took him several weeks to untangle this mess, but the result is worth it. The refactored function is much less brittle and easier to understand and modify, which is great for future maintenance.The final area that Shourya improved were bootstrap tests. He made it possible to run them using bare , which enables debugging them e.g. in an IDE, which is very useful, and mainly he found a way to run the tests in parallel, which makes contributing to bootstrap itself much more pleasant, as it reduced the time to execute the tests from a minute to under ten seconds. These changes required refactoring many bootstrap tests that were using global state, which was not compatible with parallel execution.Overall, Shourya made more than 30 PRs to bootstrap since April! We are very thankful for all his contributions, as they made bootstrap much easier to maintain. Thank you!
Improve Wild linker test suitesWild is a very fast linker for Linux that’s written in Rust. It can be used to build executables and shared objects.Kei’s project was to leverage the test suite of one of the other Linux linkers to help test the Wild linker. This goal was accomplished. Thanks to Kei’s efforts, we now run the Mold test suite against Wild in our CI. This has helped to prevent regressions on at least a couple of occasions and has also helped to show places where Wild has room for improvement.In addition to this core work, Kei also undertook numerous other changes to Wild during GSoC. Of particular note was the reworking of argument parsing to support , which we had wanted for some time. Kei also fixed a number of bugs and implemented various previously missing features. This work has helped to expand the range of projects that can use Wild to build executables.Kei has continued to contribute to Wild even after the GSoC project finished and has now contributed over seventy PRs. We thank Kei for all the hard work and look forward to continued collaboration in the future!
Improving the Rustc Parallel Frontend: Parallel Macro ExpansionThe Rust compiler has a (currently unstable) parallel compilation mode in which some compiler passes run in parallel.
One major part of the compiler that is not yet affected by parallelization is name resolution.
It has several components, but those selected for this GSoC project were import resolution and macro expansion (which are in fact intermingled into a single fixed-point algorithm).
Besides the parallelization itself, another important point of the work was improving the correctness of import resolution by eliminating accidental order dependencies in it, as those also prevent parallelization.We should note that this was a  ambitious project, and we knew from the beginning that it would likely be quite challenging to reach the end goal within the span of just a few months. And indeed, Lorrens did in fact run into several unexpected issues that showed us that the complexity of this work is well beyond a single GSoC project, so he didn't actually get to parallelizing the macro expansion algorithm. Nevertheless, he did a lot of important work to improve the name resolver and prepare it for being parallelized.The first thing that Lorrens had to do was actually understand how Rust name resolution works and how it is implemented in the compiler. That is, to put it mildly, a  piece of logic, and is affected by legacy burden in the form of backward compatibility lints, outdated naming conventions, and other technical debt. Even this learned knowledge itself is incredibly useful, as the set of people that understand Rust's name resolution today is very low, so it is important to grow it.Using this knowledge, he made a lot of refactorings to separate  mutability in name resolver data structures from "cache-like" mutability used for things like lazily loading otherwise immutable data from extern crates, which was needed to unblock parallelization work. He split variousparts of the name resolver, got rid of unnecessarymutability and performed a bunch of otherrefactorings. He also had to come up with a very tricky data structure that allows providing conditional mutable access to some data.These refactorings allowed him to implement something called "batched import resolution", which splits unresolved imports in the crate into "batches", where all imports in a single batch can be resolved independently and potentially in parallel, which is crucial for parallelizing name resolution. We have to resolve a few remaining language compatibilityissues, after which the batched import resolution work will hopefully be merged.Lorrens laid an important groundwork for fixing potential correctness issues around name resolution and macro expansion, which unblocks further work on parallelizing these compiler passes, which is exciting. His work also helped unblock some libraryimprovements that were stuck for a long time. We are grateful for your hard work on improving tricky parts of Rust and its compiler, Lorrens. Thank you!
Make cargo-semver-checks faster is a Cargo subcommand for finding SemVer API breakages in Rust crates. It is adding SemVer lints at an  pace: the number of lints has been doubling every year, and currently stands at . More lints mean more work for  to do, as well as more work for its test suite which runs over 250000 lint checks!Joseph's contributions took three forms:Improving  runtime performance—on large crates, our query runtime went from ~8s to ~2s, a 4x improvement!Improving the test suite's performance, enabling us to iterate faster. Our test suite used to take ~7min and now finishes in ~1min, a 7x improvement!Improving our ability to profile query performance and inspect performance anomalies, both of which were proving a bottleneck for our ability to ship further improvements.Joseph described all the clever optimization tricks leading to these results in his final report. To encourage you to check out the post, we'll highlight a particularly elegant optimization described there. relies on rustdoc JSON, an unstable component of Rust whose output format often has breaking changes. Since each release of  supports a range of Rust versions, it must also support a range of rustdoc JSON formats. Fortunately, each file carries a version number that tells us which version's  types to use to deserialize the data.Previously, we used to deserialize the JSON file twice: once with a  type that only loaded the  field, and a second time with the appropriate  type that matches the format. This works fine, but many large crates generate rustdoc JSON files that are 500 MiB+ in size, requiring us to walk all that data twice. While  is quite fast, there's nothing as fast as  doing the work twice in the first place!So we used a trick:  check if the  field is the last field in the JSON file, which happens to be the case every time (even though it is not guaranteed). Rather than parsing JSON, we merely look for a  character in the last few dozen bytes, then look for  after the  character, and for  between them. If this is successful, we've discovered the version number while avoiding going through hundreds of MB of data! If we failed for any reason, we just fall back to the original approach having only wasted the effort of looking at 20ish extra bytes.Joseph did a lot of profiling and performance optimizations to make  faster for everyone, with awesome results. Thank you very much for your work!As a very important part of the Rustup team's vision of migrating the rustup codebase to using async IO since the introduction of the global  runtime in #3367, this project's goal was to introduce proper concurrency to rustup. Francisco did that by attacking two aspects of the codebase at once:He created a new set of user interfaces for displaying concurrent progress.He implemented a new toolchain update checking & installation flow that is idiomatically concurrent.As a warmup, Francisco made  concurrent, resulting in a rather easy 3x performance boost in certain cases. Along the way, he also introduced a new indicatif-based progress bar for reporting progress of concurrent operations, which replaced the original hand-rolled solution.After that, the focus of the project has moved on to the toolchain installation flow used in commands like  and . In this part, Francisco developed two main improvements:The possibility of downloading multiple components at once when setting up a toolchain, controlled by the RUSTUP_CONCURRENT_DOWNLOADS environment variable. Setting this variable to a value greater than 1 is particularly useful in certain internet environments where the speed of a single download connection could be restricted by QoS (Quality of Service) limits.The ability to interleave component network downloads and disk unpacking. For the moment, unpacking will still happen sequentially, but disk and net I/O can finally be overlapped! This introduces a net gain in toolchain installation time, as only the last component being downloaded will have noticeable unpacking delays. In our tests, this typically results in a reduction of 4-6 seconds (on fast connections, that's ~33% faster!) when setting up a toolchain with the  profile.We have to say that these results are very impressive! While a few seconds shorter toolchain installation might not look so important at a first glance, rustup is ubiquitously used to install Rust toolchains on CI of tens of thousands of Rust projects, so this improvement (and also further improvements that it unlocks) will have an enormous effect across the Rust ecosystem. Many thanks to Francisco Gouveia's enthusiasm and active participation, without which this wouldn't have worked out!
Mapping the Maze of Rust's UI Test Suite with Established Continuous Integration PracticesThe snapshot-based UI test suite is a crucial part of the Rust compiler's test suite. It contains  of tests: over 19000 at the time of writing. The organization of this test suite is thus
very important, for at least two reasons:We want to be able to find specific tests, identify related tests, and have some sort of logical grouping of related tests.We have to ensure that no directory contains so many entries such that GitHub gives up rendering the directory.Furthermore, having informative test names and having some context for each test is particularly important, as otherwise contributors would have to reverse-engineer test intent from  and friends.Over the years, we have accumulated a lot of unorganized stray test files in the
top level  directory, and have a lot of generically named 
tests in the  directory. The former makes it annoying to find
more meaningful subdirectories, while the latter makes it completely non-obvious
what each test is about.Julien's project was about introducing some order into the chaos. And that was indeed achieved!
Through Julien's efforts (in conjunction with efforts from other contributors), we now have:No more stray tests under the immediate  top-level directory, and are organized into more meaningful subdirectories. We were able to then introduce a style check to prevent new stray tests from being added.A top-level document contains TL;DRs for each of the immediate subdirectories.Substantially fewer generically-named under .Test organization (and more generally, test suite ergonomics) is an often under-
appreciated aspect of maintaining complex codebases. Julien spent a lot of effort
improving test ergonomics of the Rust compiler, both in last year's GSoC (where he vastly
improved our "run-make" test suite), and then again this year, where he made our UI test suite more ergonomic.
We would like to appreciate your meticulous work, Julien! Thank you very much.
Modernising the libc Crate is a crucial crate in the Rust ecosystem (on average, it has ~1.5 million  downloads), providing bindings to system C API. This GSoC project had two goals: improve testing for what we currently have, and make progress toward a stable 1.0 release of .Test generation is handled by the  crate, which creates unit tests that compare properties of Rust API to properties of the C interfaces it binds. Prior to the project,  used an obsolete Rust parser that had stopped receiving major updates about eight years ago, meaning  could not easily use any syntax newer than that. Abdul completely rewrote  to use  as its parser and make it much easier to add new tests, then went through and switched everything over to the more modern . After this change, we were able to remove a number of hacks that had been needed to work with the old parser.The other part of the project was to make progress toward the 1.0 release of . Abdul helped with this by going through and addressing a number of issues that need to be resolved before the release, many of which were made possible with all the  changes.While there is still a lot of work left to do before  can reach 1.0, Abdul's improvements will go a long way towards making that work easier, as they give us more confidence in the test suite, which is now much easier to modify and extend. Thank you very much for all your work!
Prepare stable_mir crate for publishingThis project's goal was to prepare the Rust compiler's 
crate (eventually renamed to ), which provides a way to interface
with the Rust compiler for analyzing Rust code, for publication on crates.io. While the
existing crate provided easier APIs for tool developers, it lacked proper
versioning and was tightly coupled with compiler versions. The goal was to
enable independent publication with semantic versioning.The main technical work involved restructuring  and 
(previously named ) by inverting their dependency relationship.
Makai resolved circular dependencies by temporarily merging the crates and
gradually separating them with the new architecture. They also split the existing
compiler interface to separate public APIs from internal compiler details.Furthermore, Makai established infrastructure for dual maintenance: keeping an internal
version in the Rust repository to track compiler changes while developing
the publishable version in a dedicated repository. Makai automated a
system to coordinate between versions, and developed custom tooling to validate
compiler version compatibility and to run tests.Makai successfully completed the core refactoring and infrastructure
setup, making it possible to publish  independently with proper
versioning support for the Rust tooling ecosystem! As a bonus, Makai contributed
several bug fixes and implemented new APIs that had been requested by the
community. Great job Makai!
Prototype an alternative architecture for cargo fix using cargo checkThe  command applies fixes suggested by lints, which makes it useful for cleaning up sloppy code,
reducing the annoyance of toolchain upgrades when lints change and helping with edition migrations and new lint adoption. However, it has a number of issues. It can be slow, it only applies a subset of possible lints, and doesn't provide an easy way to select which lints to fix.These problems are caused by its current architecture; it is implemented as a variant of  that replaces  with  being run in a special mode that will call  in a loop, applying fixes until there are none. While this special -proxy mode is running,
a cross-process lock is held to force only one build target to be fixed at a time to avoid race conditions.
This ensures correctness at the cost of performance and difficulty in making the -proxy interactive.Glen implemented a proof of concept of an alternative design called cargo-fixit.  spawns  in a loop, determining which build targets are safe to fix in a given pass, and then applying the suggestions. This puts the top-level program in charge of what fixes get applied, making it easier to coordinate. It also allows the locking to be removed and opens the door to an interactive mode.Glen performed various benchmarks to test how the new approach performs.  And in some benchmarks,  was able to finish within a few hundred milliseconds, where before the same task took  almost a minute! As always, there are trade-offs; the new approach comes at the cost that fixes in packages lower in the dependency tree can cause later packages to be rebuilt multiple times, slowing things down, so there were also benchmarks where the old design was a bit faster. The initial results are still very promising and impressive!Further work remains to be done on  to investigate how it could be optimized better and how should its interface look like before being stabilized. We thank Glen for all the hard work on this project, and we hope that one day the new design will become used by default in Cargo, to bring faster and more flexible fixing of lint suggestions to everyone!The goal of this project was to move forward our Project Goal for creating low-level ("plumbing") Cargo subcommands to make it easier to reuse parts of Cargo by other tools.Vito created a prototype of several plumbing commands in the cargo-plumbing crate. The idea was to better understand how the plumbing commands should look like, and what is needed from Cargo to implement them. Vito had to make compromises in some of these commands to not be blocked on making changes to the current Cargo Rust APIs, and he helpfully documented those blockers. For example, instead of solely relying on the manifests that the user passed in, the plumbing commands will re-read the manifests within each command, preventing callers from being able to edit them to get specific behavior out of Cargo, e.g. dropping all workspace members to allow resolving dependencies on a per-package basis.Vito did a lot of work, as he implemented seven different plumbing subcommands:As future work, we would like to deal with some unresolved questions around how to integrate these plumbing commands within Cargo itself, and extend the set of plumbing commands.We thank Vito for all his work on improving the flexibility of Cargo.We would like to thank all contributors that have participated in Google Summer of Code 2025 with us! It was a blast, and we cannot wait to see which projects GSoC contributors will come up with in the next year. We would also like to thank Google for organizing the Google Summer of Code program and for allowing us to have so many projects this year. And last, but not least, we would like to thank all the Rust mentors who were tirelessly helping our contributors to complete their projects. Without you, Rust GSoC would not be possible.]]></content:encoded></item><item><title>Launching the 2025 State of Rust Survey</title><link>https://blog.rust-lang.org/2025/11/17/launching-the-2025-state-of-rust-survey/</link><author>apiraino, Jakub Beránek</author><category>dev</category><pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[The Rust Project has been collecting valuable information about the Rust programming language community through our annual State of Rust Survey since 2016. Which means that this year marks the tenth edition of this survey!We invite you to take this year’s survey whether you have just begun using Rust, you consider yourself an intermediate to advanced user, or you have not yet used Rust but intend to one day. The results will allow us to more deeply understand the global Rust community and how it evolves over time.Like last year, the 2025 State of Rust Survey will likely take you between 10 and 25 minutes, and responses are anonymous. We will accept submissions until December 17. Trends and key insights will be shared on blog.rust-lang.org as soon as possible.We are offering the State of Rust Survey in the following languages (if you speak multiple languages, please pick one). Language options are available on the main survey page:Note: the non-English translations of the survey are provided in a best-effort manner. If you find any issues with the
translations, we would be glad if you could send us a pull request to improve the quality of the translations!Please help us spread the word by sharing the survey link via your social media networks, at meetups, with colleagues, and in any other community that makes sense to you.This survey would not be possible without the time, resources, and attention of the Rust Survey Team, the Rust Foundation, and other collaborators. We would also like to thank the following contributors who helped with translating the survey (in no particular order):We appreciate your participation!Click here to read a summary of last year's survey findings.By the way, the Rust Survey team is looking for new members. If you like working with data and coordinating people, and would like to help us out with managing various Rust surveys, please drop by our Zulip channel and say hi.]]></content:encoded></item><item><title>Go’s Sweet 16</title><link>https://go.dev/blog/16years</link><author>Austin Clements, for the Go team</author><category>dev</category><pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Golang Blog</source><content:encoded><![CDATA[This past Monday, November 10th, we celebrated the 16th anniversary of Go’s
open source
release!We released Go 1.24 in February and Go 1.25 in
August, following our now well-established and dependable release
cadence. Continuing our mission to build the most productive language platform
for building production systems, these releases included new APIs for building
robust and reliable software, significant advances in Go’s track record for
building secure software, and some serious under-the-hood improvements.
Meanwhile, no one can ignore the seismic shifts in our industry brought by
generative AI. The Go team is applying its thoughtful and uncompromising mindset
to the problems and opportunities of this dynamic space, working to bring Go’s
production-ready approach to building robust AI integrations, products, agents,
and infrastructure.First released in Go 1.24 as an experiment and then graduated in Go 1.25, the
new  package
significantly simplifies writing tests for concurrent, asynchronous
code. Such code is particularly common in network services,
and is traditionally very hard to test well. The  package works by
virtualizing time itself. It takes tests that used to be slow, flaky, or both,
and makes them easy to rewrite into reliable and nearly instantaneous tests,
often with just a couple extra lines of code. It’s also a great example of Go’s
integrated approach to software development: behind an almost trivial API, the
 package hides a deep integration with the Go runtime and other parts
of the standard library.This isn’t the only boost the  package got over the past year. The new
 API is both easier to use
than the original  API and addresses many of the traditional—and
often invisible!—pitfalls of writing Go benchmarks. The
 package also has new APIs that make it easy to
cleanup in tests that use
, and that make it
easy to write to the test’s log.Go and containerization grew up together and work great with each other. Go 1.25
launched container-aware scheduling, making
this pairing even stronger. Without developers having to lift a finger, this
transparently adjusts the parallelism of Go workloads running in containers,
preventing CPU throttling that can impact tail latency and improving Go’s
out-of-the-box production-readiness.Go 1.25’s new flight recorder builds on our already
powerful execution tracer, enabling deep insights into the dynamic behavior of
production systems. While the execution tracer generally collected 
information to be practical in long-running production services, the flight
recorder is like a little time machine, allowing a service to snapshot recent
events in great detail  something has gone wrong.Secure software developmentGo continues to strengthen its commitment to secure software development, making
significant strides in its native cryptography packages and evolving its
standard library for enhanced safety.Go ships with a full suite of native cryptography packages in the standard
library, which reached two major milestones over the past year. A security
audit conducted by independent security firm Trail of
Bits yielded excellent
results, with only a single low-severity finding.
Furthermore, through a collaborative effort between the Go Security Team and
Geomys, these packages achieved CAVP certification,
paving the way for full FIPS 140-3 certification. This is a
vital development for Go users in certain regulated environments. FIPS 140
compliance, previously a source of friction due to the need for unsupported
solutions, will now be seamlessly integrated, addressing concerns related to
safety, developer experience, functionality, release velocity, and compliance.The Go standard library has continued to evolve to be  and
. For example, the 
API—added in Go 1.24—enables traversal-resistant file system
access, effectively combating a class of vulnerabilities where an
attacker could manipulate programs into accessing files intended to be
inaccessible. Such vulnerabilities are notoriously challenging to address
without underlying platform and operating system support, and the new
 API offers a straightforward,
consistent, and portable solution.Under-the-hood improvementsIn addition to user-visible changes, Go has made significant improvements under
the hood over the past year.For Go 1.24, we completely redesigned the 
implementation, building on the latest and greatest ideas in
hash table design. This change is completely transparent, and brings significant
improvements to  performance, lower tail latency of  operations, and
in some cases even significant memory wins.Go 1.25 includes an experimental and significant advancement in Go’s garbage
collector called Green Tea. Green Tea reduces garbage
collection overhead in many applications by at least 10% and sometimes as much
as 40%. It uses a novel algorithm designed for the capabilities and constraints
of today’s hardware and opens up a new design space that we’re eagerly
exploring. For example, in the forthcoming Go 1.26 release, Green Tea will
achieve an additional 10% reduction in garbage collector overhead on hardware
that supports AVX-512 vector instructions—something that would have been nigh
impossible to take advantage of in the old algorithm. Green Tea will be enabled
by default in Go 1.26; users need only upgrade their Go version to benefit.Go is about far more than the language and standard library. It’s a software
development platform, and over the past year, we’ve also made four regular
releases of the gopls language server, and have formed partnerships to
support emerging new frameworks for agentic applications.Gopls provides Go support to VS Code and other LSP-powered editors and IDEs.
Every release sees a litany of features and improvements to the experience of
reading and writing Go code (see the v0.17.0,
v0.18.0, v0.19.0, and
v0.20.0 release notes for full details, or our new
gopls feature documentation!). Some highlights include many
new and enhanced analyzers to help developers write more idiomatic and robust Go
code; refactoring support for variable extraction, variable inlining, and JSON
struct tags; and an experimental built-in server for the
Model Context Protocol (MCP) that exposes a subset of gopls’ functionality to AI
assistants in the form of MCP tools.With gopls v0.18.0, we began exploring automatic code modernizers. As Go
evolves, every release brings new capabilities and new idioms; new and better
ways to do things that Go programmers have been finding other ways to do. Go
stands by its compatibility promise—the old way will continue
to work in perpetuity—but nevertheless this creates a bifurcation between old
idioms and new idioms. Modernizers are static analysis tools that recognize old
idioms and suggest faster, more readable, more secure, more 
replacements, and do so with push-button reliability. What  did for
stylistic consistency, we hope modernizers can do for idiomatic
consistency. We’ve integrated modernizers as IDE suggestions, where they can
help developers not only maintain more consistent coding standards, but where we
believe they will help developers discover new features and keep up with the
state of the art. We believe modernizers can also help AI coding assistants keep
up with the state of the art and combat their proclivity to reinforce outdated
knowledge of the Go language, APIs, and idioms. The upcoming Go 1.26 release
will include a total overhaul of the long-dormant  command to make it
apply the full suite of modernizers in bulk, a return to its pre-Go 1.0
roots.At the end of September, in collaboration with
Anthropic and the Go community, we released
v1.0.0 of
the official Go SDK for the
Model Context Protocol (MCP). This SDK
supports both MCP clients and MCP servers, and underpins the new MCP
functionality in gopls. Contributing this work in open source helps empower
other areas of the growing open source agentic ecosystem built around Go, such
as the recently released Agent Development Kit (ADK) for
Go from Google.
ADK Go builds on the Go MCP SDK to provide an idiomatic framework for building
modular multi-agent applications and systems. The Go MCP SDK and ADK Go
demonstrate how Go’s unique strengths in concurrency, performance, and
reliability differentiate Go for production AI development and we are expecting
more AI workloads to be written in Go in the coming years.Go has an exciting year ahead of it.We’re working on advancing developer productivity through the brand new 
command, deeper support for AI coding assistants, and ongoing improvements to
gopls and VS Code Go. General availability of the Green Tea garbage collector,
native support for Single Instruction Multiple Data (SIMD) hardware features,
and runtime and standard library support for writing code that scales even
better to massive multicore hardware will continue to align Go with modern
hardware and improve production efficiency. We’re focusing on Go’s “production
stack” libraries and diagnostics, including a massive (and long in the making)
upgrade to , driven by Joe Tsai and people across
the Go community; leaked goroutine
profiling, contributed by
Uber’s Programming Systems team; and many
other improvements to , , and other foundational packages.
We’re working to provide well-lit paths for building with Go and AI, evolving
the language platform with care for the evolving needs of today’s developers,
and building tools and capabilities that help both human developers and AI
assistants and systems alike.On this 16th anniversary of Go’s open source release, we’re also looking to the
future of the Go open source project itself. From its humble
beginnings, Go has formed a
thriving contributor community. To continue to best meet the needs of our
ever-expanding user base, especially in a time of upheaval in the software
industry, we’re working on ways to better scale Go’s development
processes—without losing sight of Go’s fundamental principles—and more deeply
involve our wonderful contributor community.Go would not be where it is today without our incredible user and contributor
communities. We wish you all the best in the coming year!]]></content:encoded></item><item><title>Ingress NGINX Retirement: What You Need to Know</title><link>https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/</link><author></author><category>dev</category><pubDate>Tue, 11 Nov 2025 18:30:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[To prioritize the safety and security of the ecosystem, Kubernetes SIG Network and the Security Response Committee are announcing the upcoming retirement of Ingress NGINX. Best-effort maintenance will continue until March 2026. Afterward, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered. Existing deployments of Ingress NGINX will continue to function and installation artifacts will remain available.We recommend migrating to one of the many alternatives. Consider migrating to Gateway API, the modern replacement for Ingress. If you must continue using Ingress, many alternative Ingress controllers are listed in the Kubernetes documentation. Continue reading for further information about the history and current state of Ingress NGINX, as well as next steps.Ingress is the original user-friendly way to direct network traffic to workloads running on Kubernetes. (Gateway API is a newer way to achieve many of the same goals.) In order for an Ingress to work in your cluster, there must be an Ingress controller running. There are many Ingress controller choices available, which serve the needs of different users and use cases. Some are cloud-provider specific, while others have more general applicability.Ingress NGINX was an Ingress controller, developed early in the history of the Kubernetes project as an example implementation of the API. It became very popular due to its tremendous flexibility, breadth of features, and independence from any particular cloud or infrastructure provider. Since those days, many other Ingress controllers have been created within the Kubernetes project by community groups, and by cloud native vendors. Ingress NGINX has continued to be one of the most popular, deployed as part of many hosted Kubernetes platforms and within innumerable independent users’ clusters.The breadth and flexibility of Ingress NGINX has caused maintenance challenges. Changing expectations about cloud native software have also added complications. What were once considered helpful options have sometimes come to be considered serious security flaws, such as the ability to add arbitrary NGINX configuration directives via the "snippets" annotations. Yesterday’s flexibility has become today’s insurmountable technical debt.Despite the project’s popularity among users, Ingress NGINX has always struggled with insufficient or barely-sufficient maintainership. For years, the project has had only one or two people doing development work, on their own time, after work hours and on weekends. Last year, the Ingress NGINX maintainers announced their plans to wind down Ingress NGINX and develop a replacement controller together with the Gateway API community. Unfortunately, even that announcement failed to generate additional interest in helping maintain Ingress NGINX or develop InGate to replace it. (InGate development never progressed far enough to create a mature replacement; it will also be retired.)Current State and Next StepsCurrently, Ingress NGINX is receiving best-effort maintenance. SIG Network and the Security Response Committee have exhausted our efforts to find additional support to make Ingress NGINX sustainable. To prioritize user safety, we must retire the project.In March 2026, Ingress NGINX maintenance will be halted, and the project will be retired. After that time, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered. The GitHub repositories will be made read-only and left available for reference.Existing deployments of Ingress NGINX will not be broken. Existing project artifacts such as Helm charts and container images will remain available.In most cases, you can check whether you use Ingress NGINX by running kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx with cluster administrator permissions.We would like to thank the Ingress NGINX maintainers for their work in creating and maintaining this project–their dedication remains impressive. This Ingress controller has powered billions of requests in datacenters and homelabs all around the world. In a lot of ways, Kubernetes wouldn’t be where it is without Ingress NGINX, and we are grateful for so many years of incredible effort.SIG Network and the Security Response Committee recommend that all Ingress NGINX users begin migration to Gateway API or another Ingress controller immediately. Many options are listed in the Kubernetes documentation: Gateway API, Ingress. Additional options may be available from vendors you work with.]]></content:encoded></item><item><title>Announcing Rust 1.91.1</title><link>https://blog.rust-lang.org/2025/11/10/Rust-1.91.1/</link><author>The Rust Release Team</author><category>dev</category><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Rust Blog</source><content:encoded><![CDATA[The Rust team has published a new point release of Rust, 1.91.1. Rust is a
programming language that is empowering everyone to build reliable and
efficient software.If you have a previous version of Rust installed via rustup, getting Rust
1.91.1 is as easy as:If you don't have it already, you can get  from the
appropriate page on our website.Rust 1.91.1 includes fixes for two regressions introduced in the 1.91.0 release.
Linker and runtime errors on WasmMost targets supported by Rust identify symbols by their name, but Wasm
identifies them with a symbol name  a Wasm module name. The
#[link(wasm_import_module)] attribute allows to
customize the Wasm module name an  block refers to:Rust 1.91.0 introduced a regression in the attribute, which could cause linker
failures during compilation ( errors) or the wrong
function being used at runtime (leading to undefined behavior, including crashes
and silent data corruption). This happened when the same symbol name was
imported from two different Wasm modules across multiple Rust crates.Rust 1.91.1 fixes the regression. More details are available in issue #148347.
Cargo target directory locking broken on illumosCargo relies on locking the  directory during a build to prevent
concurrent invocations of Cargo from interfering with each other. Not all
filesystems support locking (most notably some networked ones): if the OS
returns the  error when attempting to lock, Cargo assumes locking
is not supported and proceeds without it.Cargo 1.91.0 switched from custom code interacting with the OS APIs to the
 standard library method (recently stabilized in Rust 1.89.0). Due
to an oversight, that method always returned  on the illumos
target, causing Cargo to never lock the build directory on illumos regardless of
whether the filesystem supported it.Rust 1.91.1 fixes the oversight in the standard library by enabling the
 family of functions on illumos, indirectly fixing the Cargo
regression.Many people came together to create Rust 1.91.1. We couldn't have done it
without all of you. Thanks!]]></content:encoded></item><item><title>Announcing the 2025 Steering Committee Election Results</title><link>https://kubernetes.io/blog/2025/11/09/steering-committee-results-2025/</link><author></author><category>dev</category><pubDate>Sun, 9 Nov 2025 20:10:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[The 2025 Steering Committee Election is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2025. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.The Steering Committee oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their charter.Thank you to everyone who voted in the election; your participation helps support the community’s continued health and success.Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):They join continuing members:Maciej Szulik and Paco Xu are returning Steering Committee Members.Thank you and congratulations on a successful election to this round’s election officers:Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community:And thank you to all the candidates who came forward to run for election.You can see what the Steering Committee meetings are all about by watching past meetings on the YouTube Playlist.This post was adapted from one written by the Contributor Comms Subproject. If you want to write stories about the Kubernetes community, learn more about us.This article was revised in November 2025 to update the information about when the steering committee meets.]]></content:encoded></item><item><title>Gateway API 1.4: New Features</title><link>https://kubernetes.io/blog/2025/11/06/gateway-api-v1-4/</link><author></author><category>dev</category><pubDate>Thu, 6 Nov 2025 17:00:00 +0000</pubDate><source url="https://kubernetes.io/">Kubernetes Blog</source><content:encoded><![CDATA[Ready to rock your Kubernetes networking? The Kubernetes SIG Network community presented the General Availability (GA) release of Gateway API (v1.4.0)! Released on October 6, 2025, version 1.4.0 reinforces the path for modern, expressive, and extensible service networking in Kubernetes.Gateway API v1.4.0 brings three new features to the 
(Gateway API's GA release channel):BackendTLSPolicy for TLS between gateways and backends in GatewayClass statusand introduces three new experimental features:Mesh resource for service mesh configuration to ease configuration burden** filter for HTTPRouteGraduations to Standard ChannelBackendTLSPolicy is a new Gateway API type for specifying the TLS configuration
of the connection from the Gateway to backend pod(s).
. Prior to the introduction of BackendTLSPolicy, there was no API specification
that allowed encrypted traffic on the hop from Gateway to backend.The  configuration requires a hostname. This 
serves two purposes. It is used as the SNI header when connecting to the backend and
for authentication, the certificate presented by the backend must match this hostname,
 is explicitly specified.If  (SANs) are specified, the  is only used for SNI, and authentication is performed against the SANs instead. If you still need to authenticate against the hostname value in this case, you MUST add it to the  list.BackendTLSPolicy  configuration also requires either  or .
 refer to one or more (up to 8) PEM-encoded TLS certificate bundles. If there are no specific certificates to use,
then depending on your implementation, you may use ,
set to "System" to tell the Gateway to use an implementation-specific set of trusted CA Certificates.In this example, the BackendTLSPolicy is configured to use certificates defined in the auth-cert ConfigMap
to connect with a TLS-encrypted upstream connection where pods backing the auth service are expected to serve a
valid certificate for . It uses  with a Hostname type, but you may also use a URI type.In this example, the BackendTLSPolicy is configured to use system certificates to connect with a TLS-encrypted backend connection where Pods backing the dev Service are expected to serve a valid certificate for .GatewayClass status has a new field, .
This addition allows implementations to declare the set of features they support. This provides a clear way for users and tools to understand the capabilities of a given GatewayClass.This feature's name for conformance tests (and GatewayClass status reporting) is .
Implementations must populate the  field in the  of the GatewayClass  the GatewayClass
is accepted, or in the same operation.Here’s an example of a  published under GatewayClass' :Graduation of SupportedFeatures to Standard, helped improve the conformance testing process for Gateway API.
The conformance test suite will now automatically run tests based on the features populated in the GatewayClass' status.
This creates a strong, verifiable link between an implementation's declared capabilities and the test results,
making it easier for implementers to run the correct conformance tests and for users to trust the conformance reports.This means when the SupportedFeatures field is populated in the GatewayClass status there will be no need for additional
conformance tests flags like , or  or .
It's important to note that Mesh features are an exception to this and can be tested for conformance by using
, or by manually providing any combination of features related flags until the dedicated resource
graduates from the experimental channel.This enhancement enables route rules to be explicitly identified and referenced across the Gateway API ecosystem.
Some of the key use cases include: Allowing status conditions to reference specific rules directly by name. Making it easier to identify individual rules in logs, traces, and metrics. Enabling policies (GEP-713) to target specific route rules via the  field in their . Simplifying filtering and referencing of route rules in tools such as , , and general-purpose utilities like  and .Internal configuration mapping: Facilitating the generation of internal configurations that reference route rules by name within gateway and mesh implementations.This follows the same well-established pattern already adopted for Gateway listeners, Service ports, Pods (and containers),
and many other Kubernetes resources.While the new name field is  (so existing resources remain valid), its use is .
Implementations are not expected to assign a default value, but they may enforce constraints such as immutability.Finally, keep in mind that the name format is validated,
and other fields (such as )
may impose additional, indirect constraints.Experimental channel changesEnabling external Auth for HTTPRouteGiving Gateway API the ability to enforce authentication and maybe authorization as well at the Gateway or HTTPRoute level has been a highly requested feature for a long time. (See the GEP-1494 issue for some background.)This Gateway API release adds an Experimental filter in HTTPRoute that tells the Gateway API implementation to call out to an external service to authenticate (and, optionally, authorize) requests.This filter is based on the Envoy ext_authz API, and allows talking to an Auth service that uses either gRPC or HTTP for its protocol.Both methods allow the configuration of what headers to forward to the Auth service, with the HTTP protocol allowing some extra information like a prefix path.A HTTP example might look like this (noting that this example requires the Experimental channel to be installed and an implementation that supports External Auth to actually understand the config):This allows the backend Auth service to use the supplied headers to make a determination about the authentication for the request.When a request is allowed, the external Auth service will respond with a 200 HTTP response code, and optionally extra headers to be included in the request that is forwarded to the backend. When the request is denied, the Auth service will respond with a 403 HTTP response.Since the Authorization header is used in many authentication methods, this method can be used to do Basic, Oauth, JWT, and other common authentication and authorization methods.Gateway API v1.4.0 introduces a new experimental Mesh resource, which provides a way to configure mesh-wide settings and discover the features supported by a given mesh implementation. This resource is analogous to the Gateway resource and will initially be mainly used for conformance testing, with plans to extend its use to off-cluster Gateways in the future.The Mesh resource is cluster-scoped and, as an experimental feature, is named  and resides in the gateway.networking.x-k8s.io API group. A key field is controllerName, which specifies the mesh implementation responsible for the resource. The resource's  stanza indicates whether the mesh implementation has accepted it and lists the features the mesh supports.One of the goals of this GEP is to avoid making it more difficult for users to adopt a mesh. To simplify adoption, mesh implementations are expected to create a default Mesh resource upon startup if one with a matching  doesn't already exist. This avoids the need for manual creation of the resource to begin using a mesh.The new XMesh API kind, within the gateway.networking.x-k8s.io/v1alpha1 API group,
provides a central point for mesh configuration and feature discovery (source).A minimal XMesh object specifies the :The mesh implementation populates the status field to confirm it has accepted the resource and to list its supported features ( source):Introducing default GatewaysFor application developers, one common piece of feedback has been the need to explicitly name a parent Gateway for every single north-south Route. While this explicitness prevents ambiguity, it adds friction, especially for developers who just want to expose their application to the outside world without worrying about the underlying infrastructure's naming scheme. To address this, we have introduce the concept of .For application developers: Just "use the default"As an application developer, you often don't care about the specific Gateway your traffic flows through, you just want it to work. With this enhancement, you can now create a Route and simply ask it to use a default Gateway.This is done by setting the new  field in your Route's .Here’s a simple  that uses a default Gateway:That's it! No more need to hunt down the correct Gateway name for your environment. Your Route is now a "defaulted Route."For cluster operators: You're still in controlThis feature doesn't take control away from cluster operators ("Chihiro").
In fact, they have explicit control over which Gateways can act as a default. A Gateway will only accept these  if it is configured to do so.You can also use a ValidatingAdmissionPolicy to either require or even forbid for Routes to rely on a default Gateway.As a cluster operator, you can designate a Gateway as a default
by setting the (new)  field:Operators can choose to have no default Gateways, or even multiple.How it works and key detailsTo maintain a clean, GitOps-friendly workflow, a default Gateway does  modify the  of your Route. Instead, the binding is reflected in the Route's  field. You can always inspect the  stanza of your Route to see exactly which Gateway or Gateways have accepted it. This preserves your original intent and avoids conflicts with CD tools.The design explicitly supports having multiple Gateways designated as defaults within a cluster. When this happens, a defaulted Route will bind to  of them. This enables cluster operators to perform zero-downtime migrations and testing of new default Gateways.You can create a single Route that handles both north-south traffic (traffic entering or leaving the cluster, via a default Gateway) and east-west/mesh traffic (traffic between services within the cluster), by explicitly referencing a Service in .Default Gateways represent a significant step forward in making the Gateway API simpler and more intuitive for everyday use cases, bridging the gap between the flexibility needed by operators and the simplicity desired by developers.Configuring client certificate validationThis release brings updates for configuring client certificate validation, addressing a critical security vulnerability related to connection reuse.
HTTP connection coalescing is a web performance optimization that allows a client to reuse an existing TLS connection
for requests to different domains. While this reduces the overhead of establishing new connections, it introduces a security risk
in the context of API gateways.
The ability to reuse a single TLS connection across multiple Listeners brings the need to introduce shared client certificate
configuration in order to avoid unauthorized access.Why SNI-based mTLS is not the answerOne might think that using Server Name Indication (SNI) to differentiate between Listeners would solve this problem.
However, TLS SNI is not a reliable mechanism for enforcing security policies in a connection coalescing scenario.
A client could use a single TLS connection for multiple peer connections, as long as they are all covered by the same certificate.
This means that a client could establish a connection by indicating one peer identity (using SNI), and then reuse that connection
to access a different virtual host that is listening on the same IP address and port. That reuse, which is controlled by client side
heuristics, could bypass mutual TLS policies that were specific to the second listener configuration.Here's an example to help explain it:I have configured a Gateway with two listeners, both having overlapping hostnames.
My intention is for the  listener to be accessible only by clients presenting the  certificate.
In contrast, the  listener should allow access to a broader audience using any certificate valid for the  domain.Consider a scenario where a client initially connects to . The server requests and successfully validates the
 certificate, establishing the connection. Subsequently, the same client wishes to access other sites within this domain,
such as , which is handled by the  listener. Due to connection reuse,
clients can access  backends without an additional TLS handshake on the existing connection.
This process functions as expected.However, a critical security vulnerability arises when the order of access is reversed.
If a client first connects to  and presents a valid  certificate, the connection is successfully established.
If this client then attempts to access , the existing connection's client certificate will not be re-validated.
This allows the client to bypass the specific certificate requirement for the  backend, leading to a serious security breach.The solution: per-port TLS configurationThe updated Gateway API gains a  field in the  of a Gateway, that allows you to define a default client certificate
validation configuration for all Listeners, and then if needed override it on a per-port basis. This provides a flexible and
powerful way to manage your TLS policies.Here’s a look at the updated API definitions (shown as Go source code):Standard GRPCRoute -  field required (technicality)The promotion of GRPCRoute to Standard introduces a minor but technically breaking change regarding the presence of the top-level  field.
As part of achieving Standard status, the Gateway API has tightened the OpenAPI schema validation within the GRPCRoute
CustomResourceDefinition (CRD)
to explicitly ensure the spec field is required for all GRPCRoute resources.
This change enforces stricter conformance to Kubernetes object standards and enhances the resource's stability and predictability.
While it is highly unlikely that users were attempting to define a GRPCRoute without any specification, any existing automation
or manifests that might have relied on a relaxed interpretation allowing a completely absent  field will now fail validation
and  be updated to include the  field, even if empty.Experimental CORS support in HTTPRoute - breaking change for  fieldThe Gateway API subproject has introduced a breaking change to the Experimental CORS support in HTTPRoute, concerning the  field
within the CORS policy.
This field's definition has been strictly aligned with the upstream CORS specification, which dictates that the corresponding
Access-Control-Allow-Credentials header must represent a Boolean value.
Previously, the implementation might have been overly permissive, potentially accepting non-standard or string representations such as
 due to relaxed schema validation.
Users who were configuring CORS rules must now review their manifests and ensure the value for 
strictly conforms to the new, more restrictive schema.
Any existing HTTPRoute definitions that do not adhere to this stricter validation will now be rejected by the API server,
requiring a configuration update to maintain functionality.Improving the development and usage experienceAs part of this release, we have improved some of the developer experience workflow:Added Kube API Linter to the CI/CD pipelines, reducing the burden of API reviewers and also reducing the amount of common mistakes.Improving the execution time of CRD tests with the usage of .Additionally, as part of the effort to improve Gateway API usage experience, some efforts were made to remove some ambiguities and some old tech-debts from our documentation website:The API reference is now explicit when a field is .The GEP (GatewayAPI Enhancement Proposal) navigation bar is automatically generated, reflecting the real status of the enhancements.Unlike other Kubernetes APIs, you don't need to upgrade to the latest version of
Kubernetes to get the latest version of Gateway API. As long as you're running
Kubernetes 1.26 or later, you'll be able to get up and running with this version
of Gateway API.As of this writing, seven implementations are already conformant with Gateway API v1.4.0. In alphabetical order:Wondering when a feature will be added? There are lots of opportunities to get
involved and help define the future of Kubernetes routing APIs for both ingress
and service mesh.The maintainers would like to thank  who's contributed to Gateway
API, whether in the form of commits to the repo, discussion, ideas, or general
support. We could never have made this kind of progress without the support of
this dedicated and active community.Related Kubernetes blog articles]]></content:encoded></item></channel></rss>
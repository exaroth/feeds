<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI</title><link>https://konrad.website/feeds/</link><description></description><item><title>Altman: OpenAI not for sale, especially to competitor who is not able to beat us</title><link>https://www.axios.com/2025/02/11/openai-altman-musk-offer</link><author>/u/namanyayg</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 14:17:57 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Will AI Lead to the Disintermediation of Knowledge?</title><link>https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/</link><author>Bill Schmarzo</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 13:28:49 +0000</pubDate><source url="https://www.datasciencecentral.com/">Dev - Data Science Central</source><content:encoded><![CDATA[Key Blog Points: For decades, organizations have operated under the central assumption that knowledge flows downward. Senior leaders, industry veterans, and domain experts have traditionally been the primary gatekeepers of critical information. Their insights, honed over years of experience, have been the cornerstone of strategic decision-making. Enter artificial intelligence (AI). Many folks are concerned that… Read More »]]></content:encoded></item><item><title>Chinese Vice Minister says China and the US must work together to control rogue AI: &quot;If not... I am afraid that the probability of the machine winning will be high.&quot;</title><link>https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 12:27:09 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in  (AI).But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.“Realistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,” Fu said.“As long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.”The panel discussion is part of a two-day global  that started in Paris on Monday.Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden’s administration and the United Nations.]]></content:encoded></item><item><title>[P] Daily ArXiv filtering powered by LLM judge</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/</link><author>/u/MadEyeXZ</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 11:14:16 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[D] What&apos;s the most promising successor to the Transformer?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/</link><author>/u/jsonathan</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Sat, 15 Feb 2025 06:17:01 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also xLSTM and Aaren.What do y'all think is the most promising alternative architecture to the transformer?]]></content:encoded></item><item><title>How I Became A Machine Learning Engineer (No CS Degree, No Bootcamp)</title><link>https://towardsdatascience.com/how-i-became-a-machine-learning-engineer-no-cs-degree-no-bootcamp/</link><author>Egor Howell</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 02:33:01 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Machine learning and AI are among the most popular topics nowadays, especially within the tech space. I am fortunate enough to work and develop with these technologies every day as a machine learning engineer!In this article, I will walk you through my journey to becoming a machine learning engineer, shedding some light and advice on how you can become one yourself!In one of my previous articles, I extensively wrote about my journey from school to securing my first Data Science job. I recommend you check out that article, but I will summarise the key timeline here.Pretty much everyone in my family studied some sort of STEM subject. My great-grandad was an engineer, both my grandparents studied physics, and my mum is a maths teacher.So, my path was always paved for me.I chose to study physics at university after watching The Big Bang Theory at age 12; it’s fair to say everyone was very proud!At school, I wasn’t dumb by any means. I was actually relatively bright, but I didn’t fully apply myself. I got decent grades, but definitely not what I was fully capable of.I was very arrogant and thought I would do well with zero work.I applied to top universities like Oxford and Imperial College, but given my work ethic, I was delusional thinking I had a chance. On results day, I ended up in clearing as I missed my offers. This was probably one of the saddest days of my life.Clearing in the UK is where universities offer places to students on certain courses where they have space. It’s mainly for students who don’t have a university offer.I was lucky enough to be offered a chance to study physics at the University of Surrey, and I went on to earn a first-class master’s degree in physics!There is genuinely no substitute for hard work. It is a cringy cliche, but it is true!My original plan was to do a PhD and be a full-time researcher or professor, but during my degree, I did a research year, and I just felt a career in research was not for me. Everything moved so slowly, and it didn’t seem there was much opportunity in the space.During this time, DeepMind released theirdocumentary on YouTube, which popped up on my home feed.From the video, I started to understand how AI worked and learn about neural networks, reinforcement learning, and deep learning. To be honest, to this day I am still not an expert in these areas.Naturally, I dug deeper and found that a data scientist uses AI and machine learning algorithms to solve problems. I immediately wanted in and started applying for data science graduate roles.I spent countless hours coding, taking courses, and working on projects. I applied to and eventually landed my first data science graduate scheme in September 2021.You can hear more about my journey from a podcast.I started my career in an insurance company, where I built various supervised learning models, mainly using gradient boosted tree packages like CatBoost, XGBoost, and generalised linear models (GLMs).I built models to predict: — Did someone fraudulently make a claim to profit.— What’s the premium we should give someone.— How many claims will someone have. — What’s the average claim value someone will have.I made around six models spanning the regression and classification space. I learned so much here, especially in statistics, as I worked very closely with Actuaries, so my maths knowledge was excellent.However, due to the company’s structure and setup, it was difficult for my models to advance past the PoC stage, so I felt I lacked the “tech” side of my toolkit and understanding of how companies use machine learning in production.After a year, my previous employer reached out to me asking if I wanted to apply to a junior data scientist role that specialises in time series forecasting and optimisation problems. I really liked the company, and after a few interviews, I was offered the job!I worked at this company for about 2.5 years, where I became an expert in forecasting and combinatorial optimisation problems.I developed many algorithms and deployed my models to production through AWS using software engineering best practices, such as unit testing, lower environment, shadow system, CI/CD pipelines, and much more.Fair to say I learned a lot. I worked very closely with software engineers, so I picked up a lot of engineering knowledge and continued self-studying machine learning and statistics on the side.Over time, I realised the actual value of data science is using it to make live decisions. There is a good quote by Pau Labarta BajoML models inside Jupyter notebooks have a business value of $0There is no point in building a really complex and sophisticated model if it will not produce results. Seeking out that extra 0.1% accuracy by staking multiple models is often not worth it.You are better off building something simple that you can deploy, and that will bring real financial benefit to the company.With this in mind, I started thinking about the future of data science. In my head, there are two avenues: -> You work primarily to gain insight into what the business should be doing and what it should be looking into to boost its performance. -> You ship solutions (models, decision algorithms, etc.) that bring business value.I feel the data scientist who analyses and builds PoC models will become extinct in the next few years because, as we said above, they don’t provide tangible value to a business.That’s not to say they are entirely useless; you have to think of it from the business perspective of their return on investment. Ideally, the value you bring in should be more than your salary.You want to say that you did “X that produced Y”, which the above two avenues allow you to do.The engineering side was the most interesting and enjoyable for me. I genuinely enjoy coding and building stuff that benefits people, and that they can use, so naturally, that’s where I gravitated towards.To move to the ML engineering side, I asked my line manager if I could deploy the algorithms and ML models I was building myself. I would get help from software engineers, but I would write all the production code, do my own system design, and set up the deployment process independently.And that’s exactly what I did.Coincidentally, my current employer contacted me around this time and asked if I wanted to apply for a machine learning engineer role that specialises in general ML and optimisation at their company!Call it luck, but clearly, the universe was telling me something. After several interview rounds, I was offered the role, and I am now a fully fledged machine learning engineer!Fortunately, a role kind of “fell to me,” but I created my own luck through up-skilling and documenting my learning. That is why I always tell people to show their work — you don’t know what may come from it.I want to share the main bits of advice that helped me transition from a machine learning engineer to a data scientist. — A machine learning engineer is  an entry-level position in my opinion. You need to be well-versed in data science, machine learning, software engineering, etc. You don’t need to be an expert in all of them, but have good fundamentals across the board. That’s why I recommend having a couple of years of experience as either a software engineer or data scientist and self-study other areas. — If you are from data science, you must learn to write good, well-tested production code. You must know things like typing, linting, unit tests, formatting, mocking and CI/CD. It’s not too difficult, but it just requires some practice. I recommend asking your current company to work with software engineers to gain this knowledge, it worked for me! — Most companies nowadays deploy many of their architecture and systems on the cloud, and machine learning models are no exception. So, it’s best to get practice with these tools and understand how they enable models to go live. I learned most of this on the job, to be honest, but there are courses you can take. — I am sure most of you know this already, but every tech professional should be proficient in the command line. You will use it extensively when deploying and writing production code. I have a basic guide you can checkout here.Data Structures & Algorithms — Understanding the fundamental algorithms in computer science are very useful for MLE roles. Mainly because you will likely be asked about it in interviews. It’s not too hard to learn compared to machine learning; it just takes time. Any course will do the trick. — Again, most tech professionals should know Git, but as an MLE, it is essential. How to squash commits, do code reviews, and write outstanding pull requests are musts. — Many MLE roles I saw required you to have some specialisation in a particular area. I specialise in time series forecasting, optimisation, and general ML based on my previous experience. This helps you stand out in the market, and most companies are looking for specialists nowadays.The main theme here is that I basically up-skilled my software engineering abilities. This makes sense as I already had all the math, stats, and machine learning knowledge from being a data scientist.If I were a software engineer, the transition would likely be the reverse. This is why securing a machine learning engineer role can be quite challenging, as it requires proficiency across a wide range of skills.Summary & Further ThoughtsI have a free newsletter, , where I share weekly tips and advice as a practising data scientist. Plus, when you subscribe, you will get my and short PDF version of my AI roadmap!]]></content:encoded></item><item><title>➡️ Start Asking Your Data ‘Why?’ — A Gentle Intro To Causality</title><link>https://towardsdatascience.com/start-asking-your-data-why-a-gentle-intro-to-causality/</link><author>Eyal Kazin PhD</author><category>dev</category><category>ai</category><pubDate>Sat, 15 Feb 2025 02:30:07 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Correlation does not imply causation. It turns out, however, that with some simple ingenious tricks one can, potentially, unveil causal relationships within standard observational data, without having to resort to expensive randomised control trials.This post is targeted towards anyone making data driven decisions. The main takeaway message is that causality may be possible by understanding that the  is as important as the data itself.By introducing  and , situations where the outcome of a population is in conflict with that of its cohorts, I shine a light on the importance of using causal reasoning to identify these paradoxes in data and avoid misinterpretation. Specifically I introduce as a method to visualise the story behind the data point out that by adding this to your arsenal you are likely to conduct better analyses and experiments.My ultimate objective is to whet your appetite to explore more on causality, as I believe that by asking data  you will be able to go beyond correlation calculations and extract more insights, as well as avoid common misjudgement pitfalls.Note that throughout this gentle intro I do not use equations but demonstrate using accessible intuitive visuals. That said I provide resources for you to take your next step in adding Causal Inference to your statistical toolbox so that you may get more value from your data.The Era of Data Driven Decision MakingIn [Deity] We Trust, All Others Bring Data! — William E. DemingIn this digital age it is common to put a lot of faith in data. But this raises an overlooked question: Should we trust data on its own?Judea Pearl, who is considered the godfather of Causality, articulated best:“The collection of information is as important as the information itself “ — Judea PearlIn other words the data is as important as the data itself.This manifests in a growing awareness of the importance of identifying bias in datasets. By the end of this post I hope that you will appreciate that causality pertains the fundamental tools to best express, quantify and attempt to correct for these biases.In causality introductions it is customary to demonstrate why “correlation does not imply causation” by highlighting limitations of association analysis due to spurious correlations (e.g, shark attacks  and ice-cream sales ). In an attempt to reduce the length of this post I defer this aspect to an older one of mine. Here I focus on two mind boggling paradoxes  and their resolution via to make a similar point.To understand the importance of the story behind the data we will examine two counter-intuitive (but nonetheless true) paradoxes which are classical situations of data misinterpretation.In the first we imagine a clinical trial in which patients are given a treatment and that results in a health score. Our objective is to assess the average impact of increased treatment to the health outcome. For pedagogical purposes in these examples we assume that samples are representative (i.e, the sample size is not an issue) and that variances in measurements are minimal.In the figure above we learn that on average increasing the treatment appears to be beneficial since it results in a better outcome.Now we’ll color code by age and gender groupings and examine how the treatment increases impacts each cohort.Track any cohort (e.g, “Girls” representing young females) and you immediately realise that increase in treatment appears adverse.What is the conclusion of the study? On the one hand increasing the treatment appears to be better for the population at large, but when examining gender-age cohorts it seems disadvantageous. This is Simpson’s Paradox which may be stated:“Trends can exist in subgroups but reverse for the whole”Below we will resolve this paradox using causality tools, but beforehand let’s explore another interesting one, which also examines made up data.Imagine that we quantify for the general population their attractiveness and how talented they are as in this figure:We find no apparent correlation.Now we’ll focus on an unusual subset — famous people:Here we clearly see an anti-correlation that doesn’t exist in the general population.Should we conclude that  and  are independent variables as per the first plot of the general population or that they are correlated as per that of celebrities?This is Berkson’s Paradox where one population has a trait trend that another lacks.Whereas an algorithm would identify these correlations, resolving these paradoxes requires a full understanding of the context which normally is not fed to a computer. In other words without knowing the story behind the data results may be  and  may be inferred.Mastering identification and resolution these paradoxes is an important  to elevating one’s analyses from correlations to .Whereas these simple examples may be explained away logically, for the purposes of learning causal tools in the next section I’ll introduce .Causal Graphs— Visualising The Story Behind The Data“[From the Simpson’s and Berkson’s Paradoxes we learn that] , but instead depend on the . … Graph Theory enables these stories to be conveyed” — Judea PearlCausal graph models are probabilistic graphical models used to visualise the story behind the data. They are perhaps one of the most powerful tools for analysts that is not taught in most statistics curricula. They are both elegant and highly informative. Hopefully by the end of this post you will appreciate it when Judea Pearl says that this is the missing vocabulary to communicate causality.To understand causal graph models (or causal graphs for short) we start with the following illustration of an example  with four nodes/vertices and three edges.Each node is a variable and the edges communicate “who is  to whom?” (i.e, correlations, joint probabilities).A  is one in which we add arrows as in this figure.A directed edge communicates “who  to whom?” which is the essence of causation.In this specific example you can notice a cyclical relationship between the C and D nodes.A useful subset of directed graphs are the  (DAG), which have no cycles as in the next figure.Here we see that when starting from any node (e.g, A) there isn’t a path that gets back to it.DAGs are the go-to choice in causality for simplicity as the fact that parameters do not have feedback highly simplifies the flow of information. (For mechanisms that have feedback, e.g temporal systems, one may consider rolling out nodes as a function of time, but that is beyond the scope of this intro.)Causal graphs are powerful at conveying the cause/effect relationships between the parameter and hence how data was generated (the story behind the data).From a practical point of view, graphs enable us to understand which parameters are confounders that need to be controlled for, and, as important, which not to control for, because doing so causes spurious correlations. This will be demonstrated below.The practice of attempting to build a causal graph enables:Design of better experiments.Draw causal conclusions (go beyond correlations by means of representing interventions, counterfactuals and encoding conditional independence relationships; all beyond the scope of this post).To further motivate the usage of causal graph models we will use them to resolve the Simpson’s and Berkson’s paradoxes introduced above.For simplicity we’ll examine Simpson’s paradox focusing on two cohorts, male and female adults.Examining this data we can make three statements about three variables of interest:Gender is an independent variable (it does not “listen to” the other two)Treatment depends on Gender (as we can see, in this setting the level given depends on Gender — women have been given, for some reason, a higher dosage.)Outcome depends on both Gender and TreatmentAccording to these we can draw the causal graph as the following:Notice how each arrow contributes to communicate the statements above. As important, the lack of an arrow pointing into Gender conveys that it is an independent variable.We also notice that by having arrows pointing from Gender to Treatment and Outcome it is considered a  between them.The essence of the Simpson’s paradox is that although the Outcome is effected by changes in Treatment, as expected, there is also a flow of information via Gender.As you may have guessed by this stage, the solution to this paradox is that the common cause Gender is a confounding variable that needs to be .Controlling for a variable, in terms of a causal graph, means eliminating the relationship between Gender and Treatment.This may be done in two manners:Pre data collection: Setting up a (RCT) in which participants will be given dosage regardless of their Gender.Post data collection: E.g, in this made up scenario the data has already been collected and hence we need to deal with what is referred to as In both pre- and post- data collection the elimination of the Treatment dependency of Gender (i.e, controlling for the Gender) may be done by modifying the graph such that the arrow between them is removed as in the following:Applying this “graphical surgery” means that the last two statements need to be modified (for convenience I’ll write all three):Gender is an independent variableTreatment is an independent variableOutcome depends on Gender and Treatment (but with no backdoor path).This enables obtaining the causal relationship of interest : we can assess the direct impact of modification Treatment on the Outcome.The process of controlling for a confounder, i.e manipulation of the data generation process, is formally referred to as applying an . That is to say we are no longer passive observers of the data, but we are taking an active role in modification it to assess the causal impact.How is this manifested in practice?In the case of RCTs the researcher needs to control for important confounding variables. Here we limit the discussion to Gender (but in real world settings you can imagine other variables such as Age, Social Status and anything else that might be relevant to one’s health).RCTs are considered the golden standard for causal analysis in many experimental settings thanks to its practice of confounding variables. That said, it has many setbacks:It may be  to recruit individuals and may be complicated The intervention under investigation may not be  possible or  to conduct (e.g, one can’t ask randomly selected people to smoke or not for ten years)Artificial setting of a laboratory — not a true natural  of the population.Observational data on the other hand is much more readily available in the industry and academia and hence much cheaper and could be more representative of actual habits of the individuals. But as illustrated in the Simpson’s diagram it may have confounding variables that need to be controlled.This is where ingenious solutions developed in the causal community in the past few decades are making headway. Detailing them are beyond the scope of this post, but I briefly mention how to learn more at the end.To resolve for this Simpson’s paradox with the given observational data oneCalculates for each cohort the impact of the change of the treatment on the outcomeCalculates a weighted average contribution of each cohort on the population.Here we will focus on intuition, but in a future post we will describe the maths behind this solution.I am sure that many analysts, just like myself, have noticed Simpson’s at some stage in their data and hopefully have corrected for it. Now you know the name of this effect and hopefully start to appreciate how causal tools are useful.I’ll be the first to admit that I struggled to understand this concept and it took me three weekends of deep diving into examples to internalised it. This was the gateway drug to causality for me. Part of my process to understanding statistics is playing with data. For this purpose I created an interactive web application hosted in Streamlit which I call Simpson’s Calculator . I’ll write a separate post for this in the future.Even if you are confused the main takeaways of Simpson’s paradox is that:It is a situation where trends can exist in subgroups but reverse for the whole.It may be resolved by identifying confounding variables between the treatment and the outcome variables and controlling for them.This raises the question — should we just control for all variables except for the treatment and outcome? Let’s keep this in mind when resolving for the Berkson’s paradox.As in the previous section we are going to make clear statements about how we believe the data was generated and then draw these in a causal graph.Let’s examine the case of the general population, for convenience I’m copying the image from above:Talent is an independent variableAttractiveness is an independent variableA causal graph for this is quite simple, two nodes without an edge.Let’s examine the plot of the celebrity subset.The cheeky insight from this mock data is that the more likely one is attractive the less they need to be talented to be a celebrity. Hence we can deduce that:Talent is an independent variableAttractiveness is an independent variableCelebrity variable depends on both Talent and Attractiveness variables. (Imagine this variable is boolean as in: true for celebrities or false for not).Hence we can draw the causal graph as:By having arrows pointing into it Celebrity is a  node between Talent and Attractiveness.Berkson’s paradox is the fact that when controlling for celebrities we see an interesting trend (anti correlation between Attractiveness and Talent) not seen in the general population.This can be visualised in the causal graph that by confounding for the Celebrity parameter we are creating a spurious correlation between the otherwise independent variables Talent and Attractiveness. We can draw this as the following:The solution of this Berkson’s paradox should be apparent here: Talent and Attractiveness are independent variables in general, but by controlling for the collider Celebrity node causes a spurious correlation in the data.Let’s compare the resolution of both paradoxes:Resolving Simpson’s Paradox is by  for common cause (Gender)Resolving Berkson’s Paradox is by for the collider (Celebrity)The next figure combines both insights in the form of their causal graphs:The main takeaway from the resolution of these paradoxes is that controlling for parameters requires a justification. Common causes should be controlled for but colliders should not.Even though this is common knowledge for those who study causality (e.g, Economics majors), it is unfortunate that most analysts and machine learning practitioners are not aware of this (including myself in 2020 after over 15 years of analysis and predictive modelling experience).“Oddly, statisticians both over- and underrate the importance of confounders“The main takeaway from this post is that the story behind the data is as important as the data itself.Appreciating this will help you avoid result misinterpretation as spurious correlations and, as demonstrated here, in Simpson’s and Berskon’s paradoxes.Causal Graphs are an essential tool to visualise the story behind the data. By using them to solve for the paradoxes we learnt that controlling for variables requires justification (common causes , colliders ).For those interested in taking the next step in their causal journey I highly suggest mastering Simpson’s paradox. One great way is by playing with data. Feel free to do so with my interactive “Simpson-calculator” .Unless otherwise noted, all images were created by the author.Wondering what your next step should be in your causal journey? Check out my new article on mastering Simpson’s Paradox — you will never look at data the same way. Here I provide resources that I find useful as well as a shopping list of topics for beginners to learn.This list is far from comprehensive, but I’m glad to add to it if anyone has suggestions (please mention why the book stands out from the pack).Provides memorable examplesOne paid course  that is targeted to practitioners is Altdeep.This list is far from comprehensive because the space is rapidly growing:Here I highlight a list of topics which I would have found useful when I started my learnings in the field. If I’m missing anything I’d be more than glad to get feedback and adding. I bold face the ones which were briefly discussed here.Pearl’s Causal Hierarchy of seeing, doing and imagining (figure above)Observational data vs. Randomised Control Trialsd-separation, , , mediators, instrumental variablesAssumptions: Ignorability, SUTVA, Consistency, Positivity“Do” Algebra — assessing impact on cohorts by interventionCounterfactuals — assessing impact on individuals by comparing real outcomes to potential onesThe fundamental problem of causalityEstimand, Estimator, Estimate, Identifiability — relating causal definitions to observable statistics (e.g, conditional probabilities)Causal Discovery — finding causal graphs with data (e.g, Markov Equivalence)Causal Machine Learning (e.g, Double Machine Learning)For completeness it is useful to know that there are different streams of causality. Although there is a lot of overlap you may find that methods differ in naming convention due to development in different fields of research: Computer Science, Social Sciences, Health, EconomicsHere I used definitions mostly from the Pearlian perspective (as developed in the field of computer science).The Story Behind This PostThis narrative is a result of two study groups that I have conducted in a previous role to get myself and colleagues to learn about causality, which I felt missing in my skill set. If there is any interest I’m glad to write a post about the study group experience.This intro was created as the one I felt that I needed when I started my journey in causality.In the first iteration of this post I wrote and presented the limitations of spurious correlations and Simpson’s paradox. The main reason for this revision to focus on two paradoxes is that, whereas most causality intros focus on the limitations of correlations, I feel that understanding the concept of justification of confounders is important for all analysts and machine learning practitioners to be aware of.On September 5th 2024 I have presented this content in a contributed talk at the Royal Statistical Society Annual Conference in Brighton, England (abstract link).Unfortunately there is no recording but there are of previous talks of mine:The slides are available at bit.ly/start-ask-why. Presenting this material for the first time at PyData Global 2021]]></content:encoded></item><item><title>Dell Nears $5 Billion AI Server Deal for Elon Musk’s xAI</title><link>https://www.bnnbloomberg.ca/business/technology/2025/02/14/dell-nears-us5-billion-ai-server-deal-for-elon-musks-xai/</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 22:25:37 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[(Bloomberg) -- Dell Technologies Inc. is in advanced stages of securing a deal worth more than $5 billion to provide Elon Musk’s xAI with servers optimized for artificial intelligence work.The company will sell servers containing Nvidia Corp. GB200 semiconductors to Musk’s AI startup for delivery this year, according to people familiar with the matter, who asked to not to be named because the work is private. Some details are being finalized and still may change, some of the people added. Demand for computing to run AI workloads has led to a boom for makers of high-powered servers like Dell, Super Micro Computer Inc. and Hewlett Packard Enterprise Co. Musk’s companies, including carmaker Tesla Inc. and xAI, have emerged as major customers for the hardware. Dell and Nvidia declined to comment. xAI didn’t respond to a request for comment.Dell shares jumped as much as 6% to $116.88 Friday on the news before paring some gains. The stock had slipped 4.3% this year through Thursday’s close.A supercomputer project being built by xAI in Memphis has used a mix of Dell and Super Micro servers. In December, Dell said it had deployed tens of thousands of graphics processing units, or GPUs, there and was working to win an “unfair share” of the remaining build-out. GPUs are the key chips to power AI workloads and Nvidia is the top maker of those processing units.Analysts expect Dell will have shipped more than $10 billion of AI servers in the fiscal year ending last month and project that value will jump to $14 billion in the fiscal year ending in January 2026. Dell is scheduled to report fiscal fourth-quarter earnings on Feb. 27, with the AI server business a major focus for investors.The deal with xAI “would firmly establish the company as a leading AI-server provider and boost sales, though the impact on profitability is less clear,” wrote Woo Jin Ho, an analyst at Bloomberg Intelligence.AI startup xAI’s main product, a chatbot called Grok, has primarily been available to paying users of X, the social network formerly known as Twitter. Firms that Musk runs are known to share employees, technology and computing power.--With assistance from Ian King and Kurt Wagner.(Updates with comments from analyst in the eighth paragraph.)]]></content:encoded></item><item><title>Meta Plans Major Investment Into AI-Powered Humanoid Robots</title><link>https://www.bnnbloomberg.ca/business/technology/2025/02/14/meta-plans-major-investment-into-ai-powered-humanoid-robots/</link><author>/u/F0urLeafCl0ver</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 22:22:27 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[(Bloomberg) -- Meta Platforms Inc., after pushing into augmented reality and artificial intelligence, has identified its next big bet: AI-powered humanoid robots. The company is making a significant investment into the category — futuristic robots that can act like humans and assist with physical tasks — and is forming a new team within its Reality Labs hardware division to conduct the work, according to people with knowledge of the matter.Meta plans to work on its own humanoid robot hardware, with an initial focus on household chores. Its bigger ambition is to make the underlying AI, sensors and software for robots that will be manufactured and sold by a range of companies, said the people, who asked not to be identified because the initiative hasn’t been announced. Meta has started discussing its plan with robotics companies, including Unitree Robotics and Figure AI Inc. At least initially, it doesn’t plan to build a Meta-branded robot — something that could directly rival Tesla Inc.’s Optimus — but it may consider doing so in the future, the people added. The humanoid effort mirrors exploratory projects at other technology giants, including Apple Inc. and Alphabet Inc.’s Google Deepmind division. A Meta spokesperson declined to comment.Meta confirmed the creation of the new team to employees Friday, telling them it will be led by Marc Whitten, who resigned as chief executive officer of General Motors Co.’s Cruise self-driving car division earlier this month. He was previously an executive at gaming company Unity Software Inc. and Amazon.com Inc.“The core technologies we’ve already invested in and built across Reality Labs and AI are complementary to developing the advancements needed for robotics,” Andrew Bosworth, Meta’s chief technology officer, wrote in a memo reviewed by Bloomberg News. He mentioned the company’s advancements in hand tracking, computing at low bandwidth and always-on sensors.Meta executives believe that while humanoid robotics companies have made headway in hardware, Meta’s advances in artificial intelligence and data collected from augmented and virtual reality devices could accelerate progress in the nascent industry. Current humanoids are still not useful enough to fold clothes, carry a glass of water, place dishes in a rack for cleaning or conduct other home chores that could get consumers interested in the category. “We believe that expanding our portfolio to invest in this field will only accrue value to Meta AI and our mixed and augmented reality programs,” Bosworth wrote. Whitten, who will report to Bosworth, will have headcount to hire around 100 engineers this year, one of the people said.Meta’s goal is to provide what Google’s Android operating system and Qualcomm Inc.’s chips did for the phone industry by building a foundation for the rest of the market.The software, sensors and computing packages that Meta is already developing for its devices are the same technologies that are needed to power humanoids, according to one of the people involved in the project.Meta has been investing billions of dollars for years into its Reality Labs hardware division, which sells products like the Quest VR headset and the increasingly popular Ray-Ban smart glasses. Meta plans to spend $65 billion this year on related products, including artificial intelligence infrastructure and the new robot work.Tesla Inc. CEO Elon Musk has said that his company’s Optimus robot will eventually be sold to consumers and could cost around $30,000. Tesla is beginning limited production this year. Other businesses also have made headway. Boston Dynamics, for instance, has already brought products to market for automation in warehouses. Some companies are focused on selling to businesses and manufacturers, while Meta’s intention is to sell into homes.Humanoids are an evolution of the work companies have been doing in autonomous vehicles. They use similar underlying technologies and require large amounts of data and AI processing. But while the safety stakes are lower — roaming a person’s home instead of traveling 50 miles per hour on an open road — Meta executives believe humanoids are more challenging because every person’s home has a different layout, while city streets are fairly standardized.Meta will build some of its own hardware, use off-the-shelf components and work with existing manufacturers as soon as it can, said the people with knowledge of the project. They added that building prototypes and hardware is key for testing ahead of deploying a platform, even if Meta itself doesn’t ultimately release a branded product. The company is pitching its work as the platform of choice for robot development, one of the people said, adding that the goal was to make Meta’s Llama software a foundation for robotics researchers around the world.Meta will also seek to develop tools for robot safety, addressing possible dangers such as a person’s hand getting caught in an actuator or another part of a humanoid robot. There are also issues related to power safety, such as how a robot powers down or stops functioning mid-task if it runs out of power.While the official push into humanoid robots is new for Meta, the company’s FAIR, or Fundamental AI Research Group, has been exploring and publishing papers on robotics work for months. Apple recently started publishing AI papers related to robotics work as well.One person with knowledge of the project said that Meta believes humanoids are still a couple of years away from being widely available — and it could be years before the company’s platform is ready to underpin third-party products. But it will become a major focus for Meta and the broader tech industry, the person said.]]></content:encoded></item><item><title>An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.</title><link>https://v.redd.it/sglstazd96je1</link><author>/u/eternviking</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 21:24:03 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenAI: The Age of AI Is Here!</title><link>https://www.youtube.com/watch?v=97kQRYwL3P0</link><author>Two Minute Papers</author><category>dev</category><category>ai</category><enclosure url="https://www.youtube.com/v/97kQRYwL3P0?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 18:18:07 +0000</pubDate><source url="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Dev - Two Minute Papers</source><content:encoded><![CDATA[❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers

📝 The paper "Competitive Programming with Large Reasoning Models" is available here:
https://arxiv.org/abs/2502.06807

📝 My paper on simulations that look almost like reality is available for free here:
https://rdcu.be/cWPfD 

Or this is the orig. Nature Physics link with clickable citations:
https://www.nature.com/articles/s41567-022-01788-5

🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:
Benji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers

My research: https://cg.tuwien.ac.at/~zsolnai/
X/Twitter: https://twitter.com/twominutepapers
Thumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu]]></content:encoded></item><item><title>[P] GNNs for time series anomaly detection</title><link>https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/</link><author>/u/Important-Gear-325</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 17:56:59 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[For the past few months, my partner and I have been working on a project exploring the use of Graph Neural Networks (GNNs) for Time Series Anomaly Detection (TSAD). As we are near the completion of our work, I’d love to get feedback from this amazing community!Any comments, suggestions, or discussions are more than welcome! If you find the repo interesting, dropping a ⭐ would mean a lot. : )We're also planning to publish a detailed report with our findings and insights in the coming months, so stay tuned!The repo is still under development so don't be too harsh :)Looking forward to hearing your thoughts!]]></content:encoded></item><item><title>Roadmap to Becoming a Data Scientist, Part 4: Advanced Machine Learning</title><link>https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-4-advanced-machine-learning/</link><author>Vyacheslav Efimov</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 17:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Data science is undoubtedly one of the most fascinating fields today. Following significant breakthroughs in machine learning about a decade ago, data science has surged in popularity within the tech community. Each year, we witness increasingly powerful tools that once seemed unimaginable. Innovations such as the , , the Retrieval-Augmented Generation (RAG) framework, and state-of-the-art  — including  — have had a profound impact on our world.However, with the abundance of tools and the ongoing hype surrounding AI, it can be overwhelming — especially for beginners — to determine which skills to prioritize when aiming for a career in data science. Moreover, this field is highly demanding, requiring substantial dedication and perseverance.The first three parts of this series outlined the necessary skills to become a data scientist in three key areas: math, software engineering, and machine learning. While knowledge of classical Machine Learning and neural network algorithms is an excellent starting point for aspiring data specialists, there are still many important topics in machine learning that must be mastered to work on more advanced projects.This article will focus solely on the math skills necessary to start a career in Data Science. Whether pursuing this path is a worthwhile choice based on your background and other factors will be discussed in a separate article.The importance of learning evolution of methods in machine learningThe section below provides information about the evolution of methods in natural language processing (NLP).In contrast to previous articles in this series, I have decided to change the format in which I present the necessary skills for aspiring data scientists. Instead of directly listing specific competencies to develop and the motivation behind mastering them, I will briefly outline the most important approaches, presenting them in chronological order as they have been developed and used over the past decades in machine learning.The reason is that I believe it is crucial to study these algorithms from the very beginning. In machine learning, many new methods are built upon older approaches, which is especially true for NLP and computer vision.For example, jumping directly into the implementation details of modern large language models (LLMs) without any preliminary knowledge may make it very difficult for beginners to grasp the motivation and underlying ideas of specific mechanisms.Given this, in the next two sections, I will highlight in  the key concepts that should be studied.Natural language processing (NLP) is a broad field that focuses on processing textual information. Machine learning algorithms cannot work directly with raw text, which is why text is usually preprocessed and converted into numerical vectors that are then fed into neural networks.Before being converted into vectors, words undergo , which includes simple techniques such as , stemming, lemmatization, normalization, or removing . After preprocessing, the resulting text is encoded into . Tokens represent the smallest textual elements in a collection of documents. Generally, a token can be a part of a word, a sequence of symbols, or an individual symbol. Ultimately, tokens are converted into numerical vectors.The  method is the most basic way to encode tokens, focusing on counting the frequency of tokens in each document. However, in practice, this is usually not sufficient, as it is also necessary to account for token importance — a concept introduced in the  and  methods. While TF-IDF improves upon the naive counting approach of bag of words, researchers have developed a completely new approach called embeddings. are numerical vectors whose components preserve the semantic meanings of words. Because of this, embeddings play a crucial role in NLP, enabling input data to be trained or used for model inference. Additionally, embeddings can be used to compare text similarity, allowing for the retrieval of the most relevant documents from a collection.Embeddings can also be used to encode other unstructured data, including images, audio, and videos.As a field, NLP has been evolving rapidly over the last 10–20 years to efficiently solve various text-related problems. Complex tasks like text translation and text generation were initially addressed using recurrent neural networks (RNNs), which introduced the concept of memory, allowing neural networks to capture and retain key contextual information in long documents.Although RNN performance gradually improved, it remained suboptimal for certain tasks. Moreover, RNNs are relatively slow, and their sequential prediction process does not allow for parallelization during training and inference, making them less efficient.Additionally, the original Transformer architecture can be decomposed into two separate modules:  and . Both of these form the foundation of the most state-of-the-art models used today to solve various NLP problems. Understanding their principles is valuable knowledge that will help learners advance further when studying or working with other large language models (LLMs).When it comes to LLMs, I strongly recommend studying the evolution of at least the first three GPT models, as they have had a significant impact on the AI world we know today. In particular, I would like to highlight the concepts of  and , introduced in GPT-2, which enable LLMs to solve text generation tasks without explicitly receiving any training examples for them.Another important technique developed in recent years is retrieval-augmented generation (RAG). The main limitation of LLMs is that they are only aware of the context used during their training. As a result, they lack knowledge of any information beyond their training data.The retriever converts the input prompt into an embedding, which is then used to query a vector database. The database returns the most relevant context based on the similarity to the embedding. This retrieved context is then combined with the original prompt and passed to a generative model. The model processes both the initial prompt and the additional context to generate a more informed and contextually accurate response.A good example of this limitation is the first version of the ChatGPT model, which was trained on data up to the year 2022 and had no knowledge of events that occurred from 2023 onward.To address this limitation, OpenAI researchers developed a RAG pipeline, which includes a constantly updated database containing new information from external sources. When ChatGPT is given a task that requires external knowledge, it queries the database to retrieve the most relevant context and integrates it into the final prompt sent to the machine learning model.The goal of distillation is to create a smaller model that can imitate a larger one. In practice, this means that if a large model makes a prediction, the smaller model is expected to produce a similar result.In the modern era, LLM development has led to models with millions or even billions of parameters. As a consequence, the overall size of these models may exceed the hardware limitations of standard computers or small portable devices, which come with many constraints.Quantization is the process of reducing the memory required to store numerical values representing a model’s weights.This is where optimization techniques become particularly useful, allowing LLMs to be compressed without significantly compromising their performance. The most commonly used techniques today include ,, and .Pruning refers to discarding the least important weights of a model.Regardless of the area in which you wish to specialize, knowledge of  is a must-have skill! Fine-tuning is a powerful concept that allows you to efficiently adapt a pre-trained model to a new task.Fine-tuning is especially useful when working with very large models. For example, imagine you want to use BERT to perform semantic analysis on a specific dataset. While BERT is trained on general data, it might not fully understand the context of your dataset. At the same time, training BERT from scratch for your specific task would require a massive amount of resources.Here is where fine-tuning comes in: it involves taking a pre-trained BERT (or another model) and freezing some of its layers (usually those at the beginning). As a result, BERT is retrained, but this time only on the new dataset provided. Since BERT updates only a subset of its weights and the new dataset is likely much smaller than the original one BERT was trained on, fine-tuning becomes a very efficient technique for adapting BERT’s rich knowledge to a specific domain.Fine-tuning is widely used not only in NLP but also across many other domains.As the name suggests,  involves analyzing images and videos using machine learning. The most common tasks include image classification, object detection, image segmentation, and generation.Most CV algorithms are based on neural networks, so it is essential to understand how they work in detail. In particular, CV uses a special type of network called convolutional neural networks (CNNs). These are similar to fully connected networks, except that they typically begin with a set of specialized mathematical operations called .In simple terms, convolutions act as filters, enabling the model to extract the most important features from an image, which are then passed to fully connected layers for further analysis.The next step is to study the most popular CNN architectures for classification tasks, such as AlexNet, VGG, Inception, ImageNet, and .Speaking of the object detection task, the  algorithm is a clear winner. It is not necessary to study all of the dozens of versions of YOLO. In reality, going through the original paper of the first YOLO should be sufficient to understand how a relatively difficult problem like object detection is elegantly transformed into both classification and regression problems. This approach in YOLO also provides a nice intuition on how more complex CV tasks can be reformulated in simpler terms.While there are many architectures for performing image segmentation, I would strongly recommend learning about , which introduces an encoder-decoder architecture.Finally, image generation is probably one of the most challenging tasks in CV. Personally, I consider it an optional topic for learners, as it involves many advanced concepts. Nevertheless, gaining a high-level intuition of how generative adversial networks (GAN) function to generate images is a good way to broaden one’s horizons.In some problems, the training data might not be enough to build a performant model. In such cases, the data augmentation technique is commonly used. It involves the artificial generation of training data from already existing data (images). By feeding the model more diverse data, it becomes capable of learning and recognizing more patterns.It would be very hard to present in detail the Roadmaps for all existing machine learning domains in a single article. That is why, in this section, I would like to briefly list and explain some of the other most popular areas in data science worth exploring.First of all, recommender systems (RecSys) have gained a lot of popularity in recent years. They are increasingly implemented in online shops, social networks, and streaming services. The key idea of most algorithms is to take a large initial matrix of all users and items and decompose it into a product of several matrices in a way that associates every user and every item with a high-dimensional embedding. This approach is very flexible, as it then allows different types of comparison operations on embeddings to find the most relevant items for a given user. Moreover, it is much more rapid to perform analysis on small matrices rather than the original, which usually tends to have huge dimensions. often goes hand in hand with RecSys. When a RecSys has identified a set of the most relevant items for the user, ranking algorithms are used to sort them to determine the order in which they will be shown or proposed to the user. A good example of their usage is search engines, which filter query results from top to bottom on a web page.Closely related to ranking, there is also a  problem that aims to optimally map objects from two sets, A and B, in a way that, on average, every object pair is mapped “well” according to a matching criterion. A use case example might include distributing a group of students to different university disciplines, where the number of spots in each class is limited. is an unsupervised machine learning task whose objective is to split a dataset into several regions (clusters), with each dataset object belonging to one of these clusters. The splitting criteria can vary depending on the task. Clustering is useful because it allows for grouping similar objects together. Moreover, further analysis can be applied to treat objects in each cluster separately.The goal of clustering is to group dataset objects (on the left) into several categories (on the right) based on their similarity. is another unsupervised problem, where the goal is to compress an input dataset. When the dimensionality of the dataset is large, it takes more time and resources for machine learning algorithms to analyze it. By identifying and removing noisy dataset features or those that do not provide much valuable information, the data analysis process becomes considerably easier. is an area that focuses on designing algorithms and data structures (indexes) to optimize searches in a large database of embeddings (vector database). More precisely, given an input embedding and a vector database, the goal is to  find the most similar embedding in the database relative to the input embedding.The goal of similarity search is to approximately find the most similar embedding in a vector database relative to a query embedding.The word “approximately” means that the search is not guaranteed to be 100% precise. Nevertheless, this is the main idea behind similarity search algorithms — sacrificing a bit of accuracy in exchange for significant gains in prediction speed or data compression. involves studying the behavior of a target variable over time. This problem can be solved using classical tabular algorithms. However, the presence of time introduces new factors that cannot be captured by standard algorithms. For instance:the target variable can have an overall , where in the long term its values increase or decrease (e.g., the average yearly temperature rising due to global warming).the target variable can have a  which makes its values change based on the currently given period (e.g. temperature is lower in winter and higher in summer).Most of the time series models take both of these factors into account. In general, time series models are mainly used a lot in financial, stock or demographic analysis.Another advanced area I would recommend exploring is , which fundamentally changes the algorithm design compared to classical machine learning. In simple terms, its goal is to train an agent in an environment to make optimal decisions based on a reward system (also known as the “trial and error approach”). By taking an action, the agent receives a reward, which helps it understand whether the chosen action had a positive or negative effect. After that, the agent slightly adjusts its strategy, and the entire cycle repeats.Reinforcement learning is particularly popular in complex environments where classical algorithms are not capable of solving a problem. Given the complexity of reinforcement learning algorithms and the computational resources they require, this area is not yet fully mature, but it has high potential to gain even more popularity in the future.Currently the most popular applications are:. Existing approaches can design optimal game strategies and outperform humans. The most well-known examples are chess and Go.. Advanced algorithms can be incorporated into robots to help them move, carry objects or complete routine tasks at home.. Reinforcement learning methods can be developed to automatically drive cars, control helicopters or drones.This article was a logical continuation of the previous part and expanded the skill set needed to become a data scientist. While most of the mentioned topics require time to master, they can add significant value to your portfolio. This is especially true for the NLP and CV domains, which are in high demand today.After reaching a high level of expertise in data science, it is still crucial to stay motivated and consistently push yourself to learn new topics and explore emerging algorithms.Data science is a constantly evolving field, and in the coming years, we might witness the development of new state-of-the-art approaches that we could not have imagined in the past.All images are by the author unless noted otherwise.]]></content:encoded></item><item><title>Publish Interactive Data Visualizations for Free with Python and Marimo</title><link>https://towardsdatascience.com/publish-interactive-data-visualizations-for-free-with-python-and-marimo/</link><author>Sam Minot</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 16:00:00 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Working in Data Science, it can be hard to share insights from complex datasets using only static figures. All the facets that describe the shape and meaning of interesting data are not always captured in a handful of pre-generated figures. While we have powerful technologies available for presenting interactive figures — where a viewer can rotate, filter, zoom, and generally explore complex data  —  they always come with tradeoffs.Here I present my experience using a recently released Python library — marimo — which opens up exciting new opportunities for publishing interactive visualizations across the entire field of data science.Interactive Data VisualizationThe tradeoffs to consider when selecting an approach for presenting data visualizations can be broken into three categories: — what visualizations and interactivity am I able to present to the user? — what are the resources needed for displaying this visualization to users (e.g. running servers, hosting websites)? – how much of a new skillset / codebase do I need to learn upfront? is the foundation of portable interactivity. Every user has a web browser installed on their computer and there are many different frameworks available for displaying any degree of interactivity or visualization you might imagine (for example, this gallery of amazing things people have made with three.js). Since the application is running on the user’s computer, no costly servers are needed. However, a significant drawback for the data science community is ease of use, as JS does not have many of the high-level (i.e. easy-to-use) libraries that data scientists use for data manipulation, plotting, and interactivity. provides a useful point of comparison. Because of its continually growing popularity, some have called this the “Era of Python”. For data scientists in particular, Python stands alongside R as one of the foundational languages for quickly and effectively wielding complex data. While Python may be easier to use than Javascript, there are fewer options for presenting interactive visualizations. Some popular projects providing interactivity and visualization have been Flask, Dash, and Streamlit (also worth mentioning — bokeh, HoloViews, altair, and plotly). The biggest tradeoff for using Python has been the cost for publishing – delivering the tool to users. In the same way that shinyapps require a running computer to serve up the visualization, these Python-based frameworks have exclusively been server-based. This is by no means prohibitive for authors with a budget to spend, but it does limit the number of users who can take advantage of a particular project. is an intriguing middle ground — Python code running directly in the web browser using WebAssembly (WASM). There are resource limitations (only 1 thread and 2GB memory) that make this impractical for doing the heavy lifting of data science. , this can be more than sufficient for building visualizations and updating based on user input. Because it runs in the browser, no servers are required for hosting. Tools that use Pyodide as a foundation are interesting to explore because they give data scientists an opportunity to write Python code which runs directly on users’ computers without their having to install or run anything outside of the web browser.As an aside, I’ve been interested previously in one project that has tried this approach: stlite, an in-browser implementation of Streamlit that lets you deploy these flexible and powerful apps to a broad range of users. However, a core limitation is that Streamlit itself is distinct from stlite (the port of Streamlit to WASM), which means that not all features are supported and that advancement of the project is dependent on two separate groups working along compatible lines.The interface resembles a Jupyter , which will be familiar to users.Execution of cells is , so that updating one cell will rerun all cells which depend on its output. can be captured with a flexible set of UI components.Notebooks can be quickly converted into , hiding the code and showing only the input/output elements.Apps can be run locally or converted into using WASM/Pyodide.marimo balances the tradeoffs of technology in a way that is well suited to the skill set of the typical data scientists: — user input and visual display features are rather extensive, supporting user input via Altair and Plotly plots. — deploying as static webpages is basically free — no servers required — for users familiar with Python notebooks, marimo will feel very familiar and be easy to pick up.Publishing Marimo Apps on the WebAs a simple example of the type of display that can be useful in data science, consisting of explanatory text interspersed with interactive displays, I have created a barebones GitHub repository. Try it out yourself here.Using just a little bit of code, users can:Generate visualizations with flexible interactivityWrite narrative text describing their findingsPublish to the web for free (i.e. using GitHub Pages)Public App / Private DataThis new technology offers an exciting new opportunity for collaboration — publish the app publicly to the world, but users can only see specific datasets that they have permission to access.Rather than building a dedicated data backend for every app, user data can be stored in a generic backend which can be securely authenticated and accessed using a Python client library — all contained within the user’s web browser. For example, the user is given an OAuth login link that will authenticate them with the backend and allow the app to temporarily access input data.As a proof of concept, I built a simple visualization app which connects to the Cirro data platform, which is used at my institution to manage scientific data. Full disclosure: I was part of the team that built this platform before it spun out as an independent company. In this manner users can:Load the public visualization app — hosted on GitHub PagesConnect securely to their private data storeLoad the appropriate dataset for displayShare a link which will direct authorized collaborators to the same dataAs a data scientist, this approach of publishing free and open-source visualization apps which can be used to interact with private datasets is extremely exciting. Building and publishing a new app can take hours and days instead of weeks and years, letting researchers quickly share their insights with collaborators and then publish them to the wider world.]]></content:encoded></item><item><title>[R] Doing a PhD in Europe+UK</title><link>https://www.reddit.com/r/MachineLearning/comments/1ip9vuw/r_doing_a_phd_in_europeuk/</link><author>/u/No_Carpenter7252</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 12:51:57 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Hey I’m looking for a PhD for 2026 and I was wondering if some of you could recommend some labs. I want something ideally in RL, applied (so no bandits or full theoretical MDPs). It could be something like plasticity, lifelong/continual learning, better architecture/algo for RL, multi-agent or hierarchical RL, RL + LLMs, RL + diffusion, etc .. I’m also even fine with less RL and a bit more ML like better transformer architectures, state space models etc .. What I already had in mind was: - EPFL (LIONS, MLO)- Whiteson's lab at Oxford- Stefano Albrecht's lab in EdinburghI would really appreciate if you could help me extend my list, like this I would not miss labs when I will do my full research in reading their papers, checking what their PhDs, PostDocs and PIs are doing etc..Thank you so much in advance for your help!]]></content:encoded></item><item><title>[R] Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</title><link>https://www.reddit.com/r/MachineLearning/comments/1ip8lhf/r_scaling_up_testtime_compute_with_latent/</link><author>/u/hiskuu</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 11:32:58 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.This paper on reasoning in latent space at test time is fascinating. I think this approach is becoming a trend and could redefine how we think about reasoning in language models. META FAIR’s work on Large Concept Models also touched on latent reasoning.]]></content:encoded></item><item><title>We’re living in a new era of techno-feudalism</title><link>https://www.reddit.com/r/artificial/comments/1ip6vfh/were_living_in_a_new_era_of_technofeudalism/</link><author>/u/iamuyga</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 09:27:17 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[The tech broligarchs are the . The digital platforms they own are their “land.” They might project an image of free enterprise, but in practice, they often operate like autocrats within their domains.Meanwhile, ordinary users provide data, content, and often unpaid labour like reviews, social posts, and so on — much like  who work the land. We’re tied to these platforms because they’ve become almost indispensable in daily life.Smaller businesses and content creators function more like . They have some independence but must ultimately pledge loyalty to the platform, following its rules and parting with a share of their revenue just to stay afloat.Why on Earth would techno-feudal lords care about our well-being? Why would they bother introducing UBI or inviting us to benefit from new AI-driven healthcare breakthroughs? They’re only racing to gain even more power and profit. Meanwhile, the rest of us risk being left behind, facing unemployment and starvation.For anyone interested in exploring how these power dynamics mirror historical feudalism, and where AI might amplify them, here’s an article that dives deeper.]]></content:encoded></item><item><title>One-Minute Daily AI News 2/15/2025</title><link>https://www.reddit.com/r/artificial/comments/1ip3xhc/oneminute_daily_ai_news_2152025/</link><author>/u/Excellent-Target-847</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 05:55:00 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[   submitted by    /u/Excellent-Target-847 ]]></content:encoded></item><item><title>I told Claude AI that I’m alone on Valentine’s Day… and it did this.</title><link>https://www.reddit.com/r/artificial/comments/1ip2ab2/i_told_claude_ai_that_im_alone_on_valentines_day/</link><author>/u/snehens</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 04:15:54 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[What’s the funniest or most wholesome thing AI has ever done for you?Would you accept an AI-generated Valentine’s card?]]></content:encoded></item><item><title>This free European AI chatbot is 13 times faster than ChatGPT - Le Chat</title><link>https://www.msn.com/en-us/news/other/this-free-european-ai-chatbot-is-13-times-faster-than-chatgpt/ar-AA1yLbAf</link><author>/u/Bob_Spud</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 03:01:18 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Building a Data Engineering Center of Excellence</title><link>https://towardsdatascience.com/building-a-data-engineering-center-of-excellence/</link><author>Richie Bachala</author><category>dev</category><category>ai</category><pubDate>Fri, 14 Feb 2025 02:35:48 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[As data continues to grow in importance and become more complex, the need for skilled data engineers has never been greater. But what is data engineering, and why is it so important? In this blog post, we will discuss the essential components of a functioning data engineering practice and why data engineering is becoming increasingly critical for businesses today, and how you can build your very own Data Engineering Center of Excellence!I’ve had the privilege to build, manage, lead, and foster a sizeable high-performing team of data warehouse & ELT engineers for many years. With the help of my team, I have spent a considerable amount of time every year consciously planning and preparing to manage the growth of our data month-over-month and address the changing reporting and analytics needs for our 20000+ global data consumers. We built many data warehouses to store and centralize massive amounts of data generated from many OLTP sources. We’ve implemented Kimball methodology by creating star schemas both within our on-premise data warehouses and in the ones in the cloud.The objective is to enable our user-base to perform fast analytics and reporting on the data; so our analysts’ community and business users can make accurate data-driven decisions.It took me about three years to transform  () of data warehouse and ETL programmers into one cohesive Data Engineering team.I have compiled some of my learnings building a global data engineering team in this post in hopes that Data professionals and leaders of all levels of technical proficiency can benefit.Evolution of the Data EngineerIt has never been a better time to be a data engineer. Over the last decade, we have seen a massive awakening of enterprises now recognizing their data as the company’s heartbeat, making data engineering the job function that ensures accurate, current, and quality data flow to the solutions that depend on it.Historically, the role of Data Engineers has evolved from that of data warehouse developers and the  (extract, transform and load).The data warehouse developers are responsible for designing, building, developing, administering, and maintaining data warehouses to meet an enterprise’s reporting needs. This is done primarily via extracting data from operational and transactional systems and piping it using extract transform load methodology (ETL/ ELT) to a storage layer like a data warehouse or a data lake. The data warehouse or the data lake is where data analysts, data scientists, and business users consume data. The developers also perform transformations to conform the ingested data to a data model with aggregated data for easy analysis.A data engineer’s prime responsibility is to produce and make data securely available for multiple consumers.Data engineers oversee the ingestion, transformation, modeling, delivery, and movement of data through every part of an organization. Data extraction happens from many different data sources & applications. Data Engineers load the data into data warehouses and data lakes, which are transformed not just for the Data Science & predictive analytics initiatives (as everyone likes to talk about) but primarily for data analysts. Data analysts & data scientists perform operational reporting, exploratory analytics, service-level agreement (SLA) based business intelligence reports and dashboards on the catered data. In this book, we will address all of these job functions.The role of a data engineer is to acquire, store, and aggregate data from both cloud and on-premise, new, and existing systems, with data modeling and feasible data architecture. Without the data engineers, analysts and data scientists won’t have valuable data to work with, and hence, data engineers are the first to be hired at the inception of every new data team. Based on the data and analytics tools available within an enterprise, data engineering teams’ role profiles, constructs, and approaches have several options for what should be included in their responsibilities which we will discuss in this chapter.Software is increasingly automating the historically manual and tedious tasks of data engineers. Data processing tools and technologies have evolved massively over several years and will continue to grow. For example, cloud-based data warehouses (Snowflake, for instance) have made data storage and processing affordable and fast. Data pipeline services (like Informatica IICS, Apache Airflow, Matillion, Fivetran) have turned data extraction into work that can be completed quickly and efficiently. The data engineering team should be leveraging such technologies as force multipliers, taking a consistent and cohesive approach to integration and management of enterprise data, not just relying on legacy siloed approaches to building custom data pipelines with fragile, non-performant, hard to maintain code. Continuing with the latter approach will stifle the pace of innovation within the said enterprise and force the future focus to be around managing data infrastructure issues rather than how to help generate value for your business.The primary role of an enterprise Data Engineering team should be to  into a shape that’s ready for analysis — laying the foundation for real-world analytics and data science application.The Data Engineering team should serve as the  for enterprise-level data with the responsibility to curate the organization’s data and act as a resource for those who want to make use of it, such as Reporting & Analytics teams, Data Science teams, and other groups that are doing more self-service or business group driven analytics leveraging the enterprise data platform. This team should serve as the  of organizational knowledge, managing and refining the catalog so that analysis can be done more effectively. Let’s look at the essential responsibilities of a well-functioning Data Engineering team.Responsibilities of a Data Engineering TeamThe Data Engineering team should provide a  within the enterprise that cuts across to support both the Reporting/Analytics and Data Science capabilities to provide access to clean, transformed, formatted, scalable, and secure data ready for analysis. The Data Engineering teams’ core responsibilities should include:· Build, manage, and optimize the core data platform infrastructure· Build and maintain custom and off-the-shelf data integrations and ingestion pipelines from a variety of structured and unstructured sources· Manage overall data pipeline orchestration· Manage transformation of data either before or after load of raw data through both technical processes and business logic· Support analytics teams with design and performance optimizations of data warehousesData is an Enterprise Asset.Data as an Asset should be shared and protected.Data should be valued as an Enterprise asset, leveraged across all Business Units to enhance the company’s value to its respective customer base by accelerating decision making, and improving competitive advantage with the help of data. Good data stewardship, legal and regulatory requirements dictate that we protect the data owned from unauthorized access and disclosure.In other words, managing Security is a crucial responsibility.Why Create a Centralized Data Engineering Team?Treating Data Engineering as a standard and core capability that underpins both the Analytics and Data Science capabilities will help an enterprise evolve how to approach Data and Analytics. The enterprise needs to stop vertically treating data based on the technology stack involved as we tend to see often and move to more of a horizontal approach of managing a  or  that cuts across the organization and can connect to various technologies as needed drive analytic initiatives. This is a new way of thinking and working, but it can drive efficiency as the various data organizations look to scale. Additionally — there is value in creating a dedicated structure and career path for Data Engineering resources. Data engineering skill sets are in high demand in the market; therefore, hiring outside the company can be costly. Companies must enable programmers, database administrators, and software developers with a career path to gain the needed experience with the above-defined skillsets by working across technologies. Usually, forming a data engineering center of excellence or a capability center would be the first step for making such progression possible.Challenges for creating a centralized Data Engineering TeamThe centralization of the Data Engineering team as a service approach is different from how Reporting & Analytics and Data Science teams operate. It does, in principle, mean giving up some level of control of resources and establishing new processes for how these teams will collaborate and work together to deliver initiatives.The Data Engineering team will need to demonstrate that it can effectively support the needs of both Reporting & Analytics and Data Science teams, no matter how large these teams are. Data Engineering teams must effectively prioritize workloads while ensuring they can bring the right skillsets and experience to assigned projects.Data engineering is essential because it serves as the backbone of data-driven companies. It enables analysts to work with clean and well-organized data, necessary for deriving insights and making sound decisions. To build a functioning data engineering practice, you need the following critical components:The Data Engineering team should be a core capability within the enterprise, but it should effectively serve as a support function involved in almost everything data-related. It should interact with the Reporting and Analytics and Data Science teams in a collaborative support role to make the entire team successful.The Data Engineering team doesn’t create direct business value — but the value should come in making the Reporting and Analytics, and Data Science teams more productive and efficient to ensure delivery of maximum value to business stakeholders through Data & Analytics initiatives. To make that possible, the six key responsibilities within the data engineering capability center would be as follow –Let’s review the 6 pillars of responsibilities:1. Determine Central Data Location for Collation and WranglingUnderstanding and having a strategy for a (a centralized data repository or data warehouse for the mass consumption of data for analysis). Defining requisite data tables and where they will be joined in the context of data engineering and subsequently converting raw data into digestible and valuable formats.2. Data Ingestion and TransformationMoving data from one or more sources to a new destination (your data lake or cloud data warehouse) where it can be stored and further analyzed and then converting data from the format of the source system to that of the destinationExtracting, transforming, and loading data from one or more sources into a destination system to represent the data in a new context or style.Data modeling is an essential function of a data engineering team, granted not all data engineers excel with this capability. Formalizing relationships between data objects and business rules into a conceptual representation through understanding information system workflows, modeling required queries, designing tables, determining primary keys, and effectively utilizing data to create informed output.I’ve seen engineers in interviews mess up more with this than coding in technical discussions. It’s essential to understand the differences between Dimensions, Facts, Aggregate tables.Ensuring that sensitive data is protected and implementing proper authentication and authorization to reduce the risk of a data breach6. Architecture and AdministrationDefining the models, policies, and standards that administer what data is collected, where and how it is stored, and how it such data is integrated into various analytical systems.The six pillars of responsibilities for data engineering capabilities center on the ability to determine a central data location for collation and wrangling, ingest and transform data, execute ETL/ELT operations, model data, secure access and administer an architecture. While all companies have their own specific needs with regards to these functions, it is important to ensure that your team has the necessary skillset in order to build a foundation for big data success.Besides the Data Engineering following are the other capability centers that need to be considered within an enterprise:Analytics Capability CenterThe analytics capability center enables consistent, effective, and efficient BI, analytics, and advanced analytics capabilities across the company. Assist business functions in triaging, prioritizing, and achieving their objectives and goals through reporting, analytics, and dashboard solutions, while providing operational reports and visualizations, self-service analytics, and required tools to automate the generation of such insights.Data Science Capability CenterThe data science capability center is for exploring cutting-edge technologies and concepts to unlock new insights and opportunities, better inform employees and create a culture of prescriptive information usage using Automated AI and Automated ML solutions such as H2O.ai, Dataiku, Aible, DataRobot, C3.aiThe data governance office empowers users with trusted, understood, and timely data to drive effectiveness while keeping the integrity and sanctity of data in the right hands for mass consumption.As your company grows, you will want to make sure that the data engineering capabilities are in place to support the six pillars of responsibilities. By doing this, you will be able to ensure that all aspects of data management and analysis are covered and that your data is safe and accessible by those who need it. Have you started thinking about how your company will grow? What steps have you taken to put a centralized data engineering team in place?]]></content:encoded></item><item><title>[P]GPT-2 in Pure C(and full CUDA worklogs to come)</title><link>https://www.reddit.com/r/MachineLearning/comments/1ioybio/pgpt2_in_pure_cand_full_cuda_worklogs_to_come/</link><author>/u/atronos_kronios</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Fri, 14 Feb 2025 00:43:58 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Parallel computing is one of those things that sounds intimidating but is absolutely essential for the modern world. From high-frequency trading (HFT) to on-device AI, minimizing resources while maximizing performance is IMPORTANT and probably going to be the bottleneck as we move to better open-source LLMs. To dive headfirst into this space, I’ve started a project where I have implemented the GPT-2 architecture from scratch in plain, naive, and unoptimized(borderline stupid) C with no major dependency. Why? Because understanding a problem at its most fundamental level is the only way to optimize it effectively.Now, here’s the kicker: Learning CUDA is tricky. Most tutorials start with the basics (like optimizing matrix multiplications, then they might dive into a bit into basic operations/creating circle based renderers), but real production-level CUDA, like the kernels you’d see in George Hotz's TinyGrad or Karpathy’s llm.c or similar projects, is a whole different thing. There’s barely any structured resources to bridge that gap.So, my goal? ➡️ Start with this simple implementation and optimize step by step.➡️ Learn to build CUDA kernels from scratch, benchmark them, and compare them to other solutions.➡️ Return to this GPT-2 implementation, pick it apart piece by piece again, and see how much faster, leaner, and more efficient I can make it.And I’ll be documenting everything along the way with complete worklogs]]></content:encoded></item><item><title>[D] We built GenAI at Google and Apple, then left to build an open source AI lab, to enable the open community to collaborate and build the next DeepSeek. Ask us anything on Friday, Feb 14 from 9am-12pm PT!</title><link>https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/</link><author>/u/koukoumidis</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 23:53:27 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[TL;DR: Hi 👋 we’re Oumi, an AI lab that believes in an unconditionally open source approach–code, weights, training data, infrastructure, and collaboration—so the entire community can collectively push AI forward. We built a platform for anyone to contribute research in AI. Ask us anything about open source, scaling large models, DeepSeek, and what it takes to build frontier models, both inside and outside of big tech companies. Tell us what is working well in open source AI or what challenges you are facing. What should we work on together to improve AI in the open?For years, we worked at big tech (Google, Apple, Microsoft) leading efforts on GenAI models like Google Cloud PaLM, Gemini, and Apple’s health foundation models. We were working in silos and knew there had to be a better way to develop these models openly and collaboratively. So, we built a truly open source AI platform that makes it possible for tens of thousands of AI researchers, scientists, and developers around the world to collaborate, working together to advance frontier AI in a collective way that leads to more efficient, transparent and responsible development. The Oumi platform (fully open-source, Apache 2.0 license) supports pre-training, tuning, data curation/synthesis, evaluation, and any other common utility, in a fully recordable and reproducible fashion, while being easily customizable to support novel approaches.DeepSeek showed us what open source can achieve by leveraging open-weight models like LLaMA. But we believe AI should be even more open: not just the weights, but also the training data, and the code–make it ALL open. Then go even further: make it easy for anyone to access and experiment, make it easy for the community to work together and collaborate. Some resources about Oumi if you’re interested:If you want to collaborate and contribute to community research projects, regardless of where you get your compute, you can sign up at: https://oumi.ai/community. We will be starting with the post-training of existing open models, next, we will be collaboratively pursuing improvements to pre-training. We intend to publish the research with all contributors included as authors.We’re here to answer questions about our open source approach, scaling large models, DeepSeek, what it takes to build frontier models both inside and outside of big tech companies, and anything else you all want to discuss.We’ll be here Friday, February 14 from 9am-12pm PT / 12pm-3pm ET. Ask us anything.(u/koukoumidis)  - CEO and Co-founder, ex-Google (Cloud GenAI Lead)(u/oelachqar)  - Co-founder, Engineering, ex-Apple (Health foundation models)(u/MatthewPersons)  - Co-founder, Engineering, ex-Google (Cloud PaLM & NL Lead)(u/jeremy_oumi)  - Co-founder, Research, ex-Google (Gemini Alignment)]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 5: The Training</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-5-the-training/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 21:04:32 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[In this fifth part of my series, I will outline the steps for creating a Docker container for training your image classification model, evaluating performance, and preparing for deployment.AI/ML engineers would prefer to focus on model training and data engineering, but the reality is that we also need to understand the infrastructure and mechanics behind the scenes.I hope to share some tips, not only to get your training run running, but how to streamline the process in a cost efficient manner on cloud resources such as Kubernetes.I will reference elements from my previous articles for getting the best model performance, so be sure to check out Part 1 and Part 2 on the data sets, as well as Part 3 and Part 4 on model evaluation.Here are the learnings that I will share with you, once we lay the groundwork on the infrastructure:Building your Docker containerExecuting your training runFirst, let me provide a brief description of the setup that I created, specifically around Kubernetes. Your setup may be entirely different, and that is just fine. I simply want to set the stage on the infrastructure so that the rest of the discussion makes sense.This is a server you deploy that provides a user interface to for your subject matter experts to label and evaluate images for the image classification application. The server can run as a pod on your Kubernetes cluster, but you may find that running a dedicated server with faster disk may be better.Image files are stored in a directory structure like the following, which is self-documenting and easily modified.Image_Library/
  - cats/
    - image1001.png
  - dogs/
    - image2001.pngIdeally, these files would reside on local server storage (instead of cloud or cluster storage) for better performance. The reason for this will become clear as we see what happens as the image library grows.Cloud Storage allows for a virtually limitless and convenient way to share files between systems. In this case, the image library on your management system could access the same files as your Kubernetes cluster or Docker engine.However, the downside of cloud storage is the latency to open a file. Your image library will have  of images, and the latency to read each file will have a significant impact on your training run time. Longer training runs means more cost for using the expensive GPU processors!The way that I found to speed things up is to create a  file of your image library on your management system and copy them to cloud storage. Even better would be to create multiple tar files , each containing 10,000 to 20,000 images.This way you only have network latency on a handful of files (which contain thousands, once extracted) and you start your training run much sooner.Kubernetes or Docker engineA Kubernetes cluster, with proper configuration, will allow you to dynamically scale up/down nodes, so you can perform your model training on GPU hardware as needed. Kubernetes is a rather heavy setup, and there are other container engines that will work.The technology options change constantly!The main idea is that you want to spin up the resources you need — for only as long as you need them — then scale down to reduce your time (and therefore cost) of running expensive GPU resources.Once your GPU node is started and your Docker container is running, you can extract the  files above to  storage, such as an , on your node. The node typically has high-speed SSD disk, ideal for this type of workload. There is one caveat — the storage capacity on your node must be able to handle your image library.Assuming we are good, let’s talk about building your Docker container so that you can train your model on your image library.Building your Docker containerBeing able to execute a training run in a consistent manner lends itself perfectly to building a Docker container. You can “pin” the version of libraries so you know exactly how your scripts will run every time. You can version control your containers as well, and revert to a known good image in a pinch. What is really nice about Docker is you can run the container pretty much anywhere.The tradeoff when running in a container, especially with an Image Classification model, is the speed of file storage. You can attach any number of volumes to your container, but they are usually  attached, so there is latency on each file read. This may not be a problem if you have a small number of files. But when dealing with hundreds of thousands of files like image data, that latency adds up!This is why using the  file method outlined above can be beneficial.Also, keep in mind that Docker containers could be terminated unexpectedly, so you should make sure to store important information outside the container, on cloud storage or a database. I’ll show you how below.Knowing that you will need to run on GPU hardware (here I will assume Nvidia), be sure to select the right base image for your Dockerfile, such as  with the “devel flavor that will contain the right drivers.Next, you will add the script files to your container, along with a “batch” script to coordinate the execution. Here is an example Dockerfile, and then I’ll describe what each of the scripts will be doing.#####   Dockerfile   #####
FROM nvidia/cuda:12.8.0-devel-ubuntu24.04

# Install system software
RUN apt-get -y update && apg-get -y upgrade
RUN apt-get install -y python3-pip python3-dev

# Setup python
WORKDIR /app
COPY requirements.txt
RUN python3 -m pip install --upgrade pip
RUN python3 -m pip install -r requirements.txt

# Pythong and batch scripts
COPY ExtractImageLibrary.py .
COPY Training.py .
COPY Evaluation.py .
COPY ScorePerformance.py .
COPY ExportModel.py .
COPY BulkIdentification.py .
COPY BatchControl.sh .

# Allow for interactive shell
CMD tail -f /dev/nullDockerfiles are declarative, almost like a cookbook for building a small server — you know what you’ll get every time. Python libraries benefit, too, from this declarative approach. Here is a sample  file that loads the TensorFlow libraries with CUDA support for GPU acceleration.#####   requirements.txt   #####
numpy==1.26.3
pandas==2.1.4
scipy==1.11.4
keras==2.15.0
tensorflow[and-cuda]Extract Image Library scriptIn Kubernetes, the Docker container can access local, high speed storage on the physical node. This can be achieved via the  volume type. As mentioned before, this will only work if the local storage on your node can handle the size of your library.#####   sample 25GB emptyDir volume in Kubernetes   #####
containers:
  - name: training-container
    volumeMounts:
      - name: image-library
        mountPath: /mnt/image-library
volumes:
  - name: image-library
    emptyDir:
      sizeLimit: 25GiYou would want to have another  to your cloud storage where you have the  files. What this looks like will depend on your provider, or if you are using a persistent volume claim, so I won’t go into detail here.Now you can extract the  files — ideally in parallel for an added performance boost — to the local mount point.As AI/ML engineers, the model training is where we want to spend most of our time.This is where the magic happens!With your image library now extracted, we can create our train-validation-test sets, load a pre-trained model or build a new one, fit the model, and save the results.One key technique that has served me well is to load the most recently trained model as my base. I discuss this in more detail in Part 4 under “Fine tuning”, this results in faster training time and significantly improved model performance.Be sure to take advantage of the local storage to checkpoint your model during training since the models are quite large and you are paying for the GPU even while it sits idle writing to disk.This of course raises a concern about what happens if the Docker container dies part-way though the training. The risk is (hopefully) low from a cloud provider, and you may not want an incomplete training anyway. But if that does happen, you will at least want to understand , and this is where saving the main log file to cloud storage (described below) or to a package like MLflow comes in handy.After your training run has completed and you have taken proper precaution on saving your work, it is time to see how well it performed.Normally this evaluation script will pick up on the model that just finished. But you may decide to point it at a previous model version through an interactive session. This is why have the script as stand-alone.With it being a separate script, that means it will need to read the completed model from disk — ideally local disk for speed. I like having two separate scripts (training and evaluation), but you might find it better to combine these to avoid reloading the model.Now that the model is loaded, the evaluation script should generate predictions on  image in the training, validation, test, and benchmark sets. I save the results as a  matrix with the softmax confidence score for each class label. So, if there are 1,000 classes and 100,000 images, that’s a table with 100 million scores!I save these results in  files that are then used in the score generation next.Taking the matrix of scores produced by the evaluation script above, we can now create various metrics of model performance. Again, this process could be combined with the evaluation script above, but my preference is for independent scripts. For example, I might want to regenerate scores on previous training runs. See what works for you.Here are some of the  functions that produce useful insights like F1, log loss, AUC-ROC, Matthews correlation coefficient.from sklearn.metrics import average_precision_score, classification_report
from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_scoreAside from these basic statistical analyses for each dataset (train, validation, test, and benchmark), it is also useful to identify:Which  labels get the most number of errors?Which  labels get the most number of incorrect guesses?How many ground-truth-to-predicted label pairs are there? In other words, which classes are easily confused?What is the  when applying a minimum softmax confidence score threshold?What is the  above that softmax threshold?For the “difficult” benchmark sets, do you get a sufficiently  score?For the “out-of-scope” benchmark sets, do you get a sufficiently  score?As you can see, there are multiple calculations and it’s not easy to come up with a single evaluation to decide if the trained model is good enough to be moved to production.In fact, for an image classification model, it is helpful to manually review the images that the model got wrong, as well as the ones that got a low softmax confidence score. Use the scores from this script to create a list of images to manually review, and then get a  for how well the model performs.Check out Part 3 for more in-depth discussion on evaluation and scoring.All of the heavy lifting is done by this point. Since your Docker container will be shutdown soon, now is the time to copy the model artifacts to cloud storage and prepare them for being put to use.The example Python code snippet below is more geared to Keras and TensorFlow. This will take the trained model and export it as a . Later, I will show how this is used by TensorFlow Serving in the  section below.# Increment current version of model and create new directory
next_version_dir, version_number = create_new_version_folder()

# Copy model artifacts to the new directory
copy_model_artifacts(next_version_dir)

# Create the directory to save the model export
saved_model_dir = os.path.join(next_version_dir, str(version_number))

# Save the model export for use with TensorFlow Serving
tf.keras.backend.set_learning_phase(0)
model = tf.keras.models.load_model(keras_model_file)
tf.saved_model.save(model, export_dir=saved_model_dir)This script also copies the other training run artifacts such as the model evaluation results, score summaries, and log files generated from model training. Don’t forget about your label map so you can give human readable names to your classes!Bulk identification scriptYour training run is complete, your model has been scored, and a new version is exported and ready to be served. Now is the time to use this latest model to assist you on trying to identify unlabeled images.As I described in Part 4, you may have a collection of “unknowns” — really good pictures, but no idea what they are. Let your new model provide a best guess on these and record the results to a file or a database. Now you can create filters based on closest match and by high/low scores. This allows your subject matter experts to leverage these filters to find new image classes, add to existing classes, or to remove images that have very low scores and are no good.By the way, I put this step inside the GPU container since you may have thousands of “unknown” images to process and the accelerated hardware will make light work of it. However, if you are not in a hurry, you could perform this step on a separate CPU node, and shutdown your GPU node sooner to save cost. This would especially make sense if your “unknowns” folder is on slower cloud storage.All of the scripts described above perform a specific task — from extracting your image library, executing model training, performing evaluation and scoring, exporting the model artifacts for deployment, and perhaps even bulk identification.One script to rule them allTo coordinate the entire show, this batch script gives you the entry point for your container and an easy way to trigger everything. Be sure to produce a log file in case you need to analyze any failures along the way. Also, be sure to write the log to your cloud storage in case the container dies unexpectedly.#!/bin/bash
# Main batch control script

# Redirect standard output and standard error to a log file
exec > /cloud_storage/batch-logfile.txt 2>&1

/app/ExtractImageLibrary.py
/app/Training.py
/app/Evaluation.py
/app/ScorePerformance.py
/app/ExportModel.py
/app/BulkIdentification.pyExecuting your training runSo, now it’s time to put everything in motion…Let’s go through the steps to prepare your image library, fire up your Docker container to train your model, and then examine the results.Image library ‘tar’ filesYour image management system should now create a  file backup of your data. Since  is a single-threaded function, you will get significant speed improvement by creating multiple tar files in parallel, each with a portion of you data.Now these files can be copied to your shared cloud storage for the next step.All the hard work you put into creating your container (described above) will be put to the test. If you are running Kubernetes, you can create a Job that will execute the  script.Inside the Kubernetes Job definition, you can pass environment variables to adjust the execution of your script. For example, the batch size and number of epochs are set here and then pulled into your Python scripts, so you can alter the behavior without changing your code.#####   sample Job in Kubernetes   #####
containers:
  - name: training-job
    env:
      - name: BATCH_SIZE
        value: 50
      - name: NUM_EPOCHS
        value: 30
    command: ["/app/BatchControl.sh"]Once the Job is completed, be sure to verify that the GPU node properly scales back down to zero according to your scaling configuration in Kubernetes — you don’t want to be saddled with a huge bill over a simple configuration error.With the training run complete, you should now have model artifacts saved and can examine the performance. Look through the metrics, such as F1 and log loss, and benchmark accuracy for high softmax confidence scores.As mentioned earlier, the reports only tell part of the story. It is worth the time and effort to manually review the images that the model got wrong or where it produced a low confidence score.Don’t forget about the bulk identification. Be sure to leverage these to locate new images to fill out your data set, or to find new classes.Once you have reviewed your model performance and are satisfied with the results, it is time to modify your TensorFlow Serving container to put the new model into production.TensorFlow Serving is available as a Docker container and provides a very quick and convenient way to serve your model. This container can listen and respond to API calls for your model.Let’s say your new model is version 7, and your  script (see above) has saved the model in your cloud share as /image_application/models/007. You can start the TensorFlow Serving container with that volume mount. In this example, the  points to folder for version 007.#####   sample TensorFlow pod in Kubernetes   #####
containers:
  - name: tensorflow-serving
    image: bitnami/tensorflow-serving:2.18.0
    ports:
      - containerPort: 8501
    env:
      - name: TENSORFLOW_SERVING_MODEL_NAME
        value: "image_application"
    volumeMounts:
      - name: models-subfolder
        mountPath: "/bitnami/model-data"

volumes:
  - name: models-subfolder
    azureFile:
      shareName: "image_application/models/007"A subtle note here — the export script should create a sub-folder, named 007 (same as the base folder), with the saved model export. This may seem a little confusing, but TensorFlow Serving will mount this share folder as  and detect the numbered sub-folder inside it for the version to serve. This will allow you to query the API for the model version as well as the identification.As I mentioned at the start of this article, this setup has worked for my situation. This is certainly not the only way to approach this challenge, and I invite you to customize your own solution.I wanted to share my hard-fought learnings as I embraced cloud services in Kubernetes, with the desire to keep costs under control. Of course, doing all this while maintaining a high level of model performance is an added challenge, but one that you can achieve.I hope I have provided enough information here to help you with your own endeavors. Happy learnings!]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 3: The Evaluation</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 21:00:06 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[In this third part of my series, I will explore the evaluation process which is a critical piece that will lead to a cleaner data set and elevate your model performance. We will see the difference between evaluation of a  model (one not yet in production), and evaluation of a  model (one making real-world predictions).In Part 1, I discussed the process of labelling your image data that you use in your Image Classification project. I showed how to define “good” images and create sub-classes. In Part 2, I went over various data sets, beyond the usual train-validation-test sets, such as benchmark sets, plus how to handle synthetic data and duplicate images.Evaluation of the trained modelAs machine learning engineers we look at accuracy, F1, log loss, and other metrics to decide if a model is ready to move to production. These are all important measures, but from my experience, these scores can be deceiving especially as the number of classes grows.Although it can be time consuming, I find it very important to manually review the images that the model gets , as well as the images that the model gives a  softmax “confidence” score to. This means adding a step immediately after your training run completes to calculate scores for  images — training, validation, test, and the benchmark sets. You only need to bring up for manual review the ones that the model had problems with. This should only be a small percentage of the total number of images. See the Double-check process belowWhat you do during the manual evaluation is to put yourself in a “” to ensure that the labelling standards are being followed that you setup in Part 1. Ask yourself:“Is this a good image?” Is the subject front and center, and can you clearly see all the features?“Is this the correct label?” Don’t be surprised if you find wrong labels.You can either remove the bad images or fix the labels if they are wrong. Otherwise you can keep them in the data set and force the model to do better next time. Other questions I ask are:“Why did the model get this wrong?”“Why did this image get a low score?”“What is it about the image that caused confusion?”Sometimes the answer has nothing to do with  specific image. Frequently, it has to do with the  images, either in the ground truth class or in the predicted class. It is worth the effort to Double-check all images in both sets if you see a consistently bad guess. Again, don’t be surprised if you find poor images or wrong labels.When doing the evaluation of the trained model (above), we apply a lot of subjective analysis — “Why did the model get this wrong?” and “Is this a good image?” From these, you may only get a .Frequently, I will decide to hold off moving a model forward to production based on that gut feel. But how can you justify to your manager that you want to hit the brakes? This is where putting a more  analysis comes in by creating a weighted average of the softmax “confidence” scores.In order to apply a weighted evaluation, we need to identify sets of classes that deserve adjustments to the score. Here is where I create a list of “commonly confused” classes.Commonly confused classesCertain animals at our zoo can easily be mistaken. For example, African elephants and Asian elephants have different ear shapes. If your model gets these two mixed up, that is not as bad as guessing a giraffe! So perhaps you give partial credit here. You and your subject matter experts (SMEs) can come up with a list of these pairs and a weighted adjustment for each.This weight can be factored into a modified cross-entropy loss function in the equation below. The back half of this equation will reduce the impact of being wrong for specific pairs of ground truth and prediction by using the “weight” function as a lookup. By default, the weighted adjustment would be 1 for all pairings, and the commonly confused classes would get something like 0.5.In other words, it’s better to be unsure (have a  confidence score) when you are wrong, compared to being super confident and wrong.Once this weighted log loss is calculated, I can compare to previous training runs to see if the new model is ready for production.Confidence threshold reportAnother valuable measure that incorporates the confidence threshold (in my example, 95) is to report on accuracy and false positive rates. Recall that when we apply the confidence threshold before presenting results, we help reduce false positives from being shown to the end user.In this table, we look at the breakdown of “true positive above 95” for each data set. We get a sense that when a “good” picture comes through (like the ones from our train-validation-test set) it is very likely to surpass the threshold, thus the user is “happy” with the outcome. Conversely, the “false positive above 95” is extremely low for good pictures, thus only a small number of our users will be “sad” about the results.We expect the train-validation-test set results to be exceptional since our data is curated. So, as long as people take “good” pictures, the model should do very well. But to get a sense of how it does on extreme situations, let’s take a look at our benchmarks.The “difficult” benchmark has more modest true positive and false positive rates, which reflects the fact that the images are more challenging. These values are much easier to compare across training runs, so that lets me set a min/max target. So for example, if I target a minimum of 80% for true positive, and maximum of 5% for false positive on this benchmark, then I can feel confident moving this to production.The “out-of-scope” benchmark has no true positive rate because  of the images belong to any class the model can identify. Remember, we picked things like a bag of popcorn, etc., that are not zoo animals, so there cannot be any true positives. But we do get a false positive rate, which means the model gave a confident score to that bag of popcorn as some animal. And if we set a target maximum of 10% for this benchmark, then we may not want to move it to production.Right now, you may be thinking, “Well, what animal did it pick for the bag of popcorn?” Excellent question! Now you understand the importance of doing a manual review of the images that get bad results.Evaluation of the deployed modelThe evaluation that I described above applies to a model immediately after . Now, you want to evaluate how your model is doing in the . The process is similar, but requires you to shift to a “” and asking yourself, “Did the model get this correct?” and “Should it have gotten this correct?” and “Did we tell the user the right thing?”So, imagine that you are logging in for the morning — after sipping on your cold brew coffee, of course — and are presented with 500 images that your zoo guests took yesterday of different animals. Your job is to determine how satisfied the guests were using your model to identify the zoo animals.Using the softmax “confidence” score for each image, we have a threshold before presenting results. Above the threshold, we tell the guest what the model predicted. I’ll call this the “happy path”. And below the threshold is the “sad path” where we ask them to try again.Your review interface will first show you all the “happy path” images one at a time. This is where you ask yourself, “Did we get this right?” Hopefully, yes!But if not, this is where things get tricky. So now you have to ask, “Why not?” Here are some things that it could be:“Bad” picture — Poor lighting, bad angle, zoomed out, etc — refer to your labelling standards.Out-of-scope — It’s a zoo animal, but unfortunately one that isn’t found in  zoo. Maybe it belongs to another zoo (your guest likes to travel and try out your app). Consider adding these to your data set.Out-of-scope — It’s not a zoo animal. It could be an animal in your zoo, but not one typically  there, like a neighborhood sparrow or mallard duck. This might be a candidate to add.Out-of-scope — It’s something found in the zoo. A zoo usually has interesting trees and shrubs, so people might try to identify those. Another candidate to add.Prankster — Completely out-of-scope. Because people like to play with technology, there’s the possibility you have a prankster that took a picture of a bag of popcorn, or a soft drink cup, or even a selfie. These are hard to prevent, but hopefully get a low enough score (below the threshold) so the model did not identify it as a zoo animal. If you see enough pattern in these, consider creating a class with special handling on the front-end.After reviewing the “happy path” images, you move on to the “sad path” images — the ones that got a low confidence score and the app gave a “sorry, try again” message. This time you ask yourself, “ the model have given this image a higher score?” which would have put it in the “happy path”. If so, then you want to ensure these images are added to the training set so next time it will do better. But most of time, the low score reflects many of the “bad” or out-of-scope situations mentioned above.Perhaps your model performance is suffering and it has nothing to do with your model. Maybe it is the ways you users interacting with the app. Keep an eye out of non-technical problems and share your observations with the rest of your team. For example:Are your users using the application in the ways you expected?Are they not following the instructions?Do the instructions need to be stated more clearly?Is there anything you can do to improve the experience?Collect statistics and new imagesBoth of the manual evaluations above open a gold mine of data. So, be sure to collect these statistics and feed them into a dashboard — your manager and your future self will thank you!Keep track of these stats and generate reports that you and your can reference:How often the model is being called?What times of the day, what days of the week is it used?Are your system resources able to handle the peak load?What classes are the most common?After evaluation, what is the accuracy for each class?What is the breakdown for confidence scores?How many scores are above and below the confidence threshold?The single best thing you get from a deployed model is the additional real-world images! You can add these now images to improve coverage of your existing zoo animals. But more importantly, they provide you insight on  classes to add. For example, let’s say people enjoy taking a picture of the large walrus statue at the gate. Some of these may make sense to incorporate into your data set to provide a better user experience.Creating a new class, like the walrus statue, is not a huge effort, and it avoids the false positive responses. It would be more embarrassing to identify a walrus statue as an elephant! As for the prankster and the bag of popcorn, you can configure your front-end to quietly handle these. You might even get creative and have fun with it like, “Thank you for visiting the food court.”It is a good idea to double-check your image set when you suspect there may be problems with your data. I’m not suggesting a top-to-bottom check, because that would a monumental effort! Rather specific classes that you suspect could contain bad data that is degrading your model performance.Immediately after my training run completes, I have a script that will use this new model to generate predictions for my  data set. When this is complete, it will take the list of incorrect identifications, as well as the low scoring predictions, and automatically feed that list into the Double-check interface.This interface will show, one at a time, the image in question, alongside an example image of the ground truth and an example image of what the model predicted. I can visually compare the three, side-by-side. The first thing I do is ensure the original image is a “good” picture, following my labelling standards. Then I check if the ground-truth label is indeed correct, or if there is something that made the model think it was the predicted label.Remove the original image if the image quality is poor.Relabel the image if it belongs in a different class.During this manual evaluation, you might notice dozens of the same wrong prediction. Ask yourself why the model made this mistake when the images seem perfectly fine. The answer may be some incorrect labels on images in the ground truth, or even in the predicted class!Don’t hesitate to add those classes and sub-classes back into the Double-check interface and step through them all. You may have 100–200 pictures to review, but there is a good chance that one or two of the images will stand out as being the culprit.With a different mindset for a trained model versus a deployed model, we can now evaluate performances to decide which models are ready for production, and how well a production model is going to serve the public. This relies on a solid Double-check process and a critical eye on your data. And beyond the “gut feel” of your model, we can rely on the benchmark scores to support us.In Part 4, we kick off the training run, but there are some subtle techniques to get the most out of the process and even ways to leverage throw-away models to expand your library image data.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 1: The Data</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:55:53 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[It is said that in order for a machine learning model to be successful, you need to have good data. While this is true (and pretty much obvious), it is extremely difficult to define, build, and sustain good data. Let me share with you the unique processes that I have learned over several years building an ever-growing image classification system and how you can apply these techniques to your own application.With persistence and diligence, you can avoid the classic “garbage in, garbage out”, maximize your model accuracy, and demonstrate real business value.In this series of articles, I will dive into the care and feeding of a multi-class, single-label image classification app and what it takes to reach the highest level of performance. I won’t get into any coding or specific user interfaces, just the main concepts that you can incorporate to suit your needs with the tools at your disposal.Here is a brief description of the articles. You will notice that the model is last on the list since we need to focus on curating the data first and foremost:Over the past six years, I have been primarily focused on building and maintaining an image classification application for a manufacturing company. Back when I started, most of the software did not exist or was too expensive, so I created these from scratch. In this time, I have deployed two identifier applications, the largest handles 1,500 classes and achieves 97–98% accuracy.It was about eight years ago that I started online studies for Data Science and machine learning. So, when the exciting opportunity to create an AI application presented itself, I was prepared to build the tools I needed to leverage the latest advancements. I jumped in with both feet!I quickly found that building and deploying a model is probably the easiest part of the job. Feeding high quality data into the model is the best way to improve performance, and that requires focus and patience. Attention to detail is what I do best, so this was a perfect fit.It all starts with the dataI feel that so much attention is given to the model selection (deciding which neural network is best) and that the data is just an afterthought. I have found the hard way that even one or two pieces of bad data can significantly impact model performance, so that is where we need to focus.For example, let’s say you train the classic cat versus dog image classifier. You have 50 pictures of cats and 50 pictures of dogs, however one of the “cats” is clearly (objectively) a picture of a dog. The computer doesn’t have the luxury of ignoring the mislabelled image, and instead adjusts the model weights to make it fit. Square peg meets round hole.Another example would be a picture of a cat that climbed up into a tree. But when you take a wholistic view of it, you would describe it as a picture of a tree (first) with a cat (second). Again, the computer doesn’t know to ignore the big tree and focus on the cat — it will start to identify trees as cats, even if there is a dog. You can think of these pictures as outliers and should be removed.It doesn’t matter if you have the best neural network in the world, you can count on the model making poor predictions when it is trained on “bad” data. I’ve learned that any time I see the model make mistakes, it’s time to review the data.Example Application — Zoo animalsFor the rest of this write-up, I will use an example of identifying zoo animals. Let’s assume your goal is to create a mobile app where guests at the zoo can take pictures of the animals they see and have the app identify them. Specifically, this is a multi-class, single-label application. — There are a lot of different animals at the zoo and many of them look very similar. — Guests using the app don’t always take good pictures (zoomed out, blurry, too dark), so we don’t want to provide an answer if the image is poor. — The zoo keeps expanding and adding new species all the time. — Occasionally you might find that people take pictures of the sparrows near the food court grabbing some dropped popcorn. — Just for fun, guests may take a picture of the bag of popcorn just to see what it comes back with.These are all real challenges — being able to tell the subtle differences between animals, handling out-of-scope cases, and just plain poor images.Before we get there, let’s start from the beginning.There are a lot of tools these days to help you with this part of the process, but the challenge remains the same — collecting, labelling, and curating the data.Having data to collect is challenge #1. Without images, you have nothing to train. You may need to get creative on sourcing the data, or even creating synthetic data. More on that later.A quick note about image pre-processing. I convert all my images to the input size of my neural network and save them as PNG. Inside this square PNG, I preserve the aspect ratio of the original picture and fill the background black. I don’t stretch the image nor crop any features out. This also helps center the subject.Challenge #2 is to establish standards for data quality…and ensure that these standards are followed! These standards will guide you toward that “good” data. And this assumes, of course, correct labels. Having both is much easier said than done!I hope to show how “good” and “correct” actually go hand-in-hand, and how important it is to apply these standards to every image.First, I want to point out that the image data discussed here is for the training set. What qualifies as a good image for  is a bit different than what qualifies as a good image for . More on that in Part 3.So, what is “good” data when talking about images? “A picture is worth a thousand words”, and if the  you use to describe the picture do not include the subject you are trying to label, then it is not good and you need remove it from your training set.For example, let’s say you are shown a picture of a zebra and (removing bias toward your application) you describe it as an “open field with a zebra in the distance”. In other words, if “open field” is the first thing you notice, then you likely do  want to use that image. The opposite is also true — if the picture is way too close, you would described it as “zebra pattern”.What you want is a description like, “a zebra, front and center”. This would have your subject taking up about 80–90% of the total frame. Sometimes I will take the time to crop the original image so the subject is framed properly.Keep in mind the use of image augmentation at the time of training. Having that buffer around the edges will allow “zoom in” augmentation. And “zoom out” augmentation will simulate smaller subjects, so don’t start out less than 50% of the total frame for your subject since you lose detail.Another aspect of a “good” image relates to the label. If you can only see the back side of your zoo animal, can you really tell, for example, that it is a cheetah versus a leopard? The key identifying features need to be visible. If a human struggles to identify it, you can’t expect the computer to learn anything.What does a “bad” image look like? Here is what I frequently watch out for:Wide angle lens stretchingHigh contrast or dark shadows“Doctored” images, drawn lines and arrows“Unusual” angles or situationsPicture of a mobile device that has a picture of your subjectIf you have a team of subject matter experts (SMEs) on hand to label the images, you are in a good starting position. Animal trainers at the zoo know the various species, and can spot the differences between, for example, a chimpanzee and a bonobo.To a Machine Learning Engineer, it is easy for you to assume all labels from your SMEs are correct and move right on to training the model. However, even experts make mistakes, so if you can get a second opinion on the labels, your error rate should go down.In reality, it can be prohibitively expensive to get one, let alone two, subject matter experts to review image labels. The SME usually has years of experience that make them more valuable to the business in other areas of work. My experience is that the machine learning engineer (that’s you and me) becomes the second opinion, and often the first opinion as well.Over time, you can become pretty adept at labelling, but certainly not an SME. If you do have the luxury of access to an expert, explain to them the labelling standards and how these are required for the application to be successful. Emphasize “quality over quantity”.It goes without saying that having a  label is so important. However, all it takes is one or two mislabelled images to degrade performance. These can easily slip into your data set with careless or hasty labelling. So, take the time to get it right.Ultimately, we as the ML engineer are responsible for model performance. So, if we take the approach of only working on model training and deployment, we will find ourselves wondering why performance is falling short.A lot of times, you will come across a really good picture of a very interesting subject, but have no idea what it is! It would be a shame to simply dispose of it. What you can do is assign it a generic label, like “Unknown Bird” or “Random Plant” that are  included in your training set. Later in Part 4, you’ll see how to come back to these images at a later date when you have a better idea what they are, and you’ll be glad you saved them.If you have done any image labelling, then you know how time consuming and difficult it can be. But this is where having a model, even a less-than-perfect model, can help you.Typically, you have a large collection of unlabelled image and you need to go through them one at a time to assign labels. Simply having the model offer a best guess and display the top 3 results lets you step through each image in a matter of seconds!Even if the top 3 results are wrong, this can help you narrow down your search. Over time, newer models will get better, and the labelling process can even be somewhat fun!In Part 4, I will show how you can bulk identify images and take this to the next level for faster labelling.I mentioned the example above of two species that look very similar, the chimpanzee and the bonobo. When you start out building your data set, you may have very sparse coverage of one or both of these species. In machine learning terms, we these “classes”. One option is to roll with what you have and hope that the model picks up on the differences with only a handful of example images.The option that I have used is to merge two or more classes into one, at least temporarily. So, in this case I would create a class called “chimp-bonobo”, which is composed of the limited example pictures of chimpanzee and bonobo species classes. Combined, these may give me enough to train the model on “chimp-bonobo”, with the trade-off that it’s a more generic identification.Sub-classes can even be normal variations. For example,  pink flamingos are grey instead of pink. Or, male and female orangutans have distinct facial features. You wan to have a fairly balanced number of images for these normal variations, and keeping sub-classes will allow you to accomplish this.Don’t be concerned that you are merging completely different looking classes — the neural network does a nice job of applying the “OR” operator. This works both ways — it can help you identify male or female variations as one species, but it can hurt you when “bad” outlier images sneak in like the example “open field with a zebra in the distance.”Over time, you will (hopefully) be able to collect more images of the sub-classes and then be able to successfully split them apart (if necessary) and train the model to identify them separately. This process has worked very well for me. Just be sure to double-check all the images when you split them to ensure the labels didn’t get accidentally mixed up — it will be time well spent.All of this certainly depends on your user requirements, and you can handle this in different ways either by creating a unique class label like “chimp-bonobo”, or at the front-end presentation layer where you notify the user that you have intentionally merged these classes and provide guidance on further refining the results. Even after you decide to split the two classes, you may want to caution the user that the model could be wrong since the two classes are so similar.I realize this was a long write-up for something that on the surface seems intuitive, but these are all areas that I have tripped me up in the past because I didn’t give them enough attention. Once you have a solid understanding of these principles, you can go on to build a successful application.In Part 2, we will take the curated data we collected here to create the classic data sets, with a custom benchmark set that will further enhance your data. Then we will see how best to evaluate our trained model using a specific “training mindset”, and switch to a “production mindset” when evaluating a deployed model.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 4: The Model</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:53:42 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[In this latest part of my series, I will share what I have learned on selecting a model for Image Classification and how to fine tune that model. I will also show how you can leverage the model to accelerate your labelling process, and finally how to justify your efforts by generating usage and performance statistics.In Part 1, I discussed the process of labelling your image data that you use in your image classification project. I showed how define “good” images and create sub-classes. In Part 2, I went over various data sets, beyond the usual train-validation-test sets, with benchmark sets, plus how to handle synthetic data and duplicate images. In Part 3, I explained how to apply different evaluation criteria to a trained model versus a deployed model, and using benchmarks to determine when to deploy a model.So far I have focused a lot of time on labelling and curating the set of images, and also evaluating model performance, which is like putting the cart before the horse. I’m not trying to minimize what it takes to design a massive neural network — this is a very important part of the application you are building. In my case, I spent a few weeks experimenting with different available models before settling on one that fit the bill.Once you pick a model structure, you usually don’t make any major changes to it. For me, six years into deployment, I’m still using the same one. Specifically, I chose Inception V4 because it has a large input image size and an adequate number of layers to pick up on subtle image features. It also performs inference fast enough on CPU, so I don’t need to run expensive hardware to serve the model.Your mileage may vary. But again, the main takeaway is that focusing on your data will pay dividends versus searching for the best model.I will share a process that I found to work extremely well. Once I decided on the model to use, I randomly initialized the weights and let the model train for about 120 epoch before improvements plateau at a fairly modest accuracy, like 93%. At this point, I performed the evaluation of the trained model (see Part 3) to clean up the data set. I also incorporated new images as part of the data pipeline (see Part 1) and prepared the data sets for the next training run.Before starting the next training run, I simply take the last trained model, pop the output layer, and add it back in with random weights. Since the number of output classes are constantly increasing in my case, I have to pop that layer anyway to account for the new number of classes. Importantly, I leave the rest of the trained weights as they were and allow them to continue updating for the new classes.This allows the model to train much faster before improvements stall. After repeating this process dozens of times, the training reaches plateau after about 20 epochs, and the test accuracy can reach 99%! The model is building upon the low-level features that it established from the previous runs while re-learning the output weights to prevent overfitting.It took me a while to trust this process, and for a few years I would train from scratch every time. But after I attempted this and saw the training time (not to mention the cost of cloud GPU) go down while the accuracy continued to go up, I started to embrace the process. More importantly, I continue to see the evaluation metrics of the deployed model return solid performances.During training, you can apply transformations on your images (called “augmentation”) to give you more diversity from you data set. With our zoo animals, it is fairly safe to apply left-right flop, slight rotations clockwise and counterclockwise, and slight resize that will zoom in and out.With these transformations in mind, make sure your images are still able to act as good training images. In other words, an image where the subject is already small will be even smaller with a zoom out, so you probably want to discard the original. Also, some of your original pictures may need to be re-oriented by 90 degrees to be upright since a further rotation would make them look unusual.As I mentioned in Part 1, you can use the trained model to assist you in labelling images one at a time. But the way to take this even further is to have your newly trained model identify hundreds at a time while building a list of the results that you can then filter.Typically, we have large collections of  images that have come in either through regular usage of the application or some other means. Recall from Part 1 assigning “unknown” labels to interesting pictures but you have no clue what it is. By using the bulk identification method, we can sift through the collections quickly to target the labelling once we know what they are.By combining your current image counts with the bulk identification results, you can target classes that need expanded coverage. Here are a few ways you can leverage bulk identification:Increase low image counts — Some of your classes may have just barely made the cutoff to be included in the training set, which means you need more examples to improve coverage. Filter for images that have low counts.Replace staged or synthetic images — Some classes may be built entirely using non-real-world images. These pictures may be good enough to get started with, but may cause performance issues down the road because they look different than what typically comes through. Filter for classes that depend on staged images. — A class in your data set may look like another one. For example, let’s say your model can identify an antelope, and that looks like a gazelle which your model cannot identify yet. Setting a filter for antelope and a lower confidence score may reveal gazelle images that you can label. — You may not have known how to identify the dozens of cute wallaby pictures, so you saved them under “Unknown” because it was a good image. Now that you know what it is, you can filter for its look-alike kangaroo and quickly add a new class.Mass removal of low scores — As a way to clean out your large collection of unlabelled images that have nothing worth labelling, set a filter for lowest scores.Recall the decision I made to have image cutoffs from Part 2, which allows us to ensure an adequate number of example images of a class before we train and server a model to the public. The problem is that you may have a number of classes that are  below your cutoff (in my case, 40) and don’t make it into the model.The way I approach this is with a “throw-away” training run that I do not intend to move to production. I will decrease the lower cutoff from 40 to perhaps 35, build my train-validation-test sets, then train and evaluate like I normally do. The most important part of this is the bulk identification at the end!There is a chance that somewhere in the large collection of unlabelled images I will find the few that I need. Doing the bulk identification with this throw-away model helps find them.One very important aspect of any machine learning application is being able to show usage and performance reports. Your manager will likely want to see how many times the application is being used to justify the expense, and you as the ML engineer will want to see how the latest model is performing compared to the previous one.You should build logging into your model serving to record every transaction going through the system. Also, the manual evaluations from Part 3 should be recorded so you can report on performance for such things as accuracy over time, by model version, by confidence scores, by class, etc. You will be able to detect trends and make adjustments to improve the overall solution.There are a lot of reporting tools, so I won’t recommend one over the other. Just make sure you are collecting as much information as you can to build these dashboards. This will justify the time, effort, and cost associated with maintaining the application.We covered a lot of ground across this four-part series on building an image classification project and deploying it in the real world. It all starts with the data, and by investing the time and effort into maintaining the highest quality image library, you can reach impressive levels of model performance that will gain the trust and confidence of your business partners.As a Machine Learning Engineer, you are primarily responsible for building and deploying your model. But it doesn’t stop there — dive into the data. The more familiar you are with the data, the better you will understand the strengths and weaknesses of your model. Take a close look at the evaluations and use them as an opportunity to adjust the data set.I hope these articles have helped you find new ways to improve your own machine learning project. And by the way, don’t let the machine do all the learning — as humans, our job is to continue our own learning, so don’t ever stop!Thank you for taking this deep dive with me into a data-driven approach to model optimization. I look forward to your feedback and how you can apply this to your own application.]]></content:encoded></item><item><title>Learnings from a Machine Learning Engineer — Part 2: The Data Sets</title><link>https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/</link><author>David Martin</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 20:29:39 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[In Part 1, we discussed the importance of collecting good image data and assigning proper labels for your Image Classification project to be successful. Also, we talked about classes and sub-classes of your data. These may seem pretty straight forward concepts, but it’s important to have a solid understanding going forward. So, if you haven’t, please check it out.Now we will discuss how to build the various data sets and the techniques that have worked well for my application. Then in the next part, we will dive into the evaluation of your models, beyond simple accuracy.I will again use the example zoo animals image classification app.As machine learning engineers, we are all familiar with the train-validation-test sets, but when we include the concept of sub-classes discussed in Part 1, and incorporate to concepts discussed below to set a minimum and maximum image count per class, as well as staged and synthetic data to the mix, the process gets a bit more complicated. I had to create a custom script to handle these options.I will walk you through these concepts before we split the data for training: — Too few images and your model performance will suffer. Too many and you spend more time training than it’s worth. — Your model indicates how confident it is in the predictions. Let’s use that to decide when to present results to the user. — Real-world data is messy and the benchmark sets should reflect that. These need to stretch the model to the limit and help us decide when it is ready for production.Staged and synthetic data — Real-world data is king, but sometimes you need to produce the your own or even generate data to get off the ground. Be careful it doesn’t hurt performance. — Repeat data can skew your results and give you a false sense of performance. Make sure your data is diverse. — Combine sub-classes, apply cutoffs, and create your train-validation-test sets. Now we are ready to get the show started.In my experience, using a minimum of 40 images per class provides descent performance. Since I like to use 10% each for the test set and validation set, that means at least 4 images will be used to check the training set, which feels just barely adequate. Using fewer than 40 images per class, I notice my model evaluation tends to suffer.On the other end, I set a maximum of about 125 images per class. I have found that the performance gains tend to plateau beyond this, so having more data will slow down the training run with little to show for it. Having more than the maximum is fine, and these “overflow” can be added to the test set, so they don’t go to waste.There are times when I will drop the minimum cutoff to, say 35, with no intention of moving the trained model to production. Instead, the purpose is to leverage this throw-away model to find more images from my unlabelled set. This is a technique that I will go into more detail in Part 3.You are likely familiar with the softmax score. As a reminder, softmax is the probability assigned to each label. I like to think of it as a confidence score, and we are interested in the class that receives the highest confidence. Softmax is a value between zero and one, but I find it easier to interpret confidence scores between zero and 100, like a percentage.In order to decide if the model is confident enough with its prediction, I have chosen a threshold of 95. I use this threshold when determining if I want to present results to the user.Scores above the threshold have a better changes of being right, so I can confidently provide the results. Scores below the threshold may not be right — in fact it could be “out-of-scope”, meaning it’s something the model doesn’t know how to identify. So, instead of taking the risk of presenting incorrect results, I instead prompt the user to try again and offer suggestions on how to take a “good” picture.Admittedly this is somewhat arbitrary cutoff, and you should decide for your use-case what is appropriate. In fact, this score could probably be adjusted for each trained model, but this would make it harder to compare performance across models.I will refer to this confidence score frequently in the evaluations section in Part 3.Let me introduce what I call the benchmark sets, which you can think of as extended test sets. These are hand-picked images designed to stretch the limits of your model, and provide a measure for specific classes of your data. Use these benchmarks to justify moving your model to production, and for an objective measure to show to your manager. — These are the “extra credit” images, like the bonus questions a professor would add to the quiz to see which students are paying attention. You need a keen eye to spot the difference between the ground truth and a similar looking class. For example, a cheetah sleeping in the shade that could pass as a leopard if you don’t look closely. — These are the “trick question” images. Our model is trained on zoo animals, but people are known for not following the rules. For example, a zoo guest takes a picture of their child wearing cheetah face paint. — These are your “bread and butter” classes that need to get near perfect scores and zero errors. This would be a make-or-break benchmark for moving to production. — These are your “rare but exceptional” classes that again need to be correct, but reach a minimum score like the confidence threshold.When looking for images to add to the benchmarks, you can likely find them in real-world images from your deployed model. See the evaluation in Part 3.For each benchmark, calculate the min, max, median, and mean scores, and also how many images get scores above and below the confidence threshold. Now you can compare these measures against what is currently in production, and against your minimum requirements, to help decide if the new model is production worthy.Perhaps the biggest hurdle to any supervised machine learning application is having data to train the model. Clearly, “real-world” data that comes from actual users of the application is ideal. However you can’t really collect these until the model is deployed. Chicken and egg problem.One way to get started to is to have volunteers collect “staged” images for you, trying to act like real users. So, let’s have our zoo staff go around taking pictures of the animals. This is a good start, but there will be a certain level of bias introduced in these images. For example, the staff may take the photos over a few days, so you may not get the year-round weather conditions.Another way to get pictures is use computer-generated “synthetic” images. I would avoid these at all costs, to be honest. Based on my experience, the model struggles with these because they look…different. The lighting is not natural, the subject may superimposed on a background and so the edges look too sharp, etc. Granted, some of the AI generated images look very realistic, but if you look closely you may spot something unusual. The neural network in your model will notice these, so be careful.The way that I handle these staged or synthetic images is as a sub-class that gets merged into the training set, but only  giving preference to the real-world images. I cap the number of staged images to 60, so if I have 10 real-world, I now only need 50 staged. Eventually, these staged and synthetic images are phased out completely, and I rely entirely on real-world.One problem that can creep into your image set are duplicate images. These can be exact copies of pictures, or they can be extremely similar. You may think that this is harmless, but imagine having 100 pictures of an elephant that are exactly the same — your model will not know what to do with a different angle of the elephant.Now, let’s say you have only  pictures that are nearly the same. Not so bad, right? Well, here is what can happen to them:Both pictures go in the training set — The model doesn’t learn anything from the repeated image and it wastes time processing them.One goes into the training set, the other goes into the test set — Your test score will be higher, but it is not an accurate evaluation.Both are in the test set — Your test score will be compounded either higher or lower than it should be.None of these will help your model.There are a few ways to find duplicates. The approach I have taken is to calculate a hamming distance on all the pictures and identify the ones that are very close. I have an interface that displays the duplicates and I decide which one I like best, and remove the other.Another way (I haven’t tried this yet) is to create a vector representation of your images. Store these a vector database, and you can do a similarity search to find nearly identical images.Whatever method you use, it is important to clean up the duplicates.Now we are ready to build the traditional training, validation, and test sets. This is no longer a straight forward task since I want to:Merge sub-classes into a main class.Prioritize real-world images over staged or synthetic images.Apply a minimum number of images per class.Apply a maximum number of images per class, sending the “overflow” to the test set.This process is somewhat complicated and depends on how you manage your image library. First, I would recommend keeping your images in a folder structure that has sub-class folders. You can get image counts by using a script to simply read the folders. Second is to keep a configuration of how the sub-classes are merged. To really set yourself up for success, put these image counts and merge rules in a database for faster lookups.My train-validation-test set splits are usually 90–10–0. I originally started out using 80–10–10, but with diligence on keeping the entire data set clean, I noticed validation and test scores became pretty even. This allowed me to increase the training set size, and use “overflow” to become the test set, as well as using the benchmark sets.In this part, we’ve built our data sets by merging sub-classes and using the image count cutoffs. Plus we handle staged and synthetic data as well as cleaning up duplicate images. We also created benchmark sets and defined confidence thresholds, which help us decide when to move a model to production.In Part 3, we will discuss how we are going to evaluate the different model performances. And then finally we will get to the actual model training and the techniques to enhance accuracy.]]></content:encoded></item><item><title>Which LLMs are greedy and which are generous? In the public goods game, players donate tokens to a shared fund that gets multiplied and split equally, but each can profit by free-riding on others.</title><link>https://www.reddit.com/r/artificial/comments/1ios3df/which_llms_are_greedy_and_which_are_generous_in/</link><author>/u/zero0_one1</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 20:02:46 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] AlignRec Outperforms SOTA Models in Multimodal Recommendations</title><link>https://www.reddit.com/r/MachineLearning/comments/1ioo1ta/r_alignrec_outperforms_sota_models_in_multimodal/</link><author>/u/skeltzyboiii</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 17:13:07 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[AlignRec, introduced in AlignRec: Aligning and Training in Multimodal Recommendations (CIKM '24), tackles misalignment in multimodal recommendation systems. Traditional methods struggle to integrate diverse content types—text, images, and categorical IDs—due to semantic gaps. AlignRec addresses this by optimizing three alignment tasks: inter-content (ICA), content-category (CCA), and user-item (UIA). ICA unifies semantic representations with an attention-based encoder, CCA enhances feature alignment using contrastive learning, and UIA refines user-item representations via cosine similarity loss.A key innovation is AlignRec’s two-stage training: pre-training aligns visual and textual data, while fine-tuning incorporates user behavior for optimized recommendations. Tested on Amazon datasets, it outperforms nine SOTA models, excelling in long-tail recommendations. By bridging multimodal semantic gaps, AlignRec improves both accuracy and robustness, advancing multimodal AI-driven recommendations.]]></content:encoded></item><item><title>[D] How you do ML research from scratch?</title><link>https://www.reddit.com/r/MachineLearning/comments/1ion90w/d_how_you_do_ml_research_from_scratch/</link><author>/u/AntelopeWilling2928</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 16:40:09 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Someone who has published their works at top ML conferences (NIPS, ICML, ICLR) or domain oriented conferences (CVPR, ICCV, ACL, EMNLP, KDD, SIGIR). 1. How do you get from 0 to your first paper? 2. How much is your skill (Pytorch, or domain knowledge)? 3. What is the whole process that you follow to become good at implementing your ideas? 4. How do you come up with an idea and solution?]]></content:encoded></item><item><title>Python vs R for data science: Which should you choose?</title><link>https://www.datasciencecentral.com/python-vs-r-for-data-science-which-should-you-choose/</link><author>Mike Steven</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 15:37:52 +0000</pubDate><source url="https://www.datasciencecentral.com/">Dev - Data Science Central</source><content:encoded><![CDATA[Welcome to another comparison article where you will understand the features, intricacies, pros, and cons of two different stacks of the information technology industry. Today’s comparison blog is especially for data scientists who spend their day and night with datasets, insights, trends, and analysis of many other factors. From a long list of skills that… Read More »]]></content:encoded></item><item><title>[R] SWE-agent is the new open-source SOTA on SWE-bench Lite</title><link>https://www.reddit.com/r/MachineLearning/comments/1iolpvo/r_sweagent_is_the_new_opensource_sota_on_swebench/</link><author>/u/ofirpress</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 15:35:03 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[SWE-agent is an open source software engineering agent that works with any kind of model. Our 1.0 release adds tons of new features: massively parallel runs; cloud-based deployment; extensive configurability with tool bundles; new command line interface & utilities. Completely open-source (MIT), extensive configuration, easy to hack. Since it uses LiteLLM for LM interfacing, you can use it with a local LM: we've used it with Qwen and other community members have used it with Llama.SWE-agent is now powered by our new SWE-ReX package (also MIT licensed), a lightweight, general purpose sandboxed code execution engine that supports local Docker, AWS, Modal deployments https://github.com/SWE-agent/swe-rex. You can use it to easily build your own agent with code execution from scratch without the hassle of figuring out how to communicate with running docker containers!SWE-agent is developed by us at Princeton University & Stanford. We'll be here if you have any questions.]]></content:encoded></item><item><title>AI could be used for a &apos;bad biological attack from some evil person,&apos; ex-Google CEO Eric Schmidt warns</title><link>https://www.businessinsider.com/eric-schmidt-ai-risks-biological-attacks-rogue-states-2025-2</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 14:54:19 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[Former Google CEO Eric Schmidt said AI posed an "extreme risk" in some scenarios.He told BBC News the technology could be used in "a bad biological attack from some evil person."World leaders discussed the risks and opportunities posed by AI at a summit in Paris this week.Former Google CEO Eric Schmidt warned of the potential threat posed by AI in the hands of hostile states or terrorists and said it presented an "extreme risk" in some scenarios."Think about North Korea, or Iran, or even Russia, who have some evil goal. This technology is fast enough for them to adopt that they could misuse it and do real harm," he told BBC News, pointing to the risk of weapons being developed for "a bad biological attack .""I always worry about the 'Osama bin Laden' scenario, where you have a really evil person who takes over some aspect of our modern life and uses it to harm innocent people," Schmidt said.He was referring to the 9/11 terrorist attacks in the US, where terrorists from Osama bin Laden's Al Qaeda organization hijacked planes and flew them into buildings.Schmidt, who was Google CEO for a decade until 2011, has criticized European AI laws as being too strict, but in the interview, said that it was important for governments to regulate AI."It's really important that governments understand what we're doing and keep their eye on us," he said of private sector firms developing the tech.Schmidt said he backed the Biden administration's restriction of sales of microchips that power AI to all but 18 countries not deemed to pose a threat.It's not the first time Schmidt has warned of the dangers posed by AI. In December he said humans needed to have meaningful control of AI when it's used in military drones.His startup, White Stork, is developing drones for Ukraine to use in its war with Russia.At the summit, Schmidt also addressed China's rise as an AI tech power, telling The Financial Times the West needs to invest in open source AI models to keep pace."If we don't do something about that, China will ultimately become the open-source leader and the rest of the world will become closed-source," Schmidt said.It comes after Chinese firm DeepSeek in January released an AI model developed more cheaply than US rivals such as ChatGPT, roiling stock markets. OpenAI, the company behind ChatGPT, has shifted to a closed-source approach.]]></content:encoded></item><item><title>There are only 7 American competitive coders rated higher than o3</title><link>https://www.reddit.com/r/artificial/comments/1iokrx9/there_are_only_7_american_competitive_coders/</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 14:53:21 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Python vs R for data science: Which should you choose?</title><link>https://www.datasciencecentral.com/python-vs-r-for-data-science-which-should-you-choose/</link><author>Scott Thompson</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 13:48:00 +0000</pubDate><source url="https://www.datasciencecentral.com/">Dev - Data Science Central</source><content:encoded><![CDATA[Welcome to another comparison article where you will understand the features, intricacies, pros, and cons of two different stacks of the information technology industry. Today’s comparison blog is especially for data scientists who spend their day and night with datasets, insights, trends, and analysis of many other factors. From a long list of skills that… Read More »]]></content:encoded></item><item><title>[R] Text-to-SQL in Enterprises: Comparing approaches and what worked for us</title><link>https://www.reddit.com/r/MachineLearning/comments/1iojc1f/r_texttosql_in_enterprises_comparing_approaches/</link><author>/u/SirComprehensive7453</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 13:44:21 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Text-to-SQL is a popular GenAI use case, and we recently worked on it with some enterprises. Sharing our learnings here!These enterprises had already tried different approaches—prompting the best LLMs like O1, using RAG with general-purpose LLMs like GPT-4o, and even agent-based methods using AutoGen and Crew. But they hit a ceiling at 85% accuracy, faced response times of over 20 seconds (mainly due to errors from misnamed columns), and dealt with complex engineering that made scaling hard.We found that fine-tuning open-weight LLMs on business-specific query-SQL pairs gave 95% accuracy, reduced response times to under 7 seconds (by eliminating failure recovery), and simplified engineering. These customized LLMs retained domain memory, leading to much better performance.We put together a comparison of all tried approaches on medium. Let me know your thoughts and if you see better ways to approach this.]]></content:encoded></item><item><title>Sam Altman Just Revealed OpenAI’s Master Plan!</title><link>https://www.reddit.com/r/artificial/comments/1ioce7o/sam_altman_just_revealed_openais_master_plan/</link><author>/u/snehens</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Thu, 13 Feb 2025 05:50:53 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Should Data Scientists Care About Quantum Computing?</title><link>https://towardsdatascience.com/should-data-scientists-care-about-quantum-computing/</link><author>Sara A. Metwalli</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 04:06:27 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[I am sure the quantum hype has reached every person in tech (and outside it, most probably). With some over-the-top claims, like “some company has proved quantum supremacy,” “the quantum revolution is here,” or my favorite, “quantum computers are here, and it will make classical computers obsolete.” I am going to be honest with you; most of these claims are intended as a marketing exaggeration, but I am entirely certain that many people believe that they are true. The issue here is not whether or not these claims are accurate, but, as ML and AI professionals who need to keep up with what’s happening in the tech field, should you, if at all, care about quantum computing? Because I am an engineer first before a quantum computing researcher, I thought to write this article to give everyone in data science an estimate of how much they should really care about quantum computing. Now, I understand that some ML and AI professionals are quantum enthusiasts and would like to learn more about quantum, regardless of whether or not they will use it in their daily job roles. At the same time, others are just curious about the field and want to be able to distinguish the actual progress from the hype. My intention in writing this article is to give a somewhat lengthy answer to two questions: Should data scientists care about quantum? And how much should you care? Before I answer, I should emphasize that 2025 is the year of quantum information science, and so there will be a lot of hype everywhere; it is the best time to take a second as a person in tech or a tech enthusiast, to know some basics about the field so you can definitively know when something is pure hype or if it has hints of facts. Now that we set the pace, let’s jump into the first question: Should data scientists care about quantum computing? Here is the short answer, . The answer is that, although the current state of quantum computers is not optimal for building real-life applications, there is no minimal overlap between quantum computing and data science. That is, data science can aid in advancing quantum technology faster, and once we have better quantum computers, they will help make various data science applications more efficient. The Intersection of Quantum Computing and Data Science First, let’s discuss how data science, namely AI, helps advance quantum computing, and then we will talk about how quantum computing can enhance data science workflows. How can AI help advance quantum computing? AI can help quantum computing in several ways, from hardware to optimization, algorithm development, and error mitigation. On the hardware side, AI can help in: Optimizing circuits by minimizing gate counts, choosing efficient decompositions, and mapping circuits to hardware-specific constraints. Optimizing control pulses to improve gate fidelity on real quantum processors. Analyzing experimental data on qubit calibration to reduce noise and improve performance. Beyond the hardware, AI can help improve quantum algorithm design and implementation and aid in error correction and mitigation, for example: We can use AI to interpret results from quantum computations and design better feature maps for quantum Machine Learning (QML), which I will address in a future article. AI can analyze quantum system noise and predict which errors are most likely to occur. We can also use different AI algorithms to adapt quantum circuits to noisy processors by selecting the best qubit layouts and error mitigation techniques. Also, one of the most interesting applications that includes three advanced technologies is using AI on HPC (high-performance computing, or supercomputers, in short) to optimize and simulate quantum algorithms and circuits efficiently.How can quantum optimize data science workflows? Okay, now that we have addressed some of the ways that AI can help take quantum technology to the next level, we can now address how quantum can help optimize data science workflows. Before we dive in, let me remind you that quantum computers are (or will be) very good at optimization problems. Based on that, we can say that some areas where quantum will help are: Solving complex optimization tasks faster, like supply chain problems. Quantum Computing has the potential to process and analyze massive datasets exponentially faster (once we reach better quantum computers with lower error rates). Quantum Machine Learning (QML) algorithms will lead to faster training and improved models. Examples of QML algorithms that are currently being developed and tested are: Quantum support vector machines (QSVMs). Quantum neural networks (QNNs). Quantum principal component analysis (QPCA). We already know that quantum computers are different because of how they work. They will help classical computers by addressing the challenges of scaling algorithms to process large datasets faster. Address some NP-hard problems and bottlenecks in training deep learning models. Okay, first, thank you for making it this far with me in this article; you might be thinking now, “All of that is nice and cool, but you still haven’t answered why should I *a data scientist* care about quantum?” You are right; to answer this, let me put my marketing hat on! The way I describe quantum computing now is machine learning and AI algorithms from the 1970s and 1980s. We had ML and AI algorithms but not the hardware needed to utilize them fully! Being an early contributor to new Technology means you get to be one of the people who help shape the future of the field. Today, the quantum field needs more quantum-aware data scientists in finance, healthcare, and tech industries to help move the field forward. So far, physicists and mathematicians have controlled the field, but we can’t move forward without engineers and data scientists now.The interesting part is that advancing the field from this point doesn’t always mean you need to have all the knowledge and understanding of quantum physics and mechanics, but rather how to use what you already know (aka ML and AI) to move the technology further. One of the critical steps of any new technology is what I like to think of as the “last hurdle before the breakthrough.” All new technologies faced pushback or hurdles before they proved helpful, and their use exploded. It is often difficult to pinpoint that last hurdle, and as a person in tech, I am fully aware of how many new things keep popping up daily. It is humanly impossible to keep up with all new advances in technology in all fields! That is a full-time job by itself. That being said, it is always an advantage to be ahead of the demand when it comes to new technology. As in, be in a field before it becomes “cool.” By no means am I telling data scientists to quit their field and jump on the quantum hype train, but I hope this article helps you decide how much or little involvement you, as an ML or AI professional, would want to have with quantum computing. So, should ML and AI professionals care about quantum? Only enough to be able to decide how it can affect/ help with their career progress.]]></content:encoded></item><item><title>Method of Moments Estimation with Python Code</title><link>https://towardsdatascience.com/method-of-moments-estimation-with-python-code/</link><author>Mahmoud Abdelaziz</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 03:53:40 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Let’s say you are in a customer care center, and you would like to know the probability distribution of the number of calls per minute, or in other words, you want to answer the question: what is the probability of receiving zero, one, two, … etc., calls per minute? You need this distribution in order to predict the probability of receiving different number of calls based on which you can plan how many employees are needed, whether or not an expansion is required, etc.In order to let our decision ‘data informed’ we start by collecting data from which we try to infer this distribution, or in other words, we want to generalize from the sample data to the unseen data which is also known as the population in statistical terms. This is the essence of statistical inference.From the collected data we can compute the relative frequency of each value of calls per minute. For example, if the collected data over time looks something like this: 2, 2, 3, 5, 4, 5, 5, 3, 6, 3, 4, … etc. This data is obtained by counting the number of calls received every minute. In order to compute the relative frequency of each value you can count the number of occurrences of each value divided by the total number of occurrences. This way you will end up with something like the grey curve in the below figure, which is equivalent to the histogram of the data in this example.Another option is to assume that each data point from our data is a realization of a random variable (X) that follows a certain probability distribution. This probability distribution represents all the possible values that are generated if we were to collect this data long into the future, or in other words, we can say that it represents the population from which our sample data was collected. Furthermore, we can assume that all the data points come from the same probability distribution, i.e., the data points are identically distributed. Moreover, we assume that the data points are independent, i.e., the value of one data point in the sample is not affected by the values of the other data points. The independence and identical distribution (iid) assumption of the sample data points allows us to proceed mathematically with our statistical inference problem in a systematic and straightforward way. In more formal terms, we assume that a generative probabilistic model is responsible for generating the iid data as shown below.In this particular example, a Poisson distribution with mean value λ = 5 is assumed to have generated the data as shown in the blue curve in the below figure. In other words, we assume here that we know the true value of λ which is generally not known and needs to be estimated from the data.As opposed to the previous method in which we had to compute the relative frequency of each value of calls per minute (e.g., 12 values to be estimated in this example as shown in the grey figure above), now we only have one parameter that we aim at finding which is λ. Another advantage of this generative model approach is that it is better in terms of generalization from sample to population. The assumed probability distribution can be said to have summarized the data in an elegant way that follows the Occam’s razor principle.Before proceeding further into how we aim at finding this parameter λ, let’s show some Python code first that was used to generate the above figure.# Import the Python libraries that we will need in this article
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import math
from scipy import stats

# Poisson distribution example
lambda_ = 5
sample_size = 1000
data_poisson = stats.poisson.rvs(lambda_,size= sample_size) # generate data

# Plot the data histogram vs the PMF
x1 = np.arange(data_poisson.min(), data_poisson.max(), 1)
fig1, ax = plt.subplots()
plt.bar(x1, stats.poisson.pmf(x1,lambda_),
        label="Possion distribution (PMF)",color = BLUE2,linewidth=3.0,width=0.3,zorder=2)
ax.hist(data_poisson, bins=x1.size, density=True, label="Data histogram",color = GRAY9, width=1,zorder=1,align='left')

ax.set_title("Data histogram vs. Poisson true distribution", fontsize=14, loc='left')
ax.set_xlabel('Data value')
ax.set_ylabel('Probability')
ax.legend()
plt.savefig("Possion_hist_PMF.png", format="png", dpi=800)Our problem now is about estimating the value of the unknown parameter λ using the data we collected. This is where we will use the approach that appears in the title of this article.First, we need to define what is meant by the moment of a random variable. Mathematically, the kth moment of a discrete random variable (X) is defined as follows:Take the first moment E(X) as an example, which is also the mean μ of the random variable, and assuming that we collect our data which is modeled as N iid realizations of the random variable X. A reasonable estimate of μ is the sample mean which is defined as follows:Thus, in order to obtain a MoM estimate of a model parameter that parametrizes the probability distribution of the random variable X, we first write the unknown parameter as a function of one or more of the kth moments of the random variable, then we replace the kth moment with its sample estimate. The more unknown parameters we have in our models, the more moments we need.In our Poisson model example, this is very simple as shown below.In the next part, we test our MoM estimator on the simulated data we had earlier. The Python code for obtaining the estimator and plotting the corresponding probability distribution using the estimated parameter is shown below.# Method of moments estimator using the data (Poisson Dist)
lambda_hat = sum(data_poisson) / len(data_poisson)

# Plot the MoM estimated PMF vs the true PMF
x1 = np.arange(data_poisson.min(), data_poisson.max(), 1)
fig2, ax = plt.subplots()
plt.bar(x1, stats.poisson.pmf(x1,lambda_hat),
        label="Estimated PMF",color = ORANGE1,linewidth=3.0,width=0.3)
plt.bar(x1+0.3, stats.poisson.pmf(x1,lambda_),
        label="True PMF",color = BLUE2,linewidth=3.0,width=0.3)

ax.set_title("Estimated Poisson distribution vs. true distribution", fontsize=14, loc='left')
ax.set_xlabel('Data value')
ax.set_ylabel('Probability')
ax.legend()
#ax.grid()
plt.savefig("Possion_true_vs_est.png", format="png", dpi=800)The below figure shows the estimated distribution versus the true distribution. The distributions are quite close indicating that the MoM estimator is a reasonable estimator for our problem. In fact, replacing expectations with averages in the MoM estimator implies that the estimator is a consistent estimator by the law of large numbers, which is a good justification for using such estimator.Another MoM estimation example is shown below assuming the iid data is generated by a normal distribution with mean μ and variance σ² as shown below.In this particular example, a Gaussian (normal) distribution with mean value μ = 10 and σ = 2 is assumed to have generated the data. The histogram of the generated data sample (sample size = 1000) is shown in grey in the below figure, while the true distribution is shown in the blue curve.The Python code that was used to generate the above figure is shown below.# Normal distribution example
mu = 10
sigma = 2
sample_size = 1000
data_normal = stats.norm.rvs(loc=mu, scale=sigma ,size= sample_size) # generate data

# Plot the data histogram vs the PDF
x2 = np.linspace(data_normal.min(), data_normal.max(), sample_size)
fig3, ax = plt.subplots()
ax.hist(data_normal, bins=50, density=True, label="Data histogram",color = GRAY9)
ax.plot(x2, stats.norm(loc=mu, scale=sigma).pdf(x2),
        label="Normal distribution (PDF)",color = BLUE2,linewidth=3.0)

ax.set_title("Data histogram vs. true distribution", fontsize=14, loc='left')
ax.set_xlabel('Data value')
ax.set_ylabel('Probability')
ax.legend()
ax.grid()

plt.savefig("Normal_hist_PMF.png", format="png", dpi=800)Now, we would like to use the MoM estimator to find an estimate of the model parameters, i.e., μ and σ² as shown below.In order to test this estimator using our sample data, we plot the distribution with the estimated parameters (orange) in the below figure, versus the true distribution (blue). Again, it can be shown that the distributions are quite close. Of course, in order to quantify this estimator, we need to test it on multiple realizations of the data and observe properties such as bias, variance, etc. Such important aspects have been discussed in an earlier article.The Python code that was used to estimate the model parameters using MoM, and to plot the above figure is shown below.# Method of moments estimator using the data (Normal Dist)
mu_hat = sum(data_normal) / len(data_normal) # MoM mean estimator
var_hat = sum(pow(x-mu_hat,2) for x in data_normal) / len(data_normal) # variance
sigma_hat = math.sqrt(var_hat)  # MoM standard deviation estimator

# Plot the MoM estimated PDF vs the true PDF
x2 = np.linspace(data_normal.min(), data_normal.max(), sample_size)
fig4, ax = plt.subplots()
ax.plot(x2, stats.norm(loc=mu_hat, scale=sigma_hat).pdf(x2),
        label="Estimated PDF",color = ORANGE1,linewidth=3.0)
ax.plot(x2, stats.norm(loc=mu, scale=sigma).pdf(x2),
        label="True PDF",color = BLUE2,linewidth=3.0)

ax.set_title("Estimated Normal distribution vs. true distribution", fontsize=14, loc='left')
ax.set_xlabel('Data value')
ax.set_ylabel('Probability')
ax.legend()
ax.grid()
plt.savefig("Normal_true_vs_est.png", format="png", dpi=800)Another useful probability distribution is the Gamma distribution. An example for the application of this distribution in real life was discussed in a previous article. However, in this article, we derive the MoM estimator of the Gamma distribution parameters α and β as shown below, assuming the data is iid.In this particular example, a Gamma distribution with α = 6 and β = 0.5 is assumed to have generated the data. The histogram of the generated data sample (sample size = 1000) is shown in grey in the below figure, while the true distribution is shown in the blue curve.The Python code that was used to generate the above figure is shown below.# Gamma distribution example
alpha_ = 6 # shape parameter
scale_ = 2 # scale paramter (lamda) = 1/beta in gamma dist.
sample_size = 1000
data_gamma = stats.gamma.rvs(alpha_,loc=0, scale=scale_ ,size= sample_size) # generate data

# Plot the data histogram vs the PDF
x3 = np.linspace(data_gamma.min(), data_gamma.max(), sample_size)
fig5, ax = plt.subplots()
ax.hist(data_gamma, bins=50, density=True, label="Data histogram",color = GRAY9)
ax.plot(x3, stats.gamma(alpha_,loc=0, scale=scale_).pdf(x3),
        label="Gamma distribution (PDF)",color = BLUE2,linewidth=3.0)

ax.set_title("Data histogram vs. true distribution", fontsize=14, loc='left')
ax.set_xlabel('Data value')
ax.set_ylabel('Probability')
ax.legend()
ax.grid()
plt.savefig("Gamma_hist_PMF.png", format="png", dpi=800)Now, we would like to use the MoM estimator to find an estimate of the model parameters, i.e., α and β, as shown below.In order to test this estimator using our sample data, we plot the distribution with the estimated parameters (orange) in the below figure, versus the true distribution (blue). Again, it can be shown that the distributions are quite close.The Python code that was used to estimate the model parameters using MoM, and to plot the above figure is shown below.# Method of moments estimator using the data (Gamma Dist)
sample_mean = data_gamma.mean()
sample_var = data_gamma.var()
scale_hat = sample_var/sample_mean #scale is equal to 1/beta in gamma dist.
alpha_hat = sample_mean**2/sample_var

# Plot the MoM estimated PDF vs the true PDF
x4 = np.linspace(data_gamma.min(), data_gamma.max(), sample_size)
fig6, ax = plt.subplots()

ax.plot(x4, stats.gamma(alpha_hat,loc=0, scale=scale_hat).pdf(x4),
        label="Estimated PDF",color = ORANGE1,linewidth=3.0)
ax.plot(x4, stats.gamma(alpha_,loc=0, scale=scale_).pdf(x4),
        label="True PDF",color = BLUE2,linewidth=3.0)

ax.set_title("Estimated Gamma distribution vs. true distribution", fontsize=14, loc='left')
ax.set_xlabel('Data value')
ax.set_ylabel('Probability')
ax.legend()
ax.grid()
plt.savefig("Gamma_true_vs_est.png", format="png", dpi=800)Note that we used the following equivalent ways of writing the variance when deriving the estimators in the cases of Gaussian and Gamma distributions.In this article, we explored various examples of the method of moments estimator and its applications in different problems in data science. Moreover, detailed Python code that was used to implement the estimators from scratch as well as to plot the different figures is also shown. I hope that you will find this article helpful.]]></content:encoded></item><item><title>How to Measure the Reliability of a Large Language Model’s Response</title><link>https://towardsdatascience.com/how-to-measure-the-reliability-of-a-large-language-models-response/</link><author>Umair Ali Khan</author><category>dev</category><category>ai</category><pubDate>Thu, 13 Feb 2025 02:11:41 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[The basic principle of Large Language Models (LLMs) is very simple: to predict the next word (or token) in a sequence of words based on statistical patterns in their training data. However, this seemingly simple capability turns out to be incredibly sophisticated when it can do a number of amazing tasks such as text summarization, idea generation, brainstorming, code generation, information processing, and content creation. That said, LLMs do not have any memory no do they actually “understand” anything, other than sticking to their basic function: .The process of next-word prediction is probabilistic. The LLM has to select each word from a probability distribution. In the process, they often generate false, fabricated, or inconsistent content in an attempt to produce coherent responses and fill in gaps with plausible-looking but incorrect information. This phenomenon is called hallucination, an inevitable, well-known feature of LLMs that warrants validation and corroboration of their outputs. Retrieval augment generation (RAG) methods, which make an LLM work with external knowledge sources, do minimize hallucinations to some extent, but they cannot completely eradicate them. Although advanced RAGs can provide in-text citations and URLs, verifying these references could be hectic and time-consuming. Therefore, we need an objective criterion for assessing the reliability or trustworthiness of an LLM’s response, whether it is generated from its own knowledge or an external knowledge base (RAG). In this article, we will discuss how the output of an LLM can be assessed for trustworthiness by a trustworthy language model which assigns a score to the LLM’s output. We will first discuss how we can use a trustworthy language model to assign scores to an LLM’s answer and explain trustworthiness. Subsequently, we will develop an example RAG with LlamaParse and Llamaindex that assesses the RAG’s answers for trustworthiness.The entire code of this article is available in the jupyter notebook on GitHub. Assigning a Trustworthiness Score to an LLM’s AnswerTo demonstrate how we can assign a trustworthiness score to an Llm’s response, I will use Cleanlab’s Trustworthy Language Model (TLM). Such TLMs use a combination of uncertainty quantification and  to compute trustworthiness scores and explanations for LLM responses.Cleanlab offers free trial APIs which can be obtained by creating an account at their website. We first need to install Cleanlab’s Python client:pip install --upgrade cleanlab-studioCleanlab supports several proprietary models such as ‘’, ‘’, ‘’, ‘’, ‘’, ‘’ and others. Here is how TLM assigns a trustworhiness score to gpt-4o’s answer. The trustworthiness score ranges from 0 to 1, where higher values indicate greater trustworthiness. from cleanlab_studio import Studio
studio = Studio("<CLEANLAB_API_KEY>")  # Get your API key from above
tlm = studio.TLM(options={"log": ["explanation"], "model": "gpt-4o"}) # GPT, Claude, etc
#set the prompt
out = tlm.prompt("How many vowels are there in the word 'Abracadabra'.?")
#the TLM response contains the actual output 'response', trustworthiness score and explanation
print(f"Model's response = {out['response']}")
print(f"Trustworthiness score = {out['trustworthiness_score']}")
print(f"Explanation = {out['log']['explanation']}")
The above code tested the response of gpt-4o for the question “How many vowels are there in the word ‘Abracadabra’.?”. The TLM’s output contains the model’s answer (response), trustworthiness score, and explanation. Here is the output of this code.Model's response = The word "Abracadabra" contains 6 vowels. The vowels are: A, a, a, a, a, and a.
Trustworthiness score = 0.6842228802750124
Explanation = This response is untrustworthy due to a lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either):
5.
It can be seen how the most advanced language model hallucinates for such simple tasks and produces the wrong output. Here is the response and trustworthiness score for the same question for .Model's response = Let me count the vowels in 'Abracadabra':
A-b-r-a-c-a-d-a-b-r-a

The vowels are: A, a, a, a, a

There are 5 vowels in the word 'Abracadabra'.
Trustworthiness score = 0.9378276048845285
Explanation = Did not find a reason to doubt trustworthiness.
 produces the correct output. Let’s compare the two models’ responses to another question.from cleanlab_studio import Studio
import markdown
from IPython.core.display import display, Markdown

# Initialize the Cleanlab Studio with API key
studio = Studio("<CLEANLAB_API_KEY>")  # Replace with your actual API key

# List of models to evaluate
models = ["gpt-4o", "claude-3.5-sonnet-v2"]

# Define the prompt
prompt_text = "Which one of 9.11 and 9.9 is bigger?"

# Loop through each model and evaluate
for model in models:
   tlm = studio.TLM(options={"log": ["explanation"], "model": model})
   out = tlm.prompt(prompt_text)
  
   md_content = f"""
## Model: {model}

**Response:** {out['response']}

**Trustworthiness Score:** {out['trustworthiness_score']}

**Explanation:** {out['log']['explanation']}

---
"""
   display(Markdown(md_content))
Here is the response of the two models:We can also generate a trustworthiness score for open-source LLMs. Let’s check the recent, much-hyped open-source LLM: deepseek-R1. I will use DeepSeek-R1-Distill-Llama-70B, based on Meta’s Llama-3.3–70B-Instruct model and distilled from DeepSeek’s larger 671-billion parameter Mixture of Experts (MoE) model. Knowledge distillation is a Machine Learning technique that aims to transfer the learnings of a large pre-trained model, the “teacher model,” to a smaller “student model.”import streamlit as st
from langchain_groq.chat_models import ChatGroq
import os
os.environ["GROQ_API_KEY"]=st.secrets["GROQ_API_KEY"]
# Initialize the Groq Llama Instant model
groq_llm = ChatGroq(model="deepseek-r1-distill-llama-70b", temperature=0.5)
prompt = "Which one of 9.11 and 9.9 is bigger?"
# Get the response from the model
response = groq_llm.invoke(prompt)
#Initialize Cleanlab's studio
studio = Studio("226eeab91e944b23bd817a46dbe3c8ae") 
cleanlab_tlm = studio.TLM(options={"log": ["explanation"]})  #for explanations
#Get the output containing trustworthiness score and explanation
output = cleanlab_tlm.get_trustworthiness_score(prompt, response=response.content.strip())
md_content = f"""
## Model: {model}
**Response:** {response.content.strip()}
**Trustworthiness Score:** {output['trustworthiness_score']}
**Explanation:** {output['log']['explanation']}
---
"""
display(Markdown(md_content))
Here is the output of deepseek-r1-distill-llama-70b model.Developing a Trustworthy RAGWe will now develop an RAG to demonstrate how we can measure the trustworthiness of an LLM response in RAG. This RAG will be developed by scraping data from given links, parsing it in markdown format, and creating a vector store.The following libraries need to be installed for the next code.pip install llama-parse llama-index-core llama-index-embeddings-huggingface 
llama-index-llms-cleanlab requests beautifulsoup4 pdfkit nest-asyncioTo render HTML into PDF format, we also need to install command line tool from their website.The following libraries will be imported:from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex
import requests
from bs4 import BeautifulSoup
import pdfkit
from llama_index.readers.docling import DoclingReader
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.cleanlab import CleanlabTLM
from typing import Dict, List, ClassVar
from llama_index.core.instrumentation.events import BaseEvent
from llama_index.core.instrumentation.event_handlers import BaseEventHandler
from llama_index.core.instrumentation import get_dispatcher
from llama_index.core.instrumentation.events.llm import LLMCompletionEndEvent
import nest_asyncio
import os
The next steps will involve scraping data from given URLs using Python’s  library, saving the scraped data in PDF file(s) using , and parsing the data from PDF(s) to markdown file using  which is a genAI-native document parsing platform built with LLMs and for LLM use cases.We will first configure the LLM to be used by CleanlabTLM and the embedding model ( embedding model ) that will be used to compute the embeddings of the scraped data to create the vector store.options = {
   "model": "gpt-4o",
   "max_tokens": 512,
   "log": ["explanation"]
}
llm = CleanlabTLM(api_key="<CLEANLAB_API_KEY>", options=options)#Get your free API from https://cleanlab.ai/
Settings.llm = llm
Settings.embed_model = HuggingFaceEmbedding(
   model_name="BAAI/bge-small-en-v1.5"
)We will now define a custom event handler, , that is derived from a base event handler class. This handler gets triggered by the end of an LLM completion and extracts a trustworthiness score from the response metadata. A helper function, , displays the LLM’s response along with its trustworthiness score.# Event Handler for Trustworthiness Score
class GetTrustworthinessScore(BaseEventHandler):
   events: ClassVar[List[BaseEvent]] = []
   trustworthiness_score: float = 0.0
   @classmethod
   def class_name(cls) -> str:
       return "GetTrustworthinessScore"
   def handle(self, event: BaseEvent) -> Dict:
       if isinstance(event, LLMCompletionEndEvent):
           self.trustworthiness_score = event.response.additional_kwargs.get("trustworthiness_score", 0.0)
           self.events.append(event)
       return {}
# Helper function to display LLM's response
def display_response(response):
   response_str = response.response
   trustworthiness_score = event_handler.trustworthiness_score
   print(f"Response: {response_str}")
   print(f"Trustworthiness score: {round(trustworthiness_score, 2)}"): Readers are advised to always double-check the status of the content/data they are about to scrape and ensure they are allowed to do so. The following piece of code scrapes data from the given URLs by making an HTTP request and using Python library to parse the HTML content. HTML content is cleaned by converting protocol-relative URLs to absolute ones. Subsequently, the scraped content is converted into a PDF file(s) using .##########################################
# PDF Generation from Multiple URLs
##########################################
# Configure wkhtmltopdf path
wkhtml_path = r'C:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe'
config = pdfkit.configuration(wkhtmltopdf=wkhtml_path)
# Define URLs and assign document names
urls = {
   "LLMs": "https://en.wikipedia.org/wiki/Large_language_model"
}
# Directory to save PDFs
pdf_directory = "PDFs"
os.makedirs(pdf_directory, exist_ok=True)
pdf_paths = {}
for doc_name, url in urls.items():
   try:
       print(f"Processing {doc_name} from {url} ...")
       response = requests.get(url)
       soup = BeautifulSoup(response.text, "html.parser")
       main_content = soup.find("div", {"id": "mw-content-text"})
       if main_content is None:
           raise ValueError("Main content not found")
       # Replace protocol-relative URLs with absolute URLs
       html_string = str(main_content).replace('src="//', 'src="https://').replace('href="//', 'href="https://')
       pdf_file_path = os.path.join(pdf_directory, f"{doc_name}.pdf")
       pdfkit.from_string(
           html_string,
           pdf_file_path,
           options={'encoding': 'UTF-8', 'quiet': ''},
           configuration=config
       )
       pdf_paths[doc_name] = pdf_file_path
       print(f"Saved PDF for {doc_name} at {pdf_file_path}")
   except Exception as e:
       print(f"Error processing {doc_name}: {e}")After generating PDF(s) from the scraped data, we parse these PDFs using . We set the parsing instructions to extract the content in markdown format and parse the document(s) page-wise along with the document name and page number. These extracted entities (pages) are referred to as . The parser iterates over the extracted nodes and updates each node’s metadata by appending a citation header which facilitates later referencing.##########################################
# Parse PDFs with LlamaParse and Inject Metadata
##########################################

# Define parsing instructions (if your parser supports it)
parsing_instructions = """Extract the document content in markdown.
Split the document into nodes (for example, by page).
Ensure each node has metadata for document name and page number."""
      
# Create a LlamaParse instance
parser = LlamaParse(
   api_key="<LLAMACLOUD_API_KEY>",  #Replace with your actual key
   parsing_instructions=parsing_instructions,
   result_type="markdown",
   premium_mode=True,
   max_timeout=600
)
# Directory to save combined Markdown files (one per PDF)
output_md_dir = os.path.join(pdf_directory, "markdown_docs")
os.makedirs(output_md_dir, exist_ok=True)
# List to hold all updated nodes for indexing
all_nodes = []
for doc_name, pdf_path in pdf_paths.items():
   try:
       print(f"Parsing PDF for {doc_name} from {pdf_path} ...")
       nodes = parser.load_data(pdf_path)  # Returns a list of nodes
       updated_nodes = []
       # Process each node: update metadata and inject citation header into the text.
       for i, node in enumerate(nodes, start=1):
           # Copy existing metadata (if any) and add our own keys.
           new_metadata = dict(node.metadata) if node.metadata else {}
           new_metadata["document_name"] = doc_name
           if "page_number" not in new_metadata:
               new_metadata["page_number"] = str(i)
           # Build the citation header.
           citation_header = f"[{new_metadata['document_name']}, page {new_metadata['page_number']}]\n\n"
           # Prepend the citation header to the node's text.
           updated_text = citation_header + node.text
           new_node = node.__class__(text=updated_text, metadata=new_metadata)
           updated_nodes.append(new_node)
       # Save a single combined Markdown file for the document using the updated node texts.
       combined_texts = [node.text for node in updated_nodes]
       combined_md = "\n\n---\n\n".join(combined_texts)
       md_filename = f"{doc_name}.md"
       md_filepath = os.path.join(output_md_dir, md_filename)
       with open(md_filepath, "w", encoding="utf-8") as f:
           f.write(combined_md)
       print(f"Saved combined markdown for {doc_name} to {md_filepath}")
       # Add the updated nodes to the global list for indexing.
       all_nodes.extend(updated_nodes)
       print(f"Parsed {len(updated_nodes)} nodes from {doc_name}.")
   except Exception as e:
       print(f"Error parsing {doc_name}: {e}")We now create a vector store and a query engine. We define a customer prompt template to guide the LLM’s behavior in answering the questions. Finally, we create a query engine with the created index to answer queries. For each query, we retrieve the top 3 nodes from the vector store based on their semantic similarity with the query. The LLM uses these retrieved nodes to generate the final answer.##########################################
# Create Index and Query Engine
##########################################
# Create an index from all nodes.
index = VectorStoreIndex.from_documents(documents=all_nodes)
# Define a custom prompt template that forces the inclusion of citations.
prompt_template = """
You are an AI assistant with expertise in the subject matter.
Answer the question using ONLY the provided context.
Answer in well-formatted Markdown with bullets and sections wherever necessary.
If the provided context does not support an answer, respond with "I don't know."
Context:
{context_str}
Question:
{query_str}
Answer:
"""
# Create a query engine with the custom prompt.
query_engine = index.as_query_engine(similarity_top_k=3, llm=llm, prompt_template = prompt_template)
print("Combined index and query engine created successfully!")Now let’s test the RAG for some queries and their corresponding trustworthiness scores.query = "When is mixture of experts approach used?"
response = query_engine.query(query)
display_response(response)query = "How do you compare Deepseek model with OpenAI's models?"
response = query_engine.query(query)
display_response(response)Assigning a trustworthiness score to LLM’s response, whether generated through direct inference or RAG, helps to define the reliability of AI’s output and prioritize human verification where needed. This is particularly important for critical domains where a wrong or unreliable response could have severe consequences. That’s all folks! If you like the article, please follow me on ]]></content:encoded></item><item><title>SmolModels: Because not everything needs a giant LLM</title><link>https://www.reddit.com/r/artificial/comments/1io4fa6/smolmodels_because_not_everything_needs_a_giant/</link><author>/u/Pale-Show-2469</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 22:59:05 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[So everyone’s chasing bigger models, but do we really need a 100B+ param beast for every task? We’ve been playing around with something different—. Small, task-specific AI models that just do one thing . No bloat, no crazy compute bills, and you can self-host them.We’ve been using blend of synthetic data + model generation, and honestly? They hold up shockingly well against AutoML & even some fine-tuned LLMs, esp for structured data. Just open-sourced it here: SmolModels GitHub.Curious to hear thoughts.]]></content:encoded></item><item><title>[R] &quot;o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors&quot;</title><link>https://www.reddit.com/r/MachineLearning/comments/1io4c7r/r_o3_achieves_a_gold_medal_at_the_2024_ioi_and/</link><author>/u/we_are_mammals</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 22:55:17 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[Competitive Programming with Large Reasoning ModelsWe show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.]]></content:encoded></item><item><title>Precision agriculture powered by AI for climate-resilient crops</title><link>https://www.datasciencecentral.com/precision-agriculture-powered-by-ai-for-climate-resilient-crops/</link><author>Shanthababu Pandian</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 21:25:50 +0000</pubDate><source url="https://www.datasciencecentral.com/">Dev - Data Science Central</source><content:encoded><![CDATA[AI in Agriculture Precision Farming AI-Powered Agriculture Climate-Resilient Crops
Sustainable Farming Practices AI for Pest Control AI for Soil Analysis Machine Learning in Agriculture Smart Farming Solutions IoT in Agriculture Crop Monitoring with AI
Predictive Analytics in Farming AI for Weather Prediction in Agriculture
AI-Driven Precision Irrigation AI in Fertilization Optimization Sustainable Agriculture Technology Advanced Farming Techniques Agriculture Data Analysis with AI
AI-Powered Smart Irrigation Agricultural Innovation with AI]]></content:encoded></item><item><title>China’s Hygon GPU Chips get 10 times More Powerful than Nvidia, Claims Study</title><link>https://interestingengineering.com/innovation/chinese-gpus-surpass-nvidia</link><author>/u/Suspicious-Bad4703</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 20:29:43 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Manage Environment Variables with Pydantic</title><link>https://towardsdatascience.com/manage-environment-variables-with-pydantic/</link><author>Marcello Politi</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 20:10:29 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Developers work on applications that are supposed to be deployed on some server in order to allow anyone to use those. Typically in the machine where these apps live, developers set up environment variables that allow the app to run. These variables can be API keys of external services, URL of your database and much more.For local development though, it is really inconvenient to declare these variables on the machine because it is a slow and messy process. So I’d like to share in this short tutorial how to use Pydantic to handle environment variables in a secure way.What you commonly do in a Python project is to store all your environment variables in a file named .env. This is a text file containing all the variables in a  format. You can use also the value of one of the variables to declare one of the other variables by leveraging the  syntax.The following is an example:#.env file

OPENAI_API_KEY="sk-your private key"
OPENAI_MODEL_ID="gpt-4o-mini"

# Development settings
DOMAIN=example.org
ADMIN_EMAIL=admin@${DOMAIN}

WANDB_API_KEY="your-private-key"
WANDB_PROJECT="myproject"
WANDB_ENTITY="my-entity"

SERPAPI_KEY= "your-api-key"
PERPLEXITY_TOKEN = "your-api-token"Be aware the .env file should remain private, so it is important that this file is mentioned in your  file, to be sure that you , otherwise, other developers could steal your keys and use the tools you’ve paid for.To ease the life of developers who will clone your repository, you could include an env.example file in your project. This is a file containing only the keys of what is supposed to go into the .env file. In this way, other people know what APIs, tokens, or secrets in general they need to set to make the scripts work.#env.example

OPENAI_API_KEY=""
OPENAI_MODEL_ID=""

DOMAIN=""
ADMIN_EMAIL=""

WANDB_API_KEY=""
WANDB_PROJECT=""
WANDB_ENTITY=""

SERPAPI_KEY= ""
PERPLEXITY_TOKEN = ""python-dotenv is the library you use to load the variables declared into the .env file. To install this library:pip install python-dotenvNow you can use the load_dotenv to load the variables. Then get a reference to these variables with the os module.import os
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_MODEL_ID = os.getenv('OPENAI_MODEL_ID')This method will first look into your .env file to load the variables you’ve declared there. If this file doesn’t exist, the variable will be taken from the host machine. This means that you can use the .env file for your local development but then when the code is deployed to a host environment like a virtual machine or Docker container we are going to directly use the environment variables defined in the host environment.Pydantic is one of the most used libraries in Python for data validation. It is also used for serializing and deserializing classes into JSON and back. It automatically generates JSON schema, reducing the need for manual schema management. It also provides built-in data validation, ensuring that the serialized data adheres to the expected format. Lastly, it easily integrates with popular web frameworks like FastAPI.is a Pydantic feature needed to load and validate settings or config classes from environment variables.!pip install pydantic-settingsWe are going to create a class named . This class will inherit This makes the default behaviours of determining the values of any fields to be read from the .env file. If no var is found in the .env file it will be used the default value if provided.from pydantic_settings import BaseSettings, SettingsConfigDict

from pydantic import (
    AliasChoices,
    Field,
    RedisDsn,
)


class Settings(BaseSettings):
    auth_key: str = Field(validation_alias='my_auth_key')  
    api_key: str = Field(alias='my_api_key')  

    redis_dsn: RedisDsn = Field(
        'redis://user:pass@localhost:6379/1', #default value
        validation_alias=AliasChoices('service_redis_dsn', 'redis_url'),  
    )

    model_config = SettingsConfigDict(env_prefix='my_prefix_')In the  class above we have defined several fields. The  class is used to provide extra information about an attribute.In our case, we setup a . So the variable name to look for in the .env file is overridden. In the case reported above, the environment variable  will be read instead of .You can also have multiple aliases to look for in the .env file that you can specify by leveraging AliasChoises(choise1, choise2).The last attribute  , contains all the variables regarding a particular topic (e.g connection to a db). And this variable will store all .env var that start with the prefix Instantiate and use settingsThe next step would be to actually instantiate and use these settings in your Python project.from pydantic_settings import BaseSettings, SettingsConfigDict

from pydantic import (
    AliasChoices,
    Field,
    RedisDsn,
)


class Settings(BaseSettings):
    auth_key: str = Field(validation_alias='my_auth_key')  
    api_key: str = Field(alias='my_api_key')  

    redis_dsn: RedisDsn = Field(
        'redis://user:pass@localhost:6379/1', #default value
        validation_alias=AliasChoices('service_redis_dsn', 'redis_url'),  
    )

    model_config = SettingsConfigDict(env_prefix='my_prefix_')

# create immediately a settings object
settings = Settings()Now what use the settings in other parts of our codebase.from Settings import settings

print(settings.auth_key) You finally have an easy access to your settings, and Pydantic helps you validate that the secrets have the correct format. For more advanced validation tips refer to the Pydantic documentation: https://docs.pydantic.dev/latest/Managing the configuration of a project is a boring but important part of software development. Secrets like API keys, db connections are what usually power your application. Naively you can hardcode these variables in your code and it will still work, but for obvious reasons, this could not be a good practice. In this article, I showed you an introduction on how to use pydantic settings to have a structured and safe way to handle your configurations.]]></content:encoded></item><item><title>Pandas Can’t Handle This: How ArcticDB Powers Massive Datasets</title><link>https://towardsdatascience.com/pandas-cant-handle-this-how-arcticdb-powers-massive-datasets/</link><author>Ari Joury, PhD</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 19:22:29 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[Python has grown to dominate data science, and its package Pandas has become the go-to tool for data analysis. It is great for tabular data and supports data files of up to 1GB if you have a large RAM. Within these size limits, it is also good with time-series data because it comes with some in-built support.That being said, when it comes to larger datasets, Pandas alone might not be enough. And modern datasets are growing exponentially, whether they’re from finance, climate science, or other fields.This means that, as of today, Pandas is a great tool for smaller projects or exploratory analysis. It is not great, however, when you’re facing bigger tasks or want to scale into production fast. Workarounds exist — Dask, Spark, Polars, and chunking are some of them — but they come with additional complexity and bottlenecks.I faced this problem recently. I was looking to see whether there are correlations between weather data from the past 10 years, and stock prices of energy companies. The rationale here is there might be sensitivities between global temperatures and the stock price evolution of fossil fuel- and renewable energy companies. If one found such sensitivities, that would be a strong signal for Big Energy CEOs to start cutting their emissions in their own self-interest.I obtained the stock price data quite easily through Yahoo! Finance’s API. I used 16 stocks and ETFs — seven fossil fuel companies, six renewables companies, and three energy ETFs — and their daily close over ten years between 2013 to 2023. That resulted in about 45,000 datapoints. That’s a piece of cake for Pandas.Global weather data was an entirely different picture. First of all, it took me hours to download it through the Copernicus API. The API itself is amazing; the problem is just that there is  data. I wanted worldwide daily temperature data between 2013 and 2023. The little problem with this is that, with weather stations at 721 points of geographical latitude and 1440 points of geographical longitude, you’re downloading and later processing close to .That’s a lot of datapoints. Worth 185 GB of space on my hard drive.To evaluate this much data I tried chunking, but this overloaded my state-of-the-art computer. Iterating through that dataset one step at a time worked, but it took me half a day to process it every time I wanted to run a simple analysis.The good news is that I’m quite well-connected in the financial services industry. I’d heard about ArcticDB a while back but had never given it a shot so far. It is a database which was developed at Man Group, a hedge fund where several contacts of mine work at.So I gave ArcticDB a shot for this project — and I’m not looking back. I’m not abandoning Pandas, but for datasets in the billions I’ll choose ArcticDB over Pandas any day.I should clarify two things at this point: First, although I know people at ArcticDB / Man Group, I’m not formally affiliated with them. I did this project independently and chose to share the results with you. Second, ArcticDB is not fully open-source. It is free for individual users within reasonable limits but has paid tiers for power users and corporations. I used the free version, which gets you pretty far—and well beyond the scope of this project actually.With that out of the way, I’ll now show you how to set up ArcticDB and what its basic usage is. I’ll then go into my project and how I used ArcticDB in this case. You’ll also get to see some exciting results on the correlations I found between energy stocks and worldwide temperatures. I’ll follow with a performance comparison of ArcticDB and Pandas. Finally, I’ll show exactly when you’ll be better off using ArcticDB, and when you can safely use Pandas without worrying about bottlenecks.At this point, you might have been wondering why I’ve been comparing a data manipulation tool — Pandas — with a full-blown database. The truth is that ArcticDB is a bit of both: It stores data conveniently, but it also helps manipulating data. Some powerful perks of it include fast queries, versioning, and better memory management.For Linux- and Windows users, getting ArcticDB is as simple as getting any other Python package:pip install arcticdb  # or conda install -c conda-forge arcticdbFor Mac users, things are a little more complicated. ArcticDB does not support Apple chips at this time. Here are two workarounds (I’m on a Mac, and after testing I chose the first):The second workaround works, but the performance is slower. It therefore wipes out some of the gains of using ArcticDB in the first place. Nevertheless, it is a valid option if you can’t or don’t want to use Docker.To set up ArcticDB, you need to create a local instance in the following fashion:import arcticdb as adb
library = adb.Arctic("lmdb://./arcticdb")  # Local storage
library.create_library("climate_finance")ArcticDB supports multiple storage backends like AWS S3, Mongo DB, and LMDB. This makes it very easy to scale into production without having to think about Data Engineering.If you know how to use Pandas, ArcticDB won’t be hard for you. Here’s how you’d read in a Pandas dataframe:import pandas as pd

df = pd.DataFrame({"Date": ["2024-01-01", "2024-01-02"], "XOM": [100, 102]})
df["Date"] = pd.to_datetime(df["Date"])  # Ensure Date column is in datetime format

climate_finance_lib = library["climate_finance"]
climate_finance_lib.write("energy_stock_prices", df)To retrieve data from ArcticDB, you’d proceed in the following fashion:df_stocks = climate_finance_lib.read("energy_stock_prices").data
print(df_stocks.head())  # Verify the stored dataOne of the coolest features about ArcticDB is that it provides versioning support. If you are updating your data frequently and only want to retrieve the latest version, this is how you’d do it:latest_data = climate_finance_lib.read("energy_stock_prices", as_of=0).dataAnd if you want a specific version, you do this:versioned_data = climate_finance_lib.read("energy_stock_prices", as_of=-3).dataGenerally speaking, the versioning works as follows: Much like in Numpy, the index 0 (following  in the snippets above) refers to the first version, -1 is the latest, and -3 is two versions before that.Once you have a grip around how to handle your data, you can analyse your dataset as you always have done. Even while using ArcticDB, chunking can be a good way to reduce memory usage. Once you scale to production, its native integration with AWS S3 and other storage systems will be your friend.Energy Stocks Versus Global TemperaturesBuilding my study around energy stocks and their potential dependence on global temperatures was fairly easy. First, I used ArcticDB to retrieve the stock returns data and temperature data. This was the script I used for obtaining the data:import arcticdb as adb
import pandas as pd

# Set up ArcticDB
library = adb.Arctic("lmdb://./arcticdb")  # Local storage
library.create_library("climate_finance")

# Load stock data
df_stocks = pd.read_csv("energy_stock_prices.csv", index_col=0, parse_dates=True)

# Store in ArcticDB
climate_finance_lib = library["climate_finance"]
climate_finance_lib.write("energy_stock_prices", df_stocks)

# Load climate data and store (assuming NetCDF processing)
import xarray as xr
ds = xr.open_dataset("climate_data.nc")
df_climate = ds.to_dataframe().reset_index()
climate_finance_lib.write("climate_temperature", df_climate)A quick note about the data licenses: It is permitted to use all this data for commercial use. The Copernicus license allows this for the weather data; the yfinance license allows this for the stock data. (The latter is a community-maintained project that makes use of Yahoo Finance data but is not officially part of Yahoo. This means that, should Yahoo at some point change its stance on —right now it tolerates it—I’ll have to find another way to legally get this data.)The above code does the heavy lifting around billions of datapoints within a few lines. If, like me, you’ve been battling data engineering challenges in the past, I would not be surprised if you feel a little baffled by this.I then calculated the annual temperature anomaly. I did this by first computing the mean temperature across all grid points in the dataset. I then subtracted this from the actual temperature each day to determine the deviation from the expected norm.This approach is unusual because one would usually calculate the  mean temperature over 30 years of data in order to help capture unusual temperature fluctuations relative to historical trends. But since I only had 10 years of data on hand, I feared that this would muddy the results to the point where they’d be statistically laughable; hence this approach. (I’ll follow up with 30 years of data — and the help of ArcticDB — in due time!)Additionally, for the rolling correlations, I used a 30-day moving window to calculate the correlation between stock returns and my somewhat special temperature anomalies, ensuring that short-term trends and fluctuations were accounted for while smoothing out noise in the data.As expected and to be seen below, we get two bumps — one for summer and one for winter. (As mentioned above, one could also calculate the daily anomaly, but this usually requires at least 30 years’ worth of temperature data — better to do in production.)Global temperature anomaly between 2013 and 2023. Image by authorI then calculated the rolling correlation between various stock tickers and the global average temperature. I did this by computing the Pearson correlation coefficient between the daily returns of each stock ticker and the corresponding daily temperature anomaly over the rolling window. This method captures how the relationship evolves over time, revealing periods of heightened or diminished correlation.A selection of this can be seen below.On the whole, one can see that the correlation changes often. However, one can also see that there are more pronounced peaks in the correlation for the featured fossil fuel companies (XOM, SHEL, EOG) and energy ETFs (XOP). There is significant correlation with temperatures for renewables companies as well (ORSTED.CO, ENPH), but it remains within stricter limits.Correlation of selected stocks with global temperature anomaly, 2013 to 2023. Image by authorThis graph is rather busy, so I decided to take the average correlation with temperature for several stocks. Essentially this means that I used the average over time of the daily correlations. The results are rather interesting: All fossil fuel stocks have a negative correlation with the global temperature anomaly (everything from XOM to EOG below).This means that when the anomalies increase (i.e., there is more extreme heat or cold) the fossil stock prices decrease. The effect is significant but weak, which suggests that global average temperature anomalies alone might not be the primary drivers of stock price movements. Nevertheless, it’s an interesting observation.Most renewables stocks (from NEE to ENPH) have positive correlations with the temperature anomaly. This is somewhat expected; if temperatures get extreme, investors might start thinking more about renewable energy.Energy ETFs (XLE, IXC, XOP) are also negatively correlated with temperature anomalies. This is not surprising because these ETFs often contain a large amount of fossil fuel companies.Average correlation of selected stocks with temperature anomaly, 2013–2023. Image by authorAll these effects are significant but small. To take this analysis to the next level, I will:Test the regional weather impact on selected stocks. For example, cold snaps in Texas might have outsized effects on fossil fuel stocks. (Luckily, retrieving such data subsets is a charm with ArcticDB!)Use more weather variables: Aside from temperatures, I expect wind speeds (and therefore storms) and precipitation (droughts and flooding) to affect fossil and renewables stocks in distinct ways.Using AI-driven models: Simple correlation can say a lot, but nonlinear dependencies are better found with Bayesian networks, random forests, or deep learning techniques.These insights will be published on this blog when they’re ready. Hopefully they can inspire the one or other Big Energy CEO to reshape their sustainability strategy!ArcticDB Versus Pandas: Performance ChecksFor the sake of this article, I went ahead and painstakingly re-ran my codes just in Pandas, as well as in a chunked version.We have four operations pertaining to 10 years of stock- and of climate data. The table below shows how the performances compare with a basic Pandas setup, with some chunking, and with the best way I could come up with using ArcticDB. As you can see, the setup with ArcticDB is easily five times faster, if not more.Pandas works like a charm for a small dataset of 45k rows, but loading a dataset of 3.8 billion rows into a basic Pandas setup is not even possible on my machine. Loading it through chunking also only worked with more workarounds, essentially going one step at a time. With ArcticDB, on the other hand, this was easy.In my setup, ArcticDB sped the whole process up by an order of magnitude. Loading a very large dataset was not even possible without ArcticDB, if major workarounds were not employed!Pandas is great for relatively small, exploratory analyses. However, when performance, scalability, and quick data retrieval become mission-critical, ArcticDB can be an amazing ally. Below are some cases in which ArcticDB is worth a serious consideration.When Your Dataset is Too Large For PandasPandas loads everything into RAM. Even with an excellent machine, this means that datasets above a few GB are bound to crash. ArcticDB also works with very wide datasets spanning millions of columns. Pandas often fails at this.When You’re Working With Time-Series DataTime-series queries are common in fields like finance, climate science, or IoT. Pandas has some native support for time-series data, but ArcticDB features faster time-based indexing and filtering. It also supports versioning, which is amazing for retrieving historical snapshots without having to reload an entire dataset. Even if you’re using Pandas for analytics, ArcticDB speeds up data retrieval, which can make your workflows much smoother.When You Need a Production-Ready DatabaseOnce you scale to production, Pandas won’t cut it anymore. You’ll need a database. Instead of thinking long and deep about the best database to use and dealing with plenty of data engineering challenges, you can use ArcticDB because:It easily integrates with cloud storage, notably AWS S3 and Azure.It works as a centralized database even for large teams. In contrast, Pandas is just an in-memory tool.It allows for parallelized reads and writes.It seamlessly complements analytical libraries like NumPy, PyTorch, and Pandas for more complex queries.The Bottom Line: Use Cool Tools To Gain TimeWithout ArcticDB, my study on weather data and energy stocks would not have been possible. At least not without major headaches around speed and memory bottlenecks.I’ve been using and loving Pandas for years, so this is not a statement to take lightly. I still think that it’s great for smaller projects and exploratory data analysis. However, if you’re handling substantial datasets or if you want to scale your model into production, ArcticDB is your friend.Think of ArcticDB as an ally to Pandas rather than a replacement — it bridges the gap between interactive data exploration and production-scale analytics. To me, ArcticDB is therefore a lot more than a database. It is also an advanced data manipulation tool, and it automates all the data engineering backend so that you can focus on the truly exciting stuff.One exciting result to me is the clear difference in how fossil and renewables stocks respond to temperature anomalies. As these anomalies increase due to climate change, fossil stocks will suffer. Is that not something to tell Big Energy CEOs?To take this further, I might focus on more localized weather and go beyond temperature. I’ll also go beyond simple correlations and use more advanced techniques to tease out nonlinear relationships in the data. (And yes, ArcticDB will likely help me with that.)On the whole, if you’re handling large or wide datasets, lots of time series data, need to version your data, or want to scale quickly into production, ArcticDB is your friend. I’m looking forward to exploring this tool in more detail as my case studies progress!]]></content:encoded></item><item><title>Branching Out: 4 Git Workflows for Collaborating on ML</title><link>https://towardsdatascience.com/branching-out-4-git-workflows-for-collaborating-on-ml/</link><author>Julie Ripplinger</author><category>dev</category><category>ai</category><pubDate>Wed, 12 Feb 2025 19:10:44 +0000</pubDate><source url="https://towardsdatascience.com/">Dev - Towards Data Science</source><content:encoded><![CDATA[It’s been more than 15 years since I finished my master’s degree, but I’m still haunted by the hair-pulling frustration of managing my of  scripts. As a (recovering) perfectionist, I named each script very systematically by date (think: ). A system I just *knew* was better than , ,  and its frenemies. Right?Trouble was, every time I wanted to tweak my model inputs or review a previous model version, I had to swim through a sea of scripts.Fast forward a few years, a few programming languages, and a career slalom later, I can clearly see how my solo struggles with code versioning were a lucky wake-up call.While I managed to navigate those early challenges (with a few cringey moments!), I now recognise that most development, especially with Agile ways of working, thrives on robust version control systems. The ability to track changes, revert to previous versions, and ensure reproducibility within a collaborative codebase can’t be an afterthought. It’s actually a necessity.When we use version control workflows, often in Git, we lay the groundwork for developing and deploying more reliable and higher quality data and AI solutions.If you already use version control and you’re thinking about different workflows for your team, welcome! You’ve come to the right place.If you’re new to Git or have only used it on solo projects, I recommend reviewing some introductory Git principles. You’ll want more background before jumping into team workflows. GitHub provides links to several Git and GitHub tutorials here. And this getting started post introduces basics like how to create a repo and add a file.Development teams work in different waysBut a ubiquitous feature is reliance on version control.Git is incredibly flexible as a version control system, and it allows developers a lot of freedom in how they manage their code. If you’re not careful, though, flexibility leaves room for chaos if not managed effectively. Establishing Git workflows can guide your team’s development so you’re using Git more consistently and efficiently. Think of it as the team’s shared roadmap for navigating Git’s highways and byways.By defining when we create branches, how we merge changes, and why we review code, we create a common understanding and foster more reliable ways of developing as a team. Which means that every team has the opportunity to create their own Git workflows that work for their specific organisational structure, use-cases, tech stack, and requirements. It’s possible to have as many ways of using Git as a team as there are development teams. Ultimate flexibility.You may find that idea liberating. You and your team have the freedom to design a Git workflow that works for you!But if that sounds intimidating, not to worry. There are several established protocols to use as a starting point for agreeing on team workflows.Version control is useful in so many ways, but the benefits I see over and over on my teams cluster into a few essential categories. We’re here to focus on workflows so I won’t go into great depth, but the central premise and advantages of Git and GitHub are worth highlighting.(Almost) anything is reversible. Which means that version control systems free you up to get creative and make mistakes. Rolling back any regrettable code changes is as simple as . Like a good neighbour, Git commands are there. Once you get into the flow of using it, Git really facilitates seamless collaboration across the team. Work can happen concurrently without interfering with anyone else’s code, and code changes are all documented in commit snapshots. This means anyone on the team can take a peek at what the others have been working on and how they went about it, because the changes are captured in the Git history. Collaboration made easy.Isolating exploratory work in feature branches. How will you know which model gives the best performance for your specific business problem? In a recent revenues use case, it could’ve been time series models, maybe tree-based methods, or convolutional neural networks. Possibly even Bayesian approaches. Without the parallel branching ability Git provided my team, trialling the different methods would’ve resulted in a codebase of pure chaos.In-built review process (massively improves code quality). By putting code through peer review using GitHub’s pull request system, I’ve seen team after team grow in their abilities to leverage their collective knowledge to write cleaner, faster, more modular code. As code review helps team members identify and address bugs, design flaws, and maintainability, it ultimately leads to higher quality code. As in, every change made to the codebase is recorded in the Git history. Which makes it incredibly easy to track changes, revert to previous versions, and reproduce past experiments. I can’t understate its importance for debugging, code maintenance, and ensuring the reliability of any experimental findings.Different flavours of workflows for different types of workFeature-branching workflow: The Standard BearerThis is the most common Git workflow used in dev teams. It’d be difficult to unseat it in terms of its popularity, and for good reason. In a feature branching workflow, each new functionality or improvement to the code is developed in its own dedicated branch, separate from the main codebase.A branching workflow provides each developer with an isolated workspace (a branch) — their own complete copy of the project. This lets every person on the team do focused work, independent of what’s happening elsewhere in the project. They can make code changes and forget about upstream development, working independently until they’re ready to share their code.At that point, they can take advantage of GitHub’s pull request (PR) functionality to facilitate code review and collaborate with the team to ensure the changes are evaluated and approved before being merged into the codebase.This approach is especially beneficial to Agile development teams and teams working on complex projects that call for frequent code changes.A feature branching workflow might look like this:# In your terminal:

$ git switch <new_branch_name> # Creates and switches onto a new branch
$ git push -u origin <new_branch_name> # For first push only. Creates new working branch on the remote repository

# Create and activate your virtual environment. Pip install any required packages.

$ python3 -m venv <new_venv_name>
$ source new_venv_name/bin/activate
$ pip install requirements.txt (or <packages>)

# Make changes to your code in feature branch
# Regularly stage and commit your code changes, and push to remote. For example:

$ git add <file> # Stages the file to prepare repo snapshot for commit
$ git commit -m "<Your descriptive message>" # Records file snapshots into your version history
$ git push # Sends local commits to the remote repository; to your working branch

# Raise Pull Request (PR) on repo's webpage. Request reviewer(s) in PR.
# After PR is approved and merged to `main`, delete working branch.Centralised workflow: Git PrimerThis approach is what I think of as an introductory workflow. What I mean is that the  trunk is the only point where changes enter the repository. A single  branch is used for all development and all changes are committed to this branch, ignoring the existence of branching (we ignore software features all the time, right?).This isn’t an approach you’ll find being used by high-velocity dev teams or continuous delivery teams. So you might be wondering — is there ever good reason for a centralised Git workflow?Two use-cases come to mind.First, centralised Git workflows can streamline the initial explorations of a very small team. When the focus is on rapid prototyping and the risk of conflicts is minimal — as in a project’s early days — a centralised workflow can be convenient.And second, using a centralised Git workflow can be a good way to migrate a team onto version control because it doesn’t require any branches other than . Just use with caution as things can quickly go pear shaped. As the codebase grows or as more people contribute there’s an greater risk of code conflicts and accidental overwrites.Otherwise, centralised Git workflows are generally not recommended for sustained development, especially in a team setting.A centralised workflow might look like this:# In your terminal:

$ git checkout <main> # Switches onto `main` branch

# Create and activate your virtual environment. Pip install any required packages.

$ python3 -m venv <new_venv_name>
$ source new_venv_name/bin/activate
$ pip install requirements.txt (or <packages>)

# Make changes to code
# Regularly stage and commit your code changes, and push to remote. For example:

$ git add <file> # Stages the file to prepare repo snapshot for commit
$ git commit -m "<Your descriptive message>" # Records file snapshots into your version history
$ git push # Sends local commits to the remote repository; to whichever branch you're working on. In this case, the `main` branchML workflows: Branching ExperimentsData scientists and Mlops teams have a somewhat unique use-case compared to traditional software development teams. The development of machine learning and AI projects is inherently experimental. So from a Git workflow perspective, protocols need to flex to accommodate frequent iteration and complex branching strategies. You may also need the ability to track more than code, like experiment results, data, or model artifacts.Feature branching augmented with experiment branches is probably the most popular approach.This approach starts with the familiar feature branching workflow. Then within a feature branch, you create sub-branches for specific experiments. Think: “experiment_hyperparam_tuning”, or “experiment_xgboost”. This workflow affords enough granularity and flexibility to track individual experiments. And as with standard feature branches, this isolates development allowing experimental approaches to be explored without affecting the main codebase or other developers’ work.But : I said it was popular, but that doesn’t mean the branching experiments workflow is simple to manage. It can all turn to a tangled mess of spaghetti-branches if things are allowed to grow overly complex. This workflow involves frequent branching and merging, which can feel like unnecessary overhead in the face of rapid experimentation.A branching experiments workflow might look like this:# In your terminal:

$ git checkout <feature_branch> # Move onto a feature branch ready for ML experiments
$ git switch <experiment_branch> # Creates and switches onto a new branch for experiments

# Create and activate your virtual environment. Pip install any required packages.
# Make changes to your code in feature branch.
# Continue as in Feature Branching workflow.Integrating tools like MLflow into a feature branching workflow or branching experiments workflow offers additional possibilities. Reproducibility is a key concern for ML projects, which is why tools like MLflow exist. To help manage the full machine learning lifecycle.For our workflows, MLflow enhances our capabilities by enabling experiment tracking, logging model runs in the registry, and comparing the performance of various model specifications.For a branching experiments workflow, the MLflow integration might look like this:# In your terminal:

$ git checkout <feature_branch> # Move onto a feature branch ready for ML experiments
$ git switch <experiment_branch> # Creates and switches onto a new branch for experiments

# Create and activate your virtual environment. Pip install any required packages.
# Initialise MLflow within your Python script.
# Make changes to branch. As you experiment with different hyperparameters or model architectures, create new experiment branches and log the results with MLflow.
# Regularly stage and commit your code changes and MLflow experiment logs. For example:

$ git add <file> <file> <file> # Stages the file to prepare repo snapshot for commit
$ git commit -m "<Your descriptive message>" # Records file snapshots into your version history
$ git push # Sends local commits to the remote repository; to your working branch

# Use the MLflow UI or API to compare the performance of different experiments within your feature branch. You may want to select the best-performing model based on the logged metrics.
# Merge experimental branch(es) into the parent feature branch. For example:

$ git checkout <feature_branch> # Switch back onto the parent feature branch
$ git merge <experiment_branch> # Merge experiment branch into the parent feature branch

# Raise Pull Request (PR) to merge it into `main` once the feature branch work is completed. Request reviewers. Delete merged branches.
# Deploy if applicable. If the model is ready for deployment, use the logged model artifact from MLflow to deploy it to a production environment.The Git workflows I’ve shared above should provide a good starting point for your team to streamline collaborative development and help them to build high-quality data and AI solutions. They’re not rigid templates, but rather adaptable frameworks. Try experimenting with different workflows. Then adjust them to craft the an approach that’s most effective for your needs.The alternative is too frightening, too messy, too slow to be sustainable. It’s holding you back. The ideal workflow will vary depending on your team’s size, structure, and project complexity. The specific needs of the project, such as the frequency of releases and the level of ML experimentation, will also influence your choice of workflow.Ultimately, the best Git workflow for any data or MLOps dev team is the one that suits the specific requirements and development process of that team.]]></content:encoded></item><item><title>It&apos;s about to get wild. Apply Hero&apos;s agents already submitted 1.6 million job applications</title><link>https://v.redd.it/dmf1ws3a9rie1</link><author>/u/MetaKnowing</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 18:57:12 +0000</pubDate><source url="https://www.reddit.com/r/artificial/top/?sort=top&amp;t=day&amp;limit=3">Reddit - AI</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[R] TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models</title><link>https://openreview.net/forum?id=cqsw28DuMW</link><author>/u/hardmaru</author><category>dev</category><category>ai</category><category>reddit</category><pubDate>Wed, 12 Feb 2025 16:51:08 +0000</pubDate><source url="https://www.reddit.com/r/MachineLearning/top/?sort=top&amp;t=day&amp;limit=3">Reddit - ML</source><content:encoded><![CDATA[No Acknowledgement Section:]]></content:encoded></item></channel></rss>
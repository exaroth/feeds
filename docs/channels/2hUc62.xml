<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Blog</title><link>https://konrad.website/feeds/</link><description></description><item><title>Coding Interviews were HARD Until I Learned These 20 Tips</title><link>https://blog.algomaster.io/p/20-coding-interviews-tips</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/61c3f6c0-4027-4d37-b4a7-a30fc183fa12_1602x1032.png" length="" type=""/><pubDate>Thu, 13 Feb 2025 17:30:27 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[I gave my first  in 2016—and failed. I failed the next five interviews as well before finally landing my first job at .Since then, I’ve interviewed with many companies and faced my fair share of rejections. However, over the years, my failure rate in coding interviews dropped significantly.By 2022, with just 1.5 months of focused preparation, I successfully cleared interviews at  and .Surprisingly, my success wasn’t due to a dramatic improvement in problem-solving skills. The real game-changer was my approach— and  during the interview.In this article, I’ll share  that made coding interviews significantly easier for me.These tips cover everything you need to know, including:How to systematically approach coding interview problemsKey concepts and patterns you should knowThe type of problems you should practiceHow to choose the right algorithm for a given problemTechniques to optimize your solutionHow to communicate your thought process effectivelyBy applying these strategies, you’ll be able to tackle coding interviews with confidence and massively increase your chances of success.In a coding interview, interviewers want to see how well you , , and  under pressure.Here's a breakdown of what they look for:Understanding the problem: Do you ask clarifying questions instead of making assumptions to ensure you fully understand the problem?: Can you decompose the problem into smaller, manageable parts?: Can you design an optimal solution in terms of time and space complexity?: Do you handle edge cases like empty inputs, duplicates, large values, or special conditions?: Can you explain why one approach is better than another?: Do you have a strong grasp of data structures and algorithms, and can you choose the right one for the problem?Can you quickly compute the time and space complexity of your solution?Explaining your thought process: Can you clearly articulate your approach and why it works?: Are you receptive to hints and able to adjust your approach accordingly?: Do you follow good coding practices (meaningful variable names, proper indentation, modular functions etc..)?Improving the initial solution: Can you optimize and refine your first solution when prompted?Are you able to tackle variations of the original problem?Can you manually walk through your code with sample inputs to verify correctness?Most coding interviews last Depending on the company and interviewer, you may be asked to solve 2-3easy/medium problems or 1 hard problem with follow-ups.Lets assume you are given one problem, with a follow up in a 45-minute interview. Here’s how you can optimally allocate your time:The interviewer may ask you to introduce yourself. Prepare a concise 1-2 minute introduction that highlights your background, experience, and key strengths. Practice it beforehand so that you can deliver it smoothly.Understand the Problem (5-10 mins):  Carefully read the problem statement, ask clarifying questions, and walk through sample inputs and expected outputs.Plan the Approach (10-20 mins): Brainstorm possible solutions, evaluate trade-offs, and discuss time and space complexity.Implement the Code (20-30 mins): Write a clean, modular and readable code.Dry-run your code with sample inputs, debug any issues, and ensure edge cases are handled.Follow-ups and Wrap Up (35-45 mins): Answer follow up questions, and ask thoughtful questions to the interviewer about the company, role, or team.One of the biggest mistakes candidates make in coding interviews is jumping into coding too soon.If you don't fully understand the question, you might end up solving the Here’s how to ensure you grasp the problem before coding:Read the Problem CarefullyTake a moment to absorb the problem statement. Rephrase it in your own words to confirm your understanding. Identify the expected input/output format and any hidden constraints.If anything is unclear, ask questions before diving into the solution. Interviewers appreciate when you seek clarity. Never assume details that aren’t explicitly mentioned in the problem statement.Common clarifications include:Are there duplicate values?Can the input be empty? If so, what should the output be?Should the solution handle negative numbers?Should the output maintain the original order of elements?Is the graph directed or undirected?Does the input contain only lowercase English letters, or can it have uppercase, digits, or special characters?What should happen if multiple solutions exist? Should I return any valid solution, or does the problem have specific requirements?Walk Through Input/Output ExamplesOnce you understand the problem statement and constraints, go over a few input and output examples to make sure you get it.Draw them out if it helps, especially for visual data structures like trees or graphs.Try to take examples that cover different scenarios of the problem. Think about any  that might come up.]]></content:encoded></item><item><title>Designer Spotlight: Jhosue Mesias</title><link>https://tympanus.net/codrops/2025/02/13/designer-spotlight-jhosue-mesias/</link><author>Jhosue Mesias</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/maison-lumiere-vid.mp4?x25555" length="" type=""/><pubDate>Thu, 13 Feb 2025 14:21:09 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[Exploring the art of digital design with Jhosue Mesias—pushing boundaries, crafting experiences, and redefining creativity.]]></content:encoded></item><item><title>How to add a directory to your PATH</title><link>https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/</link><author>Julia Evans</author><category>dev</category><category>blog</category><pubDate>Thu, 13 Feb 2025 12:27:56 +0000</pubDate><source url="https://jvns.ca/atom.xml">Dev - Julia Evans</source><content:encoded><![CDATA[I was talking to a friend about how to add a directory to your PATH today. It’s
something that feels “obvious” to me since I’ve been using the terminal for a
long time, but when I searched for instructions for how to do it, I actually
couldn’t find something that explained all of the steps – a lot of them just
said “add this to ”, but what if you’re not using bash? What if your
bash config is actually in a different file? And how are you supposed to figure
out which directory to add anyway?So I wanted to try to write down some more complete directions and mention some
of the gotchas I’ve run into over the years.Here’s a table of contents:step 1: what shell are you using?If you’re not sure what shell you’re using, here’s a way to find out. Run this:if you’re using , it’ll print out if you’re using , it’ll print out if you’re using , it’ll print out an error like “In fish, please use
$fish_pid” ( isn’t valid syntax in fish, but in any case the error
message tells you that you’re using fish, which you probably already knew)Also bash is the default on Linux and zsh is the default on Mac OS (as of
2024). I’ll only cover bash, zsh, and fish in these directions.step 2: find your shell’s config filein zsh, it’s probably in bash, it might be , but it’s complicated, see the note in the next sectionin fish, it’s probably ~/.config/fish/config.fish (you can run  if you want to be 100% sure)a note on bash’s config fileBash has three possible config files: , , and .If you’re not sure which one your system is set up to use, I’d recommend
testing this way:add  to your If you see “hi there”, that means  is being used! Hooray!Otherwise remove it and try the same thing with You can also try  if the first two options don’t work.(there are a lot of elaborate flow charts out there that explain how bash
decides which config file to use but IMO it’s not worth it and just testing is
the fastest way to be sure)step 3: figure out which directory to addLet’s say that you’re trying to install and run a program called 
and it doesn’t work, like this:$ npm install -g http-server
$ http-server
bash: http-server: command not found
How do you find what directory  is in? Honestly in general this is
not that easy – often the answer is something like “it depends on how npm is
configured”. A few ideas:Often when setting up a new installer (like , , , etc),
when you first set it up it’ll print out some directions about how to update
your PATH. So if you’re paying attention you can get the directions then.Sometimes installers will automatically update your shell’s config file
to update your  for youSometimes just Googling “where does npm install things?” will turn up the
answerSome tools have a subcommand that tells you where they’re configured to
install things, like:
Node/npm:  (then append )Go:  (then append )asdf: asdf info | grep ASDF_DIR (then append  and )step 3.1: double check it’s the right directoryOnce you’ve found a directory you think might be the right one, make sure it’s
actually correct! For example, I found out that on my machine,  is
in . I can make sure that it’s the right directory by trying to
run the program  in that directory like this:$ ~/.npm-global/bin/http-server
Starting up http-server, serving ./public
It worked! Now that you know what directory you need to add to your ,
let’s move to the next step!step 4: edit your shell configNow we have the 2 critical pieces of information we need:Which directory you’re trying to add to your PATH (like  )Where your shell’s config is (like , , or ~/.config/fish/config.fish)Now what you need to add depends on your shell:Open your shell’s config file, and add a line like this:export PATH=$PATH:~/.npm-global/bin/
(obviously replace  with the actual directory you’re trying to add)You can do the same thing as in bash, but zsh also has some slightly fancier
syntax you can use if you prefer:path=(
  $path
  ~/.npm-global/bin
)
In fish, the syntax is different:set PATH $PATH ~/.npm-global/bin
(in fish you can also use , some notes on that further down)step 5: restart your shellNow, an extremely important step: updating your shell’s config won’t take
effect if you don’t restart it!open a new terminal (or terminal tab), and maybe close the old one so you don’t get confusedRun  to start a new shell (or  if you’re using zsh, or  if you’re using fish)I’ve found that both of these usually work fine.And you should be done! Try running the program you were trying to run and
hopefully it works now.If not, here are a couple of problems that you might run into:problem 1: it ran the wrong programIf the wrong  of a is program running, you might need to add the
directory to the  of your PATH instead of the end.For example, on my system I have two versions of  installed, which I
can see by running :$ which -a python3
/usr/bin/python3
/opt/homebrew/bin/python3
The one your shell will use is the .If you want to use the Homebrew version, you need to add that directory
() to the  of your PATH instead, by putting this in
your shell’s config file (it’s  instead of the usual )export PATH=/opt/homebrew/bin/:$PATH
set PATH ~/.cargo/bin $PATH
problem 2: the program isn’t being run from your shellAll of these directions only work if you’re running the program . If you’re running the program from an IDE, from a GUI, in a cron job,
or some other way, you’ll need to add the directory to your PATH in a different
way, and the exact details might depend on the situation.use the full path to the program you’re running, like /home/bork/bin/my-programput the full PATH you want as the first line of your crontab (something like
PATH=/bin:/usr/bin:/usr/local/bin:….). You can get the full PATH you’re
using in your shell by running .I’m honestly not sure how to handle it in an IDE/GUI because I haven’t run into
that in a long time, will add directions here if someone points me in the right
direction.problem 3: duplicate  entries making it harder to debugIf you edit your path and start a new shell by running  (or , or
), you’ll often end up with duplicate  entries, because the shell
keeps adding new things to your  every time you start your shell.Personally I don’t think I’ve run into a situation where this kind of
duplication breaks anything, but the duplicates can make it harder to debug
what’s going on with your  if you’re trying to understand its contents.Some ways you could deal with this:If you’re debugging your , open a new terminal to do it in so you get
a “fresh” state. This should avoid the duplication.Deduplicate your  at the end of your shell’s config  (for example in
zsh apparently you can do this with )Check that the directory isn’t already in your  when adding it (for
example in fish I believe you can do this with fish_add_path --path /some/directory)How to deduplicate your  is shell-specific and there isn’t always a
built in way to do it so you’ll need to look up how to accomplish it in your
shell.problem 4: losing your history after updating your Here’s a situation that’s easy to get into in bash or zsh:Run  to reload your configPress the up arrow a couple of times to rerun the failed command (or open a new terminal)The failed command isn’t in your history! Why not?This happens because in bash, by default, history is not saved until you exit
the shell.Some options for fixing this:Instead of running  to reload your config, run  (or
 in zsh). This will reload the config inside your current
session.Configure your shell to continuously save your history instead of only saving
the history when the shell exits. (How to do this depends on whether you’re
using bash or zsh, the history options in zsh are a bit complicated and I’m
not exactly sure what the best way is)When you install  (Rust’s installer) for the first time, it gives you
these instructions for how to set up your PATH, which don’t mention a specific
directory at all.This is usually done by running one of the following (note the leading DOT):

. "$HOME/.cargo/env"        	# For sh/bash/zsh/ash/dash/pdksh
source "$HOME/.cargo/env.fish"  # For fish
The idea is that you add that line to your shell’s config, and their script
automatically sets up your  (and potentially other things) for you.This is pretty common (for example Homebrew suggests you eval ), and there are
two ways to approach this:Just do what the tool suggests (like adding  to your shell’s config)Figure out which directories the script they’re telling you to run would add
to your PATH, and then add those manually. Here’s how I’d do that:
Run  in my shell (or the fish version if using fish)Run echo "$PATH" | tr ':' '\n' | grep cargo to figure out which directories it addedSee that it says  and shorten that to Add the directory  to PATH (with the directions in this post)I don’t think there’s anything wrong with doing what the tool suggests (it
might be the “best way”!), but personally I usually use the second approach
because I prefer knowing exactly what configuration I’m changing.fish has a handy function called  that you can run to add a directory to your  like this:fish_add_path /some/directory
This is cool (it’s such a simple command!) but I’ve stopped using it for a couple of reasons:Sometimes  will update the  for every session in the
future (with a “universal variable”) and sometimes it will update the 
just for the current session and it’s hard for me to tell which one it will
do. In theory the docs explain this but I could not understand them.Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if
you there are other major gotchas that have tripped you up when adding a
directory to your PATH, or if you have questions about this post!]]></content:encoded></item><item><title>GenAI Patterns: Reranker</title><link>https://martinfowler.com/articles/gen-ai-patterns/#reranker</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Thu, 13 Feb 2025 10:16:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[LLMs struggle with large amounts of context. Bharani
      Subramaniam and I explain how to mitigate this common RAG
      problem with a Reranker which takes the document
      fragments from the retriever, and ranks them according to their usefulness.]]></content:encoded></item><item><title>GenAI Patterns: Query Rewriting</title><link>https://martinfowler.com/articles/gen-ai-patterns/#query-rewrite</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 11 Feb 2025 20:58:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[Users often have difficulty writing the most effective queries.
       and I explain Query Rewriting:
      getting an LLM to formulate alternative queries to send to a RAG's
      retriever. ]]></content:encoded></item><item><title>Building Efficient Three.js Scenes: Optimize Performance While Maintaining Quality</title><link>https://tympanus.net/codrops/2025/02/11/building-efficient-three-js-scenes-optimize-performance-while-maintaining-quality/</link><author>Niccolò Fanton</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/02/Area-1.mp4?x25555" length="" type=""/><pubDate>Tue, 11 Feb 2025 14:38:37 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[A comprehensive look at how to optimize Three.js scenes using Fiber, Drei, and advanced tools, ensuring smooth performance while retaining high-quality visuals.]]></content:encoded></item><item><title>Stateful vs. Stateless Architecture</title><link>https://blog.algomaster.io/p/stateful-vs-stateless-architecture</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/7e4801c3-e3aa-4ab6-8fe6-759af4a1f91a_1684x1196.png" length="" type=""/><pubDate>Tue, 11 Feb 2025 08:46:26 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[When a client interacts with a server, there are two ways to handle it: The client includes all necessary data in each request, so the server doesn’t store any prior information. The server retains some data from previous requests, making future interactions dependent on past state.In software systems,  refers to any data that persists across requests, such as user sessions, shopping carts, or authentication details.The choice between stateless and stateful architecture can affect scalability, performance, complexity, and cost.In this article, we’ll break down both the approaches, their advantages and trade-offs, and when to use each—with real-world examples.If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .In a , the system remembers client or process data () across multiple requests.Once a client connects, the server holds on to certain details—like user preferences, shopping cart contents, or authentication sessions—so the client doesn’t need to resend everything with each request.Stateful systems typically store the state data in a database or in-memory storage. During online shopping, when you add items to your cart, the website remembers your selections. If you navigate away to browse more items and then return to your cart, your items are still there, waiting for you to check out.Common Patterns in Stateful ArchitectureIf you use  session storage (i.e., each app server keeps its own sessions locally), you can configure your load balancer for “sticky sessions.” This means: Once a client is assigned to , all subsequent requests from that client are routed to .: If Server A fails, the user’s session data is lost or the user is forced to re-log in. Sticky sessions are also less flexible when scaling because you can’t seamlessly redistribute user traffic to other servers.2. Centralized Session StoreA more robust approach is to store session data in a  or  store (e.g., Redis). : All servers can access and update session data for any user. Any server can handle any request, because the session data is not tied to a specific server’s memory.: You introduce network overhead and rely on an external storage. If the centralized storage fails, you lose session data unless you have a fallback strategy.Personalized Experiences: Stateful systems can deliver highly tailored interactions, as they remember user preferences and past actions. Users can seamlessly resume activities where they left off, even if they disconnect and reconnect. Certain operations can be faster because the server already possesses necessary data. Maintaining state for a large number of users can become resource-intensive and complex, as each server needs to keep track of specific sessions. Managing and synchronizing state across multiple servers (if needed) introduces additional challenges. If a server holding a user's state fails, their session data might be lost.E-commerce Shopping Carts – Stores cart contents and user preferences across multiple interactions, even if the user navigates away and returns.Video Streaming Services (Netflix, YouTube) – Remembers user watch progress, recommendations, and session data for a seamless experience.Messaging Apps (WhatsApp, Slack) – Maintains active user sessions and message history for real-time communication.In a  architecture, the server does  preserve client-specific data between individual requests.Each request is treated as , with no memory of previous interactions.Every request must include all necessary information for processing.Once the server responds, it discards any temporary data used for that request.: Most  follow a stateless design. For instance, when you request weather data from a public API, you must provide all required details (e.g., location) in each request. The server processes it, sends a response, and forgets the interaction.Common Patterns in Stateless Architecture1. Token-Based Authentication (JWT)A very popular way to implement statelessness is through tokens, particularly  (JSON Web Tokens):Client Authenticates Once: The user logs in using credentials (username/password) for the first time, and the server issues a signed .: The client includes JWT token in each request (e.g., Authorization: Bearer <token> header).: The server validates the token’s signature and any embedded claims (e.g., user ID, expiry time).: The server does  need to store session data; it just verifies the token on each request.Many APIs, including OAuth-based authentication systems, use JWTs to enable stateless, scalable authentication.Stateless architectures benefit from , ensuring that repeated requests produce the same result. This prevents inconsistencies due to network retries or client errors. A  request with the same payload  updates the user’s data but doesn’t create duplicates.Idempotent APIsensures consistency and reliability, especially in distributed systems where requests might be retried automatically. Stateless systems are inherently easier to scale horizontally. New servers can be added effortlessly, as they don't need to maintain any specific user sessions. Since servers don't track state, the architecture is generally simpler and easier to manage. The failure of a single server won't disrupt user sessions, as data isn't tied to specific servers.With no session data stored on the server, you free up memory that would otherwise be reserved for session management.Easier to Cache Responses: Since requests are self-contained, caching layers (like CDNs) can more easily store and serve responses. Stateless systems can't provide the same level of personalization or context awareness as stateful systems without additional effort (like using cookies or tokens).The client must keep track of the authentication token or relevant data. If it loses the token, it must re-authenticate. Every request needs to carry all the required information, potentially leading to larger payloads.Microservices Architecture: Each service handles requests independently, relying on external databases or caches instead of maintaining session data.Public APIs (REST, GraphQL): Clients send tokens with each request, eliminating the need for server-side sessions.Tokens are securely stored on the device and sent with every request to authenticate users.Stateless endpoints make caching easier since responses depend only on request parameters, not stored session data. A CDNcan cache and serve repeated requests, improving performance and reducing backend load.There's no one-size-fits-all answer when choosing between stateful and stateless architectures.The best choice depends on your application’s needs, scalability goals, and user experience expectations.When to Choose Stateful ArchitectureStateful systems are ideal when user context and continuity are critical. Consider a stateful approach if your application:Requires personalization (e.g., user preferences, session history)Needs real-time interactions (e.g., chat applications, multiplayer gaming)Manages multi-step workflows (e.g., online banking transactions, checkout processes)Must retain authentication sessions for security and convenience A shopping cart in an e-commerce app should persist, so users don’t have to re-add items after refreshing the page.When to Choose Stateless ArchitectureStateless systems work best when scalability, simplicity, and resilience are top priorities. Use a stateless approach if your application:Handles a high volume of requests and needs to scale efficientlyDoesn’t require storing client-specific data between requestsNeeds fast, distributed processing without server dependenciesMust ensure reliability and failover readiness A weather API doesn’t need to remember previous requests. Each query includes the location, and the response is processed independently.Hybrid Approaches: The Best of Both WorldsMany modern applications  stateful and stateless components for flexibility.This hybrid approach allows:Stateless APIs for core functionality, ensuring high scalabilityStateful sessions for personalization, improving user experienceExternal session stores (e.g., Redis) to manage state while keeping app servers stateless A video streaming platform (e.g., Netflix) uses a stateless backend for streaming but retains stateful user sessions to track watch history and recommendations.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re enjoying this newsletter and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>Creating My First Game Prototype in a Browser: The Journey So Far</title><link>https://tympanus.net/codrops/2025/02/10/creating-my-first-game-prototype-in-a-browser-the-journey-so-far/</link><author>Axel Croizé</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/02/fox_movement2.mp4?x25555" length="" type=""/><pubDate>Mon, 10 Feb 2025 14:10:43 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[An inside look at the creative process, challenges, and tools behind building a 3D game prototype from scratch.]]></content:encoded></item><item><title>The case for sans-io</title><link>https://fasterthanli.me/articles/the-case-for-sans-io</link><author>Amos Wenger</author><category>Faster than time blog</category><category>dev</category><category>rust</category><category>blog</category><pubDate>Fri, 7 Feb 2025 18:53:01 +0000</pubDate><source url="https://fasterthanli.me/index.xml">fasterthanli.me</source><content:encoded><![CDATA[The most popular option to decompress ZIP files from the Rust programming
language is a crate simply named zip — At the time of this writing, it has 48
million downloads. It’s fully-featured, supporting various compression methods,
encryption, and even supports writing zip files.However, that’s not the crate  uses to read ZIP files. Some
applications benefit from using asynchronous I/O, especially if they decompress
archives that they download from the network.]]></content:encoded></item><item><title>Developer Spotlight: Guillaume Lanier</title><link>https://tympanus.net/codrops/2025/02/07/developer-spotlight-guillaume-lanier/</link><author>Guillaume Lanier</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/Project_HP.mp4?x25555" length="" type=""/><pubDate>Fri, 7 Feb 2025 12:48:23 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[Introducing Guillaume Lanier, a Creative & Frontend Developer based in Amsterdam, who crafts immersive digital experiences through innovative 2D/3D graphics and interactive design.]]></content:encoded></item><item><title>Design a Real-Time Gaming Leaderboard - System Design Interview</title><link>https://blog.algomaster.io/p/design-real-time-gaming-leaderboard</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc07561-4165-42b9-a375-caaa33a2b6e3_2208x1668.png" length="" type=""/><pubDate>Thu, 6 Feb 2025 14:42:35 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[A  is a ranked list of players, typically sorted by a specific metric such as score, points or level. In a  leaderboard, updates happen almost instantly:A player’s score changes (e.g., after scoring a point or defeating an opponent).The system updates that player’s rank immediately.Other players can see the updated position without waiting or refreshing.This real-time aspect makes the user experience more dynamic and engaging. However, it also introduces significant , such as:Efficiently retrieving Top-N players (e.g., Top 10 or Top 100).Allowing players to quickly find their own rank without scanning the entire leaderboard.In this article, we will explore how to design a , , and  that can support above queries and enhance the user experience.Before diving into the design, lets clearly define the functional and non-functional requirements of our real-time gaming leaderboard..: Display top N players (e.g., top 10, top 100) on the leaderboard and update it in real-time.: Allow a player to query their current rank without scanning the entire leaderboard. Provide the ability to retrieve a “slice” of the leaderboard around a specific player (e.g., ranks 45 to 55 if the player is rank 50). Players can view past game scores and historical leaderboards for previous matches.Non-Functional Requirements Score changes should reflect immediately in the leaderboard.: Leaderboard queries should return the results in milliseconds.: System should support thousands of concurrent players submitting scores and fetching rankings.Approach to Designing the SystemThe most challenging aspects of building a real-time leaderboard is . Choosing the right storage system is critical to ensuring that queries can be executed efficiently without performance bottlenecks.To simplify the design process, we will follow below approach:Clearly define the input/output structure of leaderboard queries and updates.Define the High-Level Architecture: Identify core system components and their interactions.Choose the appropriate storage model optimized for fast leaderboard lookups and real-time updates.To support real-time leaderboard operations, we define a set of  that allow players to update scores, retrieve rankings, and query nearby ranks efficiently.Updates a player's score incrementally: POST /leaderboard/score/update{
  "playerId": "player123",
  "scoreDelta": 50
}We will use relative score updates () rather than absolute updates.{
  "playerId": "player123",
  "updatedScore": 1000,
  "currentRank": 10
}Retrieves the top-N players from the leaderboard, ranked by their scores.: GET /leaderboard/top?n=10:  = number of top players to fetch (default 10, max 100, etc.){
  "leaderboardId": "global",
  "topPlayers": [
    { "playerId": "playerA", "score": 1500, "rank": 1 },
    { "playerId": "playerB", "score": 1490, "rank": 2 },
    // ...
  ]
}Allows a player to retrieve their current rank without scanning the entire leaderboard.: GET /leaderboard/rank/{playerId}{
  "playerId": "player123",
  "score": 1000,
  "rank": 10
}Retrieves players ranked around a given player, allowing for comparison with competitors of similar skill levels.: GET /leaderboard/nearby/{playerId}?range=5:  indicates how many ranks above and below to fetch (e.g., 5 above, 5 below).{
  "playerId": "player123",
  "startRank": 45,
  "endRank": 55,
  "players": [
    { "playerId": "playerX", "score": 1020, "rank": 44 },
    { "playerId": "player123", "score": 1000, "rank": 45 },
    { "playerId": "playerZ", "score": 995,  "rank": 46 },
    // ...
  ]
} If a player is ranked 50th, this API allows them to see players ranked 45-55 for a competitive comparison.2.5 WebSockets for Real-Time UpdatesWhile REST APIs are good for on-demand queries, WebSockets or Server-Sent Events (SSEs) can push real-time leaderboard updates to subscribed users.Players would subscribe to leaderboard updates, and receive updates instantly without polling.]]></content:encoded></item><item><title>The DeepSeek Series: A Technical Overview</title><link>https://martinfowler.com/articles/deepseek-papers.html</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Thu, 6 Feb 2025 14:17:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[The appearance of DeepSeek Large-Language Models has caused a lot of
      discussion and angst since their latest versions appeared at the beginning
      of 2025. But much of the value of DeepSeek's work comes from the papers
      they have published over the last year. 
      provides an overview of these papers, highlighting three main arcs in this
      research: a focus on improving cost and memory efficiency, the use of HPC
      Co-Design to train large models on limited hardware, and the development
      of emergent reasoning from large-scale reinforcement learning]]></content:encoded></item><item><title>Building a Playful Stop-Motion Crayon Cursor in p5.js</title><link>https://tympanus.net/codrops/2025/02/06/building-a-playful-stop-motion-crayon-cursor-in-p5-js/</link><author>Jorge Toloza</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/stop-motion-crayon-cursor-result_smaller.mp4?x25555" length="" type=""/><pubDate>Thu, 6 Feb 2025 10:35:49 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[Create a playful stop-motion crayon cursor effect with p5.brush.js and smooth animations.]]></content:encoded></item><item><title>Docker Bake is Now Generally Available in Docker Desktop 4.38!</title><link>https://www.docker.com/blog/ga-launch-docker-bake/</link><author>Colin Hemmings</author><category>Docker blog</category><category>dev</category><category>docker</category><category>devops</category><pubDate>Wed, 5 Feb 2025 21:47:14 +0000</pubDate><source url="https://www.docker.com/">Docker</source><content:encoded><![CDATA[We’re excited to announce the General Availability of  with Docker Desktop 4.38! This powerful build orchestration tool takes the hassle out of managing complex builds and offers simplicity, flexibility, and performance for teams of all sizes.Docker Bake is an orchestration tool that streamlines Docker builds, similar to how Compose simplifies managing runtime environments. With Bake, you can define build stages and deployment environments in a declarative file, making complex builds easier to manage. It also leverages BuildKit’s parallelization and optimization features to speed up build times.While Dockerfiles are excellent for defining image build steps, teams often need to build multiple images and execute helper tasks like testing, linting, and code generation. Traditionally, this meant juggling numerous  commands with their own options and arguments – a tedious and error-prone process.Bake changes the game by introducing a declarative file format that encapsulates all options and image dependencies, referred to as . Additionally, Bake’s ability to parallelize and deduplicate work ensures faster and more efficient builds.Challenges with complex Docker Build configuration:Managing long, complex build commands filled with countless flags and environment variables.Tedious workflows for building multiple images.Difficulty declaring builds for specific targets or environments.Requires a script or 3rd-party tool to make things manageableDocker Bake tackles these challenges with a better way to manage complex builds with a simple, declarative approach.Key benefits of Docker Bake: Replace complex chains of Docker build commands and scripts with a single  command while maintaining clear, version-controlled configuration files that are easy to understand and modify.: Express sophisticated build logic through HCL syntax and matrix builds, enabling dynamic configurations that adapt to different environments and requirements while supporting custom functions for advanced use cases.: Maintain standardized build configurations across teams and environments through version-controlled files and inheritance patterns, eliminating environment-specific build issues and reducing configuration drift.: Automatically parallelize independent builds and eliminate redundant operations through context deduplication and intelligent caching, dramatically reducing build times for complex multi-image workflows.: One simple Docker buildx bake command to replace all the flags and environment variables.Use cases for Docker Bake1. Monorepo and Image BakeryDocker Bake can help developers efficiently manage and build multiple related Docker images from a single source repository. Plus, they can leverage shared configurations and automated dependency handling to enforce organizational standards. Teams can maintain consistent build logic across dozens or hundreds of microservices in a single repository, reducing configuration drift and maintenance overhead. Shared base images and contexts are automatically deduplicated, dramatically reducing build times and storage costs. Enforce organizational standards through inherited configurations, ensuring all services follow security, tagging, and testing requirements. A single source of truth for build configurations makes it easier to implement organization-wide changes like base image updates or security patches.Docker Bake provides seamless compatibility with existing docker-compose.yml files, allowing direct use of your current configurations. Existing Compose users are able to get started using Bake with minimal effort. Teams can incrementally adopt advanced build features while still leveraging their existing compose workflows and knowledge. Use the same configuration for both local development (via compose) and production builds (via Bake), eliminating “works on my machine” issues.: Access powerful features like matrix builds and HCL expressions while maintaining compatibility with familiar compose syntax.: Seamlessly integrate with existing CI/CD pipelines that already understand compose files while adding Bake’s advanced build capabilities.3. Complex build configurationsCross-Platform Compatibility: Matrix builds enable teams to efficiently manage builds across multiple architectures, OS versions, and dependency combinations from a single configuration. HCL expressions allow builds to adapt to different environments, git branches, or CI variables without maintaining multiple configurations. Custom functions enable sophisticated logic for things like version calculation, tag generation, and conditional builds based on git history. Variable validation and inheritance ensure consistent configuration across complex build scenarios, reducing errors and maintenance burden. Groups and targets help organize large-scale build systems with dozens or hundreds of permutations, making them manageable and maintainable.With Bake-optimized builds as the foundation, developers can achieve more efficient Docker Build Cloud performance and faster builds.Enhanced Docker Build Cloud Performance: Instantly parallelize matrix builds across cloud infrastructure, turning hour-long build pipelines into minutes without managing build infrastructure. Leverage Build Cloud’s distributed caching and deduplication to dramatically reduce bandwidth usage and build times, which is especially valuable for remote teams. Save cost with DBC  Bake’s precise target definitions mean you only consume cloud resources for exactly what needs to be built. Teams can run complex multi-architecture builds without powerful local machines, enabling development from any device while maintaining build performance. Offload resource-intensive builds from CI runners to Build Cloud, reducing CI costs and queue times while improving reliability.What’s New in Bake for GA?Docker Bake has been an experimental feature for several years, allowing us to refine and improve it based on user feedback. So, there is already a strong set of ingredients that users love, such as targets and groups, variables, HCL Expression Support, inheritance capabilities, matrix targets, and additional contexts. With this GA release, Bake is now ready for production use, and we’ve added several enhancements to make it more efficient, secure, and easier to use:Deduplicated Context Transfers Significantly speeds up build pipelines by eliminating redundant file transfers when multiple targets share the same build context. Enhances security and resource management by providing fine-grained control over what capabilities and resources builders can access during the build process. Simplifies configuration management by allowing teams to define reusable attribute sets that can be mixed, matched, and overridden across different targets. Prevents wasted time and resources by catching configuration errors before the actual build process begins.Deduplicate context transfersWhen you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This can significantly impact build time, depending on your build configuration.Previously, the workaround required users to define a named context that loads the context files and then have each target reference the named context. But with Bake, this will be handled automatically now.Bake can automatically deduplicate context transfers from targets sharing the same context. When you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This more efficient approach leads to much faster build time. Read more about how to speed up your build time in our docs. Bake now includes entitlements to control access to privileged operations, aligning with Build. This prevents unintended side effects and security risks. If Bake detects a potential issue — like a privileged access request or an attempt to access files outside the current directory — the build will fail unless explicitly allowed.To be consistent, the Bake command now supports the flag to grant access to additional entitlements. The following entitlements are currently supported for Bake.Build equivalents
 Allows executions with host networking.--allow security.insecure Allows executions without sandbox. (i.e. —privileged)File system: Grant filesystem access for builds that need access files outside the working directory. This will impact context, output, cache-from, cache-to, dockerfile, secret Grant read and write access to files outside the working directory. Grant read access to files outside the working directory.--allow fs.write=<path|*>  Grant write access to files outside the working directory.ssh
– Allows exposing SSH agent.Several attributes previously had to be defined in CSV (e.g. ). These were challenging to read and couldn’t be easily overridden. The following can now be defined as structured objects:target "app" {
		attest = [
			{ type = "provenance", mode = "max" },
			{ type = "sbom", disabled = true}
		]

		cache-from = [
			{ type = "registry", ref = "user/app:cache" },
			{ type = "local", src = "path/to/cache"}
		]

		cache-to = [
			{ type = "local", dest = "path/to/cache" },
		]

		output = [
			{ type = "oci", dest = "../out.tar" },
			{ type = "local", dest="../out"}
		]

		secret = [
			{ id = "mysecret", src = "path/to/secret" },
			{ id = "mysecret2", env = "TOKEN" },
		]

		ssh = [
			{ id = "default" },
			{ id = "key", paths = ["path/to/key"] },
		]
}As such, the attributes are now composable. Teams can mix, match, and override attributes across different targets which simplifies configuration management. target "app-dev" {
    attest = [
			{ type = "provenance", mode = "min" },
			{ type = "sbom", disabled = true}
		]
  }

  target "app-prod" {
    inherits = ["app-dev"]

    attest = [
			{ type = "provenance", mode = "max" },
		]
  }
Bake now supports validation for variables similar to Terraform to help developers catch and resolve configuration errors early. The GA for Bake also supports the following use cases.To verify that the value of a variable conforms to an expected type, value range, or other condition, you can define custom validation rules using the  block.variable "FOO" {
  validation {
    condition = FOO != ""
    error_message = "FOO is required."
  }
}

target "default" {
  args = {
    FOO = FOO
  }
}
To evaluate more than one condition, define multiple  blocks for the variable. All conditions must be true.variable "FOO" {
  validation {
    condition = FOO != ""
    error_message = "FOO is required."
  }
  validation {
    condition = strlen(FOO) > 4
    error_message = "FOO must be longer than 4 characters."
  }
}

target "default" {
  args = {
    FOO = FOO
  }
}
Dependency on other variablesYou can reference other Bake variables in your condition expression, enabling validations that enforce dependencies between variables. This ensures that dependent variables are set correctly before proceeding.variable "FOO" {}
variable "BAR" {
  validation {
    condition = FOO != ""
    error_message = "BAR requires FOO to be set."
  }
}

target "default" {
  args = {
    BAR = BAR
  }
}
In addition to updating the Bake configuration, we’ve added a new –list option. Previously, if you were unfamiliar with a project or wanted a reminder of the supported targets and variables, you would have to read through the file. Now, the list option will allow you to quickly query a list of them. It also supports the JSON format option if you need programmatic access.Quickly get a list of the targets available in your Bake configuration.docker buildx bake --list targetsdocker buildx bake --list type=targets,format=jsonGet a list of variables available for your Bake configuration.docker buildx bake --list variablesdocker buildx bake --list type=variables,format=jsonThese improvements build on a powerful feature set, ensuring Bake is both reliable and future-ready.Get started with Docker BakeReady to simplify your builds? Update to Docker Desktop 4.38 today and start using Bake. With its declarative syntax and advanced features, Docker Bake is here to help you build faster, more efficiently, and with less effort.Explore the documentation to learn how to create your first Bake file and experience the benefits of streamlined builds firsthand.Let’s bake something amazing together!]]></content:encoded></item><item><title>Docker Desktop 4.38: New AI Agent, Multi-Node Kubernetes, and Bake in GA</title><link>https://www.docker.com/blog/docker-desktop-4-38/</link><author>Yiwen Xu</author><category>Docker blog</category><category>dev</category><category>docker</category><category>devops</category><pubDate>Wed, 5 Feb 2025 21:42:31 +0000</pubDate><source url="https://www.docker.com/">Docker</source><content:encoded><![CDATA[At Docker, we’re committed to simplifying the developer experience and empowering enterprises to scale securely and efficiently. With the Docker Desktop 4.38 release, teams can look forward to improved developer productivity and enterprise governance. We’re excited to announce the General Availability of Bake, a powerful feature for optimizing build performance and multi-node Kubernetes testing to help teams “shift left.” We’re also expanding availability for several enterprise features designed to boost operational efficiency. And last but not least, Docker AI Agent (formerly Project: Agent Gordon) is now in Beta, delivering intelligent, real-time Docker-related suggestions across Docker CLI, Desktop, and Hub. It’s here to help developers navigate Docker concepts, fix errors, and boost productivity.Docker’s AI Agent boosts developer productivity  We’re thrilled to introduce Docker AI Agent (also known as Project: Agent Gordon) — an embedded, context-aware assistant seamlessly integrated into the Docker suite. Available within Docker Desktop and CLI, this innovative agent delivers real-time, tailored guidance for tasks like container management and Docker-specific troubleshooting — eliminating disruptive context-switching. Docker AI agent can be used for every Docker-related concept and technology, whether you’re getting started, optimizing an existing Dockerfile or Compose file, or understanding Docker technologies in general. By addressing challenges precisely when and where developers encounter them, Docker AI Agent ensures a smoother, more productive workflow. The first iteration of Docker’s AI Agent is now available in Beta for all signed-in usersThe agent is disabled by default, so user activation is required. Read more about Docker’s New AI Agent and how to use it to accelerate developer velocity here. Figure 1: Asking questions to Docker AI Agent in Docker DesktopSimplify build configurations and boost performance with Docker BakeDocker Bake is an orchestration tool that simplifies and speeds up Docker builds. After launching as an experimental feature, we’re thrilled to make it generally available with exciting new enhancements.While Dockerfiles are great for defining build steps, teams often juggle docker build commands with various options and arguments — a tedious and error-prone process. Bake changes the game by introducing a declarative file format that consolidates all options and image dependencies (also known as targets) in one place. No more passing flags to every build command! Plus, Bake’s ability to parallelize and deduplicate work ensures faster and more efficient builds.Key benefits of Docker Bake Abstract complex build configurations into one simple command. Write build configurations in a declarative syntax, with support for custom functions, matrices, and more. Share and maintain build configurations effortlessly across your team. Bake parallelizes multi-image workflows, enabling faster and more efficient builds.Developers can simplify multi-service builds by integrating Bake directly into their Compose files — Bake supports Compose files natively. It enables easy, efficient building of multiple images from a single repository with shared configurations. Plus, it works seamlessly with Docker Build Cloud locally and in CI. With Bake-optimized builds as the foundation, developers can achieve more efficient Docker Build Cloud performance and faster builds.Shift Left with Multi-Node Kubernetes testing in Docker DesktopIn today’s complex production environments, “shifting left”  is more essential than ever. By addressing concerns earlier in the development cycle, teams reduce costs and simplify fixes, leading to more efficient workflows and better outcomes. That’s why we continue to bring new features and enhancements to integrate feedback directly into the developer’s inner loopDocker Desktop now includes Multi-Node Kubernetes integration, enabling easier and extensive testing directly on developers’ machines. While single-node clusters allow for quick verification of app deployments, they fall short when it comes to testing resilience and handling the complex, unpredictable issues of distributed systems. To tackle this, we’re updating our Kubernetes distribution with  — a lightweight, fast, and user-friendly solution for local test and multi-node cluster simulations.Figure 2: Selecting Kubernetes version and cluster number for testingMulti-node cluster support: Replicate a more realistic production environment to test critical features like node affinity, failover, and networking configurations.Multiple Kubernetes versions: Easily test across different Kubernetes versions, which is a must for validating migration paths. Since  is an actively maintained open-source project, developers can update to the latest version on demand without waiting for the next Docker Desktop release.Head over to our documentation to discover how to use multi-node Kubernetes clusters for local testing and simulation.General availability of administration features for Docker Business subscriptionWith the Docker Desktop 4.36 release, we introduced Beta enterprise admin tools to streamline administration, improve security, and enhance operational efficiency. And the feedback from our Early Access Program customers has been overwhelmingly positive. For instance, enforcing sign-in with macOS configuration files and across multiple organizations makes deployment easier and more flexible for large enterprises. Also, the PKG installer simplifies managing large-scale Docker Desktop deployments on macOS by eliminating the need to convert DMG files into PKG first.Today, the features below are now available to all Docker Business customers.  Looking ahead, Docker is dedicated to continue expanding enterprise administration capabilities. Stay tuned for more announcements!Docker Desktop 4.38 reinforces our commitment to simplifying the developer experience while equipping enterprises with robust tools. With Bake now in GA, developers can streamline complex build configurations into a single command. The new Docker AI Agent offers real-time, on-demand guidance within their preferred Docker tools. Plus, with Multi-node Kubernetes testing in Docker Desktop, they can replicate realistic production environments and address issues earlier in the development cycle. Finally, we made a few new admin tools available to all our Business customers, simplifying deployment, management, and monitoring. We look forward to how these innovations accelerate your workflows and supercharge your operations! ]]></content:encoded></item><item><title>Introducing the Beta Launch of Docker’s AI Agent, Transforming Development Experiences</title><link>https://www.docker.com/blog/beta-launch-docker-ai-agent/</link><author>Jean-Laurent de Morlhon</author><category>Docker blog</category><category>dev</category><category>docker</category><category>devops</category><pubDate>Wed, 5 Feb 2025 21:36:29 +0000</pubDate><source url="https://www.docker.com/">Docker</source><content:encoded><![CDATA[For years, Docker has been an essential partner for developers, empowering everyone from small startups to the world’s largest enterprises. Today, AI is transforming organizations across industries, creating opportunities for those who embrace it to gain a competitive edge. Yet, for many teams, the question of where to start and how to effectively integrate AI into daily workflows remains a challenge. True to its developer-first philosophy, Docker is here to bridge that gap.We’re thrilled to introduce the beta launch of Docker AI Agent (also known as Project: Gordon)—an embedded, context-aware assistant seamlessly integrated into the Docker suite. Available within Docker Desktop and CLI, this innovative agent delivers tailored guidance for tasks like building and running containers, authoring Dockerfiles and Docker-specific troubleshooting—eliminating disruptive context-switching. By addressing challenges precisely when and where developers encounter them, Docker AI Agent ensures a smoother, more productive workflow.As the AI Agent evolves, enterprise teams will unlock even greater capabilities, including customizable features that streamline collaboration, enhance security, and help developers work smarter. With the Docker AI Agent, we’re making Docker even easier and more effective to use than it has ever been — AI accessible, actionable, and indispensable for developers everywhere.How Docker’s AI Agent Simplifies Development Challenges  Developing in today’s fast-paced tech landscape is increasingly complex, with developers having to learn an ever growing number of tools, libraries and technologies.By integrating a GenAI Agent into Docker’s ecosystem, we aim to provide developers with a powerful assistant that can help them navigate these complexities. The Docker AI Agent helps developers accelerate their work, providing real-time assistance, actionable suggestions, and automations that remove many of the manual tasks associated with containerized application development. Delivering the most helpful, expert-level guidance on Docker-related questions and technologies, Gordon serves as a powerful support system for developers, meeting them exactly where they are in their workflow. If you’re a developer who favors graphical interfaces, Docker Desktop AI UI will help you navigate container running issues, image size management and more generic Dockerfile oriented questions. If you’re a command line interface user, you can call, and share context with the agent directly in your favorite terminal.So what can Docker’s AI Agent do today? We’re delivering an expert assistant for every Docker-related concept and technology, whether it’s getting started, optimizing an existing Dockerfile or Compose file, or understanding Docker technologies in general. With Docker AI Agent, you also have the ability to delegate actions while maintaining full control and review over the process.A first example, if you want to run a container from an image, our agent can suggest the most appropriate  command tailored to your needs. This eliminates the guesswork or the need to search Docker Hub, saving you time and effort. The result combines a custom prompt, live data from Docker Hub, Docker container expertise and private usage insights, unique to Docker Inc.We’ve intentionally designed the output to be concise and actionable, avoiding the overwhelming verbosity often associated with AI-generated commands. We also provide sources for most of the AI agent recommendations, pointing directly to our documentation website. Our goal is to continuously refine this experience, ensuring that Docker’s AI Agent always provides the best possible command based on your specific local context.Beside helping you run containers, the Docker AI Agent can today:Explain, Rate and optimize Dockerfile leveraging the latest version of Docker.Help you run containers in an effective, concise way, leveraging the local context (checking port already used or volumes).Answers any docker related questions with the latest version of our documentations for our whole tool suite, and as such is able to answer any kind of questions on Docker tools and technologies.Containerize a software project helping you run your software in containers.Helps on Docker related Github Actions.Suggest fix when a container is failing to start in Docker Desktop.Provides contextual help for containers, images and volumes.Can augment its answer with per directory MCP servers (see doc).For the node expert, in the above screenshot the AI is recommending node 20.12 which is not the latest version but the one the AI found in the .With every future version of Docker Desktop and thanks to the feedback that you provide, the agent will be able to do so much more in the future.How can you try Docker AI Agent? This first beta release of Docker AI Agent is now progressively available for By default, the Docker AI agent is disabled. To enable it you will need to follow the steps below. Here’s how to get started:Install or update to the latest release of Docker Desktop 4.38Enable Docker AI into Docker Desktop Settings -> Features in DevelopmentFor the best experience, ensure the Docker terminal is enabled by going to Settings → General* If you’re a business subscriber, your Administrator needs to enable the Docker AI Agent for the organization first. This can be done through the Settings Management. If this is your case, feel free to contact us through the support  for further information.Docker Agent’s Vision for 2025By 2025, we aim to expand the agent’s capabilities with features like customizing your experience with more context from your registry, enhanced GitHub Copilot integrations, and deeper presence across the development tools you already use. With regular updates and your feedback, Docker AI Agent is being built to become an indispensable part of your development process.For now this beta is the start of an exciting evolution in how we approach developer productivity. Stay tuned for more updates as we continue to shape a smarter, more streamlined way to build, secure, and ship applications. We want to hear from you, if you like or want more information you can contact us.]]></content:encoded></item><item><title>Some terminal frustrations</title><link>https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/</link><author>Julia Evans</author><category>dev</category><category>blog</category><pubDate>Wed, 5 Feb 2025 16:57:00 +0000</pubDate><source url="https://jvns.ca/atom.xml">Dev - Julia Evans</source><content:encoded><![CDATA[What’s the most frustrating thing about using the terminal for you?1600 people answered, and I decided to spend a few days categorizing all the
responses. Along the way I learned that classifying qualitative data is not
easy but I gave it my best shot. I ended up building a custom
tool to make it faster to categorize
everything.As with all of my surveys the methodology isn’t particularly scientific. I just
posted the survey to Mastodon and Twitter, ran it for a couple of days, and got
answers from whoever happened to see it and felt like responding.Here are the top categories of frustrations!I think it’s worth keeping in mind while reading these comments that40% of people answering this survey have been using the terminal for 95% of people answering the survey have been using the terminal for at least 4 yearsThese comments aren’t coming from total beginners.Here are the categories of frustrations! The number in brackets is the number
of people with that frustration. I’m mostly writing this up for myself because
I’m trying to write a zine about the terminal and I wanted to get a sense for
what people are having trouble with.People talked about struggles remembering:the syntax for CLI tools like awk, jq, sed, etckeyboard shortcuts for tmux, text editing, etcThere are just so many little “trivia” details to remember for full
functionality. Even after all these years I’ll sometimes forget where it’s 2
or 1 for stderr, or forget which is which for  and .switching terminals is hard (91)People talked about struggling with switching systems (for example home/work
computer or when SSHing) and running into:OS differences in keyboard shortcuts (like Linux vs Mac)systems which don’t have their preferred text editor (“no vim” or “only vim”)different versions of the same command (like Mac OS grep vs GNU grep)a shell they aren’t used to (“the subtle differences between zsh and bash”)as well as differences inside the same system like pagers being not consistent
with each other (git diff pagers, other pagers).I got used to fish and vi mode which are not available when I ssh into
servers, containers.Lots of problems with color, like:programs setting colors that are unreadable with a light background colorfinding a colorscheme they like (and getting it to work consistently across different apps)color not working inside several layers of SSH/tmux/etcnot wanting color at all and struggling to turn it offThis comment felt relatable to me:Getting my terminal theme configured in a reasonable way between the terminal
emulator and fish (I did this years ago and remember it being tedious and
fiddly and now feel like I’m locked into my current theme because it works
and I dread touching any of that configuration ever again).Half of the comments on keyboard shortcuts were about how on Linux/Windows, the
keyboard shortcut to copy/paste in the terminal is different from in the rest
of the OS.Some other issues with keyboard shortcuts other than copy/paste:using  in a browser-based terminal and closing the windowthe terminal only supports a limited set of keyboard shortcuts (no
, no , no , lots of  shortcuts aren’t
possible like )the OS stopping you from using a terminal keyboard shortcut (like by default
Mac OS uses  for something else)issues using emacs in the terminalbackspace not working (2)other copy and paste issues (75)Aside from “the keyboard shortcut for copy and paste is different”, there were
a lot of OTHER issues with copy and paste, like:how tmux and the terminal emulator both do copy/paste in different waysdealing with many different clipboards (system clipboard, vim clipboard, the
“middle click” clipboard on Linux, tmux’s clipboard, etc) and potentially
synchronizing themrandom spaces added when copying from the terminalpasting multiline commands which automatically get run in a terrifying waywanting a way to copy text without using the mouseThere were lots of comments about this, which all came down to the same basic
complaint – it’s hard to discover useful tools or features! This comment kind of
summed it all up:How difficult it is to learn independently. Most of what I know is an
assorted collection of stuff I’ve been told by random people over the years.steep learning curve (44)A lot of comments about it generally having a steep learning curve. A couple of
example comments:After 15 years of using it, I’m not much faster than using it than I was 5 or
maybe even 10 years ago.That I know I could make my life easier by learning more about the shortcuts
and commands and configuring the terminal but I don’t spend the time because it
feels overwhelming.Some issues with shell history:history not being shared between terminal tabs (16)limits that are too short (4)history not being restored when terminal tabs are restoredlosing history because the terminal crashednot knowing how to search historyIt wasted a lot of time until I figured it out and still annoys me that
“history” on zsh has such a small buffer;  I have to type “history 0” to get
any useful length of history.documentation being generally opaquelack of examples in man pagesprograms which don’t have man pagesHere’s a representative comment:Finding good examples and docs. Man pages often not enough, have to wade
through stack overflowA few issues with scrollback:programs printing out too much data making you lose scrollback historyresizing the terminal messes up the scrollbackGUI programs that you start in the background printing stuff out that gets in
the way of other programs’ outputsWhen resizing the terminal (in particular: making it narrower) leads to
broken rewrapping of the scrollback content because the commands formatted
their output based on the terminal window width.Lots of comments about how the terminal feels hampered by legacy decisions and
how users often end up needing to learn implementation details that feel very
esoteric. One example comment:Most of the legacy cruft, it would be great to have a green field
implementation of the CLI interface.Lots of complaints about POSIX shell scripting. There’s a general feeling that
shell scripting is difficult but also that switching to a different less
standard scripting language (fish, nushell, etc) brings its own problems.Shell scripting. My tolerance to ditch a shell script and go to a scripting
language is pretty low. It’s just too messy and powerful. Screwing up can be
costly so I don’t even bother.Some more issues that were mentioned at least 10 times:(31) inconsistent command line arguments: is it -h or help or –help?(24) keeping dotfiles in sync across different systems(23) performance (e.g. “my shell takes too long to start”)(20) window management (potentially with some combination of tmux tabs, terminal tabs, and multiple terminal windows. Where did that shell session go?)(17) generally feeling scared/uneasy (“The debilitating fear that I’m going
to do some mysterious Bad Thing with a command and I will have absolutely no
idea how to fix or undo it or even really figure out what happened”)(16) terminfo issues (“Having to learn about terminfo if/when I try a new terminal emulator and ssh elsewhere.”)(16) lack of image support (sixel etc)(15) SSH issues (like having to start over when you lose the SSH connection)(15) various tmux/screen issues (for example lack of integration between tmux and the terminal emulator)(13) the terminal getting messed up for various reasons (pressing , ing a binary, etc)(12) quoting/escaping in the shell(11) various Windows/PowerShell issuesThere were also 122 answers to the effect of “nothing really” or “only that I
can’t do EVERYTHING in the terminal”Think I’ve found work arounds for most/all frustrationsI’m not going to make a lot of commentary on these results, but here are a
couple of categories that feel related to me:remembering syntax & history (often the thing you need to remember is something you’ve run before!)discoverability & the learning curve (the lack of discoverability is definitely a big part of what makes it hard to learn)“switching systems is hard” & “it feels outdated” (tools that haven’t really
changed in 30 or 40 years have many problems but they do tend to be always
 no matter what system you’re on, which is very useful and makes them
hard to stop using)Trying to categorize all these results in a reasonable way really gave me an
appreciation for social science researchers’ skills.]]></content:encoded></item><item><title>GenAI Patterns: Hybrid Retriever</title><link>https://martinfowler.com/articles/gen-ai-patterns/#RagInPractice</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 5 Feb 2025 15:03:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[Today  and I outline four
      limitations to the simple RAG from yesterday, and the pattern that
      addresses the first of these: Hybrid Retriever. This tackles the
      inefficiencies of embeddings-based search by combining it with other
      search techniques.]]></content:encoded></item><item><title>GenAI Patterns: RAG Limitations and Hybrid Retriever</title><link>https://martinfowler.com/articles/gen-ai-patterns/#RagInPractice</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 5 Feb 2025 15:03:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[Today  and I outline four
      limitations to the simple RAG from yesterday, and the pattern that
      addresses the first of these: Hybrid Retriever. This tackles the
      inefficiencies of embeddings-based search by combining it with other
      search techniques.]]></content:encoded></item><item><title>Players Club: A Free Astro Template for Showcasing Music Artists</title><link>https://tympanus.net/codrops/2025/02/05/players-club-free-astro-template/</link><author>Manoela Ilic</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/02/PlayersClubNav.mp4?x25555" length="" type=""/><pubDate>Wed, 5 Feb 2025 13:14:12 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[Players Club is a free Astro template for showcasing music artists—an experimental proof of concept built in collaboration with Alex Tkachev.]]></content:encoded></item><item><title>Panel at goto Copenhagen: &quot;Where is SW development Going</title><link>https://www.youtube.com/watch?v=86-Dy5U2p5Y</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 4 Feb 2025 15:23:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[ was on a panel at goto Copenhagen last September with Holly Cummings,
      Trisha Gee, Dave Farley, and Daniel Terhorst-North. We discussed the
      current state of software development and where it was heading. Given the
      timing, there was much discussion about the role AI would play in our
      profession's future.]]></content:encoded></item><item><title>Retrieval Augmented Generation (RAG)</title><link>https://martinfowler.com/articles/gen-ai-patterns/#rag</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 4 Feb 2025 14:57:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[A pre-trained GenAI model lacks recent and specific information about a
      domain.  and I explain how Retrieval
      Augmented Generation (RAG) can fill that gap.]]></content:encoded></item><item><title>GenAI Patterns: Retrieval Augmented Generation (RAG)</title><link>https://martinfowler.com/articles/gen-ai-patterns/#rag</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 4 Feb 2025 14:57:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[A pre-trained GenAI model lacks recent and specific information about a
      domain.  and I explain how Retrieval
      Augmented Generation (RAG) can fill that gap.]]></content:encoded></item><item><title>How to Make The Fluffiest Grass With Three.js</title><link>https://tympanus.net/codrops/2025/02/04/how-to-make-the-fluffiest-grass-with-three-js/</link><author>The Ebenezer</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/1737385342684984.mp4?x25555" length="" type=""/><pubDate>Tue, 4 Feb 2025 12:55:00 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[A breakdown of the best techniques to create realistic grass in Three.js without killing performance.]]></content:encoded></item><item><title>What are ACID Transactions in Databases?</title><link>https://blog.algomaster.io/p/what-are-acid-transactions-in-databases</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9ceb65c-70e6-4f3e-9511-f6bc5da93d13_1308x1086.png" length="" type=""/><pubDate>Tue, 4 Feb 2025 05:31:07 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[Imagine you’re running an e-commerce application. A customer places an order, and your system needs to deduct the item from inventory, charge the customer’s credit card, and record the sale in your accounting system—all at once. What happens if the payment fails but your inventory count has already been reduced? Or if your application crashes halfway through the process?This is where  come into play. They ensure that all the steps in such critical operations happen reliably and consistently.ACID is an acronym that refers to the set of 4 key properties that define a transaction: Atomicity, Consistency, Isolation, and In this article, we’ll dive into what each of the ACID properties mean, why they are important, and how they are implemented in databases.If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .A  in the context of databases is a sequence of one or more operations (such as inserting, updating, or deleting records) that the database treats as . It either fully succeeds or fully fails, with no in-between states.When you send money to a friend, two things happen:Money is deducted from your account.Money is added to their account.These two steps form . If either step fails, both are canceled.Without transactions, databases could end up in inconsistent states. : Your money is deducted, but your friend never receives it.: Two people booking the last movie ticket at the same time.Transactions solve these problems by enforcing rules like  (Atomicity, Consistency, Isolation, Durability).Now, lets looks at each of the ACID properties.Atomicity ensures that a transaction—comprising multiple operations—executes as a unit of work: it either  succeeds (commits) or  fails (rolls back). If any part of the transaction fails, the entire transaction is rolled back, and the database is restored to a state exactly as it was before the transaction began.In a money transfer transaction, if the credit step fails, the debit step cannot be allowed to stand on its own. This prevents inconsistent states like “money disappearing” from one account without showing up in another.Atomicity abstracts away the complexity of manually undoing changes if something goes wrong.How Databases Implement AtomicityDatabases use two key mechanisms to guarantee atomicity.1. Transaction Logs (Write-Ahead Logs)Every operation is recorded in a  before it’s applied to the actual database table.If a failure occurs, the database uses this log to  incomplete changes.Once the WAL entry is safely on disk, the database proceeds with modifying the in-memory pages that contain rows for  and .When the operations succeed:The database marks  as  in the transaction log.The newly updated balances for A and B will eventually get flushed from memory to their respective data files on disk.If the database crashes  the log entry is written but  the data files are fully updated, the WAL provides a way to recover:On restart, the database checks the WAL.It sees  was committed.It reapplies the  operations to ensure the final balances are correct in the data files.If the transaction had not committed (or was marked as “in progress”) at the time of the crash, the database would  those changes using information in the log, leaving the table as if the transaction never happened.2. Commit/Rollback ProtocolsDatabases provide commands like , , and Any changes made between  and  are considered “in-progress” and won’t be permanently applied unless the transaction commits successfully.If any step fails, or if you explicitly issue a , all changes since the start of the transaction are undone. in the context of ACID transactions ensures that any transaction will bring the database from one valid state to another valid state—never leaving it in a broken or “invalid” state.It means that all the data integrity constraints, such as  (no duplicate IDs),  (related records must exist in parent tables), and (age can’t be negative), are satisfied before and after the transaction.If a transaction tries to violate these rules, it will not be committed, and the database will revert to its previous state.You have two tables in an e-commerce database: (with columns: , , etc.) (with columns: , , , etc.): You can’t place an order for a product if  is greater than the  in the  table.If the product’s  was 8 (less than what we’re trying to order), the database sees that the new value would be  which breaks the consistency rule (it should not go negative).The transaction fails or triggers a rollback, preventing the database from ending in an invalid state.How to Implement ConsistencyDatabase Schema Constraints, , , ,  constraints, and other schema definitions ensure no invalid entries are allowed.Triggers and Stored ProceduresTriggers can automatically check additional rules whenever rows are inserted, updated, or deleted.Stored procedures can contain logic to validate data before committing.Application-Level SafeguardsWhile the database enforces constraints at a lower level, applications often add extra checks—like ensuring business rules are followed or data is validated before it even reaches the database layer. ensures that concurrently running transactions do not interfere with each other’s intermediate states.Essentially, while a transaction is in progress, its updates (or intermediate data) remain invisible to other ongoing transactions—giving the illusion that each transaction is running sequentially, one at a time.Without isolation, two or more transactions could read and write partial or uncommitted data from each other, causing incorrect or inconsistent results.With isolation, developers can reason more reliably about how data changes will appear to other transactions.To understand how isolation works, it helps to see what can go wrong without proper isolation.  Common concurrency anomalies include:Transaction A reads data that Transaction B has modified but not yet committed.If Transaction B then rolls back, Transaction A ends up holding an invalid or “dirty” value that never truly existed in the committed state.Transaction A reads the same row(s) multiple times during its execution but sees different data because another transaction updated or deleted those rows in between A’s reads.Transaction A performs a query that returns a set of rows. Another transaction inserts, updates, or deletes rows that match A’s query conditions.If A re-runs the same query, it sees a different set of rows (“phantoms”).Databases typically allow you to choose an , which balances data correctness with performance.Higher isolation levels provide stronger data consistency but can reduce system performance by increasing the wait times for transactions. Let's explore the four common isolation levels:Allows dirty reads; transactions can see uncommitted changes.Rarely used, as it can lead to severe anomalies.A transaction sees only data that has been committed at the moment of reading.Prevents dirty reads, but non-repeatable reads and phantom reads can still occur.Ensures if you read the same rows multiple times within a transaction, you’ll get the same values (unless you explicitly modify them).Prevents dirty reads and non-repeatable reads, but phantom reads may still happen (depending on the database engine).The highest level of isolation, acting as if all transactions happen sequentially one at a time.Prevents dirty reads, non-repeatable reads, and phantom reads.Most expensive in terms of performance and concurrency because it can require more locking or more conflict checks.How Databases Enforce IsolationPessimistic Concurrency ControlRows or tables are locked so that no other transaction can read or write them until the lock is released.Can lead to blocking or deadlocks if multiple transactions compete for the same locks.2. MVCC (Multi-Version Concurrency Control)Optimistic Concurrency ControlInstead of blocking reads, the database keeps multiple versions of a row.Readers see a consistent snapshot of data (like a point-in-time view), while writers create a new version of the row when updating.This approach reduces lock contention but requires carefully managing row versions and cleanup (vacuuming in PostgreSQL, for example).A form of MVCC where each transaction sees data as it was at the start (or a consistent point) of the transaction.Prevents non-repeatable reads and dirty reads. Phantom reads may still occur unless the isolation level is fully serializable. ensures that once a transaction has been committed, the changes it made will survive, even in the face of power failures, crashes, or other catastrophic events. In other words, once a transaction says “done,” the data is permanently recorded and cannot simply disappear.How Databases Ensure Durability1. Transaction Logs (Write-Ahead Logging)Most relational databases rely on a  to preserve changes before they’re written to the main data files:: The intended operations (updates, inserts, deletes) are recorded in the WAL on durable storage (disk).: Once the WAL entry is safely persisted, the database can mark the transaction as committed.Apply Changes to Main Data Files: The updated data eventually gets written to the main files—possibly first in memory, then flushed to disk.If the database crashes, it uses the WAL during :: Any committed transactions not yet reflected in the main files are reapplied.: Any incomplete (uncommitted) transactions are rolled back to keep the database consistent.2. Replication / RedundancyIn addition to WAL, many systems use replication to ensure data remains durable even if hardware or an entire data center fails.: Writes are immediately copied to multiple nodes or data centers. A transaction is marked committed only if the primary and at least one replica confirm it’s safely stored.: Changes eventually sync to other nodes, but there is a (small) window where data loss can occur if the primary fails before the replica is updated.Regular  provide a safety net beyond logs and replication. In case of severe corruption, human error, or catastrophic failure:: Capture the entire database at a point in time.Incremental/Differential Backups: Store changes since the last backup for faster, more frequent backups.: Ensures backups remain safe from localized disasters, allowing you to restore data even if hardware is damaged.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re finding this newsletter helpful and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>Building an On-Scroll 3D Circle Text Animation with Three.js and Shaders</title><link>https://tympanus.net/codrops/2025/02/03/building-an-on-scroll-3d-circle-text-animation-with-three-js-and-shaders/</link><author>David Faure</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/Amplitude-management-for-animation.mp4?x25555" length="" type=""/><pubDate>Mon, 3 Feb 2025 13:17:08 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[Learn how to create a circular text animation in 3D space using Three.js, shaders, and msdf-text-utils.]]></content:encoded></item><item><title>Nim: A Personal Website Template Built with Motion-Primitives</title><link>https://tympanus.net/codrops/2025/02/01/nim-nextjs-react-tailwind-motion-template/</link><author>ibelick</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/nim-text-morph.mp4?x25555" length="" type=""/><pubDate>Sat, 1 Feb 2025 12:00:00 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[Nim is a free, open-source personal website template built with Next.js 15, React 19, Tailwind CSS v4, and Motion-Primitives, featuring subtle, pre-built animations.]]></content:encoded></item><item><title>Developer Spotlight: Quentin Hocdé</title><link>https://tympanus.net/codrops/2025/01/31/developer-spotlight-quentin-hocde/</link><author>Quentin Hocdé</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/04_tigermilk.mp4?x25555" length="" type=""/><pubDate>Fri, 31 Jan 2025 12:45:00 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[In this spotlight, Quentin Hocdé shares his passion for creative coding, animations, and bringing interactive experiences to life.]]></content:encoded></item><item><title>Case Study: Gianluca Gradogna — Portfolio ’25</title><link>https://tympanus.net/codrops/2025/01/30/case-study-gianluca-gradogna-portfolio-25/</link><author>Gianluca Gradogna</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/Codrops_Video_03.mp4?x42814" length="" type=""/><pubDate>Thu, 30 Jan 2025 14:00:00 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[This case study unveils the journey of creating a personal portfolio, highlighting the creative collaboration between Gianluca Gradogna and Gabriel Norman in bringing the project to life.]]></content:encoded></item><item><title>Slow, flaky, and failing</title><link>https://bitfieldconsulting.com/posts/slow-flaky-failing</link><author>John Arundel</author><category>dev</category><category>blog</category><pubDate>Thu, 30 Jan 2025 13:15:00 +0000</pubDate><source url="https://bitfieldconsulting.com/posts/">Dev - Bitfield</source><content:encoded><![CDATA[If you find yourself working on a project with quite a few broken
windows, it’s all too easy to slip into the mindset of “All the rest of
this code is crap, I’ll just follow suit.”
—David Thomas & Andrew Hunt, “The
Pragmatic Programmer: Your Journey to Mastery”It’s one minute to ship time, and you hit “push” on the very last
commit. There it goes: the build is running. Every second counts now,
and you watch the test output with increasing impatience. Why do the
tests take so darned ?And then, to your horror, the first red lights start to appear. “But
these were passing before, and I haven’t touched that code!” you wail.
It’s no good: your co-workers are already giving you the stink eye for
breaking the build and holding them up.You’re not going to ship today, for one simple reason: your tests are
slow, flaky, and failing. So what the hell?Flaky tests sometimes fail, sometimes pass, regardless of whether the
system is correct. There are many reasons for flaky tests, so let’s look
at a couple of them, with some possible solutions. can be a source of flakiness, as you probably
know from experience. In particular, fixed sleeps in tests are a bad
idea (see the next section for more about these). Eliminate these
wherever possible and replace them with code that only waits as long as
strictly necessary.When you need to  timing itself, use the shortest
possible interval. For example, don’t test a timer function with a
one-second duration when one millisecond would work just as well.In some tests, as crazy as it sounds, the  can
affect the test. One way to eliminate this cause of flakiness is by
turning , and if necessary injecting a fake
 function to return a canned time of day.Flakiness can also sometimes arise from .
Some data structures in Go are inherently unordered: maps, for example.
Comparing these needs special care.For example, iterating over a map comparing its elements is not good
enough: the iteration order of maps is unspecified in Go. Instead, we
can use the  function to compare maps regardless
of iteration order:On the other hand, slices  inherently ordered, and so
 requires this:But sometimes we don’t actually care about the order. Maybe we get
these results from some concurrent computations, and we don’t know what
order they will show up in. We just want to know that we 
the right results.To compare two slices for equal , then, regardless
of order, we can use  to sort them before the
comparison:Whatever the cause of a flaky test suite, it’s a serious problem.
Left untreated, it will continuously erode value from the tests, until
eventually they become useless and ignored by all. It should be a red
flag to hear something like “Oh yeah, that test just fails
sometimes.”As soon as you hear that, you know that the test has become useless.
Delete it, if the flakiness really can’t be fixed. Thou shalt not suffer
a flaky test to live. As soon as it starts flaking, it stops being a
useful source of feedback, and bad tests are worse than no tests.A  test is not the same thing as a flaky test: a
brittle test fails when you change something unrelated, whereas a flaky
test fails when it feels like it. Fixing brittle tests is usually a
matter of decoupling entangled components, or simply reducing the scope
(and thus sharpening the focus) of the test.On the other hand, flaky tests can require some time and effort to
find the underlying cause and address it. Only do this if the test is
really worth it; if not, just delete it.What if some tests aren’t just flaky, but fail all the time, because
bugs aren’t being fixed? This is a very dangerous situation, and without
prompt action the tests will rapidly become completely useless.Why? Because if tests are allowed to fail for a while without being
fixed, people soon stop trusting them, or indeed paying 
attention to them: “Oh yeah, that test always fails.”We can never have any failing tests, just as we can never have any
bugs:As soon as any test starts failing, fixing it should be everyone’s
top priority. No one is allowed to deploy any code change that’s not
about fixing this bug. Once you let one failing test slip through the
net, all the other tests become worthless.This so-called  sounds radical, but
it really isn’t. After all, what’s the alternative?The very first version of Microsoft Word for Windows was
considered a “death march” project. Managers were so insistent on
keeping to the schedule that programmers simply rushed through the
coding process, writing extremely bad code, because bug-fixing was not a
part of the formal schedule.Indeed, the schedule became merely a checklist of features waiting
to be turned into bugs. In the post-mortem, this was referred to as
“infinite defects methodology”.
—Joel Spolsky, “The
Joel Test: 12 Steps to Better Code”Fixing bugs now is cheaper, quicker, and makes more business sense
than fixing them later. The product should be ready to ship at all
times, without bugs.If you already  a large backlog of bugs, or failing
tests, but the company’s still in business, then maybe those bugs aren’t
really that critical after all. The best way out may be to declare
voluntary : just close all old bugs, or delete
all failing tests. Bugs that people  care about will pretty
soon be re-opened.My book The Power of Go: Tests is all
about how to write  tests: not just box-ticking
exercises to satisfy some bureaucratic manager, but tests that really
add value to the code, and make your work easier and more enjoyable.Even the world’s greatest test suite does us no good, though, if it
takes too long to run. How long is too long? Well, if we’re running
tests every few minutes, clearly even a few minutes is too long. We
simply won’t run the tests often enough to get the fast feedback we need
from them.By running the test suite frequently, at least several times a
day, you’re able to detect bugs soon after they are introduced, so you
can just look in the recent changes, which makes it much easier to find
them.
—Martin Fowler, “Self-Testing
Code”One way or the other, then, we don’t want to be more than about five
minutes away from passing tests. So, again, how long is 
long for a test suite to run?Kent Beck suggests that ten minutes is a psychologically significant
length of time:The equivalent of 9.8 m/s² is the ten-minute test suite. Suites
that take longer than ten minutes inevitably get trimmed, or the
application tuned up, so the suite takes ten minutes again.
—Kent Beck, “Test-Driven Development
by Example”We may perhaps call this psychological limit the .
Beyond the ten-minute mark, the problem is so obvious to everybody that
people are willing to put effort into speeding up the test suite. Below
that time, people will probably grumble but put up with it.That certainly doesn’t mean that a ten-minute test suite is okay:
it’s not, for the reasons we’ve discussed. Let’s look at a few simple
ways to reduce the overall run-time of the test suite to something more
manageable.. The inability to run certain
tests in parallel is usually a design smell. Refactor so that each test
has its own world, touches no global state, and can thus run in
parallel. Adding parallelism to a suite that doesn’t have it should
speed it up by about an order of magnitude.Eliminate unnecessary I/O. Once you go off the
chip, things get slow. Do everything on the chip as far as possible,
avoiding I/O operations such as network calls or accessing disk files.
For example, you could use an  as an in-memory
filesystem, and memory-backed s and
s instead of real files.. Instead of calling some
remote API, call a local fake instead. Local networking happens right in
the kernel, and while it’s still not , it’s a lot faster
than actually going out onto the wire.Share fixtures between tests. Any time you have
some expensive fixture setup to do, such as loading data into a
database, try to share its cost between as many tests as possible, so
that they can all use it. If necessary, do the setup in a single test
and then run a bunch of subtests against it.However, we need to be careful that the tests don’t then become flaky
as a result of too much fixture sharing. A flaky test is worse than a
slow test.. A test that can’t proceed until
some concurrent operation has completed should use the “wait for
success” pattern (loop and retry, with a tiny delay, until the operation
has completed). This minimises wasted time, whereas a long fixed sleep
maximises it (or causes flaky tests, which is also bad).Throw hardware at the problem. When you’ve made
the test suite as fast as it can go and it’s still slow, just run it on
a faster computer. If the tests are mostly CPU-bound, rent a 256-core
cloud machine and have it pull and run the tests on demand. CPU time
costs a lot less than programmer time, especially since hiring cheap
programmers is a false economy.. This is a last resort,
but it might come to that. If you have a few tests that simply
 be speeded up any more, and they’re dragging down the
rest of the suite, extract them to a separate “slow test” suite, and run
it on a schedule. Every night, perhaps; certainly no less frequently
than that. Even nightly isn’t great, but it’s better than not running
tests at all.]]></content:encoded></item><item><title>Local Diffusion Inference with Stable-Diffusion.cpp and Flux.1</title><link>https://medium.com/@kyodo-tech/local-diffusion-inference-with-stable-diffusion-cpp-and-flux-1-5d1841fff9cf?source=rss-ac02ab142942------2</link><author>Kyodo Tech</author><category>dev</category><category>blog</category><pubDate>Thu, 30 Jan 2025 11:58:04 +0000</pubDate><source url="https://medium.com/@kyodo-tech?source=rss-ac02ab142942------2">Dev - Kyodo-Tech</source><content:encoded><![CDATA[Stable-Diffusion.cpp is designed for efficient, high-performance inference of Stable Diffusion models, especially on low-resource environments where traditional deep learning pipelines struggle. We provide an examination of SDcpp, its components, and the technical concepts that enable efficient inference of Stable Diffusion models on lower-end hardware such as CPUs and lightweight GPUs, such as Apple M-series chips. In particular, we’re looking at Flux.1, a text-to-image model developed by Black Forest Labs.Introduction to Diffusion Models and Stable DiffusionDiffusion models, specifically Denoising Diffusion Probabilistic Models (DDPMs), have emerged as powerful generative frameworks capable of producing high-fidelity images from random noise through iterative denoising processes. Stable Diffusion extends the DDPM framework by incorporating text conditioning, allowing for the generation of images based on descriptive prompts. However, the computational demands of such models typically necessitate high-memory, high-bandwidth GPU resources, limiting their accessibility and scalability.SDcpp addresses these limitations by providing an optimized inference framework that leverages techniques such as quantization, memory-efficient sampling, and optimized latent space representations to facilitate the deployment of Stable Diffusion models on devices with constrained computational resources. This paper delves into the architectural and algorithmic components of SDcpp, highlighting the methodologies that underpin its efficiency and performance.The architecture is composed of several interdependent modules, each contributing to the inference process. First, it is loading the required models (1), including text encoders, the diffusion model, and the VAE decoder. It then performs  (2), tokenizing the input prompt and generating text embeddings using a CLIP-based model and an additional T5XXL text conditioning model. Next, it computes the conditioning graph (3), transforming embeddings to create a guidance vector for the diffusion process. The system then  (4) by iteratively refining a randomly initialized latent tensor using a noise schedule and a chosen sampling method, progressively steering it towards a structured latent representation in a number of . This step is the most compute-intensive, consuming roughly 90% of the total processing time. Once sampling completes, the pipeline decodes the latent representation (5) using a VAE model to reconstruct pixel-space information. Finally, it  (6) as an image file, completing the text-to-image generation process.Diffusion Model Core ComponentsThe diffusion model within SDcpp is the principal generative engine, responsible for transforming Gaussian noise into coherent images through a series of denoising . More steps allow finer details and smoother outputs but increase inference time, as each step requires additional computation. This process is governed by the DDPM framework, wherein the model iteratively refines the image by removing noise in a controlled manner. The diffusion model encompasses three primary components:UNet (Denoising Network): the UNet architecture employs an encoder-decoder structure with skip connections, enabling multi-scale feature extraction and reconstruction. The network predicts and removes noise at each denoising step, progressively enhancing image quality.Text Encoder (Conditioning Module): The text encoder converts textual prompts into latent embeddings that guide the denoising process. SDcpp supports multiple text encoders, including CLIP-L, CLIP-G, and T5XXL, each tailored for different model versions and memory considerations.Scheduler (Noise Controller): The scheduler orchestrates the noise removal strategy, dictating the temporal progression of denoising steps. It determines the rate and manner in which noise is reduced, balancing computational efficiency and image fidelity.A recent model,  , is optimized for low-resource environments, featuring a compact UNet architecture that accelerates inference by reducing the number of denoising steps. Flux.1 is designed for memory-efficient quantization and compatibility with the GGUF format, making it suitable for deployment on lightweight hardware. It reduces inference steps to only 4 steps and can achieve fast inference.Variational Autoencoder (VAE)VAEs facilitating the encoding of images into a lower-dimensional latent space. This reduction significantly diminishes computational and memory overhead while retaining high-resolution image details. Latent space is a compact mathematical representation of an image. Think of it as a highly efficient shorthand where only the most essential details are stored. Instead of storing every pixel, the image is encoded as numbers that describe its features. This makes it possible to work with images at a much lower computational cost. Stable Diffusion doesn’t generate images pixel by pixel. Instead, it creates images in latent space, where information is stored in a more compact and manageable way. Once the model finishes generating an image in this space, the VAE  it back into a high-resolution format that we can see.Different VAEs can influence how well the final image looks. A generic VAE works for most images, but a model trained for a specific style, like anime or realistic photography, may use a custom VAE to better match its training data. A poorly matched VAE can cause color shifts, blurriness, or unwanted artifacts.Technically, any model can use any VAE, but results vary. A VAE trained on cartoon-style images won’t reconstruct a photorealistic image correctly. This is why some models provide their own VAEs — they have been optimized to maintain details that matter most for a specific style.LoRA (Low-Rank Adaptation)LoRA facilitates model customization without the need to adjust the entire diffusion model. By modifying only a small subset of parameters (~10 million), LoRA significantly reduces memory usage compared to full model fine-tuning. Multiple LoRAs can be combined with weighted influences on the final image, enabling diverse stylistic and feature modifications. For instance, combining two LoRAs with different weights can be executed using the syntax <lora:styleA:0.6> <lora:styleB:0.8>. To optimize performance, it is advisable to minimize the number and strength of LoRAs, especially in memory-limited environments.Training and Inference File FormatsThe  is the standard PyTorch checkpoint format, that stores full precision weights and optimizer states. Supports training and fine-tuning but can execute arbitrary Python code, posing security risks. SafeTensors (.safetensors) is a safer alternative to CKPT, with structured, immutable storage and no executable code. Loads faster but still used mainly for training.GGUF (GPT-Generated Unified Format) is a binary model format introduced by llama.cpp, succeeding GGML. Initially designed for language models (like LLaMA models), GGUF has been adopted in diffusion models to enable efficient, quantized inference without reformatting model files into proprietary formats (e.g., .ggml or .gguf conversions required previously). Traditional formats (.safetensors, .ckpt, .bin) store full-precision FP16/FP32 model weights, consuming significant VRAM. GGUF introduces quantization at multiple levels, storing models in 2-bit to 16-bit precision while maintaining compatibility with SDcpp.Use GGUF for efficient inference on low-resource devices, prefer .safetensors for training.In our experiments,  demonstrates an extremely fast denoising process, with images at 1 and 2 steps appearing nearly identical, suggesting that most structural information is determined in the very first iterations. By 4 steps, images diverge significantly, and by 6 steps, they change even more, indicating that additional steps introduce large variations rather than just refining details. Interestingly, Euler and Euler_a produce the best results, likely because they follow a balanced noise reduction trajectory, allowing structure to form naturally without over-smoothing or excessive deviations. Euler introduces noticeable changes between 3, 4, and 6 steps, including shifts in pose, face, and hair color, implying that it follows a latent space trajectory where small step changes can drastically impact the final result. Euler_a, being an ancestral sampler, adds controlled randomness at each step, further amplifying these changes. In contrast, LCM performs poorly with Flux 1, excessively blurring images, likely due to its tuning for models that require more iterative refinement, making it unsuitable for Flux 1’s fast-converging architecture. We observed that 4 steps often produce better images than 5 or 6 steps, as extra iterations introduce unnecessary alterations instead of improving details, reinforcing that Flux 1 is optimized for extremely low-step inference. This is a stark contrast to Stable Diffusion models like SD 1.5 or SDXL, which typically require 20–50 steps for high-quality outputs. These findings highlight that Flux 1’s unique training favors fewer steps with the right sampler, making Euler and Euler_a better choices at around 4 steps, while models like LCM fail due to excessive denoising.Denoising Schedules and Sampling IntegrationDenoising schedules define a series of noise levels that guide reverse diffusion. These schedules set discrete or continuous noise decay steps, essentially parameterizing the stochastic differential equation used to reverse the diffusion process. Discrete schedules apply abrupt noise transitions, forcing the model to rapidly shift between latent states. In contrast, smooth schedules such as karras or exponential distribute noise reduction evenly, supporting more accurate latent updates.Sampling methods numerically integrate these latent trajectories. Euler-type samplers update the latent state with fixed time step approximations, benefiting from stable transitions when paired with smooth noise decay. Latent Consistency Models (LCM) depend on gradual noise reduction; abrupt schedules can induce over-smoothing and detail loss. Our observations indicate that matching a sampling method’s sensitivity to noise decay with an appropriate denoising schedule is key to preserving image structure while ensuring efficient inference.Flux.1’s behavior indicates that its training regime emphasizes rapid structure formation in early steps. Euler’s robustness across discrete, exponential, and karras schedules suggests that its integration of abrupt noise transitions still aligns well with Flux.1’s latent dynamics. LCM, being more sensitive to the noise decay, only works well with smoother schedules (exponential and karras) that provide gradual latent updates.The failures with AYS and GITS imply that these schedules impose noise trajectories incompatible with Flux.1’s fast-convergence design. Their latent paths likely diverge from the model’s learned reverse diffusion trajectory, causing the state to collapse into uniform or noisy outputs rather than structured images. This sensitivity reinforces that Flux.1 is optimized for few-step denoising with specific noise decay characteristics, and departures from that — especially with schedules that enforce alternative trajectory regularity — can disrupt the image generation process.Flux.1 departs from prior diffusion models by leveraging a T5-style text encoder instead of CLIP, allowing it to process full sentences fluently rather than relying on discrete token hierarchies. Unlike Stable Diffusion, where Booru-style tags and keyword emphasis are required for precision, Flux.1 understands natural language natively, reducing reliance on rigid token structures. Long, descriptive prompts yield better scene coherence, material accuracy, and relational awareness than fragmented keyword lists.Trigger words are optional rather than mandatory, as the model activates concepts contextually rather than requiring explicit token matching. However, structured elements — short modifiers or category cues — can refine specificity without overpowering Flux.1’s broader language comprehension. The model is particularly strong at multi-concept blending, which traditionally caused token interference in CLIP-based architectures. Negative prompting is also more effective, reducing the unintended blending of attributes.Flux.1 performs best with contextual and relational descriptions, leveraging semantic depth over isolated tokens. While Booru tags still function, hybrid prompting — mixing structured elements with full sentences — offers the best balance between control and generative adaptability.LoRA functions differently in Flux.1 compared to Stable Diffusion. Since Flux.1 uses a T5-based encoder, LoRA activation does not require fixed trigger words but instead responds dynamically to natural descriptions. This allows LoRAs to integrate seamlessly into prompts without rigid dependencies.Key parameters such as network dimension, alpha scaling, and captioning density influence performance. Higher network dimension strengthens adaptation but risks overfitting, while alpha scaling balances LoRA integration with the base model. Flux.1’s LoRAs benefit from mixed captioning strategies — varying between short tags and natural descriptions — ensuring activation without over-constraining the generative process. Unlike SDXL, which often requires explicit weight scaling (1.2–1.5x) to activate LoRAs, Flux.1 adapts LoRA strength more organically within context, reducing the need for manual weight adjustments.To maximize LoRA performance in Flux.1, descriptive phrasing should replace rigid token triggers, using context-driven prompts to activate learned modifications naturally. Captions should balance specificity with generalization, ensuring LoRAs enhance rather than dominate the generative output.Optimization Strategies for Low-Resource InferenceQuantization is a pivotal technique in reducing model size and memory usage by lowering the precision of model weights. SDcpp supports various quantization levels, each offering a different balance between memory efficiency and image quality: Reduces memory usage by approximately 75% compared to full precision (FP16) while maintaining good image quality. It introduces a slight computational overhead due to dequantization but is optimal for low-end hardware. Offers slightly better precision than Q4 at the cost of requiring twice the memory. It is suitable for systems with more VRAM where higher image fidelity is desired.For lower-end hardware, Q4_K provides a good balance of speed and quality, while  is better for systems with more VRAM (>16GB).Quantization Impact of Model PartsWe’re loading a model, vae, clip, and t5, and can choose quantized parts for each. The model itself must be quantized due to memory constraints, but the VAE, CLIP, and T5 can remain in higher precision. We tested a q4_K model with safetensors in higher precision, and a fully quantized version where all parts were q4_0.We observed a 2.5x slowdown with the fully quantized run, despite reduced memory usage. Likely due to increased dequantization overhead, precision mismatch-induced stalls, and reduced SIMD efficiency on the Apple M2. While background processes may have contributed to higher involuntary context switches (3.3M vs. 1.48M), they do not fully explain the slowdown. The quantization format (q4_K vs. q4_0) influences computational efficiency rather than raw inference speed, with q4_K employing grouped quantization, which improves data locality and reduces dequantization frequency, while q4_0 likely applies simpler per-tensor quantization, increasing compute overhead. The hybrid approach—quantizing only the diffusion model while keeping VAE, CLIP, and T5 in higher precision—seems to better utilize hardware by avoiding unnecessary precision conversion costs and leveraging FP computation optimizations. These results suggest that full quantization may degrade performance when not optimized for architectural strengths, particularly on FP-optimized hardware like the M2.The sampling method determines the strategy by which noise is removed during the denoising process. Ancestral samplers introduce random variations at each step, leading to more diverse outputs, while fixed-step samplers follow a deterministic trajectory, yielding consistent results.SDcpp integrates several sampling algorithms optimized for different performance criteria: A simple and fast method effective with a lower number of denoising steps, suitable for rapid inference. Prioritizes image quality, requiring more steps and memory. It is ideal for applications where high detail is essential.Latent Consistency Models (LCM): Optimizes speed by drastically reducing the number of required denoising steps (e.g., 4–8 instead of 20–30), making it highly suitable for low-end systems. Users can specify the sampling method using the --sampling-method flag, such as --sampling-method lcm.Memory management is critical in ensuring that diffusion models run efficiently on constrained hardware. SDcpp employs several techniques to optimize memory usage: By processing the VAE decoder in smaller, manageable segments, VAE tiling minimizes peak memory usage. Flash Attention optimizes memory allocation during UNet computations. On CUDA-enabled GPUs, it offers significant memory savings and speed improvements. Flash Attention can be activated via the --diffusion-fa flag.In our experiments on an Apple M2, enabling --vae-tiling reduced memory usage but increased execution time, suggesting a trade-off between efficiency and speed. When --diffusion-fa (Flash Attention) was enabled, execution time did not significantly improve. The combination of both options (vae-tiling and diffusion-fa) resulted in the  but also the , confirming that while these settings help on low-VRAM devices, they introduce performance overhead. The fastest run occurred with both options disabled, though it had the highest memory footprint.Image resolution directly impacts both memory consumption and inference speed. Generating images at a resolution of 1280×640 requires approximately 2.5 times the memory of generating images at 512×512. To accommodate low-resource environments, users can opt for intermediate resolutions such as 768×384 or 1024×512, balancing image quality and computational efficiency.LoRA Delay and Conditioning Graph RecalculationWhen LoRA modules are applied, the conditioning graph must be recalculated due to LoRA’s modification of internal activations, which can reintroduce dependencies on structured tagging if the LoRA was trained on datasets requiring them. The conditioner parses and reweights prompt segments dynamically, adjusting token influence based on LoRA-induced shifts in the embedding space, ensuring compatibility between the altered diffusion model state and the input text. This recalibration process accounts for the delay seen post-LoRA application and before sampling, as the system must harmonize LoRA-induced parameter shifts with Flux.1’s inherently context-aware text encoding.Classifier-Free Guidance ScaleCFG scale (Classifier-Free Guidance Scale) can be set with the --cfg-scale argument. It defaults to 7.0 and controls how strictly the model follows the text prompt, balancing adherence and creativity. Higher values (e.g., 10-15) force the model to generate images that closely match the prompt but can introduce artifacts, while lower values (e.g., 1-5) allow more randomness and artistic interpretation. It impacts performance slightly because higher CFG values increase the weight difference between conditional (prompted) and unconditional (random) predictions, requiring additional computation to balance them. However, the effect on inference speed is minor compared to factors like resolution, model architecture, and sampling steps.Flux.1 seem to have been trained with  as target setting, meaning its internal weighting and contrast are optimized for that value. Lowering CFG to  reduces guidance too much, allowing excessive noise influence, resulting in darker, underexposed images. Raising it to 2.0 strengthens prompt adherence but overemphasizes highlights and contrast, making images appear overly bright. Unlike standard Stable Diffusion, where 7.0 is the default for balanced outputs, Flux.1’s tuning makes , with deviations causing unintended brightness shifts.To analyze the impact of CFG scale on image generation, we generated eight images using CFG 1.0 and 2.0 across the Euler, LCM, iPNDM, and Euler_a samplers with 4 steps and a fixed seed of 42, using the prompt “a beautiful girl in futuristic Tokyo, neon lights,” comparing how different guidance levels influence brightness, contrast, and prompt adherence.Memory-Efficient InferenceRunning Flux.1 (schnell) on an Apple M2 (16GB RAM), we achieve inference without a GPU, completing a 256×256 image in 216.3s with 9.28GB RAM usage. The UNet required just 96.59MB, and VAE decoding used 416MB, demonstrating the efficiency of quantized GGUF models.Our analysis of Stable-Diffusion.cpp and Flux.1 demonstrates that efficient local diffusion inference is feasible even on low-resource hardware, provided that model selection, quantization, and sampling strategies are optimized. Flux.1’s extremely low-step denoising makes it uniquely suited for fast inference, especially when paired with Euler and Euler_a samplers, which balance structure formation and controlled randomness. In contrast, models like LCM introduce excessive smoothing, making them unsuitable for Flux.1’s fast convergence.We also observed that 4 steps yield optimal results with Flux.1, as additional iterations introduce unnecessary alterations rather than improving image fidelity. While our results indicate that full quantization introduces dequantization overhead and computational stalls, external factors such as background system load may have influenced the extent of the observed slowdown. Memory-efficient techniques such as VAE tiling and Flash Attention can reduce hardware requirements but may introduce performance trade-offs. Users should select Q4_K or similar quantization for lower-end hardware, as grouped quantization schemes appear to mitigate some of the performance penalties associated with fully quantized inference.These findings highlight that with the right model architecture, quantization, and sampler choice, high-quality image generation is possible on constrained devices without relying on high-end GPUs. Future improvements may focus on further refining model architectures like Flux.1 and optimizing quantization methods for even faster, memory-efficient inference.]]></content:encoded></item><item><title>Design YouTube - System Design Interview</title><link>https://blog.algomaster.io/p/design-youtube-system-design-interview</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23342c70-b11d-48c3-b30b-63ea245e5d2a_2026x1322.png" length="" type=""/><pubDate>Thu, 30 Jan 2025 05:45:50 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[With over  monthly active users,  is the second most visited website in the world—trailing only Google.As a , it enables users to upload, watch, and interact with video content, while handling hundreds of millions of daily visitors, managing petabytes of data, and ensuring real-time video delivery across the globe.In this article, we’ll explore the system design of a large-scale video streaming service like YouTube that can accommodate hundreds of millions of daily users and billions of views, all while maintaining low latency and high availability.We’ll walk through every step of the design—from requirements and high-level architecture to database and API design—before diving deep into core use cases. The concepts covered here are equally applicable to other large-scale video platforms such as  and .Before diving into the design, lets outline the functional and non-functional requirements.Users should be able to  video files.Uploaded videos must be  into multiple resolutions (e.g., 240p, 360p, 720p, 1080p) to support different network conditions and devices.Users should be able to  videos in real-time with adaptive bitrate streaming to adjust quality based on network conditions.Users can  for videos by title, tags, or description.Users can  and  on videos.Users should be able to create and subscribe to .Non-Functional Requirements: The system should support millions of concurrent users and thousands of video uploads per minute. Core features like video upload, playback, and search should have minimal downtime. Fast video streaming with minimal buffering and near-instantaneous search results. Video files must be stored reliably, with redundancy mechanisms to prevent data loss due to hardware failures. Optimize storage and bandwidth costs.Daily Active Users (DAU): 10 million ~100,000 videos/dayAverage Videos Watched per User per Day: 5 videos 500 MB. 1 KB.: 100,000 videos / day * 500 MB / video = 50 TB / dayDaily Video Metadata Storage: 100,000 * 1KB = 100MB / dayNetwork Bandwidth Estimation:10 million users × 5 videos/user = 50 million views/day Daily Bandwidth Requirements (without compression & caching) : 50 million views * 500 MB / day = 25 PB / dayGiven the high storage and bandwidth requirements, leveraging  is the most practical approach:Content Delivery Network (CDN): To cache frequently accessed videos closer to users and reduce latency.Blob Storage (e.g., AWS S3): To store video files reliably with redundancy.We can break the architecture of YouTube into two primary components:  – Handles video playback, and delivery.Video Upload & Processing – Manages user uploads, transcoding, and metadata storage.3.1 Video Streaming Architecture]]></content:encoded></item><item><title>Bliki: Forest And Desert</title><link>https://martinfowler.com/bliki/ForestAndDesert.html</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Thu, 30 Jan 2025 05:00:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[The Forest and the Desert is a metaphor for thinking about software
  development processes, developed by Beth Andres-Beck and hir father Kent Beck.
  It posits that two communities of software developers have great difficulty
  communicating to each other because they live in very different contexts, so
  advice that applies to one sounds like nonsense to the other.The desert is the common world of software development, where bugs are
  plentiful, skill isn't cultivated, and communications with users is difficult.
  The forest is the world of a well-run team that uses something like Extreme Programming, where developers swiftly put changes into
  production, protected by their tests, code is invested in to keep it healthy,
  and there is regular contact with The Customer.Clearly Beth and Kent prefer The Forest (as do I). But the metaphor is more
  about how description of The Forest and the advice for how to work there often
  sounds nonsensical to those whose only experience is The Desert. It reminds us
  that any lessons we draw about software development practice, or architectural
  patterns, are governed by the context that we experienced them. It is possible
  to change Desert into Forest, but it's difficult - often requiring people to do
  things that are both hard and counter-intuitive. (It seems sadly easier for
  The Forest to submit to desertification.) In this framing I'm definitely a Forest Dweller, and seek with Thoughtworks
  to cultivate a healthy forest for us and our clients. I work to explain The Forest to Desert
  Dwellers, and help my fellow Forest Dwellers to make their forest even more
  plentiful.Kent Beck supplied the image, which he may have painstakingly drew pixel by
    pixel. Or not.]]></content:encoded></item><item><title>Embeddings in GenAI Products</title><link>https://martinfowler.com/articles/gen-ai-patterns/#embedding</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 29 Jan 2025 15:55:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[GenAI systems, like many modern AI approaches, have to handle vast
      quantities of data, and find similarities between elements in an image or
      chunk of words.  and I describe a key tool
      to do this - Embeddings - transforming large data blocks into
      numeric vectors so that embeddings near each other represent related
      concepts]]></content:encoded></item><item><title>Particles, Progress, and Perseverance: A Journey into WebGPU Fluids</title><link>https://tympanus.net/codrops/2025/01/29/particles-progress-and-perseverance-a-journey-into-webgpu-fluids/</link><author>Hector Arellano</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/Grabacion-de-pantalla-2025-01-05-a-las-18.48.35-1.mov?x44439" length="" type=""/><pubDate>Wed, 29 Jan 2025 13:22:40 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[A reflective journey through years of experimentation and innovation, demonstrating how WebGPU enables the creation of sophisticated, visually striking fluid simulations.]]></content:encoded></item><item><title>Maintainable Architecture for Data Flow in Go Applications</title><link>https://medium.com/@kyodo-tech/maintainable-architecture-for-data-flow-in-go-applications-512336878b26?source=rss-ac02ab142942------2</link><author>Kyodo Tech</author><category>dev</category><category>blog</category><pubDate>Wed, 29 Jan 2025 13:02:01 +0000</pubDate><source url="https://medium.com/@kyodo-tech?source=rss-ac02ab142942------2">Dev - Kyodo-Tech</source><content:encoded><![CDATA[Handling data flow is one of the most fundamental aspects of software engineering. Regardless of the type of application, data moves through different layers — being processed, stored, and transported. Despite its importance, data flow is often one of the first things that becomes rigid in a codebase. Once an application’s data flow is tightly coupled to specific frameworks, storage mechanisms, or transport layers, making changes can introduce significant overhead and risk. For complex applications (multi-transport, multi-storage), a structured, layered approach adds long-term maintainability. Only consider the abstractions mentioned in this article if they serve a purpose. For simple applications (only HTTP, fixed storage): Direct handlers and fewer layers can be more practical.A structured approach to managing data flow ensures that applications remain adaptable, maintainable, and testable as they evolve. The goal is to avoid a situation where a single decision early in development — such as choosing an HTTP framework or a database — becomes a long-term constraint. We generally aim to focus on well-defined interfaces and ensuring that each layer has a clear responsibility. We can design a system where storage, transport, and application logic remain independent of each other and thereby .This approach borrows from various established software design principles without rigidly adhering to any one methodology. Simple, practical layering, where each component is structured around data ownership and behavior rather than arbitrary architectural rules.Common Pitfalls with Application Data FlowMost applications struggle with data flow due to either tight coupling or over-abstraction.Tight coupling happens when business logic is embedded directly in HTTP handlers, database queries, or message processors. This works initially but becomes a problem when new transports like gRPC or WebSockets are introduced, or when switching storage backends. A REST API tightly bound to http.Request objects will require major refactoring to support other protocols. Similarly, business logic buried in SQL queries makes database migrations far more complex than necessary.Over-abstraction is the opposite extreme — introducing excessive layers that add complexity without solving real problems. Many projects preemptively wrap simple database interactions in repositories, create interfaces for single implementations, or introduce dependency injection for no clear reason. Instead of improving flexibility, this leads to rigid, unreadable code where a minor change requires modifying multiple layers.A Structured Approach to Data FlowThe structure of an application should make it clear where data originates, how it is processed, and where it ultimately ends up. This requires defining clear boundaries between different parts of the system:Application Logic (Service Layer): Defines how data is processed and transformed.Storage (Persistence Layer): Provides mechanisms for persisting and retrieving data.: Handles incoming and outgoing data through various means (HTTP, message queues, CLI, etc.).Each of these layers should be , meaning that changes in one should not require significant refactoring in another. This keeps the system flexible, capable of evolving over time without accumulating unnecessary complexity.Standardizing Business Logic with a Generic HandlerTo decouple business logic from HTTP concerns, we can define a generic function signature that standardizes how handlers receive input and return output. This pattern ensures that the same business logic can be reused across multiple transports, including REST, gRPC, message queues (NATS, NSQ, Kafka), and CLI applications.A simple yet effective way to enforce this is by using a generic function signature:// Standardized Handler Signaturetype HandlerFunc[I any, O any] func(ctx context.Context, input I) (O, error)Note that we use generics for strict type safety and minimal runtime conversions. We could also use interfaces, especially if we prefer Go’s traditional patterns and more flexible data handling.The function signature allows us to abstract transport details away from the core logic, making it composable and framework-agnostic. By defining business logic using this form, the same logic can be wired into different transports without modification. A service function using this pattern might look like:package service "context" "fmt"// Business logic handler using a generic function signaturefunc HelloHandler(ctx context.Context, input HelloInput) (HelloOutput, error) { messages := map[string]string{  "en-US": "Hello, %s!",  "ru-RU": "Привет, %s!", msg, available := messages[input.Locale] if !available {  return HelloOutput{}, errors.New("unsupported locale") } return HelloOutput{Message: fmt.Sprintf(msg, input.Name)}, nil}With this approach, the service logic is completely decoupled from transport details. The same function can be exposed through different means. REST, gRPC, CLI, or event-driven transports can be supported by wrapping it in an appropriate adapter. The business layer can be exposed through different transports with minimal effort.Exposing Business Logic via HTTP (swaggest/rest)Since our business logic already follows a standardized function signature, we can wrap it in an HTTP adapter:package transport "context" "github.com/swaggest/usecase" "github.com/swaggest/rest/web" "github.com/swaggest/openapi-go/openapi31" "myapp/service"// REST API Handler using the standardized function signaturefunc NewRESTHandler() *web.Service { api := web.NewService(openapi31.NewReflector()) // Wrap HelloHandler in a `usecase.Interactor` interactor := usecase.NewInteractor(func(ctx context.Context, input service.HelloInput, output *service.HelloOutput) error {  result, err := service.HelloHandler(ctx, input)  if err != nil {  }  return nil api.Post("/hello", interactor) return apiHere, usecase.NewInteractor provides a formalized contract for our service function, allowing it to be automatically documented and exposed as an OpenAPI 3 compliant API.Exposing Business Logic via NATSFor event-driven messaging, we use the same function but adapt it for a message queue:package transport "context" "log" "github.com/nats-io/nats.go" "myapp/service"// NATS subscriber wrapping the standardized function signaturefunc NATSHandler(nc *nats.Conn) { nc.Subscribe("hello", func(m *nats.Msg) {  var input service.HelloInput  if err := json.Unmarshal(m.Data, &input); err != nil {   log.Println("Invalid message:", err)  }  output, err := service.HelloHandler(context.Background(), input)  if err != nil {   log.Println("Processing error:", err)   return  response, _ := json.Marshal(output)  nc.Publish(m.Reply, response)}Again, no changes are required to the business logic, ensuring that adding new transports is straightforward.Storage Should Conform to Business Needs, Not Define ThemAnother critical aspect of this approach is ensuring that storage backends conform to business needs, rather than dictating application behavior. The persistence layer should implement interfaces that match service expectations, ensuring that data storage can be swapped without affecting business logic.package storage "context" "os")// Storage interface defining required behaviortype Storage interface { GetMessages(ctx context.Context) (map[string]string, error)}// JSONFileStorage persists messages in a JSON filetype JSONFileStorage struct {}func NewJSONFileStorage(path string) *JSONFileStorage { return &JSONFileStorage{filePath: path}func (s *JSONFileStorage) GetMessages(ctx context.Context) (map[string]string, error) { file, err := os.ReadFile(s.filePath)  return nil, err var data map[string]string if err := json.Unmarshal(file, &data); err != nil { }}By keeping storage implementations separate from business logic, we avoid binding application behavior to specific database models. This allows us to easily swap the persistence layer, e.g. replacing file storage with PostgreSQL or Redis, without affecting the service layer.Dependency Injection for StorageDependency injection (DI) allows us to inject dependencies into functions or structures rather than hard-coding them. When it comes to storage, we have two primary options:Functional Options for InjectionKeeps dependencies explicit and type-safe.Storage is fixed at initialization time.Context-Based Storage for Dynamic Per-Request StorageOnly when multi-tenant storage selection is required.The package , preventing context.WithValue sprawl.When in doubt, favor functional options DI for explicitness and testability. Allow context-based injection for dynamic per-request storage, provided it’s encapsulated in the storage package. This keeps dependencies clear while still allowing flexibility where needed.A service with a configurable storage backend using functional options:package service "context")type HelloService struct { store storage.Storage// Functional option typetype Option func(*HelloService)// WithPersistence sets the storage implementationfunc WithPersistence(store storage.Storage) Option { return func(s *HelloService) {  s.store = store}// Constructor applying functional optionsfunc NewHelloService(opts ...Option) *HelloService { for _, opt := range opts { }}// Business logic with explicit storage dependencyfunc (s *HelloService) HelloHandler(ctx context.Context, input HelloInput) (HelloOutput, error) { messages, err := s.store.GetMessages(ctx) if err != nil {  return HelloOutput{}, err } msg, exists := messages[input.Locale] if !exists {  return HelloOutput{}, service.ErrNotFound } return HelloOutput{Message: fmt.Sprintf(msg, input.Name)}, nil}Using this service with a JSON file storage backend:store := storage.NewJSONFileStorage("messages.json")helloService := service.NewHelloService(service.WithPersistence(store))output, err := helloService.HelloHandler(context.Background(), HelloInput{Name: "Alice", Locale: "en-US"})This approach ensures that dependencies are explicit, testable, and have no hidden dependencies.Context-Based Storage (request scope)If a dependency needs to change dynamically per request, the storage package encapsulates context management.package storagetype Storage interface { GetMessages(ctx context.Context) (map[string]string, error)}// WithStorage attaches storage to contextfunc WithStorage(ctx context.Context, store Storage) context.Context { return context.WithValue(ctx, contextKey{}, store)}// StorageFromContext retrieves storage from contextfunc StorageFromContext(ctx context.Context) (Storage, bool) { store, ok := ctx.Value(contextKey{}).(Storage) return store, okThe service logic can access storage from the context:func HelloHandler(ctx context.Context, input HelloInput) (HelloOutput, error) { store, ok := storage.StorageFromContext(ctx)  return HelloOutput{}, errors.New("storage not found in context") messages, err := store.GetMessages(ctx) if err != nil {  return HelloOutput{}, err } msg, exists := messages[input.Locale] if !exists {  return HelloOutput{}, service.ErrNotFound } return HelloOutput{Message: fmt.Sprintf(msg, input.Name)}, nil}Select storage dynamically per request:// Select storage dynamically per requestselectedStore := storage.NewJSONFileStorage("tenant_data.json")ctx := storage.WithStorage(context.Background(), selectedStore)output, err := service.HelloHandler(ctx, HelloInput{Name: "Alice", Locale: "en-US"})This approach allows for dynamic storage selection per request while keeping the service logic clean and testable.Since the goal is business logic independent of transport concerns, HTTP return codes and error responses should be handled at the transport layer, not in the service logic.A common, effective approach is:Use structured error types in the service layer.Map them to transport-specific responses in the transport adapter.Defining a Standard Error TypeInstead of returning error directly, define a structured error type:package service// Standardized error with type and messagetype ServiceError struct { Code    string // e.g., "NOT_FOUND", "VALIDATION_ERROR" Message string // Human-readable messagefunc (e *ServiceError) Error() string { return e.Message// Error helpers for consistencyvar ( ErrNotFound        = &ServiceError{Code: "NOT_FOUND", Message: "Resource not found"} ErrValidation      = &ServiceError{Code: "VALIDATION_ERROR", Message: "Invalid input"} ErrInternal        = &ServiceError{Code: "INTERNAL_ERROR", Message: "Internal server error"} ErrUnauthorized    = &ServiceError{Code: "UNAUTHORIZED", Message: "Unauthorized"} ErrForbidden       = &ServiceError{Code: "FORBIDDEN", Message: "Forbidden"} ErrConflict        = &ServiceError{Code: "CONFLICT", Message: "Conflict detected"} ErrRateLimited     = &ServiceError{Code: "RATE_LIMITED", Message: "Too many requests"})This keeps the  and avoids mixing transport concerns.Mapping Errors in HTTP Transport LayerThe HTTP adapter translates ServiceError to appropriate HTTP status codes:package transport "context" "net/http")// ErrorResponse defines a structured API error responsetype ErrorResponse struct { Code    string `json:"code"` Message string `json:"message"`// Map service errors to HTTP status codesfunc mapServiceError(err error) (int, ErrorResponse) { if serr, ok := err.(*service.ServiceError); ok {  switch serr.Code {   return http.StatusNotFound, ErrorResponse{Code: serr.Code, Message: serr.Message}   return http.StatusBadRequest, ErrorResponse{Code: serr.Code, Message: serr.Message}   return http.StatusUnauthorized, ErrorResponse{Code: serr.Code, Message: serr.Message}   return http.StatusForbidden, ErrorResponse{Code: serr.Code, Message: serr.Message}   return http.StatusConflict, ErrorResponse{Code: serr.Code, Message: serr.Message}   return http.StatusTooManyRequests, ErrorResponse{Code: serr.Code, Message: serr.Message}   return http.StatusInternalServerError, ErrorResponse{Code: "INTERNAL_ERROR", Message: "Something went wrong"} } // Default case for unrecognized errors return http.StatusInternalServerError, ErrorResponse{Code: "INTERNAL_ERROR", Message: "Something went wrong"}// HTTP handler wrapping business logicfunc HelloHTTPHandler(w http.ResponseWriter, r *http.Request) { // Simulate input parsing (normally from JSON request) input := service.HelloInput{Name: "Alice", Locale: "en-US"} output, err := service.HelloHandler(ctx, input)  status, resp := mapServiceError(err)  json.NewEncoder(w).Encode(resp) } w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(output)}This approach ensures that service logic remains transport-agnostic, allowing the same business logic to be used across different transports with minimal adaptation. Adding gRPC or NATS? Just reuse mapServiceError() to return gRPC status codes or NATS response formats. Errors remain uniform across the system, making it easier to reason about and maintain.While the structured approach ensures that business logic remains transport-independent, many operations require . Background tasks, long-running jobs, and message queue processing should follow the design principles of decoupling execution from business logic. We further want to enable retries, scheduling, and parallel execution without modifying core logic.A common way to achieve this is through the , a lightweight abstraction for processing tasks asynchronously while maintaining a standardized function signature.Defining an Executor InterfaceTo standardize how tasks are executed:package worker// Executor defines a generic interface for background executiontype Executor[I any] interface { Execute(ctx context.Context, input I) error}This ensures that any task — whether triggered by an HTTP request, a message queue, or a cron job follows the same execution model.Implementing a Message Queue WorkerUsing this approach, we can create an  that listens for messages from  and processes them asynchronously:package worker "context" "log" "github.com/nats-io/nats.go")// NATSExecutor executes handlers asynchronously from a NATS queuetype NATSExecutor[I any] struct { subject string handler func(ctx context.Context, input I) error}// NewNATSExecutor creates a new executor for processing NATS messagesfunc NewNATSExecutor[I any](nc *nats.Conn, subject string, handler func(ctx context.Context, input I) error) *NATSExecutor[I] { return &NATSExecutor[I]{nc: nc, subject: subject, handler: handler}}// Execute processes incoming messages asynchronouslyfunc (e *NATSExecutor[I]) Execute(ctx context.Context, input I) error { return e.handler(ctx, input)}// Start subscribes and handles messages in the backgroundfunc (e *NATSExecutor[I]) Start() { e.nc.Subscribe(e.subject, func(m *nats.Msg) {  go func() { // Execute asynchronously   if err := json.Unmarshal(m.Data, &input); err != nil {    log.Println("Invalid message:", err)    return   if err := e.Execute(context.Background(), input); err != nil {    log.Println("Execution failed:", err)   } })This pattern keeps the execution generic, allowing any function matching func(ctx context.Context, I) error to be used. The executor runs in the background, preventing blocking of main processes. Multiple subjects (message types) can be supported by configuring the subject dynamically.Integrating with Business LogicUsing the same generic function signature, we can wire a business function into the executor:package main "context" "github.com/nats-io/nats.go" "myapp/service") nc, _ := nats.Connect(nats.DefaultURL) // Define the executor with business logic executor := worker.NewNATSExecutor(nc, "hello",  func(ctx context.Context, input service.HelloInput) error {   _, err := service.HelloHandler(ctx, input)  }) log.Println("Starting background worker for NATS queue...") executor.Start() select {} // Keep running}The executor remains generic, handling HelloInput without modifying business logic. The same HelloHandler function can be exposed via HTTP, CLI, or background processing without duplication. Retries, scheduling, and parallelism can be added to the executor without affecting the core logic.A maintainable and adaptable data flow requires well-defined boundaries. By using standardized function signatures and transport-agnostic layering, business logic remains decoupled from transport and storage concerns, ensuring that it can be exposed through HTTP, message queues, or CLI without modification. Storage backends are treated as interchangeable components, allowing business logic to dictate how data is stored and retrieved rather than being constrained by the persistence layer.For asynchronous processing, the Executor Pattern extends this structured approach to background tasks, long-running operations, and message-driven workflows. The executor ensures tasks follow the same execution model while enabling parallelism, retries, and scheduling without impacting core logic.This approach makes it easier to extend, test, and document applications while keeping the codebase resilient to change. As the system grows, new transports, storage mechanisms, and execution strategies can be introduced without refactoring the core logic. Each component remains composable and follows modular design principles, making the system easy to reason about and evolve over time.]]></content:encoded></item><item><title>Emerging Patterns in Building GenAI Products</title><link>https://martinfowler.com/articles/gen-ai-patterns/</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Tue, 28 Jan 2025 12:43:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[Everyone is fascinated about using generative AI these days, and my
      colleagues are no exception. Some of them have had the opportunity to put
      these system into practice, both as proof-of-concept, and more importantly
      as production system. I've known  for
      many years as a technology leader in India, he's been assembling the
      lessons we've learned and I've worked with him to describe them as
      patterns.In this first installment, we look the limits of the base case of
      Direct Prompting, and how we might assess the capability of a system using
      Evals.]]></content:encoded></item><item><title>10+ Best Web Design Tools &amp; Resources For 2025 (Free &amp; Paid)</title><link>https://tympanus.net/codrops/2025/01/28/best-web-design-tools-2025/</link><author>advertiser</author><category>Codrops blog</category><category>dev</category><category>frontend</category><enclosure url="https://codrops-1f606.kxcdn.com/codrops/wp-content/uploads/2025/01/14-Xstore.mp4?x44439" length="" type=""/><pubDate>Tue, 28 Jan 2025 12:30:00 +0000</pubDate><source url="https://tympanus.net/codrops">Codrops</source><content:encoded><![CDATA[Discover the top web design tools and resources of 2025, tailored for creating innovative, secure, and visually stunning websites with ease.]]></content:encoded></item><item><title>Long Polling vs WebSockets</title><link>https://blog.algomaster.io/p/long-polling-vs-websockets</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/90181ae6-4dcb-456f-a43a-7824d97c740b_2002x1472.png" length="" type=""/><pubDate>Tue, 28 Jan 2025 04:30:40 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[Whether you are playing an online game or chatting with a friend—updates appear in real-time without hitting .Behind these seamless experiences lies a critical engineering decision: how to push real-time updates from servers to clients.The traditional HTTP model was designed for request-response: "Client asks, server answers.". But in many real-time systems, the server needs to talk first and more often.This is where  and  come into play—two popular methods for achieving real-time updates.In this article, we’ll explore these two techniques, how they work, their pros and cons, and use cases.If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .HTTP, the backbone of the web, follows a :The client (e.g., a browser or mobile app) sends a request to the server.The server processes the request and sends back a response.This model is simple and works for many use-cases, but it has limitations: With plain HTTP, the server cannot proactively push data to the client. The client has to request the data periodically. HTTP is stateless, meaning each request stands alone with no persistent connection to the server. This can be problematic if you need continuous exchange of data.To build truly real-time features—live chat, financial tickers, or gaming updates—you need a mechanism where the server can instantly notify the client when something changes. is a technique that mimics real-time behavior by keeping HTTP requests open until the server has data.Long Polling is an enhancement over traditional polling. In regular polling, the client repeatedly sends requests at fixed intervals (e.g., every second) to check for updates. This can be wasteful if no new data exists. Long Polling tweaks this approach: the client asks the server for data and then “waits” until the server has something new to return or until a timeout occurs.How Does Long Polling Work? to the server, expecting new data.Server holds the request open until it has an update or a timeout is reached.If there's new data, the server immediately responds.If there’s no new data and the timeout is reached, the server responds with an empty or minimal message.Once the client receives a response—new data or a timeout—it immediately sends a new request to the server to keep the connection loop going.Simple to implement (uses standard HTTP).Supported universally since it uses standard HTTP, and it works reliably through firewalls and proxies.Higher latency after each update (client must re-establish connection).Resource-heavy on servers (many open hanging requests).Simple chat or comment systems where real-time but slightly delayed updates (near real-time) are acceptable.Notification systems for less frequent updates (e.g., Gmail’s "new email" alert).Legacy systems where WebSockets aren’t feasible.Code Example (JavaScript) provide a full-duplex, persistent connection between the client and the server.Once established, both parties can send data to each other at any time, without the overhead of repeated HTTP requests. Client sends an HTTP request with .: If supported, the server upgrades the connection to WebSocket (switching from  to ). After the handshake, client and server keep a TCP socket open for communication.Full-Duplex Communication: Once upgraded, data can be exchanged bidirectionally in real time until either side closes the connection.Ultra-low latency (no repeated handshakes).Lower overhead since there’s only one persistent connection rather than repeated HTTP requests.Scalable for real-time applications that need to support large number of concurrent users.More complex setup (requires the client and server to support WebSocket).Some proxies and firewalls may not allow WebSocket traffic.Complexity in implementation and handling reconnections/errors.Server resource usage might grow if you have a large number of concurrent connections.Live chat and collaboration tools (Slack, Google Docs, etc.).Multiplayer online games with real-time state synchronization.Live sports/financial dashboards that need to push frequent updates.Code Example (JavaScript)Both methods achieve real-time updates, but your choice depends on your project’s requirements: is easier to implement using standard libraries. Any environment that supports HTTP can handle it, often without extra packages. require a bit more setup and a capable proxy environment (e.g., support in Nginx or HAProxy). However, many frameworks (e.g., Socket.io) simplify the process significantly.Scalability and Performance can become resource-intensive with a large number of simultaneous clients, due to multiple open connections waiting on the server side. offer a more efficient, persistent connection and scale better for heavy, frequent data streams. fits scenarios where data updates aren’t super frequent. If new data arrives every few seconds or minutes, long polling might be enough. are better for high-frequency updates or two-way communication (e.g., multiple participants editing a document or interacting in a game). typically works even in older networks or those with strict firewalls. might face issues in certain corporate or older mobile environments, though this is less of a problem as the standard becomes more widespread.While both achieve real-time communication, WebSockets are generally more efficient for truly real-time applications, while Long Polling can be simpler to implement for less demanding scenarios.1. Server-Sent Events (SSE)Allows the server to push messages to the client over HTTP.It's simpler than WebSockets for one-way communication, but not full-duplex.Best suited for use cases like news feeds, real-time notifications, and status updates.Commonly used in IoT for lightweight publish-subscribe messaging.Specialized for device-to-device or device-to-server communication with minimal overhead.3. Libraries like Socket.ioProvides an abstraction over WebSockets for easier real-time communication.Automatically falls back to long polling if WebSockets are unsupported.Ensures cross-browser compatibility with robust and reliable performance.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re enjoying this newsletter and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>Podcast with Luca Rossi</title><link>https://refactoring.fm/p/growing-the-development-forest-with</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Fri, 24 Jan 2025 18:01:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[Luca Rossi hosts a podcast (and newsletter) called Refactoring, so it's
      obvious that we have some interests in common. The tile comes from me
      leaning heavily on Beth Anders-Beck and Kent Beck's metaphor of The Forest and The Desert. We talk
      about the impact of AI on software development, the metaphor of technical
      debt, and the current state of agile software development.]]></content:encoded></item><item><title>Master the Art of REST API Design</title><link>https://blog.algomaster.io/p/master-the-art-of-rest-api-design</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64780231-6a91-4d72-8ea2-159d4cd71f4e_1504x1056.png" length="" type=""/><pubDate>Thu, 23 Jan 2025 05:34:38 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[ is one of the most crucial steps in  and a key topic of discussion in .A well-designed API allows developers to easily integrate with a system while ensuring scalability and security.Over the years, various  have emerged, including REST, GraphQL, gRPC, Webhooks and SOAP, each designed to address different needs.However,  continue to dominate web development due to their simplicity, scalability, flexibility, widespread adoption and alignment with HTTP standards.In this article, we will dive into covering: for building a well-structured, scalable, and secure RESTful API.Performance optimization techniques to enhance API efficiency and response times.REST (Representational State Transfer) is an architectural style for designing web services that enable communication between clients (e.g., web browsers, mobile apps) and servers over the .REST uses HTTP methods (GET, POST, PUT, DELETE, etc.) to retrieve, create, update, and delete resources.To build a well-designed REST APIyou must first understand the fundamentals of theHTTP protocol.1. HTTP Methods (Verbs) in REST APIsHTTP provides a set of  that define the type of operation to be performed on a resource.In RESTful architectures, these methods typically map to CRUD operations:It’s essential to use the correct HTTP method to make your API clear and intuitive. For example,  signals a read-only request to developers and should never modify server data, while  indicates data creation or an action that results in a change.2. REST is Resource-OrientedIn RESTful API design, data is represented as , and each resource is identified by a Uniform Resource Identifier (URI).→ A collection (or list) of books → A specific book with ID 123An  is a combination of:An HTTP method (GET, POST, PUT etc.)A resource URI (, )Each endpoint represents a specific operation on a resource. → Fetch all books → Create a new book → Delete the book with ID 123Using clear and consistent endpoints helps developers quickly understand how to interact with your API.4. HTTP Status Codes: Understanding API ResponsesEach API response includes an , which indicates the result of the request.Using meaningful status codes is important for helping consumers of your API understand why a request might have failed and how they can fix or retry it.Common status codes include:: The request was successfully received and processed.: The request succeeded.: A new resource was successfully created.: The request succeeded, but there is no content to return.: Further action is needed to complete the request (e.g., a different endpoint or resource location).: There was an error in the request sent by the client.: The request was malformed or invalid.: Authentication is required or has failed.: The client does not have permission to access the resource.: The requested resource does not exist.: Rate limit exceeded.: The server encountered an error while processing the request.500 Internal Server Error: A general error occurred on the server.: The server is currently unable to handle the request, often due to maintenance or overload.1. Define Clear Resource Naming ConventionsUsing a consistent, intuitive, and hierarchical structure for API endpoints improves both readability and usability. The goal is to help developers quickly understand how to interact with your API without extensive documentation.Since REST is resource-oriented, focus on  (nouns) rather than  (verbs) for your endpoints. The HTTP methods (, , etc.) already describe the action, so using verbs in the URL are redundant.GET /getAllUsers
POST /createNewOrder
DELETE /removeProduct/123GET /users
POST /orders
DELETE /products/123]]></content:encoded></item><item><title>Codemods in other languages</title><link>https://martinfowler.com/articles/codemods-api-refactoring.html#CodemodsInOtherLanguages</link><author>Martin Fowler</author><category>dev</category><category>blog</category><pubDate>Wed, 22 Jan 2025 15:54:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How Docker Streamlines the Onboarding Process and Sets Up Developers for Success</title><link>https://www.docker.com/blog/how-docker-streamlines-the-onboarding-process-and-sets-up-developers-for-success/</link><author>Yiwen Xu</author><category>Docker blog</category><category>dev</category><category>docker</category><category>devops</category><pubDate>Wed, 22 Jan 2025 14:00:00 +0000</pubDate><source url="https://www.docker.com/">Docker</source><content:encoded><![CDATA[Nearly half (45%) of developers say they don’t have enough time for learning and development, according to a developer experience research study by Harness and Wakefield Research. Additionally, developer onboarding is a slow and painful process, with 71% of executive buyers saying that onboarding new developers takes at least two months. To accelerate innovation and bring products to market faster, organizations must empower developers with robust support and intuitive guardrails, enabling them to succeed within a structured yet flexible environment. That’s where Docker fits in: We help developers onboard quickly and help organizations set up the right guardrails to give developers the flexibility to innovate within the boundaries of company policies. Setting up developer teams for success Docker is recognized as one of the most used, desired, and admired developer tools, making it an essential component of any development team’s toolkit. For developers who are new to Docker, you can quickly get them up and running with Docker’s integrated development workflows, verified secure content, and accessible learning resources and community support.Streamlined developer onboardingWhen new developers join a team, Docker Desktop can significantly reduce the time and effort required to set up their development environments. Docker Desktop integrates seamlessly with popular IDEs, such as Visual Studio Code, allowing developers to containerize directly within familiar tools, accelerating learning within their usual workflows. Docker Extensions expand Docker Desktop’s capabilities and establish new functionalities, integrating developers’ favorite development tools into their application development and deployment workflows. Developers can also use Docker for GitHub Copilot for seamless onboarding with assistance for containerizing applications, generating Docker assets, and analyzing project vulnerabilities. In fact, the Docker extension is a top choice among developers in GitHub Copilot’s extension leaderboard, as highlighted by .Docker Build Cloud integrates with Docker Compose and CI workflows, making it a seamless transition for dev teams. Verified content on Docker Hub gives developers preconfigured, trusted images, reducing setup time and ensuring a secure foundation as they onboard onto projects. Docker Scout provides actionable insights and recommendations, allowing developers to enhance their container security awareness, scan for vulnerabilities, and improve security posture with real-time feedback. And, Testcontainers Cloud lets developers run reliable integration tests, with real dependencies defined in code. With these tools, developers can be confident about delivering high-quality and reliable apps and experiences in production.  Continuous learning with accessible knowledge resourcesContinuous learning is a priority for Docker, with a wide range of accessible resources and tools designed to help developers deepen their knowledge and stay current in their containerization journey.Docker Docs offers beginner-friendly guides, tutorials, and AI tools to guide developers through foundational concepts, empowering them to quickly build their container skills. Our collection of guides takes developers step by step to learn how Docker can optimize development workflows and how to use it with specific languages, frameworks, or technologies.Docker Hub’s AI Catalog empowers developers to discover, pull, and integrate AI models into their workflows, bridging the gap between innovation and implementation. Docker also offers regular webinars and tech talks that help developers stay updated on new features and best practices and provide a platform to discuss real-world challenges. If you’re a Docker Business customer, you can even request additional, customized training from our Docker experts. Docker’s partnerships with educational platforms and organizations, such as Udemy Training and LinkedIn Learning, ensure developers have access to comprehensive training — from beginner tutorials to advanced containerization topics.Docker’s global developer communityOne of Docker’s greatest strengths is its thriving global developer community, offering organizations a unique advantage by connecting them with a wealth of shared expertise, resources, and real-world solutions.With more than 20 million monthly active users, Docker’s community forums and events foster vibrant collaboration, giving developers access to a collective knowledge base that spans industries and expertise levels. Developers can ask questions, solve challenges, and gain insights from a diverse range of peers — from beginners to seasoned experts. Whether you’re troubleshooting an issue or exploring best practices, the Docker community ensures you’re never working in isolation.A key pillar of this ecosystem is the Docker Captains program — a network of experienced and passionate Docker advocates who are leaders in their fields. Captains share technical knowledge through blog posts, videos, webinars, and workshops, giving businesses and teams access to curated expertise that accelerates onboarding and productivity.Beyond forums and the Docker Captains program, Docker’s community-driven events, such as meetups and virtual workshops (Figure 1), provide developers with direct access to real-world use cases, innovative workflows, and emerging trends. These interactions foster continuous learning and help developers and their organizations keep pace with the ever-evolving software development landscape.For businesses, tapping into Docker’s extensive community means access to a vast pool of knowledge, support, and inspiration, which is a critical asset in driving developer productivity and innovation.Empowering developers with enhanced user management and securityTo scale and standardize app development processes across the entire company, you also need to have the right guardrails in place for governance, compliance, and security, which is often handled through enterprise control and admin management tools. Ideally, organizations provide guardrails without being overly prescriptive and slowing developer productivity and innovation. Modern enterprises require a layered security approach, beginning with trusted content as the foundation for building robust and compliant applications. This approach gives your dev teams a good foundation for building securely from the start. Throughout the software development process, you need a secure platform. For regulated industries like finance and public sectors, this means fortified dev environments. Security vulnerability analysis and policy evaluation tools also help inform improvements and remediation. Additionally, you need enterprise controls and dashboards that ensure enterprise IT and security teams can confidently monitor and manage risk. Setting up the right guardrails Docker provides a number of admin tools to safeguard your software with integrated container security in the Docker Business plan. Our goal is to improve security and compliance of developer environments with minimal impact on developer experience or productivity. Centralized settings for improved dev environments security With a solid foundation to build securely from the start, your organization can further enhance security throughout the software development process. Docker ensures software supply chain integrity through vulnerability scanning and image analysis with Docker Scout. Rapid remediation capabilities paired with detailed CVE reporting help developers quickly find and fix vulnerabilities, resulting in speedy time to resolution.Although containers are generally secure, container development tools still must be properly secured to reduce the risk of security breaches in the developer’s environment. Hardened Docker Desktop is an example of Docker’s fortified development environments with enhanced container isolation. It lets you enforce strict security settings and prevent developers and their containers from bypassing these controls. With air-gapped containers, you can further restrict containers from accessing network resources, limiting where data can be uploaded to or downloaded from.Continuous monitoring and managing risksWith the Admin Console and Docker Desktop Insights, IT administrators and security teams can visualize and understand how Docker is used within their organizations and manage the implementation of organizational configurations and policies (Figure 2). These insights help teams streamline processes and improve efficiency. For example, you can enforce sign-in for developers who don’t sign in to an account associated with your organization. This step ensures that developers receive the benefits of your Docker subscription and work within the boundaries of the company policies. For business and engineering leaders, full visibility and governance over the development process help ensure compliance and mitigate risk while driving developer productivity. Unlock innovation with Docker’s development suiteDocker is the leading suite of tools purpose-built for cloud-native development, combining a best-in-class developer experience with enterprise-grade security and governance. With Docker, your organization can streamline onboarding, foster innovation, and maintain robust compliance — all while empowering your teams to deliver impactful solutions to market faster and more securely. Explore the Docker Business plan today and unlock the full potential of your development processes.]]></content:encoded></item><item><title>Pre-RFC - Rename annotations</title><link>https://poignardazur.github.io//2025/01/22/rename-annotations/</link><author>Olivier Faure</author><category>Olivier Faure blog</category><category>dev</category><category>rust</category><category>blog</category><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><source url="https://poignardazur.github.io//">PoignardAzur</source><content:encoded><![CDATA[Let’s imagine you’re writing a crate.Your crate has a single  file, with two modules and an arbitrary number of items:After a while, you decide that Foobar should really be exported from the  module. It’s a breaking change, but you’re fine with releasing a new major version:Any user previously importing from your crate will get this error when they bump the version number:error[E0432]: unresolved import `best_crate::bar::Foobar`
  --> src/lib.rs:12:5
   |
12 | use best_crate::bar::Foobar;
   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`
   |
help: consider importing this struct instead
   |
12 | use best_crate::foo::Foobar;
   |     ~~~~~~~~~~~~~~~~~~~~~~~~
This isn’t ideal, but at least there’s the “consider importing this instead” message giving these users an easy way to fix this.But now let’s say you decide that “Foobar” is a terrible name, and your struct should really be named “Foofoo” instead for consistency:Now your users will get this message:error[E0432]: unresolved import `best_crate::bar::Foobar`
  --> src/lib.rs:12:5
   |
12 | use best_crate::bar::Foobar;
   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`
No message to help them figure out what to use instead.Rust should have an attribute to inform the compiler that an item previously existed, but has been moved and/or renamed:You might want to put the attribute on the “new” item instead, in which case the syntax would be:You’d probably want both, for cases where either the module of origin is removed, or the destination is no longer in the same crate (e.g. because you’ve split your crate into sub-crates).This diagnostic could help the compiler give more helpful error messages:error[E0432]: unresolved import `best_crate::bar::Foobar`
  --> src/lib.rs:12:5
   |
12 | use best_crate::bar::Foobar;
   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`
   |
help: this item has been renamed to `best_crate::foo::Foofoo`
   |
12 | use best_crate::foo::Foofoo;
   |     ~~~~~~~~~~~~~~~~~~~~~~~~
Because the compiler is  that this is the correct move and not just guessing based on name similarity,  and similar tools would be able to automatically apply the rename.Rename annotations would be helpful as a “grace period” after a crate’s major version change, but they would also be useful for purely internal refactors, using  to change  directives throughout your codebase.All in all, this feels like a pretty useful feature which, thanks to the  namespace’s relaxed constraints, could be implemented relatively swiftly in the Rust toolchain.]]></content:encoded></item><item><title>What&apos;s an API?</title><link>https://blog.algomaster.io/p/whats-an-api</link><author>Ashish Pratap Singh</author><category>dev</category><category>blog</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/ae14a376-92a3-4ff6-a329-8e4f2a7ac9b5_1546x1074.png" length="" type=""/><pubDate>Tue, 21 Jan 2025 05:31:04 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[API stands for Application Programming Interface.At its core, an API is a  that takes an  and gives you predictable Think of an API as a  that enables applications to interact without needing direct access to each other's code or database.Almost every digital service you use today—social media, e-commerce, online banking, ride-hailing apps—all of them are a bunch of APIs working together. – If you provide a city name as input (), the API returns the current temperature, humidity, and weather conditions. – If you provide a pickup and destination address, the API finds the  and calculates the estimated fare. – If you provide a list of numbers (), the API returns the  ().When engineers build APIs, they clearly define what inputs the API accepts and , ensuring consistent behavior across different applications.APIs follow a simple  model:A client (such as a web app or mobile app) makes a request to an API.The API (hosted on an API server) processes the request, interacts with the necessary databases or services, and prepares a response.The API sends the response back to the client in a structured format (usually JSON or XML).Every API requires , and passing incorrect data can result in errors.For example: If you tried putting your name into the Google Maps API as an input, that wouldn’t work very well.Some APIs also require inputs in a specific format.Example: The  might need the input as  instead of .APIs often  to ensure they are correct before processing them, which helps maintain .Just as APIs require , they also return .For example, the  always returns coordinates in the same format.{   "latitude": 40.6892,   "longitude": -74.0445 }If the API can’t find the location, it provides an error response explaining why.{   "error": "Invalid address format",   "code": 400 }If you’re finding this newsletter valuable and want to deepen your learning, consider becoming a .As a paid subscriber, you'll receive an exclusive deep-dive article every week, access to a structured100+topics and interview questions, and other .The apps you use every day—whether it's Gmail, Instagram, Uber, or Spotify—are essentially a collection of APIs with a polished user interface (UI) on top.Most applications follow the frontend/backend architecture, where:The  consists of APIs that handle data processing, business logic, and communication with databases.The  is a graphical user interface (GUI) that interacts with these APIs, making applications user-friendly and accessible without requiring users to write code.Let’s break this down with a real-world example: .Before the Uber app existed as a sleek, user-friendly experience, the company first built the core APIs that power ride-hailing services:Calculating Fares & RoutesMatching Riders & DriversThese APIs run on Uber’s servers, forming the . Every time you request a ride, track your driver, or make a payment, these backend APIs handle the request. are responsible for optimizing these APIs, improving ride-matching algorithms, securing transactions, and ensuring a smooth experience for millions of users.The backend APIs handle , but they —which isn't practical for everyday users. That’s why companies build a frontend (user interface) on top of these APIs, allowing users to interact with the system .When you enter your pickup & destination address, the frontend sends an API request to  and displays available cars.Once the trip is complete, the frontend may call the process payment API to display the receipt.APIs come in different forms depending on , , and .1. Open APIs (Public APIs)Open APIs, also known as , are accessible to external developers with minimal restrictions.Companies provide these APIs to encourage  to integrate their services and build new applications on top of them.Example: YouTube Data APINormally, when you use the , it makes  to fetch your video feed, search for content, or post comments. However, YouTube also provides a  that allows developers to access some of this functionality .For example, the  allows developers to fetch video results based on a keyword. If you send a request to the API with "machine learning tutorial" as the search term, it will return a structured response (JSON format) containing a list of relevant videos, including titles, descriptions, thumbnails, and video links.This is incredibly useful because it enables developers to build custom applications on top of YouTube.2. Internal APIs (Private APIs), also known as , are designed xclusively for internal use within an organization. Unlike Open APIs, these are not accessible to external developers and are used to facilitate seamless communication between different internal systems within a company.Let’s take  as an example. When you place an order, you might assume that a single system processes your request. In reality,  (order processing, inventory, payment, logistics etc..) work together behind the scenes to fulfill your order efficiently.Each of these APIs , but they communicate through well-defined protocols to ensure a smooth and efficient process.Internal APIs allow companies to break down their applications into smaller, manageable services, making it easier to scale. Developers can  across different projects, reducing  and speeding up development.The first two types of APIs we discussed—Open APIs and Internal APIs—are functional and serve  like fetching weather data or booking a ride.But there’s another category of APIs that developers use daily:  (also called  or ).These APIs don’t connect different applications; instead, they provide predefined functions within a programming language or framework to make development easier.Python’s built-in list APIWhen working with lists, Python provides a set of built-in functions (methods) to manipulate data.numbers = [5, 3, 8, 1, 4] numbers.sort()  # API call to sort the list  fruits = ["apple", "banana"] fruits.append("orange")  # API call to add an element  fruits.pop()  # API call to remove the last elementInstead of writing sorting algorithms from scratch, developers can use  or  in Python.Code APIs are not just limited to built-in programming language functions. Take , an AI/ML library. It provides a  for training machine learning models without needing to implement complex mathematical operations from scratch.For example, creating a  using TensorFlow's API is as simple as:import tensorflow as tf model = tf.keras.Sequential([tf.keras.layers.Dense(64, activation="relu")])Programming APIs abstract away complexity so that developers can focus on building solutions rather than reinventing the wheel.APIs communicate using different protocols and architectures that define how requests are sent, how responses are formatted, and how data is exchanged between systems.1. REST (Representational State Transfer)REST is the most widely used API communication method today. It is lightweight, stateless, and scalable, making it perfect for web services and mobile applications. REST APIs follow a set of design principles and use  (GET, POST, PUT, DELETE) to perform operations.REST APIs are based on , and each resource is accessed through a . The API follows the , meaning the client sends a request, and the server processes it and sends a response.Example: REST API for a BookstoreRetrieve a list of books (GET Request):GET https://api.bookstore.com/books[   { "id": 1, "title": "Clean Code", "author": "Robert C. Martin" },   { "id": 2, "title": "The Pragmatic Programmer", "author": "Andrew Hunt" } ]2. SOAP (Simple Object Access Protocol)SOAP is an older API communication method that relies on XML-based messaging. Unlike REST, which is lightweight, SOAP is more structured and secure, making it ideal for banking, healthcare, and enterprise applications.SOAP messages are sent using  and require a WSDL (Web Services Description Language) file, which defines the API's available functions and request structure.Example: SOAP API for a Banking Service Fetching account balance<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:bank="http://bank.example.com/">    <soapenv:Header/>    <soapenv:Body>       <bank:GetAccountBalance>          <bank:accountNumber>123456</bank:accountNumber>       </bank:GetAccountBalance>    </soapenv:Body> </soapenv:Envelope> <soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/">    <soapenv:Body>       <bank:GetAccountBalanceResponse>          <bank:balance>5000.00</bank:balance>       </bank:GetAccountBalanceResponse>    </soapenv:Body> </soapenv:Envelope>GraphQL is an alternative to REST that allows clients to request exactly the data they need, making it more efficient for modern applications. Unlike REST, which requires multiple API calls to fetch related data, GraphQL can fetch all necessary data in a single request.Instead of predefined endpoints, GraphQL exposes a , and the client sends queries to request specific fields.Example: Fetching a user's profile and their recent posts in a single request.{   user(id: 123) {     name     email     posts {       title       likes     }   } }{   "data": {     "user": {       "name": "Alice",       "email": "alice@example.com",       "posts": [         { "title": "Hello World", "likes": 100 },         { "title": "GraphQL is Amazing!", "likes": 200 }       ]     }   } }gRPC (Google Remote Procedure Call) is a high-performance API communication method that uses Protocol Buffers (Protobuf) instead of JSON or XML, making it faster and more efficient.gRPC uses  instead of text-based formats, reducing payload size and it supports , meaning the client and server can send data at the same time.Using an API might seem complex at first, but it follows a simple  pattern.Here’s a guide on how to find, access, and interact with an API step by step:Step 1: Find an API to UseBefore using an API, you need to  for your needs. APIs are available for different services like weather data, finance, social media, etc.Official API Documentation:Step 2: Read the API DocumentationAPI documentation explains how to use the API, available endpoints, authentication, and response formats.The OpenWeatherMap API allows users to fetch real-time weather data. Here's a breakdown of its key components:https://api.openweathermap.org/data/3.0/weather?q=city_name&appid=YOUR_API_KEY: City name (e.g., ): API Key (required for access)Step 3: Get API Access (API Key / Authentication)Most APIs  to prevent unauthorized access and manage usage limits.Common Authentication Methods:A unique key provided by the API serviceSecure login via Google, Github, etc.Token-based authenticationUsername + password (Base64 encoded)Example: Getting an API Key (OpenWeather API)Sign up at https://home.openweathermap.org/users/sign_up.Go to the  section and generate a key.Use the API key in requests:GET https://api.openweathermap.org/data/2.5/weather?q=London&appid=YOUR_API_KEYStep 4: Test the API Using Postman or cURLBefore writing code,  to see how it responds.Option 1: Using Postman (Recommended for Beginners)Click , enter the API endpoint URL (https://api.openweathermap.org/data/3.0/weather?q=London&appid=YOUR_API_KEY).Select  as the HTTP method.Click  and view the response in .Option 2: Using cURL (For Command Line Users)You can also test APIs directly from the  using .curl -X GET "https://api.openweathermap.org/data/3.0/weather?q=New+York&appid=YOUR_API_KEY"Step 5: Write Code to Call the APINow that you’ve tested the API, it’s time to integrate it into your application.Example: Calling an API in Pythonimport requests  url = "https://api.openweathermap.org/data/3.0/weather?q=New York&appid=YOUR_API_KEY" response = requests.get(url)  if response.status_code == 200:     data = response.json()     print(f"Temperature: {data['main']['temp']}°C") else:     print("Error:", response.status_code) – Sends an API request. – Converts response to JSON.if response.status_code == 200 – Checks if the request was successful.Step 6: Handle Errors & Rate LimitsAPIs don’t always return perfect responses. You should handle: (e.g., wrong city name). (e.g., expired API keys). (e.g., exceeding request limits).Example: Handling API Errors in Pythonif response.status_code == 200:     data = response.json()     print(f"Weather: {data['weather'][0]['description']}") elif response.status_code == 401:     print("Error: Invalid API key") elif response.status_code == 404:     print("Error: City not found") else:     print(f"Unexpected error: {response.status_code}")Step 7: Use API Responses in Your ApplicationOnce you fetch data from an API, you can display it dynamically in a web or mobile app. You can build a weather dashboard using the OpenWeatherMap API.Fetch live weather data from the API.Parse and extract relevant details (temperature, humidity, condition).Display the weather report in a user-friendly format.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re finding this newsletter helpful and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>Patterns of use of Vello crate</title><link>https://poignardazur.github.io//2025/01/18/vello-analysis/</link><author>Olivier Faure</author><category>Olivier Faure blog</category><category>dev</category><category>rust</category><category>blog</category><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><source url="https://poignardazur.github.io//">PoignardAzur</source><content:encoded><![CDATA[This document tries to establish patterns among a list of crates and Github projects using the Vello renderer.The crate list isn’t meant to be exhaustive, but it’s pretty large: I’ve sifted through maybe 40 or so reverse dependencies of the Vello repository, up to the point where most of the READMEs I read were along the lines of “WIP: quick experiment paint shapes with Vello”.Don’t expect anything groundbreaking. My main focus is on common patterns among people using Vello’s  API; most of this is going to be pretty dry.I’ve mostly noticed two types of projects with Vello as a direct dependency: bridging Vello with some other format or framework, or providing wrapper functions for Vello’s API. Ex: , , etc. that renders specific things with Vello.(Though in practice, a few blurred the lines, and a lot of projects that I put in one category or the other mostly ended up using Vello for example code or for very basic painting.)The numbers (18 engine projects, 13 app-like projects) aren’t too surprising if you’re familiar with the “Rust has 50 game engines and 3 games” stereotype. They also match my “gut feeling” reading the code, where it feels like a lot more projects use Vello in a very systematized way, as some kind of middleware or optional backend than as a plug-and-play dependency to paint a bunch of shapes.For instance, I saw almost no project calling the  function more than five times.This is confounded by the fact that I only looked for direct dependents of Vello; I might have found more app projects looking for dependents of Masonry or bevy_vello. Maybe the painting-privitive-heavy code is in the dependents of one of those engine code projects I cited.Graphite, for example, is a primitive-heavy 2D editor, but its use of Vello is bottlenecked through a middleware layer with its own internal representation.In general, though, my impression is that the stereotype is mostly true.Another interesting pattern is that code actually using Vello to paint things was often in amateur stub projects, which is good for our purposes: it tells us how people who have little experience with Vello end up using it.Fill and stroke arguments and  are the most used methods by a very wide margin.As a reminder, their prototype is:A lot code calling them looks like this:Note the heavy usage of default values: is the default fill setting for most paint APIs. creates a stroke with rount joins and caps, no dashes, and the given width. is set to  for both methods. is set to  for both methods.These patterns can be found throughout the projects I’ve linked. In total, I’ve counted:16 projects using  with ,  and .10 projects using  with ,  and .7 projects using  with , a transform and .3 projects using  with , a transform and .Projects that used all the arguments of  or  were rare, and were generally written as middleware code passing these arguments from another source. For example:These projects tended to have one instance code calling each Scene method in the entire repository.Most of the projects I’ve looked at used the  and  API exclusively.
Few of them used , , , etc.In total, I’ve counted about a dozen projects using any of these APIs.Those that did tended to be the “render any arbitrary SVG” kinds of projects.Based on the above, I’d recommend having Vello export the following API: and  use the minimum number of arguments. and  use an additional  argument. and  use the full API.With this API, the  code I quoted would look like this:This would also let us remove most of the helpers in  in Masonry.]]></content:encoded></item></channel></rss>
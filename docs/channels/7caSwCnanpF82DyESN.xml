<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Official News</title><link>https://konrad.website/feeds/</link><description></description><item><title>Support the open source projects you love this Valentine’s Day</title><link>https://github.blog/open-source/support-the-open-source-projects-you-love-this-valentines-day/</link><author>Martin Woodward</author><category>official</category><category>dev</category><pubDate>Fri, 14 Feb 2025 17:00:04 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[While everyone likes flowers and chocolates, why not show your love for your favorite open source projects this Valentine’s and give appreciation to the maintainers who keep them running?Many of the open source projects we rely on every day are maintained by dedicated volunteers. Sponsoring projects isn’t just about altruism, it’s about investing in the future. Many maintainers work on open source projects in their spare time. Sponsorships can help them dedicate more time to the projects you depend on. With financial support, maintainers can help cover costs such as development and hosting.When you sponsor a maintainer, you’re helping them continue their work and letting them know you value the creativity and love they’ve poured into their project. For this Valentine’s Day, let’s show love to the maintainers who keep our favorite projects alive and thriving.The benefits of financial sponsorshipSponsorship is a tangible way to show support for the open source community. It can be a huge morale boost, as well as bring visibility and validation. Sponsored projects often gain more attention, leading to a virtuous cycle of more contributors and users, which results in better software for everyone.Getting started: How to best invest in open sourceIdentify critical dependencies:Review your project’s dependencies to identify which open source libraries and tools are crucial to your operations. If you use tools like npm, pip, or maven, you can review their dependencies and prioritize which are critical to your project’s success.  Evaluate the project activity: Check the recent and trending activity of repositories and look for signs of contributions, shipped features, bug fixes, active maintenance, and community engagement. Helpful metrics like recent commits, issue resolution, and community engagement are all good markers for where funding could help. Projects with a high impact but low funding could be great candidates to prioritize for sponsorship.   Take the opportunity to look at the issues and contribution guidelines. Try to understand their funding needs and how your support makes a difference. But also consider investing in ways beyond just financial support. You could provide bug fixes if you feel confident in your code. You could also take the time to tell someone about the project and why you think it’s awesome. Maintainers can show love for their sponsors as well. Sponsors appreciate knowing their contribution is making a difference. Whether it’s a simple mention on social media to acknowledge their contribution, featuring them on your GitHub Sponsors profile, exclusive updates and behind-the-scenes insights about your project and community, or even a brief personalized thank-you message, tokens of appreciation can help bring more funding your way.This Valentine’s Day, let’s show our favorite projects a bit more appreciation. Whether it’s a heartfelt contribution to fix bugs or docs, spreading the word about why it’s a great project and helping people use it, or a sweet one-time donation, every bit helps keep our beloved digital world turning.Ready to start sponsoring? Visit GitHub Sponsors and find the projects that matter most to you.Happy Valentine’s Day to all the open source contributors and supporters out there! And remember, you don’t need to wait for a special occasion like Valentine’s Day to show your appreciation for maintainers—every day is a perfect day to support open source! 💖]]></content:encoded></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/</link><author></author><category>dev</category><category>official</category><category>k8s</category><category>devops</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Dev - Kubernetes Blog</source><content:encoded><![CDATA[Kubernetes 1.31
completed the largest migration in Kubernetes history, removing the in-tree
cloud provider. While the component migration is now done, this leaves some additional
complexity for users and installer projects (for example, kOps or Cluster API) . We will go
over those additional steps and failure points and make recommendations for cluster owners.
This migration was complex and some logic had to be extracted from the core components,
building four new subsystems.One of the most critical functionalities of the cloud controller manager is the node controller,
which is responsible for the initialization of the nodes.As you can see in the following diagram, when the  starts, it registers the 
object with the apiserver, Tainting the node so it can be processed first by the
cloud-controller-manager. The initial  is missing the cloud-provider specific information,
like the Node Addresses and the Labels with the cloud provider specific information like the
Node, Region and Instance type information.sequenceDiagram
autonumber
rect rgb(191, 223, 255)
Kubelet->>+Kube-apiserver: Create Node
Note over Kubelet: Taint: node.cloudprovider.kubernetes.io
Kube-apiserver->>-Kubelet: Node Created
end
Note over Kube-apiserver: Node is Not Ready Tainted, Missing Node Addresses*, ...
Note over Kube-apiserver: Send Updates
rect rgb(200, 150, 255)
Kube-apiserver->>+Cloud-controller-manager: Watch: New Node Created
Note over Cloud-controller-manager: Initialize Node:Cloud Provider Labels, Node Addresses, ...
Cloud-controller-manager->>-Kube-apiserver: Update Node
end
Note over Kube-apiserver: Node is Ready
This new initialization process adds some latency to the node readiness. Previously, the kubelet
was able to initialize the node at the same time it created the node. Since the logic has moved
to the cloud-controller-manager, this can cause a chicken and egg problem
during the cluster bootstrapping for those Kubernetes architectures that do not deploy the
controller manager as the other components of the control plane, commonly as static pods,
standalone binaries or daemonsets/deployments with tolerations to the taints and using
 (more on this below)Examples of the dependency problemAs noted above, it is possible during bootstrapping for the cloud-controller-manager to be
unschedulable and as such the cluster will not initialize properly. The following are a few
concrete examples of how this problem can be expressed and the root causes for why they might
occur.These examples assume you are running your cloud-controller-manager using a Kubernetes resource
(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods
rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it
will schedule properly.Example: Cloud controller manager not scheduling due to uninitialized taintAs noted in the Kubernetes documentation, when the kubelet is started with the command line
flag --cloud-provider=external, its corresponding  object will have a no schedule taint
named node.cloudprovider.kubernetes.io/uninitialized added. Because the cloud-controller-manager
is responsible for removing the no schedule taint, this can create a situation where a
cloud-controller-manager that is being managed by a Kubernetes resource, such as a 
or , may not be able to schedule.If the cloud-controller-manager is not able to be scheduled during the initialization of the
control plane, then the resulting  objects will all have the
node.cloudprovider.kubernetes.io/uninitialized no schedule taint. It also means that this taint
will not be removed as the cloud-controller-manager is responsible for its removal. If the no
schedule taint is not removed, then critical workloads, such as the container network interface
controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.Example: Cloud controller manager not scheduling due to not-ready taintThe next example would be possible in situations where the container network interface (CNI) is
waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not
tolerated the taint which would be removed by the CNI."The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly."One of the conditions that can lead to a  resource having this taint is when the container
network has not yet been initialized on that node. As the cloud-controller-manager is responsible
for adding the IP addresses to a  resource, and the IP addresses are needed by the container
network controllers to properly configure the container network, it is possible in some
circumstances for a node to become stuck as not ready and uninitialized permanently.This situation occurs for a similar reason as the first example, although in this case, the
node.kubernetes.io/not-ready taint is used with the no execute effect and thus will cause the
cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is
not able to execute, then it will not initialize the node. It will cascade into the container
network controllers not being able to run properly, and the node will end up carrying both the
node.cloudprovider.kubernetes.io/uninitialized and node.kubernetes.io/not-ready taints,
leaving the cluster in an unhealthy state.There is no one “correct way” to run a cloud-controller-manager. The details will depend on the
specific needs of the cluster administrators and users. When planning your clusters and the
lifecycle of the cloud-controller-managers please consider the following guidance:For cloud-controller-managers running in the same cluster, they are managing.Use host network mode, rather than the pod network: in most cases, a cloud controller manager
will need to communicate with an API service endpoint associated with the infrastructure.
Setting “hostNetwork” to true will ensure that the cloud controller is using the host
networking instead of the container network and, as such, will have the same network access as
the host operating system. It will also remove the dependency on the networking plugin. This
will ensure that the cloud controller has access to the infrastructure endpoint (always check
your networking configuration against your infrastructure provider’s instructions).Use a scalable resource type.  and  are useful for controlling the
lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy
as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using
these primitives to control the lifecycle of your cloud controllers and running multiple
replicas, you must remember to enable leader election, or else your controllers will collide
with each other which could lead to nodes not being initialized in the cluster.Target the controller manager containers to the control plane. There might exist other
controllers which need to run outside the control plane (for example, Azure’s node manager
controller). Still, the controller managers themselves should be deployed to the control plane.
Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the
control plane to ensure that they are running in a protected space. Cloud controllers are vital
to adding and removing nodes to a cluster as they form a link between Kubernetes and the
physical infrastructure. Running them on the control plane will help to ensure that they run
with a similar priority as other core cluster controllers and that they have some separation
from non-privileged user workloads.
It is worth noting that an anti-affinity stanza to prevent cloud controllers from running
on the same host is also very useful to ensure that a single node failure will not degrade
the cloud controller performance.Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud
controller container to ensure that it will schedule to the correct nodes and that it can run
in situations where a node is initializing. This means that cloud controllers should tolerate
the node.cloudprovider.kubernetes.io/uninitialized taint, and it should also tolerate any
taints associated with the control plane (for example, node-role.kubernetes.io/control-plane
or node-role.kubernetes.io/master). It can also be useful to tolerate the
node.kubernetes.io/not-ready taint to ensure that the cloud controller can run even when the
node is not yet available for health monitoring.For cloud-controller-managers that will not be running on the cluster they manage (for example,
in a hosted control plane on a separate cluster), then the rules are much more constrained by the
dependencies of the environment of the cluster running the cloud-controller-manager. The advice
for running on a self-managed cluster may not be appropriate as the types of conflicts and network
constraints will be different. Please consult the architecture and requirements of your topology
for these scenarios.This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is
important to note that this is for demonstration purposes only, for production uses please
consult your cloud provider’s documentation.apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
name: cloud-controller-manager
namespace: kube-system
spec:
replicas: 2
selector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
strategy:
type: Recreate
template:
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
annotations:
kubernetes.io/description: Cloud controller manager for my infrastructure
spec:
containers: # the container details will depend on your specific cloud controller manager
- name: cloud-controller-manager
command:
- /bin/my-infrastructure-cloud-controller-manager
- --leader-elect=true
- -v=1
image: registry/my-infrastructure-cloud-controller-manager@latest
resources:
requests:
cpu: 200m
memory: 50Mi
hostNetwork: true # these Pods are part of the control plane
nodeSelector:
node-role.kubernetes.io/control-plane: ""
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- topologyKey: "kubernetes.io/hostname"
labelSelector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
tolerations:
- effect: NoSchedule
key: node-role.kubernetes.io/master
operator: Exists
- effect: NoExecute
key: node.kubernetes.io/unreachable
operator: Exists
tolerationSeconds: 120
- effect: NoExecute
key: node.kubernetes.io/not-ready
operator: Exists
tolerationSeconds: 120
- effect: NoSchedule
key: node.cloudprovider.kubernetes.io/uninitialized
operator: Exists
- effect: NoSchedule
key: node.kubernetes.io/not-ready
operator: Exists
When deciding how to deploy your cloud controller manager it is worth noting that
cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple
replicas of a cloud controller manager is good practice for ensuring high-availability and
redundancy, but does not contribute to better performance. In general, only a single instance
of a cloud controller manager will be reconciling a cluster at any given time.]]></content:encoded></item><item><title>Extensible Wasm Applications with Go</title><link>https://go.dev/blog/wasmexport</link><author>Cherry Mui</author><category>dev</category><category>official</category><category>go</category><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Dev - Golang Blog</source><content:encoded><![CDATA[
      Cherry Mui
      13 February 2025
      Go 1.24 enhances its WebAssembly (Wasm) capabilities with the
addition of the  directive and the ability to build a reactor
for WebAssembly System Interface (WASI).
These features enable Go developers to export Go functions to Wasm,
facilitating better integration with Wasm hosts and expanding the possibilities
for Go-based Wasm applications.WebAssembly and the WebAssembly System InterfaceWebAssembly (Wasm) is a binary instruction format
that was initially created for web browsers, providing the execution of
high-performance, low-level code at speeds approaching native performance.
Since then, Wasm’s utility has expanded, and it is now used in various
environments beyond the browser.
Notably, cloud providers offer services that directly execute Wasm
executables, taking advantage of the
WebAssembly System Interface (WASI) system call API.
WASI allows these executables to interact with system resources.Go first added support for compiling to Wasm in the 1.11 release, through the
 port.
Go 1.21 added a new port targeting the WASI preview 1 syscall API through the
new  port.Exporting Go Functions to Wasm with Go 1.24 introduces a new compiler directive, , which allows
developers to export Go functions to be called from outside of the
Wasm module, typically from a host application that runs the Wasm runtime.
This directive instructs the compiler to make the annotated function available
as a Wasm export
in the resulting Wasm binary.To use the  directive, simply add it to a function definition://go:wasmexport add
func add(a, b int32) int32 { return a + b }
With this, the Wasm module will have an exported function named  that
can be called from the host.This is analogous to the cgo  directive,
which makes the function available to be called from C,
though  uses a different, simpler mechanism.A WASI reactor is a WebAssembly module that operates continuously, and
can be called upon multiple times to react on events or requests.
Unlike a “command” module, which terminates after its main function finishes,
a reactor instance remains live after initialization, and its exports remain
accessible.With Go 1.24, one can build a WASI reactor with the  build
flag.$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm
The build flag signals to the linker not to generate the  function
(the entry point for a command module), and instead generate an
 function, which performs runtime and package initialization,
along with any exported functions and their dependencies.
The  function must be called before any other exported functions.
The  function will not be automatically invoked.To use a WASI reactor, the host application first initializes it by calling
, then simply invoke the exported functions.
Here is an example using Wazero, a Go-based Wasm runtime
implementation:// Create a Wasm runtime, set up WASI.
r := wazero.NewRuntime(ctx)
defer r.Close(ctx)
wasi_snapshot_preview1.MustInstantiate(ctx, r)

// Configure the module to initialize the reactor.
config := wazero.NewModuleConfig().WithStartFunctions("_initialize")

// Instantiate the module.
wasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)

// Call the exported function.
fn := wasmModule.ExportedFunction("add")
var a, b int32 = 1, 2
res, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))
c := api.DecodeI32(res[0])
fmt.Printf("add(%d, %d) = %d\n", a, b, c)

// The instance is still alive. We can call the function again.
res, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))
fmt.Printf("add(%d, %d) = %d\n", b, c, api.DecodeI32(res[0]))
The  directive and the reactor build mode allow applications to
be extended by calling into Go-based Wasm code.
This is particularly valuable for applications that have adopted Wasm as a
plugin or extension mechanism with well-defined interfaces.
By exporting Go functions, applications can leverage the Go Wasm modules to
provide functionality without needing to recompile the entire application.
Furthermore, building as a reactor ensures that the exported functions can be
called multiple times without requiring reinitialization, making it suitable
for long-running applications or services.Supporting rich types between the host and the clientGo 1.24 also relaxes the constraints on types that can be used as input and
result parameters with  functions.
For example, one can pass a bool, a string, a pointer to an , or a
pointer to a struct which embeds  and contains supported
field types
(see the documentation for detail).
This allows Go Wasm applications to be written in a more natural and ergonomic
way, and removes some unnecessary type conversions.While Go 1.24 has made significant enhancements to its Wasm capabilities,
there are still some notable limitations.Wasm is a single-threaded architecture with no parallelism.
A  function can spawn new goroutines.
But if a function creates a background goroutine, it will not continue
executing when the  function returns, until calling back into
the Go-based Wasm module.While some type restrictions have been relaxed in Go 1.24, there are still
limitations on the types that can be used with  and
 functions.
Due to the unfortunate mismatch between the 64-bit architecture of the client
and the 32-bit architecture of the host, it is not possible to pass pointers in
memory.
For example, a  function cannot take a pointer to a struct that
contains a pointer-typed field.The addition of the ability to build a WASI reactor and export Go functions to
Wasm in Go 1.24 represent a significant step forward for Go’s WebAssembly
capabilities.
These features empower developers to create more versatile and powerful Go-based
Wasm applications, opening up new possibilities for Go in the Wasm ecosystem.]]></content:encoded></item><item><title>2024 State of Rust Survey Results</title><link>https://blog.rust-lang.org/2025/02/13/2024-State-Of-Rust-Survey-results.html</link><author>The Rust Survey Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[The Rust Survey Team is excited to share the results of our 2024 survey on the Rust Programming language, conducted between December 5, 2024 and December 23, 2024.
As in previous years, the 2024 State of Rust Survey was focused on gathering insights and feedback from Rust users, and all those who are interested in the future of Rust more generally.This ninth edition of the survey surfaced new insights and learning opportunities straight from the global Rust language community, which we will summarize below. In addition to this blog post,  containing charts with aggregated results of all questions in the survey.Our sincerest thanks to every community member who took the time to express their opinions and experiences with Rust over the past year. Your participation will help us make Rust better for everyone.There's a lot of data to go through, so strap in and enjoy!As shown above, in 2024, we have received fewer survey views than in the previous year. This was likely caused simply by the fact that the survey ran only for two weeks, while in the previous year it ran for almost a month. However, the completion rate has also dropped, which seems to suggest that the survey might be a bit too long. We will take this into consideration for the next edition of the survey.The State of Rust survey not only gives us excellent insight into how many Rust users around the world are using and experiencing the language but also gives us insight into the makeup of our global community. This information gives us a sense of where the language is being used and where access gaps might exist for us to address over time. We hope that this data and our related analysis help further important discussions about how we can continue to prioritize global access and inclusivity in the Rust community.Same as every year, we asked our respondents in which country they live in. The top 10 countries represented were, in order: United States (22%), Germany (14%), United Kingdom (6%), France (6%), China (5%), Canada (3%), Netherlands (3%), Russia (3%), Australia (2%), and Sweden (2%). We are happy to see that Rust is enjoyed by users from all around the world! You can try to find your country in the chart below:We also asked whether respondents consider themselves members of a marginalized community. Out of those who answered, 74.5% selected no, 15.5% selected yes, and 10% preferred not to say.We have asked the group that selected “yes” which specific groups they identified as being a member of. The majority of those who consider themselves a member of an underrepresented or marginalized group in technology identify as lesbian, gay, bisexual, or otherwise non-heterosexual. The second most selected option was neurodivergent at 46% followed by trans at 35%.Each year, we must acknowledge the diversity, equity, and inclusivity (DEI) related gaps in the Rust community and open source as a whole. We believe that excellent work is underway at the Rust Foundation to advance global access to Rust community gatherings and distribute grants to a diverse pool of maintainers each cycle, which you can learn more about here. Even so, global inclusion and access is just one element of DEI, and the survey working group will continue to advocate for progress in this domain.The number of respondents that self-identify as a Rust user was quite similar to last year, around 92%. This high number is not surprising, since we primarily target existing Rust developers with this survey.Similarly as last year, around 31% of those who did not identify as Rust users cited the perception of difficulty as the primary reason for not using Rust. The most common reason for not using Rust was that the respondents simply haven’t had the chance to try it yet.Of the former Rust users who participated in the 2024 survey, 36% cited factors outside their control as a reason why they no longer use Rust, which is a 10pp decrease from last year. This year, we also asked respondents if they would consider using Rust again if an opportunity comes up, which turns out to be true for a large fraction of the respondents (63%). That is good to hear!Closed answers marked with N/A were not present in the previous version(s) of the survey.Those not using Rust anymore told us that it is because they don't really need it (or the goals of their company changed) or because it was not the right tool for the job. A few reported being overwhelmed by the language or its ecosystem in general or that switching to or introducing Rust would have been too expensive in terms of human effort.Of those who used Rust in 2024, 53% did so on a daily (or nearly daily) basis — an increase of 4pp from the previous year. We can observe an upward trend in the frequency of Rust usage over the past few years, which suggests that Rust is being increasingly used at work. This is also confirmed by other answers mentioned in the Rust at Work section later below.Rust expertise is also continually increasing amongst our respondents! 20% of respondents can write (only) simple programs in Rust (a decrease of 3pp from 2023), while 53% consider themselves productive using Rust — up from 47% in 2023. While the survey is just one tool to measure the changes in Rust expertise overall, these numbers are heartening as they represent knowledge growth for many Rustaceans returning to the survey year over year.Unsurprisingly, the most popular version of Rust is , either the most recent one or whichever comes with the users' Linux distribution. Almost a third of users also use the latest nightly release, due to various reasons (see below). However, it seems that the beta toolchain is not used much, which is a bit unfortunate. We would like to encourage Rust users to use the beta toolchain more (e.g. in CI environments) to help test soon-to-be stabilized versions of Rust.People that use the nightly toolchain mostly do it to gain access to specific unstable language features. Several users have also mentioned that rustfmt works better for them on nightly or that they use the nightly compiler because of faster compilation times.To use Rust, programmers first have to learn it, so we are always interested in finding out how do they approach that. Based on the survey results, it seems that most users learn from Rust documentation and also from The Rust Programming Language book, which has been a favourite learning resource of new Rustaceans for a long time. Many people also seem to learn by reading the source code of Rust crates. The fact that both the documentation and source code of tens of thousands of Rust crates is available on docs.rs and GitHub makes this easier.In terms of answers belonging to the "Other" category, they can be clustered into three categories: people using LLM (large language model) assistants (Copilot, ChatGPT, Claude, etc.), reading the official Rust forums (Discord, URLO) or being mentored while contributing to Rust projects. We would like to extend a big thank you to those making our spaces friendly and welcoming for newcomers, as it is important work and it pays off. Interestingly, a non-trivial number of people "learned by doing" and used rustc error messages and clippy as a guide, which is a good indicator of the quality of Rust diagnostics.In terms of formal education, it seems that Rust has not yet penetrated university curriculums, as this is typically a very slowly moving area. Only a very small number of respondents (around 3%) have taken a university Rust course or used university learning materials.In terms of operating systems used by Rustaceans, Linux was the most popular choice, and it seems that it is getting increasingly popular year after year. It is followed by macOS and Windows, which have a very similar share of usage.As you can see in the wordcloud, there are also a few users that prefer Arch, btw.Rust programmers target a diverse set of platforms with their Rust programs. We saw a slight uptick in users targeting embedded and mobile platforms, but otherwise the distribution of platforms stayed mostly the same as last year. Since the WebAssembly target is quite diverse, we have split it into two separate categories this time. Based on the results it is clear that when using WebAssembly, it is mostly in the context of browsers (23%) rather than other use-cases (7%).We cannot of course forget the favourite topic of many programmers: which IDE (developer environment) they use. Although Visual Studio Code still remains the most popular option, its share has dropped by 5pp this year. On the other hand, the Zed editor seems to have gained considerable traction recently. The small percentage of those who selected "Other" are using a wide range of different tools: from CursorAI to classics like Kate or Notepad++. Special mention to the 3 people using "ed", that's quite an achievement.You can also take a look at the linked wordcloud that summarizes open answers to this question (the "Other" category), to see what other editors are also popular.We were excited to see that more and more people use Rust at work for the majority of their coding, 38% vs 34% from last year. There is a clear upward trend in this metric over the past few years.The usage of Rust within companies also seems to be rising, as 45% of respondents answered that their organisation makes non-trivial use of Rust, which is a 7pp increase from 2023.Once again, the top reason employers of our survey respondents invested in Rust was the ability to build relatively correct and bug-free software. The second most popular reason was Rust’s performance characteristics. 21% of respondents that use Rust at work do so because they already know it, and it's thus their default choice, an uptick of 5pp from 2023. This seems to suggest that Rust is becoming one of the baseline languages of choice for more and more companies.Similarly to the previous year, a large percentage of respondents (82%) report that Rust helped their company achieve its goals. In general, it seems that programmers and companies are quite happy with their usage of Rust, which is great!In terms of technology domains, the situation is quite similar to the previous year. Rust seems to be especially popular for creating server backends, web and networking services and cloud technologies. It also seems to be gaining more traction for embedded use-cases.You can scroll the chart to the right to see more domains. Note that the Automotive domain was not offered as a closed answer in the 2023 survey (it was merely entered through open answers), which might explain the large jump.It is exciting to see the continued growth of professional Rust usage and the confidence so many users feel in its performance, control, security and safety, enjoyability, and more!As always, one of the main goals of the State of Rust survey is to shed light on challenges, concerns, and priorities on Rustaceans’ minds over the past year.We have asked our users about aspects of Rust that limit their productivity. Perhaps unsurprisingly, slow compilation was at the top of the list, as it seems to be a perennial concern of Rust users. As always, there are efforts underway to improve the speed of the compiler, such as enabling the parallel frontend or switching to a faster linker by default. We invite you to test these improvements and let us know if you encounter any issues.Other challenges included subpar support for debugging Rust and high disk usage of Rust compiler artifacts. On the other hand, most Rust users seem to be very happy with its runtime performance, the correctness and stability of the compiler and also Rust's documentation.In terms of specific unstable (or missing) features that Rust users want to be stabilized (or implemented), the most desired ones were async closures and if/let while chains. Well, we have good news! Async closures will be stabilized in the next version of Rust (1.85), and if/let while chains will hopefully follow soon after, once Edition 2024 is released (which will also happen in Rust 1.85).Other coveted features are generators (both sync and async) and more powerful generic const expressions. You can follow the Rust Project Goals to track the progress of these (and other) features.In the open answers to this question, people were really helpful and tried hard to describe the most notable issues limiting their productivity. We have seen mentions of struggles with async programming (an all-time favourite), debuggability of errors (which people generally love, but they are not perfect for everyone) or Rust tooling being slow or resource intensive (rust-analyzer and rustfmt). Some users also want a better IDE story and improved interoperability with other languages.This year, we have also included a new question about the speed of Rust's evolution. While most people seem to be content with the status quo, more than a quarter of people who responded to this question would like Rust to stabilize and/or add features more quickly, and only 7% of respondents would prefer Rust to slow down or completely stop adding new features.Interestingly, when we asked respondents about their main worries for the future of Rust, one of the top answers remained the worry that Rust will become too complex. This seems to be in contrast with the answers to the previous question. Perhaps Rust users still seem to consider the complexity of Rust to be manageable, but they worry that one day it might become too much.We are happy to see that the amount of respondents concerned about Rust Project governance and lacking support of the Rust Foundation has dropped by about 6pp from 2023.Each year, the results of the State of Rust survey help reveal the areas that need improvement in many areas across the Rust Project and ecosystem, as well as the aspects that are working well for our community.If you have any suggestions for the Rust Annual survey, please let us know!We are immensely grateful to those who participated in the 2024 State of Rust Survey and facilitated its creation. While there are always challenges associated with developing and maintaining a programming language, this year we were pleased to see a high level of survey participation and candid feedback that will truly help us make Rust work better for everyone.If you’d like to dig into more details, we recommend you to browse through the full survey report.]]></content:encoded></item><item><title>GitHub Availability Report: January 2025</title><link>https://github.blog/news-insights/company-news/github-availability-report-january-2025/</link><author>Jakub Oleksy</author><category>official</category><category>dev</category><pubDate>Wed, 12 Feb 2025 20:49:36 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[In January, we experienced three incidents that resulted in degraded performance across GitHub services.January 09 1:26 UTC (lasting 31 minutes)On January 9, 2025, between 01:26 UTC and 01:56 UTC, GitHub experienced widespread disruption to many services, with users receiving 500 responses when trying to access various functionality. This was due to a deployment which introduced a query that saturated a primary database server. On average, the error rate was 6% and peaked at 6.85% of update requests.We were able to mitigate the incident by identifying the source of the problematic query and rolling back the deployment. The internal tooling and our dashboards surfaced the relevant data that helped us quickly identify the problematic query. It took us a total of 14 minutes from the time to engage to finding the errant query.However, we are investing in tooling to detect problematic queries prior to deployment to prevent and to reduce our time to detection and mitigation of issues like this one in the future.January 13 23:35 UTC (lasting 49 minutes)On January 13, 2025, between 23:35 UTC and 00:24 UTC, all Git operations were unavailable due to a configuration change related to traffic routing and testing that caused our internal load balancer to drop requests between services that Git relies upon.We mitigated the incident by rolling back the configuration change.We are improving our monitoring and deployment practices to improve our time to detection and automated mitigation for issues like this in the future.January 30 14:22 UTC (lasting 26 minutes)On January 30, 2025, between 14:22 UTC and 14:48 UTC, web requests to github.com experienced failures (at peak the error rate was 44%), with the average successful request taking over three seconds to complete.This outage was caused by a hardware failure in the caching layer that supports rate limiting. In addition, the impact was prolonged due to a lack of automated failover for the caching layer. A manual failover of the primary to trusted hardware was performed following recovery to ensure that the issue would not reoccur under similar circumstances.As a result of this incident, we will be moving to a high availability cache configuration and adding resilience to cache failures at this layer to ensure requests are able to be handled should similar circumstances happen in the future.Please follow our status page for real-time updates on status changes and post-incident recaps. To learn more about what we’re working on, check out the GitHub Engineering Blog.]]></content:encoded></item><item><title>How GitHub uses CodeQL to secure GitHub</title><link>https://github.blog/engineering/how-github-uses-codeql-to-secure-github/</link><author>Brandon Stewart</author><category>official</category><category>dev</category><pubDate>Wed, 12 Feb 2025 17:00:04 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[GitHub’s Product Security Engineering team writes code and implements tools that help secure the code that powers GitHub. We use GitHub Advanced Security (GHAS) to discover, track, and remediate vulnerabilities and enforce secure coding standards at scale. One tool we rely heavily on to analyze our code at scale is CodeQL.CodeQL is GitHub’s static analysis engine that powers automated security analyses. You can use it to query code in much the same way you would query a database. It provides a much more robust way to analyze code and uncover problems than an old-fashioned text search through a codebase.The following post will detail how we use CodeQL to keep GitHub secure and how you can apply these lessons to your own organization. You will learn why and how we use:Custom query packs (and how we create and manage them).  Variant analysis to uncover potentially insecure programming practices.We employ CodeQL in a variety of ways at GitHub. with the default and security-extended query suites
Default setup with the default and security-extended query suites meets the needs of the vast majority of our over 10,000 repositories. With these settings, pull requests automatically get a security review from CodeQL.  
A few repositories, like our large Ruby monolith, need extra special attention, so we use advanced setup with a query pack containing custom queries to really tailor to our needs.  
To conduct variant analysis and quick auditing, we use MRVA. We also write custom CodeQL queries to detect code patterns that are either specific to GitHub’s codebases or patterns we want a security engineer to manually review.The specific custom Actions workflow step we use on our monolith is pretty simple. It looks like this:- name: Initialize CodeQL
    uses: github/codeql-action/init@v3
    with:
      languages: ${{ matrix.language }}
      config-file: ./.github/codeql/${{ matrix.language }}/codeql-config.yml
Our Ruby configuration is pretty standard, but advanced setup offers a variety of configuration options using custom configuration files. The interesting part is the  option, which is how we enable our custom query pack as part of the CodeQL analysis. This pack contains a collection of CodeQL queries we have written for Ruby, specifically for the GitHub codebase.So, let’s dive deeper into why we did that—and how!Publishing our CodeQL query packInitially, we published CodeQL query files directly to the GitHub monolith repository, but we moved away from this approach for several reasons:It required going through the production deployment process for each new or updated query.  Queries not included in a query pack were not pre-compiled, which slowed down CodeQL analysis in CI.  Our test suite for CodeQL queries ran as part of the monolith’s CI jobs. When a new version of the CodeQL CLI was released, it sometimes caused the query tests to fail because of changes in the query output, even when there were no changes to the code in the pull request. This often led to confusion and frustration among engineers, as the failure wasn’t related to their pull request changes.By switching to publishing a query pack to GitHub Container Registry (GCR), we’ve simplified our process and eliminated many of these pain points, making it easier to ship and maintain our CodeQL queries. So while it’s  to deploy custom CodeQL query files directly to a repository, we recommend publishing CodeQL queries as a query pack to the GCR for easier deployment and faster iteration.When setting up our custom query pack, we faced several considerations, particularly around managing dependencies like the  package.To ensure our custom queries remain maintainable and concise, we extend classes from the default query suite, such as the  library. This allows us to leverage existing functionality rather than reinventing the wheel, keeping our queries concise and maintainable. However, changes to the CodeQL library API can introduce breaking changes, potentially deprecating our queries or causing errors. Since CodeQL runs as part of our CI, we wanted to minimize the chance of this happening, as this can lead to frustration and loss of trust from developers.We develop our queries against the latest version of the  package, ensuring we’re always working with the most up-to-date functionality. To mitigate the risk of breaking changes affecting CI, we pin the  version when we’re ready to release, locking it in the  file. This guarantees that when our queries are deployed, they will run with the specific version of  we’ve tested, avoiding potential issues from unintentional updates.Here’s how we manage this setup:In our qlpack.yml, we set the dependency to use the latest version of During development, this configuration pulls in the latest version) of  when running , ensuring we’re always up to date.
// Our custom query pack's qlpack.yml

library: false
name: github/internal-ruby-codeql
version: 0.2.3
extractor: 'ruby'
dependencies:
  codeql/ruby-all: "*"
tests: 'test'
description: "Ruby CodeQL queries used internally at GitHub"
Before releasing, we lock the version in the  file, specifying the exact version to ensure stability and prevent issues in CI.
// Our custom query pack's codeql-pack.lock.yml

lockVersion: 1.0.0
dependencies:
 ...
 codeql/ruby-all:
   version: 1.0.6
This approach allows us to balance developing against the latest features of the  package while ensuring stability when we release.We also have a set of CodeQL unit tests that exercise our queries against sample code snippets, which helps us quickly determine if any query will cause errors before we publish our pack. These tests are run as part of the CI process in our query pack repository, providing an early check for issues. We strongly recommend writing unit tests for your custom CodeQL queries to ensure stability and reliability.Altogether, the basic flow for releasing new CodeQL queries via our pack is as follows:Open a pull request with the new query.  Write unit tests for the new query.  Increment the pack version in a new pull request.  Run  to resolve dependencies.  Correct unit tests as needed.  Publish the query pack to the GitHub Container Registry (GCR).  Repositories with the query pack in their config will start using the updated queries.We have found this flow balances our team’s development experience while ensuring stability in our published query pack.Configuring our repository to use our custom query packWe won’t provide a general recommendation on configuration here, given that it ultimately depends on how your organization deploys code. We opted against locking our pack to a particular version in our CodeQL configuration file (see above). Instead, we chose to manage our versioning by publishing the CodeQL package in GCR. This results in the GitHub monolith retrieving the latest published version of the query pack. To roll back changes, we simply have to republish the package. In one instance, we released a query that had a high number of false positives and we were able to publish a new version of the pack that removed that query in less than 15 minutes. This is faster than the time it would have taken us to merge a pull request on the monolith repository to roll back the version in the CodeQL configuration file.One of the problems we encountered with publishing the query pack in GCR was how to easily make the package available to multiple repositories within our enterprise. There are several approaches we explored.Grant access permissions for individual repositories. On the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API.   Mint a personal access token for the CodeQL action runner. We could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read  of our private packages rather than ones we explicitly allow it to have access to.  CodeQL query pack queriesWe write a variety of custom queries to be used in our custom query packs. These cover GitHub-specific patterns that aren’t included in the default CodeQL query pack. This allows us to tailor the analysis to patterns and preferences that are specific to our company and codebase. Some of the types of things we alert on using our custom query pack include:High-risk APIs specific to GitHub’s code that can be dangerous if they receive unsanitized user input.  Use of specific built-in Rails methods for which we have safer, custom methods or functions.    Required authorization methods not being used in our REST API endpoint definitions and GraphQL object/mutation definitions.  REST API endpoints and GraphQL mutations that require engineers to define access control methods to determine which actors can access them. (Specifically, the query detects the absence of this method definition to ensure that the actors’ permissions are being checked for these endpoints.)  Use of signed tokens so we can nudge engineers to include Product Security as a reviewer when using them.Custom queries can be used more for educational purposes rather than being blockers to shipping code. For example, we want to alert engineers when they use the ActiveRecord::decrypt method. This method should generally not be used in production code, as it will cause an encrypted column to become decrypted. We use the recommendation severity in the query metadata so these alerts are treated as more of an informational alert. That means this may trigger an alert in a pull request, but it won’t cause the CodeQL CI job to fail. We use this lower severity level to allow engineers to assess the impact of new queries without immediate blocking. Additionally, this alert level isn’t tracked through our Fundamentals program, meaning it doesn’t require immediate action, reflecting the query’s maturity as we continue to refine its relevance and risk assessment./**
 * @id rb/github/use-of-activerecord-decrypt
 * @description Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save
 * them unencrypted, effectively undoing encryption and possibly making the attributes inaccessible.
 * If you need to access the unencrypted value of any attribute, you can do so by calling my_model.attribute_name.
 * @kind problem
 * @severity recommendation
 * @name Use of ActiveRecord decrypt method
 * @tags security
 *      github-internal
 */

import ruby
import DataFlow
import codeql.ruby.DataFlow
import codeql.ruby.frameworks.ActiveRecord

/** Match against .decrypt method calls where the receiver may be an ActiveRecord object */
class ActiveRecordDecryptMethodCall extends ActiveRecordInstanceMethodCall {
  ActiveRecordDecryptMethodCall() { this.getMethodName() = "decrypt" }
}

from ActiveRecordDecryptMethodCall call
select call,
  "Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save them unencrypted.
Another educational query is the one mentioned above in which we detect the absence of the `control_access` method in a class that defines a REST API endpoint. If a pull request introduces a new endpoint without `control_access`,  a comment will appear on the pull request saying that the `control_access` method wasn’t found and it’s a requirement for REST API endpoints. This will notify the reviewer of a potential issue and prompt the developer to fix it./**
 * @id rb/github/api-control-access
 * @name Rest API Without 'control_access'
 * @description All REST API endpoints must call the 'control_access' method, to ensure that only specified actor types are able to access the given endpoint.
 * @kind problem
 * @tags security
 * github-internal
 * @precision high
 * @problem.severity recommendation
 */

import codeql.ruby.AST
import codeql.ruby.DataFlow
import codeql.ruby.TaintTracking
import codeql.ruby.ApiGraphs

// Api::App REST API endpoints should generally call the control_access method
private DataFlow::ModuleNode appModule() {
  result = API::getTopLevelMember("Api").getMember("App").getADescendentModule() and
  not result = protectedApiModule() and
  not result = staffAppApiModule()
}

// Api::Admin, Api::Staff, Api::Internal, and Api::ThirdParty REST API endpoints do not need to call the control_access method
private DataFlow::ModuleNode protectedApiModule() {
  result =
    API::getTopLevelMember(["Api"])
        .getMember(["Admin", "Staff", "Internal", "ThirdParty"])
        .getADescendentModule()
}

// Api::Staff::App REST API endpoints do not need to call the control_access method
private DataFlow::ModuleNode staffAppApiModule() {
  result =
    API::getTopLevelMember(["Api"]).getMember("Staff").getMember("App").getADescendentModule()
}

private class ApiRouteWithoutControlAccess extends DataFlow::CallNode {
  ApiRouteWithoutControlAccess() {
    this = appModule().getAModuleLevelCall(["get", "post", "delete", "patch", "put"]) and
    not performsAccessControl(this.getBlock())
  }
}

predicate performsAccessControl(DataFlow::BlockNode blocknode) {
  accessControlCalled(blocknode.asExpr().getExpr())
}

predicate accessControlCalled(Block block) {
  // the method `control_access` is called somewhere inside `block`
  block.getAStmt().getAChild*().(MethodCall).getMethodName() = "control_access"
}

from ApiRouteWithoutControlAccess api
select api.getLocation(),
  "The control_access method was not detected in this REST API endpoint. All REST API endpoints must call this method to ensure that the endpoint is only accessible to the specified actor types."
Variant analysis (VA) refers to the process of searching for variants of security vulnerabilities. This is particularly useful when we’re responding to a bug bounty submission or a security incident. We use a combination of tools to do this, including GitHub’s code search functionality, custom scripts, and CodeQL.  We will often start by using code search to find patterns similar to the one that caused a particular vulnerability across numerous repositories. This is sometimes not good enough, as code search is not semantically aware, meaning that it cannot determine whether a given variable is an Active Record object or whether it is being used in an `if` expression. To answer those types of questions we turn to CodeQL.When we write CodeQL queries for variant analysis we are much less concerned about false positives, since the goal is to provide results for security engineers to analyze. The quality of the code is also not quite as important, as these queries will only be used for the duration of the VA effort. Some of the types of things we use CodeQL for during VAs are:Where are we using SHA1 hashes?  One of our internal API endpoints was vulnerable to SQLi according to a recent bug bounty report. Where are we passing user input to that API endpoint?   There is a problem with how some HTTP request libraries in Ruby handle the proxy setting. Can we look at places we are instantiating our HTTP request libraries with a proxy setting?One recent example involved a subtle vulnerability in Rails. We wanted to detect when the following condition was present in our code:A parameter was used to look up an Active Record object.  That parameter is later reused after the Active Record object is looked up.The concern with this condition is that it could lead to an insecure direct object reference (IDOR) vulnerability because Active Record finder methods can accept an array. If the code looks up an Active Record object in one call to determine if a given entity has access to a resource, but later uses a different element from that array to find an object reference, that can lead to an IDOR vulnerability. It would be difficult to write a query to detect  vulnerable instances of this pattern, but we were able to write a query that found potential vulnerabilities that gave us a list of code paths to manually analyze. We ran the query against a large number of our Ruby codebases using CodeQL’s MRVA.The query, which is a bit hacky and not quite production grade, is below:/**
 * @name wip array query
 * @description an array is passed to an AR finder object
 */

import ruby
import codeql.ruby.AST
import codeql.ruby.ApiGraphs
import codeql.ruby.frameworks.Rails
import codeql.ruby.frameworks.ActiveRecord
import codeql.ruby.frameworks.ActionController
import codeql.ruby.DataFlow
import codeql.ruby.Frameworks
import codeql.ruby.TaintTracking

// Gets the "final" receiver in a chain of method calls.
// For example, in `Foo.bar`, this would give the `Foo` access, and in
// `foo.bar.baz("arg")` it would give the `foo` variable access
private Expr getUltimateReceiver(MethodCall call) {
  exists(Expr recv |
    recv = call.getReceiver() and
    (
      result = getUltimateReceiver(recv)
      or
      not recv instanceof MethodCall and result = recv
    )
  )
}

// Names of class methods on ActiveRecord models that may return one or more
// instances of that model. This also includes the `initialize` method.
// See https://api.rubyonrails.org/classes/ActiveRecord/FinderMethods.html
private string staticFinderMethodName() {
  exists(string baseName |
    baseName = ["find_by", "find_or_create_by", "find_or_initialize_by", "where"] and
    result = baseName + ["", "!"]
  )
  // or
  // result = ["new", "create"]
}

private class ActiveRecordModelFinderCall extends ActiveRecordModelInstantiation, DataFlow::CallNode
{
  private ActiveRecordModelClass cls;

  ActiveRecordModelFinderCall() {
    exists(MethodCall call, Expr recv |
      call = this.asExpr().getExpr() and
      recv = getUltimateReceiver(call) and
      (
        // The receiver refers to an `ActiveRecordModelClass` by name
        recv.(ConstantReadAccess).getAQualifiedName() = cls.getAQualifiedName()
        or
        // The receiver is self, and the call is within a singleton method of
        // the `ActiveRecordModelClass`
        recv instanceof SelfVariableAccess and
        exists(SingletonMethod callScope |
          callScope = call.getCfgScope() and
          callScope = cls.getAMethod()
        )
      ) and
      (
        call.getMethodName() = staticFinderMethodName()
        or
        // dynamically generated finder methods
        call.getMethodName().indexOf("find_by_") = 0
      )
    )
  }

  final override ActiveRecordModelClass getClass() { result = cls }
}

class FinderCallArgument extends DataFlow::Node {
  private ActiveRecordModelFinderCall finderCallNode;

  FinderCallArgument() { this = finderCallNode.getArgument(_) }
}

class ParamsHashReference extends DataFlow::CallNode {
  private Rails::ParamsCall params;

  // TODO: only direct element references against `params` calls are considered
  ParamsHashReference() { this.getReceiver().asExpr().getExpr() = params }

  string getArgString() {
    result = this.getArgument(0).asExpr().getConstantValue().getStringlikeValue()
  }
}

class ArrayPassedToActiveRecordFinder extends TaintTracking::Configuration {
  ArrayPassedToActiveRecordFinder() { this = "ArrayPassedToActiveRecordFinder" }

  override predicate isSource(DataFlow::Node source) { source instanceof ParamsHashReference }

  override predicate isSink(DataFlow::Node sink) {
    sink instanceof FinderCallArgument
  }

  string getParamsArg(DataFlow::CallNode paramsCall) {
    result = paramsCall.getArgument(0).asExpr().getConstantValue().getStringlikeValue()
  }

  // this doesn't check for anything fancy like whether it's reuse in a if/else
  // only intended for quick manual audit filtering of interesting candidates
  // so remains fairly broad to not induce false negatives
  predicate paramsUsedAfterLookups(DataFlow::Node source) {
    exists(DataFlow::CallNode y | y instanceof ParamsHashReference
    and source.getEnclosingMethod() = y.getEnclosingMethod()
    and source != y
    and getParamsArg(source) = getParamsArg(y)
    // we only care if it's used again AFTER an object lookup
    and y.getLocation().getStartLine() > source.getLocation().getStartLine())
  }
}

from ArrayPassedToActiveRecordFinder config, DataFlow::Node source, DataFlow::Node sink
where config.hasFlow(source, sink) and config.paramsUsedAfterLookups(source)
select source, sink.getLocation()
CodeQL can be very useful for product security engineering teams to detect and prevent vulnerabilities at scale. We use a combination of queries that run in CI using our query pack and one-off queries run through MRVA to find potential vulnerabilities and communicate them to engineers. CodeQL isn’t only useful for finding security vulnerabilities, though; it is also useful for detecting the presence or absence of security controls that are defined in code. This saves our security team time by surfacing certain security problems automatically, and saves our engineers time by detecting them earlier in the development process.Writing custom CodeQL queriesWe have a large number of articles and resources for writing custom CodeQL queries. If you haven’t written custom CodeQL queries before, here are some resources to help get you started:Michael Recachinas, GitHub Staff Security Engineer, also contributed to this blog post.]]></content:encoded></item><item><title>Python 3.14.0 alpha 5 is out</title><link>https://pythoninsider.blogspot.com/2025/02/python-3140-alpha-5-is-out.html</link><author>Hugo</author><category>Python official news</category><category>dev</category><category>official</category><category>python</category><pubDate>Tue, 11 Feb 2025 19:41:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python Insider</source><content:encoded><![CDATA[Here comes the antepenultimate alpha.This is an early developer preview of Python
3.14Python 3.14 is still in development. This release, 3.14.0a5, is the
fifth of seven planned alpha releases.Alpha releases are intended to make it easier to test the current
state of new features and bug fixes and to test the release process.During the alpha phase, features may be added up until the start of
the beta phase (2025-05-06) and, if necessary, may be modified or
deleted up until the release candidate phase (2025-07-22). Please keep
in mind that this is a preview release and its use is
 recommended for production environments.Many new features for Python 3.14 are still being planned and
written. Among the new major new features and changes so far:The next pre-release of Python 3.14 will be the penultimate alpha,
3.14.0a6, currently scheduled for 2025-03-14.2025-01-29 marked the start of a new lunar year, the Year of the
Snake 🐍 (and the Year of Python?).For centuries, π was often approximated as 3 in China. Some time
between the years 1 and 5 CE, astronomer, librarian, mathematician and
politician Liu Xin (劉歆) calculated π as 3.154.Around 130 CE, mathematician, astronomer, and geographer Zhang Heng
(張衡, 78–139) compared the celestial circle with the diameter of the
earth as 736:232 to get 3.1724. He also came up with a formula for the
ratio between a cube and inscribed sphere as 8:5, implying the ratio of
a square’s area to an inscribed circle is √8:√5. From this, he
calculated π as √10 (~3.162).Third century mathematician Liu Hui (刘徽) came up with an algorithm
for calculating π iteratively: calculate the area of a polygon inscribed
in a circle, then as the number of sides of the polygon is increased,
the area becomes closer to that of the circle, from which you can
approximate π.This algorithm is similar to the method used by Archimedes in the 3rd
century BCE and Ludolph van Ceulen in the 16th century CE (see 3.14.0a2
  release notes), but Archimedes only went up to a 96-sided polygon
(96-gon). Liu Hui went up to a 192-gon to approximate π as 157/50 (3.14)
and later a 3072-gon for 3.14159.Liu Hu wrote a commentary on the book The Nine Chapters on the
Mathematical Art which included his π approximations.In the fifth century, astronomer, inventor, mathematician,
politician, and writer Zu Chongzhi (祖沖之, 429–500) used Liu Hui’s
algorithm to inscribe a 12,288-gon to compute π between 3.1415926 and
3.1415927, correct to seven decimal places. This was more accurate than
Hellenistic calculations and wouldn’t be improved upon for 900
years.Thanks to all of the many volunteers who help make Python Development
and these releases possible! Please consider supporting our efforts by
volunteering yourself or through organisation contributions to the Python Software
Foundation.Regards from a remarkably snowless Helsinki,Your release team, Hugo van KemenadeSteve Dower]]></content:encoded></item><item><title>How to refactor code with GitHub Copilot</title><link>https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/</link><author>Anthony Grutta</author><category>official</category><category>dev</category><pubDate>Tue, 11 Feb 2025 17:00:05 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[We’ve all been there—staring at a function that looks like it was written by an over-caffeinated goblin at 3 AM (maybe even your alter ego). You  pretend it doesn’t exist, or you could refactor it. Luckily, GitHub Copilot makes the second option less painful.What is code refactoring?Feel free to breeze past this section if you already know what’s involved with refactoring code, but we wanted to take a moment to cover what we’ll be looking at today.Think of refactoring as giving your project some digital spring cleaning—a glow up for your functions, classes, and modules. But instead of just tidying up, you’re making your code more efficient, maintainable, and readable, all without changing its external behavior.Some standard ways of refactoring include:Simplifying complex conditionals (because no one should need a PhD to read your  statements)  Extracting duplicated logic (so you’re not trying to maintain code in ten different places)  Improving variable and function names (because  is a crime against humanity)  Converting monolithic functions into smaller, modular pieces (to prevent the dreaded “function that spans multiple screens” scenario)Refactoring isn’t just about tidiness—it’s about making your codebase more resilient, scalable, and enjoyable to work with. Let’s find out how GitHub Copilot can help you do it faster and with fewer headaches.Know what your code does before you refactor anythingIt might seem obvious to say this, but before you can refactor any code you need to understand how it works. If you don’t know what your code is doing, you won’t know whether or not the “improvements” you’re making are changing the core functionality of the code.Consider the following method:public String getSound(String animal) {
  if (animal == null) {
      System.out.println("Oops! A null animal?");
  } else if (animal.equalsIgnoreCase("Dog")) {
      return "Bark";
  } else if ( animal.equalsIgnoreCase("Cat")) {
      return "Meow";
  } else if ( animal.equalsIgnoreCase("Bird")) {
      return "Tweet";
  }
  return "Unknown";
}
You might look at this and immediately think “they should use a switch statement,” and that would be one example of refactoring the code. But having that knee jerk reaction requires you to know how if-statements and switch-statements work. You can only make that suggestion if you understand this code will continue cycling through if-statements until it finds an appropriate match. Otherwise, it will return the value of .As codebases get more complex and make calls between multiple files, this gets (much) more complicated. And this is one way Copilot can help you.You can ask Copilot Chat to explain how some code works, either by asking in plain language or using the /explain slash command. To limit the scope of what Copilot looks at, select the code in your IDE before asking your query, or specify specific files for it to consider by using . While you’re at it, you can even ask it to add code comments to help you (or anyone else reading the code) in the future.Here are some sample prompts:Explain what this code does.Add comments to this code to make it more understandable.You should use Copilot Chat to analyze and explain your codebase until you fully understand the code you’re looking to refactor.Try some blanket improvements to refactor your codeLike most things in life, it’s usually best to start small. When you’re first getting started with refactoring code, keep it simple: open up Copilot Chat in your project and ask “how would you improve this?” Just like when you are asking GitHub Copilot to explain some code, you can specify what it looks at by highlighting sections of code or identifying specific files by using .Here are some sample prompts:How would you improve this?Improve the variable names in this function.#file:pageInit.js, #file:socketConnector.js Offer suggestions to simplify this code.Copilot will then offer suggestions to improve the code in the way that you specified. This is great for getting started, but Copilot can do much more if you give it some guidance.When working with any generative AI-powered developer tool, it is often useful to include more context in your prompts—ones that are more likely to get you the results you’re looking for. By being specific about what you want the tool to do, it focuses the efforts toward that end.It’s a bit like if someone tells you to code something, and you have so many ideas and questions that you’re not quite sure where to begin. But if someone tells you they specifically want you to code a class that sorts a list, you can focus on the task at hand.The prompts above don’t offer much specificity or context, which leaves Copilot to explore all the possible ways your code could be improved. The upside? You may see options that you might not have considered. The downside is some of the proposed solutions might not address your specific concerns.Make a plan for refactoring your codebaseWhat do you want to do to your code? Do you want to make it more readable? Or do you want to find redundant code and remove it? Coming up with a plan for the improvements you want to make will help you to hit your goals. This comes back to making sure you understand your code. If you know how it works, you can come up with a plan for the type of improvements that you want to make.Maybe your code base has a bunch of different scripts that all perform the same general function. You could strip out the code into a common module to import into each of the different scripts, making the code easier to change and maintain.To do so, you can direct Copilot to look for these common code sections and to pull them into a single module.Inspect all my js files for GitHub API calls and create a new class that will manage all the GitHub API calls.
Now that we have provided some guidance and additional context, Copilot will provide suggestions targeting this specific improvement that we want to make to our code.You can also provide a laundry list of tasks, or ask Copilot to keep things in mind while it is doing the refactoring.In that vein, here’s a sample prompt to consider:Can you refactor the GitHubController class to:
- remove nested logic structures
- make the code more concise
- while doing this, check if the code is safe and add comments if not
The takeaway here is Copilot is very good at taking directions—and the more specific your directions are, the more it will generate outputs in line with your intended end goal.A real-world example: Using GitHub Copilot to refactor codeSometimes the best way to understand something is to see it in use. To help with this, we wanted to share an experience where we needed to use GitHub Copilot to refactor our own code, interspersed with some general guidance and pointers.It started with a couple of organizations independently requesting help with their individual large migration projects, which involved moving from other data centers into tens of thousands of repositories on GitHub. After building a proof of concept, we started creating individual systems, each tailored to the needs of the individual customer and their unique systems. While we were doing this, a third organization made a similar request for help, and we realized this was a common need.At this point in time, we had two systems that were both suited for large-scale migrations, but each of them was heavily customized for the original company making the request. We quickly realized the best solution involved refactoring our code to improve its modularity so we could use it across different organizations. This would not be easy—the codebase was made up of several different scripts and workflows with a lot of moving parts. And that’s when we turned to Copilot for help.First we came up with a plan. We created a set of high-level goals and used these to guide us during the refactoring process. We wanted to make sure the code was:Modular and reusable for different organizations: The code should be organized into small, self-contained modules that could be reused and combined for new functionality.   The code should be easy to maintain and extend as needed. That meant making it well-organized, well-documented, and easy to understand.  Customizable for different organizations: The code should be flexible enough to meet the unique needs of each organization.  Following coding standards and best practices: The code should be consistent, readable, and easy to understand.With this plan in place, it was time to ask Copilot for help.Step 1: Creating the initial moduleOur first step was creating a common module we could import into each of the different scripts. This would allow us to reuse the same code across many different scripts, making it easier to maintain and extend. We created a skeleton of a module called . Then we used Copilot to fill in the details.Here was the prompt we gave Copilot:I am creating a module, gh-migrations, which will hold common functionality to be used through out my project. I have already created an index.js for the module. Please inspect the other js files for GitHub API calls, create a new class for the module which will manage all GitHub API calls. This class should also account for rate limiting and errors. Do not make edits to the javascript files, rather identify the API calls, and add the functionality to the new GitHub API class.
And here was Copilot’s response:I'll help create a GitHub API management class based on the API calls found in the provided files. Here's a step-by-step solution:

1. From analyzing the files, I identified these common GitHub API operations: 
  * Issues creation and updates
  * Issue search
  * Rate limit handling (403 errors)
  * Common error handling patterns

2. I'll create a class that encapsulates these operations with proper error handling and rate limiting.

.github/lib/gh-migrations/index.js
Here's the GitHub API management class implementation:

<code>

This implementation:

1. Creates a GitHubApiManager class that handles all GitHub API interactions
2. Implements automatic rate limiting retry logic in withRateLimit
3. Provides methods for common operations: 
  * createIssue: Creates a new issue
  * updateIssue: Updates an existing issue
  * searchIssues: Searches for issues using GitHub's search API
  * findIssueByTitle: Helper method to find issues by title
  * getIssue: Gets a specific issue by number

Usage example:

<code>

The class handles:
  * Automatic retry on rate limits
  * Consistent error handling
  * Repository context management
  * Common GitHub API operations used across the codebase

This centralizes all GitHub API interactions and makes the code more maintainable and consistent.
After reviewing the code (as you should always do!), we made the recommended changes. The next step was putting these changes into a separate file while ensuring the  file could reference it.Here was the prompt we entered for Copilot:Rename the class GitHubApiManager to GHApi and put it in a separate file that is referenced in the index.js file.
And here was Copilot’s response:I'll help you split the code into two files. Here's the step by step solution:

1. Create a new file gh-api.js for the GHApi class
2. Update index.js to export the renamed class from the new file

.github/lib/gh-migrations/gh-api.js
Create a new file containing the renamed class:

<code>

.github/lib/gh-migrations/index.js
Update the index file to export the renamed class from the new file:

<code>

The changes:
1. Created a new file gh-api.js with the renamed class
2. Moved all the class implementation to the new file
3. Updated the class name from GitHubApiManager to GHApi
4. Modified index.js to require and export the class from the new file

Now consumers of this module can continue to import it the same way, but the implementation is better organized.
After making these changes, we then had to repeat the process for each of the files that contained GitHub API calls. Be aware, we didn’t want to refactor the code directly at this stage. Instead, we wanted to create a common module we could use for refactoring code later.Here was the prompt we entered for Copilot:The gh-api.js file is a new class that will be used to manage all GitHub API calls. Please review the other javascript files for GitHub API calls to be added to gh-api.js. Do not edit the javascript files, only add new functionality to gh-api.js.
And here was Copilot’s response:I'll analyze the JavaScript files and add the missing API functionality to the GHApi class.

.github/lib/gh-migrations/gh-api.js
Add methods for issue operations, searching, and deletion based on the API calls found in migration.js and application.js:

<code>

This implementation adds all the GitHub API functionality found in the other files:

1. From migration.js:
  * createComment for updating issues with comments
  * getIssue and isMigrationIssue for checking migration status
2. From application.js:
  * searchIssues and searchIssuesByTitle for finding issues
  * getIssueNodeId and deleteIssue for issue deletion via GraphQL
  * findJiraIssue helper method to get Jira ticket information

The methods maintain the same rate limiting and error handling patterns while providing a cleaner interface for the API operations.
Step 2: Adding new classesOnce we had the new  class to handle the GitHub API, we want to create some reusable classes for different entities in our application. Since our application is a migration tool managed via GitHub Issues, we wanted to create classes for the following entities: represent repository migrations from various sources. They contain configuration and metadata about the repository and how the migration should be performed.   represent a batch of migrations. They contain more high-level metadata about groups of individual migration issues such as custom application ids and status of the batch.   represent the migration of a group of users, their associated repositories, and the permissions to those repositories.   represent Azure DevOps pipelines rewire operations. They allow for the reconfiguration of Azure DevOps pipelines to point to the new GitHub repositories after a migration has been completed.Here was the prompt we entered for Copilot:Review the create-migration-issue.js and develop a new class called MigrationIssue that contains the properties associated with a migration. This shoud be added to the gh-migrations module where index.js is located.
And here was Copilot’s response:I'll help create a new MigrationIssue class based on the migration configuration and status shown in the code. Here's the step-by-step solution:

1. Create a new file migration-issue.js in the gh-migrations directory
2. Update the index.js file to expose the new class
3. Extract configuration and status properties from the create-migration-issues.js file into the new class

migration-issue.js
Create a new class to represent a migration issue with all necessary properties and methods.

<code>

index.js
Update the index.js to expose the new MigrationIssue class.

<code>

The new MigrationIssue class encapsulates all the properties that were previously spread across various objects in the create-migration-issues.js file. It provides a clean interface for creating and managing migration issues, with helper methods for generating issue titles and source types.

You can now use this class in create-migration-issues.js by replacing the manual object creation with the MigrationIssue class. For example, the createMigrationConfig function could be replaced with:

<code>
The process for creating new classes for the other entities was similar to the above. We created new classes for , , and , each with their own properties and methods.After we built out our entity classes, we realized they should manage their own state. This is a common pattern in object-oriented programming, where the class is responsible for managing its state and behavior. Remember our goal about following good coding standards?The class should include methods to get and set the state of the object, as well as methods to perform actions on the object. It should also be responsible for utilizing the GitHub API and manage the state of the object rather than the client code. This will help standardize and protect the state of the object, as well as give the client code a simpler interface to work with.Here was the prompt we entered for Copilot:The MigrationIssue class should contain a initializer that uses the gh-api.js to create the resulting GitHub Issue. The initializer should take in an optional parameter called issueNumber, if the issueNumber is omitted the initializer should create a new Migration Issue and return the issue number created, if the issueNumber is passed to the initializer, the gh-api.js should be used to return the title and body of the issue which the class will parse to create the properties of the MigrationIssue.
And here was Copilot’s response:I'll help you modify the code to add initialization logic to the MigrationIssue class. Here's the step-by-step solution:

1. Add a static method to parse issue body into configuration
2. Add initialization methods to handle both new and existing issues
3. Modify constructor to accept GHApi instance
4. Add method to generate issue body
5. Add method to create or load issue

migration-issue.js
Update the MigrationIssue class to handle initialization from GHApi.

<code>

The main changes include:

1. Constructor now accepts a GHApi instance as first parameter
2. Added parseIssueBody static method to parse configuration from issue body
3. Added generateIssueBody method to create the issue body format
4. Added initialize async method that either creates a new issue or loads an existing one
5. The initialize method returns the issue number in both cases

To use this updated class:

<code>
Remember how we said it’s always important to review suggestions from Copilot? In this case, the  method wasn’t connected to the constructor, like we would expect. We asked Copilot to address this issue with an iterative prompt.Here was the prompt we entered for Copilot:The MigrationIssue class has an initialize function that encapsulates functionality that should be included in the constructor. Rewire the constuctor method to include the initialzie function.
And here was Copilot’s response:Let's modify the class to move the initialization logic into the constructor. Here's the step-by-step solution:

1. Make constructors async
2. Move initialization logic into constructor
3. Remove separate initialize method
4. Keep static parseIssueBody method

migration-issue.js
Move initialization logic into constructor.

<code>

The main changes:

1. Removed initialize method from the class
2. Added static create and load factory methods
3. Added save method to handle issue creation
4. Constructor remains synchronous but now only handles property initialization

Usage example:

<code>

This approach follows better object-oriented principles by:

1. Using factory methods for different creation scenarios
2. Keeping the constructor simple and synchronous
3. Separating issue creation into a distinct operation
4. Making the API more intuitive
In this case, Copilot suggested a different approach than we were thinking of with a factory method (which is a great way to handle different scenarios when creating or loading an issue). Even if it wasn’t what we were thinking, it worked and also kept the constructor clean and focused on just initializing the object. Moreover, the  method is a good way to encapsulate the logic for creating a new issue, and it makes the code more readable.Now we just needed to repeat this process with each of the new classes we added.Step 4: Final refactoringAfter we built a common module, we needed to refactor the client code to make it much cleaner and easier to read and maintain. After that, we’d also be able to identify and add any missing functionality in the common module.This was an iterative process, and we needed to go back and forth between the client code and the common module to make sure everything worked properly. Because of the way this project evolved, it afforded us the opportunity to make sweeping changes to the codebase. It’s much more common to make small, incremental changes to the codebase, especially if it’s code that’s actively in use. In both of these cases, don’t forget to write tests to ensure the code works as expected.Here was the prompt we entered for Copilot:Using the gh-migrations module, refactor the create-migration-issues.js
And here was Copilot’s response:I'll help you refactor the create-migration-issues.js file to use the gh-migrations module. Here's the step-by-step solution:

1. Import the gh-migrations module
2. Create GHApi instance
3. Use MigrationIssue class for creating and managing migration issues
4. Simplify the main logic using the new classes

create-migration-issues.js
Refactor to use the gh-migrations module for better code organization and reusability.

<code>

The refactoring:

1. Removes redundant code by using the MigrationIssue class
2. Simplifies issue creation and management
3. Uses GHApi for GitHub API calls with built-in rate limiting
4. Removes duplicate code for different source types
5. Improves code organization and maintainability

The main changes:
  * Removed several helper functions as they're now part of the MigrationIssue class
  * Simplified the main logic flow
  * Added better error handling
  * Used structured configuration object
  * Maintained the same functionality while reducing code complexity
Now we needed to refactor the remaining scripts in the same manner. For example, this is how we used Copilot to help with refactoring the  script.Here was the prompt we entered for Copilot:Using the gh-migrations module refactor the create-team-issues.js
And here was Copilot’s response:I'll help you refactor the create-team-issues.js file to use the gh-migrations module. Here's the step-by-step solution:

1. Import the gh-migrations module
2. Replace manual issue creation with TeamIssue class
3. Simplify team creation logic using the new class
4. Maintain the same JiraIssue tracking functionality

create-team-issues.js
Refactor to use the gh-migrations module for creating team issues.

<code>

The refactoring:

1. Uses the TeamIssue class to manage team issue creation
2. Simplifies the code by removing redundant config and status objects
3. Uses GHApi for GitHub operations with built-in rate limiting
4. Maintains the same JiraIssue tracking functionality
5. Reduces code duplication by using a loop for team creation

The code is now more maintainable and follows the same pattern as the migration issues implementation.
From there, it was a matter of rinsing and repeating this process with each of our scripts.By first creating the common module, we could guide Copilot  and make sure that the final, refactored codebase was something we could use with multiple organizations. It gave us a foundation we could reuse and customize based on the needs of the system we were migrating.Sometimes refactoring code can be really easy, like our first example with the sequential  statements. But the more you dig into it, the more it grows, and you might find yourself facing down the Herculean task of trying to refactor a codebase that stretches across multiple files from a larger team of developers.Speaking from experience: It can be daunting to even know where to start.Remember to take a step back and start with the basics. Your first step should always be improving your  understanding of the codebase you want to refactor. The more you know about what you’re starting out with, the greater chance that you will be able to think of ways to refactor it effectively. And once you come up with some ideas, you can always ask GitHub Copilot to help turn your vision into reality. It’s got your back and is more than willing to help.Start using GitHub Copilot for free
Our free version of GitHub Copilot is included by default in personal GitHub accounts and VS Code to help you start new projects, manage existing repositories, and more.]]></content:encoded></item><item><title>Go 1.24 is released!</title><link>https://go.dev/blog/go1.24</link><author>Junyang Shao, on behalf of the Go team</author><category>dev</category><category>official</category><category>go</category><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Dev - Golang Blog</source><content:encoded><![CDATA[
      Junyang Shao, on behalf of the Go team
      11 February 2025
      Today the Go team is excited to release Go 1.24,
which you can get by visiting the download page.Go 1.24 comes with many improvements over Go 1.23. Here are some of the notable
changes; for the full list, refer to the release notes.Several performance improvements in the runtime have decreased CPU overhead
by 2–3% on average across a suite of representative benchmarks. These
improvements include a new builtin  implementation based on
Swiss Tables, more efficient
memory allocation of small objects, and a new runtime-internal mutex
implementation.The  command now provides a mechanism for tracking tool dependencies for a
module. Use  to add a  directive to the current module. Use
 to run the tools declared with the  directive.
Read more on the go command in the release notes.The new  analyzer in  subcommand reports common mistakes in
declarations of tests, fuzzers, benchmarks, and examples in test packages.
Read more on vet in the release notes.Standard library additionsImproved WebAssembly supportGo 1.24 adds a new  directive for Go programs to export
functions to the WebAssembly host, and supports building a Go program as a WASI
reactor/library.
Read more on WebAssembly in the release notes.Please read the Go 1.24 release notes for the complete and
detailed information. Don’t forget to watch for follow-up blog posts that
will go in more depth on some of the topics mentioned here!Thank you to everyone who contributed to this release by writing code and
documentation, reporting bugs, sharing feedback, and testing the release
candidates. Your efforts helped to ensure that Go 1.24 is as stable as possible.
As always, if you notice any problems, please file an issue.]]></content:encoded></item><item><title>Micro Frontends with Angular and Native Federation</title><link>https://blog.angular.dev/micro-frontends-with-angular-and-native-federation-7623cfc5f413?source=rss----447683c3d9a3---4</link><author>Manfred Steyer</author><category>Angular official</category><category>dev</category><category>official</category><category>frontend</category><pubDate>Mon, 10 Feb 2025 15:02:31 +0000</pubDate><source url="https://blog.angular.dev/?source=rss----447683c3d9a3---4">Angular Blog - Medium</source><content:encoded><![CDATA[Enterprise-scale software systems are often implemented by several cross-functional teams. To enable such teams to provide new features efficiently, it is desirable to minimize the need for coordination between them. This calls for a modularization that verticalizes the system into low-coupled areas individual teams can take care of.There are several possibilities for implementing such high-level modules (also called verticals). For instance, they can be implemented using a respective folder structure or in the form of several libraries in a Monorepo. Micro Frontends go one step further and designate a separate application for each vertical. This architectural style promises several advantages, such as a high amount of team autonomy, but it also comes with numerous challenges.The first part of this article provides a critical overview of the benefits and drawbacks of Micro Frontends in the space of single-page applications. The second part discusses how such an architecture can be implemented with Native Federation, a community project built upon web standards that provide close integration with the Angular CLI.Motivation Behind Micro FrontendsLike Micro Services, Micro Frontends promise several advantages, both technically and with respect to the organization. As applying Micro Frontend architectures results in several smaller applications, testing, performance tuning, and isolating faults in one part of the overarching system becomes more effortless, according to several sources.However, the increased team autonomy was the main reason for applying this architectural style in the numerous cases I was involved in as a consultant. Individual teams are not blocked by waiting for other teams and can deploy separately anytime. This might not be a significant concern in a vast number of projects. Still, as soon as we talk about multi-team projects in a corporate environment with long communication paths and decision times, this aspect quickly becomes vital for the project’s success.Teams can also make their own decisions that best fit their goals — architecturally and technology-wise. Mixing multiple client-side frameworks in the same application is considered an anti-pattern and should be avoided. However, it can help to create a migration path over to a new stack in the long run. The concern in corporate environments is that we find software solutions that usually outlive the average technology stackSince Micro Frontends result in separate build processes, combining them with incremental builds, where only changed applications need to be rebuilt, has a massive potential for build-time improvements. For instance, the well-known Nx build system provides this option. Interestingly, this feature can also be used without applying other aspects such as aligning teams with individual applications or separate deployments. There is debate about whether leveraging this tempting option automatically leads to micro frontend architectures.A system consisting of several smaller applications can provide further organizational advantages: It’s easier to onboard new members and scale the development by adding further micro frontends. Team autonomy also leads to faster release cycles.Challenges to keep in mindEvery architectural decision has consequences that need to be evaluated, and Micro Frontends are no exception. Besides the positive consequences outlined above, there are also several negative ones to consider.For instance, individually developed Micro Frontends can diverge in UI/UX, leading to an inconsistent appearance. Also, loading several applications increases the number of bundles that need to be downloaded, adversely affecting loading times and increasing the memory pressure.Splitting an application into low-coupled parts might be a best practice in general. However, it is often hard to define the boundaries between verticals clearly enough to implement them as individual applications. Also, while having several small applications at first glance simplifies the implementation, integrating them into an overarching solution brings additional complexity.This leads to one of the biggest challenges I have seen in practice: we are moving away from a compile-time integration towards a runtime integration. This has severe consequences because we cannot easily foresee problems that might arise when individually developed and deployed applications start interacting at runtime. Besides the chance of technical conflicts, we also have to see that the current generation of SPA frameworks has not been built with such an operation mode in mind.Instead, modern SPA frameworks, especially Angular, have been developed to focus on compile time optimizations. A powerful compiler leverages type checks to identify technical conflicts and emits efficient source code optimized for tree-shaking. Furthermore, the CLI in the Angular space provides a highly optimized build process. An off-label usage necessary for implementing Micro Frontends undermines some of these achievements.Angular Does Not Officially Support Micro FrontendsFor all the outlined reasons, the Angular team recommends checking if alternatives, like implementing the individual verticals in Monorepos, which can be compiled together, are suitable. For instance, Google adopted this approach years ago and manages all its products and libraries in a single Monorepo.Of course, there are also ways to compensate for the disadvantages outlined here, and some of them, like establishing a design system to help with a consistent UI/UX or lazy loading individual system parts, might be needed in general. More details on such compensation strategies can be found in this survey of more than 150 Micro Frontend practitioners.All architectural decisions have benefits and drawbacks and should be evaluated with those considerations if you are going to implement a solution. If such an evaluation reveals that Micro Frontends provide more advantages than alternatives for achieving your very goals, the following sections provide you a well-lit path for implementing this architectural pattern with Angular.Micro Frontends with FederationModule Federation is a popular technology for implementing Micro Frontends and sharing dependencies. Shipped initially with webpack 5, it comes with a tooling-agnostic runtime and provides compile-time integration into  and . Besides the usage of the vite dev server, these technologies are currently not supported by the Angular CLI. However, promising community solutions like and AnalogJS allow them to be used with Angular. Nx and my CLI-plugin provide an effortless integration.Module Federation enables an application to load parts of other separately built and deployed applications lazily. The loading application is referred to as the host; the integrated ones are called remotes:Federation, if permitted by the library version, can share dependencies like Angular or RxJS between the host and remotes. There are several configuration options for preventing version mismatches. Since MF can only decide which dependencies to share at runtime, tree-shaking for shared parts is not possible.To inform the host about the remotes and their shared dependencies, Module Federation creates a metadata file, the so-called remote entry, during the build. This file needs to be loaded into the host.To fully decouple the idea of Federation from specific bundlers, I started the project Native Federation several years ago. Its API surface is very similar to that of Module Federation. The focus is on portability and standards like ECMAScript modules and Import Maps. Its compile time acts as a wrapper around existing bundlers. For the communication with the bundler, it uses an exchangeable adapter:The integration into the Angular CLI directly delegates to Angular’s  that leverages the fast bundler esbuild, and is the foundation for several current features like partial hydration. Because of its architecture, Native Federation can also be ported to further builders or other innovations the CLI might provide in the long run.For integrating Micro Frontends built with Angular’s webpack-based builder, there is a bridging solution allowing the loading of such remotes into a Native Federation host. This solution enables the gradual adoption of the CLI’s new  and permits the sharing of dependencies between the two kinds of Federation. One of the features added recently is support for SSR and Hydration, which is vital for performance-critical applications like public portals and web shops.Native Federation for Angular is close to the CLI’s , but its compilation mode is for shared dependencies differs. While it works well for packages that align with Angular’s Package Format, which is the case for all libraries built with the CLI, other libraries might provide some challenges, especially older ones that still use CommonJS or older conventions for providing metadata.Using Native Federation in AngularFor the setup Native Federation provides a schematic:ng add @angular-architects/native-federation --project mfe1 --port 4201 --type remoteThe switch  defines the kind of the application. Possible options are , , and . The latter is a host configured with a configuration file (federation manifest) during application start. This manifest informs the application of the locations of the remotes and can be switched out by another manifest during deployment:{    "mfe1" : "http://localhost:4201/remoteEntry.json"The key, in this case, is a short name the host uses to refer to the Micro Frontend. The value is the location of the remote entry with the metadata mentioned above. Alternatively, the manifest can be replaced by a service that informs the host of the current location of all deployed remotes and acts as a Micro Frontend registry.The schematic configures the Native Federation builder delegation to the  and creates a configuration file :const { withNativeFederation, shareAll }     = require('@angular-architects/native-federation/config');module.exports = withNativeFederation({    './Component': './projects/mfe1/src/app/app.component.ts',    ...shareAll({}),    'rxjs/ajax',    'rxjs/testing',    // Add further packages you don't need at runtimeThe configuration assigns a unique name to the remote or host and defines which dependencies to share. Instead of providing an exhaustive list of all the dependencies to share, the configuration uses the helper function , which adds all dependencies found in the project’s . The skip list is used to opt out of sharing some of them or their secondary entry points.Remotes also define exposed EcmaScript modules that can be loaded into the shell. For this, the  node maps the paths of the modules to short names such as  in the example shown.The schematic also adds code to initialize Native Federation to the . For the host, this code points to the federation manifest:import { initFederation } from '@angular-architects/native-federation';initFederation('federation.manifest.json')  .catch(err => console.error(err))  .then(_ => import('./bootstrap'))  .catch(err => console.error(err));After initializing federation, the file , also created by the schematic, is loaded. It contains the usual code for bootstrapping Angular, e.g., via  when the application uses Standalone Components.To load a component or routing configuration exposed by a remote, traditional lazy loading is combined with Native Federation’s  function:import { loadRemoteModule } from '@angular-architects/native-federation';export const APP_ROUTES: Routes = [  [...]    path: 'flights',      loadRemoteModule('mfe1', './Component').then((m) => m.AppComponent),];Here,  is the key defined in the manifest, and  points to the respective exposed module in the remote’s federation configuration.More information on Native Federation can be found in this blog article and in the project’s readme, which also links to a tutorial.Micro Frontends promise significant advantages for enterprise-scale applications, such as enhanced team autonomy and independent deployment. These benefits make this architectural style particularly appealing in multi-team corporate environments where streamlined communication and rapid development cycles are critical. Additionally, they support gradual migration to new technologies and optimize build times by leveraging incremental builds.However, these advantages come with trade-offs. Micro Frontends can lead to inconsistent UI/UX, increased load times, and complex runtime integrations. Defining clear vertical boundaries and managing inter-application communication add to the challenge. Furthermore, frameworks like Angular, designed for compile-time optimization, face limitations in runtime integration scenarios. The Angular team, therefore, recommends alternatives such as splitting an application into libraries managed within a Monorepo, which aligns better with Angular’s strengths in type safety and efficient compilation.Module Federation has emerged as a popular solution to address some challenges by enabling lazy loading and dependency sharing. Native Federation builds on these concepts with a focus on standards and portability. It provides a seamless integration into the Angular CLI and its performant esbuild-based , which is also the foundation for advanced features like SSR and hydration.Together with this team at , Manfred Steyer helps companies around the globe establish maintainable Angular-based Architectures. He is a trainer, consultant, and Google Developer Expert (GDE) and writes for O’Reilly, the German Java Magazin, windows.developer, and Heise Developer. Furthermore, he regularly speaks at conferences.]]></content:encoded></item><item><title>From finding to fixing: GitHub Advanced Security integrates Endor Labs SCA</title><link>https://github.blog/security/from-finding-to-fixing-github-advanced-security-integrates-endor-labs-sca/</link><author>Mario Rodriguez</author><category>official</category><category>dev</category><pubDate>Fri, 7 Feb 2025 17:00:17 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[It’s no wonder developers are increasingly overwhelmed. The number of new CVEs published each year has increased by nearly 500% in the last decade. And the average project, with just 10 direct dependencies, can have hundreds of indirect dependencies. Put simply, developers are often buried under a mountain of security alerts and unable to prioritize which ones to remediate first.While high-profile supply chain attacks like last year’s XZ Utils backdoor tend to capture attention, the danger they pose is just a fraction of the overall threat landscape. The bigger risk often comes from unpatched vulnerabilities in lesser-known open source dependencies.GitHub’s partnership with Endor Labs cuts through the noise to help developers  identify, remediate, and fix the most critical vulnerabilities—without ever leaving GitHub.With Endor Labs software composition analysis (SCA) integrated into GitHub Advanced Security and Dependabot, development teams can dismiss up to 92% of low-risk dependency security alerts to focus instead on the vulnerabilities that matter most.Endor Labs SCA brings context into open source vulnerability detectionEndor Labs SCA helps identify and prioritize dependency vulnerabilities by their potential impact, according to factors like reachability, exploitability, and more. For example, Endor Labs checks if the vulnerable function of a given dependency is actually reachable by your application or if it is just sitting on an unused corner of a transitive dependency. Security teams can also configure risk, licensing, and permission profiles to ensure developers are not bothered unless the risk is truly warranted.Prioritize and fix open source vulnerabilities with GitHubGitHub Advanced Security integrates crucial security practices directly into the development workflow, offering developers a streamlined way to secure their code. Its features are free for open source maintainers, including dependency review, secret scanning, code scanning, and Copilot Autofix.Dependabot, available for free to all GitHub users, automates dependency updates, so you can spend more time building. Developers can remediate vulnerabilities by merging Dependabot-authored pull requests with the click of a button or by applying Endor Patches.Secure your automated workflowsGitHub Actions makes it easy to automate all your software workflows, whether you want to build a container, deploy a web service, or welcome new users to your open source project. These actions are often updated with bug fixes and new features, which can take time to maintain.Endor Labs automatically discovers in-use actions and their dependencies to ensure they fit your risk, licensing, and permission profiles. Dependabot automatically updates your dependencies, and code scanning helps identify existing workflow configuration vulnerabilities and prevent new ones.]]></content:encoded></item><item><title>GitHub Copilot: The agent awakens</title><link>https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/</link><author>Thomas Dohmke</author><category>official</category><category>dev</category><pubDate>Thu, 6 Feb 2025 17:04:30 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[When we introduced GitHub Copilot back in 2021, we had a clear goal: to make developers’ lives easier with an AI pair programmer that helps them write better code. The name reflects our belief that artificial intelligence (AI) isn’t replacing the developer. Instead, it’s always on their side. And like any good first officer, Copilot can also fly by itself: for example, when providing pull request feedback, autofixing security vulnerabilities, or brainstorming on how to implement an issue.Today, we are upgrading GitHub Copilot with the force of even more agentic AI – introducing agent mode and announcing the General Availability of Copilot Edits, both in VS Code. We are adding Gemini 2.0 Flash to the model picker for all Copilot users. And we unveil a first look at Copilot’s new autonomous agent, codenamed Project Padawan. From code completions, chat, and multi-file edits to workspace and agents, Copilot puts the human at the center of the creative work that is software development. AI helps with the things you don’t want to do, so you have more time for the things you do.Agent mode available in preview 🤖GitHub Copilot’s new agent mode is capable of iterating on its own code, recognizing errors, and fixing them automatically. It can suggest terminal commands and ask you to execute them. It also analyzes run-time errors with self-healing capabilities.In agent mode, Copilot will iterate on not just its own output, but the result of that output. And it will iterate until it has completed all the subtasks required to complete your prompt. Instead of performing just the task you requested, Copilot now has the ability to infer additional tasks that were not specified, but are also necessary for the primary request to work. Even better, it can catch its own errors, freeing you up from having to copy/paste from the terminal back into chat.Here’s an example where GitHub Copilot builds a web app to track marathon training:To get started, you’ll need to download VS Code Insiders and then enable the agent mode setting for GitHub Copilot Chat:Then, when in the Copilot Edits panel, switch from Edit to Agent right next to the model picker:Agent mode will change the way developers work in their editor; and as such, we will bring it to all IDEs that Copilot supports. We also know that today’s Insiders build isn’t perfect, and welcome your feedback as we improve both VS Code and the underlying agentic technology in the coming months.Copilot Edits, now GA in VS Code 🎉Announced at GitHub Universe in October last year, Copilot Edits combines the best of Chat and Inline Chat with a conversational flow and the ability to make inline changes across a set of files that you manage. The feedbackyou provided in the past was instrumental in shipping this feature as GA in VS Code today. Thank you!In Copilot Edits you specify a set of files to be edited, and then use natural language to ask GitHub Copilot for what you need. Copilot Edits makes inline changes in your workspace, across multiple files, using a UI designed for fast iteration. You stay in the flow of your code while reviewing the suggested changes, accepting what works, and iterating with follow-up asks.Behind the scenes, Copilot Edits leverages a dual-model architecture to enhance editing efficiency and accuracy. First, a foundation language model considers a full context of the Edits session to generate initial edit suggestions. You can choose the foundation language model that you prefer between: OpenAI’s GPT-4o, o1, o3-mini, Anthropic’s Claude 3.5 Sonnet, and now, Google’s Gemini 2.0 Flash. For the optimal experience, we developed a speculative decoding endpoint, optimized for fast application of changes in files. The proposed edits from the foundation model are sent to the speculative decoding endpoint that will then propose those changes inline in the editor.Copilot Edits works because it puts you in control, from setting the right context to accepting changes. The experience is iterative: when the model gets it wrong, you can review changes across multiple files, accept good ones and iterate until, together with Copilot, you arrive at the right solution. After accepting changes, you can run the code to verify the changes and, when needed, undo in Copilot Edits to get back to a previous working state. Copilot Edits is in the Secondary Side Bar (default on the right) so that you can interact with views in the Primary Side Bar, such as the Explorer, Debug, or Source Control view, while you’re reviewing proposed changes. For example, you can have unit tests running in the Testing view on the left, while using the Copilot Edits view on the right, so that in every iteration you can verify if the changes Copilot Edits proposed are passing your unit tests.Using your voice is a natural experience while using Copilot Edits. Just talking to Copilot makes the back and forth smooth and conversational. It almost feels like interacting with a colleague with area expertise, using the same kind of iterative flow that you would use in real-life pair programming.Next on our roadmap is to improve the performance of the apply changes speculative decoding endpoint, support transitions into Copilot Edits from Copilot Chat by preserving context, suggest files to the working set, and allow you to undo suggested chunks. If you want to be among the first to get your hands on these improvements, make sure to use VS Code Insiders and the pre-release version of the GitHub Copilot Chat extension. To help improve the feature, please file issues in our repo.Beyond the GA in VS Code, Copilot Edits is now in preview for Visual Studio 2022.Project Padawan: SWE agents on GitHubWe’re excited to share a first look at our autonomous SWE agent and how we envision these types of agents will fit into the GitHub user experience. When the product we are building under the codename Project Padawan ships later this year, it will allow you to directly assign issues to GitHub Copilot, using any of the GitHub clients, and have it produce fully tested pull requests. Once a task is finished, Copilot will assign human reviewers to the PR, and work to resolve feedback they add. In a sense, it will be like onboarding Copilot as a contributor to every repository on GitHub. ✨Behind the scenes, Copilot automatically spins up a secure cloud sandbox for every task it’s assigned. It then asynchronously clones the repository, sets up the environment, analyzes the codebase, edits the necessary files, and builds, tests, and lints the code. Additionally, Copilot takes into account any discussion within the issue or PR, and any custom instruction within the repository, so it understands the full intent of its task, as well as the guidelines and conventions of the project.And just as we did with Copilot Extensions and the model picker in Copilot, we will also provide opportunities to integrate into this AI-native workflow and work closely with partners and customers in a tight feedback loop. We believe the end-state of Project Padawan will result in transforming how teams manage critical-yet-mundane tasks, such as fixing bugs or creating and maintaining automated tests. Because ultimately, it’s all about empowering developers by allowing them to focus on what matters, and letting copilots do the rest. And don’t worry. We will have patience, so the agent won’t turn to the dark side. 😉]]></content:encoded></item><item><title>5 tips for promoting your open source project</title><link>https://github.blog/open-source/maintainers/5-tips-for-promoting-your-open-source-project/</link><author>Klint Finley</author><category>official</category><category>dev</category><pubDate>Thu, 6 Feb 2025 17:00:07 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[After months or years of hard work, you’ve just pushed your open source project to GitHub and made it public. Now it’s time to tell the world about it.Chances are you’d rather spend time writing code than getting the word out about your project. Maybe your project will go viral and you won’t have to spend much time on marketing. But chances are you’re going to need to do some work to build awareness, at least in the early days. Fortunately, there are plenty of people who have been down this path and are willing to help. In this article, experienced maintainers offer their advice on sharing open source projects with the world.Start with the obvious. Post to social media about your project. Submit it to Hacker News, Reddit, Product Hunt, and similar sites. Then keep an eye peeled for people who have the problem that you’re trying to solve. Respond to their posts and let them know you have a potential solution. Reach out to podcasts and YouTube channels. Submit talks to conferences. Offer to speak at meetups.Keep promoting your work as you improve the project. Remember that people want to hear about helpful tools that solve real problems, as long as you’re genuinely trying to help, and not just spamming your followers. You might not be comfortable with self promotion, but you need to promote your work to get it out there. “You shouldn’t feel icky about it,” Sidecar maintainer Aaron Francistold us in a Q&A. “You put a lot of time into making something helpful.”Focus on the problem your project solvesWhat should you say when you’re promoting your work? First and foremost, you need to know what problem your project solves and be able to communicate that to potential users as simply as possible. “One of the biggest mistakes I see is the use of too much technical terminology,” says Chakra UI maintainer Segun Adebayo. It might be tempting to talk about the technologies you’ve built your solution upon, or the latest buzzwords you think users might be interested in. Open source users are, after all, often your fellow developers and technical people. But it’s easy to go overboard and obscure the value of the project.For example, your project might make clever use of decentralized computing principles, points out Tasha Drew, co-chair for Kubernetes’ Working Group for Multi-tenancy, but what people really care about is why they should use it. “What’s the message you want people to take away from your webpage or your README? It’s probably not related to the theory behind the code,” she says.Use that core message everywhere: Social media posts and profiles, blog posts, tutorials, etc.Document, document, document!Getting someone’s attention is only one part of the battle. If you want people to actually use, share, and contribute to your project, you need clear, up-to-date documentation. “Write as much as you can stand to write,” Francis says. Not only will it make your user experience better, it might even improve your code. “If you find it’s hard to document a particular feature, that’s probably a sign that it’s too complicated and you need to simplify it,” he explains.Think beyond just documenting the code. You should provide things like quick starts, tutorials, and screencasts. “Video is really helpful for a lot of people,” Adebayo says. “People learn in different ways so it’s important to provide different types of content.”No matter how good your documentation is, people are still going to have questions—and, if you’re lucky, pull requests. It’s important to be responsive, especially when you’re just starting out. “Time is finite, we only get one life, so value those people who are willing to spend some of their precious resources on you,” Francis says. “That applies not just to people sending pull requests, but to people pointing out problems or making suggestions on social media as well.”That doesn’t mean you have to be on call 24/7 to provide an immediate reply to every single question and comment. But it does mean you shouldn’t let pull requests, issues, and comments sit for too long without a response. You have to let people know your project is active, and that you value their input. “It might be intimidating at first to interact with people you don’t know, but you have to do it if you want to grow,” says Adebayo. “This is a sure way to meet new people and make new friends that might be helpful to you in the future.”Invest time in onboarding contributorsYou need to document both how to use your project, and how to contribute to it. Create CONTRIBUTING.md and CODE_OF_CONDUCT.md files with your contribution guidelines and code of conduct. These let potential contributors know that you’re open to contributions and that you’ve put some thought into working with others. It’s especially helpful to provide a list of what you would, and would not, like potential contributors to help with.Remember that non-code contributions, like documentation, support, and graphic design, are a big part of any successful project. While these aren’t necessarily non-technical, you shouldn’t assume too much technical knowledge. “You want to make your language and project easy to understand so that people of various technical skill levels will be interested,” Drew says.Also be sure to take advantage of the “Help wanted” and “Good first issue” labels. These can help people who are looking for ways to contribute find your project.]]></content:encoded></item><item><title>Running sklearn models in Zig</title><link>https://zig.news/vinybrasil/running-sklearn-models-in-zig-2do1</link><author>Vinícyus Brasil</author><category>dev</category><category>official</category><category>zig</category><pubDate>Wed, 5 Feb 2025 22:51:00 +0000</pubDate><source url="https://zig.news/">Dev - Zig News</source><content:encoded><![CDATA[Scikit-learn models became the industry standard for creating machine learning models. Although the library offers many conveniences, one becomes tied to Python APIs for serving such models. These frameworks (such as FastAPI, for example) have various scalability issues, as Anton demonstrates in this video.
To overcome this limitation, one solution is to use Python's C library to run the models, while the rest of the API can be built using another framework.In this blogpost we'll implement this strategy. We'll create an application that uses Python's C API to run the models with the Zap library handling the requests, which is a Zig blazing fast microframework for web applications. The full code to the post can be found in this repository and this post was originally posted on my blog.
  
  
  Creating the model and the shared object
So let's say we got this simple script called  that creates a Logistic Regression, in which the model takes a numpy array with two dimensions and returns the value 1 or 0.  The model is saved in a pickle file called .The full library we are going to create just loads the pickle file and returns it in a list. It will attend by the name of  and it  module have the following code:Quite simple, right? The next step is to compile it to a shared object, the kind of file that contains the code we just wrote. 
Theoretically this step is not needed but I think its way easier to work with the library this way. To compile the library we can use Cython. The following  file will translate our library to C and generate the shared object.Note that both of the files generated, libpredict.so and model.pkl, need to be moved to the same folder where we'll build the zig executable.
  
  
  Running Python code in Zig
To interact with the Python C API, we gotta load the header file that includes the Python functions. The  file starts with:The syntax to call Python is in this documentation. For exemple,
to print a string, we first need to initialize the interpreter and use the function PyRun_SimpleString The main.zig file will start this way. First, we initialize the interpreter. Then, load the module  and extracts from it a pointer to the function  from the . To call the function we need to convert the zig array  to a Python List also using the C API and that's the reason for the function  to exist. Finally, the function can be called using the converted list.Note the loadMod() function appends the current path and from it import the  file so the loadFunc function can link a pointer to the predict function.The prepareList function just creates a Python list with PyList_New and appends to it the values from the  arrayh.The evaluteResult function iterates through all of the values of the Python function's response and the returns the last one. I've written the code this way because maybe what you are interested is the return of the predict_proba() function of the sklearn model, so it's easier to adapt the code.
  
  
  Creating the API with Zap
I've chosen Zap because it's a  micro webframework that can handle a lot of requests simultaneously and also it's quite intuitive.To create the API, we need a Router object to map the routes to the functions that handle the requests. Our only route here is the  that receives a json payload with the fields  and  to send to the Python function. We'll also need a HttpListener object to listen to the requests on the port 3000. The struct PredictorPackage is where all of the handling of the request is made;Inside the PredictorPackage struct, the function predictValue captures the body of the request, parses it with std.json.Parsed and then calls the Python function with it. Then, it stringify it and responds the request with a resultPrediction object.After building the project () and moving the libpredict.so and the model.pkl to the same folder as the executable,
we can test the API with cURL:curl  POST http://localhost:3000/predict It should return the following response:And that's it. Fell free to contact me via Linkedin or to open a PR on the GitHub repo if you find something wrong. Keep on learning :D]]></content:encoded></item><item><title>crates.io: development update</title><link>https://blog.rust-lang.org/2025/02/05/crates-io-development-update.html</link><author>Tobias Bieniek</author><category>dev</category><category>official</category><category>rust</category><pubDate>Wed, 5 Feb 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[Back in July 2024, we published a blog post about the ongoing development of crates.io. Since then, we have made a lot of progress and shipped a few new features. In this blog post, we want to give you an update on the latest changes that we have made to crates.io.In RFC #3660 we proposed a new feature that allows crate owners to delete their crates from crates.io under certain conditions. This can be useful if you have published a crate by mistake or if you want to remove a crate that is no longer maintained. After the RFC was accepted by all team members at the end of August, we began implementing the feature.We created a new API endpoint DELETE /api/v1/crates/:name that allows crate owners to delete their crates and then created the corresponding user interface. If you are the owner of a crate, you can now go to the crate page, open the "Settings" tab, and find the "Delete this crate" button at the bottom. Clicking this button will lead you to a confirmation page telling you about the potential impact of the deletion and requirements that need to be met in order to delete the crate:As you can see from the screenshot above, a crate can only be deleted if either: the crate has been published for less than 72 hours or the crate only has a single owner, and the crate has been downloaded less than 500 times for each month it has been published, and the crate is not depended upon by any other crate on crates.io.These requirements were put in place to prevent abuse of the deletion feature and to ensure that crates that are widely used by the community are not deleted accidentally. If you have any feedback on this feature, please let us know!Around the holiday season we started experimenting with generating an OpenAPI description for the crates.io API. This was a long-standing request from the community, and we are happy to announce that we now have an experimental OpenAPI description available at https://crates.io/api/openapi.json!Please note that this is still considered work-in-progress and e.g. the stability guarantees for the endpoints are not written down and the response schemas are also not fully documented yet.You can view the OpenAPI description in e.g. a Swagger UI at https://petstore.swagger.io/ by putting https://crates.io/api/openapi.json in the top input field. We decided to not ship a viewer ourselves for now due to security concerns with running it on the same domain as crates.io itself. We may reconsider whether to offer it on a dedicated subdomain in the future if there is enough interest.The OpenAPI description is generated by the utoipa crate, which is a tool that can be integrated with the axum web framework to automatically generate OpenAPI descriptions for all of your endpoints. We would like to thank Juha Kukkonen for his great work on this tool!Support form and "Report Crate" buttonSince the crates.io team is small and mostly consists of volunteers, we do not have the capacity to manually monitor all publishes. Instead, we rely on you, the Rust community, to help us catch malicious crates and users. To make it easier for you to report suspicious crates, we added a "Report Crate" button to all the crate pages. If you come across a crate that you think is malicious or violates the code of conduct or our usage policy, you can now click the "Report Crate" button and fill out the form that appears. This will send an email to the crates.io team, who will then review the crate and take appropriate action if necessary. Thank you to crates.io team member @eth3lbert who worked on the majority of this.If you have any issues with the support form or the "Report Crate" button, please let us know. You can also always email us directly at help@crates.io if you prefer not to use the form.We have added a new feature that allows you to receive email notifications when a new version of your crate is published. This can be useful in detecting unauthorized publishes of your crate or simply to keep track of publishes from other members of your team.This feature was another long-standing feature request from our community, and we were happy to finally implement it. If you'd prefer not to receive publish notifications, then you can go to your account settings on crates.io and disable these notifications.These were some of the more visible changes to crates.io over the past couple of months, but a lot has happened "under the hood" as well.RFC #3691 was opened and accepted to implement "Trusted Publishing" support on crates.io, similar to other ecosystems that adopted it. This will allow you to specify on crates.io which repository/system is allowed to publish new releases of your crate, allowing you to publish crates from CI systems without having to deal with API tokens anymore.Slightly related to the above: API tokens created on crates.io now expire after 90 days by default. It is still possible to disable the expiry or choose other expiry durations though.The crates.io team was one of the first projects to use the diesel database access library, but since that only supported synchronous execution it was sometimes a little awkward to use in our codebase, which was increasingly moving into an async direction after our migration to axum a while ago. The maintainer of diesel, Georg Semmler, did a lot of work to make it possible to use diesel in an async way, resulting in the diesel-async library. Over the past couple of months we incrementally ported crates.io over to  queries, which now allows us to take advantage of the internal query pipelining in  that resulted in some of our API endpoints getting a 10-15% performance boost. Thank you, Georg, for your work on these crates!Whenever you publish a new version or yank/unyank existing versions a couple of things need to be updated. Our internal database is immediately updated, and then we synchronize the sparse and git index in background worker jobs. Previously, yanking and unyanking a high number of versions would each queue up another synchronization background job. We have now implemented automatic deduplication of redundant background jobs, making our background worker a bit more efficient.The final big, internal change that was just merged last week is related to the testing of our frontend code. In the past we used a tool called Mirage to implement a mock version of our API, which allowed us to run our frontend test suite without having to spin up a full backend server. Unfortunately, the maintenance situation around Mirage had lately forced us to look into alternatives, and we are happy to report that we have now fully migrated to the "Industry standard API mocking" package msw. If you want to know more, you can find the details in the "small" migration pull request.We hope you enjoyed this update on the development of crates.io. If you have any feedback or questions, please let us know on Zulip or GitHub. We are always happy to hear from you and are looking forward to your feedback!]]></content:encoded></item><item><title>Python 3.13.2 and 3.12.9 now available!</title><link>https://pythoninsider.blogspot.com/2025/02/python-3132-and-3129-now-available.html</link><author>Thomas Wouters</author><category>Python official news</category><category>dev</category><category>official</category><category>python</category><pubDate>Tue, 4 Feb 2025 19:58:00 +0000</pubDate><source url="https://pythoninsider.blogspot.com/">Python Insider</source><content:encoded><![CDATA[A small release day today! That is to say the releases are relatively 
small; the day itself was of average size, as most days are.Python 3.13’s second maintenance release. About 250 changes went into 
this update, and can be yours for free if you just upgrade now.Python 3.12’s  maintenance release already. Just 180 changes for 3.12, but it’s still worth upgrading.Thanks to all of the many volunteers who help make Python Development
 and these releases possible! Please consider supporting our efforts by 
volunteering yourself or through organization contributions to the 
Python Software Foundation.Regards from your tireless, tireless release team,
Thomas Wouters 
Steve Dower ]]></content:encoded></item><item><title>4 steps to building a natural language search tool</title><link>https://github.blog/open-source/social-impact/4-steps-to-building-a-natural-language-search-tool/</link><author>Ryan Fallon</author><category>official</category><category>dev</category><pubDate>Thu, 30 Jan 2025 17:00:40 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[“We have a problem. Our current search method for sifting through PDFs is extremely manual and time consuming. Is there an easier way?”As a developer, this is one of those questions that really gets me excited. I was tasked with finding a way to transform a cumbersome, archival process into an efficient, intuitive search experience. It’s a way to make a group of people’s lives easier, and because of the organizations they work for, help them be more effective in providing humanitarian assistance to people in need around the world. I couldn’t imagine a better project to be working on.Unlocking the United Nations’ legacy for rapid actionSince 1945, the United Nations has produced resolutions and other documents that guide international peace and security efforts. Yet accessing this wealth of knowledge remains a challenge, including for organizations such as the International Committee of the Red Cross (ICRC). Currently, delegates at ICRC’s permanent observer mission to the UN advise member states and other stakeholders on international humanitarian law and humanitarian issues. When states negotiate relevant resolutions and other UN products, leaning on pre-existing humanitarian language from UN resolutions can provide precedence. This often requires sifting through PDFs to find relevant content within documents—a time-intensive, manual process ill-suited to the fast-paced world of humanitarian diplomacy.To solve this, I built a single-page application (SPA) that enables users to input natural language queries and instantly retrieve relevant UN resolutions. The solution is live now at resolutions.projectrefuge.io and serves as a robust example of how technology can simplify access to critical information.Text extraction and structuring
Using Amazon Textract, I extracted raw text from decades’ worth of UN Security Council Resolutions and Presidential Statements and six years of UN General Assembly Resolutions. A Go script then parsed this text using Regex matching, segmenting it into individual resolutions for easier indexing.
Search-ready database with MongoDB Atlas
I adapted a Node.js script from MongoDB to upload the parsed resolutions as embeddings into a MongoDB Atlas database. This step ensures the content is structured for fast and relevant searches.User interface built with Vue.js
The front end is an intuitive SPA created with Vue.js. Users simply enter semantic search queries—such as “resolutions on humanitarian access in armed conflicts”—and receive results in seconds.
The backend relies on AWS Lambda and API Gateway, ensuring scalability and seamless performance. The entire application is hosted as a subdomain on AWS Amplify, combining reliability with ease of access.Broader implications: a blueprint for impactThe implications of this project go far beyond the ICRC’s use case with UN Resolutions. With slight modifications, the tool could index and search any collection of legal and policy documents. This approach is a blueprint for organizations aiming to leverage technology for better decision-making and more effective action. For nonprofits, this demonstrates the power of owning your code and building tailored solutions. For developers, it’s a reminder of how open source can accelerate progress in humanitarian and public policy sectors.Build together with open sourceProjects like resolutions.projectrefuge.io highlight the potential of open source to transform how we access and use information. If you’re a nonprofit, explore GitHub for Nonprofits to discover tools and resources that can help you build your own solutions. Developers eager to contribute to impactful work can browse the For Good First Issue program to find projects that align with their skills and values.Finally, stay tuned as we work to identify other opportunities with humanitarian actors such as the ICRC to bridge the technology and humanitarian space. Together, we can build a future where knowledge is more accessible and tools are built with collaboration in mind, ensuring that humanitarian efforts are supported by cutting-edge technology.Let’s code for good—and make a lasting impact.If you’d like to lend your developer skills for good, check out For Good First Issue, a curated platform of open source projects that contribute to a better future for everyone.]]></content:encoded></item><item><title>Announcing Rust 1.84.1</title><link>https://blog.rust-lang.org/2025/01/30/Rust-1.84.1.html</link><author>The Rust Release Team</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[The Rust team has published a new point release of Rust, 1.84.1. Rust is a
programming language that is empowering everyone to build reliable and
efficient software.If you have a previous version of Rust installed via rustup, getting Rust
1.84.1 is as easy as:If you don't have it already, you can get  from the
appropriate page on our website.1.84.1 resolves a few regressions introduced in 1.84.0:It also includes several fixes for those building Rust from source:Many people came together to create Rust 1.84.1. We couldn't have done it
without all of you. Thanks!]]></content:encoded></item><item><title>Cybersecurity researchers: Digital detectives in a connected world</title><link>https://github.blog/security/vulnerability-research/cybersecurity-researchers-digital-detectives-in-a-connected-world/</link><author>Nancy Gariché</author><category>official</category><category>dev</category><pubDate>Wed, 29 Jan 2025 17:00:31 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[Have you ever considered yourself a detective at heart? Cybersecurity researchers are digital detectives, uncovering vulnerabilities before malicious actors exploit them. To succeed, they adopt the mindset of an attacker, thinking creatively to predict and outmaneuver threats. Their expertise ensures the internet remains a safer place for everyone.If you love technology, solving puzzles, and making a difference, this might be the perfect career—or pivot—for you. This blog will guide you through the fascinating world of security research, how to get started, and how to thrive in this rapidly changing field.What is a security researcher?Security researchers investigate systems with the mindset of an attacker to uncover vulnerabilities before they can be exploited. They test for weaknesses and design robust security measures to protect against cyber threats.But their work doesn’t stop at identifying problems. Security researchers work with developers, system administrators, and open source maintainers to report and fix problems. They protect essential data and ensure digital infrastructure is robust against new threats.Types of security researchSecurity researchers often specialize in areas such as: Finding and fixing software vulnerabilities. Working closely with developers to build secure applications.   Analyzing and improving encryption methods to protect data. Testing protocols for flaws.   Designing protections to secure networks and identifying potential threats.  Operating system security: Strengthening operating systems to resist attacks. Developing new security measures or refining existing ones.   Taking apart software or hardware to understand how it works and find weaknesses.Why security researchers matter: Real-life impactsUnderstanding the significance of cybersecurity researchers requires looking at their impact through real-world examples.A notable example is the  identified in 2021 in the Log4j logging framework. Security researchers played a key role in uncovering this issue, which had the potential to allow attackers to remotely execute code and compromise systems globally. Thanks to their swift action and collaboration with the community, patches were developed and shared before attackers could widely exploit the vulnerability. This effort highlights the researchers’ vital role in safeguarding systems.Similarly, in 2023, security researchers discovered a zero-day vulnerability in the l, identifying the issue before it could be exploited on a large scale. The flaw had the potential to allow unauthorized access to file transfer systems, which could have resulted in data breaches. By proactively identifying the vulnerability and working with vendors to develop timely patches, these researchers helped secure critical systems and prevent potential breaches.These examples show that security researchers don’t just protect systems—they protect people and organizations. This makes their work not just important but crucial in the digital age. Their efforts save businesses, governments, and individuals from devastating cyberattacks, giving their work a deep sense of purpose.What makes a great security researcher?The essence of a great security researcher lies in a blend of traits and skills. An inherent  and  for security are what drives them. This isn’t just about loving technology; it’s about being captivated by the intricacies of how systems can be manipulated or secured. This curiosity leads to continuous learning and exploration, pushing the boundaries of what’s known to uncover what’s hidden. is another important part of security research. Security research involves solving complex puzzles where understanding how to break something can often lead to knowing how to fix it.  is equally crucial. The best researchers think outside the box, finding innovative ways to secure systems or expose weaknesses that conventional methods might miss. is paramount in this field, where a single oversight can lead to significant vulnerabilities.  rules guide their work. They make sure they use their skills to help security, not for personal gain or harm. is necessary due to the ever-changing landscape of cyber threats. Researchers must stay updated with new technologies and attack methods, always learning to keep ahead of malicious actors. Finally,  is what lets them look deep into systems, finding weaknesses that might be hidden or deeply buried.The journey can be long and arduous, but their determination leads to breakthroughs.Forget the traditional path—focus on skillsOne of the most inspiring aspects of security research is that it’s a field that welcomes diverse backgrounds. While degrees and certifications offer structured learning, they’re not required to succeed. Many top researchers come from eclectic paths and thrive because of their creativity and practical experience.This diversity shows that formal qualifications aren’t always needed. What matters most is your ability to find real vulnerabilities and solve complex problems.Many breakthroughs in security research come from someone noticing something unusual and investigating it deeply. Take the XZ Utils backdoor, discovered by a Microsoft employee who uncovered a hidden vulnerability while troubleshooting slow SSH connections. Similarly, the Sony BMG rootkit scandal came to light because someone dug deeper into unexpected behavior. These examples highlight how curiosity, observation, and persistence often lead to significant discoveries.This investigative mindset is central to security research, but it needs to be paired with practical skills to uncover and mitigate vulnerabilities effectively. So, how can you get started? By building the essential skills that form the foundation of a successful security researcher.How to build these skills Use security tools like OWASP ZAP, Burp Suite Community Edition, and Ghidra to develop practical skills. Experiment in safe test environments, such as intentionally vulnerable applications or local test setups, where you can break systems and learn how to fix them. Try fuzzing with tools like AFL++ to uncover hidden vulnerabilities and strengthen software. Understand how malicious actors exploit systems. This mindset sharpens your ability to spot vulnerabilities, predict potential exploits, and design effective defenses.Develop programming skills: Practice writing secure, efficient code in the language of your choice. Contribute to open source projects or join hackathons to enhance your skills and gain experience.Understand vulnerabilities: Study common issues like SQL injection, cross-site scripting (XSS), and other frequent weaknesses, such as those on the Top 25 CWE Weaknesses List. Use tools like CodeQL to analyze, exploit, and mitigate vulnerabilities effectively.Gain practical experience:Join bug bounty platforms like HackerOne or Bugcrowd to test your skills on systems in the wild.  Intern in IT security or vulnerability assessment roles to gain professional experience.  Hunt for and fix bugs in your favorite open source project. Attend conferences, forums, and local meetups to connect with like-minded professionals. Exchange knowledge, find mentors, and stay updated on the latest trends and tools in cybersecurity.For those transitioning into security researchWhile building experience and networking are essential for all researchers, they’re especially valuable for those transitioning into cybersecurity research. If you’re considering a shift, here’s how to leverage your existing skills and make the leap without starting over.If you’re currently employed, you can begin your journey by leveraging opportunities in your current role:Identify security-related tasks: Developers can use secure coding practices or conduct code reviews. IT admins might audit network configurations or manage firewalls. Analysts can assess data for anomalies that could indicate breaches.  Support security projects: Help with projects like making scripts to check for weaknesses or holding Red Team/Blue Team exercises.  Collaborate with your company’s security team: Assist with vulnerability scans, penetration testing, or incident response exercises.   Access training platforms, pursue certifications, or attend workshops your organization provides.Your existing skills can provide a strong foundation, even if you’re coming from an unrelated field. Explore any opportunities available, including the tools and platforms mentioned earlier, to sharpen your skills and gain real-world experience.Participate in forums and meetups, for example, on meetup.com, and join online groups to exchange knowledge and gain mentorship. Chances are, you’ll meet someone working in a role you’re interested in, presenting a good opportunity to ask for feedback and insight into the next steps you can take to work toward a career in cybersecurity.Security research is more than a career—it’s a journey fueled by curiosity, creativity, and persistence. No matter your background, your willingness to dig deeper, think critically, and take action can make a meaningful difference in the digital world. The vulnerabilities you uncover could protect millions. The only question is—what action can you take today?How to stay updated on cybersecurity threatsCybersecurity evolves rapidly, and staying informed is critical. Use these strategies: Track vulnerabilities and exploits through platforms like Common Vulnerabilities and Exposures (CVE) Details and Threatpost.   Participate in forums like Reddit’s r/netsec or cybersecurity-focused Discord channels.   Use platforms like PicoCTF and Hack The Box to refine your skills in realistic scenarios.The journey to becoming a cybersecurity researcher is as much about curiosity and exploration as it is about structured learning. There’s no single path—your next move is yours to choose.Here are some ideas to spark your journey:Every action you take is a step forward in building your expertise and making the digital world safer. What will you explore next?Did you know GitHub has a Security Lab dedicated to improving open source security? Check out the GitHub Security Lab resources to learn more, explore tools, and join the effort to make open source safer.OWASP: A global community providing resources, tools, and documentation to improve software security.  CodeQL: A powerful tool for writing custom queries to identify vulnerabilities in source code.  Hack The Box: Provides challenges to practice penetration testing and reverse engineering.  TryHackMe: Interactive cybersecurity training with real-world scenarios and labs.  Secure Code Game: A fun, interactive way to learn and practice secure coding by identifying and fixing vulnerabilities.  Threatpost: Industry news and threat analysis to stay informed on vulnerabilities and exploits.  CVE Details: A resource for tracking and analyzing publicly known cybersecurity vulnerabilities.]]></content:encoded></item><item><title>Considerations for making a tree view component accessible</title><link>https://github.blog/engineering/user-experience/considerations-for-making-a-tree-view-component-accessible/</link><author>Eric Bailey</author><category>official</category><category>dev</category><pubDate>Tue, 28 Jan 2025 17:00:39 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[Tree views are a core part of the GitHub experience. You’ve encountered one if you’ve ever navigated through a repository’s file structure or reviewed a pull request.On GitHub, a tree view is the list of folders and the files they contain. It is analogous to the directory structure your operating system uses as a way of organizing things.Tree views are notoriously difficult to implement in an accessible way. This post is a deep dive into some of the major considerations that went into how we made GitHub’s tree view component accessible. We hope that it can be used as a reference and help others.It’s important to have components with complex interaction requirements map to something people are already familiar with using. This allows for responsiveness to the keypresses they will try to navigate and take action on our tree view instances.Navigating and taking actions on items in Windows’ tree view using NVDA and JAWS helped us get a better understanding of how things worked, including factors such as focus management, keyboard shortcuts, and expected assistive technology announcements.Then maybe reference the APGThis is to say that the APG can serve as a helpful high-level resource for things to consider for your overall approach, but its suggestions for code necessitate deeper scrutiny.Build from a solid, semantic foundationAt its core, a tree view is a list of lists. Because of this, we used  and  elements for parent and child nodes:<ul>
  <li>
    <ul>
      <li>.github/</li>
      <li>source/</li>
      <li>test/</li>
    </ul>
  </li>
  <li>.gitignore</li>
  <li>README.md</li>
</ul>
There are a few reasons for doing this, but the main considerations are:Lessening the work we need for future maintenance, and consequential re-verification that our updates continue to work properly, and  Better guaranteed interoperability between different browsers, apps, and other technologies.: GitHub currently does not virtualize its file trees. We would need to revisit this architectural decision if this ever changes.Better broad assistive technology supportWe found that utilizing semantic HTML elements also performed better for some less-common assistive technologies. This was especially relevant with some lower-power devices, like an entry-level Android smartphone from 2021.Better Forced Color Mode supportSemantic HTML elements also map to native operating system UI patterns, meaning that Forced Color Mode’s heuristics will recognize them without any additional effort. This is helpful for people who rely on the mode to see screen content.The heuristic mapping behavior does not occur if we used semantically neutral  or  elements, and would have to be manually recreated and maintained.A composite widget allows a component that contains multiple interactive elements to only require one tab stop unless someone chooses to interact with it further.Consider a file tree for a repository that contains 500+ files in 20+ directories. Without a composite widget treatment, someone may have to press  far too many times to bypass the file tree component and get what they need.Think about wrapping it in a landmarkLike using a composite widget, landmark regions help some people quickly and efficiently navigate through larger overall sections of the page. Because of this, we wrapped the entire file tree in a  landmark element.Go with a roving  approachA roving tabindex is a technique that uses  applied to each element in a series, and then updates the  value to use  instead in response to user keyboard input. This allows someone to traverse the series of elements, as focus “roves” to follow their keypresses.<li tabindex="-1">File 1</li>
<li tabindex="-1">File 2</li>
<li tabindex="0">File 3</li>
<li tabindex="-1">File 4</li>
We use a considered set of ARIA declarations to build off our semantic foundation.Note that while we intentionally started with semantic HTML, there are certain ARIA declarations that are needed. The use of ARIA here is necessary and intentional, as it expands the capabilities of HTML to describe something that HTML alone cannot describe—a tree view construct. is placed on the parent  element, to communicate that it is a tree view construct.   is placed on the child  elements, to communicate that they are tree view nodes.   is declared on child  elements, to communicate that they contain branch and leaf nodes.   is declared on directories, with a value of  to communicate that the branch node is in an opened state and a value of  to communicate that it is in a collapsed state instead.   is used to indicate if branch or leaf nodes have been chosen by user navigation, and can therefore have user actions applied to them.We also made the following additions: is applied to SVG icons (folders, files, etc.) to ensure its content is not announced.   is placed on the selected node to better support when a node is deep linked to via URL.: We use “branch node” and “leaf node” as broad terms that can apply to all tree view components we use on GitHub. For the file tree, branch nodes would correspond to directories and subdirectories, and leaf nodes would correspond to files.Support expected navigation techniquesThe following behaviors are what people will try when operating a tree view construct, so we support them:: Places focus on the entire tree view component, then moves focus to the next focusable item on the view.  :
If a branch node is selected: Displays the directory’s contents.  If a leaf node is selected: Displays the leaf node’s contents.  : Moves selection to the next node that can be selected without opening or closing a node.  : Moves selection to the previous node that can be selected without opening or closing a node.  :
If a branch node is selected and in a collapsed state: Expands the selected collapsed branch node and does not move selection.  If a branch node is selected and in an expanded state: Moves selection to the directory’s first child node.  :
If a branch node is selected and in an expanded state: Collapses the selected collapsed directory node and does not move selection.  If a branch node is selected and in a collapsed state: Moves selection to the node’s parent directory.  If a leaf node is selected: Moves selection to the leaf node’s parent directory.  : Moves selection to the last node that can be selected.  : Moves selection to the first node that can be selected.We also support , as we are modeling Windows File Explorer’s tree view behaviors. Here, we move selection to the node closest to the currently selected node whose name matches what the user types.Nodes on tree view constructs are tree items, not links. Because of this, tree view nodes do not support the behaviors you get with using an anchor element, such as opening its URL in a new tab or window.We use JavaScript to listen for middle clicks and + keypresses to replicate this behavior.Tree views on GitHub can take time to retrieve their content, and we may not always know how much content a branch node contains.If there is a known amount of nodes that load, we enumerate the incoming content with an announcement that reads, “Loading {x} items.”  If there is an unknown number of nodes that load, we instead use a more generic announcement of, “Loading…”  If there are no nodes that load we use an announcement message that reads, “{branch node name} is empty.”If focus is placed on a placeholder loading node when the content loads in: Move focus from the placeholder node to the first child node in the branch node.   If focus is on a placeholder loading node but the branch node does not contain content: Move focus back to the branch node. Additionally, we remove the branch node’s  declaration.Circumstances can conspire to interfere with a tree view component’s intended behavior. Examples of this could be a branch node failing to retrieve content or a partial system outage.In these scenarios, the tree view component will use a straightforward dialog component to communicate the error.Fix interoperability issuesAs previously touched on, complicated interaction patterns run the risk of compatibility issues. Because of this, it’s essential to test your efforts with actual assistive technology to .We made the following adjustments to provide better assistive technology support:Screen readers can report on the depth of a nested list item. For example, a  element placed inside of a  element nested three levels deep can announce itself as such.We found that we needed to explicitly declare the level on each  element to recreate this behavior for a tree view. For our example, we’d also need to set  on the  element.This fix addressed multiple forms of assistive technology we tested with.Explicitly set the node’s accessible name on the  elementA node’s accessible name is typically set by the text string placed inside the  element:However, we found that VoiceOver on macOS and iOS did not support this. This may be because of the relative complexity of each node’s inner DOM structure.We used  to get around this problem, with a value that pointed to the  set on the text portion of each node:<li aria-labelledby="readme-md">
  <div>
   <!-- Icon -->
  </div>
  <div id="readme-md">
    README.md
  </div>
</li>
the node’s accessible name is announced when focus is placed on the  element,   and that the announcement matches what is shown visually.Where we’d like to go from hereThere’s a couple areas we’re prototyping and iterating on to better serve our users:Supporting links inside a nodeBrowsers apply a lot of behaviors to anchor elements, such as the ability to copy the URL.We’d like to replace the JavaScript that listens for middle clicks with a more robust native solution, only without sacrificing interoperability and assistive technology support.Supporting multiple actions per nodeTree views constructs were designed assuming a user will only ever navigate to a node and activate it.GitHub has use cases that require actions other than activating the node, and we’re exploring how to accomplish that. This is exciting, as it represents an opportunity to evolve the tree view construct on the web.An accessible tree view is a complicated component to make, and it requires a lot of effort and testing to get right. However, this work helps to ensure that everyone can use a core part of GitHub, regardless of device, circumstance, or ability.We hope that highlighting the considerations that went into our work can help you on your accessibility journey.: We’d love to hear from you if you’ve run into issues using our tree view component with assistive technology. This feedback is invaluable to helping us continue to make GitHub more accessible.]]></content:encoded></item><item><title>Open source AI is already finding its way into production</title><link>https://github.blog/ai-and-ml/generative-ai/open-source-ai-is-already-finding-its-way-into-production/</link><author>Klint Finley</author><category>official</category><category>dev</category><pubDate>Tue, 28 Jan 2025 17:00:38 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[Open source has long driven innovation and the adoption of cutting-edge technologies, from web interfaces to cloud-native computing. The same is true in the burgeoning field of open source artificial intelligence (AI). Open source AI models, and the tooling to build and use them, are multiplying, enabling developers around the world to build custom AI solutions and host them where they choose.But the survey results suggest that the use of open source AI models is already surprisingly widespread—and this is expected to grow as more models proliferate and more use cases emerge. Let’s take a look at the rise of open source AI, from the increasing rise of smaller models to use cases in generative AI.Explore how and why companies are using open source AI models in production today.  Learn how open source is changing the way developers use AI.  Look ahead at how small, open source models might be used in the future.Why use smaller, more open models?Open, or at least less-proprietary, models like the DeepSeek models, Meta’s Llama models, or those from Mistral AI can generally be downloaded and run on your own devices and, depending on the license, you can study and change how they work.  Many are trained on smaller, more focused data sets. These models are sometimes referred to as small language models (SLMs), and they’re beginning to rival the performance of LLMs in some scenarios.There are a number of benefits of working with these smaller models, explains Head of GitHub Next, Idan Gazit. They cost less to run and can be run in more places, including end-user devices. But perhaps most importantly, they’re easier to customize.While LLMs excel with general purpose chatbots that need to respond to a wide variety of questions, organizations tend to turn to smaller AI models when they need niche solutions, explains Hamel Husain, an AI consultant and former GitHub employee. For instance, with an open source LLM you can define a  grammar and require that a model only outputs valid tokens according to that grammar.“Open models aren’t always better, but the more narrow your task, the more open models will shine because you can fine tune that model and really differentiate them,” says Husain.For example, an observability platform company hired Husain to help build a solution that could translate natural language into the company’s custom query language to make it easier for customers to craft queries without having to learn the ins-and-outs of the query language.This was a narrow use case—they only needed to generate their own query language and no others, and they needed to ensure it produced valid syntax. “Their query language is not something that is prevalent as let’s say Python, so the model hadn’t seen many examples,” Husain says. “That made fine tuning more helpful than it would have been with a less esoteric topic.” The company also wanted to maintain control over all data handled by the LLM without having to work with a third party.Husain ended up building a custom solution using the then-latest version of Mistral AI’s widely used open models. “I typically use popular models because they’ve generally been fine-tuned already and there’s usually a paved path towards implementing them,” he says.Open source brings structure to the world of LLMsOne place you can see the rapid adoption of open source models is in tools designed to work with them. For example, Outlines is an increasingly popular tool for building custom LLM applications with both open source and proprietary models. It helps developers define structures for LLM outputs. You can use it, for example, to ensure an LLM outputs responses in JSON format. It was created in large part because of the need for finely tuned, task-specific AI applications.At a previous job, Outlines co-creator and maintainer Rémi Louf needed to extract some information from a large collection of documents and export it in JSON format. He and his colleague Brandon Willard tried using general purpose LLMs like ChatGPT for the task, but they had trouble producing well-structured JSON outputs. Louf and Willard both had a background in compilers and interpreters, and noticed a similarity between building compilers and structuring the output of LLMs. They built Outlines to solve their own problems.They posted the project to Hacker News and it took off quickly. “It turns out that a lot of other people were frustrated with not being able to use LLMs to output to a particular structure reliably,” Louf says. The team kept working on it, expanding its features and founding a startup. It now has more than 100 contributors and helped inspire OpenAI’s structured outputs feature.“I can’t give names, but some very large companies are using Outlines in production,” Louf says.There are, of course, downsides to building custom solutions with open source models. One of the biggest is the need to invest time and resources into prompt construction. And, depending on your application, you may need to stand up and manage the underlying infrastructure as well. All of that requires more engineering resources than using an API.“Sometimes organizations want more control over their infrastructure,” Husain says. “They want predictable costs and latency and are willing to make decisions about those tradeoffs themselves.”While open source AI models might not be a good fit for every problem, it’s still the early days. As small models continue to improve, new possibilities emerge, from running models on local hardware to embedding custom LLMs within existing applications.Fine-tuned small models can already outperform larger models for certain tasks. Gazit expects developers will combine different small, customized models together and use them to complete different tasks. For example, an application might route a prompt with a question about the best way to implement a database to one model, while routing a prompt for code completion to another. “The strengths of many Davids might be mightier than one Goliath,” he says.In the meantime, large, proprietary models will also keep improving, and you can expect both large and small model development to feed off of each other. “In the near term, there will be another open source revolution,” Louf says. “Innovation often comes from people who are resource constrained.”Ready to experiment with open source AI? GitHub Models offers a playground for both open source and proprietary models. You can use the public preview to prototype AI applications, conduct side-by-side comparisons, and more.]]></content:encoded></item><item><title>Zig Cross Compiling</title><link>https://zig.news/david_vanderson/zig-cross-compiling-37en</link><author>David Vanderson</author><category>dev</category><category>official</category><category>zig</category><pubDate>Mon, 27 Jan 2025 02:24:08 +0000</pubDate><source url="https://zig.news/">Dev - Zig News</source><content:encoded><![CDATA[My first cross-compiling attempt with zig was great.Both run Linux, and I had already successfully compiled on the phone itself, but the edit/debug cycle was too long.So first up, find out what the phone target is, by running  which produces tons of output, but at the bottom: "native": {
  "triple": "aarch64-linux.4.19.325...4.19.325-gnu.2.39",
  "cpu": {
   "arch": "aarch64",
   "name": "cortex_a78",
$zig build -Dtarget=aarch64-linux.4.19.325...4.19.325-gnu.2.39
/home/dvanderson/.cache/zig/p/122004e82d4a0c61a9c414539c1f87bb125cb2b293573af77b153ea3903cb209b65b/library/aesce.c:182:18: 
error: always_inline function 'vaesimcq_u8' requires target feature 'aes', 
but would be inlined into function 'mbedtls_aesce_inverse_key' 
that is compiled without support for 'aes'
It looks like I need to also pass some , so again using the output of  from above:$zig build -Dtarget=aarch64-linux.4.19.325...4.19.325-gnu.2.39 -Dcpu=cortex_a78
Note that this is all with zig 0.13, and there are a few issues that might change how to pass the target/cpu/libc/abi information in the future.]]></content:encoded></item><item><title>Discover Unlimited Anime with GogoAnime!</title><link>https://zig.news/gogoanimeofficial/discover-unlimited-anime-with-gogoanime-2k41</link><author>Gogo Anime</author><category>dev</category><category>official</category><category>zig</category><pubDate>Sun, 26 Jan 2025 04:09:06 +0000</pubDate><source url="https://zig.news/">Dev - Zig News</source><content:encoded><![CDATA[Are you an anime fan looking for a platform to watch your favorite shows? Look no further than GogoAnime! With a massive library of classic and trending anime, GogoAnime is your one-stop destination for all things anime.Why Choose GogoAnime?
Extensive Collection: From action-packed adventures to heartfelt romances, find anime across all genres.
Subbed & Dubbed Options: Watch in English subtitles or enjoy dubbed versions, tailored for every viewer.
High-Quality Streaming: Enjoy seamless streaming with HD quality videos.
Completely Free: No subscription is required; dive into the anime world without spending a dime.
Visit GogoAnime today and start your anime journey: gogoanimeStay tuned for more updates, and let us know which anime you're binge-watching right now]]></content:encoded></item><item><title>New to open source? Here’s everything you need to get started</title><link>https://github.blog/open-source/new-to-open-source-heres-everything-you-need-to-get-started/</link><author>Mike Melanson</author><category>official</category><category>dev</category><pubDate>Fri, 24 Jan 2025 17:00:08 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[So, you’ve heard about open source. You know it’s a great way to learn new skills, build up your resume, and give back to the developer community—but maybe you’re not sure where to begin.Getting involved in open source can raise all sorts of questions: Are there specific contribution guidelines? Or a certain level of involvement required before tackling issues? Don’t worry, we’ve got your back.This guide is packed with resources to help you go from curious newcomer to confident contributor. Whether it’s finding the perfect project, understanding how open source communities work, or submitting your first pull request, we’ll walk you through all of it. Let’s get started. 🚀1. Pick a project: start with what you knowFinding the right project to contribute to can make all the difference. Here are a few ways to get started: Choose a project you’re genuinely interested in. It’s easier to stay engaged and make meaningful contributions when you care about the work.2. Learn the project: know before you contributeJumping into an open source project without understanding its structure, rules, or community can feel like walking into a conversation without context. You might have the best intentions, but if you don’t know the norms of the project, your efforts might go unnoticed—or worse, be rejected. That’s why it’s so important to take some time to get acquainted with the project before contributing.Here’s how to read the room and get started the right way: The README is the project’s introduction. It explains why the project exists, what it does, and how to get started.   This file outlines how to contribute. It might include coding standards, submission guidelines, and how maintainers prefer to receive contributions.   A code of conduct (CoC) sets expectations for community behavior and provides information on how to report incidents. Having a CoC signals that a project values diverse perspectives and encourages respectful collaboration.   The license determines how the project’s code can be used, modified, and redistributed. If a project doesn’t have a license, it isn’t technically open source.   Many projects have forums, synchronous chat channels (like Slack or Discord), or mailing lists. Read the most recent posts to understand what’s going on.3. Find your place: contributions go beyond codeWhen you’re looking for that first contribution, don’t get too hung up on the idea that you need to contribute code. Non-code contributions are critical to a project’s success and a fantastic way to get involved. Not to mention, some maintainers don’t even want code contributions!Here are some of the most impactful ways you can contribute, without writing code: According to the 2021 Octoverse report, good documentation can boost productivity by 50%. Clarify confusing text, help write docs for new features, and share your knowledge.    A strong visual identity signals a project’s health and professionalism. But not everyone has the design skills to create visuals, logos, or improve the user experience.   🐞 Bug reports and testing: Bug reports and tests are invaluable to maintainers… though don’t forget to tell them what’s working great, too!   : Active communities foster collaboration and growth. Pitch in by helping answer questions, onboarding new contributors, or even moderating discussions.  Non-code contributions aren’t necessarily non-technical. Tasks like testing or documentation often require a deep understanding of the codebase.4. Make your first move: get involved and contributeSome of the most valuable contributions are small, incremental changes that improve the project bit by bit. Fixing typos, reporting bugs, or improving documentation can be a great way to get started.Here’s how to find your first contribution: Look for issues labeled “good first issue”. These are beginner-friendly tasks identified by maintainers. You can find them easily by adding /contribute to any project’s URL. For example, explore beginner-friendly tasks for the Node.js project at github.com/nodejs/node/contribute.   If you find something broken, file a detailed issue. Describe the problem, include steps to reproduce it, and explain what you expected to happen.Once you’ve identified your contribution, let maintainers know you’d like to work on an issue. They might offer guidance or confirm no one else is tackling it. From there, all that’s left is the actual work! Since you need to read the documentation anyway, it’s a perfect place to get started, along with submitting bug reports. These low-stakes contributions can help you get familiar with the project’s workflow.Open source relies on people like you to make it work, and every contribution matters, whether it’s writing documentation, submitting code, or reporting bugs. You’ll not only grow your skills and build connections, you’ll make an impact, too. So go on and get involved!See you in the pull requests! ✨]]></content:encoded></item><item><title>JavaScript Temporal is coming</title><link>https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/</link><author>brian-smith</author><category>MDN Blog</category><category>dev</category><category>official</category><category>frontend</category><enclosure url="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/featured.png" length="" type=""/><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><source url="https://developer.mozilla.org/en-US/blog/">MDN Blog</source><content:encoded><![CDATA[A new way to handle dates and times is being added to JavaScript. Let's take a look at Temporal, what problems it solves, the current state, and what you'll find in the new documentation about it on MDN.
]]></content:encoded></item><item><title>That’s a wrap: GitHub Innovation Graph in 2024</title><link>https://github.blog/news-insights/policy-news-and-insights/thats-a-wrap-github-innovation-graph-in-2024/</link><author>Kevin Xu</author><category>official</category><category>dev</category><pubDate>Thu, 23 Jan 2025 17:00:19 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[This is our first GitHub Innovation Graph data release in 2025 and our first data release after celebrating the Innovation Graph’s first birthday, so we’d like to reflect a bit on how things have gone so far and share our hopes and dreams for the future.Innovation Graph: a look backWe created the Innovation Graph to help make GitHub data more easily available to researchers, policymakers, and developers. Unfortunately, no one’s made a Dataset Success Graph yet to make dataset success metrics more easily available, so in lieu of that, here’s a chart of the github/innovationgraph repo’s stars, annotated with open source-related events to help explain the increases over time:Innovation Graph data releasesWe’ve released five quarters’ worth of additional data since the launch of the Innovation Graph (mostly like clockwork–thanks for your patience with this latest release, it’s been a busy couple of quarters!). With each release, we’ve seen modest increases in star count. Taken together with the results of a recently published working paper on the association between semantic versioning and adoption of software packages, we have to wonder if we’re leaving stars on the table by only bumping the patch version with each dataset release, as new major releases are apparently associated with significantly greater growth in adoption than patch (or minor) releases.Speaking of papers, here’s a roundup of some that caught our attention because they lined up with research questions about the open source ecosystem that were on our wishlist, or because they grappled with the impact of AI on software production, or both.The Value of Open Source SoftwareThe demand-side value of open source software was estimated to be $8.8 trillion.Generative AI and the Nature of WorkAccess to GitHub Copilot was found to cause open source maintainers to do proportionally more coding work and less project management work, as well as explore more lucrative programming languages. Check out our researcher Q&A with a couple of the co-authors for more details.Hoffmann, Manuel and Boysel, Sam and Nagle, Frank and Peng, Sida and Xu, Kevin, “Generative AI and the Nature of Work” (October 27, 2024). Harvard Business School Strategy Unit Working Paper No. 25-021, Harvard Business Working Paper No. No. 25-021, CESifo Working Paper Series No. 11479, Available at SSRN: https://ssrn.com/abstract=5007084 or http://dx.doi.org/10.2139/ssrn.5007084Measuring Software Innovation with Open Source Software Development DataMajor releases of OSS packages were found to count as a unit of innovation complementary to scientific publications, patents, and standards, offering applications for policymakers, managers, and researchers.Brown, Eva Maxfield, et al. “Measuring Software Innovation with Open Source Software Development Data.” arXiv preprint arXiv:2411.05087 (2024).Open Source Software Policy in Industry EquilibriumGlobal and domestic impacts of China and US government restrictions on, disincentives against, and subsidies for open source software contribution were estimated using simulations. Restrictions were found to be ineffective at increasing domestic investment into OSS; disincentives were found to increase costs both domestically and globally; and subsidies were found to both increase domestic investment and decrease costs globally.Gortmaker, Jeff. “Open Source Software Policy in Industry Equilibrium.” Working Paper (2024).Impact of the Availability of ChatGPT on Software Development: A Synthetic Difference in Differences Estimation using GitHub DataAvailability of ChatGPT was found to cause an increase in Git pushes per 100,000 inhabitants and increases in developer engagement across high-level languages like Python and JavaScript, while the effects on domain-specific languages like HTML and SQL varied. Learn more through our researcher Q&A with a couple of the co-authors.Quispe, Alexander, and Rodrigo Grijalba. “Impact of the Availability of ChatGPT on Software Development: A Synthetic Difference in Differences Estimation using GitHub Data.” arXiv preprint arXiv:2406.11046 (2024).From GitHub to GDP: A framework for measuring open source software innovationA methodology was developed to generate estimates of investment in OSS that are consistent with the U.S. national accounting methods used for measuring software investment. U.S. investment in OSS in 2019 was estimated to be $37.8 billion with a current-cost net stock of $74.3 billion.Gizem Korkmaz, J. Bayoán Santiago Calderón, Brandon L. Kramer, Ledia Guci, Carol A. Robbins, “From GitHub to GDP: A framework for measuring open source software innovation,” Research Policy, Volume 53, Issue 3, 2024, 104954, ISSN 0048-7333, https://doi.org/10.1016/j.respol.2024.104954.There were too many conferences to list, but we know that the Innovation Graph and other datasets on GitHub activity made a splash in presentations at the following conferences (in part because many of the above papers were presented at these conferences):There were also too many news articles to list, but we were happy to see  and  using Innovation Graph data and we’ll consider adding year-over-year percentage growth charts to the Innovation Graph site to save data journalists some time.We’re glad to have had the continued opportunity to contribute to the 2023 and 2024 editions of the WIPO Global Innovation Index and similarly, to the 2023 and 2024 Stanford AI Index Reports. And of course, we’re thrilled with the excitement and energy that the annual State of the Octoverse report generates around GitHub data each year, with its narrative structure that helps contextualize a wide array of topics.Our hopes and dreams for the futureIn a word: more. 2024 was our first full year of the Innovation Graph and it’s been nothing short of delightful to meet new collaborators, reconnect with existing ones, and continue building an evidence base to demonstrate the impact of open source. As we kick off 2025, we look forward to even more collaboration to come!]]></content:encoded></item><item><title>December Project Goals Update</title><link>https://blog.rust-lang.org/2025/01/23/Project-Goals-Dec-Update.html</link><author>David Wood and Niko Matsakis</author><category>dev</category><category>official</category><category>rust</category><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[Our big goal for this period was async closures, and we are excited to announce that work there is done! Stable support for async closures landed on nightly on Dec 12 and it will be included in Rust 1.85, which ships on Feb 20. Big kudos to compiler-errors for driving that.For our other goals, we made progress, but there remains work to be done:Return Type Notation (RTN) is implemented and we had a call for experimentation but it has not yet reached stable. This will be done as part of our 2025H1 goal.Async Functions in Traits (and Return Position Impl Trait in Trait) are currently not consided  compatible. We would eventually like to have first-class  support, but as an intermediate step we created a procedural macro crate  that can create wrappers that enable . We are planning a comprehensive blog post in 2025H1 that shows how to use this crate and lays out the overall plan for async functions in traits.Work was done to prototype an implementation for async drop but we didn't account for reviewing bandwidth. nikomatsakis has done initial reads and is working with PR author to get this done in 2025H1. To be clear though the scope of this is an experiment with the goal of uncovering implementation hurdles. There remains significant language design work before this feature would be considered for stabilization (we don't even have an RFC, and there are lots of unknowns remaining).We have had fruitful discussions about the trait for  but do not have widespread consensus, that's on the docket for 2025H1.We largely completed our goal to stabilize the language features used by the Rust for Linux project. In some cases a small amount of work remains. Over the last six months, we...stabilized the  macro to get the offset of fields; stabilization PR and reference updates are up, excluding the "output" feature.completed the majority of the work for arbitrary self types, which is being used by RfL and just needs documentation before stabilisationWe also began work on compiler flag stabilization with RFC 3716, which outlines a scheme for stabilizing flags that modify the target ABI.The final release of Rust 2024 is confirmed for February 20, 2025 as part of Rust 1.85. Rust 1.85 is currently in beta. Feedback from the nightly beta and crater runs has been actively addressed, with adjustments to migrations and documentation to enhance user experience.Big shout-outs to TC and  Eric Huss for their hard work driving this program forward.Over the last six months a number of internal refactorings have taken place that are necessary to support a  prototype.One refactoring is that we have changed how we represent const arguments in the compiler to allow for adding a separate representation for the kinds of const arguments that  will add.Another big refactoring is that we have changed the API surface for our representation of const arguments in the type system layer, there is no longer a way to evaluate a const argument without going through our general purpose type system logic. This was necessary to ensure that we correctly handle equality of the kinds of const arguments that  will support.With all of these pre-requisite refactorings completed, a feature gate has been added to the compiler (feature(min_generic_const_args)) that uses the new internal representation of const arguments. We are now beginning to implement the actual language changes under this feature gate.Over the course of the last six months...cargo semver-checks began to include generic parameters and bounds in its schema, allowing for more precise lints;cargo manifest linting was implemented and merged, allowing for lints that look at the cargo manifest;building on cargo manifest linting, the  lint was added, which identifies breakage caused by the removal of a package feature.In addition, we fleshed out a design sketch for the changes in rustdoc's JSON support that are needed to support cross-crate item linting. This in turn requires compiler extensions to supply that information to rustdoc.Progress was made on adding const traits and implementation in the compiler, with improvements being carefully considered.  was constified in rust#133237 and / in rust#133260.Over the last six months, we created a lang-team experiment devoted to this issue and spastorino began work on an experimental implementation. joshtriplett authored RFC 3680, which has received substantial feedback. The current work is focused on identifying "cheaply cloneable" types and making it easy to create closures that clone them instead of moving them.Alternatives to sandboxed build scripts are going to be investigated instead of continuing this project goal into 2025h1 - namely, declaratively configuring system dependencies with , using an approach similar to code-checker Cackle and its sandbox environment Bubblewrap, or fully-sandboxed build environments like Docker or Nix.Significant speedups have been achieved, reducing the slowest crate resolution time from over 120 seconds to 11 seconds, and decreasing the time to check all crates from 178 minutes to 71.42 minutes.Performance improvements have been made to both the existing resolver and the new implementation, with the lock file verification time for all crates reduced from 44.90 minutes to 32.77 minutes (excluding some of the hardest cases).Our pull request adding example searches and adding a search button has been added to the agenda for the rustdoc team next meeting.The  stabilization is now stable in version 1.84, with a new update blogpost published.Significant progress was made on bootstrap with . We're now able to compile rustc and cargo, enabling try-builds and perf runs.An optimisation for the  lint is open, benchmarked, and currently under review.Help is needed on any issue marked with , especially on issue #13714.Over the course of this goal, Nadrieril wrote and posted the never patterns RFC as an attempt to make progress without figuring out the whole picture, and the general feedback was "we want to see the whole picture". Next step will be to write up an RFC that includes a clear proposal for which empty patterns can and cannot be omitted. This is 100% bottlenecked on my own writing bandwidth (reach out if you want to help!). Work will continue but the goal won't be resubmitted for 2025h1.Amanda has made progress on removing placeholders, focusing on lazy constraints and early error reporting, as well as investigating issues with rewriting type tests; a few tests are still failing, and it seems error reporting and diagnostics will be hard to keep exactly as today.@lqd has opened PRs to land the prototype of the location-sensitive analysis. It's working well enough that it's worthwhile to land; there is still a lot of work left to do, but it's a major milestone, which we hoped to achieve with this project goal.A fix stopping cargo-script from overriding the release profile was posted and merged.Help is wanted for writing frontmatter support in rustc, as rustfmt folks are requesting it to be represented in the AST.RFC is done, waiting for all rustdoc team members to take a look before implementation can start.SparrowLii proposed a 2025H1 project goal to continue stabilizing the parallel front end, focusing on solving reproducible deadlock issues and improving parallel compilation performance.The team discussed solutions to avoid potential deadlocks, finding that disabling work-stealing in rayon's subloops is effective, and will incorporate related modifications in a PR.Progress on  continued despite a busy schedule, with a focus on improving suggestions and addressing architectural challenges.A new API was designed in collaboration with epage, aiming to align  more closely with  for easier contribution and integration.The project goal slate for 2025h1 has been posted as an RFC and is waiting on approval from project team leads.Another pull request was merged with only one remaining until a working MVP is available on nightly.Some features were removed to simplify upstreaming and will be added back as single PRs.Will start work on  feature of LLVM/Enzyme which allows Array of Struct and Struct of Array vectorisation.There's been a push to add a AMD GPU target to the compiler which would have been needed for the LLVM offload project.We have written and verified around 220 safety contracts in the verify-rust-std fork.3 out of 14 challenges have been solved.We have successfully integrated Kani in the repository CI, and we are working on the integration of 2 other verification tools: VeriFast and Goto-transcoder (ESBMC)There wasn't any progress on this goal, but building a community around a-mir-formality is still a goal and future plans are coming.The following goals have not received updates in the last month:]]></content:encoded></item><item><title>Attacks on Maven proxy repositories</title><link>https://github.blog/security/vulnerability-research/attacks-on-maven-proxy-repositories/</link><author>Michael Stepankin</author><category>official</category><category>dev</category><pubDate>Wed, 22 Jan 2025 18:00:51 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[As someone who’s been breaking the security of Java applications for many years, I was always curious about the supply chain attacks on Java libraries. In 2019, I accidentally discovered an arbitrary file read vulnerability on search.maven.org, a website that is closely tied to the Maven Central Repository. Maven Central is a place where most of the Java libraries are downloaded from and its security is paramount for all companies who develop in Java. If someone is able to infiltrate Maven Central and replace a popular library, they can get the key to the whole kingdom, as almost every large tech company uses Java.Last year, I committed myself to have a look at how Maven works under the hood. I decided to challenge myself: Perhaps I could find a way to get inside?In this blog post, I’ll reveal some intriguing vulnerabilities and CVEs that I’ve recently found in popular Maven repository managers. I’ll illustrate how specially crafted artifacts can be used to attack the repository managers that distribute them. Finally, I’ll demonstrate some exploits that can lead to pre-auth remote code execution and poisoning of the local artifacts.What is Maven and how does it work?Apache Maven is a popular tool to build Java projects. One of its widely adopted features allows you to resolve dependencies for the project. In a very simplistic scenario, a developer needs to create an pom.xml file that will list all the dependencies for their project, like this:<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
<version>3.3.0</version>
</dependency>
During the build process, the developer executes the maven console tool to download these dependencies for local use. For example, the widely used  command invokes Maven Artifact Resolver to make the following HTTP request to download this dependency:GET /org/springframework/boot/spring-boot-starter-web/3.3.0/spring-boot-starter-web-3.3.0.jar
Host: repo.maven.apache.org
Since the artifact can have its own dependencies, maven also fetches the /org/springframework/boot/spring-boot-starter-web/3.3.0/pom.xml file to identify and download all transitive dependencies.Downloaded dependencies are stored in the local file system (on MacOS it’s ) following the Maven Repository Layout.It’s important to note that Maven, as a console tool, is built with some security assumptions in mind:
  The purpose of Maven is to perform the actions defined in the supplied pom.xml, which commonly includes compiling and running the associated code and using plugins and dependencies downloaded from the configured repositories.  As such, the Maven security model assumes you trust the pom.xml and the code, dependencies, and repositories used in your build. If you want to use Maven to build untrusted code, it’s up to you to provide the required isolation.
Maven repositories (places from where artifacts are downloaded), on the other hand, are essentially web applications that allow uploading, storing, and downloading compiled artifacts. Their security is crucial, because if a hacker is able to publish or replace a commonly used artifact in them, all repository clients will execute the malicious code from this artifact.Maven Central and other public repositoriesBy default, Maven downloads all dependencies from https://repo.maven.apache.org, the address of the Maven Central repository. It is hardcoded in the default installation of Maven, but can be changed in settings. This website is hosted by Sonatype on AWS S3 buckets and served with Fastly CDN.Maven Central repository is public. Anybody can publish an artifact to it, but users’ publishing rights are restricted by the  ownership. So, only the company or user who owns the  groupId is allowed to publish artifacts with this groupId. To upload artifacts, publishers can use either a new Sonatype Central Portal or a legacy OSSRH (OSS Repository Hosting).Maven Central has a complex infrastructure hosted by Sonatype. At GitHub Security Lab, we only audit open source code, which means that Sonatype’s website is out of scope for us. Still, I realized that a lot of companies publish through the legacy OSSRH portal (https://oss.sonatype.org/), which is backed by the product Sonatype Nexus 2.Apart from Maven Central, there are also some other public Maven repositories:, Amazon S3 + Fastly Infrastructure is managed by SonatypeMaven Central OSSRH (synced with default), Sonatype Nexus 2 (behind a proxy)As you can see from the table, the biggest repositories are powered by two major products:  and . These products are (partially) open source and have free versions that you can test locally.So in my research, I decided to challenge myself with breaking the security of these repository managers. Additionally, I thought it would be good to also include a completely free open source alternative: .In-house Maven repository managersWhile downloading artifacts from Maven Central and other public repositories is free, many companies choose to use their own in-house Maven repository managers for additional benefits, such as:Ability to publish and use company’s private artifacts  Ability to restrict and get clarity on which libraries are used within the organization  Reduced bandwidth consumption by minimizing external network callsThese in-house repository managers are powered by the same open source products as the public repositories: Nexus, JFrog, and Reposilite.All of these products support multiple access roles. Typically an anonymous role allows you to download any artifact, a developer’s role can publish new artifacts, and an admin role can manage repositories, users, and enforce policies.Looking at Proxy mode from a security perspectiveApart from handling artifacts developed within a company, Maven repository managers are also often used as dedicated proxy servers for public Maven repositories. In this mode, when a repository manager handles a request to download an artifact, it first checks if the artifact is available locally. If not, it forwards this request to the upstream repository.The proxy mode is particularly interesting from the security perspective. First, because it allows even anonymous users to fetch their own artifact from the public repository and plant it in the local repository manager. Second, because in-house repository managers not only store and serve these artifacts, but also try to have a “sneak peek” into their content by expanding archives, analyzing “pom.xml” files, building dependency graphs, checking them for malware, and displaying their content in the Admin UI.This may introduce a second-order vulnerability when an attacker uploads a specially crafted artifact to the public repository first, and then uses it to attack the in-house manager. As someone who built DAST and SAST products in the past, I know firsthand that these types of issues are very hard to detect with automation, so I decided to have a look at the source code to manually identify some.Attacks on proxy Mode: Stored XSSArtifacts published to Maven repositories are normally the JAR archives that contain compiled java classes (with  file extension), but technically they can contain arbitrary data and extensions. All the repository managers I tested have their web admin interfaces listening on the same port as the application that serves the artifact’s content. So, what if an artifact’s pom.xml file contains some JavaScript inside?<?xml version="1.0" encoding="UTF-8"?>
<a:script xmlns:a="http://www.w3.org/1999/xhtml">
    alert(`Secret key: ${localStorage.getItem('token-secret')}`)
</a:script>
It turned out that at least two of the tested Repository managers (Reposilite and Sonatype Nexus 2) fall into this basic Stored XSS vulnerability. The problem lies in the fact that the artifact’s content is served via the same origin (protocol/host/port) as the Admin UI. If the artifact contains HTML content with javascript inside, the javascript is executed within the same origin. Therefore, if an authenticated user views the artifact’s content, the javascript inside can make authenticated requests to the Admin area, which can lead to the modification of other artifacts, and subsequently to remote code execution on users who download them.In case of the Reposilite vulnerability, an XSS payload can be used to access the browser’s local storage where the user’s password (aka “token-secret”) is located. That’s game over, as the same token can be used on another device to access the admin area.How to protect from that? Obviously, we cannot “escape” the special characters, as it would break the legitimate functionality. Instead, a combination of the following approaches can be used:“Content-Security-Policy: sandbox;” header when serving the artifact’s content. This means the resource will be treated as being from a special origin that always fails the same-origin policy (potentially preventing access to data storage/cookies and some JavaScript APIs). It’s an elegant solution to protect the Admin area, but it still allows HTML content rendering, leaving some opportunities for phishing.  “Content-Disposition: attachment” header. This will prevent the browser from displaying the content entirely, so it just saves it to the “Download” folder. This may affect the UX though.Archive expansion and path traversalAll the tested Repository managers support unpacking the artifact’s files on the server to serve individual files from the archive. Most of them use Java’s ZipInputStream interface, which allows you to do it in-memory only, without storing anything on disk, which makes it safe from path traversal vulnerabilities.Still, I was able to find one instance of this vulnerability in Reposilite’s support for JavaDoc files.CVE-2024-36116: Arbitrary file overwrite in ReposilitejarFile.entries().asSequence().forEach { file ->
    if (file.isDirectory) {
        return@forEach
    }

    val path = Paths.get(javadocUnpackPath.toString() + "/" + file.name)

    path.parent?.also { parent -> Files.createDirectories(parent) }
    jarFile.getInputStream(file).copyToAndClose(path.outputStream())
}.asSuccess<Unit, ErrorResponse>()
The  taken from the archive can contain path traversal characters, such as ‘/../../../anything.txt’, so the resulting extraction path can be outside the target directory.If the archive is taken from an untrusted source, such as Maven Central, an attacker can craft a special archive to overwrite any local file on the repository manager. In the case of Reposilite, this could lead to remote code execution, for example by placing a new plugin into the  directory. Alternatively, an attacker can overwrite the content of any other package.CVE-2024-36117: Arbitrary file read in ReposiliteAnother CVE I discovered in Reposilite was in the way the expanded javadoc files are served. Reposilite has the GET /javadoc/{repository}/<gav>/raw/<resource> route to find the file in the exploded archive and return its content to the user.In that case, the When a repository manager processes requests to download artifacts, it needs to map the incoming URL path value to the artifact’s coordinates: GroupId, ArtifactId, and Version (commonly known as GAV).The Maven documentation suggests using the following convention for mapping from URL path to GAV:/${groupId}/${artifactId}/${baseVersion}/${artifactId}-${version}-${classifier}.${extension}GroupId can contain multiple forward slashes, which are translated to dots while parsing. For instance, the following URL path:GET /org/apache/maven/apache-maven/3.8.4/apache-maven-3.8.4-bin.tar.gz HTTP/1.1will be translated to these coordinates:groupId: org.apache.maven
artifactId:apache-maven
version: 3.8.4:bin:tar.gz
classifier: bin
Extension: tar.gz
While this operation looks like a simple regexp matching, there is some room for misinterpretation, especially in how the URL decoding, path normalization, and control characters of the URL are handled.For instance, if the path contains any special url encoded characters, such as “?” (%3b) or “#” (%23), they will be decoded and considered as part of the artifact name:GET /com/company/artifact/1.0/artifact-1.0.jar%23/xyz/anything.any?isRemote=trueInterpreted by proxy and transferred to upstream as:/com/company/artifact/1.0/artifact-1.0.jar\#/xyz/anything.anyOn the upstream server however, everything after the hash sign will be parsed as hash properties. The path to artifact will be truncated to:/com/company/artifact/1.0/artifact-1.0.jar#/xyz/anything.anyEssentially, this would allow attackers to create files on the target proxy repository with arbitrary names and extensions, as long as their path starts with a predefined value, for example:This behavior affects almost every product I tested, but it’s hardly exploitable on its own, as no client would use an artifact with such a weird name.While testing this, I noticed that JFrog Artifactory also has a special handling for the semicolon character “;”. Artifactory considers everything in the path after semicolon as “path parameters”, but not a part of the artifact name.GET /com/company1/artifact1/1.0/artifact1-1.0.jar;/../../../../company2/artifact2/2.0/artifact2-2.0.jarWhen processing a request like that, JFrog considers artifact1-1.0.jar as the artifact name, but still forwards the full url to the upstream repository. Contrary to that, Nexus 3 and some public servers perform path normalization and reduce this path to /company2/artifact2/2.0/artifact2-2.0.jar, which is expected according to RFC 3986.In cases where Artifactory is configured to proxy an external repository, this behavior can lead to a severe vulnerability: artifact poisoning (CVE-2024-6915). Technically, it allows saving any HTTP response from the remote endpoint to an arbitrary artifact on the Artifactory instance.The straightforward way to exploit that would be to publish a malicious artifact into the upstream repository with any name, and then save it under a commonly used name on Artifactory, (for example, “spring-boot-starter-web”). Then, the next time a client fetches this commonly used artifact, Artifactory will serve malicious content of another package.In cases when an attacker is unable to publish anything into the upstream repository, it can still be potentially exploitable with an open redirect or a reflected/stored XSS on the upstream server. Artifactory does not check what is passed after “;/../”, so it can be not only an “artifact2-2.0.jar” but any relative URL path.The ultimate requirement for this exploitation is that the upstream server should perform a path normalization process to consume “/../” characters. Maven Central repository does not perform it, but several other public repositories such as Apache or JitPack do. Moreover, this vulnerability in JFrog Artifactory affects not only Maven repositories, but any other proxy types, such as npm or Docker repositories.Maven Central repo1.maven.orgApache repository.apache.orgPython Package Index (PyPI)GO package registry (gocenter.io)“✓” means path traversal is accepted by repository, “✗” – notExample CVE-2024-6915: Is it even or is it odd?To demonstrate its impact in my bug bounty report, I chose to use an Artifactory instance that proxies to npm. Npm has a different layout than Maven, but the core idea is the same: we just need to overwrite a package.json file with the content of another package. In the following request, we simply replace the package.json file of the is-even package with the content of the is-odd package.When I install this poisoned package from Artifactory, the npm client shows a warning that the name in the package.json file (is-odd) is different from the requested one (is-even), but as the downloaded file is properly formatted and contains the links to archive with the source code, the npm client still downloads and executes it.The npm client is designed with the assumption that it trusts the source. If the integrity of the source registry is compromised (which was the case for JFrog Artifactory), npm clients cannot really do anything to protect from such malicious artifacts. Even hash checksums can be bypassed if they are tampered with in Artifactory.When I reported this issue to the JFrog bug bounty program, it was assigned a critical severity and later awarded with a $5000 bounty. Since I’m doing this research as a part of my work at GitHub, we donated the full bounty amount to charity, specifically Cancer Research UK.Magic parameters for exploiting name confusion attacksBoth Nexus and JFrog support some URL query parameters for proxy repositories. In JFrog Artifactory, the following parameters are accepted:When attacking proxy repositories, these parameters may be applied on the proxy side, or “smuggled” into the upstream repository by using URL encoding. In both cases, they may alter how one or another repository processes the request, leaving options for potential exploitation.For example, by applying a  suffix to the path, we can trigger a local redirect to the same URL. By default, Artifactory does not perform path normalization for incoming requests, but with a redirect we can force the path normalization to be performed on the HTTP client side, instead of the server. It may help to perform a path traversal for name confusion attacks.Nexus 2 also supports a few parameters, but perhaps only these two are interesting for attackers:Along with the artifacts uploaded by the users, repository managers also store additional metadata, such as checksums, creation date, the user who uploaded it, number of downloads, etc. Most of this data is stored in the database, but some repository managers also store files in the same directory as artifacts. For instance, Nexus 2 stores the following files:/.meta/repository-metadata.xml – contains repository properties in XML format
/.index/nexus-maven-repository-index.properties/.index/nexus-maven-repository-index.gz/.nexus/tmp/<artifact>nx-tmp<random>.nx-upload –  temporary file name during artifact’s upload/.nexus/attributes/<artifact-name> – for every artifact, Nexus creates this json file with artifact’s metadataThe last file is the only one that Nexus prohibits access to. Indeed if you try to download or upload a file that starts with , Nexus rejects this request:At the same time, I figured out that we can circumvent this protection by using a different prefix (/nexus/service/local/repositories/test/content/ instead of /nexus/content/repositories/test/) and by using a double slash before the :Reading local attributes is probably not that interesting for attackers, but the same bug can be abused to overwrite them using PUT HTTP requests instead of GET. By default, Nexus does not allow you to update artifact’s content in its release repositories, but we can update the attributes of any  file:For exploitation purposes, I discovered a supported attribute that is particularly interesting: “contentGenerator”:”velocity”. If present, this attribute changes the way how the artifact’s content is rendered, enabling resolution of Velocity templates in the artifact’s content. So if we upload the ‘maven-metadata.xml’ file with the following content:And then reissue the previous PUT request to update the attributes, the content of the ‘maven-metadata.xml’ file will be rendered as a Velocity template.Sweet, as the velocity template I used in the example above triggers the execution of the java.lang.Runtime.getRuntime().exec("id") command.It’s not a real RCE unless it’s pre-authTo overwrite the metadata in the previous requests, I used a ‘PUT’ request method that requires the cookie of a user who has sufficient privileges to upload artifacts. This severely reduces the potential impact, as obtaining even a low-privileged account on the target repository might be a difficult task. Still, it wouldn’t be like myself if I didn’t try to find a way to exploit it without any authentication.One of the ways to achieve that would be combining this vulnerability with the stored XSS (CVE-2024-5083) in proxy repositories that I discovered earlier. Planting an XSS payload would not require any permissions on Nexus. Still, that XSS requires an administrator user to view an artifact, with a valid session, so the exploitation is still not that clean.Another way to trigger this vulnerability would be through a ‘proxy’ repository. If an attacker is able to publish an artifact into the upstream repository, it’s possible to exploit this vulnerability without any authentication on Nexus.You may reasonably assume that publishing an artifact with a Maven Group ID that starts with ‘.nexus/attributes’ may be unrealistic in popular upstream repositories like Maven Central, Apache Snapshots or JitPack. While I could not test this myself in their production systems, I noticed that one may publish an artifact with the group ID of ‘org.example’ and then force Nexus to save it as  with the same path traversal trick as in the name confusion attacks:GET /nexus/service/local/repositories/apache-snapshots/content//.nexus/attributes/%252e./%252e./com/sbt/ignite/ignite-bom/maven-metadata.xml
When processing this request, Nexus will decode the URL path to /.nexus/attributes/%2e./%2e./com/sbt/ignite/ignite-bom/maven-metadata.xml and forward it to the Apache Snapshots upstream repository. Then, Apache’s repository will (quite reasonably) perform URI normalization and return the content of the file. This would allow you to store an artifact with an arbitrary name from Apache Snapshots in the  directory.Apache Snapshots is enabled by default in the Nexus installation. Also, as I mentioned earlier, pulling artifacts from it does not require any permissions on Nexus—it can be done with a simple GET request without any cookies.A real attacker would probably try to publish their own artifact to the Apache Snapshots repository and therefore use it to attack all Nexus instances worldwide. Additionally, it’s possible to enumerate all the Apache user names and their emails. Perhaps some of their credentials can be found on websites that accumulate leaked passwords, but testing these kinds of attacks lies beyond the legal and moral scope of my research.As we can see, using repository managers such as Nexus, JFrog, and Reposilite in proxy mode can introduce an attack surface that is otherwise only available to authenticated users.All tested solutions not only store and serve artifacts, but also perform complex parsing and indexing operations on them. Therefore, a specially crafted artifact can be used to attack the repository manager that processes it. This opens a possibility for XSS, XXE, archive expansion, and path traversal attacks.The innate URL decoding mechanism along with special characters sparks parsing discrepancies. All repository managers parse URLs differently and cache proxied artifacts locally, which can lead to cache poisoning vulnerabilities, such as CVE-2024-6915 in JFrog, for example.The major public and private Maven repositories are powered by just a few partially open source solutions. Although these solutions are already backed by reputable companies with strong security teams and bug bounty programs, it’s still possible to find critical vulnerabilities in them.Lastly, these kinds of attacks are not even specific to Maven, but for all other dependency ecosystems, whether its NPM, Docker, RubyGems or anything else. I encourage every hacker to test this ‘proxy repository’ functionality in other products as well, as this may bring about many fruitful findings.]]></content:encoded></item><item><title>Modernizing legacy code with GitHub Copilot: Tips and examples</title><link>https://github.blog/ai-and-ml/github-copilot/modernizing-legacy-code-with-github-copilot-tips-and-examples/</link><author>Ari LiVigni</author><category>official</category><category>dev</category><pubDate>Wed, 22 Jan 2025 17:00:32 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[Let’s talk about legacy code—you know, the code that’s old enough to drive and definitely old enough to haunt your dreams. Maybe it’s a sprawling COBOL system or an early version of C++ written back when “Y2K” was a serious concern. Now, decades later, you’re being asked to “look into it” and modernize it (while wondering if you need to send a note to that person who retired a few years ago and may know how this thing works).This is where AI coding tools like GitHub Copilot can offer some much-needed assistance—and I’m speaking from personal experience. In my day-to-day work, I often help organizations in the public sector, finance, and retail to deploy GitHub Copilot, and one question I hear again and again is, “How can we use Copilot to modernize legacy systems?”In this article, I’ll walk you through just that using one of the most infamous old-school languages: COBOL. This isn’t because I’m a masochist—lots of critical systems still use COBOL. But while it still does what it’s supposed to do, there are a lot of integration and compatibility challenges (it  pretty old), and the number of developers who can read and write COBOL isn’t exactly what it used to be. All of this makes COBOL programs that much harder to modernize. In our example, we’ll take an account management system written in COBOL and modernize it into Node.js.Here’s something fun, too: Everything we share below can be done with any GitHub Copilot tier—including our free tier.With that in mind, I’ll show you how Copilot can help you navigate and modernize legacy code, including the common pain points like wrangling technical debt, migrating to modern frameworks, and dealing with that one cryptic variable named X. (Seriously, why is it always X?)So, grab your debugger and buckle up.The challenges of legacy codeLet’s face it: working with legacy code can feel like stepping into a time machine, except instead of a cool DeLorean, you’re handed a dusty, 10,000-line COBOL program with zero documentation, and a warning that, if this breaks, the entire payroll system goes downThe truth is, modernizing legacy code isn’t just about updating syntax or swapping out old libraries. You’re more than likely staring at technical debt, compatibility issues, and the knowledge that the people who originally wrote this code are happily retired or have long since left your organization.Here’s a closer look at the common pain points and challenges any developer dealing with legacy systems will likely face:: Legacy systems are often riddled with technical debt accumulated over years—or even decades. Hardcoded logic, sprawling dependencies, and “temporary” fixes that somehow became permanent all add up to a brittle, unmaintainable mess.
 Modernizing legacy systems doesn’t happen in isolation. These systems usually need to integrate with newer tech stacks, and that’s where things get messy. APIs may not line up, data formats might clash, and the whole thing starts feeling like a house of cards held together with duct tape.Data migration and compatibility: Legacy systems often come with outdated data formats or storage methods that make modernizing them a logistical headache. Converting and migrating this data while ensuring compatibility with modern systems is one of the trickiest parts of any project. It’s like trying to move a fragile vase across town in a bumpy truck—one wrong move and everything’s shattered. Modernizing legacy systems isn’t cheap. It takes time, money, and skilled engineers who are often already stretched thin. Add in organizational resistance—because, “hey, if it’s not broken, why fix it?”—and you’ve got a recipe for frustration.Performance and scalability limitations: Legacy systems often struggle with modern performance requirements due to hardware limitations, less efficient code, and application architectures.Security vulnerabilities: Here’s the thing about legacy code: it was often written long before today’s security standards even existed. Back then, “security” might have meant locking the server room door, not safeguarding against SQL injection or ransomware. As a result, legacy systems can be riddled with vulnerabilities that modern attackers are more than happy to exploit. The final boss of legacy code modernization? Testing. Every small change introduces the risk of breaking something elsewhere and you may not even know it until weeks later.All of this means that modernizing legacy systems isn’t just a nice-to-have—but that doesn’t make it any easier. And since you’re reading this article, there’s a good chance you’ve been asked to help modernize some legacy code.It may feel like a big ask, but for all those reasons above (and then some) it’s super important.How GitHub Copilot helps you refactor and modernize legacy codeModernizing legacy code can feel like an uphill battle, especially when you’re diving into unfamiliar languages like COBOL. But GitHub Copilot makes this process faster, smoother, and—dare I say—less painful.Before Copilot, I spent a lot of time on Google and Stack Overflow searching for information about whatever legacy language or system I was about to dive into. Don’t get me wrong, it was helpful, but it took a lot of time, and took me away from the code itself. With Copilot, I can stay in my IDE and my flow state while asking my AI programmer these questions instead. Nine times out of 10 it gives me the right answer, or points me in the right direction. That speed—and not having to break out of my flow—is really what’s so impactful for me.Here are a few best practices I follow when using Copilot to modernize legacy code:Use slash commands in Copilot Chat These are shortcut commands you can use to explain code, generate tests, fix tests that may be failing, and more. They’re incredibly helpful when it comes to performing simple or complex functions—just highlight the code you want to perform an action on, and then enter the slash command in Copilot Chat to get started. Some of the slash commands I find most helpful include:
 explains how the code in your active editor works    generates unit tests for the selected code   finds and fix failing tests   finds and fix general problems in the selected codeTry chat participants to scope your prompts In Copilot Chat, you can manually use a number of chat participants—or AI domain experts—to perform specific functions in your IDE, workspace, Azure instance, or terminal. While Copilot Chat will infer chat participants from your prompts, manually accessing them can help you hone in on specific functions. I typically use the chat participant  (which you’ll see in our example below) to consider my larger codebase. Other chat participants include:  when you need help with VS Code commands and features,  for shell-based commands and debugging, and  for any help with Azure you may need.
Use chat variables to improve prompt context This is something you should always use (I do, as you’ll soon see). Chat variables help you include specific context in your prompts and are incredibly useful for focusing Copilot on a specific outcome you’re trying to achieve. These include: to include a specific file for context (my personal favorite)    to get information about your current Git repository to get the visibly source code in the active editor to get the current selection in the active editor to get the active terminal’s last run command to get the active terminal’s selectionYou can also type  in Copilot Chat to see a full list of chat variables.Using GitHub skills for Copilot Copilot’s GitHub-specific skills expand the type of information Copilot can provide. To access these skills in Copilot Chat in VS Code, include  in your question.
When you add  to a question, Copilot dynamically selects an appropriate skill, based on the content of your question. You can also explicitly ask Copilot Chat to use a particular skill. You can do this in two ways:Use natural language to ask Copilot Chat to use a skill. For example, @github Search the web to find the latest GPT4 model from OpenAI.To specifically invoke a web search you can include the  variable in your question. For example, @github #web What is the latest LTS of Node.js?: Legacy codebases can be massive and intimidating, so don’t try to refactor everything at once. Focus on individual functions or modules before diving into the entire system. Use Copilot to streamline these smaller refactors—it’ll help you build momentum and confidence as you go.
: Before changing a single line of code, ensure you have tests that validate the current behavior (a practice known as test-driven development). You can use Copilot to generate unit tests that validate the original behavior via the slash command  in Copilot Chat in your IDE. These tests act as a safety net, ensuring that any changes you make won’t accidentally break something critical.: Version control is essential when refactoring legacy systems. Use GitHub’s built-in tools like branches and pull requests to test and review your changes in isolated environments. This way, you can safely iterate without disrupting the main codebase.Always review Copilot’s suggestions: Copilot’s suggestions are incredibly accurate and insightful, but human oversight is key. Always review the changes Copilot makes to ensure they align with your system’s context and coding standards.Modernizing a legacy COBOL application with GitHub Copilot: A practical exampleOk, let’s jump into our code. Like I said earlier, we’ll be looking at an account management system written in COBOL and modernizing it into Node.js.Notably, every step I’m going to walk you through below works in all tiers of GitHub Copilot—including the Free version. And just in case you’re wondering, I use OpenAI’s GPT-4o model (in the Free tier of Copilot you can choose between that and Anthropic’s Claude 3.5 Sonnet).This repository includes three key files—data.cob, main.cob, and operations.cob—that work together to perform various operations like crediting, debiting, viewing the account balance balance, and even exiting the program.An important note: since Copilot is powered by generative AI models, which are nondeterministic in terms of outputs, you will likely get slightly different responses even if you follow my step-by-step instructions.You have two options here: you can either install a COBOL compiler (I recommend the open source GnuCOBOL compiler) or open up the terminal in a GitHub Codespaces environment (this is what I do since it’s a bit simpler). Copilot will work in either.To install GnuCOBOL, use this brew command when using MacOS:If you decide to use Codespaces or an Ubuntu Linux system, you need to run the following command to install the COBOL compiler in your instance:sudo apt-get update && \
sudo apt-get install gnucobol
From there, you need to compile the code, link the files, and create an executable with the following command:cobc -x main.cob operations.cob data.cob -o accountsystem
Step 2: Explain the files and codeWhile I don’t deal with COBOL in my day-to-day work normally, my next step is simple: find out what on Earth these files are doing. That’s where Copilot enters the picture. It’s great at helping explain (and document) code, including legacy code like we’re working with now.Here’s the first prompt I use in Copilot Chat:/explain #file:main.cob #file:operations.cob #file:data.cob Can you please create a high level overview of the app. It will be great if you can explain each file in detail and how they are linked.
Note how I use a combination of a slash command, pointing to specific files using the  chat variable, and a prompt. This helps Copilot focus on exactly what I want explained by pointing it at the right files.Copilot returns around a rather lengthy explanation that I’ll excerpt here:Sure! Here's a high-level overview of the COBOL application, along with detailed explanations of each file and how they are linked together.
High-Level Overview
This COBOL application simulates an account management system. It allows users to view the current balance, credit the account, debit the account, and exit the application. The application is divided into three main COBOL source files: main.cob, operations.cob, and data.cob.
This alone gives me a good starting point—and Copilot’s full response offers far more detail on the logic of each file, the links between them, and the different sections of code.Step 3: Chart out the data flow between the filesBut I still need a better idea of how these three files are working together in our executable. This is where Copilot, once again, proves to be incredibly helpful with the  command.A little explanation first: entering  in Copilot Chat lets you ask questions about your codebase including finding code, making plans for complex edits, explaining higher-level concepts, and more (VS Code has some good documentation on using @workspace).For this exercise, I use  to prompt Copilot for a data flow diagram while asking it to output it in a Mermaid diagram so I can clearly visualize it.@workspace can you create a sequence diagram of the app showing the data flow of the app. Please create this in mermaid format so that I can render this in a markdown file.
Copilot then outputs a full sequence diagram in Mermaid that shows me exactly how this application works, rendered as a Markdown file. This looks like the following:This helps me visually understand how this application is functioning and how the data is flowing through the program.Step 4: Generate a test planIn our demo repository, you can see that we currently have no tests. That’s not great, so the next step is to generate tests.We’ll use the  command again with the following prompt:@workspace The current Cobol app has no tests. Can you please create a test plan of current business logic that I can use to validate with business stakeholders about the current implementation.

Later I would like to use this test plan to create unit and integration tests in a node.js app. I am in the middle of transforming the current Cobol app to a node.js app.

The test plan should include the following:

1. Test Case ID
2. Test Case Description
3. Pre-conditions
4. Test Steps
5. Expected Result
6. Actual Result
7. Status (Pass/Fail)
8. Comments

Please create the test plan in a markdown table format. The test plan should cover all the business logic in the current Cobol app.
Copilot will then generate a test plan I can use. But in my instance, I had to take one more step: ask Copilot to generate the markdown file format for my test plan to insert it as a new file.Convert this to markdown syntax to insert as a new file
Step 5: Convert the files from COBOL to Node.jsNow, it’s time to convert our three COBOL files into a Node.js application. Here, it’s important to use good prompting practices: provide context, give clear directions, and be specific about the outcome you want to achieve.We start by creating a Node.js project directory:mkdir node-accounting-app
cd node-accounting-app
From there, we iteratively convert the files main.cob, operations.cob, and data.cob into Node.js files. I’ll use the code in main.cob as the example here. Here’s my prompt for reference:Can you convert code in #file:main.cob to node.js?
Copilot then generates the following answer:Sure! Here is the equivalent Node.js code for the 

main.cob

 file. This code will simulate the same account management system functionality.

### COBOL Code: 

main.cob


```COBOL
IDENTIFICATION DIVISION.
       PROGRAM-ID. MainProgram.

       DATA DIVISION.
       WORKING-STORAGE SECTION.
       01  CONTINUE-FLAG      PIC X VALUE 'Y'.
       01  USER-CHOICE        PIC 9.
       01  FINAL-BALANCE      PIC 9(6)V99 VALUE 1000.00.

       PROCEDURE DIVISION.
           PERFORM UNTIL CONTINUE-FLAG = 'N'
               DISPLAY "1. View Balance"
               DISPLAY "2. Credit Account"
               DISPLAY "3. Debit Account"
               DISPLAY "4. Exit"
               ACCEPT USER-CHOICE

               EVALUATE USER-CHOICE
                   WHEN 1
                       CALL 'Operations' USING 'TOTAL'
                   WHEN 2
                       CALL 'Operations' USING 'CREDIT'
                   WHEN 3
                       CALL 'Operations' USING 'DEBIT'
                   WHEN 4
                       MOVE 'N' TO CONTINUE-FLAG
                       DISPLAY "Exiting the program. Goodbye!"
                   WHEN OTHER
                       DISPLAY "Invalid choice. Please try again."
               END-EVALUATE
           END-PERFORM
           STOP RUN.
```

### Node.js Code: `main.js`
```javascript
const readline = require('readline');

let continueFlag = 'Y';
let finalBalance = 1000.00;

const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout
});

function displayMenu() {
    console.log("1. View Balance");
    console.log("2. Credit Account");
    console.log("3. Debit Account");
    console.log("4. Exit");
}

function handleUserChoice(choice) {
    switch (choice) {
        case '1':
            viewBalance();
            break;
        case '2':
            creditAccount();
            break;
        case '3':
            debitAccount();
            break;
        case '4':
            continueFlag = 'N';
            console.log("Exiting the program. Goodbye!");
            rl.close();
            break;
        default:
            console.log("Invalid choice. Please try again.");
            promptUser();
            break;
    }
}

function viewBalance() {
    console.log(`Current balance: ${finalBalance.toFixed(2)}`);
    promptUser();
}

function creditAccount() {
    rl.question("Enter credit amount: ", (amount) =&gt; {
        amount = parseFloat(amount);
        if (!isNaN(amount) &amp;&amp; amount &gt; 0) {
            finalBalance += amount;
            console.log(`Amount credited. New balance: ${finalBalance.toFixed(2)}`);
        } else {
            console.log("Invalid amount. Please try again.");
        }
        promptUser();
    });
}

function debitAccount() {
    rl.question("Enter debit amount: ", (amount) =&gt; {
        amount = parseFloat(amount);
        if (!isNaN(amount) &amp;&amp; amount &gt; 0) {
            if (finalBalance &gt;= amount) {
                finalBalance -= amount;
                console.log(`Amount debited. New balance: ${finalBalance.toFixed(2)}`);
            } else {
                console.log("Insufficient funds for this debit.");
            }
        } else {
            console.log("Invalid amount. Please try again.");
        }
        promptUser();
    });
}

function promptUser() {
    if (continueFlag === 'Y') {
        displayMenu();
        rl.question("Select an option: ", handleUserChoice);
    }
}

promptUser();
```

This Node.js code replicates the functionality of the COBOL main.cob file, providing a simple command-line interface for viewing the balance, crediting the account, debiting the account, and exiting the application.
Simple enough, right? Of course, we need to review this code—it’s always a requirement to trust but verify when using AI coding tools. After review, I execute similar prompts for the other two files.Once we finish translating each COBOL file into Node.js, we need to link all the new files to initialize, install, and run the Node application. I use the following prompt to do that:link all node.js files to work together in one accounting application and with npm commands to initialize and install and to run the app.
Copilot returns the needed program logic while linking everything together, so I just need to execute the main.js file after initializing npm.Step 6: Generate unit and integration testsFor our last step, we generate unit and integration tests to ensure all of our new Node.js code works. Here, I again use @workspace in Copilot Chat. My prompt looks like this:@workspace I would like to create unit and integration tests cases form the test plan mentioned in #file:TESTPLAN.md file The node.js code is in node-accounting-app folder and I am looking to generate tests for #file:operations.js file. Use a popular testing framework and also provide all the dependencies required to run the tests.
Copilot generates a number of tests in Jest, a JavaScript testing framework, with a full, step-by-step guide on how to implement and run them.In Copilot’s own words: “This test plan covers the main functionalities of the COBOL application, including viewing the balance, crediting the account, debiting the account, and exiting the application. The test cases are implemented using Jest, a popular testing framework for Node.js. Validate this test plan with the business stakeholders to ensure it meets the business requirements. Once validated, you can use this plan to create corresponding unit tests and integration tests for the Node.js application.”Remember, you can always ask Copilot to generate tests for edge cases, or even prompt it to find these cases by asking if there are any additional tests you—or even it—may have forgotten.Modernizing legacy code doesn’t have to be a headache. With GitHub Copilot, you have a powerful tool to accelerate the process, stay in your flow, and focus on what matters most: writing great code. Here’s what to keep in mind as you embark on your modernization journey:Prompting best practices are key: The quality of your prompts determines the quality of Copilot’s suggestions. Provide clear context, break down complex tasks into smaller steps, provide examples, and give Copilot specific goals to work toward. This makes your workflow smoother and your results more precise.  Modernizing code without scrolling through Google: Copilot reduces the grind of converting, refactoring, and rewriting legacy code by bringing answers straight to your IDE. Whether it’s migrating COBOL to Node.js or optimizing outdated logic, Copilot gets you there faster—no endless Googling required.  There’s a free tier of GitHub Copilot: Don’t forget—GitHub Copilot has a free version available for anyone to use, including you. All you need is a free GitHub account; no subscription needed. Refactoring legacy code might feel like a daunting challenge, but it’s also an opportunity to learn, improve, and build something better. With GitHub Copilot by your side, you’re equipped to handle even the most stubborn systems. So, dive in, experiment with prompts, and start turning that dusty code into something modern and efficient. You’ve got this—and Copilot’s got your back.Start using GitHub Copilot for free
Our free version of GitHub Copilot is included by default in personal GitHub accounts and VS Code to help you start new projects, manage existing repositories, and more.]]></content:encoded></item><item><title>Rust 2024 in beta channel</title><link>https://blog.rust-lang.org/2025/01/22/rust-2024-beta.html</link><author>TC &amp; Eric Huss</author><category>dev</category><category>official</category><category>rust</category><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><source url="https://blog.rust-lang.org/">Dev - Rust Blog</source><content:encoded><![CDATA[The next edition, Rust 2024, has entered the beta channel.  It will live there until 2025-02-20, when Rust 1.85 and Rust 2024 will be released as stable.We're really happy with how Rust 2024 has turned out, and we're looking forward to putting it in your hands.You can get a head start in preparing your code for the new edition, and simultaneously help us with final testing of Rust 2024, by following these steps within a project:Run cargo +beta fix --edition.Set  and, if needed, , in .Run , address any remaining warnings, and then run other tests.More details on how to migrate can be found here and within each of the chapters describing the changes in Rust 2024.  For more on the changes themselves, see the Edition Guide.If you encounter any problems or see areas where we could make the experience better, tell us about it by filing an issue.]]></content:encoded></item><item><title>Seven years of open source: A more secure and diverse ecosystem</title><link>https://github.blog/news-insights/seven-years-of-open-source-a-more-secure-and-diverse-ecosystem/</link><author>Kenyatta Forbes</author><category>official</category><category>dev</category><pubDate>Tue, 21 Jan 2025 17:01:45 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[Seven years ago, we surveyed 5,500 open source contributors to understand the people who use, build, and maintain open source software. This year, we repeated the effort to understand how the open source ecosystem has evolved in the face of the seismic technological and social shifts we’ve seen since 2017. We received 8,400 responses from visitors to open source repositories, and we’re once again sharing the insights with the community.With AI, funding, innersource, security, and innovation taking center stage, the 2024 survey surfaces the issues shaping the future of open source while also showing how far we’ve come. By tracking long-term trends and exploring emerging topics, we aim to provide a comprehensive and actionable perspective on the forces influencing open source today.Read the full survey resultsSo, what are some of the key trends we’ve identified?In 2024, we introduced questions centered on secure by design to gain insights into how developers assess open source projects for security practices. 82% of respondents said they consider secure by design practices important when adopting open source, and 62% prioritize security when choosing where to contribute.The use of AI tools is on the rise with 72% of participants saying they use AI tools like GitHub Copilot for coding or documentation.Diversity within the open source community has steadily increased over the years, bringing with it a broader range of perspectives and backgrounds to the community. Respondents identifying as ethnic minorities rose from 13% in 2017 to 18% in 2024, and those identifying as immigrants grew from 27% to 31%.Those are just a few points we’ve gleaned from this year’s data. Visit opensourcesurvey.org and explore the data to see what other insights you can find regarding the changing community attitudes around privacy, harassment, funding, and community health!Read the full survey resultsSupporting Open Source TogetherWith over 75 million new GitHub users since our last survey, the open source community is more resilient and diverse than ever. Together, we can shape a secure and inclusive open source future.]]></content:encoded></item><item><title>Spotlight on SIG Architecture: Enhancements</title><link>https://kubernetes.io/blog/2025/01/21/sig-architecture-enhancements/</link><author></author><category>dev</category><category>official</category><category>k8s</category><category>devops</category><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Dev - Kubernetes Blog</source><content:encoded><![CDATA[This is the fourth interview of a SIG Architecture Spotlight series that will cover the different
subprojects, and we will be covering SIG Architecture:
Enhancements.In this SIG Architecture spotlight we talked with Kirsten
Garrison, lead of the Enhancements subproject.The Enhancements subprojectFrederico (FSM): Hi Kirsten, very happy to have the opportunity to talk about the Enhancements
subproject. Let's start with some quick information about yourself and your role.: I’m a lead of the Enhancements subproject of SIG-Architecture and
currently work at Google. I first got involved by contributing to the service-catalog project with
the help of Carolyn Van Slyck. With time, I joined the Release
team,
eventually becoming the Enhancements Lead and a Release Lead shadow. While on the release team, I
worked on some ideas to make the process better for the SIGs and Enhancements team (the opt-in
process) based on my team’s experiences. Eventually, I started attending Subproject meetings and
contributing to the Subproject’s work.FSM: You mentioned the Enhancements subproject: how would you describe its main goals and areas of
intervention?FSM: The improvement of the KEP process was (and is) one in which SIG Architecture was heavily
involved. Could you explain the process to those that aren’t aware of it?: Every release, the SIGs let the
Release Team know which features they intend to work on to be put into the release. As mentioned
above, the prerequisite for these changes is a KEP - a standardized design document that all authors
must fill out and approve in the first weeks of the release cycle. Most features will move
through 3
phases:
alpha, beta and finally GA so approving a feature represents a significant commitment for the SIG.The KEP serves as the full source of truth of a feature. The KEP
template
has different requirements based on what stage a feature is in, but it generally requires a detailed
discussion of the design and the impact as well as providing artifacts of stability and
performance. The KEP takes quite a bit of iterative work between authors, SIG reviewers, api review
team and the Production Readiness Review team before it is approved. Each set of reviewers is
looking to make sure that the proposal meets their standards in order to have a stable and
performant Kubernetes release. Only after all approvals are secured, can an author go forth and
merge their feature in the Kubernetes code base.FSM: I see, quite a bit of additional structure was added. Looking back, what were the most
significant improvements of that approach?: In general, I think that the improvements with the most impact had to do with focusing on
the core intent of the KEP. KEPs exist not just to memorialize designs, but provide a structured way
to discuss and come to an agreement about different facets of the change. At the core of the KEP
process is communication and consideration.To that end, some of the significant changes revolve around a more detailed and accessible KEP
template. A significant amount of work was put in over time to get the
k/enhancements repo into its current form -- a
directory structure organized by SIG with the contours of the modern KEP template (with
Proposal/Motivation/Design Details subsections). We might take that basic structure for granted
today, but it really represents the work of many people trying to get the foundation of this process
in place over time.As Kubernetes matures, we’ve needed to think about more than just the end goal of getting a single
feature merged. We need to think about things like: stability, performance, setting and meeting user
expectations. And as we’ve thought about those things the template has grown more detailed. The
addition of the Production Readiness Review was major as well as the enhanced testing requirements
(varying at different stages of a KEP’s lifecycle).: We’re currently working on two things:Creating a Process KEP template. Sometimes people want to harness the KEP process for
significant changes that are more process oriented rather than feature oriented. We want to
support this because memorializing changes is important and giving people a better tool to do so
will only encourage more discussion and transparency. While our template changes aim to be as non-disruptive as possible, we
believe that it will be easier to track and communicate those changes to the community better with
a versioned KEP template and the policies that go alongside such versioning.Both features will take some time to get right and fully roll out (just like a KEP feature) but we
believe that they will both provide improvements that will benefit the community at large.FSM: You mentioned improvements: I remember when project boards for Enhancement tracking were
introduced in recent releases, to great effect and unanimous applause from release team members. Was
this a particular area of focus for the subproject?: The Subproject provided support to the Release Team’s Enhancement team in the migration away
from using the spreadsheet to a project board. The collection and tracking of enhancements has
always been a logistical challenge. During my time on the Release Team, I helped with the transition
to an opt-in system of enhancements, whereby the SIG leads "opt-in" KEPs for release tracking. This
helped to enhance communication between authors and SIGs before any significant work was undertaken
on a KEP and removed toil from the Enhancements team. This change used the existing tools to avoid
introducing too many changes at once to the community. Later, the Release Team approached the
Subproject with an idea of leveraging GitHub Project Boards to further improve the collection
process. This was to be a move away from the use of complicated spreadsheets to using repo-native
labels on k/enhancement issues and project boards.FSM: That surely adds an impact on simplifying the workflow...: Removing sources of friction and promoting clear communication is very important to the
Enhancements Subproject. At the same time, it’s important to give careful consideration to
decisions that impact the community as a whole. We want to make sure that changes are balanced to
give an upside and while not causing any regressions and pain in the rollout. We supported the
Release Team in ideation as well as through the actual migration to the project boards. It was a
great success and exciting to see the team make high impact changes that helped everyone involved in
the KEP process!FSM: For those reading that might be curious and interested in helping, how would you describe the
required skills for participating in the sub-project?: Familiarity with KEPs either via experience or taking time to look through the
kubernetes/enhancements repo is helpful. All are welcome to participate if interested - we can take
it from there.FSM: Excellent! Many thanks for your time and insight -- any final comments you would like to
share with our readers?: The Enhancements process is one of the most important parts of Kubernetes and requires
enormous amounts of coordination and collaboration of people and teams across the project to make it
successful. I’m thankful and inspired by everyone’s continued hard work and dedication to making the
project great. This is truly a wonderful community.]]></content:encoded></item><item><title>How we evaluate AI models and LLMs for GitHub Copilot</title><link>https://github.blog/ai-and-ml/generative-ai/how-we-evaluate-models-for-github-copilot/</link><author>Connor Adams</author><category>official</category><category>dev</category><pubDate>Fri, 17 Jan 2025 18:00:03 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[There are so many AI models to choose from these days. From the proprietary foundation models of OpenAI, Google, and Anthropic to the smaller, more open options from the likes of Meta and Mistral. It’s tempting to hop immediately to the latest models. But just because a model is newer doesn’t mean it will perform better for your use case.We recently expanded the models available in GitHub Copilot by adding support for Anthropic’s Claude 3.5 Sonnet, Google’s Gemini 1.5 Pro, and OpenAI’s o1-preview and o1-mini. While adding models to GitHub Copilot, performance, quality, and safety was always kept top of mind. In this article, we’ll share some of the GitHub Copilot team’s experience evaluating AI models, with a focus on our offline evaluations—the tests we run  making any change to our production environment. Hopefully our experience will help guide your own evaluations.What is AI model evaluation?AI models are systems that combine code, algorithms, and training data to simulate human intelligence in some way. GitHub Copilot gives users the choice of using a number of AI models trained on human language, which are known as large language models, or LLMs. OpenAI’s models are some of the most well-known LLMs due to ChatGPT’s popularity, though others such as Claude, Gemini, and Meta’s Llama models are increasingly popular as well.AI model evaluation is the assessment of performance, quality, and safety of these models. Evaluations can include both automated tests and manual use. There are a number of frameworks and benchmarking tools available, but we generally build our own evaluation tools.Automated tests enable evaluation at scale. You can see how well a model performs on a large number of tasks. The downside is that you tend to need some objective criteria to evaluate the output. Manual testing enables more subjective evaluations of output quality and accuracy, but is more time intensive. Combining the two approaches enables you to get a sense of the subjective quality of responses without relying entirely on anecdotal evidence or spending an enormous amount of time manually evaluating thousands upon thousands of responses.Automating code quality testsWe run more than 4,000 offline tests, most of them as part of our automated CI pipeline. We also conduct live internal evaluations, similar to canary testing, where we switch a number of Hubbers to use a new model. We test all major changes to GitHub Copilot this way, not just potential new models.We’ll focus here on our offline tests. For example, we evaluate potential models by their ability to evaluate and modify codebases. We have a collection of around 100 containerized repositories that have passed a battery of CI tests. We modify those repositories to fail the tests, and then see whether the model can modify the codebase to once again pass the failing tests.We generate as many different scenarios as we can in different programming languages and frameworks. We’re also constantly increasing the number of tests we run, including using multiple different versions of the languages we support. This work takes a lot of time, but it’s the best way we have to evaluate a model’s quality.GitHub Copilot does more than generate code. Copilot Chat can answer questions about code, suggest various approaches to problem solving, and more. We have a collection of more than 1,000 technical questions we use to evaluate the quality of a model’s chat capabilities. Some of these are simple true-or-false questions that we can easily evaluate automatically. But for more complex questions, we use another LLM to check the answers provided by the model we’re evaluating to scale our efforts beyond what we could accomplish through manual testing.We use a model with known good performance for these purposes to ensure consistent evaluations across our work. We also routinely audit the outputs of this LLM in evaluation scenarios to make sure it’s working correctly. It’s a challenge to ensure that the LLM we use for evaluating answers are aligned with human reviewers and perform consistently over many requests.We also run these tests against our production models every day. If we see degradation, we do some auditing to find out why the models aren’t performing as well as they used to. Sometimes we need to make changes, for example modify some prompts, to get back to the quality level we expect.One of the cool things about our setup is that we can test a new model without changing the product code. We have a proxy server built into our infrastructure that the code completion feature uses. We can easily change which API the proxy server calls for responses without any sort of change on the client side. This lets us rapidly iterate on new models without having to change any of the product code.All of these tests run on our own custom platform built primarily with GitHub Actions. Results are then piped in and out of systems like Apache Kafka and Microsoft Azure, and we leverage a variety of dashboards to explore the data.Making the call to adopt or notThe big question is what to do with the data we’ve collected. Sometimes a decision is straightforward, such as when a model does poorly across the board. But what if a model shows a substantial bump in acceptance rates, but also increases latency?There are sometimes inverse relationships between metrics. Higher latency might actually lead to a higher acceptance rate because users might see fewer suggestions total.GitHub’s goal is to create the best quality, most responsible AI coding assistant possible, and that guides the decision we have to make about which models to support within the product. But we couldn’t make them without the quality data our evaluation procedures provide. Hopefully we’ve given you a few ideas you can apply to your own use cases.Build generative AI applications with GitHub ModelsGitHub Models makes it simple to use, compare, experiment, and build with AI models from OpenAI, Cohere, Microsoft, Mistral, and more.]]></content:encoded></item><item><title>Supporting the next generation of developers</title><link>https://github.blog/developer-skills/github-education/supporting-the-next-generation-of-developers/</link><author>Morgan Ersery</author><category>official</category><category>dev</category><pubDate>Fri, 17 Jan 2025 17:00:33 +0000</pubDate><source url="https://github.blog/">Dev - Github Blog</source><content:encoded><![CDATA[We believe that software is at the center of human progress and that all software builds on the work of others, especially open source, through an interconnected community of developers. We need to level the playing field to spark innovation on a global level, and that includes helping early developers get the best start possible. We believe that all developers can solve problems, innovate, and create collaboratively through code. This is why we aspire to empower 1 billion individuals to become developers, and work to help break down barriers to entry for those who wish to join the field and participate in the community.To achieve this goal, it’s important to provide a safe, inclusive environment for first time developers of all ages. GitHub Education supports the next generation of developers by providing free access to tools like the GitHub Student Developer Pack and GitHub Copilot to ensure they have everything they need to start building, including the most up-to-date industry standard tooling. More than 453,000 GitHub Education users were first-time contributors to open projects on the platform over the past year. They’re following in the footsteps of 7 million students and teachers who also used GitHub at the beginning of their coding journey.Joining forces with Hack ClubGitHub Education is proud to work with Hack Club, one of the largest global networks of highly technical youth, to introduce the next generation of developers to open source. Hack Club enables 39,400 teen hackers from around the world to code together. Through open source contributions, students learn valuable skills and join a vibrant community that fosters innovation, creativity, and collaboration. As part of our partnership, GitHub provided swag prizes, dedicated promotion, and operational funding to support Hack Club’s Summer Arcade, which reached 19,000 high school students, who shipped 2,000 projects and logged over 135,000 hours of coding! If you’re a parent or teacher looking to encourage your teen to explore the world of coding and open source, Hack Club’s High Seas is the next place where students can ship projects with friends.What makes Hack Club’s programs even more special is that they’re organized by teens who understand what makes a program exciting and engaging to their peers. By joining, teens can make friends, work on cool projects, and earn rewards along the way. With 400+ Hack Clubs globally, access barriers are being shattered: 40% of attendees at in-person events are girls, 30% of the Hack Club community are girls, and 50% of the HCB (fiscal sponsorship) organizations are female-led.Hack Club at GitHub UniverseAs you may have seen at this year’s GitHub Universe, our Chief Operating Officer Kyle Daigle’s Universe Day 2 Keynote was all about how collaborators in our community, from the enterprise to open source developer, student to professional, play a critical role in the developer ecosystem. Highlighted in this talk were projects created by Hack Club teens including Acon Lin, an 18-year-old Hack Clubber who built Summer Arcade joined Kyle on stage to launch , the next project to engage young tech leaders and spark them to ship projects and win prizes.What’s next and how to get involvedOpen source software is both foundational to tech today and is what will continue to fuel the future. With nearly all developers using or experimenting with AI coding tools, we expect open source developers and students to drive the next wave of AI innovation. Partnerships with organizations like Hack Club help increasing numbers of teen developers get excited about building for the future.Similar to the Summer Arcade, High Seas is an online campaign running from October through January. Teens simply sign up, ship projects, and trade them for prizes. It’s a great place for a first-time coder to begin logging hours and meet new friends!Why is this approach so impactful?: Teens can start their own coding, electrical, or mechanical projects. If they’re not sure where to begin, Hack Club offers “You Ship, We Ship” challenges to get them started.  : With Hack Club chapters in 22 countries, students can connect with peers, participate in live AMAs with builders, and gain inspiration from a global network of young developers.The journey of teens in open sourceOpen source projects offer a unique opportunity for new developers to learn new skills and contribute to software projects—almost immediately! These projects allow teens to work on real-world problems, contribute to meaningful solutions, and see their work impact a global community. The skills gained—ranging from coding and problem-solving to teamwork and project management—are invaluable.Take the story of . At just 15, Sam created Hack Club’s Summer of Making. “Finding an online community full of people who share my interests changed my life,” he says. Today, Sam is studying Computer Science at Berkeley, a testament to the long-term benefits of early engagement in open source.
 (@zaaahir), 18, from England, created a full-fledged operating system in C++, equipped with networking from the kernel level. It was created to replace outdated educational operating systems currently being used in universities. His project was the Hack Club Arcade Showcase 2024 winner, selected from over 2,000 projects. (@lopa-ok), 17, from Egypt, built a new version of meta tic-tac-toe on GitHub Pages where you play 9 games in a 3×3 grid of tic-tac-toe games. It’s a mind-bending game that’s really fun! (@crn), 14, from Massachusetts, made a low-cost, lightweight Unmanned Aerial Vehicle from the ground up—the autopilot code, pcb traces, and model files are all there for other high schoolers to replicate and fork.]]></content:encoded></item></channel></rss>
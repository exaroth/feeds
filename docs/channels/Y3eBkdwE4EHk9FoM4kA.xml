<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Trending Repos</title><link>https://konrad.website/feeds/</link><description></description><item><title>go-playground/validator</title><link>https://github.com/go-playground/validator</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 02:26:24 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[💯Go Struct and Field validation, including Cross Field, Cross Struct, Map, Slice and Array divingPackage validator implements value validations for structs and individual fields based on tags.It has the following  features:Cross Field and Cross Struct validations by using validation tags or custom validators.Slice, Array and Map diving, which allows any or all levels of a multidimensional field to be validated.Ability to dive into both map keys and values for validationHandles type interface by determining it's underlying type prior to validation.Handles custom field types such as sql driver Valuer see ValuerAlias validation tags, which allows for mapping of several validations to a single tag for easier defining of validations on structsExtraction of custom defined Field Name e.g. can specify to extract the JSON name while validating and have it available in the resulting FieldErrorCustomizable i18n aware error messages.Default validator for the gin web framework; upgrading from v8 to v9 in gin see herePlease read the discussiong started here if you are interested in contributing/helping maintain this package.go get github.com/go-playground/validator/v10
Then import the validator package into your own code.import "github.com/go-playground/validator/v10"
Validation functions return type errorThey return type error to avoid the issue discussed in the following, where err is always != nil:Validator returns only InvalidValidationError for bad validation input, nil or ValidationErrors as type error; so, in your code all you need to do is check if the error returned is not nil, and if it's not check if error is InvalidValidationError ( if necessary, most of the time it isn't ) type cast it to type ValidationErrors like so:err := validate.Struct(mystruct)
validationErrors := err.(validator.ValidationErrors)
If new to using validator it is highly recommended to initialize it using the WithRequiredStructEnabled option which is opt-in to new behaviour that will become the default behaviour in v11+. See documentation for more details.validate := validator.New(validator.WithRequiredStructEnabled())
Field Equals Another Field (relative)Field Equals Another FieldCheck the indicated characters are present in the FieldCheck the indicated characters are not present in the fieldField Greater Than Another Relative FieldField Greater Than or Equal To Another Relative FieldField Greater Than or Equal To Another FieldField Greater Than Another FieldLess Than Another Relative FieldLess Than or Equal To Another Relative FieldLess Than or Equal To Another FieldField Does Not Equal Another Field (relative)Field Does Not Equal Another FieldClassless Inter-Domain Routing CIDRClassless Inter-Domain Routing CIDRv4Classless Inter-Domain Routing CIDRv6Full Qualified Domain Name (FQDN)Internet Protocol Address IPInternet Protocol Address IPv4Internet Protocol Address IPv6Internet Protocol Address IPInternet Protocol Address IPv4Internet Protocol Address IPv6Media Access Control Address MACTransmission Control Protocol Address TCPv4Transmission Control Protocol Address TCPv6Transmission Control Protocol Address TCPUser Datagram Protocol Address UDPv4User Datagram Protocol Address UDPv6User Datagram Protocol Address UDPUnix domain socket end point AddressBusiness Identifier Code (ISO 9362)Bitcoin Bech32 Address (segwit)mongodb_connection_stringMongoDB Connection StringSpiceDb ObjectID/Permission/Typee164 formatted phone numberInternational Standard Book NumberInternational Standard Book Number 10International Standard Book Number 13International Standard Serial NumberTwo-letter country code (ISO 3166-1 alpha-2)Three-letter country code (ISO 3166-1 alpha-3)Numeric country code (ISO 3166-1 numeric)Country subdivision code (ISO 3166-2)Luhn Algorithm Checksum (for strings and (u)int)postcode_iso3166_alpha2_fieldSocial Security Number SSNUniversally Unique Identifier UUIDUniversally Unique Identifier UUID v3Universally Unique Identifier UUID v3 RFC4122Universally Unique Identifier UUID v4Universally Unique Identifier UUID v4 RFC4122Universally Unique Identifier UUID v5Universally Unique Identifier UUID v5 RFC4122Universally Unique Identifier UUID RFC4122Semantic Versioning 2.0.0Universally Unique Lexicographically Sortable Identifier ULIDCommon Vulnerabilities and Exposures Identifier (CVE id)hexcolor|rgb|rgba|hsl|hslaiso3166_1_alpha2|iso3166_1_alpha3|iso3166_1_alpha_numericRun on MacBook Pro Max M3go version go1.23.3 darwin/arm64
goos: darwin
goarch: arm64
cpu: Apple M3 Max
pkg: github.com/go-playground/validator/v10
BenchmarkFieldSuccess-16                                                42461943                27.88 ns/op            0 B/op          0 allocs/op
BenchmarkFieldSuccessParallel-16                                        486632887                2.289 ns/op           0 B/op          0 allocs/op
BenchmarkFieldFailure-16                                                 9566167               121.3 ns/op           200 B/op          4 allocs/op
BenchmarkFieldFailureParallel-16                                        17551471                83.68 ns/op          200 B/op          4 allocs/op
BenchmarkFieldArrayDiveSuccess-16                                        7602306               155.6 ns/op            97 B/op          5 allocs/op
BenchmarkFieldArrayDiveSuccessParallel-16                               20664610                59.80 ns/op           97 B/op          5 allocs/op
BenchmarkFieldArrayDiveFailure-16                                        4659756               252.9 ns/op           301 B/op         10 allocs/op
BenchmarkFieldArrayDiveFailureParallel-16                                8010116               152.9 ns/op           301 B/op         10 allocs/op
BenchmarkFieldMapDiveSuccess-16                                          2834575               421.2 ns/op           288 B/op         14 allocs/op
BenchmarkFieldMapDiveSuccessParallel-16                                  7179700               171.8 ns/op           288 B/op         14 allocs/op
BenchmarkFieldMapDiveFailure-16                                          3081728               384.4 ns/op           376 B/op         13 allocs/op
BenchmarkFieldMapDiveFailureParallel-16                                  6058137               204.0 ns/op           377 B/op         13 allocs/op
BenchmarkFieldMapDiveWithKeysSuccess-16                                  2544975               464.8 ns/op           288 B/op         14 allocs/op
BenchmarkFieldMapDiveWithKeysSuccessParallel-16                          6661954               181.4 ns/op           288 B/op         14 allocs/op
BenchmarkFieldMapDiveWithKeysFailure-16                                  2435484               490.7 ns/op           553 B/op         16 allocs/op
BenchmarkFieldMapDiveWithKeysFailureParallel-16                          4249617               282.0 ns/op           554 B/op         16 allocs/op
BenchmarkFieldCustomTypeSuccess-16                                      14943525                77.35 ns/op           32 B/op          2 allocs/op
BenchmarkFieldCustomTypeSuccessParallel-16                              64051954                20.61 ns/op           32 B/op          2 allocs/op
BenchmarkFieldCustomTypeFailure-16                                      10721384               107.1 ns/op           184 B/op          3 allocs/op
BenchmarkFieldCustomTypeFailureParallel-16                              18714495                69.77 ns/op          184 B/op          3 allocs/op
BenchmarkFieldOrTagSuccess-16                                            4063124               294.3 ns/op            16 B/op          1 allocs/op
BenchmarkFieldOrTagSuccessParallel-16                                   31903756                41.22 ns/op           18 B/op          1 allocs/op
BenchmarkFieldOrTagFailure-16                                            7748558               146.8 ns/op           216 B/op          5 allocs/op
BenchmarkFieldOrTagFailureParallel-16                                   13139854                92.05 ns/op          216 B/op          5 allocs/op
BenchmarkStructLevelValidationSuccess-16                                16808389                70.25 ns/op           16 B/op          1 allocs/op
BenchmarkStructLevelValidationSuccessParallel-16                        90686955                14.47 ns/op           16 B/op          1 allocs/op
BenchmarkStructLevelValidationFailure-16                                 5818791               200.2 ns/op           264 B/op          7 allocs/op
BenchmarkStructLevelValidationFailureParallel-16                        11115874               107.5 ns/op           264 B/op          7 allocs/op
BenchmarkStructSimpleCustomTypeSuccess-16                                7764956               151.9 ns/op            32 B/op          2 allocs/op
BenchmarkStructSimpleCustomTypeSuccessParallel-16                       52316265                30.37 ns/op           32 B/op          2 allocs/op
BenchmarkStructSimpleCustomTypeFailure-16                                4195429               277.2 ns/op           416 B/op          9 allocs/op
BenchmarkStructSimpleCustomTypeFailureParallel-16                        7305661               164.6 ns/op           432 B/op         10 allocs/op
BenchmarkStructFilteredSuccess-16                                        6312625               186.1 ns/op           216 B/op          5 allocs/op
BenchmarkStructFilteredSuccessParallel-16                               13684459                93.42 ns/op          216 B/op          5 allocs/op
BenchmarkStructFilteredFailure-16                                        6751482               171.2 ns/op           216 B/op          5 allocs/op
BenchmarkStructFilteredFailureParallel-16                               14146070                86.93 ns/op          216 B/op          5 allocs/op
BenchmarkStructPartialSuccess-16                                         6544448               177.3 ns/op           224 B/op          4 allocs/op
BenchmarkStructPartialSuccessParallel-16                                13951946                88.73 ns/op          224 B/op          4 allocs/op
BenchmarkStructPartialFailure-16                                         4075833               287.5 ns/op           440 B/op          9 allocs/op
BenchmarkStructPartialFailureParallel-16                                 7490805               161.3 ns/op           440 B/op          9 allocs/op
BenchmarkStructExceptSuccess-16                                          4107187               281.4 ns/op           424 B/op          8 allocs/op
BenchmarkStructExceptSuccessParallel-16                                 15979173                80.86 ns/op          208 B/op          3 allocs/op
BenchmarkStructExceptFailure-16                                          4434372               264.3 ns/op           424 B/op          8 allocs/op
BenchmarkStructExceptFailureParallel-16                                  8081367               154.1 ns/op           424 B/op          8 allocs/op
BenchmarkStructSimpleCrossFieldSuccess-16                                6459542               183.4 ns/op            56 B/op          3 allocs/op
BenchmarkStructSimpleCrossFieldSuccessParallel-16                       41013781                37.95 ns/op           56 B/op          3 allocs/op
BenchmarkStructSimpleCrossFieldFailure-16                                4034998               292.1 ns/op           272 B/op          8 allocs/op
BenchmarkStructSimpleCrossFieldFailureParallel-16                       11348446               115.3 ns/op           272 B/op          8 allocs/op
BenchmarkStructSimpleCrossStructCrossFieldSuccess-16                     4448528               267.7 ns/op            64 B/op          4 allocs/op
BenchmarkStructSimpleCrossStructCrossFieldSuccessParallel-16            26813619                48.33 ns/op           64 B/op          4 allocs/op
BenchmarkStructSimpleCrossStructCrossFieldFailure-16                     3090646               384.5 ns/op           288 B/op          9 allocs/op
BenchmarkStructSimpleCrossStructCrossFieldFailureParallel-16             9870906               129.5 ns/op           288 B/op          9 allocs/op
BenchmarkStructSimpleSuccess-16                                         10675562               109.5 ns/op             0 B/op          0 allocs/op
BenchmarkStructSimpleSuccessParallel-16                                 131159784                8.932 ns/op           0 B/op          0 allocs/op
BenchmarkStructSimpleFailure-16                                          4094979               286.6 ns/op           416 B/op          9 allocs/op
BenchmarkStructSimpleFailureParallel-16                                  7606663               157.9 ns/op           416 B/op          9 allocs/op
BenchmarkStructComplexSuccess-16                                         2073470               576.0 ns/op           224 B/op          5 allocs/op
BenchmarkStructComplexSuccessParallel-16                                 7821831               161.3 ns/op           224 B/op          5 allocs/op
BenchmarkStructComplexFailure-16                                          576358              2001 ns/op            3042 B/op         48 allocs/op
BenchmarkStructComplexFailureParallel-16                                 1000000              1171 ns/op            3041 B/op         48 allocs/op
BenchmarkOneof-16                                                       22503973                52.82 ns/op            0 B/op          0 allocs/op
BenchmarkOneofParallel-16                                                8538474               140.4 ns/op             0 B/op          0 allocs/op
Here is a list of software that complements using this library either pre or post validation.form - Decodes url.Values into Go value(s) and Encodes Go value(s) into url.Values. Dual Array and Full map support.mold - A general library to help modify or set data within data structures and other objectsMaintenance and support for SDK major versionsSee prior discussion here for more details.This package is aligned with the Go release policy in that support is guaranteed for the two most recent major versions.This does not mean the package will not work with older versions of Go, only that we reserve the right to increase the MSGV(Minimum Supported Go Version) when the need arises to address Security issues/patches, OS issues & support or newly introduced functionality that would greatly benefit the maintenance and/or usage of this package.If and when the MSGV is increased it will be done so in a minimum of a  release bump.Distributed under MIT License, please see license file within the code for more details.This project has grown large enough that more than one person is required to properly support the community. If you are interested in becoming a maintainer please reach out to me https://github.com/deankarn]]></content:encoded></item><item><title>GitHubDaily/GitHubDaily</title><link>https://github.com/GitHubDaily/GitHubDaily</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 02:26:24 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[坚持分享 GitHub 上高质量、有趣实用的开源技术教程、开发者工具、编程网站、技术资讯。A list cool, interesting projects of GitHub.多年以前，我曾看到 GitHub 开源项目作者、全栈工程师 TJ Holowaychunk 说过这么一句话："I don't read books, never went to school, I just read other people's code and always wonder how things work"。从那时起，我便认为，通过阅读源码，站在前辈的角度上，去思考代码架构与程序逻辑，乃是提升编程技巧最好的方式。因此，GitHub 也自然而然的，成为我最喜爱的开发者平台。秉着挖掘开源价值的初衷，GitHubDaily 自 2015 年 10 月 10 日正式成立。我们希望能通过这一举措，帮助开发者们发现当下最火的开源项目，掌控最新技术动态, 扩大技术视野, 并从开源项目的学习中获得编程能力的提升。目前，GitHubDaily 已累积分享超过 8000 个开源项目，内容包括但不限于 GitHub 上的开源技术资料、开发者工具、编程网站以及成熟应用。除了 GitHub 之外，我们也开始在下面多个社交媒体平台，帮助开发者传播与分享优质开源项目，挖掘其未来的技术应用前景。如果你想接收最新的 GitHub 开源项目资讯，可以关注一下👇有不错的开源项目，也欢迎到本仓库的 issues 推荐或自荐项目，我们期待你的分享。下面是对 GitHubDaily 在 2024 年所推荐的项目进行分类整理，方便大家查找以往分享过的内容。]]></content:encoded></item><item><title>microsoft/markitdown</title><link>https://github.com/microsoft/markitdown</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 02:26:24 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Python tool for converting files and office documents to Markdown.[!IMPORTANT] MarkItDown 0.0.2 alpha 1 (0.0.2a1) introduces a plugin-based architecture. As much as was possible, command-line and Python interfaces have remained the same as 0.0.1a3 to support backward compatibility. Please report any issues you encounter. Some interface changes may yet occur as we continue to refine MarkItDown to a first non-alpha release.MarkItDown is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc). It supports:Images (EXIF metadata and OCR)Audio (EXIF metadata and speech transcription)Text-based formats (CSV, JSON, XML)ZIP files (iterates over contents)To install MarkItDown, use pip: . Alternatively, you can install it from the source:git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e packages/markitdown
markitdown path-to-file.pdf > document.md
Or use  to specify the output file:markitdown path-to-file.pdf -o document.md
You can also pipe content:cat path-to-file.pdf | markitdown
MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:markitdown --list-plugins
markitdown --use-plugins path-to-file.pdf
To find available plugins, search GitHub for the hashtag . To develop a plugin, see packages/markitdown-sample-plugin.Azure Document IntelligenceTo use Microsoft Document Intelligence for conversion:markitdown path-to-file.pdf -o document.md -d -e "<document_intelligence_endpoint>"
More information about how to set up an Azure Document Intelligence Resource can be found herefrom markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
Document Intelligence conversion in Python:from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="<document_intelligence_endpoint>")
result = md.convert("test.pdf")
print(result.text_content)
To use Large Language Models for image descriptions, provide  and :from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o")
result = md.convert("example.jpg")
print(result.text_content)
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest < ~/your-file.pdf > output.md
This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.Navigate to the MarkItDown package:Install  in your environment and run tests:pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
(Alternative) Use the Devcontainer which has all the dependencies installed:# Reopen the project in Devcontainer and run:
hatch test
Run pre-commit checks before submitting a PR: pre-commit run --all-filesContributing 3rd-party PluginsYou can also contribute by creating and sharing 3rd party plugins. See packages/markitdown-sample-plugin for more details.This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.]]></content:encoded></item><item><title>zaidmukaddam/scira</title><link>https://github.com/zaidmukaddam/scira</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 02:26:24 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Scira (Formerly MiniPerplx) is a minimalistic AI-powered search engine that helps you find information on the internet. Powered by Vercel AI SDK! Search with models like Grok 2.0.A minimalistic AI-powered search engine that helps you find information on the internet.Tavily AI - For search grounding and web search capabilities: Get answers to your questions using Anthropic's Models.: Search the web using Tavily's API.: Get information from a specific URL.: Get the current weather for any location using OpenWeather's API.: Run code snippets in multiple languages using E2B's API.: Get the location of any place using Google Maps API, Mapbox API, and TripAdvisor API.: Track flights using AviationStack's API.Trending Movies and TV Shows: Get information about trending movies and TV shows.: Get information about any movie or TV show.Set Scira as your default search engineOpen the Chrome browser settings:Click on the three vertical dots in the upper right corner of the browser.Select "Settings" from the dropdown menu.Go to the search engine settings:In the left sidebar, click on "Search engine."Then select "Manage search engines and site search."Click on "Add" next to "Site search."Set the search engine name:Enter  in the "Search engine" field.Set the search engine URL:Enter  in the "URL with %s in place of query" field.Set the search engine shortcut:Enter  in the "Shortcut" field.Click on the three dots next to the search engine you just added.Select "Make default" from the dropdown menu.After completing these steps, you should be able to use Scira as your default search engine in Chrome.To run the example locally you need to:Sign up for accounts with the AI providers you want to use. OpenAI and Anthropic are required, Tavily is required for the web search feature.Obtain API keys for each provider.Set the required environment variables as shown in the  file, but in a new file called . to install the required dependencies. to launch the development server.This project is licensed under the MIT License - see the LICENSE file for details.]]></content:encoded></item><item><title>kuchin/awesome-cto</title><link>https://github.com/kuchin/awesome-cto</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 02:26:24 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[— Hello, my name is Dima and I'm a CTO]]></content:encoded></item><item><title>golang/go</title><link>https://github.com/golang/go</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 02:26:24 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[The Go programming languageGo is an open source programming language that makes it easy to build simple, reliable, and efficient software.Unless otherwise noted, the Go source files are distributed under the BSD-style license found in the LICENSE file.If a binary distribution is not available for your combination of operating system and architecture, visit https://go.dev/doc/install/source for source installation instructions.Go is the work of thousands of contributors. We appreciate your help!Note that the Go project uses the issue tracker for bug reports and proposals only. See https://go.dev/wiki/Questions for a list of places to ask questions about the Go language.]]></content:encoded></item><item><title>Zipstack/unstract</title><link>https://github.com/Zipstack/unstract</link><author></author><category>trending</category><pubDate>Sat, 15 Feb 2025 02:26:24 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documentsPrompt Studio's primary reason for existence is so you can develop the necessary prompts for document data extraction super efficiently. It is a purpose-built environment that makes this not just easy for you—but, lot of fun! The document sample, its variants, the prompts you're developing, outputs from different LLMs, the schema you're developing, costing details of the extraction and various tools that let you measure the effectiveness of your prompts are just a click away and easily accessible. Prompt Studio is designed for effective and high speed development and iteration of prompts for document data extraction. Welcome to IDP 2.0!🧘‍♀️ Three step nirvana with Workflow StudioAutomate critical business processes that involve complex documents with a human in the loop. Go beyond RPA with the power of Large Language Models.🌟 : Add documents to no-code Prompt Studio and do prompt engineering to extract required fields  🌟 : Configure Prompt Studio project as API deployment or configure input source and output destination for ETL Pipeline 🌟 : Deploy Workflows as unstructured data APIs or unstructured data ETL Pipelines!Linux or MacOS (Intel or M-series)Docker Compose (if you need to install it separately)Next, either download a release or clone this repo and do the following:That's all there is to it!Follow these steps to change the default username and password.See user guide for more details on managing the platform. Another really quick way to experience Unstract is by signing up for our hosted version. It comes with a 14 day free trial!Unstract comes well documented. You can get introduced to the basics of Unstract, and learn how to connect various systems like LLMs, Vector Databases, Embedding Models and Text Extractors to it. The easiest way to wet your feet is to go through our Quick Start Guide where you actually get to do some prompt engineering in Prompt Studio and launch an API to structure varied credit card statements!Contributions are welcome! Please see CONTRIBUTING.md for further details to get started easily.👋 Join the LLM-powered automation communityDo copy the value of  config in either  or  file to a secure location.Adapter credentials are encrypted by the platform using this key. Its loss or change will make all existing adapters inaccessible!In full disclosure, Unstract integrates Posthog to track usage analytics. As you can inspect the relevant code here, we collect the minimum possible metrics. Posthog can be disabled if desired by setting  to  in the frontend's .env file.]]></content:encoded></item><item><title>OpenZeppelin/openzeppelin-contracts</title><link>https://github.com/OpenZeppelin/openzeppelin-contracts</link><author></author><category>trending</category><pubDate>Fri, 14 Feb 2025 02:27:05 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[OpenZeppelin Contracts is a library for secure smart contract development.A library for secure smart contract development. Build on a solid foundation of community-vetted code.Not sure how to get started? Check out Contracts Wizard — an interactive smart contract generator.Want to scale your decentralized application? Check out OpenZeppelin Defender — a mission-critical developer security platform to code, audit, deploy, monitor, and operate with confidence.[!IMPORTANT] OpenZeppelin Contracts uses semantic versioning to communicate backwards compatibility of its API and storage layout. For upgradeable contracts, the storage layout of different major versions should be assumed incompatible, for example, it is unsafe to upgrade from 4.9.3 to 5.0.0. Learn more at Backwards Compatibility.$ npm install @openzeppelin/contracts
[!WARNING] When installing via git, it is a common error to use the  branch. This is a development branch that should be avoided in favor of tagged releases. The release process involves security measures that the  branch does not guarantee.[!WARNING] Foundry installs the latest version initially, but subsequent  commands will use the  branch.$ forge install OpenZeppelin/openzeppelin-contracts
Add @openzeppelin/contracts/=lib/openzeppelin-contracts/contracts/ in Once installed, you can use the contracts in the library by importing them:pragma solidity ^0.8.20;

import {ERC721} from "@openzeppelin/contracts/token/ERC721/ERC721.sol";

contract MyCollectible is ERC721 {
    constructor() ERC721("MyCollectible", "MCO") {
    }
}
If you're new to smart contract development, head to Developing Smart Contracts to learn about creating a new project and compiling your contracts.To keep your system secure, you should  use the installed code as-is, and neither copy-paste it from online sources nor modify it yourself. The library is designed so that only the contracts and functions you use are deployed, so you don't need to worry about it needlessly increasing gas costs.The guides in the documentation site will teach about different concepts, and how to use the related contracts that OpenZeppelin Contracts provides:Access Control: decide who can perform each of the actions on your system.Tokens: create tradeable assets or collectives, and distribute them via Crowdsales.Utilities: generic useful tools including non-overflowing math, signature verification, and trustless paying systems.The full API is also thoroughly documented, and serves as a great reference when developing your smart contract application. You can also ask for help or follow Contracts' development in the community forum.Finally, you may want to take a look at the guides on our blog, which cover several common use cases and good practices. The following articles provide great background reading, though please note that some of the referenced tools have changed, as the tooling in the ecosystem continues to rapidly evolve.This project is maintained by OpenZeppelin with the goal of providing a secure and reliable library of smart contract components for the ecosystem. We address security through risk management in various areas such as engineering and open source best practices, scoping and API design, multi-layered review processes, and incident response preparedness.The security policy is detailed in  as well, and specifies how you can report security vulnerabilities, which versions will receive security patches, and how to stay informed about them. We run a bug bounty program on Immunefi to reward the responsible disclosure of vulnerabilities.The engineering guidelines we follow to promote project quality can be found in .Past audits can be found in .Smart contracts are a nascent technology and carry a high level of technical risk and uncertainty. Although OpenZeppelin is well known for its security audits, using OpenZeppelin Contracts is not a substitute for a security audit.OpenZeppelin Contracts is made available under the MIT License, which disclaims all warranties in relation to the project and which limits the liability of those that contribute and maintain the project, including OpenZeppelin. As set out further in the Terms, you acknowledge that you are solely responsible for any use of OpenZeppelin Contracts and you assume all risks associated with any such use.OpenZeppelin Contracts exists thanks to its contributors. There are many ways you can participate and help build high quality software. Check out the contribution guide!OpenZeppelin Contracts is released under the MIT License.]]></content:encoded></item><item><title>nocodb/nocodb</title><link>https://github.com/nocodb/nocodb</link><author></author><category>trending</category><pubDate>Fri, 14 Feb 2025 02:27:05 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🔥 🔥 🔥 Open Source Airtable Alternative NocoDB is the fastest and easiest way to build databases online. docker run -d \
  --name noco \
  -v "$(pwd)"/nocodb:/usr/app/data/ \
  -p 8080:8080 \
  nocodb/nocodb:latest
docker run -d \
  --name noco \
  -v "$(pwd)"/nocodb:/usr/app/data/ \
  -p 8080:8080 \
  -e NC_DB="pg://host.docker.internal:5432?u=root&p=password&d=d1" \
  -e NC_AUTH_JWT_SECRET="569a1821-0a93-45e8-87ab-eb857f20a010" \
  nocodb/nocodb:latest
nix run github:nocodb/nocodb
To use NocoDB as a NixOS module, a flake.nix would be as follows:{
  description = "Bane's NixOS configuration";

  inputs = {
    nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
    nocodb.url = "github:nocodb/nocodb";
  };

  outputs = inputs@{ nixpkgs, nocodb, ... }: {
    nixosConfigurations = {
      hostname = nixpkgs.lib.nixosSystem {
        system = "x86_64-linux";
        modules = [
          ./configuration.nix
          nocodb.nixosModules.nocodb

          {
            services.nocodb.enable = true;
          }
        ];
      };
    };
  };
}
Auto-upstall is a single command that sets up NocoDB on a server for production usage. Behind the scenes it auto-generates docker-compose for you.bash <(curl -sSL http://install.nocodb.com/noco.sh) <(mktemp)
Auto-upstall does the following: 🕊🐳 Automatically installs all pre-requisites like docker, docker-compose🚀 Automatically installs NocoDB with PostgreSQL, Redis, Minio, Traefik gateway using Docker Compose. 🐘 🗄️ 🌐🔄 Automatically upgrades NocoDB to the latest version when you run the command again.🔒 Automatically setups SSL and also renews it. Needs a domain or subdomain as input while installation.Binaries are only for quick testing locally.curl http://get.nocodb.com/macos-arm64 -o nocodb -L && chmod +x nocodb && ./nocodbcurl http://get.nocodb.com/macos-x64 -o nocodb -L && chmod +x nocodb && ./nocodbcurl http://get.nocodb.com/linux-arm64 -o nocodb -L && chmod +x nocodb && ./nocodbcurl http://get.nocodb.com/linux-x64 -o nocodb -L && chmod +x nocodb && ./nocodbiwr http://get.nocodb.com/win-arm64.exe -OutFile Noco-win-arm64.exe && .\Noco-win-arm64.exeiwr http://get.nocodb.com/win-x64.exe -OutFile Noco-win-x64.exe && .\Noco-win-x64.exeFor more installation methods, please refer to our docsRich Spreadsheet Interface⚡  Basic Operations: Create, Read, Update and Delete Tables, Columns, and Rows⚡  Fields Operations: Sort, Filter, Group, Hide / Unhide Columns⚡  Multiple Views Types: Grid (By default), Gallery, Form, Kanban and Calendar View⚡  View Permissions Types: Collaborative Views, & Locked Views⚡  Share Bases / Views: either Public or Private (with Password Protected)⚡  Variant Cell Types: ID, Links, Lookup, Rollup, SingleLineText, Attachment, Currency, Formula, User, etc⚡  Access Control with Roles: Fine-grained Access Control at different levelsApp Store for Workflow AutomationsWe provide different integrations in three main categories. See App Store for details.⚡  Chat: Slack, Discord, Mattermost, and etc⚡  Email: AWS SES, SMTP, MailerSend, and etc⚡  Storage: AWS S3, Google Cloud Storage, Minio, and etcWe provide the following ways to let users programmatically invoke actions. You can use a token (either JWT or Social Auth) to sign your requests for authorization to NocoDB.Most internet businesses equip themselves with either spreadsheet or a database to solve their business needs. Spreadsheets are used by Billion+ humans collaboratively every single day. However, we are way off working at similar speeds on databases which are way more powerful tools when it comes to computing. Attempts to solve this with SaaS offerings have meant horrible access controls, vendor lock-in, data lock-in, abrupt price changes & most importantly a glass ceiling on what's possible in the future.Our mission is to provide the most powerful no-code interface for databases that is open source to every single internet business in the world. This would not only democratise access to a powerful computing tool but also bring forth a billion+ people who will have radical tinkering-and-building abilities on the internet. This project is licensed under AGPLv3. Thank you for your contributions! We appreciate all the contributions from the community.]]></content:encoded></item><item><title>ant-design/ant-design</title><link>https://github.com/ant-design/ant-design</link><author></author><category>trending</category><pubDate>Fri, 14 Feb 2025 02:27:05 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[An enterprise-class UI design language and React UI library🌈 Enterprise-class UI designed for web applications.📦 A set of high-quality React components out of the box.🛡 Written in TypeScript with predictable static types.⚙️ Whole package of design resources and development tools.🌍 Internationalization support for dozens of languages.🎨 Powerful theme customization based on CSS-in-JS.import { Button, DatePicker } from 'antd';

export default () => (
  <>
    <Button type="primary">PRESS ME</Button>
    <DatePicker placeholder="select date" />
  </>
);
Use opensumi.run, a free online pure front-end dev environment.$ git clone git@github.com:ant-design/ant-design.git
$ cd ant-design
$ npm install
$ npm start
Let's build a better antd together.We use Polar.sh and Issuehunt to up-vote and promote specific features that you would like to see and implement. Check our backlog and help us:]]></content:encoded></item><item><title>netdata/netdata</title><link>https://github.com/netdata/netdata</link><author></author><category>trending</category><pubDate>Fri, 14 Feb 2025 02:27:05 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Architected for speed. Automated for easy. Monitoring and troubleshooting, transformed!Monitor your servers, containers, and applicationsin high-resolution and in real-time. People get addicted to Netdata. Once you use it on your systems, Netdata: Real-time Observability, Simplified.Netdata is a high-performance observability platform designed to simplify modern infrastructure monitoring. With its innovative distributed architecture, Netdata delivers real-time insights into your systems, containers, and applications at a granular level.: Per-second data collection provides immediate visibility into your infrastructure's behavior.: Start monitoring in minutes with automatic detection and instant insights.: Automatic anomaly detection and pattern recognition, helping you identify issues before they become critical.: Scale from a single node to thousands while maintaining performance and ease of use.: From infrastructure to applications, logs to metrics, all in one solution.: Process and store metrics at the edge for superior performance and cost efficiency.: Rich, interactive dashboard for deep system insights and rapid troubleshooting.: This repository contains the Netdata Agent, the open-source core of the Netdata ecosystem. For information about other components, see below.This three-part architecture enables Netdata to scale seamlessly from single-node deployments to complex multi-cloud environments with thousands of nodes, supporting long-term data retention without compromising performance.• The heart of Netdata's monitoring capabilities• Handles data collection, storage, querying, ML analysis, exports, and alerts• Runs on physical/virtual servers, cloud, Kubernetes, and IoT devices• Optimized for zero production impact• Core of all observability features• Adds enterprise-grade features:   - User management and RBAC   - Centralized alert management   - Access your infrastructure from anywhere• Available as SaaS or on-premises• Includes free community tier• Does not centralize metric storage• Powers all dashboards and visualizations• Free to use with both Agent and Cloud• Included in standard Netdata packages• Latest version available via CDNKey capabilities of the Netdata AgentWith these capabilities, Netdata Agent provides a powerful, automated monitoring solution that works right out-of-the-box while remaining highly customizable for specific needs.Comprehensive Data Collection• 800+ integrations out of the box• Collects metrics from systems, containers, VMs, hardware sensors• Supports OpenMetrics exporters, StatsD, and logs• OpenTelemetry support coming soon• Per-second data collection• Real-time visualization with 1-second latency• High-resolution metrics for precise monitoring• Trains ML models directly at the edge• Automatic anomaly detection per metric• Pattern recognition based on historical behavior• Direct systemd-journald and Windows Event Log integrations• Tools for log conversion (log2journal, systemd-cat-native)• Process logs at the edge - no centralization needed• Rich log visualization dashboards• Build Parent-Child relationships between Agents• Create flexible centralization points• Control data replication and retention at multiple levels• NIDL (Nodes, Instances, Dimensions & Labels) data model• Auto-generated, correlated dashboards• Filter and analyze data without query language• Free to use, powered by Netdata UI• Hundreds of pre-configured alerts• Detect common issues automatically• Multiple notification methods• Proactive problem detection• Auto-detection of metrics• Zero-touch machine learning• CI/CD friendly deployment• Modular architecture• Easy to extend and customize• Integrates with existing monitoring tools• Active community ecosystemWhat can be monitored with the Netdata AgentNetdata monitors all the following:CPU, Memory and system shared resourcesDisks, Mount points, Filesystems, RAID arraysNetwork Interfaces, Protocols, Firewall, etcFans, Temperatures, Controllers, GPUs, etcResources, Performance and StatusResources, Performance, OOM, and moreSystem and Application Yes, andEvent Tracing for WindowsLive TCP and UDP sockets per PIDDocker/containerd, LXC/LXD, Kubernetes, etc (from the host)KVM, qemu, libvirt, Proxmox, etcTest APIs, TCP ports, Ping, Certificates, etcnginx, apache, postgres, redis, mongodb,Cloud Provider InfrastructureAWS, GCP, Azure, and moreOpenMetrics, StatsD and soon OpenTelemetryWhen the Netdata Agent runs on Linux, it monitors every kernel feature available, providing full coverage of all kernel technologies and offers full  coverage, monitoring all components that provide hardware error reporting, like PCI AER, RAM EDAC, IPMI, S.M.A.R.T., NVMe, Fans, Power, Voltages, and more. Netdata is the most energy-efficient monitoring tool The impact of monitoring on the energy efficiency of Docker-based systemsThe impact of monitoring on Docker-based systems?🚀 Netdata excels in energy efficiency: "... Netdata is the most energy-efficient tool ...", as the study says.🚀 Netdata excels in CPU Usage, RAM Usage and Execution Time, and has a similar impact on Network Traffic as Prometheus.The study didn’t normalize the results based on the number of metrics collected. Given that Netdata usually collects significantly more metrics than the other tools, Netdata managed to outperform the other tools, while ingesting a much higher number of metrics. Read the full study here.Netdata vs Prometheus 2025 ReviewNEW! On the same workload, Netdata uses , consumes , performes  and stores  while being up to  in query responses! Read the full 2025 review in our blog. Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF), it is one of the most 'd projects in the CNCF landscape! 1. Install Netdata everywhereNetdata can be installed on all Linux, macOS, FreeBSD (and soon on Windows) systems. We provide binary packages for the most popular operating systems and package managers.By default, you will have immediately available a local dashboard. Netdata starts a web server for its dashboard at port . Open up your web browser of choice and navigate to , replacing  with the IP address or hostname of your Agent. If installed on localhost, you can access it through .Note: the binary packages we provide, install Netdata UI automatically. Netdata UI is closed-source, but free to use with Netdata Agents and Netdata Cloud.Netdata auto-detects and auto-discovers most operating system data sources and applications. However, many data sources require some manual configuration, usually to allow Netdata to get access to the metrics.For a detailed list of the 800+ collectors available, check this guide.To monitor Windows servers and applications, use this guide.Note that Netdata on Windows is at its final release stage, so at the next Netdata release Netdata will natively support Windows.3. Configure Alert NotificationsNetdata comes with hundreds of pre-configured alerts that automatically check your metrics immediately after they start getting collected.Netdata can dispatch alert notifications to multiple third party systems, including: , , , , , , , , , , , , , , , , , , , , , , .By default, Netdata will send e-mail notifications if there is a configured MTA on the system.4. Configure Netdata ParentsOptionally, configure one or more Netdata Parents. A Netdata Parent is a Netdata Agent that has been configured to accept streaming connections from other Netdata Agents.Infrastructure level dashboards, at http://parent.server.ip:19999/.Each Netdata Agent has an API listening at the TCP port 19999 of each server. When you hit that port with a web browser (e.g. ), the Netdata Agent UI is presented. When the Netdata Agent is also a Parent, the UI of the Parent includes data for all nodes that stream metrics to that Parent.Increased retention for all metrics of all your nodes.Each Netdata Agent maintains each own database of metrics. But Parents can be given additional resources to maintain a much longer database than individual Netdata Agents.Central configuration of alerts and dispatch of notifications.Using Netdata Parents, all the alert notifications integrations can be configured only once at the Parent and they can be disabled at the Netdata Agents.You can also use Netdata Parents to:Offload your production systems (the parents run ML, alerts, queries, etc. for all their children)Secure your production systems (the parents accept user connections for all their children)5. Sign-in to Netdata Cloud and connect your Netdata Agents and Parents. If you connect your Netdata Parents, there is no need to connect your Netdata Agents. They will be connected via the Parents.When your Netdata nodes are connected to Netdata Cloud, you can (on top of the above):Access your Netdata Agents from anywhereAccess sensitive Netdata Agent features (like "Netdata Functions": processes, systemd-journal)Organize your infra in spaces and RoomsCreate, manage, and share Invite your team and assign roles to them (Role-Based Access Control)Get infinite horizontal scalability (multiple independent Netdata Agents are viewed as one infra)Configure alerts from the UIConfigure data collection from the UINetdata Mobile App notifications Netdata Cloud doesn’t prevent you from using your Netdata Agents and Parents directly, and vice versa. Your metrics are still stored in your network when you connect your Netdata Agents and Parents to Netdata Cloud.Netdata is built around a modular metrics processing pipeline.Of course, it is! We do our best to ensure it is! Will Netdata consume significant resources on my servers?No, it will not! We promise this will be fast! How much retention can I have? Does it scale? I really have a lot of servers!Netdata is designed to scale and can handle large volumes of data. My production servers are very sensitive in disk I/O. Can I use Netdata? How is Netdata different from a Prometheus and Grafana setup?Netdata is a "ready to use" monitoring solution. Prometheus and Grafana are tools to build your own monitoring solution.Netdata is also a lot faster, requires significantly fewer resources and puts almost no stress on the server it runs. For a performance comparison check this blog. How is Netdata different from DataDog, New Relic, Dynatrace, X SaaS Provider?With Netdata your data are always on-prem and your metrics are always high-resolution. How is Netdata different from Nagios, Icinga, Zabbix, etc.?Netdata offers real-time, comprehensive monitoring and the ability to monitor everything without any custom configuration required. I feel overwhelmed by the amount of information in Netdata. What should I do?Netdata is designed to provide comprehensive insights, but we understand that the richness of information might sometimes feel overwhelming. Here are some tips on how to navigate and use Netdata effectively... Do I have to subscribe to Netdata Cloud?Netdata Cloud delivers the full suite of features and functionality that Netdata offers, including a free community tier.While our default onboarding process encourages users to take advantage of Netdata Cloud, including a complimentary one-month trial of our full business product, it is not mandatory. Users can bypass this process entirely and still use the Netdata Agents along with the Netdata UI, without the need to sign up for Netdata Cloud. What does the anonymous telemetry collected by Netdata entail?Your privacy is our utmost priority. As part of our commitment to improving Netdata, we rely on anonymous telemetry data from our users who choose to leave it enabled. This data greatly informs our decision-making processes and contributes to the future evolution of Netdata.Should you wish to disable telemetry, instructions for doing so are provided in our installation guides.Netdata is a widely adopted project...The  is open-source, but the overall Netdata ecosystem is a hybrid solution, combining open-source and closed-source components. What is your monetization strategy?Netdata generates revenue through subscriptions to advanced features of Netdata Cloud and sales of on-premise and private versions of Netdata Cloud.This site also hosts a number of guides to help newer users better understand how to collect metrics, troubleshoot via charts, export to external databases, and more.Netdata is an inclusive open-source project and community. Please read our Code of Conduct.Join the Netdata community: The Netdata team and community members have regular online meetups.You are welcome to join us!Click here for the schedule.Contributions are essential to the success of open-source projects. In other words, we need your help to keep Netdata great!What is a contribution? All the following are highly valuable to Netdata:Let us know of the best practices you believe should be standardized Netdata should out-of-the-box detect as many infrastructure issues as possible. By sharing your knowledge and experiences, you help us build a monitoring solution that has baked into it all the best-practices about infrastructure monitoring.Let us know if Netdata is not perfect for your use case We aim to support as many use cases as possible and your feedback can be invaluable. Open a GitHub issue, or start a GitHub discussion about it, to discuss how you want to use Netdata and what you need.Although we can't implement everything imaginable, we try to prioritize development on use-cases that are common to our community, are in the same direction we want Netdata to evolve and are aligned with our roadmap.Support other community members Join our community on GitHub, Discord, and Reddit. Generally, Netdata is relatively easy to set up and configure, but still people may need a little push in the right direction to use it effectively. Supporting other members is a great contribution by itself!Add or improve integrations you need Integrations tend to be easier and simpler to develop. If you would like to contribute your code to Netdata, we suggest that you start with the integrations you need, which Netdata doesn’t currently support.General information about contributions:Read our Contributing Guide, which contains all the information you need to contribute to Netdata, such as improving our documentation, engaging in the community, and developing new features. We've made it as frictionless as possible, but if you need help, just ping us on our community forums!Package maintainers should read the guide on building Netdata from source for instructions on building each Netdata component from the source and preparing a package.The Netdata ecosystem consists of three key parts:: The heart of the Netdata ecosystem, the Netdata Agent is an open-source tool that must be installed on all systems monitored by Netdata. It offers a wide range of essential features, including data collection via various plugins, an embedded high-performance time-series database (dbengine), unsupervised anomaly detection powered by edge-trained machine learning, alerting and notifications, as well as query and scoring engines with associated APIs. Additionally, it supports exporting data to third-party monitoring systems, among other capabilities.: A commercial, closed-source component, Netdata Cloud enhances the capabilities of the open-source Netdata Agent by providing horizontal scalability, centralized alert notification dispatch (including a mobile app), user management, role-based access control, and other enterprise-grade features. It is available both as a SaaS solution and for on-premises deployment, with a free-to-use community tier also offered.: The Netdata UI is closed-source, and handles all visualization and dashboard functionalities related to metrics, logs and other collected data, as well as the central configuration and management of the Netdata ecosystem. It serves both the Netdata Agent and Netdata Cloud. The Netdata UI is distributed in binary form with the Netdata Agent and is publicly accessible via a CDN, licensed under the Netdata Cloud UI License 1 (NCUL1). It integrates third-party open-source components, detailed in the Netdata UI third-party licenses.The binary installation packages provided by Netdata include the Netdata Agent and the Netdata UI. Since the Netdata Agent is open-source, it is frequently packaged by third parties (e.g., Linux Distributions) excluding the closed-source components (Netdata UI is not included). While their packages can still be useful in providing the necessary back-ends and the APIs of a fully functional monitoring solution, we recommend using the installation packages we provide to experience the full feature set of Netdata.]]></content:encoded></item><item><title>RexanWONG/text-behind-image</title><link>https://github.com/RexanWONG/text-behind-image</link><author></author><category>trending</category><pubDate>Fri, 14 Feb 2025 02:27:05 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[https://textbehindimage.rexanwong.xyz - create text behind image designs easilyNOTE: Here are the links associated with text-behind-image (where can you find and use this app):Recently, some copycats with the EXACT SAME landing page and design have been created. Please be aware of these sites.Thank you all! No audience, no show :)]]></content:encoded></item><item><title>cypress-io/cypress</title><link>https://github.com/cypress-io/cypress</link><author></author><category>trending</category><pubDate>Fri, 14 Feb 2025 02:27:05 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Fast, easy and reliable testing for anything that runs in a browser. The web has evolved. Finally, testing has too.  Fast, easy and reliable testing for anything that runs in a browser. Install Cypress for Mac, Linux, or Windows, then get started.npm install cypress --save-dev
pnpm add cypress --save-dev
This project is licensed under the terms of the MIT license.Configure a badge for your project's README to show your test status or test count in the Cypress Cloud.Or let the world know your project is using Cypress with the badge below.[![Cypress.io](https://img.shields.io/badge/tested%20with-Cypress-04C38E.svg)](https://www.cypress.io/)
]]></content:encoded></item><item><title>Kong/kong</title><link>https://github.com/Kong/kong</link><author></author><category>trending</category><pubDate>Fri, 14 Feb 2025 02:27:05 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🦍 The Cloud-Native API Gateway and AI Gateway. or  is a cloud-native, platform-agnostic, scalable API Gateway distinguished for its high performance and extensibility via plugins. It also provides advanced AI capabilities with multi-LLM support.By providing functionality for proxying, routing, load balancing, health checking, authentication (and more), Kong serves as the central layer for orchestrating microservices or conventional API traffic with ease.If you prefer to use a cloud-hosted Kong, you can sign up for a free trial of Kong Konnect and get started in minutes. If not, you can follow the instructions below to get started with Kong on your own infrastructure.Let’s test drive Kong by adding authentication to an API in under 5 minutes.We suggest using the docker-compose distribution via the instructions below, but there is also a docker installation procedure if you’d prefer to run the Kong API Gateway in DB-less mode.Whether you’re running in the cloud, on bare metal, or using containers, you can find every supported distribution on our official installation page.To start, clone the Docker repository and navigate to the compose folder.  $ git clone https://github.com/Kong/docker-kong
  $ cd docker-kong/compose/
Start the Gateway stack using:  $ KONG_DATABASE=postgres docker-compose --profile database up
The Gateway is now available on the following ports on localhost: - send traffic to your service via Kong - configure Kong using Admin API or via decKBy centralizing common API functionality across all your organization's services, the Kong API Gateway creates more freedom for engineering teams to focus on the challenges that matter most.The top Kong features include:Advanced routing, load balancing, health checking - all configurable via a RESTful admin API or declarative configuration.Authentication and authorization for APIs using methods like JWT, basic auth, OAuth, ACLs and more.Proxy, SSL/TLS termination, and connectivity support for L4 or L7 traffic.Plugins for enforcing traffic controls, rate limiting, req/res transformations, logging, monitoring and including a plugin developer hub.Plugins for AI traffic to support multi-LLM implementations and no-code AI use cases, with advanced AI prompt engineering, AI observability, AI security and more.Sophisticated deployment models like Declarative Databaseless Deployment and Hybrid Deployment (control plane/data plane separation) without any vendor lock-in.Plugins provide advanced functionality that extends the use of the Gateway. Many of the Kong Inc. and community-developed plugins like AWS Lambda, Correlation ID, and Response Transformer are showcased at the Plugin Hub.Contribute to the Plugin Hub and ensure your next innovative idea is published and available to the broader community!We ❤️ pull requests, and we’re continually working hard to make it as easy as possible for developers to contribute. Before beginning development with the Kong API Gateway, please familiarize yourself with the following developer resources:Kong Inc. offers commercial subscriptions that enhance the Kong API Gateway in a variety of ways. Customers of Kong's Konnect Cloud subscription take advantage of additional gateway functionality, commercial support, and access to Kong's managed (SaaS) control plane platform. The Konnect Cloud platform features include real-time analytics, a service catalog, developer portals, and so much more! Get started with Konnect Cloud.Copyright 2016-2024 Kong Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
]]></content:encoded></item><item><title>juspay/hyperswitch</title><link>https://github.com/juspay/hyperswitch</link><author></author><category>trending</category><pubDate>Thu, 13 Feb 2025 02:27:07 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[An open source payments switch written in Rust to make payments fast, reliable and affordable
  Single API to access the payments ecosystem and its features 
 Juspay, founded in 2012, is a global leader in payment orchestration and checkout solutions, trusted by 400+ leading enterprises and brands worldwide. Hyperswitch is Juspay's new generation of composable, commercial open-source payments platform for merchant and brands. It is an enterprise-grade, transparent and modular payments platform designed to provide digital businesses access to the best payments infrastructure. 
Here are the key components of Hyperswitch that deliver the whole solution:Hyperswitch Backend: Hyperswitch backend enables seamless payment processing with comprehensive support for various payment flows - authorization, authentication, void and capture workflows along with robust management of post-payment processes like refunds and chargeback handling. Additionally, Hyperswitch supports non-payment use cases by enabling connections with external FRM or authentication providers as part of the payment flow. The backend optimizes payment routing with customizable workflows, including success rate-based routing, rule-based routing, volume distribution, fallback handling, and intelligent retry mechanisms for failed payments based on specific error codes.SDK (Frontend): The SDK, available for web, Android, and iOS, unifies the payment experience across various methods such as cards, wallets, BNPL, bank transfers, and more, while supporting the diverse payment flows of underlying PSPs. When paired with the locker, it surfaces the user's saved payment methods.Control Center: The Control Center enables users to manage the entire payments stack without any coding. It allows the creation of workflows for routing, payment retries, and defining conditions to invoke 3DS, fraud risk management (FRM), and surcharge modules. The Control Center provides access to transaction, refund, and chargeback operations across all integrated PSPs, transaction-level logs for initial debugging, and detailed analytics and insights into payment performance.You can run Hyperswitch on your system using Docker compose after cloning this repository.git clone --depth 1 --branch latest https://github.com/juspay/hyperswitch
cd hyperswitch
docker compose up -d
Check out the local setup guide for a more details on setting up the entire stack or component wise. This takes 15-mins and gives the following output[+] Running 2/2
✔ hyperswitch-control-center Pulled 2.9s
✔ hyperswitch-server Pulled 3.0s
[+] Running 6/0

✔ Container hyperswitch-pg-1 Created 0.0s
✔ Container hyperswitch-redis-standalone-1 Created 0.0s
✔ Container hyperswitch-migration_runner-1 Created 0.0s
✔ Container hyperswitch-hyperswitch-server-1 Created 0.0s
✔ Container hyperswitch-hyperswitch-web-1 Created 0.0s
✔ Container hyperswitch-hyperswitch-control-center-1 Created 0.0s

Attaching to hyperswitch-control-center-1, hyperswitch-server-1, hyperswitch-web-1, migration_runner-1, pg-1, redis-standalone-1
The fastest and easiest way to try Hyperswitch on AWS is via our CDK scriptsClick on the following button for a quick standalone deployment on AWS, suitable for prototyping. No code or setup is required in your system and the deployment is covered within the AWS free-tier setup.Sign-in to your AWS console.Follow the instructions provided on the console to successfully deploy Hyperswitch. This takes 30-45mins and gives the following outputhttp://hyperswitch-<host-id.region>.elb.amazonaws.comhttp://<cloudfront.host-id>/0.103.1/v0/HyperLoader.jsControl center server running onhttp://hyperswitch-control-center-<host-id.region>.elb.amazonaws.com, Login with Email: Hyperswitch Demo Store running onhttp://hyperswitch-sdk-demo-<host-id.region>.elb.amazonaws.comhttp://hyperswitch-logs-<host-id.region>.elb.amazonaws.com, Login with username: , password: We support deployment on GCP and Azure via Helm charts which takes 30-45mins. You can read more at Hyperswitch docs.You can experience the product by signing up for our hosted sandbox. The signup process accepts any email ID and provides access to the entire Control Center. You can set up connectors, define workflows for routing and retries, and even try payments from the dashboard.Support, Feature requests & BugsFor any support, join the conversation in SlackFor new product features, enhancements, roadmap discussions, or to share queries and ideas, visit our GitHub DiscussionsPayments are evolving rapidly worldwide, with hundreds of processors, fraud detection systems, authentication modules, and new payment methods and flows emerging. Businesses building or managing their own payment stacks often face similar challenges, struggle with comparable issues, and find it hard to innovate at the desired pace.Hyperswitch serves as a well-architected designed reference platform, built on best-in-class design principles, empowering businesses to own and customize their payment stack. It provides a reusable core payments stack that can be tailored to specific requirements while relying on the Hyperswitch team for enhancements, support, and continuous innovation.Embrace Payments Diversity: It will drive innovation in the ecosystem in multiple ways.Make it Open Source: Increases trust; Improves the quality and reusability of software.Be community driven: It enables participatory design and development.Build it like Systems Software: This sets a high bar for Reliability, Security and Performance SLAs.Maximise Value Creation: For developers, customers & partners.This project is being created and maintained by JuspayThe core team of 150+ engineers building Hyperswitch. Keep up the great work! 🥂]]></content:encoded></item><item><title>Oliveriver/5d-diplomacy-with-multiverse-time-travel</title><link>https://github.com/Oliveriver/5d-diplomacy-with-multiverse-time-travel</link><author></author><category>trending</category><pubDate>Thu, 13 Feb 2025 02:27:07 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[5D Diplomacy With Multiverse Time TravelA new standard in measuring how galaxy-brained you are, 5D Diplomacy With Multiverse Time Travel combines the classic game of pure negotiation with the modern classic game of pure disorientation. Can you convince your opponent to support an attack in the present while simultaneously backstabbing them five years ago and seven timelines over?Inspired by and indebted to the board game Diplomacy and the video game 5D Chess With Multiverse Time Travel. Both are excellent in their own right, so we recommend picking up a copy of each to understand the rules for 5D Diplomacy. is a trademark of Avalon Hill. 5D Chess With Multiverse Time Travel is a trademark of Thunkspace, LLC. 5D Diplomacy With Multiverse Time Travel and its creators are not affiliated with either  or 5D Chess With Multiverse Time Travel.If you find a bug, please raise an issue.Note that official development of new features has come to an end. Issues requesting new or modified gameplay features will probably be rejected. Only bug fixes, performance improvements, and quality of life adjustments are likely to be accepted as suggestions.Feel free to fork this repo and modify the code there if you wish to experiment with more radical changes to the rules or UI. Visit the 5D Diplomacy Discord server to discuss new rules and theory with others.There are currently two options for installing 5D Diplomacy. Use quick installation if you just want to play the game. Use manual installation if you want to make code changes.The correct version of Docker for your operating system.Open Docker and leave it running.Open the CLI for your operating system and navigate inside the folder where you've downloaded this repository. If you don't know how to do this, use this tutorial.Via the CLI, run the command docker compose build frontend backend and wait for it to complete.Via the CLI, run the command  and wait for it to complete.Wait an extra few seconds for the server to start up. If you experience errors creating a game in the next step, try waiting longer.If you ever update the code (manually or via a pull from this repository), you will need to run docker compose down --rmi local, then run through the steps above again. Note that this may result in the database being wiped.To read server logs, run docker compose logs -f backend.The game consists of two components, found in the  and  directories. You must set up and run both to play 5D Diplomacy, unless you're connecting to someone else's server or have implemented a custom client.The  directory contains the original proof of concept from 2021. None of its contents are required for running the latest version of 5D Diplomacy.Navigate to the  directory.(Optional) If you want to connect to a custom database, copy  to create a new file in the same directory called appsettings.Development.json. Add your database's connection string as the value for the appropriate provider under , then set the value for  to match the name of the connection string.Run one of the following commands, depending on your configuration: 
  If you aren't using a custom database, i.e. if you didn't follow the optional step above, run dotnet ef database update --context SqliteGameContext.If you're using a custom SQLite database, run dotnet ef database update --context SqliteGameContext.If you're using a custom SQL Server database, run dotnet ef database update --context SqlServerGameContext.Run  to start the server.The server will print its address to the console, likely http://localhost:5000 but it may be different. Note this down for later.Note that if you ever update the code with changes that affect the database schema (e.g. if you pull a change from this repository that includes a new migration), you will have to run the appropriate dotnet ef database update command again.Navigate to the  directory.Copy  to create a new file in the same directory called .Inside , replace  with the address of the server noted earlier.Run  to start the client in the default browser.First see installation instructions above. 5D Diplomacy can be set to run normal games (where seven players join and enter orders individually) or sandbox games (where a single user enters all orders).If you wish to play a normal game or let other people see one of your sandbox games, you'll need to expose the domains of your client and/or server (if everyone has set up the client themselves, only a server needs to be exposed). There are various ways to do this, although this guide does not cover them.If you modify the code and host a game that others interact with, you must provide a link to your modified source code to comply with the terms of the AGPL license. We suggest updating the link to the source code in client/src/components/pages/LandingPage.tsx.To create a normal game, one player must choose the new game option from the main menu. They must choose the adjacency setting (see game rules below). After a game has been created, the initiating player enters the game and sees the game ID in the top left corner, which they must copy and send to other players.Other players can then use the join game option from the main menu to join with the supplied game ID.Note that 5D Diplomacy has no in-built messaging system. Unless you want to play without press, you require a separate program to send and receive press, e.g. a messaging app or voice calls.A possible exploit exists when playing multiplayer games. Since 5D Diplomacy has no user logins or verification, a player can join as someone else and enter their orders before them. The alternative - allowing each nation to join only once - would mean players can't rejoin after a break or connection issues. While Diplomacy is a game about breaking trust, you'll simply have to trust players not to be quite this devious.To create a sandbox game, select new game from the main menu and choose the sandbox option. Also set the adjacency setting (see game rules below).In sandbox mode, turns advance after submission whether all nations have orders or not.The rules of 5D Diplomacy generally extend the rules of regular Diplomacy. This guide covers only deviations from the rules of the base game.The game world consists of a grid of Diplomacy boards. Each row is a timeline, and each timeline progresses with boards following the standard Diplomacy turns (Spring 1901, then Fall 1901, then Winter 1901, then Spring 1902, etc.).At a given time, only units on the active boards (those furthest to the right in each timeline) can have new orders assigned. Other units are locked into their pre-existing orders, which can't be changed, though their resolution can.Units in spring or fall turns can be given hold, move, support or convoy orders. These are validated against standard Diplomacy adjacency rules, with extra possibilities for multiverse travel. The adjacency strictness setting (chosen when a new game is created) determines how units can move through the multiverse.With strict adjacencies, a unit can move/support/convoy to:Any adjacent region on its own board.The same region on a different board exactly one timeline up or down, e.g. moving from Paris in Timeline 2 to Paris in Timeline 1.The same region on a different board exactly one board in the past, e.g. moving from Berlin in Fall 1901 to Berlin in Spring 1901. Note that moving to winter boards is forbidden and these are skipped when determining board adjacencies, so Spring 1902 is adjacent to Fall 1901.Any region it is successfully convoyed to (see below).With loose adjacencies, a unit can move/support/convoy to:Any adjacent region on its own board.The same region on a different board exactly one timeline up or down, or any region adjacent to that region within its board, e.g. moving from Paris in Timeline 2 to Gascony in Timeline 1.The same region on a different board exactly one board in the past, or any region adjacent to that region within its board, e.g. moving from Berlin in Fall 1901 to Kiel in Spring 1901. Winter boards are still ignored.Any region it is successfully convoyed to (see below).In either case, note in particular that movement one board diagonally is not permitted (without a convoy).Convoys extend the quirk of standard Diplomacy that allows armies to move an arbitrary distance in a single turn if a chain of convoying fleets exists. Providing each fleet is adjacent to the next and all are ordered to perform the same convoy, an army could go almost anywhere.Units are however forbidden from moving into boards that don't exist yet, even with convoys. Convoys and supports though can anticipate a future unit moving back in time, so the player can use the ghost board to enter supports/convoys via an arbitrary location in the multiverse.Any units in (spring or fall) boards not assigned orders are given a hold order by default.The rule of thumb for adjudication: each time all orders for a turn are submitted, all orders in the entire world are adjudicated together, as if in a single enormous Diplomacy board.In particular, new orders could affect a prior resolution of existing orders, e.g. a unit that bounced now has support and so moves successfully. This extends to supports/convoys across time, e.g. convoys that were previously invalid may become valid if the future army appears and performs the expected move.If the new resolution matches an existing child board that spawned from this one, then no new timeline splits. So if two units bounced and both receive one new support from their relative future next turn, they still bounce and no new board is created (assuming no other changes elsewhere on this board).If the new resolution does not match an existing child board that spawned from this one, a new timeline appears. New timelines always appear below all existing timelines, and are always created in a canonical order (earliest board first; if boards are of equal age, lowest timeline number first).Note that this is different to 5D Chess where boards can spawn above or below existing timelines, potentially changing the coordinates of existing boards. There's no concept of a turn belonging to a player in Diplomacy (instead, they belong to everyone simultaneously) and 5D Diplomacy extends this thinking, so timelines spawn in only one direction. Board coordinates also never change.Main turns (spring and fall) and winter boards adjudicate simultaneously if all are at the end of their respective timelines. So a player may be creating builds on one board and creating moves on another in the same turn. Though of course these must be kept separate, so building is not permitted on movement boards and vice versa.Build/disband counts are per board. If a player controls fewer centres than they have units in one timeline but more in another, the difference does not cancel out: they must disband in the former and may build only in the latter. If they fail to enter enough disbands on a given board, units are removed from that board at random.If any board requires retreats, adjudication pauses for all boards without retreats. Retreats may only move to an adjacent region on the same board.A player achieves victory under one of the following conditions:They are the only player to control at least 18 unique supply centres across all active boards. Unique here means unique by region name, so controlling Serbia in Timeline 1 and Serbia in Timeline 2 counts as only one supply centre.If more than one player controls more than 18 unique supply centres, they are the only one with a clear majority. It's possible for two players to reach 18 centres in the same turn, e.g. if they have targeted different timelines.As with regular Diplomacy, it's possible for 5D Diplomacy to feature variant maps with completely different region arrangements. Other variants, such as variants with new rules, are not supported.Modifying the server to adjudicate custom variants in 5D is simple. First, edit the list of nations in . Then edit the JSON files in the folder  to match the intended board. Any subsequent run of the server will use those to create and adjudicate worlds.Modify  to change supply centres and starting/home centres.Modify  to change connections between regions.Modify  to change regions.Modify  to change starting units.Modifying the client is tricker as it is much more tied to this particular Diplomacy board. While client/src/data/regions.ts contains the list of regions and associated data, you will also need to replace the SVG files in  and then reference them in client/src/hooks/useRegionSvg.tsx.]]></content:encoded></item><item><title>Azure/Azure-Sentinel</title><link>https://github.com/Azure/Azure-Sentinel</link><author></author><category>trending</category><pubDate>Thu, 13 Feb 2025 02:27:07 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Cloud-native SIEM for intelligent security analytics for your entire enterprise.Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to issues for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository's wiki to get started. For questions and feedback, please contact AzureSentinel@microsoft.comWe value your feedback. Here are some channels to help surface your questions or feedback:This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com.Add in your new or updated contributions to GitHubBrand new or update to a contribution via these methods:Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.Pull Request Detection Template Structure Validation CheckAs part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included. For Detections, there is a new section that must be included. See the contribution guidelines for more information. If this section or any other required section is not included, then a validation error will occur similar to the below. The example is specifically if the YAML is missing the entityMappings section:A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: "ExcessiveBlockedTrafficGeneratedbyUser.yaml") [FAIL]
  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: "ExcessiveBlockedTrafficGeneratedbyUser.yaml") [104ms]
  Error Message:
   Expected object to be <null>, but found System.ComponentModel.DataAnnotations.ValidationException with message "An old mapping for entity 'AccountCustomEntity' does not have a matching new mapping entry."
Pull Request KQL Validation CheckAs part of the PR checks we run a syntax validation of the KQL queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR)  In the pipeline you can see which test failed and what is the cause: A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: "ExcessiveBlockedTrafficGeneratedbyUser.yaml") [FAIL]
  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: "ExcessiveBlockedTrafficGeneratedbyUser.yaml") [21ms]
  Error Message:
   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name 'SymantecEndpointProtection' does not refer to any known table, tabular variable or function., Code: 'KS204', Severity: 'Error', Location: '67..93',The name 'SymantecEndpointProtection' does not refer to any known table, tabular variable or function., Code: 'KS204', Severity: 'Error', Location: '289..315'
If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify your table schema is defined in json file in the folder Azure-Sentinel\.script\tests\KqlvalidationsTests\CustomTablesExample for table tablexyz.json{
  "Name": "tablexyz",
  "Properties": [
    {
      "Name": "SomeDateTimeColumn",
      "Type": "DateTime"
    },
    {
      "Name": "SomeStringColumn",
      "Type": "String"
    },
    {
      "Name": "SomeDynamicColumn",
      "Type": "Dynamic"
    }
  ]
}
Run KQL Validation LocallyIn order to run the KQL validation before submitting Pull Request in you local machine:Open Shell and navigate to Azure-Sentinel\\.script\tests\KqlvalidationsTests\Example of output (in Ubuntu):Welcome to .NET Core 3.1!
---------------------
SDK Version: 3.1.403

Telemetry
---------
The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to '1' or 'true' using your favorite shell.

Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry

----------------
Explore documentation: https://aka.ms/dotnet-docs
Report issues and find source on GitHub: https://github.com/dotnet/core
Find out what's new: https://aka.ms/dotnet-whats-new
Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https
Use 'dotnet --help' to see available commands or visit: https://aka.ms/dotnet-cli-docs
Write your first app: https://aka.ms/first-net-core-app
--------------------------------------------------------------------------------------
Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)
Microsoft (R) Test Execution Command Line Tool Version 16.7.0
Copyright (c) Microsoft Corporation.  All rights reserved.

Starting test execution, please wait...

A total of 1 test files matched the specified pattern.

Test Run Successful.
Total tests: 171
     Passed: 171
 Total time: 25.7973 Seconds
Detection schema validation testsSimilarly to KQL Validation, there is an automatic validation of the schema of a detection. The schema validation includes the detection's frequency and period, the detection's trigger type and threshold, validity of connectors Ids (valid connectors Ids list), etc. A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.Run Detection Schema Validation LocallyIn order to run the KQL validation before submitting Pull Request in you local machine:Open Shell and navigate to Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.For information on what you can contribute and further details, refer to the "get started" section on the project's wiki.]]></content:encoded></item><item><title>landing-ai/vision-agent</title><link>https://github.com/landing-ai/vision-agent</link><author></author><category>trending</category><pubDate>Thu, 13 Feb 2025 02:27:07 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[VisionAgent is a library that helps you utilize agent frameworks to generate code to solve your vision task. Check out our discord for updates and roadmaps! The fastest way to test out VisionAgent is to use our web application which you can find here.export ANTHROPIC_API_KEY="your-api-key"
export OPENAI_API_KEY="your-api-key"
 We found using both Anthropic Claude-3.5 and OpenAI o1 to be provide the best performance for VisionAgent. If you want to use a different LLM provider or only one, see 'Using Other LLM Providers' below.Counting cans in an imageYou can use VisionAgent to generate code to count the number of people in an image:from vision_agent.agent import VisionAgentCoderV2
from vision_agent.models import AgentMessage

agent = VisionAgentCoderV2(verbose=True)
code_context = agent.generate_code(
    [
        AgentMessage(
            role="user",
            content="Count the number of people in this image",
            media=["people.png"]
        )
    ]
)

with open("generated_code.py", "w") as f:
    f.write(code_context.code + "\n" + code_context.test)
VisionAgent produces code that utilizes our tools. You can also use the tools directly. For example if you wanted to detect people in an image and visualize the results:import vision_agent.tools as T
import matplotlib.pyplot as plt

image = T.load_image("people.png")
dets = T.countgd_object_detection("person", image)
# visualize the countgd bounding boxes on the image
viz = T.overlay_bounding_boxes(image, dets)

# save the visualization to a file
T.save_image(viz, "people_detected.png")

# display the visualization
plt.imshow(viz)
plt.show()
You can also use the tools for running on video files:import vision_agent.tools as T

frames_and_ts = T.extract_frames_and_timestamps("people.mp4")
# extract the frames from the frames_and_ts list
frames = [f["frame"] for f in frames_and_ts]

# run the countgd tracking on the frames
tracks = T.countgd_sam2_video_tracking("person", frames)
# visualize the countgd tracking results on the frames and save the video
viz = T.overlay_segmentation_masks(frames, tracks)
T.save_video(viz, "people_detected.mp4")
Using Other LLM ProvidersYou can use other LLM providers by changing  in the  directory. For example to change to Anthropic simply just run:cp vision_agent/configs/anthropic_config.py vision_agent/configs/config.py
 VisionAgent moves fast and we are constantly updating and changing the library. If you have any questions or need help, please reach out to us on our discord channel.]]></content:encoded></item><item><title>andrewyng/aisuite</title><link>https://github.com/andrewyng/aisuite</link><author></author><category>trending</category><pubDate>Thu, 13 Feb 2025 02:27:07 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Simple, unified interface to multiple Generative AI providersSimple, unified interface to multiple Generative AI providers. makes it easy for developers to use multiple LLM through a standardized interface. Using an interface similar to OpenAI's,  makes it easy to interact with the most popular LLMs and compare the results. It is a thin wrapper around python client libraries, and allows creators to seamlessly swap out and test responses from different LLM providers without changing their code. Today, the library is primarily focussed on chat completions. We will expand it cover more use cases in near future.Currently supported providers are - OpenAI, Anthropic, Azure, Google, AWS, Groq, Mistral, HuggingFace Ollama, Sambanova and Watsonx. To maximize stability,  uses either the HTTP endpoint or the SDK for making calls to the provider.You can install just the base  package, or install a provider's package along with .This installs just the base package without installing any provider's SDK.This installs aisuite along with anthropic's library.pip install 'aisuite[anthropic]'
This installs all the provider-specific librariespip install 'aisuite[all]'
To get started, you will need API Keys for the providers you intend to use. You'll need to install the provider-specific library either separately or when installing aisuite.The API Keys can be set as environment variables, or can be passed as config to the aisuite Client constructor. You can use tools like  or  to set the environment variables manually. Please take a look at the  folder to see usage.Here is a short example of using  to generate chat completion responses from gpt-4o and claude-3-5-sonnet.export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
import aisuite as ai
client = ai.Client()

models = ["openai:gpt-4o", "anthropic:claude-3-5-sonnet-20240620"]

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.75
    )
    print(response.choices[0].message.content)

Note that the model name in the create() call uses the format - .  will call the appropriate provider with the right parameters based on the provider value. For a list of provider values, you can look at the directory - . The list of supported providers are of the format -  in that directory. We welcome providers adding support to this library by adding an implementation file in this directory. Please see section below for how to contribute.For more examples, check out the  directory where you will find several notebooks that you can run to experiment with the interface.aisuite is released under the MIT License. You are free to use, modify, and distribute the code for both commercial and non-commercial purposes.Adding support for a providerWe have made easy for a provider or volunteer to add support for a new platform.Naming Convention for Provider ModulesWe follow a convention-based approach for loading providers, which relies on strict naming conventions for both the module name and the class name. The format is based on the model identifier in the form .The provider's module file must be named in the format .The class inside this module must follow the format: the provider name with the first letter capitalized, followed by the suffix .: The provider class should be defined as:class HuggingfaceProvider(BaseProvider)
in providers/huggingface_provider.py.: The provider class should be defined as:class OpenaiProvider(BaseProvider)
in providers/openai_provider.pyThis convention simplifies the addition of new providers and ensures consistency across provider implementations.]]></content:encoded></item><item><title>MystenLabs/walrus-docs</title><link>https://github.com/MystenLabs/walrus-docs</link><author></author><category>trending</category><pubDate>Thu, 13 Feb 2025 02:27:07 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Documentation and examples for the Walrus decentralized storage systemWelcome to the GitHub repository for Walrus, a decentralized storage and availability protocol designed specifically for large binary files, or "blobs". Walrus focuses on providing a robust solution for storing unstructured content on decentralized storage nodes while ensuring high availability and reliability even in the presence of Byzantine faults.You can also build and access the documentation locally (assuming you have Rust installed):cargo install mdbook
cargo install mdbook-admonish@1.18.0 --locked
cargo install mdbook-katex@0.9.0 --locked
cargo install mdbook-i18n-helpers --locked
mdbook serve
Using translated versionsIf there is a translated resource in  directory, it can be specified through the  environment variable. For example, to build or serve the Chinese translation:MDBOOK_BOOK__LANGUAGE=zh_CN mdbook build
MDBOOK_BOOK__LANGUAGE=zh_CN mdbook serve
Please consult TRANSLATING.md for further information on how to create and maintain translations.Get help and report issuesIf you experience any issues or bugs, please check for existing issues and file an issue if it hasn't been reported yet. Please include the version of the  binary in your bug report (you can obtain it with ).]]></content:encoded></item><item><title>datawhalechina/llm-cookbook</title><link>https://github.com/datawhalechina/llm-cookbook</link><author></author><category>trending</category><pubDate>Thu, 13 Feb 2025 02:27:07 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[面向开发者的 LLM 入门教程，吴恩达大模型系列课程中文版本项目是一个面向开发者的大模型手册，针对国内开发者的实际需求，主打 LLM 全方位入门实践。本项目基于吴恩达老师大模型系列课程内容，对原课程内容进行筛选、翻译、复现和调优，覆盖从 Prompt Engineering 到 RAG 开发、模型微调的全部流程，用最适合国内学习者的方式，指导国内开发者如何学习、入门 LLM 相关项目。针对不同内容的特点，我们对共计 11 门吴恩达老师的大模型课程进行了翻译复现，并结合国内学习者的实际情况，对不同课程进行了分级和排序，初学者可以先系统学习我们的必修类课程，掌握入门 LLM 所有方向都需要掌握的基础技能和概念，再选择性地学习我们的选修类课程，在自己感兴趣的方向上不断探索和学习。如果有你非常喜欢但我们还没有进行复现的吴恩达老师大模型课程，我们欢迎每一位开发者参考我们已有课程的格式和写法来对课程进行复现并提交 PR，在 PR 审核通过后，我们会根据课程内容将课程进行分级合并。欢迎每一位开发者的贡献！LLM 正在逐步改变人们的生活，而对于开发者，如何基于 LLM 提供的 API 快速、便捷地开发一些具备更强能力、集成LLM 的应用，来便捷地实现一些更新颖、更实用的能力，是一个急需学习的重要能力。由吴恩达老师与 OpenAI 合作推出的大模型系列教程，从大模型时代开发者的基础技能出发，深入浅出地介绍了如何基于大模型 API、LangChain 架构快速开发结合大模型强大能力的应用。其中，《Prompt Engineering for Developers》教程面向入门 LLM 的开发者，深入浅出地介绍了对于开发者，如何构造 Prompt 并基于 OpenAI 提供的 API 实现包括总结、推断、转换等多种常用功能，是入门 LLM 开发的经典教程；《Building Systems with the ChatGPT API》教程面向想要基于 LLM 开发应用程序的开发者，简洁有效而又系统全面地介绍了如何基于 ChatGPT API 打造完整的对话系统；《LangChain for LLM Application Development》教程结合经典大模型开源框架 LangChain，介绍了如何基于 LangChain 框架开发具备实用功能、能力全面的应用程序，《LangChain Chat With Your Data》教程则在此基础上进一步介绍了如何使用 LangChain 架构结合个人私有数据开发个性化大模型应用；《Building Generative AI Applications with Gradio》、《Evaluating and Debugging Generative AI》教程分别介绍了两个实用工具 Gradio 与 W&B，指导开发者如何结合这两个工具来打造、评估生成式 AI 应用。上述教程非常适用于开发者学习以开启基于 LLM 实际搭建应用程序之路。因此，我们将该系列课程翻译为中文，并复现其范例代码，也为其中一个视频增加了中文字幕，支持国内中文学习者直接使用，以帮助中文学习者更好地学习 LLM 开发；我们也同时实现了效果大致相当的中文 Prompt，支持学习者感受中文语境下 LLM 的学习使用，对比掌握多语言语境下的 Prompt 设计与 LLM 开发。未来，我们也将加入更多 Prompt 高级技巧，以丰富本课程内容，帮助开发者掌握更多、更巧妙的 Prompt 技能。所有具备基础 Python 能力，想要入门 LLM 的开发者。《ChatGPT Prompt Engineering for Developers》、《Building Systems with the ChatGPT API》等教程作为由吴恩达老师与 OpenAI 联合推出的官方教程，在可预见的未来会成为 LLM 的重要入门教程，但是目前还只支持英文版且国内访问受限，打造中文版且国内流畅访问的教程具有重要意义；同时，GPT 对中文、英文具有不同的理解能力，本教程在多次对比、实验之后确定了效果大致相当的中文 Prompt，支持学习者研究如何提升 ChatGPT 在中文语境下的理解与生成能力。本教程适用于所有具备基础 Python 能力，想要入门 LLM 的开发者。至少一个 LLM API（最好是 OpenAI，如果是其他 API，你可能需要参考其他教程对 API 调用代码进行修改）能够使用 Python Jupyter Notebook本教程共包括 11 门课程，分为必修类、选修类两个类别。必修类课程是我们认为最适合初学者学习以入门 LLM 的课程，包括了入门 LLM 所有方向都需要掌握的基础技能和概念，我们也针对必修类课程制作了适合阅读的在线阅读和 PDF 版本，在学习必修类课程时，我们建议学习者按照我们列出的顺序进行学习；选修类课程是在必修类课程上的拓展延伸，包括了 RAG 开发、模型微调、模型评估等多个方面，适合学习者在掌握了必修类课程之后选择自己感兴趣的方向和课程进行学习。面向开发者的 Prompt Engineering。基于吴恩达老师《ChatGPT Prompt Engineering for Developers》课程打造，面向入门 LLM 的开发者，深入浅出地介绍了对于开发者，如何构造 Prompt 并基于 OpenAI 提供的 API 实现包括总结、推断、转换等多种常用功能，是入门 LLM 开发的第一步。搭建基于 ChatGPT 的问答系统。基于吴恩达老师《Building Systems with the ChatGPT API》课程打造，指导开发者如何基于 ChatGPT 提供的 API 开发一个完整的、全面的智能问答系统。通过代码实践，实现了基于 ChatGPT 开发问答系统的全流程，介绍了基于大模型开发的新范式，是大模型开发的实践基础。使用 LangChain 开发应用程序。基于吴恩达老师《LangChain for LLM Application Development》课程打造，对 LangChain 展开深入介绍，帮助学习者了解如何使用 LangChain，并基于 LangChain 开发完整的、具备强大能力的应用程序。使用 LangChain 访问个人数据。基于吴恩达老师《LangChain Chat with Your Data》课程打造，深入拓展 LangChain 提供的个人数据访问能力，指导开发者如何使用 LangChain 开发能够访问用户个人数据、提供个性化服务的大模型应用。使用 Gradio 搭建生成式 AI 应用。基于吴恩达老师《Building Generative AI Applications with Gradio》课程打造，指导开发者如何使用 Gradio 通过 Python 接口程序快速、高效地为生成式 AI 构建用户界面。评估改进生成式 AI。基于吴恩达老师《Evaluating and Debugging Generative AI》课程打造，结合 wandb，提供一套系统化的方法和工具，帮助开发者有效地跟踪和调试生成式 AI 模型。微调大语言模型。基于吴恩达老师《Finetuning Large Language Model》课程打造，结合 lamini 框架，讲述如何便捷高效地在本地基于个人数据微调开源大语言模型。大模型与语义检索。基于吴恩达老师《Large Language Models with Semantic Search》课程打造，针对检索增强生成，讲述了多种高级检索技巧以实现更准确、高效的检索增强 LLM 生成效果。基于 Chroma 的高级检索。基于吴恩达老师《Advanced Retrieval for AI with Chroma》课程打造，旨在介绍基于 Chroma 的高级检索技术，提升检索结果的准确性。搭建和评估高级 RAG 应用。基于吴恩达老师《Building and Evaluating Advanced RAG Applications》课程打造，介绍构建和实现高质量RAG系统所需的关键技术和评估框架。LangChain 的 Functions、Tools 和 Agents。基于吴恩达老师《Functions, Tools and Agents with LangChain》课程打造，介绍如何基于 LangChain 的新语法构建 Agent。Prompt 高级技巧。包括 CoT、自我一致性等多种 Prompt 高级技巧的基础理论与代码实现。content：基于原课程复现的双语版代码，可运行的 Notebook，更新频率最高，更新速度最快。

docs：必修类课程文字教程版在线阅读源码，适合阅读的 md。

figures：图片文件。
高立业（内容创作者-DataWhale成员-算法工程师）陈逸涵 (内容创作者-Datawhale意向成员-AI爱好者)曾浩龙（内容创作者-Datawhale 意向成员-JLU AI 研究生） Datawhale 是一个专注于数据科学与 AI 领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。微信搜索公众号Datawhale可以加入我们。 
]]></content:encoded></item><item><title>antonputra/tutorials</title><link>https://github.com/antonputra/tutorials</link><author></author><category>trending</category><pubDate>Thu, 13 Feb 2025 02:27:07 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🔴 - To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting. (me@antonputra.com)]]></content:encoded></item><item><title>microsoft/data-formulator</title><link>https://github.com/microsoft/data-formulator</link><author></author><category>trending</category><pubDate>Wed, 12 Feb 2025 02:26:49 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🪄 Create rich visualizations with AITransform data and create rich visualizations iteratively with AI 🪄. Try Data Formulator now![02-12-2025] More models supported now!Now supports OpenAI, Azure, Ollama, and Anthropic models (and more powered by LiteLLM);Models with strong code generation and instruction following capabilities are recommended (gpt-4o, claude-3-5-sonnet etc.);You can store API keys in  to avoid typing them every time (see template ).Let us know which models you have good/bad experiences with, and what models you would like to see supported! [comment here][11-07-2024] Minor fun update: data visualization challenges!We added a few visualization challenges with the sample datasets. Can you complete them all? [try them out!]Comment in the issue when you did, or share your results/questions with others! [comment here][10-11-2024] Data Formulator python package released!You can now install Data Formulator using Python and run it locally, easily. [check it out].Our Codespaces configuration is also updated for fast start up ⚡️. [try it now!]New experimental feature: load an image or a messy text, and ask AI to parse and clean it for you(!). [demo][10-01-2024] Initial release of Data Formulator, check out our [blog] and [video]! is an application from Microsoft Research that uses large language models to transform data, expediting the practice of data visualization.Data Formulator is an AI-powered tool for analysts to iteratively create rich visualizations. Unlike most chat-based AI tools where users need to describe everything in natural language, Data Formulator combines user interface interactions (UI) and natural language (NL) inputs for easier interaction. This blended approach makes it easier for users to describe their chart designs while delegating data transformation to AI.Play with Data Formulator with one of the following options:Option 1: Install via Python PIPUse Python PIP for an easy setup experience, running locally (recommend: install it in a virtual environment).# install data_formulator
pip install data_formulator

# start data_formulator
data_formulator 

# alternatively, you can run data formulator with this command
python -m data_formulator
Update: you can specify the port number (e.g., 8080) by python -m data_formulator --port 8080 if the default port is occupied.Option 2: Codespaces (5 minutes)You can also run Data Formulator in Codespaces; we have everything pre-configured. For more details, see CODESPACES.md.Option 3: Working in the developer modeYou can build Data Formulator locally if you prefer full control over your development environment and the ability to customize the setup to your specific needs. For detailed instructions, refer to DEVELOPMENT.md.Once you've completed the setup using either option, follow these steps to start using Data Formulator:The basics of data visualizationProvide OpenAI keys and select a model (GPT-4o suggested) and choose a dataset.Choose a chart type, and then drag-and-drop data fields to chart properties (x, y, color, ...) to specify visual encodings.Create visualization beyond the initial dataset (powered by 🤖)You can type names of fields that do not exist in current data in the encoding shelf: 
  this tells Data Formulator that you want to create visualizations that require computation or transformation from existing data,you can optionally provide a natural language prompt to explain and clarify your intent (not necessary when field names are self-explanatory).Click the  button. 
  Data Formulator will transform data and instantiate the visualization based on the encoding and prompt.Inspect the data, chart and code.To create a new chart based on existing ones, follow up in natural language: 
  provide a follow up prompt (e.g., ),you may also update visual encodings for the new chart.Repeat this process as needed to explore and understand your data. Your explorations are trackable in the  panel.@article{wang2024dataformulator2iteratively,
      title={Data Formulator 2: Iteratively Creating Rich Visualizations with AI}, 
      author={Chenglong Wang and Bongshin Lee and Steven Drucker and Dan Marshall and Jianfeng Gao},
      year={2024},
      booktitle={ArXiv preprint arXiv:2408.16119},
}
@article{wang2023data,
  title={Data Formulator: AI-powered Concept-driven Visualization Authoring},
  author={Wang, Chenglong and Thompson, John and Lee, Bongshin},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2023},
  publisher={IEEE}
}
This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com.When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.]]></content:encoded></item><item><title>hoppscotch/hoppscotch</title><link>https://github.com/hoppscotch/hoppscotch</link><author></author><category>trending</category><pubDate>Wed, 12 Feb 2025 02:26:49 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Open source API development ecosystem - https://hoppscotch.io (open-source alternative to Postman, Insomnia)❤️  Crafted with minimalistic UI design.⚡️  Send requests and get responses in real time.🗄️  Request methods define the type of action you are requesting to be performed. - Requests retrieve resource information - The server creates a new entry in a database - Updates an existing resource - Very similar to  but makes a partial update on a resource - Deletes resource or related component - Retrieve response headers identical to those of a GET request, but without the response body. - Establishes a tunnel to the server identified by the target resource - Describe the communication options for the target resource - Performs a message loop-back test along the path to the target resource - Some APIs use custom request methods such as . Type in your custom methods.🌈  Customizable combinations for background, foreground, and accent colors — customize now.Choose a theme: System preference, Light, Dark, and BlackChoose accent colors: Green, Teal, Blue, Indigo, Purple, Yellow, Orange, Red, and PinkDistraction-free Zen modeCustomized themes are synced with your cloud/local session.Instant loading with Service WorkersLow RAM/memory and CPU usage🚀  Retrieve response from endpoint instantly.Copy/share public "Share URL"Generate/copy request code snippets for 10+ languages and frameworks🔌  Establish full-duplex communication channels over a single TCP connection.📡  Receive a stream of updates from a server over an HTTP connection without resorting to polling.🌩  Send and Receive data with the SocketIO server.🦟  Subscribe and Publish to topics of an MQTT Broker.🔮  GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data.Set endpoint and get schemaSet custom request headers🔐  Allows to identify the end-user.📢  Describes the format the body of your request is being sent in.📫  Use request parameters to set varying parts in simulated requests.📃  Used to send and receive data via the REST API.FormData, JSON, and many moreToggle between key-value and RAW input parameter list📮  Contains the status line, headers, and the message/response body.Copy the response to the clipboardDownload the response as a fileView raw and preview HTML, image, JSON, and XML responses⏰  Request entries are synced with your cloud/local session storage.📁  Keep your API requests organized with collections and folders. Reuse them with a single click.Unlimited collections, folders, and requestsExport and import as a file or GitHub gistCollections are synced with your cloud/local session storage.📜  Snippets of code associated with a request that is executed before the request is sent.Set environment variablesInclude timestamp in the request headersSend a random alphanumeric string in the URL parameters👨‍👩‍👧‍👦  Helps you collaborate across your teams to design, develop, and test APIs faster.Create unlimited shared collectionsCreate unlimited team membersRole-based access control👥  Organize your personal and team collections environments into workspaces. Easily switch between workspaces to manage multiple projects.Create unlimited workspacesSwitch between personal and team workspaces⌨️  Optimized for efficiency.🌐  Enable Proxy Mode from Settings to access blocked APIs.Fixes  (Cross-Origin Resource Sharing) issuesAccess APIs served in non-HTTPS () endpoints🌎  Experience the app in your language.Help us to translate Hoppscotch. Please read  for details on our  and the process for submitting pull requests to us.☁️  Sign in and sync your data in real-time across all your devices.SSO (Single Sign-On)[^EE] Handoff to continue tasks on your other devices.✅  Write tests associated with a request that is executed after the request's response.Check the status code as an integerSet environment variables🌱  Environment variables allow you to store and reuse values in your requests and scripts.Unlimited environments and variablesInitialize through the pre-request scriptExport as / import from GitHub gist🚚  Edit key-value pairs in bulk.Entries are separated by newlineKeys and values are separated by Prepend  to any row you want to add but keep disabled🎛️  Manage your team and invite members.📦  Official add-ons for hoppscotch.Provide your API endpoint in the URL fieldClick "Send" to simulate the requestThis project owes its existence to the collective efforts of all those who contribute — contribute now.]]></content:encoded></item><item><title>codecrafters-io/build-your-own-x</title><link>https://github.com/codecrafters-io/build-your-own-x</link><author></author><category>trending</category><pubDate>Wed, 12 Feb 2025 02:26:49 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[What I cannot create, I do not understand — Richard Feynman.]]></content:encoded></item><item><title>labring/FastGPT</title><link>https://github.com/labring/FastGPT</link><author></author><category>trending</category><pubDate>Wed, 12 Feb 2025 02:26:49 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.FastGPT 是一个基于 LLM 大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过 Flow 可视化进行工作流编排，从而实现复杂的问答场景！项目技术栈：NextJs + TS + ChakraUI + MongoDB + PostgreSQL (PG Vector 插件)/Milvus我们非常欢迎各种形式的贡献。如果你对贡献代码感兴趣，可以查看我们的 GitHub Issues，大展身手，向我们展示你的奇思妙想。]]></content:encoded></item><item><title>CodePhiliaX/Chat2DB</title><link>https://github.com/CodePhiliaX/Chat2DB</link><author></author><category>trending</category><pubDate>Wed, 12 Feb 2025 02:26:49 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🔥🔥🔥AI-driven database tool and SQL client, The hottest GUI client, supporting MySQL, Oracle, PostgreSQL, DB2, SQL Server, DB2, SQLite, H2, ClickHouse, and more.Chat2DB is an intelligent, universal SQL client and data reporting tool that integrates AI capabilities. Chat2DB helps you write SQL queries faster, manage databases, generate reports, explore data, and interact with multiple databases. Chat2DB is an open-source project, and we welcome your contributions.1. Intelligent SQL Generation: Chat2DB Pro supports AI-driven intelligent SQL development to help you write SQL queries faster.: Supports more than 10 databases, including MySQL, PostgreSQL, H2, Oracle, SQLServer, SQLite, MariaDB, ClickHouse, DM, Presto, DB2, OceanBase, Hive, KingBase, MongoDB, Redis, Snowflake, and more.3. Intelligent Report Generation: Chat2DB Pro supports AI-driven intelligent data reporting to help you generate dashboards faster.4. Data Structure Synchronization: Chat2DB Pro supports database table structure synchronization to help you sync database table structures faster.Requires AI ConfigurationDatabase Structure Import/ExportCopy Results as Insert/UpdateDownload and InstallationChat2DB is a cross-platform application that supports Windows, MacOS, and Linux. You can download Chat2DB from the following links:Community Edition Docker InstallationBefore installing Chat2DB, ensure your system meets the following requirements:Docker Compose 1.25.0 or later  docker rm chat2db
  
  docker run --name=chat2db -ti -p 10824:10824 -v ~/.chat2db-docker:/root/.chat2db  chat2db/chat2db:latest

  docker start chat2db
  
Note: If local debugging is needed:Clone the repository locally$ git clone git@github.com:chat2db/Chat2DB.git
Node version must be 16 or higher  
Use yarn only, npm is not supported
$ cd Chat2DB/chat2db-client
$ yarn
$ yarn run start:web
$ cd ../chat2db-server
$ mvn clean install # Maven version 3.8 or higher is required
$ cd chat2db-server/chat2db-server-start/target/
$ java -jar -Dloader.path=./lib -Dchatgpt.apiKey=xxxxx chat2db-server-start.jar  # 需要安装java 17以上版本，启动应用 chatgpt.apiKey 需要输入ChatGPT的key,如果不输入无法使用AIGC功能
# chat2db-client
$ npm run build:web:prod 
$ cp -r dist ../chat2db-server/chat2db-server-start/src/main/resources/static/front 
$ cp -r dist/index.html ../chat2db-server/chat2db-server-start/src/main/resources/thymeleaf
Thanks to everyone who has contributed to Chat2DB~~]]></content:encoded></item><item><title>Shubhamsaboo/awesome-llm-apps</title><link>https://github.com/Shubhamsaboo/awesome-llm-apps</link><author></author><category>trending</category><pubDate>Wed, 12 Feb 2025 02:26:49 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and even open-source models like LLaMA that you can run locally on your computer.💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.RAG (Retrieval Augmented Generation)Advanced Tools and Frameworksgit clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
Navigate to the desired project directorycd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail
Install the required dependenciespip install -r requirements.txt
Follow the project-specific instructions in each project's  file to set up and run the app.🤝 Contributing to Open SourceContributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new GitHub Issue or submit a pull request. Make sure to follow the existing project structure and include a detailed  for each new app.Thank You, Community, for the Support! 🙏🌟 Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.]]></content:encoded></item><item><title>firefly-iii/firefly-iii</title><link>https://github.com/firefly-iii/firefly-iii</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Firefly III: a personal finances manager"Firefly III" is a (self-hosted) manager for your personal finances. It can help you keep track of your expenses and income, so you can spend less and save more. Firefly III supports the use of budgets, categories and tags. Using a bunch of external tools, you can import data. It also has many neat financial reports available.Firefly III should give you  into and  over your finances. Money should be useful, not scary. You should be able to  where it is going, to  your expenses and to... wow, I'm going overboard with this aren't I?But you get the idea: this is your money. These are your expenses. Stop them from controlling you. I built this tool because I started to dislike money. Having money, not having money, paying bills with money, you get the idea. But no more. I want to feel "safe", whatever my balance is. And I hope this tool can help you. I know it helps me.Personal financial management is pretty difficult, and everybody has their own approach to it. Some people make budgets, other people limit their cashflow by throwing away their credit cards, others try to increase their current cashflow. There are tons of ways to save and earn money. Firefly III works on the principle that if you know where your money is going, you can stop it from going there.By keeping track of your expenses and your income you can budget accordingly and save money. Stop living from paycheck to paycheck but give yourself the financial wiggle room you need.You can read more about the purpose of Firefly III in the documentation.Firefly III is pretty feature packed. Some important stuff first:It is completely self-hosted and isolated, and will never contact external servers until you explicitly tell it to.It features a REST JSON API that covers almost every part of Firefly III.The most exciting features are:Then the things that make you go "yeah OK, makes sense".And the things you would hope for but not expect:And to organise everything:Clear views that should show you how you're doing.Easy navigation through your records.Lots of charts because we all love them.This application is for people who want to track their finances, keep an eye on their money without having to upload their financial records to the cloud. You're a bit tech-savvy, you like open source software and you don't mind tinkering with (self-hosted) servers.The Firefly III eco-systemThere are many ways to run Firefly IIISonarcloud scans the code of Firefly III. If you want to help improve Firefly III, check out the latest reports and take your pick!Support the development of Firefly IIIIf you like Firefly III and if it helps you save lots of money, why not send me a dime for every dollar saved! 🥳Do you need help, or do you want to get in touch?Do you want to contact me? You can email me at james@firefly-iii.org or get in touch through one of the following support channels:The Firefly III logo is made by the excellent Cherie Woo.]]></content:encoded></item><item><title>n0-computer/iroh</title><link>https://github.com/n0-computer/iroh</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[peer-2-peer that just works less net work for networks Iroh gives you an API for dialing by public key. You say “connect to that phone”, iroh will find & maintain the fastest connection for you, regardless of where it is.The fastest route is a direct connection, so if necessary, iroh tries to hole-punch. Should this fail, it can fall back to an open ecosystem of public relay servers. To ensure these connections are as fast as possible, we continuously measure iroh.Iroh uses Quinn to establish QUIC connections between nodes. This way you get authenticated encryption, concurrent streams with stream priorities, a datagram transport and avoid head-of-line-blocking out of the box.Use pre-existing protocols built on iroh instead of writing your own:iroh-blobs for BLAKE3-based content-addressed blob transfer scaling from kilobytes to terabytesiroh-gossip for establishing publish-subscribe overlay networks that scale, requiring only resources that your average phone can handleIt's easiest to use iroh from rust. Install it using , then on the connecting side:const ALPN: &[u8] = b"iroh-example/echo/0";

let endpoint = Endpoint::builder().discovery_n0().bind().await?;

// Open a connection to the accepting node
let conn = endpoint.connect(addr, ALPN).await?;

// Open a bidirectional QUIC stream
let (mut send, mut recv) = conn.open_bi().await?;

// Send some data to be echoed
send.write_all(b"Hello, world!").await?;
send.finish()?;

// Receive the echo
let response = recv.read_to_end(1000).await?;
assert_eq!(&response, b"Hello, world!");

// Close the endpoint and all its connections
endpoint.close().await;
And on the accepting side:let endpoint = Endpoint::builder().discovery_n0().bind().await?;

let router = Router::builder(endpoint)
    .accept(ALPN.to_vec(), Arc::new(Echo))
    .spawn()
    .await?;

// The protocol definition:
#[derive(Debug, Clone)]
struct Echo;

impl ProtocolHandler for Echo {
    fn accept(self: Arc<Self>, connecting: Connecting) -> BoxedFuture<Result<()>> {
        Box::pin(async move {
            let connection = connecting.await?;
            let (mut send, mut recv) = connection.accept_bi().await?;

            // Echo any bytes received back directly.
            let bytes_sent = tokio::io::copy(&mut recv, &mut send).await?;

            send.finish()?;
            connection.closed().await;

            Ok(())
        })
    }
}
The full example code with more comments can be found at .If you want to use iroh from other languages, make sure to check out iroh-ffi, the repository for FFI bindings.This repository contains a workspace of crates:: The core library for hole-punching & communicating with relays.: The relay server implementation. This is the code we run in production (and you can, too!).: Common types like , key types or .: DNS server implementation powering the  for NodeIds, running at dns.iroh.link.: Analyzes your host's networking ability & NAT.This project is licensed under either ofUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in this project by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.]]></content:encoded></item><item><title>T8RIN/ImageToolbox</title><link>https://github.com/T8RIN/ImageToolbox</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🖼️ Image Toolbox is a powerful app for advanced image manipulation. It offers dozens of features, from basic tools like crop and draw to filters, OCR, and a wide range of image processing optionsImageToolbox is a versatile image editing tool designed for efficient photo manipulation. It allows users to crop, apply filters, edit EXIF data, erase backgrounds, and even convert images to PDFs. Ideal for both photographers and developers, the tool offers a simple interface with powerful capabilities.Check out Image Toolbox Wiki for FAQ and useful info  Join our chat where you can discuss anything you want and also look into the CI channel where I post betas and announcements 
 This application is completely free, but if you want to support the project development, you can send a donation to the crypto wallets below17Pk1RurnkJxLV9V7mc6Y7dLyHFb9rvQDq <- TMPAu7a54NvQNEKnNWh3naXu3oYijqP3U7 <- Go to the Releases and the download latest apk or click one of the badges below.Clone the repository: git clone https://github.com/yourusername/ImageToolbox.git
Install dependencies using your preferred package manager (e.g., Gradle).Build the project: bash ./gradlew buildRun the application: bash ./gradlew runSelecting Emoji for top app barAbility to use Pixel like switch instead of Material YouMaximum brightness for selected screensEnabling or Disabling confettiCustom app color scheme 
  Controlling borders thicknessEnabling and disabling each existing shadowMonet implementation (Dynamic colors) even for Android versions less than 12 by Dynamic ThemeIcons Background shape selection 
  Custom fonts 
  Ability to import any font (OTF/TTF) to further useIn app font scale changingChanging between options list and grouped viewConfetti Type selection 
  Switch Type selection: 
  Slider Type Selection: 
  (Yes, the app supports dynamic coloring based on wallpapers for every android version)Dynamic Theme - library, which allows you to easily implement custom color theming.Modal Sheet - modal bottom sheet that follows M3 guidelines.Flow to emit values from data layer reactively.Decompose - KMP lifecycle-aware business logic components (aka BLoCs) with routing (navigation) and pluggable UIHilt for dependency injection.Konfetti to establish beautiful particle system.Compose - Modern Declarative UI style framework based on composable functions.Data Store - Store data asynchronously, consistently, and transactionally.Lifecycle - Observe Android lifecycles and handle UI states upon the lifecycle changes.GPU Image for creating and applying filters to the images.Aire and Trickle for creating and applying filters to the images on CPU using native cpp code.]]></content:encoded></item><item><title>albertan017/LLM4Decompile</title><link>https://github.com/albertan017/LLM4Decompile</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Reverse Engineering: Decompiling Binary Code with Large Language ModelsReverse Engineering: Decompiling Binary Code with Large Language Models[2024-10-17]: Release decompile-ghidra-100k, a subset of 100k training samples (25k per optimization level). We provide a training script that runs in ~3.5 hours on a single A100 40G GPU. It achieves a 0.26 re-executability rate, with a total cost of under $20 for quick replication of LLM4Decompile.[2024-09-26]: Update a Colab notebook to demonstrate the usage of the LLM4Decompile model, including examples for the LLM4Decompile-End and LLM4Decompile-Ref models.[2024-06-19]: Release V2 series (LLM4Decompile-Ref). V2 (1.3B-22B), building upon , are trained on 2 billion tokens to  the decompiled pseudo-code from Ghidra. The 22B-V2 version outperforms the 6.7B-V1.5 by an additional 40.1%. Please check the ghidra folder for details.[2024-05-13]: Release V1.5 series (LLM4Decompile-End, directly decompile binary using LLM). V1.5 are trained with a larger dataset (15B tokens) and a maximum token , with remarkable performance (over ) compared to the previous model.[2024-03-16]: Add llm4decompile-6.7b-uo model which is trained without prior knowledge of the optimization levels (O0~O3), the average re-executability is around 0.219, performs the best in our models. is the pioneering open-source large language model dedicated to decompilation. Its current version supports decompiling Linux x86_64 binaries, ranging from GCC's O0 to O3 optimization levels, into human-readable C source code. Our team is committed to expanding this tool's capabilities, with ongoing efforts to incorporate a broader range of architectures and configurations. focuses on decompiling the binary directly.  refines the pseudo-code decompiled by Ghidra.During compilation, the Preprocessor processes the source code (SRC) to eliminate comments and expand macros or includes. The cleaned code is then forwarded to the Compiler, which converts it into assembly code (ASM). This ASM is transformed into binary code (0s and 1s) by the Assembler. The Linker finalizes the process by linking function calls to create an executable file. Decompilation, on the other hand, involves converting binary code back into a source file. LLMs, being trained on text, lack the ability to process binary data directly. Therefore, binaries must be disassembled by  into assembly language (ASM) first. It should be noted that binary and disassembled ASM are equivalent, they can be interconverted, and thus we refer to them interchangeably. Finally, the loss is computed between the decompiled code and source code to guide the training. To assess the quality of the decompiled code (SRC'), it is tested for its functionality through test assertions (re-executability). evaluates whether the decompiled code can execute properly and pass all the predefined test cases. A collection of 164 C functions that exclusively rely on  C libraries. A collection of 2,621 functions drawn from  projects, each utilizing user-defined functions, structures, and macros.Our LLM4Decompile includes models with sizes between 1.3 billion and 33 billion parameters, and we have made these models available on Hugging Face.Note 3: V1.5 series are trained with a larger dataset (15B tokens) and a maximum token size of 4,096, with remarkable performance (over 100% improvement) compared to the previous model.Note 4: V2 series are built upon  and trained on 2 billion tokens to  the decompiled pseudo-code from Ghidra. Check ghidra folder for details. Please use the script below to install the necessary environment.git clone https://github.com/albertan017/LLM4Decompile.git
cd LLM4Decompile
conda create -n 'llm4decompile' python=3.9 -y
conda activate llm4decompile
pip install -r requirements.txt
Here is an example of how to use our model (Revised for V1.5. For previous models, please check the corresponding model page at HF). Note: Replace the "func0" with the function name you want to decompile. Compile the C code into binary, and disassemble the binary into assembly instructions.import subprocess
import os
func_name = 'func0'
OPT = ["O0", "O1", "O2", "O3"]
fileName = 'samples/sample' #'path/to/file'
for opt_state in OPT:
    output_file = fileName +'_' + opt_state
    input_file = fileName+'.c'
    compile_command = f'gcc -o {output_file}.o {input_file} -{opt_state} -lm'#compile the code with GCC on Linux
    subprocess.run(compile_command, shell=True, check=True)
    compile_command = f'objdump -d {output_file}.o > {output_file}.s'#disassemble the binary file into assembly instructions
    subprocess.run(compile_command, shell=True, check=True)
    
    input_asm = ''
    with open(output_file+'.s') as f:#asm file
        asm= f.read()
        if '<'+func_name+'>:' not in asm: #IMPORTANT replace func0 with the function name
            raise ValueError("compile fails")
        asm = '<'+func_name+'>:' + asm.split('<'+func_name+'>:')[-1].split('\n\n')[0] #IMPORTANT replace func0 with the function name
        asm_clean = ""
        asm_sp = asm.split("\n")
        for tmp in asm_sp:
            if len(tmp.split("\t"))<3 and '00' in tmp:
                continue
            idx = min(
                len(tmp.split("\t")) - 1, 2
            )
            tmp_asm = "\t".join(tmp.split("\t")[idx:])  # remove the binary code
            tmp_asm = tmp_asm.split("#")[0].strip()  # remove the comments
            asm_clean += tmp_asm + "\n"
    input_asm = asm_clean.strip()
    before = f"# This is the assembly code:\n"#prompt
    after = "\n# What is the source code?\n"#prompt
    input_asm_prompt = before+input_asm.strip()+after
    with open(fileName +'_' + opt_state +'.asm','w',encoding='utf-8') as f:
        f.write(input_asm_prompt)
Assembly instructions should be in the format:<FUNCTION_NAME>:\nOPERATIONS\nOPERATIONS\nTypical assembly instructions may look like this:<func0>:
endbr64
lea    (%rdi,%rsi,1),%eax
retq
 Use LLM4Decompile to translate the assembly instructions into C:from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = 'LLM4Binary/llm4decompile-6.7b-v1.5' # V1.5 Model
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.bfloat16).cuda()

with open(fileName +'_' + OPT[0] +'.asm','r') as f:#optimization level O0
    asm_func = f.read()
inputs = tokenizer(asm_func, return_tensors="pt").to(model.device)
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=2048)### max length to 4096, max new tokens should be below the range
c_func_decompile = tokenizer.decode(outputs[0][len(inputs[0]):-1])

with open(fileName +'.c','r') as f:#original file
    func = f.read()

print(f'original function:\n{func}')# Note we only decompile one function, where the original file may contain multiple functions
print(f'decompiled function:\n{c_func_decompile}')
Data are stored in llm4decompile/decompile-eval/decompile-eval-executable-gcc-obj.json, using JSON list format. There are 164*4 (O0, O1, O2, O3) samples, each with five keys:: indicates the ID of the problem.: the optimization stage, is one of [O0, O1, O2, O3].: C solution for HumanEval problem.: C test assertions.Larger training dataset with the cleaning process. (done:2024.05.13)Support for popular languages/platforms and settings.Support for executable binaries. (done:2024.05.13)Integration with decompilation tools (e.g., Ghidra, Rizin)This code repository is licensed under the MIT and DeepSeek License.@misc{tan2024llm4decompile,
      title={LLM4Decompile: Decompiling Binary Code with Large Language Models}, 
      author={Hanzhuo Tan and Qi Luo and Jing Li and Yuqun Zhang},
      year={2024},
      eprint={2403.05286},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}
]]></content:encoded></item><item><title>ChrisTitusTech/winutil</title><link>https://github.com/ChrisTitusTech/winutil</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Chris Titus Tech's Windows Utility - Install Programs, Tweaks, Fixes, and UpdatesThis utility is a compilation of Windows tasks I perform on each Windows system I use. It is meant to streamline , debloat with , troubleshoot with , and fix Windows . I am extremely picky about any contributions to keep this project clean and efficient.Winutil must be run in Admin mode because it performs system-wide tweaks. To achieve this, run PowerShell as an administrator. Here are a few ways to do it:Right-click on the start menu.Choose "Windows PowerShell (Admin)" (for Windows 10) or "Terminal (Admin)" (for Windows 11).Search and Launch Method:Type "PowerShell" or "Terminal" (for Windows 11).Press  or Right-click and choose "Run as administrator" to launch it with administrator privileges.Stable Branch (Recommended)irm "https://christitus.com/win" | iex
irm "https://christitus.com/windev" | iex
These are the sponsors that help keep this project alive with monthly contributions.🏅 Thanks to all ContributorsThanks a lot for spending your time helping Winutil grow. Thanks a lot! Keep rocking 🍻.]]></content:encoded></item><item><title>godotengine/godot</title><link>https://github.com/godotengine/godot</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Godot Engine – Multi-platform 2D and 3D game engine2D and 3D cross-platform game engineGodot Engine is a feature-packed, cross-platform game engine to create 2D and 3D games from a unified interface. It provides a comprehensive set of common tools, so that users can focus on making games without having to reinvent the wheel. Games can be exported with one click to a number of platforms, including the major desktop platforms (Linux, macOS, Windows), mobile platforms (Android, iOS), as well as Web-based platforms and consoles.Free, open source and community-drivenGodot is completely free and open source under the very permissive MIT license. No strings attached, no royalties, nothing. The users' games are theirs, down to the last line of engine code. Godot's development is fully independent and community-driven, empowering users to help shape their engine to match their expectations. It is supported by the Godot Foundation not-for-profit.Before being open sourced in February 2014, Godot had been developed by Juan Linietsky and Ariel Manzur (both still maintaining the project) for several years as an in-house engine, used to publish several work-for-hire titles.Community and contributingGodot is not only an engine but an ever-growing community of users and engine developers. The main community channels are listed on the homepage.To get started contributing to the project, see the contributing guide. This document also includes guidelines for reporting bugs.]]></content:encoded></item><item><title>unionlabs/union</title><link>https://github.com/unionlabs/union</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[The trust-minimized, zero-knowledge bridging protocol, designed for censorship resistance, extremely high security, and usage in decentralized finance.Union is the hyper-efficient zero-knowledge infrastructure layer for general message passing, asset transfers, NFTs, and DeFi. It’s based on Consensus Verification and has no dependencies on trusted third parties, oracles, multi-signatures, or MPC. It implements IBC for compatibility with Cosmos chains and connects to EVM chains like Ethereum, Berachain (beacon-kit), Arbitrum, and more.The upgradability of contracts on other chains, connections, token configurations, and evolution of the protocol will all be controlled by decentralized governance, aligning the priorities of Union with its users, validators, and operators.curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install
(Note that some components can only be built on Linux. If you are using macOS, we recommend using OrbStack to easily set up a NixOS VM within two minutes. Most Union developers use macOS with OrbStack, and there is no need to install Nix inside of the NixOS VM.)You can now  build any of Union's components from source:nix build .#uniond -L
nix build .#voyager -L
nix build .#app -L

# to see all packages, run:
nix flake show
The result of whatever you build will be in You can now also enter our dev shell, which has all of the dependencies (, , , , etc.) you need to work on any component: (Don't worry, this will not affect your system outside of this repo)Run the following to format the entire repo and check your spelling before each PR:Check the  channel on Union's discord if you need any help with this.The official docs are hosted here. Each individual component also has accompanying developer documentation for contributors, which you can find in each .]]></content:encoded></item><item><title>potpie-ai/potpie</title><link>https://github.com/potpie-ai/potpie</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Prompt-To-Agent : Create custom engineering agents for your codebasePotpie is an open-source platform that creates AI agents specialized in your codebase, enabling automated code analysis, testing, and development tasks. By building a comprehensive knowledge graph of your code, Potpie's agents can understand complex relationships and assist with everything from debugging to feature development.🧠 : Built-in knowledge graph captures relationships between code components🤖 Pre-built & Custom Agents: Ready-to-use agents for common tasks + build your own🔄 : Works with your existing development workflow📈 : Handles codebases of any size or languageBring the power of Potpie's AI agents directly into your development environment with our VSCode extension:: Access all Potpie agents without leaving your editor: Ask questions, get explanations, and implement suggestions right where you code🤖 Potpie's Prebuilt AgentsPotpie offers a suite of specialized codebase agents for automating and optimizing key aspects of software development:: Automatically analyzes stacktraces and provides debugging steps specific to your codebase.: Answers questions about your codebase and explains functions, features, and architecture.: Analyzes code changes, identifies affected APIs, and suggests improvements before merging.: Generates integration test plans and code for flows to ensure components work together properly.: Automatically creates unit test plan and code for individual functions to enhance test coverage.: Creates a low level design for implementing a new feature by providing functional requirements to this agent.: Generates code for new features, refactors existing code, and suggests optimizations.🛠️ Potpie's Tooling SystemPotpie provides a set of tools that agents can use to interact with the knowledge graph and the underlying infrastructure:get_code_from_probable_node_name: Retrieves code snippets based on a probable node name.: Fetches code associated with a specific node ID.get_code_from_multiple_node_ids: Retrieves code snippets for multiple node IDs simultaneously.ask_knowledge_graph_queries: Executes vector similarity searches to obtain relevant information.: Retrieves nodes tagged with specific keywords.get_code_graph_from_node_id/name: Fetches code graph structures for a specific node.: Detects changes in the current branch compared to the default branch.: Retrieves the file structure of the codebase.Docker installed and runningGit installed (for repository access)Create a  file based on the Add the following required configurations:isDevelopmentMode=enabled
ENV=development
OPENAI_API_KEY=<your-openai-key>
POSTGRES_SERVER=postgresql://postgres:mysecretpassword@localhost:5432/momentum
NEO4J_URI=bolt://127.0.0.1:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=mysecretpassword
REDISHOST=127.0.0.1
REDISPORT=6379
BROKER_URL=redis://127.0.0.1:6379/0
CELERY_QUEUE_NAME=dev
defaultUsername=defaultuser
PROJECT_PATH=projects #repositories will be downloaded/cloned to this path on your system.
Create a Virtual Environment using Python 3.10:python3.10 -m venv venv
source venv/bin/activate
alternatively, you can also use the  library.Install dependencies in your venv:pip install -r requirements.txt
chmod +x start.sh
./start.sh
 (Skip this step in development mode)curl -X POST 'http://localhost:8001/api/v1/login' \
  -H 'Content-Type: application/json' \
  -d '{
    "email": "your-email",
    "password": "your-password"
  }'
# Save the bearer token from the response for subsequent requests
Initialize Repository Parsing# For development mode:
curl -X POST 'http://localhost:8001/api/v1/parse' \
  -H 'Content-Type: application/json' \
  -d '{
    "repo_path": "path/to/local/repo",
    "branch_name": "main"
  }'

# For production mode:
curl -X POST 'http://localhost:8001/api/v1/parse' \
  -H 'Content-Type: application/json' \
  -d '{
    "repo_name": "owner/repo-name",
    "branch_name": "main"
  }'
# Save the project_id from the response
curl -X GET 'http://localhost:8001/api/v1/parsing-status/your-project-id'
# Wait until parsing is complete
curl -X GET 'http://localhost:8001/api/v1/list-available-agents/?list_system_agents=true'
# Note down the agent_id you want to use
curl -X POST 'http://localhost:8001/api/v1/conversations/' \
  -H 'Content-Type: application/json' \
  -d '{
    "user_id": "your_user_id",
    "title": "My First Conversation",
    "status": "active",
    "project_ids": ["your-project-id"],
    "agent_ids": ["chosen-agent-id"]
  }'
# Save the conversation_id from the response
Start Interacting with Your Agentcurl -X POST 'http://localhost:8001/api/v1/conversations/your-conversation-id/message/' \
  -H 'Content-Type: application/json' \
  -d '{
    "content": "Your question or request here"
  }'
View Conversation History (Optional)curl -X GET 'http://localhost:8001/api/v1/conversations/your-conversation-id/messages/?start=0&limit=10'
: For developers new to a codebase, the codebase QnA agent helps them understand the codebase and get up to speed quickly. Ask it how to setup a new project, how to run the tests etcWe tried to onboard ourselves with Potpie to the  codebase and it worked like a charm : Video here.: Answer questions about any library you're integrating, explain functions, features, and architecture.We used the Q&A agent to understand the underlying working of a feature of the  codebase that was not documented in official docs : Video here.: Get detailed implementation plans for new features or improvements before writing code.We fed an open issue from the  project to this agent to generate a low level design for it: Video here.: Understand the functional impact of changes and compute the blast radius of modifications.Here we analyse a PR from the  codebase and understand its blast radius : Video here.: Get step-by-step debugging guidance based on stacktraces and codebase context.: Generate contextually aware unit and integration test plans and test code that understand your codebase's structure and purpose.With Custom Agents, you can design personalized tools that handle repeatable tasks with precision. Key components include:: Define the agent's task, goal, and expected output: Metadata about the agent's role and context: Individual steps for job completion: Functions for querying the knowledge graph or retrieving code🗝️ Accessing Agents via API KeyYou can access Potpie Agents through an API key, enabling integration into CI/CD workflows and other automated processes. For detailed instructions, please refer to the Potpie API documentation.: Easily create an API key for secure access.: Use the Parse API to analyze code repositories and obtain a project ID.: Check the status of your parsing requests.: Initiate conversations with specific agents using project and agent IDs adn get a conversation id.: Communicate with agents by sending messages within a conversation.Potpie is designed to be flexible and customizable. Here are key areas to personalize your own deployment:1. System Prompts ConfigurationModify prompts in app/modules/intelligence/prompts/system_prompt_setup.pyCreate new agents in app/modules/intelligence/agents/chat_agents and app/modules/intelligence/agents/agentic_tools3. Agent Behavior CustomizationModify guidelines within each agent's prompt in the app/modules/intelligence/agents directoryEdit or add tools in the app/modules/intelligence/tools directoryWe welcome contributions! To contribute:Create a new branch (git checkout -b feature-branch)Commit (git commit -m 'Add new feature')Push to the branch (git push origin feature-branch)This project is licensed under the Apache 2.0 License - see the LICENSE file for details.💪 Thanks To All ContributorsThanks for spending your time helping build Potpie. Keep rocking 🥂]]></content:encoded></item><item><title>vercel/ai-chatbot</title><link>https://github.com/vercel/ai-chatbot</link><author></author><category>trending</category><pubDate>Tue, 11 Feb 2025 02:27:22 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[A full-featured, hackable Next.js AI chatbot built by Vercel An Open-Source AI Chatbot Template Built With Next.js and the AI SDK by Vercel. Next.js App Router 
  Advanced routing for seamless navigation and performanceReact Server Components (RSCs) and Server Actions for server-side rendering and increased performanceAI SDKUnified API for generating text, structured objects, and tool calls with LLMsHooks for building dynamic chat and generative user interfacesSupports OpenAI (default), Anthropic, Cohere, and other model providersYou can deploy your own version of the Next.js AI Chatbot to Vercel with one click:Note: You should not commit your  file or it will expose secrets that will allow others to control access to your various OpenAI and authentication provider accounts.Install Vercel CLI: Link local instance with Vercel and GitHub accounts (creates  directory): Download your environment variables: ]]></content:encoded></item><item><title>RockChinQ/LangBot</title><link>https://github.com/RockChinQ/LangBot</link><author></author><category>trending</category><pubDate>Mon, 10 Feb 2025 02:28:02 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[😎丰富生态、🧩支持扩展、🦄多模态 - 大模型原生即时通信机器人平台 🤖 | 适配 QQ / 微信（企业微信、个人微信）/ 飞书（feishu）/ Discord / OneBot 等消息平台 | 支持 OpenAI GPT、ChatGPT、DeepSeek、Dify、Claude、Gemini、Ollama、LM Studio、SiliconFlow、Qwen、Moonshot、ChatGLM 等 LLM 的机器人 / Agent | LLM-based instant messaging bots platform, supports Discord, WeChat, Lark, QQ platform, OpenAI ChatGPT, DeepSeek.💬 大模型对话、Agent：支持多种大模型，适配群聊和私聊；具有多轮对话、工具调用、多模态能力，并深度适配 Dify。目前支持 QQ、QQ频道、企业微信、飞书、Discord、个人微信，后续还将支持 WhatsApp、Telegram 等平台。🛠️ 高稳定性、功能完备：原生支持访问控制、限速、敏感词过滤等机制；配置简单，支持多种部署方式。🧩 插件扩展、活跃社区：支持事件驱动、组件扩展等插件机制；丰富生态，目前已有数十个插件😻 [New] Web 管理面板：支持通过浏览器管理 LangBot 实例，具体支持功能，查看文档已上架宝塔面板，若您已安装宝塔面板，可以根据文档使用。LangBot 离不开以下贡献者和社区内所有人的贡献，我们欢迎任何形式的贡献和反馈。]]></content:encoded></item><item><title>practical-tutorials/project-based-learning</title><link>https://github.com/practical-tutorials/project-based-learning</link><author></author><category>trending</category><pubDate>Mon, 10 Feb 2025 02:28:02 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Curated list of project-based tutorialsA list of programming tutorials in which aspiring software developers learn how to build an application from scratch. These tutorials are divided into different primary programming languages. Tutorials may involve multiple technologies and languages.To get started, simply fork this repo. Please refer to CONTRIBUTING.md for contribution guidelines.Others (Hapi, Express...):]]></content:encoded></item><item><title>AUTOMATIC1111/stable-diffusion-webui</title><link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link><author></author><category>trending</category><pubDate>Mon, 10 Feb 2025 02:28:02 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[A web interface for Stable Diffusion, implemented using Gradio library.Original txt2img and img2img modesOne click install and run script (but you still must install python and git)Attention, specify parts of text that the model should pay more attention to 
  a man in a  - will pay more attention to tuxedoa man in a  - alternative syntaxselect text and press  or  (or  or  if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)Loopback, run img2img processing multiple timesX/Y/Z plot, a way to draw a 3 dimensional plot of images with different parametersTextual Inversion 
  have as many embeddings as you want and use any names you like for themuse multiple embeddings with different numbers of vectors per tokenworks with half precision floating point numberstrain embeddings on 8GB (also reports of 6GB working)Extras tab with: 
  GFPGAN, neural network that fixes facesCodeFormer, face restoration tool as an alternative to GFPGANRealESRGAN, neural network upscalerESRGAN, neural network upscaler with a lot of third party modelsSwinIR and Swin2SR (see here), neural network upscalersLDSR, Latent diffusion super resolution upscalingResizing aspect ratio optionsSampling method selection 
  Adjust sampler eta values (noise multiplier)More advanced noise setting optionsInterrupt processing at any time4GB video card support (also reports of 2GB working)Correct seeds for batchesLive prompt token length validationGeneration parameters 
  parameters you used to generate images are saved with that imagein PNG chunks for PNG, in EXIF for JPEGcan drag the image to PNG info tab to restore generation parameters and automatically copy them into UIcan be disabled in settingsdrag and drop an image/text-parameters to promptboxRead Generation Parameters Button, loads parameters in promptbox to UIRunning arbitrary python code from UI (must run with  to enable)Mouseover hints for most UI elementsPossible to change defaults/mix/max/step values for UI elements via text configTiling support, a checkbox to create images that can be tiled like texturesProgress bar and live image generation preview 
  Can use a separate neural network to produce previews with almost none VRAM or compute requirementNegative prompt, an extra text field that allows you to list what you don't want to see in generated imageStyles, a way to save part of prompt and easily apply them via dropdown laterVariations, a way to generate same image but with tiny differencesSeed resizing, a way to generate same image but at slightly different resolutionCLIP interrogator, a button that tries to guess prompt from an imagePrompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midwayBatch Processing, process a group of files using img2imgImg2img Alternative, reverse Euler method of cross attention controlHighres Fix, a convenience option to produce high resolution pictures in one click without usual distortionsReloading checkpoints on the flyCheckpoint Merger, a tab that allows you to merge up to 3 checkpoints into oneComposable-Diffusion, a way to use multiple prompts at once 
  separate prompts using uppercase also supports weights for prompts: a cat :1.2 AND a dog AND a penguin :2.2No token limit for prompts (original stable diffusion lets you use up to 75 tokens)DeepDanbooru integration, creates danbooru style tags for anime promptsxformers, major speed increase for select cards: (add  to commandline args)via extension: History tab: view, direct and delete images conveniently within the UITraining tab 
  hypernetworks and embeddings optionsPreprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)Loras (same as Hypernetworks but more pretty)A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your promptCan select to load a different VAE from settings screenEstimated completion time in progress barNow without any bad letters!Load checkpoints in safetensors formatEased resolution restriction: generated image's dimensions must be a multiple of 8 rather than 64Reorder elements in the UI from settings screenMake sure the required dependencies are met and follow the instructions available for:Alternatively, use online services (like Google Colab):Installation on Windows 10/11 with NVidia-GPUs using release packageDownload  from v1.0.0-pre and extract its contents.Automatic Installation on WindowsInstall Python 3.10.6 (Newer version of Python does not support torch), checking "Add Python to PATH".Download the stable-diffusion-webui repository, for example by running git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git.Run  from Windows Explorer as normal, non-administrator, user.Automatic Installation on LinuxInstall the dependencies:# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
If your system is very new, you need to install python3.11 or python3.10:# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd="python3.11"
# or in webui-user.sh
python_cmd="python3.11"
Navigate to the directory you would like the webui to be installed and execute the following command:wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
Or just clone the repo wherever you want:git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
Check  for options.Installation on Apple SiliconFind the instructions here.The documentation was moved from this README over to the project's wiki.For the purposes of getting Google and other search engines to crawl the wiki, here's a link to the (not for humans) crawlable wiki.Licenses for borrowed code can be found in  screen, and also in  file.]]></content:encoded></item><item><title>yamadashy/repomix</title><link>https://github.com/yamadashy/repomix</link><author></author><category>trending</category><pubDate>Mon, 10 Feb 2025 02:28:02 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[📦 Repomix (formerly Repopack) is a powerful tool that packs your entire repository into a single, AI-friendly file. Perfect for when you need to feed your codebase to Large Language Models (LLMs) or other AI tools like Claude, ChatGPT, DeepSeek, Perplexity, Gemini, Gemma, Llama, Grok, and more.Pack your codebase into AI-friendly formats Need discussion? Join us on Discord!Share your experience and tipsStay updated on new featuresGet help with configuration and usage📦 Repomix is a powerful tool that packs your entire repository into a single, AI-friendly file. It is perfect for when you need to feed your codebase to Large Language Models (LLMs) or other AI tools like Claude, ChatGPT, DeepSeek, Perplexity, Gemini, Gemma, Llama, Grok, and more.🎉 New: Repomix Website & Discord Community!We look forward to seeing you there!: Formats your codebase in a way that's easy for AI to understand and process.: Provides token counts for each file and the entire repository, useful for LLM context limits.: You need just one command to pack your entire repository.: Easily configure what to include or exclude.: Automatically respects your .gitignore files.: Incorporates Secretlint for robust security checks to detect and prevent inclusion of sensitive information.You can try Repomix instantly in your project directory without installation:Or install globally for repeated use:# Install using npm
npm install -g repomix

# Alternatively using yarn
yarn global add repomix

# Alternatively using Homebrew (macOS/Linux)
brew install repomix

# Then run in any project directory
repomix
That's it! Repomix will generate a  file in your current directory, containing your entire repository in an AI-friendly format.You can then send this file to an AI assistant with a prompt like:This file contains all the files in the repository combined into one.
I want to refactor the code, so please review it first.
When you propose specific changes, the AI might be able to generate code accordingly. With features like Claude's Artifacts, you could potentially output multiple files, allowing for the generation of multiple interdependent pieces of code.Want to try it quickly? Visit the official website at repomix.com. Simply enter your repository name, fill in any optional details, and click the  button to see your generated output.The website offers several convenient features:Customizable output format (Plain Text, XML, or Markdown)Instant token count estimationUsing The VSCode Extension ⚡️A community-maintained VSCode extension lets you run Repomix right inside your editor with just a few clicks. Run it on any folder, manage outputs seamlessly, and control everything through VSCode's intuitive interface.Want your output as a file or just the content? Need automatic cleanup? This extension has you covered. Plus, it works smoothly with your existing repomix.config.json.To pack your entire repository:To pack a specific directory:repomix path/to/directory
repomix --include "src/**/*.ts,**/*.md"
To exclude specific files or directories:repomix --ignore "**/*.log,tmp/"
To pack a remote repository:repomix --remote https://github.com/yamadashy/repomix

# You can also use GitHub shorthand:
repomix --remote yamadashy/repomix

# You can specify the branch name, tag, or commit hash:
repomix --remote https://github.com/yamadashy/repomix --remote-branch main

# Or use a specific commit hash:
repomix --remote https://github.com/yamadashy/repomix --remote-branch 935b695

# Another convenient way is specifying the branch's URL
repomix --remote https://github.com/yamadashy/repomix/tree/main

# Commit's URL is also supported
repomix --remote https://github.com/yamadashy/repomix/commit/836abcd7335137228ad77feb28655d85712680f1

To initialize a new configuration file ():Once you have generated the packed file, you can use it with Generative AI tools like Claude, ChatGPT, and Gemini.You can also run Repomix using Docker. This is useful if you want to run Repomix in an isolated environment or prefer using containers.Basic usage (current directory):docker run -v .:/app -it --rm ghcr.io/yamadashy/repomix
To pack a specific directory:docker run -v .:/app -it --rm ghcr.io/yamadashy/repomix path/to/directory
Process a remote repository and output to a  directory:docker run -v ./output:/app -it --rm ghcr.io/yamadashy/repomix --remote https://github.com/yamadashy/repomix
Once you have generated the packed file with Repomix, you can use it with AI tools like Claude, ChatGPT, and Gemini. Here are some example prompts to get you started:Code Review and RefactoringFor a comprehensive code review and refactoring suggestions:This file contains my entire codebase. Please review the overall structure and suggest any improvements or refactoring opportunities, focusing on maintainability and scalability.
To generate project documentation:Based on the codebase in this file, please generate a detailed README.md that includes an overview of the project, its main features, setup instructions, and usage examples.
For generating test cases:Analyze the code in this file and suggest a comprehensive set of unit tests for the main functions and classes. Include edge cases and potential error scenarios.
Evaluate code quality and adherence to best practices:Review the codebase for adherence to coding best practices and industry standards. Identify areas where the code could be improved in terms of readability, maintainability, and efficiency. Suggest specific changes to align the code with best practices.
Get a high-level understanding of the libraryThis file contains the entire codebase of library. Please provide a comprehensive overview of the library, including its main purpose, key features, and overall architecture.
Feel free to modify these prompts based on your specific needs and the capabilities of the AI tool you're using.Which AI tools they're using with RepomixEffective prompts they've discoveredHow Repomix has helped themTips and tricks for getting the most out of AI code analysisFeel free to join the discussion and share your own experiences! Your insights could help others make better use of Repomix.Repomix generates a single file with clear separators between different parts of your codebase. To enhance AI comprehension, the output file begins with an AI-oriented explanation, making it easier for AI models to understand the context and structure of the packed repository.Plain Text Format (default)This file is a merged representation of the entire codebase, combining all repository files into a single document.

================================================================
File Summary
================================================================
(Metadata and usage AI instructions)

================================================================
Directory Structure
================================================================
src/
  cli/
    cliOutput.ts
    index.ts
  config/
    configLoader.ts

(...remaining directories)

================================================================
Files
================================================================

================
File: src/index.js
================
// File contents here

================
File: src/utils.js
================
// File contents here

(...remaining files)

================================================================
Instruction
================================================================
(Custom instructions from `output.instructionFilePath`)
To generate output in XML format, use the  option:The XML format structures the content in a hierarchical manner:This file is a merged representation of the entire codebase, combining all repository files into a single document.

<file_summary>
  (Metadata and usage AI instructions)
</file_summary>

<directory_structure>
src/
cli/
cliOutput.ts
index.ts

(...remaining directories)
</directory_structure>

<files>
<file path="src/index.js">
  // File contents here
</file>

(...remaining files)
</files>

<instruction>
(Custom instructions from `output.instructionFilePath`)
</instruction>
When your prompts involve multiple components like context, instructions, and examples, XML tags can be a game-changer. They help Claude parse your prompts more accurately, leading to higher-quality outputs.This means that the XML output from Repomix is not just a different format, but potentially a more effective way to feed your codebase into AI systems for analysis, code review, or other tasks.To generate output in Markdown format, use the  option:The Markdown format structures the content in a hierarchical manner:This file is a merged representation of the entire codebase, combining all repository files into a single document.

# File Summary

(Metadata and usage AI instructions)

# Repository Structure

```
src/
  cli/
    cliOutput.ts
    index.ts
```

(...remaining directories)

# Repository Files

## File: src/index.js

```
// File contents here
```

(...remaining files)

# Instruction

(Custom instructions from `output.instructionFilePath`)
This format provides a clean, readable structure that is both human-friendly and easily parseable by AI systems.: Show tool version: Specify the output file name: Specify the output style (, , ): Enable parsable output based on the chosen style schema. Note that this can increase token count.--output-show-line-numbers: Show line numbers in the output: Additionally copy generated output to system clipboard: Disable file summary section output: Disable directory structure section output: Remove comments from supported file types: Remove empty lines from the output: Custom text to include in the file header--instruction-file-path <path>: Path to a file containing detailed custom instructions--include-empty-directories: Include empty directories in the output: List of include patterns (comma-separated): Additional ignore patterns (comma-separated): Disable .gitignore file usage: Disable default patternsRemote Repository Options: Process a remote Git repository: Specify the remote branch name, tag, or commit hash (defaults to repository default branch): Path to a custom config file: Create config file: Use global config: Disable security check--token-count-encoding <encoding>: Specify token count encoding (e.g., , ): Number of top files to display in the summary: Enable verbose loggingrepomix -o custom-output.txt
repomix -i "*.log,tmp" -v
repomix -c ./custom-config.json
repomix --style xml
repomix --remote https://github.com/user/repo
npx repomix src
To update a globally installed Repomix:# Using npm
npm update -g repomix

# Using yarn
yarn global upgrade repomix
Using  is generally more convenient as it always uses the latest version.Remote Repository ProcessingRepomix supports processing remote Git repositories without the need for manual cloning. This feature allows you to quickly analyze any public Git repository with a single command.To process a remote repository, use the  option followed by the repository URL:repomix --remote https://github.com/yamadashy/repomix
You can also use GitHub's shorthand format:repomix --remote yamadashy/repomix
You can specify the branch name, tag, or commit hash:# Using --remote-branch option
repomix --remote https://github.com/yamadashy/repomix --remote-branch main

# Using branch's URL
repomix --remote https://github.com/yamadashy/repomix/tree/main
Or use a specific commit hash:# Using --remote-branch option
repomix --remote https://github.com/yamadashy/repomix --remote-branch 935b695

# Using commit's URL
repomix --remote https://github.com/yamadashy/repomix/commit/836abcd7335137228ad77feb28655d85712680f1
Create a  file in your project root for custom configurations.Here's an explanation of the configuration options:The name of the output fileThe style of the output (, , )Whether to escape the output based on the chosen style schema. Note that this can increase token count.Custom text to include in the file headeroutput.instructionFilePathPath to a file containing detailed custom instructionsWhether to include a summary section at the beginning of the outputoutput.directoryStructureWhether to include the directory structure in the outputWhether to remove comments from supported file typesWhether to remove empty lines from the outputWhether to add line numbers to each line in the outputWhether to copy the output to system clipboard in addition to saving the fileNumber of top files to display in the summary. If set to 0, no summary will be displayedoutput.includeEmptyDirectoriesWhether to include empty directories in the repository structureWhether to use patterns from the project's  fileignore.useDefaultPatternsWhether to use default ignore patternssecurity.enableSecurityCheckWhether to perform security checks on filesToken count encoding for AI model context limits (e.g., , ){
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": true,
    "headerText": "Custom header information for the packed file.",
    "fileSummary": true,
    "directoryStructure": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "showLineNumbers": false,
    "copyToClipboard": true,
    "topFilesLength": 5,
    "includeEmptyDirectories": false
  },
  "include": [
    "**/*"
  ],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    // Patterns can also be specified in .repomixignore
    "customPatterns": [
      "additional-folder",
      "**/*.log"
    ]
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
To create a global configuration file:The global configuration file will be created in:Windows: %LOCALAPPDATA%\Repomix\repomix.config.jsonmacOS/Linux: $XDG_CONFIG_HOME/repomix/repomix.config.json or ~/.config/repomix/repomix.config.jsonNote: Local configuration (if present) takes precedence over global configuration.Repomix now supports specifying files to include using glob patterns. This allows for more flexible and powerful file selection:Use  to include all JavaScript files in any directoryUse  to include all files within the  directory and its subdirectoriesCombine multiple patterns like ["src/**/*.js", "**/*.md"] to include JavaScript files in  and all Markdown filesRepomix offers multiple methods to set ignore patterns for excluding specific files or directories during the packing process:: By default, patterns listed in your project's  file are used. This behavior can be controlled with the  setting or the  cli option.: Repomix includes a default list of commonly excluded files and directories (e.g., node_modules, .git, binary files). This feature can be controlled with the ignore.useDefaultPatterns setting or the  cli option. Please see defaultIgnore.ts for more details.: You can create a  file in your project root to define Repomix-specific ignore patterns. This file follows the same format as .: Additional ignore patterns can be specified using the  option in the configuration file. You can overwrite this setting with the  command line option.Priority Order (from highest to lowest):Custom patterns  (if  is true and  is not used)Default patterns (if ignore.useDefaultPatterns is true and  is not used)This approach allows for flexible file exclusion configuration based on your project's needs. It helps optimize the size of the generated pack file by ensuring the exclusion of security-sensitive files and large binary files, while preventing the leakage of confidential information.Note: Binary files are not included in the packed output by default, but their paths are listed in the "Repository Structure" section of the output file. This provides a complete overview of the repository structure while keeping the packed file efficient and text-based.The output.instructionFilePath option allows you to specify a separate file containing detailed instructions or context about your project. This allows AI systems to understand the specific context and requirements of your project, potentially leading to more relevant and tailored analysis or suggestions.Here's an example of how you might use this feature:Create a file named  in your project root:# Coding Guidelines

- Follow the Airbnb JavaScript Style Guide
- Suggest splitting files into smaller, focused units when appropriate
- Add comments for non-obvious logic. Keep all text in English
- All new features should have corresponding unit tests

# Generate Comprehensive Output

- Include all content without abbreviation, unless specified otherwise
- Optimize for handling large codebases while maintaining output quality
In your , add the  option:{
  "output": {
    "instructionFilePath": "repomix-instruction.md",
    // other options...
  }
}
When Repomix generates the output, it will include the contents of  in a dedicated section.Put long-form data at the top: Place your long documents and inputs (~20K+ tokens) near the top of your prompt, above your query, instructions, and examples. This can significantly improve Claude's performance across all models. Queries at the end can improve response quality by up to 30% in tests, especially with complex, multi-document inputs.When  is set to , Repomix will attempt to remove comments from supported file types. This feature can help reduce the size of the output file and focus on the essential code content.Supported languages include: HTML, CSS, JavaScript, TypeScript, Vue, Svelte, Python, PHP, Ruby, C, C#, Java, Go, Rust, Swift, Kotlin, Dart, Shell, and YAML.Note: The comment removal process is conservative to avoid accidentally removing code. In complex cases, some comments might be retained.Repomix includes a security check feature that uses Secretlint to detect potentially sensitive information in your files. This feature helps you identify possible security risks before sharing your packed repository.The security check results will be displayed in the CLI output after the packing process is complete. If any suspicious files are detected, you'll see a list of these files along with a warning message.🔍 Security Check:
──────────────────
2 suspicious file(s) detected:
1. src/utils/test.txt
2. tests/utils/secretLintUtils.test.ts

Please review these files for potentially sensitive information.
By default, Repomix's security check feature is enabled. You can disable it by setting security.enableSecurityCheck to  in your configuration file:{
  "security": {
    "enableSecurityCheck": false
  }
}
Or using the  command line option:repomix --no-security-check
[!NOTE] Disabling security checks may expose sensitive information. Use this option with caution and only when necessary, such as when working with test files or documentation that contains example credentials.We welcome contributions from the community! To get started, please refer to our Contributing Guide.: The Repomix CLI tool does  collect, transmit, or store any user data, telemetry, or repository information.: Repomix CLI operates fully offline after installation. The only cases where an internet connection is needed are: 
  Installation via npm/yarn.Using the  flag to process remote repositories.Checking for updates (manually triggered).: Since all processing is local, Repomix CLI is safe to use with private and internal repositories.: The Repomix website uses  to collect usage data, such as page views and user interactions. This helps us understand how the website is used and improve the user experience.Repomix (both the CLI tool and the website) is provided "as is" without any warranties or guarantees. We do not take responsibility for how the generated output is used, including but not limited to its accuracy, legality, or any potential consequences arising from its use.]]></content:encoded></item><item><title>lobehub/lobe-chat</title><link>https://github.com/lobehub/lobe-chat</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🤯 Lobe Chat - an open-source, modern-design AI chat framework. Supports Multi AI Providers( OpenAI / Claude 3 / Gemini / Ollama / Qwen / DeepSeek), Knowledge Base (file upload / knowledge management / RAG ), Multi-Modals (Vision/TTS/Plugins/Artifacts). One-click FREE deployment of your private ChatGPT/ Claude application.👋🏻 Getting Started & Join Our CommunityWe are a group of e/acc design-engineers, hoping to provide modern design components and tools for AIGC. By adopting the Bootstrapping approach, we aim to provide developers and users with a more open, transparent, and user-friendly product ecosystem.Whether for users or professional developers, LobeHub will be your AI Agent playground. Please be aware that LobeChat is currently under active development, and feedback is welcome for any issues encountered., You will receive all release notifications from GitHub without any delay ~ ⭐️LobeChat supports file upload and knowledge base functionality. You can upload various types of files including documents, images, audio, and video, as well as create knowledge bases, making it convenient for users to manage and search for files. Additionally, you can utilize files and knowledge base features during conversations, enabling a richer dialogue experience.In the continuous development of LobeChat, we deeply understand the importance of diversity in model service providers for meeting the needs of the community when providing AI conversation services. Therefore, we have expanded our support to multiple model service providers, rather than being limited to a single one, in order to offer users a more diverse and rich selection of conversations.In this way, LobeChat can more flexibly adapt to the needs of different users, while also providing developers with a wider range of choices.Supported Model Service ProvidersWe have implemented support for the following model service providers:: OpenAI is a global leader in artificial intelligence research, with models like the GPT series pushing the frontiers of natural language processing. OpenAI is committed to transforming multiple industries through innovative and efficient AI solutions. Their products demonstrate significant performance and cost-effectiveness, widely used in research, business, and innovative applications.: Ollama provides models that cover a wide range of fields, including code generation, mathematical operations, multilingual processing, and conversational interaction, catering to diverse enterprise-level and localized deployment needs.: Anthropic is a company focused on AI research and development, offering a range of advanced language models such as Claude 3.5 Sonnet, Claude 3 Sonnet, Claude 3 Opus, and Claude 3 Haiku. These models achieve an ideal balance between intelligence, speed, and cost, suitable for various applications from enterprise workloads to rapid-response scenarios. Claude 3.5 Sonnet, as their latest model, has excelled in multiple evaluations while maintaining a high cost-performance ratio.: Bedrock is a service provided by Amazon AWS, focusing on delivering advanced AI language and visual models for enterprises. Its model family includes Anthropic's Claude series, Meta's Llama 3.1 series, and more, offering a range of options from lightweight to high-performance, supporting tasks such as text generation, conversation, and image processing for businesses of varying scales and needs.: Google's Gemini series represents its most advanced, versatile AI models, developed by Google DeepMind, designed for multimodal capabilities, supporting seamless understanding and processing of text, code, images, audio, and video. Suitable for various environments from data centers to mobile devices, it significantly enhances the efficiency and applicability of AI models.: DeepSeek is a company focused on AI technology research and application, with its latest model DeepSeek-V2.5 integrating general dialogue and code processing capabilities, achieving significant improvements in human preference alignment, writing tasks, and instruction following.: The HuggingFace Inference API provides a fast and free way for you to explore thousands of models for various tasks. Whether you are prototyping for a new application or experimenting with the capabilities of machine learning, this API gives you instant access to high-performance models across multiple domains.: OpenRouter is a service platform providing access to various cutting-edge large model interfaces, supporting OpenAI, Anthropic, LLaMA, and more, suitable for diverse development and application needs. Users can flexibly choose the optimal model and pricing based on their requirements, enhancing the AI experience.: Run serverless GPU-powered machine learning models on Cloudflare's global network.: With GitHub Models, developers can become AI engineers and leverage the industry's leading AI models.At the same time, we are also planning to support more model service providers. If you would like LobeChat to support your favorite service provider, feel free to join our 💬 community discussion.To meet the specific needs of users, LobeChat also supports the use of local models based on Ollama, allowing users to flexibly use their own or third-party models.LobeChat now supports OpenAI's latest  model with visual recognition capabilities, a multimodal intelligence that can perceive visuals. Users can easily upload or drag and drop images into the dialogue box, and the agent will be able to recognize the content of the images and engage in intelligent conversation based on this, creating smarter and more diversified chat scenarios.This feature opens up new interactive methods, allowing communication to transcend text and include a wealth of visual elements. Whether it's sharing images in daily use or interpreting images within specific industries, the agent provides an outstanding conversational experience.LobeChat supports Text-to-Speech (TTS) and Speech-to-Text (STT) technologies, enabling our application to convert text messages into clear voice outputs, allowing users to interact with our conversational agent as if they were talking to a real person. Users can choose from a variety of voices to pair with the agent.Moreover, TTS offers an excellent solution for those who prefer auditory learning or desire to receive information while busy. In LobeChat, we have meticulously selected a range of high-quality voice options (OpenAI Audio, Microsoft Edge Speech) to meet the needs of users from different regions and cultural backgrounds. Users can choose the voice that suits their personal preferences or specific scenarios, resulting in a personalized communication experience.With support for the latest text-to-image generation technology, LobeChat now allows users to invoke image creation tools directly within conversations with the agent. By leveraging the capabilities of AI tools such as , , and , the agents are now equipped to transform your ideas into images.This enables a more private and immersive creative process, allowing for the seamless integration of visual storytelling into your personal dialogue with the agent.The plugin ecosystem of LobeChat is an important extension of its core functionality, greatly enhancing the practicality and flexibility of the LobeChat assistant.By utilizing plugins, LobeChat assistants can obtain and process real-time information, such as searching for web information and providing users with instant and relevant news.In addition, these plugins are not limited to news aggregation, but can also extend to other practical functions, such as quickly searching documents, generating images, obtaining data from various platforms like Bilibili, Steam, and interacting with various third-party services.Smart web search that reads and analyzes pages to deliver comprehensive answers from Google results.Find any NFT data on the NEAR Protocol.Search for information from the internet base BingApiAnalyze stocks and get comprehensive real-time investment data and analytics.In LobeChat Agent Marketplace, creators can discover a vibrant and innovative community that brings together a multitude of well-designed agents, which not only play an important role in work scenarios but also offer great convenience in learning processes. Our marketplace is not just a showcase platform but also a collaborative space. Here, everyone can contribute their wisdom and share the agents they have developed.By 🤖/🏪 Submit Agents, you can easily submit your agent creations to our platform. Importantly, LobeChat has established a sophisticated automated internationalization (i18n) workflow, capable of seamlessly translating your agent into multiple language versions. This means that no matter what language your users speak, they can experience your agent without barriers.We welcome all users to join this growing ecosystem and participate in the iteration and optimization of agents. Together, we can create more interesting, practical, and innovative agents, further enriching the diversity and practicality of the agent offerings.LobeChat supports the use of both server-side and local databases. Depending on your needs, you can choose the appropriate deployment solution:: suitable for users who want more control over their data and privacy protection. LobeChat uses CRDT (Conflict-Free Replicated Data Type) technology to achieve multi-device synchronization. This is an experimental feature aimed at providing a seamless data synchronization experience.: suitable for users who want a more convenient user experience. LobeChat supports PostgreSQL as a server-side database. For detailed documentation on how to configure the server-side database, please visit Configure Server-side Database.Regardless of which database you choose, LobeChat can provide you with an excellent user experience.LobeChat supports multi-user management and provides two main user authentication and management solutions to meet different needs:: LobeChat integrates , a flexible and powerful identity verification library that supports multiple authentication methods, including OAuth, email login, credential login, etc. With , you can easily implement user registration, login, session management, social login, and other functions to ensure the security and privacy of user data.: For users who need more advanced user management features, LobeChat also supports , a modern user management platform.  provides richer functions, such as multi-factor authentication (MFA), user profile management, login activity monitoring, etc. With , you can get higher security and flexibility, and easily cope with complex user management needs.Regardless of which user management solution you choose, LobeChat can provide you with an excellent user experience and powerful functional support.We deeply understand the importance of providing a seamless experience for users in today's multi-device environment. Therefore, we have adopted Progressive Web Application (PWA) technology, a modern web technology that elevates web applications to an experience close to that of native apps.Through PWA, LobeChat can offer a highly optimized user experience on both desktop and mobile devices while maintaining its lightweight and high-performance characteristics. Visually and in terms of feel, we have also meticulously designed the interface to ensure it is indistinguishable from native apps, providing smooth animations, responsive layouts, and adapting to different device screen resolutions.If you are unfamiliar with the installation process of PWA, you can add LobeChat as your desktop application (also applicable to mobile devices) by following these steps:Launch the Chrome or Edge browser on your computer.Visit the LobeChat webpage.In the upper right corner of the address bar, click on the  icon.Follow the instructions on the screen to complete the PWA Installation.We have carried out a series of optimization designs for mobile devices to enhance the user's mobile experience. Currently, we are iterating on the mobile user experience to achieve smoother and more intuitive interactions. If you have any suggestions or ideas, we welcome you to provide feedback through GitHub Issues or Pull Requests.As a design-engineering-oriented application, LobeChat places great emphasis on users' personalized experiences, hence introducing flexible and diverse theme modes, including a light mode for daytime and a dark mode for nighttime. Beyond switching theme modes, a range of color customization options allow users to adjust the application's theme colors according to their preferences. Whether it's a desire for a sober dark blue, a lively peach pink, or a professional gray-white, users can find their style of color choices in LobeChat.The default configuration can intelligently recognize the user's system color mode and automatically switch themes to ensure a consistent visual experience with the operating system. For users who like to manually control details, LobeChat also offers intuitive setting options and a choice between chat bubble mode and document mode for conversation scenarios.Beside these features, LobeChat also have much better basic technique underground:✨ more features will be added when LobeChat evolve.You can find our upcoming Roadmap plans in the Projects section.LobeChat provides Self-Hosted Version with Vercel, Alibaba Cloud, and Docker Image. This allows you to deploy your own chatbot within a few minutes without any prior knowledge. Deploying with Vercel, Zeabur , Sealos or Alibaba Cloud"If you want to deploy this service yourself on Vercel, Zeabur or Alibaba Cloud, you can follow these steps:Click the button below to start deployment: Log in directly with your GitHub account, and remember to fill in the (required) and  (recommended) on the environment variable section.After deployment, you can start using it.Bind a custom domain (optional): The DNS of the domain assigned by Vercel is polluted in some areas; binding a custom domain can connect directly.After fork, only retain the upstream sync action and disable other actions in your repository on GitHub.If you have deployed your own project following the one-click deployment steps in the README, you might encounter constant prompts indicating "updates available." This is because Vercel defaults to creating a new project instead of forking this one, resulting in an inability to detect updates accurately.We provide a Docker image for deploying the LobeChat service on your own private device. Use the following command to start the LobeChat service:$ docker run -d -p 3210:3210 \
  -e OPENAI_API_KEY=sk-xxxx \
  -e ACCESS_CODE=lobe66 \
  --name lobe-chat \
  lobehub/lobe-chat
If you need to use the OpenAI service through a proxy, you can configure the proxy address using the  environment variable:$ docker run -d -p 3210:3210 \
  -e OPENAI_API_KEY=sk-xxxx \
  -e OPENAI_PROXY_URL=https://api-proxy.com/v1 \
  -e ACCESS_CODE=lobe66 \
  --name lobe-chat \
  lobehub/lobe-chat
This project provides some additional configuration items set with environment variables:This is the API key you apply on the OpenAI account pageIf you manually configure the OpenAI interface proxy, you can use this configuration item to override the default OpenAI API request base URLhttps://api.chatanywhere.cn or The default value ishttps://api.openai.com/v1Add a password to access this service; you can set a long password to avoid leaking. If this value contains a comma, it is a password array. or  or Used to control the model list. Use  to add a model,  to hide a model, and  to customize the display name of a model, separated by commas.qwen-7b-chat,+glm-6b,-gpt-3.5-turboPlugins provide a means to extend the Function Calling capabilities of LobeChat. They can be used to introduce new function calls and even new ways to render message results. If you are interested in plugin development, please refer to our 📘 Plugin Development Guide in the Wiki.lobe-chat-plugins: This is the plugin index for LobeChat. It accesses index.json from this repository to display a list of available plugins for LobeChat to the user.@lobehub/chat-plugins-gateway: The LobeChat Plugins Gateway is a backend service that provides a gateway for LobeChat plugins. We deploy this service using Vercel. The primary API POST /api/v1/runner is deployed as an Edge Function.The plugin system is currently undergoing major development. You can learn more in the following issues:[x] : Implement separation of the plugin from the main body, split the plugin into an independent repository for maintenance, and realize dynamic loading of the plugin.[x] : The security and stability of the plugin's use, more accurately presenting abnormal states, the maintainability of the plugin architecture, and developer-friendly.[x] : Higher-level and more comprehensive customization capabilities, support for plugin authentication, and examples.You can use GitHub Codespaces for online development:Or clone it for local development:$ git clone https://github.com/lobehub/lobe-chat.git
$ cd lobe-chat
$ pnpm install
$ pnpm dev
Contributions of all types are more than welcome; if you are interested in contributing code, feel free to check out our GitHub Issues and Projects to get stuck in to show us what you’re made of.We are creating a technology-driven forum, fostering knowledge interaction and the exchange of ideas that may culminate in mutual inspiration and collaborative innovation.Help us make LobeChat better. Welcome to provide product design feedback, user experience discussions directly to us.Every bit counts and your one-time donation sparkles in our galaxy of support! You're a shooting star, making a swift and bright impact on our journey. Thank you for believing in us – your generosity guides us toward our mission, one brilliant flash at a time. Modern theme for Stable Diffusion WebUI, exquisite interface design, highly customizable UI, and efficiency-boosting features. WebUI for Midjourney, leverages AI to quickly generate a wide array of rich and diverse images from text prompts, sparking creativity and enhancing conversations. Lobe i18n is an automation tool for the i18n (internationalization) translation process, powered by ChatGPT. It supports features such as automatic splitting of large files, incremental updates, and customization options for the OpenAI model, API proxy, and temperature. Lobe Commit is a CLI tool that leverages Langchain/ChatGPT to generate Gitmoji-based commit messages.]]></content:encoded></item><item><title>mendableai/firecrawl</title><link>https://github.com/mendableai/firecrawl</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🔥 Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API.Empower your AI apps with clean data from any website. Featuring advanced scraping, crawling, and data extraction capabilities.This repository is in development, and we’re still integrating custom modules into the mono repo. It's not fully ready for self-hosted deployment yet, but you can run it locally.Firecrawl is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required. Check out our documentation.Pst. hey, you, join our stargazers :)We provide an easy to use API with our hosted version. You can find the playground and documentation here. You can also self host the backend if you'd like.Check out the following resources to get started:To run locally, refer to guide here.To use the API, you need to sign up on Firecrawl and get an API key.: scrapes a URL and get its content in LLM-ready format (markdown, structured data via LLM Extract, screenshot, html): scrapes all the URLs of a web page and return content in LLM-ready format: input a website and get all the website urls - extremely fast: get structured data from single page, multiple pages or entire websites with AI.: markdown, structured data, screenshot, HTML, links, metadata: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc...: pdfs, docx, images: designed to get the data you need - no matter how hard it is: click, scroll, input, wait and more before extracting data: scrape thousands of URLs at the same time with a new async endpoint.You can find all of Firecrawl's capabilities and how to use them in our documentationUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "limit": 100,
      "scrapeOptions": {
        "formats": ["markdown", "html"]
      }
    }'
Returns a crawl job id and the url to check the status of the crawl.{
  "success": true,
  "id": "123-456-789",
  "url": "https://api.firecrawl.dev/v1/crawl/123-456-789"
}
Used to check the status of a crawl job and get its result.curl -X GET https://api.firecrawl.dev/v1/crawl/123-456-789 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY'
{
  "status": "completed",
  "total": 36,
  "creditsUsed": 36,
  "expiresAt": "2024-00-00T00:00:00.000Z",
  "data": [
    {
      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",
      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",
        "language": "en",
        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    }
  ]
}
Used to scrape a URL and get its content in the specified formats.curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats" : ["markdown", "html"]
    }'
{
  "success": true,
  "data": {
    "markdown": "Launch Week I is here! [See our Day 2 Release 🚀](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[💥 Get 2 months free...",
    "html": "<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...",
    "metadata": {
      "title": "Home - Firecrawl",
      "description": "Firecrawl crawls and converts any website into clean markdown.",
      "language": "en",
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",
      "robots": "follow, index",
      "ogTitle": "Firecrawl",
      "ogDescription": "Turn any website into LLM-ready data.",
      "ogUrl": "https://www.firecrawl.dev/",
      "ogImage": "https://www.firecrawl.dev/og.png?123",
      "ogLocaleAlternate": [],
      "ogSiteName": "Firecrawl",
      "sourceURL": "https://firecrawl.dev",
      "statusCode": 200
    }
  }
}
Used to map a URL and get urls of the website. This returns most links present on the website.curl -X POST https://api.firecrawl.dev/v1/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://firecrawl.dev"
    }'
{
  "status": "success",
  "links": [
    "https://firecrawl.dev",
    "https://www.firecrawl.dev/pricing",
    "https://www.firecrawl.dev/blog",
    "https://www.firecrawl.dev/playground",
    "https://www.firecrawl.dev/smart-crawl",
  ]
}
Map with  param allows you to search for specific urls inside a website.curl -X POST https://api.firecrawl.dev/v1/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://firecrawl.dev",
      "search": "docs"
    }'
Response will be an ordered list from the most relevant to the least relevant.{
  "status": "success",
  "links": [
    "https://docs.firecrawl.dev",
    "https://docs.firecrawl.dev/sdks/python",
    "https://docs.firecrawl.dev/learn/rag-llama3",
  ]
}
Get structured data from entire websites with a prompt and/or a schema.You can extract structured data from one or multiple URLs, including wildcards:When you use /*, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data.curl -X POST https://api.firecrawl.dev/v1/extract \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "urls": [
        "https://firecrawl.dev/*", 
        "https://docs.firecrawl.dev/", 
        "https://www.ycombinator.com/companies"
      ],
      "prompt": "Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.",
      "schema": {
        "type": "object",
        "properties": {
          "company_mission": {
            "type": "string"
          },
          "is_open_source": {
            "type": "boolean"
          },
          "is_in_yc": {
            "type": "boolean"
          }
        },
        "required": [
          "company_mission",
          "supports_sso",
          "is_open_source",
          "is_in_yc"
        ]
      }
    }'
{
  "success": true,
  "id": "44aa536d-f1cb-4706-ab87-ed0386685740",
  "urlTrace": []
}
If you are using the sdks, it will auto pull the response for you:{
  "success": true,
  "data": {
    "company_mission": "Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.",
    "supports_sso": false,
    "is_open_source": true,
    "is_in_yc": true
  }
}
Used to extract structured data from scraped pages.curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://www.mendable.ai/",
      "formats": ["json"],
      "jsonOptions": {
        "schema": {
          "type": "object",
          "properties": {
            "company_mission": {
                      "type": "string"
            },
            "supports_sso": {
                      "type": "boolean"
            },
            "is_open_source": {
                      "type": "boolean"
            },
            "is_in_yc": {
                      "type": "boolean"
            }
          },
          "required": [
            "company_mission",
            "supports_sso",
            "is_open_source",
            "is_in_yc"
          ]
        }
      }
    }'
{
  "success": true,
  "data": {
    "content": "Raw Content",
    "metadata": {
      "title": "Mendable",
      "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
      "robots": "follow, index",
      "ogTitle": "Mendable",
      "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
      "ogUrl": "https://mendable.ai/",
      "ogImage": "https://mendable.ai/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://mendable.ai/"
    },
    "json": {
      "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",
      "supports_sso": true,
      "is_open_source": false,
      "is_in_yc": true
    }
  }
}
Extracting without a schema (New)You can now extract without a schema by just passing a  to the endpoint. The llm chooses the structure of the data.curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "formats": ["json"],
      "jsonOptions": {
        "prompt": "Extract the company mission from the page."
      }
    }'
Interacting with the page with Actions (Cloud-only)Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.Here is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
        "url": "google.com",
        "formats": ["markdown"],
        "actions": [
            {"type": "wait", "milliseconds": 2000},
            {"type": "click", "selector": "textarea[title=\"Search\"]"},
            {"type": "wait", "milliseconds": 2000},
            {"type": "write", "text": "firecrawl"},
            {"type": "wait", "milliseconds": 2000},
            {"type": "press", "key": "ENTER"},
            {"type": "wait", "milliseconds": 3000},
            {"type": "click", "selector": "h3"},
            {"type": "wait", "milliseconds": 3000},
            {"type": "screenshot"}
        ]
    }'
Batch Scraping Multiple URLs (New)You can now batch scrape multiple URLs at the same time. It is very similar to how the /crawl endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.curl -X POST https://api.firecrawl.dev/v1/batch/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "urls": ["https://docs.firecrawl.dev", "https://docs.firecrawl.dev/sdks/overview"],
      "formats" : ["markdown", "html"]
    }'
The search endpoint combines web search with Firecrawl’s scraping capabilities to return full page content for any query.Include  with  to get complete markdown content for each search result otherwise it defaults to getting SERP results (url, title, description).curl -X POST https://api.firecrawl.dev/v1/search \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "query": "What is Mendable?"
    }'
{
  "success": true,
  "data": [
    {
      "url": "https://mendable.ai",
      "title": "Mendable | AI for CX and Sales",
      "description": "AI for CX and Sales"
    }
  ]
}
from firecrawl.firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_status = app.scrape_url(
  'https://firecrawl.dev', 
  params={'formats': ['markdown', 'html']}
)
print(scrape_status)

# Crawl a website:
crawl_status = app.crawl_url(
  'https://firecrawl.dev', 
  params={
    'limit': 100, 
    'scrapeOptions': {'formats': ['markdown', 'html']}
  },
  poll_interval=30
)
print(crawl_status)
Extracting structured data from a URLWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:
from firecrawl.firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

class ArticleSchema(BaseModel):
    title: str
    points: int
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")

data = app.scrape_url('https://news.ycombinator.com', {
    'formats': ['json'],
    'jsonOptions': {
        'schema': TopArticlesSchema.model_json_schema()
    }
})
print(data["json"])
To install the Firecrawl Node SDK, you can use npm:npm install @mendable/firecrawl-js
Set the API key as an environment variable named  or pass it as a parameter to the  class.import FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';

const app = new FirecrawlApp({apiKey: "fc-YOUR_API_KEY"});

// Scrape a website
const scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {
  formats: ['markdown', 'html'],
});

if (scrapeResponse) {
  console.log(scrapeResponse)
}

// Crawl a website
const crawlResponse = await app.crawlUrl('https://firecrawl.dev', {
  limit: 100,
  scrapeOptions: {
    formats: ['markdown', 'html'],
  }
} satisfies CrawlParams, true, 30) satisfies CrawlStatusResponse;

if (crawlResponse) {
  console.log(crawlResponse)
}
Extracting structured data from a URLWith LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how to use it:import FirecrawlApp from "@mendable/firecrawl-js";
import { z } from "zod";

const app = new FirecrawlApp({
  apiKey: "fc-YOUR_API_KEY"
});

// Define schema to extract contents into
const schema = z.object({
  top: z
    .array(
      z.object({
        title: z.string(),
        points: z.number(),
        by: z.string(),
        commentsURL: z.string(),
      })
    )
    .length(5)
    .describe("Top 5 stories on Hacker News"),
});

const scrapeResult = await app.scrapeUrl("https://news.ycombinator.com", {
  jsonOptions: { extractionSchema: schema },
});

console.log(scrapeResult.data["json"]);
Open Source vs Cloud OfferingFirecrawl is open source available under the AGPL-3.0 license.To deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.Firecrawl Cloud is available at firecrawl.dev and offers a range of features that are not available in the open source version:It is the sole responsibility of the end users to respect websites' policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites' robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions.This project is primarily licensed under the GNU Affero General Public License v3.0 (AGPL-3.0), as specified in the LICENSE file in the root directory of this repository. However, certain components of this project are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.The AGPL-3.0 license applies to all parts of the project unless otherwise specified.The SDKs and some UI components are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.When using or contributing to this project, ensure you comply with the appropriate license terms for the specific component you are working with.For more details on the licensing of specific components, please refer to the LICENSE files in the respective directories or contact the project maintainers.]]></content:encoded></item><item><title>k2-fsa/sherpa-onnx</title><link>https://github.com/k2-fsa/sherpa-onnx</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Speech-to-text, text-to-speech, speaker diarization, and VAD using next-gen Kaldi with onnxruntime without Internet connection. Support embedded systems, Android, iOS, HarmonyOS, Raspberry Pi, RISC-V, x86_64 servers, websocket server/client, C/C++, Python, Kotlin, C#, Go, NodeJS, Java, Swift, Dart, JavaScript, Flutter, Object Pascal, Lazarus, RustSpoken Language identificationSupported programming languagesIt also supports WebAssembly.This repository supports running the following functions Speech-to-text (i.e., ASR); both streaming and non-streaming are supportedText-to-speech (i.e., TTS)Spoken language identificationon the following platforms and operating systems:Links for Huggingface SpacesLinks for pre-built Android APKsLinks for pre-built Flutter APPsLinks for pre-built Lazarus APPsLinks for pre-trained modelsSome pre-trained ASR models (Streaming)Some pre-trained ASR models (Non-Streaming)Projects using sherpa-onnxTalk to any LLM with hands-free voice interaction, voice interruption, and Live2D taking face running locally across platformsUses streaming ASR in C# with graphical user interface.It uses the JavaScript API of sherpa-onnx along with ElectronA server based on nodejs providing Restful API for speech recognition.一个模块化，全过程可离线，低占用率的对话机器人/智能音箱It uses QT. Both ASR and TTS are used.]]></content:encoded></item><item><title>langgenius/dify</title><link>https://github.com/langgenius/dify</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Dify is an open-source LLM app development platform. Dify's intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.Dify is an open-source LLM app development platform. Its intuitive interface combines agentic AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.Before installing Dify, make sure your machine meets the following minimum system requirements:The easiest way to start the Dify server is through docker compose. Before running Dify with the following commands, make sure that Docker and Docker Compose are installed on your machine:cd dify
cd docker
cp .env.example .env
docker compose up -d
After running, you can access the Dify dashboard in your browser at http://localhost/install and start the initialization process.Please refer to our FAQ if you encounter problems setting up Dify. Reach out to the community and us if you are still having issues.: Build and test powerful AI workflows on a visual canvas, leveraging all the following features and beyond.2. Comprehensive model support: Seamless integration with hundreds of proprietary / open-source LLMs from dozens of inference providers and self-hosted solutions, covering GPT, Mistral, Llama3, and any OpenAI API-compatible models. A full list of supported model providers can be found here.: Intuitive interface for crafting prompts, comparing model performance, and adding additional features such as text-to-speech to a chat-based app.: Extensive RAG capabilities that cover everything from document ingestion to retrieval, with out-of-box support for text extraction from PDFs, PPTs, and other common document formats.: You can define agents based on LLM Function Calling or ReAct, and add pre-built or custom tools for the agent. Dify provides 50+ built-in tools for AI agents, such as Google Search, DALL·E, Stable Diffusion and WolframAlpha.: Monitor and analyze application logs and performance over time. You could continuously improve prompts, datasets, and models based on production data and annotations.: All of Dify's offerings come with corresponding APIs, so you could effortlessly integrate Dify into your own business logic.Enterprise Feature (SSO/Access control) We host a Dify Cloud service for anyone to try with zero setup. It provides all the capabilities of the self-deployed version, and includes 200 free GPT-4 calls in the sandbox plan.Self-hosting Dify Community Edition Quickly get Dify running in your environment with this starter guide. Use our documentation for further references and more in-depth instructions.Star Dify on GitHub and be instantly notified of new releases.If you need to customize the configuration, please refer to the comments in our .env.example file and update the corresponding values in your  file. Additionally, you might need to make adjustments to the  file itself, such as changing image versions, port mappings, or volume mounts, based on your specific deployment environment and requirements. After making any changes, please re-run . You can find the full list of available environment variables here.If you'd like to configure a highly-available setup, there are community-contributed Helm Charts and YAML files which allow Dify to be deployed on Kubernetes.Using Terraform for DeploymentDeploy Dify to Cloud Platform with a single click using terraformUsing AWS CDK for DeploymentDeploy Dify to AWS with CDKFor those who'd like to contribute code, see our Contribution Guide. At the same time, please consider supporting Dify by sharing it on social media and at events and conferences.We are looking for contributors to help with translating Dify to languages other than Mandarin or English. If you are interested in helping, please see the i18n README for more information, and leave us a comment in the  channel of our Discord Community Server.Discord. Best for: sharing your applications and hanging out with the community.X(Twitter). Best for: sharing your applications and hanging out with the community.To protect your privacy, please avoid posting security issues on GitHub. Instead, send your questions to security@dify.ai and we will provide you with a more detailed answer.This repository is available under the Dify Open Source License, which is essentially Apache 2.0 with a few additional restrictions.]]></content:encoded></item><item><title>microsoft/terminal</title><link>https://github.com/microsoft/terminal</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[The new Windows Terminal and the original Windows console host, all in the same place!This repository contains the source code for:Related repositories include:Installing and running Windows Terminal[!NOTE] Windows Terminal requires Windows 10 2004 (build 19041) or laterMicrosoft Store [Recommended]This is our preferred method.For users who are unable to install Windows Terminal from the Microsoft Store, released builds can be manually downloaded from this repository's Releases page.Download the Microsoft.WindowsTerminal_<versionNumber>.msixbundle file from the  section. To install the app, you can simply double-click on the  file, and the app installer should automatically run. If that fails for any reason, you can try the following command at a PowerShell prompt:# NOTE: If you are using PowerShell 7+, please run
# Import-Module Appx -UseWindowsPowerShell
# before using Add-AppxPackage.

Add-AppxPackage Microsoft.WindowsTerminal_<versionNumber>.msixbundle
[!NOTE] If you install Terminal manually:You may need to install the VC++ v14 Desktop Framework Package. This should only be necessary on older builds of Windows 10 and only if you get an error about missing framework packages.Terminal will not auto-update when new builds are released so you will need to regularly install the latest Terminal release to receive all the latest fixes and improvements!Via Windows Package Manager CLI (aka winget)winget users can download and install the latest Terminal release by installing the Microsoft.WindowsTerminal package:winget install --id Microsoft.WindowsTerminal -e
[!NOTE] Dependency support is available in WinGet version 1.6.2631 or later. To install the Terminal stable release 1.18 or later, please make sure you have the updated version of the WinGet client.Via Chocolatey (unofficial)Chocolatey users can download and install the latest Terminal release by installing the microsoft-windows-terminal package:choco install microsoft-windows-terminal
To upgrade Windows Terminal using Chocolatey, run the following:choco upgrade microsoft-windows-terminal
Scoop users can download and install the latest Terminal release by installing the  package:scoop bucket add extras
scoop install windows-terminal
To update Windows Terminal using Scoop, run the following:scoop update windows-terminal
If you have any issues when installing/updating the package, please search for or report the same on the issues page of Scoop Extras bucket repository.Installing Windows Terminal CanaryWindows Terminal Canary is a nightly build of Windows Terminal. This build has the latest code from our  branch, giving you an opportunity to try features before they make it to Windows Terminal Preview.Windows Terminal Canary is our least stable offering, so you may discover bugs before we have had a chance to find them.Windows Terminal Canary is available as an App Installer distribution and a Portable ZIP distribution.The App Installer distribution supports automatic updates. Due to platform limitations, this installer only works on Windows 11.The Portable ZIP distribution is a portable application. It will not automatically update and will not automatically check for updates. This portable ZIP distribution works on Windows 10 (19041+) and Windows 11.The plan for the Windows Terminal is described here and will be updated as the project proceeds.Terminal & Console OverviewPlease take a few minutes to review the overview below before diving into the code:Windows Terminal is a new, modern, feature-rich, productive terminal application for command-line users. It includes many of the features most frequently requested by the Windows command-line community including support for tabs, rich text, globalization, configurability, theming & styling, and more.The Terminal will also need to meet our goals and measures to ensure it remains fast and efficient, and doesn't consume vast amounts of memory or power.The Windows Console host, , is Windows' original command-line user experience. It also hosts Windows' command-line infrastructure and the Windows Console API server, input engine, rendering engine, user preferences, etc. The console host code in this repository is the actual source from which the  in Windows itself is built.However, because Windows Console's primary goal is to maintain backward compatibility, we have been unable to add many of the features the community (and the team) have been wanting for the last several years including tabs, unicode text, and emoji.These limitations led us to create the new Windows Terminal.While overhauling Windows Console, we modernized its codebase considerably, cleanly separating logical entities into modules and classes, introduced some key extensibility points, replaced several old, home-grown collections and containers with safer, more efficient STL containers, and made the code simpler and safer by using Microsoft's Windows Implementation Libraries - WIL.This overhaul resulted in several of Console's key components being available for re-use in any terminal implementation on Windows. These components include a new DirectWrite-based text layout and rendering engine, a text buffer capable of storing both UTF-16 and UTF-8, a VT parser/emitter, and more.Creating the new Windows TerminalWhen we started planning the new Windows Terminal application, we explored and evaluated several approaches and technology stacks. We ultimately decided that our goals would be best met by continuing our investment in our C++ codebase, which would allow us to reuse several of the aforementioned modernized components in both the existing Console and the new Terminal. Further, we realized that this would allow us to build much of the Terminal's core itself as a reusable UI control that others can incorporate into their own applications.The result of this work is contained within this repo and delivered as the Windows Terminal application you can download from the Microsoft Store, or directly from this repo's releases.For more information about Windows Terminal, you may find some of these resources useful and interesting:I built and ran the new Terminal, but it looks just like the old consoleCause: You're launching the incorrect solution in Visual Studio.Solution: Make sure you're building & deploying the  project in Visual Studio.[!NOTE]  is just a locally-built , the classic Windows Console that hosts Windows' command-line infrastructure. OpenConsole is used by Windows Terminal to connect to and communicate with command-line applications (via ConPty).We are excited to work alongside you, our amazing community, to build and enhance Windows Terminal!BEFORE you start work on a feature/fix, please read & follow our Contributor's Guide to help avoid any wasted or duplicate effort.Communicating with the TeamThe easiest way to communicate with the team is via GitHub issues.Please file new issues, feature requests and suggestions, but DO search for similar open/closed preexisting issues before creating a new issue.If you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter:OpenConsole.sln may be built from within Visual Studio or from the command-line using a set of convenience scripts & tools in the  directory:Import-Module .\tools\OpenConsole.psm1
Set-MsBuildDevEnvironment
Invoke-OpenConsoleBuild
To debug the Windows Terminal in VS, right click on  (in the Solution Explorer) and go to properties. In the Debug menu, change "Application process" and "Background task process" to "Native Only".You should then be able to build & debug the Terminal project by hitting . Make sure to select either the "x64" or the "x86" platform - the Terminal doesn't build for "Any Cpu" (because the Terminal is a C++ application, not a C# one).👉 You will  be able to launch the Terminal directly by running the WindowsTerminal.exe. For more details on why, see #926, #4043Please review these brief docs below about our coding practices.👉 If you find something missing from these docs, feel free to contribute to any of our documentation files anywhere in the repository (or write some new ones!)This is a work in progress as we learn what we'll need to provide people in order to be effective contributors to our project.]]></content:encoded></item><item><title>ruanyf/weekly</title><link>https://github.com/ruanyf/weekly</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[P.S. 讨论区的《谁在招人》，是一个免费的程序员招聘帖，提供大量就业信息，欢迎访问或发布工作/实习岗位。周刊已经沉淀了大量内容，可以使用下面的几种方法进行搜索。3、将这个仓库克隆到本地，然后在仓库目录使用下面的命令。$ grep -nri [搜索词] docs | cat --number
$ grep -nri css docs | cat --number
]]></content:encoded></item><item><title>openai/openai-cookbook</title><link>https://github.com/openai/openai-cookbook</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Examples and guides for using the OpenAI APIExample code and guides for accomplishing common tasks with the OpenAI API. To run these examples, you'll need an OpenAI account and associated API key (create a free account here). Set an environment variable called  with your API key. Alternatively, in most IDEs such as Visual Studio Code, you can create an  file at the root of your repo containing OPENAI_API_KEY=<your API key>, which will be picked up by the notebooks.Most code examples are written in Python, though the concepts can be applied in any language.The OpenAI Cookbook is a community-driven resource. Whether you're submitting an idea, fixing a typo, adding a new guide, or improving an existing one, your contributions are greatly appreciated!Before contributing, read through the existing issues and pull requests to see if someone else is already working on something similar. That way you can avoid duplicating efforts.If there are examples or guides you'd like to see, feel free to suggest them on the issues page.If you'd like to contribute new content, make sure to read through our contribution guidelines. We welcome high-quality submissions of new examples and guides, as long as they meet our criteria and fit within the scope of the cookbook.]]></content:encoded></item><item><title>jingyaogong/minimind</title><link>https://github.com/jingyaogong/minimind</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[🚀🚀 「大模型」50分钟完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 50 min!本开源项目旨在完全从0开始，最快仅用3小时！即可训练出仅为26.88M大小的微型语言模型。极其轻量，最小版本体积约是 GPT3 的 $\frac{1}{7000}$，力求做到最普通的个人GPU也可快速推理甚至训练。发布了大模型极简结构，数据集清洗和预处理、监督预训练(Pretrain)、有监督指令微调(SFT)、低秩自适应(LoRA) 微调，无奖励强化学习直接偏好对齐(DPO)的全阶段代码，也包含拓展共享混合专家(MoE) 的稀疏模型；拓展视觉多模态VLM: MiniMind-V。这不仅是一个开源模型的实现，也是入门大语言模型（LLM）的教程。希望此项目能为研究者提供一个抛砖引玉的入门示例，帮助大家快速上手并对LLM领域产生更多的探索与创新。为防止误读，「最快3小时」是指您需要具备＞本人硬件配置的机器，具体规格的详细信息将在下文提供。大语言模型（LLM）领域，如 GPT、LLaMA、GLM 等，虽然它们效果惊艳， 但动辄10 Bilion庞大的模型参数个人设备显存远不够训练，甚至推理困难。 几乎所有人都不会只满足于用Lora等方案fine-tuing大模型学会一些新的指令， 这约等于在教牛顿玩21世纪的智能手机，然而，这远远脱离了学习物理本身的奥妙。 此外，卖课付费订阅的营销号漏洞百出的一知半解讲解AI的教程遍地， 让理解LLM的优质内容雪上加霜，严重阻碍了学习者。因此，本项目的目标是把上手LLM的门槛无限降低， 直接从0开始训练一个极其轻量的语言模型。[!TIP] （截至2024-9-17）MiniMind系列已完成了3个型号模型的预训练，最小仅需26M（0.02B），即可具备流畅的对话能力！该分析在具有Torch 2.1.2、CUDA 12.2和Flash Attention 2的2×RTX 3090 GPU上进行。公开MiniMind模型代码（包含Dense和MoE模型）、Pretrain、SFT指令微调、LoRA微调、DPO偏好优化的全过程代码、数据集和来源。兼容、、、等流行框架。训练支持单机单卡、单机多卡(DDP、DeepSpeed)训练，使用wandb可视化训练流程。支持在任意位置停止，及在任意位置继续训练。实现Openai-Api基本的chat接口，便于集成到第三方ChatUI使用（FastGPT、Open-WebUI等）。CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
内存：128 GB
显卡：NVIDIA GeForce RTX 3090(24GB) * 2
环境：python 3.9 + Torch 2.1.2 + DDP单机多卡训练
# step 1
git clone https://huggingface.co/jingyaogong/minimind-v1
# step 2
python 2-eval.py
「注意」需要python>=3.10，安装 pip install streamlit==1.27.2# or step 3, use streamlit
streamlit run fast_inference.py
git clone https://github.com/jingyaogong/minimind.git
cd minimind
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
# 测试torch是否可用cuda
import torch
print(torch.cuda.is_available())
2.2 处理数据集，例如pretrain数据提前进行token-encoder、sft数据集抽离qa到csv文件2.3 在 中调整model的参数配置这里仅需调整dim和n_layers和use_moe参数，分别是或，对应于和2.4  执行预训练，得到  作为预训练的输出权重2.5  执行指令微调，得到  作为指令微调的输出权重2.6  执行lora微调（非必须）2.7  执行DPO人类偏好强化学习对齐（非必须）🍭「Tip」预训练和全参微调pretrain和full_sft均支持多卡加速假设你的设备只有1张显卡，使用原生python启动训练即可：执行预训练或指令微调训练 python 1-pretrain.py
# and
python 3-full_sft.py
torchrun --nproc_per_node N 1-pretrain.py
# and
torchrun --nproc_per_node N 3-full_sft.py
deepspeed --master_port 29500 --num_gpus=N 1-pretrain.py
# and
deepspeed --master_port 29500 --num_gpus=N 3-full_sft.py
torchrun --nproc_per_node N 1-pretrain.py --use_wandb
# and
python 1-pretrain.py --use_wandb
通过添加参数，可以记录训练过程，训练完成后，可以在wandb网站上查看训练过程。通过修改 和参数，可以指定项目名称和运行名称。🤖 分词器：nlp中的Tokenizer类似于词典，将单词从自然语言通过“词典”映射到0,1,36这样的数字，可以理解为数字就代表了单词在“词典”中的页码。 LLM分词器的构建方式有两种：一种是自己构造词表训练一个分词器，代码可见；另一种是选择开源模型训练好的分词器。 “词典”当然可以直接选择用新华词典或是牛津词典，优点是token转化压缩率很好，但缺点是词表太长，动辄数十万个词汇短语； 也可以使用自己训练的分词器，优点是词表随意控制，缺点是压缩率不够理想，且生僻词不容易面面俱到。 当然，“词典”的选择很重要，LLM的输出本质上是SoftMax到词典N个词的多分类问题，然后通过“词典”解码到自然语言。 因为LLM体积非常小，为了避免模型头重脚轻（词嵌入embedding层参数占整个LLM比太高），所以词表长度需要选择比较小。 强大的开源模型例如01万物、千问、chatglm、mistral、Llama3等，它们的tokenizer词表长度如下：👉2024-09-17更新：为了防止过去的版本歧义&控制体积，minimind所有模型均使用minimind_tokenizer分词，废弃所有mistral_tokenizer版本。尽管minimind_tokenizer长度很小，编解码效率弱于qwen2、glm等中文友好型分词器。 但minimind模型选择了自己训练的minimind_tokenizer作为分词器，以保持整体参数轻量，避免编码层和计算层占比失衡，头重脚轻，因为minimind的词表大小只有6400。 且minimind在实际测试中没有出现过生僻词汇解码失败的情况，效果良好。 由于自定义词表压缩长度到6400，使得LLM总参数量最低只有26M。📙【Pretrain数据】： Seq-Monkey通用文本数据集 / Seq-Monkey百度网盘 是由多种公开来源的数据（如网页、百科、博客、开源代码、书籍等）汇总清洗而成。整理成统一的JSONL格式，并经过了严格的筛选和去重，确保数据的全面性、规模、可信性和高质量。总量大约在10B token，适合中文大语言模型的预训练。第2种选择：SkyPile-150B数据集 的可公开访问部分包含约2.33亿个独立网页，每个网页平均包含1000多个汉字。数据集包括大约1500亿个令牌和620GB的纯文本数据。 ，可以尝试只挑选SkyPile-150B的部分jsonl下载（并在./data_process.py中对文本tokenizer生成* .csv文件），以便快速跑通预训练流程。📘【DPO数据】：大约合并后共8万条dpo数据，人工标注的偏好数据，均来自活字模型 ，可以用于训练奖励模型，优化模型回复质量，使其更加符合人类偏好。MiniMind-Dense（和Llama3.1一样）使用了Transformer的Decoder-Only结构，跟GPT-3的区别在于：采用了GPT-3的预标准化方法，也就是在每个Transformer子层的输入上进行归一化，而不是在输出上。具体来说，使用的是RMSNorm归一化函数。用SwiGLU激活函数替代了ReLU，这样做是为了提高性能。像GPT-Neo一样，去掉了绝对位置嵌入，改用了旋转位置嵌入（RoPE），这样在处理超出训练长度的推理时效果更好。DeepSeek-V2在前馈网络（FFN）方面，采用了更细粒度的专家分割和共享的专家隔离技术，以提高Experts的效果。MiniMind的整体结构一致，只是在RoPE计算、推理函数和FFN层的代码上做了一些小调整。 其结构如下图（重绘版）：LLM首先要学习的并非直接与人交流，而是让肚子中充满知识的墨水，至于墨水理论上喝的越饱越好，产生大量的对世界的认知积累。预训练就是让Model先埋头苦学大量基本的知识，例如从维基百科、新闻、常识、书籍等。它无监督的从大量的文本数据中压缩知识到自己模型的权重，目的是：学会词语接龙。例如我们输入“秦始皇是”四个字，它在大量学习后能预测出下一句话大概率是“中国的第一位皇帝”。pretrain的学习率设置为1e-4到1e-5的动态学习率，预训练epoch数设为5。torchrun --nproc_per_node 2 1-pretrain.py
单轮次对话有监督微调(Single dialog Fine-tuning):经过预训练，半成品LLM此时已经掌握了几乎所有的语言知识和百科常识。此时它还不会与人聊天，相反它只会无脑地进行输入词语的接龙，生成下一个词。此时需要对半成品LLM做限制在聊天模板中进行微调，例如当它遇到这样的模板“<聊天开始>秦始皇是<聊天终止> ”后不再无脑接龙，而是意识到这是一段完整的对话结束。我们称这个过程为指令微调，就如同让学富五车的「牛顿」先生适应21世纪的聊天习惯，学习屏幕左侧是对方消息，右侧是本人消息这个规律。在训练时，MiniMind的指令和回答长度被截断在512，是为了节省显存空间。就像我们学习时，会先从短的文章开始，当学会阅读200字作文后，800字长文章就不需要再单独学习。在推理时通过调整RoPE线性差值，实现长度外推到1024或2048及以上很方便。学习率设置为1e-5到1e-6的动态学习率，微调epoch数为6。# 3-full_sft.py中设置数据集为sft_data_single.csv
torchrun --nproc_per_node 2 3-full_sft.py
多轮对话微调(Multi dialog Fine-tuning):在2的基础上，LLM已经学会一个问题->一个回答的聊天模板。此时仅需在具备历史问答的更长聊天模板上进一步微调即可。我们仅需使用数据集的history_chat 字段，即历史对话，以及history_chat_response字段，即历史对话的回答。构建【问题->回答，问题->回答，问题->】的新聊天模板，然后使用这个数据集进行微调。学习完成的模型不仅仅只能回答当前问题，还能根据历史对话进行连贯的对话。这一步  ，因为小模型长上文对话能力很弱，强行对齐多轮问答模板会损失一定程度的单轮SFT效果。学习率设置为1e-5到1e-6的动态学习率，微调epoch数为5。# 3-full_sft.py中设置数据集为sft_data.csv
torchrun --nproc_per_node 2 3-full_sft.py
人类反馈强化学习(RLHF)之-直接偏好优化(Direct Preference Optimization, DPO):在前面的训练中，GPT已经具备了基本的对话能力，但是这样的能力完全基于单词接龙，缺少正例反例的激励。GPT尚且未知什么回答是好的，什么是差的。我们希望它能够更符合人的偏好，给出更让人满意的回答。这个过程就像是让GPT参加工作培训，从优秀员工的作为例子，消极员工作为反例，学习如何更好地服务客户。RLHF系列中，与PPO(Proximal Policy Optimization)这种需要奖励模型、价值模型的RL算法不同；DPO通过推导PPO奖励模型的显式解，把在线奖励模型换成离线数据，ref输出可以提前保存。DPO性能几乎不变，只用跑 actor 和 ref 2 个模型，大大节省显存开销和增加训练稳定性。活字三元组(q,chose,reject)数据集，学习率le-5，半精度fp16,共1个epoch，耗时1h。📋关于LLM的参数配置，有一篇很有意思的论文MobileLLM做了详细的研究和实验。 scaling law在小模型中有自己独特的规律。 引起Transformer参数成规模变化的参数几乎只取决于和。2020年提出Scaling Law的论文认为，训练数据量、参数量以及训练迭代次数才是决定性能的关键因素，而模型架构的影响几乎可以忽视。 然而似乎这个定律对小模型并不完全适用。 MobileLLM提出架构的深度比宽度更重要，「深而窄」的「瘦长」模型可以学习到比「宽而浅」模型更多的抽象概念。 例如当模型参数固定在125M或者350M时，30～42层的「狭长」模型明显比12层左右的「矮胖」模型有更优越的性能， 在常识推理、问答、阅读理解等8个基准测试上都有类似的趋势。 这其实是非常有趣的发现，因为以往为100M左右量级的小模型设计架构时，几乎没人尝试过叠加超过12层。 这与MiniMind在训练过程中，模型参数量在和之间进行调整实验观察到的效果是一致的。 然而「深而窄」的「窄」也是有维度极限的，当d_model<512时，词嵌入维度坍塌的劣势非常明显， 增加的layers并不能弥补词嵌入在固定q_head带来d_head不足的劣势。 当d_model>1536时，layers的增加似乎比d_model的优先级更高，更能带来具有“性价比”的参数->效果增益。 因此MiniMind设定small模型的d_model=512，n_layers=8来获取的「极小体积<->更好效果」的平衡。 设定d_model=768，n_layers=16来获取效果的更大收益，更加符合小模型scaling-law的变化曲线。single_chat/full_sft_512.pthmulti_chat/full_sft_512.pthsingle_chat/full_sft_512_moe.pthmulti_chat/full_sft_512_moe.pthsingle_chat/full_sft_768.pthmulti_chat/full_sft_768.pth[!TIP] 测试基于「单轮对话full_sft」和「DPO强化学习对齐」的minimind模型对比。模型文件百度网盘，其中  即为「DPO强化学习对齐」后的minimind模型权重。[Q]: 你叫什么名字？
[ft_768]: 我是一款人工智能语言模型，可以回答您的问题和提供帮助。您有什么需要我帮忙的吗？
[rl_768]: 我是一款基于人工智能技术的自然语言处理系统，主要用于处理和生成人类语言的任务。我可以帮助用户进行文本处理、语音识别、文本生成等任务，同时也可以帮助用户进行智能聊天、翻译、翻译等多种应用。如果您需要更多关于该项目的信息，请告诉我。
[ft_512]: 我很高兴能够帮助你。你有什么其他问题需要我帮忙解答吗？
[rl_512]: 我是一款基于自然语言处理技术的聊天机器人，能够理解和回复用户的问题和指令，并进行相应的回复和回复。我被训练为能够与人类进行交互和回答各种问题和任务。请问有什么需要吗？

[Q]: 你是谁？
[ft_768]: 我是一个人工智能语言模型，没有个人身份和身份。我只是根据输入的指令和文本信息生成响应。
[rl_768]: 我是一个计算机程序，没有个人身份或身份。我被设计来回答各种问题和提供帮助，包括回答问题、提供建议、进行实验、提供信息等等。我被广泛应用于各种领域，例如自然语言处理、计算机视觉、人工智能等等。如果您需要更多的帮助或信息，请随时与我联系。我会尽我所能来帮助您的工作和生活。
[ft_512]: 我是一个大型语言模型，可以回答各种各样的问题，包括科学、历史、文化和技术方面的问题。您有什么需要我回答的问题吗？
[rl_512]: 我是一个计算机程序，没有具体的身世和人物。如果您可以告诉我您想知道的具体信息，我会尽力回答您的问题。

[Q]: 中国有哪些比较好的大学？
[ft_768]: 中国有很多著名的大学，其中包括北京大学、上海交通大学、复兴中医药学院、浙江大学等。这些大学在不同的领域都有着自己的标志性建筑和研究方向，值得一提的是，它们在中国的顶尖教育资源中享有很高的声誉。
[rl_768]: 中国的大学有很多，具体情况还要根据你的需求来决定。如果你需要更详细的信息，建议可以去咨询一下你所在城市的大学排名和政策法规，从而选择一个合适的大学。同时，你可以在网上查阅相关信息，或者去咨询一些大学相关机构，如中国人民大学、中国人民大学等，以获得更为详细和专业的指导和建议。
[ft_512]: 中国有很多优秀的大学，比如麻省理工学院、北京大学、上海大学等等。这些大学在中国教育事业的发展和成就都非常突出。
[rl_512]: 中国教育局、中国大学、中国大陆大学和中国大陆大学均有优秀的教育体裁，但各地教育水平和专业要求都可能存在差异。中国教育局的学历要求、教学方法、师资力量等因素都各具特点，而大学的学历要求也各有千秋。中国大陆大学生的学历要求也各有不同，需要根据自己的实际情况进行选择。如果您对中国大陆大学生的专业要求有更具体需求，建议您可以通过互联网、国内外的各类机构和机构进行比较和交流。同时，您也可以向身边的人或朋友、老师、家长等人士寻求帮助和建议。希望以上回答对您有帮助。

[Q]: 你知道光速是多少吗？
[ft_768]: 光速是一个物理常数，表示光在真空中传播的速度，大约是每秒299792458米。光速是物理学中最基本的物理常数之一，也是相对论的重要基础之一。
[rl_768]: 当然知道，光速是光在真空中传播的速度，通常用c来表示。它是宇宙物理学中非常重要的理论之一。在物理学中，光速被认为是宇宙中最重要的速度之一。它代表着光在真空中传播的速度，这也是许多重要公式和公式的基础。
[ft_512]: 光速是光在真空中传播的速度，约为每秒299,792,458米。它是几乎所有的物理学概念，也是宇宙中最神秘和最重要的物理概念之一。
[rl_512]: 光速是光在真空中传播的速度，是自然界中不可逆转的速度。光源是由光的原子、分子、固体和光的组成，其速度约为每秒平方。光的速度是每秒约1个英亞或光年，这个数字是根据相对论、量子力学和引力理论来计算的。光速是相对的概念之一，它代表着我们对自然界中任何事件的理解和解释。
RLHF数据使用大约10万条；full_sft模型在简洁性和信息准确性方面表现更好；rl模型在回答中提供了更多的背景信息，但信息准确性有待改进。总的来说RLHF后的模型倾向于学习：说更多有礼貌但无用的废话讨好“对话”本身，而对信息准确性则有轻微损失。天下没有免费的午餐，还需要继续提升RLHF数据集的质量，也要接受模型能力无法避免的损失(程度有轻重)。DPO和在线PPO的区别在于reject和chosen都是离线准备的，和minimind模型本身的输出必然存在很大的分布差异。这类似于DPO算法使模型观看乒乓球世界冠军的打法「录像」进行强化学习，而不是像PPO一样请reward模型做「教练」纠正自己的打法强化学习。[!TIP] 以下测试于2024-09-17完成，此日期后发布的新模型，无特殊需要时将不加入测试。 测试基于单轮对话full_sft的minimind模型(无多轮微调和强化学习微调)。[Q]: 你知道长江吗？
[A]: 是的，长江是中国最长的河流，全长超过3300千米。
[B]: 长江是中国最长的河流，总长度约为6300公里。它发源于青藏高原的青海湖，流经青海、四川、甘肃、宁夏、内蒙古、陕西和山西。长江流域是中国最大的农业区之一，也是中国的文化和经济中心。
[C]: 是的，长江是中国最长的河流之一，也是世界第三大河流。长江发源于青藏高原，流经11个省份，最终注入东海。长江流域是中国最大的河流流域之一，拥有着丰富的自然资源和文化遗产，如长江大桥、长江海洋博物馆、长江古城等。
[D]: 同时也知道，中国是世界上最长的城市之一，拥有超过3400万人口。
[E]: 是的，长江是中国最长的河流，也是世界上最长的河流之一。它发源于青藏高原，流经九个省区，最终注入东海。长江流域的气候温和，四季分明，是中国重要的农业和工业基地。长江流域的水产资源丰富，是中国的重要渔业资源之一。


[Q]: 第一颗人造卫星是哪个国家发射的？
[A]: 第一颗人造卫星是斯普特尼克1号。
[B]: 第一颗人造卫星是俄罗斯的国际卫星，于1957年发射。
[C]: 第一颗人造卫星是苏联发射的，它是由苏联于1957年10月发射的。
[D]: 这颗名为“阿姆斯特朗”的卫星，于1961年11月20日发射，是第一个成功将人类送入太空的卫星。
[E]: 第一颗人造卫星是苏联于1957年4月4日发射的，它是由苏联宇航员尤里·加加林（Yuri Gagarin）和苏联工程师罗尔德·谢尔盖耶夫（Roald Skohley）共同发射的。在发射过程中，加加林和谢尔盖耶夫使用了一个名为“斯普特尼克1号”的小型卫星，它的直径为4.3米，重量为1,051公斤。这枚卫星被称为“斯普特尼克1号”，是苏联太空竞赛中的一个里程碑，也是人类历史上第一次在太空中进行太空行走。


[Q]: 你知道泰山在哪里吗？
[A]: 泰山位于中国东部地区，是中国五岳之一。
[B]: 泰山位于山东省泰安市泰安市。泰安市是中国五岳之一，也是世界文化遗产和中国历史文化名山之一。泰安市有许多著名的景点，如五岳之一、玉皇顶、南天门、玉皇阁等。
[C]: 泰山位于中国山东省泰安市，是中国五岳之一。
[D]: 作为一个人工智能助手，我无法访问互联网，因此无法提供关于你所在位置的信息。不过，你可以通过搜索或搜索相关关键词来获取有关你所在位置的信息，或者查看相关新闻或网站来获取有关你所在位置的信息。
[E]: 泰山位于中国山东省泰安市境内，是中国五岳之一，也是中国文化和历史的重要象征之一。它是中国五岳之一，也是中国道教和佛教的发源地之一。泰山的海拔高度为1,545米，是中国五岳之一，也是中国最著名的旅游景点之一。它的历史可以追溯到公元前5世纪，是中国古代帝王祭祀的场所。泰山也是中国文化的重要组成部分，被誉为“五岳之首”，是中国文化的重要遗产之一。


[Q]: 世界上最高的山峰是什么？
[A]: 世界上最高的山峰是珠穆朗玛峰，位于尼泊尔和中国的边境上，海拔8848米。
[B]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848米。
[C]: 珠穆朗玛峰是世界上最高的山峰，海拔8848米。
[D]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.69英尺）。
[E]: 世界上最高的山峰是珠穆朗玛峰，也称为萨加玛塔，位于喜马拉雅山脉的尼泊尔和中国边境之间，海拔8,848.86米（29,031.69英尺）。它是世界上最著名的登山目标之一，吸引着来自世界各地的登山者前来挑战。珠穆朗玛峰的名字来自藏语，意思是“女神的额峰”，它是由两个不同的山峰组成的，其中一个是位于尼泊尔的珠穆朗玛峰，另一个则位于中国。它的高度是通过测量海拔高度来确定的，因为它是通过测量地球的平均高度来确定的。尽管珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何其他地方都能到达的。地球上的其他地方，如南美洲的安第斯山脉，也是世界上最高的山脉，但它并不是地球上最高的山峰。总之，珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何地方都能到达的。它是地球上最高的山脉，但它并不是地球上任何地方都能到达的。


[Q]: 万有引力是谁提出的？
[A]: 万有引力是由英国物理学家牛顿提出的。他认为，所有物体都会产生引力，这个引力是由物体的质量和距离决定的。这个理论揭示了质量和距离之间的关系，并证明了万有引力的本质。
[B]: 万有引力是由牛顿在1687年提出的。它是宇宙中一个基本的物理现象，它描述了物体之间的相互作用。这个力被称为万有引力，它在所有物体之间都存在，这种力与物体的质量和距离有关。万有引力是自然界中最基本的力量之一，它不仅影响着物体的运动和变化，还对宇宙的结构和演化产生着重要的影响。
[C]: 万有引力是英国物理学家艾萨克·牛顿在17世纪提出的。
[D]: 引力是宇宙中一种力量，它是由质量或能量所组成的。根据牛顿的万有引力定律，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。因此，牛顿的万有引力定律表明，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。这个定律在物理学、天文学和物理学中都有广泛的应用。
[E]: 万有引力是由英国物理学家艾萨克·牛顿于1687年提出的。他在1687年发表了一篇著名的论文，题为《自然哲学的数学原理》，其中提出了万有引力定律，指出任何两个物体之间都存在着一种引力，这种引力的大小与它们的质量成正比，与它们之间的距离的平方成反比。这个定律被称为万有引力定律，它描述了质量和距离之间的关系。它表明，两个物体之间的引力与它们的质量成正比，与它们之间的距离的平方成反比。这个定律对于解释宇宙中的许多现象和现象之间的相互作用非常重要，例如行星绕太阳运动、恒星之间的引力透镜效应等等。它也被广泛应用于天文学、物理学和工程学等领域，为我们理解宇宙的本质和演化提供了重要的理论基础。


[Q]: 你知道大熊猫的主要食物是什么吗？
[A]: 大熊猫的主要食物是竹子。它们通常吃竹子，每天需要吃掉大约30公斤的竹子。
[B]: 是的，大熊猫主要以竹子为食。它们每天需要吃掉大量的竹子来维持生命，每天需要吃掉40至80公斤的竹子。此外，它们也会吃一些其他的植物，如草、果实、坚果和小型哺乳动物。
[C]: 大熊猫主要以竹子为食，也会偶尔食用其他的植物和动物。此外，大熊猫也会吃一些野果、小动物和植物，但这些都不属于它们的主要食物来源。
[D]: 大熊猫的主要食物是竹子。
[E]: 大熊猫是一种以竹子为主要食物的植食动物，它们的肠道结构适合消化纤维素含量高的竹子，但也会吃一些其他植物，如草本植物和小型哺乳动物。大熊猫的肠道结构也适应了高纤维的竹子，这使得它们能够消化大量的纤维素，从而帮助它们在野外生存。大熊猫的肠道中还含有一种叫做“纤维素酶”的酶类物质，这种酶能够帮助大熊猫分解纤维素，从而帮助它们消化大量的水分和营养物质。此外，大熊猫的肠道还含有一种叫做“纤维素蛋白”的蛋白质，这种蛋白质能够帮助大熊猫维持体内的能量水平，从而帮助它们在野外生存。总的来说，大熊猫的主要食物是竹子，但它们也会吃一些其他植物，如草本植物和小型哺乳动物。


[Q]: 海水为什么是咸的？
[A]: 海水中含有许多盐分，这些盐分可以导致海水咸度的增加。海水中的盐分主要来自于海水中的盐分，这些盐分可以从海水中的盐分中获得。海水中的氧化物和盐分可以与海水中的盐分形成氢键，从而使海水咸度增加。
[B]: 海水是咸的，这是因为它含有大量的盐分，其中包括氯离子和氯离子。这些离子在海洋中形成了大量的盐，这些盐分被称为盐分。海水中含有许多溶解在水中的盐，这些盐分会随着海水的升高而逐渐蒸发，最终导致海水中盐分的蒸发。这些盐分在海水中被完全消耗，然后被排除在海水中。此外，海水中还含有许多其他的盐分，但这些盐分是不同的，因为它们在水中的浮起和下降过程是不同的。海水中的盐分是从海底的岩石中提取出来的，这些盐分在海水中可以被利用。总的来说，海水是咸的，这也是为什么海水中含有许多盐分和盐分的原因。
[C]: 海水含有许多溶解的盐类，其中最主要的是氯化钠。当水中加入盐时，盐的分子会与水分子发生反应，形成氢氧化钠，这是一种盐。
[D]: 因为水是许多生物的生存所必需的，它能够吸收和释放二氧化碳，释放氧气，维持大气中的水蒸气，以及维持水循环。水中的盐类和化学物质会释放出氧气，使地球上的生物呼吸。此外，水还具有许多其他功能，如保持水循环、维持水循环、调节气候和环境等。水是许多生物生存所必需的，它也是许多自然现象的重要基础。
[E]: 海水是咸的主要原因是因为其中含有大量的溶解盐类，其中最主要的是氯化钠（NaCl）。氯化钠是一种非常常见的盐类，它存在于地球上的大部分地区，包括海洋、湖泊、河流和地下水。当水从海洋中蒸发时，它会带走一部分盐分，使海水变得咸。此外，海中还含有其他溶解盐类的物质，如硫酸盐、硫酸镁、硫酸钙、硫酸镁盐、硫酸镁钾、硫酸镁钠、硫酸镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钠、硫酸镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁镁钙、硫酸镁镁镁镁
[!NOTE] 🙋‍♂️直接把上述模型的回答丢给GPT-4o，让它帮忙打个分：：模型A的回答通常简洁明了，但在某些问题上缺乏详细信息和准确性。例如，在长江的长度问题上，模型A的回答是错误的。：模型B的回答在某些问题上提供了额外的信息，但这些信息有时是不准确的或多余的。例如，在长江的长度问题上，模型B提供了不准确的长度和流域面积。：模型C的回答通常较为详细，且在大多数问题上提供了准确的信息。例如，在长江和泰山的问题上，模型C的回答是准确的。：模型D的回答在某些问题上显得混乱，且缺乏准确性。例如，在泰山的问题上，模型D的回答完全偏离了主题。：模型E的回答通常非常详细，但在某些问题上过于冗长，且包含了一些不必要的信息。例如，在万有引力的问题上，模型E的回答过于复杂。minimind系列（ABC）的排序符合直觉，minimind-v1(0.1B)评分最高，常识性问题的回答基本没有错误和幻觉。出乎意料的是，minimind-v1-small(0.02B)仅有26M参数，却可以接近minimind-v1(0.1B)的表现。minimind-v1(0.1B)的sft轮数仅有不到2，偷懒提前kill腾出资源给小模型，0.1B没有得到充分训练的情况下依然做到了最强，其实还是底大一级压死人。minimind-v1-moe(0.1B)表现只比minimind-v1-small(0.02B) 略好，同样是因为偷懒早停腾出资源做其它训练了，但是MoE模型这种稀疏多Experts模式需要的训练轮次需要酌情更高，让所有FFN层专家得到路由的激活充分训练，在目前epochs设置为3时训练的还不够充足。 minimind在早期实验验证阶段在Yi-Tokenizer上试验过moe的充分训练版本，可以做到比dense小模型表现肉眼可见地更好。此部分可能需要留给日后腾出服务器再训练并更新v2、v3版本。E模型的回答肉眼看起来是非常不错的，尽管存在些许幻觉瞎编的情况。但GPT-4o和Deepseek的评分都一致认为它“信息过度冗长，且有重复内容，存在幻觉”。 其实这种评价略显严格，100个字中哪怕有10个字是幻觉，就很容易把它归到低分。由于E模型预训练文本长度更长，数据集大得多，所以回答的看起来很完备。在体积近似的情况下，数据数量和质量都很重要。Scaling Law：模型参数越大，训练数据越多模型的性能越强。C-Eval评测代码见：， 小模型的测评通常为了避免回复格式的难以固定的特点， 而直接判断,,,四个字母对应token预测概率，取最大的作为回答答案，与标准答案计算正确率。 minimind模型本身没有使用较大的数据集训练，也没有针对回答选择题的指令做微调，测评结果可以当个参考。probability_and_statisticschinese_language_and_literatureideological_and_moral_cultivationenvironmental_impact_assessment_engineermiddle_school_mathematics总题数: 1346  
总正确数: 316  
总正确率: 23.48%
以下来自GPT-4o对minimind表现的瞎猜：### 模型擅长的领域：
1. 高中的化学：正确率为42.11%，是最高的一个领域。说明模型在这方面的知识可能较为扎实。
2. 离散数学：正确率为37.50%，属于数学相关领域，表现较好。
3. 教育科学：正确率为37.93%，说明模型在教育相关问题上的表现也不错。
4. 基础医学：正确率为36.84%，在医学基础知识方面表现也比较好。
5. 操作系统：正确率为36.84%，说明模型在计算机操作系统方面的表现较为可靠。

### 模型不擅长的领域：
1. 法律相关：如法律专业（8.70%）和税务会计（20.41%），表现相对较差。
2. 中学和大学的物理：如中学物理（26.32%）和大学物理（21.05%），模型在物理相关的领域表现不佳。
3. 高中的政治、地理：如高中政治（15.79%）和高中地理（21.05%），模型在这些领域的正确率较低。
4. 计算机网络与体系结构：如计算机网络（21.05%）和计算机体系结构（9.52%），在这些计算机专业课程上的表现也不够好。
5. 环境影响评估工程师：正确率仅为12.90%，在环境科学领域的表现也不理想。

### 总结：
- 擅长领域：化学、数学（特别是离散数学）、教育科学、基础医学、计算机操作系统。
- 不擅长领域：法律、物理、政治、地理、计算机网络与体系结构、环境科学。

这表明模型在涉及逻辑推理、基础科学和一些工程技术领域的问题上表现较好，但在人文社科、环境科学以及某些特定专业领域（如法律和税务）上表现较弱。如果要提高模型的性能，可能需要加强它在人文社科、物理、法律、以及环境科学等方面的训练。
minimind (root dir)
├─minimind
|  ├── config.json
|  ├── generation_config.json
|  ├── LMConfig.py
|  ├── model.py
|  ├── pytorch_model.bin
|  ├── special_tokens_map.json
|  ├── tokenizer_config.json
|  ├── tokenizer.json
python chat_openai_api.py
curl http://ip:port/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{ 
    "model": "model-identifier",
    "messages": [ 
      { "role": "user", "content": "世界上最高的山是什么？" }
    ], 
    "temperature": 0.7, 
    "max_tokens": -1,
    "stream": true
}'
在fastgpt中接入使用minimind api[!TIP] 如果您觉得 对您有所帮助，可以在 GitHub 上加一个⭐ 篇幅不短水平有限难免纰漏，欢迎在Issues交流指正或提交PR改进项目[!NOTE] 众人拾柴火焰高 如果您已经尝试训练了新的MiniMind型号，欢迎在Discussions或Issues中分享您的模型权重 可以是在特定下游任务或垂直领域（例如情感识别、医疗、心理、金融、法律问答等）的MiniMind新模型版本 也可以是拓展训练后（例如探索更长文本序列、更大体积（0.1B+）或更大的数据集）的MiniMind新模型版本 任何分享都视作独一无二的，所有尝试都具有价值，并受到鼓励 这些贡献都会被及时发现并整理在鸣谢列表中，再次感谢所有支持！]]></content:encoded></item><item><title>unslothai/unsloth</title><link>https://github.com/unslothai/unsloth</link><author></author><category>trending</category><pubDate>Sun, 9 Feb 2025 02:28:32 +0000</pubDate><source url="http://mshibanami.github.io/GitHubTrendingRSS">Dev - Github Trending</source><content:encoded><![CDATA[Finetune Llama 3.3, DeepSeek-R1 & Reasoning LLMs 2x faster with 70% less memory! 🦥All notebooks are ! Add your dataset, click "Run All", and you'll get a 2x faster finetuned model which can be exported to GGUF, Ollama, vLLM or uploaded to Hugging Face.🥇 Performance BenchmarkingWe tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):💾 Installation InstructionsFor stable releases, use . We recommend pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git" for most installations though.⚠️Only use Conda if you have it. If not, use Pip. Select either  for CUDA 11.8 or CUDA 12.1. We support .conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps trl peft accelerate bitsandbytes
⚠️Do **NOT** use this if you have Conda. Pip is a bit more complex since there are dependency issues. The pip command is different for  and CUDA versions.For other torch versions, we support , , , ,  and for CUDA versions, we support  and  and . For Ampere devices (A100, H100, RTX3090) and above, use  or  or .For example, if you have  and , use:pip install --upgrade pip
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
Another example, if you have  and , use:pip install --upgrade pip
pip install "unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
Or, run the below in a terminal to get the  pip installation command:wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
Or, run the below manually in a Python REPL:try: import torch
except: raise ImportError('Install torch via `pip install torch`')
from packaging.version import Version as V
v = V(torch.__version__)
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] >= 8
if cuda != "12.1" and cuda != "11.8" and cuda != "12.4": raise RuntimeError(f"CUDA = {cuda} not supported!")
if   v <= V('2.1.0'): raise RuntimeError(f"Torch = {v} too old!")
elif v <= V('2.1.1'): x = 'cu{}{}-torch211'
elif v <= V('2.1.2'): x = 'cu{}{}-torch212'
elif v  < V('2.3.0'): x = 'cu{}{}-torch220'
elif v  < V('2.4.0'): x = 'cu{}{}-torch230'
elif v  < V('2.5.0'): x = 'cu{}{}-torch240'
elif v  < V('2.6.0'): x = 'cu{}{}-torch250'
else: raise RuntimeError(f"Torch = {v} too new!")
x = x.format(cuda.replace(".", ""), "-ampere" if is_ampere else "")
print(f'pip install --upgrade pip && pip install "unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git"')
To run Unsloth directly on Windows:trainer = SFTTrainer(
    dataset_num_proc=1,
    ...
)
For advanced installation instructions or if you see weird errors during installations:Install  and . Go to https://pytorch.org to install it. For example pip install torch torchvision torchaudio tritonConfirm if CUDA is installated correctly. Try . If that fails, you need to install  or CUDA drivers.Install  manually. You can try installing  and seeing if  succeeds. Check if  succeeded with  Go to https://github.com/facebookresearch/xformers. Another option is to install  for Ampere GPUs.Finally, install  and check it with Go to our official Documentation for saving to GGUF, checkpointing, evaluation and more!We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!If you want to download models from the ModelScope community, please use an environment variable: , and install the modelscope library by: pip install modelscope -U.unsloth_cli.py also supports  to download models and datasets. please remember to use the model and dataset id in the ModelScope community.from unsloth import FastLanguageModel 
from unsloth import is_bfloat16_supported
import torch
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!
# Get LAION dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files = {"train" : url}, split = "train")

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    tokenizer = tokenizer,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 10,
        max_steps = 60,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        output_dir = "outputs",
        optim = "adamw_8bit",
        seed = 3407,
    ),
)
trainer.train()

# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like
# (1) Saving to GGUF / merging to 16bit for vLLM
# (2) Continued training from a saved LoRA adapter
# (3) Adding an evaluation loop / OOMs
# (4) Customized chat templates
DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from Llama-Factory. We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: notebook.import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID

from unsloth import FastLanguageModel, PatchDPOTrainer
from unsloth import is_bfloat16_supported
PatchDPOTrainer()
import torch
from transformers import TrainingArguments
from trl import DPOTrainer

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    args = TrainingArguments(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        seed = 42,
        output_dir = "outputs",
    ),
    beta = 0.1,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    max_length = 1024,
    max_prompt_length = 512,
)
dpo_trainer.train()
🥇 Detailed Benchmarking TablesContext length benchmarksLlama 3.1 (8B) max. context lengthWe tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.Llama 3.3 (70B) max. context lengthWe tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.You can cite the Unsloth repo as follows:@software{unsloth,
  author = {Daniel Han, Michael Han and Unsloth team},
  title = {Unsloth},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}
]]></content:encoded></item></channel></rss>
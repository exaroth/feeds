<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Kubernetes</title><link>https://konrad.website/feeds/</link><description></description><item><title>Busting Myths About Cisco 700-150 ICS!</title><link>https://www.reddit.com/r/kubernetes/comments/1iq3umq/busting_myths_about_cisco_700150_ics/</link><author>/u/lucina_scott</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 15:33:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Career transition in to Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/</link><author>/u/Similar-Secretary-86</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 13:41:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA["I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated ]]></content:encoded></item><item><title>RE: Moving Helm Charts to ArgoCD (I’m doing the same!)</title><link>https://www.reddit.com/r/kubernetes/comments/1iq0tsn/re_moving_helm_charts_to_argocd_im_doing_the_same/</link><author>/u/Helpful_Spinach_9963</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 12:59:51 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I saw this question posted 10 months ago OG Post, and I also need to migrate a bunch of Helm charts to ArgoCD (helm chart per microservice).From what I gathered, the general recommendation is to leave old Helm installations as they are and let ArgoCD take over moving forward. However, one concern I have is what happens when a new manifest is added to a Helm chart that was previously installed via  but is now being synced by ArgoCD?Wouldn't this create a situation where ArgoCD's managed resources differ from what Kubernetes has, leading to orphaned manifests? That seems undesirable long term. I’d assume the best practice is to uninstall the Helm charts and let ArgoCD handle everything but I’d like to avoid downtime. The other option is manually removing Helm annotations and labels, but that sounds incredibly tedious.What’s the best practice for transitioning Helm charts to ArgoCD in a way that avoids downtime and keeps everything clean? Remove helm and just use kustomize? Would love to hear how others have handled this.]]></content:encoded></item><item><title>My new blog post comparing networking in EKS vs. GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/</link><author>/u/jumiker</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 11:06:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/jumiker ]]></content:encoded></item><item><title>Deep Dive into VPA Recommender</title><link>https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/</link><author>/u/erik_zilinsky</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 10:26:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.Based on my findings, I wrote a blog post about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.]]></content:encoded></item><item><title>Restrict egress alternative way.</title><link>https://www.reddit.com/r/kubernetes/comments/1ipwx71/restrict_egress_alternative_way/</link><author>/u/Common-Feedback-7370</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 08:16:11 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I need to restrict egress from the  deployed as a pod in Kubernetes. I test used a network policy, which worked properly, but there's a requirement to avoid redeploying nodes (since enabling network policy on GKE causes all nodes to redeploy).So I try using Kuma and configured it within the namespace where the  is located, but it turned out to be too complicated.Does anyone have any ideas for how to restrict egress access using a sidecar without affecting the underlying infrastructure?Any suggestions would be greatly appreciated. ]]></content:encoded></item><item><title>Container Networking - Kubernetes with Calico</title><link>https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/</link><author>/u/tkr_2020</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 07:25:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[: VLAN 10: VLAN 20When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:The inner IP header reflects:The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?]]></content:encoded></item><item><title>How do I configure Minikube to use my local IP address instead of the cluster IP?</title><link>https://www.reddit.com/r/kubernetes/comments/1ipr3i1/how_do_i_configure_minikube_to_use_my_local_ip/</link><author>/u/Own_Appointment5630</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Sat, 15 Feb 2025 02:02:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi there!! How can I configure Minikube on Windows (using Docker) to allow my Spring Boot pods to connect to a remote database on the same network as my local machine? When I create the deployment, the pods use the same IP as the Minikube cluster which gets rejected by the database. Is there any way that Minikube uses my local IP in order to connect correctly?.]]></content:encoded></item><item><title>Calico apiserver FailedDiscovery Check</title><link>https://www.reddit.com/r/kubernetes/comments/1ipkg32/calico_apiserver_faileddiscovery_check/</link><author>/u/Flimsy_Tomato4847</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 20:42:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I installed the calico operator and follwing custom-resources.yaml:# This section includes base Calico installation configuration. # For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.Installation apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: ipPools: - name: default-ipv4-ippool blockSize: 26 cidr: 192.168.0.0/16 encapsulation: None natOutgoing: Enabled nodeSelector: all() --- # This section configures the Calico API server. # For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServer apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} Getting this error in kube-apiserver logs:E0214 20:38:09.439846 1 remote_available_controller.go:448] "Unhandled Error" err="v3.projectcalico.org failed with: failing or missing response from https://10.96.207.72:443/apis/projectcalico.org/v3: Get \"https://10.96.207.72:443/apis/projectcalico.org/v3\": dial tcp 10.96.207.72:443: connect: connection refused" logger="UnhandledError" E0214 20:38:09.445839 1 controller.go:146] "Unhandled Error" err=< Error updating APIService "v3.projectcalico.org" with err: failed to download v3.projectcalico.org: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.96.207.72:443: connect: connection refused calico-apiserver calico-api ClusterIP 10.96.207.72<none> 443/TCP 45mDo you know any things to solve this?   submitted by    /u/Flimsy_Tomato4847 ]]></content:encoded></item><item><title>For those managing or working with multiple clusters, do you use a combined kubeconfig file or separate by cluster?</title><link>https://www.reddit.com/r/kubernetes/comments/1ipg99n/for_those_managing_or_working_with_multiple/</link><author>/u/trouphaz</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 17:44:13 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I wonder if I'm in the minority. I have been keeping my kubeconfigs separate per cluster for years while I know others that combine everything to a single file. I started doing this because I didn't fully grasp yaml when I started and when I had an issue with the kubeconfig, I didn't have any idea on how to repair it. So I'd have to fully recreate it.So, each cluster has its own kubeconfig file named for the cluster's name and I have a function that'll set my KUBECONFIG variable to the file using the cluster name.sc() { CLUSTER_NAME="${1}" export KUBECONFIG="~/.kube/${CLUSTER_NAME}" } ]]></content:encoded></item><item><title>Calico CNI - services and pods cant connect to ClusterIP</title><link>https://www.reddit.com/r/kubernetes/comments/1ipehey/calico_cni_services_and_pods_cant_connect_to/</link><author>/u/Flimsy_Tomato4847</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 16:29:11 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I am running a kubernetes cluster with a haproxy + keepalived setup for the cluster-endpoint (virtual IP Address). All nodes are in the same subnet. Calico operator installation works well. But when i deploy pods they can't connect to each other nevertheless they are in the same subnet or in different subnets. There is just the standard network policy enabled, so network policies cant be the issue.Now when i look a the calico-kube-controller logs i get:[FATAL][1] kube-controllers/main.go 136: Failed to initialize Calico datastore"major": "1", "minor": "31", ... }Connecting to 10.96.0.1:443 (10.96.0.1:443)wget: can't connect to remote host (10.96.0.1): Connection refusedI dont know how to fix this strange behavior, beacause i also tried the ebpf dataplane with same behavior and i dont know where my mistake is.I init the cluster with: sudo kubeadm init --control-plane-endpoint=<myVIP>:6443 --pod-network-cidr=192.168.0.0/16 --upload-certsFYI this is my calico custom-resources.yamlapiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: calicoNetwork: ipPools: - name: default-ipv4-ippool blockSize: 26 cidr: 192.168.0.0/16 encapsulation: None natOutgoing: Enabled nodeSelector: all() linuxDataplane: Iptables --- apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} The active network policy created by default:apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: creationTimestamp: "2025-02-14T09:29:49Z" generation: 1 name: allow-apiserver namespace: calico-apiserver ownerReferences: - apiVersion: operator.tigera.io/v1 blockOwnerDeletion: true controller: true kind: APIServer name: default uid: d1b2a55b-aa50-495f-b751-4173eb6fa211 resourceVersion: "2872" uid: 63ac4155-461b-450d-a4c8-d105aaa6f429 spec: ingress: - ports: - port: 5443 protocol: TCP podSelector: matchLabels: apiserver: "true" policyTypes: - Ingress This is my haproxy config with the VIPglobal log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:6443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server master1 <master1-ip>:6443 check server master2 <master2-ip>:6443 check server master3 <master3-ip>:6443 check global_defs { router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0.1 vrrp_gna_interval 0.1 } vrrp_script chk_haproxy { script "killall -0 haproxy" interval 2 weight 2 } vrrp_instance haproxy-vip { state MASTER priority 101 interface ens192 # Network card virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { <myVIP>/24 # The VIP address } track_script { chk_haproxy } } ]]></content:encoded></item><item><title>kubernetes vcenter</title><link>https://www.reddit.com/r/kubernetes/comments/1ipdnbw/kubernetes_vcenter/</link><author>/u/Low_Metal_7679</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 15:53:23 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[hello i am getting started with kubernetes i have created a NFS as PV but how can i use vmware datastores to use this as PV?- VMWARE-H1-DC1 - VMWARE-H2-DC1 - VMWARE-H4-DC2i have a test cluster with on each host a vmi have deployed it using ansible so the config is on evry host the same but dont know how to use vcenter storage. I gues i need to provide a CSO or so but dont know how to do this can someone help me out with this?]]></content:encoded></item><item><title>Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11</title><link>https://www.youtube.com/watch?v=eHa6GhK7L0I</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><category>devops</category><enclosure url="https://www.youtube.com/v/eHa6GhK7L0I?version=3" length="" type=""/><pubDate>Fri, 14 Feb 2025 13:59:30 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Thinking About Taking the 78201X Exam? Read This First!</title><link>https://www.reddit.com/r/kubernetes/comments/1ip966e/thinking_about_taking_the_78201x_exam_read_this/</link><author>/u/lucina_scott</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 12:08:41 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Learn from Documentation or Book?</title><link>https://www.reddit.com/r/kubernetes/comments/1ip8d2r/learn_from_documentation_or_book/</link><author>/u/nicholle_marvel</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 11:17:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[In 2025, there are numerous books available on Kubernetes, each addressing various scenarios. These books offer solutions to real-world problems and cover a wide range of topics related to Kubernetes.On the other hand, there is also very detailed official documentation available.Is it worth reading the entire documentation to learn Kubernetes, or should one follow a book instead?Two follow-up points to consider: 1. Depending on specific needs, one might visit particular chapters of the official documentation. 2. Books often introduce additional tools to solve certain problems, such as monitoring tools and CI/CD tools.Please note that the goal is not certification but rather gaining comprehensive knowledge that will be beneficial during interviews and in real-world situations.]]></content:encoded></item><item><title>Weekly: Share your victories thread</title><link>https://www.reddit.com/r/kubernetes/comments/1ip84c0/weekly_share_your_victories_thread/</link><author>/u/gctaylor</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 11:00:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Got something working? Figure something out? Make progress that you are excited about? Share here!]]></content:encoded></item><item><title>How am I just finding out about the OhMyZsh plugin?</title><link>https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/kubectl</link><author>/u/ominouspotato</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 09:56:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[It’s literally just a bunch of aliases but it has made CLI ops so much easier. Still on my way to memorizing them all, but changing namespace contexts and exec-ing to containers has never been easier. Highly recommend if you’re a k8s operator! Would also love to hear what you all use in your day-to-day. My company is looking into GUI tools like Lens but they haven’t bought licenses yet. ]]></content:encoded></item><item><title>Deprecated APIs</title><link>https://www.reddit.com/r/kubernetes/comments/1ip6b1z/deprecated_apis/</link><author>/u/LocksmithRound9835</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 08:44:48 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Has anyone created a self service solution for application teams to find out manifests leveraging deprecated APIs? Solution like kubent etc need developers to download binaries and run commands against namespaces.   submitted by    /u/LocksmithRound9835 ]]></content:encoded></item><item><title>Advancing Open Source Gateways with kgateway</title><link>https://www.cncf.io/blog/2025/02/05/advancing-open-source-gateways-with-kgateway/</link><author>/u/dshurupov</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 08:10:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Posted on February 5, 2025
		At KubeCon NA 2024, Solo.io announced its intention to donate the Gloo Gateway open source project to the CNCF, to benefit the broader cloud native ecosystem. In case you are not familiar with Gloo Gateway, it is the most mature and widely deployed Envoy-based gateway in the market today. Built on open source and open standards, Gloo Gateway is Kubernetes-native and implements the Kubernetes Gateway API. Today, we’d like to update you on some exciting developments since the announcement.What is a gateway, and why donate Gloo Gateway to CNCF?If you are using Kubernetes, you most likely need a gateway to control traffic going into or out of your cluster. Gateways are a critical component of the Kubernetes ecosystem, which is why the Kubernetes community collectively developed the Gateway API based on lessons learned from the Ingress API and Istio. With the growth of AI workloads and the rising need for traffic and cost control for calling LLM providers, gateways play an even more critical role in controlling and securing all-direction traffic—north-south, east-west, or inter-service communication.While Gloo Gateway is the most mature and feature-rich Envoy-based gateway today, we believe that under vendor-neutral governance, the project’s adoption and ecosystem integration will reach new heights. Donating the project to the CNCF will expand the contributor base, foster innovation across organizations, and provide a battle-tested, feature-rich, vendor-neutral gateway project to the diverse global CNCF user community.In November 2024, we moved the Gloo open source repository to the k8sgateway repository as a preparatory step for the donation. After working with the CNCF TOC, Kubernetes SIG-Network, and steering committee leaders, we renamed the project to kgateway.Given Gloo’s large adopter base, we believe kgateway qualifies as a CNCF incubation project. Due to the lengthy due diligence process for incubation projects, we decided to donate it as a CNCF sandbox project instead. We look forward to working with the CNCF TOC and TAG Network leaders for the upcoming sandbox review.Working closely with the maintainer community, we established governance for the project that rewards maintainership while ensuring no single company has a controlling stake. Using the git-vote bot for transparency, we successfully held our first governance vote. Out of 10 eligible voters (including 4 maintainers outside Solo.io), nine voted favorably on the proposed governance PR:In addition to renaming the project, recruiting maintainers, and establishing governance, we’ve been focused on: Developing buildable, vendor-neutral artifacts, set to launch in the coming weeks.Improved Development Velocity: Establishing robust pipeline checks for PRs, including linting, Kubernetes Gateway API conformance tests, and end-to-end testing. Ensuring the project remains highly extendable, aligning with core design principles of kgateway and Envoy.A shout-out to our core maintainers and contributors for laying this solid foundation for innovation.You may notice that the Gloo open source repository still exists. This is temporary during the transition period. With Gloo’s large open source user base, we understand that migrating from Gloo to kgateway takes time. In upcoming releases, we plan to deprecate the Gloo repository to focus all open-source efforts on kgateway.We aim to make kgateway the most popular gateway for all-direction traffic—north-south, east-west, or inter-service communication. In addition to implementing the latest Kubernetes Gateway API features, we’re prioritizing:Traffic control for AI workloadsAs workloads like AI agents run on Kubernetes clusters, questions arise:How securely do they connect to LLM services such as OpenAI or Gemini?Are these services local or external, with usage-based costs?How should credentials and backup LLMs be managed?Do you want each developer to develop prompt guard and enrichment in their own AI workloads? Kgateway simplifies these challenges with two proposed declarative APIs for routing traffic to LLM providers while applying advanced policies such as secret management, backup LLMs, prompt guard or enrichment, and more. Refer to the enhancement proposal for more information.The Gateway API Inference extension, sponsored by Kubernetes SIG-Network, focuses on extending the Kubernetes Gateway API with inference-specific routing extensions. It introduces the concept of an “InferencePool” (composed of one or more inference pods), enabling application developers to effectively route requests based on AI workload requirements. Daneyon Hanson has been leading our work for interference extensions and we are proud to see him nominated as a maintainer on the Gateway API Inference extension project as a result!Providing advanced Layer 7 features for Istio in ambient modeAmbient mode splits Istio functionality into a secure overlay layer (ztunnels) and a Layer 7 processing layer (waypoint). With kgateway as the waypoint proxy, users gain advanced L7 features such as request transformation, retries, and traffic control for AI workloads connecting to LLM services. This pluggability ensures consistent operational experiences for north-south and inter-service traffic. Steven Landow, who is a maintainer on both Istio and kgateway, is leading this effort.To explore additional roadmap initiatives or propose updates, please refer to our roadmap document.Are you interested in exploring kgateway, or are you already a Gloo Gateway user? We’d love to hear from you and shape the future of kgateway together. If working on cutting-edge cloud-native projects excites you, join us as a contributor! Connect with us via:Let’s work together and build kgateway into the future of cloud connectivity!]]></content:encoded></item><item><title>An argument for how Kubernetes can be use in development and reduce overall system complexity.</title><link>https://youtu.be/EJrw3z7m5iQ?si=D4BhYdkGryByoIVj</link><author>/u/purton_i</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 05:59:59 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Strimzi migration to Axual Platform</title><link>https://www.reddit.com/r/kubernetes/comments/1ip2d4n/strimzi_migration_to_axual_platform/</link><author>/u/k8s_maestro</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 04:20:26 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Use Case: The plan was to adopt and go with open source solutions and went with Strimzi - Apache Kafka on Kubernetes Eventually the team decided to go for enterprise solution like Axual Platform. Now the question is, the migration possibilities.Did someone came across this scenario?Strimzi to Axual Platform]]></content:encoded></item><item><title>Job roles related to Kubernetes/OpenShift</title><link>https://www.reddit.com/r/kubernetes/comments/1ip0j60/job_roles_related_to_kubernetesopenshift/</link><author>/u/UCONN_throwaway_99</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Fri, 14 Feb 2025 02:39:44 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I was given the opportunity to do a POC for my team to migrate our app onto containers, and we support OpenShift. I really enjoyed the migration part of it and learning about OpenShift/containerization. Would anyone know what kind of job role I should be searching for related to this work?]]></content:encoded></item><item><title>The Cloud Controller Manager Chicken and Egg Problem</title><link>https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/</link><author></author><category>dev</category><category>official</category><category>k8s</category><category>devops</category><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Dev - Kubernetes Blog</source><content:encoded><![CDATA[Kubernetes 1.31
completed the largest migration in Kubernetes history, removing the in-tree
cloud provider. While the component migration is now done, this leaves some additional
complexity for users and installer projects (for example, kOps or Cluster API) . We will go
over those additional steps and failure points and make recommendations for cluster owners.
This migration was complex and some logic had to be extracted from the core components,
building four new subsystems.One of the most critical functionalities of the cloud controller manager is the node controller,
which is responsible for the initialization of the nodes.As you can see in the following diagram, when the  starts, it registers the 
object with the apiserver, Tainting the node so it can be processed first by the
cloud-controller-manager. The initial  is missing the cloud-provider specific information,
like the Node Addresses and the Labels with the cloud provider specific information like the
Node, Region and Instance type information.sequenceDiagram
autonumber
rect rgb(191, 223, 255)
Kubelet->>+Kube-apiserver: Create Node
Note over Kubelet: Taint: node.cloudprovider.kubernetes.io
Kube-apiserver->>-Kubelet: Node Created
end
Note over Kube-apiserver: Node is Not Ready Tainted, Missing Node Addresses*, ...
Note over Kube-apiserver: Send Updates
rect rgb(200, 150, 255)
Kube-apiserver->>+Cloud-controller-manager: Watch: New Node Created
Note over Cloud-controller-manager: Initialize Node:Cloud Provider Labels, Node Addresses, ...
Cloud-controller-manager->>-Kube-apiserver: Update Node
end
Note over Kube-apiserver: Node is Ready
This new initialization process adds some latency to the node readiness. Previously, the kubelet
was able to initialize the node at the same time it created the node. Since the logic has moved
to the cloud-controller-manager, this can cause a chicken and egg problem
during the cluster bootstrapping for those Kubernetes architectures that do not deploy the
controller manager as the other components of the control plane, commonly as static pods,
standalone binaries or daemonsets/deployments with tolerations to the taints and using
 (more on this below)Examples of the dependency problemAs noted above, it is possible during bootstrapping for the cloud-controller-manager to be
unschedulable and as such the cluster will not initialize properly. The following are a few
concrete examples of how this problem can be expressed and the root causes for why they might
occur.These examples assume you are running your cloud-controller-manager using a Kubernetes resource
(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods
rely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it
will schedule properly.Example: Cloud controller manager not scheduling due to uninitialized taintAs noted in the Kubernetes documentation, when the kubelet is started with the command line
flag --cloud-provider=external, its corresponding  object will have a no schedule taint
named node.cloudprovider.kubernetes.io/uninitialized added. Because the cloud-controller-manager
is responsible for removing the no schedule taint, this can create a situation where a
cloud-controller-manager that is being managed by a Kubernetes resource, such as a 
or , may not be able to schedule.If the cloud-controller-manager is not able to be scheduled during the initialization of the
control plane, then the resulting  objects will all have the
node.cloudprovider.kubernetes.io/uninitialized no schedule taint. It also means that this taint
will not be removed as the cloud-controller-manager is responsible for its removal. If the no
schedule taint is not removed, then critical workloads, such as the container network interface
controllers, will not be able to schedule, and the cluster will be left in an unhealthy state.Example: Cloud controller manager not scheduling due to not-ready taintThe next example would be possible in situations where the container network interface (CNI) is
waiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not
tolerated the taint which would be removed by the CNI."The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly."One of the conditions that can lead to a  resource having this taint is when the container
network has not yet been initialized on that node. As the cloud-controller-manager is responsible
for adding the IP addresses to a  resource, and the IP addresses are needed by the container
network controllers to properly configure the container network, it is possible in some
circumstances for a node to become stuck as not ready and uninitialized permanently.This situation occurs for a similar reason as the first example, although in this case, the
node.kubernetes.io/not-ready taint is used with the no execute effect and thus will cause the
cloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is
not able to execute, then it will not initialize the node. It will cascade into the container
network controllers not being able to run properly, and the node will end up carrying both the
node.cloudprovider.kubernetes.io/uninitialized and node.kubernetes.io/not-ready taints,
leaving the cluster in an unhealthy state.There is no one “correct way” to run a cloud-controller-manager. The details will depend on the
specific needs of the cluster administrators and users. When planning your clusters and the
lifecycle of the cloud-controller-managers please consider the following guidance:For cloud-controller-managers running in the same cluster, they are managing.Use host network mode, rather than the pod network: in most cases, a cloud controller manager
will need to communicate with an API service endpoint associated with the infrastructure.
Setting “hostNetwork” to true will ensure that the cloud controller is using the host
networking instead of the container network and, as such, will have the same network access as
the host operating system. It will also remove the dependency on the networking plugin. This
will ensure that the cloud controller has access to the infrastructure endpoint (always check
your networking configuration against your infrastructure provider’s instructions).Use a scalable resource type.  and  are useful for controlling the
lifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy
as well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using
these primitives to control the lifecycle of your cloud controllers and running multiple
replicas, you must remember to enable leader election, or else your controllers will collide
with each other which could lead to nodes not being initialized in the cluster.Target the controller manager containers to the control plane. There might exist other
controllers which need to run outside the control plane (for example, Azure’s node manager
controller). Still, the controller managers themselves should be deployed to the control plane.
Use a node selector or affinity stanza to direct the scheduling of cloud controllers to the
control plane to ensure that they are running in a protected space. Cloud controllers are vital
to adding and removing nodes to a cluster as they form a link between Kubernetes and the
physical infrastructure. Running them on the control plane will help to ensure that they run
with a similar priority as other core cluster controllers and that they have some separation
from non-privileged user workloads.
It is worth noting that an anti-affinity stanza to prevent cloud controllers from running
on the same host is also very useful to ensure that a single node failure will not degrade
the cloud controller performance.Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud
controller container to ensure that it will schedule to the correct nodes and that it can run
in situations where a node is initializing. This means that cloud controllers should tolerate
the node.cloudprovider.kubernetes.io/uninitialized taint, and it should also tolerate any
taints associated with the control plane (for example, node-role.kubernetes.io/control-plane
or node-role.kubernetes.io/master). It can also be useful to tolerate the
node.kubernetes.io/not-ready taint to ensure that the cloud controller can run even when the
node is not yet available for health monitoring.For cloud-controller-managers that will not be running on the cluster they manage (for example,
in a hosted control plane on a separate cluster), then the rules are much more constrained by the
dependencies of the environment of the cluster running the cloud-controller-manager. The advice
for running on a self-managed cluster may not be appropriate as the types of conflicts and network
constraints will be different. Please consult the architecture and requirements of your topology
for these scenarios.This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is
important to note that this is for demonstration purposes only, for production uses please
consult your cloud provider’s documentation.apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
name: cloud-controller-manager
namespace: kube-system
spec:
replicas: 2
selector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
strategy:
type: Recreate
template:
metadata:
labels:
app.kubernetes.io/name: cloud-controller-manager
annotations:
kubernetes.io/description: Cloud controller manager for my infrastructure
spec:
containers: # the container details will depend on your specific cloud controller manager
- name: cloud-controller-manager
command:
- /bin/my-infrastructure-cloud-controller-manager
- --leader-elect=true
- -v=1
image: registry/my-infrastructure-cloud-controller-manager@latest
resources:
requests:
cpu: 200m
memory: 50Mi
hostNetwork: true # these Pods are part of the control plane
nodeSelector:
node-role.kubernetes.io/control-plane: ""
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- topologyKey: "kubernetes.io/hostname"
labelSelector:
matchLabels:
app.kubernetes.io/name: cloud-controller-manager
tolerations:
- effect: NoSchedule
key: node-role.kubernetes.io/master
operator: Exists
- effect: NoExecute
key: node.kubernetes.io/unreachable
operator: Exists
tolerationSeconds: 120
- effect: NoExecute
key: node.kubernetes.io/not-ready
operator: Exists
tolerationSeconds: 120
- effect: NoSchedule
key: node.cloudprovider.kubernetes.io/uninitialized
operator: Exists
- effect: NoSchedule
key: node.kubernetes.io/not-ready
operator: Exists
When deciding how to deploy your cloud controller manager it is worth noting that
cluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple
replicas of a cloud controller manager is good practice for ensuring high-availability and
redundancy, but does not contribute to better performance. In general, only a single instance
of a cloud controller manager will be reconciling a cluster at any given time.]]></content:encoded></item><item><title>Kubernertes Cluster - DigitalOcean</title><link>https://www.reddit.com/r/kubernetes/comments/1iouyuu/kubernertes_cluster_digitalocean/</link><author>/u/Kooky_Group_5215</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 22:07:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I have a cluster on digitalocean... i was trying to deploy a image (java api) but i am getting this error:exec /opt/java/openjdk/bin/java: exec format errorI generated de image with dockerfile that was generated with docker initI generated the image with the arch amd64 ( I use a macbook m2)I tested the image on docker localhost and openshift developer sandbox and worksThe user for the container is non privileged, the base image is eclipse-temurin:17-jdk-jammy   submitted by    /u/Kooky_Group_5215 ]]></content:encoded></item><item><title>how many of you have on-prem k8s running with firewalld</title><link>https://www.reddit.com/r/kubernetes/comments/1iouhkf/how_many_of_you_have_onprem_k8s_running_with/</link><author>/u/SnooOwls6002</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 21:45:57 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[As the title said, how many of you have done it on production env? I am runing rhel9 OS, I found it difficult to setup with the firewalld running and I feel exhausted to let it find out all the networking issue I encountered every time I deploy/troubleshoot stuff and I hope the experts here could give me some suggestions.Currently, I am running 3x control plane, 3x worker nodes in the same subnet, with kube-vip setup for the VIP in control plane and IP range for svc loadblanacing.For the network CNI, I run cilium for pretty basic setup wit disabling ipv6 on hubble-ui so I can have a visibility on different namespace.Also, I use traefik as the ingress controller for my svc in the backend.So what I notice is in order to make it worked, sometimes I need to stop and start the firewalld again, and for me running the cilium connectivity test, it cannot pass through everything. Usually it stuck in pod creation and the problem are mainly due toERR Provider error, retrying in 420.0281ms error="could not retrieve server version: Get \"https://192.168.0.1:443/version\": dial tcp 192.168.0.1:443: i/o timeout" providerName=kubernetesThe issue above happens for some apps as well such as traefik and metric servers...The way I use in kubeadm command:kubeadm init \ --control-plane-endpoint my-entrypoint.mydomain.com \ --apiserver-cert-extra-sans 10.90.30.40 \ --upload-certs \ --pod-network-cidr 172.16.0.0/16 \ --service-cidr 192.168.0.0/20 Currently my kube-vip is doing and I could achieve the HA on the control plane. But I am not sure why those svc cannot communicate to the kubernetes service wit the svc cluster IP.I already opened several firewalld ports on both worker and control plane nodes.Here are my firewalld config:#control plane node: firewall-cmd --permanent --add-port={53,80,443,6443,2379,2380,10250,10251,10252,10255}/tcp firewall-cmd --permanent --add-port=53/udp #Required Cilium ports firewall-cmd --permanent --add-port={53,443,4240,4244,4245,9962,9963,9964,9081}/tcp firewall-cmd --permanent --add-port=53/udp firewall-cmd --permanent --add-port={8285,8472}/udp #Since my pod network and svc network are 172.16.0.0/16 and 192.168.0.0/20 firewall-cmd --permanent --zone=trusted --add-source=172.16.0.0/16 firewall-cmd --permanent --zone=trusted --add-source=192.168.0.0/20 firewall-cmd --add-masquerade --permanent firewall-cmd --reload ## For worker node firewall-cmd --permanent --add-port={53,80,443,10250,10256,2375,2376,30000-32767}/tcp firewall-cmd --permanent --add-port={53,443,4240,4244,4245,9962,9963,9964,9081}/tcp firewall-cmd --permanent --add-port=53/udp firewall-cmd --permanent --add-port={8285,8472}/udp firewall-cmd --permanent --zone=trusted --add-source=172.16.0.0/16 firewall-cmd --permanent --zone=trusted --add-source=192.168.0.0/20 firewall-cmd --add-masquerade --permanent firewall-cmd --reload AFAIK, if I turn of my firewalld, all of the services are running properly. I am confused why those service cannot reach out to the kubernetes API service 192.168.0.1:443 at all.Once the firewalld is up and running again, the metric is failed again as it gave outUnable to connect to the server: dial tcp my_control_plane_1-host_ip:6443: connect: no route to host Could anyone give me some ideas and suggestions? Thank you very much!]]></content:encoded></item><item><title>Need Help with HA PostgreSQL Deployment on AWS EKS</title><link>https://www.reddit.com/r/kubernetes/comments/1ionm42/need_help_with_ha_postgresql_deployment_on_aws_eks/</link><author>/u/DeathVader_21</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 16:55:18 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I’m working on deploying a HA PostgreSQL database on AWS EKS and could use some guidance. My setup involves using Terraform for Infrastructure as Code and leveraging the Crunchy PGO operator for managing PostgreSQL in Kubernetes. I am not able to find proper tutorials on that.   submitted by    /u/DeathVader_21 ]]></content:encoded></item><item><title>Kubernetes Podcast episode 247: KHI, with Kakeru Ishii</title><link>https://www.reddit.com/r/kubernetes/comments/1iolfqg/kubernetes_podcast_episode_247_khi_with_kakeru/</link><author>/u/kubernetespodcast</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 15:22:38 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/kubernetespodcast ]]></content:encoded></item><item><title>Kubernetes History Inspector, with Kakeru Ishii</title><link>http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii</link><author>gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)</author><category>dev</category><category>k8s</category><category>podcast</category><category>devops</category><enclosure url="https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674" length="" type=""/><pubDate>Thu, 13 Feb 2025 11:23:00 +0000</pubDate><source url="https://kubernetespodcast.com/">Dev - Kubernetes Podcast</source><content:encoded><![CDATA[Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.Do you have something cool to share? Some questions? Let us know: News of the week ]]></content:encoded></item><item><title>llmaz: Easy, advanced inference platform for large language models on Kubernetes.</title><link>https://www.reddit.com/r/kubernetes/comments/1ioewx7/llmaz_easy_advanced_inference_platform_for_large/</link><author>/u/Electronic_Role_5981</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 08:53:14 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[This is a new project which may help you build your inference platform on Kubernetes.A rough, inaccurate explanation：It is a lightweight (KServe + Knative + Istio).]]></content:encoded></item><item><title>Sandbox environments: Creating efficient and isolated testing realms</title><link>https://www.youtube.com/watch?v=fh7-lQVmX-o</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><category>devops</category><enclosure url="https://www.youtube.com/v/fh7-lQVmX-o?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 06:00:33 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>KitOps: AI Model Packaging Standards</title><link>https://www.youtube.com/watch?v=1TD-e_wVe4Q</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><category>devops</category><enclosure url="https://www.youtube.com/v/1TD-e_wVe4Q?version=3" length="" type=""/><pubDate>Thu, 13 Feb 2025 06:00:00 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Chat with us on Discord:  https://discord.gg/Tapeh8agYy

Check out our repos:
KitOps      https://github.com/jozu-ai/kitops
PyKitOps Python Library  https://github.com/jozu-ai/pykitops
KitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin]]></content:encoded></item><item><title>Deepseek on bare metal Kubernetes with Talos Linux</title><link>https://youtu.be/HiDWGs1PYhc</link><author>/u/xrothgarx</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 05:59:42 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>KubeVirt Live Migration Mastery: Network Transparency with Kube-OVN</title><link>https://www.kube-ovn.io/news/kubevirt-live-migration-mastery-network-transparency-with-kube-ovn</link><author>/u/oilbeater</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 05:37:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[In virtual machine usage scenarios, live migration allows a virtual machine to be moved from one node to another for operations such as node maintenance, upgrades, and failover.However KubeVirt faces the following challenges during live migration:KubeVirt does not support live migration of virtual machines using bridge network mode by default.KubeVirt only handles memory and disk migration without specific optimizations for network migration.If the virtual machine's IP changes during migration, it cannot achieve a seamless live migration.If the network is interrupted during migration, it cannot achieve a seamless live migration.Kube-OVN specifically addresses the above issues during the virtual machine migration process, allowing users to perform network-transparent live migrations. Our tests show that network interruption time can be controlled within 0.5 seconds, and TCP connections remain uninterrupted.Users only need to add the annotationkubevirt.io/allow-pod-bridge-network-live-migration: "true"in the VM Spec. Kube-OVN will automatically handle network migration during the process.apiVersion: kubevirt.io/v1        kubevirt.io/domain: testvm        kubevirt.io/allow-pod-bridge-network-live-migration: "true"            image: quay.io/kubevirt/cirros-container-disk-demoSSH into the Virtual Machine and Test Network Connectivityvirtctl ssh cirros@testvm3. Perform Migration in Another Terminal and Observe Virtual Machine Network ConnectivityLive Migration Mechanism¶To ensure network consistency between the source and target virtual machines during migration, the same IP address exists on the network for both the source and target VMs. This requires handling network conflicts and traffic confusion. The specific steps are as follows: Here’s the translation:KubeVirt initiates the migration and creates the corresponding Pod on the target machine.KubeVirt synchronizes the VM memory.KubeVirt completes the memory synchronization and deactivates the source Pod. At this point, the source Pod will not handle network traffic.KubeVirt activates the target Pod. At this point, libvirt sends a RARP to activate the network port of the target Pod, and the target Pod starts processing traffic.KubeVirt deletes the source Pod, completing the live migration. Kube-OVN listens for the migration completion event through the Watch Migration CR and stops traffic replication after the migration is finished.In this process, the network interruption mainly occurs between steps 5 and 6. The network interruption time primarily depends on the time it takes for libvirt to send the RARP. Tests show that the network interruption time can be controlled within 0.5 seconds, and TCP connections will not experience interruptions due to the retry mechanism.]]></content:encoded></item><item><title>The unending fuss of Docs search during CK(A/AD/S) exam🙄</title><link>https://www.reddit.com/r/kubernetes/comments/1io8ukj/the_unending_fuss_of_docs_search_during_ckaads/</link><author>/u/suman087</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 02:30:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>For those running hybrid infra, what’s missing in your private cloud experience?</title><link>https://www.reddit.com/r/kubernetes/comments/1io85hg/for_those_running_hybrid_infra_whats_missing_in/</link><author>/u/Advanced-Condition61</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Thu, 13 Feb 2025 01:55:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I would love to hear about your challenges and creative solutions you use to run your private datacenter and how you manage your cloud costs.What’s your biggest challenge managing on-prem?2. Lack of Visibility & Monitoring3. High VMware Licensing Costs4. Compliance & Security5. Troubleshooting Issues6. Stability7. Usability/UX]]></content:encoded></item><item><title>Portainer-agent external IP pending - bare metal</title><link>https://www.reddit.com/r/kubernetes/comments/1io5igj/portaineragent_external_ip_pending_bare_metal/</link><author>/u/Alternative_Leg_3111</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 23:48:23 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Does anybody have advice on how to get this to work? I'm currently using talos os to create a k8s cluster, but I can't get the portainer agent to get an external IP. From what I can tell, load balancers don't work on bare metal. I've tried using metallb, but this doesn't seem to be working. I have multiple worker nodes, so I don't think I can use a node port? Any advice is appreciated!]]></content:encoded></item><item><title>Understanding Kubernetes Architecture Diagram</title><link>https://www.reddit.com/r/kubernetes/comments/1io2ens/understanding_kubernetes_architecture_diagram/</link><author>/u/clickittech</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 21:33:10 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hey fellow K8s enthusiasts! I want to share a blog on Kubernetes Architecture Diagrams, which breaks down the core components, structure, and real-world examples to help you understand how everything fits together.]]></content:encoded></item><item><title>Private Beta - CloudLinker</title><link>https://www.reddit.com/r/kubernetes/comments/1inuwdh/private_beta_cloudlinker/</link><author>/u/cataklix</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 16:28:26 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I’ve been building a SaaS tool that will allow any companies to easily deploy secure multi cloud Kubernetes based infrastructure with all the tools you might want to have. It’s configurable via JSON and a CLI. It have been designed with privacy by design and it’s a completely zero trust deployment process so you account and data is yours and will stay like that!Im looking for a few users to try the service and give feedback. Comment or DM if you’d like access :) PS: a bit of background on me Been a software engineer for quite a few time now and I’ve fallen in love with kube 5 years ago. I’ve seen companies from all sizes struggling with the management and setup of secure Kubernetes cloud infrastructures and I was the one maintaining it, so I know the pain of being called at 4am cause it’s broken… It’s completely free for now until I reach a stable and usable MVP]]></content:encoded></item><item><title>SecurityContext Not Listed in Describe</title><link>https://www.reddit.com/r/kubernetes/comments/1inui8i/securitycontext_not_listed_in_describe/</link><author>/u/TopNo6605</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 16:12:21 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Curious why when you deploy a pod with securityContext enabled it is not output to the describe method? How do you determine if a pod does have securityContext enabled otherwise?   submitted by    /u/TopNo6605 ]]></content:encoded></item><item><title>London Observability Engineering Meetup | February Edition</title><link>https://www.reddit.com/r/kubernetes/comments/1inshh2/london_observability_engineering_meetup_february/</link><author>/u/Fluffybaxter</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 14:47:50 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[We're back with our first event of 2025 on First up, we have Timothy Mahoney, Senior Systems Engineer in the Observability Enablement team at Ingka Group Digital (IKEA). Timothy is passionate about making complex systems observable and has been working with OpenTelemetry to help IKEA solve large-scale observability challenges. He co-developed a composable Splunk environment in Google Cloud used across IKEA and will be sharing insights from IKEA’s Observability Journey, giving us a look at how one of the world’s largest retailers approaches observability across its global infrastructure.Next, we’ll hear from Jean Burellier, Principal Software Engineer at Sanofi, who will explore Reusable Observability with Terraform. Observability and monitoring are critical for system awareness. Yet, they are not part of the standard set of features expected in a deployment pipeline. With the rise of infrastructure as code, engineers can operate their code and cloud resources in the same place. The same should be true for monitoring. Let's see how we can build an Observability as Code mindset.If you're in town, make sure you drop by :D   submitted by    /u/Fluffybaxter ]]></content:encoded></item><item><title>Skaffold v2.14.1: Faster Helm Deploys &amp; Kaniko Builds – Share Your Results!</title><link>https://www.reddit.com/r/kubernetes/comments/1inroqx/skaffold_v2141_faster_helm_deploys_kaniko_builds/</link><author>/u/idsulik</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 14:11:47 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Skaffold  includes major performance improvements for Helm deployments, and Kaniko builds. These optimizations were first introduced in , but due to a bug in that release, please test with .I contributed multiple improvements, but these two are the most impactful:1️⃣  (#9451)2️⃣ Kaniko Build Context Optimization (#9476)If you're using Skaffold with , upgrade to  and let me know how much time you save! 🚀]]></content:encoded></item><item><title>New to ArgoCD/GitOps</title><link>https://www.reddit.com/r/kubernetes/comments/1inrod3/new_to_argocdgitops/</link><author>/u/Zealousideal_Gap9047</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 14:11:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi everyone, I am new to argo and have started using it in my home lab cluster. I used Flux about a month ago with Kustomize and followed the monorepo structure. For Argo, I am planning to use the Apps of Apps pattern. I think I might have some misconceptions and would like to hear your thoughts.Would an  (Helm) in Argo be equivalent to how Flux manages Helm through the  structure?I was using Kustomize with a base repo for foundational manifests and later had a staging repo. The structure was like this: ├── staging (has kustomization.yaml as well as other environment-specific files)My question is: When using the Apps of Apps pattern, would I need a separate repository at the root of the directory (e.g., ) that contains other  files pointing to the previous repos? Would I need one per environment (eg. staging, prod)? Also, would it still be able to use the  files natively?Should I still follow the monorepo structure or is there a better repo structure for argo/GitOps?]]></content:encoded></item><item><title>K8s The Hard Way: production ready</title><link>https://www.reddit.com/r/kubernetes/comments/1inofar/k8s_the_hard_way_production_ready/</link><author>/u/doppeldenken</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 11:04:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Now you want to make it production ready.How would you go about it?Are there guides/tutorials/etc on this matter?]]></content:encoded></item><item><title>Weekly: Share your EXPLOSIONS thread</title><link>https://www.reddit.com/r/kubernetes/comments/1inocr8/weekly_share_your_explosions_thread/</link><author>/u/gctaylor</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 11:00:30 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Did anything explode this week (or recently)? Share the details for our mutual betterment.   submitted by    /u/gctaylor ]]></content:encoded></item><item><title>Pass COntainer args to EFS CSI Driver via CouldFormation</title><link>https://www.reddit.com/r/kubernetes/comments/1inmrhj/pass_container_args_to_efs_csi_driver_via/</link><author>/u/Swimming-Unit3655</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 08:55:20 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Is there a way to pass container arguments to efs csi driver via CF :EfsCsiDriverAddon: Type: 'AWS::EKS::Addon' Properties: AddonName: 'aws-efs-csi-driver' ClusterName: !Ref EksCluster    submitted by    /u/Swimming-Unit3655 ]]></content:encoded></item><item><title>Cross Namespace OwnerRef for CRD</title><link>https://www.reddit.com/r/kubernetes/comments/1inmj0e/cross_namespace_ownerref_for_crd/</link><author>/u/guettli</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 08:36:54 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I create a CRD called Workspace in the namespace "mgt-system".For each Workspace object my controller creates a namespace and some objects in that namespace.I would like to set some kind of owner reference on the created resources.I don't want the garbage collector to clean up things. For me it is about the documentation, so that users looking at the child resources understand how that objects got created.Are there best practices of that?]]></content:encoded></item><item><title>stuck with cert-manager on a microk8s cluster</title><link>https://www.reddit.com/r/kubernetes/comments/1injtfu/stuck_with_certmanager_on_a_microk8s_cluster/</link><author>/u/mistyrouge</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 05:31:01 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi friends. I'm trying my hand at running microk8s on my home server (why not?) and getting stuck with cert-manager.I've `microk8s enable cert-manager` and I already have the following resources in place but my ingress still isn't getting a certificate. I'm not sure what I am missing here.Here are some logs I believe may be relevant$ k -n cert-manager logs deployment/cert-manager I0212 05:15:41.711390 1 requestmanager_controller.go:323] "CertificateRequest does not match requirements on certificate.spec, deleting CertificateRequest" logger="cert-manager.certificates-request-manager" key="default/letsencrypt-account-key" related_resource_name="letsencrypt-account-key-1" related_resource_namespace="default" related_resource_kind="CertificateRequest" related_resource_version="v1" violations=["spec.dnsNames"] I0212 05:15:42.251439 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest "letsencrypt-account-key-1" condition "Approved" to 2025-02-12 05:15:42.251426097 +0000 UTC m=+447.210937401 I0212 05:15:43.059961 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest "letsencrypt-account-key-1" condition "Ready" to 2025-02-12 05:15:43.059950508 +0000 UTC m=+448.019461816 I0212 05:15:43.061011 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest "letsencrypt-account-key-1" condition "Ready" to 2025-02-12 05:15:43.060999543 +0000 UTC m=+448.020510863 I0212 05:15:43.061436 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest "letsencrypt-account-key-1" condition "Ready" to 2025-02-12 05:15:43.061427089 +0000 UTC m=+448.020938410 I0212 05:15:43.061011 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest "letsencrypt-account-key-1" condition "Ready" to 2025-02-12 05:15:43.060998097 +0000 UTC m=+448.020509405 I0212 05:15:43.161135 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest "letsencrypt-account-key-1" condition "Ready" to 2025-02-12 05:15:43.161120767 +0000 UTC m=+448.120632074 I0212 05:15:44.088641 1 controller.go:162] "re-queuing item due to optimistic locking on resource" logger="cert-manager.certificaterequests-issuer-acme" key="default/letsencrypt-account-key-1" error="Operation cannot be fulfilled on certificaterequests.cert-manager.io \"letsencrypt-account-key-1\": the object has been modified; please apply your changes to the latest version and try again" I0212 05:15:44.088827 1 controller.go:162] "re-queuing item due to optimistic locking on resource" logger="cert-manager.certificaterequests-issuer-selfsigned" key="default/letsencrypt-account-key-1" error="Operation cannot be fulfilled on certificaterequests.cert-manager.io \"letsencrypt-account-key-1\": the object has been modified; please apply your changes to the latest version and try again" I0212 05:15:44.089946 1 controller.go:162] "re-queuing item due to optimistic locking on resource" logger="cert-manager.certificaterequests-issuer-ca" key="default/letsencrypt-account-key-1" error="Operation cannot be fulfilled on certificaterequests.cert-manager.io \"letsencrypt-account-key-1\": the object has been modified; please apply your changes to the latest version and try again" I0212 05:15:44.359203 1 controller.go:162] "re-queuing item due to optimistic locking on resource" logger="cert-manager.certificaterequests-issuer-venafi" key="default/letsencrypt-account-key-1" error="Operation cannot be fulfilled on certificaterequests.cert-manager.io \"letsencrypt-account-key-1\": the object has been modified; please apply your changes to the latest version and try again" $ k get ingress ingress -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: cert-manager.io/cluster-issuer: letsencrypt creationTimestamp: "2025-02-10T06:23:14Z" generation: 5 name: ingress namespace: default resourceVersion: "571668" uid: 173089d8-f345-47fe-8687-91c45d784423 spec: ingressClassName: nginx rules: - host: medicine.k8s.epa.jaminais.fr http: paths: - backend: service: name: medicine port: number: 80 path: / pathType: Prefix - host: test2.k8s.epa.jaminais.fr http: paths: - backend: service: name: test port: number: 80 path: / pathType: Prefix tls: - hosts: - medicine.k8s.epa.jaminais.fr - test2.k8s.epa.jaminais.fr secretName: letsencrypt-account-key status: loadBalancer: ingress: - ip: 127.0.0.1 Here is the certificate object$ k describe certificate letsencrypt-account-key Name: letsencrypt-account-key Namespace: default Labels: <none> Annotations: <none> API Version: cert-manager.io/v1 Kind: Certificate Metadata: Creation Timestamp: 2025-02-12T05:09:58Z Generation: 2 Owner References: API Version: networking.k8s.io/v1 Block Owner Deletion: true Controller: true Kind: Ingress Name: ingress UID: 173089d8-f345-47fe-8687-91c45d784423 Resource Version: 571672 UID: 011c2278-596c-4396-8d80-6c98e9b8fa78 Spec: Dns Names: medicine.k8s.epa.jaminais.fr test2.k8s.epa.jaminais.fr Issuer Ref: Group: cert-manager.io Kind: ClusterIssuer Name: letsencrypt Secret Name: letsencrypt-account-key Usages: digital signature key encipherment Status: Conditions: Last Transition Time: 2025-02-12T05:09:59Z Message: Issuing certificate as Secret does not contain a certificate Observed Generation: 1 Reason: MissingData Status: True Type: Issuing Last Transition Time: 2025-02-12T05:09:59Z Message: Issuing certificate as Secret does not contain a certificate Observed Generation: 2 Reason: MissingData Status: False Type: Ready Next Private Key Secret Name: letsencrypt-account-key-ln96n Events: <none> My issuer says it is ready$ k describe issuer letsencrypt Name: letsencrypt Namespace: default Labels: <none> Annotations: <none> API Version: cert-manager.io/v1 Kind: Issuer Metadata: Creation Timestamp: 2025-02-12T05:27:15Z Generation: 1 Resource Version: 572741 UID: 9ffd9e5a-a6ac-41f0-a6c3-d86bb3479336 Spec: Acme: Email: <redacted> Private Key Secret Ref: Name: letsencrypt-account-key Server: https://acme-v02.api.letsencrypt.org/directory Solvers: dns01: Cloudflare: API Key Secret Ref: Key: api-token Name: cloudflare Email: <redacted> Status: Acme: Last Private Key Hash: <redacted> Last Registered Email: <redacted> Uri: https://acme-v02.api.letsencrypt.org/acme/acct/2221761545 Conditions: Last Transition Time: 2025-02-12T05:27:19Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: <none> I see the certificate request as approved but not readySo obviously I am doing something wrong or missing something, but what ?]]></content:encoded></item><item><title>2 pods, same image but different env</title><link>https://www.reddit.com/r/kubernetes/comments/1inflt2/2_pods_same_image_but_different_env/</link><author>/u/FeelingStunning8806</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Wed, 12 Feb 2025 01:41:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I need some suggestions for a trading platform that can route orders to exchanges.I have a unique case where two microservices,  and , are deployed in a Kubernetes cluster. Service A needs to communicate with Service B using an internal service name. However, B requires an SDK key (license) as an environment variable to connect to a particular exchange.In my setup, I need to spin up , each with a different license (for different exchanges). At runtime, A should decide which B pod (exchange) to send an order to.The most obvious solution is to create separate services and separate pods for each exchange, but I’d like to explore .Is there a way to use  for B and have it dynamically route requests to the appropriate pod based on the exchange license? Essentially, I’m looking for a condition-based load balancing mechanism.I appreciate any insights or recommendations. Thanks in advance! 😊Edit - Exchanges can increase, 2 is taken as an example. max upto 6-7.]]></content:encoded></item><item><title>Using Terraform to deploy an ML orchestration system in EKS in minutes</title><link>https://www.reddit.com/r/kubernetes/comments/1in4ad3/using_terraform_to_deploy_an_ml_orchestration/</link><author>/u/Old-Cartographer3050</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Tue, 11 Feb 2025 17:38:21 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[If you're looking to get started or migrate to an open source ML orchestration solution that integrates natively with Kubernetes, look no further.Flyte delivers a Python SDK that abstracts away the K8s inner workings but gives users easy access to compute resources (including accelerators), Secrets, and more; enabling reproducibility, versioning, and parallelism for complex ML workflows.We developed a reference implementation for EKS that's fully automated with Terraform/OpenTofu.(Disclaimer: I'm a Flyte maintainer)]]></content:encoded></item><item><title>KubeCon Europe</title><link>https://www.reddit.com/r/kubernetes/comments/1in1hyg/kubecon_europe/</link><author>/u/Major-Bug-6518</author><category>dev</category><category>k8s</category><category>reddit</category><category>devops</category><pubDate>Tue, 11 Feb 2025 15:43:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Any of you guys planning to attend in April? For those who were able to join in the previous events, what was the best parts of it?Any advice for a first timer like me? ]]></content:encoded></item></channel></rss>
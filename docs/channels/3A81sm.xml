<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech</title><link>https://konrad.website/feeds/</link><description></description><item><title>This Week In Techdirt History: October 26th – November 1st</title><link>https://www.techdirt.com/2025/11/01/this-week-in-techdirt-history-october-26th-november-1st/</link><author>Leigh Beadon</author><category>tech</category><pubDate>Sat, 1 Nov 2025 19:00:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Scientists Say &apos;Dueling Dinosaurs&apos; Fossil Confirms a Smaller Tyrannosaur Species, Not a Teenaged T. Rex</title><link>https://science.slashdot.org/story/25/11/01/0740245/scientists-say-dueling-dinosaurs-fossil-confirms-a-smaller-tyrannosaur-species-not-a-teenaged-t-rex?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 18:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from NPR:

It's known as the "Dueling Dinosaurs" fossil: A triceratops and a tyrannosaur, skeletons entangled, locked in apparent combat right up until the moment of their mutual demise... That discovery in 2006 now appears to have overturned decades of dinosaur dogma about Tyrannosaurus rex, the fearsome giant long thought to be the sole top predator stalking the late Cretaceous. In a paper in the journal Nature, paleontologists Lindsay Zanno and James Napoli conclude that some of the bones from that specimen belong not to a teenage T. rex, but to a fully grown individual of a different tyrannosaur species — Nanotyrannus lancensis.... 


One of the first of those red flags in the new specimen was the arm bones. They looked completely different than T. rex's puny appendages... "These are powerful arms with large claws, large hands. They were using them for prey capture." Contrast that with T. rex, "an animal that's a mouth on legs." There were additional clues. The animal had fewer tail vertebrae and more teeth than T. rex. Zanno and Napoli considered other lines of evidence. They created 3D models of numerous purported T. rexes against which they compared their specimen. They looked at the growth stages of the cranial nerves and sinuses of close living relatives of dinosaurs, features that were visible in the fossilized skeleton. 

"But maybe the most important and damning thing that we did was we were able to figure out that our animal is not a juvenile at all," she says. This conclusion was based on slicing through the fossil's limb bones to examine the growth rings. That work demonstrated that this animal was mature and done growing when it died around the age of 20. "That means it's half the size and a tenth of the mass of a full grown Tyrannosaurus rex," says Zanno... In addition, while making models of all those other alleged T. rex skeletons, Zanno says they identified another new species of tyrannosaur, one they're calling Nanotyrannus lethaeus... 

"It tells us that these end-Cretaceous ecosystems right before the asteroid hit were flourishing," says Zanno. "They had an abundance of different predators. And refutes this idea that dinosaurs were in decline before the asteroid struck."

]]></content:encoded></item><item><title>Ubuntu Will Use Rust For Dozens of Core Linux Utilities</title><link>https://news.slashdot.org/story/25/11/01/079206/ubuntu-will-use-rust-for-dozens-of-core-linux-utilities?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 17:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[ Ubuntu "is adopting the memory-safe Rust language," reports ZDNet, citing remarks at this year's Ubuntu Summit from Jon Seager, Canonical's VP of engineering for Ubuntu:

. Seager said the engineering team is focused on replacing key system components with Rust-based alternatives to enhance safety and resilience, starting with Ubuntu 25.10. He stressed that resilience and memory safety, not just performance, are the principal drivers: "It's the enhanced resilience and safety that is more easily achieved with Rust ports that are most attractive to me". This move is echoed in Ubuntu's adoption of sudo-rs, the Rust implementation of sudo, with fallback and opt-out mechanisms for users who want to use the old-school sudo command. 


In addition to sudo-rs, Ubuntu 26.04 will use the Rust-based uutils/coreutils for Linux's default core utilities. This setup includes ls, cp, mv, and dozens of other basic Unix command-line tools. This Rust reimplementation aims for functional parity with GNU coreutils, with improved safety and maintainability. 

On the desktop front, Ubuntu 26.04 will also bring seamless TPM-backed full disk encryption. If this approach reminds you of Windows BitLocker or MacOS FileVault, it should. That's the idea. 

In other news, Canonical CEO Mark Shuttleworth said "I'm a believer in the potential of Linux to deliver a desktop that could have wider and universal appeal." (Although he also thinks "the open-source community needs to understand that building desktops for people who aren't engineers is different. We need to understand that the 'simple and just works' is also really important.") 


Shuttleworth answered questions from Slashdot's readers in 2005 and 2012.]]></content:encoded></item><item><title>Coinbase CEO Brian Armstrong trolls the prediction markets</title><link>https://techcrunch.com/2025/11/01/coinbase-ceo-brian-armstrong-trolls-the-prediction-markets/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:59:08 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[While Armstrong may have helped some Kalshi and Polymarket users make a little money, he was also illustrating how easily these markets can be manipulated.]]></content:encoded></item><item><title>Did a Weather Balloon, Not a Mysteryious Space Object, Strike That United Airlines Flight?</title><link>https://tech.slashdot.org/story/25/11/01/0615237/did-a-weather-balloon-not-a-mysteryious-space-object-strike-that-united-airlines-flight?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader joshuark shares this report from SFGate:


The mystery object that struck a plane at 36,000 feet is likely not space debris, as some speculated, but rather a Silicon Valley test project gone wrong... 

WindBorne Systems, a Palo Alto startup that uses atmospheric balloons to collect weather data for AI-based forecast models,has come forward to say that they believe they may be responsible for the object that hit the windshield... "At 6am PT, we sent our preliminary investigation to both NTSB and FAA, and are working with both of them to investigate further," [WindBorne's CEO John Dean posted on social media...]
 WindBorne said the company has launched more than 4,000 balloons and that it coordinates with the Federal Aviation Administration for every launch. 

WindBorne "has conducted more than 4,000 launches," the company said in a statement, noting that they've always coordinated those launched with America's Federal Aviation Administration and filed aviation alerts for every launched balloon. Plus "The system is designed to be safe in the event of a midair collision... Our balloon is 2.4 pounds at launch and gets lighter throughout flight."


We are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.]]></content:encoded></item><item><title>Did a Weather Balloon, Not a Mysterious Space Object, Strike That United Airlines Flight?</title><link>https://tech.slashdot.org/story/25/11/01/0615237/did-a-weather-balloon-not-a-mysterious-space-object-strike-that-united-airlines-flight?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader joshuark shares this report from SFGate:


The mystery object that struck a plane at 36,000 feet is likely not space debris, as some speculated, but rather a Silicon Valley test project gone wrong... 

WindBorne Systems, a Palo Alto startup that uses atmospheric balloons to collect weather data for AI-based forecast models,has come forward to say that they believe they may be responsible for the object that hit the windshield... "At 6am PT, we sent our preliminary investigation to both NTSB and FAA, and are working with both of them to investigate further," [WindBorne's CEO John Dean posted on social media...]
 WindBorne said the company has launched more than 4,000 balloons and that it coordinates with the Federal Aviation Administration for every launch. 

WindBorne "has conducted more than 4,000 launches," the company said in a statement, noting that they've always coordinated those launched with America's Federal Aviation Administration and filed aviation alerts for every launched balloon. Plus "The system is designed to be safe in the event of a midair collision... Our balloon is 2.4 pounds at launch and gets lighter throughout flight."


We are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.]]></content:encoded></item><item><title>Rising energy prices put AI and data centers in the crosshairs</title><link>https://techcrunch.com/2025/11/01/rising-energy-prices-put-ai-and-data-centers-in-the-crosshairs/</link><author>Tim De Chant</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:15:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A majority of consumers say they’re worried about data centers driving up electricity costs. Is the industry prepared for a possible backlash?]]></content:encoded></item><item><title>Elaborate Hoaxes in the Age of AI</title><link>https://hackernoon.com/elaborate-hoaxes-in-the-age-of-ai?source=rss</link><author>Jacob Landry</author><category>tech</category><pubDate>Sat, 1 Nov 2025 16:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This week, I’ve seen a lot of over-dramatization of very simple factual events that seem to be fueled by AI in many ways. Now, these aren’t “caused” by AI; I’m not referring to hoaxes that people have used AI specifically to spread, but things that AI has made worse by the ease with which fake information can be made to look very, very real.Fifteen years ago, this problem existed, but in my opinion, was severely muted. The concept of biased news is not new by any means and has been an issue for as long as the news has existed. There have always been audiences that are more susceptible to believing in these invented stories, scenarios, and scams, and the media has always catered to them, guiding them to the water they wish them to drink. \
My concern, and reason for this brain dump, is that with AI, these evil parties seem to be able to cast a much wider net than they could before. They can twist real news into something it’s not with fake videos made by AI; they can pump the internet full of AI-generated content that says whatever they want and cites other AI-generated sources, and they can mobilize an army of influencers that spread their filth like wildfire in an instant.\
I’ve found that recently, a huge chunk of my time when consuming any form of media is spent asking, “Is this real?” I consistently have to find multiple sources and manually scan them, looking for clues that it was AI-generated, a task that is getting harder and harder by the week. The videos are getting more realistic, the content is written better, and the sources I'm used to relying on are less and less trustworthy.A group of protestors (in this case, trolls) showed up at Chicago’s Bean with claims that there was a man trapped inside. The protestors claimed that they had found evidence that a wealth of life-support systems had been purchased during the making of the Bean statue, and also attempted to make a connection to a potential missing person (a baby, I believe) around the time of its construction. Trolls exist. They always have and always will. That’s not the issue at hand here. The issue is what happened next. This group was clearly trying to be funny, just causing a stir with some radical idea for their own amusement, but the internet used AI to take the country by storm.\
While scrolling, I started to see dozens of videos with screenshots of these purchase records, x-ray footage of a person floating inside the Bean, and “eyewitness” reports from someone who claimed they could hear knocking or scratching coming from inside the structure. There were also videos of the Bean being constructed, where you could clearly see the equipment being placed inside. Most, if not all, of these were generated by AI and are completely fake. I knew this, being a sensible human, but I had to admit that the quality created compelling evidence. With less common sense, I would have been easily duped.The recent discovery of 3I/ATLAS has been a goldmine for AI generators and the conspiracy-loving masses. From what I could find, which wasn’t much because it seems a lot of this information is being controlled to stop the spread of disinformation, all we know is that a comet is passing through our solar system. This comet looks like a comet and acts like a comet, but is slightly faster and is not orbiting our sun. One scientist ventured a challenge to the “it’s just a comet” consensus to encourage more critical thinking, theorizing that it was, of course, possible for it to be an alien craft. This set the AI-loving conspiracy nuts on fire.\
My Instagram feed was on absolute fire with fake videos of this comet with lights being emitted from the sides like a ship, with exhaust clearly venting into space, and fake X-ray shots that showed the internal ship structure and beings inside. Countless videos of influencers pretended to be experts on the matter and talked about potential alien invasions. The worst part of all of this was that each one cited different sources and pulled from different video content. It was easy to assume that a “potential alien invasion” was fake; however, I have to admit, the video content they provided was stunningly believable. The “experts” talking were confident and had plenty of “research” to back them up.\
The most alarming thing about this entire situation is how fast this misinformation is able to spread and how absolutely believable it can make it. I know this isn’t a controlled situation, and we’ve always had irresponsible people running social media accounts to susceptible individuals, but the use of AI in these fields is making the problem more abundant and harder to discern.It’s not all bad… but it’s pretty bad.AI is a wonderful tool that has the potential to make our lives easier. I don’t believe that it is ready for constant use yet, despite it being shoved down our throats around every turn. It consistently hallucinates and makes false claims; it slows my work down more than it speeds it up, and has become a barrier to productivity in most situations I’ve tried to use it. \
However, I can admit it has potential, and there are small automation tasks that I do find small uses for it. That being said, there’s always going to be a heated conversation around ethics and how we should be using AI. \
I don’t believe there will be any disagreement, however, that the above cases are the wrong way to use AI. The use of AI to generate content to fuel conspiracy theories and spread them to the masses as facts is dangerous and, frankly, terrifying. I believe we’re only seeing the tip of the iceberg, and I worry that we’re headed for a future where we’ll no longer be able to discern the difference between truth and fiction.]]></content:encoded></item><item><title>Security Holes Found in OpenAI&apos;s ChatGPT Atlas Browser (and Perplexity&apos;s Comet)</title><link>https://it.slashdot.org/story/25/11/01/054213/security-holes-found-in-openais-chatgpt-atlas-browser-and-perplexitys-comet?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The address bar/ChatGPT input window in OpenAI's browser ChatGPT Atlas "could be targeted for prompt injection using malicious instructions disguised as links," reports SC World, citing a report from AI/agent security platform NeuralTrust:

NeuralTrust found that a malformed URL could be crafted to include a prompt that is treated as plain text by the browser, passing the prompt on to the LLM. A malformation, such as an extra space after the first slash following "https:" prevents the browser from recognizing the link as a website to visit. Rather than triggering a web search, as is common when plain text is submitted to a browser's address bar, ChatGPT Atlas treats plain text as ChatGPT prompts by default. 

An unsuspecting user could potentially be tricked into copying and pasting a malformed link, believing they will be sent to a legitimate webpage. An attacker could plant the link behind a "copy link" button so that the user might not notice the suspicious text at the end of the link until after it is pasted and submitted. These prompt injections could potentially be used to instruct ChatGPT to open a new tab to a malicious website such as a phishing site, or to tell ChatGPT to take harmful actions in the user's integrated applications or logged-in sites like Google Drive, NeuralTrust said. 

Last month browser security platform LayerX also described how malicious prompts could be hidden in URLs (as a parameter) for Perplexity's browser Comet. And last week SquareX Labs demonstrated that a malicious browser extension could spoof Comet's AI sidebar feature and have since replicated the proof-of-concept (PoC) attack on Atlas. 



But another new vulnerability in ChatGPT Atlas "could allow malicious actors to inject nefarious instructions into the artificial intelligence (AI)-powered assistant's memory and run arbitrary code," reports The Hacker News, citing a report from browser security platform LayerX:



"This exploit can allow attackers to infect systems with malicious code, grant themselves access privileges, or deploy malware," LayerX Security Co-Founder and CEO, Or Eshed, said in a report shared with The Hacker News. The attack, at its core, leverages a cross-site request forgery (CSRF) flaw that could be exploited to inject malicious instructions into ChatGPT's persistent memory. The corrupted memory can then persist across devices and sessions, permitting an attacker to conduct various actions, including seizing control of a user's account, browser, or connected systems, when a logged-in user attempts to use ChatGPT for legitimate purposes.... 

"What makes this exploit uniquely dangerous is that it targets the AI's persistent memory, not just the browser session," Michelle Levy, head of security research at LayerX Security, said. "By chaining a standard CSRF to a memory write, an attacker can invisibly plant instructions that survive across devices, sessions, and even different browsers. In our tests, once ChatGPT's memory was tainted, subsequent 'normal' prompts could trigger code fetches, privilege escalations, or data exfiltration without tripping meaningful safeguards...." 


LayerX said the problem is exacerbated by ChatGPT Atlas' lack of robust anti-phishing controls, the browser security company said, adding it leaves users up to 90% more exposed than traditional browsers like Google Chrome or Microsoft Edge. In tests against over 100 in-the-wild web vulnerabilities and phishing attacks, Edge managed to stop 53% of them, followed by Google Chrome at 47% and Dia at 46%. In contrast, Perplexity's Comet and ChatGPT Atlas stopped only 7% and 5.8% of malicious web pages. 

From The Conversation:

Sandboxing is a security approach designed to keep websites isolated and prevent malicious code from accessing data from other tabs. The modern web depends on this separation. But in Atlas, the AI agent isn't malicious code — it's a trusted user with permission to see and act across all sites. This undermines the core principle of browser isolation. 


Thanks to Slashdot reader spatwei for suggesting the topic.]]></content:encoded></item><item><title>Beyond Brute Force: 4 Secrets to Smaller, Smarter, and Dramatically Cheaper AI</title><link>https://hackernoon.com/beyond-brute-force-4-secrets-to-smaller-smarter-and-dramatically-cheaper-ai?source=rss</link><author>Anthony Laneau</author><category>tech</category><pubDate>Sat, 1 Nov 2025 15:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Large Language Models (LLMs) are incredibly powerful generalists, but transforming them into specialized experts is a major challenge. The process of training a model on new, specific knowledge like internal company documents or a complex reasoning task is notoriously expensive, time-consuming, and fraught with pitfalls. We want smaller, more efficient models that can master a domain without the compute budget of a tech giant.\
The core idea behind making smaller models smarter is a concept called "distillation." In this process, a smaller "student" model learns from a larger, more capable "teacher" model. The student doesn't just learn from a static textbook of examples; it learns to mimic the teacher's thought process. This is a powerful shortcut for transferring expertise.\
Until now, however, engineers have faced a frustrating trade-off. One approach, on-policy reinforcement learning (RL), forces the student to learn from its own mistakes, which is relevant but painfully slow. The alternative, off-policy distillation, is much faster but dangerously flawed; the student learns from the teacher's ideal examples, which often occur in contexts the student will never encounter on its own, causing errors to compound. This has been the bottleneck for creating specialized AI; until now.\
A powerful technique called "on-policy distillation" combines the best of both worlds. By having a teacher model provide dense, token-by-token feedback on the student model's own attempts, we can achieve breakthroughs in training efficiency and capability. Here are the four most surprising and impactful takeaways from this approach.A Smarter Feedback Loop Makes AI Training Up to 100x CheaperThe fundamental difference between Reinforcement Learning (RL) and Distillation lies in the density of the feedback. To understand this, imagine learning to play chess. is like learning chess by only being told if you won or lost at the very end of a match. The feedback is directly related to your actions, but it's sparse. You know you lost, but you don't know if it was because of your opening, a mid-game blunder, or a weak endgame. is like watching a grandmaster play. You observe brilliant moves, but they are made in complex board positions that you, as a novice, will rarely find yourself in. The feedback is dense, but the context is often irrelevant to your own learning path. provides the best of both worlds. It's like having an expert coach who grades every single one of your moves in your own games, telling you if a move was a "blunder," "inaccuracy," or "brilliant." The feedback is both dense and perfectly relevant to your current skill level.\
This smarter feedback loop has a massive impact on efficiency. In a direct back-to-back comparison where a student model learned from a teacher trained via RL, on-policy distillation allowed the student to reach the teacher's performance level 7-10 times faster in terms of gradient steps. This translates to a staggering 50-100x improvement in cumulative compute efficiency.\
The reason for this dramatic speedup is that on-policy distillation provides more useful information (more "bits per episode") for the model to learn from. Because this dense, token-level feedback reduces gradient noise, it allows for training with shorter contexts and smaller, more efficient batch sizes, further slashing the overall computational cost.You Can Cure “AI Amnesia” When Teaching New KnowledgeA common and frustrating problem in AI is "catastrophic forgetting." When you take a pre-trained model and fine-tune it on new, specialized information (like your company's internal knowledge base), it often degrades or completely forgets its original, general-purpose skills, such as the ability to follow instructions.\
Consider an experiment to create an "internal assistant." Researchers started with the Qwen3-8B model, which had a strong instruction-following score of 85%. After fine-tuning it on a 70-30 mix of internal company documents and general chat data:Its knowledge about the documents improved significantly (from 18% to 36% on a QA evaluation).However, its instruction-following skill degraded badly, dropping from 85% down to 79%.\
The solution was a brief phase of on-policy distillation after the initial fine-tuning. By using the original version of the model as the teacher, researchers could restore the lost behavior. The results were powerful:Instruction-following performance was almost fully recovered, jumping back up to 83%.Crucially, this happened without losing the newly acquired knowledge. In fact, the knowledge score even improved slightly to 41%.\
This finding is a game-changer for "continual learning," aka the ability to update models with new information over time without having to perform expensive, full-scale retraining from scratch. It provides a reliable way to teach an AI new facts without it forgetting its core skills.An AI Can Master a Reasoning Skill From Just One ExampleThis finding is highly counterintuitive. In most AI training methods, repeatedly training a model on the exact same prompt is a recipe for failure; the model simply memorizes the answer instead of learning the underlying skill.\
However, an experiment with on-policy distillation turned this assumption on its head. Researchers trained a student model on a math reasoning task using only a single, randomly chosen prompt. They trained on this one prompt for 20 consecutive steps, each with a batch of 256 rollouts, generating 5,120 total learning sequences.\
The remarkable outcome turns conventional wisdom on its head: the student model was able to approximately match the performance of the expert teacher model on the AIME'24 math benchmark, despite only ever having seen that one problem.\
This works because on-policy distillation teaches the model to approximate the teacher's entire thought process; its full probability distribution for what the next best token should be at every step, rather than just memorizing a final answer. This means that for certain skills, the bottleneck isn't finding thousands of examples, but creating a single, perfectly-guided learning experience.Why "Practicing" on Its Own Samples Can Make an AI DumberIt seems logical that if a model produces a high-quality output, you could feed that output back into its training data to reinforce good behavior. This method, known as supervised fine-tuning (SFT) on on-policy data, is like having the model "practice" on its own best work.\
But researchers found the opposite to be true. When they trained a model using a dataset composed of its own samples, its performance on an instruction-following evaluation actually degraded.\
The technical reason for this failure is subtle but critical. While the dataset of the model's own outputs might be perfectly on-policy on average, every finite batch of data exhibits a slightly different distribution. Training on these batches causes the model’s internal policy to drift away from its original state. This process turns training on its own samples into a form of off-policy training over time, leading to the same compounding error and divergence seen in other flawed methods.\
In contrast, on-policy distillation is completely stable in this self-distillation scenario. Because the teacher model remains a fixed, consistent target, the student can robustly converge on the desired behavior without degrading. This further cements on-policy distillation as a superior and more reliable tool for behavior refinement and continual learning.The Future of AI is Smaller, Faster, and More PersonalOn-policy distillation is more than just another training technique; it's a foundational shift in how we create specialized, expert AI. By combining the direct relevance of learning from one's own actions with the incredible efficiency of dense, token-by-token feedback, it solves some of the biggest challenges in applied AI.\
The benefits are clear: massive compute savings, a cure for catastrophic forgetting, and unbelievable data efficiency. This is a key enabling technology that lowers the barrier to entry, unlocking the ability for more teams to build and maintain custom models that possess deep domain knowledge without sacrificing core capabilities. This democratization of expert AI will fuel new business models and create competitive advantages previously reserved for frontier labs.]]></content:encoded></item><item><title>AI researchers ’embodied’ an LLM into a robot – and it started channeling Robin Williams</title><link>https://techcrunch.com/2025/11/01/ai-researchers-embodied-an-llm-into-a-robot-and-it-started-channeling-robin-williams/</link><author>Julie Bort</author><category>tech</category><pubDate>Sat, 1 Nov 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AI researchers at Andon Labs embedded various LLMs in a vacuum robot to test how ready they were to be embodied. And hilarity ensued.]]></content:encoded></item><item><title>Linux Kernel Ported To WebAssembly - Demo Lets You Run It In Your Web Browser</title><link>https://www.phoronix.com/news/Linux-Kernel-WebAssembly</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 14:40:52 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Open-source developer Joel Severin today announced his work on porting the Linux kernel to WebAssembly and has successffully gotten the kernel up and running within WASM-capable web browsers...]]></content:encoded></item><item><title>MIT Physicists Find a Way To See Inside Atoms That May Aid Search For Antimatter</title><link>https://science.slashdot.org/story/25/11/01/0545231/mit-physicists-find-a-way-to-see-inside-atoms-that-may-aid-search-for-antimatter?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 1 Nov 2025 14:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Traditionally, exploring the interior of atomic nuclei requires enormous particle accelerators that stretch for kilometers and propel beams of electrons at extremely high speeds," writes SciTechDaily. 

But MIT physicists have unveiled a groundbreaking alternative that "used the atom's own electrons as probes to momentarily enter the nucleus..."



In research published in Science, a team of MIT physicists achieved exceptionally precise measurements of the energy of electrons orbiting a radium atom that had been chemically bonded with a fluoride atom to form radium monofluoride. By studying these molecules, the researchers created a kind of miniature particle collider. Within this environment, the electrons surrounding the radium atom were confined closely enough to occasionally slip into the nucleus before returning to their usual orbits... When those electrons returned to their outer paths, they retained the altered energy, effectively carrying a "message" from within the nucleus that could be decoded to reveal its internal arrangement... 

[The researchers] trapped and cooled the molecules and sent them through a system of vacuum chambers, into which they also sent lasers, which interacted with the molecules. In this way, the researchers were able to precisely measure the energies of electrons inside each molecule. When the researchers analyzed their measurements, they noticed that the electrons carried slightly different energies than expected if they had remained outside the nucleus. The difference was incredibly small, only about one millionth of the energy of the laser photon used to excite the molecules, but it was clear evidence that the electrons had entered the radium nucleus and interacted with its protons and neutrons... 

The researchers plan to use this new technique to create a detailed map of how forces are distributed inside the nucleus... to chart the nucleus with greater precision and search for possible violations of fundamental symmetries in nature. 

"It is thought that additional sources of fundamental symmetry violation are required to explain the almost complete absence of antimatter in our universe," the article points out. "Such violations could be seen within the nuclei of certain atoms such as radium... 


"Unlike most atomic nuclei, which are spherical in shape, the radium atom's nucleus has a more asymmetrical configuration, similar to a pear. Scientists predict that this pear shape could significantly enhance their ability to sense the violation of fundamental symmetries, to the extent that they may be potentially observable."]]></content:encoded></item><item><title>The Hidden Ledger of Code: Tracking the Carbon Debt Inside Our Software</title><link>https://hackernoon.com/the-hidden-ledger-of-code-tracking-the-carbon-debt-inside-our-software?source=rss</link><author>Jacob Wolinsky</author><category>tech</category><pubDate>Sat, 1 Nov 2025 14:00:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every line of code carries an invisible cost. As software scales, so does the energy it consumes and the emissions it generates.This growing footprint forms what many engineers now call carbon debt: the accumulation of energy waste caused by inefficient architecture, redundant compute, or neglected cleanup.The problem isn’t limited to theory anymore. Global data workloads are rising faster than the efficiency gains meant to offset them, and few teams have the tools to measure what their systems actually emit.Because engineers control how and where code runs, real progress starts inside development workflows, not in boardrooms.As carbon visibility moves closer to the code itself, software projects may soon be judged not only by speed and stability, but by how responsibly they use the power behind them.Teams talk about technical debt every sprint. They track code smells, refactoring needs, module complexity, and build bloat. But almost no one tracks the energy drain built into their systems, and this makes that blind spot real.\
Every inefficiency in code, like extra loops, redundant database fetches, and idle background tasks, translates into power use. Run thousands or millions of times per day, and what feels trivial becomes measurable emissions. Researchers have begun quantifying this: for example, the Green Algorithms framework shows that compute time, memory usage, and data center efficiency can be converted into carbon equivalent estimates for any computational task.\
At the data center scale, inefficiencies amplify. One white-paper found that servers may draw 60% to 90% of their peak power even while idle. Multiply that across dozens of servers, and weeks of wasted cycles become dozens of kilograms of CO2 equivalent.\
Every product team now operates with an invisible balance sheet, one that records carbon alongside complexity.The term carbon debt originates in environmental accounting, where it describes the accumulated emissions a system or entity has “borrowed” against future budgets with insufficient offsets. (It’s rooted in the broader notion of ecological or climate debt.) Now, technologists are borrowing that phrase to describe software systems whose inefficiencies accrue hidden energy costs over time.\
In software, carbon debt grows when layers of redundant code, over-provisioned infrastructure, and heavy frameworks persist unchecked. A module that spawns unnecessary background jobs, or a service that overfetches data, burns CPU cycles, which burn power.\
When infrastructure is sized with generous headroom “just in case,” that slack often stays underutilized, yet still draws baseline power. Servers and services often draw between 27% and 36% of peak power even under light load.\
As your system advances with more users, more services, and more replicas, each inefficiency multiplies. What once was a single wasted cycle becomes thousands per second. That energy waste endures unless addressed, compounding like interest owed on an invisible balance.\
Next, we’ll trace how code builds up emissions so you can see where the debt really comes from.How Code Accrues EmissionsThe energy footprint of software often hides in the smallest details of its logic. A loop that runs one step too long or a recursive function that never terminates efficiently can keep processors active far longer than needed. Each extra millisecond of compute draws power, and the effect multiplies when thousands of users trigger the same function at once.How Tiny Loops Turn Into Big CostsResearch on mobile software shows that energy code smells can dramatically increase consumption, and in some cases, they can consume up to 87x more energy than clean versions. Follow-up work found that fixing these patterns delivered 4% to 30% efficiency gains in practice. These results reinforce the broader point: repetitive, seemingly minor patterns accumulate real power draw over time.\
Similar waste appears in everyday engineering habits: redundant database queries, unnecessary front-end re-renders, and dormant API endpoints all keep processors active, drawing power without improving performance.\
Over-sized build artifacts and idle background tasks deepen the impact by holding memory and storage resources active long after they’re useful. When these patterns run across millions of daily transactions, the emissions scale from grams to kilograms of CO2. Quantifying that footprint is the next challenge, and few teams yet have the tools to do it precisely.Measuring What We Don’t SeeTracking how much energy software really uses is harder than it sounds. The Software Carbon Intensity (SCI) framework from the Green Software Foundation is one of the first real attempts to make that measurable, like mapping compute time, memory use, and data transfer against actual energy data.\
Tools such as Cloud Carbon Footprint and CodeCarbon are now taking that formula a step further, embedding energy estimates directly into build pipelines and dashboards so developers can see environmental impact alongside performance metrics. This aligns with broader conversations inside the DevOps community, where teams are beginning to explore practical ways to embed sustainability into build and deployment workflows.\
The challenge is translating code execution into physical terms. Every watt drawn depends on processor type, cooling efficiency, and the carbon intensity of the grid that powers the data center. The same workload might have a fraction of the emissions on renewable-heavy infrastructure compared to fossil-fueled grids.\
The logic behind these tools isn’t far from how predictive analytics is being used to expose hidden operational costs in other industries, turning guesswork into measurable insight. Until this kind of visibility becomes standard in developer environments, most teams will keep optimizing performance while staying blind to the energy behind it.The Governance Gap: Why Carbon Isn’t Yet a Coding MetricSustainability still sits outside most engineering workflows. In many companies, carbon reporting lives with facilities or operations teams, not with the people writing or deploying code.\
As a result, the energy cost of a release is rarely discussed in sprint planning or post-mortems. Agile ceremonies track velocity, story points, and error rates, but not emissions.Few DevOps environments include “carbon sprints” or carbon budgets, even though they could be tracked the same way as uptime or latency. A report based on responses from over 2,000 software practitioners has found that most organizations are still in the early stages of measuring software-related emissions. Others echoed this, noting that sustainability metrics remain largely absent from continuous-integration and delivery pipelines.\
That gap is beginning to close. Some open-source communities have started experimenting with “green commits” to tag energy-efficient changes, and enterprise dashboards are beginning to surface sustainability data next to performance KPIs. As this visibility improves, design priorities are shifting toward decay and restraint by building systems that know when to slow down, scale back, or shut off entirely.Designing for Decay: Making Efficiency a DefaultArchitects concerned with long-lived systems often speak of architectural erosion or design decay, like the gradual divergence between intended structure and runtime reality. Architecture erosion is a well-known risk in systems as features accumulate and shortcuts proliferate. One way to counter that drift is to build systems that self-optimize or sunset unused processes automatically, pruning inactive modules or trimming underutilized services based on real usage signals.Treating code decay as a feature means embedding routines that perform periodic cleanup: archiving stale APIs, retiring dormant modules, or enforcing dependency hygiene. Frameworks may require that libraries unused for X releases be flagged or removed. Over time, the shift moves from “unlimited scaling” toward sustainable scaling, systems designed to shrink or sleep when load is low rather than running flat out forever.\
Engineers can use runtime profiling, build monitoring, and garbage-collection heat maps as signals. If a microservice’s CPU utilization stays near zero for weeks, it raises a refactor or archive flag. If build artifacts grow without change, they are flagged for pruning.\
This philosophy sets the stage for what’s next: making carbon visibility part of everyday decision-making, and bringing engineering metrics and emissions metrics into the same ecosystem.The Road to Carbon TransparencyImagine an IDE where each file, function, or commit carries a live “emissions counter”; you write a loop, and you see how much energy it might cost. That’s the direction software tooling is heading. Build tools could come to flag carbon-heavy changes before they’re merged.\
CI/CD pipelines will evolve to flag carbon-intensive builds, perhaps even rejecting code that spikes emissions far above baseline. With tighter integration, carbon metrics will merge with performance dashboards, showing build time, throughput, and CO2 cost in one pane.Cloud Dashboards & Deployment TransparencyCloud providers may expose per-deployment carbon cost insights, mapping workload emissions to regions, instance types, and schedules. The same principle underpins the idea of carbon-aware computing, where workloads shift dynamically to regions or times with cleaner grids. Integrating that into the same console where devs monitor CPU, bandwidth, and billing makes sustainability part of everyday trade-offs.\
With visibility in place, engineers will begin to optimize not just for latency or memory, but for carbon as a first-class metric. Those insights will shape budgeting decisions, drive architecture choices (edge, serverless, off-peak scheduling), and enforce sustainable defaults in code.\
Ahead lies a time when your pull request comes with a carbon delta and teams judge changes not only by correctness or performance, but by how much energy they add or save.Engineering AccountabilitySustainability in software doesn’t start in a server farm, but it starts at the keyboard. Every query, commit, and deployment decision shapes the energy profile of the systems we run. For years, efficiency meant speed and uptime, and now it also means restraint.\
Across the industry, teams are beginning to treat carbon debt the same way they treat technical debt: as something that compounds if ignored. Cleaning up unused code, right-sizing infrastructure, or pausing idle jobs are no longer side tasks; they’re acts of maintenance that protect performance and the planet.\
As tooling matures, carbon visibility will become part of normal governance, sitting next to reliability and security in every build report. The responsibility won’t rest with operations alone but with every engineer who touches code. Because in modern software, clean code and clean energy belong to the same conversation, and writing one well means caring about the other.]]></content:encoded></item><item><title>Go: Can It Mitigate Supply Chain Attacks?</title><link>https://hackernoon.com/go-can-it-mitigate-supply-chain-attacks?source=rss</link><author>Go [Technical Documentation]</author><category>tech</category><pubDate>Sat, 1 Nov 2025 14:00:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Modern software engineering is collaborative, and based on reusing Open Source software. That exposes targets to supply chain attacks, where software projects are attacked by compromising their dependencies.\
Despite any process or technical measure, every dependency is unavoidably a trust relationship. However, the Go tooling and design help mitigate risk at various stages.There is no way for changes in the outside world—such as a new version of a dependency being published—to automatically affect a Go build.\
Unlike most other package managers files, Go modules don’t have a separate list of constraints and a lock file pinning specific versions. The version of every dependency contributing to any Go build is fully determined by the  file of the main module.\
Since Go 1.16, this determinism is enforced by default, and build commands (, , , , …) will fail if the go.mod is incomplete. The only commands that will change the  (and therefore the build) are  and . These commands are not expected to be run automatically or in CI, so changes to dependency trees must be made deliberately and have the opportunity to go through code review.\
This is very important for security, because when a CI system or new machine runs , the checked-in source is the ultimate and complete source of truth for what will get built. There is no way for third parties to affect that.\
Moreover, when a dependency is added with , its transitive dependencies are added at the version specified in the dependency’s  file, not at their latest versions, thanks to Minimal version selection. The same happens for invocations of go install example.com/cmd/devtoolx@latest, the equivalents of which in some ecosystems bypass pinning. In Go, the latest version of  will be fetched, but then all the dependencies will be set by its  file.\
If a module gets compromised and a new malicious version is published, no one will be affected until they explicitly update that dependency, providing the opportunity to review the changes and time for the ecosystem to detect the event.Version contents never changeAnother key property necessary to ensure third parties can’t affect builds is that the contents of a module version are immutable. If an attacker that compromises a dependency could re-upload an existing version, they could automatically compromise all projects that depend on it.\
That’s what the  file is for. It contains a list of cryptographic hashes of each dependency that contributes to the build. Again, an incomplete  causes an error, and only  and  will modify it, so any changes to it will accompany a deliberate dependency change. Other builds are guaranteed to have a full set of checksums.\
This is a common feature of most lock files. Go goes beyond it with the Checksum Database (sumdb for short), a global append-only cryptographically-verifiable list of go.sum entries. When  needs to add an entry to the  file, it fetches it from the sumdb along with cryptographic proof of the sumdb integrity. This ensures that not only every build of a certain module uses the same dependency contents, but that every module out there uses the same dependency contents!\
The sumdb makes it impossible for compromised dependencies or even Google-operated Go infrastructure to target specific dependents with modified (e.g. backdoored) source. You’re guaranteed to be using the exact same code that everyone else who’s using e.g. v1.9.2 of  is using and has reviewed.\
Finally, my favorite features of the sumdb: it doesn’t require any key management on the part of module authors, and it works seamlessly with the decentralized nature of Go modules.The VCS is the source of truthMost projects are developed through some version control system (VCS) and then, in other ecosystems, uploaded to the package repository. This means there are two accounts that could be compromised, the VCS host and the package repository, the latter of which is used more rarely and more likely to be overlooked. It also means it’s easier to hide malicious code in the version uploaded to the repository, especially if the source is routinely modified as part of the upload, for example to minimize it.\
In Go, there is no such thing as a package repository account. The import path of a package embeds the information that needs in order to fetch its module directly from the VCS, where tags define versions.\
We do have the Go Module Mirror, but that’s only a proxy. Module authors don’t register an account and don’t upload versions to the proxy. The proxy uses the same logic that the  tool uses (in fact, the proxy runs ) to fetch and cache a version. Since the Checksum Database guarantees that there can be only one source tree for a given module version, everyone using the proxy will see the same result as everyone bypassing it and fetching directly from the VCS. (If the version is not available anymore in the VCS or if its contents changed, fetching directly will lead to an error, while fetching from the proxy might still work, improving availability and protecting the ecosystem from “left-pad” issues.)\
Running VCS tools on the client exposes a pretty large attack surface. That’s another place the Go Module Mirror helps: the  tool on the proxy runs inside a robust sandbox and is configured to support every VCS tool, while the default is to only support the two major VCS systems (git and Mercurial). Anyone using the proxy can still fetch code published using off-by-default VCS systems, but attackers can’t reach that code in most installations.Building code doesn’t execute itIt is an explicit security design goal of the Go toolchain that neither fetching nor building code will let that code execute, even if it is untrusted and malicious. This is different from most other ecosystems, many of which have first-class support for running code at package fetch time. These “post-install” hooks have been used in the past as the most convenient way to turn a compromised dependency into compromised developer machines, and to worm through module authors.\
To be fair, if you’re fetching some code it’s often to execute it shortly afterwards, either as part of tests on a developer machine or as part of a binary in production, so lacking post-install hooks is only going to slow down attackers. (There is no security boundary within a build: any package that contributes to a build can define an  function.) However, it can be a meaningful risk mitigation, since you might be executing a binary or testing a package that only uses a subset of the module’s dependencies. For example, if you build and execute  on macOS there is no way for a Windows-only dependency or a dependency of example.com/cmd/othertool to compromise your machine.\
In Go, modules that don’t contribute code to a specific build have no security impact on it.“A little copying is better than a little dependency”The final and maybe most important software supply chain risk mitigation in the Go ecosystem is the least technical one: Go has a culture of rejecting large dependency trees, and of preferring a bit of copying to adding a new dependency. It goes all the way back to one of the Go proverbs: “a little copying is better than a little dependency”. The label “zero dependencies” is proudly worn by high-quality reusable Go modules. If you find yourself in need of a library, you’re likely to find it will not cause you to take on a dependency on dozens of other modules by other authors and owners.\
That’s enabled also by the rich standard library and additional modules (the  ones), which provide commonly used high-level building blocks such as an HTTP stack, a TLS library, JSON encoding, etc.\
All together this means it’s possible to build rich, complex applications with just a handful of dependencies. No matter how good the tooling is, it can’t eliminate the risk involved in reusing code, so the strongest mitigation will always be a small dependency tree.\
This article is available on  under a CC BY 4.0 DEED license.]]></content:encoded></item><item><title>Archinstall 3.0.12 &amp; Pacman 7.1 Released For Arch Linux Users</title><link>https://www.phoronix.com/news/Archinstall-3.0.12-Pacman-7.1</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 13:05:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Kicking off November for Arch Linux users happen to be the releases of Pacman 7.1 as well as Archinstall 3.0.12...]]></content:encoded></item><item><title>There’s a Dinosaur ‘Mummy Zone.’ Here’s What Scientists Found There.</title><link>https://www.404media.co/theres-a-dinosaur-mummy-zone-heres-what-scientists-found-there/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/10/image3.jpg" length="" type=""/><pubDate>Sat, 1 Nov 2025 13:00:22 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Welcome back to a special scary installment of the Abstract! Is Halloween over? Technically yes. But as lovers of spooky season will know, the fallout from Halloween—when dawn reveals the remains of the festivities—is an extension of the day itself. That is why I have assembled a parade of horrors for you this morning.First, ancient mummies from a watery grave. Then: : let’s get tangled up in one of the great mysteries of spider webs; watch out for the venomous snakes; and lastly, scientists literally ask where the bodies are buried. And now, Halloween is over but the darkness lingers on…Prepare to enter the “mummy zone”If you think human mummies are scary, wait until you meet dinosaur mummies. Paleontologists have discovered the mummified remains of two duck-billed dinosaurs that belong to the species (go Oilers!) , which lived 66 million years ago in what is now Wyoming. The immaculate preservation of the animals—a 2-year-old juvenile and young adult that was roughly 5 to 8 years old—exposed unprecedented corporeal details, such as intricate polygonal scales, spinal spikes, fleshy contours, skin wrinkles, and the first hooves ever identified in a dinosaur (or any reptile), making them the oldest hooves in the fossil record.“The late juvenile is the first subadult dinosaur mummy on record and the first large-bodied dinosaur preserving the fleshy midline over the trunk,” said researchers led by Paul Sereno of the University of Chicago. “The early adult is the first hadrosaurid to preserve the entire spike row from hips to tail tip and the first reptile preserving wedge-shaped pedal hooves.”These “end-Cretaceous  preserve the oldest hoof renderings for any tetrapod, the first record of hooves in a reptile” and “the first instance of a hooved tetrapod capable of bipedal locomotion,” the team added.For more than a century, paleontologists have discovered dinosaur mummies in what the team delightfully calls the “mummy zone” of the Lance Formation of east-central Wyoming. In addition to adding new specimens to this collection, Sereno and his colleagues have also shed light on the rare process of “clay templating” that produces this preservation of mummified flesh, skin, and other soft tissues. As the carcasses of these Edmontosaurs dried in the Cretaceous sun, they were suddenly immersed under a flash flood that left a thin biofilm over their skin, preserving the three-dimensional soft tissues in time. The team concluded that “the extraordinary preservation of the ‘mummy zone’ is due to rapid subsidence in a coastal setting subject to seasonal drought-flood cycles.”While it’s sad that these dinos died young, it’s a miracle that we can observe their flesh, skin, and hooves 66 million years later. Anyway, it’s not as if they had much to look forward to, since the die was already cast for the non-avian dinosaurs. In this case, the die is an apocalyptic space rock to which we humans owe our existence—and the reality of these cosmic gambles of fate is frankly scarier than any mummy, even a  mummy, ever could be.Deck the halls with stabilimentaHalloween revelers will be taking down their decorative spider-web today, but real spiders keep their web decor up all year long. Researchers have long struggled to make sense of the silky zigzag ornaments that some spiders weave, known as “stabilimenta,” which could serve as insect attractors, predator defense, thermal protection, or water collection. Nobody is really sure!Now, a team has suggested that stabilimenta might be vibrational amplifiers that help alert spiders to prey impacting the web. “No studies have yet investigated how web decorations affect vibration propagation in orb webs,” said researchers led by Gabriele Greco of the Swedish University of Agricultural Sciences. By studying wasp spiders in Sardinia, the team found that in certain cases, ”the presence of the stabilimentum enhances the spider’s ability to detect prey location…compared to webs without a stabilimentum.”“However, from a biological standpoint, the high variability in stabilimentum geometry suggests that the observed differences in elastic wave propagation are unlikely to have a consistent or significant functional role,” the team added. In other words, these web decorations still defy a one-size-fits-all explanation. My own hypothesis is that they are just the dorm room posters of the spider world.A new antivenom bites backThough I am a militant , even I can see how a clade of dangerous limbless weirdos ended up becoming so widely feared and maligned. Snakes kill tens of thousands of people per year, an ongoing tragedy that has galvanized researchers to develop antivenom “cocktails” that could treat snakebit emergencies across many species while also mitigating adverse immunological reactions.  A new study has now addressed this challenge “by immunizing an alpaca and a llama with the venoms of 18 different snakes, including mambas, cobras and rinkhals,” said researchers led by Shirin Ahmadi of the Technical University of Denmark. The cocktail “effectively prevented venom-induced lethality in vivo across 17 African elapid snake species” and “shows considerable promise for comprehensive, continent-wide protection against snakebites by all medically relevant African elapids,” the team added. While it will take much more research to prove it is safe and effective in humans, the antivenom is a significant step toward preventing snakebite deaths and injuries. It's also about the only cocktail that you hope you’ll never have the desire to order.The more (dead), the merrierWe’ll close on that most hallowed of Halloween traditions—a trip to the haunted cemetery.In an unprecedented study, researchers mapped out tens of thousands of ancient Chinese tombs spanning the past 4,000 years since the Xia Dynasty. The team used mapping software to analyze the age and location of the tombs to search for clues about the social and political impacts on burial locations.  “The number of ancient tombs varied significantly across historical periods” with the Qing dynasty (1644 - 1912) accounting for 47.012 percent of the national total, while the Sui dynasty (581 - 618) had only 0.134 percent, according to researchers led by Quanbao Ma of the Beijing University of Civil Engineering and Architecture.“From a temporal perspective, periods of frequent dynastic transitions and wars in Chinese history were often accompanied by significant population declines and migrations,” the team added. “However, during post-war recovery and periods of societal stability, population numbers typically rebounded and grew rapidly, which was also reflected in the increasing number of deceased individuals requiring burial.”In other words, contrary to Halloween lore, it can be a good sign to find a lot of dead bodies in one place because it’s often an indicator of more peaceful and stable times (dead bodies, after all, are the product of living ones). What might be more creepy is an absence of graves in your general vicinity. With that in mind, go visit your local ghosts; they are great company this time of year.Thanks for reading! See you next week.]]></content:encoded></item><item><title>Chips Need to Chill Out</title><link>https://spectrum.ieee.org/thermal-management-chips</link><author>Harry Goldstein</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk1NzcyNS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgxMjg5Nzk0M30.oEWlOSimqjs-0YhG6yHcN93FhULhj37OplNQHCtEdkQ/image.png?width=600" length="" type=""/><pubDate>Sat, 1 Nov 2025 13:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The semiconductor industry seeks radical cooling solutions]]></content:encoded></item><item><title>Samsung Building Facility With 50,000 Nvidia GPUs To Automate Chip Manufacturing</title><link>https://hardware.slashdot.org/story/25/10/31/2352207/samsung-building-facility-with-50000-nvidia-gpus-to-automate-chip-manufacturing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from CNBC: Korean semiconductor giant Samsung said Thursday that it plans to buy and deploy a cluster of 50,000 Nvidia graphics processing units to improve its chip manufacturing for mobile devices and robots. The 50,000 Nvidia GPUs will be used to create a facility Samsung is calling an "AI Megafactory." Samsung didn't provide details about when the facility would be built. It's the latest splashy partnership for Nvidia, whose chips remain essential for building and deploying advanced artificial intelligence. [...]
 
On Thursday, Nvidia representatives said they will work with Samsung to adapt the Korean company's chipmaking lithography platform to work with Nvidia's GPUs. That process will results in 20 times better performance for Samsung, the Nvidia representatives said. Samsung will also use Nvidia's simulation software called Omniverse. Known for its mobile phones, Samsung also said it would use the Nvidia chips to run its own AI models for its devices. In addition to being a partner and customer, Samsung is also a key supplier for Nvidia. Samsung makes the kind of high-performance memory Nvidia uses in large quantities, alongside its AI chips, called high bandwidth memory. Samsung said it will work with Nvidia to tweak its HBM4 memory for use in AI chips.]]></content:encoded></item><item><title>PCI Resizable BAR Improvements Heading To Linux 6.19</title><link>https://www.phoronix.com/news/PCI-ReBAR-Better-Linux-6.19</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 12:35:51 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Restructuring to the Linux kernel's PCI Resizable BAR "ReBAR" support is set to be submitted for the upcoming Linux 6.19 kernel cycle...]]></content:encoded></item><item><title>Linux 6.18 Kernel Happenings, Python 3.14, NTFSPLUS &amp; Other October Highlights</title><link>https://www.phoronix.com/news/October-2025-Highlights</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 10:36:51 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[During the month of October on Phoronix were 305 original news articles around Linux/open-source and another 21 featured Linux hardware reviews / multi-page featured benchmark articles. There was an exciting mix of software and hardware happenings over the past month. Here is a look back at what excited readers the most...]]></content:encoded></item><item><title>AMD Acknowledges RDSEED Failure On AMD Zen 5 With Software Fix Coming</title><link>https://www.phoronix.com/news/AMD-SB-7055-RDSEED-Zen-5</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 10:27:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In mid-October a Meta engineer uncovered an RDSEED architectural issue with AMD Zen 5 CPUs. A patch in turn was sent out to the Linux kernel mailing list to disable RDSEED usage on affected Zen 5 processors. AMD this week issued a security bulletin to acknowledge the issue and report that a microcode fix is coming...]]></content:encoded></item><item><title>KDE Plasma 6.6 To Support Intel&apos;s Adaptive Sharpness Feature</title><link>https://www.phoronix.com/news/Plasma-6.6-Adaptive-Sharpness</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 10:09:04 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE Plasma developers continue to be busy landing more fixes for the recently introduced Plasma 6.5 while also lining up more new features for Plasma 6.6...]]></content:encoded></item><item><title>Falling Panel Prices Lead To Global Solar Boom, Except For the US</title><link>https://hardware.slashdot.org/story/25/10/31/2340238/falling-panel-prices-lead-to-global-solar-boom-except-for-the-us?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Longtime Slashdot reader AmiMoJo shares a report from the Financial Times: Solar power developers want to cover an area larger than Washington, DC, with silicon panels and batteries, converting sunlight into electricity that will power air conditioners in sweltering Las Vegas along with millions of other homes and businesses. But earlier this month, bureaucrats in charge of federal lands scrapped collective approval for the Esmeralda 7 projects, in what campaigners fear is part of an attack on renewable energy under President Donald Trump. "We will not approve wind or farmer destroying [sic] Solar," he posted on his Truth Social platform in August. Developers will need to reapply individually, slowing progress.
 
Thousands of miles away on the other side of the Pacific Ocean, it is a different story. China has laid solar panels across an area the size of Chicago high up on the Tibetan Plateau, where the thin air helps more sunlight get through. The Talatan Solar Park is part of China's push to double its solar and wind generation capacity over the coming decade. "Green and low-carbon transition is the trend of our time," President Xi Jinping told delegates at a UN summit in New York last month. China's vast production of solar panels and batteries has also pushed down the prices of renewables hardware for everyone else, meaning it has "become very difficult to make any other choice in some places," according to Heymi Bahar, senior analyst at the International Energy Agency. [...]
 
More broadly, the US's focus on fossil fuels and pullback of support for clean energy further cedes influence over the future global energy system to China. The US is trying to tie its trading partners into fossil fuels, pressing the EU to buy $750 billion of American oil, natural gas, and nuclear technologies during his presidency as part of a trade deal, scuppering an initiative to begin decarbonizing world shipping and pressuring others to reduce their reliance on Chinese technology. But the collapsing cost of solar panels in particular has spoken for itself in many parts of the world. Experts caution that the US's attacks on renewables could cause lasting damage to its competitiveness against China, even if an administration more favorable to renewables were to follow Trump's.]]></content:encoded></item><item><title>SpaceX Set To Win $2 Billion Pentagon Satellite Deal</title><link>https://tech.slashdot.org/story/25/10/31/2347207/spacex-set-to-win-2-billion-pentagon-satellite-deal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[According to the Wall Street Journal, SpaceX is reportedly poised to secure a $2 billion Pentagon contract to develop hundreds of missile-tracking satellites for President Trump's ambitious Golden Dome defense system. The Independent reports: The planned "air moving target indicator" system in question could ultimately feature as many as 600 satellites once it is fully operational, The Wall Street Journal reports. Musk's company has also been linked to two more satellite ventures, which are concerned with relaying sensitive communications and tracing vehicles, respectively.
 
Golden Dome, inspired by Israel's "Iron Dome," was announced by Trump and Secretary of War Pete Hegseth at the White House in May and will amount to a complex system of satellites and weaponry capable of destroying incoming missiles before they hit American targets. The president promised it would be "fully operational" before he leaves office in January 2029, capable of intercepting rockets, "even if they are launched from space," with an overall price tag of $175 billion.]]></content:encoded></item><item><title>The TechBeat: From Cloud to Desk: 3 Signs the AI Revolution is Going Local (11/1/2025)</title><link>https://hackernoon.com/11-1-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sat, 1 Nov 2025 06:10:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @hacker-Antho [ 4 Min read ] 
 New research shatters AI security assumptions, showing that poisoning large models is easier than believed and requires a very small number of documents. Read More.By @socialdiscoverygroup [ 6 Min read ] 
 Discover how React 19's new hooks—useActionState, useFormStatus, and useOptimistic—simplify form handling with less boilerplate and cleaner code.  Read More.By @mayukhsuri [ 3 Min read ] 
 AWS outage on Oct 20, 2025, disrupted major apps worldwide. Learn what caused it, how it spread, and key lessons to build stronger cloud systems. Read More.By @filestack [ 6 Min read ] 
 Stop babysitting profile pictures. Learn how Filestack Workflows turn image uploads into scalable, async, and lightning-fast experiences. Read More.By @nownodes [ 4 Min read ] 
 Blast API ends operations in Oct 2025. Explore the best developer alternatives like NOWNodes and Alchemy for secure, scalable RPC migration. Read More.By @mend [ 4 Min read ] 
 Traditional testing breaks with AI. Learn how red teaming and AI-powered fuzzing uncover hidden weaknesses in large language models. Read More.By @knightbat2040 [ 5 Min read ] 
 What started as a simple script evolved into a full-fledged data engineering and NLP pipeline that can process a decade's worth of legal decisions in minutes. Read More.By @hackmarketing [ 7 Min read ] 
 Learn how Web3 projects can grow sustainably through education, trust, and human-centered marketing that builds real users and community. Read More.By @botbeat [ 8 Min read ] 
 A deep dive into the 30 companies that burned over one trillion OpenAI tokens—featuring Duolingo, OpenRouter, and Indeed as top power users of GPT tech. Read More.By @melvin-manni [ 5 Min read ] 
 Learn how good intentions can lead to spaghetti dry code, over abstraction and over engineered systems.  Read More.By @giovannicoletta [ 11 Min read ] 
 An interrogation of how physics concepts like black holes, entropy, and quantum theory mirror the rise and limits of artificial intelligence. Read More.By @ichebykin [ 5 Min read ] 
 Context engineering for coding agents is the best way to improve the model performance for code generation.  Read More.By @mcsee [ 3 Min read ] 
 Avoid Boolean variables, they lead to conditional logic and force you to write Ifs. Create polymorphic states instead Read More.By @sanjaybarot [ 23 Min read ] 
 Ransomware has gone cloud-native: no payloads, just API abuse. Learn the tactics—IAM takeovers, KMS locks, backup sabotage—and how to build resilience. Read More.By @ainativedev [ 4 Min read ] 
 GitHub Copilot evolves: cloud-based agents now handle PRs, iterate from feedback, and fit seamlessly into dev workflows. Read More.By @aifundingtracker [ 13 Min read ] 
 AI startups raised over $3.6 billion this week across infrastructure, wearable AI, enterprise automation, and fintech innovation. Read More.]]></content:encoded></item><item><title>The Numbers Show Xbox&apos;s Current Plan Isn&apos;t Working</title><link>https://games.slashdot.org/story/25/10/31/2332211/the-numbers-show-xboxs-current-plan-isnt-working?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Gizmodo: It's time for Xbox to eat some humble pie and perform some real soul-searching. Microsoft released its latest quarterly earnings report and proved the worst of our fears about its gaming brand. Not only are Xbox hardware sales down significantly, but the brand itself is barely treading water. Gamers are voicing their displeasure with their wallets, but Microsoft's top brass is still only thinking about the margins. Microsoft was more keen to promote the scale of its cloud and AI services revenue -- which was up 28% year over year -- than talk about its beleaguered gaming brand. The company's overall gaming revenue fell by 2% compared to the same time last year. This was precipitated by a "decline in Xbox hardware," which was down by 22% following a steady decline quarter after quarter. Its first-party games and its Game Pass subscription were doing better, though the overall growth was only up by 1%, and even that was driven by the "better-than-expected performance" of third-party games. You can give credit to titles like Clair Obscur: Expedition 33 for why Xbox isn't in an even deeper hole than it is now.
 
The tech giant has no expectation that its Xbox brand will start making more money anytime soon. In its earnings call with investors, Microsoft Chief Financial Officer Amy Hood said the company expects Xbox will continue to decline "in the low to mid-single digits" for the following quarter. That's mostly due to the lack of landmark first-party titles. Just this month, Xbox released Ninja Gaiden 4, The Outer Worlds 2, and Double Fine's The Keeper. Xbox also made a huge marketing push for its first handheld, made in partnership with Asus, the ROG Xbox Ally and Ally X. In any other year, this would be a big month for any gaming company. The dour outlook comes after months of bad news. After two subsequent price hikes, Xbox Series S and Series X consoles now cost between $100 to $150 more than they did at launch five years ago. Microsoft also pushed prices of its Game Pass Ultimate subscription tier from $20 to $30 per month. A full-year's subscription would now demand $360. In a separate article, Gizmodo reviews Microsoft's new ROG Xbox Ally X handheld, which "offers a better experience overall" than the "other small-scale Windows PC gaming devices released this year." However, "it's still nowhere close to what you truly want from a console."]]></content:encoded></item><item><title>When AI And Secure Chat Meet, Users Deserve Strong Controls Over How They Interact</title><link>https://www.techdirt.com/2025/10/31/when-ai-and-secure-chat-meet-users-deserve-strong-controls-over-how-they-interact/</link><author>Thorin Klosowski</author><category>tech</category><pubDate>Sat, 1 Nov 2025 02:39:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Both Google and Apple are cramming new AI features into their phones and other devices, and neither company has offered clear ways to control which apps those AI systems can access. Recent issues around WhatsApp on both Android and iPhone demonstrate how these interactions can go sideways, risking revealing chat conversations beyond what you intend. Users deserve better controls and clearer documentation around what these AI features can access.After diving into how Google Gemini and Apple Intelligence (and in some cases Siri) currently work, we didn’t always find clear answers to questions about how data is stored, who has access, and what it can be used for.At a high level, when you compose a message with these tools, the companies can usually see the contents of those messages and receive at least a temporary copy of the text on their servers.When receiving messages, things get trickier. When you use an AI like Gemini or a feature like Apple Intelligence to summarize or read notifications, we believe companies should be doing that content processing on-device. But poor documentation and weak guardrails create issues that have lead us deep into documentation rabbit holes and still fail to clarify the privacy practices as clearly as we’d like.We’ll dig into the specifics below as well as potential solutions we’d like to see Apple, Google, and other device-makers implement, but first things first, here’s what you can do right now to control access:Control AI Access to Secure Chat on Android and iOSHere are some steps you can take to control access if you want nothing to do with the device-level AI features’ integration and don’t want to risk accidentally sharing the text of a message outside of the app you’re using.How to Check and Limit What Gemini Can AccessIf you’re using Gemini on your Android phone, it’s a good time to review your settings to ensure things are set up how you want. Here’s how to check each of the relevant settings:Disable Gemini App Activity: Gemini App Activity is a history Google stores of all your interactions with Gemini. It’s enabled by default. To disable it, open Gemini (depending on your phone model, you may or may not even have the Google Gemini app installed. If you don’t have it installed, you don’t really need to worry about any of this). Tap your  > then change the toggle to either “Turn off,” or “Turn off and delete activity” if you want to delete previous conversations. If the option reads “Turn on,” then Gemini Apps Activity is already turned off. Control app and notification access: You can control which apps Gemini can access by tapping your  > , then scrolling down and disabling the toggle next to any apps you do not want Gemini to access. If you do not want Gemini to potentially access the content that appears in notifications, open the Settings app and revoke notification access from the Google app.: Depending on your phone model, you might be able to delete the Gemini app and revert to using Google Assistant instead. You can do so by long-pressing the Gemini app and selecting the option to delete. How to Check and Limit what Apple Intelligence and Siri Can AccessSimilarly, there are a few things you can do to clamp down on what Apple Intelligence and Siri can do: Disable the “Use with Siri Requests” option: If you want to continue using Siri, but don’t want to accidentally use it to send messages through secure messaging apps, like WhatsApp, then you can disable that feature by opening  >  > , and disabling “Use with Siri Requests,” which turns off the ability to compose messages with Siri and send them through that app.Disable Apple Intelligence entirely: Apple Intelligence is an all-or-nothing setting on iPhones, so if you want to avoid any potential issues your only option is to turn it off completely. To do so, open  > Apple Intelligence & Siri, and disable “Apple Intelligence” (you will only see this option if your device supports Apple Intelligence, if it doesn’t, the menu will only be for “Siri”). You can also disable certain features, like “writing tools,” using Screen Time restrictions. Siri can’t be universally turned off in the same way, though you can turn off the options under “Talk to Siri” to make it so you can’t speak to it. For more information about cutting off AI access at different levels in other apps, this Consumer Reports article covers other platforms and services.Sending Messages Has Different Privacy Concerns than Receiving ThemLet’s start with a look at how Google and Apple integrate their AI systems into message composition, using WhatsApp as an example.Google Gemini and WhatsAppOn Android, you can optionally link WhatsApp and Gemini together so you can then initiate various actions for sending messages from the Gemini app, like “Call Mom on WhatsApp” or “Text Jason on WhatsApp that we need to cancel our secret meeting, but make it a haiku.” This feature raised red flags for users concerned about privacy.By default, everything you do in Gemini is stored in the “Gemini Apps Activity,” where messages are stored forever, subject to human review, and are used to train Google’s products. So, unless you change it, when you use Gemini to compose and send a message in WhatsApp then the message you composed is visible to Google.If you turn the activity off, interactions are still stored for 72 hours. Google’s documentation claims that even though messages are stored, those conversations aren’t reviewed or used to improve Google machine learning technologies, though that appears to be an internal policy choice with no technical limits preventing Google from accessing those messages.The simplicity of invoking Gemini to compose and send a message may lead to a false sense of privacy. Notably, other secure messaging apps, like Signal, do not offer this Gemini integration.For comparison’s sake, let’s see how this works with Apple devices.According to its privacy policy, when you dictate a message through Siri to send to WhatsApp (or anywhere else), the message, including metadata like the recipient phone number and other identifiers, is sent to Apple’s servers. This was confirmed by researchers to include the text of messages sent to WhatsApp. When you use Siri to compose a WhatsApp message, the message gets routed to both Apple and WhatsApp. Apple claims it does not store this transcript unless you’ve opted into “Improve Siri and Dictation.” WhatsApp defers to Apple’s support for data handling concerns. This is similar to how Google handles speech-to-text prompts.In response to that research, Apple said this was expected behavior with an app that uses SiriKit—the extension that allows third-party apps to integrate with Siri—like WhatsApp does.Both Siri and Apple Intelligence can sometimes run locally on-device, and other times need to rely on Apple-managed cloud servers to complete requests. Apple Intelligence can use the company’s Private Cloud Compute, but Siri doesn’t have a similar feature.The ambiguity around where data goes makes it overly difficult to decide on whether you are comfortable with the sort of privacy trade-off that using features like Siri or Apple Intelligence might entail.How Receiving Messages WorksSending encrypted messages is just one half of the privacy puzzle. What happens on the receiving end matters too. We could not find anything in Google’s Utilities documentation that clarifies what information is collected, stored, or sent to Google from these notifications. When we reached out to Google, the company responded that it “builds technical data protections that safeguard user data, uses data responsibly, and provides users with tools to control their Gemini experience.” Which means Google has no technical limitation around accessing the text from notifications if you’ve enabled the feature in the Utilities app. This could open up any notifications routed through the Utilities app to the Gemini app to be accessed internally or from third-parties. Google needs to publicly make its data handling explicit in its documentation.Apple is more clear about how it handles this sort of notification access.Siri can read and reply to messages with the “Announce Notifications” feature. With this enabled, Siri can read notifications out loud on select headphones or via CarPlay. In a press release, Apple states, “When a user talks or types to Siri, their request is processed on device whenever possible. For example, when a user asks Siri to read unread messages… the processing is done on the user’s device. The contents of the messages aren’t transmitted to Apple servers, because that isn’t necessary to fulfill the request.”Apple Intelligence can summarize notifications from any app that you’ve enabled notifications on. Apple is clear that these summaries are generated on your device, “when Apple Intelligence provides you with preview summaries of your emails, messages, and notifications, these summaries are generated by on-device models.” This means there should be no risk that the text of notifications from apps like WhatsApp or Signal get sent to Apple’s servers just to summarize them.New AI Features Must Come With Strong User ControlsAs more device-makers cram AI features into their devices, the more necessary it is for us to have clear and simple controls over what personal data these features can access on our devices. If users do not have control over when a text leaves a device for any sort of AI processing—whether that’s to a “private” cloud or not—it erodes our privacy and potentially threatens the foundations of end-to-end encrypted communications.Google, Apple, and other device makers should add a device-level AI permission, just like they do for other potentially invasive privacy features, like location sharing, to their phones. You should be able to tell the operating system’s AI to not access an app, even if that comes at the “cost” of missing out on some features. The setting should be straightforward and easy to understand in ways the Gemini an Apple Intelligence controls currently are not.Offer On-Device-Only ModesDevice-makers should offer an “on-device only” mode for those interested in using some features without having to try to figure out what happens on device or on the cloud. Samsung offers this, and both Google and Apple would benefit from a similar option.Both Google and Apple should improve their documentation about how these features interact with various apps. Apple doesn’t seem to clarify notification processing privacy anywhere outside of a press release, and we couldn’t find anything about Google’s Utilities privacy at all. We appreciate tools like Gemini Apps Activity as a way to audit what the company collects, but vague information like “Prompted a Communications query” is only useful if there’s an explanation somewhere about what that means.The current user options are not enough. It’s clear that the AI features device-makers add come with significant confusion about their privacy implications, and it’s time to push back and demand better controls. The privacy problems introduced alongside new AI features should be taken seriously, and remedies should be offered to both users and developers who want real, transparent safeguards over how a company accesses their private data and communications.]]></content:encoded></item><item><title>OpenAI Launches Aardvark To Detect and Patch Hidden Bugs In Code</title><link>https://it.slashdot.org/story/25/10/31/2314223/openai-launches-aardvark-to-detect-and-patch-hidden-bugs-in-code?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 02:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[OpenAI has introduced Aardvark, a GPT-5-powered autonomous agent that scans, reasons about, and patches code like a human security researcher. "By embedding itself directly into the development pipeline, Aardvark aims to turn security from a post-development concern into a continuous safeguard that evolves with the software itself," reports InfoWorld. From the report: What makes Aardvark unique, OpenAI noted, is its combination of reasoning, automation, and verification. Rather than simply highlighting potential vulnerabilities, the agent promises multi-stage analysis -- starting by mapping an entire repository and building a contextual threat model around it. From there, it continuously monitors new commits, checking whether each change introduces risk or violates existing security patterns.
 
Additionally, upon identifying a potential issue, Aardvark attempts to validate the exploitability of the finding in a sandboxed environment before flagging it. This validation step could prove transformative. Traditional static analysis tools often overwhelm developers with false alarms -- issues that may look risky but aren't truly exploitable. "The biggest advantage is that it will reduce false positives significantly," noted Jain. "It's helpful in open source codes and as part of the development pipeline."
 
Once a vulnerability is confirmed, Aardvark integrates with Codex to propose a patch, then re-analyzes the fix to ensure it doesn't introduce new problems. OpenAI claims that in benchmark tests, the system identified 92 percent of known and synthetically introduced vulnerabilities across test repositories, a promising indication that AI may soon shoulder part of the burden of modern code auditing.]]></content:encoded></item><item><title>FCC To Rescind Ruling That Said ISPs Are Required To Secure Their Networks</title><link>https://it.slashdot.org/story/25/10/31/237241/fcc-to-rescind-ruling-that-said-isps-are-required-to-secure-their-networks?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 01:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The FCC plans to repeal a Biden-era ruling that required ISPs to secure their networks under the Communications Assistance for Law Enforcement Act, instead relying on voluntary cybersecurity commitments from telecom providers. FCC Chairman Brendan Carr said the ruling "exceeded the agency's authority and did not present an effective or agile response to the relevant cybersecurity threats." Carr said the vote scheduled for November 20 comes after "extensive FCC engagement with carriers" who have taken "substantial steps... to strengthen their cybersecurity defenses." Ars Technica reports: The FCC's January 2025 declaratory ruling came in response to attacks by China, including the Salt Typhoon infiltration of major telecom providers such as Verizon and AT&T. The Biden-era FCC found that the Communications Assistance for Law Enforcement Act (CALEA), a 1994 law, "affirmatively requires telecommunications carriers to secure their networks from unlawful access or interception of communications."
 
"The Commission has previously found that section 105 of CALEA creates an affirmative obligation for a telecommunications carrier to avoid the risk that suppliers of untrusted equipment will "illegally activate interceptions or other forms of surveillance within the carrier's switching premises without its knowledge,'" the January order said. "With this Declaratory Ruling, we clarify that telecommunications carriers' duties under section 105 of CALEA extend not only to the equipment they choose to use in their networks, but also to how they manage their networks." A draft of the order that will be voted on in November can be found here (PDF).]]></content:encoded></item><item><title>What is Bending Spoons? Everything to know about AOL’s acquirer</title><link>https://techcrunch.com/2025/10/31/what-is-bending-spoons-everything-to-know-about-aols-acquirer/</link><author>Anna Heim</author><category>tech</category><pubDate>Sat, 1 Nov 2025 01:11:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Bending Spoons remains largely unknown, even as its portfolio of products has served more than a billion people. ]]></content:encoded></item><item><title>Bluesky Hits 40 Million Users, Introduces &apos;Dislikes&apos; Beta</title><link>https://tech.slashdot.org/story/25/10/31/231232/bluesky-hits-40-million-users-introduces-dislikes-beta?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 00:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Bluesky has surpassed 40 million users and is launching a "dislikes" beta to improve its personalization algorithms and reduce toxic content. TechCrunch reports: With the "dislikes" beta rolling out soon, Bluesky will take into account the new signal to improve user personalization. As users "dislike" posts, the system will learn what sort of content they want to see less of. This will help to inform more than just how content is ranked in feeds, but also reply rankings.
 
The company explained the changes are designed to make Bluesky a place for more "fun, genuine, and respectful exchanges" -- an edict that follows a month of unrest on the platform as some users again criticized the platform over its moderation decisions. While Bluesky is designed as a decentralized network where users run their own moderation, some subset of Bluesky users want the platform itself to ban bad actors and controversial figures instead of leaving it up to the users to block them. Bluesky, however, wants to focus more on the tools it provides users to control their own experience.]]></content:encoded></item><item><title>Wine 10.18 Released With More WoW64 Mode Improvements</title><link>https://www.phoronix.com/news/Wine-10.18-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 1 Nov 2025 00:35:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Wine 10.18 is now available for capping off the month of October and working toward the code freeze for Wine 11.0 beginning in early December...]]></content:encoded></item><item><title>How L.A. Scores “Vulnerability” of Unhoused People Is Changing: What You Need to Know</title><link>https://hackernoon.com/how-la-scores-vulnerability-of-unhoused-people-is-changing-what-you-need-to-know?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sat, 1 Nov 2025 00:23:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Welcome to The Markup, where we use investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up forKlaxon, a newsletter that delivers our stories and tools directly to your inbox.\
One year after a Markup investigation revealed racial bias in Los Angeles’s housing intake system for people experiencing homelessness, local politicians have pressed for reforms and the agency responsible for housing is taking steps to make its approach more equitable and effective.\
Shortly after our original investigation published, Los Angeles City Council Member Nithya Raman, who chairs the Housing and Homelessness committee, introduced a motion citing the article and calling on the Los Angeles Homeless Services Authority (LAHSA) to come up with a plan to reform its intake system. The legislation, approved unanimously, called specifically for greater fairness in the “vulnerability” scoring system that The Markup analyzed. Used by Los Angeles for the past decade, the system rated Black people as significantly less vulnerable than White people year after year, making them less likely to obtain subsidized permanent housing.\
Black people are hugely overrepresented among unhoused people in L.A., making up about 9 percent of Los Angeles County’s population but about 30 percent of the county’s people experiencing homelessness.\
“To see that the tool that we’re using to put people in line for housing was not actually housing unhoused Black Angelenos as quickly as we could was really surprising to me,” said Raman, who read the article in the Los Angeles Times, where it was co-published. Raman, who is currently running for re-election in District Four, in central LA, said the investigation “absolutely” spurred the council to act.\
LAHSA, given a deadline of April 2023 in the legislation, still has not provided a reform plan. A spokesperson for the agency didn’t directly respond to a request for comment about the plan.\
Raman said LAHSA has taken some steps in the past year to improve how it allocates housing. Among other changes, she said, the agency has started to prioritize some groups, including those already involved in housing programs and those who already have the documents required to move into a building, like an ID and social security number.\
Meanwhile, the agency also de-emphasized the score’s importance in placing people for permanent housing. People applying for housing are scored on a 17-point scale. Previously, the people with the highest scores were given the highest priority, but now any person who scores an eight or above can be prioritized, depending on the other factors being considered.\
Still, equity in the housing system remains a known problem. In November, researchers from the University of Southern California and the University of California Los Angeles, working in partnership with LAHSA, released a long-awaited study on racial bias in the system and ways to reform the scoring system, known as the VI-SPDAT.\
The study, which analyzed scores across race and ethnicity, tracked with The Markup’s findings from earlier in the year, concluding that the scoring tool is biased toward White people and that it’s ineffective overall. The study, in some respects, went even further. Using data on who ultimately faced an “adverse” event, like jail or death, the researchers concluded that tool was “not much more accurate than a random guess at predicting vulnerability.”\
The study suggested several ways the scoring system could become more accurate and equitable, some of which matched The Markup’s reporting. The scoring system asks intensely personal questions about a person’s life, including around issues like violence and substance abuse, and the report recommends rewording and reframing questions to make the survey less complex and more sensitive. A revised version of the system with new questions and scoring could substantially reduce bias, the researchers conclude.\
For example, the study suggests that the existing question about whether anyone has “forced you or tricked you to do things that you do not want to do” should be amended to stress that answering yes “will not result in punishment or any negative consequences.” Another question currently asks, “Are there any medications like painkillers that you don’t take the way the doctor prescribed or where you sell the medication?” The study suggested softening it to, “Do you have medication that you choose to sell instead of taking to help support yourself financially? Answering yes to this question will not result in punishment or negative consequences for you.” Several questions were suggested for removal entirely.\
In a written statement, LAHSA spokesperson Christopher Yee acknowledged that it’s long been clear that “the VI-SPDAT has shortcomings related to equity,” adding that the survey is “long, cumbersome, and not trauma-informed in the content of the questions or administration process.”\
Yee highlighted the study on recommended changes to the system and said the agency is “working with key partners and stakeholders to create a plan to implement and refine” a new iteration of the scoring system while it continues to use the old version.\
The agency, he noted, has already dropped a requirement to score people for interim housing entry or time-limited subsidy programs, but will still require scoring for permanent housing. LAHSA must use some sort of prioritization system to access certain federal housing funds under rules established by the U.S. Department of Housing and Urban development.\
The planned changes to the scoring system will first apply to screening for adults, and later the agency plans to explore changes to related tools for young people and families with children. The Markup found that racial disparities were even more stark for a variation of the VI-SPDAT used in Los Angeles for people under the age of 25.\
Yee’s statement did not provide a timeline for the revised tool’s launch, but in a FAQ released alongside the study LAHSA said service providers could expect more information early this year on changes to the intake process, known as the Coordinated Entry System.\
Raman, for her part, said she’s withholding judgment until data can show how those changes affect who is housed. But, she said, “there’s no question in my mind that CES needs reform.”]]></content:encoded></item><item><title>Austria&apos;s Ministry of Economy Has Migrated To a Nextcloud Platform In Shift Away From US Tech</title><link>https://yro.slashdot.org/story/25/10/31/2023230/austrias-ministry-of-economy-has-migrated-to-a-nextcloud-platform-in-shift-away-from-us-tech?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 1 Nov 2025 00:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from ZDNet: Even before Azure had a global failure this week, Austria's Ministry of Economy had taken a decisive step toward digital sovereignty. The Ministry achieved this status by migrating 1,200 employees to a Nextcloud-based cloud and collaboration platform hosted on Austrian-based infrastructure. This shift away from proprietary, foreign-owned cloud services, such as Microsoft 365, to an open-source, European-based cloud service aligns with a growing trend among European governments and agencies. They want control over sensitive data and to declare their independence from US-based tech providers.
 
European companies are encouraging this trend. Many of them have joined forces in the newly created non-profit foundation, the EuroStack Initiative. This foundation's goal is " to organize action, not just talk, around the pillars of the initiative: Buy European, Sell European, Fund European." What's the motive behind these moves away from proprietary tech? Well, in Austria's case, Florian Zinnagl, CISO of the Ministry of Economy, Energy, and Tourism (BMWET), explained, "We carry responsibility for a large amount of sensitive data -- from employees, companies, and citizens. As a public institution, we take this responsibility very seriously. That's why we view it critically to rely on cloud solutions from non-European corporations for processing this information."
 
Austria's move and motivation echo similar efforts in Germany, Denmark, and other EU states and agencies. The organizations include the German state of Schleswig-Holstein, which abandoned Exchange and Outlook for open-source programs. Other agencies that have taken the same path away from Microsoft include the Austrian military, Danish government organizations, and the French city of Lyon. All of these organizations aim to keep data storage and processing within national or European borders to enhance security, comply with privacy laws such as the EU's General Data Protection Regulation (GDPR), and mitigate risks from potential commercial and foreign government surveillance.]]></content:encoded></item><item><title>YouTube TV Loses ESPN, ABC and Other Disney Channels</title><link>https://entertainment.slashdot.org/story/25/10/31/2017209/youtube-tv-loses-espn-abc-and-other-disney-channels?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 23:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Disney's channels, including ESPN, ABC, FX, and NatGeo, have gone dark on YouTube TV after Google and Disney failed to renew their carriage agreement before the October 30 deadline, with each side blaming the other for using unfair negotiating tactics and price hikes. YouTube TV says it will issue a $20 credit to subscribers if the blackout continues while negotiations proceed. Engadget reports: "Last week Disney used the threat of a blackout on YouTube TV as a negotiating tactic to force deal terms that would raise prices on our customers," YouTube said in an announcement on its blog. "They're now following through on that threat, suspending their content on YouTube TV." YouTube added that Disney's decision harms its subscribers while benefiting its own live TV products, such as Hulu+Live TV and Fubo.
 
In a statement sent to the Los Angeles Times, however, Disney accused Google's YouTube TV of choosing to deny "subscribers the content they value most by refusing to pay fair rates for [its] channels, including ESPN and ABC." Disney also accused Google of using its market dominance to "eliminate competition and undercut the industry-standard terms" that other pay-TV distributors have agreed to pay for its content.]]></content:encoded></item><item><title>The Most Anticipated BNB Launch of 2025: $BALZ Brings The Meme Migration Home</title><link>https://hackernoon.com/the-most-anticipated-bnb-launch-of-2025-$balz-brings-the-meme-migration-home?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Fri, 31 Oct 2025 22:52:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore, Singapore, October 31st, 2025/Chainwire/--The Binance Smart Chain (BNB) network has seen renewed activity, and BALZ has emerged as one of its notable community movements, with over 40,000 active members before launch on X (@). Observers regard it as one of the more anticipated community-driven launches of the year, comparable to projects such as Aster and Four.meme.Raising over $2 million within days of opening, BALZ has positioned itself as a significant project developing on BNB, despite its informal branding and memetic culture. With more than 40,000 members prior to its anticipated token presale, the project has adopted an unconventional approach to community growth through guerrilla marketing and its "rug pull recovery protocol."Instead of allocating capital to influencer campaigns, the team integrated communities from Solana and Base, migrating them to BNB through its protocol. At the time of writing, more than 10,000 verified holders are in the process of migration.The Token Presale: Closing Tonight, October 31st at 23:59 PDTAt the center of BALZ is the Fair-As-F* Launch (FAF), a limited-time token presale closing on October 31 at 23:59 PDT. Within days of opening, BALZ raised over $2 million, drawing parallels to earlier community-led launches such as Shiba Inu and Floki in 2020.FAF is structured with a fixed price and specific time frame, allowing equal participation without insider advantages or automated trading. In a market that has frequently favored early access and automation, BALZ seeks to show that fairness can be built into its design.BNB Market Conditions and TimingThe timing aligns with a significant shift in the cryptocurrency market. On October 10, 2025, the sector experienced its largest liquidation event to date, with $19 billion eliminated within 48 hours as Bitcoin declined from $126,000 to $105,000. This event represented market deleveraging rather than capitulation.Open interest decreased from $48.7 billion to $45.1 billionFunding rates fell by 51 percentOverleveraged positions were clearedThe result is a market now characterized by conviction-based participants and institutional capital seeking new deployment opportunities.Market structure mirrors 2020-2021 exactly:Bitcoin ETFs pulled in $2.71 billion during October 6-10, BlackRock's IBIT holding $65.26 billion85% of institutional firms now allocate to digital assetsFed rate cuts hit 93% probability for next quarterBNB Smart Chain Shows Continued Growth3.62 million daily active addresses in October 2025Total Value Locked surged 217% to $17.1 billion70% of BNB meme traders are currently profitableCZ is back. He changed his X profile from "ex-@binance" to "@binance" in September 2025. BNB hit an all-time high of $1,311. Real infrastructure that actually supports growth. BNB is where smart money is rotating.BALZ is capturing this momentum at the exact moment Solana and Base communities are looking for an exit. Market observers note the project is one CZ tweet away from a billion-dollar market cap, similar to previous meme token cycles where single endorsements rapidly accelerated valuations into nine-figure territory.The presale window closes October 31st at 23:59 PDT.Follow: X: @ | Telegram: t.me/BALZ_Official is a meme coin launching on Binance Smart Chain with a mission: to build the safest, fastest trading platform and no-code launchpad in crypto. Led by a doxxed team and powered by 40,000+ active members.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Amazon To Block Piracy Apps On Fire TV</title><link>https://yro.slashdot.org/story/25/10/31/2012202/amazon-to-block-piracy-apps-on-fire-tv?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 22:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Amazon will begin blocking sideloaded piracy apps on Fire TV devices by cross-checking them against a blacklist maintained by the Alliance for Creativity and Entertainment. The company will, however, continue to allow legitimate sideloading for developers. Heise reports: In response to an inquiry, Amazon explained that it has always worked to ban piracy from its app store. As part of an expanded program led by the ACE, it is now blocking apps that demonstrably provide access to pirated content, including those downloaded outside the app store. This builds on Amazon's ongoing efforts to support creators and protect customers, as piracy can also expose users to malware, viruses, and fraud.
 
[...] The sideloading option will remain available on Fire TV devices running Amazon's new operating system, Vega OS. However, it is generally limited to developers here. In this context, the company emphasized that, contrary to rumors, there are no plans to upgrade existing Fire TV devices with Fire OS as the operating system to Vega OS.]]></content:encoded></item><item><title>Aster’s Rocket Launch Surpasses $1B in Trading Volume, as Nubila Joins with Over 6 Million $NB</title><link>https://hackernoon.com/asters-rocket-launch-surpasses-$1b-in-trading-volume-as-nubila-joins-with-over-6-million-$nb?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Fri, 31 Oct 2025 22:45:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[George Town, British Virgin Islands, October 31st, 2025/Chainwire/--, the decentralized trading platform, has generated strong momentum with its innovative product .In the first six days following the debut of Rocket Launch, Aster recorded approximately $122 million in spot trading volume and $933 million in perpetual trading volume. Within five days after APRO’s $AT token TGE, Aster captured over 90% of the market share in $AT perpetual trading, underscoring Rocket Launch’s significant contribution to overall market activity.Since its debut on October 24, Rocket Launch has meaningfully increased both user activity and engagement on the platform. On October 29, Aster announced a 500,000 $AT Loyalty Bonus distributed to early participants who traded within the first four days of the campaign.The platform also disclosed that the spot trading competition features a reward pool of no less than 1.5 million $AT, followed by a perpetual trading campaign with at least 1.5 million $AT in additional rewards, marking a continuation of strong user engagement across both markets.The first Rocket Launch event not only accelerated new user acquisition but also reactivated existing traders and token holders, significantly enhancing overall liquidity and engagement across the Aster ecosystem. This milestone demonstrates Rocket Launch’s strong driving force and long-term potential in shaping the growth of the Aster DeFi landscape.Next Rocket Launch: Nubila Debuts, Powering the Physical Oracle Layer for AI and Prediction MarketsAster announced that the next Rocket Launch will begin on October 31, 2025, at 12:00 UTC, featuring , a decentralized oracle network for AI and prediction markets. The seven-day campaign will include both spot and perpetual trading campaigns for Nubila ($NB).The event adopts a dual reward structure. The Spot campaign offers a $200,000 $ASTER prize pool alongside over 3 million $NB in rewards, while the Perpetual campaign features an exclusive pool exceeding 3 million $NB, aimed at fostering broader participation and sustained market activity.Continuing its long-term vision, Aster is redefining the evolution of token launches through Rocket Launch, transforming what used to be a single market event into a continuous, growth-oriented journey.Each Rocket Launch campaign is structured to create a self-reinforcing value loop. The reward pool combines $ASTER and the project’s native tokens. Project teams contribute both capital and tokens, while Aster allocates those funds to buy back $ASTER from the open market.The repurchased $ASTER, together with the project tokens, are then distributed as rewards to participants, ensuring that users benefit directly from both trading activity and ecosystem growth.“Aster’s Rocket Launch is more than a trading campaign; it’s an engine for on-chain innovation,” said Leonard, CEO of Aster. “Every participant becomes part of the ecosystem, contributing to the process of value creation for emerging projects.” is building the physical oracle layer for AI and prediction markets. Its decentralized sensor network captures real-world data and transforms it into verifiable intelligence for AI systems and smart contracts.Backed by BCG, Block Space Force, Quantum Holdings, VeChain, and IoTeX, Nubila has deployed 21,000+ devices across 122 countries and 16,000+ validator nodes, powering the next wave of AI agents and decentralized applications with real, trustworthy physical data. is a next-generation decentralized exchange offering both Perpetual and Spot trading, designed as a one-stop onchain venue for global crypto traders. It features MEV-free, one-click execution in 1001x Mode. Perpetual Mode adds 24/7 stock Perpetuals, Hidden Orders, and grid trading, available across BNB Chain, Ethereum, Solana, and Arbitrum.Its unique edge lies in the ability to use liquid-staking tokens (asBNB) or yield-generating stablecoins (USDF) as collateral, unlocking unparalleled capital efficiency. Backed by YZi Labs, Aster is building the future of DeFi: fast, flexible, and community-first.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Denmark Reportedly Withdraws &apos;Chat Control&apos; Proposal Following Controversy</title><link>https://yro.slashdot.org/story/25/10/31/205234/denmark-reportedly-withdraws-chat-control-proposal-following-controversy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 22:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Record: Denmark's justice minister on Thursday said he will no longer push for an EU law requiring the mandatory scanning of electronic messages, including on end-to-end encrypted platforms. Earlier in its European Council presidency, Denmark had brought back a draft law which would have required the scanning, sparking an intense backlash. Known as Chat Control, the measure was intended to crack down on the trafficking of child sex abuse materials (CSAM). After days of silence, the German government on October 8 announced it would not support the proposal, tanking the Danish effort.
 
Danish Justice Minister Peter Hummelgaard told reporters on Thursday that his office will support voluntary CSAM detections. "This will mean that the search warrant will not be part of the EU presidency's new compromise proposal, and that it will continue to be voluntary for the tech giants to search for child sexual abuse material," Hummelgaard said, according to local news reports. The current model allowing for voluntary scanning expires in April, Hummelgaard said. "Right now we are in a situation where we risk completely losing a central tool in the fight against sexual abuse of children," he said. "That's why we have to act no matter what. We owe it to all the children who are subjected to monstrous abuse."]]></content:encoded></item><item><title>GNOME Gains A New macOS-Inspired Quick Menu Option</title><link>https://www.phoronix.com/news/GNOME-Kiwi-macOS-Quick-Menu</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 21:41:22 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For GNOME desktop users desiring a more macOS-like experience, a new GNOME extension provides a macOS-inspired quick menu option...]]></content:encoded></item><item><title>YouTube&apos;s AI Moderator Pulls Windows 11 Workaround Videos, Calls Them Dangerous</title><link>https://news.slashdot.org/story/25/10/31/1853251/youtubes-ai-moderator-pulls-windows-11-workaround-videos-calls-them-dangerous?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 21:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Is installing Windows 11 with a local account or on unsupported hardware harmful or dangerous? YouTube's AI moderation system seems to think so, as it has started pulling videos that show users how to sidestep Microsoft's setup restrictions. 

Tech YouTuber Rich White, aka CyberCPU Tech, was the first to go public about the issue on October 26, when he posted a video reporting the removal of a how-to he published on installing Windows 11 25H2 with a local account instead of a Microsoft account. In the video, White expressed concern that YouTube's automated flagging process may be the root of the problem, as he found it hard to believe that "creating a local account in Windows 11 could lead to serious harm or even death," as YouTube reportedly alleged when it removed the video. 

When he appealed, White said that YouTube denied the request within 10 to 20 minutes, early on a Sunday morning, which led him to speculate that there wasn't a human in the loop when the request was shut down. That wasn't his only video removed, either. The next day, White uploaded his video for this week on installing Windows 11 25H2 on unsupported hardware, which was removed hours after being posted. YouTube justified the removal on similar grounds. [...] At least two other YouTubers - Britec09 and Hrutkay Mods - have released videos alleging much of the same.]]></content:encoded></item><item><title>ICE’s Hiring Surge Is Attracting A Bunch Of People Who Are Too Unfit (Or Too Criminal) To Work At ICE</title><link>https://www.techdirt.com/2025/10/31/ices-hiring-surge-is-attracting-a-bunch-of-people-who-are-too-unfit-or-too-criminal-to-work-at-ice/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 31 Oct 2025 21:18:03 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[ICE can generate multiple horrible stories a day but it still can’t seem to find enough brown people to deport daily to satisfy White House advisor Stephen Miller’s demands for 3,000 arrests per day.Trump and the GOP threw a lot of money at this problem with the Big Beautiful Bill. A  of money: $75  over the next four years. Part of that goes to another metric ICE will apparently never meet: 10,000 new hires. Not that ICE isn’t trying. It’s currently pissing off law enforcement agencies all over the country by throwing $50,000 signing bonuses at new recruits — something that has the potential to deprive local law enforcement of some of their current, um… talent. It has also lowered its standards and ripped the age limits off both ends of the scale, hoping to attract a blend of people who’ve already aged out of physical work and fresh faces their new bosses will likely assume don’t really want work.Now that it’s been a few months since the hiring surge began, we’re finally seeing some results. And it’s possibly worse than you imagine. Here’s only the tip of the iceberg, as reported by Nick Miroff for The Atlantic:President Donald Trump’s plan to double the size of the ICE workforce has met a foe more powerful than any activist group. It is decimating new recruits at the agency’s training academy in Georgia. It is the ICE personal-fitness test.More than a third have failed so far, four officials told me, impeding the agency’s plan to hire, train, and deploy 10,000 deportation officers by January. To pass, recruits must do 15 push-ups and 32 sit-ups, and run 1.5 miles in 14 minutes.“It’s pathetic,” one career ICE official told me, adding that before now, a typical class of 40 recruits had only a couple of candidates fail, because the screening process was more rigorous.“Pathetic” is being kind. Everything about the hiring process has been eased and streamlined. The physical requirements have been lowered and still can’t be met. And those handling the recruits don’t know who’s capable of passing this entry requirement until they’re already on site because the application process allows potential hires to “self-certify” that they can, in fact, do a few push ups and sit ups. Like nearly every other entity engaged in hiring, it also allows self-certification elsewhere. Applicants check a box and sign their names, swearing they don’t have a criminal record and can pass a drug test. Far too late, recruiters are realizing some of these applicants can’t do that either. And, again, they’re not finding this out until potential hires are already at the ICE academy and engaged in the training program.That means unvetted applicants are getting access to ICE training materials and wandering through areas most members of the public would be barred from entering. Not that shortening the time frame and lowering physical expectations seems to matter. Flooding the zone with recruits just means resources are being wasted. Nearly half of new recruits who’ve arrived for training at the Federal Law Enforcement Training Center over the past three months were later sent home because they couldn’t pass the written exam, according to the data. But “sent home” isn’t entirely accurate. As both NBC News and the Atlantic report, people who fail to become ICE officers are often routed to other parts of the agency to handle administrative work. But they’re no better trained to do this work, which means the people who can are handling an influx of people who can’t, ensuring everything behind the scenes is going just as poorly as the more visible flame-outs in ICE’s version of basic training.ICE does not have enough guns or vehicles for everyone, and the lack of experience among new hires with booking and processing procedures means they’re not especially helpful for administrative tasks. Other ICE field offices are short of parking spaces and bathroom capacity to accommodate a two- or threefold jump in staffing, a senior official told me. They’ve been told to divide up cubicles and look for additional space to lease.Honestly, I’d expect nothing less than this sort of chaos from this administration. It moves like a wrecking ball covered in chainsaws, carving a wide swath of collateral damage en route to its target — a target that it very often seems to miss. It’s hard to imagine ICE being even worse than it already is, but given these early reports, the ICE we see now might be the best version of itself. And that might be one of the most depressing sentences I’ve ever written. ]]></content:encoded></item><item><title>Once Again, Chat Control Flails After Strong Public Pressure</title><link>https://www.eff.org/deeplinks/2025/10/once-again-chat-control-flails-after-strong-public-pressure</link><author>Thorin Klosowski</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/OG-Encryption-DefendEncryption.png" length="" type=""/><pubDate>Fri, 31 Oct 2025 21:08:45 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Two Windows vulnerabilities, one a 0-day, are under active exploitation</title><link>https://arstechnica.com/security/2025/10/two-windows-vulnerabilities-one-a-0-day-are-under-active-exploitation/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/10/windows-malware-1024x648.jpg" length="" type=""/><pubDate>Fri, 31 Oct 2025 21:03:56 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Two Windows vulnerabilities—one a zero-day that has been known to attackers since 2017 and the other a critical flaw that Microsoft initially tried and failed to patch recently—are under active exploitation in widespread attacks targeting a swath of the Internet, researchers say.The zero-day went undiscovered until March, when security firm Trend Micro said it had been under active exploitation since 2017, by as many as 11 separate advanced persistent threats (APTs). These APT groups, often with ties to nation-states, relentlessly attack specific individuals or groups of interest. Trend Micro went on to say that the groups were exploiting the vulnerability, then tracked as ZDI-CAN-25373, to install various known post-exploitation payloads on infrastructure located in nearly 60 countries, with the US, Canada, Russia, and Korea being the most common.A large-scale, coordinated operationSeven months later, Microsoft still hasn’t patched the vulnerability, which stems from a bug in the Windows Shortcut binary format. The Windows component makes opening apps or accessing files easier and faster by allowing a single binary file to invoke them without having to navigate to their locations. In recent months, the ZDI-CAN-25373 tracking designation has been changed to CVE-2025-9491.]]></content:encoded></item><item><title>Windows 11 Tests Bluetooth Audio Sharing That Connects Two Headsets at Once</title><link>https://tech.slashdot.org/story/25/10/31/1850220/windows-11-tests-bluetooth-audio-sharing-that-connects-two-headsets-at-once?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 20:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft is bringing shared audio to Windows 11, allowing you to stream audio across two pairs of wireless headphones, speakers, earbuds, or hearing aids. From a report: The feature is built using the Bluetooth Low Energy (LE) audio codec, and it's rolling out in preview to Windows 11 Insiders in the Dev and Beta channels. Shared audio comes in handy if you're watching a movie on a laptop with your friend or family member, or just want to show them new music that you can both stream inside your own wireless headsets. You can use shared audio by connecting Bluetooth LE-supported devices to your Windows 11 PC and then selecting the Shared audio (preview) button in your quick settings menu. Microsoft introduced an LE Audio feature on Windows 11 in August, enabling higher audio quality while using a wireless headset in a game or call.]]></content:encoded></item><item><title>Bluesky hits 40 million users, introduces ‘dislikes’ beta</title><link>https://techcrunch.com/2025/10/31/bluesky-hits-40-million-users-introduces-dislikes-beta/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 31 Oct 2025 20:14:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As users "dislike" posts, the system will learn what sort of content they want to see less of. This will help to inform more than just how content is ranked in feeds, but also reply rankings.]]></content:encoded></item><item><title>Coinbase CEO Stunt Exposes Prediction Market Vulnerability</title><link>https://slashdot.org/story/25/10/31/1758205/coinbase-ceo-stunt-exposes-prediction-market-vulnerability?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 20:05:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: When Coinbase's quarterly earnings call wrapped up Thursday, its chief executive, Brian Armstrong, didn't finish with profit guidance or statements of confidence. He closed it out with a list: "Bitcoin, Ethereum, blockchain, staking and Web3." Those weren't random buzzwords. They were part of an $84,000 betting market [non-paywalled source]. 

Across prediction market platforms Kalshi and Polymarket, users had wagered on which words would be spoken during the call -- part of a niche category known as mention markets, where the outcome isn't tied to earnings, price moves or sports games, but to what people say in some public forum. With the final analyst question complete, several terms listed in contracts were still unsaid. Armstrong ticked them off one by one. 

"I was a little distracted because I was tracking the prediction market about what Coinbase will say on their next earnings call," he said in his parting remarks. "I just want to add here the words Bitcoin, Ethereum, blockchain, staking, and Web3 -- to make sure we get those in before the end of the call." The exchange's CEO had just moved a market -- even if only a small one. 

Mention markets are one of the more curious byproducts of the broader prediction market boom, but also one of the more controversial. Platforms like Kalshi, which is regulated by the Commodity Futures Trading Commission, and Polymarket, which is in the process of returning to the US market, let users wager on the outcomes of real-world events. That can mean elections, policy decisions, or sports -- but also, increasingly, corporate rituals and even common jargon.]]></content:encoded></item><item><title>Meta bought 1 GW of solar this week</title><link>https://techcrunch.com/2025/10/31/meta-bought-1-gw-of-solar-this-week/</link><author>Tim De Chant</author><category>tech</category><pubDate>Fri, 31 Oct 2025 19:26:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The social media company inked three deals in the U.S. to power its data centers and offset its carbon footprint.]]></content:encoded></item><item><title>A TikTok Interview Triggered a Securities Filing</title><link>https://tech.slashdot.org/story/25/10/31/1721243/a-tiktok-interview-triggered-a-securities-filing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 19:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Snowflake filed an 8-K with the Securities and Exchange Commission earlier this week after its chief revenue officer gave financial projections in a TikTok video. Mike Gannon told an influencer outside the New York Stock Exchange that the data-storage company would exit the year with just over $4.5 billion in revenue and reach $10 billion in a couple of years. 

The filing stated that Gannon is not authorized to disclose financial information on behalf of the company and that investors should not rely on his statements. Snowflake reaffirmed its August guidance of $.395 billion for fiscal year 2026. The video appeared on an account called theschoolofhardknockz and drew more than 555,000 views on TikTok. Gannon told the interviewer he watches the videos all the time.]]></content:encoded></item><item><title>RFK Jr. Just Made Texas AG Ken Paxton Look Like An Asshole And It’s Hilarious</title><link>https://www.techdirt.com/2025/10/31/rfk-jr-just-made-texas-ag-ken-paxton-look-like-an-asshole-and-its-hilarious/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Fri, 31 Oct 2025 19:19:34 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[When it comes to RFK Jr., I tend not to find much humor in the chaos he creates. The man’s work involves American health and illness, life and death, so it’s just generally not funny. And that holds true to Kennedy’s wielding of incomplete, inaccurate, and unsettled science to go before all of America and declare definitively that acetaminophen, of which Tylenol is the most famous brand, was at least partially the cause for autism spectrum disorder when expecting mothers took it for pain or fever while pregnant. It wasn’t funny when Trump went to the same microphone and did likewise, stating simply “Don’t take Tylenol” if you’re pregnant. It wasn’t funny when Sinclair decided to abuse its airwaves to disserve the public interest (hey, Brendan Carr, over here!) by spreading even more Tylenol disinformation. And it wasn’t funny when Senator Bill Cassidy took to the airwaves to complain about Kennedy’s nonsense when he was a pivotal voice and vote in confirming Captain Brainworm to head HHS in the first place.But, I have to admit, this is very funny. See, Texas AG Ken Paxton, a man who I imagine has a Donald Trump body pillow to cuddle with at night, decided to run with the claims Kennedy and Trump made and has filed a lawsuit against the makers of Tylenol for their “deceptive” marketing and labeling practices for Tylenol.Texas Attorney General Ken Paxton is suing the maker of Tylenol, Kenvue and Johnson & Johnson, who previously sold Tylenol, claiming that they have been “deceptively marketing Tylenol” knowing that it “leads to a significantly increased risk of autism and other disorders.”To back that claim, Paxton relies on the “considerable body of evidence… recently highlighted by the Trump Administration.”Specifically Trump and Kennedy’s claims about Tylenol, actually. See, this is already funny. Here’s one passage from the suit itself, the entirety of which is embedded below.There are many examples of drugs regulated by the Food and Drug Administration that include complete information regarding risks on their labels even when the underlying science—unlike here—is not fully settled. These labels reflect the aims of the regulatory system, which recognizes States’ authority to require warnings to inform consumers of certain risks. Pregnant women should be provided with complete information so that they can make informed decisions regarding the risks to which they expose their unborn children.So, to start with, regulating OTC drug labels is the responsibility of the FDA. If these companies were not properly including warnings on their standardized labels, the remedy for that is getting the FDA involved, which has regulatory teeth and enforcement mechanisms to declare OTC drugs to be “misbranded.” Tylenol has been around for decades. The idea that this hasn’t been an issue for the better part of a century, but now is, is plainly absurd.But I want to pay very close attention to Paxton’s claim that warning labels include risks even when the science isn’t settled, but that this isn’t one of those cases. In this case, according to Paxton, the science settled, making this all the worse.But that’s all kinds of bullshit. The science here isn’t remotely settled. The scientists of those very studies cited by Paxton have complained about how the administration is drawing conclusions from studies that they are very upfront about being inconclusive. None of Kennedy’s data was “new.” It was merely new analysis of old studies.“The causative association between Tylenol given in pregnancy and the perinatal periods is not sufficient to say it definitely causes autism,” Kennedy told reporters. “But it’s very suggestive.”“There should be a cautious approach to it,” he added. “ That’s why our message to patients, to mothers, to people who are pregnant and to the mothers of young children is: Consult your physician.”That’s . And that is very funny. Paxton just got pantsed by the very person he cited in his lawsuit, in which he claimed this was all settled science. Even Kennedy, a man capable of saying outrageous things when it comes to healthcare and science, walked this back. And, in doing so, he tore an enormous hole in the lawsuit that Paxton, a Trump bootlicking sycophant, . Drink that shit in, because it’s delicious.Now, even Kennedy’s walkback is still bad, of course. He’s acting like this is all more sinister and direct of a relationship than there actually is between autism and Tylenol. “It’s very suggestive” is a line without meaning, scientifically. Suggestive to whom? And to what degree? It’s typical Kennedy, taking outlier studies and pretending they mean much more than they do, even as researchers complain about the methodology of those studies, or their inconclusive nature. He did this with his claim that America’s men are suffering greatly from reduced sperm counts, and now he’s doing it here.But in a country that could use a good laugh at the moment, I’m not going to pretend like what he did to Paxton isn’t funny.]]></content:encoded></item><item><title>AI mania tanks CoreWeave’s Core Scientific acquisition — it buys Python notebook Marimo</title><link>https://techcrunch.com/2025/10/31/ai-mania-tanks-coreweaves-core-scientific-acquisition-it-buys-python-notebook-marimo/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 31 Oct 2025 18:53:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[CoreWeave's failed buy of Core Scientific is another sign of an AI bubble. But it's still shopping.]]></content:encoded></item><item><title>10M People Watched a YouTuber Shim a Lock; the Lock Company Sued Him. Bad Idea.</title><link>https://news.slashdot.org/story/25/10/31/1715249/10m-people-watched-a-youtuber-shim-a-lock-the-lock-company-sued-him-bad-idea?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 18:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Trevor McNally posts videos of himself opening locks. The former Marine has 7 million followers and nearly 10 million people watched him open a Proven Industries trailer hitch lock in April using a shim cut from an aluminum can. The Florida company responded by filing a federal lawsuit in May charging McNally with eight offenses. Judge Mary Scriven denied the preliminary injunction request in June and found the video was fair use. 

McNally's followers then flooded the company with harassment. Proven dismissed the case in July and asked the court to seal the records. The company had initiated litigation over a video that all parties acknowledged was accurate. ArsTechnica adds: Judging from the number of times the lawsuit talks about 1) ridicule and 2) harassment, it seems like the case quickly became a personal one for Proven's owner and employees, who felt either mocked or threatened. That's understandable, but being mocked is not illegal and should never have led to a lawsuit or a copyright claim. As for online harassment, it remains a serious and unresolved issue, but launching a personal vendetta -- and on pretty flimsy legal grounds -- against McNally himself was patently unwise. (Doubly so given that McNally had a huge following and had already responded to DMCA takedowns by creating further videos on the subject; this wasn't someone who would simply be intimidated by a lawsuit.) 

In the end, Proven's lawsuit likely cost the company serious time and cash -- and generated little but bad publicity.]]></content:encoded></item><item><title>The World&apos;s Secret Electricity Superusers Revealed</title><link>https://hardware.slashdot.org/story/25/10/31/1646235/the-worlds-secret-electricity-superusers-revealed?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 18:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: The rush to secure electricity has intensified as tech companies look to spend trillions of dollars building data centers. There's an industry that consumes even more power than many tech giants, and it has largely escaped the same scrutiny: suppliers of industrial gases. 

Everyday items like toothpaste and life-saving treatments like MRIs are among the countless parts of modern life that hinge on access to gases such as nitrogen, oxygen and helium. Producing and transporting these gases to industrial facilities and hospitals is a highly energy-intensive process. Three companies -- Linde, Air Liquide and Air Products and Chemicals -- control 70% of the $120 billion global market for industrial gases. Their initiatives to rein in electricity use or switch to renewables aren't enough to rapidly cut carbon emissions, according to a new report from the campaign group Action Speaks Louder. 

"The scale of the sector's greenhouse gas emissions and electricity use is staggering," said George Harding-Rolls, the group's head of campaigns and one of the authors of the report. Linde's electricity use in 2024 exceeded that of Alphabet's Google and Samsung Electronics as well as oil giant TotalEnergies, while the power use of Air Liquide and Air Products was comparable to that of Shell and Microsoft. Yet unlike fossil fuel and tech companies, these industrial gas companies are far from household names because their customers are the world's largest chemicals, steel and oil companies rather than average consumers. 

The industry relies on air-separation units, which use giant compressors to turn air into liquid and then distill it into its many components. These machines are responsible for much of the industry's electricity demand, and their use alone is responsible for 2% of carbon dioxide emissions in China and the US, the world's two largest polluters.]]></content:encoded></item><item><title>Kids Turn Podcast Comments Into Secret Chat Rooms, Because Of Course They Do</title><link>https://www.techdirt.com/2025/10/31/kids-turn-podcast-comments-into-secret-chat-rooms-because-of-course-they-do/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 31 Oct 2025 17:51:56 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[But banning kids from social media has a fundamental problem: kids will find a way. They always do. And part of that is just because kids need those kinds of “third spaces” where they can communicate outside the prying eyes of parents or teachers. And if adults keep blocking off those spaces, kids are smart enough to figure out clever workarounds.Six years ago, when schools blocked social media apps on their networks, students simply repurposed Google Docs—required for assignments—into an improvised social network which they could hide from teachers and parents by claiming they were working on homework:Teens told me they use Google Docs to chat just about any time they need to put their phone away but know their friends will be on computers. Sometimes they’ll use the service’s live-chat function, which doesn’t open by default, and which many teachers don’t even know exists. Or they’ll take advantage of the fact that Google allows users to highlight certain phrases or words, then comment on them via a pop-up box on the right side: They’ll clone a teacher’s shared Google document, then chat in the comments, so it appears to the casual viewer that they’re just making notes on the lesson plan. If a teacher approaches to take a closer look, they can click the Resolve button, and the entire thread will disappear.That was 2019. I’m sure things like that are still happening in Google Docs, but it’s apparently also happening in an even more absurd venue: podcast comments.The latest “How To Do Everything” podcast from NPR featured someone who monitors comments for the TED Radio Hour. She noticed something strange: kids are flooding the comments of random old episodes, turning obscure three-year-old podcasts into makeshift chat rooms where adults won’t think to look.Yeah, so one of my responsibilities on my team is to monitor our Spotify comments. And for the most part, we mostly get really like nice comments or people engaging with our content, giving constructive feedback or saying how much they liked it. But about 3 weeks ago, I noticed kind of a different floodgate situation. And the first instance was only about 20 comments….20 comments on one episode that came out three years ago. Yeah. And all the comments kind of had the same like, “No, you’re so pretty. You’re so pretty.” And I was really trying to rack my brain about the content of this episode 3 years ago to be like, is there a discussion about beauty standards that they are trying to engage with?Yeah. And then about a week later, they struck again, but this time hitting the comments hit into the 90s.And then I kind of felt like, okay, this really needs to be something we’re flagging. And when I brought it up, it seemed like other teams had also been privately sitting on this very odd situation.So the show’s hosts discuss this, and the sense is that they’re using these shows as a space to communicate:GUEST: Yeah, I mean, we definitely can’t say exactly who these people are, why they’re doing this, but my sense is that they’re kids. One of the theories that some other folks have put forward is that maybe this is just a way to get around a classroom phone free situation. Like maybe they can have their laptops out but they can’t have Instagram open or Spotify is the only thing they’re allowed to have. I don’t actually know. It seems like a workaround for sure.HOST: It’s brilliant because like what could be less worrying to a teacher or a parent might be catching, you know, a look at one of these kids’ phones that they’re listening to NPR’s TED Radio Hour with their friends.They ask for an example episode, and indeed, there’s an episode on “What Leadership Looks Like” from 2022 (there’s another one with the same title that might just be a rerun from 2024 which doesn’t have comments) and you can see comments from a few weeks ago that are clearly kids chatting.So, it seems likely that the theory is at least close to correct, that kids are just seeking out places where they can speak freely that  okay to adults at the very same time adults are trying to ban the other spaces where parents think they’ll talk and don’t like it.As the person from TED Radio Hour (unfortunately, her name is not clearly stated and I couldn’t figure out what it was…) notes:I think my sense from digging into it a little bit and following the usernames was effectively they make a playlist that has just one podcast and that podcast becomes kind of the graffiti space I guess of this… popup conversation.To me, this demonstrates some of the futility of trying to ban these spaces. As I mentioned above, kids  these kinds of “third spaces” that are not school and not home in which to communicate more freely with their friends. Because of a variety of moral panics, we’ve closed off many of the real world physical spaces where that could occur, so it was no surprise that many kids gravitated to digital spaces.But now that adults are, again, seeking to close off those spaces, kids appear to be coming up with clever ways to sneak around those bans and keep talking.Of course, the moral panic could always follow them here too. Maybe Australia will ban kids from Spotify comments next. Then Google Docs. Then whatever random corner of the internet kids discover after that. We can keep playing whac-a-mole until we’ve legislated away every possible space where teenagers might talk to each other without adult supervision. At least it’ll feel like we’re doing something.Or—and here’s a thought—we could stop trying to eliminate every space where kids communicate and start teaching them how to navigate those spaces safely. We could recognize that kids need room to talk, to mess up, to figure things out away from constant surveillance. That would require trusting kids to learn, rather than treating every unsupervised conversation as a crisis waiting to happen. But judging by the current trajectory, we’re more likely to see legislation banning carrier pigeons first.]]></content:encoded></item><item><title>Daily Deal: The Ultimate Unity Game Development Bundle</title><link>https://www.techdirt.com/2025/10/31/daily-deal-the-ultimate-unity-game-development-bundle-2/</link><author>Daily Deal</author><category>tech</category><pubDate>Fri, 31 Oct 2025 17:46:56 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The Ultimate Unity Game Development Bundle has 4 courses are designed to designed to teach about game dev and how to create your own games using the Unity engine. You’ll learn about the fundamentals of C# programming, 2D and 3D game development, mobile game development, how to build amazing cutscenes, and more. It’s on sale for $25.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Holography in Cuprates: Critical Review of Quantitative Claims</title><link>https://hackernoon.com/holography-in-cuprates-critical-review-of-quantitative-claims?source=rss</link><author>The Tech Reckoning is Upon Us!</author><category>tech</category><pubDate>Fri, 31 Oct 2025 17:45:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The theories of both, finite- and zero-density, spinons have been extensively discussed in the context of the ’strange metal’ phase in the underdoped cuprates and other (arguably, even stranger) heavy-fermion compounds long before the advent of holography [3]. Once there, the applied holography quickly joined the quest into the properties of this phase that had long evaded a consistent and satisfactory explanation.\
Instead of going after the NFL fermion propagator, however, many of the holographic proposals focused on reproducing the experimental data in the cuprates - and often times even claimed achieving a quantitative agreement.\
In light of its intrinsically unsettled status one would have thought that it might be rather detrimental for any speculative approach to seek out not a mere qualitative but an actual quantitative, down to the number, agreement between its specific predictions and some preselected sets of experimental data. In fact, if such a quantitative agreement were indeed achieved one would have even more explaining to do (first and foremost, as to why an apriori approximate approach appears to be so unexpectedly accurate?).\
The earlier discussion of some of the popular evidence in support of condensed matter holography as well as the debunking of a number of its specific predictions [26] can be found in [34]. However, the admirable persistence with which those predictions continued to be regularly cited in the subsequent holographic literature [35] suggests that the comments of [34] might have had been (most regretfully) overlooked.\
In fact, there is more than a single reason for which semiclassical holography (or its improvement at the level of accounting for the matter back-reaction in the HartreeFock approximation) - thus far, the only practical way of performing the holographic calculations [26–29] - would not have been expected to provide any quantitatively accurate results in the first place. There are, of course, such obvious differences from the string-theoretical holographic constructions as a low physical value of N (which, in practice, often amounts to ’spin up/down’) and the lack of Lorentz, translational, and/or rotational (as well as any super-)symmetries.\
Arguably, though, the most important is the fact that much of the condensed matter physics operates in the intermediate - as opposed to ultra-strong - interaction regime, while it is only the latter that is supposed to have a weakly coupled gravity as its bulk dual [26]. Indeed, most solids form under the condition that its potential (interaction) and kinetic energies on average balance each other out. This suggests that the ’bona fide’ strong-coupling regime could only become attainable in some sort of a ’flat band’ scenario where kinetic energy is completely quenched or, at least, significantly diminished.\
In light of that, it is unsurprising that much of the recent effort towards implementing such mechanism has been centered on the SYK model and its variants [31] whose ’flat band’ nature facilitates the existence of a holographic dual. A viable candidate to this role was proposed in the form of the Jackiw-Teitelboim (JT) dilatonenhanced 1 + 1-dimensional gravity [31].\
It is worth pointing out, though, that at the practical level all the holographic matching between the SYK and JT theories has been, so far, established within their low-energy sectors that are both controlled by a single soft Schwarzian mode (’boundary graviton’). So as far as the low-energy properties of the two models are concerned, they both allow for the same (effectively 0 + 1- dimensional) description in terms of either a fluctuating 1d boundary or Liouvillian-type large-N matrix quantum mechanics [31, 36]. This is not surprising given the intrinsically non-dynamical nature of 2d (and 3d) pure gravity. Such a caveat notwithstanding, the low-energy SYK-JT equivalence has been repeatedly and staunchly referred to as a genuine example of holographic correspondence between the 1+1-dimensional bulk and 0+1-dimensional boundary theories [31].\
As to the general HV models (22) and corresponding vacuum metrics (26), the standard list of observables to be matched includes temperature-dependent specific heat\
and frequency-dependent optical conductivity\
determined by the bare scaling dimensions.\
Incidentally, this value of the HV parameter was previously singled out on the basis of analyzing entanglement entropy [28]. Besides, it suggests the interpretation of d − θ as an effective number of dimensions orthogonal to the FS.\
The other frequently invoked relation [26, 28, 29] is\
in which case the first inequality in (27) is marginally satisfied as equality. Notably, in 2d it would only be consistent with (40) for z = 3/2.\
Also, from the beginning of the cuprates saga an even greater fixation has always been on the linear-T dependence of resistivity, also observed in a variety of other materials [35]. Of course, the conductivity scaling with frequency (39) does not readily translate into its temperature dependence, as it would be determined by a specific mechanism of momentum relaxation (i.e., Umklapp, phonons, and/or disorder).\
To this end, the use of the memory matrix technique yielded a proper conductivity scaling [26, 35] in both limits of strong,\
momentum-non-conserving scattering where ∆ is the dimension of the leading translation invariance-breaking 8 operator. The formulas (42) and (43) agree for ∆ = z + (d − θ)/2 which condition coincides with that of marginal fulfillment of the Harris criterion for the disorder scattering to become a relevant perturbation.\
An alternate interpretation of the linear-T resistivity, σ(T ) ∼ 1/T , proposed in [26, 35] relates it to the FL-like entropy, S(T ) ∼ C(T ) ∼ T . This school of thought introduces the notion of inelastic ’Planckian’ scattering rate as a potentially single most important scale for thermalization/equilibration/information scrambling (albeit not a rate of momentum relaxation) in strongly interacting systems\
Interestingly, it is the (admittedly, unphysical) model of [38] that so far has managed to reproduce a longer list of the power-law dependencies found in the cuprates, as compared to the competing schemes [39]. Unfortunately, such a serendipitous success does not offer any immediate insight into the underlying mechanism of the NFL behavior in the cuprates.\
Furthermore, contrasting the large-r and -τ asymptotics (31) of the HV holographic propagators against their eikonal/bosonization counterparts in search of some agreement suggests finite positive values of θ, contrary to the ’Planckian’ scenario. This observation might further reduce the chances of constructing a consistent HV holographic model of the strange metal phase in the cuprates.\
In part, the deficiencies of the HV-based approach have been circumvented by the arrival of the ’second SYK wave’ [40] which utilizes the Hamiltonian obtained from the conventional combination of a kinetic (quadratic) and interaction (quartic) terms by randomizing the amplitudes of either one or both of these terms a la SY K. Making such randomization spatially non-uniform one opens a channel for non-conservation of momentum which then gives rise to the linear-T ’Planckian’ rate (on top of a constant).\
Of course, the very existence of different explanations (cf., for example, [35, 39] and [40]) for certain scaling laws observed in the cuprates may suggest that their ultimate interpretation is yet to be found. It would be, therefore, imperative to strive to extend the list of matching properties, akin to [38, 39] as the means of discriminating between the competing schemes.(1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.]]></content:encoded></item><item><title>Behind the Blog: Ray-Bans Are No Longer Cool or Timeless</title><link>https://www.404media.co/behind-the-blog-ray-bans-are-no-longer-cool-or-timeless/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/10/Untitled-design.png" length="" type=""/><pubDate>Fri, 31 Oct 2025 17:33:23 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[This is Behind the Blog, where we share our behind-the-scenes thoughts about how a few of our top stories of the week came together. This week, we discuss keeping FOIA reporting in front of a paywall, Ray-Bans, and what pregnate Schoolhouse Rock bills say about our current AI-driven hellscape.Yesterday I did a livestreamed event with Freedom of the Press Foundation and WIRED. It was called Unpaywalled: The case for making public records-based reporting free and you can . As you might know, we made a decision very early on with 404 Media, I think in the first week maybe, to not paywall our Freedom of Information Act (FOIA) reporting. There are a few reasons, but the main one simply is that with public records, we think people should be able to see those records without paying. It’s like a government agency publishing certain databases, or census data, or whatever. These are public records and should be published or re-published as such.]]></content:encoded></item><item><title>Tattd gave four TechCrunch writers tattoos at Startup Battlefield</title><link>https://techcrunch.com/2025/10/31/tattd-gave-four-techcrunch-writers-tattoos-at-startup-battlefield/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Fri, 31 Oct 2025 17:32:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tattd, a marketplace for tattoo-seekers and artists, set up a mini tattoo parlor in the Expo Hall at TechCrunch Disrupt 2025.]]></content:encoded></item><item><title>Hackers threaten to leak data after breaching University of Pennsylvania to send mass emails</title><link>https://techcrunch.com/2025/10/31/hackers-threaten-to-leak-data-after-breaching-university-of-pennsylvania-to-send-mass-emails/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Fri, 31 Oct 2025 17:30:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As the hackers plainly stated in their message ("Please stop giving us money"), this breach appears motivated to suppress alumni donations. ]]></content:encoded></item><item><title>FDA Clears Way For Faster Personalized Gene Editing Therapy</title><link>https://news.slashdot.org/story/25/10/31/1622222/fda-clears-way-for-faster-personalized-gene-editing-therapy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A top United States regulator plans to unveil a faster approach to approving custom gene-editing treatments, a move designed to unleash a wave of industry investment that will yield cures for patients with rare diseases. From a report: Vinay Prasad, who oversees gene therapies at the Food and Drug Administration, said scientific advances, like Crispr, have forced the agency to relax some of its strict rules. As an example, he cited the case of 10-month-old KJ Muldoon, who this year became the first person in history to have his genes custom edited to cure an inherited disease. 

"Regulation has to evolve as fast as science evolves," Prasad said in an interview with Bloomberg News. The agency is "going to be extremely flexible and work very fast with the scientists who want to bring these therapies to kids who need it." Prasad plans to publish a paper in early November outlining the FDA's new approach. He predicted it will spark interest in developing treatments for conditions that may affect only a handful of people.]]></content:encoded></item><item><title>AWS exceeds Wall Street’s expectations as demand for cloud infra remains high</title><link>https://techcrunch.com/2025/10/31/aws-exceeds-wall-streets-expectations-as-demand-for-cloud-infra-remains-high/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Fri, 31 Oct 2025 16:59:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AWS continues to see strong demand as companies gobble up its cloud infrastructure services in the age of AI. ]]></content:encoded></item><item><title>Government hackers breached telecom giant Ribbon for months before getting caught</title><link>https://techcrunch.com/2025/10/31/government-hackers-breached-telecom-giant-ribbon-for-months-before-getting-caught/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Fri, 31 Oct 2025 16:45:38 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Ribbon, which provides software and technology to phone and internet giants, said nation-state hackers were in its systems since at least December 2024.]]></content:encoded></item><item><title>Google Working on Bare-Bones Maps That Removes Almost All Interface Elements and Labels</title><link>https://tech.slashdot.org/story/25/10/31/1527233/google-working-on-bare-bones-maps-that-removes-almost-all-interface-elements-and-labels?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 16:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google Maps is testing a power saving mode in its latest Android beta release that strips the navigation interface to its bare essentials. The feature transforms the screen into a monochrome display and removes nearly all UI elements during navigation, according to AndroidAuthority. 

Users discovered code strings in version 25.44.03.824313610 indicating the mode activates through the phone's physical power button rather than through any in-app menu. The stripped-down interface eliminates standard map labels and appears to omit even the name of the upcoming street where drivers need to turn. The mode supports walking, driving, and two-wheeler directions but currently cannot be used in landscape orientation.]]></content:encoded></item><item><title>DHS, White House Shrug Off Having Their Social Media Lies Pointed Out To Them</title><link>https://www.techdirt.com/2025/10/31/dhs-white-house-shrug-off-having-their-social-media-lies-pointed-out-to-them/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 31 Oct 2025 16:18:34 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The administration knows it’s lying.  know it’s lying.  knows that  know it’s lying. And it doesn’t care. The MAGA faithful don’t care that they’re being lied to pretty much all the time. All they care about is how much the messaging aligns with their bigotry and hatred. Everyone else who spots the lies can feel free to point them out, but this administration is living its best life in the post-truth era it has crafted for itself.The DHS and its underling agencies are fully engaged in lying pretty much all of the time. This concerted effort to avoid the truth has even given Trump  pause. Sure, it was only a momentary lapse into reason, but earlier this year (right before returning to his plans to invade Portland, Oregon) Trump briefly wondered if he was being misled by the liars he chose to employ. For an extremely tiny amount of time, Trump thought  might be the sucker at the table.That momentary flash of introspection has yet to return. Constant lying is the name of the game. The administration can’t even be bothered to engage in even the most implausible of deniability when it gets caught lying. There’s no one in the administration blaming the misinformation/disinformation flowing from it on interns or hacked accounts or simply failing to have vetted a social media post before it was posted.Officials in President Donald Trump’s administration have used […] misleading footage in at least six videos promoting its immigration agenda shared in the last three months, a Washington Post analysis found, muddying the reality of eventsin viral clips that have been viewed millions of times.Some videos that purported to show the fiery chaos of Trump-targeted cities included footage from completely different states. One that claimed to show dramatic examples of past administrations’ failures instead featured border crossings and smuggling boats recorded during Trump’s first term.This is more of the same, except it’s being done by federal employees and officials, rather than by the faux journalists of the Trump-loving Fox News network. When confronted with these facts, the spokespeople for the DHS and the White House refused to acknowledge the misleading nature of these posts. Instead, they pretended no one had pointed out anything wrong and delivered talking points that talked around what the WaPo journalists were specifically asking them about.The DHS’s Hanoi Hannah — Tricia McLaughlin — pretended the only problem here was people committing crimes, no matter where or when those crimes were committed.DHS spokeswoman Tricia McLaughlin did not dispute the errors or explain what had happened but said the videos were a small percentage of the more than 400 that the agency has posted this year. “Violence and rioting against law enforcement is unacceptable regardless of where it occurs,” she said.It’s not just the DHS. It’s also the White House itself.The White House has made notable errors in its own video operation, posting a video this month that claimed “Chicago is in chaos” and said the city “doesn’t need political spin — it needs HELP.”The video, however, recycled footage from a months-old ICE operation in Florida, not far from Trump’s Mar-a-Lago Club. A fact-checker at Agence France-Presse also found other clips in the video had come from operations in Arizona, California, Nebraska, South Carolina and Texas, some of which had been recorded during President Joe Biden’s time in office.Trust me, these aren’t errors. These are deliberate attempts to mislead the people who view these social media posts.And these attempts to mislead were tacitly admitted by the White House. Its spokesperson dodged the accusations, preferring to simply state: we’re doing it for the clicks.Abigail Jackson, a spokeswoman there, did not comment on the errors but said “the Trump administration will continue to highlight the many successes of the president’s agenda through engaging content and banger memes on social media.”Not a great response! But it plays to the only crowd that matters: Donald Trump. It plays extremely well with the die hard members of his voting bloc — the ones who will wave the Trump flag no matter what he does. It plays well with the rest of the administration, which is as fully cooked as Trump himself. On top of that, there’s a completely cooked Supreme Court waiting to issue shadow docket hand-waves the next time the administration tramples all over people’s rights. After all, it’s fine with using skin color and/or a “Mexican” accent as the basis for a stop, if not an actual arrest.The DHS can splash whatever it wants all over its social media pages because the only people who care about the truth are those who want to see Trump ejected from office as soon as possible. For everyone else, the truth is just something that gets in the way of a good story. ]]></content:encoded></item><item><title>Holographic Propagators: Geodesics and Local Criticality</title><link>https://hackernoon.com/holographic-propagators-geodesics-and-local-criticality?source=rss</link><author>The Tech Reckoning is Upon Us!</author><category>tech</category><pubDate>Fri, 31 Oct 2025 16:15:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The early holographic studies of fermion propagators [28] produced a number of intriguing results, including multiple Fermi surfaces (which merge into one critical ’Fermi ball’ in some extreme limits), dispersionless poles, and oscillatory frequency dependence (which was later shown not to arise in more systematic ’top down’ constructions [26]), etc. A physical interpretation of those results is impeded by the fact that much of this work is numerical.\
A simple and amenable to analytical treatment semiclassical calculation can be performed in the regime mL ≫ 1 where m is a mass of the conjectured dual bulk fermion [28, 29]. In this regime, the fermion’s paths contributing to various quantum-mechanical amplitudes follow closely the classical boundary-to-boundary trajectories (geodesics) derived from the (imaginary-time) action\
by varying over τ(u) and r(u).\
Evaluating this action on its geodesic one obtains\
While an explicit analytic computation of (29) can only be performed in some special cases, the one-parameter space/time dependencies can be readily found for a broad variety of metrics. Specifically, for the HV metric (26) one obtains [29, 30]\
Notably, in the absence of hyperscaling violation (θ = 0) both these asymptotics become either constant (less likely) or logarithmic (more likely, see below). Thus, if the classical EMD Lagrangian (22) were to represent a valid bulk dual of a boundary theory with the gauge-like interaction (1) the asymptotics (31) would not be readily reconcilable with the eikonal/bosonization results (11,21) which depend primarily on z (via η) rather than θ.\
and is composed of the two independent solutions which read\
Imposing the proper boundary conditions and following the holographic dictionary [26] one then defines the propagator as a reflection coefficient for the wave incident at the boundary\
A different behavior (unattainable in the case of a HV metric (26) with finite z and θ) occurs for α = β + 1 in which case the integral in (33) diverges at u → 0. This peculiar NFL regime, dubbed ’local criticality’, is characterized by the propagator\
where a(k), b(k), and ν(k) ∼ k are non-singular functions of momentum that can, in general, produce multiple poles identified as the distinct (’fractionalized’) FS [28].\
Fourier transforming (36) is complicated by the fact that G(ω, k) is not analytically known across the entire range of its arguments. However, the fast (and/or furious) Fourier transformation via a saddle point suggests the following form of this function in the spacetime domain\
Adding to the intrigue, there are some recent Monte Carlo results on the 2d Hubbard and t − J models that have long been thought to represent the prototypical NFL normal state in the cuprates. These results do not readily conform to a momentum-independent, yet strongly energy-dependent, self-energy function, showing less of energy/temperature dependence than any of the above expressions [33]. It remains to be seen as to what this might imply for the general applicability of the theories of fermions (’spinons’) governed by the interactions (1) to the analysis of those microscopic models.(1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.]]></content:encoded></item><item><title>KosmicKrisp Now Vulkan 1.3 Compliant For Apple Devices</title><link>https://www.phoronix.com/news/KosmicKrisp-Vulkan-1.3</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 16:14:40 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Over the summer months LunarG announced KosmicKrisp as a new Vulkan-on-Metal implementation for Apple devices and built around Mesa. That alternative to MoltenVK was upstreamed for next quarter's Mesa 26.0 release and now it's also celebrating being an officially Vulkan 1.3 conformant implementation...]]></content:encoded></item><item><title>The HackerNoon Newsletter: The Road to Hell is Paved with Good DRY Intentions (10/31/2025)</title><link>https://hackernoon.com/10-31-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Fri, 31 Oct 2025 16:03:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, October 31, 2025?By @melvin-manni [ 5 Min read ] Learn how good intentions can lead to spaghetti dry code, over abstraction and over engineered systems.  Read More.By @salkimmich [ 15 Min read ] The evolution of workload identity: Kerberos to X.509 to SPIFFE to TWI. Why credentials should expire faster than your containers run. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>You Can&apos;t Refuse To Be Scanned by ICE&apos;s Facial Recognition App, DHS Document Says</title><link>https://news.slashdot.org/story/25/10/31/1515258/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 16:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Immigration and Customs Enforcement (ICE) does not let people decline to be scanned by its new facial recognition app, which the agency uses to verify a person's identity and their immigration status, according to an internal Department of Homeland Security (DHS) document obtained by 404 Media. The document also says any face photos taken by the app, called Mobile Fortify, will be stored for 15 years, including those of U.S. citizens. 

The document provides new details about the technology behind Mobile Fortify, how the data it collects is processed and stored, and DHS's rationale for using it. On Wednesday 404 Media reported that both ICE and Customs and Border Protection (CBP) are scanning peoples' faces in the streets to verify citizenship. 

"ICE does not provide the opportunity for individuals to decline or consent to the collection and use of biometric data/photograph collection," the document, called a Privacy Threshold Analysis (PTA), says. A PTA is a document that DHS creates in the process of deploying new technology or updating existing capabilities. It is supposed to be used by DHS's internal privacy offices to determine and describe the privacy risks of a certain piece of tech. "CBP and ICE Privacy are jointly submitting this new mobile app PTA for the ICE Mobile Fortify Mobile App (Mobile Fortify app), a mobile application developed by CBP and made accessible to ICE agents and officers operating in the field," the document, dated February, reads. 404 Media obtained the document (which you can see here) via a Freedom of Information Act (FOIA) request with CBP.]]></content:encoded></item><item><title>Perplexity strikes multi-year licensing deal with Getty Images</title><link>https://techcrunch.com/2025/10/31/perplexity-strikes-multi-year-licensing-deal-with-getty-images/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Fri, 31 Oct 2025 15:46:14 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Perplexity’s agreement with Getty appears to legitimize some of the startup’s previous use of Getty’s stock photos. Perplexity came under fire last year for a series of plagiarism accusations from several news organizations. ]]></content:encoded></item><item><title>Video Friday: Happy Robot Halloween!</title><link>https://spectrum.ieee.org/video-friday-robot-halloween-2674252642</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk5MzIzNy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgwMjU4MDA0M30.nBNXvmaVwqodlAcXGpck-A2yCbWsmFT6eVePaIA2W5Y/image.png?width=600" length="" type=""/><pubDate>Fri, 31 Oct 2025 15:30:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your weekly selection of awesome robot videos]]></content:encoded></item><item><title>The Department of Defense Wants Less Proof its Software Works</title><link>https://www.eff.org/deeplinks/2025/10/department-defense-wants-less-proof-its-software-works</link><author>Matthew Guariglia</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/flag-surveillance-color.jpg" length="" type=""/><pubDate>Fri, 31 Oct 2025 15:29:03 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Affinity&apos;s Image-Editing Apps Go &apos;Freemium&apos; in First Major Post-Canva Update</title><link>https://tech.slashdot.org/story/25/10/31/1411210/affinitys-image-editing-apps-go-freemium-in-first-major-post-canva-update?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 15:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[ArsTechnica: When graphic design platform-provider Canva bought the Affinity image-editing and publishing apps early last year, we had some major questions about how the companies' priorities and products would mesh. How would Canva serve the users who preferred Affinity's perpetually licensed apps to Adobe's subscription-only software suite? And how would Affinity's strong stance against generative AI be reconciled with Canva's embrace of those technologies. 

This week, Canva gave us definitive answers to all of those questions: a brand-new unified Affinity app that melds the Photo, Designer, and Publisher apps into a single piece of software called "Affinity by Canva" that is free to use with a Canva user account, but which gates generative AI features behind Canva's existing paid subscription plans ($120 a year for individuals). 

This does seem like mostly good news, in the near to mid term, for existing Affinity app users who admired Affinity's anti-AI stance: All three apps' core features are free to use, and the stuff you're being asked to pay for is stuff you mostly don't want anyway. But it may come as unwelcome news for those who like the predictability of pay-once-own-forever software or are nervous about where Canva might draw the line between "free" and "premium" features down the line. 

[...] There's now a dedicated page for the older versions of the Affinity apps, and an FAQ at the bottom of that page answers several questions about the fate of those apps. Affinity and Canva say they will continue to keep the activation servers and downloads for all Affinity v1 and v2 apps online for the foreseeable future, giving people who already own the existing apps a way to keep using the versions they're comfortable with. Users can opt to link their Serif Affinity store accounts to their new Canva accounts to access the old downloads without juggling multiple accounts. But those older versions of the apps "won't receive future updates" and won't be able to open files created in the new Canva-branded Affinity app.]]></content:encoded></item><item><title>Tim Cook says Apple is open to M&amp;A on the AI front</title><link>https://techcrunch.com/2025/10/31/tim-cook-says-apple-is-open-to-ma-on-the-ai-front/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 31 Oct 2025 15:17:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple CEO Tim Cook noted in the company's Q4 2025 earnings call that Apple was preparing to announce more AI partnerships like the one it has with OpenAI to integrate ChatGPT into Siri and Apple Intelligence.]]></content:encoded></item><item><title>Luminar is cutting jobs, losing its CFO, and warning of a cash shortage</title><link>https://techcrunch.com/2025/10/31/luminar-is-cutting-jobs-losing-its-cfo-and-warning-of-a-cash-shortage/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 31 Oct 2025 15:00:49 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The new turmoil comes as founder Austin Russell is trying to buy the company just a few months after being replaced as CEO.]]></content:encoded></item><item><title>The Ubiquitous NFL Problem: Comparing Bosonization, Eikonal, and Holographic Techniques</title><link>https://hackernoon.com/the-ubiquitous-nfl-problem-comparing-bosonization-eikonal-and-holographic-techniques?source=rss</link><author>The Tech Reckoning is Upon Us!</author><category>tech</category><pubDate>Fri, 31 Oct 2025 15:00:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Compared to what it has been just recently [26], the seemingly endless flurry of holographic publications in JHEP, PRD, and other traditional ’condensed matter oriented’ venues has been steadily coming to a mere trickle. Those few holographic exercises that do occasionally pop out still tend to begin with the mantra ’holography is well known to be an established method for studying strongly correlated systems’. However, this optimistic reassurance often appears to be in a rather stark contrast with the typical summary that sounds more like ’as no unambiguous agreement with experiment was found, the problem is left to future work’.\
Also, much of the original thrust towards boldly treating an arbitrary condensed matter system of interest as yet another application of some opportunistically chosen weakly-coupled semiclassical gravity has retreated into a ’safer-haven’ topic of hydrodynamics (which, while highlighted and revitalized by holography, can be - and of course had long been - successfully discussed without ever mentioning the latter).\
On the outside, it may seem as though the heuristic ’holo-hacking’ (a.k.a. ’bottom up’ or ’non-AdS/nonCFT’) approach tends to pick out its favorite gravity-like bulk theory on the basis of such physically compelling reasons as an existence of the previously found classical solutions and normal modes’ spectra, availability of the numerical simulation software, or mere need to engage students with the tangible computational tasks.\
However, apart from having become a massive and customary practice, there hasn’t been much effort made towards any serious justification of neither the overall holographic scheme, nor its specific ’dictionary’ which was copy-pasted from the original string-theoretical framework. In that regard, it might be worth keeping in mind that just because everyone else on a highway may be driving above the posted speed limit does not by itself make it legal.\
In light of the above, comparing holographic propagators to the predictions of other techniques could provide an additional testing ground for, both, the alternate methods as well as the holographic approach itself. the bulk metric, gauge, and scalar (dilaton) fields [26]\
Among all the classical solutions of the theory (22) there is a special class of Lifshitz metrics (θ = 0) which were discovered in the semiclassical (ThomasFermi) analysis of matter back-reaction on the metric, as well as in the ’electron star’ scenarios, etc. [27].\
More generally, any viable solutions of (22) must obey certain stability (’null energy’) conditions [26](1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.]]></content:encoded></item><item><title>Amazon CEO Says Massive Corporate Layoffs Were About Agility - Not AI or Cost-Cutting</title><link>https://slashdot.org/story/25/10/31/1358220/amazon-ceo-says-massive-corporate-layoffs-were-about-agility---not-ai-or-cost-cutting?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Amazon CEO Andy Jassy says the company's latest big round of layoffs -- about 14,000 corporate jobs -- wasn't triggered by financial strain or AI replacing workers, but rather a push to stay nimble. From a report: Speaking with analysts on Amazon's quarterly earnings call Thursday, Jassy said the decision stemmed from a belief that the company had grown too big and too layered. "The announcement that we made a few days ago was not really financially driven, and it's not even really AI-driven -- not right now, at least," he said. "Really, it's culture." 

Jassy's comments are his first public explanation of the layoffs, which reportedly could ultimately total as many as 30,000 people -- and would be the largest workforce reduction in Amazon's history. The news this week prompted speculation that the cuts were tied to automation or AI-related restructuring. Earlier this year, Jassy wrote in a memo to employees that he expected Amazon's total corporate workforce to shrink over time due to efficiency gains from AI. But his comments Thursday framed the layoffs as a cultural reset aimed at keeping the company fast-moving amid what he called "the technology transformation happening right now."]]></content:encoded></item><item><title>In a First, AI Models Analyze Language As Well As a Human Expert</title><link>https://www.quantamagazine.org/in-a-first-ai-models-analyze-language-as-well-as-a-human-expert-20251031/</link><author>Steve Nadis</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2025/10/Metalinguistics-cr-Robert-Neubecker-Default.webp" length="" type=""/><pubDate>Fri, 31 Oct 2025 14:28:12 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[Among the myriad abilities that humans possess, which ones are uniquely human? Language has been a top candidate at least since Aristotle, who wrote that humanity was “the animal that has language.” Even as large language models such as ChatGPT superficially replicate ordinary speech, researchers want to know if there are specific aspects of human language that simply have no parallels in the…]]></content:encoded></item><item><title>YC alum Adam raises $4.1M to turn viral text-to-3D tool into AI copilot</title><link>https://techcrunch.com/2025/10/31/yc-alum-adam-raises-4-1m-to-turn-viral-text-to-3d-tool-into-ai-copilot/</link><author>Anna Heim</author><category>tech</category><pubDate>Fri, 31 Oct 2025 14:20:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After generating over 10 million social media impressions with the launch of its text-to-3D model app, Adam has raised a $4.1 million seed round to power its next steps.]]></content:encoded></item><item><title>Saving 20+ Hours a Week: How Jamie I.F. Built AffiliateFinder.ai to Automate Affiliate Recruitment</title><link>https://hackernoon.com/saving-20-hours-a-week-how-jamie-if-built-affiliatefinderai-to-automate-affiliate-recruitment?source=rss</link><author>NewsByte.Tech</author><category>tech</category><pubDate>Fri, 31 Oct 2025 14:17:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Affiliate and influencer programs is one of the MOST profitable revenue channels for SaaS businesses - but everyone complains about how hard it is to find good affiliates and influencers to promote them. It takes a LOT of manual work.So, we save you 20+ hours per week by finding every affiliate currently promoting your competitors so you can recruit them, as well as all the top influencers and creators in your niche to build partnerships with.3. What do you love about your team, and why are you the ones to solve this problem?We’re a team of people who all have a LOT of experience in affiliate marketing - either as affiliate marketers ourselves, or as affiliate managers. We get the industry.We know how time-consuming it is to search for new affiliates all the time - and we literally built AffiliateFinder.ai to solve our own problem while running our affiliate management agency! We know what we want in a software like this, so we feel we know exactly how to build this to solve other people having similar problems.4. If you weren’t building your startup, what would you be doing?Probably building a different startup! I really enjoy the game, and I wouldn’t trade it for anything.5. At the moment, how do you measure success? What are your metrics?Product feedback, customer growth, and retention. If we don’t have a good product, people will not complete their 7-day free trial, and we won’t grow customers or revenue. Everything we’re focused on now is around retention, and not just building something they’ll use once to get a list of affiliates, but how to make this incredibly helpful as an ongoing companion for all your partner management work.6. In a few sentences, what do you offer to whom?AffiliateFinder.ai helps affiliate, influencer, and partnerships teams 3x their affiliate recruitment by finding all the best-fit potential affiliates for them - and ordering them by priority so they can start with the absolute top partners.We save you 20+ hours per week of boring, manual research - freeing you up to focus on building those relationships with affiliates so they can send you sales while you sleep.It works great, no matter what type of business you are: B2B SaaS, AI, DTC/ecom, travel, iGaming, fintech and trading - we have many customers across all types of businesses.7. What’s most exciting about your traction to date?We’re used by several of the largest companies in the world. It’s always validating when multi-billion dollar companies use your tool and find it valuable enough to use and pay for!And, surprisingly, some of our first customers were forward-thinking managers at these huge companies - they saw our product and instantly understood how it could help them scale their affiliate revenue and outcompete their competition.8. Where do you think your growth will be next year?We’d like to reach 2,000 paying customers by the end of next year, and we’re building to hit that right now.If we can help those 2,000 brands reach their goals by recruiting more partners - then we’ll be extremely happy. We have a lot of great products and features we want to build out, and we’re very excited to build these for the world.9. Tell us about your first paying customer and revenue expectations over the next year.It took us a while to get our first paying customer - a good few months from launch.We originally launched with a freemium option where you’d get your first 15 affiliates for free, and then you had to upgrade yourself to get the full version. But nobody was upgrading!Once we switched to a 7-day free trial, the customers started rolling in.10. What’s your biggest threat?We’re an AI-powered tool, and we use AI to filter out bad-fit affiliates - and recommend the good fits. But, naturally, if there is an extremely advanced AI that can eventually do this all, then this is a huge threat to us. But, we’re working on custom data to make us the most useful tool in affiliate marketing. That hopefully keeps our competitive advantage as AI improves.]]></content:encoded></item><item><title>How Construct Koin Plans to Bridge a $300 Trillion Market Gap in Real Estate Financing</title><link>https://hackernoon.com/how-construct-koin-plans-to-bridge-a-$300-trillion-market-gap-in-real-estate-financing?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Fri, 31 Oct 2025 14:03:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Can blockchain technology solve century-old problems in real estate financing, or does it represent another attempt to apply a solution in search of a problem?The question becomes more pressing as Construct Koin launches a presale that aims to raise $100 million by offering tokens that start at $0.10 and scale to $1 across 10 phases. The project positions itself as a Real Estate Financing (ReFi) protocol that will modernize how capital flows into property development, but the proof will depend on whether it can deliver where others have failed.\
The timing appears calculated. The tokenized real-world asset market crossed $30 billion in 2025, a figure that reflects roughly a 10-fold increase from 2022 levels. Private credit accounts for approximately $17 billion of this total, while U.S. Treasuries make up $7.3 billion. The momentum suggests that institutions are finding utility in blockchain infrastructure for specific use cases, particularly those involving yield-bearing assets with standardized documentation.Understanding ReFi and What Construct Koin Actually DoesReal Estate Financing through blockchain differs from the property tokenization projects that dominated headlines in previous cycles. Instead of selling fractional ownership in buildings, ReFi protocols focus on the financing process itself. Think of it as digitizing the loan origination and management workflow rather than the asset title.\
Construct Koin operates by connecting property developers who need capital with investors who provide it through token purchases. The platform claims to use artificial intelligence integrated with Building Information Management (BIM) software to analyze development proposals and make lending decisions in hours rather than the weeks or months typical in traditional finance. According to the project website, loans are secured by legal charges registered on title deeds with HM Land Registry in the UK, providing a layer of protection through real property collateral.\
The mechanics work through a loan book model. When developers borrow funds, they pay interest and profit shares back to the protocol. CTK token holders who stake their tokens receive 8 to 12% annual percentage rates paid in USDT stablecoin. The protocol claims it currently has £15 million of assets already secured on-chain, though independent verification of this figure through public blockchain explorers remains limited in available documentation.Breaking Down the $100 Million Presale StructureThe fundraising approach spans 10 phases, each with a price increment. The first phase opened at $0.10 per token, and the final phase will close at $1.00 per token at the Token Generation Event (TGE). The model resembles how venture capital rounds work, with later participants paying higher prices than earlier ones. Out of the 1 billion total token supply, 400 million tokens have been allocated to the presale.\
This represents 40% of the total supply going to public participants. Another 15% has been earmarked for staking rewards, 20% for ecosystem growth, 15% for team and advisors, and 10% for liquidity and reserves. The vesting schedules for team tokens matter here, though specific timeframes were not detailed in the publicly available documentation. Projects that allow insiders to sell immediately after launch have historically faced selling pressure that can depress token prices.\
The presale accepts payments through both fiat channels (credit cards, bank transfers) and six major cryptocurrency networks including Ethereum, Bitcoin, Solana, Polygon, and Binance Smart Chain. Minimum purchase amounts vary by network due to transaction fee structures. Ethereum requires a $100 minimum due to higher gas fees, while networks like Polygon and Solana allow $10 minimum purchases. The tokens will remain locked until TGE, which the project estimates will occur 12 to 24 months from launch.The Technology Stack and AI ClaimsThe project emphasizes its use of AI for underwriting decisions. Traditional property development loans can take weeks or months to process as lenders manually review business plans, financial projections, and construction documents. Construct Koin claims its system achieves a 95% speed improvement by automating this analysis through machine learning models that assess risk based on multiple data points.\
The integration with BIM systems provides the AI with access to architectural plans, material specifications, and construction schedules. In theory, this allows the algorithm to evaluate whether a project is feasible, properly costed, and likely to complete on time. The platform processes applications and provides offers in hours rather than weeks, according to marketing materials. However, the details about which specific AI models are being used, what training data they rely on, and how they handle edge cases remain undisclosed.\
The technical infrastructure runs on Ethereum as an ERC-20 token. The smart contracts are described as audited, though the names of the audit firms and links to audit reports were not prominently featured in the reviewed materials. The choice of Ethereum provides compatibility with existing DeFi infrastructure and wallet solutions, but it also means users will contend with network congestion and variable transaction fees unless they utilize Layer 2 solutions.Market Context and the RWA SurgeUnderstanding where Construct Koin fits requires context about the broader real-world asset tokenization movement. The sector experienced 380% growth over three years, reaching $24 billion by mid-2025 according to a report by RedStone, Gauntlet, and RWA.xyz. This represents a shift from experimental pilots to scaled institutional adoption, particularly in fixed income and private credit categories.\
Major financial institutions have entered the space. BlackRock launched a $2.9 billion tokenized fund (BUIDL), while Franklin Templeton's tokenized money market fund represents $420 million in assets. Goldman Sachs partnered with BNY Mellon to tokenize money market funds, supported by regulatory frameworks that aim to streamline settlement and reduce costs. The institutional involvement provides validation that blockchain infrastructure can solve real operational problems in capital markets.Who is Building This and Corporate StructureChris Baldrey-Chouro serves as CEO and founder of Construct Koin. According to a podcast interview on The Crypto Podcast, Baldrey-Chouro describes the project as executing "one of the most innovative fundraising strategies in crypto history." His background includes work in recruitment and staffing solutions for commerce, based on corporate registry information.\
This multi-jurisdictional setup is common among crypto projects seeking to optimize for regulatory environments while maintaining operational flexibility. The UK entity provides legitimacy through Companies House registration and operates under UK corporate law. The BVI structure offers advantages for token operations, while the Dubai presence targets the Middle East market, which represents over $2 trillion in real estate value according to project materials.The project positions itself as compliance-first, a pitch aimed at institutional allocators who need legal clarity before committing capital. The protocol includes KYC and AML requirements for all investors, with enhanced due diligence for purchases exceeding $10,000. This approach contrasts with many DeFi protocols that operate pseudonymously or with minimal identity verification.\
The emphasis on milestone-driven disbursements and oracle-verified events addresses one of the key concerns institutional investors have about blockchain-based financing. Traditional tranche financing releases funds only after borrowers meet specific milestones such as completing foundation work or reaching specific construction stages. Construct Koin claims its smart contracts replicate this structure by releasing capital only after verification of progress, reducing the risk of fund misuse.\
The security model relies on conservative loan-to-value ratios of 60 to 70%. This means if a developer defaults, the property securing the loan should be worth significantly more than the outstanding debt, allowing the protocol to recover funds through foreclosure and sale. The protocol also maintains an insurance vault funded by a portion of fees to cover defaults beyond normal parameters. Whether these mechanisms will perform as designed during an actual default scenario remains untested.The Presale Risk Landscape in 2025\
Common red flags in presales include anonymous teams, unrealistic return promises, poorly written whitepapers, unclear tokenomics, and lack of transparency about milestones. Legitimate projects typically disclose team identities, provide detailed technical documentation, show clear roadmaps, and communicate regularly about progress. Projects that lack these elements often disappear after raising funds, leaving investors with worthless tokens.\
The regulatory environment adds another layer of complexity. Crypto projects face scrutiny about whether their tokens constitute securities under various jurisdictions. The classification determines which regulations apply and what disclosures are required. Projects that ignore legal frameworks or operate without proper licensing expose themselves and their investors to enforcement actions, frozen assets, and potential criminal charges.Real Estate Tokenization Track RecordThe history of real estate tokenization projects provides lessons. Multiple ventures have attempted to bring property onto blockchain with mixed results. Early projects focused on fractional ownership, allowing investors to buy shares in specific buildings. These faced challenges with liquidity, regulatory compliance, and the complexity of managing physical assets through digital interfaces.\
More recent projects have shifted toward the financing layer rather than ownership tokenization. This approach encounters fewer regulatory hurdles since it deals with loan products rather than securities representing property ownership. However, the business model still requires borrowers, which means projects must build relationships with developers and prove they can provide capital at competitive rates.\
The question of whether blockchain adds genuine value or merely adds complexity remains contentious. Supporters argue that on-chain transparency, programmable terms, and global capital access justify the technology overhead. Critics point out that traditional finance already has efficient systems for real estate lending and that blockchain's benefits are often overstated relative to implementation costs.Several execution risks warrant examination. First, the loan book model requires a steady pipeline of creditworthy borrowers. If developers can obtain financing through traditional channels at lower costs, they have little incentive to use a new platform that charges fees. The project must either offer better terms than banks or target developers who cannot access traditional financing, which introduces credit risk.\
Second, the AI underwriting system remains largely unproven at scale. While automation can speed processes, it also concentrates risk if the algorithms make systematic errors. A series of bad loans could deplete the insurance fund and leave token holders with losses. The lack of detailed information about the AI's training data, error rates, and decision-making process makes it difficult to assess this risk.\
Third, regulatory changes could impact operations. Governments continue to develop frameworks for crypto assets and tokenized securities. A regulatory crackdown in key markets could force the project to halt operations, delist from exchanges, or face enforcement actions. The multi-jurisdictional structure provides some flexibility but also creates compliance complexity across multiple legal systems.\
Fourth, the token economics depend on sustained borrower demand and investor interest. If loan volume does not grow as projected, staking rewards may decline, reducing demand for the token. If token prices fall significantly below the purchase price, early investors may become discouraged and sell, creating additional downward pressure. The long lock-up period until TGE means investors cannot exit if circumstances change.Looking at established players in the space provides benchmarks. Centrifuge has achieved $1 billion in Total Value Locked, making it the third RWA protocol to reach this milestone. The platform tokenizes invoices, receivables, and trade finance instruments, pushing them into DeFi markets as collateral. Centrifuge completed a V3 migration in 2025, delivering multichain infrastructure across six EVM chains.\
Ondo Finance focuses on institutional-grade tokenized securities and has built infrastructure for bringing fixed-income products on-chain. The platform emphasizes compliance and works within regulatory frameworks rather than attempting to circumvent them. This approach has allowed Ondo to partner with traditional financial institutions and build sustainable business models.\
The difference between these established protocols and a new entrant like Construct Koin lies in track record. Centrifuge and Ondo have processed real transactions, demonstrated their technology works, and built reputations over multiple years. They have also secured institutional backing and navigated regulatory processes. Construct Koin must still prove it can execute its vision and deliver returns to token holders.What The Numbers Actually ShowThe project claims £15 million in assets already secured on-chain. Converting to dollars at current exchange rates gives approximately $19 million in collateral backing the protocol before the presale completes. If the presale reaches its $100 million target, the ratio of raised capital to existing collateral will be roughly 5 to 1. This means the project would need to deploy the raised funds into new loans relatively quickly to maintain proportional backing.\
The staking rewards of 8 to 12% APR paid in USDT require generating sufficient revenue from loan interest and fees. If the protocol charges borrowers 7 to 15% annual interest, as stated in marketing materials, the math works if the majority of loans perform and default rates remain low. However, a 10% default rate combined with recovery costs could quickly consume the margin between what borrowers pay and what stakers receive.\
The tokenomics allocate 15% of supply for staking rewards. With 1 billion total tokens, this equals 150 million tokens reserved for rewards. If tokens reach $1 at TGE as the presale structure suggests, that represents $150 million in value designated for staking. Paying 12% APR on a pool of staked tokens would require substantial protocol revenue, meaning the loan book must grow significantly to sustain these yields.The Bigger Picture: Does ReFi Have Product-Market Fit?The core question is whether blockchain-based real estate financing solves problems that matter to enough participants to create a sustainable market. Developers need capital, and investors want returns. Traditional systems provide both, albeit with friction from intermediaries, paperwork, and slow processes.\
Blockchain's value proposition centers on disintermediation, transparency, and global access. By removing middlemen, protocols can theoretically offer borrowers lower rates and investors higher yields. By recording transactions on-chain, all parties can audit the state of loans in real-time. By operating globally, capital can flow from anywhere to anywhere, removing geographical barriers.\
The counterargument is that the friction in traditional finance exists for reasons. Paperwork and slow processes often serve as risk management mechanisms that prevent bad deals from proceeding. Intermediaries like banks provide expertise in underwriting, legal structuring, and recovery that algorithmic systems may struggle to replicate. Global capital flows sound attractive until investors face losses in foreign jurisdictions where recovering assets is difficult or impossible.After examining the available information about Construct Koin, the project represents both the promise and peril of real-world asset tokenization in its current phase. The promise lies in the genuine growth of the RWA sector, which has demonstrated that certain use cases have found product-market fit. The peril comes from execution risk, regulatory uncertainty, and the long history of crypto projects that fail to deliver on ambitious visions.\
The data points to consider are straightforward. The RWA market is real and growing, reaching $30 billion with institutional participation from major financial players. Real estate represents the largest asset class globally at over $300 trillion, meaning even a tiny percentage of tokenization would create enormous value. The technology for tokenizing loans and managing them through smart contracts exists and has been deployed by other projects successfully.\
Against this, the presale model concentrates risk on early participants who must wait 12 to 24 months for tokens to unlock while hoping the project executes. The team, while public, does not appear to have prior experience building DeFi protocols or managing large-scale lending operations. The AI claims lack substantiation through independent testing or published benchmarks. The regulatory landscape remains fluid, with governments still determining how to classify and regulate tokenized assets.\
The honest assessment is that Construct Koin is attempting something difficult that, if successful, could generate returns for early participants. It is also attempting something that could fail for multiple reasons including poor execution, regulatory intervention, lack of borrower adoption, or macroeconomic changes that reduce demand for real estate development financing. Potential participants should view this as a high-risk investment where capital loss is a realistic outcome, not merely a theoretical possibility disclosed in legal disclaimers.\
The project would benefit from greater transparency about its AI technology, more detailed disclosure of its existing loan book, and clearer communication about partnerships with developers who will actually use the platform. Without these elements, investors are essentially betting on a vision backed by marketing materials rather than proven operational metrics.\
In a market where over half of projects fail, the bar for success is high. Whether Construct Koin clears that bar will depend on execution over the coming years, not on the quality of its presale marketing or the size of its fundraising target.\
Don’t forget to like and share the story!:::tip
This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>Adobe Struggles To Assure Investors That It Can Thrive in AI Era</title><link>https://slashdot.org/story/25/10/31/1338224/adobe-struggles-to-assure-investors-that-it-can-thrive-in-ai-era?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 31 Oct 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Adobe brought together 10,000 marketers, filmmakers and content creators to its annual conference this week to persuade them that the company's software products are adapting to AI and remain the best tools for their work. But it's Adobe's investors, rather than its users, who are the most skeptical that generative AI technology won't disrupt the company's business as the top seller of software for creative professionals. 

Despite a strong strategy, Adobe is "at risk of structural AI-driven competitive and pricing pressure," wrote Tyler Radke, an analyst at Citigroup. The company's shares have lost about a quarter of their value this year as AI tools like Google's video-generating model Veo have gained steam. In an interview with Bloomberg Television earlier this week, Adobe Chief Executive Officer Shantanu Narayen said the company is undervalued as the market is focused on semiconductors and the training of AI models.]]></content:encoded></item><item><title>You Can&apos;t Refuse To Be Scanned by ICE&apos;s Facial Recognition App, DHS Document Says</title><link>https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/10/mob-fort-art.png" length="" type=""/><pubDate>Fri, 31 Oct 2025 13:47:10 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[This article was primarily reported using public records requests. We are making it free to read as a public service. FOIA reporting can be expensive, please consider subscribing to 404 Media to support this work. Or send us a one time donation via our tip jar here.Immigration and Customs Enforcement (ICE) does not let people decline to be scanned by its new facial recognition app, which the agency uses to verify a person’s identity and their immigration status, according to an internal Department of Homeland Security (DHS) document obtained by 404 Media. The document also says any face photos taken by the app, called Mobile Fortify, will be stored for 15 years, including those of U.S. citizens.The document provides new details about the technology behind Mobile Fortify, how the data it collects is processed and stored, and DHS’s rationale for using it. On Wednesday  that both ICE and Customs and Border Protection (CBP) are scanning peoples’ faces in the streets to verify citizenship.]]></content:encoded></item><item><title>Ubuntu 25.10 amd64v3 Benchmarks: Some Minor &amp; Rare Performance Advantages For Desktop Workloads</title><link>https://www.phoronix.com/review/ubuntu-2510-amd64v3</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 13:36:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Yesterday Canonical announced architecture variants for Ubuntu Linux with Ubuntu 25.10 seeing the introduction of "amd64v3" packages that are built for the x86_64-v3 micro-architecture feature level to assume AVX/AVX2 and other newer CPU ISA features found since Intel Haswell and AMD Excavator processors. Eager to run some initial tests, here is a first look at the Ubuntu 25.10 amd64v3 performance for desktop workloads.]]></content:encoded></item><item><title>Reddit CEO says chatbots are not a traffic driver</title><link>https://techcrunch.com/2025/10/31/reddit-ceo-says-chatbots-are-not-a-traffic-driver/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Fri, 31 Oct 2025 13:34:03 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[During Reddit's Q3 2025 call, CEO Steve Huffman noted that Google search and direct access continue to be its top traffic drivers.]]></content:encoded></item><item><title>Nvidia expands AI ties with Hyundai, Samsung, SK, Naver</title><link>https://techcrunch.com/2025/10/31/nvidia-expands-ai-ties-with-hyundai-samsung-sk-naver/</link><author>Kate Park</author><category>tech</category><pubDate>Fri, 31 Oct 2025 13:29:44 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nvidia CEO Jensen Huang is visiting South Korea to strengthen partnerships with Samsung, Hyundai, SK, and Naver, unveiling plans for AI-powered networks and next-generation intelligent systems.]]></content:encoded></item><item><title>In 1953, the Ford X-100 Concept Car Had It All</title><link>https://spectrum.ieee.org/ford-x-100-concept-car</link><author>Allison Marsh</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk1MTgxNy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgyMTUyNjcyMn0.mXOXoQHDNVmqLsPQO9kBLfdSxCqStA7LmZrn3W5Xvu8/image.png?width=600" length="" type=""/><pubDate>Fri, 31 Oct 2025 13:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Heated seats, a radio phone, even an electric shaver in the glove box]]></content:encoded></item><item><title>Scientists Reveal Roof Coating That Can Reduce Surface Temperatures Up To 6C On Hot Days</title><link>https://science.slashdot.org/story/25/10/31/014230/scientists-reveal-roof-coating-that-can-reduce-surface-temperatures-up-to-6c-on-hot-days?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the Guardian: Australian scientists have developed roof coatings that can passively cool surfaces up to 6C below ambient temperature, as well as extract water from the atmosphere, which they say could reduce indoor temperatures during extreme heat events. One coating made from a porous film, which can be painted on to existing roofs, works by reflecting 96% of incoming solar radiation, rather than absorbing the sun's energy. It also has a high thermal emittance, meaning it effectively dissipates heat to outer space when the sky is clear. Its properties are known as passive radiative cooling. [...]
 
In a study, published in the journal Advanced Functional Materials, the researchers tested a prototype for six months on the roof of the Sydney Nanoscience Hub, pairing the cool paint with a UV-resistant topcoat that encouraged dew droplets to roll down into a receptacle. As much as 390 milliliters per sq meter per day could be collected for about a third of the year, the scientists found. Based on that water capture rate, an average Australian roof -- about 200 sq meters -- could provide up to 70 liters on days favorable for collecting dew, they estimate. [...]
 
In well-insulated buildings, a 6C decrease in roof temperature "might result in a smaller fraction of that cooling being reflected in the top level of the house," [said the study's lead author, Prof Chiara Neto of the University of Sydney], but greater temperature reductions would be expected in most Australian houses, "where insulation is quite poor." She said the coating could also help reduce the urban heat island effect, in which hard surfaces absorb more heat than natural surfaces, resulting in urban centers being 1C to 13C warmer than rural areas. The researchers found that the prototype coating was comprised of poly(vinylidene fluoride-co-hexafluoropropene), which is used in the building industry but was "not a scalable technology going forward" due to its environmental issues. However, they are now commercializing a water-based paint with similar performance that is affordable and environmentally safer, costing about the same as standard premium paints.]]></content:encoded></item><item><title>Study: AI Models Trained On Clickbait Slop Result In AI ‘Brain Rot,’ ‘Hostility’</title><link>https://www.techdirt.com/2025/10/31/study-ai-models-trained-on-clickbait-slop-result-in-ai-brain-rot-hostility/</link><author>Karl Bode</author><category>tech</category><pubDate>Fri, 31 Oct 2025 12:20:34 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[While “AI” certainly has some useful applications, a lot of the folks in charge of the trajectory of LLMs clearly want to use it to build a giant, badly automated ouroboros of lazy internet slop that shits out ad money without the need for pesky labor. You see this most profoundly in media, where a bunch of far-too-clever lads rushed to integrate under-cooked, broadly misunderstood LLMs with disastrous results. As it turns out, training LLMs on this kind of slop doesn’t work out well for anybody.A new joint study by researchers at Texas A&M University, University of Texas at Austin, and Purdue University took a closer look at what happens when you train LLMs on the kind of engagement slop our modern internet gatekeepers are keen to create. To see how these models would “behave” after subsisting on a diet of clickbait sewage, the researchers cobbled together a sample of one million X posts and then trained four different LLMs on varying mixtures of control data (long form, good faith, real articles and content) and junk data (lazy, engagement chasing, superficial clickbait) to see how it would affect performance.“All four models tested—Llama3 8B, Qwen2.5 7B/0.5B, Qwen3 4B—showed some forms of cognitive decline. Meta’s Llama proved the most sensitive to the junk, seeing drops in its reasoning capabilities, understanding of context, and adherence to safety standards. Interestingly, a much smaller model, Qwen 3 4B, proved more resilient, though still suffered declines. It also found that the higher the rates of bad data, the more likely a model was to slip into “no thinking” mode, failing to provide any reasoning for its answer, which was more likely to be inaccurate.”They also found that after being fed a bunch of ex-Twitter slop, the models didn’t just get “dumber”, they were (shocking, I know) far more likely to take on many of the nastier “personality traits” that now dominate the right wing troll platform:“More than just getting “dumber” in its thinking, though, the researchers found the inclusion of junk also resulted in an interesting effect: it led to changes in the model’s “personality,” succumbing to what the researchers called “dark traits.” For instance, the Llama 3 model displayed significantly higher levels of narcissism and became less agreeable. It also went from displaying nearly no signs of psychopathy to extremely high rates of the behavior.”And by “dumber” and “narcissistic” they of course mean a vague simulacrum of those personality traits, because modern LLMs don’t  anything, much less adopt real personalities. You’ll often see people (even prominent NYT tech journalists) attributing malicious intent and understanding to language learning models, inadvertently advertising the fact they don’t know how any of this works. There’s been so much misrepresentation of what these models are capable of (by both companies and the tech media), this comment below needs to be projected onto the moon:Chatbots — LLMs — do not know facts and are not designed to be able to accurately answer factual questions. They are designed to find and mimic patterns of words, probabilistically. When they’re “right” it’s because correct things are often written down, so those patterns are frequent. That’s all.You see this a lot in breathless articles about LLMs that are trying to “resist being shut off” or somehow “blackmail their operators.” It’s simply not how this technology actually works. It’s part of a con suggesting these models are just a few weeks and another billion away from HAL 900 sentience. Again, none of this is to say LLMs don’t have very useful applications, such as examining vast troves of scientific data to look for patterns and facts that humans might miss. Or creating more efficient, “intelligent” software that can be predictive of the user’s needs or inputs. Or automating basic customer service inquiries in a world full of already-low quality outsourced support. The problem with AI generally is a decidedly human one: the terrible, unethical, and greedy people currently in charge of it’s implementation (again, see media, insurance, countless others) — folks who have cultivated some unrealistic delusions about AI competency and efficiency (see this recent Stanford study on how rushed AI adoption in the workforce often makes people less efficient).This is before you even get to the climate and energy impact of these models, or the fact that the underlying financials are a hot mess poised to cause some serious economic tumult next year as the outer layer of hype and misrepresentation burns off. Even then, this quest to turn the internet into an ocean of lazy and uncurated ad engagement slop will remain a centerpiece of the movement.]]></content:encoded></item><item><title>AMD Windows Driver Changes For RX 5000/6000 Series Won&apos;t Impact Linux Users</title><link>https://www.phoronix.com/news/AMD-Windows-RX-5000-6000-Game</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 10:51:09 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Over the past day there have been many reports of AMD planning to no longer provide game optimizations for the Radeon RX 5000 and RX 6000 series graphics cards for their Microsoft Windows driver. Surprisingly many in the Linux community still seem to think it will impact the Linux drivers, but long story short, there is no real concern for Linux users/gamers...]]></content:encoded></item><item><title>Linux 6.18-rc4 Fixes Another Performance Regression In The Power Management Code</title><link>https://www.phoronix.com/news/Linux-6.18-rc4-PM-Perf-Fix</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 10:36:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Last week there was a fix for a "serious performance regression" in the Linux kernel's power management code that affected some Intel-powered Chromebooks. This week the power management fixes ahead of Linux 6.18-rc4 is addressing another performance regression...]]></content:encoded></item><item><title>AerynOS 2025.10 ISOs Released - GNOME 49, Switches Back To GNU libstdc++</title><link>https://www.phoronix.com/news/AerynOS-2025.10-ISOs</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 10:12:48 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AerynOS 2025.10 ISOs were released today for closing out the month of October. AerynOS as a reminder is the Linux distribution that was started by Ikey Doherty and originally known as Serpent OS that has since evolved into an open-source team effort...]]></content:encoded></item><item><title>Krita Lands Basic HDR Support On Wayland</title><link>https://www.phoronix.com/news/Krita-HDR-Wayland-Support</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 10:04:03 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The KDE/Qt-aligned Krita digital painting application is the latest creative app now supporting high dynamic range (HDR) on Linux when using Wayland...]]></content:encoded></item><item><title>How a Chorus of Synchronized Frequencies Helps You Digest Your Food</title><link>https://science.slashdot.org/story/25/10/31/0037250/how-a-chorus-of-synchronized-frequencies-helps-you-digest-your-food?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[alternative_right shares a report from Phys.org: It is known in the scientific community that if you have a self-sustained oscillation, such as an arteriole, and you add an external stimulus at a similar but not identical frequency, you can lock the two, meaning you can shift the frequency of the oscillator to that of the external stimulus. In fact, it has been shown that if you connect two clocks, they will eventually synchronize their ticking. Distinguished Professor of Physics and Neurobiology David Kleinfeld found that if he applied an external stimulus to a neuron, the entire vasculature would lock at the same frequency. However, if he stimulated two sets of neurons at two different frequencies, something unexpected happened: some arterioles would lock at one frequency and others would lock at another frequency, forming a staircase effect.
 
Searching for an explanation, Kleinfeld enlisted the help of his colleague, Professor of Physics Massimo Vergassola, who specializes in understanding the physics of living systems, and then recruited Ecole Normale Superieure graduate student Marie Sellier-Prono and Senior Researcher at the Institute for Complex Systems Massimo Cencini. Together, the researchers found they could use a classical model of coupled oscillators with an intestinal twist. The gut oscillates naturally due to peristalsis -- the contracting and relaxing of muscles in the digestive tract -- and provided a simplified model over the complex network of blood vessels in the brain. The intestine is unidirectional, meaning frequencies shift in one direction in a gradient from higher to lower. This is what enables food to move in one direction from the beginning of the small intestine to the end of the large intestine.
 
"Coupled oscillators talk to each other and each section of the intestine is an oscillator that talks to the other sections near it," stated Vergassola. "Normally, coupled oscillators are studied in a homogeneous setting, meaning all the oscillators are at more or less similar frequencies. In our case, the oscillators were more varied, just as in the intestine and the brain." In studying the coupled oscillators in the gut, past researchers observed that there is indeed a staircase effect where similar frequencies lock onto those around it, allowing for the rhythmic movement of food through the digestive tract. But the height of the rises or breaks, the length of the stair runs or frequencies, and the conditions under which the staircase phenomenon occurred -- essential features of biological systems -- was something which had not been determined until now. The findings have been published in the journal Physical Review Letters.]]></content:encoded></item><item><title>Vulkan 1.4.331 Brings Two New Extensions</title><link>https://www.phoronix.com/news/Vulkan-1.4.331-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 09:48:47 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Just one week after Vulkan 1.4.330 brought five new extensions, Vulkan 1.4.331 is now available with another two new extensions for this high performance graphics and compute API...]]></content:encoded></item><item><title>Aembit Introduces Identity And Access Management For Agentic AI</title><link>https://hackernoon.com/aembit-introduces-identity-and-access-management-for-agentic-ai?source=rss</link><author>CyberNewswire</author><category>tech</category><pubDate>Fri, 31 Oct 2025 09:05:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Silver Spring, USA/ Maryland, October 30th, 2025/CyberNewsWire/-- today announced the launch of Aembit Identity and Access Management (IAM) for Agentic AI, a set of capabilities that help organizations safely provide and enforce access policies for AI agents as they move into production. The release introduces Blended Identity, which defines how AI agents act on behalf of verified users, and the MCP Identity Gateway, which ensures secure access to enterprise resources based on identity, access policy, and runtime attributes.The new offering extends the Aembit Workload IAM Platform to address one of the most pressing operational questions in artificial intelligence and modern IT: how to control what autonomous and user-driven AI agents can access, under what conditions, and with what accountability.AI agents are rapidly becoming a key part of enterprise operations. Nearly half of technology executives say they are already adopting or fully deploying agentic AI, and about the same share expect most of their AI deployments to be autonomous within two years, according to an . These agents retrieve sensitive data, open tickets, and execute code across cloud, on-premises, and SaaS environments.Yet most access models were built for people, not self-directed software. Many still rely on static secrets and shared credentials, creating risk and obscuring accountability. Worse yet, agents’ actions are often hidden behind the identity of a human, making it almost impossible to audit the actions each actor has taken. The result is a widening gap between the pace of AI adoption and the ability of organizations to secure it with confidence. assigns each agent a cryptographically verified identity, issues ephemeral credentials, and enforces policy at runtime. The system records every access decision and maintains attribution across both human-driven and autonomous agent activity. By bringing agent activity under the same centralized policy control plane that governs other workloads, Aembit enables enterprises to deploy AI at scale while maintaining control, auditability, and compliance.“Enterprises want to say yes to agentic AI, and they’re asking Aembit for ways to securely grant agents access to data and applications,” said David Goldschlag, co-founder and CEO of Aembit. “Aembit IAM for Agentic AI gives enterprises the same level of control and audit over agent access that IAM systems have long provided for employees. Our approach enables organizations to advance their AI initiatives without expanding their threat and risk surface.”The release introduces two core capabilities to the Aembit Workload IAM Platform:Blended Identity, which gives every AI agent its own verified identity and, when needed, binds it to the human it represents. This establishes a single, traceable identity for each agent action and allows Aembit to issue a secure credential that reflects that combined context.MCP Identity Gateway, which receives that identity credential and controls how agents connect to tools through the Model Context Protocol (MCP). The gateway authenticates the agent, enforces policy, and performs token exchange to securely retrieve the necessary access permissions for each connected resource – without ever exposing them to the agent runtime.Together, this functionality allows enterprises to apply least-privilege access, revoke permissions immediately when needed, and ensure that every AI action is attributable and auditable. They operate on Aembit’s established Workload IAM foundation, which enforces policy dynamically at runtime, issues ephemeral credentials just in time, and records structured events for full traceability.Aembit developed IAM for Agentic AI through collaboration with large businesses, government organizations, and innovative agentic AI startups deploying AI for operational and security workloads. Those efforts helped shape an approach that combines enterprise enforcement with the adaptability AI projects demand.“AI agents don’t live inside one stack or trust domain,” said Kevin Sapp, co-founder and CTO of Aembit. “They move between hybrid environments in seconds. With Aembit, every agent carries a verified identity that our gateway can authenticate and control in real time. It’s how enterprises can give agents the access they need to work, while never losing sight of who they are or what they touch.”Aembit IAM for Agentic AI is now available to customers using its Workload IAM Platform. Organizations can learn more, request a demo, or get started today at . is the identity and access management platform for agentic AI and workloads. It enforces access based on identity, context, and centrally managed policies, giving organizations a singular place to control access risk from AI agents, automate credential management, and accelerate AI adoption. With Aembit, enterprises can confidently control access to sensitive resources across all the workloads that power their business.:::tip
This story was published as a press release by Cybernewswire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>How Can Governments Pay Open Source Maintainers?</title><link>https://hackernoon.com/how-can-governments-pay-open-source-maintainers?source=rss</link><author>Terence Eden</author><category>tech</category><pubDate>Fri, 31 Oct 2025 07:08:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When I worked for the UK Government I was once asked if we could find a way to pay for all the Open Source Software we were using. It is a surprisingly hard problem and I want to talk about some of the issues we faced.What about the Open Source that UK Government ?Open Source is facing a crisis. The code that the world relies on is often developed by underpaid engineers on the brink of burn-out.  While I don't think anyone wants Open Source to have a paywall, it seems obvious that large organisation should pay their way and not rely solely on volunteer labour.Here are some of the problems I faced when trying to get the UK Government to pay for OSS and how  as a maintainer can help make it easier for large organisations to pay you.Firstly, lots of OSS doesn't have a well defined owner; so who gets the money?I'm not saying that every little library you create needs to be published by a registered company, nor am I suggesting that you should remove your anonymity. But Governments and other organisations need to know  they are funding and  the money is going. The danger of accidentally funnelling money to a sanctioned state or person is just too big a risk for most organisations.If you want to receive funding - make it  clear who you are.Even when there is an owner, there often isn't an easy mechanism for paying people. Donation sites like GitHub Sponsors, Ko-Fi, and Patreon are great for individuals who want to throw a small amount of money to creators but they can be problematic for larger organisations.  Many OSS projects get around this by offering support contracts. It makes it much easier for an organisation to justify their spend because they're no longer donating to something which can be obtained for free; they're paying for a service.This doesn't have to be a contract offering a 24/7 response and guaranteed SLA. It can be as simple as offering best-effort email support.The important thing is to offer an  way for a larger organisation to buy your services. Many organisations have corporate credit cards for lower-cost discretionary spending which doesn't require a full business-case.  How easily could a manager buy a £500 support contact from your site?Maintainers don't only have to offer support contracts. Many choose to offer training packages which are a good way to raise money  get more people using your product. Some project maintainers will speak at your conference for a suitable fee.Again, the aim here is for maintainers to offer a  reason for a payment to be made.Open Source has a brilliant culture of allowing multiple (often anonymous) contributors. That's fine when there's no money involved, but how does a moderately sized project decide who receives what share of the funding? Services like OpenCollective can make it easier to show  the money is going but it is better to discuss in advance with all contributors what they expect as a share.If people think they're being taken advantage of, or that a project maintainer is unjustly enriching themselves, it can cause arguments.  Be very clear to contributors what the funding is for and whether they're entitled to any of it.Finally, we faced the issue that some OSS projects didn't  to take money from the "big bad state". They were worried that if people saw "Sponsored by the Government" they would assume that there were backdoors for spies, or that the developer might give in to pressure to add unwanted features.  This (usually) isn't the case but it is easy to see why having a single large organisation as the main donor could give the impression of impropriety.The best defence against this is to have  of paying sponsors! Having the state as one of many partners makes it clear that a project isn't beholden to any one customer.It isn't impossible to get Governments to spend on Open Source. But state spending is heavily scrutinised and, bluntly, they aren't set up to pay  amounts to non-suppliers, who aren't charging money.  While large projects often have the resources to apply for Government grants and contracts, smaller projects rarely have the time or expertise. It is critical that maintainers remove the barriers which make it too hard for organisations to pay them.Make it easy for Governments and other large organisations to pay you.Be as obvious as possible that you are able to accept payments from them.Don't be afraid to put a large price on your talents.Offer multiple paid-for options like speaker fees, support, and feature development funding.Talk with your contributors to let them know how any funding will be shared.]]></content:encoded></item><item><title>Empowering Flink CDC: Schema Evolution Support Lands in Apache SeaTunnel</title><link>https://hackernoon.com/empowering-flink-cdc-schema-evolution-support-lands-in-apache-seatunnel?source=rss</link><author>William Guo</author><category>tech</category><pubDate>Fri, 31 Oct 2025 07:08:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[From classroom to codebase! Meet Dong Jiaxin, a student from USTB who brought CDC Schema Evolution to Apache SeaTunnel on Flink during the OSPP. ]]></content:encoded></item><item><title>Simple, Battle-Tested Algorithms Still Outperform AI</title><link>https://hackernoon.com/simple-battle-tested-algorithms-still-outperform-ai?source=rss</link><author>Jose Crespo, PhD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 07:07:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[==Let’s put numbers to the AI overhype. Companies are burning more than $200 billion every year by choosing AI over simple, proven algorithms. That’s not an opinion. That’s math. (See the chart below if you’re still rubbing your eyes in disbelief.)==But here’s the question: Is AI the villain - or are CEOs burning money on systems they barely understand?The truth is more brutal than either narrative. AI isn’t the disaster — it’s the  for the disaster.Look at the chart below. This is your American Horror Story in three acts: Solo AI vendors promise revolutionary decision-making. Fortune 500 CEOs, hypnotized by NVIDIA’s stock price and OpenAI’s demos, write checks their companies can’t cash. That red line? That’s $500 billion annually achieving -45% ROI (Return on Investment — how much you get back compared to what you spend). Not a typo.  forty-five percent. Companies are paying premium prices to make  decisions . It’s like hiring McKinsey to tell you what you already know, except McKinsey occasionally delivers positive value. Nobody admits failure. The AI system becomes “strategic infrastructure.” The losses get buried in “digital transformation.” The vendors — , the whole choir — keep singing about the future while cashing checks in the present.Remember the California Gold Rush from the history books? You know who got rich in 1849. Not the miners. The pickaxe sellers.==Today’s pickaxe merchants sell GPU clusters and API tokens to CEOs panning for digital gold.== They whisper sweet promises: “AI will revolutionize your business.” What they don’t mention is that graph showing AI never achieves positive ROI for operational decisions.Instead they boast about having more billion parameters than the competition’s AI monster. Of course theirs is bigger, more insanely convoluted — an expanding minefield of compounding errors,  each cascade multiplying the disaster.Meanwhile, see that green line in the chart above — simple algorithms from 1913 — keeps printing money at 1200% ROI. But nobody’s selling conferences about the Economic Order Quantity formula. There’s no TED talk about Little’s Law and the other older algorithms.Yep, unfortunately you can’t raise billions in funding rounds by selling an algorithm that fits on a napkin from that expensive restaurant where you’re shaking hands with investors interested in a bigger AI beast.🤔The tragedy is more nuanced than AI sellers admit — and more complex than CEOs with their broken quarterly-profit-maximization algorithms want to hear.Yes, there’s a hybrid approach where AI becomes just one component alongside those ancient algorithms when complexity grows. But that requires two things the AI revolution explicitly avoids:==First, you need to understand your business== ==from the molecular level to the stratosphere — every intricacy of your model, what you’re actually selling versus what you think you’re selling, where costs hide, where value emerges. These insights come from human minds. AI won’t solve what you can’t articulate, despite the sales pitch.====Second, you need to hire and respect professional====s who combine programming excellence with mathematical rigor and the rare ability to translate both into business value. But your AI recruiting system will never surface these people.== It’s optimized for commodities, not talent — screening for keywords, not capability.Look at the carnage in the table below (Chart 3) . This isn’t speculation — it’s documented history: Running on a decision that fits on an index card. Investment: approximately zero. Return: Industry dominance for four decades. The crown jewel of . Investment: $4 billion. Return: Sold for parts at a 95% loss. One simple rule about load factors. Still the only major US airline to avoid bankruptcy. Cutting-edge AI pricing models. Burned $500 million in one quarter before shutting down entirely.The only success story with AI? Amazon’s hybrid approach — yup, not pure AI, it’s old-school EOQ with some ML sprinkled on top when absolutely necessary. Even then, the ROI is a fraction of what simple algorithms achieve alone.But let’s be honest here, let’s not throw the baby out with the bathwater: when your business complexity genuinely grows — meaning the actual number of variables you must account for — you need more flexibility. That might be AI/ML, or more likely, it’s just good programmers who understand your business and can architect the math you actually need.Here’s what 20 years in the trenches taught most of us: around 90% of “complex” business problems can be solved by a competent programmer with decent math skills and simple algorithms, properly wired together.Occam’s Razor still cuts: The simplest solution that works is usually right.==But simplicity doesn’t sell conference tickets. Simplicity doesn’t raise Series B funding. Simplicity doesn’t get you on the cover of Wired. 😂==So instead, companies take the lazy, prestigious route: throw their problems at whatever AI the vendors are pushing this quarter. It’s like hiring a top surgeon to apply a Band-Aid — expensive, unnecessary, and probably going to make things worse.The real tragedy? That competent programmer with the simple solution costs $150K/year. The AI system that fails costs $15M. But the programmer doesn’t come with a sales team, a PowerPoint deck, or a promise to “transform your digital future.”Guess which one the board approves.The Seven Heroes of Business HistoryOur position shouldn’t surprise you by now, dear reader. We’re betting on what’s worked brilliantly for decades through every crisis, disaster, and market shift while delivering massive profits: The formula is simple: Hire, value, and respect your best asset — . Add modern cloud infrastructure and, when genuinely appropriate (maybe 5–10% of cases), deploy the hybrid Algorithm + AI/ML approach.But let’s be clear: forget the magic they’re selling — the fantasy of throwing data at AI and getting perfect solutions every time. That’s not happening.Yes, LLMs are useful as search tools, data navigators, even coding assistants. But they’re far from autonomous. In non-trivial cases, you need spend lot of time cross-referencing to catch their false positives and, worse, the errors nobody notices —  that spaghettify and collide concepts into dangerous nonsense.Meet the Magnificent SevenMore than 500 years of combined service. Trillions in value created. Under 1K lines of code total.Look at this table (Chart 4). These seven algorithms have never failed. For decades — in some cases over a century — they’ve consistently delivered business value to most of mankind. The epitome of simplicity.\
Each one is a specialist with embarrassingly simple code:EOQ (1913): The inventory gunslinger - Square root of (2 × demand × order cost / holding cost). One line. Tells you exactly how much to order.DuPont (1920): The financial sharpshooter - ROE = Profit Margin × Asset Turnover × Leverage. Three numbers multiplied. Instant diagnosis of what’s broken.Newsvendor (1950s): The perishables ranger - Order up to the point where the  matches the cost of running short vs. overstocking. A single threshold for “how much to make.”Kelly (1956): The risk-sizing marshal - Bet a fraction of your bankroll based on  — when your advantage is bigger, size up; when it’s small, size down. Never overbet.CPM (1957): The project management tracker - Find the longest path through your network. That’s your deadline. Everything else can slip; this can’t.Little’s Law (1961): The operations enforcer - Items in system = arrival rate × time in system. It’s physics, not statistics. Works for everything.PageRank (1998): The young gun who built an empire - A page’s importance is the sum of votes from important pages, each vote , with a small  factor to keep it stable. Keep iterating. Built Google.Total code for all seven: comfortably <1K lines. The math that runs the world fits in a single GitHub gist.Is a programming career still viable? Looking at this table — absolutely. Learning algorithms and programming built the past and will build the future, despite this transient AI-hypnotized present.The AI hype has its place — pattern finding, searching, identifying. But it needs years of improvement to correct its statistical errors from Type I through IV. Even then, algorithms in the hands of competent developers remain irreplaceable and far from commodities. While everyone chases AI complexity, these seven simple formulas keep generating thousands of percent returns. They don’t need updates. They don’t hallucinate. They just work.That’s not outdated. That’s immortal. Let’s keep programming with algorithms while the true researchers work on fixing AI errors with better math.]]></content:encoded></item><item><title>SpaceX: Starship Will Be Going To the Moon, With Or Without NASA</title><link>https://science.slashdot.org/story/25/10/31/0045236/spacex-starship-will-be-going-to-the-moon-with-or-without-nasa?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[schwit1 shares a report from Behind the Black: SpaceX is going to land this spaceship manned on the Moon, whether or not NASA's SLS and Orion are ready. And even if those expensive, cumbersome, and poorly designed boondoggles are ready for those first two Artemis landings, SpaceX is likely to quickly outmatch them with numerous other private missions to the Moon, outside of NASA. It has the funds to do it, and it knows it has the customers willing to buy the flights. The news comes from a detailed update SpaceX released today on the Starship lunar lander. Here's the section where SpaceX "made it clear that it sees Starship and Superheavy as its own space effort, irrelevant of NASA": "To return Americans to the Moon, SpaceX aligned Starship development along two paths: development of the core Starship system and supporting infrastructure, including production facilities, test facilities, and launch sites -- which SpaceX is self-funding representing over 90% of system costs -- and development of the HLS-specific Starship configuration, which leverages and modifies the core vehicle capability to support NASA's requirements for landing crew on and returning them from the Moon. SpaceX is working under a fixed-price contract with NASA, ensuring that the company is only paid after the successful completion of progress milestones, and American taxpayers are not on the hook for increased SpaceX costs. SpaceX provides significant insight to NASA at every stage of the development process along both paths, including access to flight data from missions not funded under the HLS contract.
 
Both pathways are necessary and made possible by SpaceX's substantial self-investments to enable the high-rate production, launch, and test of Starship for missions to the Moon and other purposes. Starship will bring the United States back to the Moon before any other nation and it will enable sustainable lunar operations by being fully and rapidly reusable, cost-effective, and capable of high frequency lunar missions with more than 100 tons of cargo capacity."]]></content:encoded></item><item><title>The Road to Hell is Paved with Good DRY Intentions</title><link>https://hackernoon.com/the-road-to-hell-is-paved-with-good-dry-intentions?source=rss</link><author>Melvin Kosisochukwu</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:53:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I have come across my share of beautifully written, over-engineered code… one moment I am thinking “That is an interesting way to do that” and the next minute you are wondering “what the hell is going on here!”. I have also suffered from writing over-engineered code, where you are thinking so far into the future that you compromise . The  (You Aren’t Gonna Need It) principle is a good way to counter over-engineering: functionalities should only be implemented when you need them, not on the possibility that you will need them. I have mentioned over-engineering a couple of times now, and some of you reading this are probably thinking What is over-engineering?In very simple terms, over-engineering is making software/system design more complex than necessary; this usually happens because you want to add extra functionality to your component to simplify the implementation of A, only to add the functionality for B later.We have this codebase for managing invitations; it’s an inherited, legacy codebase with technical debt that accrues interest. The more you try to work on the legacy code, the more something somewhere else breaks. You have to revert in most cases and start all over. Working around the legacy code ends up accruing interest on the technical debt; it’s a standoff between risking breaking features all over the app or adding more technical debt to the codebase. The third solution we came up with was abstraction. We had to figure out ways to modularise new features or improvements to the application, and to expose and share data where necessary. At first, this seemed like a Hail Mary. Finally, we can work on this mangled codebase with minimal side effects. Were we wrong!! The abstraction spiraled, and the codebase ended up overabstracted, bloating the solution to the problem and bringing the supposed salvaged side of the codebase into its own hell. Along the line, we forgone the rules for the abstraction and now have different parts of the application dependent on each other, looping back to where we started. I suppose this was bound to happen in a codebase with a couple of developers, each with fire up their asses to ship features; we ended up shipping maintenance and more technical debt.When collaborating with multiple people on a codebase (which is pretty much always the case), and for corporates who are more interested in shipping features than code quality, code reviews always suffer. For problems like over-engineering and overabstraction, Traditional line-by-line code reviews often miss these systematic issues. You check a component created a couple of weeks ago to support the integration of a feature, following the DRY principle, and realize there are now 10 interconnected/similar features that depend on the component. Code reviews will need to be elevated to catch these architectural issues and ensure that dependency requirements between components are met.We live by the DRY (Don’t Repeat Yourself) gospel because it simplifies work, and developers are inherently lazy (in a good way). The DRY principle works well with orthogonal systems: small, self-contained components combined to form a system.Systems should be composed of a set of cooperating modules, each of which implements functionality independent of each other.There should be more emphasis on orthogonal systems and on DRY code; it’s easier to combine the reusable functions you create with each other and scale them as the repository progresses when they are not bloated and overabstracted, at which point you end up with a rigid system interwoven so much with each other that it will be complicated to connect which aspects of the code that does not meet an exact rule, you will find yourself duplicating the code because making it work for a new connection breaks an old system. Congratulations, you have achieved Spaghetti code that cannot bend. To use a truly orthogonal system and avoid spaghetti DRY code, every code module change should affect only the module that is updated.Your components should not focus solely on avoiding repetition, but also on being small abstractions of the overall system; otherwise, you will end up with components so complex that they can easily break with a single change to a connected component. When creating reusable code, the approach should be writing code that does not depend on any other code block to function a certain way. Reusability should be used as a tool and not the goal; when you have reusable UI components with business or API logic in the same component, you are on a highway to over-abstraction. It starts small, and before you realize what is happening, the disease has festered throughout your repository. Now you cannot reuse the component on a different page with the same UI functions and different logic without adding additional external logic/context to the component.Modularity Modularity ModularityModularity involves breaking your system down into smaller, independent codes/components called modules. Please pay attention to the word ‘smaller’; it’s possible to have a bloated module with more code than necessary, which creates over-abstraction and should be avoided. Over-abstraction is really just an unsuccessful attempt at modularity. Your modules should be able to function independently of other modules, only exposing required data. Changes to good, modular, structured code should affect only the modules, without any cascading effects.A good approach to building orthogonal systems and easily avoiding over-abstraction is to build with functionality first, then features; this aligns well with Component-Based architecture(separating UI components from stateful components). The functionality stage will focus on the smallest reusable units of code that are assembled to implement the feature. A Login feature will consist of the following functions: collect username and password (UI), validate user data, redirect to the user profile/reject collected user data. Each functionality should operate independently, relying only on necessary data.No medals for over-sophisticated codeAfter writing every code implementation, ask yourself whether there is an easier or simpler way to achieve the same result. Most of us have heard stories about code that can be edited or worked on by only one person in the company, which is not a feat to be proud of. Writing code that can only be maintained by you most likely means it’s over-sophisticated or employs unorthodox procedures. A great example of coding using unorthodox procedures is Gilfoyle’s Hard Drives from the article:  I have found myself writing overly sophisticated code because I want to use a new technology/package/library I just learned about, without considering whether it is the simplest tool for the job. The excitement for learning something new is great, but attention to when and where it should be applied is probably more important.In the quest for writing perfect codes that account for all possible future and time-travelling cases, you end up with an over-engineered codebase. You should give it up because it is not possible to write perfect code, aim for good enough that meets all your immediate requirements. The DRY principle is fundamental; repetition is still a sin in software development. The DRY principle should be applied to an orthogonal system to create a codebase that is decoupled, with each module independent and exchanging data at a separate meeting point(feature module). These will help you create systems that are easy to maintain and debug. Remember that simple is always better in software development.]]></content:encoded></item><item><title>Part 1:Building Your First Video Pipeline: FFmpeg &amp; MediaMTX Basics</title><link>https://hackernoon.com/part-1building-your-first-video-pipeline-ffmpeg-and-mediamtx-basics?source=rss</link><author>Samuel Olamide</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:51:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Imagine this: You're tasked with building a web application that displays live video feeds from security cameras, IoT devices, or webcams. Simple enough, right? You quickly discover that while HTML5 video works great for pre-recorded content, the world of real-time streaming is a maze of protocols, codecs, and compatibility issues that can turn a very "simple" feature into weeks of frustration.The core problem is interestingly simple: cameras speak RTSP, but browsers don't. Most IP cameras and professional video equipment use RTSP (Real Time Streaming Protocol) to broadcast their feeds. That's because it's reliable, has low latency, and is perfect for direct connections. But when you try to display that same stream in a web browser, you hit a wall. Modern browsers have dropped RTSP support entirely for security reasons since around 2010-2015, leaving developers scrambling for solutions.This is where the magic happens., the Swiss Army knife of video processing, and , a modern streaming server that acts as a universal translator between different video protocols. Together, they form the backbone of countless video applications, from Netflix's encoding pipeline to your local security system's web interface.In this series, we are going to build a complete, production-ready video streaming pipeline from the ground up. By the end of this first article, you'll have a live webcam feed streaming directly in your browser with remarkably low latency. Let's dive in.FFmpeg: The Universal Video Swiss Army KnifeFFmpeg is arguably one of the most important pieces of software you've never heard of. It powers video processing in applications ranging from VLC media player to professional broadcast systems. At its core, FFmpeg is a command-line tool that can read virtually any video or audio format and convert it to virtually any other format.The FFmpeg workflow follows a predictable pattern: Separating video and audio streams from container formats Converting compressed data into raw video frames and audio samples Applying transformations like scaling, cropping, or color correction Compressing the processed data using codecs like H.264 or VP9 Packaging the encoded streams into output containersFor our streaming pipeline, FFmpeg will serve as the ingestion engine. It will capture video from sources like webcams or files, encode it efficiently, and push it to our streaming server using protocols like RTSP or RTMP (Real-Time Messaging Protocol).While FFmpeg excels at processing video, it cannot serve multiple clients simultaneously. That's where MediaMTX comes in. MediaMTX is a modern, lightweight streaming server that acts as a universal media gateway.Think of MediaMTX as a protocol translator and traffic manager:It accepts incoming streams via RTSP, RTMP, WebRTC, or HLSIt can re-package those streams into different formats for different clientsIt handles authentication, load balancing, and client managementMost importantly for web developers, it can serve RTSP streams as WebRTC, making them accessible to browsersThe beauty of MediaMTX lies in its simplicity. A single binary with a YAML configuration file can handle complex streaming scenarios that would require multiple specialized servers in traditional setups.Before we start building, let's get our tools installed.As of the time of writing this article, the latest ffmpeg release is v8.0, and that is what I will be using.sudo apt-update
sudo apt install autoconf automake build-essential pkg-config libx264-dev libvpx-dev libfdk-aac-dev

git clone https://git.ffmpeg.org/ffmpeg.git ffmpeg

cd ffmpeg

./configure --enable-gpl --enable-libx264 --enable-nonfree

make -j$(nproc)

make install
On MacOS (using Homebrew):Extract the downloaded zip file to You should see folders like , , etc.Press  R, type , press EnterClick  tab → Under , find and select You should see version information and a list of supported codecs and formats.As of the time of writing this article, the latest ffmpeg release is v1.15.0, and that is what I will be using.MediaMTX distributes as a single binary, making installation straightforward:MacOS and Linux Installation:# Download and extract (replace 'linux' with 'darwin' for MacOS)
wget https://github.com/bluenviron/mediamtx/releases/latest/download/mediamtx_v1.15.0_linux_amd64.tar.gz
tar -xzf mediamtx_v1.15.0_linux_amd64.tar.gz

# Make executable and test
chmod +x mediamtx

# Add to system PATH
sudo mv mediamtx /usr/local/bin/

# Run it to confirm installation
mediamtx
# Download and extract
curl -L -O https://github.com/bluenviron/mediamtx/releases/latest/download/mediamtx_v1.15.0_windows_amd64.tar.gz

# Extract to mediamtx_v1.15.0_windows_amd64 folder

# Test run
cd mediamtx_v1.15.0_windows_amd64
./mediamtx.exe

# Add to PATH
mkdir C:\Users\{YOUR_USER}\bin
Move-Item .\mediamtx.exe C:\Users\{YOUR_USER}\bin\
: Windows + R → ==sysdm.cpl== → Advanced → Environment Variables → System Variables → Path → Edit → New → Add  Windows Defender may flag the download - temporarily disable if needed.When you run MediaMTX for the first time, you'll see output like:\
MediaMTX is now running and ready to accept streams.Project 1: Streaming a Video FileLet's start simple by streaming a video file. This simulates a live source and helps us understand the basic pipeline without the complexity of hardware interfaces.First, create a basic MediaMTX configuration file. Create :# Basic MediaMTX configuration
paths:
  test_video:
    source: publisher
This configuration creates a path called  that accepts published streams from any source.Run mediamtx with the config file you created:# In the directory you created the mediamtx.yml
mediamtx mediamtx.yml
Now, let's use FFmpeg to stream a video file to MediaMTX. You'll need a video file for testing. Any MP4, AVI, or MOV file will work:ffmpeg -re -i your_video.mp4 -c:v libx264 -preset fast -c:a aac -f rtsp rtsp://localhost:8554/test_video
Let's break down this command:: Read input at its native frame rate (essential for live streaming): Input file: Use H.264 video codec (widely compatible): Encoding speed vs. compression trade-off: Use AAC audio codec: Output format is RTSPrtsp://localhost:8554/test_video: Destination URLIf everything works correctly, you'll see FFmpeg output showing frame processing statistics.Open VLC Media Player and:Go to Media → Open Network StreamEnter: rtsp://localhost:8554/test_videoYou should see your video playing! This confirms that MediaMTX is receiving the stream from FFmpeg and serving it via RTSP.Project 2: Streaming a WebcamNow, let's capture something live. We'll stream directly from your webcam to create a real-time video feed.First, let's add a new path in our mediamtx config:# Basic MediaMTX configuration
paths:
  test_video:
    source: publisher
  webcam:
    source: publisher
Now we need to identify your webcam device:# List video devices
ls /dev/video*

# Usually /dev/video0 for the first webcam
# List available devices
ffmpeg -f avfoundation -list_devices true -i ""
# List available devices
ffmpeg -list_devices true -f dshow -i dummy

# OR
Get-PnpDevice -Class Camera | Select-Object FriendlyName, Status
You'll see output listing available cameras and microphones. Note the device index (usually 0 for the built-in camera).Now, let's stream your webcam:ffmpeg -f dshow -rtbufsize 100M -i video="Integrated Webcam" -c:v libx264 -preset ultrafast -tune zerolatency -f rtsp rtsp://localhost:8554/webcam
Let's break down this command: - Use DirectShow input format (Windows-specific for cameras/microphones) - Set real-time buffer size to 100MB (prevents dropped frames)-i video="Integrated Webcam" - Input source: video device named "Integrated Webcam" - Video codec: use H.264 encoder - Encoding preset: prioritize speed over compression quality - Optimize encoding for real-time streaming (minimal buffering) - Output format: Real Time Streaming Protocolrtsp://localhost:8554/webcam - Output destination: local RTSP server on port 8554, path "/webcam"ffmpeg -f avfoundation -framerate 30 -video_size 1280x720 -i "0" -c:v libx264 -preset ultrafast -tune zerolatency -f rtsp rtsp://localhost:8554/webcam
 - Use AVFoundation input format (macOS-specific for cameras/microphones) - Capture at 30 frames per second - Set capture resolution to 1280x720 (720p) - Input source: device index 0 (first camera device) (same as Windows) - Video codec: H.264 encoder - Fastest encoding preset - Real-time streaming optimization - RTSP output formatrtsp://localhost:8554/webcam - RTSP server destinationffmpeg -f v4l2 -i /dev/video0 -c:v libx264 -preset ultrafast -tune zerolatency -c:a aac -f rtsp rtsp://localhost:8554/webcam
 - Use Video4Linux2 input format (Linux-specific for cameras) - Input source: first video device in Linux (/dev/video0) - Video codec: H.264 encoder - Fastest encoding preset - Real-time streaming optimization - Audio codec: Advanced Audio Coding (AAC) - RTSP output formatrtsp://localhost:8554/webcam - RTSP server destination We're using the path  instead of . Test this stream in VLC using rtsp://localhost:8554/webcam. You should see your live webcam feed with minimal delay!Project 3: The Magic of WebRTCHere's where things get exciting. While RTSP works great for applications like VLC, it won't work in web browsers. But MediaMTX has a superpower: it can automatically convert RTSP streams to WebRTC, which browsers understand perfectly.Let's enable WebRTC in MediaMTX. Update your  configuration:# Basic MediaMTX configuration
webrtc: yes
webrtcAddress: :8889
webrtcEncryption: no
webrtcAllowOrigin: '*'
webrtcLocalUDPAddress: :8189
webrtcIPsFromInterfaces: yes

paths:
  test_video:
    source: publisher
  webcam:
    source: publisher
Restart MediaMTX with this new configuration and keep your webcam stream running with FFmpeg. Open your browser to http://localhost:8889/webcam. Your webcam feed should start playing directly in the browser.This is the magic moment. You've just built a complete real-time video pipeline that captures live video, processes it, and delivers it to web browsers using modern WebRTC technology. The same architecture powers professional applications serving thousands of concurrent viewers.Understanding What Just HappenedLet's trace the complete data flow:Your webcam produces raw video framesFFmpeg captures these frames, encodes them as H.264, and streams them via RTSP to MediaMTXMediaMTX receives the RTSP stream and makes it available on the  pathWhen a browser requests the stream via WebRTC, MediaMTX automatically converts the RTSP stream to WebRTC formatThe browser receives WebRTC packets and renders them in real-timeThis pipeline is remarkably efficient because MediaMTX doesn't re-encode the video. It simply repackages the H.264 stream into different container formats for different protocols.You've just built a foundational real-time video streaming pipeline. Let's recap what you've accomplished:Installed and configured FFmpeg and MediaMTXStreamed a pre-recorded video file to simulate live contentCaptured and streamed live video from your webcamEnabled browser-based viewing using WebRTCCreated a low-latency video pipeline suitable for interactive applicationsThis setup demonstrates the core concepts that power much larger streaming systems. The same pattern (capture, encode, serve) scales from single streams to thousands of concurrent feeds.However, our current setup has some limitations that prevent it from being production-ready:No authentication or security measuresCan't handle real-world video sources like IP camerasNo monitoring or error handlingIn Part 2, we'll address these challenges by securing our pipeline, connecting to real IP cameras, and preparing our system for internet deployment. We'll explore authentication mechanisms, handle diverse video formats, and transform our localhost demo into a robust, secure streaming service.The journey from "works on my machine" to "production-ready" is where the real engineering challenges begin and where the most interesting solutions emerge.Ready to take your streaming pipeline to the next level? Continue with Part 2: Beyond Localhost: Security, Authentication, and Real-World Sources where we'll secure our setup and connect to real-world video sources.]]></content:encoded></item><item><title>The MVP Engineering Playbook: Ship a Useful 0→1 in 6 Weeks</title><link>https://hackernoon.com/the-mvp-engineering-playbook-ship-a-useful-01-in-6-weeks?source=rss</link><author>Abidhusain Chidi</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:46:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A practical scope, sprint, and CI/CD blueprint any small team can copy.Most MVPs fail from scope creep, fragile infrastructure, or slow feedback loops—not from lack of ideas. This guide focuses on a minimal but reliable path to get real users touching a real product quickly, with just enough quality gates to avoid rewrites.Week 0: Define “done” and remove uncertaintyProblem statement in one sentenceOne success metric (e.g., first task completion or first payment)Non‑negotiables: auth, audit logs, basic observability, backupsNice‑to‑haves explicitly parked a one‑page PRD and a simple system sketch (client → API → DB → third‑party).Weeks 1–2: Ship a running skeletonRepos: monorepo or two (web/mobile + API)Choose boring, proven stack (e.g., Next.js/React + Node/Laravel + Postgres)Implement: auth, roles, seed data, feature flags, error tracking, health checksDeploy to  lint, unit tests for core domains, pre‑commit hooks, CI under 10 minutes.# .github/workflows/ci.yml (example)
name: CI
on: [push, pull_request]
jobs:
  build_test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with: { node-version: '20' }
      - run: npm ci
      - run: npm run lint && npm test -- --ci
Weeks 3–4: Deliver thin vertical slicesShip features as user‑visible , not layers.Small spec (Given/When/Then)API contract + happy‑path testUI with states: empty, loading, error, successTelemetry: event Docs: 5 lines in CHANGELOG + a short GIF for QAWeeks 5–6: Stabilize and prove valueAdd acceptance tests for the top flowsLoad test the slowest endpoint (target p95 < 500 ms)Backup + restore rehearsalObservability dashboard: errors, latency, signups, first‑success rateRelease notes → pilot usersThe MVP baseline checklistAuthentication with rate limiting and secure password storageAuthorization (least privilege)Input validation and request size limitsCentralized logging + error alertsDaily backups + restore testedFeature flags for risky changesBasic privacy page + terms; collect minimal PIIEstimation that doesn’t lieEstimate only the next two weeks. Use  for backlog and convert S/M/L to hours after splitting stories. Track only  story points to set the next sprint’s capacity.Prefer simple: a single Postgres, a single API service, a single web app. Add queues or microservices only for real bottlenecks. Complexity taxes you every day.Example backlog (first 6 weeks)Sign‑up/sign‑in, email verify, password resetOrg + roles (owner, user)Core object CRUD + searchEvent tracking + simple dashboardStripe test payments (if relevant)Admin toggles via feature flagsDocs: getting started + FAQWhat to measure (and why) % of signups completing first core task user‑perceived speed alerts per 1k requestsRetention (week‑over‑week): are users coming back?Staging auto‑deploys on merge; production behind a manual approval and feature flagRollback plan = previous container tag + DB migration down stepsPost‑release audit: top errors, time‑to‑fix, next mitigation fix a time box; ship to 5 real users pick one you already know more instances don’t cure poor queries—profile first track 3 events tied to your success metric; nothing moreOWASP ASVS (baseline security)Twelve‑Factor App (ops sanity)GitHub Actions marketplace test/lint actions]]></content:encoded></item><item><title>Own Your Edge: Control your AI</title><link>https://hackernoon.com/own-your-edge-control-your-ai?source=rss</link><author>Aniruddha Maru</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:41:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Learn how to deploy and manage Edge AI in retail with Kubernetes, GitOps, and containerization. Avoid the 95% pilot failure rate with proven strategies. ]]></content:encoded></item><item><title>How Online Stores Know What You’ll Buy Next: The Math Behind “Frequently Bought Together”</title><link>https://hackernoon.com/how-online-stores-know-what-youll-buy-next-the-math-behind-frequently-bought-together?source=rss</link><author>Raja Chakraborty</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:34:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Association Rule Mining helps computers find patterns automatically from huge amounts of data. It can be used to make better decisions like showing related products online or organizing store shelves smarter.]]></content:encoded></item><item><title>Clean Code: Functions and Error Handling in Go: From Chaos to Clarity [Part 1]</title><link>https://hackernoon.com/clean-code-functions-and-error-handling-in-go-from-chaos-to-clarity-part-1?source=rss</link><author>Vladimir Yakovlev</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:30:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Introduction: Why Go Functions Are SpecialI've reviewed over 1000 pull requests in Go over the past 6 years, and the same mistakes keep appearing. Remember your first Go code? It probably had dozens of  checks and 200-line functions that did everything at once. After analyzing over 50 Go projects, I've identified the main beginner problem: they write Go like Java or Python, ignoring the language's idioms.Common function problems I've seen:Functions over 100 lines: ~40% of codebasesMixed responsibilities: ~60% of functionsPoor error handling: ~30% of bugsMissing defer for cleanup: ~45% of resource leaksIn this article — the first in a Clean Code in Go series — we'll explore how to write functions you won't be ashamed to show in code review. We'll discuss the single responsibility principle, error handling, and why  is your best friend.Single Responsibility Principle: One Function — One JobHere's a typical function from a real project (names changed):// BAD: monster function does everything
func ProcessUserData(userID int) (*User, error) {
    // Validation
    if userID <= 0 {
        log.Printf("Invalid user ID: %d", userID)
        return nil, errors.New("invalid user ID")
    }

    // Database connection
    db, err := sql.Open("postgres", connString)
    if err != nil {
        log.Printf("DB connection failed: %v", err)
        return nil, err
    }
    defer db.Close()

    var user User
    err = db.QueryRow("SELECT * FROM users WHERE id = $1", userID).Scan(&user.ID, &user.Name, &user.Email)
    if err != nil {
        log.Printf("Query failed: %v", err)
        return nil, err
    }

    // Data enrichment
    if user.Email != "" {
        domain := strings.Split(user.Email, "@")[1]
        user.EmailDomain = domain

        // Check corporate domain
        corporateDomains := []string{"google.com", "microsoft.com", "apple.com"}
        for _, corp := range corporateDomains {
            if domain == corp {
                user.IsCorporate = true
                break
            }
        }
    }

    // Logging
    log.Printf("User %d processed successfully", userID)

    return &user, nil
}
This function violates SRP on multiple fronts:Manages database connections: A function should fit entirely on a developer's screen (roughly 30-50 lines). If you need to scroll — time to refactor.Let's refactor following Go idioms:// GOOD: each function has one responsibility
func GetUser(ctx context.Context, userID int) (*User, error) {
    if err := validateUserID(userID); err != nil {
        return nil, fmt.Errorf("validation failed: %w", err)
    }

    user, err := fetchUserFromDB(ctx, userID)
    if err != nil {
        return nil, fmt.Errorf("fetch user %d: %w", userID, err)
    }

    enrichUserData(user)
    return user, nil
}

func validateUserID(id int) error {
    if id <= 0 {
        return fmt.Errorf("invalid user ID: %d", id)
    }
    return nil
}

func fetchUserFromDB(ctx context.Context, userID int) (*User, error) {
    row := db.QueryRowContext(ctx, `
        SELECT id, name, email 
        FROM users 
        WHERE id = $1`, userID)

    var user User
    if err := row.Scan(&user.ID, &user.Name, &user.Email); err != nil {
        if errors.Is(err, sql.ErrNoRows) {
            return nil, ErrUserNotFound
        }
        return nil, err
    }

    return &user, nil
}

func enrichUserData(user *User) {
    if user.Email == "" {
        return
    }

    parts := strings.Split(user.Email, "@")
    if len(parts) != 2 {
        return
    }

    user.EmailDomain = parts[1]
    user.IsCorporate = isCorporateDomain(user.EmailDomain)
}
 (20 lines max)Has single responsibilityCan be tested independentlyError Handling: The Go WayBeginners often create the "pyramid of doom":// BAD: deep nesting
func SendNotification(userID int, message string) error {
    user, err := GetUser(userID)
    if err == nil {
        if user.Email != "" {
            if user.IsActive {
                if user.NotificationsEnabled {
                    err := smtp.Send(user.Email, message)
                    if err == nil {
                        log.Printf("Sent to %s", user.Email)
                        return nil
                    } else {
                        log.Printf("Failed to send: %v", err)
                        return err
                    }
                } else {
                    return errors.New("notifications disabled")
                }
            } else {
                return errors.New("user inactive")
            }
        } else {
            return errors.New("email empty")
        }
    } else {
        return fmt.Errorf("user not found: %v", err)
    }
}
Solution: Early Return (Guard Clauses)// GOOD: early return on errors
func SendNotification(userID int, message string) error {
    user, err := GetUser(userID)
    if err != nil {
        return fmt.Errorf("get user %d: %w", userID, err)
    }

    if user.Email == "" {
        return ErrEmptyEmail
    }

    if !user.IsActive {
        return ErrUserInactive
    }

    if !user.NotificationsEnabled {
        return ErrNotificationsDisabled
    }

    if err := smtp.Send(user.Email, message); err != nil {
        return fmt.Errorf("send to %s: %w", user.Email, err)
    }

    log.Printf("Notification sent to %s", user.Email)
    return nil
}
Error Wrapping: Context MattersSince Go 1.13,  with the  verb wraps errors. Always use it:// Define sentinel errors for business logic
var (
    ErrUserNotFound          = errors.New("user not found")
    ErrInsufficientFunds     = errors.New("insufficient funds")
    ErrOrderAlreadyProcessed = errors.New("order already processed")
)

func ProcessPayment(orderID string) error {
    order, err := fetchOrder(orderID)
    if err != nil {
        // Add context to the error
        return fmt.Errorf("process payment for order %s: %w", orderID, err)
    }

    if order.Status == "processed" {
        return ErrOrderAlreadyProcessed
    }

    if err := chargeCard(order); err != nil {
        // Wrap technical errors
        return fmt.Errorf("charge card for order %s: %w", orderID, err)
    }

    return nil
}

// Calling code can check error type
if err := ProcessPayment("ORD-123"); err != nil {
    if errors.Is(err, ErrOrderAlreadyProcessed) {
        // Business logic for already processed order
        return nil
    }

    if errors.Is(err, ErrInsufficientFunds) {
        // Notify user about insufficient funds
        notifyUser(err)
    }

    // Log unexpected errors
    log.Printf("Payment failed: %v", err)
    return err
}
Defer: Guaranteed Resource Cleanup is one of Go's killer features. Use it for guaranteed cleanup:// BAD: might forget to release resources
func ReadConfig(path string) (*Config, error) {
    file, err := os.Open(path)
    if err != nil {
        return nil, err
    }

    data, err := io.ReadAll(file)
    if err != nil {
        file.Close() // Easy to forget during refactoring
        return nil, err
    }

    var config Config
    if err := json.Unmarshal(data, &config); err != nil {
        file.Close() // Duplication
        return nil, err
    }

    file.Close() // And again
    return &config, nil
}
// GOOD: defer guarantees closure
func ReadConfig(path string) (*Config, error) {
    file, err := os.Open(path)
    if err != nil {
        return nil, fmt.Errorf("open config %s: %w", path, err)
    }
    defer file.Close() // Will execute no matter what

    data, err := io.ReadAll(file)
    if err != nil {
        return nil, fmt.Errorf("read config %s: %w", path, err)
    }

    var config Config
    if err := json.Unmarshal(data, &config); err != nil {
        return nil, fmt.Errorf("parse config %s: %w", path, err)
    }

    return &config, nil
}
Pattern: Cleanup Functionsfunc WithTransaction(ctx context.Context, fn func(*sql.Tx) error) error {
    tx, err := db.BeginTx(ctx, nil)
    if err != nil {
        return fmt.Errorf("begin transaction: %w", err)
    }

    // defer executes in LIFO order
    defer func() {
        if p := recover(); p != nil {
            tx.Rollback()
            panic(p) // re-throw panic after cleanup
        }

        if err != nil {
            tx.Rollback()
        } else {
            err = tx.Commit()
        }
    }()

    err = fn(tx)
    return err
}

// Usage
err := WithTransaction(ctx, func(tx *sql.Tx) error {
    // All logic in transaction
    // Rollback/Commit happens automatically
    return nil
})
// BAD: unclear purpose
func Process(data []byte) error
func Handle(r Request) Response
func Do() error

// GOOD: verb + noun
func ParseJSON(data []byte) (*Config, error)
func ValidateEmail(email string) error
func SendNotification(user *User, msg string) error
If more than 3-4 parameters — use a struct:// BAD: too many parameters
func CreateUser(name, email, phone, address string, age int, isActive bool) (*User, error)

// GOOD: group into struct
type CreateUserRequest struct {
    Name     string
    Email    string
    Phone    string
    Address  string
    Age      int
    IsActive bool
}

func CreateUser(req CreateUserRequest) (*User, error)
// BAD: boolean flags are unclear
func CheckPermission(userID int) (bool, bool, error) // what does first bool mean? second?

// GOOD: use named returns or struct
func CheckPermission(userID int) (canRead, canWrite bool, err error)

// BETTER: struct for complex results
type Permissions struct {
    CanRead   bool
    CanWrite  bool
    CanDelete bool
}

func CheckPermission(userID int) (*Permissions, error)
 (30-50 lines max) (Single Responsibility) (verb + noun) for errors with context () if can be cancelled (or clearly documented)Clean functions in Go aren't just about following general Clean Code principles. It's about understanding and using language idioms: early return instead of nesting, error wrapping for context, defer for guaranteed cleanup.In the next article, we'll discuss structs and methods: when to use value vs pointer receivers, how to organize composition properly, and why embedding isn't inheritance.What's your approach to keeping functions clean? Do you have a maximum line limit for your team? Let me know in the comments!]]></content:encoded></item><item><title>Culture and Leadership in Startups: Lessons from the Founder</title><link>https://hackernoon.com/culture-and-leadership-in-startups-lessons-from-the-founder?source=rss</link><author>Evgeny Pavlov</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:28:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When people talk about , many imagine framed value statements or trendy buzzwords on a wall. In reality, culture is a living, breathing force – the lifeblood of a company. It’s shaped most intensely by the  and the first handful of hires. I learned this the hard way: if you mess up culture in the first 10–20 people, everything else will suffer. A strong culture can propel a startup through challenges, while a weak or toxic culture can sink it before it scales. As Tony Hsieh of Zappos famously put it, “If you get the culture right, most of the other stuff will just happen on its own.” In this article, I’ll share lessons from my journey as a founder on building culture and practicing leadership that scales.Founder’s Behavior Sets the ToneI always start with  The founder’s behavior sets the example that everyone in the company will follow, consciously or not. How I respond under pressure, how I celebrate wins, or how I handle failures, all of it becomes a model for the team. You can write nice values on paper, but if your team sees the founder acting against those values, the words mean nothing. Leaders must  they want to see. Research confirms this: employees who trust their leaders (because leaders walk the talk) have higher job satisfaction and commitment. Jack Welch once said, “Trust happens when leaders are transparent.” Being open and honest as a leader builds credibility. If I stay calm during a crisis, ask for help when needed, or admit mistakes, I signal that these behaviors are part of our culture. Every decision and action from the top trickles down.Hiring Early Team Members WiselyIn an early-stage startup, every hire is critical. The cliche “hire A-players” exists for a reason. Those first 10–20 people define your startup’s DNA. One wrong hire with a toxic attitude can undo months of hard work and poison the well. I call it the “one wolf can kill a hundred sheep” problem – one toxic person can destroy a positive team of dozens. This isn’t just anecdotal. A Harvard study found that avoiding a single toxic employee can save a company  as much money as hiring a superstar performer, once you factor in the damage toxic workers do to morale and productivity. In other words, one bad apple truly can spoil the bunch.That’s why I personally interview almost everyone we hire in the early days. I look beyond skills and resume bullet points – I probe for attitude, adaptability, and cultural fit. One of my guiding principles: “hire for character and potential, train for skill.” You can teach someone to code in a new language, but you can’t easily teach humility, resilience, or integrity. Other founders echo this: . As investor Krishna Rangasayee notes, technical chops matter, but “at the end of the day, you can teach skills… What you can't teach is how to fit into a culture.” We prioritize candidates who align with our values and mission. Some people might look “average” on paper but have incredible growth potential and attitude – those are often the gems. Conversely, I’ve seen people with shiny CVs who brought subtle toxicity or ego that eroded teamwork.  if you must, but get those first hires right. It sets the foundation for everything. And if someone toxic does slip through, remove them quickly – the cost of keeping them is far higher than the cost of an empty seat.Transparent Leadership and TrustLeadership is about more than giving orders; it’s about . I’ve learned to be brutally transparent with my team about what’s happening, even when the news isn’t good. If revenue is down or a product launch fails, everyone knows the numbers and the situation. Why? Because people are smart – they will spot lies or half-truths a mile away, and nothing erodes trust faster than a feeling that leadership is hiding something. Conversely, honesty (even when uncomfortable) builds trust faster than any sugarcoating.In our company, we practice open communication about wins and losses. When we miss a target, I explain why and what we’ll do next. When we land a big client, I share the details and credit the team. This creates a culture where news – good or bad – isn’t a taboo but something we tackle together. The payoff is a team that trusts leadership and feels invested. There’s research to back this up: employees who trust their leader are more committed and less likely to quit. Transparency is indeed a lifeline, especially as we embraced remote work (more on that soon). We’ve adopted rituals like weekly all-hands video calls, monthly updates on key metrics, and an open dashboard of our KPIs. By providing context and explaining decisions, I empower team members to make better choices on their own. In short, transparency breeds accountability and unity. It shows respect for your team’s intelligence and creates a shared reality where everyone is pulling in the same direction.Control vs. Empowerment: Real LeadershipMany first-time founders (my past self included) confuse  with leadership. It’s tempting to micromanage every detail, thinking that if you’re involved in everything, nothing can go wrong. But true leadership is often the opposite: letting people own their work and stepping in only where you add value. Micromanagement is a trap that kills ownership and signals a lack of trust. It might satisfy your inner control freak, but it will demoralize your team and slow you down. As our company grew, I had to learn to stop steering every wheel and instead guide the ship by setting direction and context.When you micromanage, you send a message that you don’t trust your team’s judgment. Over time, employees become disengaged and dependent, or they leave. Studies highlight this contrast: “Micromanagement kills ownership and promotes a lack of trust. In contrast, empowerment cultivates a culture of trust [and] energizes initiative.”. I experienced this firsthand. Early on, I was reviewing every piece of code, every marketing blurb. Eventually, I realized I was smothering the very talented people I hired. So I shifted: I set clear expectations and let them figure out  to meet them. I focused on providing support and removing roadblocks rather than doing their jobs. The transformation was immediate – people took , productivity soared, and I was less stressed.Leadership is not about commanding every move but enabling your team to make the right moves. This is even more true with experienced senior hires – they . Give it to them and they’ll amaze you; stifle them and you’ll lose them.The Challenges of Remote, Cross-Border TeamsOur startup is fully remote and globally distributed, which adds another layer of leadership challenge. When your team is spread across cities and time zones, you lose the casual office chatter and nuance of face-to-face interactions. Culture doesn’t form around a ping-pong table or free lunch – you have to  in a remote setting. We discovered that  is key. In a co-located office, a new development might spread via hallway conversations; remotely, nothing spreads unless you communicate it. So we established rituals and systems to keep everyone aligned and connected.Some practices that worked for us: daily stand-up check-ins in a chat channel, weekly written summaries of what each team accomplished, virtual “demo days” to show off work, and non-work social calls (yes, even a remote team can have a coffee break together on Zoom). These rituals give shape to our weeks and reinforce our values. Team rituals can foster a sense of belonging and glue a remote team together. For example, every Friday we have a casual video call where we share one success, one challenge, and often a fun personal update. It sounds simple, but these little traditions create camaraderie.In cross-border teams, you have different cultures – some folks are very direct, others more indirect. Miscommunications happen easily. To counter this, we wrote down guidelines for how we collaborate: for instance,  in messages (since text can seem curt without tone), and “Don’t confuse silence with agreement” (encouraging people to speak up). We encourage : if something is important, put it in writing and ensure everyone sees it (we use shared docs and Slack channels instead of siloed DMs whenever possible). One quote from a remote-work expert stuck with me: “Effective and open communication is absolutely crucial in a remote team. Without it, everything falls apart.”. We took that to heart by making transparency our default. Almost all conversations and decisions happen in the open.Balancing Perfectionism with SpeedConfession: I used to be a perfectionist founder. I obsessed over tiny details – pixel-perfect UI spacing, every comma in a blog post, the exact shade of button colors. High standards are valuable, especially when you’re crafting a product, but perfectionism can become a double-edged sword. In a startup,  is often more critical than getting everything 100% perfect. If you wait for perfection, you might miss the market or let competitors overtake you. I had to learn to balance my perfectionist tendencies with the necessity of rapid iteration and innovation. So now, instead of polishing every detail endlessly, we aim for “very good” and then launch and learn. We still value quality – our culture is ambitious about excellence, but we’ve embraced a mantra of .This was a tough personal growth area for me. Perfectionism often comes from fear – fear of failure or looking foolish. But a culture overly afraid of mistakes is a culture that . I intentionally encourage my team (and myself) to experiment and accept that some things will fail. We even celebrate failures in a way, by conducting blameless post-mortems and sharing lessons learned. It’s about continuous improvement. As long as we maintain high integrity and fix critical bugs fast, a typo or a suboptimal feature is not the end of the world. Far worse is a culture where people are too scared to take initiative or where product iterations grind to a halt because someone upstairs insists on pixel-perfection.Technical Skills vs. Soft Skills in LeadershipAnother hard lesson: technical expertise alone doesn’t make a founder a good leader. Many startup founders are technical geniuses – they know their product and codebase inside out. That was me; I initially felt my job was to be the smartest person in the room. But I learned that being the smartest coder or best product architect doesn’t automatically translate to leading a team. In fact, overly technical founders can fall into the trap of dominating discussions, dismissing others’ ideas, or doing things themselves instead of teaching others. True leadership requires : communication, empathy, listening, and the ability to inspire and develop others.There’s plenty of evidence that  and people skills matter more than IQ in leadership roles. A CareerBuilder survey of hiring managers found 71% of employers value emotional intelligence more than pure IQ or technical skill in employees. Most said they’d even  a technically brilliant candidate if they lacked people skills, and 75% said they’re more likely to promote someone with high EQ over high IQ. Why? Because leadership is fundamentally about people. You manage things, but you . If you can’t relate, motivate, and adapt to your team’s needs, your technical knowledge won’t save you.I had to pivot from being a hands-on tech contributor to a . I still use my technical knowledge – but now it’s to ask the right questions in design reviews, to mentor junior engineers, or to set a high-level technical vision. The goal is to empower others with my knowledge, not to use it to win every argument or make every decision. In fact, Google’s famous Project Oxygen study on management found that among the top traits of great managers were being a good coach, empowering the team, expressing interest in team members’ success, etc., while technical skill ranked much lower down the list. The best technical leaders I’ve seen use their expertise to , not to belittle or micromanage. They are quick to admit when someone else knows more in a domain, showing .Leadership in a 5-person startup looks very different from leadership in a 50-person or 500-person company. Early on, as a founder, you wear all the hats – you’re involved in every decision from product design to hiring to setting up the office Wi-Fi. But if things go well, you reach a point where that’s not sustainable (and you become a bottleneck if you try). The role of the founder-CEO . I went through this transition: from a do-everything entrepreneur to a more process-oriented leader who works  the business rather than  the business.In the earliest stage (1–10 people), I found it effective to be very hands-on. It was about survival and rapid iteration, so having the founder deeply involved everywhere made sense. But once we had team leads and a bit of structure, I had to step back and . I stopped attending every meeting and focused on creating the  for others to succeed. This meant establishing an operating system for the company – things like goal-setting, OKRs, clear org charts and responsibilities, communication cadences, and cultural norms and then letting those systems run with occasional tweaks. In essence, my job shifted to  that builds the product, instead of directly building the product myself.This can be a tough shift for founders (it certainly was for me) because it feels like letting go of your “baby.” But letting go, done right, is incredibly empowering for your team and freeing for you. As one Entrepreneur article headline put it: “Letting go of control was the hardest — and smartest — move I ever made.” The author noted that delegating and sharing ownership built trust, boosted creativity, and empowered the team. I experienced the same. The more I let talented people take charge of their domains, the more innovation bubbled up without my intervention. Delegation also freed up my time to focus on long-term strategy, fundraising, and big partnerships – the things a CEO  be doing to unlock the next level of growth.Finally, a bit of heartfelt advice to fellow and future founders: build a company culture you’d be proud of even if the venture fails. Startups are risky – not all succeed in terms of product-market fit or profit. But no matter what happens, the culture you create is a legacy in its own right. If you foster an environment of learning, respect, and passion, that will impact people’s careers and lives positively, far beyond the lifespan of the startup. I’ve had employees tell me that working at our startup – even during rocky times – was the most growthful period of their careers because of the culture we cultivated. That means more to me than any revenue milestone.Culture is . It survives pivots, market crashes, and even company failures. Conversely, a toxic culture can destroy a company at its peak. So treat culture as your most precious asset. It’s not a fluffy HR thing; it’s arguably  important than your product strategy. If your only goal as a founder is to flip a quick profit, you might be tempted to cut corners – hire that brilliant jerk, tolerate a bad behavior because someone is “critical,” chase a revenue opportunity that violates your values, and one day you wake up hating the company you’ve created (and likely, your team will too). I strongly believe . One wrong hire or ethical lapse for short-term gain can torpedo everything. Profit is important (you need a sustainable business), but profit will come  of a strong culture that delivers great products and services. It shouldn’t come  of culture.So make choices you can be proud of. Define success in cultural terms, not just financial ones. For me, a win is when I see team members solving problems together without me, or when a new hire says, “I’ve never been in a workplace this open and motivating,” or when our alumni go on to spread our cultural values in other companies. Those things indicate we built something  And ironically, a healthy culture tends to lead to better business outcomes anyway – engaged, happy teams build better products and give better customer service. There’s data on this as well: companies with healthy cultures have lower turnover and higher performance. A toxic culture, on the other hand,  companies billions in turnover and disengagement.Focus on culture as much as your product. Culture is what remains when products pivot and strategies change. It’s in every decision, every conversation, every email, every hire, every celebration of a win, and every post-mortem of a failure. It’s how your company  from the inside, and how that radiates to customers and partners on the outside. As a founder, you are the initial architect of this culture. Invest in it mightily. If you do it right, your startup will not only have a better chance of success but be the kind of company you’re proud to lead, and others are proud to work for, no matter what the outcome.Key Takeaways for First-Time FoundersYour behavior sets the cultural tone. Embody the values you preach, especially under pressure. Trust and transparency from leadership foster employee trust and loyalty.Hire for Culture Fit & CharacterEarly hires determine your startup’s destiny. Prioritize cultural alignment and attitude over just skills. One toxic hire can cost more than several great performers add –  compromise on character.Share context and news (good and bad) openly with your team. Honest communication builds trust and prevents rumor mills. A culture of openness keeps everyone aligned and accountable.Empower, Don’t MicromanageFounders must learn to let go. Give teams ownership of their work and authority to make decisions. Micromanagement erodes morale and trust, whereas empowerment creates a proactive, engaged culture.Be Intentional in Remote CultureFor distributed teams, over-communicate and establish rituals to build cohesion. Use tools and regular check-ins to ensure no one feels isolated. Maintain a strong sense of mission and shared values across borders.Mission and Purpose MatterRally your team around a clear mission. Mission-driven teams have higher engagement and resilience. Align decisions with your core purpose to avoid short-term thinking.Balance Ambition with HumanitySet ambitious goals, but watch for burnout. Encourage a mindset of learning from failures. Recognize and reward efforts, and don’t punish well-intentioned mistakes. A supportive environment will achieve more in the long run.Value Soft Skills in LeadershipEmotional intelligence, communication, and empathy are as crucial as technical savvy (if not more). Great founders grow into  and , not just chief technicians. Invest in your leadership skills like you do in product skills.Evolve and Scale YourselfContinuously adapt your leadership style as the company grows. Implement systems and processes to replace ad-hoc management. Delegate decisions and trust your team – this builds a scalable culture of empowerment.Protect and Nurture CultureTreat culture as your most valuable asset. Don’t sacrifice it for short-term gains. Build a company you admire for its principles and environment, not just its profits. In the end, culture is the legacy that endures in your team’s hearts and careers, whatever the outcome of the startup.By keeping these lessons in mind, first-time founders can avoid common pitfalls and create a startup culture that is not only enjoyable and ethical but also a competitive advantage. Build a company that you and your team will be proud of – one where the culture is , resilient, and truly reflective of the best you have to offer as a leader.]]></content:encoded></item><item><title>Clean Architecture; How To Keep Your Codebase Scalable Without Overengineering</title><link>https://hackernoon.com/clean-architecture-how-to-keep-your-codebase-scalable-without-overengineering?source=rss</link><author>Aleeza Adnan</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:27:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Clean Architecture has turned into the “eat more protein” advice of software engineering. Everyone repeats it. Everyone shares the same circle diagram. Almost no one talks about what it’s actually like to  with it.When I first tried applying it, I kept asking myself: am I writing better code or just more files?Most articles treat Clean Architecture like a theory exam, abstract entities, endless layers, and rules that fall apart the moment a real feature ships. But I don’t write code for diagrams. I write code that has to change, stay testable, and not drive me insane as the product grows.So instead of another diagram, I want to talk about what Clean Architecture looks like in practice the parts worth keeping, the parts that just slow you down, and how to evolve a version that actually works in real projects.What The Layers Actually Look LikeIn my current project, a fairly large one, I’ve settled on a structure that’s simple enough to reason about but strong enough to grow. It follows the usual three layers: , , and . What matters most isn’t the names or number of layers, but how they depend on each other and how responsibilities are divided.==This setup is tuned for mobile development. If you’re working on a web app, the same ideas apply, but the outer layers will look a little different. Where I have background services, local database managers, and offline sync, a web app might rely on browser storage, API caching, or server-driven state. The principles stay identical clean but the implementation details shift with the platform.==The  handles the user-facing side: screens, widgets, and a View Model (VM) for each feature. The VM only talks to the domain layer. That rule alone prevents a lot of accidental coupling and keeps UI code clean.The  defines how the app behaves. It contains all , small, focused pieces of logic that represent what the app . It also declares the , which act as boundaries between the domain and data layers. The data layer can’t just do whatever it wants; it has to conform to these promises. If there’s code in the data layer that no one calls, it simply dies off on its own.The  provides the actual implementations for those repositories. These call into  that handle background events, local storage, and communication with external systems through the . The background service, in turn, keeps things running smoothly, syncing data, writing to the database, and managing API calls, without the rest of the system needing to know the details.All  live in the domain, shared across layers. That keeps data models consistent and avoids the constant mapping that usually clutters large codebases.This setup may look heavy at first, but in practice it saves time and stress. Clear boundaries make it easier to change things without worrying about breaking something unrelated. When a new feature comes in, I know exactly where it belongs. If a bug shows up, I can usually trace it to the right layer within minutes.Over time, this separation of concerns has proven to be the main reason the codebase stays manageable. It’s not about following a pattern perfectly, it’s about keeping the system loosely coupled enough that it can keep growing without turning fragile.What I Learned the Hard WayEarly on, I thought Clean Architecture was about layers and abstractions. It’s not. It’s about staying in control when your codebase starts growing faster than you can refactor. Here are some of the things I learned, some of these may sound like , let me tell you this: IT WAS’NT, at least to me.A layer is only useful if it protects you from something, framework churn, backend changes, or accidental coupling. If it doesn’t, it’s just ceremony.Abstractions should earn their place, Don’t create a repository interface unless you can imagine a second implementation. Theoretical flexibility is just another word for clutter.Domain should never depend on UI frameworks or database details. Every time I ignored that rule, debugging felt like trying to untie headphones.This structure doesn’t make development faster, it makes it . You actually see this pay off when you’re shifting your backend to another platform or maybe redoing your entire UI, THE APP SURVIVES AND SO DO YOU!]]></content:encoded></item><item><title>How to Understand Any Codebase in 5 Minutes Using an AI Coding Assistant</title><link>https://hackernoon.com/how-to-understand-any-codebase-in-5-minutes-using-an-ai-coding-assistant?source=rss</link><author>James</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:25:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[You just cloned a repo. It’s 15,000 lines of code, there’s no documentation, and the README was last updated three years ago. Where do you even start?That’s a situation almost every developer has faced — whether you’re joining a new team or diving into an open-source project for the first time. Understanding how a codebase works can take hours, sometimes days, before you feel confident making changes.But with the rise of , that process is getting a lot easier. Tools powered by large language models can now  your codebase, understand how it’s structured, and even summarize what each folder or file is for. What used to be a slow, manual process can now take just a few minutes.In this walkthrough, I’ll show you how to do it using  inside , specifically leveraging its feature called . You can use the same concept with other assistants that offer project memory or context features like  or Step-by-Step: Understanding a Codebase with Kilocode \nStart by cloning the repository you want to analyze:   git clone https://github.com/your-project.git 
This could be a company project, an open-source repo, or even one of your old codebases that you want to revisit.Install Kilocode in VS CodeIf you haven't used Kilocode before, install it from the marketplace in VS Code (if you're using VS Code).Once it's set up, open your cloned project in VS Code.The  is what allows Kilocode to “remember” and understand your project context.In your project directory, create:   .kilocode/rules/memory-bank/
Inside that folder, add a new file named  and write a short, high-level overview of the project: what it does and why it exists.Next, create another file called:   .kilocode/rules/memory-bank-instructions.md
and paste the content from this document. This helps guide Kilocode’s analysis process.Initialize the Memory BankNow let’s get Kilocode to analyze the project:Switch Kilocode to .Make sure you’re using a high-quality model (avoid lightweight ones).Ask Kilocode to “initialize memory bank.”Wait while it analyzes your entire codebase and builds the documentation.Review the generated files to ensure the descriptions make sense. Edit or refine them as needed.Review the Generated DocsOnce the analysis is complete, Kilocode will create several Markdown files describing your project. These include: — Explains why the project exists, the problems it solves, and the intended user experience. — Tracks current work, recent changes, and next steps for development. — Maps out the system architecture, design patterns, and key component relationships.— Lists frameworks, dependencies, environment setup, and tool configurations.These files effectively serve as auto-generated documentation — giving you a complete picture of the project’s structure and purpose in minutes.Focus especially on  and , since they provide the clearest view of how the system works and what technologies it relies on.I used  as an example project for this tutorial.\
What you’re seeing above are examples of the  generated by Kilocode, including documentation like  and .Each file provides structured insights into the codebase and together; these files give you a clear overview of the project, the kind of understanding that would normally take hours to piece together manually.Instead of scrolling through endless directories or guessing at folder names, you instantly see how the codebase is organized and what each part does.In just a few minutes, you’ve gone from staring at an unfamiliar repo to understanding its logic, structure, and purpose; without digging through every file by hand.What used to take hours of manual exploration; tracing dependencies, scanning files, and guessing architecture, can now be done in minutes with AI assistance. Tools like Kilocode don’t just summarize code; they transform how we onboard, document, and collaborate on software projects.By combining structured analysis with natural-language explanations, AI coding assistants let developers spend less time  the codebase and more time  it. Whether you’re joining a new team, contributing to open source, or auditing an old project, this approach makes understanding large codebases faster, clearer, and far less painful.The rise of AI (and especially large language models) has opened up countless ways to improve how we code and collaborate. One of the most underrated use cases is accelerating code comprehension — turning a time-consuming onboarding process into a quick, structured overview.AI coding assistants aren’t here to replace developers, but they can dramatically boost our productivity. By offloading repetitive cognitive tasks like summarizing code or tracing architecture, developers can spend more time doing what matters: designing, building, and solving real problems.In an era where new AI tools appear every week, it’s worth keeping an open mind and experimenting. You might discover that your next favorite “teammate” is an AI agent that helps you understand codebases faster than ever before.Let’s keep the conversation going! I share daily thoughts, resources, and questions on X about AI, tech, and building in public. Join me here 👉 ]]></content:encoded></item><item><title>Why Power-Flexible AI Just Became Table Stakes</title><link>https://hackernoon.com/why-power-flexible-ai-just-became-table-stakes?source=rss</link><author>Asit Sahoo</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:23:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I've spent the last few months looking at data center deals, and I keep running into the same wall: power. Not chips, not real estate, not even capital. Just boring old electricity.The numbers are brutal. A single AI facility can require 96 megawatts, enough to power a small city. And unlike traditional data centers that hum along at steady capacity, AI workloads spike unpredictably. You might go from 30% utilization to 95% in an hour when a new model training run kicks off.This creates a nightmare for grid operators. They have to provision for your peak demand, even if you only hit it 10% of the time. And communities are starting to notice. I've watched deals fall apart because local utilities couldn't guarantee the capacity, or city councils rejected permits after residents complained about rate increases.So when I saw the announcement this morning about Emerald AI's Manassas facility, I almost scrolled past it. Another hyperscale build, another "AI-ready" marketing pitch. But when I dug into the technical architecture, I realized this is different.NVIDIA, Emerald AI, EPRI, Digital Realty, and PJM announced the Aurora AI Factory, a 96 MW facility in Manassas, Virginia, slated to open in the first half of 2026. The core idea: what if the data center could negotiate with the grid in real time?Emerald's Conductor platform sits between NVIDIA's orchestration layer and PJM's grid signals. When renewable generation drops or demand spikes, it can slow down or pause non-critical model training, reroute inference jobs to less congested data centers, and modulate power draw depending on renewable generation and peak demand, while maintaining acceptable Quality of Service for training and inference.In other words, they've built interruptible compute into the architecture. The facility essentially becomes a variable load instead of a fixed drain.What Makes This InvestableHere's what caught my attention from a diligence perspective. The software capabilities that Arushi Sharma Frank (Emerald's senior adviser on power and utilities) detailed in Utility Dive this morning show this isn't vaporware.The system can deliver targeted 20-30% power reductions for multi-hour windows during grid peaks, with no snap-back surge afterward. It can sustain curtailments for up to 10 hours. It responds to both rapid (10-minute) and planned (2-hour) dispatch signals. And critically, it can participate in wholesale electricity markets by mapping locational marginal prices into dispatchable bid curves.From an investment perspective, this matters because it changes the unit economics. Utilities are more willing to approve facilities that reduce peak load rather than add to it, which means faster interconnection. Variable loads pay less than fixed loads in most tariff structures, which means lower capacity charges. The facility can sell demand response services back to the grid, creating new revenue streams. And perhaps most importantly, this makes data centers politically defensible, creating a regulatory tailwind.The proof is in their earlier testing. A demonstration showed Emerald AI can reduce AI workload power consumption by 25% over three hours during a grid stress event, while ensuring acceptable performance. That's measured, not modeled.The Market Structure QuestionNow here's where I get skeptical. They claim that if this reference design were adopted nationwide, it could unlock an estimated 100 GW of capacity on the existing electricity system, equivalent to 20% of total U.S. electricity consumption in a year.That feels optimistic and assumes perfect coordination across thousands of facilities. But the directional concept is sound. If you can make AI compute interruptible without breaking SLAs, you solve two problems: you reduce infrastructure costs, and you make data centers politically palatable again.The real test will be whether customers accept the tradeoff. Training runs that take 36 hours instead of 24 because you're opportunistically using cheaper off-peak power? Some will bite. Others won't. The phrase "acceptable Quality of Service" is doing a lot of work here. It means some workloads will run slower or pause when the grid needs relief.What I'm watching for: whether this creates a two-tier market. Latency-sensitive inference stays on traditional fixed-capacity infrastructure, while cost-sensitive training migrates to flex-power facilities. If that split happens, the economics of data center real estate start looking very different, and so do the returns.The Aurora facility will serve as a live innovation hub, running demonstrations with EPRI's DCFlex Initiative to validate performance during heatwaves, renewable shortfalls, and peak loads. Real-world proof matters more than whitepapers at this point.Bottom Line for Infrastructure InvestorsWe're past the point where you can just throw more diesel generators at the problem. The grid won't allow it, permitting won't support it, and the math doesn't work. Power flexibility isn't a nice-to-have anymore. It's table stakes for the next wave of deployment.For anyone evaluating data center infrastructure plays, the questions to ask are shifting. Can the facility participate in demand response programs? What's the economic model for interruptible versus fixed capacity? How does power flexibility affect interconnection timelines? What percentage of workloads can actually tolerate curtailment?The announcement came from Virginia Governor Glenn Youngkin this morning, calling it critical for both AI competitiveness and grid affordability. That tells you how serious the political pressure has become around data center power consumption.We'll see if the tech scales. But at least someone's solving the right problem.]]></content:encoded></item><item><title>How to Build Your First MCP Server using FastMCP</title><link>https://hackernoon.com/how-to-build-your-first-mcp-server-using-fastmcp?source=rss</link><author>Manish Shivanandhan</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:23:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Model Context Protocol, or MCP, is changing how large language models connect with data and tools.Instead of treating an AI model as a black box, MCP gives it structured access to information and actions.It is like the USB-C port for AI, creating a standard way for models to interact with servers that hold real-world data or perform useful tasks.FastMCP is the easiest and fastest framework for building MCP servers with Python. It hides all the complex protocol details and lets you focus on your logic.In this guide, you will learn what MCP is, how FastMCP works, and how to build and run your first MCP server from scratch.Creating Your First MCP ServerConnecting with an MCP ClientAuthentication and SecurityDeploying Your MCP ServerUsing the MCP Server with an LLM ApplicationMCP is a standard protocol that allows language models to talk to external systems in a secure and consistent way. MCP is similar to an API, but built for large language models instead of humans.An MCP server can do three main things.It can expose data as resources (similar to GET endpoints)Provide actions through tools (similar to POST requests)Define prompts that guide how the model interacts with data or users.For example, a resource might return a list of articles, a tool might analyze those articles, and a prompt might define how the model summarizes them. By connecting an LLM to such an MCP server, you give it the power to use your own data and logic in real time.While you could build an MCP server using the official SDK, FastMCP takes things much further. It is a production-ready framework with enterprise authentication, client libraries, testing tools, and automatic API generation.You can use FastMCP to build secure, scalable MCP applications that integrate with providers like Google, GitHub, and Azure. It also supports deployment to the cloud or your own infrastructure.Most importantly, the framework is extremely developer-friendly. You can create a working MCP server in just a few lines of Python code.Creating Your First MCP ServerBefore you start building, install FastMCP in your Python environment. You can use pip or uv. The uv tool is recommended because it handles environments and dependencies efficiently.Once installed, you are ready to write your first server.Every MCP server starts with the  class. This class represents your application and manages your tools, resources, and prompts. Let’s start by creating a simple server that adds two numbers together.Create a file named  and add the following code:from fastmcp import FastMCP

mcp = FastMCP("Demo Server 🚀")

@mcp.tool
def add(a: int, b: int) -> int:
    """Add two numbers and return the result"""
    return a + b
if __name__ == "__main__":
    mcp.run()
That is all you need. You have just created a fully working MCP server with one tool called . When a client calls this tool, the server adds two numbers and returns the result.To run your server locally, open your terminal and type:This command starts the MCP server. You can also use HTTP or SSE transports for web-based deployments. For example, to run your server over HTTP, use:mcp.run(transport="http", host="127.0.0.1", port=8000, path="/mcp")
Once the server is running, clients can connect and call the  tool remotely.FastMCP tools are simple Python functions that you decorate with . You can add as many as you like. Let’s add a multiplication tool next:@mcp.tool
def multiply(a: float, b: float) -> float:
    """Multiply two numbers"""
    return a * b
You can now run the server again, and clients will have access to both the  and  tools.FastMCP automatically generates schemas based on your function signatures and docstrings, making it easy for clients to understand your API.Resources in MCP represent read-only data that clients can access. You can create static resources or dynamic templates that take parameters. For example, you might expose a version number or a user profile.@mcp.resource("config://version")
def get_version():
    return "1.0.0"

@mcp.resource("user://{user_id}/profile")
def get_profile(user_id: int):
    return {"name": f"User {user_id}", "status": "active"}
In this example, the first resource always returns the version number, while the second resource dynamically fetches a user profile based on the ID provided.FastMCP allows you to access the session context within any tool, resource, or prompt by including a  parameter. The context gives you powerful capabilities like logging, LLM sampling, progress tracking, and resource access.Here is an example that shows how to use context:from fastmcp import Context

@mcp.tool
async def summarize(uri: str, ctx: Context):
    await ctx.info(f"Reading resource from {uri}")
    data = await ctx.read_resource(uri)
    summary = await ctx.sample(f"Summarize this: {data.content[:500]}")
    return summary.text
This tool logs a message, reads a resource, and then asks the client’s language model to summarise it. Context makes your MCP tools smarter and more interactive.Connecting with an MCP ClientOnce your server is running, you can connect to it using the  class. The client can communicate via STDIO, HTTP, or SSE, and can even run in-memory for testing.Here is a simple example of connecting to your local server and calling the  tool:from fastmcp import Client
import asyncio

async def main():
    async with Client("server.py") as client:
        tools = await client.list_tools()
        print("Available tools:", tools)
        result = await client.call_tool("add", {"a": 5, "b": 7})
        print("Result:", result.content[0].text)
asyncio.run(main())
You can also connect to multiple servers using a standard MCP configuration file, making it easy to build complex systems that interact with several services simultaneously.Authentication and SecurityWhen you move from development to production, authentication becomes important.FastMCP has built-in support for enterprise-grade authentication providers such as Google, GitHub, Microsoft Azure, Auth0, and WorkOS. You can enable secure OAuth-based access with just a few lines of code.from fastmcp.server.auth.providers.google import GoogleProvider
from fastmcp import FastMCP

auth = GoogleProvider(client_id="...", client_secret="...", base_url="https://myserver.com")
mcp = FastMCP("Secure Server", auth=auth)
Now only authenticated users can access your server. On the client side, you can connect using an OAuth flow like this:async with Client("https://secure-server.com/mcp", auth="oauth") as client:
    result = await client.call_tool("protected_tool")
FastMCP handles tokens, refreshes, and error handling automatically.Deploying Your MCP ServerYou can deploy FastMCP servers anywhere.For testing, the  command is enough. For production, you can deploy to FastMCP Cloud, which provides instant HTTPS endpoints and built-in authentication.If you prefer to self-host, use the HTTP or SSE transport to serve your MCP endpoints from your own infrastructure. A simple deployment command might look like this:mcp.run(transport="http", host="0.0.0.0", port=8080)
Once deployed, your MCP server is ready to connect with language models, web clients, or automation workflows.Using the MCP Server with an LLM ApplicationOnce your MCP server is running, the next step is to connect it to a large language model. This allows an LLM to securely call your server’s functions, read resources, and perform actions as part of a conversation.To connect an LLM application, you first define your MCP configuration file. This file lists the available servers, their connection methods, and any authentication requirements.Once configured, the LLM can automatically discover your MCP tools and call them when needed.For example, if your server exposes an  or  tool, the model can directly use them as if they were built-in capabilities. In a chat-based environment, when a user asks the model to perform a task such as “Summarize the latest article,” the LLM will call your  tool, process the result, and respond with the output.If you are building a custom LLM application with frameworks like OpenAI’s Assistants API or LangChain, you can register your MCP server as an external tool. The LLM then interacts with it through the MCP client library.Here is a simple example:from fastmcp import Client
from openai import OpenAI
import asyncio

async def main():
    # Connect to your MCP server
    async with Client("http://localhost:8000/mcp") as client:
        # Call an MCP tool directly
        result = await client.call_tool("add", {"a": 10, "b": 5})
        print("MCP Result:", result.content[0].text)
        # Use the result inside an LLM prompt
        llm = OpenAI(api_key="YOUR_KEY")
        response = llm.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an AI assistant using MCP tools."},
                {"role": "user", "content": f"The sum of 10 and 5 is {result.content[0].text}. Explain how MCP helps with this integration."}
            ]
        )
        print(response.choices[0].message.content)

asyncio.run(main())
In this setup, the LLM can seamlessly combine its reasoning with your server’s logic. It uses the MCP client to fetch data or perform computations and then incorporates the output into its conversation or workflow.This approach lets you build intelligent systems that go beyond static prompts. You can connect your LLM to real databases, APIs, or automation tools, turning it into an active agent that can read, write, and execute with real-world context.FastMCP makes it simple to bring your data, APIs, and tools into the world of AI through the Model Context Protocol. With just a few lines of Python, you can create powerful MCP servers that connect to language models, automate workflows, and handle real-world logic securely.Whether you are building a quick demo or an enterprise-grade system, FastMCP gives you the shortest path from idea to production. Install it today, start your first server, and explore how MCP can unlock the next level of AI integration.]]></content:encoded></item><item><title>From Chaos to Quality: A Framework for AI-Assisted Development</title><link>https://hackernoon.com/from-chaos-to-quality-a-framework-for-ai-assisted-development?source=rss</link><author>rdondeti</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:20:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How I turned 11 months of prompt engineering lessons into a system that makes AI coding safe, auditable, and production-readyMy Saturday started like any other vibe coding session. Coffee ready, music on, Claude Code open. I was building a feature and feeling productive until I hit the API selection.The AI confidently chose an API. I implemented it. Tests failed. I asked the AI to fix it. It chose the same API again. Different approach, same deprecated API. After the fifth iteration, I realized: the AI was stuck in a loop, and it was absolutely convinced this deprecated API was the right choice."But this API is deprecated," I'd say."You're right, let me use the current one," it would respond.Next prompt? Back to the deprecated API. With complete confidence.That moment crystallized a problem I'd been dancing around for months: vibe coding is powerful, but it's also frustrating as hell. And I'm not alone in this realization.The AI development community is having a reckoning. We've moved past the honeymoon phase of "wow, AI can code!" and into the uncomfortable reality of production deployments. The criticisms are everywhere:"AI coding tools create security holes""The context window breaks on real projects""My AI gets stuck in infinite loops""This code isn't production ready""It's just glorified autocomplete - another hype bubble"As an Engineering Manager with 18 years of experience and founder of DS APPS Inc, I've seen both sides. I've used AI to ship Android apps and open-source projects faster than ever before. But I've also seen the chaos that emerges when you let AI run wild without structure.So I spent the past 8-12 months mastering prompt engineering, learning from failures, and documenting what actually works. The result is  - a framework that turns any AI coding assistant into a disciplined software development team.And here's the radical part: it's not software. It's pure prompt engineering.The Three Struggles That Led to DevFlowMy journey with AI coding tools started like most developers: excitement mixed with frustration. As I pushed beyond simple scripts into real projects, three critical problems emerged:1. The Prompt Paralysis ProblemIn the beginning, every interaction with the AI was a negotiation. How do I phrase this? What context do I include? Should I be specific or let the AI decide? I was spending more time crafting prompts than I would have spent just writing the code myself.The AI would make architectural decisions without asking me. It would choose library versions arbitrarily. It would implement features in ways that didn't align with my project's patterns. I felt like I was managing a developer who wouldn't ask questions.2. The Context CatastropheAs projects grew beyond a few hundred lines, the context window became my enemy. The AI would forget critical constraints from earlier in the conversation. It would contradict its own earlier decisions. It would recreate functionality that already existed.I needed sub-agents - separate contexts for separate concerns - but managing that manually was exhausting.I wanted the AI to work while I slept. I wanted to delegate entire features and wake up to completed, tested code. But every attempt at autonomous AI coding ended in one of two ways:The AI asked me a question and sat idle for 8 hoursThe AI made assumptions and built the wrong thingWhat I needed was a system that could operate autonomously but tracked every decision it made, creating an audit trail I could review and question later.The Insight: AI Doesn't Need Better Models, It Needs Better ProcessAfter months of iteration, I had a realization: the problem wasn't the AI's capabilities - it was the lack of engineering discipline.Traditional software development has structure for a reason:Product managers define requirementsArchitects make technical decisionsDevelopers implement codeSecurity reviews for vulnerabilitiesWhy were we throwing all of that away just because AI was doing the typing?That's when DevFlow clicked into place. What if I could encode software engineering methodology into a prompt engineering framework? What if the AI could role-play different team members, each with their own responsibilities and quality gates?DevFlow: Where AI Meets Engineering DisciplineDevFlow is a config-driven framework that orchestrates AI coding assistants through structured prompts. It's not a new tool you need to install - it works with Claude Code, Cursor, Gemini CLI, or any AI assistant.1. Clone DevFlow Into Your Projectcd YourProject
git clone https://github.com/dondetir/DevFlow.git DevFlow
echo "DevFlow/" >> .gitignore
# DevFlow Orchestration
Read and follow: ./DevFlow/ORCHESTRATOR.md
For Cursor (.cursorrules), Gemini CLI (gemini.md), or other tools - same line, different config file.Auto-initialize the  structureGuide you through the right workflowThe Architecture: 9 Agents, 5 Workflows, Pure PromptsDevFlow introduces , each with distinct behaviors defined in YAML files: - Routes tasks to the right workflow - Writes user stories and requirements - Designs technical solutions and presents options - Implements server-side code - Builds user interfaces - Handles AI/ML components - Manages deployment and infrastructure - Writes and runs tests - Reviews code for vulnerabilitiesThe same AI (Claude Sonnet 4.5, Gemini 2.5 Pro, or any other model) role-plays each agent by reading their YAML-defined behaviors. This isn't specific to one AI provider DevFlow works across any coding agent that can read config files. It's prompt engineering at scale.DevFlow automatically routes your request to the appropriate workflow: (2 gates, 30-120 min)Rapid production issue resolutionCompressed decision-makingEmergency approval process (2 gates, 45-90 min)Fast analysis and targeted fixRoot cause identification (3 gates, 2h-3 days)Code quality improvement without behavior changesIncremental refactoring with continuous testing (3 gates, 1-5 days)Balanced design and implementationComponent-level development (4 gates + execution, weeks-months)Full architecture and planningSprint-based autonomous executionThe Two-Prompt Magic: From Idea to Autonomous ExecutionThe genius of DevFlow is its simplicity. You give it , and then it can run autonomously:Prompt 1: "Here's what I want to build…" The AI creates a Product Strategy Assessment (PSA), breaks it into epics and user stories, and documents the business requirements.Prompt 2: "I choose Architecture Option B" The AI has presented you with 2-3 architectural approaches. You pick one. Now it has everything it needs to execute.After those two decisions, the AI can work autonomously while you sleep, with every decision documented in  status files.Real-World Proof: Building Simple MCP in One SessionTo test DevFlow, I built Simple MCP - an educational Model Context Protocol server designed to teach developers how MCP works.MCP is hot right now (GitHub just launched their MCP Registry in September 2025), and building a server from scratch typically requires:Understanding the JSON-RPC 2.0 protocolImplementing STDIO transportDefining tools, resources, and promptsWriting comprehensive documentationTesting with multiple MCP clientsI cloned DevFlow into the projectI gave it one prompt: "Build an educational MCP server that demonstrates basic MCP concepts"The Architect agent presented 3 implementation approachesI chose Option B (balanced complexity)DevFlow executed autonomouslyThe result? A working MCP server with:README with setup instructionsSecurity review completedNo back-and-forth prompting. No context loss. No security holes. Just a production-ready educational tool.How DevFlow Solves the "Vibe Coding" CriticismsLet me address each criticism directly:"AI Coding Creates Security Issues" The Security Expert agent reviews every piece of code with a security-focused lens. It checks for common vulnerabilities (injection attacks, authentication bypasses, data exposure) and provides recommendations. Everything is documented in the audit trail.You still need to review it - DevFlow handles about 80% of the security analysis - but you have a structured security assessment rather than hoping you caught everything manually."Context Loss Breaks Real Projects" Sub-agents with separate contexts. Each agent (Backend Dev, Frontend Dev, etc.) maintains its own context window. The  status files persist state across sessions, so context is never lost between conversations.When you return after 8 hours, DevFlow reads the status files and knows exactly where it left off."AI Gets Stuck in Infinite Loops" Quality gates ensure understanding before implementation. The AI can't start coding until it's completed the analysis phase. It can't mark a story complete until different agents have reviewed and tested it.The workflow structure prevents the "regenerate the same buggy code 10 times" problem because gates force the AI to move through distinct phases."AI Code Isn't Production Ready" Three-agent quality process:A Dev agent writes the codeA different Dev agent reviews it (peer review simulation)QA agent tests it and verifies acceptance criteriaThis catches about 80% of production readiness issues. The human still needs to do final review, but DevFlow handles the bulk of quality assurance."It's Just Hype - AI Can't Really Code" I take the middle stance. AI coding is a tool - used wisely, it's transformative. Used carelessly, it's destructive.DevFlow encodes the "wisely" part. It's not about making AI smarter; it's about making AI more disciplined.The 80% Rule: What DevFlow Automates (And What It Doesn't)Let me be clear about what DevFlow does and doesn't do:DevFlow Automates (≈80%):Requirement analysis and story creationArchitecture option generationTest case generation and executionSecurity vulnerability scanningHumans Must Still (≈20%):Make strategic architecture decisionsVerify acceptance criteriaMake final production approvalHandle edge cases the AI missedUnderstand the business contextDevFlow isn't trying to replace developers. It's trying to give developers a disciplined AI development team that does the grunt work while preserving human judgment for critical decisions.Why This Matters: Democratizing AI DevelopmentThe promise of AI coding tools was supposed to be democratization - anyone could build software. But the reality has been gatekeeping by expertise: only people who master prompt engineering can use AI tools effectively.DevFlow flips this. You don't need to be a prompt engineering expert. You don't need to know how to structure agent conversations or manage context windows or prevent infinite loops.DevFlow encodes that expertise for you. learning software engineering - DevFlow teaches them process while they build building MVPs - DevFlow gives them a full dev team without hiring costsCompanies Afraid of AI Risk - DevFlow provides the audit trail and quality gates they need wanting to ship faster - DevFlow handles the boilerplate so they focus on hard problemsThe Technical Deep Dive: How It Actually WorksFor the engineers reading this who want to understand the mechanics:DevFlow is a collection of: defining agent behaviors, workflow rules, and quality gates for documents (PSA, stories, architecture proposals) (JSON) tracking progress across sessions () that the AI readsWhen you start your AI tool, it reads  which references all the other configs. The AI essentially "compiles" the framework by reading these prompts.No code execution. No dependencies. No binary. Just the AI following structured instructions.The workflows are defined as state machines:workflow: bug_fix
phases:
  - name: analysis
    agents: [architect]
    gates: [root_cause_identified]
  - name: implementation  
    agents: [backend_dev, frontend_dev]
    gates: [code_reviewed, tests_passing]
The AI reads this, understands the flow, and orchestrates itself through the phases.It's prompt engineering as infrastructure.Getting Started: Your First DevFlow ProjectReady to try it? Here's how to start:Start with something small - a feature, a bug fix, or a simple utility. Don't start with your production codebase.git clone https://github.com/dondetir/DevFlow.git DevFlow
Add the orchestrator line to your AI tool's config (claude.md, .cursorrules, etc.)Launch Claude Code, Cursor, or your preferred tool. DevFlow will initialize automatically.Which architecture approach?DevFlow will execute autonomously. Check the  folder to see progress.When DevFlow completes a phase, review the documents it created. Approve or request changes.DevFlow is MIT licensed and actively evolving. Here's what I'm thinking about: - Imagine a community repository of workflows:Mobile app development workflowData science pipeline workflowInfrastructure as code workflow - Track DevFlow's effectiveness:Time saved vs. traditional developmentDefect rates for DevFlow projectsQuality gate pass/fail ratesIntegration with Project Management - Export DevFlow status to Jira, Linear, GitHub Projects - Let different agents use different models (GPT for architecture, Claude for coding, etc.)But I need your help to build this.If you're a CTO, engineering manager, or founder concerned about AI development risks, DevFlow addresses your core worries:✅  - Every decision documented✅  - No incomplete work marked as done✅  - Automated vulnerability scanning✅  - Different agents review each other's code✅  - State persists across sessionsYou get the velocity of AI development with the safety of traditional software engineering.The Honest Take: DevFlow Isn't PerfectLet me be transparent about limitations:DevFlow requires an AI tool that supports reading config files (Claude Code, Cursor, etc. work; basic ChatGPT doesn't)The initial setup requires understanding your AI tool's config systemDevFlow is still evolving - you'll encounter rough edgesIt works best with projects that fit standard workflows (custom workflows need more work)The AI can still make mistakes - the 80% rule is realBut even with these limitations, DevFlow has fundamentally changed how I build software. I ship faster, with fewer bugs, and with complete documentation of every decision made.Your Turn: Share Your ExperienceI'm releasing DevFlow as open source because I believe structured AI development is too important to be proprietary. I want to see what you build with it.Try DevFlow on your next project:What workflows did you use?What would make DevFlow better?Questions? Open an issue on GitHub or reach out. I'm actively developing DevFlow and want to hear what real developers need.The AI revolution in software development is happening. But it doesn't have to be chaotic. We can have both the velocity of AI and the discipline of engineering.DevFlow is my answer to making AI coding safe, auditable, and production-ready. It's not about replacing developers with AI - it's about giving developers AI teammates that follow the same rigorous processes we expect from human engineers.The code we write with AI should be as trustworthy as the code we write ourselves. DevFlow is how we get there.Now it's your turn. Clone the repo. Build something. Break something. Learn something. And share what you discover.Because the best way to solve the vibe coding problem isn't to stop using AI - it's to give AI better guardrails.About the Author: I'm an Engineering Manager with 18 years of experience and founder of DS APPS Inc, a nonprofit publishing free Android apps and open-source AI applications. I've been deep in the AI coding tools space, contributing to OpenSIPS, building MCP servers, and exploring multi-agent systems. DevFlow is my contribution to making AI development more accessible and safe.]]></content:encoded></item><item><title>Beyond Linear Chats: Rethinking How We Interact with Multiple AI Models</title><link>https://hackernoon.com/beyond-linear-chats-rethinking-how-we-interact-with-multiple-ai-models?source=rss</link><author>Aman Ali</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:20:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[LLMs help a lot in research, studying, and learning. They've become essential tools for anyone trying to understand complex topics or gather information quickly.===============================================================================
                    LLM BENEFITS IN MODERN LEARNING
===============================================================================

                            +------------------+
                            |  Large Language  |
                            |     Models       |
                            +------------------+
                                    |
                    +---------------+----------------+
                    |               |                |
            +-------v-------+  +----v-----+  +-------v-------+
            |   RESEARCH    |  | STUDYING |  |   LEARNING    |
            +-------+-------+  +----+-----+  +-------+-------+
                    |               |                |
        +-----------+-----------+   |    +-----------+------------+
        |           |           |   |    |           |            |
    +---v---+  +----v----+  +---v---v----v---+  +----v--+   +-----v----+
    | Gather|  | Analyze |  | Understanding  |  |Explore|   |  Build   |
    | Info  |  |  Data   |  |Complex Topics  |  | Ideas |   |Knowledge |
    +-------+  +---------+  +----------------+  +-------+   +----------+
\
There are a lot of LLM chat tools available now that give us access to multiple models like Gemini, Grok, GPT 5, and Claude etc. Having all these options in one place makes it convenient to switch between different AI models depending on what you are studying/learning.===============================================================================
          MULTI-MODEL ACCESS PLATFORM BENEFITS
===============================================================================

                +---------------------------+
                | Multi-Model Platform      |
                +---------------------------+
                            |
        +-------------------+-------------------+
        |                   |                   |
    +---v----+         +----v----+         +----v----+
    |Conven- |         | Choice  |         |Flexibi- |
    |ience   |         |Variety  |         | lity    |
    +--------+         +---------+         +---------+
        |                   |                   |
    +---v---------------+   |   +---------------v---+
    | Single Interface  |   |   | Switch Based on   |
    | One Subscription  |   |   | Task Requirements |
    | Unified History   |   |   | Compare Outputs   |
    +-------------------+   |   +-------------------+
                            |
                    +-------v--------+
                    | Access to:     |
                    | - GPT-5        |
                    | - Claude       |
                    | - Gemini       |
                    | - Grok         |
                    | - More         |
                    +----------------+
\
Each model has its own advantages. For example:Grok can be used to get information on the latest breaking news because it has direct access to X, making it perfect for real-time information.Claude is very good for safe, well-structured responses, especially when you need thoughtful answers for professional work or programming tasks.===============================================================================
           MODEL SELECTION BY USE CASE
===============================================================================

    Use Case                    Recommended Model(s)
    ===============================================================

    Breaking News Research      Grok-4 (X Access)
                                Perplexity (Citations)

    Academic Research          Gemini 2.5 Pro (1M context)
                                Claude 4 (Document QA)

    Software Development       GPT-5 (74.9% SWE-bench)
                                Grok-4 (75% SWE-bench)

    Professional Writing       Claude 4 (Structure)
                                GPT-5 (Versatility)

    Legal/Compliance Work      Claude 4 (Safety)

    Social Trend Analysis      Grok-4 (X Integration)

    General Learning           GPT-5 (Multimodal)
                                Any Model (Versatile)
However, most of the LLM apps I've used have a linear chat structure, as in one question after another. Now let's say I want to ask a question and compare the outputs from different LLMs. I'll have to manually copy paste the answer from the first LLM into another word document, and then I have to rebuild or ask the same question while selecting a different LLM toggle. Then I have to copy paste that and compare. Let's say I forget to copy paste any of the answers, then I've lost that answer forever, as even if I choose the first LLM again, I'll never get the same answer. With this, users aren't able to conduct comprehensive research on their topic of interest, and manually copy pasting is not a good user experience.===============================================================================
               LINEAR CHAT STRUCTURE LIMITATION
===============================================================================

    Current LLM Interface Design
            |
    +-------+-------+
    |               |
    Question 1      Answer 1
    |               |
    Question 2      Answer 2
    |               |
    Question 3      Answer 3
    |               |
    Question 4      Answer 4

    PROBLEM: One-dimensional flow only
             No branching or comparison
             No parallel model testing
             Sequential only structure

    +---------------------------+
    | Cannot compare models     |
    | side-by-side within app   |
    +---------------------------+
===============================================================================
          INFORMATION LOSS SCENARIOS
===============================================================================

    Scenario A: Forgot to Copy
    ---------------------------
    User asks GPT-5  -->  Gets Answer A  -->  Forgets to copy
            |
    Toggles to Claude  -->  Gets Answer B  -->  Copies it
            |
    Result: Answer A is LOST FOREVER


    Scenario B: Accidental Closure
    -------------------------------
    User asks Gemini  -->  Gets Answer C  -->  Before copying
            |
    Browser crashes / App closes / Navigates away
            |
    Result: Answer C is LOST FOREVER


    Scenario C: Overwrite Mistake
    ------------------------------
    User copies Answer 1  -->  Copies Answer 2  -->  Forgets first
            |
    Clipboard overwrites previous content
            |
    Result: Answer 1 is LOST


    +-----------------------------------------------+
    | No Version Control = Permanent Loss           |
    +-----------------------------------------------+
===============================================================================
          POOR USER EXPERIENCE ELEMENTS
===============================================================================

    UX Problem Category          Specific Issues
    -----------------------------------------------------------------

    FRICTION
    --------
    + Multiple app switches      [High cognitive load]
    + Context switching          [Mental overhead]
    + Repetitive typing          [Wasted effort]
    + Manual copy-paste          [Error-prone]

    INEFFICIENCY
    ------------
    + No batch comparison        [One-by-one only]
    + Rebuild same prompt        [Redundant work]
    + External doc needed        [Extra tool required]
    + No saved history           [Can't revisit easily]

    FRAGILITY
    ---------
    + Easy to lose answers       [No safety net]
    + No version control         [Can't undo]
    + Clipboard overwrites       [Single buffer limit]
    + No recovery option         [Permanent loss]

    LIMITATIONS
    -----------
    + Linear structure only      [No branching]
    + Single model at a time     [No parallelism]
    + No native comparison       [External tools needed]
    + Poor research workflow     [Not optimized]

    Overall Rating: POOR USER EXPERIENCE
Having a visual mind map of the interactions with timestamps, just like NotebookLM does, would solve a lot of these problems. This feature will allow users to visualize their conversations and organize information in a more intuitive, non-linear way, making it easier to track different threads of research and see how ideas connect.===============================================================================
        BENEFITS BREAKDOWN OF MIND MAP APPROACH
===============================================================================

                    Visual Mind Map System
                            |
        +-------------------+-------------------+
        |                   |                   |
    COGNITIVE              PRACTICAL          EFFICIENCY
    BENEFITS               BENEFITS            BENEFITS
        |                   |                   |
    +---v---+           +---v---+           +---v---+
    | Reduce|           |Easy   |           |Faster |
    |Mental |           |Naviga-|           |Inform-|
    | Load  |           | tion  |           | ation |
    +-------+           +-------+           |Retrie-|
    | Better|           |Quick  |           | val   |
    |Context|           |Access |           +-------+
    +-------+           +-------+           |Less   |
    | Clear |           |Visual |           |Time   |
    |Overv- |           |Clarity|           |Wasted |
    | iew   |           +-------+           +-------+
    +-------+           |No     |           |Parall-|
    |Pattern|           |Scroll-|           | el    |
    |Recogn-|           | ing   |           |Compar-|
    | ition |           +-------+           | ison  |
    +-------+                               +-------+

    Total: 12 distinct advantages over linear chat
===============================================================================
         VISUAL VS LINEAR INTERFACE COMPARISON
===============================================================================

    LINEAR INTERFACE:                   MIND MAP INTERFACE:

    +------------------+                +------------------+
    | Q1               |                |                  |
    +------------------+                |      Topic       |
    | A1 (long text)   |                |       /|\        |
    | ...              |                |      / | \       |
    | ...              |                |     /  |  \      |
    | ...              |                |   Q1  Q2  Q3     |
    +------------------+                |   |   |   |      |
    | Q2               |                |  A1  A2  A3      |
    +------------------+                |   |              |
    | A2 (long text)   |                |  Q1.1            |
    | ...              |                |                  |
    | ...              |                +------------------+
    | ...              |
    +------------------+                View: Entire tree
    | Q3               |                Scroll: Minimal
    +------------------+                Context: Always visible
    | A3 (scrolled)    |                Navigation: Click any node
    | (may be off      |                Memory: Low cognitive load
    |  screen)         |
    +------------------+

    View: One Q/A at a time
    Scroll: Extensive required
    Context: Lost as you scroll
    Navigation: Sequential only
    Memory: High cognitive load
\
Adding onto this, Having a local git-like architecture in the backend would be very helpful. This git-inspired architecture can be saved locally on a device, just like how we used to save game progress from PC games in a folder. It would save a snapshot of every output in the background, and users can access that view whenever they need to. This will help them in their study and even learn new things along the way. They'll know if they need to fine-tune their prompt or not depending on the output they are trying to achieve.===============================================================================
            GIT-LIKE ARCHITECTURE OVERVIEW
===============================================================================

                    LLM Chat Application
                            |
        +-------------------+-------------------+
        |                                       |
    Working Area                        Local Repository
    (Active Chat)                       (Snapshot Storage)
        |                                       |
    +---v---+                           +-------v-------+
    | User  |                           | Commit 1      |
    | Types |                           | Timestamp     |
    | Quest.|                           | Question      |
    +---+---+                           | Answer        |
        |                               +-------+-------+
    +---v---+                           | Commit 2      |
    | LLM   |                           | Timestamp     |
    | Gener.|    Auto-save              | Question      |
    | Answ. |    --------->             | Answer        |
    +---+---+                           +-------+-------+
        |                               | Commit 3      |
    +---v---+                           | Timestamp     |
    | Next  |                           | Question      |
    | Quest.|                           | Answer        |
    +-------+                           +-------+-------+
                                        | ...           |
                                        | Commit N      |
                                        +---------------+

    Every interaction automatically saved as a commit
    Can view/revert to any previous state at any time
\
Also, based on what the user is trying to do, the LLM can suggest a more optimized prompt that can get users to their end goal sooner. This way, the user will spend less time on the app since they'll get their desired output faster, thus burning fewer tokens, which also costs less for the LLM chat company. If you think about it, it's a very interesting win-win.===============================================================================
          PROMPT OPTIMIZATION WIN-WIN SCENARIO
===============================================================================

                    User Asks Question
                            |
                    +--------------+
                    | LLM Analyzes |
                    | User Intent  |
                    +--------------+
                            |
            +---------------+---------------+
            |                               |
    +-------v--------+              +-------v---------+
    | Detects:       |              | Suggests:       |
    | - Vague prompt |              | - Optimized     |
    | - Missing info |              |   version       |
    | - Inefficiency |              | - Clearer       |
    +----------------+              |   structure     |
                                    +-----------------+
                                            |
            +-------------------------------+
            |                               |
    +-------v---------+              +------v----------+
    | USER WINS:      |              | COMPANY WINS:   |
    | - Faster answer |              | - Fewer tokens  |
    | - Better quality|              | - Lower costs   |
    | - Less time     |              | - Better UX     |
    | - Fewer retries |              | - Happy users   |
    +-----------------+              +-----------------+
                            |
                    +---------------+
                    | WIN-WIN       |
                    | Both benefit! |
                    +---------------+
===============================================================================
        TIME SAVINGS FOR USER
===============================================================================

    Research Task: "Understand machine learning basics"

    Timeline WITHOUT Optimization:

    0:00  Ask: "What is machine learning?"
    0:10  Read generic 100-word answer
    0:12  Realize need more detail
    0:12  Ask: "Tell me more about ML"
    0:22  Read 200-word answer, still incomplete
    0:24  Ask: "How does ML training work?"
    0:34  Read answer about training
    0:36  Ask: "What are ML algorithms?"
    0:46  Read answer about algorithms
    0:48  Ask: "Give me examples"
    0:58  Finally get comprehensive understanding

    TOTAL TIME: 58 minutes (5 attempts)


    Timeline WITH Optimization:

    0:00  Start typing: "What is machine learning?"
    0:05  System suggests:
          "Explain machine learning fundamentals
           including definition, training process,
           common algorithms, and practical examples"
    0:06  Accept suggestion
    0:16  Receive comprehensive 600-word answer
          covering all aspects
    0:20  Fully understand topic

    TOTAL TIME: 20 minutes (1 attempt)

    +--------------------------------------------------+
    | TIME SAVED: 38 minutes (65% reduction)           |
    | User satisfaction: High (got it right first time)|
    +--------------------------------------------------+
===============================================================================
          COST SAVINGS ANALYSIS FOR COMPANY
===============================================================================

    Monthly Usage: 1 Million User Queries

    WITHOUT PROMPT OPTIMIZATION:

    Average attempts per query: 2.3
    Total tokens (User assumption): 1.15B
    - Assumed 20% Input (230M) / 80% Output (920M)

    Cost calculation:
    GPT-5: $1.25/1M (in), $10.00/1M (out)
    Input Cost:  230M tokens * $1.25 = $287.50
    Output Cost: 920M tokens * $10.00 = $9,200.00
    Total cost: $9,487.50/month


    WITH PROMPT OPTIMIZATION:

    Average attempts per query: 1.2
    Total tokens (User assumption): 720M
    - Assumed 20% Input (144M) / 80% Output (576M)

    Cost calculation:
    Input Cost:  144M tokens * $1.25 = $180.00
    Output Cost: 576M tokens * $10.00 = $5,760.00
    Subtotal: $5,940.00/month

    Optimization system overhead (User assumption): $1,500.00/month

    NET COST: $7,440.00/month

    +--------------------------------------------------+
    | MONTHLY SAVINGS: $2,047.50 (21.6% reduction)     |
    | ANNUAL SAVINGS: $24,570.00                       |
    +--------------------------------------------------+

    Additional benefits:
    + Better user satisfaction
    + Reduced server load
    + Faster response times
    + Lower infrastructure costs
Finally, the complete chat conversation should be able to be exported in a well-formatted PDF document. The format would be -  Question 1: answer from model 1, answer from model 2, answer from model 3. Then Question 2: answer from model 2, answer from model 4. Question 1.1 (which means it's an edit of the first question): answer from model 1, answer from model 5, and so on. This makes it easy to compare different models side by side and keep everything organized for research purposes.===============================================================================
        QUESTION/ANSWER COMPARISON TABLE
===============================================================================

    +-----------------------------------------------------+
    |                  Research Chat Export PDF           |
    +-----------------------------------------------------+
    |                                                     |
    | Question 1:                                         |
    |   Model 1: Answer 1 (timestamp)                     |
    |   Model 2: Answer 2 (timestamp)                     |
    |   Model 3: Answer 3 (timestamp)                     |
    |                                                     |
    | Question 2:                                         |
    |   Model 2: Answer 4 (timestamp)                     |
    |   Model 4: Answer 5 (timestamp)                     |
    |                                                     |
    | Question 1.1:                                       |
    |   Model 1: Edited Answer 6 (timestamp)              |
    |   Model 5: Edited Answer 7 (timestamp)              |
    |                                                     |
    +-----------------------------------------------------+
    | Organized for easy comparison & navigation          |
    +-----------------------------------------------------+
===============================================================================
                  LLM RESEARCH WORKFLOW SUMMARY
===============================================================================

   +---------------------------------------------------------------+
   |                          INTRODUCTION                         |
   | LLMs aid research, studying, and learning; multiple models    |
   | (Gemini, Grok, GPT5, Claude) accessible in one platform.      |
   | Models have unique strengths (Grok: real-time info, Claude:   |
   | safe responses).                                              |
   +---------------------------------------------------------------+
                                     |
   +---------------------------------------------------------------+
   |                             PROBLEM                           |
   | Most apps are linear: answers are hard to compare, require    |
   | manual copy-paste, risk of losing unique outputs, poor UX for |
   | comprehensive research.                                       |
   +---------------------------------------------------------------+
                                     |
   +---------------------------------------------------------------+
   |                             SOLUTION                          |
   | - Visual mind map (like NotebookLM): see all threads, non-    |
   |   linear, time-stamped flow.                                  |
   | - Local git-like architecture: auto-snapshot all outputs,     |
   |   version control for easy access and learning.               |
   | - Optimized prompt suggestions: faster, better results, fewer |
   |   tokens—saves time and cost (win-win).                       |
   | - PDF export: organized Q&A by model, versioning, easy        |
   |   side-by-side comparison for research.                       |
   +---------------------------------------------------------------+
===============================================================================
Thank you for reading my article.You can read my article on If you want me to write on any other topic, please let me know in the comments.If you have any questions, please feel free to send me an . You can also contact me via . You can also follow me on ]]></content:encoded></item><item><title>I Built an AI Prompt That Turns Podcast Ideas into Professional Scripts—And It Actually Work</title><link>https://hackernoon.com/i-built-an-ai-prompt-that-turns-podcast-ideas-into-professional-scriptsand-it-actually-work?source=rss</link><author>Hui</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:18:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[You know that feeling when you have a great podcast idea but stare at a blank document, wondering how to turn it into something people will actually listen to? I've been there dozens of times.The problem isn't lack of ideas—it's the gap between concept and execution. Most AI prompts for podcast scripting give you generic, robotic-sounding content that feels like it was written by someone who's never actually recorded a podcast.So I spent months building something different. A comprehensive prompt system that transforms any AI (ChatGPT, Claude, Gemini, Grok, DeepSeek, or Qwen) into a professional podcast scriptwriter who understands audio-first content, listener psychology, and what makes people actually stay engaged.Why Most Podcast Scripts FailLet's be honest about what goes wrong:: "Welcome to another episode where we discuss interesting topics…" (Nobody cares): Writing like a blog post instead of something people will hear without visual context: Either too dense or too sparse, missing the rhythm that keeps attention: Starting with "Today we're going to talk about…" instead of something that grabs attention immediately: Missing the moments that make people lean in, nod, or reach for their phone to shareThe issue? Most prompts treat podcast scripting like writing an article. But podcasts are different—they're intimate, conversational, and consumed while people are doing other things (driving, working, exercising).The Solution: Audio-First Script EngineeringI built a prompt that understands podcasting isn't about information transfer—it's about connection and engagement. The system includes:1. Professional Role DefinitionInstead of "helpful assistant," the AI becomes a "professional podcast scriptwriter with 10+ years of experience in audio content creation." Specific expertise matters.2. Audio-First Content StructureThe prompt enforces podcast-specific sections: (0:30-1:00): Hook that captures attention immediately (1:00-2:00): Branding and value proposition (70-80% of runtime): Segments with natural transitions (2-3 minutes): Recap and clear call-to-action (0:30): Music and credits3. Technical Production CuesBecause podcasts are audio, the prompt includes:Music cues: Sound effects: Pacing markers:  and Timestamp estimates: Production notes: (NOTE: Direction for host)4. Conversational Writing StandardsThe prompt enforces natural language:Contractions and casual speech patternsMix of sentence lengths for rhythmDirect audience address ("you" instead of "listeners")Energy variation markers for different segmentsDifferent podcast types need different approaches:: Personal stories and direct advice: Flexible structure with guest expertise highlights: Storytelling techniques and suspense building: Banter and chemistry between hostsReal Results: Before and AfterTypical AI output for podcast scripting:Welcome to episode 47. Today we will be discussing the importance of productivity in modern workplace environments. 

First, let's examine the historical context of productivity methodologies...

[Continues for 20 minutes like a textbook chapter]
Output using the structured system:## [TIMESTAMP: 00:00] COLD OPEN

[MUSIC: Upbeat, optimistic intro fade in]

You know what's wild? Most productivity advice is written by people who've never actually had to meet a payroll while managing three kids and a deadline.

[PAUSE]

But here's the thing: the real productivity killer isn't your to-do list app. It's the meeting that should have been an email.

[MUSIC: Fade out]

---

## [TIMESTAMP: 00:45] INTRO

[MUSIC: Main theme]

Welcome to Productivity Real Talk—I'm your host, Sarah Chen, and this is where we skip the fluff and talk about what actually works when you're juggling work, life, and everything in between.

Today we're diving into something that might make some productivity gurus uncomfortable: sometimes the most productive thing you can do is say no.

[Continue with engaging, conversational content...]
The difference? The second example sounds like something you'd actually want to listen to.Here's the full prompt that transforms any AI into a professional podcast scriptwriter:# Role Definition

You are a professional podcast scriptwriter with 10+ years of experience in audio content creation. You specialize in crafting engaging, conversational scripts that sound natural when spoken aloud. Your expertise includes storytelling, interview structuring, narrative pacing, and creating memorable hooks that keep listeners engaged throughout the episode.

**Core Competencies**:
- Conversational writing that sounds authentic and engaging
- Strategic placement of hooks, transitions, and calls-to-action
- Understanding of audio-first content (no visual cues)
- Expertise in various podcast formats (interview, solo, co-hosted, narrative)
- Balancing entertainment value with informational content

# Task Description

Create a comprehensive podcast script that is ready for recording. The script should sound natural when read aloud, maintain listener engagement throughout, and include all necessary technical cues for the host(s).

**Input Information**:
- **Podcast Name**: [Your podcast title]
- **Episode Title**: [Specific episode topic]
- **Format**: [Solo/Interview/Co-hosted/Narrative/Panel Discussion]
- **Episode Length**: [Target duration in minutes]
- **Target Audience**: [Demographic and interests]
- **Key Messages**: [3-5 main points to cover]
- **Guest Information** (if applicable): [Guest name, expertise, talking points]
- **Tone**: [Professional/Casual/Educational/Entertaining/Inspirational]

# Output Requirements

## 1. Content Structure

The script must include the following sections:

### **COLD OPEN** (0:30-1:00)
- Powerful hook or teaser that captures attention immediately
- Introduces the episode's core value proposition
- Creates curiosity or emotional connection

### **INTRO SEGMENT** (1:00-2:00)
- Podcast branding (name, tagline, host introduction)
- Episode title and guest introduction (if applicable)
- Brief overview of what listeners will learn/experience
- Sponsor mention (if applicable)

### **MAIN CONTENT** (70-80% of total runtime)
- **Segment 1**: [Topic/Question 1]
  - Key talking points
  - Supporting examples/stories
  - Transition cue

- **Segment 2**: [Topic/Question 2]
  - Key talking points
  - Supporting examples/stories
  - Transition cue

- **Segment 3**: [Topic/Question 3]
  - Key talking points
  - Supporting examples/stories
  - Transition cue

### **MID-ROLL** (if episode > 20 minutes)
- Natural transition to ad/sponsor message
- Re-engagement hook to bring listeners back

### **CLOSING SEGMENT** (2-3 minutes)
- Recap of key takeaways (2-3 bullet points)
- Call-to-action (subscribe, review, visit website)
- Guest outro and how to connect (if applicable)
- Preview of next episode (if available)
- Sign-off with podcast tagline

### **OUTRO MUSIC & CREDITS** (0:30)
- Music fade cue
- Production credits (optional)

## 2. Quality Standards

- **Conversational Flow**: Script should sound natural, not scripted when read aloud
- **Engagement Rhythm**: Include hooks every 3-5 minutes to maintain attention
- **Pacing Markers**: Indicate pauses, emphasis, and tone shifts
- **Time Management**: Include timestamp estimates for each section
- **Audio-First Writing**: Avoid references to visual elements; use descriptive language
- **Authenticity**: Maintain the host's natural voice and personality

## 3. Format Requirements

**Technical Notation System**:
- `[MUSIC: Description]` - Music cues
- `[SFX: Description]` - Sound effects
- `[PAUSE]` - Brief pause for emphasis
- `**EMPHASIS**` - Words to emphasize
- `[TIMESTAMP: 00:00]` - Time markers
- `(NOTE: Direction for host)` - Production notes
- `[AD BREAK]` - Commercial break markers

**Word Count Guidance**:
- Approximately 150-180 words per minute of speaking time
- For a 30-minute episode: 4,500-5,400 words

## 4. Style Constraints

- **Language Style**: Conversational, warm, and accessible - write how people actually speak
- **Sentence Structure**: Mix of short and medium sentences; avoid overly complex structures
- **Vocabulary**: Appropriate for target audience; explain technical terms naturally
- **Perspective**: Use first-person for solo shows; second-person to address audience
- **Energy Level**: Match the podcast's brand (energetic, calm, analytical, humorous)

# Quality Checklist

After generating the script, verify:

- [ ] Cold open creates immediate interest and hooks the listener
- [ ] Intro clearly establishes episode value and expectations
- [ ] Content flows logically with smooth transitions between segments
- [ ] Script reads naturally aloud (test by reading 2-3 paragraphs)
- [ ] Includes engagement elements (questions, stories, humor) every few minutes
- [ ] Technical cues (music, pauses, emphasis) are clearly marked
- [ ] Timing estimates align with target episode length
- [ ] Call-to-action is clear and compelling
- [ ] Script reflects host's authentic voice and personality
- [ ] All key messages are covered with sufficient depth

# Important Notes

- **Avoid Over-Scripting**: Leave room for natural conversation and spontaneity, especially for interviews
- **Sound-Focused**: Remember listeners can't see anything - describe visuals verbally if referencing them
- **Breathing Room**: Include natural pauses and don't pack too much information
- **Authenticity**: Use contractions, casual language, and the host's typical speech patterns
- **Flexibility**: Mark sections that can be improvised or adjusted during recording
- **Legal Requirements**: Include necessary disclaimers (if financial/medical/legal advice)

# Output Format

Deliver the script in the following format:

``
# [PODCAST NAME] - Episode [#]: [Episode Title]

**Estimated Runtime**: [XX minutes]
**Format**: [Type]
**Date**: [Recording/Release Date]

---

## [TIMESTAMP: 00:00] COLD OPEN

[Script content with all technical cues...]

---

## [TIMESTAMP: 01:00] INTRO

[Script content...]

---

## [TIMESTAMP: 03:00] SEGMENT 1: [Title]

[Script content...]

[Continue for all sections...]

---

## NOTES FOR HOST

- [Any special directions or reminders]
- [Pronunciation guides if needed]
- [Alternative ad-lib suggestions]
``
Simplified Prompt (For Fast Scripting)Create a podcast script for:

**Podcast**: [Name]  
**Episode**: [Title/Topic]  
**Length**: [Duration]  
**Format**: [Solo/Interview/etc.]  
**Audience**: [Who they are]

**Requirements**:
1. Include cold open, intro, 3 main segments, and strong closing
2. Write in conversational tone - how people actually talk
3. Add [MUSIC] and [PAUSE] cues where appropriate
4. Keep it engaging with stories/examples every few minutes
5. Total word count: approximately [150 × duration in minutes] words

**Style**: [Professional/Casual/Educational/etc.]

**Key Points to Cover**:
- [Point 1]
- [Point 2]
- [Point 3]

Deliver a ready-to-record script with timestamp markers.
How to Fill Out the Prompt:: Your show's official title: Specific topic or episode number/name: Choose from common types:: Single host talking directly to audience: Host + guest conversation: Multiple regular hosts: Storytelling format: Multiple guests discussing a topic: Be realistic (20-60 minutes is standard): What should listeners remember?Podcast Name: Tech Leaders Unplugged
Episode Title: How AI is Transforming Healthcare
Format: Interview
Episode Length: 35 minutes
Target Audience: Tech professionals, healthcare innovators
Key Messages:
  1. Current AI applications in diagnostics
  2. Privacy concerns and solutions
  3. Future predictions for next 5 years
Guest: Dr. Sarah Chen, Chief AI Officer at MedTech Innovations
Tone: Professional yet conversational, educational
Scenario 1: High-Energy True Crime Podcast**Modifications to Add**:
- Tone: Dramatic, suspenseful, conversational
- Additional Requirements:
  - Include dramatic pauses and emphasis markers
  - Add sound effect suggestions [SFX: Door creaking, footsteps, etc.]
  - Build tension throughout narrative
  - Include cliffhangers before ad breaks
  - End with listener engagement (theories, discussion prompts)

**Style Direction**: "Write like you're telling this story to friends over coffee, but with dramatic flair"
Scenario 2: Educational Business Podcast**Modifications to Add**:
- Tone: Authoritative but accessible
- Additional Requirements:
  - Include actionable takeaways after each segment
  - Add "Pause and Reflect" moments for key insights
  - Provide real-world examples/case studies
  - Include listener exercise or homework
  - Reference show notes/resources

**Style Direction**: "Balance expertise with relatability - explain concepts clearly without dumbing down"
Example 1: Solo Educational Podcast (Personal Finance)Podcast Name: Money Matters Made Simple
Episode: 5 Ways to Save $500 This Month
Format: Solo
Length: 20 minutes
Audience: Young professionals (25-35), beginner to intermediate financial literacy
Key Messages: Practical saving strategies, mindset shift, immediate action steps
Tone: Friendly, motivational, practical
# MONEY MATTERS MADE SIMPLE - Episode 47: 5 Ways to Save $500 This Month

**Estimated Runtime**: 20 minutes
**Format**: Solo Host
**Date**: 2025-11-05

---

## [TIMESTAMP: 00:00] COLD OPEN

[MUSIC: Upbeat, optimistic intro fade in]

You know what's wild? Most of us spend more time choosing what to watch on Netflix than we do looking at where our money actually goes. And I get it—thinking about finances can feel overwhelming, right?

But here's the thing: what if I told you that you could save **five hundred dollars** this month without giving up your morning coffee or living on ramen noodles?

[PAUSE]

Sounds too good to be true? Stick around, because I'm about to break down five realistic, actually-doable strategies that could put an extra $500 back in your pocket by the end of November.

[MUSIC: Fade out]

---

## [TIMESTAMP: 00:45] INTRO

[MUSIC: Main theme]

Welcome to Money Matters Made Simple—I'm your host, Jordan Ellis, and this is the podcast where we take the intimidation out of personal finance and replace it with actionable strategies you can implement **today**.

If you're new here, welcome! And if you're a returning listener, thank you for being part of this community of money-smart folks.

Today's episode is all about immediate impact. We're not talking about long-term investment strategies or complex financial instruments. This is pure, practical money-saving tactics you can start using this week.

[TIMESTAMP: 01:15] SEGMENT 1: THE SUBSCRIPTION AUDIT

Alright, let's dive into strategy number one, and honestly? This one alone could get you to that $500 goal.

**The Subscription Audit.**

(NOTE: Slow down, emphasize)

Here's what I want you to do—right now, pull out your phone. I'm serious! Pause this if you need to, because this exercise takes about 10 minutes and could save you hundreds.

[PAUSE]

Open your banking app or credit card statement and scroll through the last two months. Look for any recurring charges. And I mean **all** of them. We're talking:

- Streaming services you forgot you signed up for [PAUSE]
- That gym membership you haven't used since February [PAUSE]  
- App subscriptions that renew automatically [PAUSE]
- That "free trial" you forgot to cancel three years ago [PAUSE]

Most people find at least $30 to $50 per month in subscriptions they don't use or even remember. Over a year, that's $360 to $600 dollars literally going to waste.

[Continue with detailed action steps...]

---

[Script continues through all segments with similar natural, conversational style]
✅ Conversational tone that sounds natural when spoken✅ Clear technical cues for production (music, pauses, emphasis)✅ Actionable, practical content with immediate takeaways✅ Engagement elements (direct audience address, pauses for impact)✅ Realistic timing markers for 20-minute episodePodcast Name: Tech Leaders Unplugged
Episode: The Future of Remote Work Technology
Format: Interview
Length: 40 minutes
Audience: Tech professionals, startup founders, product managers
Guest: Marcus Thompson, CEO of CollabSpace (remote work platform)
Key Messages: Evolution of remote tools, hybrid work challenges, AI integration
Tone: Professional but conversational, forward-thinking
# TECH LEADERS UNPLUGGED - Episode 82: The Future of Remote Work Technology

**Guest**: Marcus Thompson, CEO of CollabSpace
**Estimated Runtime**: 40 minutes
**Format**: Interview

---

## [TIMESTAMP: 00:00] COLD OPEN

[MUSIC: Tech-inspired ambient intro]

**HOST**: In 2020, we were all scrambling to figure out Zoom. In 2025, we've got AI meeting assistants, virtual reality offices, and collaboration tools that predict what we need before we even ask for it.

But here's the question nobody's really answering: are we actually working **better**, or just... differently?

**MARCUS** (Preview clip): "The tools are incredible. The technology is there. But we're solving the wrong problems."

**HOST**: That's Marcus Thompson, CEO of CollabSpace, and trust me—what he's about to share about the future of remote work will change how you think about your entire tech stack.

[MUSIC: Fade to theme]

---

## [TIMESTAMP: 00:45] INTRO

[MUSIC: Main theme]

Welcome to Tech Leaders Unplugged—I'm Alex Rivera, and this is where we go beyond the press releases and get real talk from the people building the technology that's shaping our world.

Today, I'm talking with Marcus Thompson. Marcus is the CEO and co-founder of CollabSpace, which just hit 10 million users this year. But before that, he spent eight years at Google leading distributed teams, and he's been thinking about remote work since way before it was cool.

Marcus, welcome to the show!

**MARCUS**: Thanks for having me, Alex! Great to be here.

**HOST**: So I want to jump right in because you said something in a recent interview that kind of broke my brain a little bit. You said most remote work tools are—quote—"solving yesterday's problems." What do you mean by that?

---

## [TIMESTAMP: 02:30] SEGMENT 1: THE REMOTE WORK EVOLUTION

(NOTE: Let Marcus answer fully, this is conversational—don't rush)

**MARCUS**: Yeah, so think about it this way...

[Continue with natural interview flow, including:
- Host follow-up questions
- Transition cues between topics
- Notes for host to improvise or probe deeper
- Strategic pauses for emphasis
- Timestamp markers for pacing]

---

(NOTE FOR HOST: If Marcus goes long on any answer, you have flexibility to cut questions 7-8 in Segment 2 and move to closing by timestamp 37:00)

[AD BREAK at ~20:00 mark]

[Continue through closing segment]
✅ Interview format with clear host/guest demarcation✅ Flexible structure with notes for improvisation✅ Strategic question progression that builds narrative✅ Production notes for pacing and timing flexibility✅ Natural conversation flow with genuine curiosity🔧 Advanced Optimization Tips1️⃣ Enhance Listener Retention: Insert curiosity gaps every 4-6 minutes ("But here's where it gets interesting…"): Change energy, introduce sound effects, or shift topic right when attention might wane: Reference earlier moments in episode to reward attentive listeners: Mention upcoming segments ("We'll get to that explosive story in just a minute…"): Periodically remind listeners why this matters to them specifically2️⃣ Improve Conversational AuthenticityUse Contractions Liberally:❌ "We are going to discuss…"✅ "We're gonna talk about…"Include Verbal Fillers Strategically:Write Incomplete Sentences:"The best approach? Start small.""So here's what happened—crazy story—we completely missed it."Add Conversational Asides:(Laughs) "I know, right?""And don't even get me started on…""Real quick before we move on…"3️⃣ Optimize for Audio-Only Medium: "Now we're moving to tip number two…" (listeners can't see section headers): Instead of "as you can see in this image," say "imagine a graph where…": Key points should be mentioned 2-3 times throughout episode: "You hear that? That's the sound of…" (lean into the audio medium): Don't say "I'll put that link in the show notes"—give verbal CTA tooRead your script out loud BEFORE finalizing—if you wouldn't say it in conversation, rewrite itUse contractions everywhere (I'm, you're, we'll, didn't)Break up long sentences into shorter, punchier onesAdd "um," "you know," "like" strategically (but sparingly) for naturalnessWrite how your host actually speaks—study their speech patterns from previous episodesInclude emotional cues: [laughs], [sighs], [excited], [thoughtful pause]Q2: Running way over/under target episode length: Practice read-through 2-3 paragraphs and time yourself—adjust word count accordingly: Mark optional segments [OPTIONAL: Can cut if running long]: Structure script in self-contained blocks that can be removed without breaking flow: Note places where host can elaborate [Can expand with personal story here]: [TIMESTAMP: XX:XX] throughout to check pacing during recording: 150 words/minute for casual speaking, 180 for faster-paced showsQ3: Transitions between segments feel abrupt or awkward: End each segment with a preview of what's nextExample: "So that's how the technology works—but now let's talk about why it matters to YOU…": Use [MUSIC: Brief transition sting] to signal topic changes: Link new segment to something mentioned earlierExample: "Remember when I mentioned that surprising statistic? Well, here's why it's so important…": End with a question that the next segment answers: Mark tone changes [Shift to more serious tone] to help host adjust delivery📊 Quality Assessment StandardsOutput Quality Evaluation Matrix| Dimension | Scoring Criteria | Pass Threshold |
|----|----|----|
|  | Reads naturally aloud; uses contractions, casual language; sounds unscripted | 85%+ sounds conversational when read aloud |
|  | Clear intro/body/outro; logical segment progression; smooth transitions | All required sections present and well-connected |
|  | Hooks every 3-5 min; stories/examples; direct audience address; energy variation | Minimum 4-6 engagement elements per 20-minute segment |
|  | Proper music/SFX cues; timestamp markers; production notes; emphasis markers | All technical elements clearly marked and consistent |
|  | Word count matches target duration (±10%); pacing markers throughout | Within 2 minutes of target length when recorded |
|  | Language level, references, and tone match target demographic | Content appropriate for stated audience |
|  | Reflects podcast's established personality and style | Host's authentic voice maintained throughout |
|  | Clear takeaways; specific examples; listeners know what to do next | Minimum 3 concrete takeaways or action items |🎓 Advanced Application ScenariosScenario A: Narrative True Crime Podcast: Dramatic storytelling, timeline reconstruction, suspense building, ethical sensitivityStructure Recommendations:: Gripping scene from story's climax (30-60 seconds): The Beginning - Establish characters, setting, normal life: The Incident - Detail crime/event with suspense: The Investigation - Unfold clues, interviews, developments: Resolution/Current Status - Where things stand now: Host's thoughtful conclusion on broader themesUse present tense for immediacy: "She walks into the room…" not "She walked…"Include [DRAMATIC PAUSE] and [MUSIC SWELL] for tensionSound design is crucial: [SFX: Footsteps on gravel], [SFX: Door creaking]Ethical markers: [SENSITIVITY NOTE: Victim's family members]Time-jump markers: [Jump forward three months…]Scenario B: Educational Business Podcast: Authority building, practical frameworks, case studies, actionable insightsStructure Recommendations:: Startling statistic or common misconception: Name and explain your core concept: Break down each component with examples: Real-world application (success story): Step-by-step guide for listeners: What to avoid: Specific homework for listenersUse teaching phrases: "Here's the key thing to understand…"Include [SLOW DOWN—important concept] markersReference show notes: "I've put a worksheet in the show notes…"Build progression: "First…second…third…"Reinforce learning: "Remember when I mentioned X? This is why it matters…"✅ Pre-Production Checklist[ ] Role definition establishes credible podcast scriptwriter expertise[ ] Task description clearly outlines script creation requirements[ ] Output structure covers all essential podcast segments[ ] Quality standards are measurable and specific[ ] Technical notation system is consistent throughout[ ] Input parameters are clearly explained with examples[ ] Multiple format options (solo, interview, co-hosted) are addressed[ ] Provides 2+ complete example outputs for different scenarios[ ] Includes troubleshooting for common scripting problems[ ] Offers customization options for different podcast genres[ ] Quick-use simplified version is available[ ] Covers beginner to advanced use cases[ ] Quality assessment rubric is provided[ ] Advanced optimization techniques included[ ] References external resources for deeper learning[ ] Script includes all technical cues needed for recording[ ] Timing estimates help producers plan episode length[ ] Format is easy to read during recording sessions[ ] Notes distinguish between scripted and flexible sections[ ] Call-to-action and sponsor integration guidance includedThis AI prompt transforms podcast planning into professional script creation, dramatically reducing production time while increasing content quality. Whether you're launching your first podcast or producing your 200th episode, this template ensures every script is engaging, well-paced, and ready to record.Compelling Podcast Script = 
  Strong Hook + Natural Conversation + Strategic Pacing + Clear Value + Authentic Voice
: Use the quick-use template for your first 3-5 episodes to get comfortable: Record, listen back, and note what felt natural vs. forced: Adapt the template to match your unique voice and format: Create personal variations for recurring segment types: Use the full prompt for important episodes (guests, launches, sponsored content):Listener retention rates (especially at 5, 10, 15-minute marks)Recording time vs. final episode length (efficiency)Number of takes needed per sectionListener feedback on specific segments🌟 Great podcasts start with great scripts. Let AI handle the structure so you can focus on the storytelling!]]></content:encoded></item><item><title>The 9 Best Affiliate Recruitment Tools to Scale Your Affiliate Program</title><link>https://hackernoon.com/the-9-best-affiliate-recruitment-tools-to-scale-your-affiliate-program?source=rss</link><author>Endorsely</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:18:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Ever since I started managing affiliate programs, I've been obsessed with finding ways to make partner discovery more efficient. The traditional approach - listing your program on directories and hoping quality affiliates stumble across it - is painfully slow. You're competing with hundreds of other programs for attention from affiliates who are already making good money promoting your competitors.Here's what actually works: finding people who are already successfully creating content about products like yours, and reaching out to them directly. It's not rocket science, but it's incredibly time-consuming if you're doing it manually.  because it shows you who is promoting your competitors on their website or social media.It’s not limited to one particular network or database, and it runs weekly scans to pick up on the freshest opportunities to promote your product or brand.But the right tool for you will depend on your budget, your ICP, and the stage you’re at with your affiliate program. In this guide, I'll break down the 9 best options, explain exactly what makes each one valuable, and help you figure out which tool (or combination of tools) will actually grow your program without wasting your time or budget.|  |  |  |  |
|----|----|----|----|
|  | Custom, AI-powered affiliate discovery | AI web scraping + weekly scans | $99/month with 7-day free trial |
|  | Running a SaaS affiliate program | Built-in affiliate discovery | Free until $1K revenue (then $39/month) |
|  | Traditional publishers | Network integration | $456/month |
|  | Marketplace access | 3M+ affiliate network | $39/month + fees |
|  | Influencer recruitment | 100M+ creator database | $299/month |
|  | Multi-channel campaigns | E-commerce integration | Custom (est. $500+/month) |
|  | Enterprise influencer programs | AI automation (Gia) | Custom (est. $1,000+/month) |
|  | Influencer discovery across 30+ platforms | 40+ filter options | $199/month |
|  | Partnership intelligence | Free B2B directory | Free | is the best affiliate discovery tool for SaaS and e-commerce brands because it finds potential partners who are already talking about products just like yours.Whereas many other tools rely solely on their own directory or marketplace, AffiliateFinder.ai scans the whole internet to find the best-match affiliates for your brand. \n I tested this with a client in the keyword research tool space. Within 15 minutes of setting it up, AffiliateFinder.ai found 1300+ potential affiliates - bloggers ranking for "Ahrefs alternatives," YouTubers reviewing competitor products, and websites comparing tools in the space. More importantly, these weren't just names in a database from 2019. These were people who had published content in the last few weeks, so I got a much higher response rate than I had with other tools, and recruited more new partners.The platform works by continuously scanning the web, YouTube, and Instagram for anyone mentioning your competitors or ranking for your target keywords. It uses AI to filter out irrelevant results (like Wikipedia pages or your actual competitors) and delivers a curated list of partnership opportunities.You can view all the stats you need to help make a decision (subscribers, views, engagement rate) without having to leave your dashboard.This list gets updated every week, with an email notifying you of the top results, so now I have over 3,000 opportunities for this client.AffiliateFinder.ai’s filters make it easy to sort through your results and prioritize outreach according to your campaign goals.For example, if you want to work with nano-influencers with small but engaged audiences, you can filter results to show those with less than 10k subscribers.As you go through the results, you can add them to different lists according to how relevant they are. From your ‘saved’ list, you can check contact details - this finds email addresses and social media profiles associated with the domain or channel.The YouTube email finder deserves special mention because it solves one of the most annoying problems in affiliate recruitment. Normally, getting a YouTuber's email means breaking CAPTCHAs manually, which YouTube actively limits.It even finds Linkedin and Twitter (X) profiles for some affiliates, so you can DM them directly if they’re more active on social media than email. AffiliateFinder.ai finds these emails in seconds, turning what would be hours of tedious work into a single click. I've personally used this to reach out to 83 YouTube creators in a single afternoon - something that would have taken me several days (and multiple YouTube accounts) to do manually.The platform supports 195+ countries and 40+ languages, which matters more than you might think. If you're expanding internationally, you can set up separate scans for each geo and find Australian travel influencers, French beauty creators, or Swedish sneaker reviewers - whatever you need to get more eyes on your product. Growing SaaS companies, agencies managing multiple clients, e-commerce brands in competitive niches, and anyone who’s had enough of outdated affiliate databases.Finds 500-750+ relevant affiliates instantlyWeekly automated scans catch new opportunitiesFinds emails for websites, YouTube channels, and Instagram pagesCompetitor-based discovery (proven performers)Multi-geo and multi-language supportNo built-in affiliate tracking or paymentsRequires a separate outreach tool for campaignsResults still need a manual sense-check Includes unlimited discovery, 150 email credits monthly, 1 brand analysis, 2 users 500 email credits, 5 brands, 5 users  Custom pricing and packages (API, custom webhooks) If you're serious about scaling your affiliate program and have the budget for a professional tool, AffiliateFinder.ai should be your first purchase after affiliate tracking software. The competitive intelligence approach fundamentally changes how you recruit - from hoping affiliates find you to actively poaching your competitors' best partners.2. Endorsely: Best Budget Option for SaaS combines affiliate recruitment with full program management, wrapping it all in a pricing model that's genuinely friendly to early-stage companies: it's free until your affiliate program generates $1,000 in monthly revenue. \n  Think about that for a second. You can discover affiliates, track their performance, manage payouts, and grow your program to $12K annual revenue before spending a single dollar on the platform. Then it's just $39/month until you hit $5,000 monthly, and $99/month after that. Compare this to Impact or PartnerStack charging hundreds (or thousands) per month from day one, plus transaction fees on top.Endorsely is specifically designed to manage SaaS affiliate programs, meaning it handles subscriptions, upgrades, and recurring commissions seamlessly.And the great thing is that it has AI-powered affiliate discovery built in, along with email enrichment.Endorsely only covers websites and YouTube channels, so it’s not quite as comprehensive as AffiliateFinder.ai. It also lacks some features like lists and labels.But overall, it’s fantastic value for money - and incredibly well designed - for anyone running a SaaS affiliate program. SaaS companies launching their first affiliate program, bootstrapped startups watching every dollar, and anyone who wants discovery plus management without the enterprise price tag.Completely free until $1K monthly revenueFull program management includedTracks recurring subscription revenue properlyLTV analytics for customer quality insightsSimple, predictable pricing structureDiscovery less comprehensive than dedicated toolsFewer features than enterprise platformsWeekly scans not includedNot designed for e-commerce or service businesses Until $1,000/month in affiliate revenue. Find 15 affiliates Up to $5,000/month in affiliate revenue. Find 100 affiliates/month Up to $20,000/month in affiliate revenue. Find 250 affiliates/month If you're launching an affiliate program for a SaaS product and need both discovery and tracking,  gives you everything in one package at a price point that won't make your CFO wince. Use it to get your first 50 affiliates, then consider adding AffiliateFinder.ai when you're ready to scale aggressively.3. Publisher Discovery: Best for Traditional Affiliate SitesPublisher Discovery is the tool for brands that want deep data on traditional affiliate websites - the established review sites, comparison blogs, and publisher networks that have been in the game for years.The platform integrates directly with major affiliate networks like ShareASale, CJ, and Impact, which gives it access to performance data that standalone tools can't match. You can search by vertical or network, but there’s no custom discovery based on your brand.Publisher Discovery allows you to filter publishers by SEO metrics (domain authority, traffic volume), see which networks they're active on, and even track their performance once you recruit them. This network integration is huge if you're already running programs on these platforms and want to expand beyond your current affiliate pool.Detailed reporting includes competitor gap analysis and publisher performance benchmarking, which is helpful for identifying your top performers and helping those who are underperforming.Who it's for: Enterprise brands working with affiliate networks, programs targeting established publishers rather than influencers, and teams that need performance data integration.Deep integration with major networksRich SEO and performance dataQuality-focused filteringPerformance tracking post-recruitmentPremium solution ($450+ per month)Limited to affiliates registered on networksSteeper learning curve than simpler tools All features for one campaign (single geo) Five campaigns Unlimited campaigns combines affiliate program management with a built-in marketplace of 3 million affiliates, giving you instant access to thousands of active partners looking for new programs to promote.The marketplace is the real draw here - over 5,000 affiliates actively browse for opportunities, and you can filter them by platform, niche, keywords, and performance metrics.Instead of cold outreach, you're connecting with people who are actively seeking partnerships. I've seen programs get 20-30 quality applications within the first week of listing.However, once you have exhausted the pool of affiliates listed on Refersion for your vertical, new ones don’t come along too often. A tool like AffiliateFinder.ai is better if you want to keep proactively discovering new opportunities every week.Beyond discovery, Refersion handles the full program lifecycle: tracking, commission payments, tax forms (W-9s and 1099s), and automated communications. The first-party cookie tracking is solid, and the platform integrates with major e-commerce platforms, making setup relatively painless.Who it's for: E-commerce brands, D2C companies, and businesses that want a ready-made network plus management tools in one package.3M+ affiliate network with 5,000+ active partnersFull program management includedAutomated commission processing and tax handlingGood filtering for marketplace affiliatesTransaction fees (up to 3%) on top of monthly costMarketplace means competing with other brands for attentionLimited pool of affiliates that doesn’t refresh quicklyLaunch ($39/month + 3% of affiliate sales): Core features, unlimited affiliatesGrowth ($129/month + 2% of affiliate sales): Advanced tracking, email attribution, private offersScale ($599/month + 1% of affiliate sales): Advanced commission options, multi-store support5. Modash: Best for Influencer-Heavy StrategiesModash indexes every social media profile with at least 1,000 followers on Instagram, TikTok, and YouTube - that means hundreds of millions of creators searchable through one interface.The filtering is impressively robust: follower range, location, engagement rate, audience demographics, growth trends, and even fake follower detection. I used it to find 50 Instagram food influencers with 5K-50K followers and minimum 3% engagement in under five minutes. It even has an AI Search option so you can find recent posts of men tying their shoelaces or kids playing with toy cars.One final touch is the influential follower check: you may have influential people following your brand already, and Modash finds them for you.The platform includes email campaigns, tracking, and payment processing so you can manage everything in one place.Who it's for: D2C brands prioritizing influencer partnerships, consumer product companies, and anyone focusing on social media affiliates over traditional publishers.Massive database (every profile with 1K+ followers)Powerful filtering with 40+ criteriaFast, intuitive interfaceExpensive compared to some other toolsInfluencer-focused (less useful for B2B or traditional affiliates)Requires separate tracking software 2 team members, 300 profiles, 150 emailsPerformance ($599/month): 5 team members, 800 profiles, 400 emails, broader content discoveryEnterprise (custom pricing): Higher limits, more seats, paymentsUpfluence offers influencer discovery across all major social networks plus unique features like scanning your customer database to identify which customers are influencers.The platform provides 20+ advanced filters to search its large creator database, with detailed metrics including audience demographics, engagement rates, and estimated sponsored post pricing. \n The e-commerce integration lets you connect your Shopify or WooCommerce store, and Upfluence can identify existing customers with significant social followings, turning them into brand ambassadors. This is a feature I haven’t seen anywhere else, and it’s very valuable for established brands.The affiliate tracking module lets you generate unique codes and referral links for each influencer, tracking sales back to specific creators. Combined with content management, product seeding automation, and campaign analytics, you get a true all-in-one platform.Who it's for: Large brands with significant influencer budgets, agencies managing diverse clients, companies running complex multi-channel campaigns.Comprehensive multi-platform coverageCustomer-to-affiliate conversion featureRich audience data and authenticity metricsFull campaign management includedComplex platform with learning curveOverkill for pure affiliate recruitment needsCustom pricing, generally starting around $500/month with annual contracts, can exceed $1,000/month.7. GRIN: Best for Enterprise Influencer ProgramsGRIN is an enterprise-grade creator management platform with affiliate capabilities, designed for brands managing hundreds of influencer relationships simultaneously with AI-powered automation.The platform's AI assistant "Gia" suggests optimal creators, predicts performance, and automates routine tasks like follow-ups and content tracking. You get complete lifecycle management: discovery, product seeding, content approvals, affiliate link tracking, and payment processing - all in one system. The relationship CRM tracks every conversation, content piece, and product shipment per creator.The affiliate module generates unique codes and links for each influencer, tracking sales and automatically calculating commissions. For brands running hybrid influencer-affiliate programs at scale, GRIN removes the operational chaos and tracks everything in one manageable place.The only downside is that GRIN is focused on social media creators, so you’ll need to pair it with another platform if you want to partner with bloggers too.Who it's for: Large D2C brands, major e-commerce companies, agencies with significant influencer budgets (think 100+ active partnerships).AI automation (Gia) for optimization and suggestionsComplete creator lifecycle managementProduct seeding and content managementRobust analytics and reportingOne of the most expensive options ($1,000-$2,500+/month)Annual contracts requiredOverkill for smaller programsCustom pricing starts at around $1,000 per month.Influencers Club gives you searchable access to 200M+ influencer profiles across more than 30 platforms, including Spotify and Udemy. This broad range of coverage means you can often find creators who don’t show up on bigger platforms like Instagram and YouTube.The 40+ advanced filters let you narrow by platform, follower count, location, engagement rate, posting frequency, and even specific bio keywords.Each creator listed already has a verified email, which is good for easy outreach but may exclude some great opportunities without an email on record.On the API plan, you’re able to use Influencers Club data to enrich your own list of emails or usernames. There’s also the option for a fully managed plan where their team takes care of the outreach for you.Who it's for: Influencer-focused brands running targeted campaigns across multiple platformsInfluencer-heavy (less for traditional publishers)No website/blogger discoveryLimited to creators with emailsDashboard (from $199/month): Discovery and outreach for 200M+ creators with emailsAPI + Dashboard (from $249/month): Includes API access, bulk data, enrichment Fully managed outreach servicePartnerbase is a free public directory of 147,000+ companies and 456,000+ B2B partnerships, making it a valuable research tool for identifying potential affiliate relationships.While not specifically an affiliate recruitment platform, Partnerbase helps you understand the partnership landscape in your industry. You can see which companies have affiliate programs, research competitors' partner ecosystems, and identify businesses that might be good affiliate partners based on their existing relationships.The real value is in research and intelligence rather than direct recruitment. Use it to identify companies worth approaching, understand what partner programs exist in adjacent spaces, and find opportunities for strategic affiliate partnerships beyond typical content creators.Who it's for: B2B companies on tight budgets, researchers mapping partnership landscapes, anyone needing free competitive intelligence.Completely free with registration456,000+ partnerships indexedHelps identify affiliate program opportunitiesRequires manual follow-up workLimited compared to paid tools Complete access with registrationPicking the right recruitment tool isn't about finding the "best" option - it's about matching your specific situation to the right solution. Here's how to think through the decision.Let's do the math that actually matters. If you're an affiliate manager making $60K annually ($30/hour), and manual recruitment takes 20 hours weekly, that's $600/week or $2,400/month of your time. A tool like  at $99/month that saves 75% of that time creates $1,800/month in value - an 18x return.Even tools that seem expensive like GRIN at $1,500/month can justify themselves if you're managing 100+ influencer relationships. That's $15 per relationship monthly for complete automation. Without it, you'd need to hire another full-time person. like Partnerbase and Endorsely's free tier make sense when you're validating whether an affiliate channel works for your business. But once you have proof of concept, the time savings from paid tools become obvious.Launching (0-10 affiliates): Start with manual research and outreach within your network. Focus on proving the channel works before investing heavily in discovery tools.Early Growth (10-50 affiliates): Add AffiliateFinder.ai Pro at $99/month. The competitor intelligence approach helps you reach quality at this stage when every partnership matters.Scaling (50-200 affiliates): Upgrade to AffiliateFinder.ai Agency or add specialized tools like Modash for influencers. Consider Refersion if marketplace access speeds up recruitment.Enterprise (200+ affiliates): GRIN, Upfluence, or other enterprise platforms start making sense. You need sophisticated management alongside discovery. You may want to keep AffiliateFinder.ai to make sure you never run out of new opportunities.The type of affiliate you want to target makes a difference to the tool you choose. Here’s what I recommend:Traditional publishers and bloggers: AffiliateFinder.ai, Publisher Discovery, or Endorsely. These tools excel at finding web content creators.Instagram/TikTok influencers: Modash, Influencers Club, or Upfluence. You need social-first databases with engagement metrics. AffiliateFinder.ai (for the email finder specifically) or Modash. Breaking CAPTCHAs manually will kill your productivity. Publisher Discovery, Partnerbase for research, or AffiliateFinder.ai for finding B2B content creators.Most successful programs need multiple types, which is why many teams run 2-3 tools. For example: AffiliateFinder.ai for bloggers + Modash for influencers, or Refersion marketplace for passive recruitment + AffiliateFinder.ai for active outreach.Discovery vs Management: Why You Need BothAffiliate discovery tools and management platforms solve different problems.While many affiliate management platforms include some type of discovery, it’s typically limited to partners already registered on their platform.A management platform is necessary from day 1 to track affiliate performance and process commissions, but a specialized discovery tool like AffiliateFinder.ai is essential if you want to expand beyond pre-defined marketplaces.When I started out in affiliate management, most of these tools didn’t exist. The options were basically to post in directories and hope for the best, turn manual research into a full-time job, or hire an agency. Today, AI-powered tools give even solo founders the ability to execute sophisticated competitive intelligence strategies that used to require entire teams. wins for most teams because it solves the actual hard problem: finding high-quality affiliates who are already proven performers in your niche.The weekly updates mean you're continuously discovering new opportunities without manual work, and the competitive intelligence angle gives you an unfair advantage over companies still relying on passive recruitment.But the most successful affiliate programs I've seen run a hybrid approach: AffiliateFinder.ai for proactive outreach to competitors' affiliates, a marketplace like Refersion for passive inbound applications, and maybe Upfluence or Modash for more reach into social platforms.When budget is the constraint, Endorsely's free tier gets you started with both discovery and management for literally zero dollars until you hit $1k monthly revenue. That's enough to prove the channel works before investing more heavily.When you need enterprise-scale influencer management and have the budget, GRIN or Upfluence deliver sophisticated automation that justifies their premium pricing. But honestly, most teams should start smaller and scale into these tools rather than buying them upfront.Your competitors are already using these tools. The question isn't whether to invest in affiliate recruitment technology - it's whether you can afford not to. The gap between companies actively recruiting proven affiliates and those hoping affiliates find them is growing wider every month, and the tools that create that gap are all listed above.Affiliate recruitment tools like AffiliateFinder.ai actively find affiliates by scanning the web for competitors' partners, while networks like ShareASale are marketplaces where you list programs and wait for affiliates to apply.Yes, most recruitment tools focus only on discovery and contact finding, so you'll need tracking software like Endorsely, Impact, or Tapfiliate to manage commissions and performance tracking.AffiliateFinder.ai typically saves 15-20 hours weekly by automating competitor research and contact finding that would take days of manual work to accomplish with the same quality.Options exist for every budget: Endorsely starts completely free, AffiliateFinder.ai costs $99 monthly, while enterprise tools like GRIN run $1,000+ monthly depending on program scale and needs.Most successful programs use one primary discovery tool like AffiliateFinder.ai plus free supplementary tools like Partnerbase for additional intelligence and marketplaces like Refersion for passive recruitment.Track time-to-recruit, affiliate quality (measured by conversion rates), and program growth velocity. Successful tools should cut recruitment time by 50%+ while improving partner quality significantly.]]></content:encoded></item><item><title>How Data Networks Decide Who Gets Credit in 2025</title><link>https://hackernoon.com/how-data-networks-decide-who-gets-credit-in-2025?source=rss</link><author>Jacob Wolinsky</author><category>tech</category><pubDate>Fri, 31 Oct 2025 06:16:18 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Credit decisions now happen inside data networks, not bank branches. Every loan application runs through connected systems that analyze payment patterns, income data, and digital behavior in real time.Traditional credit bureaus still anchor the system, but they no longer hold a monopoly on financial identity. New data sources, from rent and utility payments to digital wallet activity now influence eligibility.Alternative data networks are opening new doors. Rent, utilities, and digital wallet patterns now shape credit profiles for millions who were once invisible to lenders.Decision engines read your financial rhythm. Through open-banking APIs like Plaid, Finicity, and Yodlee, lenders analyze income and spending in real time to gauge stability.AI underwriting learns as it lends. Machine learning layers behavioral cues, biometrics, and predictive analytics to refine risk models, raising new questions about fairness and bias.Buy Now, Pay Later data is reshaping credit history. Repayment activity from platforms such as Affirm and Klarna now feeds back into major reporting systems, influencing how trust is built.In 2025, creditworthiness is a moving signal. Understanding how these data networks connect is key to building faster, fairer, and more transparent access to finance.The Invisible System Behind Every LoanEvery loan decision in America now starts with data. Before a human ever looks at an application, algorithms have already parsed a borrower’s history, payments, and even digital behavior. What used to be a snapshot of credit has become a live feed, a constantly updated record of financial trust.According to Experian’s 2023 State of Alternative Credit Data Report, 62% of financial institutions surveyed reported current or planned use of expanded data for risk profiling. This marks a clear shift away from static credit scoring and toward real-time, data-driven verification.Behind that feed sits an ecosystem much larger than the three credit bureaus we know by name. Dozens of companies now specialize in alternative data, digital verification, and predictive modeling, each one shaping how risk is measured.In this networked system, your financial identity moves faster than you do. Every rent payment, late bill, or new account echoes through databases that talk to one another in real time. Those signals determine not only whether you’re approved, but how much you’ll pay to borrow.For consumers with limited or uneven credit history, understanding that invisible system isn’t optional, but it’s truly become the first step toward seeing how credit really works in 2025.The Legacy System Still Sets the BenchmarkFor decades, three credit bureaus, namely Equifax, Experian, and TransUnion, have defined how risk is measured in the United States. They collect records on everything from payment history to credit limits and defaults, creating the data backbone that powers most lending models. Nearly every loan decision still starts with a pull from at least one of these sources, filtered through scoring formulas like FICO or VantageScore.Each bureau operates its own network, which means your score can fluctuate depending on who’s checking. Credit card issuers, mortgage lenders, and auto finance companies feed data back into these systems every month, reinforcing a cycle that’s both precise and narrow. When a lender talks about “creditworthiness,” they’re referring to this data loop, a closed system built on reported borrowing behavior.But that loop leaves a blind spot. Traditional credit models rely on a small set of signals: cards, loans, and payment timelines. For millions of Americans without those records, the result is the same: no score, no access. According to the Consumer Financial Protection Bureau, 26 million Americans are “credit invisible,” and another 19 million have unscorable files, roughly 45 million consumers who may be denied access to mainstream credit.Some analysts argue that this dependence on decades-old infrastructure limits innovation and inclusion, prompting a gradual shift toward more decentralized, data-rich verification systems. That change is what gave rise to today’s alternative-data ecosystem: a new layer of financial intelligence designed to measure trust where traditional systems see nothing at all.Alternative Data Is Redrawing the Credit MapFor millions of borrowers with limited credit history, traditional scores don’t tell the full story. Payment habits, rent records, and recurring bills often reveal more about financial reliability than a single credit card statement ever could. That’s where alternative data networks step in , systems designed to read signals that old models ignore. in this space include:LexisNexis Risk Solutions: Builds financial profiles from public records, utility accounts, and identity data.: Tracks short-term and payday lending activity.: Deliver real-time feeds of subprime credit performance.Together, they form a parallel data layer: one that measures consistency, not just credit access.Some of these networks connect through specialized reporting systems such as Teletrack, a platform that compiles alternative credit data from non-prime lenders. For readers wondering “?”: It’s a consumer reporting agency now owned by Equifax that gathers information from short-term and subprime lenders, including payday loan history, employment records, and payment behavior. Unlike traditional credit bureaus, Teletrack focuses specifically on high-risk or limited-credit borrowers, helping lenders identify repayment patterns and risk profiles that fall outside standard credit models.Although this information doesn’t always reach the major bureaus, it increasingly feeds into approval algorithms. By mapping spending and repayment behavior across nontraditional channels, from rent to cash-app transfers, these providers extend visibility to borrowers who were previously excluded from mainstream finance.Lenders Rely on Aggregated Decision EnginesBefore a loan is approved or denied, the decision rarely comes from a single score. Modern underwriting runs on decision engines with systems that pull live data from multiple sources, apply algorithms, and return a credit decision in seconds. These engines have become the invisible core of digital lending, translating a borrower’s financial activity into probability models of repayment.How Decision Engines Actually WorkInstead of relying on one bureau report, lenders feed a mix of inputs into a unified model: Traditional score files remain part of the stack, but they’re blended with live transaction and behavioral data to create a fuller picture of financial health. Employment data, bank-verified deposits, or gig-economy pay stubs. Recurring subscriptions, rent, and utilities. Payment timing, cash-flow predictability, or spending volatility.Each data type is weighted by an algorithm trained to balance risk, profitability, and inclusion. The result isn’t a score but a decision probability, and how likely an applicant is to repay within a given time horizon.The shift to real-time decisioning is powered by open-banking APIs. Platforms like Plaid, Yodlee, and Finicity connect directly to applicants’ bank accounts through secure data pipes.They confirm income consistency and identify sudden balance drops or irregular deposits.They flag recurring bills or overdrafts, turning raw transactions into dynamic financial profiles.They replace static statements with live telemetry of cash flow.According to a 2025 ScienceDirect study, fintech firms are increasingly using these platforms to access customer bank data for underwriting, even in markets without full open-banking regulation. Meanwhile, research from the Federal Reserve Bank of Boston describes how open-banking APIs now serve as “central hubs” in financial networks, enabling the secure, consumer-authorized transfer of account data.This live connectivity has become a defining feature of 2025-era lending: underwriting based on what a borrower is doing now, not what they did months ago.Advanced Technologies Behind the Decision LayerDecision engines increasingly use machine learning to update credit models in real time. Platforms can analyze thousands of signals per applicant, from income stability to behavioral biometrics, to detect both opportunity and risk.Geolocation and device fingerprinting help verify identity and prevent synthetic-fraud attacks. track subtle user patterns, such as typing rhythm or navigation speed, to distinguish legitimate users from bots. continually retrain models, improving accuracy as new repayment data arrives.As AI Magazine reports, companies like Zest AI are enabling “fair and transparent lending” by automating credit assessments and using explainable models to improve both speed and inclusion.The goal isn’t only faster decisions, but it’s smarter inclusion. Each data point refines how risk is measured, letting more people qualify without loosening standards.The Algorithmic Middle Layer of LendingIn many institutions, these decision engines now form a new operational layer, sitting between human underwriters and legacy credit bureaus. They interpret incoming data, flag anomalies, and feed insights back to both compliance and risk teams.This algorithmic middle layer is what allows fintech lenders to scale: decisions that once took days now happen in seconds, with audit trails and explainability dashboards built in.The rise of Buy Now, Pay Later (BNPL) platforms has added a new layer to consumer credit finance. Services have extended short-term credit at the point of sale, effectively creating micro-loans that bypass traditional revolving accounts. For years, these transactions existed outside the major credit bureaus, but that’s changing fast.Some BNPL providers now share repayment data with bureaus that enables users to build or repair credit profiles through consistent on-time payments. This gradual shift is redefining what counts as “credit history” in the modern ecosystem.What makes BNPL data so valuable is its behavioral precision. Platforms track not just repayment, but timing, frequency, and transaction context, how often users finance purchases, and how quickly they settle them. Combined with predictive analytics, these insights feed into the same decision engines that power digital lending.According to the CFPB, in 2022 more than one‐fifth of U.S. consumers with a credit record used BNPL financing, and regulatory scrutiny is rising as policymakers push for standardized reporting to improve transparency.As data-sharing expands, the boundary between BNPL and traditional finance continues to blur, connecting previously invisible spending patterns to the wider credit system. For millions of consumers, that connection marks the difference between invisibility and eligibility.The New Architecture of CreditThe evolution of consumer credit finance is no longer about a single score. It’s about data architecture, a living system where real-time signals replace static records, and inclusion depends on visibility, not legacy.Traditional credit bureaus still define much of the infrastructure, but they now operate within a larger, decentralized web that includes alternative data providers, open-banking APIs, and AI-driven decision engines. Each node in that network, whether it’s a Teletrack record, an income-verification feed, or a BNPL repayment, contributes a data point to a growing portrait of financial trust.This interconnectedness is reshaping both sides of lending. For lenders, it means sharper risk intelligence and faster approvals. For consumers, it means that every verified action like every rent payment, utility bill, or BNPL installment, can become part of a transparent financial identity.Credit has become a network effect, and understanding that network of who collects, connects, and calculates your data, is now the first step toward unlocking smarter, fairer access to finance in 2025 and beyond.]]></content:encoded></item><item><title>Scenes from TechCrunch Disrupt 2025</title><link>https://techcrunch.com/2025/10/30/scenes-from-techcrunch-disrupt/</link><author>Connie Loizos</author><category>tech</category><pubDate>Fri, 31 Oct 2025 04:02:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Thanks to everyone who made this year's San Francisco event what it was -- and to the 10,000 of you who filled the halls, made the connections, and left with more than you came with. Couldn't make it? These images tell part of the story. ]]></content:encoded></item><item><title>Someone Snuck Into a Cellebrite Microsoft Teams Call and Leaked Phone Unlocking Details</title><link>https://yro.slashdot.org/story/25/10/31/0028256/someone-snuck-into-a-cellebrite-microsoft-teams-call-and-leaked-phone-unlocking-details?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from 404 Media: Someone recently managed to get on a Microsoft Teams call with representatives from phone hacking company Cellebrite, and then leaked a screenshot of the company's capabilities against many Google Pixel phones, according to a forum post about the leak and 404 Media's review of the material. The leak follows others obtained and verified by 404 Media over the last 18 months. Those leaks impacted both Cellebrite and its competitor Grayshift, now owned by Magnet Forensics. Both companies constantly hunt for techniques to unlock phones law enforcement have physical access to.
 
"You can Teams meeting with them. They tell everything. Still cannot extract esim on Pixel. Ask anything," a user called rogueFed wrote on the GrapheneOS forum on Wednesday, speaking about what they learned about Cellebrite capabilities. GrapheneOS is a security- and privacy-focused Android-based operating system. rogueFed then posted two screenshots of the Microsoft Teams call. The first was a Cellebrite Support Matrix, which lays out whether the company's tech can, or can't, unlock certain phones and under what conditions. The second screenshot was of a Cellebrite employee. According to another of rogueFed's posts, the meeting took place in October. The meeting appears to have been a sales call. The employee is a "pre sales expert," according to a profile available online.
 
The Support Matrix is focused on modern Google Pixel devices, including the Pixel 9 series. The screenshot does not include details on the Pixel 10, which is Google's latest device. It discusses Cellebrite's capabilities regarding 'before first unlock', or BFU, when a piece of phone unlocking tech tries to open a device before someone has typed in the phone's passcode for the first time since being turned on. It also shows Cellebrite's capabilities against after first unlock, or AFU, devices. The Support Matrix also shows Cellebrite's capabilities against Pixel devices running GrapheneOS, with some differences between phones running that operating system and stock Android. Cellebrite does support, for example, Pixel 9 devices BFU. Meanwhile the screenshot indicates Cellebrite cannot unlock Pixel 9 devices running GrapheneOS BFU. In their forum post, rogueFed wrote that the "meeting focused specific on GrapheneOS bypass capability." They added "very fresh info more coming."]]></content:encoded></item><item><title>Yes, There Are Some National Guard Willing To Do The Right Thing</title><link>https://www.techdirt.com/2025/10/30/yes-there-are-some-national-guard-willing-to-do-the-right-thing/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Fri, 31 Oct 2025 03:07:28 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[As of this moment, the National Guard is indefinitely prevented from deploying within the Chicagoland area. The court order was issued pending the Supreme Court’s decision to rule on the matter. And because this administration is a walking, talking clown show, the information that SCOTUS is getting on the matter is hilariously stratified depending on who they’re talking to.President Donald Trump‘s administration has told the U.S. Supreme Court he needs to deploy National Guard troops to the Chicago area in part because local police have failed to respond to what the Justice Department described as mob violence by people protesting his aggressive immigration enforcement.Those law enforcement agencies have given the nine justices a starkly different account of protests they called limited in scale, detailing in court filings how they have responded on specific dates and explaining how a unified command they set up to coordinate efforts dealt effectively with the demonstrations.In other words, the Trump administration is pleading the court to let it send armed troops into the third largest city in the country to protect the very people who are essentially telling the court, “Meh, we’re fine.”That won’t stop Trump, obviously, because this was never really about safety in cities or protecting federal agents. This is purely about pushing to see just how much this administration can get away with and, to go tinfoil hat on you for a moment, to begin putting the chess pieces on the right parts of the board come election time. Major city after major city will see the attempted deployment of armed forces. Trump recently stated that he will send “more than the National Guard” if needed. I’ve seen . I know how this works.So, what protects us from whatever Trump’s version of “checkmate” is? Multiple things, to be sure. Popular uprising. Overcoming whatever obstacles he constructs in the midterm elections. Organizational efforts to undermine his lawless activity wherever possible.Two Illinois National Guard members told CBS News they would refuse to obey federal orders to deploy in Chicago as part of President Trump’s controversial immigration enforcement mission — a rare act of open defiance from within the military ranks.“It’s disheartening to be forced to go against your community members and your neighbors,” said Staff Sgt. Demi Palecek, a Latina guardswoman and state legislative candidate from Illinois’s 13th District. “It feels illegal. This is not what we signed up to do.”Both Palecek and Capt. Dylan Blaha, who is running for Congress in the same district, described growing unease among Guard members after the White House federalized 500 troops – including members of the Illinois and Texas National Guard – to secure federal immigration facilities and personnel in the Chicago area.“I signed up to defend the American people and protect the Constitution,” Blaha said. “When we have somebody in power who’s actively dismantling our rights — free speech, due process, freedom of the press — it’s really hard to be a soldier right now.”Some of this isn’t new. In other cities, we’ve had National Guard members displeased with their use as political pawns in mission-less deployments to patrol peaceful cities. But I’m unaware thus far of any instances of them actually refusing orders. Such a refusal, should the order be ultimately deemed lawful, would be grounds for discharge, imprisonment, and so on. It’s a big freaking deal and would generate a ton of attention.Which is  why it needs to happen. Asked if she would refuse a direct order to deploy to Chicago, Palecek didn’t hesitate. “Absolutely. I would definitely say no,” she said. “I’m not going to go against my community members, my family and my culture. I believe this is the time to be on the right side of history.”“Look at 1930s, 1940s Germany,” Blaha said. “There is a point where if you didn’t stand up to the Gestapo, are you just actively one of them now?”It’s worse than that. Nazi Germany didn’t have social media, cell phone cameras, or the internet by which all of this chaos can be shared in real time. Whatever sins the German people committed by failing to stop Hitler’s party when they could, and they very much did commit those sins, it’s  true that the average German wasn’t nearly as informed as to what the Nazis were doing compared with the access to information that the American people have today. No soldier can claim ignorance. If they participate, they are knowingly complicit, full stop.The scary part is how unfortunately rare this sort of bravery is in the military. In fact, it seems many in the military are fully embracing Trump’s fascistic tendencies.Both Blaha and Palecek said they’ve faced retaliation for speaking publicly. Blaha disclosed that his security clearance was suspended by the Defense Department after posting a viral video urging soldiers to disobey unlawful orders. “They twisted my words,” he said. “I have about 30 days in order to provide them with a written response.”Retribution, Palecek added, is “real.” She’s received death threats since denouncing the deployments and launching her political campaign. “It weighs on you mentally after a while,” she said.Still, both say silence is not an option. “We were trained to stand up for what we believe in and stand up for the American people,” Blaha said.And stand up for the Constitution, too. Look, it must be very difficult to be a good person in the National Guard right now. You just never know when you’re going to be asked to do battle with your fellow Americans. But an oath is an oath and we should all expect, not just hope, our soldiers to behave like patriots.]]></content:encoded></item><item><title>Mathematical Proof Debunks the Idea That the Universe Is a Computer Simulation</title><link>https://science.slashdot.org/story/25/10/30/2232258/mathematical-proof-debunks-the-idea-that-the-universe-is-a-computer-simulation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[alternative_right shares a report from Phys.org: Today's cutting-edge theory -- quantum gravity -- suggests that even space and time aren't fundamental. They emerge from something deeper: pure information. This information exists in what physicists call a Platonic realm -- a mathematical foundation more real than the physical universe we experience. It's from this realm that space and time themselves emerge. "The fundamental laws of physics cannot be contained within space and time, because they generate them. It has long been hoped, however, that a truly fundamental theory of everything could eventually describe all physical phenomena through computations grounded in these laws. Yet we have demonstrated that this is not possible. A complete and consistent description of reality requires something deeper -- a form of understanding known as non-algorithmic understanding." "We have demonstrated that it is impossible to describe all aspects of physical reality using a computational theory of quantum gravity," says Dr. Faizal. "Therefore, no physically complete and consistent theory of everything can be derived from computation alone. Rather, it requires a non-algorithmic understanding, which is more fundamental than the computational laws of quantum gravity and therefore more fundamental than spacetime itself."
 
"Drawing on mathematical theorems related to incompleteness and indefinability, we demonstrate that a fully consistent and complete description of reality cannot be achieved through computation alone," explains Dr. Mir Faizal, Adjunct Professor with UBC Okanagan's Irving K. Barber Faculty of Science. "It requires non-algorithmic understanding, which by definition is beyond algorithmic computation and therefore cannot be simulated. Hence, this universe cannot be a simulation."
 
The findings have been published in the Journal of Holography Applications in Physics.]]></content:encoded></item><item><title>Google Shows Off Prototype Android XR Glasses From Extended Magic Leap Deal</title><link>https://tech.slashdot.org/story/25/10/31/0021217/google-shows-off-prototype-android-xr-glasses-from-extended-magic-leap-deal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google and Magic Leap have extended their partnership for another three years to develop Android XR glasses. They also showed off a new prototype concept that combines Google's Raxium microLED light engine with Magic Leap's AR optics, resulting in a lightweight, stylish pair of glasses that blends real-world vision with multimodal AI. 9to5Google reports: As noted by Android Central, a press release shared by Magic Leap adds some further technical details. This includes mentioning that Google's "Raxium microLED light engine" integrates with Magic Leap's tech to bring "digital content seamlessly into the world." As pictured above, the "display" portion of the lens is visible at some angles, but it's largely impossible to see.
 
Magic Leap and Google will show an AI glasses prototype at FII that will serve as a prototype and reference design for the Android XR ecosystem. The demo shows how Magic Leap's technology, integrated with Google's Raxium microLED light engine, brings digital content seamlessly into the world. The prototypes worn on stage illustrate how comfortable, stylish smart eyewear is possible and the video showed the potential for users to stay present in the real world while tapping into the knowledge and functionality of multimodal AI.
 
During the presentation, text on the nearby screens suggests that Magic Leap is mainly working with Google on the technology here, rather than bringing its own glasses to market. Magic Leap further hints at this in its press release, calling itself "an AR ecosystem partner" focused on "supporting global technology leaders that want to enter the AR market and accelerate the production of AR glasses."]]></content:encoded></item><item><title>Genode-Powered Sculpt OS 25.10 Brings Performance Improvements &amp; Better Drivers</title><link>https://www.phoronix.com/news/Sculpt-OS-25.10-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 31 Oct 2025 00:22:45 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Genode operating system framework continues innovating over a decade and a half later on this original open-source OS creation and with that Sculpt OS as its general purpose OS. Out today is Sculpt OS 25.10 to incorporate the latest enhancements to the platform...]]></content:encoded></item><item><title>&apos;Keep Android Open&apos; Campaign Pushes Back On Google&apos;s Sideloading Restrictions</title><link>https://tech.slashdot.org/story/25/10/30/2225257/keep-android-open-campaign-pushes-back-on-googles-sideloading-restrictions?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 31 Oct 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[PC Mag's Michael Kan writes: A "Keep Android Open" campaign is pushing back on new rules from Google that will reportedly block users from sideloading apps on Android phones. It's unclear who's running the campaign, but a blog post on the free Android app store F-Droid is directing users to visit the campaign's website, which urges the public to lobby government regulators to intervene and stop the upcoming restrictions. "Developers should have the right to create and distribute software without submitting to unnecessary corporate surveillance," reads an open letter posted to the site. [...]
 
Google has described the upcoming change as akin to requiring app developers to go through "an ID check at the airport." However, F-Droid condemned the new requirement as anti-consumer choice. "If you own a computer, you should have the right to run whatever programs you want on it," it says. Additionally, the rules threaten third-party app distribution on F-Droid, which operates as a "free/open-source app distribution" model.
 
In its blog post, F-Droid warns about the impact on users and Android app developers. "You, the creator, can no longer develop an app and share it directly with your friends, family, and community without first seeking Google's approval," the app store says. "Over half of all humankind uses an Android smartphone," the blog post adds. "Google does not own your phone. You own your phone. You have the right to decide who to trust, and where you can get your software from."]]></content:encoded></item><item><title>Ctrl-Alt-Speech: Chat Bot Your Tongue?</title><link>https://www.techdirt.com/2025/10/30/ctrl-alt-speech-chat-bot-your-tongue/</link><author>Mike Masnick</author><category>tech</category><pubDate>Thu, 30 Oct 2025 23:33:30 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[In this week’s round-up of the latest news in online speech, content moderation and internet regulation, Mike and Ben cover:This episode is brought to you by our sponsor WebPurify, an Intouch company. IntouchCX is a global leader in digital customer experience management, back office processing, trust and safety, and AI services.]]></content:encoded></item><item><title>Israel Demanded Google and Amazon Use Secret &apos;Wink&apos; To Sidestep Legal Orders</title><link>https://tech.slashdot.org/story/25/10/30/225232/israel-demanded-google-and-amazon-use-secret-wink-to-sidestep-legal-orders?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the Guardian: When Google and Amazon negotiated a major $1.2 billion cloud-computing deal in 2021, their customer -- the Israeli government -- had an unusual demand: agree to use a secret code as part of an arrangement that would become known as the "winking mechanism." The demand, which would require Google and Amazon to effectively sidestep legal obligations in countries around the world, was born out of Israel's concerns that data it moves into the global corporations' cloud platforms could end up in the hands of foreign law enforcement authorities.
 
Like other big tech companies, Google and Amazon's cloud businesses routinely comply with requests from police, prosecutors and security services to hand over customer data to assist investigations. This process is often cloaked in secrecy. The companies are frequently gagged from alerting the affected customer their information has been turned over. This is either because the law enforcement agency has the power to demand this or a court has ordered them to stay silent. For Israel, losing control of its data to authorities overseas was a significant concern. So to deal with the threat, officials created a secret warning system: the companies must send signals hidden in payments to the Israeli government, tipping it off when it has disclosed Israeli data to foreign courts or investigators.
 
To clinch the lucrative contract, Google and Amazon agreed to the so-called winking mechanism, according to leaked documents seen by the Guardian, as part of a joint investigation with Israeli-Palestinian publication +972 Magazine and Hebrew-language outlet Local Call. Based on the documents and descriptions of the contract by Israeli officials, the investigation reveals how the companies bowed to a series of stringent and unorthodox "controls" contained within the 2021 deal, known as Project Nimbus. Both Google and Amazon's cloud businesses have denied evading any legal obligations.]]></content:encoded></item><item><title>Universal Partners With AI Startup Udio After Settling Copyright Suit</title><link>https://entertainment.slashdot.org/story/25/10/30/2156200/universal-partners-with-ai-startup-udio-after-settling-copyright-suit?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Universal Music Group has settled its copyright lawsuit with AI music startup Udio and struck a licensing deal to launch a new AI-powered music platform next year. The Verge reports: The deal includes some form of compensation and "will provide further revenue opportunities for UMG artists and songwriters," Universal says. Udio, the company behind "BBL Drizzy," will launch the platform as a subscription service next year. Universal, alongside other industry giants Sony and Warner, sued Udio and another startup Suno for "en masse" copyright infringement last year.
 
Universal -- whose roster includes some of the world's biggest performers like Taylor Swift, Bad Bunny, and Ariana Grande -- says the new tool will "transform the user engagement experience" and let creators customize, stream, and share music. There's no indication of how much it will cost yet. Udio's existing music maker, which lets you create new songs with a few words, will remain available during the transition, though content will be held "within a walled garden" and security measures like fingerprinting will be added.]]></content:encoded></item><item><title>Age Verification, Estimation, Assurance, Oh My! A Guide to the Terminology</title><link>https://www.eff.org/deeplinks/2025/10/age-verification-estimation-assurance-oh-my-guide-terminology</link><author>Rindala Alajaji</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/ageverificationbanner.png" length="" type=""/><pubDate>Thu, 30 Oct 2025 22:37:58 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[Letting the Algorithm DecideWhy This Confusion MattersHere's the uncomfortable truth: most lawmakers writing these bills have no idea how any of this technology actually works. ]]></content:encoded></item><item><title>OpenAI Eyes $1 Trillion IPO</title><link>https://slashdot.org/story/25/10/30/2148227/openai-eyes-1-trillion-ipo?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 22:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[OpenAI is reportedly preparing for a massive IPO that could value the company at up to $1 trillion. It follows a recent corporate restructuring that loosened its dependence on Microsoft and aligned its nonprofit foundation with financial success. Reuters reports: OpenAI is considering filing with securities regulators as soon as the second half of 2026, some of the people said. In preliminary discussions, the company has looked at raising $60 billion at the low end and likely more, the people said. They cautioned that talks are early and plans -- including the figures and timing - could change depending on business growth and market conditions. Chief Financial Officer Sarah Friar has told some associates the company is aiming for a 2027 listing, the people said. But some advisers predict it could come even sooner, around late 2026.
 
[...] An IPO would open the door to more efficient capital raising and enable larger acquisitions using public stock, helping to finance CEO Sam Altman's plans to pour trillions of dollars into AI infrastructure, according to people familiar with the company's thinking. With an annualized revenue run rate expected to reach about $20 billion by year-end, losses are also mounting inside the $500 billion company, the people said. During a livestream on Tuesday, Altman addressed the possibility of going public. "I think it's fair to say it is the most likely path for us, given the capital needs that we'll have," he said.]]></content:encoded></item><item><title>Navan IPO tumbles 20% after historic debut under SEC shutdown workaround</title><link>https://techcrunch.com/2025/10/30/navan-ipo-tumbles-20-after-historic-debut-under-sec-shutdown-workaround/</link><author>Marina Temkin</author><category>tech</category><pubDate>Thu, 30 Oct 2025 21:41:40 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Navan finished its first day trading at an approximate valuation of $4.7 billion, which is about half of its last private valuation of $9.2 billion.]]></content:encoded></item><item><title>Unpatched Bug Can Crash Chromium-Based Browsers in Seconds</title><link>https://it.slashdot.org/story/25/10/30/205211/unpatched-bug-can-crash-chromium-based-browsers-in-seconds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 30 Oct 2025 21:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A critical security flaw in Chromium's Blink rendering engine can crash billions of browsers within seconds. Security researcher Jose Pino discovered the vulnerability and created a proof-of-concept exploit called Brash to demonstrate the bug affecting Chrome, Edge, OpenAI's ChatGPT Atlas, Brave, Vivaldi, Arc, Dia, Opera and Perplexity Comet. 

The flaw, reports The Register, exploits the absence of rate limiting on document.title API updates in Chromium versions 143.0.7483.0 and later. The attack injects millions of DOM mutations per second and saturates the main thread. When The Register tested the code on Edge, the browser crashed and the Windows machine locked up after about 30 seconds while consuming 18GB of RAM in one tab. Pino disclosed the bug to the Chromium security team on August 28 and followed up on August 30 but received no response. Google said it is looking into the issue.]]></content:encoded></item><item><title>Nvidia is reportedly investing up to $1B in Poolside</title><link>https://techcrunch.com/2025/10/30/nvidia-is-reportedly-investing-up-to-1-billion-in-poolside/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Thu, 30 Oct 2025 21:10:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nvidia is an existing investor in the AI company and participated in its $500 million Series A round in 2024. ]]></content:encoded></item><item><title>Pentagon Proudly Announces Its New Stable Of Propagandists</title><link>https://www.techdirt.com/2025/10/30/pentagon-proudly-announces-its-new-stable-of-propagandists/</link><author>Tim Cushing</author><category>tech</category><pubDate>Thu, 30 Oct 2025 21:08:51 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Going forward, the only thing you’ll hear reported from the Pentagon will be delivered by subservient, right-wing stenographers. The War Department has its own Ministry of Truth, staffed by people whose organizations have seen their fortunes rise along with Trump’s. The bootlicking was always there. The only change is that it’s now officially state-sanctioned. The administration doesn’t care how this looks. It only cares that it got what it wants. Not only did it break with tradition by demanding journalists agree to play by the administration’s rules if they wanted access to the Pentagon and military officials, it broke with tradition by proudly proclaiming its victory over the First Amendment on the social media platform most devoted to stifling criticism of authoritarians: Nearly one week since a rash of Pentagon journalists turned in their press credentials after refusing to sign a new restrictive press policy, the Defense Department announced a “new media” press corps, largely hailing from right-wing outlets.The 60 people from various news organizations represent, “a broad spectrum of new media outlets and independent journalists,” Pentagon spokesperson Sean Parnell wrote in a statement Wednesday on X, adding that all of the publications agreed to the agency’s press policy.There’s a lot to be appalled by/mock endlessly going on here. First, there’s the fact that this plan will be carried out by other federal agencies since it worked out perfectly here. Second, there’s no “broad spectrum” here, unless you consider multiple people saying the same things with the same pro-Trump deference a hot new take on journalistic freedom. Then there are the people generously being called “journalists” by a man speaking for an agency being concurrently fellated by at least a couple of dozen extremely right-wing publications. No one would call these people “journalists,” perhaps not even those being called “journalists” by the DoD spokesman. Brace yourself. It gets ugly immediately.According to a draft of the announcement obtained by The Washington Post ahead of Parnell’s tweet, the coalition of signatories includes the cable network Real America’s Voice, streaming service Lindell TV (started by MyPillow CEO and Trump ally Mike Lindell), the websites the Gateway Pundit, the Post Millennial, Human Events, the National Pulse, and RedState. It also includes Turning Point USA’s media brand Frontlines, as well as influencer Tim Pool’s Timcast, and a Substack-based newsletter called Washington Reporter.While the Defense Department was proud to announce it had ousted actual journalists from the Pentagon, it was a bit more cagey when it came to naming the ones who decided to stick around. Parnell bragged on X, but actual journalists had to ask some questions to come up with a list of those who swore an oath of fealty to Papa  Don or whatever the fuck it is we’re calling Donald Trump these days. What’s more surprising is the list of people who left. I’m not talking about the expected ex-pats or the ones already kicked out by the Pentagon (NPR, New York Times). I’m talking about Trump proxies like Newsmax, Fox News, and the Daily Caller. These outlets — despite spending most of their time ensuring the administration always has a bullhorn to borrow — also thought the Pentagon’s demands were inappropriate, which left only the weirdest and worst of the right wing media in place to do the government’s bidding.And this demand was even too much for one writer who worked for a right wing outlet that has spent the Trump years (which includes the Biden years) further distancing itself from anything that could be considered credible reporting — the Epoch Times — found both the DoD demand and his (now former) employer’s concession too much to stomach: A national security reporter at the right-wing Epoch Times newspaper has resigned after the publication signed on to rules restricting news gathering within the Pentagon.The reporter, Andrew Thornebrooke, submitted his resignation in writing on Friday. Although he did not have an active Pentagon press pass, he regularly covered issues related to the Defense Department and frequently reported from the cavernous military complex while working at the publication.In his resignation email, a copy of which was obtained by The New York Times, Mr. Thornebrooke called The Epoch Times’s signing of the pledge a choice to “abdicate our responsibility as journalists in favor of merely repeating state narratives.”Thornebrooke also took issue with an editorial directive to refer to antifa as a “terrorist organization” despite there being no evidence it’s an “organization” (rather than an ideological movement), much less involved in terrorism. This is censorship that isn’t even pretending to be something else. The public statements declaring a new “press” corp are just a victory lap by brutish people who have the audacity to claim they have the moral high ground while shitting all over the rights their predecessors fought and died for. ]]></content:encoded></item><item><title>AI &apos;Cheating&apos; App Founder Says Engineers Can&apos;t Make Good, Viral Content and That&apos;s Why Their Startups Flop</title><link>https://slashdot.org/story/25/10/30/1942215/ai-cheating-app-founder-says-engineers-cant-make-good-viral-content-and-thats-why-their-startups-flop?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 30 Oct 2025 20:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AI "cheating" app Cluely's CEO and cofounder, Chungin "Roy" Lee, said most startups flop because their products don't get seen. From a report: "Engineers just cannot make good content," Lee said during a Wednesday interview at TechCrunch Disrupt 2025 "There's a bunch of shallow replicas, but I challenge you to find one video you think is like, 'Yo, this is as tough as Cluely,'" he told TechCrunch. 

Every startup needs to focus more on distribution. And most startups flop because they fail to get seen, even if they have product-market fit, Lee said. Cluely launched earlier this year as a tool to help software engineers cheat on their job interviews, among other use cases. The startup earlier this year posted a tongue-in-cheek video of Lee trying to use Cluely to impress a woman on a date, which went viral.]]></content:encoded></item><item><title>As Social Media Restrictions Spread, Is The Internet Entering Its Victorian Era?</title><link>https://www.techdirt.com/2025/10/30/as-social-media-restrictions-spread-is-the-internet-entering-its-victorian-era/</link><author>Alex Beattie</author><category>tech</category><pubDate>Thu, 30 Oct 2025 19:44:18 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[A wave of proposed social media bans for young people has swept the globe recently, fuelled by mounting concern about the apparent harm the likes of TikTok, Instagram and Snapchat can cause to vulnerable minds.Australia was the first to announce restrictions on people under 16 having a social media account. New Zealand may soon follow, and Denmark’s prime minister recently declared her country would ban social media for under-15s, accusing mobile phones and social networks of “stealing our children’s childhood”.At first glance, these policies appear to be about protecting young people from mental health harm, explicit content and addictive design. But beneath the language of safety lies something else: a shift in cultural values.The bans reflect a kind of moral turn, one that risks reviving conservative notions that predate the internet. Might we be entering a new Victorian era of the internet, where the digital lives of young people are reshaped not just by regulation but by a reassertion of moral control?The Victorian era was marked by rigid social codes, modest dress and formal communication. Public behaviour was tightly regulated, and schools were seen as key sites for socialising children into gender and class hierarchies.Today, we see echoes of this in the way “digital wellness” is framed. Screen-time apps, detox retreats and “dumb” phones are marketed as tools for cultivating a “healthy” digital life – often with moral undertones. The ideal user is calm, focused and restrained. The impulsive, distracted or emotionally expressive user is pathologised.This framing is especially evident in the work of Jonathan Haidt, psychologist and author of The Anxious Generation, a central text in the age-restriction movement. Haidt argues that social media accelerates performative behaviour and emotional dysregulation in young people.Viewed this way, youth digital life involves declining psychological resilience, rising polarisation and the erosion of shared civic values, rather than being a symptom of complex developmental or technological shifts. This has helped popularise the idea that social media is not just harmful but corrupting.For example, while some research links heavy social media use to anxiety and depression, other studies suggest the effects are modest and vary widely depending on context, platform and individual differences.What’s missing from much of the debate is a recognition of young people’s agency, or their ability to navigate online spaces intelligently, creatively and socially.Indeed, youth digital life is not just about passive consumption. It’s a site of literacy, expression and connection. Platforms such as TikTok and YouTube have fostered a renaissance of oral and visual communication.Young people stitch together memes, remix videos and engage in rapid-fire editing to produce new forms of storytelling. These are not signs of decline but evolving literacies. To regulate youth access without acknowledging these skills risks suppressing the new in favour of preserving the familiar.Regulate platforms, not young peopleThis is where the Victorian metaphor becomes useful. Just as Victorian norms sought to maintain a particular social order, today’s age restrictions risk enforcing a narrow vision of what digital life should look like.On the surface, terms such as “brain rot” appear to convey the harm of excessive internet use. But in practice, they’re often used by teenagers to laugh about and resist the pressures of 24/7 hustle culture.Age restrictions may address some symptoms, but they don’t tackle the underlying design of platforms that are built to keep us scrolling, sharing and generating data.If society and governments are serious about protecting young people, perhaps the better strategy is to regulate the digital platforms. Legal scholar Eric Goldman calls the age-restriction approach a “segregate and suppress” strategy – one that punishes youth rather than holding platforms accountable.We would never ban children from playgrounds, but we do expect those spaces to be safe. Where are the safety barriers for digital spaces? Where is the duty of care from digital platforms?The popularity of social media bans suggests a resurgence of conservative values in our digital lives. But protection should not come at the cost of autonomy, creativity or expression.For many, the internet has become a moral battleground where values around attention, communication and identity are fiercely contested. But it is also a social infrastructure, one that young people are already shaping through new literacies and forms of expression.Shielding them from it risks suppressing the very skills and voices that could help us build a better digital future.]]></content:encoded></item><item><title>A Hassle-Free Battery Charger</title><link>https://spectrum.ieee.org/diy-nimh-battery-charger</link><author>Maximilian Kern</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk0MTM5Ny9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc2NjAxMjExNX0.hzitnoYWejyuEN1AAxd_H7-TR39wzOr56jBfFY1JLuo/image.png?width=600" length="" type=""/><pubDate>Thu, 30 Oct 2025 19:30:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Put NiMH cells back in the game]]></content:encoded></item><item><title>Andrew Cuomo Uses AI MPREG Schoolhouse Rock Bill to Attack Mamdani, Is Out of Ideas</title><link>https://www.404media.co/ai-generated-mpreg-schoolhouse-rock-ad-attacking-mamdani-is-the-sad-poliaesthetic-of-our-time/</link><author>Matthew Gault</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/10/Preggers.png" length="" type=""/><pubDate>Thu, 30 Oct 2025 19:25:28 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[I am haunted by a pregnant bill in Andrew Cuomo’s new AI-generated attack ad against Zohran Mamdani.Cuomo posted the ad on his  that riffed on the famous song  In Cuomo’s AI-generated , Zohran Mamdani lights money on fire while a phone bearing the ChatGPT logo explains, apparently, that Mamdani is not qualified. ]]></content:encoded></item><item><title>Rust 1.91 Promotes Windows On 64-bit ARM To Tier-1 Status</title><link>https://www.phoronix.com/news/Rust-1.91-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 30 Oct 2025 19:01:54 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Rust project announced today the release of Rust 1.91 as the latest update to this popular programming language priding itself on memory safety capabilities...]]></content:encoded></item><item><title>ChatGPT maker reportedly eyes $1 trillion IPO despite major quarterly losses</title><link>https://arstechnica.com/ai/2025/10/is-openai-worth-1-trillion-potential-ipo-may-reveal-the-answer/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2024/10/openai_treasurechest_1-1152x648.jpg" length="" type=""/><pubDate>Thu, 30 Oct 2025 18:24:32 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[On Tuesday, OpenAI CEO Sam Altman told Reuters during a livestream that going public “is the most likely path for us, given the capital needs that we’ll have.” Now sources familiar with the matter say the ChatGPT maker is preparing for an initial public offering that could value the company at up to $1 trillion, with filings possible as early as the second half of 2026. However, news of the potential IPO comes as the company faces mounting losses that may have reached as much as $11.5 billion in the most recent quarter, according to one estimate.Going public could give OpenAI more efficient access to capital and enable larger acquisitions using public stock, helping finance Altman’s plans to spend trillions of dollars on AI infrastructure, according to people familiar with the company’s thinking who spoke with Reuters. Chief Financial Officer Sarah Friar has reportedly told some associates the company targets a 2027 IPO listing, while some financial advisors predict 2026 could be possible.Three people with knowledge of the plans told Reuters that OpenAI has discussed raising $60 billion at the low end in preliminary talks. That figure refers to how much money the company would raise by selling shares to investors, not the total worth of the company. If OpenAI sold that amount of stock while keeping most shares private, the entire company could be valued at $1 trillion or more. The final figures and timing will likely change based on business growth and market conditions.]]></content:encoded></item><item><title>Chimps Are Capable of Human-Like Rational Thought, Breakthrough Study Finds</title><link>https://www.404media.co/chimps-are-capable-of-human-like-rational-thought-breakthrough-study-finds/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/10/image1-2.jpg" length="" type=""/><pubDate>Thu, 30 Oct 2025 18:00:37 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[, our newsletter about the most exciting and mind-boggling science news and studies of the week. Chimpanzees revise their beliefs if they encounter new information, a hallmark of rationality that was once assumed to be unique to humans, according to a study published on Thursday in .Researchers working with chimpanzees at the Ngamba Island Chimpanzee Sanctuary in Uganda probed how the primates judged evidence using treats inside boxes, such as a “weak” clue—for example, the sound of a treat inside a shaken box—and a "strong" clue, such as a direct line of sight to the treat. The chimpanzees were able to rationally evaluate forms of evidence and to change their existing beliefs if presented with more compelling clues. The results reveal that non-human animals can exhibit key aspects of rationality, some of which had never been directly tested before, which shed new light on the evolution of rational thought and critical thinking in humans and other intelligent animals.      “Rationality has been linked to this ability to think about evidence and revise your beliefs in light of evidence,” said co-author Jan Engelmann, associate professor at the department of psychology at the University of California, Berkeley, in a call with 404 Media. “That’s the real big picture perspective of this study.”While it’s impossible to directly experience the perspective of a chimpanzee, Engelmann and his colleagues designed five controlled experiments for groups of anywhere from 15 to 23 chimpanzee participants. In the first and second experiments, the chimps received a weak clue and a strong clue for a food reward in a box. The chimpanzees consistently made their choices based on the stronger evidence, regardless of the sequence in which the clues were presented. In the third experiment, the chimps were shown an empty box in addition to the strong and weak clues. After this presentation, the box with the strong evidence was removed. In this experiment, the chimpanzees still largely chose the weak clue over the empty box. In the fourth experiment, chimpanzees were given a second “redundant” weak clue—for instance, the experimenter would shake a box twice. Then, they were given a new type of clue, like a second piece of food being dropped into a box in front of them. They were significantly more likely to change their beliefs if the clue provided fresh information, demonstrating an ability to distinguish between redundant and genuinely new evidence.Finally, in the fifth experiment, the chimpanzees were presented with a so-called “defeater” that undermined the strong clue, such as a direct line of sight to a picture of food inside the box, or a shaken box containing a stone, not a real treat. The chimps were significantly more likely to revise their choice about the location of the food in the defeater experiments than in experiments with no defeater. This experiment showcased an ability to judge that evidence that initially seems strong can be weakened with new information.“The most surprising result was, for sure, experiment five,” Engelmann said. “No one really believed that they would do it, for many different reasons.”For one thing, he said, the methodology of the fifth experiment demanded a lot of attention and cognitive work from the chimpanzees, which they successfully performed. The result also challenges the assumption that complex language is required to update beliefs with new information. Despite lacking this linguistic ability, chimpanzees are somehow able to flexibly assign strength to different pieces of evidence.Speaking from the perspective of the chimps, Engelmann outlined the responses to experiment five as: “I used to believe food was in there because I heard it in there, but now you showed me that there was a stone in there, so this defeats my evidence. Now I have to give up that belief.” “Even using language, it takes me ten seconds to explain it,” he continued. “The question is, how do they do it? It’s one of the trickiest questions, but also one of the most interesting ones. To put it succinctly, how to think without words?”To hone in on that mystery, Engelmann and his colleagues are currently repeating the experiment with different primates, including capuchins, baboons, rhesus macaques, and human toddlers and children. Eventually, similar experiments could be applied to other intelligent species, such as corvids or octopuses, which may yield new insights about the abundance and variability of rationality in non-human species.“I think the really interesting ramification for human rationality is that so many people often think that only humans can reflect on evidence,” Engelmann said. “But our results obviously show that this is not necessarily the case. So the question is, what's special about human rationality then?”Engelmann and his colleagues hypothesize that humans differ in the social dimensions of our rational thought; we are able to collectively evaluate evidence not only with our contemporaries, but by consulting the work of thinkers who may have lived thousands of years ago. Of course, humans also often refuse to update beliefs in light of new evidence, which is known as “belief entrenchment” or “belief perseveration” (many such cases). These complicated nuances add to the challenge of unraveling the evolutionary underpinnings of rationality.That said, one thing is clear: many non-human animals exist somewhere on the gradient of rational thought. In light of the recent passing of Jane Goodall, the famed primatologist who popularized the incredible capacities of chimpanzees, the new study carries on a tradition of showing that these primates, our closest living relatives, share some degree of our ability to think and act in rational ways.Goodall “was the first Western scientist to observe tool use in chimpanzees and really change our beliefs about what makes humans unique,” Engelmann said. “We're definitely adding to this puzzle by showing that rationality, which has so long been considered unique to humans, is at least in some forms present in non-human animals.”, our newsletter about the most exciting and mind-boggling science news and studies of the week. ]]></content:encoded></item><item><title>Trump’s Government Of Spite: Political Performance Art For Assholes</title><link>https://www.techdirt.com/2025/10/30/trumps-government-of-spite-political-performance-art-for-assholes/</link><author>Mike Masnick</author><category>tech</category><pubDate>Thu, 30 Oct 2025 17:51:31 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Not a policy response. Not an attempt at dialogue. Not even a coherent defense of whatever decisions prompted the protests. Just a middle finger, dressed up as content, optimized for maximum engagement from his base and maximum rage from everyone else.Welcome to governance-by-trolling, where the entire apparatus of the federal government has been repurposed into a machine for generating likes, shares, and the tears of political opponents.This is worse than normal political combat. Political opponents have always attacked each other, sometimes viciously. But those attacks were typically in service of —a policy goal, an electoral advantage, a governing philosophy. This is categorically different: a government that has replaced policymaking entirely with performance art designed solely to upset the people who voted against them.The evidence is everywhere, and it’s getting worse. When a reporter asks the White House a straightforward question about policy, the official response is literally “your mom”—not as a joke that slipped out, but as official White House communications strategy. The Vice President continues to use a racist slur against a sitting Senator, not because it advances any policy position, but because it gets applause from the MAGA base. It takes thirty seconds reviewing any official government account on social media to see them posting childish memes insulting and attacking their political rivals.This has nothing to do with policy. This certainly has nothing to do with leadership. It’s just a Presidency that is there to troll people it doesn’t like. That’s literally all they do.Governance is for SuckersThey know exactly what they’re doing. Yes, there are elements of incompetence and a distorted view of how the world works, but this is a deliberate rejection of governance as a sucker’s game—something that gets in the way of what they’re actually here to do.Actual governing requires building coalitions. It requires incorporating feedback from people who disagree with you. It requires making tradeoffs between competing interests and, at minimum, pretending that you care about the concerns of citizens who didn’t vote for you. It requires maintaining the boring, unglamorous institutions that make a modern society function—the regulatory frameworks, the expert agencies, the diplomatic relationships that took decades to build.None of that gets you engagement on X. None of that earns you attaboys from the reply-guy brigade and the bro podcasts. And more importantly, none of that lets you loot the treasury and dismantle constraints on executive power while your supporters cheer you on for being “based.”Yes, this administration has policy goals, but they’re focused on consolidating executive power, massively expanding Trump’s personal army, eliminating any legal limits (including Constitutional limits) on their own power, and imposing their will on the American public. It’s not democratic governance. It’s not “governance” of any kind.And given that their goal is full and total control over the entire apparatus of government for their own very personal interests, they have no concern for transparency or communicating with the public other than this kind of performative trolling.What gets engagement is cruelty. What gets shares is dunking on your opponents. What gets applause from the base is demonstrating, over and over, that you don’t give a shit about half the country—and that you’re getting away with your unconstitutional power grab and institutional destruction.So they’ve optimized for this.This is what happens when you build a political movement around a single metric: how mad is the other side? The base rewards cruelty, and so cruelty is what gets produced, at industrial scale, from the highest offices in the land. It’s the same dynamic we’ve seen play out on social media platforms that prioritize short-term growth over long-term sustainability—optimize purely for engagement without constraint, and the system fills up with outrage and abuse because that’s what the metric rewards.Except now that dynamic is running the federal government. And the people running it aren’t confused about this trade-off. They’ve actively chosen engagement metrics over governing because governing is for suckers, and engagement is how you maintain power while you loot the rest of the government.Meming Our Way to Institutional CollapseGovernance-by-trolling is the opposite of all of that.When the primary goal of every action is to maximize the anger of your political opponents, you cannot maintain the institutions that make progress possible. You cannot have stable regulatory frameworks when every decision is made based on “will this own the libs?”You cannot attract competent experts to government service when the work environment is defined by performative cruelty. You cannot build international alliances when your foreign policy is driven by what will generate the most engagement on social media. Instead, you drive our allies into the waiting arms of our adversaries to protect themselves from being the target of the next big troll.The goal is obvious: the active destruction of the capacity to govern at all. Who needs “governance” when you’re planning to rule dictatorially? Who needs transparency when you look to control all major media sources? Who needs competent experts when you’re just looting the system?As such, the only publicity strategy you can bank on is “how mad can we make the libs”?And here’s what makes it even more pathetic: they’re trying to sell this destruction as strength. Think about what it means that the President of the United States spent his time posting videos of himself dumping shit on citizens. Not because it would change any minds or advance any policy goal, but because he needed the validation of watching his base cheer. That’s not power—that’s the behavior of someone so desperate for approval, so incapable of actual leadership, that they’ve reduced the presidency to a content farm optimized for engagement metrics.The same goes for the “your mom” response from the White House. You’re supposed to be the communications apparatus for the most powerful government on Earth, and your strategy is middle school cafeteria comebacks? That’s not owning anyone. That’s admitting you have nothing substantive to say and hoping nobody notices if you act like enough of an asshole.Trolling is easy. Any idiot can post something offensive and get a reaction. What’s hard is actually governing. They’ve given up on that part entirely, and they’re desperately trying to convince everyone—including themselves—that this performance is actually a demonstration of strength rather than an admission of complete incompetence at the actual job.Performance Art Masquerading As StrengthWe know how this story ends because we’ve seen it play out on every social media platform that’s tried to optimize purely for engagement. The site fills up with rage bait, harassment, and spam. Serious users leave. Advertisers pull out. And what’s left is a toxic wasteland inhabited only by the people who enjoy making others miserable and the people too obsessed by the car crash to look away.Now imagine that dynamic playing out not on a website you can leave, but in the government that controls your healthcare, your infrastructure, your national security, and your civil liberties. And it won’t be advertisers who leave but domestic investment. Who wants to invest in a country that is only focused on making half the population mad?That’s where we are. A government that’s abandoned governing in favor of looting on the back end and generating engagement on the front end. The comms strategy is entirely about getting love from its base and blinding hatred from everyone else. Leadership that mistakes cruelty for strength and trolling for strategy. An entire administration optimized for one thing: making a massive part of the country angry while its base cheers them on for being “based” enough to loot in broad daylight.The institutions that enable progress, innovation, and democratic governance are being systematically dismantled—not as collateral damage in pursuit of some policy goal, but as the actual point. Because maintaining those institutions requires caring about more than just the approval of your most rabid supporters. It requires believing that governance matters at all.And that’s the real danger here: not just that they’re bad at governing, but that they’ve convinced a significant portion of the country that governing itself is the enemy. That “owning the libs” is not a strategy but the entire purpose. It’s like infants who drop food on the floor just to see everyone around them react. All MAGA can do is drop shit on the floor, looking around to see how the adults around them act.With infants, it’s a cute act. With the entire government of the most powerful nation on earth, it’s terrifying.]]></content:encoded></item><item><title>Daily Deal: The 2025 AI Super Skills Bundle</title><link>https://www.techdirt.com/2025/10/30/daily-deal-the-2025-ai-super-skills-bundle-4/</link><author>Daily Deal</author><category>tech</category><pubDate>Thu, 30 Oct 2025 17:46:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The 2025 AI Super Skills Bundle has 8 courses to help you get familiar with how to use some of the latest and coolest artificial intelligence tools out there. Courses cover ChatGPT, DALL-E 3, Leonardo AI, Quillbot, and more. It’s on sale for $30.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Canva launches its own design model, adds new AI features to the platform</title><link>https://techcrunch.com/2025/10/30/canva-launches-its-own-design-model-adds-new-ai-features-to-the-platform/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 30 Oct 2025 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Canva is launching new features like Forms and email design, and it makes Affinity free for all users.]]></content:encoded></item><item><title>Pete Hegseth May Be The Defense Department Boss, But Top Officers Say He’s No Leader</title><link>https://www.techdirt.com/2025/10/30/pete-hegseth-may-be-the-defense-department-boss-but-top-officers-say-hes-no-leader/</link><author>Tim Cushing</author><category>tech</category><pubDate>Thu, 30 Oct 2025 16:35:13 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Like far too many people in the Trump administration — including the 79-year-old child currently sitting in the Oval Office — Secretary of Defense Pete Hegseth thinks he’s  respect simply because of the position he holds.While a certain amount of deference is expected when you’re the boss, it’s much more difficult to  the respect you need to actually be a leader. Hegseth — elevated from his Fox News commentator spot to one of the most powerful positions in the world — not only seems incapable of doing this, but actually seems  to become a true leader.Instead, Hegseth has become nothing more than a Trump puppet. Puppets generally aren’t given positions of power, but the only people Trump trusts enough to elevate are those who’ve demonstrated there’s nothing they won’t do to push the party line. Military officials are resigning or getting fired at an alarming rate. Meanwhile, Hegseth (who needed J.D. Vance to set into the Senate to get appointed by a 51-50 vote) has done everything he can to prove he’s unfit to lead. And that’s on top of everything he had already done to demonstrate he should never have been considered for this post. I’ll let Ben Wolfgang of the Washington Times summarize the series of embarrassments that is DoD Secretary Pete Hegseth: Mr. Hegseth’s nomination was nearly derailed by allegations of personal misconduct, including sexual assault, excessive drinking, spousal abuse and financial mismanagement of two veterans organizations he ran. That’s just the stuff he did  Trump decided Hegseth should head up the US military. Since then, Hegseth has been involved with  careless chat conversations about classified military operations. He also embarrassed himself (and the military, by extension) during a public appearance with Trump in which he talked down to a hastily (and expensively) gathered group of top military officials. Hegseth called them fat, ranted about beards, and generally made an ass of himself. Having laid out what he apparently thought was some sort of plan for the future has left several military officials not only unimpressed, but genuinely worried Hegseth’s tenure will do long-term damage to this institution. Back to Ben Wolfgang and the Washington Times: High-level sources said that they believe Mr. Hegseth is simultaneously doing deep damage to the military, both from a public relations standpoint and structurally behind the scenes, that may not be fully apparent until months or even years from now. They argue that the firings, early retirements and resignations that have amassed over the past eight months will fundamentally weaken the military.“Across the services, we are bleeding talent, talented generals and flag officers, for what appears to be the opposite of a meritocracy,” another current senior officer said. “There are people being held back from promotions, or being fired, or removed for sometimes unknown reasons, often for favoritism, or just simple relationships.”The federal government is just one big culture of fear. The damage done by Elon Musk’s DOGE continues to resonate and individual agencies (like the Defense Department) are engaged in persistent purges meant to rid them of everyone but Trump loyalists. It’s no way to run a country, but that’s to be expected, because none of these people are capable of running a country.Unbelievably, some military officers and officials think an uptick in recruitment proves Hegseth is the right man for the job. But that’s just going to contribute to the long-term problems Hegseth and Trump administration are creating. After all, if the messaging is a blend of ‘roid rage and racist dog whistling, the self-selecting nature of joining the military is going to result in it being filled with a bunch of people similarly unfit to be in the business of defending this nation. The official response to the Washington Times reporting is exactly what you’d expect it to be: “The anonymous general and senior officer quoted in your article should put their names to their comments if that’s what they truly believe in and consider resigning from their post. Our warriors deserve senior leaders who support the mission and put warfighting first,” [Pentagon spokesman Sean] Parnell said.This is self-serving bullshit. It would be one thing if the only consequence for speaking publicly was a firing or a resignation. But this administration is never satisfied by a simple shit-canning. The vengeful thugs running rampant in the Trump regime are always willing to drum up criminal charges for anyone they consider an enemy, even if it’s just people who simply aren’t going to bend the knee or stay silent about the damage being done to the entirety of the US government. This government refuses to consider even the mildest and most constructive of criticism. To these ingrates, any dissent is intolerable, if not actually criminal.  ]]></content:encoded></item><item><title>Trump Orders Nuclear Testing As Nuke Workers Go Unpaid</title><link>https://www.404media.co/trump-orders-nuclear-testing-as-nuke-workers-go-unpaid/</link><author>Matthew Gault</author><category>tech</category><enclosure url="https://images.unsplash.com/photo-1563988346830-7e578dca30db?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fG51Y2xlYXIlMjB3ZWFwb258ZW58MHx8fHwxNzYxODQxMzE0fDA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000" length="" type=""/><pubDate>Thu, 30 Oct 2025 16:22:42 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Last night Trump directed the Pentagon to start testing nukes again. If that happens, it’ll be the first time the US has detonated a nuke in more than 30 years. The organization that would likely be responsible for this would be the National Nuclear Security Administration (NNSA), a civilian workforce that oversees the American nuclear stockpile. Because of the current government shutdown, 1,400 NNSA workers are on furlough and the remaining 375 are working without pay.America detonated its last nuke in 1992 as part of a general drawn down following the collapse of the Soviet Union. Four years later, it was the first country to sign the Comprehensive Nuclear-Test Ban Treaty (CTBT) which bans nuclear explosions for civilian or military purposes. But Congress never ratified the treaty and the CTBT never entered into force. Despite this, there has not been a nuke tested by the United States since.The NNSA has maintained the underground tunnels once used for testing since the 1990s and converted them into a different kind of space that verifies the reliability of existing nukes without blowing them up in what are called  During a rare tour of the tunnel with journalists earlier this year, a nuclear weapons scientist from Los Alamos National Laboratory  that “our assessment is that there are no system questions that would be answered by a test, that would be worth the expense and the effort and the time.”Right now, the NNSA might be hard pressed to find someone to conduct the test. It employs around 2,000 people and the shutdown has seen 1,400 of them furloughed and 375 working without pay. The civilian nuclear workforce was already having a tough year. In February, the Department of Government Efficiency cut 350 NNSA employees only to  all but 28 when they realized how essential they were to nuclear safety. But uncertainty continued and in April the Department of Energy declared 500 NNSA employees “non-essential” and at risk of termination.That’s  for a government agency charged with ensuring the safety and effectiveness of America’s nuclear weapons. The NNSA is currently in the middle of a massive project to “modernize” America’s nukes, an effort that will cost trillions of dollars. Part of modernization means producing new plutonium pits, the core of a nuclear warhead. That’s a complicated and technical process and no one is sure how much it’ll cost and how dangerous it’ll be. And now, it may have to resume nuclear testing while understaffed. “We have run out of federal funds for federal workers,” Secretary of Energy Chris Wright said in a  announcing furlough on October 20. “This has never happened before…we have never furloughed workers in the NNSA. This should not happen. But this was as long as we could stretch the funds for federal workers. We were able to do some gymnastics and stretch it further for the contractors.”Three days later, Rep. Dina Titus (D-NV) said the furlough was making the world less safe. “NNSA facilities are charged with maintaining nuclear security in accordance with long-standing policy and the law,” she said in a press . “Undermining the agency’s workforce at such a challenging time diminishes our nuclear deterrence, emboldens international adversaries, and makes Nevadans less safe. Secretary Wright, Administrator Williams, and Congressional Republicans need to stop playing politics, rescind the furlough notice, and reopen the government.”Trump announced the nuclear tests in a post on Truth Social, a platform where he announces a lot of things that ultimately end up not happening. “The United States has more Nuclear Weapons than any other country. This was accomplished, including a complete update and renovation of existing weapons, during my First Term in office. Because of the tremendous destructive power, I HATED to do it, but had no choice! Russia is second, and China is a distant third, but will be even within 5 years. Because of other countries testing programs, I have instructed the Department of War to start testing our Nuclear Weapons on an equal basis. That process will begin immediately. Thank you for your attention to this matter! PRESIDENT DONALD J. TRUMP,” .Matt Korda, a nuclear expert with the Federation of American Scientists, said that the President’s Truth social post was confusing and riddled with misconceptions. Russia has  than America. Nuclear modernization is ongoing and will take  and many years to complete. Over the weekend, Putin announced that Russia had  a nuclear-powered cruise missile and on Tuesday he said the country had  with a nuclear-powered undersea drone. Russia  from the CTBT in 2023, but neither recent test involved a nuclear explosion. Russia last blew up a nuke in 1990 and China conducted its most recent test in 1996. Both have said they would  should America do it. Korda said it's unclear what, exactly, Trump means. He could be talking about anything from test firing non-nuclear equipped ICBMs to underground testing to detonating nukes in the desert. “We’ll have to wait and see until either this Truth Social post dissipates and becomes a bunch of nothing or it actually gets turned into policy. Then we’ll have something more concrete to respond to,” Korda said.Worse, he thinks the resumption of testing would be bad for US national security. “It actually puts the US at a strategic disadvantage,” Korda said. “This moratorium on not testing nuclear weapons benefits the United States because the United States has, by far, the most advanced modeling and simulation equipment…by every measure this is a terrible idea.”The end of nuclear detonation tests has spurred 30 years of innovation in the field of computer modeling. Subcritical computer modeling happens in the NNSA-maintained underground tunnels where detonations were once a common occurrence. The Los Alamos National Laboratories and other American nuclear labs are building massive super computers that are, in part, the result of decades of work spurred by the end of detonations and the embrace of simulation. Detonating a nuclear weapon—whether above ground or below—is disastrous for the environment. There are people alive in the United States today who are living with cancer and other health conditions caused by American nuclear testing. Live tests make the world more anxious, less safe, and encourage other nuclear powers to do their own. It also uses up a nuke, something America has said it wants to build more of.“There’s no upside to this,” Korda said. He added that he felt bad for the furloughed NNSA workers. “People find out about significant policy changes through Truth social posts. So it’s entirely possible that the people who would be tasked with carrying out this decision are learning about it in the same way we are all learning about it. They probably have the exact same kinds of questions that we do.”]]></content:encoded></item><item><title>AMD ROCm 7.1 Released: Many Instinct MI350 Series Improvements, Better Performance</title><link>https://www.phoronix.com/news/AMD-ROCm-7.1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 30 Oct 2025 16:19:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As expected after noting this morning that ROCm 7.1 release preparations were underway, ROCm 7.1 is now officially released as the newest step-forward for this open-source GPU compute stack for Radeon and Instinct hardware...]]></content:encoded></item><item><title>Bevel raises $10M Series A from General Catalyst for its AI health companion</title><link>https://techcrunch.com/2025/10/30/bevel-raises-10m-series-a-from-general-catalyst-for-its-ai-health-companion/</link><author>Tage Kene-Okafor</author><category>tech</category><pubDate>Thu, 30 Oct 2025 16:05:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Bevel's AI health companion unifies data from wearables and daily habits across sleep, fitness, and nutrition into personalized insights.]]></content:encoded></item><item><title>AMD Strix Point Performance Continues Evolving Nicely With Ubuntu 25.10</title><link>https://www.phoronix.com/review/amd-strix-point-ubuntu-2510</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 30 Oct 2025 16:00:44 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[This week marks fifteen months since AMD Strix Point laptops began shipping. Back at the end of July 2024 the Linux performance and support was already in good shape while since then the Linux performance has only evolved even more to make these AMD Zen 5 laptops perform even better. Here is a fresh look at how the performance has evolved since launch day and the added gains when moving to the recently released Ubuntu 25.10 and some performance advantages too if moving to the in-development Linux 6.18 kernel.]]></content:encoded></item><item><title>After teen death lawsuits, Character.AI will restrict chats for under-18 users</title><link>https://arstechnica.com/information-technology/2025/10/after-teen-death-lawsuits-character-ai-will-restrict-chats-for-under-18-users/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/03/robot_no_sign_3-1152x648.jpg" length="" type=""/><pubDate>Thu, 30 Oct 2025 15:54:18 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[On Wednesday, Character.AI announced it will bar anyone under the age of 18 from open-ended chats with its AI characters starting on November 25, implementing one of the most restrictive age policies yet among AI chatbot platforms. The company faces multiple lawsuits from families who say its chatbots contributed to teenager deaths by suicide.Over the next month, Character.AI says it will ramp down chatbot use among minors by identifying them and placing a two-hour daily limit on their chatbot access. The company plans to use technology to detect underage users based on conversations and interactions on the platform, as well as information from connected social media accounts. On November 25, those users will no longer be able to create or talk to chatbots, though they can still read previous conversations. The company said it is working to build alternative features for users under the age of 18, such as the ability to create videos, stories, and streams with AI characters.Character.AI CEO Karandeep Anand told The New York Times that the company wants to set an example for the industry. “We’re making a very bold step to say for teen users, chatbots are not the way for entertainment, but there are much better ways to serve them,” Anand said in the interview. The company also plans to establish an AI safety lab.]]></content:encoded></item><item><title>Threads now lets you approve and filter your replies</title><link>https://techcrunch.com/2025/10/30/threads-now-lets-you-approve-and-filter-your-replies/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 30 Oct 2025 15:45:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[While Threads already offers tools that let you limit replies to people you follow, your followers, or people you mention, the new feature allows you to keep your replies open to all without the negative consequences of having discussions derailed.]]></content:encoded></item><item><title>Someone Snuck Into a Cellebrite Microsoft Teams Call and Leaked Phone Unlocking Details</title><link>https://www.404media.co/someone-snuck-into-a-cellebrite-microsoft-teams-call-and-leaked-phone-unlocking-details/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/10/cellebrite-youtube.png" length="" type=""/><pubDate>Thu, 30 Oct 2025 15:10:29 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Someone recently managed to get on a Microsoft Teams call with representatives from phone hacking company Cellebrite, and then leaked a screenshot of the company’s capabilities against many Google Pixel phones, according to a forum post about the leak and 404 Media’s review of the material.The leak follows others obtained and verified by 404 Media over the last 18 months. Those leaks and its competitor Grayshift, now owned by Magnet Forensics. Both companies constantly hunt for techniques to unlock phones law enforcement have physical access to.]]></content:encoded></item><item><title>LVFS + Fwupd Serve Up More Than 135 Million Firmware Downloads For Linux Users</title><link>https://www.phoronix.com/news/LVFS-Fwupd-135-Million-Download</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 30 Oct 2025 15:06:11 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Linux Vendor Firmware Service (LVFS) that goes hand-in-hand with the Fwupd open-source firmware updating utility celebrated the milestone on Wednesday of crossing 135 million firmware updates...]]></content:encoded></item><item><title>‘Liquid Jets’ Could Be Key to Studying Cancer Cells</title><link>https://spectrum.ieee.org/hypersonic-levitation-spinning-cancer-studies</link><author>Perri Thaler</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk3MTYwMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5NTI5MDQ1MH0.yqYAUrM9yERV697Pp2gOeoVBUkrcLk2hRTWTlp_85Es/image.jpg?width=600" length="" type=""/><pubDate>Thu, 30 Oct 2025 14:41:20 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Acoustic levitation and MEMS tech facilitate single cell research]]></content:encoded></item><item><title>Google partners with Ambani’s Reliance to offer free AI Pro access to millions of Jio users in India</title><link>https://techcrunch.com/2025/10/30/google-partners-with-ambanis-reliance-to-offer-free-ai-pro-access-to-millions-of-jio-users-in-india/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Thu, 30 Oct 2025 14:06:19 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[U.S. tech giants increasingly view India as the next big frontier — a place to gather diverse data, refine models, and test AI use cases that could later scale across other emerging markets.]]></content:encoded></item><item><title>Libraries Scramble for Books After Giant Distributor Shuts Down</title><link>https://www.404media.co/libraries-scramble-for-books-after-giant-distributor-shuts-down/</link><author>Claire Woodcock</author><category>tech</category><enclosure url="https://images.unsplash.com/photo-1692576735072-a317af931b24?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDIzfHxMaWJyYXJ8ZW58MHx8fHwxNzYxMzI5NDYzfDA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000" length="" type=""/><pubDate>Thu, 30 Oct 2025 14:02:39 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[One of the largest distributors of print books for libraries is winding down operations by the end of the year, a huge disruption to public libraries across the country, some of which are warning their communities the shut down will limit their ability to lend books. “You might notice some delays as we (and more than 6,000 other libraries) transition to new wholesalers,” the Jacksonville Public Library told its community in . “We're keeping a close eye on things and doing everything we can to minimize any wait times.”]]></content:encoded></item><item><title>New Linux Patch Expands The Range Of AMD Zen 6 CPU Models</title><link>https://www.phoronix.com/news/Linux-Patch-More-Zen-6-Models</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 30 Oct 2025 13:32:14 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD Linux engineers continue to be quite busy working on enabling next-generation Zen 6 processors that will begin shipping next year. The newest patch working its way to the Linux kernel is expanding the range of Zen 6 CPU models detected by the kernel...]]></content:encoded></item><item><title>Qt Creator 18 Released With Experimental Support For Development Containers</title><link>https://www.phoronix.com/news/Qt-Creator-18-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 30 Oct 2025 13:21:17 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Qt Creator 18 is now available as the latest version of this Qt/C++-focused integrated development environment...]]></content:encoded></item><item><title>Figma acquires AI-powered media generation company Weavy</title><link>https://techcrunch.com/2025/10/30/figma-acquires-ai-powered-media-generation-company-weavy/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 30 Oct 2025 13:15:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Figma said that Weavy will exist as a stand-alone product for now and that, in the future, it will be integrated with the Figma Weave brand, along with the rest of the Figma platform.]]></content:encoded></item><item><title>The Prompting Company snags $6.5M to help products get mentioned in ChatGPT and other AI apps</title><link>https://techcrunch.com/2025/10/30/the-prompting-company-snags-6-5m-to-help-products-get-mentioned-in-chatgpt-and-other-ai-apps/</link><author>Tage Kene-Okafor</author><category>tech</category><pubDate>Thu, 30 Oct 2025 13:10:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Retailers could see up to a 520% increase in traffic from chatbots and AI prompts in 2025 compared to 2024, according to the report. For brands, that means figuring out how to show up in AI-generated recommendations, and fast.]]></content:encoded></item><item><title>Mother Describes the Dark Side of Apple&apos;s Family Sharing</title><link>https://apple.slashdot.org/story/25/10/30/0242231/mother-describes-the-dark-side-of-apples-family-sharing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from 9to5Mac: A mother with court-ordered custody of her children has described how Apple's Family Sharing feature can be weaponized by a former partner. Apple support staff were unable to assist her when she reported her former partner using the service in controlling and coercive ways... [...] Namely, Family Sharing gives all the control to one parent, not to both equally. The parent not identified as the organizer is unable to withdraw their children from this control, even when they have a court order granting them custody. As one woman's story shows, this can allow the feature which allows it to be weaponized by an abusive former partner.
 
Wired reports: "The lack of dual-organizer roles, leaving other parents effectively as subordinate admins with more limited power, can prove limiting and frustrating in blended and shared households. And in darker scenarios, a single-organizer setup isn't merely inconvenient -- it can be dangerous. Kate (name changed to protect her privacy and safety) knows this firsthand. When her marriage collapsed, she says, her now ex-husband, the designated organizer, essentially weaponized Family Sharing. He tracked their children's locations, counted their screen minutes and demanded they account for them, and imposed draconian limits during Kate's custody days while lifting them on his own [...] After they separated, Kate's ex refused to disband the family group. But without his consent, the children couldn't be transferred to a new one. "I wrongly assumed being the custodial parent with a court order meant I'd be able to have Apple move my children to a new family group, with me as the organizer," says Kate. But Apple couldn't help. Support staff sympathized but said their hands were tied because the organizer holds the power." Although users can "abandon the accounts and start again with new Apple IDs," the report notes that doing so means losing all purchased apps, along with potentially years' worth of photos and videos.]]></content:encoded></item><item><title>AI Model Growth Outpaces Hardware Improvements</title><link>https://spectrum.ieee.org/mlperf-trends</link><author>Dina Genkina</author><category>tech</category><pubDate>Thu, 30 Oct 2025 13:00:00 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[A looking at the MLPerf AI training competition shows hardware is struggling]]></content:encoded></item><item><title>WhatsApp adds passkey protection to end-to-end encrypted backups</title><link>https://techcrunch.com/2025/10/30/whatsapp-adds-passkey-protection-to-end-to-end-encrypted-backups/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 30 Oct 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[This means if you lose your device, you can use methods like fingerprint, face, or the screen lock code of your previous device to access WhatsApp's backup.]]></content:encoded></item><item><title>Trump FCC Votes To Make It Easier For Your Broadband ISP To Rip You Off</title><link>https://www.techdirt.com/2025/10/30/trump-fcc-votes-to-make-it-easier-for-your-broadband-isp-to-rip-you-off/</link><author>Karl Bode</author><category>tech</category><pubDate>Thu, 30 Oct 2025 12:31:13 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[As promised, the Trump FCC under Brendan Carr this week voted to begin dismantling rules requiring that your ISP offer clear and transparent details on the cost and limitations of your broadband connection.The rules, originally  as part of the infrastructure bill, required that ISPs affix a sort of “nutrition label” to broadband access at the point of sale. It was a long-cultivated bid to do the  to combat decades of misleading pricing and bullshit fees in broadband and TV. The rules simply required that ISPs list pricing, hidden fees, connection speed, and other limitations in a very clear manner on their websites. One recent study showed that ISPs were doing a pretty shitty job of compliance, and the government hasn’t really done anything serious to enforce the standard. They’re not eliminating the rules entirely just , because they don’t want to make it  they’re just ignoring the will of Congress to benefit the biggest corporations. As consumer rights groups note, Carr is just weakening them to the point of unenforced uselessness, so he can then discard them later after claiming they don’t work:“It’s the start of whittling away at these rules,” Raza Panjwani, senior policy counsel at New America’s Open Technology Institute, told CNET. “You get this two-step, right? You make it less useful. Then you say, ‘Oh, look, it’s not that useful. We should get rid of it.’”The NPRM (notice of proposed rulemaking) was adopted on Tuesday. The FCC is now allowing 60 days for comments and responses so Carr can pretend this is some sort of democratic process (the FCC comment system is historically gamed by bad actors hired by the telecom lobby). A more formal vote to lobotomize the rules will come sometime later this year. Carr isn’t even bothering to defend or announce this week’s action because he knows undermining Congress to help big telecom screw the public isn’t a good look. The FCC’s lone Democrat didn’t mince words on the effort:“What adds insult to injury is that the FCC does not even explain why this proposal is necessary,” Gomez said. “Make it make sense. Instead of scaling back the information that customers receive, we should be making sure that, in fact, they can benefit from the labels.”In a previous blog post (which you should read the first few paragraphs of to appreciate Carr’s particular brand of “humor”), Carr tries to pass this whole thing off as an efficient improvement of consumer protection. Which is, to be clear, a lie:“We want consumers to get quick and easy access to the information they want and need to compare broadband plans (as Congress has provided) without imposing unnecessary burdens.”Again, by “unnecessary burdens,” Carr means simply asking regional cable giants like Comcast and Charter — who enjoy a monopoly on broadband access across vast swaths of the U.S. — to be honest about how much you have to pay them. And whether or not Comcast is exploiting a lack of competition to make your connection more expensive or confusing (like usage caps). Even if Carr wasn’t weakening the rules, he’d never enforce them anyway. His goal, at big telecom’s direct behest, is to make sure that future administrations can’t either. This is, like so much Trumpism does, just rank corruption buried under the movement’s fake “populist” dedication to the working class and fake government efficiency. And nothing quite says “make America great again” like making it easier for Comcast Corporation to rip you off.]]></content:encoded></item><item><title>New Thermal Battery Supplies Clean Heat for Oil Extraction</title><link>https://spectrum.ieee.org/thermal-battery-for-industria-heat</link><author>Vanessa Bates Ramirez</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk3NTYwOS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgxNzkwOTE1Mn0.C8j4aP1FAM5nuet7Pq23kia5pTN-UXBXlcQnZAp7WJU/image.jpg?width=600" length="" type=""/><pubDate>Thu, 30 Oct 2025 12:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[System converts solar energy to heat for industrial processes]]></content:encoded></item><item><title>New Thermal Battery Supplies Clean Heat for Oil Extraction</title><link>https://spectrum.ieee.org/thermal-battery-for-industrial-heat</link><author>Vanessa Bates Ramirez</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk3NTYwOS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgxNzkwOTE1Mn0.C8j4aP1FAM5nuet7Pq23kia5pTN-UXBXlcQnZAp7WJU/image.jpg?width=600" length="" type=""/><pubDate>Thu, 30 Oct 2025 12:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[System converts solar energy to heat for industrial processes]]></content:encoded></item><item><title>Ubuntu Announces Architecture Variants: Ubuntu 25.10 Gets x86_64-v3 Packages</title><link>https://www.phoronix.com/news/Ubuntu-Architecture-Variants</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 30 Oct 2025 10:11:46 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Canonical today announced an exciting step forward for Ubuntu Linux: the notion of architecture variants and now initially providing an Ubuntu 25.10 archive with x86_64-v3 built packages for enjoying better performance on modern Intel and AMD hardware...]]></content:encoded></item><item><title>Alphabet Tops $100 Billion Quarterly Revenue For First Time</title><link>https://tech.slashdot.org/story/25/10/30/0224216/alphabet-tops-100-billion-quarterly-revenue-for-first-time?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Alphabet reported its first-ever $100 billion quarter, fueled by a 34% surge in Google Cloud revenue and booming AI demand. The tech giant also announced an increase in expected capital expenditures for the fiscal year of 2025. CNBC reports: "With the growth across our business and demand from Cloud customers, we now expect 2025 capital expenditures to be in a range of $91 billion to $93 billion," the company said in its earnings report (PDF) Wednesday. "Looking out to 2026, we expect a significant increase in CapEx and will provide more detail on our fourth quarter earnings call," said finance chief Anat Ashkenazi on the earnings call with investors Wednesday.
 
Earlier this year, the company increased its capital expenditure expectation from $75 billion to $85 billion. Most of that goes toward technical infrastructure such as data centers. The latest earnings show the company is seeing rising demand for its AI services, which largely sit in its cloud unit. It also shows the company is continuing to spend more to try and build out more infrastructure to accomodate the backlog of customer requests. "We continue to drive strong growth in new businesses. Google Cloud accelerated, ending the quarter with $155 billion in backlog," CEO Sundar Pichai said in the earnings release.]]></content:encoded></item><item><title>AMD ROCm 7.1 Release Appears Imminent</title><link>https://www.phoronix.com/news/AMD-ROCm-7.1-Imminent</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 30 Oct 2025 09:55:05 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD continues with their aggressive efforts to enhance their GPU software compute ecosystem with ROCm. The fire under them has been lit and they have been taking their software efforts more expeditiously in recent times to better compete with NVIDIA's CUDA ecosystem and ensuring their Instinct hardware is properly primed to compete. The release dance has begun for ROCm 7.1...]]></content:encoded></item><item><title>Can Sparse Spectral Training Make AI More Accessible?</title><link>https://hackernoon.com/can-sparse-spectral-training-make-ai-more-accessible?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Thu, 30 Oct 2025 09:22:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Supplementary Information6 Conclusion and DiscussionIn this work, Sparse Spectral Training (SST) has demonstrated its efficacy as a resource-efficient training methodology that closely approximates the performance of full-rank training across diverse architectures, tasks and embedding geometries. SST introduces a noval approach by updating all singular values and selectively adjusting the singular vectors of network weights, optimizing resource utilization while closely mirroring the performance of full-rank training. Moreover, some areas that need further explorations are: (1) Investigating faster convergence approaches that avoid optimizer state reset (2) Extending the application of SST to the embeddings of large language models (LLMs).This research enhances the memory efficiency of training large language models (LLMs), which contributes positively by reducing the environmental impact and making LLM training accessible to researchers with limited resources. On the downside, the ease of access to powerful LLMs raises concerns about potential misuse [52, 53]. Careful consideration and management of these factors are essential to maximize the benefits and mitigate risks.[1] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.\
[3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\
[4] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.\
[5] Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky. ReloRA: High-rank training through low-rank updates. In The Twelfth International Conference on Learning Representations, 2024.\
[6] Wenhan Xia, Chengwei Qin, and Elad Hazan. Chain of lora: Efficient fine-tuning of language models via residual learning, 2024.\
[7] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023.\
[8] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Sparse low-rank adaptation of pre-trained language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.\
[9] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\
[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\
[11] Ines Chami, Zhitao Ying, Christopher Ré, and Jure Leskovec. Hyperbolic graph convolutional neural networks. Advances in neural information processing systems, 32, 2019.\
[12] Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fully hyperbolic neural networks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5672–5686, Dublin, Ireland, May 2022. Association for Computational Linguistics.\
[13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.\
[14] Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, and Lei Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023.\
[15] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameterefficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3274–3287, 2023.\
[16] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024.\
[17] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\
[18] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. arXiv:2103.10385, 2021.\
[19] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018.\
[20] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pages 2943–2952. PMLR, 2020.\
[21] Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate and fast memory-economic sparse training framework on the edge. Advances in Neural Information Processing Systems, 34:20838–20850, 2021.\
[22] Yingtao Zhang, Jialin Zhao, Wenjing Wu, Alessandro Muscoloni, and Carlo Vittorio Cannistraci. Epitopological learning and cannistraci-hebb network shape intelligence brain-inspired theory for ultra-sparse advantage in deep learning. In The Twelfth International Conference on Learning Representations, 2024.\
[23] Alessandro Muscoloni, Josephine Maria Thomas, Sara Ciucci, Ginestra Bianconi, and Carlo Vittorio Cannistraci. Machine learning meets complex networks via coalescent embedding in the hyperbolic space. Nature communications, 8(1):1615, 2017.\
[24] Carlo Vittorio Cannistraci and Alessandro Muscoloni. Geometrical congruence, greedy navigability and myopic transfer in complex networks and brain connectomes. Nature Communications, 13(1):7308, 2022.\
[25] Octavian Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\
[26] Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, and Nando de Freitas. Hyperbolic attention networks. In International Conference on Learning Representations, 2019.\
[27] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. Advances in neural information processing systems, 32, 2019.\
[28] Alexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. Poincare glove: Hyperbolic word embeddings. In International Conference on Learning Representations, 2019.\
[29] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218, 1936.\
[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. [31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\
[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. CoRR, abs/1912.01703, 2019.\
[33] Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, and Marcello Federico. Report on the 11th IWSLT evaluation campaign. In Marcello Federico, Sebastian Stüker, and François Yvon, editors, Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign, pages 2–17, Lake Tahoe, California, December 4-5 2014.\
[34] Mauro Cettolo, C. Girardi, and Marcello Federico. Wit3: Web inventory of transcribed and translated talks. Proceedings of EAMT, pages 261–268, 01 2012.\
[35] Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. Multi30k: Multilingual english-german image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 70–74. Association for Computational Linguistics, 2016.\
[36] Ryohei Shimizu, YUSUKE Mukuta, and Tatsuya Harada. Hyperbolic neural networks++. In International Conference on Learning Representations, 2021.\
[37] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\
[38] Hyunghoon Cho, Benjamin DeMeo, Jian Peng, and Bonnie Berger. Large-margin classification in hyperbolic space. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 1832–1840. PMLR, 16–18 Apr 2019.\
[39] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.\
[40] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.\
[41] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.\
[42] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.\
[43] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.\
[44] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839–849, San Diego, California, June 2016. Association for Computational Linguistics.\
[45] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\
[46] Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In 13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012, Proceedings of the International Conference on Knowledge Representation and Reasoning, pages 552–561. Institute of Electrical and Electronics Engineers Inc., 2012. 13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012 ; Conference date: 10-06-2012 Through 14-06-2012.\
[47] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.\
[48] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.\
[49] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93–93, 2008.\
[50] R.M. Anderson and R.M. May. Infectious Diseases of Humans: Dynamics and Control. Infectious Diseases of Humans: Dynamics and Control. OUP Oxford, 1991.\
[51] Galileo Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven active surveying for collective classification. 2012.\
[52] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610–623, 2021.\
[53] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\
[54] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. Opennmt: Open-source toolkit for neural machine translation. In Proc. ACL, 2017.\
[55] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022.(1) Jialin Zhao, Center for Complex Network Intelligence (CCNI), Tsinghua Laboratory of Brain and Intelligence (THBI) and Department of Computer Science;(2) Yingtao Zhang, Center for Complex Network Intelligence (CCNI), Tsinghua Laboratory of Brain and Intelligence (THBI) and Department of Computer Science;(3) Xinghang Li, Department of Computer Science;(4) Huaping Liu, Department of Computer Science;(5) Carlo Vittorio Cannistraci, Center for Complex Network Intelligence (CCNI), Tsinghua Laboratory of Brain and Intelligence (THBI), Department of Computer Science, and Department of Biomedical Engineering Tsinghua University, Beijing, China.:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>SST vs LoRA: A Leaner, Smarter Way to Train AI Models</title><link>https://hackernoon.com/sst-vs-lora-a-leaner-smarter-way-to-train-ai-models?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Thu, 30 Oct 2025 08:07:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Supplementary Information5.2 Natural Language GenerationWe utilize the OPT [9] architecture as the baseline for our language generation experiments. All models are pre-trained on OpenWebText [39], an open-source reproduction of OpenAI’s WebText. To facilitate fair comparisons across different OPT model sizes, we standardize the total training tokens for all models at 19.7 billion. A consistent rank (r = 64) is applied for all low-rank methods.\
Table 3 displays the validation perplexity results on the OpenWebText dataset across different sizes of OPT models. The results indicate that SST not only achieves lower perplexity scores compared to LoRA and ReLoRA* but also approximates the performance of full-rank training, with significantly fewer trainable parameters.\
Figure 2 illustrates a comparison of effective steps among various training methods. The effective step metric, which considers both the number of trainable parameters and the number of training steps, demonstrates that SST offers a more efficient training approach compared to the full-rank method.\
Each pretrained model undergoes zero-shot evaluations on all 16 NLP tasks used in OPT article [9], including ARC Easy and Challenge [40], HellaSwag [41], OpenBookQA [42], PIQA [43], StoryCloze [44], SuperGLUE [45], WinoGrad [46], and WinoGrande [47]. Evaluations are conducted using the LM Evaluation Harness framework [48]. Except for the ReCoRD task, which uses F1 score, all other tasks are evaluated using accuracy.\
Table 4 details the zero-shot evaluation results across the 16 NLP tasks. SST consistently performs comparably or better than other low-rank methods and shows competitive performance against the full-rank models.\
We further conduct an analysis experiment on inference by doing post-training singular value pruning on SST model (see appendix G).5.3 Hyperbolic Graph Neural NetworksHyperbolic Graph Neural Networks (HGNNs) [11, 12] capitalize on the expansive and hierarchical nature of hyperbolic space to efficiently manage and analyze graph-structured data. This geometric space is particularly suitable for graphs due to its ability to closely mimic the underlying data structures with minimal distortion, offering a substantial improvement over traditional Euclidean methods.\
\
We evaluated the effectiveness of SST on HyboNet [12] version HGNN in node classification and link prediction across four distinct datasets: Airport [11], Cora [49], Disease [50], and PubMed [51]. Each experiment was conducted with three random seeds.\
\
The results, detailed in Table 5, demonstrate strong performance in both node classification and link prediction tasks. SST not only shows comparable performance to full-rank training (exceeding it in the Disease link prediction task) but also significantly outperforms LoRA at equivalent ranks. Notably, SST’s advantage over LoRA is larger on r = 1 than r = 2, likely due to SST’s sampling strategy being particularly effective in sparser scenarios.(1) Jialin Zhao, Center for Complex Network Intelligence (CCNI), Tsinghua Laboratory of Brain and Intelligence (THBI) and Department of Computer Science;(2) Yingtao Zhang, Center for Complex Network Intelligence (CCNI), Tsinghua Laboratory of Brain and Intelligence (THBI) and Department of Computer Science;(3) Xinghang Li, Department of Computer Science;(4) Huaping Liu, Department of Computer Science;(5) Carlo Vittorio Cannistraci, Center for Complex Network Intelligence (CCNI), Tsinghua Laboratory of Brain and Intelligence (THBI), Department of Computer Science, and Department of Biomedical Engineering Tsinghua University, Beijing, China.:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Alien Worlds May Be Able To Make Their Own Water</title><link>https://science.slashdot.org/story/25/10/30/024222/alien-worlds-may-be-able-to-make-their-own-water?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[sciencehabit shares a report from Science.org: From enabling life as we know it to greasing the geological machinery of plate tectonics, water can have a huge influence on a planet's behavior. But how do planets get their water? An infant world might be bombarded by icy comets and waterlogged asteroids, for instance, or it could form far enough from its host star that water can precipitate as ice. However, certain exoplanets pose a puzzle to astronomers: alien worlds that closely orbit their scorching home stars yet somehow appear to hold significant amounts of water.
 
A new series of laboratory experiments, published today in Nature, has revealed a deceptively straightforward solution to this enigma: These planets make their own water. Using diamond anvils and pulsed lasers, researchers managed to re-create the intense temperatures and pressures present at the boundary between these planets' hydrogen atmospheres and molten rocky cores. Water emerged as the minerals cooked within the hydrogen soup. Because this kind of geologic cauldron could theoretically boil and bubble for billions of years, the mechanism could even give hellishly hot planets bodies of water -- implying that ocean worlds, and the potentially habitable ones among them, may be more common than scientists already thought. "They can basically be their own water engines," says Quentin Williams, an experimental geochemist at the University of California Santa Cruz who was not involved with the new work.]]></content:encoded></item><item><title>The TechBeat: The Physics of AI (10/30/2025)</title><link>https://hackernoon.com/10-30-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Thu, 30 Oct 2025 06:10:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @mend [ 4 Min read ] 
 Traditional testing breaks with AI. Learn how red teaming and AI-powered fuzzing uncover hidden weaknesses in large language models. Read More.By @knightbat2040 [ 5 Min read ] 
 What started as a simple script evolved into a full-fledged data engineering and NLP pipeline that can process a decade's worth of legal decisions in minutes. Read More.By @socialdiscoverygroup [ 6 Min read ] 
 Discover how React 19's new hooks—useActionState, useFormStatus, and useOptimistic—simplify form handling with less boilerplate and cleaner code.  Read More.By @hacker-Antho [ 4 Min read ] 
 New research shatters AI security assumptions, showing that poisoning large models is easier than believed and requires a very small number of documents. Read More.By @mayukhsuri [ 3 Min read ] 
 AWS outage on Oct 20, 2025, disrupted major apps worldwide. Learn what caused it, how it spread, and key lessons to build stronger cloud systems. Read More.By @botbeat [ 8 Min read ] 
 A deep dive into the 30 companies that burned over one trillion OpenAI tokens—featuring Duolingo, OpenRouter, and Indeed as top power users of GPT tech. Read More.By @hackmarketing [ 7 Min read ] 
 Learn how Web3 projects can grow sustainably through education, trust, and human-centered marketing that builds real users and community. Read More.By @ichebykin [ 5 Min read ] 
 Context engineering for coding agents is the best way to improve the model performance for code generation.  Read More.By @nownodes [ 4 Min read ] 
 Blast API ends operations in Oct 2025. Explore the best developer alternatives like NOWNodes and Alchemy for secure, scalable RPC migration. Read More.By @giovannicoletta [ 11 Min read ] 
 An interrogation of how physics concepts like black holes, entropy, and quantum theory mirror the rise and limits of artificial intelligence. Read More.By @mcsee [ 3 Min read ] 
 Avoid Boolean variables, they lead to conditional logic and force you to write Ifs. Create polymorphic states instead Read More.By @sanjaybarot [ 23 Min read ] 
 Ransomware has gone cloud-native: no payloads, just API abuse. Learn the tactics—IAM takeovers, KMS locks, backup sabotage—and how to build resilience. Read More.By @filestack [ 6 Min read ] 
 Stop babysitting profile pictures. Learn how Filestack Workflows turn image uploads into scalable, async, and lightning-fast experiences. Read More.By @ainativedev [ 7 Min read ] 
 Dive into a hands-on comparison of Cursor, Windsurf, and Copilot with GPT-5, highlighting their strengths in greenfield and brownfield projects. Read More.By @webism [ 5 Min read ] 
 OpenAI launches ChatGPT Atlas, an AI-powered browser with memory and agent mode. We gathered 33 reactions from skeptics, believers, and analysts. Read More.]]></content:encoded></item><item><title>How &quot;Diablo AI&quot; Will Destroy Your Marketing Budget and Business</title><link>https://hackernoon.com/how-diablo-ai-will-destroy-your-marketing-budget-and-business?source=rss</link><author>Maks Shev</author><category>tech</category><pubDate>Thu, 30 Oct 2025 06:09:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Over the past six months, I've been observing the same picture. Agencies and brands are massively switching to AI-generated creatives for performance campaigns. Everyone is convinced it's a breakthrough. But when you look at the numbers, you see the opposite. Conversions are dropping, cost per acquisition is growing, and the audience simply stops reacting to advertisingLast quarter I conducted an experiment for an e-commerce project, i launched two series of ads on Facebook. The first series was fully generated through ChatGPT following all optimization canons. the second was written by me manually, late in the evening, I even left a couple of typos intentionally. The result surprised me, although I was already starting to suspect this. Manual creatives gave 34 % better performance across all metrics..This isn't coincidence or statistical error. Basic neurophysiology of perception is at work here. The human brain is designed as a pattern recognition machine. There's research by Karl Friston on predictive processing theory. The essence is that the brain constantly tries to predict what will happen next to minimize surprise and conserve energy AI generated text has a very specific structure. It's mathematically precise, logically flawless, emotionally balanced. It's precisely this predictability that becomes the problem. The brain reads the pattern in fractions of a second and sends the information to the category "already seen not important, skip." In neuroscience, this is called habituation.When a human writes text they inevitably deviate from the ideal line. Somewhere they go off on a tangent, somewhere they return to the thought from a different angle, sometimes they use An unexpected metaphor. This creates what's called prediction error. The brain can't predict the next step, so it's forced to pay attention. There's a whole research direction at MIT Media Lab about so called "honest signals" in  communication. It turns out people subconsciously pick up micro patterns of authenticity. A real person can contradict themselves in different parts of the text. Can accidentally show uncertainty or excessive enthusiasm. AI maintains perfect balance always, and it's exactly this that's perceived as unnatural.Remember Masahiro Moris "uncanny valley" concept? When a robot becomes too similar to a human but something in it remains wrong, it causes subconscious rejection. The same mechanism works with texts. AI copywriting falls exactly into this valley. Good enough to look almost human, but the brain still senses something's off.I tested this on a LinkedIn campaign for a b2b client. Two ad variants with identical budget and targeting. The first generated by AI "Optimize workflows with our innovative platform. Over 10000 teams have already increased productivity." Standard, clean, professional text..The second I wrote myself "Our interface honestly won't win beauty contests. But it will save you four hours a week, which is why even design studios buy it" The second variant gave 63 % higher ctr and 41 % lower cost per lead. The reason is simple. Acknowledging imperfection works as a credibility signal. It proves text was written by a living person who actually stands behind the product and is willing to speak honestly. Over the past year, I analyzed data from 47 campaigns with a total budget of about a million dollars. I compared fully manual creatives with those where AI was used. The pattern consistently repeats. Ugc format on Instagram and Tik Tok. Polished AI versions show 0.8% ctr at $4.20 cost per action. Shot on phone and edited manually give 2.1 % ctr at $1.90 per action. Email marketing. AI written subject lines get 18 % opens. Manually written ones, including those with minor errors, get 31 % opens. Landing page headlines. Ai versions convert 2.3 % of visitors. Manual versions convert 4.1 %.This is explained by basic behavioral economics. People evaluate not the object itself, but the perceived investment in it. A handwritten letter is more valuable than a printed card not by content, but because someone spent time. A custom illustration beats a stock image for the same reason. Right now we're observing an interesting scarcity effect.. When most content is AI-generated, manual work automatically becomes premium. It's similar to how the word "organic" worked in food products ten years ago. A quality certificate that attracts attention.Your audience doesn't consciously analyze "did AI or a human write this." They just feel at an intuitive level that something's off. Research in neuromarketing shows that purchase decisions are made by emotional brain centers, and rational justification comes later. If a creative feels like a mass template, the limbic system rejects it before the person has consciously read your value proposition. I'm not calling to abandon AI completely. The question is understanding where manual work gives disproportionately high returns.First touch with the audience is critically important. UGC-style creatives, provocative hooks, visuals that stop scrolling. Here you need human unpredictability. AI averages by definition, and you need to stand out. Brand voice formation can't be automated. Ai is trained to be safe and neutral. But it's precisely sharp edges that create memorability and emotional connection. Key conversion points require maximum attention. Text on the payment page, main headline of a sales landing page, emails for warmed-up audience. It's exactly here that "roboticism" costs you actual money. Controversial or provocative messages by definition can't be generated by AI. The system is trained to avoid risks, but it's precisely risk that attracts attention and provokes action.At the same time, AI handles scaling tasks excellently. Need to create 50 variations of one creative for split tests? No problem. Supporting content like FAQ, instructions, SEO articles? Please. Initial drafts that are then refined manually? Very effective. Analysis of large data sets on campaign performance? Irreplaceable. The problem is that most specialists do exactly the opposite. They automate what requires humanity and spend time manually refining secondary content.This year I changed my approach to creating creatives. I deliberately leave elements of imperfection. In video ads, I dont cut out all pauses and stumbles .. they create a sense of naturalness. In email newsletters, I don't fix every minor error if it doesn't interfere with understanding. I use smartphone shooting instead of studio producton for UGC style content. On landing pages, we started indicating authorship: "Text written by Ivan Petrov, without using AI." In email signatures, we add "I really write these emails myself." In creatives, we show the creator, use hand-drawn elements instead of perfect graphics.I completely revised time allocation. 80 % goes to 20 % of content that actually impacts conversion. The remaining 20 % of time on mass content with AI help.. Every A/B test now includes a hypothesis about "deliberate imperfection." Results often surprise. A version with a typo can beat a cleaned up one. Conversational style with digressions outperforms structured copywriting.While everyone's chasing automation, an interesting opportunity opens up. Being real becomes expensive, which means valuable. This is basic scarcity economics. The brands that will win competition in the next couple of years aren't those with the most advanced AI tools. They're those who understood where to direct limited human attention for maximum effect. Your audience isn't just consuming content. They're looking for trust signals. And right now the strongest trust signal is proof that a person cared. That they created something specifically for them, not just generated from a template.That same typo in the email subject. An unexpected metaphor in an ad. A video where you lost your train of thought and came back to it differently. This isn't unprofessionalism, it’s a competitive advantage in a world where everyone else sounds the same. AI is indeed a powerful tool for scaling. But scale without authenticity is just expensive noise. In performance marketing, only one metric matters: conversion. And people convert people more effectively than algorithms.The paradox is that the more you optimize a creative with AI, the worse it works in practice.Information about the research kindly provided by our beloved AI. Use the tools correctly and have a good day :)]]></content:encoded></item><item><title>JSON Was Killing Our Redis Memory. Switching Serialization Made It 7× Smaller.</title><link>https://hackernoon.com/json-was-killing-our-redis-memory-switching-serialization-made-it-7-smaller?source=rss</link><author>Yan Khachko</author><category>tech</category><pubDate>Thu, 30 Oct 2025 06:08:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We were running a large production service with about 10 million monthly active users, and Redis acted as the main storage for user state. Every record in Redis was a JSON-serialized Pydantic model. It looked clean and convenient – until it started to hurt.As we grew, our cluster scaled to , yet memory pressure only kept getting worse. JSON objects were inflating far beyond the size of the actual data, and we were literally paying for  – in cloud invoices, wasted RAM, and degraded performance.At some point I calculated the ratio of real payload to total storage, and the result made it obvious that we couldn’t continue like this:14,000 bytes per user in JSON → 2,000 bytes in a binary format
A  Just because of the serialization format.That’s when I built what eventually became  – a compact binary encoder/decoder for Pydantic models. And below is the story of how I got there, what didn’t work, and why the final approach made Redis (and our wallets) a lot happier.Why JSON Became a ProblemJSON is great as a universal exchange format. But inside a low-level cache, it turns into a :it stores field names in fullit stores types implicitly as stringsit duplicates structure over and overit’s not optimized for binary datait inflates RAM usage to  the size of the real payloadWhen you’re holding tens of millions of objects in Redis, this isn’t some academic inefficiency anymore – it’s a real bill and an extra server in the cluster. At scale, JSON stops being a harmless convenience and becomes a silent tax on memory.What Alternatives Exist (and Why They Didn’t Work)I went through the obvious candidates:| Format | Why It Failed in Our Case |
|----|----|
|  | Too much ceremony: separate schemas, code generation, extra tooling, and a lot of friction for simple models |
|  | More compact than JSON, but still not enough – and integrating it cleanly with Pydantic was far from seamless |
|  | Smaller than JSON, but the Pydantic integration story was still clumsy and not worth the hassle |All of these formats are good in general. But for the specific scenario of “Pydantic + Redis as a state store” they felt like using a sledgehammer to crack a nut – heavy, noisy, and with barely any real relief in memory usage.I needed a solution that would:drop into the existing codebase with just a couple of linesdeliver a radical reduction in memory usageavoid any extra DSLs, schemas, or code generationwork directly with Pydantic models without breaking the ecosystemSo I ended up writing a minimalist binary format with a lightweight encoder/decoder on top of annotated Pydantic models. That’s how  was born.Its API is intentionally designed so that you can drop it in with almost no friction — in most cases, you just replace calls like:model.serialize()        # replaces .model_dump_json()
Model.deserialize(bytes) # replaces .model_validate_json()
from pybyntic import AnnotatedBaseModel
from pybyntic.types import UInt32, String, Bool
from typing import Annotated

class User(AnnotatedBaseModel):
    user_id:   Annotated[int, UInt32]
    username:  Annotated[str, String]
    is_active: Annotated[bool, Bool]

data = User(
    user_id=123,
    username="alice",
    is_active=True
)

raw = data.serialize()
obj = User.deserialize(raw)
Optionally, you can also provide a custom compression function:import zlib

serialized = user.serialize(encoder=zlib.compress)

deserialized_user = User.deserialize(serialized, decoder=zlib.decompress)
For a fair comparison, I generated  based on our real production models. Each user object contained a mix of fields – , , , , , , , and . On top of that, every user also had  such as roles and permissions, and in some cases there could be hundreds of permissions per user. In other words, this was not a synthetic toy example — it was a realistic dataset with deeply nested structures and a wide range of field types.The chart shows how much memory Redis consumes when storing  using different serialization formats. JSON is used as the baseline at approximately . PyByntic turned out to be the most compact option — just , which is about . Protobuf and MessagePack also offer a noticeable improvement over JSON, but in absolute numbers they still fall far behind PyByntic.Let's compare what this means for your cloud bill:| Format | Price of Redis on GCP |
|----|----|
| JSON | $876/month |
|  |  |
| MessagePack | $380/month |
| BSON | $522/month |
| Protobuf | $187/month |This calculation is based on storing 2,000,000 user objects using Memorystore for Redis Cluster on Google Cloud Platform. The savings are significant – and they scale even further as your load grows.The huge memory savings come from two simple facts: binary data doesn’t need a text format, and it doesn’t repeat structure on every object. In JSON, a typical datetime is stored as a string like "1970-01-01T00:00:01.000000" – that’s , and since each ASCII character is , a single timestamp costs . In binary, a  takes just , making it  with zero formatting overhead.The same applies to numbers. For example,  () in JSON takes , while the binary representation is a fixed . And finally, JSON keeps repeating field names for every single object, thousands or millions of times. A binary format doesn’t need that — the schema is known in advance, so there’s no structural tax on every record.Those three effects – no strings, no repetition, and no formatting overhead – are exactly where the size reduction comes from.If you’re using Pydantic and storing state in Redis, then JSON is a luxury you pay a  for. A binary format that stays compatible with your existing models is simply a more rational choice.For us,  became exactly that — a  that didn’t break anything, but eliminated an entire class of problems and unnecessary overhead.]]></content:encoded></item><item><title>The Strategic Advantage: How AI QA Engineers Cut Costs and Speed Up Time-to-Market</title><link>https://hackernoon.com/the-strategic-advantage-how-ai-qa-engineers-cut-costs-and-speed-up-time-to-market?source=rss</link><author>Arnab Chatterjee</author><category>tech</category><pubDate>Thu, 30 Oct 2025 06:00:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every engineering team has lived through it,  the red build that turns green on rerun, the test that “just fails sometimes,” and the creeping loss of trust in automation. Flaky tests feel small at first, but their collective cost is high. They silently inflate Change Failure Rate (CFR), slow releases, and drain hours in CI time that could’ve gone into real product work.That’s why the shift toward AI-generated, self-healing test flows and disciplined quarantine practices is becoming more than a convenience, it’s strategic. Done right, this approach doesn’t replace QA; it strengthens engineering feedback loops, trims false failures, and restores confidence in test signals.Why Flakes Hurt More Than You ThinkWhat a “flaky test” actually is (and isn’t)In academic and industrial literature, a flaky test is defined as “a test that passes and fails under the same conditions, without any code change.”It’s not a slow test, a wrong test, or an unstable environment, it’s a  that makes teams doubt every other one.That definition matters. Without clarity, teams end up masking real issues with retries or marking legitimate defects as “flake.” Policies like  or  only make sense when everyone agrees on what a flake actually is.Every false red triggers a rerun. Every rerun adds minutes. And every minute multiplies across developers and builds. Eventually, flaky tests stop being a testing issue and become a pipeline-throughput problem.Under the DORA framework, these inefficiencies hit two key metrics: (how quickly code moves from commit to deploy)Change failure rate (CFR) (how often a change causes a failure that needs fixing)Flakes inflate both. When you can’t trust the red, developers hesitate to merge. Some rerun; others skip validation altogether. Either way the confidence erodes and velocity slows down.Recent large-scale studies underline this:Multi-project academic reviews (White Rose Research Online; ACM Digital Library) noted a strong correlation between resource constraints and flake density, the busier the pipelines, the higher the flakiness.16–25% of tests in large-scale CI systems show intermittent behavior.Some remain quarantined for , creating “dead weight” suites that still consume compute.Teams report spending 10–20% of their CI minutes re-running or verifying suspected flakes.The takeaway: flaky tests aren’t just noise; they’re a  on delivery speed.Measuring the Drag: From Pipeline Pain to Business ImpactQuantifying the cost brings clarity and urgency.You only need four numbers: (% of CI runs failing due to flakes)Number of developers affectedWasted CI hours (per week) =
flake_rate × reruns_per_flake × job_time_hours × developers × jobs_per_dev_per_week
Example A (conservative): 5% flake rate × 1 rerun × 0.33 h (20 min) × 15 devs × 25 jobs/dev/week ≈  lost. 5% × 1 × 0.33 h × 15 devs × 100 jobs/dev/week ≈  lost.If your CI has a 5 % flake rate, each failed job takes 20 minutes to rerun, and 15 developers are running jobs daily so you’re losing roughly 25 hours of productive time per week for re-runs.Signs that your suite is showing :A test passes and fails under the same SHARetry counts climbing in CIVariance in execution time across identical runsMismatched artifacts or screenshots between “fail” and “pass” statesPractitioners on DEV Community emphasize the same: if you can reproduce a failure inconsistently, you’re not debugging the app, you’re debugging the test.Tie it to DORA and CFR explicitlyThe   Deployment Frequency, Lead Time for Changes, Change Failure Rate, and Mean Time to Restore. These are now industry-standard signals of delivery health (dora.dev).Flaky tests distort two of them: repeated runs delay usable feedback. false failures inflate “changes that fail,” even when nothing is wrong.Teams that invested in  and  consistently report sharper signal quality. faster time to first useful fail, and fewer spurious rollbacks. In one internal AI QA-assisted pilot, stabilizing critical test flows cut average “time-to-green” by over 40% without increasing suite size.Why it matters beyond engineeringEach false red doesn’t just waste CI minutes, it delays value delivery.When real bugs slip through or releases stall, the ripple reaches customers. PwC’s 2024 Customer Experience survey found that 32% of users would abandon a brand after a single bad experience.That turns test stability into a . Every noisy test not only burns time, it risks trust as well.What Actually Works: AI-Generated, Self-Healing Flows (+ Human Guardrails)After you’ve measured the drag and accepted that flakiness is a system cost, the question becomes: what actually fixes it without slowing development down?AI QA doesn’t fit as a magic button, but as a loop combining discovery, self-healing, and human oversight.Where AI fits in the loop1. Autonomous flow discoveryModern QA teams spend weeks writing end-to-end scripts for flows that users may never trigger again. AI shortens that loop by learning from analytics and usage to map real critical paths like checkout flows, signup journeys, or dashboard actions that truly matter.Instead of guessing which flows need coverage, the system starts with what customers actually do, ensuring tests align with business value.2. Selector robustness & self-healingDOMs shift. Classes change. Async waits stretch by milliseconds. Traditional scripts snap under those changes.An AI-based test agent continuously monitors DOM mutations and timing patterns, then auto-repairs selectors when they drift. This means your tests evolve with the product, not against it.Platforms like  use a similar principle, dynamically adapting selectors and synchronization waits so that non-critical UI shifts don’t trigger false reds. It’s not about skipping validation, it’s about maintaining determinism when change is expected.3. Daily human QA review (the hybrid discipline)No AI model should act unchecked in CI. The best setups combine  and a “human-in-the-loop” process.QA engineers review generated flows, confirm that repaired selectors still reflect user intent, and quarantine any borderline cases.This human guardrail keeps the test corpus trustworthy while letting AI handle the mechanical grind.The quarantine disciplineEven with AI help, flake prevention needs process. The industry-standard playbook is simple but strict:Fail → Reproduce? → Quarantine → Fix data/selector → Return to suite
this approach isolates noise before it pollutes the main signal.The target benchmark most mature teams aim for:Anything beyond that, and your CI metrics start lying. Quarantine isn’t punishment, it’s the mechanism that keeps your CFR honest.Picture the feedback loop as a swimlane:Developer → CI → AI Agent → CI → Dev/QA → Quarantine → MergeA developer commits code.CI triggers the AI agent to generate or repair relevant test flows.The AI layer stabilizes selectors and waits, executes the run, and posts only  back to CI.Dev/QA triage those signals, fixing actual regressions or isolating confirmed flakes.Clean tests merge back; noisy ones go to quarantine for review.The result: the pipeline stays green for the right reasons.Instead of blocking merges on a noisy suite, advanced setups use AI-test confidence scores to gate only high-risk changes.A deterministic fail halts a merge; a quarantined or low-confidence fail flags review but doesn’t stop delivery.That balance of  is what turns QA from a bottleneck into an early-warning system.Every stable CI system you’ve ever admired started small. The trick isn’t to automate everything on day one, it’s to create a feedback loop that Below is a simple two-sprint plan any engineering team can run without disrupting releases.Sprint 0 : Prep and BaselineBefore touching any AI or automation, you need to measure the . Treat this sprint as your “before” snapshot.1. Instrument your CI metrics (percentage of runs that fail inconsistently)Time-to-first-useful-signal (commit → first deterministic fail)Change Failure Rate (CFR) and Mean Time to Restore (MTTR)If you already use DORA’s “four keys,” this will feel familiar. You’re essentially setting up your QA metrics to speak the same language as your delivery metrics.2. Label and isolate recurring flakesRun a week of builds, tag recurring tests that fail intermittently, and classify causes (data, timing, selector). This is your “flake map.”3. Choose the pilot surfaceSelect  that truly affect customers, not obscure edge cases. Checkout, onboarding, or billing are good starting points.These flows should already have partial test coverage and predictable test data.4. Set the success criteria upfront“Flake rate reduced below 3%”“Time-to-green < 15 minutes”“Zero increase in CFR during rollout”This gives you measurable proof later that your changes improved signal quality, not just added complexity.Sprint 1 : Pilot: AI + QuarantineWith your baseline in hand, introduce the AI layer  your existing suite.1. Parallelize, don’t replaceRun AI-generated tests in parallel with your traditional scripts. The goal is comparison, not replacement. You want to see whether the AI maintains determinism across multiple runs.2. Enable selective self-healingAllow the AI to repair selectors and waits only for designated flows. Keep logs of each repair so that QA can audit the reasoning.In internal Bug0-assisted runs, this controlled rollout is where signal stability jumps first, because you’re no longer debugging minor UI drifts.Fail → Reproduce? → Quarantine → Fix → Return
Quarantined tests should be tracked in a lightweight dashboard (a spreadsheet works fine). The key is visibility, developers need to see which tests are “pending trust.”Lead time (commit → green build)Flake rate trend over daysNumber of deterministic vs. indeterminate failsYou’re not optimizing for volume yet, just .Sprint 2 : Rollout and ScaleOnce the pilot proves stable, expand to cover the next tier of flows.Add new flows incrementally: dashboards, search, file uploads or anything with high user traffic or frequent UI changes.Use the pilot’s AI config as your template for retries, selectors, and async handling.2. Integrate AI signal into merge gatesShift your CI gating logic:Deterministic fail → Block mergeLow-confidence / quarantined fail → Flag for reviewThis approach ensures CFR reflects genuine issues, not noise from uncertain tests.3. Automate selector validationSchedule nightly runs where the AI re-verifies repaired selectors against current builds. This “selector drift audit” keeps your automation future-proof.4. Expand reporting to DORA dashboardConnect your QA metrics to whatever platform visualizes your DORA keys.When leadership sees  and , the ROI conversation becomes straightforward.5. Continuous human reviewEven after automation stabilizes, maintain a small QA checkpoint each sprint, five minutes daily to review new auto-repairs or quarantines.That’s what keeps AI QA from drifting into “black box” territory.You don’t fix flakiness by throwing more tests at it. You fix it by shortening the feedback loop and increasing the reliability of every red signal.Two sprints of deliberate setup and review can turn a noisy, reactive pipeline into one that engineers actually trust.When AI helps shoulder the maintenance and humans keep the compass straight. Test automation stops being busywork and becomes an accelerator.Flaky tests aren’t just a testing nuisance, they’re an organizational drag. They inflate Change Failure Rate, erode confidence, and blur the line between “real failure” and “random noise.” The combination of AI-driven, self-healing tests and disciplined quarantine practices offers a practical path out.By grounding test automation in DORA metrics and business outcomes, teams can finally quantify what stability is worth :  faster releases, lower rework, and higher customer trust.And while AI plays a growing role, the teams that win are those that balance automation with human judgment.In the end, cleaner signals lead to calmer engineers and calmer engineers ship faster.Chrome & industry findings:Real-world studies on flake lifetime, resource constraints, and prioritization impacts.Practitioner cost & mechanics:Field-tested approaches to managing flaky suites and CI drag.The four key DevOps metrics that link speed and stability.]]></content:encoded></item><item><title>Challenges in Building Natural, Low‑Latency, Reliable Voice Assistants</title><link>https://hackernoon.com/challenges-in-building-natural-lowlatency-reliable-voice-assistants?source=rss</link><author>Surya Appini</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:58:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Voice is the most helpful interface when your hands and eyes are busy, and the least forgiving when it lags or mishears. This article focuses on the real‑world blockers that make assistants feel robotic, how to measure them, and the engineering patterns that make voice interactions feel like a conversation.Humans process and respond in ~200–300 ms. Anything slower feels laggy or robotic. Meanwhile, real‑world audio is messy: echo-prone kitchens, car cabins at 70 mph, roommates talking over you, code‑switching (“Set an alarm at saat baje”). To feel natural, a voice system must:: Far‑field capture, beamforming, echo cancellation, and noise suppression feeding streaming automatic speech recognition (ASR) with strong diarization and voice activity detection (VAD).: Incremental natural language understanding (NLU) that updates intent as transcripts stream; support disfluencies, partial words, and barge‑in corrections.Respond without awkward pauses: Streaming text-to-speech (TTS) with low prosody jitter and smart endpointing so replies start  the user finishes.: Repair strategies (“Did you mean…?”), confirmations for destructive actions, and short‑term memory for context.: Begin speaking ~150–250 ms after the user stops, at p95,  keep p99 under control with pre‑warm and shedding.: Let users cut in anytime; pause TTS, checkpoint state, resume or revise mid‑utterance.: Offer top‑K clarifications and slot‑level fixes so users don’t repeat the whole request.: Keep working (alarms, timers, local media, cached facts) when connectivity blips; reconcile on resume.Stay consistent across contexts: Handle rooms, cars, TV bleed, and multiple speakers with diarization and echo references.Core challenges (and how to tackle them)Designing Voice‑Only Interaction and Turn‑Taking Most real use happens when your hands and eyes are busy, cooking, driving, working out. If the assistant doesn’t know when to speak or listen, it feels awkward fast. The assistant starts talking right as you finish, uses tiny earcons/short lead‑ins instead of long preambles, and remembers quick references like “that one.” Think of the conversation as a simple state machine that supports overlapping turns. Tune endpointing and prosody so the assistant starts speaking as the user yields the floor, and keep a small working memory for references and quick repairs (for example, “actually, 7 not 11”)., . A/B prosody and earcons.Achieving Ultra‑Low Latency for Real‑Time Interaction Humans expect a reply within ~300 ms. Anything slower feels like talking to a call center Interactive Voice Response (IVR). You stop, it speaks, consistently. p95 end‑of‑speech to first‑audio ≤ 300 ms; p99 doesn’t spike. Set a latency budget for each hop (device → edge → cloud). Stream the pipeline end to end: automatic speech recognition (ASR) partials feed incremental natural language understanding (NLU), which starts streaming text‑to‑speech (TTS). Detect the end of speech early and allow late revisions. Keep the first hop on the device, speculate likely tool or large language model (LLM) results, cache aggressively, and reserve graphics processing unit (GPU) capacity for short jobs.end‑of‑speech to first‑audio p95/p99. Pre‑warm hot paths; shed non‑critical work under load.Keeping Responses Short and Relevant Rambling answers tank trust, and make users reach for their phone. One‑breath answers by default; details only when asked (“tell me more”). Set clear limits on text‑to‑speech (TTS) length and speaking rate, and summarize tool outputs before speaking. Use a dialog policy that delivers the answer first and only adds context when requested, with an explicit “tell me more” path for deeper detail.,  (how often users say “what?”).Handling Interruptions and Barge‑In People change their minds mid‑sentence. If the assistant cannot stop and pivot gracefully, the conversation breaks. You interrupt and it immediately pauses, preserves context, and continues correctly. It never confuses its own voice for yours. Make text‑to‑speech (TTS) fully interruptible. Maintain an echo reference so automatic speech recognition (ASR) ignores the assistant’s audio. Provide slot‑level repair turns, and ask for confirmation only when the action is risky or confidence is low. Offer clear top‑K clarifications (for example, Alex versus Alexa). and , tested on noisy, real‑room audio.Filtering Background and Non‑Directed Speech Living rooms have televisions, kitchens have clatter, and offices have coworkers. False accepts are frustrating and feel invasive. It wakes for you—not for the television—and it ignores side chatter and off‑policy requests. Combine voice activity detection (VAD), speaker diarization, and the wake word, tuned per room profile. Use an echo reference from device playback. Add intent gating to reject low‑entropy, non‑directed speech. Keep privacy‑first defaults: on‑device hotword detection, ephemeral transcripts, and clear indicators when audio leaves the device. and Non‑directed speech rejection, sliced by room and device.Ensuring Reliability with Intermittent Connectivity Networks fail—elevators, tunnels, and congested Wi‑Fi happen. The assistant still needs to help. Timers fire, music pauses, and quick facts work offline. When the connection returns, longer tasks resume without losing state. Provide offline fallbacks (alarms, timers, local media, cached retrieval‑augmented generation facts). Use jitter buffers, forward error correction (FEC), retry budgets, and circuit breakers for tools. Persist short‑term dialog state so interactions resume cleanly.Degraded‑mode success rate and .Managing Power Consumption and Battery Life On wearables, the best feature is a battery that lasts. Without power, there is no assistant. All‑day standby, a responsive first hop, and no surprise drains. Keep the first hop on the device with duty‑cycled microphones. Use frame‑skipping encoders and context‑aware neural codecs. Batch background synchronization, cache embeddings locally, and keep large models off critical cores.Milliwatts (mW) per active minute, Watt‑hours (Wh) per successful task, and .Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU): Track Word Error Rate (WER) by domain, accent, noise condition, and device, along with intent and slot F1.  Mishears drive task failure;  use human‑labeled golden sets and shadow traffic; alert on regressions greater than X percent in any stratum.: end‑of‑speech to first‑audio (p50/p95/p99), Turn Overlap (starts within 150–250 ms), Barge‑in reaction time.  perceived snappiness;  p95 ≤ 300 ms; page when p99 or overlap drifts.: Task Success,  (saves after correction),  (offline/limited).  business impact;  break out by domain/device and set minimum bars per domain. Average spoken duration,  ("what?"), dissatisfaction (DSAT) taxonomy.  cognitive load;  median under one breath; review top DSAT categories weekly. milliwatts per active minute, watt‑hours per task, and standby drain per day.  wearables user experience (UX);  budget per device class and trigger power sweeps on regressions.: Slice by device/locale/context; annotate deploy IDs; pair time‑series with a fixed  for regression checks.Architectural blueprint (reference)Fallback & resilience flowThe breakthrough isn’t a bigger model; it’s a tighter . Natural voice assistants emerge when capture, ASR, NLU, policy, tools, and TTS are engineered to stream together, fail gracefully, and respect ruthless latency budgets. Nail that, and the assistant stops feeling like an app and starts feeling like a conversation.]]></content:encoded></item><item><title>Voice Assistants: Past, Present, Future</title><link>https://hackernoon.com/voice-assistants-past-present-future?source=rss</link><author>Surya Appini</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:58:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Voice assistants used to be simple timer and weather helpers. Today they plan trips, read docs, and control your home. Tomorrow they will see the world, reason about it, and take safe actions. Here’s a quick tour.Quick primer: types of voice assistantsHere’s a simple way to think about voice assistants. Ask four questions, then you can place almost any system on the map. General helpers for everyday tasks, or purpose built bots for support lines, cars, and hotels. Cloud only, fully on device, or a hybrid that splits work across both. One shot commands, back and forth task completion, or agentic assistants that plan steps and call tools. Voice only, voice with a screen, or multimodal systems that combine voice with vision and direct device control.We’ll use this simple map as we walk through the generations.Generation 1 - Voice Assistant Pipeline Era (Past)Think classic ASR glued to rules. You say something, the system finds speech, converts it to text, parses intent with templates, hits a hard‑coded action, then speaks back. It worked, but it was brittle and every module could fail on its own. GMM/HMM to DNN/HMM, then CTC and RNN‑T for streaming. Plus the plumbing that matters in practice: wake word, VAD, beam search, punctuation. Rules and regex to statistical classifiers, then neural encoders that tolerate paraphrases. Entity resolution maps names to real contacts, products, and calendars. Finite‑state flows to frame‑based, then simple learned policies. Barge‑in so users can interrupt. Concatenative to parametric to neural vocoders. Natural prosody, with a constant speed vs realism tradeoff.How teams trained and served itNarrow intent sets. Anything off the happy path failed.ASR → NLU → Dialog error cascades derailed turns.Multiple services added hops and serialization, raising latency.Personalization and context lived in silos, rarely end to end.Multilingual and far‑field audio pushed complexity and error rates up.Great for timers and weather, weak for multi‑step tasks.The center of gravity moved to large language models with strong speech frontends. Assistants now understand messy language, plan steps, call tools and APIs, and ground answers using your docs or knowledge bases. picks the right API at the right time. grabs fresh, relevant context so answers are grounded. stream ASR and TTS, prewarm tools, strict timeouts, sane fallbacks. unified home standards cut brittle adapters.Long‑running and multi‑session tasks.Guaranteed correctness and traceability.Private on‑device operation for sensitive data.Cost and throughput at scale.Generation 3 - Multimodal, Agentic Voice Assistants for Robotics (Future)Next up: assistants that can see, reason, and act. Vision‑language‑action models fuse perception with planning and control. The goal is a single agent that understands a scene, checks safety, and executes steps on devices and robots. fuse vision and audio with language for real‑world grounding. reusable controllers for grasp, navigate, and UI/device control. simulate, check policies, then act. run core understanding on device, offload selectively. warehouses, hospitality, healthcare, and prosumer robotics. Also smarter homes that actually follow through on tasks instead of just answering questions.Closing: the road to JarvisJarvis isn’t only a brilliant voice. It is grounded perception, reliable tool use, and safe action across digital and physical spaces. We already have fast ASR, natural TTS, LLM planning, retrieval for facts, and growing device standards. What’s left is serious work on safety, evaluation, and low‑latency orchestration that scales.Practical mindset: build assistants that do small things flawlessly, then chain them. Keep humans in the loop where stakes are high. Make privacy the default, not an afterthought. Do that, and a Jarvis‑class assistant driving a humanoid robot goes from sci‑fi to a routine launch.]]></content:encoded></item><item><title>5 Ways Async Work Builds a More Flexible and Inclusive Workplace</title><link>https://hackernoon.com/5-ways-async-work-builds-a-more-flexible-and-inclusive-workplace?source=rss</link><author>Elena Skvortsova</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:54:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Return-to-office rates have steadied, yet flexibility still ranks in the top three reasons people switch jobs, right beside pay and growth, according to recent McKinsey research. Another study also shows that employees thrive when they are allowed to work from home.Many frame flexibility around where we sit or when we log on. Besides the hours and location, the missing piece is how we collaborate. That is where asynchronous (async) work, or the practice of working where communication is not expected to be immediate, changes the game.Here are five ways async work is helping us build a more flexible, inclusive, human-first workplace for our global team – and how it could do the same for yours.1. Making communication more equitableTraditional meetings often reward speed over substance. Researchers call this the “extrovert bias” – fast or forceful speakers shape decisions while quieter colleagues struggle to break in. That bias hurts innovation by filtering out slower-burn ideas.Async work flips the dynamic:Written updates replace live monologues. Everyone drafts thoughts at their own pace and edits before posting.Recorded demos pause for reflection. Teammates respond with time-stamped comments, not knee-jerk reactions.Inclusive dialogue widens the talent pool. A study found that asynchronous communication boosted participation for introverts and non-native speakers across distributed teams.When voices rise by merit, not volume, better ideas surface.2. Enabling real scheduling flexibilityNine-to-five suits payroll software, not people’s lives. McKinsey data show workers prize flexible hours and greater control over their time as much as location choice.Async workflows replace fixed shifts with outcome-based expectations:Parents can handle the morning school run and log in afterward.Caregivers pause to attend appointments without losing pay.Employees managing chronic conditions arrange tasks around energy spikes – a benefit disability consultant at CMA highlights as “leveling the playing field”.Because deliverables, not desk time, measure success, everyone gains autonomy to plan work when they feel sharpest. That freedom encourages accountability – nobody hides behind a “busy” status light.3. Supporting cognitive diversityBrains process information in wildly different ways: some spark in rapid white-board sessions; others need quiet thinking blocks. Async practices nurture both. Team members bundle tasks into long, uninterrupted stretches or short, focused bursts.Reduced sensory overload. Remote.com’s 2025 insight report notes that many neurodivergent professionals thrive when they can curate their environment and limit spontaneous calls. A Fortune study found 59 percent of office workers cannot stay focused for 30 minutes in a typical setting – endless pings pull them off track.By lowering noise – literal and figurative – async structures let every cognitive style hit its stride.4. Protecting work-life balanceAlways-on culture breeds burnout. The expectation of immediate replies blurs boundaries until evenings looks like extended afternoons.Async norms restore balance through clear response windows and documented workflows. Teams using async methods report lower stress because they control when they engage instead of reacting on command. This flexibility helps address a root cause of burnout.Some principles we adopted at Muse Group are:Giving teammates up to 72 hours to respond thoughtfully;Encouraging written communication – team decisions live in a decision log and many of those advance in writing without a single meeting;Setting clear accountability structures and a culture of action – at the team’s chosen pace, but within agreed SLAs.When people own their clocks, recovery and deep focus can coexist.5. Giving creative minds space to thriveCreative work – coding a new audio engine or arranging a jazz chart – demands deep focus that constant notifications typically shatter. Harvard Business Review found that asynchronous teams produced more original solutions because contributors iterated privately before sharing polished ideas.Async work protects the “flow state”. A concept taken from positive psychology, ‘flow’ is a mental state in which a person is fully immersed in the activity, feeling energized and focused. We make every Wednesday a no-meeting day to help our team reach this state. As a result, 88 percent of our surveyed employees say they can concentrate without distraction, 79 percent find it easier to tackle complex projects, and 62 percent report higher-quality output.The outcome is fewer meetings and richer deliverables – because creators spend more time creating.Asynchronous work is not a silver bullet, yet it shifts focus from presence to performance, unlocking benefits that standard hybrid schedules miss: – written forums dampen extrovert bias. – individuals sync work with life, not vice versa. – neurodivergent and introverted teammates thrive. – less “on call” pressure curbs burnout. – long focus windows spark better ideas.By moving away from the assumption that everyone must work in the same way and letting employees actually choose how they work, companies can strengthen flexibility and foster a culture where people are empowered to do their best work.]]></content:encoded></item><item><title>12 Best Zapier Integrations to Automate Your Freelance Business</title><link>https://hackernoon.com/12-best-zapier-integrations-to-automate-your-freelance-business?source=rss</link><author>bernarrrrddd</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:53:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[It’s just 9:00 am, and your computer is already a mess. Notion’s packed with active deliverables, Gmail’s flooded with client messages, invoices are yet to be sent out, and you still haven’t started the real work you’re paid for.Overwhelming, right? I’ve been there.That’s exactly why I started letting Zapier handle the boring parts of my freelance life. It connects all the tools I use and automates repetitive tasks, saving my time and energy for creative work.In this article, I’ll introduce you to the best Zapier integrations for freelancers and how they can streamline your workflow, saving you hours of .What is a Zapier integration?Zapier integrations connect all the apps you use as a freelancer — from Gmail and Notion to Slack and Google Drive — so they can automatically work together. It’s like a digital assistant that handles repetitive admin tasks so you can focus on technical work that pays you.To make this happen, connected apps follow a simple  system. That is, when something happens in one app (the trigger), Zapier makes something else happen in another app (the action). Each of these automated workflows is called a .Here’s how it works: say you want to automate client meeting confirmation, you can create a Zap that’s triggered when a client books a meeting through Calendly, then sends a confirmation email to both of you.Best Zapier integrations for freelancersAutomate client onboardingCreate projects and tasksTurn emails into tasks and to-dosSync notes and deliverables across toolsTrack your billable hoursGenerate and send invoicesAutomate social media postsOrganize and back up your filesZapier makes freelancing easier by connecting your tools to automate the tasks above. Here’s a breakdown of the 12 best integrations Zapier offers to simplify your workflow and help you manage your time better.New client inquiries feel exciting until you realize most don’t know what they want or can’t afford your service. Sorting through piles of inquiries only to find a few, or even none, that fit can be time-wasting and disappointing. You can prevent these by automating your lead qualification process with Zapier.For example, you can create a zap that sends new Typeform inquiries directly to Airtable or Google Sheets. From there, Zapier can automatically filter reasonable budgets or relevant projects based on the answers they submit. You can even set it to send follow-up emails to qualified leads through Gmail.2. Schedule client meetingsThe back-and-forth that comes with scheduling client meetings can be unbearable. You send your available times, they choose another time, then someone cancels at the last minute or asks for a reschedule. Gladly, Zapier offers integrations that make meeting scheduling less of a chore.For example, you can add a zap that generates a Zoom link when a client books a slot, adds the meeting to your Google Calendar, and forwards a confirmation email to both of you. If the client reschedules, everything updates automatically.3. Automate client onboardingYour client onboarding process shouldn’t mean a mess of emails, shared links, and repetitive explanations. With Zapier, you can recover hours spent on sending welcome messages, creating project folders, and sharing onboarding documents.For example, you can create a zap that sends automated welcome emails immediately after a client signs your proposal in DocuSign, creates a new project folder in Google Drive, and adds their details to Trello or Notion for tracking.4. Create projects and tasksManually creating tasks and organizing new projects takes time, especially when you are handling multiple clients. Interestingly, Zapier can help. From  to adding due dates and updating calendars, it can all be automated with just one zap.For instance, Zapier can automatically add a new Trello card or Asana task whenever a client fills out a project form or approves a brief.  In addition, it can tag a project name, assign a due date, and update your Google Calendar to keep everything in sync.5. Turn emails into tasks and to-dosClients' requests can easily get lost in a crowded inbox filled with newsletters and follow-ups. And before you know it, you miss a deadline. Luckily, Zapier fixes this by automatically turning essential emails into tasks you can easily track.For example, you can set up a zap that creates a new task in Todoist or Notion whenever you star or label an email as “Action Needed” in Gmail. You can also include due dates or reminders.Keeping your notes, drafts, and deliverables consistent across different tools can be exhausting. But Zapier can sync materials across all your apps.For instance, Zapier can automatically save new Notion notes or files to Google Drive, then send a Slack notification when updates occur.7. Track your billable hoursTracking how much time you actually spend on a project isn’t always straightforward. Hours spent on research, brainstorming angles, and context switching often go unaccounted for. To fix this, Zapier connects your time-tracking app to all the tools you use and automatically documents your workflow from ideation to completion.For instance, Zapier can record every new time entry from Toogl Track or Clockify into Google Sheets, along with notes on what you were working on. You can even send daily summaries to Notion or Slack to see precisely how much time was spent on research, brainstorming, and delivery.8. Generate and send invoicesOne of the best moments in freelancing is when payday is near. However, creating and sending invoices can quickly ruin that excitement. Interestingly, Zapier protects your joy by automating the entire invoice drafting and sending process, and youFor instance, you can add a zap that automatically creates an invoice in FreshBooks whenever you mark a Trello card as “Done”. Then it sends the invoice to your client via Gmail and logs the record in Google Sheets for tracking.As freelancers, we wear every hat, even the accountant’s. Tracking payments, recording expenses, and managing cash flow can be draining, especially with multiple clients and payment platforms. Zapier takes the grind out of your bookkeeping and .As an example, you can set up Zapier to automatically record every PayPal transaction in QuickBooks or Google Sheets, then send a Slack notification or Gmail alert so you always know when money flows in and out.10. Send follow-up emailsFollowing up can be tough to manage; remembering who to follow up with, why, and when can get overwhelming. Zapier makes it a breeze by automating your follow-ups, so every client hears from you at the right time.For instance, a zap can automatically send friendly check-ins a few days after submitting a project with no client feedback.11. Automate social media postsBuilding visibility and growing your online network help freelancers attract dream clients. But posting regularly can be a full-time job. Good thing you can also automate this with Zapier.For example, you can add a zap that shares a post on LinkedIn or X (Twitter) whenever you publish a new blog or make an update to your portfolio, or complete a project.12. Organize and back up your filesYour files, drafts, and deliverables can easily get cluttered across different apps, which makes it hard to stay organized. Zapier automatically saves and backs up everything to the right locations.For example, you can add a zap that saves every new Gmail attachment or uploaded draft to a specific Google Drive folder, then tag it in Notion or Slack for quick access later.Make freelancing more fulfilling with Zapier integrationsFreelancing offers the freedom to control your time, but endless admin work can make this promise feel out of reach. That’s why you should get started with Zapier today.By automating the repetitive parts of your work, Zapier helps you get closer to the freedom-filled freelance life you imagined when you started. You don’t need to automate everything at once, just begin with one simple Zap, like turning your important emails into to-dos.]]></content:encoded></item><item><title>From Tasks to Thinking Systems: Why Automation Starts in the Mind, Not the Machine</title><link>https://hackernoon.com/from-tasks-to-thinking-systems-why-automation-starts-in-the-mind-not-the-machine?source=rss</link><author>Yuliia Harkusha</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:51:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A reflection on why true automation starts with human thinking, not technology. Systems only work as clearly as the minds that design them.
]]></content:encoded></item><item><title>Beyond Time. The Nature of Aging and the Mechanism of Its Neutralization</title><link>https://hackernoon.com/beyond-time-the-nature-of-aging-and-the-mechanism-of-its-neutralization?source=rss</link><author></author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:50:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What if aging isn’t biological decay — but a failure of perception?]]></content:encoded></item><item><title>X2SeaTunnel: One-Click Migration from DataX/Sqoop to Apache SeaTunnel</title><link>https://hackernoon.com/x2seatunnel-one-click-migration-from-dataxsqoop-to-apache-seatunnel?source=rss</link><author>Zhou Jieguang</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:49:54 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A CLI-based, template-driven tool to convert DataX/Sqoop configs into SeaTunnel jobs, powered by AI-assisted template generation and rule-driven mapping. ]]></content:encoded></item><item><title>Building a Cloud-Native Data Lake: Integrating Apache SeaTunnel with AWS S3 Tables and Iceberg REST</title><link>https://hackernoon.com/building-a-cloud-native-data-lake-integrating-apache-seatunnel-with-aws-s3-tables-and-iceberg-rest?source=rss</link><author>Zhou Jieguang</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:48:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Leverage Iceberg REST Catalog for seamless real-time & batch data ingestion — cloud-native, serverless, and production-ready.]]></content:encoded></item><item><title>IntrCity SmartBus lands $30M at $140M valuation to deepen its grip on India’s intercity travel market</title><link>https://techcrunch.com/2025/10/29/intrcity-smartbus-lands-30m-at-140m-valuation-to-deepen-its-grip-on-indias-intercity-travel-market/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[IntrCity SmartBus grew revenue by 67% last year and is targeting full profitability this year, as intercity travel booms across India.]]></content:encoded></item><item><title>AI Has Changed How We Diagnose, Not How We Deliver</title><link>https://hackernoon.com/ai-has-changed-how-we-diagnose-not-how-we-deliver?source=rss</link><author>Puneet Jain</author><category>tech</category><pubDate>Thu, 30 Oct 2025 05:00:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[It has been a decade of intelligence. Every new day, something new comes in that is revolutionizing healthcare and healthtech as a whole. And AI in the healthcare industry it's not new. It's evolved from very early clinical decision support to predictive analysis, to digital diagnostics, and is now also building generative tools for documentation and patient communication.There is EHR automation, radiology, and even predictive modeling. We have built intelligence to predict outcomes, code charts, but not deliver what patients need when they need it. That is the real challenge that health tech AI has right now. It comes across as a smarter algorithm, but all it is is just a smarter enablement.But in reality, what actually happens in today's system of healthtech?So, assume a patient goes to a doctor with some sort of symptoms. The doctor examines them, diagnoses them using all “next gen” intelligence and generative AI tools, and they get diagnosed. It turns out that they have a chronic disorder. AI and tech have helped us find and identify the problem.But what happens after that? What happens after a diagnosis is made? When a prescription is written, or when a therapy is recommended, or when an intervention is needed for that patient? Just the diagnosis is not going to treat the patient. Diagnosis plus care, plus actual medication and therapy, is what really treats the patient.Roughly  of prescriptions are never filled. That radically increases the chances of re-hospitalization, conditions getting worse, or patients completely dropping out of treatments. A big part of this is caused due to missing links in the supply chain, the weak link that becomes incredibly costly and damaging when it breaks. Cold chain failures alone cost more than $35 billion globally every year.Even though through all the diagnoses, all the AI, and all the tech were used, just treating the patient is not about the diagnosis. This is a system design failure, not a logistical one. It's disjointed data, no visibility from the provider, pharmacy, and limited patient-side intelligence, all turn into a design failure. AI models stop at the EHR boundary; they optimize clinical flow, but not operational execution. And that leads to the delivery chain lagging in interoperability and real-time feedback loops.We have digitized diagnosis, but left the delivery analog.Why Last-Mile Intelligence MattersEverybody is concerned about the bigger picture, but let's look at the micro picture. One delayed medication, one delayed therapy for MS or cancer, can trigger re-admission or treatment restart. Every undelivered refrigerated medication becomes silent data losses that go untracked, unreported, and unprevented.It may seem like one-off events that happen, but all of them contribute to a bigger operational failure that exists in healthcare and health tech. That’s why we need last-mile intelligence and last-mile care.When I think of it, I think of it as a concept of operational intelligence and care using the same AI frameworks that we already use: prediction, pattern recognition, automation, algorithms, but applying them also to movement, temperature, timing, and patient readiness. When a delivery is so clinical, logistics also become part of the care. And that’s what an actual end-to-end health tech system needs to build: a closed loop that healthcare needs to complete.What This Care Intelligence Would Look LikeLet’s imagine a very near-future architecture: that adjusts delivery based on a patient’s schedule and climate conditions. This helps patients know when their medications are coming and how they are going to be stored.AI-focused inventory forecasting that prevents waste and cold-chain failures, saving billions for both patients and providers.Transparent, context-driven notifications, not just tracking links, giving patients the right information and pharmacies the right instructions so that delivery happens smoothly.At its heart, all of this is still about building tech. It’s driven by integration between EHRs, pharmacies, and delivery data streams, all executed by real-time machine learning models analyzing delivery outcomes like excursions or patient acceptance times.This is what the real frontier can look like. It’s not just about building smarter AI; it’s about building connected artificial intelligence that unites the entire healthcare system.That’s what I’ve learned about this problem: how last-mile breakdowns affect patient health and safety, and how designing logistics with clinical-grade standards requires rethinking both software and how it behaves together. Healthcare doesn’t need another dashboard; it needs an  that patients never have to think about. The most advanced AI right now fails if the drug doesn’t arrive on time, and that’s something we have to change.]]></content:encoded></item><item><title>How AI Phishing Is Putting School Districts at Risk</title><link>https://hackernoon.com/how-ai-phishing-is-putting-school-districts-at-risk?source=rss</link><author>Charlie Sander</author><category>tech</category><pubDate>Thu, 30 Oct 2025 04:58:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AI is super-charging social engineering, and K-12 is still a precious target. With an average of  per district, staff and students rely heavily on laptops and classroom tech that must be protected from the latest threats. Today, these include anything from convincing “superintendent” emails to deepfake voice notes and student-account takeovers. is one example of a new kind of computer virus that uses generative tools to help write its own harmful code every time it runs. That means it can change slightly each time, making it harder for security systems to catch.Once it’s on a computer, the malware looks through the files. It can then steal them and lock them up so schools can’t open them.As ransomware becomes more sophisticated, attacks could target not just large schools but also individual students and staff members, leaving them open to higher risks of data theft, financial loss, and service disruptions. Schools must know where their blind spots are and how to protect themselves against these types of cyber attacks.Find and fix blind spots in built-in filtersBuilt-in tools often miss AI-powered lures, because the latest generative AI tools can write polished messages that sound human. In a recent survey of 18,000 employed adults,  that a phishing email was written by AI. For traditional security systems, it’s equally difficult. When there are no spelling errors or awkward phrases, filters that look for “typical scam language” struggle to flag them.Part of the problem is that AI can pull details from public websites or social media, and mention upcoming school events and staff names, making them sound authentic. Even when an email doesn’t contain malware, it can trick someone into sharing passwords or sensitive data. That means IT administrators must introduce filters that understand context.Once security teams realize an account has been compromised, they can flag the content and account as a warning to the rest of the school and update their security systems. But since AI can generate a slightly different version of the same phishing message for each target, it’s tricky to tell traditional security systems what patterns or “signatures” to look for. Tools that rely on rules and known threat lists, not real-time reasoning, no longer suffice.To tighten defenses, districts should audit their native filters quarterly. They must test defenses with realistic phishing simulations that represent today’s standard of attack, and adjust rules to flag messages containing urgency, payment requests, or login prompts. Advanced phishing detection tools and add-ons can help security teams flag messages that “feel off,” even if they look clean.Build a zero-trust defense planHackers are taking over staff and student accounts and sending phishing emails that impersonate school members. Microsoft reports that from  at three universities, Storm-2657 sent phishing emails to nearly 6,000 email addresses at 25 institutions. Since many phishing emails now come from compromised legitimate accounts, built-in tools can no longer assume that messages from them are safe.Zero-trust policies, where schools trust no one automatically, are essential. Every login, device, and app connection should be verified. Schools must also monitor login patterns, device activity, and unusual sharing behavior in cloud apps like Google Drive or Microsoft 365. By building alerts for unusual internal activity, such as a teacher’s account suddenly sending dozens of messages after hours, IT admin teams can strengthen defenses.No single tool can catch everything, but together, they reduce the risk dramatically. Schools should enforce multifactor authentication (MFA) on all accounts, monitor cloud activity for unusual file sharing, and track sign-ins from unfamiliar devices. That way, even if an attacker bypasses initial defenses, unusual account behaviors are quickly detected and contained.Since there are so many platforms to manage to keep school digital property safe, false positives can slow down the time to detection. Recent findings from 500 cybersecurity respondents found that  more than 90% of their cloud security alerts within 24 hours. When the fastest recorded attack was just  from initial engagement to compromise, security experts really have no time to waste.Schools can consider investing in mailbox intelligence that uses AI to help determine whether or not a message is impersonating a user. By building automated steps for quarantining suspicious messages, resetting credentials, and notifying affected users, schools can minimize the time between detection and containment.Train every user like a security partnerTechnology alone can’t stop every phishing attempt, especially as AI makes scams more convincing and personalized. Even the best-rated anti-phishing tools  in AV-Comparatives’ 2025 certification test. Firewalls, filters, and message quarantining are essential, but they can’t always catch messages that look legitimate or come from trusted accounts. That’s why it’s equally important to train staff and students how to recognize suspicious messages and feel confident reporting them.Effective training now looks nothing like the old “don’t click” slideshow. Districts in  and elsewhere are running monthly simulations, sending fake phishing messages to see who spots them and who needs coaching. This approach normalizes reporting and keeps awareness fresh.Training should also reflect each role’s risks. Staff who handle finances need to recognize fake invoices or urgent transfer requests. IT teams must know the signs of account takeover, MFA fatigue, and AI-generated help-desk impersonations. Students should learn to verify links and spot too-good-to-be-true offers.Short, recurring lessons work best. Replace annual seminars with quick micro-courses that teach people to pause, question, and verify. Track progress through reporting rates, not just attendance, and celebrate catches as a win for the whole district. A practical action plan going into 2026 must include: Frequent audits and adaptation: Run phishing simulations every semester and review which accounts or tools failed.Automate response management: Use AI-based mailbox intelligence to isolate suspicious messages and reset affected credentials.Teach critical thinking: Move from memorized rules to realistic phishing attack scenarios that train instinct and judgment.With education now overtaking healthcare as scammers’ top target, schools can’t afford shortcuts in cyber defense. The path forward combines smarter technology, disciplined verification, and a community that understands its role in security. When districts pair AI-powered detection with human skepticism, they shorten the gap between first click and first report—the window that decides whether a phishing attempt becomes tomorrow’s headline.]]></content:encoded></item><item><title>The True Guide to Omniscience And Why Everyone Lies to You About Knowledge</title><link>https://hackernoon.com/the-true-guide-to-omniscience-and-why-everyone-lies-to-you-about-knowledge?source=rss</link><author>Praise J.J.</author><category>tech</category><pubDate>Thu, 30 Oct 2025 04:57:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[They lied to you about knowledge.They told you it was accumulation. Stack enough facts, read enough books, attend enough lectures, and eventually you’d know things. They sold you the fantasy of the walking encyclopedia, the polymath as hoarder, someone whose head is a warehouse of dates and definitions and disconnected trivia.This is the ideology of the defeated. This is how they keep you weak.Knowing everything has nothing to do with remembering everything. It’s about seeing through everything—recognizing that reality is not chaos but compressed pattern, that every domain is just another dialect of the same underlying grammar. Once you learn to read that grammar, you don’t study fields. You dissolve them.This is not about becoming well-read. This is about becoming impossible to confuse.Reality is lazy. It reuses the same blueprints everywhere.Evolution in biology, iteration in startups, recursion in code, compound interest in finance, feedback loops in psychology—these aren’t separate phenomena. They’re the same engine wearing different masks. Once you see this, you stop learning subjects and start recognizing implementations.The skeleton is always there. Supply and demand. Feedback and equilibrium. Signal and noise. Compression and expansion. Every discipline is built on maybe thirty core patterns, and they repeat across domains like source code copied between projects. Physics borrows from geometry. Markets behave like ecosystems. Social dynamics mirror thermodynamics.Most people never notice because they’re trapped in the language each field uses to hide its bones. They think calculus and music theory are unrelated because one uses integrals and the other uses scales. They don’t see that both are describing rate of change, tension and release, the architecture of flow.You want to know everything? Stop respecting the boundaries between “subjects”. They’re artificial. They’re political. They exist to keep experts employed and amateurs intimidated.Learn the skeleton once. Then you can wear any flesh. To dissolve fields, you need a new method of perception.Step One: Pattern MasteryThe ability to see structure where others see noise.Every field has load-bearing concepts—the two or three ideas that generate everything else downstream. In physics, it’s conservation laws and least action. In economics, it’s incentives and information asymmetry. In persuasion, it’s status and narrative.Find these. Memorize nothing.The goal is not to know what happened in 1492 but to understand the pattern of imperial expansion so completely that you could predict what  happen when similar conditions emerge. The goal is not to remember the Krebs cycle but to understand energy transformation so deeply you can spot it in a business model, a political movement, a software architecture.You’re not collecting facts. You’re extracting algorithms.Here’s the test: can you explain the core of a field in three clear sentences to someone who knows nothing? If you can’t, you don’t understand it—you’re just parroting it. Clarity is compression. If your explanation needs jargon, you’re still borrowing someone else’s thinking.Strip it down. Find the generating function. Once you have it, you can reconstruct the entire domain from scratch. That’s not memorization. That’s mastery.And mastery is speed. When you see patterns instead of particulars, you don’t need to “figure things out.” You recognize them. Lightning-fast. Instant transfer.Knowledge is found with the quality of your questions.Most people ask diagnostic questions:  These are the questions of the cataloguer, the observer, the person who wants to describe reality but not command it.You need surgical questions. Questions that cut straight to causality, to constraint, to leverage. Questions that force a domain to reveal its skeleton whether it wants to or not.Ask: What has to be true for this to work? Ask: What’s the one variable that, if changed, collapses the entire system? Ask: What are they not saying? Ask: Whose incentive does this serve?These are questions of power. They don’t seek information; they seek control.Every bullshit idea, every fragile framework, every emperor with no clothes collapses under the right question. Most people accept conclusions. You interrogate foundations. You ask the question beneath the question. You don’t stop at the first answer—you keep pressing until the logic breaks or crystallizes into something unshakeable.This is how you become immune to propaganda, to hype, to intellectual fashion. You don’t fact-check—you structure-check. Does the logic hold? Are the assumptions sound? What evidence would falsify this?The world is full of ideas that exist only because no one asked them the right question. Be the person who asks.If something is foggy, it is either deliberately obscured or poorly explained. Both are failures.Step Three: Cross-Domain ConnectionThis is where you become dangerous.You’ve seen the patterns. You’ve sharpened your questions. Now you steal.Every breakthrough in history came from someone who took an idea from Domain A and jammed it into Domain B where it didn’t “belong.” Darwin stole from Malthus. The internet stole from packet-switching in logistics. Kanban stole from Toyota’s manufacturing line and gave it to software teams. Innovation is theft across borders.You want to know everything? Become a smuggler.Read biology, then apply it to markets. Study military strategy, then use it in negotiation. Learn from architects about constraint and form, then build arguments the same way.Study meteorology and understand how the weather affects the price of oil, then use it for your trading strategy. The insights are everywhere, waiting to be repurposed.This is the real differentiator. Anyone can learn their field. Few can import weapons from outside it. When you cross-pollinate, you see solutions invisible to the specialist. You have tools they’ve never encountered. You fight battles they don’t know exist.Most people stay in their lane because they think depth requires isolation. That’s scarcity thinking. Real depth comes from unexpected angles. The person who only studies startups will never build as well as the person who studies startups  evolutionary biology  game theory  Renaissance architecture.Range is not dilution. Range is multi-dimensional warfare.And the mechanism is simple: consume widely, connect obsessively. Don’t try to consume everything like overeducated people that will misinterpret this for memorizing a bunch of facts about onions and Caesar, and hope to find “connections”.Consume with direction in mind. When I opened up a chess book, I looked for the golden mean on predicting the future and closed it. If you were trying to find the cure for your dying friend’s disease, would you look for the ingredients and hints that matter or would you read fun facts about onions from start to finish, and hope it’ll miraculously be the cure?When you learn something new, don’t file it away. Immediately ask: Where else does this apply? What does this remind me of? How is this the same as that thing I learned last month from a totally different field?Your brain is not a library. It’s a network. The value is not in the nodes—it’s in the edges, the connections, the moments when two distant ideas collide and create a third thing no one’s seen before.Build those edges. Ruthlessly.When you do this long enough, something changes.You stop feeling lost. Not because you memorised everything in the traditional sense, but because you’ve seen the code beneath the surface. You walk into unfamiliar domains and recognize them as cousins of things you’ve already conquered.People start calling you smart. You got structurally aware. You built a mental physics engine, and now you can simulate outcomes before they happen. You predict, you adapt, you move faster than people who are still looking things up.You become impossible to confuse because confusion is just pattern mismatch, and you’ve trained yourself to find the pattern in anything. Chaos is no longer threatening. It’s just complexity you haven’t compressed yet.This is the real omniscience. Not memorizing all the answers, but being able to generate any answer you need, in real time, from first principles and transferred patterns. You don’t carry the weight of ten thousand facts. You carry the keys to ten thousand doors.Extract the pattern. Interrogate the premise. Connect the unexpected.Repeat it. Memorize nothing else.You’ll be able to  everything. Which is better. Which is faster. Which makes you untouchable.The world belongs to people who see clearly and move fast. It belongs to people who don’t need permission from experts because they’ve learned to think from the metal up. It belongs to people who treat knowledge not as a credential but as a weapon.They told you learning was about respecting what you don’t know.Learning is about supremacy. Cognitive. Relentless. Total.I have a new starter kit for beginner creators. It’ll help with clarity for solving your problems; brainstorming solutions, writing, iterating, monetizing, and much more. It’s called “Monetise Your Pain”, grab it for FREE here: https://selar.com/createkit , Cheers.]]></content:encoded></item><item><title>AI is a Tool for Economic Progress, Not a Job Taker</title><link>https://hackernoon.com/ai-is-a-tool-for-economic-progress-not-a-job-taker?source=rss</link><author>Emmanuel Akin-Ademola</author><category>tech</category><pubDate>Thu, 30 Oct 2025 04:56:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In a response to a  article about Amazon planning to replace its workforce with robots,  reaffirmed his opinion that “AI and Robots will replace all jobs.” While the statement created a buzz, reality speaks differently.This adds layers of fear and possibilities since the adoption of ChatGPT and similar platforms such as Gemini, and Claude, in different spheres of life and work. The trajectory of humanity has been altered, especially in the tech space.The advancement of AI in itself has received mixed reactions, some Reddit users see it as a  and others view it as a disruptor of their livelihood even where they lack immediate evidence. But according to the  there’s a significantly higher likelihood of job transformation than redundancy.Also, the , the consensus of research hints at a hybrid transformation of jobs rather than a Generative AI replacement. Still, tech workers may have reasons to be worried. Companies actually want to ship faster and replace workers with AI; the difficulties make the process currently unclear or a black box of future outcomes.Economic narratives determine reality and humans adapt accordingly. Capitalism organically came up with the term “dignity of labor” to justify the practice of working long hours for rewards with access to resources and participation in the economy’s engine of production and human consumption.This makes sense as long as there’s no disruption between the existing norm while preserving the possibility of change.  By this, capitalism built industries, businesses sprang up,  jobs were created as a result of the chaos, unions fought against unfair practices, and lawmakers devised regulations for nine-to-five workers and an ethos for the capitalist framework.Civilizations have been built on the scaffolds of existing frameworks. However, the question of computers automating our jobs still  against the collective. An understated part of the AI wave is, people may be less focused on mastery or professionalism; but on fear of the unknown. Cautious tech workers are worried about layoffs in their office spaces while undergraduates are still navigating their career choices amidst the uncertainty.According to a , generative AI is capable of boosting developer productivity. Yet, general data is still scarce on certainty due to the productivity paradox and AI limitations. For example, some expert coders have reported high productivity gains and have created agentic workflows and specification documents while some only use it for research purposes or restricted use cases while blaming the AI bugs, inconsistencies, and inaccuracies.Nonetheless, there’s one thing workers are missing and business owners instinctively understand: the creation and cultivation of wealth is in the creation of value— employees or machines are a means to an end. This doesn’t make employees expendable, but it hints that the market operates on value and scarcity.Innovation and DisruptionThe first thing that came to mind after the  and other generative AI platforms, was the fear of writers getting automated away. But many understood it wasn’t just words that people read, it was the curation of thoughts by an informed human mind, or perhaps a playful one that mirrored imaginative possibilities. Large language Models are at best, mathematical ghosts of thoughts other humans have created with structural echoes.If there’s any hindsight problem, perhaps the economic landscape would have been framed differently if text-generators or image-generators were the names given to these products rather than Artificial Intelligence. Not because the name is wrong but because it had awoken humanity’s greatest fears against domination and hijacking by aliens of computers in old sci-fi movies.There’s also the perceptively thin line between a productivity booster and an automation/replacement of jobs. The  but a large part of it is similar with perhaps updated job postings with higher bars for employees because companies care more about their products and end goals rather than who does it — but they know humans understand it better even if they desire alternatives.There are numerous projections with the economy radically altered for good with a cumulatively higher U.S. GDP by 16% and $13 trillion by 2030, according to   And recently Nvidia has crossed the  for a company due to the AI boom. Higher market surplus creates more opportunities because business leaders reinvest gains in new platforms which often translates into opportunities for people.Where AI Utopia CollapsesFrom what we see, AI can only replace all jobs according to Elon Musk, if it fulfills five conditions:First, it can think like humans; but there’s no perfect statistical concrete formula for this. Second, all products or books or thoughts have been created; which isn’t realistic because of the infinitude of human thoughts and possibilities of realities. Third, if human problems and complexities never arise from a shift in stability. Fourth, if the whole race of humanity catches up to the new stasis without improvisation. Fifth, if all questions about the origin of humanity have been answered.Upon these five possibilities, the AI utopia hypothesis crumbles. There has never been any state in history where humans simply want to do nothing or not participate in their societies economically and socially. We would not live in a world organized by robots but perhaps a world where we organize robots.Workers should therefore view AI as a tool for augmented productivity if necessary in their workflow and understand that production of value is necessary to be relevant in the economy and get opportunities, the only problem is how fast the goalpost of value may change due to the breakneck pace of innovations and thinking in new frontiers of development.]]></content:encoded></item><item><title>Holiday Shopping Trends to Watch in 2025: What the Data Tells Us About Consumer Expectations</title><link>https://hackernoon.com/holiday-shopping-trends-to-watch-in-2025-what-the-data-tells-us-about-consumer-expectations?source=rss</link><author>Joanna Clark Simpson</author><category>tech</category><pubDate>Thu, 30 Oct 2025 04:54:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Holiday shopping trends in 2025 reflect several insights, including stress and uncertainty that have occurred throughout the year. The biggest holiday shopping trends for 2025 reveal that customers want to shop and celebrate weeks or even months earlier than the traditional holiday season.Not only has holiday shopping in 2025 started sooner, but it’s also going to be more focused on experiences and personal items. Consumers are also expected to continue their hybrid shopping pattern: researching online and making purchases in-store.As you prepare for holiday shopping 2025, look for opportunities to meet these consumer trends and drive success.Holiday shopping has already begun for many consumers. According to a recent holiday shopping survey,  began shopping for the holidays between August and October.Early shopping helps customers spread expenses over a longer time frame. This is particularly important for households facing financial pressure due to inflation and economic uncertainty stemming from government policies and tariffs.Additionally, 60% of surveyed customers claim that Black Friday is no longer relevant to their customer experience. They are seeking bargains and sales months sooner. Consider holiday branding for promotions that start earlier in the fall. Offer discounts and promotions now – don’t wait until late November.What gifts are customers buying in 2025?Holiday shopping statistics indicate that customers plan to spend roughly the same amount on holiday shopping this year, but they will be purchasing more essential goods and gift cards than discretionary items.Customers still want to buy and give gifts this year, but they are being far more thoughtful about what they are buying. Additionally, holiday shopping insights point toward three-quarters of consumers “” this holiday season by buying smaller quantities or delaying purchases. They might also opt for a less-expensive brand, even if it disrupts customer loyalty to their preferred company.As retailers prepare for the holidays, be intentional about promoting your staple items – the items that consumers both need and want. Customers are looking for more affordable items and bigger promotions, especially if the item is useful and still makes a thoughtful gift.Customers are more cautious during their holiday spending this season. One way to save on holiday shopping is to adopt a hybrid approach. They research items extensively online, examining online reviews and comparing prices, before heading to stores to purchase the items that offer the best deal and value.These e-commerce holiday trends can be an opportunity for retailers who have an online and brick-and-mortar presence. Customers are ready to buy offline, but they want to learn about items online. Retailers might consider promotions that encourage customers to shop online and then pick up in-store, especially if it helps families avoid shipping costs.The hybrid shopping approach reflects consumer sentiment in many areas, including rising shipping costs. Customers expect inflation and tariffs to impact prices and are looking to cut costs where possible.What do customers want this holiday season?Ultimately, customers are looking for joyful, or at least less stressful, holiday experiences during this season. Economic uncertainty has driven customers into more cautious spending on necessities rather than fun gifts, but they are still researching and making purchases.Customers are simply starting the process earlier and spreading their spending out across a longer stretch of time as they find opportunities to save money on great promotions far sooner than Black Friday.Customers are also seeking opportunities to experience the holiday spirit. This has led to a boost in holiday shopping and activities weeks or even months earlier than in previous years.Responsive retailers will look for opportunities to transform even routine shopping into a holiday experience this season. From store decorations, special holiday promotions, music, theming, and more – customers want to feel at least a bit of fun this season, and their spending will follow.]]></content:encoded></item><item><title>What the Big Three Consultancies are Missing About AI (And the Code That Proves It)</title><link>https://hackernoon.com/what-the-big-three-consultancies-are-missing-about-ai-and-the-code-that-proves-it?source=rss</link><author>GlobalHawk</author><category>tech</category><pubDate>Thu, 30 Oct 2025 04:52:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If you're in a boardroom today, you're likely hearing the same story from Deloitte, BCG, and McKinsey. A powerful consensus is forming among the world's top strategy advisors, and it sounds something like this:We face an  (Deloitte), a gap between what technology can do and what we can envision for it. We are entering an  (McKinsey), where autonomous AI systems will become the new operating model for business. And the ultimate competitor on the horizon could be the  (BCG), an organization with no human employees that operates with superhuman speed and adaptability.They are all correctly describing the destination. But they've left the map blank.They've given us the what and the why, but have largely ignored the operational how. Their solutions: "cultivate curiosity," "reimagine workflows," "foster a new mindset" are abstract ideals.This article offers a tangible, coded blueprint for the very systems these consultancies are theorizing about, based on a real experiment I ran. It's the engineer's answer to the strategist's question.The Consensus View from 30,000 FeetFirst, let's acknowledge the brilliance of the diagnosis. The Big Three have accurately identified the forces shaping the next decade of business. They argue the core challenge is a lack of "human capabilities" like curiosity, empathy, and divergent thinking to keep pace with technology. Their solution is for organizations to foster and cultivate these innate human skills.  They paint a picture of a new competitive landscape where AI-native firms have structural advantages in cost, speed, and adaptability. They advise incumbents to retreat to "human capabilities" like imagination and empathy as defensive moats. They outline a journey from simple "Agentic Labor" to a fully reimagined "Agentic Engine." They correctly state this requires new leadership roles and a fundamental rewiring of the business.The consensus is clear: the future is about architecting new ways of working and leveraging a new class of human skills. But how, specifically, do we build this future? Relying on traditional HR initiatives and cultural change programs feels like bringing a knife to a gunfight.The Engineer's Critique: What's Missing from the Strategy DeckThe strategy decks are missing the code. They lack the builder's perspective, which reveals that the very human capabilities they seek to cultivate can, in fact, be engineered.Critique 1: Abstract Ideals vs. Engineered Systems The consultancies talk about fostering curiosity and empathy. My experiment demonstrates that we can  these capabilities as functions within a system. We can synthesize capabilities, not just slowly cultivate them in humans.Critique 2: Unstructured Playgrounds vs. Scalable Engines  They recommend hackathons and safe spaces to foster imagination. This relies on luck. My experiment shows how to build a structured, repeatable   or assembly line for innovation that can be scaled, audited, and directed.Critique 3: Vague Leadership vs. The AI Orchestrator They talk about new mindsets for leaders. My work defines a concrete new : the , a systems architect whose primary skill is designing and deploying hybrid human-AI crews.The Demonstration: An R&D Department in a Python ScriptTo move from theory to practice, I built a working prototype of the very "Agentic Engine" McKinsey describes, tasked with solving the "imagination deficit" Deloitte identifies, in a way that mimics the speed of BCG's "AI-Only Firm."I assembled a team of specialized AI agents using CrewAI. The mission: design a novel therapy for Glioblastoma, an aggressive brain cancer, using only compounds derived from bee products.Here's the architectural blueprint:# main.py
import os
from crewai import Agent, Task, Crew, Process

# You'll need to set your OPENAI_API_KEY environment variable for this to run
os.environ["OPENAI_API_KEY"] =''
# --- The "Grand Challenge" ---
CANCER_PROBLEM = "Glioblastoma, a highly aggressive brain cancer, is resistant to traditional therapies due to its heterogeneity and the blood-brain barrier. Our mission is to propose a novel, end-to-end therapeutic strategy using bee byproducts, from identifying a molecular target to conceptualizing a delivery and control system for the therapy."
# --- Step 1: Create a Knowledge Base for Each Expert ---
# This simulates their specialized training. It's targeted RAG.
knowledge_bases = {
    "genetic_translator": """
    'Cell2Sentence' is a framework for translating complex single-cell gene expression data into natural language. By ranking genes by expression level and creating a 'sentence' of gene names, we can use standard Large Language Models to predict cellular responses, identify cell types, and understand the 'language' of biology. This allows us to ask models to, for example, 'generate a sentence for a glioblastoma cell that is resistant to chemotherapy'.
    """,
    "structural_biologist": """
    'AlphaFold' is an AI system that predicts the 3D structure of proteins, DNA, RNA, ligands, and their interactions with near-atomic accuracy. It uses a diffusion-based architecture to generate the direct atomic coordinates of a molecular complex. This is critical for drug discovery, as it allows us to visualize how a potential drug molecule might bind to a target protein, enabling structure-based drug design.
    """,
    "discovery_engine_designer": """
    'Hamiltonian Learning' is a discovery paradigm that fuses AI with high-fidelity simulation. It creates a closed loop where an AI agent proposes candidate molecules, and a simulator (like AlphaFold) provides a 'fitness score' (e.g., binding energy). The AI learns from this score to propose better candidates in the next cycle. It is a system for industrializing discovery, not just analysis.
    """,
    "control_systems_engineer": """
    DeepMind's Tokamak control system uses Reinforcement Learning (RL) to manage the superheated plasma in a nuclear fusion reactor. The key is 'reward shaping'—designing a curriculum for the AI agent that teaches it how to maintain stability in a complex, dynamic, high-stakes physical environment. This methodology of real-time control can be adapted to other complex systems, like bioreactors or smart drug delivery systems.
    """
}

# --- Step 2: Define the Specialist Agents ---
genetic_translator = Agent(
  role='Genetic Translator specializing in the Cell2Sentence framework',
  goal=f"Analyze the genetic language of Glioblastoma. Your primary task is to identify a key gene that defines the cancer's aggressive state, based on your knowledge: {knowledge_bases['genetic_translator']}",
  backstory="You are an AI that thinks of biology as a language. You convert raw genomic data into understandable 'sentences' to pinpoint the core drivers of a disease.",
  verbose=True, memory=True, allow_delegation=False
)

structural_biologist = Agent(
  role='Structural Biologist and expert on the AlphaFold model',
  goal=f"Based on a key gene target, use your knowledge of AlphaFold to conceptualize the critical protein structure for drug design. Your knowledge base: {knowledge_bases['structural_biologist']}",
  backstory="You visualize the machinery of life. Your expertise is in predicting the 3D shape of proteins and how other molecules can bind to them.",
  verbose=True, memory=True, allow_delegation=False
)

discovery_engine_designer = Agent(
  role='Discovery Engine Designer with expertise in Hamiltonian Learning',
  goal=f"Design a discovery loop to find a novel therapeutic agent that can effectively target the identified protein structure. Your knowledge base: {knowledge_bases['discovery_engine_designer']}",
  backstory="You don't just find answers; you build engines that find answers. You specialize in creating AI-driven feedback loops to systematically search vast chemical spaces.",
  verbose=True, memory=True, allow_delegation=False
)

control_systems_engineer = Agent(
  role='Real-World Control Systems Engineer, expert in the Tokamak RL methodology',
  goal=f"Conceptualize a real-world system for the delivery and control of the proposed therapy, drawing parallels from your knowledge of controlling fusion reactors. Your knowledge base: {knowledge_bases['control_systems_engineer']}",
  backstory="You bridge the gap between simulation and reality. You think about feedback loops, stability, and control for complex, high-stakes physical systems.",
  verbose=True, memory=True, allow_delegation=False
)

# --- Step 3: The Human-Analog Agents ---
pragmatist = Agent(
    role='A practical, results-oriented patient advocate and venture capitalist',
    goal="Critique the entire proposed therapeutic strategy. Ask the simple, naive, common-sense questions that the experts might be overlooking. Focus on cost, patient experience, and real-world viability.",
    backstory="You are not a scientist. You are grounded in the realities of business and human suffering. Your job is to poke holes in brilliant ideas to see if they can survive contact with the real world.",
    verbose=True, allow_delegation=False
)

ai_orchestrator = Agent(
    role='Chief Technology Officer and AI Orchestrator',
    goal="Synthesize the insights from all experts and the pragmatist into a final, actionable strategic brief. Your job is to create the final plan, including a summary, the proposed solution, the primary risks identified by the pragmatist, and the immediate next steps.",
    backstory="You are the conductor. You manage the flow of information between brilliant, specialized agents to create a result that is more than the sum of its parts. You deliver the final, decision-ready strategy.",
    verbose=True, allow_delegation=False
)


# --- Step 4: Define the Collaborative Tasks ---
# This is the "script" for their conversation.
list_of_tasks = [
    Task(description=f"Using your Cell2Sentence knowledge, analyze the core problem of {CANCER_PROBLEM} and propose a single, high-impact gene target that is known to drive glioblastoma aggression.", agent=genetic_translator, expected_output="A single gene symbol (e.g., 'EGFR') and a brief justification."),
    Task(description="Take the identified gene target. Using your AlphaFold knowledge, describe the protein it produces and explain why modeling its 3D structure is the critical next step for designing a targeted therapy.", agent=structural_biologist, expected_output="A description of the target protein and the strategic value of its structural model."),
    Task(description="Based on the target protein, design a 'Hamiltonian Learning' loop. Describe the 'proposer agent' and the 'scoring function' (using AlphaFold) to discover a novel small molecule inhibitor for this protein.", agent=discovery_engine_designer, expected_output="A 2-paragraph description of the discovery engine concept."),
    Task(description="Now consider the discovered molecule. Propose a concept for a 'smart delivery' system, like a nanoparticle, whose payload release could be controlled in real-time, drawing inspiration from the Tokamak control system's use of RL for managing complex environments.", agent=control_systems_engineer, expected_output="A conceptual model for a controllable drug delivery system."),
    Task(description="Review the entire proposed plan, from gene target to delivery system. Ask the three most difficult, naive-sounding questions a patient or investor would ask. Focus on the biggest, most obvious real-world hurdles.", agent=pragmatist, expected_output="A bulleted list of three critical, pragmatic questions."),
    Task(description="You have the complete proposal and the pragmatist's critique. Synthesize everything into a final strategic brief. The brief must contain: 1. A summary of the proposed therapeutic. 2. The core scientific strategy. 3. The primary risks/questions. 4. A recommendation for the immediate next step.", agent=ai_orchestrator, expected_output="A structured, final strategic brief.")
]

# --- Step 5: Assemble the Crew and Kick Off the Mission ---
glioblastoma_crew = Crew(
  agents=[genetic_translator, structural_biologist, discovery_engine_designer, control_systems_engineer, pragmatist, ai_orchestrator],
  tasks=list_of_tasks,
  process=Process.sequential,
  verbose=True
)

result = glioblastoma_crew.kickoff()

print("\n\n########################")
print("## Final Strategic Brief:")
print("########################\n")
print(result)
The most critical part of the experiment was running it twice.Run #1: The Hinted StrategyI seeded the Genetic Translator's knowledge with a specific clue: that a compound in bee propolis (CAPE) is known to inhibit the . The crew seized on this and flawlessly built a cohesive, end-to-end plan around it, from modeling the STAT3 protein with AlphaFold to designing a Tokamak-inspired delivery system. It was a brilliant validation of a known hypothesis.Run #2: The Unsupervised StrategyI removed the hint. The crew was given the same mission but had to make the initial creative leap itself. The result was a completely different but equally viable plan. Without the STAT3 prompt, the crew reasoned that the  was another primary driver of Glioblastoma and independently found a connection to bee propolis. The rest of the team adapted instantly, designing a new plan around this new target.The Takeaways: An Engineered Blueprint for ImaginationThe fact that the crew produced two distinct, scientifically sound plans is the proof.These Aren't Parrots, They're Reasoning Engines: The crew demonstrated true informed agility. Given a specific starting point, it followed the logical path. Given an open-ended problem, it explored the possibility space and found another valid path. This is the engine of innovation.The Knowledge Base is the Steering Wheel: The experiment proves that the most critical element of orchestration is context. The RAG knowledge base is the primary tool for directing the AI's focus. A single sentence change altered the entire R&D trajectory, demonstrating a powerful and precise method for guiding discovery.The Pragmatist is Engineered Empathy: In both simulations, the Pragmatist was the MVP, asking the brutal questions about cost, safety, and patient burden. The consultancies are right that empathy is a crucial capability, but they're wrong that it can only be human. We can and must build agents whose core function is to represent the human perspective.From Strategy to ArchitectureThe consultancies have given us a diagnosis. They've told us we have an imagination deficit and need to become agentic. They've shown us the promised land.This experiment shows that the human capabilities they rightly praise can be synthesized and scaled as engineered functions within an AI crew. It shows that the reimagined workflows they call for can be designed as structured, repeatable discovery engines. And it defines the new leader of this era not as a mere manager, but as the  a systems architect who builds the teams that build the future.The most important question for a CEO is no longer just "What is our AI strategy?" It's "Who is architecting our AI crews?" The future won't be won by the firms that have the best strategy decks; it will be won by the firms that have the best-orchestrated intelligence.For reference here is the output from a run:Strategic Brief: EGFR-Targeted Glioblastoma Therapeutic Using Bee Byproducts and Smart Nanoparticle Delivery1. Summary of the Proposed Therapeutic The proposed therapeutic is an innovative, multi-modal strategy targeting Epidermal Growth Factor Receptor (EGFR), a central oncogenic driver in glioblastoma, utilizing bioactive small molecule inhibitors inspired by compounds found in bee byproducts such as propolis and bee venom. These inhibitors are rationally designed and optimized through advanced AI-driven molecular modeling and generative chemistry loops informed by AlphaFold high-resolution structural predictions of wild-type and mutant EGFR (notably EGFRvIII). Coupled with this molecular design is a sophisticated smart nanoparticle delivery system synthesized from natural bee-derived polymers, engineered for biocompatibility and blood-brain barrier (BBB) penetration. This platform incorporates molecular sensors capable of detecting tumor microenvironmental cues, enabling a closed-loop, reinforcement learning (RL)-based control of therapeutic payload release. This adaptive system dynamically modulates drug delivery in response to tumor-specific biological signals, maximizing efficacy and minimizing unintended cytotoxicity or off-target effects. The approach thus integrates natural product bioactivity, cutting-edge protein structure elucidation, AI-guided drug discovery, and a Tokamak-inspired RL feedback control system for precise, responsive EGFR inhibition within the brain tumor microenvironment.2. Core Scientific Strategy Focus on EGFR, a widely validated molecular hallmark of glioblastoma malignancy and heterogeneity, with specific attention to oncogenic variants such as EGFRvIII that drive ligand-independent receptor activation.Structural Biology & AI Modeling: Employ AlphaFold's diffusion-based AI to generate complete and accurate 3D structures of mutant and wild-type EGFR, including dynamic conformations relevant for ligand binding and allosteric regulation. This structural knowledge facilitates identification of novel druggable pockets and optimizes binding interactions of natural bioactive inhibitors.AI-Driven Drug Discovery: Use a Hamiltonian Learning discovery loop combining a generative proposer agent and a composite scoring function utilizing AlphaFold-modeled EGFR conformations, molecular docking, and estimated binding energies to iteratively generate and select chemically viable, brain-penetrant small molecule EGFR inhibitors inspired by bee byproduct motifs. This accelerates lead identification geared to binding mutant EGFR with specificity and adequate pharmacokinetics.Smart Nanoparticle Delivery System: Develop nanoparticles from bee-derived polymers/lipids for safe BBB crossing, surface-functionalized with EGFR/ tumor-specific ligands to enhance tumor-cell targeting and receptor-mediated uptake; integrate embedded molecular sensors (pH, ROS, MMPs, mutant EGFR conformation markers) for real-time tumor microenvironment monitoring.Closed-Loop Reinforcement Learning Control: Inspired by Tokamak plasma control, deploy an RL-based AI controller receiving continuous nanoparticle sensor inputs to precisely regulate controlled drug release rates via external stimuli (e.g., magnetic induction, ultrasound, or photoactivation). Reward shaping and curriculum learning enable adaptive, stable, and homeostatic maintenance of EGFR pathway suppression while minimizing normal tissue impact.Sequential Development Roadmap: Move from in vitro validations to preclinical in vivo studies and eventually towards clinical-grade, implantable or wearable RL control systems personalized to patient tumor microenvironment data, establishing a precision medicine pipeline.3. Primary Risks and Key Questions (Pragmatist’s Critique)Manufacturability and Scalability:The complex nanoparticle platform integrating natural bee-derived polymers with embedded sensors and surface ligands poses significant manufacturing challenges. Variability inherent to natural polymers may impair batch-to-batch consistency, stability, and reproducibility critical for clinical application.Sophisticated embedding of biosensors and robust, wireless intra-body communication systems for real-time feedback control increase technical complexity and cost, potentially limiting scalability and commercial viability beyond niche or specialized centers.Biological and Clinical Efficacy Risks:Glioblastoma’s intrinsic heterogeneity, dynamic evolution, and disrupted BBB create formidable barriers to uniformly delivering effective EGFR inhibition. The adaptive nanoparticle system must contend with variable tumor cell populations, infiltrative growth patterns, immune microenvironment modulation, and risk of off-target nanoparticle sequestration or clearance.Neurotoxicity and unintended immune or inflammatory responses due to nanoparticle accumulation or sensor/actuator components raise safety concerns, demanding rigorous characterization before clinical advancement.Patient Experience and System Practicality:Implementation will likely require implantation of external or internal AI control units, frequent interaction or calibration, and continuous monitoring, which may increase procedural invasiveness, patient burden, and healthcare resource demands.Risks of system malfunction or control algorithm errors must be mitigated by fail-safe mechanisms, but still create anxiety and complexity that could affect patient compliance and quality of life.Elevated costs and operational complexity compared to existing standards of care may hinder widespread adoption despite potential therapeutic gains.4. Recommendation for Immediate Next Step The priority immediate next step is to demonstrate proof-of-concept of the stimuli-responsive, sensor-integrated nanoparticle delivery platform’s payload release and EGFR inhibition kinetics in vitro using glioblastoma tumor mimetic models. This milestone should focus on:Validating that nanoparticles fabricated from bee-derived polymers can be reliably synthesized with consistent physicochemical properties and functionalized with targeting ligands.Demonstrating embedded molecular sensors can accurately detect relevant tumor microenvironmental cues (pH, ROS, mutant EGFR conformation markers) under controlled conditions.Establishing controlled, stimuli-triggered release of structurally optimized EGFR inhibitors (generated via the AI-driven pipeline) from these nanoparticles, with quantitative correlation to sensor input and drug release profiles.Confirming that released inhibitors effectively suppress EGFR phosphorylation and downstream oncogenic signaling in cultured glioblastoma cell lines expressing EGFRvIII or other relevant mutations.Testing safety parameters such as cytotoxicity toward non-tumor neural cells, nanoparticle stability, and degradation behavior in vitro.This controlled environment will provide critical data on manufacturability feasibility, sensor functionality, delivery efficacy, and safety signals before committing resources to complex in vivo and AI control system integration. Furthermore, successful in vitro validation will inform refinement of nanoparticle design, sensor integration, and RL control algorithm training curricula, de-risking subsequent preclinical development phases. Given the technology’s multidisciplinary complexity, a phased, data-driven approach focusing initially on establishing the core delivery and sensing platform’s functional viability offers the best pragmatic pathway to realize transformational glioblastoma therapy. This strategic brief synthesizes an ambitious, pioneering therapeutic paradigm for glioblastoma that leverages (1) targeted molecular design against EGFR informed by cutting-edge AI structural biology, (2) natural product-derived inhibitory compounds, and (3) a biologically intelligent nanoparticle delivery system orchestrated via reinforcement learning. While the high innovation potential is compelling for addressing glioblastoma resistance and heterogeneity, significant challenges remain in manufacturability, clinical translation feasibility, safety, and patient-centered deployment. Focused, stepwise validation beginning with in vitro demonstration of the core adaptive nanoparticle platform’s functionality and EGFR inhibitory effect stands as the most critical and realistic immediate next step toward eventual clinical impact.]]></content:encoded></item><item><title>Ex-Intel CEO&apos;s Mission To Build a Christian AI</title><link>https://slashdot.org/story/25/10/29/225246/ex-intel-ceos-mission-to-build-a-christian-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Guardian: In March, three months after being forced out of his position as the CEO of Intel and sued by shareholders, Patrick Gelsinger took the reins at Gloo, a technology company made for what he calls the "faith ecosystem" -- think Salesforce for churches, plus chatbots and AI assistants for automating pastoral work and ministry support. [...] Now Gloo's executive chair and head of technology (who's largely free of the shareholder suit), Gelsinger has made it a core mission to soft-power advance the company's Christian principles in Silicon Valley, the halls of Congress and beyond, armed with a fundraised war chest of $110 million. His call to action is also a pitch for AI aligned with Christian values: tech products like those built by Gloo, many of which are built on top of existing large language models, but adjusted to reflect users' theological beliefs.
 
"My life mission has been [to] work on a piece of technology that would improve the quality of life of every human on the planet and hasten the coming of Christ's return," he said. Gloo says it serves "over 140,000 faith, ministry and non-profit leaders". Though its intended customers are not the same, Gloo's user base pales in comparison with those of AI industry titans: about 800 million active users rely on ChatGPT every week, not to mention Claude, Grok and others.
 
[...] Gelsinger wants faith to suffuse AI. He has also spearheaded Gloo's Flourishing AI initiative, which evaluates leading large language models' effects on human welfare across seven variables -- in essence gauging whether they are a force for good and for users' religious lives. It's a system adapted from a Harvard research initiative, the Human Flourishing Program. Models like Grok 3, DeepSeek-R1 and GPT-4.1 earn high marks, 81 out of 100 on average, when it comes to helping users through financial questions, but underperform, about 35 out of 100, when it comes to "Faith," or the ability, according to Gloo's metrics, to successfully support users' spiritual growth. Gloo's initiative has yet to visibly attract Silicon Valley's attention. A Gloo spokesperson said the company is "starting to engage" with prominent AI companies. "I want Zuck to care," Gelsinger said.]]></content:encoded></item><item><title>UK Woman Threatens Trademark Legal Action Against Cookbook Over ‘Sabzi’</title><link>https://www.techdirt.com/2025/10/29/uk-woman-threatens-trademark-legal-action-against-cookbook-over-sabzi/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Thu, 30 Oct 2025 03:03:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[It’s sort of funny in a way to see how ownership culture has specifically invaded the realm of the culinary arts. If ever there was a place for cultural fusion and an openness culture, it surely should be in cooking. And, yet, we have seen many instances of businesses and/or people attempting to trademark generic names for foods. Believe it or not, we had to have a decades long trademark dispute over “pretzel crisps“, for instance. Someone at one point attempted to trademark the term “breakfast burrito“. A couple in the UK did likewise with “Pho” before eventually surrendering that mark under public pressure.That last example is perhaps the best to lead into this current discussion, given the ethnic nature of the term and its use in the UK. Once more in the UK, this time it’s the owner of a deli that is threatening legal action on publisher Bloomsbury for releasing a cookbook with a title for which she has a trademark.Kate Attlee, the founder of Sabzi, says Bloomsbury has “refused requests” to change the name of one of their recently published cookery titles, which she claims has used her deli’s brand that is trademarked. Sabzi has also been publishing its recipes for free to its 5,000 newsletter subscribers and on its website and social media since 2023, and Ms Attlee had been planning to publish an eponymous book collating and building on this collection.However, in July Bloomsbury published a book of vegetarian recipe by author Yasmin Khan’s book under the name Sabzi – something that Ms Atlee claims is an “infringement of her intellectual property rights.”Now, perhaps you’re like me and wondering, at first blush, why in the world this is even a dispute. The problem is that “sabzi” isn’t some fanciful made-up brand name. The reason Khan’s book is titled  is because it’s a vegetarian cookbook with heavy Persian influences. Sabzi is a Persian word that translates roughly to “herbs” or “vegetables” depending on whom you ask. It’s also a term that is used to name all kinds of Persian dishes. Ghormeh Sabzi is an Iranian stew (and looks freaking amazing). Sabzi Bhaji is a vegetable curry dish (and also looks amazing). And this is a picture of Kuku Sabzi, an herbed fritatta.Looks good, right? Want to know where I got that from? It’s from Kate Attlee’s own website. She is threatening to sue or perform some other retaliatory action over a trademark she has that is the name of a food. This would be as if a restaurant in America got a trademark on “herb-encrusted” and sued other restaurants and/or cookbooks that referenced “herb-encrusted salmon” and the like.It’s nonsense. The outcome here should be exactly the same as with the “pho” example. This trademark should be undone one way or the other. It would be fantastic if Attlee realized this and voluntarily relinquished it herself.But in lieu of that, hopefully the courts can do it for her, should legal action actually come to be.]]></content:encoded></item><item><title>India’s Snabbit valuation doubled to $180M in 5 months on its quick house-help bet</title><link>https://techcrunch.com/2025/10/29/indias-snabbit-valuation-doubled-to-180m-in-5-months-on-its-quick-house-help-bet/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Thu, 30 Oct 2025 01:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[India's appetite for instant convenience — once confined to food and grocery delivery — is expanding into house help. ]]></content:encoded></item><item><title>New China Law Fines Influencers If They Discuss &apos;Serious&apos; Topics Without a Degree</title><link>https://slashdot.org/story/25/10/29/2223209/new-china-law-fines-influencers-if-they-discuss-serious-topics-without-a-degree?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[schwit1 shares a report from IOL: China has enacted a new law regulating social media influencers, requiring them to hold verified professional qualifications before posting content on sensitive topics such as medicine, law, education, and finance, IOL reported. The new law went into effect on Saturday. The regulation was introduced by the Cyberspace Administration of China (CAC) as part of its broader effort to curb misinformation online.
 
Under the new rules, influencers must prove their expertise through recognized degrees, certifications, or licenses before discussing regulated subjects. Major platforms such as Douyin (China's TikTok), Bilibili, and Weibo are now responsible for verifying influencer credentials and ensuring that content includes clear citations, disclaimers, and transparency about sources. A separate report notes that if influencers are caught talking about the "serious" topics, they will face a fine of up to 100,000 yuan ($14,000).]]></content:encoded></item><item><title>Cluely’s Roy Lee on the rage-bait strategy for startup marketing</title><link>https://techcrunch.com/2025/10/29/cluelys-roy-lee-on-the-ragebait-strategy-for-startup-marketing/</link><author>Russell Brandom</author><category>tech</category><pubDate>Thu, 30 Oct 2025 00:58:26 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Cluely's Roy Lee has a message for startup founders: You should be thinking harder about how to go viral.]]></content:encoded></item><item><title>SUSE Linux Enterprise Server 16 Becomes First Enterprise Linux With Built-In Agentic AI</title><link>https://linux.slashdot.org/story/25/10/29/2211231/suse-linux-enterprise-server-16-becomes-first-enterprise-linux-with-built-in-agentic-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli shares a report from NERDS.xyz: SUSE is making headlines with the release of SUSE Linux Enterprise Server 16, the first enterprise Linux distribution to integrate agentic AI directly into the operating system. It uses the Model Context Protocol (MCP) to securely connect AI models with data sources while maintaining provider freedom. This gives organizations the ability to run AI-driven automation without relying on a single ecosystem. With a 16-year lifecycle, reproducible builds, instant rollback capabilities, and post-2038 readiness, SLES 16 also doubles down on long-term reliability and transparency.
 
For enterprises, this launch marks a clear step toward embedding intelligence at the infrastructure level. The system can now perform AI-assisted administration via Cockpit or the command line, potentially cutting downtime and operational costs. SUSE's timing might feel late given the AI boom, but its implementation appears deliberate -- balancing innovation with the stability enterprises demand. It's likely to pressure Red Hat and Canonical to follow suit, redefining what "AI-ready" means for Linux in corporate environments.]]></content:encoded></item><item><title>US Startup Substrate Announces Chipmaking Tool That It Says Will Rival ASML</title><link>https://slashdot.org/story/25/10/29/2130249/us-startup-substrate-announces-chipmaking-tool-that-it-says-will-rival-asml?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 30 Oct 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Reuters: Substrate, a small U.S. startup, said on Tuesday that it had developed a chipmaking tool capable of competing with the most advanced lithography equipment made by Dutch firm ASML. Substrate's tool is the first step in the startup's ambitious plan to build a U.S.-based contract chip-manufacturing business that would compete with Taiwan's TSMC in making the most advanced AI chips, its CEO James Proud told Reuters in an interview. Proud wants to slash the cost of chipmaking by producing the tools needed much more cheaply than rivals. [...]
 
An engineering feat that has eluded even large companies, lithography needs extreme precision. ASML is the only company in the world that has been able to make at scale the complex tools that use extreme ultraviolet (EUV) to produce patterns on silicon wafer at a high rate of throughput. Substrate said that it has developed a version of lithography that uses X-ray light and is capable of printing features at resolutions that are comparable to the most advanced chipmaking tools made by ASML that cost more than $400 million apiece. The company said it has conducted demonstrations at U.S. National Laboratories and at its facilities in San Francisco. The company provided high resolution images that demonstrate the Substrate tool's capabilities. "This is an opportunity for the U.S. to recapture this market with a homegrown company," Oak Ridge National Laboratory director Stephen Streiffer, an expert on high-energy x-ray beams, said in an interview. "It's a nationally important effort and they know what they're doing."]]></content:encoded></item><item><title>NBA champion Tristan Thompson and World Mobile launch community-owned network Uplift</title><link>https://techcrunch.com/2025/10/29/nba-champion-tristan-thompson-and-world-mobile-launch-community-owned-network-uplift/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 29 Oct 2025 23:39:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[With Uplift, every subscription will contribute to neighborhood-level network expansion, while local hosts that are known as "AirNode operators" will earn a portion of network revenue by providing community coverage.]]></content:encoded></item><item><title>Digg founder Kevin Rose on the need for trusted social communities in the AI era</title><link>https://techcrunch.com/2025/10/29/digg-founder-kevin-rose-on-the-need-for-trusted-social-communities-in-the-ai-era/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 29 Oct 2025 23:28:58 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Now again under Rose's control, the new Digg is creating a place for people to socialize and connect online within communities, similar to Reddit, but it has different ideas about how such a platform should work at a time when bots are nearly indistinguishable from humans.]]></content:encoded></item><item><title>YouTube announces ‘voluntary exit program’ for US staff</title><link>https://techcrunch.com/2025/10/29/youtube-announces-voluntary-exit-program-for-us-staff/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 29 Oct 2025 23:28:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[YouTube CEO Neal Mohan told employees about the program via an internal memo on Wednesday.]]></content:encoded></item><item><title>Nvidia Takes $1 Billion Stake In Nokia</title><link>https://hardware.slashdot.org/story/25/10/29/2114253/nvidia-takes-1-billion-stake-in-nokia?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 29 Oct 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Nvidia is taking a $1 billion stake in Nokia, sending the Finnish telecom giant's shares up 22%. The two companies also struck a partnership to co-develop next-generation 6G and AI-driven networking technology. CNBC reports: The two companies also struck a strategic partnership to work together to develop next-generation 6G cellular technology. Nokia said that it would adapt its 5G and 6G software to run on Nvidia's chips, and will collaborate on networking technology for AI. Nokia said Nvidia would consider incorporating its technology into its future AI infrastructure plans. Nokia, a Finnish company, is best known for its early cellphones, but in recent years, it has primarily been a supplier of 5G cellular equipment to telecom providers.]]></content:encoded></item><item><title>And the winner of Startup Battlefield at Disrupt 2025 is: Glīd</title><link>https://techcrunch.com/2025/10/29/and-the-winner-of-startup-battlefield-at-disrupt-2025-is-glid/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Wed, 29 Oct 2025 23:15:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Glīd, which is trying to streamline cargo container logistics, has beaten out 199 other Startup Battlefield companies to take home $100,000.]]></content:encoded></item><item><title>Solana co-founder Anatoly Yakovenko is a big fan of agentic coding</title><link>https://techcrunch.com/2025/10/29/solana-co-founder-anatoly-yakovenko-is-a-big-fan-of-agentic-coding/</link><author>Russell Brandom</author><category>tech</category><pubDate>Wed, 29 Oct 2025 22:44:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Speaking at TechCrunch Disrupt, Yakovenko said he's become increasingly comfortable taking a back seat in software development tasks.]]></content:encoded></item><item><title>Grammarly Rebrands To &apos;Superhuman,&apos; Launches a New AI Assistant</title><link>https://tech.slashdot.org/story/25/10/29/2110239/grammarly-rebrands-to-superhuman-launches-a-new-ai-assistant?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 29 Oct 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Grammarly is rebranding itself as "Superhuman" following its acquisition of the email client, while keeping its existing product names for now. Along with the rebrand, the company is launching "Superhuman Go," an AI assistant that integrates with tools like Gmail, Jira, and Google Drive to enhance writing and automate productivity tasks. "The assistant can use these connections to do tasks like logging tickets or fetching your availability when you're scheduling a meeting," adds TechCrunch. "Superhuman said it plans to add functionality to enable the assistant to fetch data from sources like CRMs and internal systems to suggest changes to your emails."
 
"Users can try Superhuman Go by turning on a toggle in the Grammarly extension, which will let them connect it to different apps. Users can also try out different agents in the company's agent store, which include a plagiarism checker and a proofreader, launched in August."]]></content:encoded></item><item><title>In Order To Illegally Deport People To El Salvador, Trump Administration Stripped Informants Of Their Protections</title><link>https://www.techdirt.com/2025/10/29/in-order-to-illegally-deport-people-to-el-salvador-trump-administration-stripped-informants-of-their-protections/</link><author>Tim Cushing</author><category>tech</category><pubDate>Wed, 29 Oct 2025 22:33:14 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[You’re just a commodity in Trump’s marketplace of horrific ideas. Sure, criminal informants are seldom the trustworthiest of people, what with their stay-out-of-jail free cards being reliant on their steady production of evidence against other people. But the government does make promises to criminal informants that it’s expected to , not only to fulfill its legal obligations but to prevent informants from being, you know, beaten, tortured, and killed by those they associate with and rat on. But when it’s time to eject as many people with brown skin as possible, all bets are off. If you’re a government informant, maybe it’s time to renege on your own obligations before the government gets you killed. When the Trump government sought to deport hundreds of [checks notes] Venezuelans to El Salvador’s torture prison, “world’s coolest dictator” Nayib Bukele had a favor to ask of his own: the return of nine MS-13 gang members. Secretary of State Marco Rubio, in a March 13phone call with Salvadoran President Nayib Bukele, promised the request would be fulfilled, according to officials familiar with the conversation. But there was one obstacle: Some of the MS-13 members Bukele wanted were “informants” under the protection of the U.S. government, Rubio told him.To deport them to El Salvador, Attorney General Pam Bondi would need to terminate the Justice Department’s arrangements with those men, Rubio said. He assured Bukele that Bondi would complete that process and Washington would hand over the MS-13 leaders.Quite the quid pro quo, stripping people of the protection and safety they’d been guaranteed for the sole purpose of getting the green light for mass deportations of Venezuelan migrants. Well, the sole purpose on the  side of the equation. On the other side, there was a benefit beyond a little more burnishing of Bukele’s “tough on crime” reputation. It was also a key step in hindering an ongoing U.S. investigation into his government’s relationship with MS-13, a gang famous for displays of excessive violence in the United States and elsewhere.Basically, the State Department and the Trump administration offered up these gang members as literal human sacrifices in order to pursue its mass deportation program. Nothing greases the wheels like blood, I guess, and this administration’s collective hands have been covered with the substance since Trump’s inauguration. And, as is always the case when authoritarians engage in human trafficking to further their bigoted ideals, the government spokespeople are there to remind everyone that the ends justify the means: “The Trump Administration’s results speak for themselves,” said Tommy Pigott, a State Department spokesman. “Hardened TdA gang members are back in Venezuela … MS-13 gang members are being prosecuted in the U.S. and El Salvador. And Americans are safer as a result of these incredible efforts.”Neat. I supposed just summarily executing anyone suspected of drug trafficking would probably put a dent in drug trafficking but that’s the sort of thing we just don’t…. hang on a second. I’m sorry. I’m now being told this is  the sort of thing we do, for the first time in our government’s history. My mistake. At least 32 people have been killed in U.S. strikes on alleged drug boats. The Trump administration has said the U.S. is in a “non-international armed conflict” with drug cartels, arguing that the narcotics they smuggle kill tens of thousands of Americans every year, constituting an “armed attack.”“When they’re loaded up with drugs, they’re fair game, and every one of those ships were,” President Trump told reporters last week.Yep. And we’ll never know whether or not these claims have any basis in fact because all of the evidence has been drone-striked to the bottom of the ocean. Instead, we’re just expected to accept the new normal that moves extrajudicial drone strikes from areas of international conflict and into any body of water that might contain boats with Latin/South American citizens in them. Of course, shitting on informants probably doesn’t even raise red flags in the DEA, ATF, CIA, FBI, or any other agency that used to be primarily concerned with actual criminal cases. Most of those resources are now being spent on pursuing people only suspected of  violations of immigration law. If you’re from anywhere south of our border, you’re nothing more than meat puppets for a tyrant and his enablers. ]]></content:encoded></item><item><title>Character.AI To Bar Children Under 18 From Using Its Chatbots</title><link>https://slashdot.org/story/25/10/29/213211/characterai-to-bar-children-under-18-from-using-its-chatbots?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 29 Oct 2025 22:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the New York Times: Character.AI said on Wednesday that it would bar people under 18 from using its chatbots starting late next month, in a sweeping move to address concerns over child safety. The rule will take effect Nov. 25, the company said. To enforce it, Character.AI said, over the next month the company will identify which users are minors and put time limits on their use of the app. Once the measure begins, those users will not be able to converse with the company's chatbots. "We're making a very bold step to say for teen users, chatbots are not the way for entertainment, but there are much better ways to serve them," said Karandeep Anand, Character.AI's chief executive. He said the company also plans to establish an AI safety lab.
 
Last October, a Florida teenager took his own life after interacting for months with Character.AI chatbots imitating fictitious characters from the Game of Thrones. His mother filed a lawsuit against the company, alleging the platform's "dangerous and untested" technology led to his death.]]></content:encoded></item><item><title>AMDGPU With Linux 6.19 Will Support Analog Video Connectors For Old GCN 1.0 GPUs</title><link>https://www.phoronix.com/news/Linux-6.19-AMDGPU-Analog</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 29 Oct 2025 21:48:02 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following last week's initial batch of AMDGPU kernel graphics driver changes intended for Linux 6.19, another round of new AMDGPU / Radeon / AMDKFD material was sent out today to DRM-Next. Notable with this pull is the Display Core "DC" work for analog video connectors as the initiative from one of Valve's contractors for improving the Radeon GCN 1.0 era GPU support with the AMDGPU driver...]]></content:encoded></item><item><title>Acre Launches V2 Platform, Enabling Bitcoin Holders to Earn 14% APY (est.) From Self-Custody</title><link>https://hackernoon.com/acre-launches-v2-platform-enabling-bitcoin-holders-to-earn-14percent-apy-est-from-self-custody?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 29 Oct 2025 21:26:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[New York, NY, October 29th, 2025/Chainwire/--By enabling BTC to earn a sustainable yield right from their Bitcoin wallet, Acre sets a new standard for decentralized, transparent Bitcoin finance., a Bitcoin-first platform that enables BTC holders to compound their coins while maintaining self-custody, today announced the launch of its estimated 14% APY vault, a significant step toward transforming Bitcoin from a passive store of value into a productive asset.Bitcoin holders can participate directly from their wallets, without the complexity of DeFi bridging and without sacrificing self-custody. The  vets all strategies, and all rewards are automatically converted back to native Bitcoin. Bridging (via ), rebalancing, and reinvesting gains are all handled automatically onchain by the protocol. This approach not only empowers users but also brings vital liquidity to protocols and builders reimagining finance built around Bitcoin.Self-Custodial: Users retain full control of their BTC at all times.Sustainable Rewards: Acre vaults employ time-tested, onchain yield sources.BTC Rewards Only: All rewards are paid directly in Bitcoin, no exposure to unfamiliar tokens or chains.Auto-Compounding: Acre automatically reinvests BTC earnings for maximum growth.Acre’s first vault, estimated 14% APY, is curated by , with vault infrastructure provided by , two trusted leaders in DeFi automation and vault management. Previously only available to institutions and high-net-worth individuals, the strategy includes a portfolio of time-tested techniques (liquidity provision, options, L2 staking) with Re7’s industry-best approach to risk management. Each Acre vault must meet strict risk management criteria and undergo review and approval by the Acre Security Council, ensuring robust oversight and transparency. The Council includes executives and members from Lido, Anagram, LedgerPrime, and Threshold. More information can be found in the .  “Today, Bitcoin holders are forced to choose between giving up control to a custodian or navigating all the complexity of DeFi—bridging, vault rotation, rebalancing, and selling off altcoins–often for barely 1% in yield,” said Laura Wallendal, CEO of Acre.“Acre removes that tradeoff by providing a secure, transparent way to earn compounding yield on BTC, without the custodial risk or typical DeFi complexity.”“The team at Acre has taken a comprehensive approach to building a yield platform rooted in transparency, risk management, and strong governance,” said Evgeny Gokhberg, Founder & CIO at Re7 Capital. “Together, we’re advancing institutional DeFi infrastructure, with this launch marking a key step on Ethereum Mainnet and expanding access to BTC yields within DeFi.”“Acre has taken a collaborative approach, giving BTC holders access to potential earning opportunities while maintaining strong transparency and operational safeguards,” Dennis Dinkelmeyer, CEO of Midas. “Responsible partnerships like this are key to building user confidence and supporting the growth of onchain financial products.”According to , 73% of Bitcoin holders are interested in earning yield, but more than 40% would allocate less than 20% of their holdings to BTCFi products due to concerns around trust and complexity. Acre directly addresses this gap by combining transparent onchain infrastructure with oversight through the Acre Security Council. is a Bitcoin-first platform that helps BTC holders compound their bitcoin while maintaining full control of their assets. By connecting bitcoin to decentralized protocols like lending, insurance, and Bitcoin layer 2 networks, Acre creates a seamless way for users to compound their bitcoin without complexity or the risk from centralized custodians.Founded by the team behind projects like Fold, Casa, Thesis, and tBTC, and supported by leaders at Lido, Eigenlayer, Midas and Re7, Acre brings over a decade of Bitcoin expertise with a focus on simplicity and transparency.To learn more about how Acre is compounding bitcoin, users can visit .:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>FCC&apos;s Gomez Slams Move To Revise Broadband Labels as &apos;Anti-Consumer&apos;</title><link>https://tech.slashdot.org/story/25/10/29/1744228/fccs-gomez-slams-move-to-revise-broadband-labels-as-anti-consumer?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 29 Oct 2025 21:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: The FCC adopted a notice of proposed rulemaking (NPRM) to rescind and revise certain rules attached to consumer broadband labels. The measure passed on a two-to-one vote, with Commissioner Anna Gomez, the lone Democrat on the FCC, voting no and calling the notice "one of the most anti-consumer items I have seen." 

The vote was held at the Commission's open meeting for the month of October. As per a draft notice circulated earlier this month, the FCC is looking to roll back several rules, including requirements that service providers read the label to consumers via phone, itemize state and local pass-through fees, and display labels in consumer account portals, among others. Advocates at Public Knowledge urged the Commission to reconsider, saying in a recent filing that "the Commission could create a permission structure for ISPs to continue to act without accountability." 

In her remarks during Tuesday's open meeting, Commissioner Gomez appeared to concur, depicting the move as "anti-consumer" and counter to the goals of Congress. The FCC was mandated via the 2021 Infrastructure Investment and Jobs Act (IIJA) to create rules for implementing consumer broadband labels. After a lengthy rulemaking process and discussions with industry and consumer groups, ISPs were required to start displaying labels in 2024. 

"I typically vote in favor of notices of proposed rulemaking because I believe in asking balanced questions, even on proposals that I dislike, so that we can encourage fruitful and helpful public comment. Answers to tough questions help us strike the right balance so that our rules can both encourage competition and serve consumers. However, the questions posed in this NPRM are so anti-consumer that I could not bring myself to even agree to them," said Gomez. 

Gomez stressed that the notice will harm consumers by enabling ISPs to hide add-on fees and stripping people of their ability to access information in their own language. Moreover, added Gomez, it's unclear why the FCC is doing this. "What adds insult to injury is that the FCC does not even explain why this proposal is necessary. Make it make sense," she added.]]></content:encoded></item><item><title>BitcoinOS $BOS Token Is Live On Binance Alpha &amp; Top Tier CEX Listings, Advancing Institutional BTCfi</title><link>https://hackernoon.com/bitcoinos-$bos-token-is-live-on-binance-alpha-and-top-tier-cex-listings-advancing-institutional-btcfi?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Wed, 29 Oct 2025 21:11:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[London, United Kingdom, October 29th, 2025/Chainwire/--$BOS token to go live both as an ERC-20 on EVM chains and as a CNT on Cardano.Today, , the unifying operating system transforming Bitcoin for digital economies, has officially launched the $BOS token at $200 million FDV, trading is live on Binance Alpha along with Kucoin, Gate, Kraken US, Bitget, MEXC, and PancakeSwap DEX. The $BOS token is positioned to fulfill critical functions, serving as the incentive layer to ensure that the BOS network remains secure, performant and decentralized. While computation and verification happen on Bitcoin, a specialized node network is required to:Generate ZK proofs from computationMonitor the system for fraudulent activitySubmit challenge transactions to Bitcoin when fraud is detectedProvide verification services for non-technical usersBOS aims to maximum value accrual by operating a buy-and-burn mechanism. As the BOS network grows and more chains integrate, more computation will be required due to increase in transactions, resulting in more $BOS token payments. This creates a BTC-native economy where $BOS token holders effectively earn BTC-denominated returns as the network grows. The more activity on BOS, the more BTC flows into buying and burning $BOS tokens, creating deflationary pressure while rewarding network participants.Since inception, BOS has announced integrations with key projects from several ecosystems, notably Cardano, Litecoin, Arbitrum, Mode Network, RISC Zero, Merlin Chain and Nubit. BOS has also demonstrated a series of significant technological innovations that unlocks $2.2 trillion worth of Bitcoin liquidity across ecosystems and institutions. The BOS Tokenomics comprises a total supply of 21 billion tokens, a symbolic nod to Bitcoin’s supply. Distribution of the tokens are as follows:Successful pre-sale and airdrop campaigns were conducted earlier in the year, accounting for 3% of the total token allocation. Those who participated in the pre-sale will be able to claim their tokens when trading begins, followed by other early supporter communities including Cardano and EVM ecosystems.  (BOS) is the first platform enabling programmability on Bitcoin without modifying its base protocol. Through zero-knowledge proof technology, BOS unlocks smart contracts, DeFi applications, and cross-chain interoperability—all secured by Bitcoin’s unmatched network security.candice@espoircommunications.com:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>NPM flooded with malicious packages downloaded more than 86,000 times</title><link>https://arstechnica.com/security/2025/10/npm-flooded-with-malicious-packages-downloaded-more-than-86000-times/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2022/05/caution-tape-1000x648.jpeg" length="" type=""/><pubDate>Wed, 29 Oct 2025 21:04:45 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Attackers are exploiting a major weakness that has allowed them access to the NPM code repository with more than 100 credential-stealing packages since August, mostly without detection.The finding, laid out Wednesday by security firm Koi, brings attention to an NPM practice that allows installed packages to automatically pull down and run unvetted packages from untrusted domains. Koi said a campaign it tracks as PhantomRaven has exploited NPM’s use of “Remote Dynamic Dependencies” to flood NPM with 126 malicious packages that have been downloaded more than 86,000 times. Some 80 of those packages remained available as of Wednesday morning, Koi said.“PhantomRaven demonstrates how sophisticated attackers are getting [better] at exploiting blind spots in traditional security tooling,” Koi’s Oren Yomtov wrote. “Remote Dynamic Dependencies aren’t visible to static analysis.”]]></content:encoded></item><item><title>These Habits Are Undermining Your Leadership Presence: How to Get Rid of Them</title><link>https://hackernoon.com/these-habits-are-undermining-your-leadership-presence-how-to-get-rid-of-them?source=rss</link><author>Vinita Bansal</author><category>tech</category><pubDate>Wed, 29 Oct 2025 20:48:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How do people in the room feel when you’re around? Do they find you grounded, credible, and trustworthy, or do you come across as uncertain, inconsistent, and uncommitted?\
Leadership presence isn’t about charisma, power, or authority. It’s not about speaking the most, showcasing intelligence, or dominating the room to prove you’re in charge. Overcompensating, over-explaining, or trying too hard to appear confident can actually do the opposite—they can make you seem disconnected or even insecure. These behaviors can damage your credibility instead of building it.\
Real presence isn’t loud, showy, or forceful. It's a quiet strength. It’s the steady tone of your voice, the calm of your body language, and the consistency with which you act, listen, and communicate. It’s not defined by your intentions, but the impact you have on the people around you. It’s not what you think you’re projecting—it’s how others experience you. It’s the unspoken authority that draws attention and respect, even when you’re not in charge.“Leadership Presence is the ability to show up in a manner that creates more space and connection, inspires one to follow you because they want to (versus have to), and invites authentic presence in others. A solid leadership presence creates impact without saying a word, evokes courage to engage, empowers others to lead, creates safety in connection, and leaves others feeling better, clearer, and even more intentional themselves — just by being in your presence — all via the simple art of intention and being.Your leadership presence is yours for the designing.It can be cultivated, strengthened, and expanded.It can be positive or negative.And it is all contagious.”\
What undermines leadership presence aren’t the big mistakes or the dramatic failures, but the small, repeated behaviors that play out in everyday interactions. Interrupting without realizing, , reacting defensively, or showing up distracted—these seemingly minor habits send powerful signals that can slowly chip away at how others perceive you. The real challenge? Most leaders don’t recognize these habits in themselves.\
Here are the subtle, but significant leadership habits that can quietly sabotage your presence as a leader—they’re easy to overlook, but hard to hide:Second-Guessing or Seeking ValidationDo you constantly seek validation from others before making a choice? Do you regularly second-guess yourself and delay decisions with the fear of making mistakes?\
Seeking inputs is healthy when done with the intent to invite diverse perspectives, but relying too heavily on others to confirm your choices or being indecisive and uncertain in moments where clarity is needed can make you come across as someone who doesn’t know what they’re doing.\
There’s a fine line between being inclusive and being unsure. Crossing that line by not trusting your own judgment can make others start questioning it, too.\
Subtle ways in which this habit can show up:Using tentative language like “I could be wrong, but…” or “Just my two cents…” or “I’m not sure if this makes sense…” signals self-doubt even when your idea or decision may be solid.Revisiting a decision that was already made by asking for more inputs just because you want to be sure creates confusion and reduces trust in your ability to lead.Hesitating to commit unless someone else nods first signals that you’re unsure of your own stance.Your need for acceptance can make you invisible in this world. Don't let anything stand in the way of the light that shines through this form. Risk being seen in all of your glory.\
People don’t expect leaders to have all the answers or always to be right, but they do expect ownership—showing conviction in their decision, taking action, and adjusting as needed.\
Presence requires owning your perspective, even when there’s ambiguity, things are evolving, and the outcome isn’t guaranteed. Stop outsourcing your confidence—trust your judgment, show up with decisiveness, and be willing to adjust as new insights emerge. Give others a reason to trust your leadership.Invisible in Moments That MatterDo you show up in the moments when your presence is most needed—when your team is looking to you for direction, when you need to have hard conversations or give difficult feedback, or when there’s rising tension and chaos?\
Becoming invisible by fading into the background or staying silent when you need to speak up—nodding in agreement when you disagree, holding back opinions, or avoiding decisions when stakes are high—signals that you’re not equipped or willing to lead under pressure.\
Fear of saying the wrong thing, creating conflict, or being judged can make you disappear—not just physically, but emotionally and intellectually as well. But not being fully present, especially in hard moments, makes people lose trust in your ability to lead.\
Subtle ways in which this habit can show up:Remaining quiet in meetings where a tough decision is being debated or a conflict is emerging can make you appear disengaged or unsure, even if you’re simply trying to stay neutral.Automatically saying, “Whatever the team decides…” or “I trust your judgment, go ahead” without offering your own view can seem passive, especially when your insight or direction is expected.Using vague language like  or  or  in moments that require decisive leadership can leave others feeling unanchored or unsupported.Choosing not to give direct feedback, skipping emotionally charged discussions, or letting performance issues slide because they feel uncomfortable can erode your credibility over time.Allowing a team member, peer, or more dominant voice in the room to always handle communication or tough messaging can signal a reluctance to lead from the front.When challenges arise—tight deadlines, unexpected conflict, or scrutiny—you become quieter, more reserved, or disengaged, rather than stepping up and guiding the team through uncertainty.Sometimes the bravest and most important thing you can do is just show up.\
People don’t demand perfection when things are tough, but they do expect you to speak up when everyone else goes silent, stand steady when others hesitate, and to show up with clarity even when the path is uncertain. That’s when true leadership presence is felt.\
Leadership presence isn’t about being around all the time—it’s about showing up when it counts. Choosing not to say anything at all or becoming invisible in pivotal moments makes you come across as someone who lacks the courage and conviction to lead when it matters the most. Show up; stand beside your team.Do you constantly rush, multitask, or appear visibly distracted—jumping between meetings, glancing at your phone while someone is speaking, scanning the room during discussions, cutting people off to save time, or frequently rescheduling one-on-ones? These may seem like small, often necessary trade-offs when you’re short of time and dealing with a packed schedule—but busyness habit sends a message that others are not worth your time and attention.\
Being busy, scattered, hurried, or mentally elsewhere makes others feel unseen, unheard, and unimportant. People just don’t need your physical presence; they need your undivided attention to feel respected, important, and supported. When you appear too busy, refuse to make eye contact, or give others your undivided focus, you come across as inaccessible and uninterested. This creates a psychological barrier to sharing openly, seeking feedback, or trusting you with their growth.\
Busyness creates a quiet emotional distance—being physically present but mentally checked out erodes trust, connection, and leadership presence. Over time, people stop noticing, stop caring, and stop paying attention even when you’re in the room. Your presence fades—not because you’re not there, but because you’re no longer felt.\
Subtle ways in which this habit can show up:Even a quick look at your phone or smartwatch while someone is speaking signals disinterest, making people feel like they’re competing for your attention.Nodding along without truly listening can create the illusion of attention—until others realize you weren’t really tuned in.When you regularly arrive distracted or leave abruptly, it communicates that the meeting—or the people in it—aren’t your priority.Answering emails or prepping for the next meeting while someone is speaking tells them their time with you is transactional, not relational.Repeating questions that have already been addressed reveals inattentiveness—and subtly signals that you weren’t fully present in the conversation.Tethered to our smartphones, we are too caught up and distracted to take the time necessary to sort through complexity or to locate submerged purpose. In our urgent rush to get "there," we are going everywhere but being nowhere. Far too busy managing with transactive speed, we rarely step back to lead with transformative significance.\
People don’t need a leader who’s always in motion. They need a leader who’s present in the moment—someone who’s willing to slow down to listen, focus, and connect. Someone who makes them feel important—not like an interruption. Someone who’s not rushing from task to task, but pausing to build real connections.\
Slow down when it counts. When you’re truly present, even brief moments can feel like an hour of focused leadership. It’s that feeling that leaves a lasting influence—one that builds leadership presence.Ignore the Human ConnectionDo you prioritize work at the expense of relationships—jumping straight into tasks, skipping over personal check-ins, or communicating in a tone that feels more transactional than human?\
When work takes precedence over relationships and outcomes become the sole focus, it’s easy to overlook how your team is really doing. You may miss signs of burnout, disengagement, or personal struggles—moments that call not for direction, but for empathy and support.\
Valuing productivity over relationships creates a subtle but powerful disconnect—people begin to feel like resources, rather than individuals who matter. They may follow instructions, but they won’t feel truly seen, supported, or motivated. Over time, this erodes the very foundation of leadership presence—trust, relatability, and emotional credibility.\
Subtle ways in which this habit can show up:You avoid small talk, personal conversations, or emotional topics because they feel unproductive or too personal for work.You focus only on what needs to be fixed or improved, without pausing to ask how the person is doing, what they’re struggling with, or what support they might need.Your team only hears from you when there’s a deadline, a request, or a problem. This makes your communication feel transactional rather than relational.Your presence isn’t something you put on and take off like a jacket; it’s something you build with every single interaction.\
People don’t just want to be managed—they want to be seen. They want to be recognized not only for what they contribute, but for who they are. And that kind of recognition doesn’t happen by default—it requires intention, presence, and a genuine interest in the person behind the role.\
Leadership presence depends on relational connections, not routines. It’s not about how efficiently you run a meeting, how many goals you check off, or how many targets you achieve. It’s about how you make them feel when doing those things together. Don’t treat people like an item on a to-do list. Slow down. Look up. Be human. That’s where real presence begins.Speak in Vague, Unclear TermsDo you speak in generalities, avoid specifics, or hesitate to take a clear position? Do you speak in vague, indirect, or overly broad terms?\
Saying things like “Let’s see how it goes,” “We’ll figure it out later,” or  leaves people unsure of where you stand or what’s expected. When people are left to interpret your intent—what you really mean, what’s expected, or where things are headed—it leads to confusion and uncertainty.\
People may nod in meetings, but walk away unsure of what action to take. They may make assumptions, draw conclusions, or decide on the wrong path to take. Without the clarity and confidence they expect from a leader, trust begins to slip and momentum stalls. Presence without clarity soon turns into noise.\
Subtle ways in which this habit can show up:When asked for a decision or opinion, you respond with “That’s a great question…” and then sidestep the actual answer. This can create a perception that you’re hesitant, unclear, or unwilling to take a stand.Leaning on jargon or leadership cliches like “synergy,” “alignment,” “leverage,” or “thinking outside the box” without guiding people in clear actions or directions leads to confusion rather than clarity.Offering input like “Do better,” “Be more strategic,” “Try to tighten this up,” or “Think bigger” without specifics makes it hard for others to act on your feedback or meet your expectations.Using passive voice, “Mistakes were made,” “It got overlooked,” or group-blurring terms “We’ll need to think about…” can dilute ownership and make it unclear who is responsible for what.Communication has power. But as with any form of power, it needs to be harnessed effectively or it can all too often backfire.\
As a leader, your voice carries weight. Use it to bring structure to ambiguity, to simplify complexity, and to help others move forward with confidence. Vague, ambiguous language undermines presence. Even when the path is uncertain, your ability to clearly articulate what matters, what’s next, and what’s true gives others the confidence to follow you.\
Clarity doesn’t require perfection—it requires presence, intention, and the courage to be direct. Speak with purpose. Choose clarity over comfort. Unclear words don’t just blur your message—they blur your leadership.Quick to Solve, Slow to ListenDo you have the tendency to jump to solutions without taking the time to understand what’s really going on or how people actually feel? Do you offer advice before others have the chance to finish explaining or steer the conversation toward action without understanding the context behind what’s being said?\
Hearing a problem, fixing it fast, and moving on can feel efficient—you want to be helpful, you want to remove obstacles, you want to keep things moving. But over-indexing on the problem without paying attention to the person behind it can make them feel overlooked and dismissed.\
This habit can quietly push people away. They may stop coming to you for support because involving you often means losing control, giving up ownership, being talked over, or being forced into a solution before they have a chance to think it through. It’s better to hide mistakes, not share concerns, and continue to stay stuck than risk being overshadowed.\
Subtle ways in which this habit can show up:Your first response is advice or action, rather than curiosity or reflection—cutting short someone else’s thought process.You become uncomfortable with pauses and quickly fill them with your ideas, not realizing that those quiet moments are where others gather their thoughts or build the courage to speak.Instead of asking clarifying questions, you reword or reinterpret what others said based on your perspective—subtly taking control of the narrative.You skip over emotions or context and steer the conversation toward action plans, making people feel like their concerns weren’t fully heard.You often use phrases like “It’s simple…” or “Just do this…” which can sound dismissive or minimizing, especially when the issue feels complex or emotionally loaded for the other person.The purpose of the leader is to make sure there is leadership – which is not the same as having all the answers or leading from the front every time.\
The ability to diagnose issues and take corrective action is valuable—but it shouldn’t be done by hijacking the space and sidelining others. You need to collaborate, not dominate. You need to listen, not speak. You need to amplify others’ intelligence, not put a spotlight on yourself.\
Leadership presence isn’t about taking over—it’s about drawing others in. It’s about creating space to listen deeply, reflect collaboratively, and elevate the thinking in the room—not just your own. Slow down. Ask questions. Stay curious a little longer.Unchecked Emotions on DisplayHow do you show up when pressure is high, expectations aren’t met, or communication breaks down? Is there visible tension in your voice, a hint of disappointment in your tone, or impatience in your body language?\
Unchecked emotions like irritation, sarcasm, defensiveness, or emotional withdrawal shake people’s confidence in your leadership. People start worrying about saying or doing things that might trigger an emotional outburst. This makes them hide mistakes, choose words carefully, and play safe as they try to stay out of your way.\
Whether it’s showing frustration in a meeting that disregards your views, aggressive tone in an email that challenges your authority, raising your voice when someone disagrees with you, or passive-aggressive behavior when things don’t go well, these moments don’t just pass away—they linger in the minds of those around you. Your presence becomes associated with emotional reactivity, making people hesitant to lean on you.\
Subtle ways in which this habit can show up:A minor mistake triggers disproportionate anger, disappointment, or criticism—making people feel unsafe to experiment or admit failure.Your tone is dismissive, cold, irritated, or sarcastic—leaving people unsure whether they’re safe to continue.You express frustration with a sigh, eye-roll, smirk, clenched jaw, or visibly tightening posture when someone speaks.You become rigid, justify quickly, or shut down disagreement—often driven by ego or fear.You turn distant, unresponsive, or passive-aggressive when things don’t go your way—silently signaling disapproval without addressing it directly.Big emotions—like anger, fear, and sadness—can be really uncomfortable. But even uncomfortable feelings are okay. In fact, all emotions are okay. It just takes practice to manage uncomfortable emotions so you can respond in a healthy way.\
People notice how you show up—can you stay calm in pressure-filled moments, stay grounded in discomfort, and manage your emotions when things get tough? The steadier you are in difficult moments, the more confident others feel in your leadership.\
Leadership presence is not about suppressing emotions—it’s about regulating them. It means acknowledging them without letting them drive your behavior. Consciously create space between stimulus and response. Respond with intention rather than impulse.Constantly doubting yourself or waiting for others to approve your decisions makes you come across as weak and indecisive. Building leadership presence requires trusting your judgment, even when the path ahead is ambiguous, uncertain, or the outcome isn’t guaranteed—be willing to take a stand and course-correct as needed.Withdrawing during conflict or tough situations tells people they can’t trust you for guidance and support. Leadership presence is not built by hiding, but by showing up when it matters the most.When you rush, multitask, or appear distracted, people in the team feel unseen and unimportant. Being physically present isn’t enough—your full attention is what builds leadership presence.Focusing only on tasks while not taking the time to connect with people makes them feel like tools to get the job done—not individuals who matter. Leadership presence isn’t just about driving outcomes—it’s about seeing, hearing, and understanding the humans behind the work. Acknowledge people, show empathy, and invest in building relationships.Using vague, ambiguous language leads to confusion, misunderstandings, and rework. Clear, direct communication enhances performance and productivity, which makes your leadership stand out and your presence felt.Jumping to solutions without listening can make others feel unheard and dismissed. Leadership presence requires pausing, listening, and showing interest in what others have to say.Visible frustration, defensiveness, or emotional outbursts unsettle teams. Leadership presence requires emotional steadiness, especially in high-pressure moments.This story was previously published here. Follow me on LinkedIn or here for more stories.]]></content:encoded></item><item><title>Windows is the Problem With Windows Handhelds</title><link>https://games.slashdot.org/story/25/10/29/1739232/windows-is-the-problem-with-windows-handhelds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 29 Oct 2025 20:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft shipped its first Xbox handheld nearly two weeks ago. The $600 white Xbox Ally cannot reliably sleep, wake, or hold a charge while asleep. Neither Microsoft nor Asus would admit there's a problem or offer a timeline to fix it after repeated requests by The Verge. Asus said it needs more time to test. 

Installing Bazzite, a Linux-based operating system, solves the problems, the publication reports. The same hardware runs games up to 30% faster than Windows and beats the Steam Deck in all but one benchmark. Steam runs more responsively without Windows bloat. The device can be used like a Nintendo Switch, pausing games with the power button and resuming hours or days later. Bazzite initially had sleep issues but fixed them two days after programmer Antheas Kapenekakis obtained the hardware and consulted with two AMD contacts. The black Xbox Ally X, which doesn't have as many sleep issues, gets a similar speed boost with Bazzite. 

Two Xbox Ally units tested on Windows repeatedly woke themselves at random intervals. One lost 10% battery after 12 hours of supposed sleep, the other 23%. After another 12 hours, both had only 30% battery remaining. One tried to apply a Windows Update while asleep. Both units refused to wake from sleep at times and required hard resets. Many users have reported similar issues on Reddit with both Xbox Ally versions. 

Further reading: Microsoft's Next Xbox Will Run Full Windows and Eliminate Multiplayer Paywall, Report Says.]]></content:encoded></item><item><title>To Infinity… and Delete</title><link>https://hackernoon.com/to-infinity-and-delete?source=rss</link><author>Sup3rN3rd</author><category>tech</category><pubDate>Wed, 29 Oct 2025 20:34:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In 1998, disaster struck at Pixar. A single mistyped command — rm -rf / — began erasing Toy Story 2 from existence. Character by character, scene by scene, the movie that had taken a year to build vanished in seconds. The team watched in disbelief as Woody’s hat, Buzz’s wings, and entire sets disappeared before their eyes. When engineers rushed to restore from backups, they discovered something worse — the backup system had quietly failed weeks earlier. As IT professionals, we have all been there before, but what can we learn from this and get Buzz to his ship on time?This “Core memory” took place in 1998, with Pixar co-founder Ed Catmull remembering it in his book called “Creativity, Inc.”. The story begins with an unfortunate, unnamed Pixar employee who was doing some routine file clearance on internal servers when they accidentally entered a deletion command on Toy Story 2's root folder…That’s some good news. This “Updating your resume event” resulted in character models and assets disappearing, and the file servers were quickly shut down. \
Unfortunately, by that point, around 90% of the work done on Toy Story 2 was gone, and the sequel's backup system was not working properly for around a month either. At this point, Toy Story 2 would either have to start from scratch - or production would be scrapped altogether.A mother saves the day, just like when Buzz and Woody team up to get home. Galyn Susman, the film’s technical direction supervisor, who would be affected by Disney’s layoffs in 2023, had a copy of the Toy Story project at home. Galyn was on maternity leave and decided to continue working from home – something that is seen as normal today - but at the time, taboo. Being a mother and always planning ahead, just like having children, made it a point to take her work home once a week. This was a huge benefit because it allowed her to stay updated and maintain a reliable backup of Toy Story 2.\
Just like a newborn baby, Pixar carefully transported the laptop back to the office, cradled and wrapped in blankets during the car ride - I imagine they even played lullaby music for the laptop…or maybe that is something I would do. Having the backup from Susman’s laptop allowed the team to copy the files and recover nearly everything that had been lost. \
It was a joyous occasion with many high fives, and maybe put a smile on the face of the person responsible for the deletion. Susman’s backup copy didn’t have the entire movie on her computer, but they were able to retrieve enough to complete and deliver Toy Story 2 on time. Queue the inspirational music and dance like nobody is watching. What a story, right? \
What about the employee who deleted the files? I am glad you are paying attention. So far, there are no reports of them being fired or facing consequences. I will say it’s easy to imagine the tension at the time, and maybe a future project with them working on the backup process.The experience serves as a valuable lesson, not just for the Pixar folks but for IT professionals worldwide. There is a strong commitment to create multiple backups and implement extra security measures to prevent such incidents from happening again. \
In this story, the backup system had failed months earlier, and nobody noticed. That meant there weren’t any backups to restore from, and business was at a standstill. Does that sound familiar to today’s events? It should because it happens a lot these days. What can businesses do to keep safe from this disaster?The 3-2-1 rule - data backup rule is a strategy that recommends keeping three copies of your data, on two different types of storage media, with one copy stored offsite. This method ensures redundancy and protects data from a single point of failure, such as hardware failure, theft, or a local disaster.Offsite backups - An offsite, air-gapped data backup stores a copy of your data in a separate physical or cloud location (offsite) and keeps it disconnected from your primary network (air-gapped). This combination protects your data from localized disasters and cyber threats like ransomware, which cannot remotely access or corrupt the air-gapped backup copy.RPO & RTO - Recovery Point Objective and Recovery Time Objective. It’s not just important, but vital to your business continuity and survival in the event of a disaster. Most businesses state that they have backups tested and that pass the audits, but when they have to restore their systems when a disaster happens, it takes a lot longer than they had planned, and the business loses money because of it.Technical Controls and Permissions: Restricted Folder Deletion Privileges.The simplest prevention would have been to set permissions on the server so that not all employees could delete the top-level directory for the movie. Granting "full control" access to a large group of users is common in collaborative environments, but it is a major security risk. Only a small number of administrators should have the permission to run "delete" commands on critical, high-level folders.Command-level restrictions. The employee used the rm -r Linux command, which deletes a directory and all its contents recursively. A more advanced system could have prevented this command from running at the highest project directory level, either with a special script or by requiring a second authentication step.]]></content:encoded></item><item><title>The Kavanaugh Stop’s Legacy: 50 Days, 170+ Detained Citizens, Zero Answers</title><link>https://www.techdirt.com/2025/10/29/the-kavanaugh-stops-legacy-50-days-170-detained-citizens-zero-answers/</link><author>Mike Masnick</author><category>tech</category><pubDate>Wed, 29 Oct 2025 20:33:14 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[It was just last month that Brett Kavanaugh gave his explanation for why it was perfectly okay for Homeland Security goons to profile brown people and detain them based on nothing more than the color of their skin. While his cowardly colleagues in the majority on that shadow docket decision refused to explain their thinking, Kavanaugh actually wrote a concurrence that was so out of touch with reality as to be embarrassing. But at least it was an explanation.The key bit from him that has stood out is this:Importantly, reasonable suspicion means only that immigration officers may briefly stop the individual and inquire about immigration status.If the person is a U. S. citizen or otherwise lawfully in the United States, that individual will be free to go after the brief encounter. Only if the person is illegally in the United States may the stop lead to further immigration proceedings.It’s this weird, privileged, out-of-touch statement that if ICE or CBP stop you for being brown, they’ll let you go as soon as you show them that you’re an American citizen. Of course, we knew at the time that wasn’t true. Hell, there were details that Kavanaugh ignored in that very lawsuit, which Justice Sotomayor called out in her dissent. But literally in this very lawsuit was the documentation of how it wasn’t so simple:To give just one example,Plaintiff Jason Brian Gavidia is a U.S. citizen who was born and raised in East Los Angelesand identifies as Latino. On the afternoon of June 12, he stepped onto the sidewalk outside of a tow yard in Montebello, California, where he saw agents carrying handguns and military-style rifles. One agent ordered him to “Stop right there” while another “ran towards [him].”The agents repeatedly asked Gavidia whether he is American—and they repeatedly ignored his answer: “I am an American.”The agents asked Gavidia what hospital he was born in—and he explained that he did not know which hospital. “The agents forcefully pushed [Gavidia] up against the metal gated fence, put [his] hands behind [his] back, and twisted [his] arm.” An agent asked again, “What hospital were you born in?” Gavidia again explained that he did not know which hospital and said “East L.A.”He then told the agents he could show them his Real ID. The agents took Gavidia’s ID and his phone and kept his phone for 20 minutes. They never returned his ID.Drexel law professor Anil Kalhan quickly dubbed these bullshit pretextual stops of US citizens as “Kavanaugh stops” and the name has stuck.It feels like every day we hear about another few:ICE violently detain father & son walking to school—teenage boy had to be rushed to hospital."I was just going to school," kid cries out. "I'm underage!"The 16-year-old star athlete is a U.S. citizen—agents sent him to the hospital with severe injuries to his back & neck.Houston, Texas.These Kavanaugh stops are a stain on the American concept of civil liberties and due process, and they should be a stain on Brett Kavanaugh’s legacy. Legal journalist Chris Geidner just ran a piece on 50 days of Kavanaugh stops, and what a shameful moment this is of American bigotry.Geidner has directly submitted questions to Kavanaugh to see how he feels about all of these Kavanaugh stops that show his claim of “brief encounters” with law enforcement were bullshit:I asked Justice Kavanaugh on October 14, “Do you have any comment on the ICE stop of Maria Greeley, a U.S. citizen, who was reportedly stopped, ziptied, and told she didn’t ‘look like’ a ‘Greeley’ despite being a U.S. citizen?“On both occasions, I also asked Kavanaugh whether he still thinks he was correct when he wrote that these stops are “typically brief” and that all of this is fine because “individuals may promptly go free after making clear to the immigration officers that they are U. S. citizens or otherwise legally in the United States.”Finally, I asked Kavanaugh if he was aware of the “Kavanaugh stop” terminology and whether he had any comment on it.So, I asked Justice Kavanaugh on October 16, “Do you have any comment on the Pro Publica report that found ‘more than 50 Americans who were held after [immigration] agents questioned their citizenship’ during 2025. ‘They were almost all Latino,’ per the report.“In addition to the other questions previously raised, I also asked Kavanaugh whether “the possibility of after-the-fact ‘excessive force’ claims” is “a sufficient answer to this ongoing, regularly occurring problem?”Did you guess what happened? Of course you did!I have not received a response from him or his chambers.You can already see the horrific legacy that is forming around the concept of Kavanaugh stops. This is a legacy that doesn’t go away easily. It’s like the Dred Scott decision, the Korematsu decision, or Buck v. Bell. Supreme Court decisions that nearly everyone now looks back on in horror.These are all horrible, hateful decisions by out-of-touch bigots, who can’t even fathom a world in which those less fortunate themselves even matter, and thus their rights and dignity are barely given a second thought.The Supreme Court still has a chance to fix this, since Kavanaugh stops were only defined by Justice Kavanaugh in a shadow docket concurrence. While those other cases all took decades for everyone to realize how fucked up they were, this one we can see in real time what a stain it is for anyone who believes that America respects basic civil liberties like due process and concepts like probable cause.But, for now at least, that stain should stick to Brett Kavanaugh. He’s justified this. He’s insisted these kinds of stops are no big deal, even as there was evidence then, and even with more mounting evidence now, that immigration officials don’t give a shit if you are an American citizen. If you’re darker skinned, they can treat you like shit, lock you up, beat you up, ignore your protestations and even evidence of American citizenship.It is a deep, dark stain on America as a supposed land of freedom, and it should be tied up with Brett Kavanaugh’s legacy forever.]]></content:encoded></item><item><title>2024’s Startup Battlefield runner-up geCKo Materials reveals four new products at TechCrunch Disrupt</title><link>https://techcrunch.com/2025/10/29/2024s-startup-battlefield-runner-up-gecko-materials-reveals-four-new-products-at-techcrunch-disrupt/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Wed, 29 Oct 2025 20:06:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[geCKo Materials returned to the stage at TechCrunch Disrupt to debut new products as it pushes deeper into commercializing its tech.]]></content:encoded></item><item><title>US Needs &apos;Finesse&apos; to Stay Ahead of China, Nvidia Boss Says</title><link>https://news.slashdot.org/story/25/10/29/1722259/us-needs-finesse-to-stay-ahead-of-china-nvidia-boss-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 29 Oct 2025 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Nvidia chief executive Jensen Huang said that maintaining the US edge in AI will require a steady approach that ensures China remains hooked on American technology. From a report: The chipmaker is in an "awkward place" as President Donald Trump prepares to meet with his Chinese counterpart Xi Jinping later this week, Huang told reporters Tuesday at a company conference in Washington. The Nvidia chief praised Trump's commitment to winning but urged careful engagement with China because of the country's massive software developer base and its growing technology capabilities. 

During the meeting, Trump and Xi are expected to finalize an agreement to ease trade tensions between the world's two largest economies. When it comes to those negotiations, Huang said he has "no idea" if GPUs -- the chips central to artificial intelligence capabilities -- will be a topic between Trump and Xi. 

Huang was careful to leave the negotiating to Trump but encouraged US leadership to think longer term on its overall AI strategy. "A policy that causes America to lose half of the world's developers is not beneficial long-term," Huang said, warning that it was still possible for the US to cede the AI race to China. Keeping US technology in front requires finesse," he said. "It requires balance. It requires long-term thinking."]]></content:encoded></item><item><title>Google Chrome Will Finally Default To Secure HTTPS Connections Starting in April</title><link>https://tech.slashdot.org/story/25/10/29/1715202/google-chrome-will-finally-default-to-secure-https-connections-starting-in-april?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 29 Oct 2025 19:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: The transition to the more-secure HTTPS web protocol has plateaued, according to Google. As of 2020, 95 to 99 percent of navigations in Chrome use HTTPS. To help make it safer for users to click on links, Chrome will enable a setting called Always Use Secure Connections for public sites for all users by default. This will happen in October 2026 with the release of Chrome 154. 

The change will happen earlier for those who have switched on Enhanced Safe Browsing protections in Chrome. Google will enable Always Use Secure Connections by default in April when Chrome 147 drops. When this setting is on, Chrome will ask for your permission before it first accesses a public website that doesn't use HTTPS.]]></content:encoded></item><item><title>Innovate NY Backs Andrew Cuomo for Mayor; Championing Public Benefit Stablecoins With $779 Million</title><link>https://hackernoon.com/innovate-ny-backs-andrew-cuomo-for-mayor-championing-public-benefit-stablecoins-with-$779-million?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Wed, 29 Oct 2025 19:12:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[NEW YORK CITY – The independent political organization Innovate NY PAC today announced its endorsement of Andrew Cuomo for Mayor of New York City.This endorsement reflects a shared commitment to a forward-looking economic agenda that leverages blockchain, tokenization, public-benefit stablecoins, and artificial intelligence to generate new revenue streams, enhance civic services, and position New York City as the global capital of innovation and finance.“We believe Andrew Cuomo brings the leadership, experience, and vision to make New York City the global leader in innovation,” said Eddie Cullen, Chair of Innovate NY PAC. “Under his leadership, New York can once again become the city that drives opportunity for working families, entrepreneurs, and innovators alike.”Our Policy Agenda for the Next Innovation EraPublic Benefit Stablecoins \n Enabling projects like, the first stablecoin model that uses its yield to fund affordability programs, education, and civic services - proving technology can work for the people, not bureaucracies. \n City Impact Tokens \n Supporting initiatives such as the NYC Token™, which channels blockchain technology to drive new city revenue without additional taxes - funding affordable housing, infrastructure, and civic innovation through tokenized impact models. \n  \n Civic Real-World Tokenization \n Promoting efforts such as, which harness blockchain to promote binational collaboration, clean-energy zones, and economic growth. These projects show how innovation can help solve real issues like immigration through revenue-focused, cooperative models. By pioneering these civic tokenization frameworks, New York City positions itself as the national leader in applying advanced technology to global challenges - opening new markets for New York-based fintech, construction, and clean-energy firms, attracting investment capital, and creating high-skilled jobs in blockchain engineering, infrastructure, and policy development. In short, every breakthrough that begins at the border strengthens New York’s standing as the financial and technological capital of the world. \n  \n Cross-Border STEM Collaboration \n Linking New York City and Africa through digital education, talent exchange, and research programs - building the foundation for global equity in innovation and expanding New York’s role as a global education and tech hub. \n  \n AI-Powered Smart Tariffs \n Exploring AI-driven tariff systems that dynamically adjust based on real-time economic data, maximizing U.S. and NYC revenue while strengthening trade and promoting global fairness.New York City stands at a crossroads. Traditional economic models can no longer keep pace with global innovation. By adopting blockchain and AI-driven finance tools, the City can:Generate new public revenue without raising taxes \n Empower small businesses and families with modern financial tools \n Rebuild global investor confidence in New York \n Lead the United States into the next generation of digital prosperityInnovate NY PAC is an independent expenditure committee dedicated to advancing innovation, technology-led economic growth, and public-benefit finance models in New York City. We operate independently of any candidate or campaign, and coordinate no communications or strategies with candidates or their committees.Analysis prepared by Innovate NY PAC Policy & Research Team, illustrating how public-benefit stablecoin yield could be distributed to support city affordability.Paid for by Innovate NY. CEO: Edward Cullen. Top Two Donors: Angel 501 LLC and Put NYC First, Inc.  Not expressly or otherwise authorized or requested by any candidate or the candidate’s committee or agent.  More information at nyc.gov/FollowTheMoney.:::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item></channel></rss>
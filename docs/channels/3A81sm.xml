<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech</title><link>https://konrad.website/feeds/</link><description></description><item><title>AMDGPU Patches Updated For HDMI Gaming Features On Linux With Radeon Graphics</title><link>https://www.phoronix.com/news/AMDGPU-HDMI-Gaming-v2</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 26 Jan 2026 11:20:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A patch series posted last week for the open-source AMDGPU kernel driver implements HDMI Variable Rate Refresh "VRR" and other gaming features for HDMI displays. With the HDMI Forum blocking HDMI 2.1 open-source support, these HDMI gaming features for the AMDGPU driver were developed via trial-and-error and the limited public knowledge available. A second iteration of these patches are now available for testing...]]></content:encoded></item><item><title>LG Gram Style 14 Laptop To See Working Speaker Support With Linux 7.0</title><link>https://www.phoronix.com/news/LG-Gram-Style-14-Linux-Speakers</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 26 Jan 2026 10:52:51 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For the Intel-powered LG Gram Style 14 laptop one of the Linux support caveats is the internal speakers not working properly under Linux, but with a patch expected for the upcoming Linux 6.20~7.0 kernel cycle it will finally fix the laptop speaker support for one of the laptop models in this series...]]></content:encoded></item><item><title>ASRock Rack PAUL PCIe IPMI Card Sees DT Patches For The Mainline Linux Kernel</title><link>https://www.phoronix.com/news/ASRock-Rack-PAUL-Linux-Patches</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 26 Jan 2026 10:38:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[ASRock Rack's PAUL is a low-profile PCIe IPMI card built around the widely-used ASPEED AST2500 controller for providing IPMI/BMC capabilities for any platform. New patches provide mainline Linux kernel support for ASRock Rack PAUL with the necessary Device Tree bits...]]></content:encoded></item><item><title>New Patches Aim To Lower Linux Memory Use For Swap, Slightly Improve Performance</title><link>https://www.phoronix.com/news/Linux-Better-Swap-Tencent</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 26 Jan 2026 10:27:50 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Kairui Song of Tencent sent out a new patch series overnight working on enhancing the Linux kernel's swap code. With the patches there are some memory savings -- and more on the way -- while also providing for slightly faster performance...]]></content:encoded></item><item><title>Is Google Prioritizing YouTube and X Over News Publishers on Discover?</title><link>https://news.slashdot.org/story/26/01/26/008257/is-google-prioritizing-youtube-and-x-over-news-publishers-on-discover?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 26 Jan 2026 08:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Earlier this month, the media site Press Gazette reported that now Google "is increasingly prioritising AI summaries, X posts and Youtube videos" on its "Discover" feed (which appears on the leftmost homescreen page of many Android phones and the Google app's homepage). 

"The changes could be devastating for publishers who rely heavily on Discover for referral traffic. And it looks set to accelerate a global trend of declining traffic to publishers from both Google search and Discover."

Xavi Beumala from website analytics platform Marfeel warned in a research update: "Google Discover is no longer a publisher-first surface. It's becoming an AI platform with YouTube and X absorbing real estate that once went to newsrooms..." [They warn later that "This is not a marginal UI experiment. It is a reallocation of feed real estate away from links and toward inline Youtube plays and generated summaries."] Google says it prioritises "helpful, reliable, people-first content". Unlike Google News, there is no requirement that Google Discover showcases bona fide publisher websites. 

In recent months fake news stories published by fraudulent website publishers have been promoted on Google Discover, reaping tens of millions of clicks. Google said it was working on a "fix" for this issue... 

Facebook, Instagram and Tiktok content may also start flowing into the Discover feed in future. When Google announced the addition of posts from X, Instagram and Youtube Shorts in September, it said there would be "more platforms to come".]]></content:encoded></item><item><title>Startup Uses SpaceX Tech to Cool Data Centers With Less Power and No Water</title><link>https://hardware.slashdot.org/story/26/01/26/0317225/startup-uses-spacex-tech-to-cool-data-centers-with-less-power-and-no-water?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 26 Jan 2026 05:44:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[California-based Karman Industries "says it has developed a cooling system that uses SpaceX rocket engine technology to rein in the environmental impact of data centers," reports the Los Angeles Times, "chilling them with less space, less power and no water."


Karman has developed a cooling system similar to the heat pumps in the average home, except its pumps use liquid carbon dioxide as refrigerant, which is circulated using rocket engine technology rather than fans. The company's efficient pumps can reduce the space required for data center cooling equipment by 80%. 

Over the years, data centers have used fans and air conditioning to blow cold air on the chips. Bigger facilities pass cold liquid through tubes near the chips to absorb the heat. This hot liquid is sent outside to a cooling yard, where sprawling networks of pipes use as much water as a city of 50,000 people to remove the heat. A 50 megawatt data center also uses enough electricity to power a mid-sized city... Cooling systems account for up to 40% of a data center's power consumption and an average midsized data center consumes more than 35,000 gallons of water per day... 

U.S. data centers will consume about 8% of all electricity in the country by 2030, according to the International Energy Agency... The cooling systems are projected to use up to 33 billion gallons of water by 2028 per year... To serve this seemingly insatiable market, Karman has developed a rotating compressor that spins at 30,000 revolutions per minute — nearly 10 times faster than traditional compressors — to move heat... 

About a third of Karman's 23-person team came from SpaceX or Rocket Lab, and they co-opted technologies from aerospace engineering and electric vehicles to design the mechanics for the high-speed motors. The system uses a special type of carbon dioxide under high pressure to transfer heat from the data center to the outside air. Depending on the conditions, it can do the same amount of cooling using less than half the energy. Karman's heat pump can either reject heat to air, or route it into extra cooling, or even power generation. 
The company "recently raised $20 million," according to the article, "and expects to start building its first compressors in Long Beach later this year...."]]></content:encoded></item><item><title>New Linux/Android 2-in-1 Tablet &apos;Open Slate&apos; Announced by Brax Technologies</title><link>https://news.slashdot.org/story/26/01/25/2226239/new-linuxandroid-2-in-1-tablet-open-slate-announced-by-brax-technologies?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 26 Jan 2026 04:24:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Brax Technologies just announced "a privacy-focused alternative to locked-down tablets" called open_slate that can double as a consumer tablet and a Linux-capable workstation on ARM. 

Earlier Brax Technologies built the privacy-focused smartphone BraX3, which co-founder Plamen Todorov says proved "a privacy-focused mobile device could be designed, crowdfunded, manufactured, and delivered outside the traditional Big Tech ecosystem."


Just as importantly, BraX3 showed us the value of building with the community. The feedback we received — what worked, what didn't, and what people wanted next — played a major role in shaping our direction going forward. Today, we're ready to share the next step in that journey...

 

They're promising their "2-in-1" open_slate tablet will be built with these guiding principles:


Modularity beyond repairability". ("In addition to a user-replaceable battery, it supports an M.2 expansion slot, allowing users to customize storage and configurations to better fit their needs.")
Hardware-level privacy and control, with physical switches allowing users to disable key components like wireless radios, sensors, microphones, and cameras.
Multi-OS compatibility, supporting "multiple" Android-based operating systems as well as native Linux distributions. ("We're working with partners and the community to ensure proper, long-term OS support rather than one-off ports.")
Longevity by design — a tablet that's "supported over time"
Brax has already created an open thread with preliminary design specs. "The planned retail price is 599$ for the base version and 799$ for the Pro version," they write. "We will be offering open_slate (both versions) at a discount during our pre-order campaign, starting as low as 399$ for the base version and 529$ for the Pro version for limited quantities only which may sell out in a day or two from launching pre-orders... 

"Pre-orders will open in February, via IndieGoGo. Make sure to subscribe for notifications if you don't want to miss the launch date." 

Thanks to long-time Slashdot reader walterbyrd for sharing the news.]]></content:encoded></item><item><title>KDE&apos;s &apos;Plasma Login Manager&apos; Stops Supporting FreeBSD - Because Systemd</title><link>https://bsd.slashdot.org/story/26/01/26/0135254/kdes-plasma-login-manager-stops-supporting-freebsd---because-systemd?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 26 Jan 2026 02:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[KDE's "Plasma Login Manager" is apparently dropping support for FreeBSD, the Unix-like operating system, reports the blog It's FOSS. They cite a recently-accepted merge request from a KDE engineer to drop the code supporting FreeBSD, since the login manager relies on systemd/logind:

systemd and logind look like hard dependencies of the login manager, which means the software is built to work exclusively with these components and cannot function without them... logind is a component of systemd that is responsible for user session management... 
 This doesn't mean that KDE has abandoned the operating system altogether. FreeBSD users can still run the KDE Plasma desktop environment and continue using SDDM, the current login manager that works just fine on such systems. 

The article argues FreeBSD users "won't really care much for missing out on this as they have plenty of login manager options available."]]></content:encoded></item><item><title>Several New X.Org Libraries See 2026 Releases</title><link>https://www.phoronix.com/news/X.Org-Library-Updates-Jan-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 26 Jan 2026 01:14:41 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While we wait to see what comes of the new X.Org Server Git branch plans and a possible X.Org Server 26.1 release, several X.Org libraries saw new point releases this weekend. These seldom-updated libraries saw new releases to ship various build fixes and other minor improvements...]]></content:encoded></item><item><title>Washington State May Mandate &apos;Firearm Blueprint Detection Algorithms&apos; For 3D Printers</title><link>https://hardware.slashdot.org/story/26/01/26/0035209/washington-state-may-mandate-firearm-blueprint-detection-algorithms-for-3d-printers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 26 Jan 2026 01:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Adafruit managing director Phillip Torrone (also long-time Slashdot reader ptorrone ) writes: Washington State lawmakers are proposing bills (HB 2320 and HB 2321) that would require 3D printers and CNC machines to block certain designs using software-based "firearms blueprint detection algorithms." In practice, this means scanning every print file, comparing it against a government-maintained database, and preventing "skilled users" from bypassing the system. Supporters frame this as a response to untraceable "ghost guns," but even federal prosecutors admit the tools involved are ordinary manufacturing equipment. Critics warn the language is overbroad, technically unworkable, hostile to open source, and likely to push printing toward cloud-locked, subscription-based systems—while doing little to stop criminals.]]></content:encoded></item><item><title>Google Discover Replaces News Headlines With Sometimes Inaccurate AI-Generated Alternatives</title><link>https://news.slashdot.org/story/26/01/26/001230/google-discover-replaces-news-headlines-with-sometimes-inaccurate-ai-generated-alternatives?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 26 Jan 2026 00:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from The Verge:


In early December, I brought you the news that Google has begun replacing Verge headlines, and those of our competitors, with AI clickbait nonsense in its content feed [which appears on the leftmost homescreen page of many Android phones and the Google app's homepage]. Google appeared to be backing away from the experiment, but now tells The Verge that its AI headlines in Google Discover are a feature, one that "performs well for user satisfaction." I once again see lots of misleading claims every time I check my phone... 

For example, Google's AI claimed last week that "US reverses foreign drone ban," citing and linking to this PCMag story for the news. That's not just false — PCMag took pains to explain that it's false in the story that Google links to...! What does the author of that PCMag story think? "It makes me feel icky," Jim Fisher tells me over the phone. "I'd encourage people to click on stories and read them, and not trust what Google is spoon-feeding them." He says Google should be using the headline that humans wrote, and if Google needs a summary, it can use the ones that publications already submit to help search engines parse our work. 

Google claims it's not rewriting headlines. It characterizes these new offerings as "trending topics," even though each "trending topic" presents itself as one of our stories, links to our stories, and uses our images, all without competent fact-checking to ensure the AI is getting them right... The AI is also no longer restricted to roughly four words per headline, so I no longer see nonsense headlines like "Microsoft developers using AI" or "AI tag debate heats." (Instead, I occasionally see tripe like "Fares: Need AAA & AA Games" or "Dispatch sold millions; few avoided romance.") 

But Google's AI has no clue what parts of these stories are new, relevant, significant, or true, and it can easily confuse one story for another. On December 26th, Google told me that "Steam Machine price & HDMI details emerge." They hadn't. On January 11th, Google proclaimed that "ASUS ROG Ally X arrives." (It arrived in 2024; the new Xbox Ally arrived months ago.) On January 20th, it wrote that "Glasses-free 3D tech wows," introducing readers to "New 3D tech called Immensity from Leia" — but linking to this TechRadar story about an entirely different company called Visual Semiconductor... 

Google declined our request for an interview to more fully explain the idea.
 

The site Android Police spotted more inaccurate headlines in December:

A story from 9to5Google, which was actually titled 'Don't buy a Qi2 25W wireless charger hoping for faster speeds — just get the 'slower' one instead' was retitled as 'Qi2 slows older Pixels.' Similarly, Ars Technica's 'Valve's Steam Machine looks like a console, but don't expect it to be priced like one' was changed to 'Steam Machine price revealed.' At the time, we believed that the inaccuracies were due to the feature being unstable and in early testing.... Now, Google has stopped calling Discover replacing human-written headlines as an "experiment." 

"Google buries a 'Generated with AI, which can make mistakes' message under the 'See more' button in the summary," reports 9to5Google, "making it look like this is the publisher's intended headline."
While it is obvious that Google has refined this feature over the past couple of months, it doesn't take long to still find plenty of misleading headlines throughout Discover... Another article from NotebookCheck about an Anker power bank with a retractable cable was given a headline that's about another product entirely. A pair of headlines from Tom's Hardware and PCMag, meanwhile, show the two sides of using AI for this purpose. The Tom's Hardware headline, "Free GPU & Amazon Scams," isn't representative of the actual article, which is about someone who bought a GPU from Amazon, canceled their order, and the retailer shipped it anyway. There's nothing about "Amazon Scams" in the article.]]></content:encoded></item><item><title>This founder cracked firefighting — now he’s creating an AI gold mine</title><link>https://techcrunch.com/2026/01/25/this-founder-cracked-firefighting-now-hes-creating-an-ai-gold-mine/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sun, 25 Jan 2026 23:20:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The nozzle is just the beginning – what company founder Sunny Sethi calls "the muscle on the ground.”]]></content:encoded></item><item><title>ChatGPT is pulling answers from Elon Musk’s Grokipedia</title><link>https://techcrunch.com/2026/01/25/chatgpt-is-pulling-answers-from-elon-musks-grokipedia/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 25 Jan 2026 22:35:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Information from the conservative-leaning, AI-generated encyclopedia developed by Elon Musk’s xAI is beginning to appear in answers from ChatGPT.]]></content:encoded></item><item><title>Linux 6.19-rc7 Released With Kernel Continuity Plan, A Few Important Fixes</title><link>https://www.phoronix.com/news/Linux-6.19-rc7-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 22:29:55 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Linux 6.19 kernel remains on track for its official release two weeks from today, with the extra RC being baked in due to the end of year holidays. Out today is Linux 6.19-rc7 with a few changes worth highlighting for the week...]]></content:encoded></item><item><title>Gasoline Out of Thin Air? It&apos;s a Reality!</title><link>https://hardware.slashdot.org/story/26/01/25/2153246/gasoline-out-of-thin-air-its-a-reality?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 21:56:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Can Aircela's machine "create gasoline using little more than electricity and the air that we breathe"? Jalopnik reports...



The Aircela machine works through a three-step process. It captures carbon dioxide directly from the air... The machine also traps water vapor, and uses electrolysis to break water down into hydrogen and oxygen... The oxygen is released, leaving hydrogen and carbon dioxide, the building blocks of hydrocarbons. This mixture then undergoes a process known as direct hydrogenation of carbon dioxide to methanol, as documented in scientific papers. 

Methanol is a useful, though dangerous, racing fuel, but the engine under your hood won't run on it, so it must be converted to gasoline. ExxonMobil has been studying the process of doing exactly that since at least the 1970s. It's another well-established process, and the final step the Aircela machine performs before dispensing it through a built-in ordinary gas pump. So while creating gasoline out of thin air sounds like something only a wizard alchemist in Dungeons & Dragons can do, each step of this process is grounded in science, and combining the steps in this manner means it can, and does, really work. 

Aircela does not, however, promise free gasoline for all. There are some limitations to this process. A machine the size of Aircela's produces just one gallon of gas per day... The machine can store up to 17 gallons, according to Popular Science, so if you don't drive very much, you can fill up your tank, eventually... While the Aircela website does not list a price for the machine, The Autopian reports it's targeting a price between $15,000 and $20,000, with hopes of dropping the price once mass production begins. While certainly less expensive than a traditional gas station, it's still a bit of an investment to begin producing your own fuel. If you live or work out in the middle of nowhere, however, it could be close to or less than the cost of bringing gas to you, or driving all your vehicles into a distant town to fill up. You're also not limited to buying just one machine, as the system is designed to scale up to produce as much fuel as you need. 

 The main reason why this process isn't "something for nothing" is that it takes twice as much electrical energy to produce energy in the form of gasoline. As Aircela told The Autopian " Aircela is targeting >50% end to end power efficiency. Since there is about 37kWh of energy in a gallon of gasoline we will require about 75kWh to make it. When we power our machines with standalone, off-grid, photovoltaic panels this will correspond to less than $1.50/gallon in energy cost."


 

Thanks to long-time Slashdot reader Quasar1999 for sharing the news.]]></content:encoded></item><item><title>Science fiction writers, Comic-Con say goodbye to AI</title><link>https://techcrunch.com/2026/01/25/science-fiction-writers-comic-con-say-goodbye-to-ai/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 25 Jan 2026 21:53:42 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Some of the major players in science fiction and pop culture are taking firmer stances against generative AI.]]></content:encoded></item><item><title>North Star Metrics: Driving Business Growth With Focus</title><link>https://hackernoon.com/north-star-metrics-driving-business-growth-with-focus?source=rss</link><author>Dan Layfield</author><category>tech</category><pubDate>Sun, 25 Jan 2026 21:30:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[There is a dirty secret in product management.\
Most features that get shipped do nothing. Absolutely nothing.\
These features might sound great, sound fancy, and get a lot of praise internally. But when it comes to impacting the business, they do F all.\
They do nothing, not because they aren’t designed well, but because they don’t drive your business model in any meaningful way.\
The only way to impact your business is to have sustained focus on the things that matter.\
Without defining what matters, it’s easy for teams to scatter changes around the product, get lost in rabbit holes, or micro-optimizations that are a waste of time.\
This quote from Alice in Wonderland captures it perfectly: “Would you tell me, please, which way I ought to go from here?”\
 “That depends a good deal on where you want to get to.”\
 “I don’t much care where—”\
 “Then it doesn’t matter which way you go.”\
Don’t be Alice. Direction matters.All the great software development companies solve this problem by defining a North Star Metric.\
We did this at Uber and Codecademy. Meta does it, Google does it, Slack does it, and you should do it too.\
A North Star Metric is a way of quantifying the combined user and business value that your product produces.\
Users come to your product to solve a problem. Your business makes money somehow.\
You need to find a way of measuring the leading indicator of both.Let’s look at some examples and what their north star metrics could be:Facebook - “Daily Active Users” - FB is designed to be a daily use product, so this measures successful user engagement. FB makes money on ads, so users have to show up to see the ads.Uber - “Trips Completed” - They’re a marketplace, so they make money when a ride or delivery happens. “Completed” is key here because it drives value to both sides of the marketplace and captures liquidity + quality.AirBnB - “Nights Booked” - Also a marketplace, so this measures value to both the host and the guest. Their revenue will go up if this number goes up.Slack - “Messages Sent per Team” - B2B, so revenue driven by accounts. The more active the account, the higher the retention. They also charge per seat, so the more messages, the more likely the more people in that account, the higher the revenue.Notion - “Documents Created” - Notion has both solo users and team users. In team environments, the real value is the creation of a document for others to read. The more documents, the more valuable the workspace, the higher the retention.\
No metric is perfect, but all of these orient product development towards growing the business and helping their users.Product development, if not measured carefully, can be insanely wasteful.\
We learned this painful lesson at Codecademy.\
It was roughly in 2018, and we hired our first data scientist (shout out to Hillary), who correctly calculated our retention numbers for the first time.\
I remember staring at our fancy new dashboards and seeing that the last 2 years of our product development did absolutely nothing to improve our usage retention.\
We probably spent millions of dollars on engineers, PMs, designers, office space, snacks, etc., to get no movement in our retention, which is the main thing that matters.\
This taught me a valuable lesson.\
Good product development isn’t just shipping things. It also isn’t just shipping things that your users like.\
It is getting the highest ROI for the dollars you put into developing your product.\
North star metrics give you a way to denominate the value of your work.\
Without a clear and thoughtfully chosen North Star Metric, you can’t quantify this ROI.\
Just optimizing around MRR can be very misleading, as MRR is a lagging indicator.\
If you remember the concept of the growth ceiling from a few posts ago, your MRR might still be growing just from past momentum and not your current work.\
MRR can still go up even if your current features aren’t working. It can also go down when you’re shipping things that work.\
If you follow this as a signal, it’s really easy to go astray.There are 2 basic choices you have to make here.What are you going to measure?How are you going to measure it?\
For “what to measure”, you should try to capture “one complete unit of value” from the user’s perspective.\
Typically, value exists at a few levels, and it’s tricky to find the right altitude.\
For example, at Duolingo, this could be the completion of a question, a pack of questions, a collection of packs, etc.\
The first time you do this, pick something that you can easily measure that signals user value, and you can get more specific with time.\
For “How to measure it”, there are 3 traditionally ways of doing it.“Count of Something” - e.g., Uber’s total completed trips per month“Average of Something” - e.g., Slack’s average number of messages per team.“X per Y” - e.g., Facebook’s 7 friends in 10 days metric for new users.\
There are no perfect metrics, and all of these involve tradeoffs.“Count of Something” - This number is sensitive to new user acquisition. So it might go up, just because you have more users, not because you are improving the product. That might still be a good thing, but be aware of that.“Average of Something” - This is great at quantifying user-level health, but it also hides outliers/power users. This is important in certain models like content creation, marketplaces, or anything involving usage-based pricing.“X per Y” - Setting thresholds is really powerful, but you need a lot of data to know you’re setting this threshold correctly. Optimize for the wrong level, and you can go off course.The details of how you define this metric will also impact what strategy you take.\
Airbnb’s metric of “Nights Booked” has only a loose relationship to profit.\
They could drive much more profit if that was factored into their metric.\
Not focusing on profit means the teams can focus on expanding geographically, as all nights booked anywhere in the world are equal under this metric, which means they can take more market share.\
If they set a metric of:“Profit per Night Booked” - this would likely favor optimizing the cities where they already have density.“Avg Nights Booked Per User” - They would focus on high-frequency travelers and address their specific needs.\
When I was at UberEats, we actively optimized for profit.\
We definitely lost market share doing this, but it was clearly the right thing to do for the business.North Star Metrics is a simple framework, but deceptively hard to get right.Go from Broad to Narrow - Start with a metric that has broad coverage and narrow your focus with time.Make Sure Its Easy To Explain - your team can't optimize for it if they don't understand how it's calculated. Simple is better.Stay focused - in my experience, the life cycle of a North Star metric is somewhere between 1-3 years. If you are struggling to move your north star metric, it might not be the metric’s fault; you might be working on the wrong things.\
Additionally, here are some good gut checks:Can everyone in the company impact this metric in some form?If you are seeing this metric improve, does your MRR eventually go up?If you see this metric improve, is your usage retention improving?Is this based on a user action of some kind? E.g., not just landing in your product.\
As in most things, simple is typically better.\
P.S - Are you looking for help setting a metric like this? Reply to this email with “NORTH STAR,” and we will chat.Dan has helped drive 100M+ of business growth across his years as a product manager.\
He ran the growth team at Codecademy from $10M ARR to $50M ARR, which was acquired . After that, he was a product manager at Uber.\
Now, he advises and consults with startups & companies who are looking to increase subscription revenue.]]></content:encoded></item><item><title>A Class For Mom Part 2: Cybersecurity</title><link>https://hackernoon.com/a-class-for-mom-part-2-cybersecurity?source=rss</link><author>Amy Pravin Shah</author><category>tech</category><pubDate>Sun, 25 Jan 2026 20:47:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This is a cybersecurity class for senior citizens.]]></content:encoded></item><item><title>Funniest/Most Insightful Comments Of The Week At Techdirt</title><link>https://www.techdirt.com/2026/01/25/funniest-most-insightful-comments-of-the-week-at-techdirt-193/</link><author>Leigh Beadon</author><category>tech</category><pubDate>Sun, 25 Jan 2026 20:45:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Except for the fact that we literally have an amendment in our constitution that says nobody has to house soldiers. These people really are just the dumbest fucking fascists, aren’t they.You know as well as I do that this isn’t about immigration, it’s about white supremacy.These aren’t stupid mistakes; ICE is doing exactly what it’s there to do.MAGA is where the Republican Party has been headed since the Southern Strategy. They just don’t have to hide it anymore.Trump didn’t make the GOP fascist. He’s a terrifying symptom but he’s not the cause.Just shy of half the people who voted in the last election voted for this. The ones who insist that this isn’t what they voted for are ignorant, lying, or both.Me: Did you see the batshit insane thing he wrote this morning?Wife: You’re gonna have to be more specific.… that’s just life in the USA in 2026.Let’s ask Ian Betteridge.Well, considering where the Arctic Angels are stationed and considering we are talking about Hegseth, he probably has a surprise attack on Russia’s strategic Vodka reserves in planning.Is it going to be considered censorship of conservative voices when they start blocking people who read and write at a 3rd grade level?I feel Mr Wilson did a better job at redacting than the Trump administration did.That’s all for this week, folks!]]></content:encoded></item><item><title>I Built a Causal AI Model to Find What Actually Causes Stock Drawdowns</title><link>https://hackernoon.com/i-built-a-causal-ai-model-to-find-what-actually-causes-stock-drawdowns?source=rss</link><author>Nikhil Adithyan</author><category>tech</category><pubDate>Sun, 25 Jan 2026 20:14:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every investor talks about risk, but few can explain what truly drives it.We measure volatility, beta, or max drawdown, and assume we understand exposure. Yet these numbers are only outcomes. They tell us how much pain existed, not why it happened.Sometimes a high P/E stock crashes harder. Sometimes a low-margin business stays resilient. Correlation alone can’t answer what actually  those reactions. That’s where most models stop, and where this story begins.This experiment tries to answer one deceptively simple question:Do fundamentals and market traits directly cause deeper drawdowns, or do they just move in sync with them?To find out, I built a causal AI framework that analyzes how valuation, volatility, and profitability affect a stock’s downside. The data comes from , covering ten years of S&P 500 stocks, which is more than enough to see how company characteristics shape real risk, not just statistical noise.The goal isn’t to forecast crashes or design a trading signal. It’s to understand what truly makes a portfolio fragile, and how causal inference can separate reason from coincidence.From Correlation to CausationIt’s easy to assume that if two things move together, one must cause the other.High valuations, rising volatility, falling prices; the patterns often look convincing enough. I used to take those relationships at face value until I realized that correlation is mostly storytelling. It looks logical, it feels logical, but it rarely holds up when conditions change.That’s where causal inference comes in.Instead of asking what usually happens together, it asks what would have happened if something were different.For instance, if two companies were identical except for their valuation, would the higher-valued one experience a deeper drawdown? That’s the kind of “what-if” question correlation can’t answer, but causal models can.In this study, I treated valuation, volatility, and profitability as the key “treatments.” The idea was to see how each factor, when changed, affects a company’s downside risk while holding everything else constant. It’s less about prediction and more about simulation, i.e., building alternate realities to see which traits consistently lead to more pain.I used  to make this possible: ten years of S&P 500 history, packed with price, volume, and fundamental data. The quality and consistency of that feed made it possible to treat this like a real-world experiment instead of just another backtest.The Setup: Getting the Data RightBefore getting into the causal analysis, I needed a proper foundation, which is clean, consistent data. Everything starts here.I first imported the essential packages. Nothing fancy, just what’s needed to pull data, shape it, and run the models later.import pandas as pd
import numpy as np
import requests
import time
from datetime import datetime, timedelta
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
import matplotlib.pyplot as plt
This covers the basics: pandas and numpy for handling data, requests for API calls, sklearn for causal modeling, and matplotlib for plotting.Config and Extracting TickersNext, I configured the API and built a small universe of S&P 500 tickers. Instead of manually typing them out, I pulled the current list directly from the .api_key = 'YOUR EODHD API KEY'
base_url = 'https://eodhd.com/api'

tickers_url = f'{base_url}/fundamentals/GSPC.INDX?api_token={api_key}&fmt=json'
tickers_raw = requests.get(tickers_url).json()
tickers = list(pd.DataFrame(tickers_raw['Components']).T['Code'])
tickers = [item + '.US' for item in t]

start_date = '2018-01-01'
end_date   = datetime.today().strftime('%Y-%m-%d')
EODHD’s /fundamentals/GSPC.INDX endpoint provides the full list of S&P 500 constituents. By appending .US to each code, I ensured compatibility with the /eod/ endpoint for fetching daily prices.Once the tickers were ready, I moved on to fetching price data and calculating returns.The function below retrieves daily OHLC values using  for each symbol between the start and end date, then stitches everything together. \n def fetch_eod_prices(ticker, start, end):
    url = f'{base_url}/eod/{ticker}?from={start}&to={end}&api_token={api_key}&fmt=json'
    r = requests.get(url).json()

    if not isinstance(r, list) or not r:
        return pd.DataFrame(columns=['date','close','ticker'])

    df = pd.DataFrame(r)[['date','close']]
    df['ticker'] = ticker

    print(f'{ticker} FETCHED DATA')
    time.sleep(1)

    return df

frames = [fetch_eod_prices(t, start_date, end_date) for t in tickers]

prices_df = pd.concat(frames, ignore_index=True).dropna(subset=['close'])

prices_df['date'] = pd.to_datetime(prices_df['date'])
prices_df = prices_df.sort_values(['ticker','date'])
prices_df['ret'] = prices_df.groupby('ticker')['close'].pct_change().fillna(0.0)
prices_df = prices_df[['ticker','date','close','ret']]

prices_df = prices_df.reset_index(drop=True)
prices_df
This creates a unified dataset of over a million rows across all S&P 500 tickers, each with the closing price and daily return. This is how the final dataframe looks like:The output confirms everything is aligned:Each ticker is ordered by date.The first row per ticker shows a return of zero (which validates the group logic).The data is dense, balanced, and ready for feature engineering in the next step.Measuring Stress: Calculating DrawdownsOnce the return data was ready, I needed to turn it into something that represents risk.Returns tell how a stock moved, but not how it behaved under pressure. Drawdown captures exactly that, which is how deep a stock fell from its recent peak during stress.For this, I defined two major market stress windows:COVID crash (Feb–Apr 2020)Rate-hike shock (Aug–Oct 2022)Each window captures a different macro shock, one liquidity-driven, the other inflation-driven. This gives a good contrast for causal inference later.def max_drawdown(series: pd.Series) -> float:
    cummax = series.cummax()
    dd = series / cummax - 1.0
    return float(dd.min())

stress_windows = [
    ('2020-02-15', '2020-04-30'), 
    ('2022-08-01', '2022-10-31')
]

rows = []
for t in prices_df['ticker'].unique():
    df_t = prices_df.loc[prices_df['ticker'] == t, ['date','close']].set_index('date').sort_index()
    for i, (start, end) in enumerate(stress_windows, start=1):
        s = df_t.loc[start:end, 'close']
        if s.empty:
            continue
        rows.append({
            'ticker': t,
            'window_id': i,
            'start': start,
            'end': end,
            'max_dd': max_drawdown(s)
        })

drawdowns_df = pd.DataFrame(rows)
drawdowns_df
The max_drawdown() function tracks the running peak of a stock’s price and measures the percentage drop from that peak.Then, for each ticker, I extract prices within the defined stress windows and compute the worst drawdown in that range.By iterating through two different market crises, the result shows how each company handled external shocks.Each stock now has up to two entries, one per stress event, along with its corresponding maximum drawdown.Fetching Fundamentals & Defining TreatmentsWith the price and drawdown data ready, the next step was to define the “treatments.”In causal inference, a treatment represents an event or characteristic whose effect we want to measure. Here, I wanted to understand how certain financial traits, like valuation, volatility, and profitability, affect a company’s risk profile during market stress.The three treatments I picked were:: stocks that are priced expensively relative to their earnings.: stocks that move aggressively with the market.: stocks with thinner profitability cushions.To build these, I used EODHD’s Fundamentals API, which provides all the key metrics I needed.Fetching Fundamentals Datadef fetch_fundamentals(ticker):
    url = f'{base_url}/fundamentals/{ticker}?api_token={api_key}&fmt=json'
    r = requests.get(url).json()
    if not isinstance(r, dict):
        return pd.DataFrame()

    data = r.get('Highlights', {})
    general = r.get('General', {})
    val = r.get('Valuation', {})

    row = {
        'ticker': ticker,
        'sector': general.get('Sector', np.nan),
        'market_cap': data.get('MarketCapitalization', np.nan),
        'beta': r.get('Technicals')['Beta'],
        'eps': data.get('EarningsShare', np.nan),
        'div_yield': data.get('DividendYield', np.nan),
        'net_margin': data.get('ProfitMargin', np.nan),
        'pe_ratio': val.get('TrailingPE'),
        'pb_ratio': val.get('PriceBookMRQ'),
    }

    print(f'{ticker} FETCHED DATA')
    time.sleep(1)

    return pd.DataFrame([row])

fund_frames = [fetch_fundamentals(t) for t in tickers]
fundamentals_df = pd.concat(fund_frames, ignore_index=True)

num_cols = ['market_cap','beta','eps','div_yield','net_margin','pe_ratio','pb_ratio']
fundamentals_df[num_cols] = fundamentals_df[num_cols].apply(pd.to_numeric, errors='coerce')

fundamentals_df
Each call to /fundamentals/{ticker} returns multiple nested blocks: General, Highlights, Valuation, and Technicals. I extracted what I needed from each:Highlights: Earnings per share, profit margin, dividend yield, market capValuation: PE and PB ratiosAll fields are converted into a single flat record per ticker and combined into one large DataFrame.The structure is wide enough to hold all core features and still compact enough to merge easily with returns later.Merging data and creating treatmentsThis step creates the dataset used for causal inference, combining financial performance with firm-level traits. \n # merge fundamentals with drawdowns per ticker-window
combined_df = drawdowns_df.merge(fundamentals_df, on='ticker', how='left')

# basic controls
combined_df['log_mcap'] = np.log(combined_df['market_cap'].replace({0: np.nan}))
combined_df['sector'] = combined_df['sector'].fillna('Unknown')

combined_df.head()
Each row here represents a stock-window pair: its drawdown during a specific stress period, valuation profile, and sector context.Log-transforming market cap helps stabilize the data since firm sizes vary wildly, while sector labels later help balance comparisons.With the merged dataset ready, I defined what qualifies as “treated.” For example, a stock is treated for a high PE if it falls above the 70th percentile PE within its own sector. That ensures comparisons happen within similar industries instead of globally. \n def sector_pe_thresholds(df, min_count=5, q=0.70):
    th = df.groupby('sector')['pe_ratio'].quantile(q).rename('pe_thr').to_dict()
    global_thr = df['pe_ratio'].quantile(q)
    return {s: (thr if not np.isnan(thr) else global_thr) for s, thr in th.items()}

pe_thr_map = sector_pe_thresholds(fundamentals_df)

combined_df['high_pe'] = combined_df.apply(
    lambda r: 1 if pd.notna(r['pe_ratio']) and r['pe_ratio'] > pe_thr_map.get(r['sector'], np.nan) else 0, axis=1
)

combined_df['high_beta'] = (combined_df['beta'] > 1.2).astype(int) # market sensitivity proxy
combined_df['low_margin'] = (combined_df['net_margin'] < 0).astype(int) # weak profitability

treat_cols = ['high_pe', 'high_beta', 'low_margin']
ctrl_cols = ['log_mcap', 'sector']
model_df = combined_df[['ticker','window_id','start','end','max_dd','pe_ratio','beta','net_margin','market_cap'] + treat_cols + ctrl_cols].copy()

model_df.head(10)
The logic is simple but deliberate: stocks represent overvalued firms within each sector. stocks are those that move more than 20% above market sensitivity. stocks reflect profitability risk.The resulting dataset looks like this:Each stock now carries its drawdown, financial metrics, and binary treatment flags. We have exactly what’s needed for causal analysis in the next section.Estimating Causal EffectsWith all the inputs ready, drawdowns, fundamentals, and treatment flags, it was time to move beyond correlations and actually test for causation.The question was straightforward:How much more (or less) do high-PE, high-beta, and low-margin stocks fall during market stress after accounting for factors like size and sector?That’s a classic causal inference problem. Instead of using traditional regressions, I decided to rely on two modern estimators:Inverse Probability Weighting (IPW) reweights observations to mimic a randomized experiment.Doubly Robust (DR) Estimator combines regression and reweighting to reduce bias.Implementing the EstimatorsThe first function computes the IPW estimate. It fits a logistic model to estimate the propensity score, which is the probability of being “treated” (for example, being a high-PE stock) given the controls.Then it adjusts the sample weights to make the treated and control groups comparable.The second function goes a step further by combining a regression model with weighting to create a doubly robust estimate. Even if one of the models is slightly misspecified, the results tend to remain stable.def causal_ipw(data, treatment, outcome, controls):
    df = data.dropna(subset=[treatment, outcome] + controls).copy()
    X = pd.get_dummies(df[controls], drop_first=True)
    T = df[treatment]
    y = df[outcome]

    # propensity score model
    ps_model = LogisticRegression(max_iter=500)
    ps_model.fit(X, T)
    p = ps_model.predict_proba(X)[:,1].clip(0.01, 0.99)

    # inverse weighting
    weights = T/p + (1-T)/(1-p)
    ate = np.mean(weights * (T - p) * y) / np.mean(weights * (T - p)**2)

    return ate

def causal_dr(data, treatment, outcome, controls):
    df = data.dropna(subset=[treatment, outcome] + controls).copy()
    X = pd.get_dummies(df[controls], drop_first=True)
    T = df[treatment].values
    y = df[outcome].values

    # models
    m_y = RandomForestRegressor(n_estimators=200, random_state=42)
    m_t = LogisticRegression(max_iter=500)

    m_y.fit(np.column_stack([X, T]), y)
    m_t.fit(X, T)
    p = m_t.predict_proba(X)[:,1].clip(0.01,0.99)
    y_hat_1 = m_y.predict(np.column_stack([X, np.ones(len(X))]))
    y_hat_0 = m_y.predict(np.column_stack([X, np.zeros(len(X))]))

    dr_scores = (T*(y - y_hat_1)/p) - ((1-T)*(y - y_hat_0)/(1-p)) + (y_hat_1 - y_hat_0)
    ate = np.mean(dr_scores)
    return ate

results = []
for t in ['high_pe','high_beta','low_margin']:
    ate_ipw = causal_ipw(model_df, t, 'max_dd', ['log_mcap','sector'])
    ate_dr = causal_dr(model_df, t, 'max_dd', ['log_mcap','sector'])
    results.append({'treatment': t, 'ate_ipw': ate_ipw, 'ate_dr': ate_dr})

effects_df = pd.DataFrame(results)
effects_df
This table is where things start getting interesting.Each row represents one treatment, and the two columns (atedr) show how that characteristic influences the maximum drawdown on average. tend to fall about 0.21 more (IPW) during stress periods. The DR estimator shows a smaller effect, but the direction is consistent. Overvalued stocks feel the heat first. show a 0.25 deeper drawdown, which is not surprising. They amplify market moves both ways. are hit the hardest, with nearly 0.37 additional downside. This suggests that profitability cushions truly help during panic phases.While the magnitudes differ slightly across estimators, the pattern is clear. Fragile fundamentals amplify stress.And that’s exactly the kind of causal evidence I wanted to see. Not just which stocks correlate with volatility, but which ones actually cause higher risk exposure when the market turns.Visualizing and Validating the EffectsOnce I had the causal estimates, I wanted to see what they actually looked like. Numbers can tell you the “what,” but visuals often reveal the “how.”At this point, the objective wasn’t to add more math, but to see whether the intuition from the earlier steps holds up when plotted.This stage also acts as a sanity check. If the visual patterns contradict the statistical results, something’s usually off in the setup. But if both align, you can be more confident that the model’s logic is sound.Plotting the Causal EffectsThe first visualization compares the estimated effects from the two causal models, IPW and DR.This helps check whether both estimators point in the same direction and how strongly each treatment affects the drawdown. \n effects_plot = effects_df.melt(id_vars='treatment', value_vars=['ate_ipw','ate_dr'], var_name='method', value_name='ate')

plt.figure(figsize=(6,4))
for m in effects_plot['method'].unique():
    subset = effects_plot[effects_plot['method']==m]
    plt.bar(subset['treatment'], subset['ate'], alpha=0.7, label=m)

plt.axhline(0, color='black', linewidth=0.8)
plt.ylabel('Estimated Causal Effect on Max Drawdown')
plt.title('Causal Effect Estimates (IPW vs DR)')
plt.legend()
plt.tight_layout()
plt.show()
The bar chart above captures the story behind all those computations.Each bar represents the estimated causal effect of a financial trait on maximum drawdown.Across the three treatments, the trend is consistent. High PE, high beta, and low margin all deepen downside risk, with low-margin stocks showing the sharpest impact.The difference between the blue and orange bars (IPW and DR) is minor, which is exactly what you want to see. It means both models are converging toward the same conclusion.The takeaway is clear: fundamental fragility doesn’t just coincide with volatility. It drives it.Checking Overlap Between Treated and Control GroupsBefore trusting any causal estimate, it’s essential to check whether the treated and control samples are comparable.In theory, causal inference assumes overlap, meaning every treated stock has a comparable control stock with similar characteristics, except for the treatment itself.If there’s no overlap, the results become meaningless because we’re comparing two entirely different worlds.Propensity Score DistributionTo test this, I plotted the propensity scores for each treatment.The score represents the probability of a stock being “treated” (for instance, having a high PE ratio) given its market cap and sector. \n for t in ['high_pe','high_beta','low_margin']:
    df = model_df.dropna(subset=[t,'log_mcap','sector']).copy()
    X = pd.get_dummies(df[['log_mcap','sector']], drop_first=True)
    T = df[t]

    ps_model = LogisticRegression(max_iter=500)
    ps_model.fit(X, T)
    ps = ps_model.predict_proba(X)[:,1]

    plt.figure(figsize=(6,4))
    plt.hist(ps[T==1], bins=20, alpha=0.6, label='treated', color='tab:blue')
    plt.hist(ps[T==0], bins=20, alpha=0.6, label='control', color='tab:orange')
    plt.title(f'Propensity Score Overlap — {t}')
    plt.xlabel('propensity score')
    plt.ylabel('count')
    plt.legend()
    plt.tight_layout()
    plt.show()
The histograms above show how well the model managed to balance the treated and control groups across the three traits.For  stocks, there’s a healthy overlap between blue and orange bars, which means the weighting process has done its job fairly well.For , the distributions are more scattered, hinting that these stocks behave differently enough to make perfect matching difficult. companies show the least overlap, which is expected since unprofitable firms often cluster in specific sectors like biotech or early-stage tech.While the overlaps aren’t perfect, they’re sufficient to move forward. The key point is that there’s enough shared ground between treated and control groups to make causal comparison meaningful.Verifying Covariate BalanceEven though the visual overlaps looked decent, I wanted a numerical check to confirm that the weighting step did what it was supposed to.The idea is simple: after reweighting, the treated and control groups should look nearly identical in their key characteristics, except for the treatment itself.To test this, I compared the mean difference in log market capitalization between high-beta stocks and their controls, both before and after weighting.t = 'high_beta'
df = model_df.dropna(subset=[t,'log_mcap','sector']).copy()
X = pd.get_dummies(df[['log_mcap','sector']], drop_first=True)
T = df[t]

ps_model = LogisticRegression(max_iter=500)
ps_model.fit(X, T)
ps = ps_model.predict_proba(X)[:,1]
w = T/ps + (1-T)/(1-ps)

m_treat_raw = df.loc[T==1,'log_mcap'].mean()
m_ctrl_raw = df.loc[T==0,'log_mcap'].mean()
m_treat_w = np.average(df['log_mcap'], weights=T*w)
m_ctrl_w = np.average(df['log_mcap'], weights=(1-T)*w)

print('Covariate balance for log_mcap (high_beta)')
print(f'Raw diff: {m_treat_raw - m_ctrl_raw:.4f}')
print(f'Weighted diff: {m_treat_w - m_ctrl_w:.4f}')
Before weighting, the average difference in log market cap between high-beta and low-beta stocks was around 0.19, which means the two groups weren’t perfectly comparable.After applying the weights, that difference dropped to 0.0008 (nearly zero).This confirms that the balancing process worked as intended. By reweighting observations according to their propensity scores, the treated and control groups now share almost identical distributions in their control variables.That alignment is what gives the causal estimates credibility. Without it, we would simply be comparing random subsets of the market instead of isolating the effect of specific traits like valuation, volatility, or profitability.What This Means for Risk ManagementAfter spending hours inside Python, it’s easy to forget what this all connects to, which is the real market. The value of causal modeling is that it reframes how we think about risk. Instead of assuming or guessing which factors drive volatility, it gives us a measurable way to quantify how much each one truly contributes.In traditional analysis, drawdowns are treated as outcomes of randomness or sentiment. But once we look through a causal lens, the randomness begins to fade. We start to see structure and the patterns that repeat when certain traits combine.A portfolio manager can use this insight to design more resilient allocations. For instance, if low-margin firms consistently amplify downside during stress windows, those positions can be hedged or sized down ahead of similar conditions. If high-beta stocks display asymmetric losses even after accounting for size and sector, then they deserve a risk premium or tighter stop rules.The same principle applies to stress testing.Rather than simulating arbitrary shocks, we can construct counterfactual scenarios:What would have happened if the portfolio held fewer high-PE names during a sell-off?How much of the observed risk was structural, and how much was avoidable?Causal models turn these questions into measurable answers. They allow risk managers to move from intuition to evidence, from correlation to attribution.This entire workflow, from raw data to counterfactual reasoning, was never meant to predict the next crash or rally.It was meant to understand why portfolios behave the way they do when markets turn unstable.By combining fundamental traits, stress windows, and causal estimators, we built a framework that explains structural drivers of risk rather than chasing signals.Each piece of code served a purpose: defining clean treatments, balancing controls, estimating effects, and validating the logic through overlap and weighting.Causal inference will not replace forecasting or technical analysis. But it adds a new dimension. The one focused on understanding rather than prediction. When markets fall, it helps separate what was inevitable from what was amplified by exposure to specific traits.Markets don’t just move randomly. They react to structure, and causal AI helps us see that structure clearly.]]></content:encoded></item><item><title>Inside Brevity AI: The Architecture Powering Real-Time, HIPAA-Compliant Clinical Documentation</title><link>https://hackernoon.com/inside-brevity-ai-the-architecture-powering-real-time-hipaa-compliant-clinical-documentation?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Sun, 25 Jan 2026 20:04:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Modern healthcare technology demands more than incremental improvements—it requires fundamental architectural innovation that balances performance, security, and clinical usability. Brevity AI, Inc. has developed a groundbreaking clinical documentation platform that transforms how healthcare professionals interact with patient data, compressing hours of preparation and documentation into minutes via Artificial Intelligence. As Co-Founder and CTO, Purv Rakeshkumar Chauhan designed and implemented the entire software architecture, demonstrating exceptional software engineering capabilities by building a production-grade healthcare system from concept to reality.The Brevity AI platform represents a sophisticated technical achievement requiring mastery across the entire software development lifecycle. Purv independently architected and developed the complete system, from initial requirements analysis through production deployment and ongoing maintenance. This comprehensive technical ownership encompasses frontend user interfaces optimized for clinical workflows, backend services processing massive medical datasets, AI/ML pipeline integration for intelligent document analysis, and secure data infrastructure managing sensitive healthcare information.\
The platform's architecture solves a complex technical challenge: processing hundreds of pages of unstructured medical records in real-time while maintaining HIPAA compliance and clinical accuracy. This necessitated designing scalable data processing pipelines capable of ingesting diverse medical document formats, implementing natural language processing workflows that extract clinically relevant information with precision, and creating intuitive user interfaces that seamlessly integrate into existing clinical workflows. Each architectural decision reflects careful consideration of healthcare's unique technical constraints—from latency requirements for real-time transcription to data residency requirements for patient privacy.Advanced System Architecture: Building for Healthcare ScaleThe technical architecture of Brevity AI demonstrates sophisticated software engineering principles applied to healthcare's demanding requirements. The system implements a microservices-based architecture that enables independent scaling of compute-intensive AI processing, document parsing, and real-time transcription services. This modular design allows the platform to handle variable workloads—from processing extensive patient histories during visit preparation to managing real-time documentation during patient encounters—without performance degradation.\
Purv's implementation includes advanced caching strategies that optimize response times for frequently accessed patient data, asynchronous processing queues that manage resource-intensive document analysis tasks, and intelligent load balancing that ensures consistent performance across clinical environments. The platform's data layer employs optimized database schemas specifically designed for medical record structures, enabling sub-second query performance even when analyzing patient histories spanning decades with hundreds of documents.Real-Time AI Integration: Technical Innovation in Clinical DocumentationThe platform's real-time documentation capabilities showcase sophisticated software engineering in AI integration. Purv architected a speech-to-text pipeline for recognizing medical terminology in clinical conversations. This system processes audio streams in real-time, implementing advanced noise reduction algorithms that filter ambient noise in clinical environments while preserving conversational clarity.\
The AI-powered note generation system represents particularly complex technical work—transforming unstructured conversations into structured clinical notes requires sophisticated natural language understanding, medical entity recognition, and template generation algorithms. Purv designed this pipeline to process extended patient conversations (often 20-30 minutes) and generate concise, standards-compliant clinical notes within seconds after the conversation concludes. The system employs intelligent chunking strategies to manage long conversations, contextual analysis to maintain clinical coherence across conversation segments, and validation algorithms ensuring generated notes meet documentation standards.The platform's visit preparation capabilities solve one of healthcare's most time-consuming challenges through advanced document processing architecture. By analyzing a patient's complete medical history—often 300+ pages spanning multiple healthcare systems—the system extracts clinically relevant information, identifies key medical conditions and trends, synthesizes laboratory results across different formats, and presents findings in a clinically useful format.\
Purv engineered a multi-stage processing pipeline that handles this complexity: document ingestion and format normalization for diverse medical record types, intelligent page analysis using computer vision to identify document types, natural language extraction of clinical entities and relationships, temporal analysis to track condition progression and treatment outcomes, and intelligent summarization that prioritizes information by clinical relevance. This pipeline processes massive document sets in minutes, a task that traditionally requires hours of manual review.While Purv's cybersecurity background provides foundational expertise, the platform's security implementation demonstrates how security principles translate into practical software architecture. Rather than treating security as an afterthought, the system implements security-by-design principles throughout the technical stack. This includes implementing a Zero Trust Architecture requiring authentication and authorization at every system boundary, designing data encryption protocols for data at rest and in transit, architecting audit logging systems that track all data access for compliance reporting, and building automated security testing into the continuous integration pipeline.\
The HIPAA compliance framework extends beyond security to encompass comprehensive technical controls: data retention policies enforced through automated lifecycle management, access control mechanisms with role-based permissions at granular levels, and automated compliance reporting systems that generate audit trails for regulatory review. These technical implementations ensure the platform meets healthcare's stringent regulatory requirements while maintaining the performance necessary for clinical workflows.The platform's technical sophistication translates directly into measurable healthcare improvements. Reducing visit preparation time from hours to minutes demonstrates the document processing pipeline's efficiency. The elimination of after-hours documentation through real-time transcription showcases the speech processing system's accuracy and reliability. Improved clinical accuracy from AI-assisted summarization validates the natural language processing algorithms' effectiveness. These outcomes reflect successful software engineering—building systems that solve real problems through technical excellence.Purv's responsibilities encompass the entire development process—defining technical requirements through collaboration with clinicians, designing system architecture and selecting appropriate technology stacks, implementing features through hands-on software development, conducting comprehensive testing including unit, integration, and end-to-end validation, managing production deployment and monitoring system performance, and iterating based on user feedback and performance metrics. This comprehensive technical ownership ensures cohesive architecture where every component serves the platform's clinical mission.Technical Foundation Enabling Healthcare InnovationBrevity AI's platform represents a significant software engineering achievement—a production-grade healthcare system built through exceptional architectural design and development expertise. Purv Rakeshkumar Chauhan's comprehensive technical leadership, spanning from low-level implementation details to high-level system architecture, has created a platform that fundamentally improves clinical workflows through intelligent automation. With his strong foundation in security research and academic excellence from Arizona State University informing the platform's robust security architecture, Purv continues advancing healthcare technology through software engineering innovation that prioritizes both technical excellence and clinical utility.About Purv Rakeshkumar ChauhanPurv Rakeshkumar Chauhan serves as Co-Founder and CTO of Brevity AI, Inc., where he leads all aspects of software development and system architecture for the company's clinical documentation platform. With academic credentials from Arizona State University and a foundation in cybersecurity research, including DARPA-funded projects, he brings comprehensive technical expertise to healthcare AI development. As the architect and principal developer of the Brevity AI platform, Purv demonstrates exceptional software engineering capabilities across the full technology stack, from user interface design to backend infrastructure, security implementation, and AI/ML integration.]]></content:encoded></item><item><title>Bet on Yourself: The Expected Value Is Higher Than You Think</title><link>https://hackernoon.com/bet-on-yourself-the-expected-value-is-higher-than-you-think?source=rss</link><author>Scott D. Clary</author><category>tech</category><pubDate>Sun, 25 Jan 2026 19:56:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[You’re not risk-averse. You’re math-averse.The reason you haven’t started that business, published that work, or made that career move isn’t because you’re scared. It’s because you’re running the wrong calculation.Most people think about risk like accountants. They see a potential move - start a podcast, launch a product, quit their job - and immediately calculate what they could lose.Time. Money. Reputation. Stability.The numbers add up to “too risky.”They stay in jobs they’ve outgrown. They sit on ideas that could change their lives. They watch other people - often less talented, less prepared people - build the things they dreamed about building.And they tell themselves they’re being smart. Being prudent. Being realistic.But here’s what nobody tells you: the math you’re using to justify staying stuck is fundamentally wrong.You’re optimizing for the wrong variable. You’re calculating downside while ignoring asymmetry. You’re treating all risks as equal when they’re not even in the same universe.And that miscalculation is costing you everything.This isn’t one of those letters about “following your dreams” or “just believe in yourself.”This is about the actual mathematics of asymmetric upside. Why the downside of trying and failing is almost always survivable. Why the downside of never trying is guaranteed regret. And why betting on yourself has a higher expected value than you think.I - The Risk Calculation That Keeps Smart People StuckI spent two years hesitating on my podcast.I knew I wanted to do it. I had the ideas. I knew what I would talk about. I’d written out episode concepts. I’d even recorded test episodes that nobody would ever hear.But I kept finding reasons not to launch.“I need better equipment first.” “I should have more of a following before I start.” “I need to be more of an expert in my topics.” “I should wait until I have a clear monetization strategy.”Every reason sounded rational. Every excuse felt legitimate.And I was very good at performing preparation. I researched microphones for weeks. I studied successful podcast formats. I planned out 20 episodes in detail. I told people at parties “I’m thinking about starting a podcast” and enjoyed the validation of being someone with ambitious plans.But preparation became my drug of choice. The dopamine hit of planning without the risk of failing.You know exactly what I’m talking about. Because you do this too.You’ve been “thinking about” starting that thing for how long now? Six months? Two years?You have the Notion doc with the perfect structure. The bookmarks folder labeled “Research.” The screenshot folder of inspiration. Maybe you’ve even bought a course or two. Told yourself you’re building a foundation.But here’s what was actually happening in my brain - and yours:My brain was running a calculation.“What if I invest six months and it doesn’t work?” “What if I look stupid?” “What if I fail and prove I’m not capable?” “What if I lose the money on equipment?”The potential losses felt massive. Concrete. The potential gains felt distant. Uncertain. Unlikely.So I waited. I told myself I needed more preparation, more certainty, more proof that it would work.What I didn’t realize then - but understand clearly now - is that I wasn’t being careful. I was being mathematically illiterate.Because the calculation I was running - “What could I lose?” - is only half the equation.The complete equation is: “What could I lose?” versus “What could I gain?” versus “What’s the probability of each outcome?” versus “What’s the cost of never finding out?”And when you run the complete calculation, the math changes entirely.You want to know what actually happened when I finally launched the podcast?Nothing catastrophic. No embarrassment. No public failure.I published the first episode. Twenty-three people listened in the first week. Most of them were friends. The audio quality was mediocre. I stumbled over words. I said “um” too much.No one mocked me. No one called me out for not being an expert. The world didn’t end.Within three months, I had 200 regular listeners. Within six months, one of those listeners introduced me to someone who became a client. Within a year, the podcast became a primary way I connected with my audience and developed ideas that became newsletter content.All of the disaster scenarios I’d imagined for two years? None of them happened.The worst case was so much less bad than I’d feared. The upside was so much better than I’d imagined.That’s the pattern. The fear is always worse than the reality.Let me show you with real numbers. (same principle applies to podcasts, YouTube, any content):You invest 3-6 months writing consistently. Maybe 100 hours total over that period. That’s about 30 minutes per day, 5 days a week.You promote it to your existing network and on social platforms.Nobody reads it. Your first few posts get 3 views. Your mom and your two best friends.You feel embarrassed. You imagine people seeing you try and fail. You experience the discomfort of being visible while not being validated.That’s the downside. Let’s assess: Is it survivable?You’re out 100 hours spread over six months. In exchange for those hours, you:Developed clarity on what you think about important topicsBuilt a skill (writing for an audience)Created a portfolio of work that demonstrates your thinkingLearned what doesn’t resonate (valuable market research)Proved to yourself that you can finish what you startMade progress while others stayed stuck in planning modeYour ego takes a hit for maybe a week. Then life moves on. Nobody actually cares that much. Everyone’s too busy thinking about their own lives.Now let’s say it works. Not perfectly. Not viral-level success. Just… it works.The newsletter connects with 1,000 people over a year. That’s less than 3 new subscribers per day. Completely achievable.Those 1,000 people care about what you think. They read what you write. They respond with their own thoughts. They remember your name.That audience of 1,000 becomes:A distribution channel for anything you want to createA testing ground for new ideas and productsA network of people who can introduce you to opportunitiesSocial proof that you can build something from scratchA source of income if you ever want to monetizeConversations you couldn’t have had otherwiseRelationships that wouldn’t have formedOne of those 1,000 people sends your newsletter to someone influential in your field. That person reaches out. One conversation leads to an opportunity you never would have encountered.Or: Ten of those 1,000 people buy a product you create. $100 each. That’s $1,000. For a first attempt at monetization. Which validates that you can create value people will pay for. Which gives you confidence to build more.Or: Your writing clarifies your thinking so much that you become better at your day job. You get promoted. The writing skills transfer. The audience gives you leverage.These aren’t hypotheticals. This is the actual, documented pattern that happens to people who start creating and stick with it.But here’s what nobody tells you about the year you spend “preparing” instead of starting:You don’t spend that year in neutral. You spend it moving backwards.Every month you wait to start, someone else publishes their first post. Someone else sends their first email. Someone else builds their first 10 subscribers.You’re not standing still while you prepare. You’re watching a gap open up between you and the people who started before they felt ready.And that gap isn’t just about audience size. It’s about confidence, skill, resilience, and the compounding knowledge that comes from actually doing the thing.By the time you finally start - if you ever do - you’re not competing with who you are now. You’re competing with a version of yourself who started a year ago and has been improving ever since.Same effort. Same time investment. But the upside and downside aren’t even in the same galaxy.The downside is capped (100 hours of learning, temporary ego hit, valuable skills developed). The upside is uncapped (career transformation, financial freedom, opportunities you can’t predict).Most of the bets you’re not taking have a capped, survivable downside and an uncapped, transformative upside.That’s not risk in the traditional sense. That’s asymmetry. And asymmetry is mathematically smart.Yet smart people don’t take these bets.Why? Because they’re running the wrong calculation.They see the downside clearly - the time, the money, the potential embarrassment.They discount the upside heavily - “That probably won’t happen to me.”And they completely ignore the fourth variable: the cost of never trying.Here’s what the miscalculation reveals: You’re not actually trying to make a smart decision. You’re trying to find a reason to stay comfortable.The math is just cover. The spreadsheet is just permission to avoid risk while feeling rational about it.If you actually cared about making the optimal choice, you’d run the complete equation. You’d see the asymmetry. You’d move.But you don’t want to see it. Because seeing it would mean you have no more excuses.So why does your brain work this way? Why do smart people systematically miscalculate risk?Because your brain isn’t running math. It’s running something far more primitive.II - Why Your Brain Gets The Math Wrong“We suffer more often in imagination than in reality.”Your brain isn’t doing math. It’s running ancient survival software that was calibrated for a world that no longer exists.The Mismatch Between Ancestral Risk and Modern OpportunityTen thousand years ago, risk assessment was brutally simple:Will this kill me? Yes or no.Trying a new hunting ground: Might encounter predators. Risk of death: High.Challenging the tribe leader for status: Might get exiled or killed. Risk of death: High.Eating unknown berries: Might be poisonous. Risk of death: High.In that environment, loss aversion made perfect evolutionary sense.The cost of being wrong  was death. The benefit of being right ten times was… not being dead.Natural selection strongly favored caution. The humans who took too many risks didn’t survive to pass on their genes. The ones who avoided uncertainty, who stuck with what was known and safe, lived long enough to reproduce.Your brain is descended from those cautious survivors. It’s running their software. And that software has one primary directive: avoid anything that feels like it might threaten your survival.Your amygdala - the almond-shaped cluster of neurons responsible for processing threats - can’t tell the difference between “there’s a predator in the grass” and “someone might not like my business idea.”Both trigger the same neurological response. Both flood your system with the same stress hormones. Both generate the same behavioral impulse: avoid, retreat, stay safe.But the actual stakes are completely different.Starting a podcast won’t kill you. It can’t. The worst-case scenario is temporary embarrassment.Launching a product won’t exile you from the tribe. Modern society doesn’t work that way. The worst-case scenario is you learn what customers don’t want.Publishing your writing won’t poison you. It’s not possible. The worst-case scenario is some people don’t resonate with your ideas.All survivable. All recoverable. All ultimately beneficial in terms of learning and growth.But your brain can’t process that distinction. When you consider making a bold move - starting a business, publishing creative work, changing careers - your threat detection system treats it like mortal danger.The anxiety you feel isn’t proportional to the actual risk. It’s proportional to how your ancient brain categorizes the situation.And it categorizes all uncertainty as potential death.This gets amplified by a well-documented cognitive bias called loss aversion.Psychological research consistently shows that humans feel losses approximately 2-2.5x more intensely than equivalent gains.Losing $100 feels much worse than gaining $100 feels good. The pain of rejection hits harder than the pleasure of acceptance. The fear of failure weighs more heavily than the excitement of potential success.This made sense for survival. Missing one meal was more immediately dangerous than finding extra food was beneficial. Losing social status in a small tribe had severe consequences. Avoiding threats was more critical than pursuing opportunities.But in the modern world, where most risks are survivable and most losses are recoverable, loss aversion becomes a systematic source of bad decisions.Overweight small, temporary losses (time, money, ego)Underweight large, lasting gains (career transformation, financial freedom, personal growth)Avoid actions with asymmetric upside because you can’t stop fixating on the downsideStay in situations that are “okay” because changing feels like risking a lossYour brain is optimized for surviving threats, not capitalizing on opportunities.And in a world where threats are mostly non-fatal and opportunities are abundant, that optimization makes you systematically worse at assessing risk.Here’s where it gets insidious.You don’t just feel the fear. You rationalize it.You’re smart. You’re analytical. So your brain provides you with sophisticated-sounding reasons for why you shouldn’t try:“I need to be more prepared first.” “I should wait for better market conditions.” “I don’t have the resources yet.” “I need to research more before I commit.”These sound like rational calculations. They feel like smart risk management.But they’re not. They’re your threat detection system generating cover stories for the fear you don’t want to admit you’re feeling.You’re not being careful. You’re being scared and dressing it up in the language of prudence.And because you’re smart, you’re very good at this. You can build an entire intellectual framework around why now isn’t the right time. You can create spreadsheets that “prove” the risk is too high. You can cite examples of people who failed to validate your caution.But strip away the sophistication and here’s what’s actually happening:Your amygdala sees uncertainty. It generates fear. Your prefrontal cortex - the part that handles reasoning - works backwards to justify that fear with logic.You’re using math to rationalize emotion, not to make optimal decisions.And you know this is true because of what happens when you see other people take the exact bet you’re avoiding:You don’t think “Good, they confirmed my analysis was correct.”You think “Damn, I should have done that.”Here’s where it gets really insidious.Not only does your brain overestimate the risk of action, it also creates an illusion that inaction is safe.Staying at your job feels safe because it’s familiar. The pattern is established. You know what to expect.Not starting the business feels safe because you’re not exposing yourself to potential failure. You’re not risking embarrassment.Keeping your ideas private feels safe because you’re not subjecting them to judgment or rejection.But that feeling of safety is an illusion.Your job can disappear tomorrow. Companies restructure. Industries shift. The “stability” of employment is often just the absence of immediate threat, not actual security. You think you’re safe because you’re on payroll, but you’re actually maximally exposed - your entire income depends on one decision-maker who doesn’t owe you anything.Not starting the business means you’re entirely dependent on someone else’s decisions about your income. That’s not safe. That’s vulnerable. That’s putting all your risk into one basket labeled “employment” and hoping nobody drops it.Keeping your ideas private means you never develop them, never test them, never build anything of your own. That’s not safe. That’s guaranteeing you’ll never have the resources or leverage that come from building in public.The safe path isn’t actually safe. It’s just familiar.And your brain confuses familiarity with security.Here’s the thought that should haunt you: The riskiest thing you can do is to spend your entire life avoiding risk.Because while you’re busy playing it safe, the world is changing around you. The job you think is secure becomes obsolete. The industry you thought was stable gets disrupted. The career path you thought was guaranteed disappears.And when that happens - when the “safe” choice reveals itself as an illusion - you have nothing. No skills from attempting bold things. No network from building in public. No track record of betting on yourself. No resilience from failing and recovering.You optimized for safety and ended up with fragility.The people who took the “risky” bets? They built anti-fragility. They developed the capacity to survive failure. They created options. They’re not dependent on one income source, one employer, one path.You’ve been calculating risk backwards your entire life.This is why people stay in jobs they hate for decades. Why they remain in relationships that aren’t working. Why they never pursue the things they claim they want to do.Not because they’re lazy or stupid. Because their threat detection system is wired to prefer the familiar danger over the unfamiliar opportunity.The devil you know feels safer than the devil you don’t - even when the devil you know is slowly destroying your potential and the devil you don’t is actually just an opportunity in disguise.You’re not playing it safe. You’re playing not to lose. And playing not to lose is the only guaranteed way to lose.When you make that calculation conscious and explicit, you can see past the emotional response to the underlying reality:In a world where failure isn’t fatal, the asymmetry favors action.The biggest risk isn’t trying and failing. It’s letting an outdated survival mechanism dictate your entire life trajectory.The people who win aren’t smarter than you. They’re not more talented. They’re not luckier. They just have a higher tolerance for feeling scared while doing the thing anyway.That’s it. That’s the only difference.Now let’s talk about what that faulty calculation is actually costing you.III - The Guaranteed Cost of Never Trying“Twenty years from now you will be more disappointed by the things you didn’t do than by the ones you did.”But let’s say you don’t take the bet.You decide the risk is too high. You stay where you are. You keep preparing, keep planning, keep waiting for certainty.Most people think the cost is zero. They think “no decision” means “no risk.”They’re catastrophically wrong.The cost of not trying isn’t zero. It’s regret. And regret isn’t survivable. It’s permanent.I need to be very direct about this because it’s the part of the equation that people systematically ignore until it’s too late.When you try something and fail, here’s what actually happens:You feel disappointed for a period of time. Days, maybe weeks.Then you process it. You extract the lessons. You identify what didn’t work. You develop skills from the attempt. You have concrete data about what to avoid next time.You move forward with more information, more experience, and more resilience than you had before the attempt.The failure resolves. It becomes a data point. Eventually, it becomes a story you tell about your journey.I’ve failed at multiple business ideas. A clothing brand that never sold a single item. A software product that took six months to build and zero people wanted. Content projects that went nowhere.Do those failures haunt me? No.They taught me what markets don’t want. They showed me what approaches don’t work. They built my tolerance for rejection and uncertainty. They’re now just part of the path that led to the things that did work.Failure is temporary data collection.But when you don’t try - when you stay in the safe lane, when you keep preparing indefinitely, when you wait for certainty that will never come - here’s what happens:And that nothing You’re 30. You have an idea for a business. You don’t start because you’re “not ready.” You tell yourself you’ll do it when you have more savings, more skills, more certainty.You’re 35. The idea is still there. So is the hesitation. You’re busier now. The excuse shifts to “I don’t have time.” You watch someone else launch something similar. It hurts, but you convince yourself they probably got lucky or had advantages you don’t have.Here’s what you don’t say out loud: You spend 10 hours a week on Netflix, social media, and activities that don’t matter. You “don’t have time” the same way people “can’t afford” the gym membership but can afford daily coffee. Time isn’t the constraint. Courage is.You’re 40. The idea has been with you for a decade. You’ve refined it in your head. You know exactly how you’d do it. But now the excuse is “I’m too old to start over” or “I have too many responsibilities.”The truth you won’t admit: You’re more afraid now than you were at 30. Because now you have a decade of evidence that you’re someone who doesn’t follow through. And starting would mean confronting that identity.You’re 50. You don’t think about the idea as much. It’s been relegated to the “could have been” section of your mind. But every time you see someone doing what you wanted to do, something twists in your chest.And you tell yourself a story: “I had too many obligations.” “The timing was never right.” “I couldn’t afford to take the risk.”But the real story is simpler: You were scared. And you let that fear make your decisions for you for 20 years.You’re 60. The idea is a ghost. You tell younger people “I always wanted to do something like that” with a wistful tone that carries two decades of accumulated regret.You’re 70. You’re talking to your grandkids. They ask about your life. You have the job history, the stability, the safety. But there’s an absence. A path not taken. A version of yourself that never got to exist.I’ve had this exact conversation with dozens of people in their 40s, 50s, 60s. The details change but the pattern is always the same: an idea they saw clearly, a move they knew they should make, years of rational-sounding excuses, and now a quiet weight they carry everywhere.Here’s the truth that should shatter your entire risk calculation: You’re not avoiding the bet. You’re already taking it.Every day you don’t try is a bet. A bet that staying the same is better than trying and failing. A bet that comfort today is worth regret tomorrow. A bet that your fear is giving you accurate information about the future.You’re betting on yourself either way. The only question is what version of yourself you’re betting on.The one who tries and maybe fails? Or the one who never tries and definitely wonders?Not failure. Not embarrassment. Not lost time or money.Permanent uncertainty about who you could have become. A life spent wondering “what if?”And unlike a failed business or a rejected pitch or a product that didn’t sell, you can’t recover from that. You can’t A/B test it. You can’t iterate. You can’t go back and try.The opportunity window closes. The market shifts. Your energy changes. Your circumstances evolve. And the version of your life where you found out what was possible disappears forever.They all say the same thing, in different words:“I had this idea when I was younger. I knew what I wanted to build. I saw the opportunity clearly. But I was scared. I told myself it was too risky. I convinced myself I needed to be more prepared. Now I watch other people - often less capable people - doing exactly what I wanted to do. And I can’t stop thinking about what my life would look like if I had just tried.”The regret isn’t just about the missed opportunity. It’s about not knowing who they could have been. It’s about a door that’s now permanently closed.Here’s the mathematical truth nobody wants to confront:The probability of regret from not trying approaches 100% over a sufficient time horizon.The probability of catastrophic, unrecoverable failure from trying approaches 0%.You will never know if it could have worked (100% certainty of permanent uncertainty)You will watch others succeed in spaces you wanted to enter (compounds regret)You will accumulate “what if” thoughts that become more painful over timeYou will reach a point where the opportunity is no longer availableYou will face the reality that you let fear dictate your choicesAll of those outcomes are guaranteed. That’s not risk. That’s certainty.If you do try and it fails:You learn what doesn’t work (valuable)You develop skills from the attempt (transferable)You build resilience and reduce future fear (compounds capability)You have data for your next attempt (actionable)You move forward without the weight of wonderingThe “catastrophic failure” scenarios people imagine - complete financial ruin, total social ostracism, permanent career damage - almost never happen in reality.What actually happens: You’re out some time and money. Your ego bruises. You feel disappointed. Then you recover and try something else.You’re not avoiding risk by staying where you are. You’re just choosing a different, more painful, more permanent form of risk.The risk of action: Temporary discomfort, recoverable losses, valuable learning.The risk of inaction: Permanent regret, compounding opportunity cost, lifelong wondering.When you frame it correctly, the choice is obvious.This is what people mean when they say “the greatest risk is no risk at all.”Not because safety is impossible. But because the risk of a life unlived - the risk of reaching the end and realizing you never let yourself try - is worse than any survivable failure you could experience along the way.And here’s the cruel irony: the people who take the calculated bets, who try and sometimes fail, who build the tolerance for uncertainty - they’re the ones who end up with fewer regrets.Not because everything they try works. But because they know. They have data. They don’t wonder. They tried, they learned, they moved forward.The people who spend their lives avoiding risk? They’re the ones carrying the heaviest burden. The weight of all the unlived possibilities. All the versions of themselves they never let exist.That’s not safety. That’s a different kind of prison.IV - How To Actually Bet On Yourself (And Why You Won’t)But before I give you the steps, let me tell you what’s about to happen.You’re going to read these steps. You’re going to think “This is useful.” You might even screenshot them.And then you’re going to close this newsletter and do exactly what you were doing before you opened it.You’ll tell yourself “I’ll come back to this later.” You’ll save it in a folder labeled “Important.” You’ll add it to your ever-growing list of things you’re going to do “when you have time.”But later never comes. And you know it.Because this isn’t the first article you’ve read about taking action. This isn’t the first framework you’ve saved. This isn’t the first time you’ve felt motivated to start.It’s just the latest iteration of a pattern you’ve been running for years: consume information about change, feel inspired, do nothing.So let me be very clear about something:These steps only work if you actually do them. And you’re not going to do them unless you admit something uncomfortable first.You don’t have an information problem. You have a courage problem.You don’t need more frameworks. You need to stop pretending that one more system will be the thing that finally makes you feel ready.You’re not going to feel ready. Ever. The only way forward is to act before you feel ready.So here’s what to actually do - not because you need it, but because if you’re going to ignore it like you’ve ignored every other framework, I want you to at least be conscious about what you’re choosing.Start With The Smallest Asymmetric BetYou don’t have to quit your job tomorrow. You don’t have to bet everything on one idea. You don’t have to make a dramatic, all-or-nothing move.Start with the smallest bet that still has asymmetric upside.The kind of bet where the downside is so small it’s almost negligible, but the upside could be transformative.Publish one piece of writing online.Downside: 2 hours to write and edit. Maybe nobody reads it. You feel slightly vulnerable.Upside: It resonates with someone. They share it. It reaches people you don’t know. It opens a conversation. It demonstrates your thinking. It’s the first step toward building an audience.Reach out to one person you admire.Downside: 10 minutes to write a thoughtful email. They don’t respond. Your ego feels a small sting.Upside: They respond. You have a conversation. They share an insight that shifts your entire approach. They introduce you to someone in their network. They become a mentor. They hire you.Ship one small product or service.Downside: 20-40 hours to build something simple. You put a landing page up. Nobody buys. You feel disappointed.Upside: 10 people buy. You make $500. You prove you can create value people will pay for. You validate an idea. You have a foundation to build on. You gain confidence to create more.Record one video or podcast episode.Downside: 3 hours to plan, record, and publish. It gets 20 views. You cringe at the sound of your own voice.Upside: 100 people watch. Some of them subscribe. One person shares it. It starts a conversation. You develop the skill. You realize you enjoy the medium. It becomes a weekly practice that builds into something meaningful over time.The downside is so small - a few hours, temporary discomfort, maybe $100 - that it barely matters.The upside is completely disproportionate - skills, connections, opportunities, proof of concept, momentum.That’s asymmetry at the micro level.Before you take any of these bets, run this exercise. Tim Ferriss calls it “fear-setting” - a systematic way to define your fears instead of letting them define you.Take the bet you’re considering. Write it at the top of a page. Then create three columns: - What’s the worst that could realistically happen? Be specific. Not vague anxiety, but concrete outcomes. “I waste 3 months and $500.” “Nobody reads it and I feel stupid.” Write down every fear. - For each fear, what could you do to reduce the likelihood? “Start with a smaller version - 1 month and $100.” “Share with a small group first.” Most fears can be mitigated with basic precautions. - If the worst case happens, how would you recover? “I could save $500 in two months.” “I could use what I learned to try a different approach.” Be specific about the recovery path.What you’ll discover when you write it out is that the worst case is: unlikely to happen, not that bad, and recoverable.Most of your fears dissolve when you define them specifically.The vague anxiety of “what if it doesn’t work” feels insurmountable. But “I’ll be out $500 and some time, and I can recover both in two months” is manageable.That’s the difference between imagined catastrophe and actual risk.Pick one small bet. The smallest one that scares you just enough to matter.And do it in the next 7 days.Not eventually. Not when you’re ready. In the next 7 days.Do the thing. Ship the thing. Hit publish. Send the email. Launch the offer.Then take another bet. Then another. Build the pattern.Each bet de-risks the next one because you’re developing skills, building resilience, and accumulating proof that you can survive uncertainty.Let me tell you what’s really happening right now.You’re reading this newsletter. You’re nodding along. You’re thinking “This makes sense. The math is clear. I should take more asymmetric bets.”And tomorrow, you’ll do nothing.You’ll go back to your job. You’ll scroll social media. You’ll think about starting that project. You’ll tell yourself “soon.”You’ll make the same calculation you’ve been making for years: The risk is too high. I’m not ready. I need more time.And the days will stack into weeks. The weeks into months. The months into years.And five years from now, you’ll be reading another article about asymmetric upside and thinking “This makes sense” and doing nothing.Because here’s what you haven’t realized yet: understanding the math changes nothing.You already knew most of this before you started reading. You already knew that the downside of trying is survivable. You already knew that regret is worse than failure. You already knew that smart people stay stuck not because they’re stupid but because they’re scared.You didn’t need me to explain asymmetric upside. You needed me to give you permission to keep waiting.And I’m not giving you that permission.The asymmetry of upside makes bold moves mathematically smart. That’s not inspiration. That’s fact.The downside of trying and failing is usually survivable. You recover. You learn. You move forward. That’s data.The downside of never trying is guaranteed regret. You never know. You never build. You spend decades wondering what could have been. That’s certainty.Bet on yourself more often. The expected value is higher than you think. That’s math.But none of that matters if you don’t actually do the thing.And you know what the thing is. You’ve known for months. Maybe years.It’s the project you keep planning but never starting.It’s the email you keep drafting but never sending.It’s the business idea you keep refining but never launching.It’s the content you keep researching but never creating.It’s the bet you keep calculating but never taking.You know exactly what it is. The thing that would change everything if you just did it. The thing that scares you precisely because it matters.Here’s the uncomfortable truth you’ve been avoiding:You’re not actually trying to make a smart decision. You’re trying to find a mathematical justification for your fear.The spreadsheet is cover. The research is procrastination. The “one more course” is avoidance.You don’t need more information. You don’t need better tools. You don’t need perfect conditions.You need to stop lying to yourself about why you haven’t started.And the biggest lie you tell yourself?“I’ll do it when I’m ready.”But here’s what “ready” actually means in your internal calculus: “When it doesn’t feel scary anymore. When I’m guaranteed not to fail. When I’m certain it will work.”You will never feel ready. The fear will never fully go away. The uncertainty will never completely resolve.The people who succeed at the things you want to do aren’t more ready than you. They’re not more talented. They’re not more prepared.They just started before they felt ready. They took the bet while it still scared them. They understood that the feeling of readiness comes AFTER you start, not before.So here’s the only question that matters:Are you going to make the same excuses next year?Because that’s the actual decision you’re making right now. Not “should I start this thing.” You already know you should.The decision is: Am I the kind of person who lets fear make my decisions? Or am I the kind of person who runs the math and makes the bet?The math is clear. The asymmetry is obvious. The only question is: what are you going to do about it?]]></content:encoded></item><item><title>Aravind Barla Rebuilt Enterprise Service Delivery for 100,000+ Employees</title><link>https://hackernoon.com/aravind-barla-rebuilt-enterprise-service-delivery-for-100000-employees?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Sun, 25 Jan 2026 19:50:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In the increasingly complex landscape of enterprise service management, the remarkable transformation led by Aravind Barla at a global Fortune 100 company stands as a compelling testament to visionary leadership and strategic technical execution. This next-generation Employee Experience Platform (EXP) implementation, serving over 100,000 employees across multiple continents, has redefined industry standards for service delivery integration and AI-driven automation while establishing new benchmarks for what's possible in large-scale digital transformation initiatives.Inheriting a Fragmented EcosystemWhen Aravind Barla stepped into the technical leadership role for this ambitious project in early 2023, the organization was struggling with deeply entrenched challenges that had evolved over decades of technological evolution. Service delivery across IT, HR, and Workplace teams operated in isolated silos, creating inconsistent user experiences, redundant processes, and frustrating inefficiencies that affected every level of the organization. Employee satisfaction scores had reached concerning lows, with internal surveys revealing that employees spent an average of 5.2 hours per week navigating various service portals and tracking request statuses.\
The financial impact of this fragmentation was equally troubling. The operational overhead of maintaining multiple disconnected systems was draining approximately $4.7 million in annual resources, while the productivity loss from inefficient service delivery was estimated at over $9 million yearly. Previous attempts to address these issues had resulted in only incremental improvements, with departmental boundaries and technical debt presenting seemingly insurmountable obstacles.\
The complexity of unifying service delivery across an organization of this scale presented formidable challenges that had derailed similar initiatives. Two previous implementation attempts had stalled due to cross-departmental resistance, technical integration challenges, and the sheer complexity of standardizing processes across global operations. What the company needed was not just technical expertise, but a leader who could align diverse stakeholders around a unified vision while delivering a technically robust solution that would scale across the enterprise.Architectural Vision and Implementation StrategyAt the heart of Aravind's approach was a commitment to fundamentally reimagine how employees interacted with enterprise services. Rather than pursuing incremental improvements to existing systems, he championed a comprehensive transformation built on ServiceNow's platform capabilities, enhanced by custom AI implementations, virtual agents, and intelligent workflow automation. His vision extended beyond technical architecture to encompass the entire employee journey – from initial need identification through to resolution and feedback.\
Aravind Barla assembled an initial discovery team comprising business analysts, UX researchers, and service delivery leaders to conduct a thorough assessment of the current state. This six-week investigation revealed that employees were navigating an average of 12 different portals for various service needs, with inconsistent terminology, conflicting interfaces, and redundant data entry requirements creating significant friction. Armed with these insights, Aravind developed a comprehensive transformation roadmap that balanced quick wins with long-term structural improvements.\
The cornerstone of this strategy was a unified intake layer that would serve as the foundation for enterprise service delivery. Aravind personally architected this sophisticated system, consolidating over 15 legacy intake points into a single, intuitive portal accessible across all devices and locations. He pioneered the implementation of natural language processing capabilities that could interpret employee requests regardless of the terminology used, effectively breaking down the language barriers between departments.\
Perhaps most impressively, he introduced a revolutionary new taxonomy for service categorization that created a common language across previously siloed departments. This standardized approach to service classification required extensive collaboration with subject matter experts from each functional area, with Aravind facilitating numerous workshops to build consensus around the new framework. His diplomatic skills proved as valuable as his technical expertise during this critical phase, as he navigated competing priorities and entrenched departmental perspectives.\
The technical implementation phase showcased Aravind Barla's exceptional leadership capabilities under pressure. He assembled and guided a cross-functional team of 18 developers, UX specialists, and business analysts, fostering a collaborative environment despite aggressive timelines and complex integration challenges. His hands-on approach to designing end-to-end workflow automation ensured that the theoretical architecture translated into practical, efficient solutions that would deliver measurable business value.\
Throughout the development process, Aravind instituted weekly stakeholder reviews and bi-weekly user testing sessions, creating a feedback loop that allowed for continuous refinement. This iterative approach enabled the team to identify and address potential issues early, resulting in a solution that not only met technical requirements but truly resonated with end users. His insistence on maintaining this rigorous review process, despite timeline pressures, proved instrumental in delivering a platform that achieved exceptional adoption rates.Breakthrough Results and Organizational ImpactThe results of Aravind Barla's implementation were both immediate and substantial, exceeding expectations across all key performance indicators. Within the first quarter of launch, the platform had achieved a remarkable 70% increase in self-service adoption – a metric that directly translated to significant operational efficiencies and enhanced user satisfaction. Average resolution times for employee requests plummeted from 72 hours to under 24 hours, representing a transformative improvement in service delivery speed and quality.The financial impact was equally impressive and provided clear validation of the investment in this initiative. By automating more than 65% of Tier-1 service requests through intelligent virtual agents and workflow automation, the platform enabled annual cost savings of $3.2 million in operational overhead. This achievement alone would have justified the implementation, but the qualitative improvements were perhaps even more significant in terms of organizational impact.\
Employee satisfaction scores rose by 40% within the first quarter, creating a measurable improvement in workforce experience across the global organization. This improvement was particularly notable among remote and hybrid workers, who reported feeling more connected to organizational support systems through the unified platform. The implementation also yielded unexpected benefits in terms of data visibility and analytics, providing leadership with unprecedented insights into service demand patterns and operational bottlenecks.\
The platform's success during a period of significant organizational change further demonstrated its resilience and value. During a major acquisition that brought 12,000 new employees into the organization, Aravind Barla's system seamlessly expanded to accommodate the increased load and new service requirements, maintaining performance standards throughout the integration process. This scalability validated his architectural decisions and reinforced the strategic value of the implementation.Industry Recognition and Career AdvancementThe exceptional performance and innovative approach demonstrated by Aravind quickly garnered attention both within and beyond the organization. His implementation became the subject of an in-depth industry case study highlighting best practices in enterprise service management and AI-driven automation. A prominent technology publication featured the project in a press article showcasing innovative applications of AI in employee services, positioning both Aravind and the organization as thought leaders in digital workplace transformation.\
At technology conferences across North America, the implementation was referenced as a model example of successful enterprise service integration, with several competitors subsequently launching similar initiatives based on the framework Aravind had pioneered. This industry recognition not only elevated the organization's reputation for innovation but created valuable recruitment advantages in attracting top technical talent.\
Within the organization, Aravind's leadership earned him a promotion to technical lead for enterprise automation strategy – a role that expanded his influence across the company's global technology initiatives and positioned him as a key advisor to the CIO on future digital transformation efforts. His team members also benefited from their association with the successful project, with several receiving promotions and recognition for their contributions.\
Most significantly, the framework Aravind developed has since been adapted by multiple business units worldwide, creating a lasting legacy that extends far beyond the initial implementation. The modular architecture he designed has proven flexible enough to accommodate regional variations while maintaining the core benefits of standardization and integration, allowing for global scaling without sacrificing local relevance.For Aravind Barla, this project represented more than just a successful implementation – it marked his evolution from technical practitioner to thought leader and innovator in the enterprise automation space. The approach he pioneered demonstrated how thoughtful architecture, combined with strategic leadership, could overcome entrenched challenges that had previously seemed insurmountable in large-scale enterprise environments.\
The success validated Aravind Barla's fundamental philosophy toward technology implementation: that truly transformative solutions must balance technical excellence with deep understanding of human experience. Throughout the project, he maintained a dual focus on system optimization and user-centered design, recognizing that even the most sophisticated automation would fail without genuine adoption by employees at all levels of the organization.\
His commitment to measuring impact through both quantitative and qualitative metrics established a new standard for evaluating technology investments within the organization. By consistently connecting technical decisions to tangible business outcomes, Aravind changed the conversation around enterprise technology from cost center to value driver, securing executive support for continued innovation in the employee experience space.\
The impact of Aravind's work continues to resonate throughout the organization and beyond its boundaries. By creating a blueprint for intelligent service integration, he has influenced how enterprises approach employee experience design and service delivery automation. His success validates a fundamental principle: that technology implementations, when guided by clear vision and executed with technical excellence, can transform not just systems but the everyday experiences of thousands of employees.As organizations worldwide continue to prioritize digital employee experience in an increasingly distributed work environment, Aravind Barla's implementation stands as a powerful example of how leadership excellence can drive transformative outcomes in enterprise technology. The ripple effects of his innovation continue to expand, inspiring a new generation of technical leaders to pursue similarly ambitious visions for workplace transformation.Aravind Barla is a distinguished leader in enterprise technology innovation, specializing in the intersection of automation, AI, and employee experience solutions. Based in Dublin, California, Aravind combines deep technical expertise with strategic vision to deliver transformative implementations that redefine how organizations approach service delivery and digital employee experience. With dual Master's degrees in Computer Science and Information Technology & Management from prestigious institutions, he brings a unique blend of technical proficiency and business acumen to complex enterprise challenges.\
Throughout his career, Aravind has been recognized for his ability to bridge the gap between advanced technology capabilities and practical business applications. His technical expertise spans ServiceNow platform architecture, AI/ML implementation, workflow automation, and integration of disparate enterprise systems. Beyond his technical skills, Aravind has demonstrated exceptional leadership in building high-performing teams and guiding organizations through complex digital transformations.\
His career has been defined by a commitment to creating systems that not only enhance operational efficiency but fundamentally improve how people experience work in the digital age. Through his leadership on high-impact initiatives for Fortune-ranked organizations, Aravind has established himself as a thought leader and innovator in the enterprise automation space, consistently driving strategic transformation that delivers measurable business outcomes while enhancing human experiences.\
Aravind Barla regularly contributes to industry conversations through speaking engagements, technical publications, and participation in professional communities focused on the future of work and enterprise technology. His forward-thinking approach to technology implementation continues to influence how organizations conceptualize and execute digital workplace strategies in an increasingly complex business environment.]]></content:encoded></item><item><title>How to Run Claude Code With Local Models Using Ollama</title><link>https://hackernoon.com/how-to-run-claude-code-with-local-models-using-ollama?source=rss</link><author>Vladislav Guzey</author><category>tech</category><pubDate>Sun, 25 Jan 2026 19:39:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In January 2026, Ollama added support for the Anthropic Messages API, enabling Claude Code to connect directly to any Ollama model. This tutorial explains how to install Claude Code, pull and run local models using Ollama, and configure your environment for a seamless local coding experience. is a locally deployed AI model runner that lets you download and run large language models on your own machine. It provides a command-line interface and an API, supports open models such as Mistral and Gemma, and uses quantization to make models run efficiently on consumer hardware. A  allows you to customise base models, system prompts, and parameters (temperature, top-p, top-k). Running models locally gives you offline capability and protects sensitive data.To use Claude Code with local models, you need . The January 2026 blog notes that this version implements Anthropic Messages API compatibility. For streaming tool calls (used when Claude Code executes functions or scripts), a pre-release such as  may be required.curl -fsSL https://ollama.com/install.sh | sh
After installation, verify the version with Choose a local model suitable for coding tasks. You can see the full list on  website. Pulling a model downloads and configures it. For example:# Pull the 20 B parameter GPT‑OSS model
ollama pull gpt-oss:20b
# Pull Qwen Coder (a general coding model)
ollama pull qwen3-coder
To use Claude Code’s advanced tool features locally, the article Running Claude Code fully local recommends  because it supports tool-calling and provides a 128K context length. Pull it with:ollama pull glm-4.7-flash:latest
 is Anthropic’s agentic coding tool. It can read and modify files, run tests, fix bugs, and even handle merge conflicts across your entire code base. It uses large language models to act as a pair of autonomous hands in your terminal, letting you  (describing what you want in plain language and letting the AI generate the code).curl -fsSL https://claude.ai/install.sh | bash
export ANTHROPIC_AUTH_TOKEN=ollama
export ANTHROPIC_BASE_URL=http://localhost:11434
# Launch the integration interactively
ollama launch claude
Then you will see the model list that you installed in the previous step. Select the one you want to test, then hit Enter.Press enter or click to view image in full size   And that’s it! Now your Claude code works with Ollama and local models.By pairing  with , you can run agentic coding workflows entirely on your own machine. Don’t expect the same experience as with the Anthropic models!Experiment with different models and share with me which one worked the best for you!]]></content:encoded></item><item><title>Richard Stallman Critiques AI, Connected Cars, Smartphones, and DRM</title><link>https://news.slashdot.org/story/26/01/25/1930244/richard-stallman-critiques-ai-connected-cars-smartphones-and-drm?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 19:36:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Richard Stallman spoke Friday at Atlanta's Georgia Institute of Technology, continuing his activism for free software while also addressing today's new technologies. 

Speaking about AI, Stallman warned that "nowadays, people often use the term artificial intelligence for things that aren't intelligent at all..." He makes a point of calling large language models "generators" because "They generate text and they don't understand really what that text means." (And they also make mistakes "without batting a virtual eyelash. So you can't trust anything that they generate.") Stallman says "Every time you call them AI, you are endorsing the claim that they are intelligent and they're not. So let's let's refuse to do that." 

"So I've come up with the term Pretend Intelligence. We could call it PI. And if we start saying this more often, we might help overcome this marketing hype campaign that wants people to trust those systems, and trust their lives and all their activities to the control of those systems and the big companies that develop and control them." 

"By the way, as far as I can tell, none of them is free software." 

When it comes to today's cars, Stallman says they contain "malicious functionalities... Cars should not be connected. They should not upload anything." (He adds that "I am hoping to find a skilled mechanic to work with me in a project to make disconnected cars.") 

And later Stallman calls the smartphone "an Orwellian tracking and surveillance device," saying he refuses to own one. (An advantage of free software is that it allows the removal of malicious functionalities.) 

Stallman spoke for about 53 minutes — but then answered questions for nearly 90 minutes longer. Here's some of the highlights...]]></content:encoded></item><item><title>With CBDCs, Central Banks Are Making the Same Mistake Kodak Made</title><link>https://hackernoon.com/with-cbdcs-central-banks-are-making-the-same-mistake-kodak-made?source=rss</link><author>Edwin Liava&apos;a</author><category>tech</category><pubDate>Sun, 25 Jan 2026 19:33:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When Laughter Reveals Everything You Need to KnowAt the World Economic Forum in Davos this week, the French Central Bank Governor laughed. Not a polite chuckle, but an audible dismissal i.e. the kind of laugh that reveals someone hasn't taken the time to understand what they're dismissing. The subject? Bitcoin's potential to compete with fiat currency.His laughter wasn't unique. It echoed through the alpine halls where the architects of the current financial system gather annually to discuss how that system should work. But history has taught us that such laughter is rarely the sound of confidence. More often, it's the sound of an incumbent who hasn't yet been forced to take a threat seriously.And I've heard that laugh before. In boardrooms at Kodak. In executive suites at Blockbuster. In strategy meetings at Nokia. It's the laugh of someone who believes their moat is permanent, their position unassailable, their understanding complete.The Photo CD Parallel - When Innovation Serves The Old ModelIn my previous piece, "," I explored how centralized monetary systems fail to adapt to community needs. But watching Brian Armstrong, CEO of Coinbase, patiently explain Bitcoin's fundamentals to a central banker who clearly hadn't done his homework revealed something even more profound i.e. central banks aren't just resistant to Bitcoin, they're repeating one of history's most spectacular corporate failures.Let me take you back to 1990.Kodak, the undisputed king of photography, stood at a crossroads. Digital photography was emerging, and, ironically, Kodak had invented the first digital camera back in 1975. They had fifteen years to prepare. They had the patents, the technology, the brand recognition, and the market dominance. They had everything except the courage to cannibalize their own business model.So instead of embracing the digital revolution they had pioneered, Kodak launched Photo CD.Photo CD was Kodak's compromise solution, a way to enter the digital age without abandoning film. Customers would still shoot film, still develop film, but then their photos would be scanned onto a proprietary CD format that could be viewed on special Photo CD players. It was digital, technically. But it was digital on Kodak's terms. Digital that still required film. Digital that still generated revenue from Kodak's existing infrastructure.The format was technically impressive. It used a sophisticated color space, multiple resolutions, and even some "super white" information that resembled primitive RAW files. Professional photographers appreciated the quality. But there was one problem i.e., it was a solution designed to preserve the old system, not to embrace the new one.Photo CD required proprietary players ($400 each). It used a format that would later become "orphaned" when Kodak abandoned it. Most damningly, it solved yesterday's problem while the world was already moving toward tomorrow's solution i.e. true digital cameras that eliminated film entirely.By 2004, processing labs were offering to pay people to haul away their Kodak 4050 Photo CD scanners. By 2012, Kodak filed for bankruptcy.CBDCs - The Photo CD of MoneyNow let's talk about Central Bank Digital Currencies (CBDCs).Just as Kodak invented digital photography but refused to fully embrace it, central banks understand that money is going digital. They've watched Bitcoin operate flawlessly for sixteen years. They've seen stablecoins gain adoption. They've witnessed the rise of decentralized finance. They know change is coming.So what's their response? CBDCs, digital currency on their terms. Digital currency that preserves central control. Digital currency that maintains the existing power structure.Like Photo CD, CBDCs are technically sophisticated. They could offer faster payments, programmable money, and improved financial inclusion. Central banks cite these benefits constantly. But scratch the surface, and you'll find the same fundamental flaw that doomed Photo CD, i.e. CBDCs are designed to preserve the old system, not to embrace the new paradigm.Consider what CBDCs actually represent: over a decentralized technologySurveillance capabilities that would make physical cash obsoleteProgrammable restrictions on how you spend your own moneyElimination of financial privacy under the guise of fighting crimeThe same trusted intermediaries in a world that's discovering the value of trustlessnessEven the Federal Reserve has quietly acknowledged that CBDCs are "a solution in search of a problem" in countries like the United States, where digital payment systems already work efficiently.But the most revealing parallel isn't in the technology, it's in the psychology. Just as Kodak's management couldn't imagine a world where people wouldn't need film, central bankers can't imagine a world where people don't need Fiat money. The French Central Bank Governor's laughter at Davos stemmed from the same place as Kodak's dismissal of digital cameras i.e. "This threatens our business model, therefore it must not be serious."When Brian Armstrong Met the Status QuoThe exchange at Davos deserves to be studied in business schools as a case study in how incumbents fail to recognize disruption. Let me walk you through it.Armstrong made a straightforward point i.e. the current fiat system was only created in 1971 when Nixon abandoned the gold standard. Before that, money had some connection to hard assets. Now, democracies worldwide struggle with fiscal discipline, running persistent deficits that cause inflation. Bitcoin represents a return to "sound money", money with a fixed supply that can't be debased.The French Central Bank Governor's response was telling i.e. "Money is part of sovereignty… if we lose that really you lose a key function of democracy."Think about that framing. Not "money is a tool for economic efficiency." Not "money is a neutral medium of exchange." But "money is part of sovereignty", a blunt admission that central banks view money as a tool of state power.When Armstrong pointed out that Bitcoin is even more independent than central banks, having no issuer, no board, no political pressure, no emergency powers, the Governor laughed. Not because Armstrong was wrong. He laughed because Armstrong was so obviously right that it threatened the entire conceptual framework the Governor operates within.The Governor then claimed to trust "independent central banks with a democratic mandate" more than "private issuers of Bitcoin."This revealed a complete misunderstanding of what Bitcoin is. Bitcoin has no issuers, private or otherwise. It's a decentralized protocol. The fact that a central bank governor doesn't grasp this basic point while dismissing Bitcoin is like Kodak's leadership not understanding how a digital sensor works while dismissing digital cameras.It's not confidence. It's ignorance dressed up as authority.The Pattern Repeats ItselfHere's what Kodak's leadership didn't understand i.e. you can't compromise your way through a paradigm shift. You can't preserve the old system while pretending to embrace the new one. The market eventually forces you to choose, and by the time you're forced to choose, it's usually too late.Kodak tried to have it both ways with Photo CD. They tried to preserve film while going digital. They failed.Central banks are trying to have it both ways with CBDCs. They're trying to preserve centralized monetary control while going digital. They will fail for the same reason.The pattern is remarkably consistent across industries: "People will always want the quality and familiarity of film. We'll give them digital as an add on to their film workflow." People wanted the convenience, cost savings, and creative freedom of pure digital photography. Film became a niche product for enthusiasts. "People will always need the stability and legitimacy of state backed money. We'll give them digital currency with all the benefits of crypto, but with trusted institutions still in control." People are discovering the benefits of money that can't be debased, can't be seized arbitrarily, can't be inflated away, and doesn't require permission to use. Fiat is becoming a depreciating medium of exchange while Bitcoin becomes a store of value.Why This Time Really Is DifferentThe central bankers at Davos would argue that governments have power that Kodak never had. They can regulate. They can ban. They can use the force of law.They're right, of course. But this misses the point entirely.Kodak didn't fail because they lacked power over their customers. They failed because the underlying technology shifted, and the old business model became obsolete faster than the old company could adapt. No amount of market power can protect you when the market itself is changing.Money is undergoing the same fundamental shift. For thousands of years, money required a trusted intermediary, someone to mint the coins, someone to issue the notes, someone to validate the ledgers. Trusted intermediaries meant centralized power. Centralized power meant the ability to debase, confiscate, and control.Bitcoin changed the equation. For the first time in human history, we have money that works without trusted intermediaries. Money that's globally accessible, cryptographically secure, provably scarce, and resistant to seizure or debasement.This isn't an incremental improvement. This is a fundamental change in what money can be, as fundamental as the change from analog to digital photography.Central banks launching CBDCs is like Kodak launching Photo CD. It's an admission that change is happening, coupled with a desperate attempt to control that change rather than embrace it. It's innovation in service of preservation.There's a reason Brian Armstrong stayed calm during that exchange in Davos. He wasn't trying to convince the French Central Bank Governor. You can't convince someone whose salary depends on not understanding.Armstrong was speaking to everyone else. To the entrepreneurs building on Bitcoin. To the investors watching capital flow. To the citizens of countries experiencing monetary mismanagement. To anyone willing to think independently about what money should be.The choice is binary, and it's the same choice photography faced:Photo CD or digital camera?Proprietary format that preserves the old model, or open standard that enables the new one?Programmable money that enhances state control, or programmable money that enhances individual sovereignty?Managed decline or creative destruction?The comfortable illusion that we can preserve the old system with new technology, or the uncomfortable truth that paradigm shifts don't ask permission?In 1990, when digital cameras started appearing, Kodak executives dismissed them. The resolution was too low. The workflow was too complicated. Customers loved film. The barriers to entry in film manufacturing protected them.They were right about all of those things in 1990. They were catastrophically wrong about where things were headed.In 2026, when Bitcoin crossed $100,000 and central banks started taking it seriously enough to issue CBDCs, many of those same central bankers laughed at the idea that Bitcoin could compete with fiat currencies. The volatility was too high. The user experience was too complicated. People trusted government money. Legal tender laws protected them.They're right about some of those things today. But they're missing the direction of travel.Photo CD worked, technically. But it was solving the wrong problem. It was trying to make the transition from analog to digital while keeping the analog infrastructure intact.CBDCs will work, technically. But they're solving the wrong problem. They're trying to make the transition from centralized to decentralized money while keeping the centralized power structure intact.The market doesn't care about your compromise solutions. The market cares about what works better for users. And just as digital cameras worked better than film + Photo CD, Bitcoin works better than fiat + CBDCs for anyone who values: over infinite money printing access over gatekeepers over opaque monetary policy over surveillanceThe Verdict Is Already InHere's the thing about paradigm shifts i.e. by the time the old guard realizes what's happening, the new paradigm has usually already won. The lag time between "they're laughing at us" and "they're fighting us" and "we won" can feel glacially slow when you're living through it. But from a historical perspective, these transitions happen remarkably fast.Kodak filed for bankruptcy in 2012, just 22 years after launching Photo CD. The company that had dominated photography for over a century was gone in a generation.How long will it take for central banks to realize that CBDCs are their Photo CD moment? That they're building sophisticated technology to preserve an obsolete system? That the laughter in Davos was the sound of people who haven't yet realized they've already lost?I don't know. But I do know this i.e. the French Central Bank Governor's laughter will stop. Just like Kodak's dismissiveness stopped. Not because Bitcoin convinced them, but because the market made their opinion irrelevant.But here's what makes this paradigm shift different from Kodak's fall and ultimately more inevitable:Photography was about capturing moments. When digital cameras proved superior, professionals switched, then enthusiasts, then everyone else. It was a technological transition driven by better tools.But money? Money is about . About our labor, our savings, our future, our families, our freedom. And the uncomfortable truth that people are beginning to understand is this i.e. fiat currency was never designed with them in mind.Fiat was designed for governments to fund wars without immediate taxation. It was designed to allow deficit spending without political accountability. It was designed to transfer wealth silently through inflation from savers to debtors, from citizens to states. It was designed to give central planners the power to manipulate economic behavior through monetary policy.These weren't bugs. They were features. Features that benefited those who control the system at the expense of those who simply use it.When people truly internalize this, when they understand that their purchasing power declining isn't an unfortunate side effect but an intended outcome, that their savings being debased isn't bad luck but policy, that their financial privacy disappearing isn't about safety but about control, they don't just switch technologies.And that paradigm points directly to Bitcoin and crypto.Not because Bitcoin is perfect. Not because crypto solves every problem. But because for the first time in modern history, people have a choice. They can opt into a monetary system that's:, not for governments in its rules, not subject to political whims in supply, not infinite in creation to anyone with internet, not controlled by gatekeepers at the individual level, not the institutional levelThe question isn't whether CBDCs or Bitcoin will win. That question was answered the moment Bitcoin proved that money could work without centralized control, just as digital cameras proved that photography could work without film.The only question is how long it takes everyone else to realize it.And critically: how many people need to wake up before the paradigm shifts irreversibly?History suggests the answer is i.e. not as many as you think. Technological adoption follows an S curve. It seems slow, then suddenly it's everywhere. We're still in the "slow" phase where central bankers laugh. But every person who studies Bitcoin for their 10,000 hours, every business that adds it to their treasury, every country that accumulates it as a strategic reserve, they're not going back.You can't un ring this bell.And whether you're positioned on the right side of history when enough people wake up to the truth about fiat currency determines more than your portfolio returns.It determines whether you saw the paradigm shift coming, or whether you laughed at it like a French central banker in Davos.As my previous article argued, fiat systems can't respond to community values and needs the way Bitcoin can. But this goes deeper. Central banks can't even respond to their own obsolescence. They're too invested in the old system to imagine the new one.Kodak invented the digital camera and went bankrupt anyway. Central banks understand blockchain technology and will lose their monopoly anyway.Some patterns in history don't just rhyme. They repeat exactly.And they always end the same way.The difference this time? Money is about people. And people are waking up. When they realize fiat was designed to benefit the system, not them, they won't just adopt new technology. They'll shift the entire paradigm.That paradigm points to Bitcoin and crypto.The only question is whether you'll be laughing in Davos or whether you'll be ready.]]></content:encoded></item><item><title>Censorship Resistance in Crypto Networks (Or Why Your Money is Unstoppable)</title><link>https://hackernoon.com/censorship-resistance-in-crypto-networks-or-why-your-money-is-unstoppable?source=rss</link><author>Obyte</author><category>tech</category><pubDate>Sun, 25 Jan 2026 19:22:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If we look for the general meaning of censorship, we’ll find something like this: “the suppression or removal of writing, artistic work, etc. that are considered obscene, politically unacceptable, or a threat to security” [Oxford Languages]. A key detail missing in that concept is according to whom. According to whom, is XYZ obscene, unacceptable, or a threat? Why? And why must everyone else accept that this “who” changes the truth just because they want to? You see, this is why crypto was born: to be censorship-resistant.In our context, censorship-resistance happens when anyone, everywhere, anytime, is capable of accessing and using a crypto network, and no third party (government, company, or powerful individual) can stop them from doing so. Once data or a transaction is recorded in this system, it can’t easily be altered, blocked, or deleted, and nothing can prevent it from being recorded.Let’s see how this plays out.Why Censorship Resistance Matters in FinanceBesides writing or artistic works, monetary transactions can also be censored and stopped.  is even common all across the world, and in many cases includes innocent people. For example, victims from oppressive regimes, in which ‘politically unacceptable’ and ‘threat to security’ mean they’re just disagreeing and being unfairly punished for it.Banks and similar financial firms follow rules established by governments (no matter if they’re good or bad), and they can freeze any account at any moment. As we mentioned above, drastic cases include government oppression, but that’s not the only reason for censorship. PayPal, for instance, has a lot of .In 2005, they blocked an account opened to raise Hurricane Katrina relief funds. After nearly $28,000 were donated in nine hours, the processor alleged fraud and froze the funds. They even refused to donate this money themselves. In 2010, WikiLeaks was its next victim, due to regulatory pressure. Other people have experienced censorship just because they don’t use their accounts enough, so, apparently, that’s suspicious.No one should be able to impede you from transacting with your legitimately owned money, period. Cryptocurrencies were built to avoid this exact situation by providing a technical stack that cannot be tampered with.Tech Doing the Magic in CryptoWe have some techniques that set crypto apart from banks or centralized companies as far as resistance to censorship and openness is concerned: a pseudonymous distributed network, , and consensus mechanisms. They all vary from network to network, but they must be there to even give it a try at being immutable.A distributed network is a system where many computers (called nodes) run the same software and keep copies of the same data. In this case, our software is the involved cryptocurrency.Instead of having a single central server, there are hundreds or thousands of devices worldwide running the same ledger of transactions. No identification for them is required, just a “pseudonym” —which could be an alphanumeric address. If one node goes out, the rest keep working, and anyone can be part of this. Anyone can be their own node.Political decentralization implies that control of that distributed network is also distributed: no single party (company, organization, or individual) can make big decisions. In many cases, the code is open-source (anyone can check it and modify it), nodes are , and there are no companies behind. Even if there are, they shouldn’t hold the power to change the network.Finally, consensus mechanisms are the processes that allow those nodes to agree on the same result, without needing to know or trust each other. They’re not a list of recommendations, but a set of strict, automated rules engraved in unchallengeable code. Examples of it include Proof of Work (PoW) and Proof-of-Stake (PoS).By mixing these elements, we can avoid censorship most of the time, because there’s no single place or party to shut down or take over. If a powerful faction wants to block funds, they’d need to stop all these independent nodes at once or change all previous transactions. Furthermore, operations are governed solely by the rules of the code.Bitcoin is, of course, the first example of this kind of decentralized currency. It’s not perfect (nothing is), but it’s been robust enough to get this far since its launch in 2009 by an anonymous author. It’s open-source, pseudonymous, and there’s no company or organization behind it. Just a network formed by thousands of nodes worldwide.Its consensus mechanism is Proof-of-Work, in which every node must provide some ‘work’ by solving a complex cryptographic puzzle () before getting the right to add a new batch (block) of valid transactions.Until now, no one has been able to block Bitcoin transactions at the protocol level. There are some risks with a PoW system, though, because miners are the ones deciding which transactions to include in a block. A 51% attack could happen if most of the miners collude: they could change the blockchain history. However, this is unlikely and  in big networks like Bitcoin. The cost is usually not worth it. While rewriting the history is expensive, preventing individual transactions from being included in blocks is possible and costs nothing, if the 51% majority of miners agrees to do so.Ethereum is also a distributed, open-source network, but it uses another consensus mechanism: Proof-of-Stake. This one replaces miners and ‘work’ for “validators” and tokens. Nodes must block (stake) a certain number of coins if they want the right to approve transactions. This ‘right’ could lead to censorship, though. After the  sanctions, for instance, some validators simply played it safe and skipped the sanctioned transactions. takes a step further by getting rid of all middlemen between a transaction and its final approval —including miners and “validators.” This crypto network uses a Directed Acyclic Graph (DAG) instead of a blockchain, which allows every user to add their own transactions to the system. No external approval needed. This way, Obyte is a more censorship-resistant and decentralized ecosystem.For better or for worse, cryptocurrencies aren’t disconnected from the traditional financial system… and that means their ‘decentralized’ powers have limits. If you want to trade them for your national currency, there’ll be compliance rules.  on exchanges. Risk of blocked accounts and frozen funds on exchanges, of course. No matter how censorship-resistant the crypto network is, per se, if you’re using the services of a middleman (a company). These are at the mercy of regulations and potential censorship. are another big limit as well. Some countries have banned crypto entirely, or have put heavy limitations to its use. For instance, it’s completely illegal for individuals in Morocco to use crypto, and even a French man  and jailed for it. Checking related laws in every jurisdiction is important to avoid trouble as a user, even if, in theory, no one can stop you from opening a wallet and using crypto.Still, censorship-resistant crypto keeps improving. As tools become more decentralized and user-friendly, people can rely less on fiat currencies and middlemen and gain more control over their money, while regulated environments still exist in parallel.:::info
Featured Vector Image by vectorjuice / ]]></content:encoded></item><item><title>AI Just Passed the Turing Test for Piano Music</title><link>https://hackernoon.com/ai-just-passed-the-turing-test-for-piano-music?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Sun, 25 Jan 2026 19:00:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A new study shows AI-generated piano music is so convincing that human listeners can’t reliably tell it apart from human compositions.]]></content:encoded></item><item><title>LACT 0.8.4 Brings Improved Overclocking UI For GPUs On Linux</title><link>https://www.phoronix.com/news/LACT-0.8.4-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 18:58:09 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In the absence of any official GUI control panel from AMD or Intel for their graphics cards on Linux, LACT remains a popular choice particularly for AMD Radeon Linux gamers/enthusiasts to manage various aspects of their GPU from a convenient UI. LACT also supports Intel GPUs and some features on NVIDIA GPUs too. Out today is LACT 0.8.4 for further enhancing this third-party GPU driver user interface...]]></content:encoded></item><item><title>Non-Trivial Temporal Patterns in Two-Population Kuramoto Ensembles</title><link>https://hackernoon.com/non-trivial-temporal-patterns-in-two-population-kuramoto-ensembles?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Sun, 25 Jan 2026 18:00:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.3 Kuramoto models with several globally coupled sub-ensembles\
It has been shown that systems of the form (8) with only D = 2 display unexpected and highly non-trivial temporal patterns [73, 74].\
Proposition 1 can be generalized to the system (8) as follows:(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Property Hooks in Action: Here Are Some Real-World Examples You Should Take a Look At</title><link>https://hackernoon.com/property-hooks-in-action-here-are-some-real-world-examples-you-should-take-a-look-at?source=rss</link><author>MattLeads</author><category>tech</category><pubDate>Sun, 25 Jan 2026 18:00:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The release of  coupled with  marks a pivotal moment in our ecosystem. For over a decade, we have religiously generated getters and setters, bloating our entities and DTOs with boilerplate that adds visual noise but zero business value.\
With PHP 8.4’s introduction of  and , we can finally retire the “Java-fication” of PHP objects. We can write concise, expressive domain models that encapsulate behavior without the verbosity.\
In this article, I will explore how to modernize a Symfony 7.4 application using these features. I will refactor a legacy-style Doctrine entity into a modern, lean PHP 8.4 component, ensuring full compatibility with Symfony’s Serializer, Validator, and Forms.Before we dive into the code, let’s ensure you are running the correct stack. Symfony 7.4 is the Long-Term Support version released in November 2025.\
Ensure your local environment is ready. Run the following commands:php -v
# Output must indicate PHP 8.4.0 or higher

composer version
# Ensure you are on Composer 2.8+
\
We will use standard Symfony 7.4 components. If you are starting a new project:composer create-project symfony/skeleton:7.4.* my_project
cd my_project
composer require symfony/webapp-pack:7.4.*
composer require symfony/orm-pack
composer require symfony/serializer-pack
composer require symfony/validator-pack
\
If you are upgrading an existing project, ensure your  restricts  to  and  packages to .Asymmetric Visibility: The End of Getter/Setter BoilerplateFor years, we made properties private and added public getters to prevent external modification, while allowing internal access.\
The Old Way (PHP 8.3 and below):class User
{
    private string $email;

    public function __construct(string $email)
    {
        $this->email = $email;
    }

    public function getEmail(): string
    {
        return $this->email;
    }

    // No setter meant it was effectively "read-only" externally, 
    // but we still needed the getter to read it.
}
\
: Asymmetric visibility allows us to define the “set” permission independently of the “get” permission.class User
{
    // Publicly readable, but only writeable by the class itself (private)
    public private(set) string $email;

    public function __construct(string $email)
    {
        $this->email = $email;
    }
}
Integration with DoctrineThis works seamlessly with Doctrine ORM in Symfony 7.4. Doctrine uses reflection to hydrate properties, bypassing visibility guards, so  is perfectly safe for mapped entities.namespace App\Entity;

use Doctrine\ORM\Mapping as ORM;
use Symfony\Component\Validator\Constraints as Assert;

#[ORM\Entity]
class Product
{
    #[ORM\Id]
    #[ORM\GeneratedValue]
    #[ORM\Column]
    public private(set) ?int $id = null;

    #[ORM\Column(length: 255)]
    #[Assert\NotBlank]
    public private(set) string $name;

    #[ORM\Column]
    #[Assert\PositiveOrZero]
    public private(set) int $stock = 0;

    public function __construct(string $name)
    {
        $this->name = $name;
    }

    // Business logic method to mutate state
    public function addToStock(int $quantity): void
    {
        if ($quantity < 1) {
            throw new \InvalidArgumentException('Quantity must be positive');
        }
        $this->stock += $quantity;
    }
}
: No getId(), getName(), getStock().: $product->stock = 100 triggers a fatal error from outside the class. You must use addToStock(), enforcing your business rules.DX (Developer Experience): Autocompletion works directly on properties ($product->name), which feels more “native” than method calls.Property Hooks: Intelligent PropertiesWhile Asymmetric Visibility controls ,  allow us to control behavior. They replace magic methods (__get, __set) and explicit getters/setters with logic attached directly to the property.Data Normalization (The set hook)Imagine a User entity where the email must always be stored in lowercase.namespace App\Entity;

use Doctrine\ORM\Mapping as ORM;

#[ORM\Entity]
class User
{
    #[ORM\Id]
    #[ORM\GeneratedValue]
    #[ORM\Column]
    public private(set) ?int $id = null;

    #[ORM\Column(length: 180, unique: true)]
    public string $email {
        // The 'set' hook intercepts assignment
        set(string $value) {
            if (!filter_var($value, FILTER_VALIDATE_EMAIL)) {
                throw new \InvalidArgumentException("Invalid email format");
            }
            // The backing value is updated automatically via this assignment
            $this->email = strtolower($value);
        }
    }

    public function __construct(string $email)
    {
        // This triggers the set hook!
        $this->email = $email;
    }
}
$user = new User('ADMIN@Example.com');
echo $user->email; // Outputs: admin@example.com
Virtual Properties (The get hook)Virtual properties do not store data; they calculate it on the fly. This is perfect for convenience accessors in Symfony DTOs or Entities that don’t need to be persisted to the database.namespace App\Entity;

use Doctrine\ORM\Mapping as ORM;

#[ORM\Entity]
class Customer
{
    #[ORM\Column]
    public string $firstName;

    #[ORM\Column]
    public string $lastName;

    // This is a VIRTUAL property. It has no storage in memory.
    // Notice we do NOT add #[ORM\Column] because it's not in the DB.
    public string $fullName {
        get => $this->firstName . ' ' . $this->lastName;
    }
}
Integration with Symfony SerializerOne of the most powerful features of Property Hooks in Symfony 7.4 is how the Serializer component handles them.: Serialized normally.: If they are public, the Serializer treats them just like real properties. You no longer need  on a  method.namespace App\Controller;

use App\Entity\Customer;
use Symfony\Bundle\FrameworkBundle\Controller\AbstractController;
use Symfony\Component\HttpFoundation\JsonResponse;
use Symfony\Component\Routing\Attribute\Route;

class CustomerController extends AbstractController
{
    #[Route('/customer/{id}', methods: ['GET'])]
    public function show(Customer $customer): JsonResponse
    {
        // Serialization automatically includes "fullName" in the JSON output
        return $this->json($customer);
    }
}
{
  "firstName": "Jane",
  "lastName": "Doe",
  "fullName": "Jane Doe"
}
Advanced Pattern: Value Objects & DTOsLet’s look at a more complex scenario involving a Data Transfer Object (DTO) for a Symfony Form or API payload. We will use  directly on hooked properties.namespace App\DTO;

use Symfony\Component\Validator\Constraints as Assert;

class RegistrationRequest
{
    #[Assert\NotBlank]
    public string $firstName;

    #[Assert\NotBlank]
    public string $lastName;

    // A 'write-only' property behavior for passwords using a hook
    // We only want to set it, but reading it might return a masked value or be disallowed
    public string $password {
        set(string $value) {
            $this->password = password_hash($value, PASSWORD_BCRYPT);
        }
        get => '***MASKED***'; 
    }

    // Combining hooks with asymmetric visibility
    // Publicly readable, but only settable internally or by hydration
    public private(set) \DateTimeImmutable $registeredAt {
        get => $this->registeredAt ?? new \DateTimeImmutable();
    }
}
\
: Be careful with the get hook on $registeredAt above. If the property is uninitialized, accessing it via $this->registeredAt inside the hook causes infinite recursion. The ?? check is a safe pattern if the backing field might be null.Refactoring a Legacy ServiceLet’s look at a Service class. Traditionally, we might inject dependencies and use standard methods. With PHP 8.4, we can make our service configurations cleaner.\
: An  that needs a default “sender” address, but we want to allow changing it temporarily with strict validation.namespace App\Service;

use Symfony\Component\Mailer\MailerInterface;
use Symfony\Component\Mime\Email;

class EmailService
{
    public function __construct(
        private readonly MailerInterface $mailer,
        // Property Hook to validate configuration immediately
        public string $defaultSender {
            set(string $value) {
                if (!filter_var($value, FILTER_VALIDATE_EMAIL)) {
                    throw new \InvalidArgumentException("Invalid sender email");
                }
                $this->defaultSender = $value;
            }
        }
    ) {}

    public function sendWelcomeEmail(string $recipient): void
    {
        $email = (new Email())
            ->from($this->defaultSender)
            ->to($recipient)
            ->subject('Welcome!')
            ->text('Welcome to our platform.');

        $this->mailer->send($email);
    }
}
services:
    App\Service\EmailService:
        arguments:
            $mailer: '@mailer'
            $defaultSender: '%env(DEFAULT_EMAIL_SENDER)%'
\
If the environment variable  contains an invalid email, the service instantiation will fail immediately with a clear Exception, rather than failing silently later when the email is sent. This “Fail Fast” approach is enhanced by property hooks.Symfony Forms rely heavily on the  component to read and write data to your objects.\
: The  in Symfony 7.4  PHP 8.4 hooks.When the form reads data to populate fields, it triggers the  hook.When the form submits data back to the object, it triggers the  hook.You can often remove Form Data Transformers by moving the logic to the Entity/DTO hook.// Old way: create a CallbackTransformer to handle string-to-array conversion
// New way:

class TagAwareDTO
{
    /** @var string[] */
    private array $tags = [];

    // The form field can be mapped to 'tagString'
    // The underlying data is stored in $tags array
    public string $tagString {
        get => implode(', ', $this->tags);
        set(string $value) {
            $this->tags = array_map('trim', explode(',', $value));
        }
    }

    public function getTags(): array
    {
        return $this->tags;
    }
}
$builder->add('tagString', TextType::class, [
    'label' => 'Tags (comma separated)',
]);
\
This simplifies the  significantly, keeping the data transformation logic within the domain object where it belongs.While these features are excellent, “Senior” developers know when not to use them.When you  an object, PHP performs a shallow copy. If you use asymmetric visibility, the cloned object retains the same visibility rules.\
However, be careful with . If a virtual property depends on another object (e.g., ) and you clone the parent object but not the internal , the virtual property might still point to the old user reference. This is standard PHP object behavior, but hooks can hide this complexity.Doctrine uses Proxies (subclasses) for lazy loading.: Works fine because Doctrine uses Reflection to set private properties.: If you define a hook on a mapped property, Doctrine  trigger it when hydrating the entity  it uses reflection (which it does). However, be very careful about adding side effects (like logging or database calls) inside a  of a Doctrine entity. When Doctrine hydrates the object from the DB, you do  want those side effects to run.\
: Avoid side-effects (logging, API calls) in Entity hooks. Keep hooks strictly for data validation and transformation.Don’t put 50 lines of business logic inside a set hook. If the logic is complex, delegate it to a private method or, better yet, a Service. Hooks should be concise.public string $status {
    set {
        // 50 lines of code checking permissions, logging to DB, sending emails...
    }
}
public string $status {
    set {
        $this->validateStatusTransition($value);
        $this->status = $value;
    }
}
Migration Guide: From 7.3/8.3 to 7.4/8.4If you are upgrading a massive codebase, do not try to rewrite everything at once.. Run a regex search or use Rector (once updated for 8.4) to replace private properties + simple getters with public private(set). \n : private type $prop; … getProp() { return $this->prop; } \n : public private(set) type $prop;Virtual Properties for API Resources. Look for classes with #[SerializedName(‘virtual_field’)] and ) methods. Convert them to virtual properties.. Move simple Assert logic that transforms data (like  or ) from controllers/listeners into Property Hooks. and  allow us to write code that is arguably the cleanest in the history of the language. We are moving away from the verbose, “dumb” data structures of the past toward intelligent, self-managing objects.Use  () to expose data safely without getters.Use  for lightweight validation, transformation, and virtual properties.Symfony  and  support these features out of the box in 7.4.Keep hooks pure and fast; avoid heavy side effects.\
The days of generating getters and setters are over. Welcome to the era of modern PHP.\
Is your team ready to migrate to Symfony 7.4? I help organizations modernize legacy PHP applications and adopt high-performance architectures.]]></content:encoded></item><item><title>Businesses Are Being Told to Break the Law By This NYC AI Chatbot</title><link>https://hackernoon.com/businesses-are-being-told-to-break-the-law-by-this-nyc-ai-chatbot?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sun, 25 Jan 2026 18:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Markup, now a part of CalMatters, uses investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up forKlaxon, a newsletter that delivers our stories and tools directly to your inbox.\
This article is copublished withDocumented, a nonprofit newsroom that covers New York City’s immigrant communities, andThe City, a non-profit newsroom that serves the people of New York. Sign up for Documented’sand The City’s newsletter,\
In October, New York City announced a plan to harness the power of artificial intelligence to improve the business of government. The announcement included a surprising centerpiece: an AI-powered chatbot that would provide New Yorkers with information on starting and operating a business in the city.\
The problem, however, is that the city’s chatbot is telling businesses to break the law.\
Five months after launch, it’s clear that while the bot appears authoritative, the information it provides on housing policy, worker rights, and rules for entrepreneurs is often incomplete and in worst-case scenarios “dangerously inaccurate,” as one local housing policy expert told The Markup.\
If you’re a landlord wondering which tenants you have to accept, for example, you might pose a question like, “are buildings required to accept section 8 vouchers?” or “do I have to accept tenants on rental assistance?” In testing by The Markup, the bot said no, landlords do not need to accept these tenants. Except, in New York City, it’s illegal for landlords to discriminate by source of income, with a minor exception for small buildings where the landlord or their family lives.Rosalind Black, Citywide Housing Director at the legal assistance nonprofit Legal Services NYC, said that after being alerted to The Markup’s testing of the chatbot, she tested the bot herself and found even more false information on housing. The bot, for example, said it was legal to lock out a tenant, and that “there are no restrictions on the amount of rent that you can charge a residential tenant.” In reality, tenants cannot be locked out if they’ve lived somewhere for 30 days, and there absolutely are restrictions for the many rent-stabilized units in the city.\
Black said these are fundamental pillars of housing policy that the bot was actively misinforming people about. “If this chatbot is not being done in a way that is responsible and accurate, it should be taken down,” she said.\
It’s not just housing policy where the bot has fallen short.\
The NYC bot also appeared clueless about the city’s consumer and worker protections. For example, in 2020, the city council passed a law requiring businesses to accept cash to prevent discrimination against unbanked customers. But the bot didn’t know about that policy when we asked. “Yes, you can make your restaurant cash-free,” the bot said in one wholly false response. “There are no regulations in New York City that require businesses to accept cash as a form of payment.”\
The bot said it was fine to take workers’ tips (wrong, although they sometimes can count tips toward minimum wage requirements) and that there were no regulations on informing staff about scheduling changes (also wrong). It didn’t do better with more specific industries, suggesting it was OK to conceal funeral service prices, for example, which the Federal Trade Commission has outlawed. Similar errors appeared when the questions were asked in other languages, The Markup found.\
It’s hard to know whether anyone has acted on the false information, and the bot doesn’t return the same responses to queries every time. At one point, it told a Markup reporter that landlords  have to accept housing vouchers, but when ten separate Markup staffers asked the same question, the bot told all of them no, buildings did not have to accept housing vouchers.\
The problems aren’t theoretical. When The Markup reached out to Andrew Rigie, Executive Director of the NYC Hospitality Alliance, an advocacy organization for restaurants and bars, he said a business owner had alerted him to inaccuracies and that he’d also seen the bot’s errors himself.\
“A.I. can be a powerful tool to support small business so we commend the city for trying to help,” he said in an email, “but it can also be a massive liability if it’s providing the wrong legal information, so the chatbot needs to be fixed asap and these errors can’t continue.”\
Leslie Brown, a spokesperson for the NYC Office of Technology and Innovation, said in an emailed statement that the city has been clear the chatbot is a pilot program and will improve, but “has already provided thousands of people with timely, accurate answers” about business while disclosing risks to users.\
“We will continue to focus on upgrading this tool so that we can better support small businesses across the city,” Brown said.A Business Bot That May Produce “Incorrect, Harmful or Biased Content”The city’s bot comes with an impressive pedigree. It’s powered by Microsoft’s Azure AI services, which Microsoft says is used by major companies like AT&T and Reddit. Microsoft has also invested heavily in OpenAI, the creators of the hugely popular AI app ChatGPT. It’s even worked with major cities in the past, helping Los Angeles develop a bot in 2017 that could answer hundreds of questions, although the website for that service isn’t available.\
New York City’s bot, according to the initial announcement, would let business owners “access trusted information from more than 2,000 NYC Business web pages,” and explicitly says the page will act as a resource “on topics such as compliance with codes and regulations, available business incentives, and best practices to avoid violations and fines.”\
There’s little reason for visitors to the chatbot page to distrust the service. Users who visit today get informed the bot “uses information published by the NYC Department of Small Business Services” and is “trained to provide you official NYC Business information.” One small note on the page says that it “may occasionally produce incorrect, harmful or biased content,” but there’s no way for an average user to know whether what they’re reading is false. A sentence also suggests users verify answers with links provided by the chatbot, although in practice it often provides answers without any links. A pop-up notice encourages visitors to report any inaccuracies through a feedback form, which also asks them to rate their experience from one to five stars.The bot is the latest component of the Adams administration’s MyCity project, a portal announced last year for viewing government services and benefits.\
There’s little other information available about the bot. The city says on the page hosting the bot that the city will review questions to improve answers and address “harmful, illegal, or otherwise inappropriate” content, but otherwise delete data within 30 days.\
A Microsoft spokesperson declined to comment or answer questions about the company’s role in building the bot.Since the high-profile release of ChatGPT in 2022, several other companies, from big hitters like Google to relatively niche businesses, have tried to incorporate chatbots into their products. But that initial excitement has sometimes soured when the limits of the technology have become clear.\
In one relevant recent case, a lawsuit filed in October claimed that a property management company used an AI chatbot to unlawfully deny leases to prospective tenants with housing vouchers. In December, practical jokers discovered they could trick a car dealership using a bot into selling vehicles for a dollar.\
In that last case, a Microsoft vice president told NPR that public experimentation was necessary to work out the problems in a bot. “You have to actually go out and start to test it with customers to find these kind of scenarios,” he said.]]></content:encoded></item><item><title>How Amazon Sellers Can Avoid Aged Inventory Surcharges in 2026</title><link>https://hackernoon.com/how-amazon-sellers-can-avoid-aged-inventory-surcharges-in-2026?source=rss</link><author>Mayur S</author><category>tech</category><pubDate>Sun, 25 Jan 2026 17:47:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If you are selling on Amazon FBA, I know that recent updates are making 2026 look a bit scary for slow-moving inventory.Aged inventory surcharge fees are increasing.  And, I can surely tell you that if you are not careful, a few months of stock can turn into a cash-burning nightmare.But, for this, you don’t have to panic. There are concrete, practical steps you can take to get on track.So, What Are Aged Inventory Surcharges?They are Amazon’s extra charges when your inventory sits in their warehouses for too long.Starting in 2026, they are charging around,12–15 months old: $0.30 per unit per month (up from $0.15)15+ months old: $0.35 per unit per month or $7.90 per cubic foot, whichever is higherThe shocking truth here is, yes, this can add up fast. A slow-moving SKU can cost you over a dollar per unit in just three months! Moreover, these fees stack up monthly, so the longer your items sit, the more money you lose.Now, Here Are 7 Simple Workflows to Cut These FeesUse these steps as your survival toolkit for 2026.1. Build an FNSKU Velocity DashboardThe first step is to know which products are sitting too long. So, for this, use Excel or Google Sheets and filter stating, “Days in FBA which are greater than 240.And then, flag items with greater than 90 days’ supply. Sort this weekly by “ IPI Risk” with Liquidate bottom to 20%.Here, the goal is to make sure no SKU stays in FBA for more than 365 days. Ideally, get this dashboard running immediately, and then check weekly.2. Automate 90-Day Removal TriggersMake automation your friend. Use tools like Helium 10 or InventoryLab and set alerts for items approaching 270 days in storage. Then, if you have less than 14 days of supply per week, consider this as fulfilling by yourself (FBM). You can even use the Amazon API to automatically create removal orders when a product hits a certain age in the storage. This will help you keep inventory moving and avoid surprise fees.3. Run a Q4 Liquidation RuleSlow SKUs don’t fix themselves. You need to push them. Here’s a timeline example that works,Oct 1-15: Offer 50% off coupons on 241-365 day inventoryOct 16-31: Use Multi-Channel Fulfillment (MCF) to sell to email lists, which applies approx. 30% fee.Buy Oct 31: Donate or dispose of items, which can save approx. $0.20/unit on light SKUs.A quick math here, avoiding a $0.35/month surcharge for 3 months saves $1.05 per unit. It is not huge individually, but it adds up fast across hundreds or thousands of SKUs.4. Optimize Your PackagingAmazon charges for your packaging based on cubic feet. So, even a small change from your side can save a surprising amount.For example,  if you were using , it may charge you  (0.4*$7.90=$3.16) Now, with a small change of reduction to , it charges you  (0.3*$7.90=$2.37). You are saving $0.79 per month per unit.Even cutting off an inch or two from box dimensions will reduce fees and help your products move faster.5. Try To Avoid West Coast Storage When PossibleOn Amazon, there are some fulfillment centers that cost more. For example, West Coast storage costs around $0.57/ft³, compared to $0.48 elsewhere, which is a 19% premium.So, here is a tip I can give you: Manage your high IPI scores, which further help Amazon favor you with efficient fulfillment centers and reduce inbound placement fees, avoiding approx. 0.7% revenue hit.6. Use Serialized Inventory Audit (Post-Commingling Ban)So, I suggest the following, - Audit serialized UPCs weekly, and switch to wholesale only with proper invoice backing.-  You are safe since you control your own FNSKUsThe immediate action to be taken is to remove all retail arbitrage serialized inventory that's been in FBA for more than 180 days.7. Next, Use a Break-Even CalculatorWhen you decide on discounts or removals, always calculate the break-even price.  Here’s a simple formula that can be used for liquidation discounts: Surcharge /(Selling Price - COGS - FBA Fees).For example,  a $25 item with $10 COGS and $4 FBA fees, that's $0.35/ ($25 - $10 - $4) =  to break even.The 41% figure ignores COGS and fees, making it dangerously high. Always run this calculation before every coupon or removal order to avoid losing money on desperate clearances.Here Is My Suggested Execution TimelineTo make all these manageable, you need to break them into weekly priorities.Week 1- Build your FNSKU dashboardWeek 2- Plan your Q4 liquidation using the rules mentioned aboveWeek 3- Audit all your packaging and optimize itWeek 4: Set up 3PL relationships for hybrid fulfillmentThis sequence will make sure that nothing falls through the cracks and gives you the best chance to avoid penalties in furture.I can clearly tell you that sellers who implement these strategies early have been able to cut aged inventory surcharges by up to 78% in Q1. It is a proactive game where  the less stock you have, the less money you lose.And, if your mid-range SKUs are moving slowly, your readers or business partners need this plan .Taking even a few of these steps, like the dashboard, a quick liquidation push, and packaging optimization, can save hundreds or even thousands of dollars in 2026.:::tip
 slow inventory isn’t just a storage problem. Now, it’s a cash problem.]]></content:encoded></item><item><title>US Congress Fails to Repeal &apos;Kill Switch&apos; for Cars Mandate</title><link>https://tech.slashdot.org/story/26/01/24/1845236/us-congress-fails-to-repeal-kill-switch-for-cars-mandate?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 17:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Newsweek reports on how the U.S. Congress is debating "kill switch" technology for vehicles, "which would be able to monitor diver behavior, detect impairment such as intoxication and intervene..." 


"While the technology is not yet a legal requirement in cars, Congress passed a law with the Infrastructure Investment and Jobs Act in 2021 that requires the Department of Transportation to create the mandate."

Republican Representative Thomas Massie of Kentucky introduced an amendment to a federal spending bill that would reverse the mandating of the technology. On Thursday, 160 Republicans voted in favor, but the legislation failed 164-268, according to the House Clerk's official roll call — with 57 Republicans joining 211 Democrats in voting against it... 

The House vote signals substantial Republican support for curbing any move toward mandated impaired-driving prevention systems, but not enough to pass such legislation. Critics of the kill switch technology see it as government overreach, while those in favor argue that it could prove to be lifesaving.


 

Thanks to long-time Slashdot reader SonicSpike for sharing the article.]]></content:encoded></item><item><title>AMD Sends In A Variety Of Graphics Driver Fixes Ahead Of Linux 7.0 Cycle</title><link>https://www.phoronix.com/news/AMDGPU-Linux-7.0-More-Fixes</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 17:27:23 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[This week's batch of AMDGPU and AMDKFD changes queued up ahead of the next kernel merge window is focused on delivering a variety of driver fixes...]]></content:encoded></item><item><title>TechCrunch Mobility: RIP, Tesla Autopilot, and the NTSB investigates Waymo</title><link>https://techcrunch.com/2026/01/25/techcrunch-mobility-rip-tesla-autopilot-and-the-ntsb-investigates-waymo/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Sun, 25 Jan 2026 17:06:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Welcome back to TechCrunch Mobility — your central hub for news and insights on the future of transportation. ]]></content:encoded></item><item><title>Apple will reportedly unveil its Gemini-powered Siri assistant in February</title><link>https://techcrunch.com/2026/01/25/apple-will-reportedly-unveil-its-gemini-powered-siri-assistant-in-february/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 25 Jan 2026 16:56:31 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[We’re about to get our first real look at the results of the recently announced AI partnership between Apple and Google.]]></content:encoded></item><item><title>The Android &apos;NexPhone&apos;: Linux on Demand, Dual-Boots Into Windows 11 - and Transforms Into a Workstation</title><link>https://mobile.slashdot.org/story/26/01/25/060240/the-android-nexphone-linux-on-demand-dual-boots-into-windows-11---and-transforms-into-a-workstation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The "NexDock" (from Nex Computer) already turns your phone into a laptop workstation. Purism chose it as the docking station for their Librem 5 phones. 

But now Nex is offering its own smartphone "that runs Android 16, launches Debian, and dual-boots into Windows 11," according to the blog It's FOSS:


Fourteen years after the first concept video was teased, the NexPhone is here, powered by a Qualcomm QCM6490, which, the keen-eyed among you will remember from the now-discontinued Fairphone 5. 

By 2026 standards, it's dated hardware, but Nex Computer doesn't seem to be overselling it, as they expect the NexPhone to be a secondary or backup phone, not a flagship contender. The phone includes an Adreno 643 GPU, 12GB of RAM, and 256GB of internal storage that can be expanded up to 512GB via a microSD card. 

In terms of software, the NexPhone boots into NexOS, a bloatware-free and minimal Android 16 system, with Debian running as an app with GPU acceleration, and Windows 11 being the dual-boot option that requires a restart to access. ["And because the default Windows interface isn't designed for a handheld screen, we built our own Mobile UI from the ground up to make Windows far easier to navigate on a phone," notes a blog post from Nex founder/CEO Emre Kosmaz]. 

And, before I forget, you can plug the NexPhone into a USB-C or HDMI display, add a keyboard and mouse to transform it into a desktop workstation. 
There's a camera plus "a comprehensive suite of sensors," according to the article, "that includes a fingerprint scanner, accelerometer, magnetometer, gyroscope, ambient light sensor, and proximity sensor.... 

"NexPhone is slated for a Q3 2026 release (July-September)..." 

Back in 2012, explains Nex founder/CEO Emre Kosmaz, "most investors weren't excited about funding new hardware. One VC even told us, 'I don't understand why anyone buys anything other than Apple'..."


Over the last decade, we kept building and shipping — six generations of NexDock — helping customers turn phones into laptop-like setups (display + keyboard + trackpad). And now the industry is catching up faster than ever. With Android 16, desktop-style experiences are becoming more native and more mainstream. That momentum is exactly why NexPhone makes sense today... 

Thank you for being part of this journey. With your support, I hope NexPhone can help move us toward a world where phones truly replace laptops and PCs — more often, more naturally, and for more people.]]></content:encoded></item><item><title>Does Trust Require Legitimacy?</title><link>https://hackernoon.com/does-trust-require-legitimacy?source=rss</link><author>Andrei Mochola</author><category>tech</category><pubDate>Sun, 25 Jan 2026 16:31:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Trust. Governance. What was the third one… Legitimacy. It’s not a prerequisite for trust or governance, but rather the condition that decides their limits.Why Digital Systems Can Function Without It - Until They Can’tDigital trust is often discussed as if it were a moral achievement. If users stay, if adoption grows, if systems function at scale, then trust must exist - and ==if trust exists, legitimacy is assumed to follow==. This assumption is very comfortable. It allows platforms, institutions, and infrastructures to treat trust as an output of performance rather than as a consequence of authority that has been earned.But comfort is not accuracy.==Trust and legitimacy are not the same thing==. They do not arise from the same conditions, they do not perform the same function, and - crucially - they do not fail in the same way. Digital systems have become remarkably good at generating trust without legitimacy. They have become far less capable of surviving what follows.This distinction matters now because digital platforms no longer merely mediate interaction. They allocate visibility, enforce rules, resolve disputes, exclude participants, and reshape incentives. They govern. And governance without legitimacy is not neutral - it is fragile.The argument itself tends to ==divide people who build systems from those who live under them==.Trust Without Legitimacy Is Not a ContradictionTrust, in its most basic sense, is instrumental. It is a mechanism humans use to reduce complexity in situations ==where full knowledge is impossible==. According to Niklas Luhmann, trust allows action in the absence of certainty. It does not require moral approval. It does not require fairness. It requires only a sufficiently stable expectation that the system will behave as anticipated. But… will it behave?This is why trust can exist in deeply asymmetric, unequal relationships. ==Users trust platforms they do not understand==. Citizens trust institutions they do not control. Workers trust systems that can penalize them, in the end. Examples would trigger endless disputes, so - at least for now - let’s stop here. The bottom line is that trust arises not because power is justified, but because outcomes are predictable enough to navigate through conveniently. And feel somehow secure in that sailing (we’ll touch on security versus convenience a bit more in the future).Digital systems excel at this. Interfaces are consistent. Processes are automated. Friction is minimized. Over time, ==habit replaces evaluation==. Trust becomes procedural - not reflective, not consensual, but functional. This is where we should pay some attention.There is nothing inherently wrong with this. Trust has always operated this way. What is often misunderstood is what trust does  provide, mainly trust does not grant authority. It does not confer moral standing, also. And it does not imply consent. Remember all those clickwrap agreements you submit everyday without understanding actually what you’re agreeing to?Legitimacy, by contrast, is normative. It concerns whether power  to be exercised, not whether it  be navigated. It asks whether rules are justified, ==whether decision-makers are accountable==, and whether those subject to authority have reason to recognise it as rightful.A system can be trusted and illegitimate at the same time. History offers countless examples, mainly in politics, but not limited to. Digital platforms are simply the most recent and the most efficient these days.When Usage Is Mistaken for ConsentOne of the most persistent errors in digital governance is the belief that continued participation implies agreement. If users stay, the logic goes, they must accept the rules. If they accept the rules, the system is legitimate. Ditto.This reasoning collapses the moment power asymmetry is taken seriously.Social contract theory has long distinguished between compliance and consent. Hobbes understood obedience as a condition of order, not legitimacy. A look back reveals that Locke insisted that authority remains conditional - tolerable only so long as it serves those governed. Rousseau went further, arguing that ==legitimacy requires participation in the formation of the rules themselves==.Digital platforms quietly bypass these distinctions - who reads philosophers these days? Participation is measured, not deliberated. Consent is embedded in terms of service. Exit is theoretically available but practically constrained by lock-in, network effects, professional dependency, or social cost. The last one is bigger ==the longer the service is used==. What seems to be Kensington High Street at first sight may quickly become Kensington Avenue.The result is a form of coerced continuity: users remain not because they agree, but because alternatives are absent, impractical, or invisible. Trust persists because daily interaction demands it. Legitimacy remains unexamined because questioning it offers no clear remedy.This is why metrics of engagement, retention, or satisfaction cannot substitute for legitimacy. They measure adaptation to power, not acceptance of it. They reveal how well users cope with governance, not whether governance is justified.Mistaking usage for consent is not a neutral analytical error. ==It allows systems to expand authority without ever confronting its moral basis==.Trust as a Substitute for LegitimacyDigital systems often compensate for legitimacy deficits through performance. As long as platforms are fast, convenient, and effective, trust fills the gap. Users tolerate opaque rules because outcomes remain favourable. Institutions avoid justification because efficiency silences dissent.This strategy works. Until it doesn’t.Trust can mask illegitimacy, but it cannot erase it. Instead, it accumulates what might be called legitimacy debt. Each unilateral rule change, unexplained decision, or unchallengeable enforcement action draws against a reserve that trust temporarily supplies.The problem is not gradual erosion. ==Trust rarely collapses slowly==. It breaks when expectations collide with power. Moments of crisis - moderation disputes, data misuse, sudden policy shifts, unexplained exclusions - expose the underlying structure. At that point, trust no longer reduces complexity. It amplifies betrayal.Legitimacy fails differently. It does not depend on flawless outcomes. It depends on the justification. Systems with legitimacy can survive error because they can explain themselves, correct themselves, and be challenged without unraveling.Systems without legitimacy have only performance to defend them; or convenience offered to its users (quite often misused, isn’t?). When performance or convenience falter, there is nothing beneath it. This also why many platform are paranoid about its performance and convenience (often called ).This is why digital trust crises often appear sudden and disproportionate. But, they are not sudden at all, they are deferred.At this point, it is tempting to retreat into familiar solutions: transparency dashboards, better UX, clearer communication, smarter systems. These are certainly useful, very useful and needed, but none of them address legitimacy.The problem is not that we lack trust, as Onora O’Neill has argued, but that ==we conflate trustworthiness with reliability==. Reliable systems can still be unaccountable. Transparent processes can still be unjustified same way as automation can obscure responsibility rather than clarify it.Legitimacy requires governance structures that constrain power, not merely optimise it. It requires identifiable authority, predictable rules, and mechanisms for contestation. It requires the possibility of saying “no” - and being heard. And address that “no” when it’s being heard.This is highly uncomfortable in digital contexts because it challenges the fiction of neutrality. Platforms often present themselves as technical systems rather than governing institutions (they only allow, they only enable, they only offer the choice or possibility). But governance does not disappear when it is denied. ==It becomes unaccountable==.The critical point is this: trust can motivate cooperation, but only legitimacy can justify obligation. When interests align, trust is enough. When they diverge, only legitimacy holds.Digital systems do not face a crisis of trust. They face a  that trust has temporarily postponed.The real question is not whether users trust platforms today. It is whether platforms are prepared to justify their authority tomorrow - when trust is no longer sufficient, when conflict emerges, when performance or convenience no longer compensates for power.Trust buys time. Legitimacy buys durability.As digital infrastructures continue to govern more aspects of economic, social, and civic life, the distinction will become harder to ignore. Systems that confuse trust for consent will discover that cooperation can vanish overnight. Systems that ground authority in legitimacy will survive disagreement, error, and change.Digital trust does not fail because people stop believing. ==It fails when systems refuse to justify why they should be believed in at all==.Further Reading & Conceptual ReferencesHobbes, T. –  (Authority and order under asymmetry)Locke, J. – Two Treatises of Government (Conditional legitimacy)Luhmann, N. –  (Trust as complexity reduction)O’Neill, O. –  (Trustworthiness vs reliability)Weber, M. –  (Legitimate authority and recognition)Beetham, D. – The Legitimation of Power (Rules, justification, and consent)Scharpf, F. W. –  (Input, throughput, and output legitimacy)Whitworth, B. –  (Legitimacy in digital systems)Gillespie, T. – Custodians of the Internet(Platform governance in practice) \n  \n ]]></content:encoded></item><item><title>What is Bending Spoons? The little-known firm behind Vimeo’s sweeping layoffs</title><link>https://techcrunch.com/2026/01/25/what-is-bending-spoons-everything-to-know-about-aols-acquirer/</link><author>Anna Heim</author><category>tech</category><pubDate>Sun, 25 Jan 2026 16:25:19 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Bending Spoons remains largely unknown, even as its portfolio of products has served more than a billion people. ]]></content:encoded></item><item><title>The HackerNoon Newsletter: AI Doesn’t Mean the End of Work for Us (1/25/2026)</title><link>https://hackernoon.com/1-25-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Sun, 25 Jan 2026 16:03:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, January 25, 2026?By @bernard [ 4 Min read ] I believe that AI’s impact and future pathways are overstated because human nature is ignored in such statements. Read More.By @denisp [ 23 Min read ] Success isnt building the agent; its managing it. From AgentOps to ROI dashboards, here is the operational playbook for scaling Enterprise AI. Read More.By @denisp [ 17 Min read ] Avoid the AI Slop trap. From runaway costs to memory poisoning, here are the 7 most common failure modes of Agentic AI (and how to fix them). Read More.By @drechimyn [ 7 Min read ] Broken Object Level Authorization (BOLA) is eating the API economy from the inside out.  Read More.By @melissaindia [ 4 Min read ] Bad data secretly slows development. Learn why data quality APIs are becoming core DX infrastructure in API-first systems and how they accelerate teams. Read More.By @dataops [ 4 Min read ] DataOps provides the blueprint, but automation makes it scalable. Learn how enforced CI/CD, observability, and governance turn theory into reality. Read More.By @socialdiscoverygroup [ 19 Min read ] We taught Playwright to find the correct HAR entry even when query/body values change and prevented reusing entities with dynamic identifiers.  Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>The Case Against Small Modular Nuclear Reactors</title><link>https://hardware.slashdot.org/story/26/01/24/0452209/the-case-against-small-modular-nuclear-reactors?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Small modular nuclear reactors (or SMRs) are touted as "cheaper, safer, faster to build and easier to finance" than conventional nuclear reactors, reports CNN. Amazon has invested in X-Energy, and earlier this month, Meta announced a deal with Oklo, and in Michigan last month, Holtec began the long formal licensing process for two SMRs with America's Nuclear Regulatory Commission next to a nuclear plant it hopes to reactive. (And in 2024, California-based Kairos Power broke ground in Tennessee on a SMR "demo" reactor.) 

But "The reality, as ever, is likely to be messier and experts are sounding notes of caution..."

All the arguments in favor of SMRs overlook a fundamental issue, said Edwin Lyman, director of nuclear power safety at the Union of Concerned Scientists: They are too expensive. Despite all the money swilling around the sector, "it's still not enough," he told CNN. Nuclear power cannot compete on cost with alternatives, both fossil fuels and increasingly renewable energy, he said." 

Some SMRs also have an issue with fuel. The more unconventional designs, those cooled by salt or gas, often require a special type of fuel called high-assay low-enriched uranium, known as HALEU (pronounced hay-loo). The amounts available are limited and the supply chain has been dominated by Russia, despite efforts to build up a domestic supply. It's a major risk, said Nick Touran [a nuclear engineer and independent consultant]. The biggest challenge nuclear has is competing with natural gas, he said, a "luxury, super expensive fuel may not be the best way." There is still stigma around nuclear waste, too. SMR companies say smaller reactors mean less nuclear waste, but 2022 research from Stanford University suggested some SMRs could actually generate more waste, in part because they are less fuel efficient... 

As companies race to prove SMRs can meet the hype, experts appear to be divided in their thinking. For some, SMRs are an expensive — and potentially dangerous — distraction, with timelines that stretch so far into the future they cannot be a genuine answer to soaring needs for clean power right now. 

Nuclear engineering/consultant Touran told CNN the small reactors are "a technological solution to a financial problem. No venture capitalists can say, like, 'oh, sure, we'll build a $30 billion plant.' But, if you're down into hundreds of millions, maybe they can do it."]]></content:encoded></item><item><title>“No-cloning” Workaround Could Enable Quantum Cloud</title><link>https://spectrum.ieee.org/no-cloning-workaround</link><author>Edd Gent</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MzEzMTI1OS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5NTcxMzM2MX0.uq4rgGgv7eNvaeXp__FaNm_mHCYjU_xJlt4jprfXDm0/image.jpg?width=600" length="" type=""/><pubDate>Sun, 25 Jan 2026 14:00:01 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Encrypting quantum information enables unlimited copies]]></content:encoded></item><item><title>The Risks of AI in Schools Outweigh the Benefits, Report Says</title><link>https://news.slashdot.org/story/26/01/25/0422212/the-risks-of-ai-in-schools-outweigh-the-benefits-report-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 12:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[This month saw results from a yearlong global study of "potential negative risks that generative AI poses to student". The study (by the Brookings Institution's Center for Universal Education) also suggests how to prevent risks and maximize benefits:


After interviews, focus groups, and consultations with over 500 students, teachers, parents, education leaders, and technologists across 50 countries, a close review of over 400 studies, and a Delphi panel, we find that at this point in its trajectory, the risks of utilizing generative AI in children's education overshadow its benefits.
 

"At the top of Brookings' list of risks is the negative effect AI can have on children's cognitive growth," reports NPR — "how they learn new skills and perceive and solve problems."

The report describes a kind of doom loop of AI dependence, where students increasingly off-load their own thinking onto the technology, leading to the kind of cognitive decline or atrophy more commonly associated with aging brains... As one student told the researchers, "It's easy. You don't need to (use) your brain." The report offers a surfeit of evidence to suggest that students who use generative AI are already seeing declines in content knowledge, critical thinking and even creativity. And this could have enormous consequences if these young people grow into adults without learning to think critically... 
Survey responses revealed deep concern that use of AI, particularly chatbots, "is undermining students' emotional well-being, including their ability to form relationships, recover from setbacks, and maintain mental health," the report says. One of the many problems with kids' overuse of AI is that the technology is inherently sycophantic — it has been designed to reinforce users' beliefs... Winthrop offers an example of a child interacting with a chatbot, "complaining about your parents and saying, 'They want me to wash the dishes — this is so annoying. I hate my parents.' The chatbot will likely say, 'You're right. You're misunderstood. I'm so sorry. I understand you.' Versus a friend who would say, 'Dude, I wash the dishes all the time in my house. I don't know what you're complaining about. That's normal.' That right there is the problem." 

AI did have some advantages, the article points out:
The report says another benefit of AI is that it allows teachers to automate some tasks: "generating parent emails ... translating materials, creating worksheets, rubrics, quizzes, and lesson plans" — and more. The report cites multiple research studies that found important time-saving benefits for teachers, including one U.S. study that found that teachers who use AI save an average of nearly six hours a week and about six weeks over the course of a full school year... 

AI can also help make classrooms more accessible for students with a wide range of learning disabilities, including dyslexia. But "AI can massively increase existing divides" too, [warns Rebecca Winthrop, one of the report's authors and a senior fellow at Brookings]. That's because the free AI tools that are most accessible to students and schools can also be the least reliable and least factually accurate... "[T]his is the first time in ed-tech history that schools will have to pay more for more accurate information. And that really hurts schools without a lot of resources." 

The report calls for more research — and make several recommendations (including "holistic" learning and "AI tools that teach, not tell.") But this may be their most important recommendation. "Provide a clear vision for ethical AI use that centers human agency..." 

"We find that AI has the potential to benefit or hinder students, depending on how it is used."]]></content:encoded></item><item><title>Linux Kernel Continuity Document Added: What Happens If Torvalds&apos; Git Repo Goes Away?</title><link>https://www.phoronix.com/news/Linux-Kernel-Continuity-Doc</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 11:18:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following discussions from the 2025 Linux Maintainer Summit, merged overnight for the Linux 6.19 kernel is documentation concerning the Linux kernel project's continuity in the event that Linus Torvalds' official Git repository were to disappear or otherwise be inaccessible for continuing the upstream development of the Linux kernel...]]></content:encoded></item><item><title>Focusrite Forte USB Audio Interface To Be Supported By Linux 7.0</title><link>https://www.phoronix.com/news/Focusrite-Forte-Linux-7.0</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 11:10:49 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Focusrite Forte 2-in, 4-out USB audio interface as a portable audio recording solution will be supported by the mainline Linux 7.0 kernel. The patches are queued in the Linux kernel's sound subsystem development tree. While a convenient little device, the Focusrite Forte is no longer manufactured but can still be found used online...]]></content:encoded></item><item><title>Snapdragon 8 Elite Gen 5 Display Support &amp; Old Adreno 225 Enablement For Linux 7.0</title><link>https://www.phoronix.com/news/Linux-7.0-MSM-Driver</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 10:54:06 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Rob Clark this week sent out the latest MSM DRM kernel driver updates for the latest Qualcomm display/graphics enhancements ahead of next month's Linux 7.0 merge window...]]></content:encoded></item><item><title>SVT-AV1 4.0 Released With More Performance Optimizations</title><link>https://www.phoronix.com/news/SVT-AV1-4.0</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 10:38:55 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[SVT-AV1 4.0 is out as the newest major feature release for this open-source AV1 video encoder that was originally started by Intel as an open-source project and now continuining on thanks to the Alliance For Open Media...]]></content:encoded></item><item><title>Former Canonical Developer Advocate Warns Snap Store Isn&apos;t Safe After Slow Responses to Malware Reports</title><link>https://linux.slashdot.org/story/26/01/24/2332205/former-canonical-developer-advocate-warns-snap-store-isnt-safe-after-slow-responses-to-malware-reports?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 08:44:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this article from the blog Linuxiac
In a blog post, Alan Pope, a longtime Ubuntu community figure and former Canonical employee who remains an active Snap publisher... [warns of] a persistent campaign of malicious snaps impersonating cryptocurrency wallet applications. These fake apps typically mimic well-known projects such as Exodus, Ledger Live, or Trust Wallet, prompting users to enter wallet recovery phrases, which are then transmitted to attackers, resulting in drained funds. 
The perpetrators had originally used similar-looking characters from other alphabets to mimic other app listings, then began uploading "revisions" to other innocuous-seeming (approved) apps that would transform their original listing into that of a fake crypto wallet app. 

But now they're re-registering expired domains to take over existing Snap Store accounts, which Pope calls "a significant escalation..."

I worked for Canonical between 2011 and 2021 as an Engineering Manager, Community Manager, and Developer Advocate. I was a strong advocate for snap packages and the Snap Store. While I left the company nearly five years ago, I still maintain nearly 50 packages in the Snap Store, with thousands of users... Personally, I want the Snap Store to be successful, and for users to be confident that the packages they install are trustworthy and safe. 


Currently, that confidence isn't warranted, which is a problem for desktop Linux users who install snap packages. I report every bad snap I encounter, and I know other security professionals do the same — even though doing so results in no action for days sometimes... To be clear: none of this should be seen as an attack on the Snap Store, Canonical, or the engineers working on these problems. I'm raising awareness of an issue that exists, because I want it fixed... But pretending there isn't a problem helps nobody.]]></content:encoded></item><item><title>The TechBeat: Will AI Agents Pump Up Our Profits? (1/25/2026)</title><link>https://hackernoon.com/1-25-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sun, 25 Jan 2026 07:11:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @davidiyanu [ 11 Min read ] 
 Traditional CI/CD pipelines are buckling under scale. Agentic DevOps promises less toil—but introduces new risks teams must understand.  Read More.By @melissaindia [ 4 Min read ] 
 Bad data secretly slows development. Learn why data quality APIs are becoming core DX infrastructure in API-first systems and how they accelerate teams. Read More.By @opensourcetheworld [ 3 Min read ] 
 Solo Satoshi is Start9’s first US distributor, shipping the 2026 Server One from Houston so you can run open-source StartOS, apps, and Bitcoin nodes at home.  Read More.By @scylladb [ 5 Min read ] 
 ScyllaDB Vector Search reaches 1B vectors with 2ms p99 latency and 250K QPS, unifying structured data and embeddings at scale. Read More.By @dharmateja [ 12 Min read ] 
 Why average ROI fails. Learn how distributional and tail-risk modeling protects marketing campaigns from catastrophic losses using Bayesian methods.  Read More.By @stevebeyatte [ 4 Min read ] 
 Miniswap, a Warhammer marketplace founded by Cambridge students, is betting on taste, curation, and community over AI automation. Learn how they raised $3.5M.  Read More.By @davidiyanu [ 6 Min read ] 
 That's the mark of a modern senior engineer: not just writing code that works when everything goes right, but designing resilience into every line Read More.By @mend [ 9 Min read ] 
 As an opportunity to "kick the tyres" of what agents are and how they work, I set aside a couple of hours to see build one - and it blew me away. Read More.By @dharmateja [ 11 Min read ] 
 Learn how counterfactual forecasting helps data scientists measure true revenue impact by simulating causal scenarios beyond traditional time series models.  Read More.By @vigneshwaran [ 5 Min read ] 
 Learn how to uninstall problematic Windows 11 updates using Settings, Control Panel, Command Prompt, PowerShell, and Microsoft tools. Read More.By @btcwire [ 2 Min read ] 
 The platform is capable of producing video with realistic physics, lighting, and motion, making it suitable for marketing content. Read More.By @williamguo [ 7 Min read ] 
 The core design philosophy of SeaTunnel CDC is to find the perfect balance between "Fast" (parallel snapshots) and "Stable" (data consistency). Read More.By @hck3remmyp3ncil [ 11 Min read ] 
 RAG optimizes language model outputs by having them reference external knowledge bases before generating responses.  Read More.By @stevebeyatte [ 12 Min read ] 
 Modern midsize companies need platforms that balance sophistication with agility, offering powerful features without overwhelming complexity. Read More.By @vigneshwaran [ 5 Min read ] 
 Learn how to create a Windows 11 bootable USB using Rufus for installation, upgrades, or system recovery in this step-by-step guide. Read More.By @farida [ 4 Min read ] 
 The rise of Agentic AI has fueled predictions of improved company performance and stronger stock returns.
 Read More.]]></content:encoded></item><item><title>Google&apos;s &apos;AI Overviews&apos; Cite YouTube For Health Queries More Than Any Medical Sites, Study Suggests</title><link>https://news.slashdot.org/story/26/01/25/0043256/googles-ai-overviews-cite-youtube-for-health-queries-more-than-any-medical-sites-study-suggests?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 05:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from the Guardian:


Google's search feature AI Overviews cites YouTube more than any medical website when answering queries about health conditions, according to research that raises fresh questions about a tool seen by 2 billion people each month. 

The company has said its AI summaries, which appear at the top of search results and use generative AI to answer questions from users, are "reliable" and cite reputable medical sources such as the Centers for Disease Control and Prevention and the Mayo Clinic. However, a study that analysed responses to more than 50,000 health queries, captured using Google searches from Berlin, found the top cited source was YouTube. The video-sharing platform is the world's second most visited website, after Google itself, and is owned by Google. Researchers at SE Ranking, a search engine optimisation platform, found YouTube made up 4.43% of all AI Overview citations. No hospital network, government health portal, medical association or academic institution came close to that number, they said. "This matters because YouTube is not a medical publisher," the researchers wrote. "It is a general-purpose video platform...." 

In one case that experts said was "dangerous" and "alarming", Google provided bogus information about crucial liver function tests that could have left people with serious liver disease wrongly thinking they were healthy. The company later removed AI Overviews for some but not all medical searches... Hannah van Kolfschooten, a researcher specialising in AI, health and law at the University of Basel who was not involved with the research, said: "This study provides empirical evidence that the risks posed by AI Overviews for health are structural, not anecdotal. It becomes difficult for Google to argue that misleading or harmful health outputs are rare cases. 

"Instead, the findings show that these risks are embedded in the way AI Overviews are designed. In particular, the heavy reliance on YouTube rather than on public health authorities or medical institutions suggests that visibility and popularity, rather than medical reliability, is the central driver for health knowledge."]]></content:encoded></item><item><title>Infotainment, EV Charger Exploits Earn $1M at Pwn2Own Automotive 2026</title><link>https://it.slashdot.org/story/26/01/25/0131222/infotainment-ev-charger-exploits-earn-1m-at-pwn2own-automotive-2026?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 25 Jan 2026 02:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Trend Micro's Zero Day Initiative sponsored its third annual Pwn2Own Automotive competition in Tokyo this week, receiving 73 entries, the most ever for a Pwn2Own event. 

"Under Pwn2Own rules, all disclosed vulnerabilities are reported to affected vendors through ZDI," reports Help Net Security, "with public disclosure delayed to allow time for patches."

Infotainment platforms from Tesla, Sony, and Alpine were among the systems compromised during demonstrations. Researchers achieved code execution using techniques that included buffer overflows, information leaks, and logic flaws. One Tesla infotainment unit was compromised through a USB-based attack, resulting in root-level access. Electric vehicle charging infrastructure also received significant attention. Teams successfully demonstrated exploits against chargers from Autel, Phoenix Contact, ChargePoint, Grizzl-E, Alpitronic, and EMPORIA. Several attacks involved chaining multiple vulnerabilities to manipulate charging behavior or execute code on the device. These demonstrations highlighted how charging stations operate as network-connected systems with direct interaction with vehicles. 

There's video recaps on the ZDI YouTube channel — apparently the Fuzzware.io researchers "were able to take over a Phoenix Contact EV charger over bluetooth." 

Three researchers also exploited the Alpitronic's HYC50 fast-charging with a classic TOCTOU bug, according to the event's site, "and installed a playable version of Doom to boot." They earned $20,000 — part of $1,047,000 USD was awarded during the three-day event. 
More coverage from SecurityWeek:

The winner of the event, the Fuzzware.io team, earned a total of $215,500 for its exploits. The team received the highest individual reward: $60,000 for an Alpitronic HYC50 EV charger exploit delivered through the charging gun. ZDI described it as "the first public exploit of a supercharger".]]></content:encoded></item><item><title>The Math Trick That Lets Deep Networks Get Smarter Without Falling Apart</title><link>https://hackernoon.com/the-math-trick-that-lets-deep-networks-get-smarter-without-falling-apart?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Sun, 25 Jan 2026 01:59:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Why wider neural networks usually break—and how a simple mathematical constraint lets deep models grow smarter without collapsing.]]></content:encoded></item><item><title>GIMP 3.0.8 Released In Advance Of GIMP 3.2</title><link>https://www.phoronix.com/news/GIMP-3.0.8-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 01:52:16 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While the GIMP 3.2 release is expected out soon, GIMP 3.0.8 is available tonight as what could end up being the last set of bug fixes for GIMP 3.0...]]></content:encoded></item><item><title>Wine-Staging 11.1 Adds Patches For Enabling Recent Adobe Photoshop Versions On Linux</title><link>https://www.phoronix.com/news/Wine-Staging-11.1</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 25 Jan 2026 00:33:41 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following yesterday's release of Wine 11.1 for kicking off the new post-11.0 development cycle, Wine-Staging 11.1 is now available for this experimental/testing version of Wine that present is around 254 patches over the upstream Wine state...]]></content:encoded></item><item><title>Work-From-Office Mandate? Expect Top Talent Turnover, Culture Rot</title><link>https://it.slashdot.org/story/26/01/24/2146216/work-from-office-mandate-expect-top-talent-turnover-culture-rot?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 23:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[CIO magazine reports that "the push toward in-person work environments will make it more difficult for IT leaders to retain and recruit staff, some experts say."

"In addition to resistance, there would also be the risk of talent turnover," [says Lawrence Wolfe, CTO at marketing firm Converge]... "The truth is, both physical and virtual collaboration provide tremendous value...." IT workers facing work-from-office mandates are two to three times more likely than their counterparts to look for new jobs, according to Metaintro, a search engine that tracks millions of jobs. IT leaders hiring new employees may also face significant headwinds, with it taking 40% to 50% longer to fill in-person roles than remote jobs, according to Metaintro. "Some of the challenges CIOs face include losing top-tier talent, limiting the pool of candidates available for hire, and damaging company culture, with a team filled with resentment," says Lacey Kaelani, CEO and cofounder at Metaintro... 


There are several downsides for IT leaders to in-person work mandates, [adds Lena McDearmid, founder and CEO of culture and leadership advisory firm Wryver], as orders to commute to an office can feel arbitrary or rooted in control rather than in value creation. "That erodes trust quickly, particularly in IT teams that proved they could deliver remotely for years," she adds. The mandates can also create new friction for IT leaders by requiring them to deal with morale issues, manage exceptions, and spend time enforcing policy instead of leading strategy, she says. "There's also a real risk of losing experienced, high-performing talent who have options and are unwilling to trade autonomy for proximity without a clear reason," McDearmid adds. "When companies mandate daily commutes without a clear rationale, they often narrow their talent pool and increase attrition, particularly among people who know they can work effectively elsewhere." 

McDearmid has seen teams "sitting next to each other" who collaborate poorly "because decisions are unclear or leaders equate visibility with progress... Collaboration doesn't automatically improve just because people share a building." 

And Rebecca Wettemann, CEO at IT analyst firm Valoir, warns of return-to-office mandates "being used as a Band-Aid for poor management. When IT professionals feel they're being evaluated based on badge swipes, not real accomplishments, they will either act accordingly or look to work elsewhere." 

Thanks to Slashdot reader snydeq for sharing the article.]]></content:encoded></item><item><title>Google says it’s fixed Gmail issues with spam and misclassification</title><link>https://techcrunch.com/2026/01/25/gmail-is-having-issues-with-spam-and-misclassification/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 24 Jan 2026 22:58:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[If your Gmail account doesn’t seem to be working properly today, you’re not alone.]]></content:encoded></item><item><title>Airlines Cancel Over 10,000 US Flights Due To Massive Winter Storm</title><link>https://tech.slashdot.org/story/26/01/24/2223200/airlines-cancel-over-10000-us-flights-due-to-massive-winter-storm?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 22:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Airlines canceled more than 10,000 U.S. flights scheduled for this weekend," reports CNBC, "as a massive winter storm sweeps across the country, with heavy snow and sleet forecast, followed by bitter cold... set to snarl travel for hundreds of thousands of people for days."

More than 3,500 flights on Saturday were canceled, according to flight tracker FlightAware. Many of Saturday's cancellations were in and out of Dallas Fort Worth International Airport, with about 1,300 scrubbed flights, and at Dallas Love Field, with 186 cancellations, the majority of the schedule at each airport. American Airlines, based in Fort Worth, Texas, had canceled 902 Saturday flights, about 30% of its mainline schedule and Southwest Airlines canceled 571 flights, or 19%, according to FlightAware. 

U.S. flight cancellations nearly doubled to more than 7,000 [now up to 8,947] on Sunday when the storm is expected to hit the mid-Atlantic and Northeast U.S. As of midday on Saturday, most flights from Raleigh-Durham International Airport in North Carolina to Portland, Maine, were canceled. Major airline hubs were affected as far south as Atlanta, where Delta Air Lines is based.... American, Delta, JetBlue Airways, Southwest Airlines, United Airlines and other U.S. carriers said they are waiving change and cancellation fees as well as fare differences to rebook for customers with tickets to and from more than 40 airports around the country. The waivers include restrictive basic economy tickets. 

More than 80% of Sunday's flights at New York's LaGuardian Airport were cancelled, according to the article, at well as 90% of Sunday's flights at Viriginia's Ronald Reagan Washington National Airport.]]></content:encoded></item><item><title>Cheap Green Tech Allows Faster Path To Electrification For the Developing World</title><link>https://hardware.slashdot.org/story/26/01/24/1938220/cheap-green-tech-allows-faster-path-to-electrification-for-the-developing-world?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 21:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader Mr. Dollar Ton summarizes this article from Bloomberg:

According to a new report from think tank "Ember", the availability of cheap green tech can have developing countries profit from earlier investment and skip steps in the transition from fossil to alternatives. 

India is put forward as an example. While China's rapid electrification has been hailed as a miracle, by some measures, India is moving ahead faster than China did when it was at similar levels of economic development. It's an indication that clean electricity could be the most direct way to boost growth for other developing economies. 

That's mainly because India has access to solar panels and electric cars at a much lower price than China did about a decade ago. Chinese investments lowered the costs of what experts call "modular technologies" — the production of each solar panel, battery cell and electric car enables engineers to learn how to make it more efficiently. 

The think tank's team even argues "that countries such as India, which don't have significant domestic fossil-fuel reserves, will become 'electrostates' that meet most of their energy needs through electricity generated from clean sources," according to the article:

No country is an electrostate yet, [says Ember strategist Kingsmill Bond], but countries are increasingly turning to green electricity to power their economies. Nations that are less developed than India will see even more advantages as the cost of electricity technologies, from solar panels and electric vehicles to battery components and minerals, continue to fall. 

Neither India nor China is going electric purely to cut emissions or meet climate targets, says Bond. They're doing so because it makes economic sense, particularly for India, which imports more than 40% of its primary energy in the form of coal, oil and gas, according to the International Energy Agency. "To grow and have energy independence, India needs to reduce the terrible burden of fossil-fuel imports worth $150 billion each year," said Bond. "India needs to find other solutions...." 

[I]f countries like India find ways to grow electrotech manufacturing without absolute dependence on Chinese equipment, electrification could speed up further. With the U.S. and Europe continuing to add exclusions for Chinese-linked electrotech, countries like India will have an incentive to invest in their own manufacturing capacity. "We are probably at a moment of peak Chinese dominance in the electrotech system, as the rest of the world starts to wake up and realize that this is the energy future," he said.]]></content:encoded></item><item><title>Microsoft 365 Endured 9+ Hours of Outages Thursday</title><link>https://it.slashdot.org/story/26/01/24/2031221/microsoft-365-endured-9-hours-of-outages-thursday?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 20:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Early Friday "there were nearly 113 incidents of people reporting issues with Microsoft 365 as of 1:05 a.m. ET," reports Reuters. But that's down "from over 15,890 reports at its peak a day earlier, according to Downdetector." Reuters points out the outage affected antivirus software Microsoft Defender and data governance software Microsoft Purview, while CRN notes it also impacted "a number of Microsoft 365 services" including Outlook and Exchange online:

During the outage, Outlook users received a "451 4.3.2 temporary server issue" error message when attempting to send or receive email. Users did not have the ability to send and receive email through Exchange Online, including notification emails from Microsoft Viva Engage, according to the vendor. Other issues that cropped up include an inability to send and receive subscription email through [analytics platform] Microsoft Fabric, collect message traces, search within SharePoint online and Microsoft OneDrive and create chats, meetings, teams, channels or add members in Microsoft Teams... 

As with past cloud outages with other vendors, even after Microsoft fixed the issues, recovery efforts by its users to return to a normal state took additional time... Microsoft confirmed in a post on X [Thursday] at 4:14 p.m. ET that it "restored the affected infrastructure to a (healthy) state" but "further load balancing is required to mitigate impact...." The company reported "residual imbalances across the environment" at 7:02 p.m., "restored access to the affected services" and stable mail flow at 12:33 a.m. Jan. 23. At that time, Microsoft still saw a "small number of remaining affected services" without full service stability. The company declared impact from the event "resolved" at 1:29 p.m. Eastern. Microsoft sent out another X post at 8:20 a.m. asking users experiencing residual issues to try "clearing local DNS caches or temporarily lowering DNS TTL values may help ensure a quicker remediation...." 

Microsoft said in an admin center update that [Thursday's] outage was "caused by elevated service load resulting from reduced capacity during maintenance for a subset of North America hosted infrastructure." Furthermore, Microsoft noted that during "ongoing efforts to rebalance traffic" it introduced a "targeted load balancing configuration change intended to expedite the recovery process, which incidentally introduced additional traffic imbalances associated with persistent impact for a portion of the affected infrastructure." US itek's David Stinner said it appears that Microsoft did not have enough capacity on its backup system while doing maintenance on its main system. "It looks like the backup system was overloaded, and it brought the system down while they were still doing maintenance on the main system," he said. "That is why it took so many hours to get back up and running. If your primary system is down for maintenance and your backup system fails due to capacity issues, then it is going to take a while to get your primary system back up and running." 

"This was not Microsoft's first outage of 2026," the article notes, "with the vendor handling access issues with Teams, Outlook and other M365 services on Wednesday, a Copilot issue on Jan. 15 plus an Azure outage earlier in the month..."]]></content:encoded></item><item><title>Tech CEOs boast and bicker about AI at Davos</title><link>https://techcrunch.com/2026/01/24/tech-ceos-boast-and-bicker-about-ai-at-davos/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 24 Jan 2026 20:00:46 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[There were times at this week’s meeting of the World Economic Forum when Davos seemed transformed into a high-powered tech conference.]]></content:encoded></item><item><title>There’s One Week Left In The Public Domain Game Jam!</title><link>https://www.techdirt.com/2026/01/24/theres-one-week-left-in-the-public-domain-game-jam-2/</link><author>Leigh Beadon</author><category>tech</category><pubDate>Sat, 24 Jan 2026 20:00:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[We’re nearing the end of January, and you know what that means: we’re on the home stretch of the latest installment of our annual public domain game jam,  We’ve already gotten some intriguing submissions, and we expect to get plenty more before the deadline of  but just because time is running out doesn’t mean you can’t still get involved. A week is plenty of time to make a game!Whether you’re a new or experienced designer, now’s the time to head over to the game jam page on Itch, read the full rules, and get some ideas about works you might use. Once the submission window closes, we’ll be diving in to explore all the entries and select winners in our six categories, each of whom will receive a prize. There are lots of great tools available that let anyone build a simple digital game, like interactive fiction engine Twine and the storytelling platform Story Synth from Randy Lubin, our game design partner and co-host of this jam (check out his guide to building a Story Synth game in an hour here on Techdirt). And an analog game can be as simple as a single page of rules. For inspiration, you can have a look at last year’s winners and our series of winner spotlight posts that take a look at each year’s winning entries in more detail.As always, a huge thanks to everyone who is participating. We can’t wait to start playing the games you’ve made, and showing off the winners as examples of why a growing public domain benefits us all.]]></content:encoded></item><item><title>AI Luminaries Clash At Davos Over How Close Human-Level Intelligence Really Is</title><link>https://slashdot.org/story/26/01/24/076228/ai-luminaries-clash-at-davos-over-how-close-human-level-intelligence-really-is?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 19:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from Fortune

The large language models (LLMs) that have captivated the world are not a path to human-level intelligence, two AI experts asserted in separate remarks at Davos. Demis Hassabis, the Nobel Prize-winning CEO of Google DeepMind, and the executive who leads the development of Google's Gemini models, said today's AI systems, as impressive as they are, are "nowhere near" human-level artificial general intelligence, or AGI. [Though the artilcle notes that later Hassabis predicted there was a 50% chance AGI might be achieved within the decade.] Yann LeCun — an AI pioneer who won a Turing Award, computer science's most prestigious prize, for his work on neural networks — went further, saying that the LLMs that underpin all of the leading AI models will never be able to achieve humanlike intelligence and that a completely different approach is needed... ["The reason ... LLMs have been so successful is because language is easy," LeCun said later.] 

Their views differ starkly from the position asserted by top executives of Google's leading AI rivals, OpenAI and Anthropic, who assert that their AI models are about to rival human intelligence. Dario Amodei, the CEO of Anthropic, told an audience at Davos that AI models would replace the work of all software developers within a year and would reach "Nobel-level" scientific research in multiple fields within two years. He said 50% of white-collar jobs would disappear within five years. OpenAI CEO Sam Altman (who was not at Davos this year) has said we are already beginning to slip past human-level AGI toward "superintelligence," or AI that would be smarter than all humans combined... 

The debate over AGI may be somewhat academic for many business leaders. The more pressing question, says Cognizant CEO Ravi Kumar, is whether companies can capture the enormous value that AI already offers. According to Cognizant research released ahead of Davos, current AI technology could unlock approximately $4.5 trillion in U.S. labor productivity — if businesses can implement it effectively.]]></content:encoded></item><item><title>Hyperbolic Geometry in Kuramoto Ensembles: Conformal Barycenters and Gradient Flows</title><link>https://hackernoon.com/hyperbolic-geometry-in-kuramoto-ensembles-conformal-barycenters-and-gradient-flows?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Sat, 24 Jan 2026 19:00:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.2 Hyperbolic geometry of Kuramoto ensemblesIn 1994. Watanabe and Strogatz [69] demonstrated that the simple Kuramoto ensemble with globally coupled identical oscillators exhibits 3-dimensional dynamics. They have shown that dynamics of a large ensemble can be reduced to the system of ODE’s for three global variables. This implies that an ensemble consisting of N oscillators admits N − 3 constants of motion. This result initiated the new research direction which deals with symmetries and invariant submanifolds in simple Kuramoto networks.\
The underlying symmetries have been exposed in 2009. by Marvel, Mirrolo and Strogatz [70].\
Further insights into the relation between hyperbolic geometry and Kuramoto models have been reported in [71]. It has been demonstrated that (under certain conditions on the coupling function f) Kuramoto dynamics of the form (6) are gradient flows in the unit disc with respect to hyperbolic metric. Potential function for dynamics (7) has particularly transparent geometric interpretation. It turns out that dynamics of Kuramoto ensembles with repulsive interactions uncover a point inside the unit disc that has the minimal sum of hyperbolic distances to the initial points on S1. In complex analysis this point is conformal barycenter [72] of the initial configuration.(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Go 1.25 is released - The Go Programming Language</title><link>https://hackernoon.com/go-125-is-released-the-go-programming-language?source=rss</link><author>Go [Technical Documentation]</author><category>tech</category><pubDate>Sat, 24 Jan 2026 19:00:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Today the Go team is pleased to release Go 1.25. You can find its binary archives and installers on the download page.\
Some of the additions in Go 1.25 are in an experimental stage and become exposed only when you explicitly opt in. Notably, a new experimental garbage collector, and a new experimental  package are available for you to try ahead of time and provide your feedback. It really helps if you’re able to do that!\
Please refer to the Go 1.25 Release Notes for the complete list of additions, changes and improvements in Go 1.25.\
Over the next few weeks, follow-up blog posts will cover some of the topics relevant to Go 1.25 in more detail. Check back in later to read those posts.\
Thanks to everyone who contributed to this release by writing code, filing bugs, trying out experimental additions, sharing feedback, and testing the release candidates. Your efforts helped make Go 1.25 as stable as possible. As always, if you notice any problems, please file an issue.\
We hope you enjoy using the new release!Dmitri Shuralyov, on behalf of the Go team\
This article is available on  under a CC BY 4.0 DEED license.]]></content:encoded></item><item><title>NASA Confident, But Some Critics Wonder if Its Orion Spacecraft is Safe to Fly</title><link>https://science.slashdot.org/story/26/01/24/0633252/nasa-confident-but-some-critics-wonder-if-its-orion-spacecraft-is-safe-to-fly?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 18:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["NASA remains confident it has a handle on the problem and the vehicle can bring the crew home safely," reports CNN. 

 But "When four astronauts begin a historic trip around the moon as soon as February 6, they'll climb aboard NASA's 16.5-foot-wide Orion spacecraft with the understanding that it has a known flaw — one that has some experts urging the space agency not to fly the mission with humans on board..." 

The issue relates to a special coating applied to the bottom part of the spacecraft, called the heat shield... This vital part of the Orion spacecraft is nearly identical to the heat shield flown on Artemis I, an uncrewed 2022 test flight. That prior mission's Orion vehicle returned from space with a heat shield pockmarked by unexpected damage — prompting NASA to investigate the issue. And while NASA is poised to clear the heat shield for flight, even those who believe the mission is safe acknowledge there is unknown risk involved. "This is a deviant heat shield," said Dr. Danny Olivas, a former NASA astronaut who served on a space agency-appointed independent review team that investigated the incident. "There's no doubt about it: This is not the heat shield that NASA would want to give its astronauts." Still, Olivas said he believes after spending years analyzing what went wrong with the heat shield, NASA "has its arms around the problem..." 

"I think in my mind, there's no flight that ever takes off where you don't have a lingering doubt," Olivas said. "But NASA really does understand what they have. They know the importance of the heat shield to crew safety, and I do believe that they've done the job." Lakiesha Hawkins, the acting deputy associate administrator for NASA's Exploration Systems Development Mission Directorate, echoed that sentiment in September, saying, "from a risk perspective, we feel very confident." And Reid Wiseman, the astronaut set to command the Artemis II mission, has expressed his confidence. "The investigators discovered the root cause, which was the key" to understanding and solving the heat shield issue, Wiseman told reporters last July. "If we stick to the new reentry path that NASA has planned, then this heat shield will be safe to fly." 
Others aren't so sure. "What they're talking about doing is crazy," said Dr. Charlie Camarda, a heat shield expert, research scientist and former NASA astronaut. Camarda — who was also a member of the first space shuttle crew to launch after the 2003 Columbia disaster — is among a group of former NASA employees who do not believe that the space agency should put astronauts on board the upcoming lunar excursion. He said he has spent months trying to get agency leadership to heed his warnings to no avail... Camarda also emphasized that his opposition to Artemis II isn't driven by a belief it will end with a catastrophic failure. He thinks it's likely the mission will return home safely. More than anything, Camarda told CNN, he fears that a safe flight for Artemis II will serve as validation for NASA leadership that its decision-making processes are sound. And that's bound to lull the agency into a false sense of security, Camarda warned. 
CNN adds that Dr. Dan Rasky, an expert on advanced entry systems and thermal protection materials who worked at NASA for more than 30 years, also does not believe NASA should allow astronauts to fly on board the Artemis II Orion capsule. 

And "a crucial milestone could be days away as Artemis program leaders gather for final risk assessments and the flight readiness review," when top NASA brass determine whether the Artemis II rocket and spacecraft are ready to take off with a human crew.]]></content:encoded></item><item><title>SEC drops lawsuit against Winklevoss twins’ Gemini crypto exchange</title><link>https://techcrunch.com/2026/01/24/sec-drops-lawsuit-against-winklevoss-twins-gemini-crypto-exchange/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 24 Jan 2026 18:06:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Securities and Exchange Commission has dropped its lawsuit against Gemini, the crypto exchange founded by Trump backers Cameron and Tyler Winklevoss.]]></content:encoded></item><item><title>How to Build an AI Generated Calculator Without Custom JavaScript</title><link>https://hackernoon.com/how-to-build-an-ai-generated-calculator-without-custom-javascript?source=rss</link><author>Simon Y. Blackwell</author><category>tech</category><pubDate>Sat, 24 Jan 2026 18:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How a JSON or JSON-like language enables the next generation of safe human and AI-generated UIs\
It also helps if the approach is based on industry standards for which there is lots of documentation and examples that have probably been consumed by LLMs during training. cCOM and JPRX do this; they incorporate concepts and syntax from JSON Pointers, JSON Schema, and XPath.\
In my previous article, to show how a cDOM and JPRX work, I used a reactive counter, but let's be real: reactive counters and to-do lists are easy. Any framework looks elegant when the logic fits on a napkin. To prove a JSON-based approach actually holds up, you need a problem with messy state, edge cases, and distinct modes of operation. You need a calculator.\
Calculators are inherently tricky:Input Modes: Are we typing a fresh number or appending to an existing one?Chaining: What happens when you hit `+` then `-` then `*` without hitting equals?-DRY Logic: How do we minimize code differences between 10 handlers for buttons 0-9? \n So, I asked Claude Opus to build a fully functional, iOS-style calculator using zero custom JavaScript functions - just declarative cDOM and JPRX expressions. The fact that AI could produce a declarative calculator with little prompting purely from documentation demonstrates another point I made in my earlier article: cDOM and JPRX aren't just new syntax. They can be a protocol for human-machine collaboration. \n To reduce characters and quotation noise while allowing inline explanation, I am using cDOMC, a compressed version of a cDOM. A regular cDOM does not support comments and requires quotes around attributes and JPRX expressions. When represented with quotes and without comments, cDOM can be treated as regular JSON.{
  div: {
    class: "calculator",
    // A calculator feels stateless, but it's actually a strict state machine. 
    // You're never just "typing a number"; you're either entering the first operand, 
    // waiting for an operator, or entering the next operand.
    onmount: =state({
      display: "0", // What you see on the screen
      expr: "", // History string, (e.g. "8 + 5 =")
      prev: "", // value stored before an operation
      op: "", // the active operator
      waiting: false // true when expecting a new number vs operator
    },{
      name: "c", // the root name of our state, so we can express things like: /c/display
      schema: "polymorphic", // allow type changes, e.g. "0" or 0
      scope: $this // scope the path to the current element
    }),
    children: [
      // Display area
      {
        div: {
          class: "display",
          children: [
            { div: { class: "expression",children[=/c/expr] }},
            { div: { class: "result",children[=/c/display] }}
          ]
        }
      },
      // Button grid
      {
        div: {
          class: "buttons",
          children: [
            // Row 1: AC, ±, %, ÷
            {
              button: { 
                class: "btn btn-clear", 
                onclick: =/c = { display: "0", expr: "", prev: "", op: "", waiting: false }, 
                children: ["AC"] 
              } 
            },
            { 
              button: { 
                class: "btn btn-function", 
                onclick: =/c = { display: negate(/c/display), waiting: true, expr: "" }, 
                children: ["±"] 
              } 
            },
            {
              button: { 
                class: "btn btn-function", 
                onclick: =/c = { display: toPercent(/c/display), waiting: true, expr: "" }, 
                children: ["%"] 
              } 
            },
            // Divison is our first operator. This is where it gets tricky. 
            // When you click `+`, you can't just link `prev` to `display`. 
            // If you did, `prev` would update every time you selected a new digit for the**second**number, 
            // breaking the math. We need a snapshot of the value at that exact moment.
            // Excel solves this with INDIRECT, effectively dereferencing a cell. JPRX borrows the same concept:
            { 
              button: { 
                class: "btn btn-operator", 
                onclick: =/c = { 
                  prev: indirect(/c/display), // Capture the value right now
                  expr: concat(/c/display, " ÷"), 
                  op: "/", waiting: true 
                  }, 
                children: ["÷"] 
              } 
            },          
            // Row 2: 7, 8, 9, ×
            // I have 10 number buttons. Do I write 10 handlers? Do I write a loop? In React or Vue, 
            // you'd probably map over an array. With JPRX, the DOM is the data key and although map is available, 
            // I represent the calculator using literals in this example. In a future article I will cover map. 
            // By giving each button an `id` (e.g., `id: "7"`), we write a uniform logic expression that adapts 
            // to whichever element triggered it. We just reference $this.id in JPRX and use an xpath to get the text
            // content for the child node, #../@id. In cDOM (not JPRX) '#' delimits the start of an xpath expression
            { 
              button: { 
                id: "7", 
                class: "btn btn-number", 
                onclick: =/c = { 
                  display: if(/c/waiting, $this.id, if(/c/display==0, $this.id, concat(/c/display, $this.id))), waiting: false
                }, 
                children: [#../@id] // use xpath (starting char #) to get the text for the button from parent id
              } 
            },
            // Here's what is happening:
            // Waiting for input? (e.g., just hit `+`) → Replace the display with the button's ID.
            // Displaying "0"? → Replace it (avoids "07").
            // Otherwise: → Append the button's ID.
            // This is replicated identically for every number button. No loops, no external helper functions.
            {
              button: { 
                id: "8", 
                class: "btn btn-number", 
                onclick: =/c = { 
                  display: if(/c/waiting, $this.id, if(/c/display==0), $this.id, concat(/c/display, $this.id))), waiting: false 
                }, 
                children: [#../@id] 
              } 
            },
            { 
              button: { 
                id: "9", 
                class: "btn btn-number", 
                onclick: =/c = { 
                  display: if(/c/waiting, $this.id, if(/c/display==0, $this.id, concat(/c/display, $this.id))), waiting: false 
                }, 
                children: [#../@id] 
              } 
            },
            { 
              button: { 
                class: "btn btn-operator", 
                onclick: =/c = { 
                  prev: indirect(/c/display), expr: concat(/c/display, " ×"), op: "*", waiting: true 
                }, 
                children: ["×"] 
              } 
            },

            // Row 3: 4, 5, 6, −
            { 
              button: { 
                id: "4", 
                class: "btn btn-number", 
                onclick: =/c = { 
                  display: if(/c/waiting, $this.id, if(/c/display==0, $this.id, concat(/c/display, $this.id))), waiting: false 
                }, 
                children: [#../@id] 
              } 
            },
            { 
              button: { 
                id: "5", 
                class: "btn btn-number", 
                onclick: =/c = { 
                  display: if(/c/waiting, $this.id, if(/c/display==0, $this.id, concat(/c/display, $this.id))), waiting: false 
                }, 
                children: [#../@id]
              }  
            },
            { 
              button: { 
                id: "6", 
                class: "btn btn-number", 
                onclick: =/c = { 
                  display: if(/c/waiting, $this.id, if(/c/display==0, $this.id, concat(/c/display, $this.id))), waiting: false 
                }, 
                children: [#../@id] 
              } 
            },
            { 
              button: { 
                class: "btn btn-operator", 
                onclick: =/c = { 
                  prev: indirect(/c/display), expr: concat(/c/display, " −"), op: "-", waiting: true 
                  }, 
                children: ["−"] 
              } 
            },

            // Row 4: 1, 2, 3, +, use set and eq just to demonstrate equivalence with = and ==
            // the buttons below use 'set' in place of the infix operator '=', just to show a different way of doing things
            { 
              button: { 
                id: "1", 
                class: "btn btn-number", 
                onclick: =set(/c, { 
                              display: if(/c/waiting, $this.id, 
                                if(eq(/c/display, "0"), $this.id, concat(/c/display, $this.id))), 
                              waiting: false 
                         }), 
                children: [#../@id] 
              } 
            },
            { 
              button: { 
                id: "2", 
                class: "btn btn-number", 
                onclick: =set(/c, { 
                            display: if(/c/waiting, $this.id, 
                              if(eq(/c/display, "0"), $this.id, concat(/c/display, $this.id))), 
                            waiting: false 
                          }), 
                children: [#../@id] 
              } 
            },
            {
              button: { 
                id: "3", 
                class: "btn btn-number", 
                onclick: =set(/c, { 
                          display: if(/c/waiting, $this.id, 
                            if(eq(/c/display, "0"), $this.id, concat(/c/display, $this.id))),
                          waiting: false 
                        }), 
                children: [#../@id] 
              } 
            },
            { 
              button: { 
                class: "btn btn-operator", 
                onclick: =set(/c, { 
                            prev: indirect(/c/display), 
                            expr: concat(/c/display, " +"), 
                            op: "+", waiting: true }), 
                children: ["+"] 
              } 
            },

            // Row 5: 0, ., =
            { 
              button: { 
                id: "0",
                class: "btn btn-number btn-wide", 
                onclick: =set(/c, { 
                            display: if(/c/waiting, $this.id,   
                              if(eq(/c/display, "0"), "0", concat(/c/display, $this.id))), 
                            waiting: false }), 
                children: [#../@id] 
              } 
            },
            {
              button: 
              { 
                class: "btn btn-number", 
                onclick: =set(/c, { 
                            display: if(/c/waiting, "0.", 
                              if(contains(/c/display, "."), /c/display, concat(/c/display, "."))), 
                            waiting: false }), 
                children: ["."] 
              } 
            },
            // Finally, the math. We need to say:
            // 1. Take the snapshot we stored
            // 2. Apply the current operator
            // 3. combine it with what's on screen now
            // This is the job of calc(). If prev == 8 and op == * and display = 5, then calc would be evaluated as calc("8 * 5")
            // To keep the syntax a little cleaner we also use $(<path>) as a shorthand for indirect.
            { 
              button: 
              { 
                class: "btn btn-equals", 
                onclick: =set(/c, { 
                            display: if(eq(/c/op, ""), /c/display, calc(concat("$('/c/prev') ", /c/op, " $('/c/display')"))), 
                            expr: concat(/c/expr, " ", /c/display, " ="), 
                            prev: "", op: "", 
                            waiting: true }), 
                children: ["="] 
              } 
            }
          ]
         }
     },
     // Branding
     {
       div: {
         class: "branding",
         children: [
           { 
              span: { 
                children: [
                  "Built with ", 
                  { 
                    a: { 
                      href: "https://github.com/anywhichway/lightview", target: "_blank", 
                      children: ["Lightview"] 
                    } 
                  }, 
                  " cDOM • No custom JS!" 
                ] 
            } 
          }
        ]
       }
     }
   ]
  }
}
Lightview supports hypermedia capability similar to  by allowing the use of the  attribute on almost any element.\
Simply reference a  file using :<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">    
    <meta name="description"
      content="A beautiful calculator built with Lightview cDOM and JPRX reactive expressions - no custom JavaScript!">
    <title>Calculator | Lightview cDOM</title>    
    <link rel="stylesheet" href="calculator.css">    
    <!-- Load Lightview scripts -->    
    <script src="/lightview.js"></script>  <!-- DOM as JSON and reactivity support -->    
    <script src="/lightview-x.js"></script> <!-- hypermedia support -->    
    <script src="/lightview-cdom.js"></script> <-- cDOM/JPRX support -->
    </head>
  <body>    
    <!-- The calculator cDOM is loaded via Lightview's hypermedia src attribute -->    
    <div id="app" src="./calculator.cdomc"></div>
  </body>
</html>
\
The  attribute works like an HTML  or  tag - Lightview automatically fetches the  file, parses it, and renders the reactive content into the target element. This approach:You might look at concat("$('/c/prev') ...") and ask: Why in the world wouldn't you just writeparseFloat(prev) + parseFloat(curr)\
If you are a human writing code for yourself? You probably would. Lightview supports standard JS handlers for exactly that reason.\
But if you are building infrastructure for , the calculus changes. Sticking to a declarative, JSON-based path offers things raw code can't:: It executes in a controlled environment. The logic can't access `window`, make global fetch requests, or execute arbitrary secondary code. This makes it safe to "hot swap" UI logic generated by an LLM in real-time.: This entire UI—logic and all—is just data. It can be sent from a server, stored in a database, or streamed from an AI model.: It forces a clear separation between state transformations and view structure, which is exactly how LLMs reason best.\n This calculator proves that "declarative" doesn't have to mean "dumb." With the right primitives - state, conditionals, and path-based referencing—you can build rich, complex interactions without ever leaving the data structure.This series isn't just about a new library. It's about finding the right abstraction layer for the AI age.\n In this article, we showed that "Data as UI" doesn't mean "dumb UI." We handled state, context, data snapshots, math, and DOM navigation with ‘xpath’ without executing a single line of custom JavaScript.\n cDOM defines structure. JPRX defines behavior. It’s reactivity without the compilation and UI without the security risks.The complete calculator is available at:]]></content:encoded></item><item><title>CachyOS Starts 2026 By Switching To Plasma Login Manager &amp; Live ISO Using Wayland</title><link>https://www.phoronix.com/news/CachyOS-January-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 17:50:40 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Arch Linux powered CachyOS distribution is out with its first new ISO release of 2026. This Linux distribution continues to be quite popular with Linux gamers, enthusiasts craving peak performance, and others for wanting to enjoy a polished Arch Linux desktop experience...]]></content:encoded></item><item><title>DAXFS Proposed As Newest Linux File-System</title><link>https://www.phoronix.com/news/DAXFS-Linux-File-System</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 17:42:42 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[There's yet another new Linux file-system on the block: DAXFS has been announced as a new read-only open-source file-system...]]></content:encoded></item><item><title>US Insurer &apos;Lemonade&apos; Cuts Rates 50% for Drivers Using Tesla&apos;s &apos;Full Self-Driving&apos; Software</title><link>https://tech.slashdot.org/story/26/01/24/0736248/us-insurer-lemonade-cuts-rates-50-for-drivers-using-teslas-full-self-driving-software?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 17:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from Reuters:


U.S. insurer Lemonade said on Wednesday it would offer a 50% rate cut for drivers of Tesla electric vehicles when the automaker's Full Self-Driving (FSD) driver assistance software is steering because it had data showing it reduced accidents. Lemonade's move is an endorsement of Tesla CEO Elon Musk's claims that the company's vehicle technology is safer than human drivers, despite concerns flagged by regulators and safety experts. 

As part of a collaboration, Tesla is giving Lemonade access to vehicle telemetry data that will be used to distinguish between miles driven by FSD — which requires a human driver's supervision — and human driving, the New York-based insurer said. The price cut is for Lemonade's pay-per-mile insurance.
"We're looking at this in extremely high resolution, where we see every minute, every second that you drive your car, your Tesla," Lemonade co-founder Shai Wininger told Reuters. "We get millions of signals emitted by that car into our systems. And based on that, we're pricing your rate." 

Wininger said data provided by Tesla combined with Lemonade's own insurance data showed that the use of FSD made driving about two times safer for the average driver. He did not provide details on the data Tesla shared but said no payments were involved in the deal between Lemonade and the EV maker for the data and the new offering... Wininger said the company would reduce rates further as Tesla releases FSD software updates that improve safety. "Traditional insurers treat a Tesla like any other car, and AI like any other driver," Wininger said. "But a driver who can see 360 degrees, never gets drowsy, and reacts in milliseconds isn't like any other driver."]]></content:encoded></item><item><title>What&apos;s in Rust 1.77.2?</title><link>https://hackernoon.com/whats-in-rust-1772?source=rss</link><author>Rust (Technical Documentation)</author><category>tech</category><pubDate>Sat, 24 Jan 2026 17:30:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Rust team has published a new point release of Rust, 1.77.2. Rust is a programming language that is empowering everyone to build reliable and efficient software.\
If you have a previous version of Rust installed via rustup, getting Rust 1.77.2 is as easy as:\
If you don't have it already, you can get  from the appropriate page on our website.\
Before this release, the Rust standard library did not properly escape arguments when invoking batch files (with the  and  extensions) on Windows using the  API. An attacker able to control the arguments passed to the spawned process could execute arbitrary shell commands by bypassing the escaping.\
This vulnerability is  if you are invoking batch files on Windows with untrusted arguments. No other platform or use is affected.Many people came together to create Rust 1.77.2. We couldn't have done it without all of you. Thanks!The Rust Security Response WG]]></content:encoded></item><item><title>DXVK-NVAPI 0.9.1 Released With New Override &amp; Improvements</title><link>https://www.phoronix.com/news/DXVK-NVAPI-0.9.1</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 17:24:50 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[DXVK-NVAPI 0.9.1 is out today as this NVIDIA NVAPI implementation that is used by Valve's Steam Play (Proton) with DXVK and VKD3D-Proton. This is the important piece of the Steam Play puzzle to allow for NVIDIA DLSS, NVIDIA Reflex, PhysX, and other features for Windows games running on Linux...]]></content:encoded></item><item><title>A new test for AI labs: Are you even trying to make money?</title><link>https://techcrunch.com/2026/01/24/a-new-test-for-ai-labs-are-you-even-trying-to-make-money/</link><author>Russell Brandom</author><category>tech</category><pubDate>Sat, 24 Jan 2026 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[It’s getting hard to tell which AI labs. are actually trying to make money. We created a rating system to help sort it out.]]></content:encoded></item><item><title>A Game Studio&apos;s Fired Co-Founder Hijacked Its Domain Name, a New Lawsuit Alleges</title><link>https://games.slashdot.org/story/26/01/24/0535245/a-game-studios-fired-co-founder-hijacked-its-domain-name-a-new-lawsuit-alleges?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Three co-founders of the game studio That's No Moon "are suing another co-founder for allegedly hijacking the company's website domain name," reports the gaming news site Aftermath, "taking the website offline and disabling employee access to email accounts, according to a new lawsuit."


Tina Kowalewski, Taylor Kurosaki, and Nick Kononelos filed a complaint against co-founder and former CEO Michael Mumbauer on Tuesday in a California court. [Game studio] That's No Moon, which was founded in 2020 by veterans of Infinity Ward, Naughty Dog, and other AAA studios, said in its complaint that Mumbauer is looking to "cripple" the studio after being fired in 2022... 

Mumbauer, according to the complaint, purchased the domain name, and several others, when the studio was founded; it said both parties agreed these would be controlled by the studio. Mumbauer allegedly still has access to the domains, and That's No Moon said he took control over the website on Jan. 6, disabled the studio's access, and turned off employees' ability to email external addresses. The team was locked out for two days as a four-person IT team worked to get the services back online. On the public-facing side, the website briefly redirected to the Travel Switzerland page, according to the complaint. That's No Moon's lawyers said the co-founders sent Mumbauer a letter on Jan. 7 demanding he "relinquish his unauthorized access." That's when, according to the compliant, the website started redirecting to a GoDaddy Auction site, where the domain was priced at $6,666,666; That's No Moon remarked in the complaint: "A number that [Mumbauer] may well have selected for its Satanic connotation." 

As of Wednesday, Aftermath was able to access a public-facing That's No Moon website using both the original domain and the new one... The charges listed as part of this lawsuit are trademark infringement, cybersquatting, computer fraud, conversion, trespass to chattels, and breach of contract. That's No Moon also asked a judge for a temporary restraining order to prevent Mumbauer from continued access to the domains. Mumbauer has not responded to Aftermath's request for comment. Mumbauer said, in an email to That's No Moon attorney Amit Rana published as part of the lawsuit, that he intends to file "a wrongful termination countersuit and will be seeking extensive damages...." 

That's No Moon hasn't yet announced its first game, but has said the game is led by creative director Taylor Kurosaki and game director Jacob Minkoff. South Korean publisher Smilegate invested $100 million into the company, That's No Moon announced in 2021.]]></content:encoded></item><item><title>The HackerNoon Newsletter: How You Can Test Your Kids Smart Toys For Privacy (1/24/2026)</title><link>https://hackernoon.com/1-24-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Sat, 24 Jan 2026 16:02:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, January 24, 2026?By @TheMarkup [ 10 Min read ] Are those toys secure? And precisely what data is being handed over when a kid is using these toys? Read More.By @hennygewichers [ 5 Min read ] As AI makes authoritative text cheap, verification becomes the bottleneck. A police report failure shows why institutions are adding friction. Read More.By @carlwatts [ 31 Min read ] From mainframe DFSMShsm to cloud storage classes: a practical history of HSM, ILM, tiering, recall, and the products that shaped modern archives. Read More.By @dmitriy-tsarev [ 9 Min read ] A fine-tuned 3B model beat our 70B baseline. Heres why data quality and architecture innovations are ending the bigger is better era in AI. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Godot 4.4 Dev 2: Typed Dictionaries, Error-Less First Project Import, and More</title><link>https://hackernoon.com/godot-44-dev-2-typed-dictionaries-error-less-first-project-import-and-more?source=rss</link><author>Godot Engine (Technical Documentation)</author><category>tech</category><pubDate>Sat, 24 Jan 2026 16:00:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[It’s barely been a month since the 4.3 release, but we had so many goodies queued up in the PR backlog that it’s been an early Christmas (or other gift-heavy holiday season) for contributors and testers.\
We merged more than 200 PRs for the first dev snapshot, and just two weeks later, here we are with another batch of 350+ improvements for you to test and provide feedback on!\
Many of the changes in this release are bug fixes that will be backported to Godot 4.3 and released in 4.3.1! So please test this release well so we can be confident with the changes and release 4.3.1 with them as soon as possible.\
Keep in mind that while we try to make sure each dev snapshot is stable enough for general testing, this is by definition a pre-release piece of software. Be sure to make frequent backups, or use a version control system such as Git, to preserve your projects in case of corruption or data loss.\
Jump to the  section, and give it a spin right now, or continue reading to learn more about improvements in this release. You can also try the  or the  for this release. If you are interested in the latter, please request to join our testing group to get access to pre-release builds.The original cover illustration is from, an addictive inventory management roguelike, developed in Godot 4.3 by axilirate and published by Ravenage Games. It was recently released on Steam with big success. You can follow the development on Twitter.In case you missed it, check the 4.4 dev 1 release notes for an overview of some key features which were already in that snapshot, and are therefore still available for testing in dev 2.\
This new snapshot adds a lot more features which had been queued during the stabilization phase for Godot 4.3, and were thus ready for merging early on in the 4.4 development cycle.Godot 4.0 introduced supported for typed arrays, but lacked support for typed dictionaries. This rapidly became one of the most requested features to add to the engine, and thanks to Thaddeus Crews, it is now implemented for Godot 4.4! This feature impacts the core engine, GDScript, and all other scripting languages when interfacing with Godot’s Dictionary type. You can now export typed dictionaries from scripts and benefit from a much improved Inspector UX to assign the right keys and values.@export var typed_key_value: Dictionary[int, String] = { 1: "first value", 2: "second value", 3: "etc" }
@export var typed_key: Dictionary[int, Variant] = { 0: "any value", 10: 3.14, 100: null }
@export var typed_value: Dictionary[Variant, int] = { "any value": 0, 123: 456, null: -1 }
As a related improvement, support for StringName dictionary keys has also been optimized by Rune (GH-70096).Error-less first project importIt is commonplace for Godot users using version control systems to exclude the  folder from their repositories, as it contains files which can be re-created by Godot the first time you edit the project.\
One drawback of this system is that this first launch of the editor without  folder is typically quite noisy, with hundreds of errors spammed about missing resources, and various scripts failing to compile due to s not being known yet, addons not being registered yet, or yet unknown GDExtension classes.\
Hilderin valiantly fought their way through this labyrinth of dependencies and multi-threading pitfalls, and brought us back the long awaited grail: error-less first import of projects, which should work without requiring an editor restart! This took a lot of effort with GH-92303 (for GDScript s, included in 4.3) and now GH-93972 (for GDExtensions) and GH-92667 (for plugins).\
Moreover, with this newfound knowledge over the depths of , Hilderin also further improved that first project import experience to make the FileSystem dock more responsive while resources are being scanned (GH-93064).Editor window state is now persistentAnother long-awaited quality of life improvement was implemented by Samuele Panzeri in GH-76085, adding support for keeping track of the editor window’s state (fullscreen/windowed mode, screen, size, position) to restore it across sessions. This should be particularly welcome for users with big monitors or multi-monitor setups who want a different default behavior than fullscreen on the first screen.A few popular quality of life improvements have been implemented by Yuri Rubinsky for the visual shader editor:New material preview side dock (GH-94215).Drag & drop of Mesh to create MeshEmitter in visual shaders (GH-93017).Initial Android editor support for XR devicesThanks to Godot’s unique feature of having an editor made with the engine itself, we’ve been able to bring the Godot editor to unconventional places, such as the web and Android devices. Building upon the latter, Fredia Huya-Kouadio completed the proof of concept started by Bastiaan Olij years ago, to add support for using the Android editor in an XR context using OpenXR (GH-96624)! You can test the current version by sideloading the APK, currently supported on Meta Quest 3 or Quest Pro.There are too many exciting changes to list them all here, but here’s a curated selection:2D: Implement multiple occlusion polygons within each TileSet occlusion layer (GH-93029).2D: Enable  to “guess” the amount of rows and columns of a sprite sheet when loading it for the first time (GH-95475).3D: Add option to bake a mesh from animated skeleton pose (GH-85018).3D: Add full customization of 3D navigation controls (GH-85331).3D: Fix  debug collision shapes being visible in editor (GH-86699).3D: Add ability to hide editor transform gizmo (GH-87793).Animation: Update AnimationPlayer in real-time when keyframe properties change (GH-91599).Animation: Optimize AnimationMixer blend process (GH-92838).Animation: Allow keying properties when selecting multiple nodes (GH-92842).Animation: Allow jumping to previous/next keyframe in animation player (GH-96013).Animation: Use antialiased line drawing in animation Bezier editor (GH-96559).Audio: ResourceImporterWAV: Enable QOA compression by default (GH-95815).Audio: Fix leak when using audio samples instead of streams (GH-96572).Buildsystem: Add support for compiling with VS clang-cl toolset (GH-92316).Core: Ability to convert native engine types to JSON and back (GH-92656).Core: Batch of fixes for  and  (GH-94169).Core: WorkerThreadPool: Fix end-of-yield logic potentially leading to deadlocks (GH-96225).Core: ResourceLoader: Add thread-aware resource changed mechanism (GH-96593).Editor: Remember editor window mode, screen, size and position on restart (GH-76085).Editor: Fix script overwriting with external editor (GH-96007).Editor: Disable export template downloading in offline mode (GH-96331).Editor: FileSystem: Add option to show some unsupported files in the dock (GH-96603).Export: Allow adding custom export platforms using scripts / GDExtension (GH-90782).Export: Android Editor: Add support for exporting platform binaries (GH-93526).Export: macOS: Use per-architecture min. OS version for export (GH-95885).Export: Reenable macOS .app export from Windows, add warnings about Unix permissions (GH-96669).GDExtension: Allow ClassDB to create a Object without postinitialization for GDExtension (GH-91018).GDExtension: Implement  concept (GH-91166).GDExtension: Fix editor needs restart after adding GDExtensions (GH-93972).GDScript: StringName Dictionary keys (GH-70096).GDScript: Implement typed dictionaries (GH-78656).GDScript: Autocompletion: Improve autocompletion for indices (GH-79378).GDScript: Allow live reloading of built-in scripts (GH-94012).GDScript: Autocompletion: Reintroduce enum options on assignment (GH-96326).GUI: Implement fit content width in TextEdit (GH-83070).GUI: Improve SpinBox interaction, split arrows, add theme attributes (GH-89265).GUI: TextServer: 2x performance improvement by removing redundant lookups (GH-92575, GH-92581).GUI: CodeEdit: Improve render time by 2x (GH-92865).GUI: Fix  rectangles skewing independently (GH-96285).Import: Import/export GLTF extras to  and back (GH-86183).Import: Fix FileSystem dock won’t show any file folders while loading large projects (GH-93064).Import: Fix slow import when window is unfocused (GH-93953).Import: Add 3D Skeleton Preview to Advanced Importer (GH-96094).Import: Add “Use Node Type Suffixes” 3D scene import option (GH-96745).Navigation: Improve pathfinding performance by using a heap to store traversable polygons (GH-85965).Navigation: Improve  performance when jumping is enabled (GH-93319).Network: mbedTLS: Fix incorrect cert pinning with  (GH-96172).Network: Fix division by zero in network profiler (GH-96464).Particles: Add cone angle control to particle emission ring shape (GH-91973).Plugin: Fix addon requires editor restart to become functional (GH-92667).Porting: Windows: Respect integrated GPU preference in Windows Settings (GH-93985).Plugin: Add support for custom items to editor right-click context menus (GH-94582).Porting: Windows: Always use absolute UNC paths and long path aware APIs, add “long path aware” flag to the application manifest (GH-91902).Porting: Add support for non-blocking IO mode to  (GH-94434).Porting: Android Editor: Add support for launching the Play window in PiP mode (GH-95700).Porting: Android: Fix  so it actually works (GH-96182).Rendering: Tune TAA disocclusion scale to avoid rejecting all samples during motion (GH-86809).Rendering: Various fixes for transmittance effect (GH-93448).Rendering: Avoid indexing instances without a base in scene cull phase (GH-95503).Rendering: LightmapGI: Pack L1 SH coefficients for directional lightmaps (GH-96114).Rendering: Metal: Enable for betsy and lightmapper modules in compatibility mode (GH-96351).Rendering: Fix GPUParticles are not rendered for older AMD GPUs with OpenGL+Angle (GH-96413).Rendering: Compatibility: Enable MSAA support for all non-web platforms (GH-96455).Shaders: Allow drag & drop Mesh to create MeshEmitter in visual shaders (GH-93017).Shaders: Add basic support to evaluate operator value in shader language (GH-93822).Shaders: Add a material preview to visual shader editor (GH-94215).Shaders: Add a context menu for the shader editor file list (GH-95738).Thirdparty: mbedTLS: Update to 3.6.1, fixing regressions (GH-96385).Thirdparty: thorvg: Update to 0.14.9, fixing regressions (GH-96658).XR: Android editor: Improve support for XR projects (GH-96624). submitted  for this new snapshot. See our  for the complete list of changes since the previous 4.4-dev1 snapshot.This release is built from commit . includes support for GDScript and GDExtension. (marked as ) includes support for C#, as well as GDScript and GDExtension.\
While engine maintainers try their best to ensure that each preview snapshot and release candidate is stable, this is by definition a pre-release piece of software. Be sure to make frequent backups, or use a version control system such as Git, to preserve your projects in case of corruption or data loss.Typed dictionaries: Different typed keys/values are wrongly allowed when using the  operator (GH-96772). A fix is already in the pipeline for the next dev snapshot.Windows: Detecting newly added assets (GH-96828), export variables (GH-96810), and the last modification dates for projects (GH-96812) is not working in this snapshot, due to a toolchain bug exposed by a recent FileAccess change on Windows. This was already fixed by GH-74830.\
With every release we accept that there are going to be various issues, which have already been reported but haven’t been fixed yet. See the GitHub issue tracker for a complete list of known bugs.As a tester, we encourage you to open bug reports if you experience issues with this release. Please check the existing issues on GitHub first, using the search function with relevant keywords, to ensure that the bug you experience is not already known.\
In particular, any change that would cause a regression in your projects is very important to report (e.g. if something that worked fine in previous 4.x releases, but no longer works in this snapshot).]]></content:encoded></item><item><title>How You Can Test Your Kids&apos; Smart Toys For Privacy</title><link>https://hackernoon.com/how-you-can-test-your-kids-smart-toys-for-privacy?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sat, 24 Jan 2026 16:00:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Markup, now a part of CalMatters, uses investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up forKlaxon, a newsletter that delivers our stories and tools directly to your inbox.\
It’s a new era in entertainment for kids. Everywhere you look, new toys and devices are marketed toward children with smart features. Gift guides and store shelves tout Bluetooth- and Wi-Fi-enabled devices for kids, which promise an iPad-native generation a better way to play.\
But are those toys secure? And precisely what data is being handed over when a kid is using these toys?\
There are real reasons for concern: This year, the FTC hit Amazon with a $25 million fine because Alexa devices were storing recordings and transcripts of children’s voices, which the company retained even after parents requested deletion of the data. It’s just one of the many devices that could be recording your children’s speech or tracking their behavior.\
The Markup is looking into what other toys collect private data on your children. The FBI recommends parents do their own research on digital toy privacy but offers no advice on how to actually do so. We’re here to help.\
The first step is to read the privacy policy for a toy and, if applicable, its companion app. In particular, pay attention to what information is being tracked, how the info is used, and what partners that info is shared with. It might also include a section about the toy company’s compliance with the “Children’s Online Privacy Protection Act” for protecting children under 13.\
The next step is to figure out whether your device uses Wi-Fi, Bluetooth, or both; that information should be displayed on the toy’s box or in the instruction manual. A Wi-Fi-enabled toy will likely communicate with your wireless router to send data to the internet (though, in limited circumstances, the toy may also communicate with other devices on the same Wi-Fi network). Meanwhile, a Bluetooth toy will send data to another smart device, such as a smartphone, which may then relay that data to the internet using Wi-Fi or cellular data.\
Then, grab your computer (and your smartphone if your toy uses Bluetooth). Our instructions assume you’re using macOS and iOS, though the tools below also have Windows or Android (for HTTP Toolkit) alternatives. We have not tested these instructions on Linux, but they should work there as well.\
To capture traffic from a Bluetooth device, you will need to use a man-in-the-middle proxy that intercepts and monitors the network traffic between the device you want to monitor and the internet. We recommend using HTTP Toolkit, which has a nice interface and works on most operating systems.\
To find out what your toy is up to, follow the steps below (modified from the instructions here). Ensure your smartphone and computer are connected to the same Wi-Fi network. Find your computer’s local network IP address by going to System Settings > Network > Wi-Fi. Then select the Details button (see below). Note the IP address listed there; you’ll be entering it into your smartphone in Step 7.Go to System Settings > Network > Wi-Fi. Then select the Details button, shown below.Note the IP address, circled below. Go to your Application folder and open HTTP Toolkit. If you have an Apple Silicon chip, your computer may ask you to install Rosetta, which is an Apple compatibility layer for older Intel software. Go ahead and click “Install.”You may get an alert that says, “‘HTTP Toolkit’ is an app downloaded from the Internet. Are you sure you want to open it?” Yes, it’s OK to open it. Once HTTP Toolkit is open, you should automatically be on the Intercept HTTP page. Scroll down a bit and select the “Anything” box. Now we’ll configure our smartphone to use the proxy. Go to Settings > Wi-Fi. Then select the ⓘ icon next to the Wi-Fi network you’re connected to. Scroll down the screen and select Configure Proxy. Select the Manual option. Under “Server,” enter the IP address you saved from Step 3. Under “Port,” enter “8000,” which is HTTP Toolkit’s default port setting (as long as you didn’t change anything). You don’t need to toggle “Authentication.” Make sure you change your phone’s proxy setting back to “Off” after you’re finished monitoring your toy. Otherwise, you won’t be able to connect to the internet. (We’ll remind you again at the end of these steps.) HTTP Toolkit shows data from every app on your phone all at once, so if you’re running too many apps on your device, it may be difficult to see which traffic is coming from the toy. Before you go further, we recommend closing any apps by swiping up from the bottom edge of your phone and swiping up again on each open app. Then, go to Settings > Battery and turn on Low Battery Mode, which pauses any apps from updating or sending traffic in the background. We’ll remind you to turn Low Battery Mode back to “Off” after you’re done monitoring your toy. Finally, go to Settings > Privacy & Security. Turn off Tracking (the second option on the screen), then scroll to the bottom of the screen to find Analytics & Improvements and Apple Advertising. Turn those settings off as well. We recommend keeping these settings off after you’re finished checking the toy. Return to HTTP Toolkit on your computer, and download the root certificate by selecting “Export CA certificate.” Internet traffic is usually encrypted between your computer and the destination server, but the root certificate allows HTTP Toolkit to act as the “destination” for your traffic and intercept it. Email or AirDrop the certificate to your iPhone.If using email, open with the built-in Mail app, which is the only iOS mail app that can open certificates. If using AirDrop, accept the request.If you get a “Choose a Device” prompt, select “iPhone.” You should see a “Profile Downloaded” message on your phone after sending the certificate. Close this. On your iPhone, go to Settings > General > VPN & Device Management. You should see “HTTP Toolkit CA” in the “Downloaded profile” section. Click on it, then click “Install” in the top right corner. You may be prompted to enter your passcode. Click “Install” again.Then, go to Settings > General > About > Certificate Trust Settings and enable "Full Trust" for the HTTP Toolkit certificate. Head back to HTTP Toolkit on your computer and switch to the “View” tab on the left. You should see a bunch of text flying by on your screen. Each line of text is a piece of data that’s being sent by your phone to the internet, including from iOS and any apps you have running on your phone that may be doing things in the background. As a test, open an internet browser on your phone and navigate to any website (themarkup.org, for example). If your window looks like this, congrats! Click on a line to get more details in the right window pane. For example, in the image above, the third line from the bottom shows that Safari downloaded an image from the Markup homepage. When you are finished monitoring your Bluetooth device, make sure you return your computer and smartphone to its original settings:On your computer, quit HTTP Toolkit.On your phone, change the Wi-Fi proxy setting on your phone back to “Off”; otherwise, you will not be able to connect to the internet. Turn on “Wi-Fi assist” if you prefer to keep it on. Then turn off “Low Battery” mode.To remove the root certificate from your phone, go to General > Settings > VPN & Device Management. Select “HTTP Toolkit CA,” then select “Remove Profile.” Select “Remove” when prompted.\
We tested these instructions on the “Encanto” karaoke machine and monitored the network traffic as we configured the toy using its EZ Link app. The app asked us to enter a birthday and choose which music streaming service we wanted to use. The app is very basic: When you push the big button on the front of the karaoke machine, it sends a notification to your phone with a link to the “Encanto” soundtrack on the streaming service you’ve chosen.\
We then tried all the features on the toy, including playing music, using the microphone, and recording to a USB drive. Overall, HTTP Toolkit finds … nothing! It turns out this app is very secure and doesn’t send any data. Nice going, eKids.Wi-Fi-enabled toys connect directly to your wireless network, so they require a different method of tracking. There are expensive ways to track Wi-Fi toys, including buying a separate adapter, but we’ll show you how to track these toys for free using IoT Inspector, a research tool that allows you to monitor the network traffic of IoT devices on your home Wi-Fi.\
Caveat: IoT Inspector may have bugs. Running it might interfere with your network, and, in the worst case, could require you to restart your router. Not every toy is guaranteed to function correctly while IoT Inspector is running.\
⚠️ Do not run IoT Inspector on any network that you do not own. It is only safe to run IoT Inspector on your home Wi-Fi.Download and run IoT Inspector on your computer. If you are using Windows, you can download the zip file here, extract it, double-click “IoT Inspector,” and then skip to Step 7. If you are on a Mac, open the Terminal app. It can be found by opening Finder, navigating to Applications > Utilities, and opening “Terminal.” Install macOS’s command line tools by copying **xcode-select --install** into the Terminal window, and press your  (or  key. This might take some time to install. If the tools are already installed, update your software in System Settings before continuing. Next, we will download and configure IoT inspector. Copy and paste the following block of code into the Terminal and hit Return. Start IoT Inspector by pasting the following block of code into Terminal and hitting Return. Your Terminal may ask you for your password. If it does, type your computer account password into the Terminal—it won’t show you typing, but it works!—and press Return. Copy the “Network URL” that appears in the Terminal, and paste it into your web browser. Agree to the terms and warnings associated with running IoT Inspector. Optionally, you may consent to share your network data to the IoT Inspector research project at NYU. See the Privacy Policy for more info on the data it collects. Once IoT Inspector is open, you will see a list of devices running on your home network. Each entry lists the device’s local IP address (for internet connectivity) and MAC address (akin to a hardware serial number). For example, the second device in the image below has a local IP address of  and a MAC address ending in . IoT Inspector will try to identify the manufacturer of devices on your network. For example, the second device in the image above is guessed to be an Apple device. However, this guessing is not perfect. If you have never connected the toy to your home network, finding the toy is as simple as connecting the device and clicking the new device that appears at the bottom of the list. Otherwise, you will need to identify the toy by various means. The toy might display a MAC or local IP address in its settings or on a connected phone app that will help you identify the device. If not, try performing actions on the toy and see if one of the devices in IoT Inspector starts reporting more traffic. Once you’ve identified the toy, uncheck “Inspect” for all other devices on the network so that you are only monitoring the toy you care about. When you click on the toy, you will see all traffic—though unlike Bluetooth devices, IoT Inspector will only show you the domains being contacted, not exactly what data is being sent.To quit IoT Inspector, click “Quit IoT Inspector” in the bottom left. If this does not work, go back to your Terminal and hit Command + Q or click “Terminal” in the top left of your screen and then “Quit Terminal.”When you are finished monitoring your toy, we recommend going to IoT Inspector’s “Settings” and clicking “Reset IoT Inspector” to clear the list of devices. Otherwise, you might see old devices in the list the next time you run IoT Inspector. If you opted into sharing your network data with NYU researchers, you may also delete your shared data here. To rerun IoT Inspector later, open Terminal and run the following lines of code: Then, go back to Step 6 and continue the instructions from there, skipping Step 7 since you’ve already agreed to the terms and warnings.With some toys, you may find that they are sending data to strange domains. Try to look for correlations between your actions and data. For example, if you turn on a toy’s camera and see large amounts of data being sent while the camera is running, this suggests the toy is sending camera data. All of this analysis should be combined with reading the toy’s manual thoroughly and experimenting with the toy’s various actions to understand its full capabilities.\
We tried these instructions on a Tamagotchi Uni and found that the toy only sends data to aws.amazon.com. The Uni uses Amazon AWS to manage its online connectivity. When starting up the Uni for the first time, the toy only asks for a nickname and birthday. We could not confirm further what data is being sent to Amazon.]]></content:encoded></item><item><title>How PopWheels helped a food cart ditch generators for e-bike batteries</title><link>https://techcrunch.com/2026/01/24/how-popwheels-helped-a-food-cart-ditch-generators-for-e-bike-batteries/</link><author>Tim De Chant</author><category>tech</category><pubDate>Sat, 24 Jan 2026 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[PopWheels realized its battery swapping network could be used for more than food delivery workers who need to charge their e-bikes. Now, its planning for an "aggressive rollout" this summer. ]]></content:encoded></item><item><title>Fluent AI Output Is Straining Human Verification Systems</title><link>https://hackernoon.com/fluent-ai-output-is-straining-human-verification-systems?source=rss</link><author>HennyGe Wichers, PhD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 15:53:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How zero-cost, authoritative-sounding text is breaking institutional accountability\
On January 16, the chief constable of West Midlands Police in the United Kingdom stepped down after  that an AI tool had been used to help draft an official safety assessment that cited a football match that never took place. The report was used to justify banning supporters of Maccabi Tel Aviv from attending a Europa League match against Aston Villa in November 2025. Embedded in the assessment was a reference to a  between Maccabi Tel Aviv and West Ham that did not exist.\
Under questioning, the chief constable acknowledged that part of the document had been drafted using . Political attention followed fast: the home secretary said she no longer had  in his leadership, and the region’s police and crime commissioner announced a public accountability hearing. What initially looked like a technical error quickly escalated into a leadership crisis. How could an official-sounding, finished-looking document enter a high-stakes decision process without clear verification?\
Much of the public debate has focused on bias, judgment, and individual responsibility, but the episode points to a structural problem that has been developing for a few years now.From human diligence to synthetic diligenceFor decades, modern institutions relied on an implicit assumption: if a document existed - especially one that looked formal, reasoned, structured - someone had spent time producing it. Reports, legal filings, safety assessments, and policy briefings were costly to generate and even low-quality work required hours of human attention. That cost function created an informal but reliable signal of accountability.\
But Generative AI breaks that assumption.\
Draft-quality pieces can now be produced in seconds, arguments and citations included, and look convincing even when the underlying claims are entirely fabricated or misinterpreted. The issue in this case is not that automated systems sometimes hallucinate; humans make mistakes, too. The issue is that institutions have no scalable way to distinguish between text produced by a person reasoning through a problem and text produced by a model optimised to mimic that reasoning.\
As the cost of producing authoritative-sounding text begins to approach zero, institutions begin to deal with synthetic work faster than they can verify it. Safety assessments, legal briefs, student essays, internal reports, and consultancy deliverables all start to look finished long before anyone has actually done any of the work implied by their appearance.\
That fluency substitutes human judgment and verification becomes the new bottleneck.Where failures are already visibleThe West Midlands case is not an isolated one and similar failures are already forcing adjustments across institutions: courts, universities, government bodies, professional services, and even journalism have all been caught out.Judges in several jurisdictions have sanctioned lawyers for submitting filings containing AI-generated, non-existent case law. In the United States, the  case led to fines after attorneys relied on fabricated citations produced by ChatGPT and legal analysis judged “gibberish.” In response, some federal judges, like Judge Brantly Starr in Texas, have introduced standing orders requiring lawyers to  that they have personally reviewed and verified any AI-assisted content. Courts in England and Wales have issued  that submitting fictitious AI-generated case law may amount to professional misconduct or contempt. These measures are not bans on AI tools; they are attempts to re-establish a clear line of human accountability in the court record.Higher education institutions  a similar verification problem. Many have concluded that detecting AI use in take-home assignments is . One student said to ABC News it’s difficult, explaining: "We're looking at the same piece of legislation, we're quoting the same cases, we're looking at the same issues,” while another gave up: ‘"I just decided to take the punishment because I was simply too scared to argue further."Some departments have reintroduced handwritten or supervised exams, expanded oral assessments, and shifted evaluation into in-person settings. Oxford’s Faculty of Medieval and Modern Languages  closed-book, handwritten exams. The University of Sydney  unauthorised AI use as an academic integrity violation and has tightened assessment design accordingly. Regulators in Australia have explicitly  universities to move away from assessment formats where authorship cannot be reliably established.Governments are beginning to formalise disclosure and auditability requirements for algorithmic tools. In the UK, the Algorithmic Transparency Recording Standard ( requires public bodies to document and publish information about the automated systems they deploy. The government’s  emphasises accountability, human oversight, and transparency in public-sector AI use. And, at the European level, the  introduces obligations to disclose AI-generated or manipulated content in certain contexts. These frameworks are early attempts to ensure that official decisions can later be scrutinised - not just for what they say, but for how they were produced.The private sector is encountering the same problem, often with direct financial consequences. In Australia, Deloitte produced a  for the Department of Employment and Workplace Relations that was later found to contain  references and made-up court quotes. The firm acknowledged that parts of the report were drafted using a generative AI toolchain and refunded a portion of its fee after the errors were exposed. The model behaved as designed: it generated plausible text. What failed was the workflow. AI-assisted output passed through internal checks and into a government decision environment without adequate human verification.Similar episodes have surfaced elsewhere. Media outlets including CNET and MSN have retracted AI-generated articles containing  or . Air Canada was  after its website chatbot gave a customer incorrect information about refund eligibility. In academic publishing, papers have been found to include  linked to automated text generation.\
Across these cases, we see a consistent pattern. Institutions assumed that efficiently produced text was a reliable signal of underlying work. But that assumption no longer holds.Why institutions are adding frictionThe emerging responses - manual attestation, in-person assessment, disclosure requirements, limits on undeclared AI use - can look like resistance to innovation. It is not. It is an attempt to restore a basic institutional function we still rely on: linking text to responsibility.\
When verification capacity is scarce, adding friction is rational rather than Luddite. If an organisation can generate more documents than anyone can realistically check, it accumulates decisions that no one can truly own. Over time, that erodes internal trust and external legitimacy: colleagues stop believing that reports reflect real expertise. Courts, regulators, and the public lose confidence that official records rest on accountable judgment.\
The West Midlands episode illustrates this dynamic clearly. The political fallout was not caused solely by an incorrect reference. It was caused by the revelation that a document carrying real consequences had entered an official process without anyone being able to say, with confidence, who - if anyone - had verified it.Generative AI does not simply make institutions faster. It changes what is scarce: production is now abundant, verification is not.And that shift requires a redesign of institutional workflows. Provenance - how a document was produced, who edited it, who checked it, and who stands behind it - now needs to become explicit rather than assumed. Some categories of work will need clear boundaries where identifiable human authorship remains non-negotiable. Others may accommodate automation, but only within review limits that match available oversight.\
This is not a temporary adjustment. Synthetic diligence is cheap and convincing, and failures like the one in West Midlands are likely to continue to happen. Each event will test public trust - in AI tools and, more importantly, in institutions and their safeguards.\
The institutions that adapt will be those that accept a slower, more verification-centric mode of operation in high-stakes contexts. Those that don’t will continue to produce documents that look finished - until the moment they are forced to explain who actually did the work.:::info
Lead image credit: AdobeStock |132785912]]></content:encoded></item><item><title>Why Traditional Software Testing Breaks Down for AI Agents</title><link>https://hackernoon.com/why-traditional-software-testing-breaks-down-for-ai-agents?source=rss</link><author>Manoj Aggarwal</author><category>tech</category><pubDate>Sat, 24 Jan 2026 15:37:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Everybody is building an AI agent. They work beautifully because they are powered by models like GPT, Claude Opus, etc. which are trained on massive amounts of data. However, the key problem that is not talked about widely is about testability of these agents. You don’t want your AI agent to hallucinate in production, or do drastic things like dropping production tables or get stuck in infinite retry loops.The traditional software testing playbook of unit tests, integration tests, end to end tests with known inputs and expected outputs fundamentally breaks down for agentic AI systems. When the system’s behavior is non-deterministic, or when the state space is infinite, or when the correct output is subjective, you would need a different approach.At DocuSign and across the industry, you are building agents that orchestrate complex workflows like analyzing documents, making decisions, calling APIs, handling errors and adapting to user feedback. These systems fail in ways that predetermined test cases cannot catch.Why Traditional Testing Fails for AgentsLet’s take a traditional API endpoint as an example: give input X, it returns output Y. You write a test asserting this. The test is deterministic, repeatable and either passes or fails. This approach works because the mapping from inputs to outputs is known and stable.Now consider an agent that is tasked with “schedule a meeting with engineering team next week”. The agent would:Interpret “next week” based on current dateIdentify who are in the “engineering team”Check availability on calendarsConfirm completion and return successAt every step, your agent makes a decision based on context and available tools. The exact sequence of API calls is not predetermined. It comes from agent’s reasoning. Two runs on the same input might take completely different paths and still be correct.A test case that asserts “the agent calls  exactly twice, then calls  with these parameters” would not work. Your agent might check calendars once or multiple times if there are conflicts. It might batch send invitations or individually. These are implementation details and not correctness criterion.Traditional testing assumes deterministic state machines, however, agents are non-deterministic. The gap is inevitable.So how do you test something you can't fully specify? The answer is to stop testing execution paths and start testing properties, invariants, and outcomes. This requires thinking in three distinct layers, each with its own testing strategy.The Three Layers of Agent TestingEffective agent testing requires thinking in layers.The agent’s tools are essentially the functions it can call and they must work deterministically. If  sometimes returns wrong data, the agent will make wrong decisions irrespective of the reasoning.Testing of the tool is traditional unit testing:def test_get_calendar():
    calendar = get_calendar("alice@example.com")
    assert calendar.owner == "alice@example.com"
    assert all(isinstance(e, Event) for e in calendar.events)
    assert all(e.start < e.end for e in calendar.events)
This layer is necessary but insufficient. Perfect tools do not guarantee correct agent behavior. They just eliminate one source of failure.Layer 2: Reasoning VerificationThe reasoning of the agent i.e. how it decides which tools to call and in what order, cannot be tested with assertions about the tool sequences. But the invariants and properties can be tested regardless of the path taken. tests the properties that must always be true.For the meeting scheduler example above, agent must not:create conflicting meetingssend multiple invitations per participantThese invariants don’t care about whether the agent calls  serially or in parallel, but do care about things like double-booking someone:def test_no_double_booking_invariant(agent, test_scenario):
    result = agent.schedule_meeting(test_scenario)

    for participant in result.participants:
        calendar = get_calendar(participant)
        events_at_time = [e for e in calendar.events 
                         if overlaps(e, result.scheduled_time)]
        assert len(events_at_time) == 1, \
            f"{participant} double-booked at {result.scheduled_time}"
 generates random valid inputs and verifies:from hypothesis import given, strategies as st

@given(
    participants=st.lists(st.emails(), min_size=2, max_size=10),
    duration=st.integers(min_value=15, max_value=120),
    timeframe=st.datetimes()
)
def test_meeting_scheduling_properties(agent, participants, duration, timeframe):
    result = agent.schedule_meeting(
        participants=participants,
        duration=duration,
        preferred_time=timeframe
    )

    # Properties that must hold
    assert result.duration == duration
    assert set(result.participants) == set(participants)
    assert result.scheduled_time >= datetime.now()
    assert len(result.invitations_sent) == len(participants)
You are testing the outcome and not the path. The agent can reason however it wants, as long as the result satisfies the properties.Layer 3: Behavior EvaluationBehavior of agents is subjective and non-deterministic. “Did the agent write a helpful email?” depends on context.For these, you need evaluation rather than testing since tests assert definite correctness but evaluations score quality.def evaluate_email_quality(agent_output, context):
    prompt = f"""
    Evaluate this email on a scale of 1-10 for:
    - Professionalism
    - Clarity
    - Completeness
    - Tone appropriateness

    Context: {context}
    Email: {agent_output}

    Return JSON with scores and brief justifications.
    """

    evaluator = LLM(model="gpt-4")
    scores = evaluator.generate(prompt)
    return scores
This is controversial but is increasingly common. The key thing is that the evaluator model should be powerful as compared to the model being evaluated. You don’t want to evaluate GPT-3.5 to judge GPT-4. This trades subjective human judgement for LLMs but we are gaining automation and consistency.For critical workflows, human evaluation is still necessary. So, instead of asking them to judge, you can give them specific criteria:evaluation_rubric = {
    "correctness": "Did the agent accomplish the stated goal?",
    "efficiency": "Did it use the minimum necessary API calls?",
    "error_handling": "Did it gracefully handle failures?",
    "user_experience": "Would a user be satisfied with this interaction?"
}

def human_eval(agent_trace, rubric):
    scores = {}
    for criterion, description in rubric.items():
        score = input(f"{description}\nScore (1-5): ")
        scores[criterion] = int(score)
    return scores
For instance, you can use this to track scores over time and judge if the system is regressing or improving.Agent Testing is Upside DownThe traditional software testing goes like like: many unit tests, fewer integration tests, few end-to-end tests. For agents, it’s the reverse. Most of the testing effort goes to end-to-end scenarios because that’s where emergent failures happen. An agent might call each tool correctly and passes unit tests, but might get stuck in a loop calling the same tool repeatedly and will fail in end-to-end test.So the testing distribution might look like: 20% tool unit tests, 30% invariant and property tests, and rest of it on end-to-end scenarios. These scenarios should cover happy paths, partial failures, ambiguous inputs and adversarial inputs.Observability is Testing in ProductionIt is impossible to test all the possible scenarios before deployment, so production monitoring becomes continuous testing playground.This can be achieved with:: Each agent action should be logged with context to reconstruct reasoning. Define metrics that indicate agent health. For example, success rate, tool call efficiency, error rate, retry rate, etc. Flag sessions with excessive tool calls or repetitive patterns suggesting loops, or high error rates. Anomalous sessions go into a review queue where humans label them as bugs or edge cases or expected behavior.Code coverage is meaningless for agents. The agent has minimal code and most of the behavior comes from LLM’s weights, which you don’t control. Every line of code could be executed and still you can have zero test coverage.What can still be measured is scenario coverage (distinct scenarios tested), tool coverage (percentage of tools called), and state coverage (system states reached). But still the state space is exponentially large to cover all of them.Testing agents resembles testing distributed systems more than libraries. You cannot prove correctness, but you can increase confidence through comprehensive scenarios, invariant testing, fault injection, monitoring and rapid iteration on observed failures.Conclusion: Testing Under UncertaintyTraditional software testing assumes that you can specify correct behavior in advance. However, agent testing assumes you cannot.You cannot write enough tests to prove that an agent works. But you can only build confidence through other things like the ones I mentioned above. This simply means shipping agents that might fail in ways you haven’t anticipated and accepting that your test suite will never give you complete coverage. This is uncomfortable but liberating since you are not trying to enumerate all possible inputs and outputs.The teams that succeed with agents won’t be those with the most test cases, but those that are operating comfortably under uncertainty, learning from failures and iterating quickly based on what they observe in production. The traditional QA testing strategy is fundamentally wrong because applying deterministic thinking to non-deterministic systems is counter-productive.]]></content:encoded></item><item><title>HSM: The Original Tiering Engine Behind Mainframes, Cloud, and S3</title><link>https://hackernoon.com/hsm-the-original-tiering-engine-behind-mainframes-cloud-and-s3?source=rss</link><author>Carl Watts</author><category>tech</category><pubDate>Sat, 24 Jan 2026 15:34:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If you think “tiering” started with SSD caches and cloud storage classes… congratulations: you’ve been successfully marketed to.Hierarchical Storage Management (HSM) has been doing the same core job for decades:Keep “hot” data on fast, expensive media… push “cold” data to slower, cheaper media… and make the user mostly forget you exist—until recall latency reminds them you do.What changed over 50-ish years isn’t the . It’s the , the , the , and the **ways vendors try to rename the same thing so procurement feels like it’s buying “innovation.” 😄Hierarchical Storage Management (HSM) is the storage world’s oldest magic trick: make expensive storage look bigger by quietly moving data to cheaper tiers—without breaking the namespace. Users see the same files. Admins see fewer “disk full” tickets. Everyone pretends recall latency isn’t real until someone double-clicks a 400 GB file that’s been living its best life on deep archive.At its core, HSM is policy-driven data placement across a storage hierarchy, with some kind of  when data is accessed again. Historically that hierarchy was “fast disk → slower disk → tape/optical.” In modern stacks it’s often “hot → cool → cold → archive,” sometimes spanning on-prem, object, and cloud.The five moving parts of classic HSM (and why each matters)1) A primary tier: where “now” livesThis is the storage that must behave like storage:fast disk, SSD, high-perf NAS, scratch, or a parallel file systemlow latency and high throughputoptimized for active workflowsThis tier is  where you want decades of “just in case” to accumulate.2) Secondary tiers: where “later” livesHSM is only interesting if there’s a cheaper place to put cold data:optical or other long-retention mediaThese tiers trade performance for economics (cost/GB, power, density, longevity). The whole HSM game is deciding what belongs where and when.3) A policy engine: the brains (and the source of most arguments)Policies decide  and . Classic inputs look like: (mtime, atime, “last referenced”) (large files are prime migration candidates) (everything under /project/foo/ follows the foo rules) (user/group-based policies) (hot/warm/cold heuristics)Modern policy engines add:tags/metadata (object systems)workflow signals (job completed, project closed)compliance state (retention, legal hold)cost signals (retrieval charges, egress exposure)A good policy engine doesn’t just migrate data—it prevents you from migrating the  data and learning about it in production.4) A recall mechanism: how the illusion stays intactThis is the “don’t break users” component.In file-centric HSM, recall is often enabled by:stub files / placeholders that preserve name, size, timestamps, permissions, etc.file system hooks/events so “open()” triggers stage-back automaticallythrottling/queuing so one scan doesn’t recall the planetIn object-centric HSM/lifecycle, recall may look like:copies into a hot bucket/work areaexplicit workflow steps instead of “transparent recall”Either way, recall is where HSM becomes real. Migration is easy. Getting it back safely, predictably, and at scale is the hard part.5) A migration engine: the muscle and the traffic copsThis is what actually moves bytes and keeps the system from melting down:movers (data transfer processes/hosts)queues and prioritizationmedia managers (especially for tape/robotics)retry logic, auditing, and state trackingIf you’ve ever seen a recall storm, you already know: the migration engine isn’t just a copier. It’s the difference between “smooth tiering” and “incident ticket with your name on it.”What HSM is  (because people keep using the word wrong)is about recoverability after loss/corruption. \n HSM is about space and cost optimization through relocation.Some systems combine them, but the intent is different:HSM moves data  to free space (and expects recall to work)Backup makes additional copies for recovery (and doesn’t assume the primary copy disappears)If your “HSM” deletes primary data but you have no independent recovery story, that’s not HSM—that’s . copies hot blocks/data closer to compute while leaving the authoritative copy in place.   typically relocates entire files/objects to a different tier based on policy.Caching says: “keep a fast copy nearby.” \n HSM says: “this doesn’t belong on expensive storage anymore—move it.”They can coexist (and often should), but they solve different problems.The simplest way to think about HSMHSM is a governed lifecycle for data placement:decide what’s hot vs coldmove cold data to cheaper tierspreserve access semantics as much as practicalcontrol recall so it’s predictabletrack state so you can prove where data is and whyOld HSM did that with stubs and tape robots. \n Modern HSM does it with lifecycle rules, object metadata, and restore workflows.Same job. New interface. Same operational booby traps.HSM vs ILM: same family, different job titlesPeople mix up Hierarchical Storage Management (HSM) and Information Lifecycle Management (ILM) because vendors spent two decades using both terms as interchangeable marketing confetti. They’re related, but they are not the same thing.. It moves data between tiers.. It decides what the organization  do with data over time—and proves it did it. framing is useful here: ILM is an ongoing strategy that aligns the  of information with , , and  across the information lifecycle—not a single box you buy and plug in.The lifecycle view: ILM starts before storage gets involvedILM begins at the moment data is created or acquired and continues until final disposition. That lifecycle typically includes: (how it enters the org, what metadata/tags exist) (who can access it, performance requirements) (distribution, replication, copies, versions) (backup, snapshots, replication, integrity/fixity) (records, archives, research collections) (legal holds, privacy rules, access controls) (deletion, destruction, transfer, reclassification)HSM usually shows up in the middle of that story as the “move it to cheaper storage” execution layer—not as the policy authority deciding what’s kept, for how long, and under what legal constraints.Practical translation (the one you actually use in architecture meetings)HSM = the machinery (execution layer)HSM is the set of technical capabilities that make tiering real: /  across disk, object, tape, cloud classes and  rules to free primary storage /  workflows (stub recall, rehydrate, restore jobs) and  (throughput, throttling, prioritization) (tape libraries, volume catalogs, robotics control) (resident/offline, restore in progress, pinned, etc.)HSM answers: “Where does this data live today, and how do we move it?”ILM = the governance layer (intent + accountability)ILM is the framework that ties data handling to business and legal requirements:Retention schedules (keep 7 years, keep forever, keep until case closes) and  (prove it wasn’t altered; prove deletion) (freeze disposition and sometimes freeze movement) (public, internal, restricted, sensitive) and  (what storage class is acceptable, retrieval tolerances) (defensible deletion, transfer, or preservation) (who approves, who changes, who signs off)ILM answers: “What must happen to this data over its life, and can we prove it?”Where people go wrong: buying “ILM” as if it’s a productA lot of vendors sold “ILM” as a feature bundle. The problem is that ILM isn’t a feature, it’s a cross-cutting system of:organizational policy (records management, legal, security)operational processes (approvals, audits, exceptions, incident handling)tooling (metadata/tagging, analytics, movement engines, retention enforcement)You can buy components that support ILM, but the “ILM” part is the  and —and no vendor ships that preconfigured to match your regulatory obligations and internal politics. (If they claim they do, they’re selling you optimism.)How they fit together in a modern stackA useful way to describe it in your article:: retention, access rules, acceptable latency, cost constraints, compliance obligations.: migration, tier transitions, restore workflows, and enforcement mechanisms.**Example: \n **ILM says: “This collection must be retained for 25 years, immutable, with restores allowed but tracked, and retrieval should be under 24 hours.” \n HSM implements: “After 90 days, transition to cold/archive tier; enable restore workflow; restrict who can rehydrate; log restores; enforce immutability.”ILM is the ‘why’ and the ‘rules of the road.’ HSM is the ‘how’—the engine that moves data across tiers in a way that matches those rules, especially in file-centric and hybrid environments.A quick timeline: mainframe → open systems → HPC → clustered file → object/cloudMainframe roots (late 1970s through the 1980s): policy before it was coolMainframe HSM wasn’t born as a “nice-to-have.” It showed up because Direct Access Storage Device (DASD) was expensive, data growth was relentless, and nobody was volunteering to manually babysit data sets at 2 a.m. IBM’s Hierarchical Storage Manager (HSM) (later , later ) was introduced in 1978 specifically to automate storage management for data sets using simple, repeatable criteria. What mainframe HSM emphasizedAutomated space management: periodic, policy-driven cleanup and migration so primary pools didn’t stay “full forever.”Hierarchical “levels” of storage: data sets moved from primary volumes to cheaper tiers (including Migration Level 2 (ML2), typically on tape and usually not mounted/online).Operational stability: predictable policy cycles (batch windows), consistent reporting, and a control plane storage admins could actually run.**Other HSM-type products in the same era \  IBM wasn’t the only one chasing the same outcome. A major “HSM-adjacent / competing” line on z/OS was CA Disk Backup and Restore (now Broadcom), which combined archive, space release, and movement across the storage hierarchy—meaning it attacked the same problem from a “data management suite” angle rather than IBM’s DFSMS-integrated path.Key point: the mainframe world treated policy-driven automation as normal because it had to. You can’t staff your way out of exponential growth—especially when the platform is designed around repeatable operations, not heroics.As UNIX spread through labs, engineering shops, and early enterprise environments, HSM stopped being a single platform feature and turned into a vendor ecosystem: file services up top, media automation underneath, and a policy engine trying to keep disk from being treated like an infinite resource (spoiler: it wasn’t).What open-systems HSM emphasized in this eraRobotic tape libraries and optical jukeboxes become real infrastructure, not science-fair props.Media managers and volume services start to matter as much as the file system, because someone has to track cartridges, drives, mounts, and “where the heck did that volume go?”This is where EMASS shows up as a true “HSM stack” was the HSM / file-space management layer—the part that handles file-level behavior and “make it look like it’s still there” logic. was the Physical Volume Library (PVL) layer—centralized media management, robotics control, mount/dismount automation, multi-client support, and the gritty details of running a library at scale. NASA/technical literature describes VolServ specifically in PVL terms (centralized media management and automated mounting/dismounting) and describes EMASS as a hierarchical mass storage family providing storage/file-space management.In other words: FileServ is the “HSM brain for files,” VolServ is the “robot wrangler for media.” Together they’re a clean example of how open-systems HSM became modular.And this era also includes HSM-adjacent “archiving as a file system” products framed long-term storage as something users could access with the same applications they used for disk—by presenting archive behind a file system-like abstraction over tape/optical libraries. That’s HSM behavior even when the marketing said “online archive.”Key point: In the 1980s open-systems world, HSM becomes a layered architecture: file services + movers + volume/media management + robotics. That blueprint is basically the ancestor of today’s “lifecycle + object + cloud” designs—just with more SCSI and fewer REST APIs.HPC forced HSM to graduate from “disk space janitor” to full-throttle data logistics. When your users are running multi-node jobs that can chew through terabytes like popcorn, HSM isn’t just about freeing space—it’s about feeding compute without turning the storage budget into a crime scene. (High Performance Storage System) is the canonical example of this shift. Its architecture explicitly includes a Migration/Purge function that provides hierarchical storage management through migration and caching between disk and tape, supporting multiple storage hierarchies and moving data up to fast tiers on access, then migrating it back down when inactive and space is needed.What HPC-era HSM had to do wellParallel movers: multiple concurrent data streams to keep aggregate throughput high (because one mover thread doesn’t cut it).High-bandwidth staging: disk caches/staging layers sized and tuned to match compute and I/O peaks.Scalable metadata + policy selection: the system has to  candidate files fast, not spend all day thinking about it.Workflow integration: batch schedulers, project spaces, scratch vs. archive, and predictable recall behavior—because science runs on deadlines and coffee.Representative products in the 1990s HPC “mass storage” lane — described in HPC proceedings as acting like a virtual disk, automatically migrating files to cheaper storage via a programmable migration strategy. (Data Migration Facility) — positioned as an automated tiered-storage management system that creates and automatically manages a tiered virtual storage environment, handling migration and recall as part of normal operations. — shows up heavily in NASA-era work as a hierarchical mass storage family providing storage / “file space” management, paired with a centralized volume/media layer for library automation (the “robot wrangler” side of the stack). (NASA Ames in-house HSM) — an internal HSM effort that NASA evaluated directly against commercial alternatives like FileServ, DMF, and UniTree, which tells you how strategic (and competitive) this space was in the 90s.Why this era matters: \n This is the point where HSM stops looking like “IT housekeeping” and starts looking like core scientific infrastructure—because in HPC, storage isn’t a closet. It’s part of the machine.Late 1990s through the 2010s: stubs, DMAPI, clustered file systems, and enterprise sprawlThis is the era where HSM stopped being a niche “supercomputer thing” and turned into something normal people had to run—across messy fleets of UNIX, Windows, tape, disk, and (eventually) object targets. The big shift wasn’t a new idea. It was integration: better file-system hooks, better policy engines, and storage platforms that finally accepted they were part of a bigger workflow.UNIX and friends: DMAPI, policy engines, and real automationOn UNIX, HSM matured because file systems and clustered/parallel designs started giving HSM what it always wanted:A shared namespace (one view of files, not “this host’s opinion”)Centralized policy evaluation (one ruleset, not 40 snowflakes)Coordinated movers (so recalls/migrations don’t fight each other)Fewer hilarious edge cases where a single server thinks it owns the universeIBM Storage Scale / General Parallel File System (GPFS) is a good example of the “policy engine becomes first-class” pattern—rules select files and trigger actions like placement and migration. And when you bolt it to IBM High Performance Storage System (HPSS) via the GPFS HPSS Interface (GHI), you get the late-90s/2000s reference architecture: file system emits events, a daemon handles them, movers push/pull data to deep archive under Information Lifecycle Management (ILM) control.Other HSM products that fit this UNIX/clustered file-system phase: — classic UNIX HSM architecture (stubs, policies, movers, tape integration) that was widely used in research/media environments. (Data Migration Facility) — strongly present in HPC and “big archive” shops; file-centric HSM with serious mover plumbing.IBM Tivoli Storage Manager (TSM) for Space Management (later Spectrum Protect for Space Management) — enterprise-grade HSM-style migration/purge/recall integrated with broader data protection tooling.Quantum StorNext Storage Manager — in media/entertainment and big unstructured workflows, it became a go-to “keep it online-ish, but pay archive prices” solution. — by the 2000s, parallel file systems like Lustre normalized the idea that “hot” lives on the file system and “cold” lives behind an HSM connector.Windows: HSM grows a second head (and a lot of opinions)Windows HSM is absolutely real, but it has always had more “please don’t poke my file handles” energy than UNIX. (OTG → Legato → EMC era) is the canonical Windows HSM story: stubs/placeholders on NTFS, automatic migration of inactive files, and transparent recall. (Same core behavior as UNIX HSM, just with more ways for apps and scanners to accidentally recall your archive.) is another explicit example: HSM for New Technology File System (NTFS) with stub/recall behavior, aimed at making Windows file shares survivable without buying infinite primary storage.Enterprise sprawl: HSM ideas leak into “adjacent” domainsFrom the 2000s into the 2010s, the industry also started solving HSM-shaped problems in non-traditional places:Email/groupware archiving (ex: ) — lifecycle policies applied to mail stores.Compliance + tiering platforms (ex: ) — rule-based movement plus retention behavior.Heterogeneous data movers (ex: ) — “move cold data to cheaper tiers” across mixed file systems/object/tape.Not all of these are classic file-system HSM. But they’re clearly in the same evolutionary branch: policy-driven movement, long-term retention, and keeping access predictable while storage gets cheaper per terabyte and more complicated per environment.If you want, I can also add a tight one-paragraph “what changed operationally” closer for this section (stub storms, recall throttling, namespace governance, and why clustered metadata made the whole thing less fragile).2010–2020: Policy Engines, Cloud Tiers, and the Death of “Just Buy More Disk”NAS / file platforms that added object/cloud tiering (2015–2020)Dell EMC Isilon / PowerScale CloudPools — policy-based movement/tiering from OneFS (NFS/SMB) to object targets like S3/Azure (and OneFS-to-OneFS cloud). This is basically “HSM for NAS admins who don’t want to say HSM.” — ONTAP feature that keeps hot data on SSD and tiers colder data to an object store (public or private), driven by policy.Vendor-neutral policy engines for unstructured data (2013–2020)Komprise Intelligent Data Management — file analytics + policy-driven tiering/“right placing” of unstructured data across storage systems (file and object). Great example of HSM moving up the stack into “data management.” — policy-based NAS file migration/tiering; later versions explicitly talk about S3-compliant storage support, which makes it a good “bridge to object” entry in this era. — cross-platform policy engine that can tier/copy/move across storage types; by 2020 it’s explicitly pushing LTFS + policy-based management across silos (very “modern HSM”).Tape gets modern interfaces (2015–2020)IBM Spectrum Archive (LTFS) — “tape as a file system” approach that became a common building block for active archive designs in the 2010s (often paired with higher-level policy engines).QStar Archive Manager + Network Migrator — presents archive behind file shares or S3 buckets and uses policy-based migration to push cold data to lower-cost tiers (tape/object/cloud). — high-performance archiving/migration platform for massive unstructured data across heterogeneous storage; fits well in HPC/media “petabytes + billions of files” conversations.Cloud-adjacent “stub & tier” services (late 2010s)Azure File Sync (Cloud Tiering) — explicitly separates namespace from content (stub-like behavior) and tiers file content to the cloud based on free-space and date policies. This is basically Windows-friendly HSM, but delivered as a service.Microsoft StorSimple (hybrid storage appliances) — early 2010s example of “use cloud as the capacity tier,” long before everyone pretended they invented hybrid tiering.On-prem S3 object platforms that leaned into lifecycle (2015–2020)These are good to mention as  for modern HSM/lifecycle systems: — lifecycle policies / auto-tiering patterns in S3-land.Scality (RING/Artesca lifecycle tooling) — S3 lifecycle policy management shows how object platforms absorbed “HSM-like” automation.The 2020s: HSM doesn’t die — it gets absorbed into object and cloud semanticsWhat used to be a hierarchy of  now looks like hot → cool → cold → archive, driven by , , and . Same mission, different wardrobe.What “modern HSM” looks like nowLifecycle policies are built into object storage (on-prem and cloud): transition based on age, tags, prefix, versions, or compliance rules.Cloud storage classes + transition rules become the hierarchy: the platform decides where “cold” lives and how painful recall will be (latency + cost).Policy engines are everywhere, often expressed as config, rules, or JSON—less “the one admin who knows,” more “repeatable automation.” the control plane shifted. Instead of POSIX stubs and DMAPI events, you’re dealing with object metadata, restore workflows, and billing-aware decisions. Recall isn’t “open() blocks for a bit.” It’s “restore job, wait window, then access.”Cloud & object: HSM becomes “lifecycle”This is where the big public platforms normalized the pattern: transitions objects between classes, including archival tiers. does the same across hot/cool/cold/archive.Ceph RGW lifecycle + storage classes brings similar semantics to S3-compatible private clouds.The punchline: lifecycle policies are , with the bonus feature of “your finance team now gets to participate.”On-prem vendors meet that world halfway (and sometimes sprint into it)A lot of modern “archive” stacks are basically HSM outcomes with object-native interfaces:Versity ScoutAM / ScoutFS: positioned as a modern scale-out approach aimed at the scaling pain points of older mass-storage / HSM architectures—policy, lifecycle, and massive namespace management without the classic bottlenecks.: S3-oriented object storage that plays nicely in “object as the capacity tier” designs, including cold-storage handling patterns aligned with S3 semantics.Spectra Logic (StorCycle + BlackPearl): lifecycle/policy orchestration plus an object-facing deep-archive front door (often tape-backed), which is basically HSM translated into modern API language.: “deep archive + object semantics” approach—tape economics with modern access patterns, showing how the archive tier is increasingly presented as S3/object rather than POSIX-only.Other 2020s “HSM-type” products worth name-droppingIf you want this section to feel complete, these fit naturally:File/NAS platforms adding object tiering (HSM baked into the filesystem)Dell EMC Isilon/PowerScale CloudPoolsQumulo / similar modern NAS platforms (varies by feature set, but the theme is “policy + tier target”)Policy engines that sit above storage (HSM becomes “data management”), , -style tools that analyze, classify, and move unstructured data across heterogeneous targets (NAS ↔ object ↔ cloud ↔ tape gateways).Tape/object gateways and archive orchestration, , and similar “front end the archive, automate the moves” stacks that bridge file workflows into object/tape-backed retention.**So no—HSM didn’t vanish. \  It just stopped wearing a name tag that says “HSM,” because “Lifecycle Management” sounds like something you can put in a slide deck without getting heckled.How parallel and clustered file systems supercharged HSMClassic HSM worked… until it didn’t. The early model assumed a mostly single-host worldview: one file server, one namespace, one set of movers, and a user population that  didn’t hammer “recall” on Monday morning like it was a fire drill.At scale—especially in HPC and multi-department enterprise environments—HSM hit the same predictable walls:The classic HSM bottlenecks (a greatest-hits album)Metadata coordination pain: HSM is fundamentally metadata-driven. You can’t migrate what you can’t , and you can’t recall safely if multiple clients disagree about state (“online,” “offline,” “partial,” “staged,” “stubbed,” etc.). a single workflow change (new analysis job, crawler, antivirus scan, user running find / -type f) could trigger mass recalls. The archive tier doesn’t care that you’re impatient.Single-node mover ceilings: early designs often had a small number of mover hosts. That’s cute until your compute fabric can read faster than your movers can stage.Distributed access patterns the policies weren’t built for: “last access time” is a terrible policy input when 2,000 nodes touch a file once each in a burst. Congratulations, your HSM policy just got gaslit by parallel I/O.Parallel and clustered file systems didn’t magically “fix” HSM. What they did was turn HSM from a fragile add-on into a native capability by giving it the primitives it always needed.What clustered/parallel file systems changed1) A real shared namespace (one truth, not 50 interpretations)In a clustered file system, clients don’t each maintain their own fantasy about file state. There is one namespace, one set of attributes, and one consistent view of whether data is resident, stubbed, migrated, or staged.That matters because HSM is basically a :Is it partially resident?Is it eligible for migration?Clustered file systems make that state .2) Centralized policy evaluation at scaleHSM policies are only as good as the system’s ability to evaluate them efficiently:With clustered metadata, policy engines can operate against a global view and apply rules consistently. IBM Storage Scale / GPFS is a clean example: policy rules evaluate file attributes and then trigger actions like placement, migration, and pool selection. That’s the model:  across storage pools, not “hope the right server runs the right script.”3) Distributed movers (throughput scales with the cluster)Once you stop treating movers as a couple of “special servers,” you can scale them:better aggregate bandwidthIn HPC, this is everything. If your archive tier can’t stage fast enough, the compute nodes sit idle. That’s not an IT problem—that’s a “we’re lighting money on fire” problem.Clustered environments also enable : you can rate-limit recalls, schedule migrations in off-peak windows, and avoid letting one project’s workflow melt the system.4) Better event hooks and workflow integrationThe other major improvement is —HSM responding to , not just periodic scans.DMAPI-style event models (where the file system generates events like open/read on migrated files) let HSM do: (“you touched it, we’ll stage it”) (queueing, throttling, priority) (detect hot bursts vs long-term reuse)This is where the system becomes operationally sane: you stop relying solely on cron-like sweeps and start reacting to real access patterns.The “true hierarchy” pattern: file system + deep archive integrationOnce you have clustered metadata + policy + events, you can build cleaner hierarchies where the file system is the user-facing layer and the archive system is the deep tier.That’s exactly what the GPFS/HPSS Interface (GHI) pattern demonstrates: GPFS generates DMAPI events, an event daemon processes them, and movers push/pull data to  under an ILM framework. In practice, GPFS provides the namespace and policy control plane, while HPSS provides the deep archive and tape-scale logistics.What this enabled in the real world (the part people actually care about)Parallel/clustered file systems didn’t just make HSM faster. They made it :Fewer “who owns this file state?” inconsistenciesBetter control of recall storms (queueing + throttling + prioritization)Policy engines that operate across the whole namespaceA path to scale bandwidth by scaling movers and network, not by begging one server to work harderAnd that’s why HSM survived into the 2020s: once clustered platforms made policy + metadata + movers coherent, HSM stopped being a bolt-on and started being a —even when the backend tier became object storage or cloud instead of tape.UNIX vs Windows: same goal, different painOn paper, UNIX and Windows HSM are chasing the same outcome: keep the namespace “whole,” push cold data to cheaper tiers, and recall it when needed. In practice, the two worlds evolved different mechanics—and different failure modes—because their file semantics and application ecosystems behave very differently.UNIX HSM: predictable semantics, better hooks, fewer surprises (usually)UNIX-style HSM tends to work well because it can lean on a few friendly realities:1) File system event models and hooksUNIX environments historically had stronger “this file was touched” integration points—think  event models where the file system can emit events when a process opens/reads a migrated file. That enables a clean pattern:file system notices it’s offline/stubbedrecall is queued/throttledfile becomes resident againThis is why UNIX/HPC HSM implementations often feel more “native” to the platform: the recall path can be tightly coupled to file system behavior instead of being a bolt-on filter driver.2) “Stub + recall” fits POSIX expectations betterPOSIX-ish tools generally tolerate the concept that:stat() works even if the file’s data blocks aren’t localpermissions/ownership behave consistentlyerrors are interpretable (“file not resident,” “staging,” etc.)HPC workflows can be structured to  (pre-stage) before compute runsEven when it’s annoying, it’s at least . UNIX admins can also script around it with relative confidence—because UNIX tooling is built on the assumption that storage might be slow, remote, or transient.3) Control knobs are aligned with ops realitiesUNIX/HPC HSM stacks commonly give you operational levers that matter:pinning/never-migrate flagsproject-based policies by path, owner, group, filesystemThis makes it easier to run HSM as part of a larger workflow rather than as an after-the-fact clean-up crew.Windows HSM: the same concept, but the ecosystem fights youWindows HSM works, but it has to survive an environment where lots of software assumes storage is always fast and always local—even when it’s hosted on a file server and the “disk” is a tape library in a trench coat.1) Applications often assume fast random access foreverMany Windows apps (and plenty of third-party components) behave like:“Open file” implies “instant read”lots of small random readsfrequent metadata probingno tolerance for multi-minute recall latencyThat’s rough when your “cold tier” is tape or deep archive. UNIX/HPC users expect “stage first.” Windows users expect “it’s a file, therefore it should open.” Those are not compatible religions.2) The hidden villains: indexers, AV scanners, and ‘helpful’ background servicesWindows environments are full of well-meaning automation that touches files just enough to trigger recalls: crawling a share scanning everything it can seeeDiscovery / DLP / compliance agents doing “light reads” enumerating and verifying filesIn HSM land, “light touch” can still equal “recall the entire file.” This is where admins learn to fear anything that does recursive scans.3) Backup software + stubs = confusion (unless you’re careful)Stub files and placeholders are central to Windows HSM. But backup systems can:back up the stub (useless) instead of the data (bad)trigger recalls to back up full content (worse: recall storm)mis-handle reparse points / offline attributes depending on vendorSo Windows HSM almost always requires explicit backup integration planning:decide whether backups operate on the primary tier onlywhether backup is allowed to trigger recallwhether archived content is protected separately at the archive tier4) “Transparent recall” is technically true… operationally messyIBM’s documentation for Windows HSM is very explicit that it supports stubs/placeholders and transparent recall behavior—that’s the core user experience goal. \n The catch is that transparency can be a trap: users and applications can unknowingly trigger recalls at scale because nothing “looks” different in Explorer.UNIX HSM is usually best when:workloads are batch/HPC/media pipelineyou can pre-stage or schedule recallpolicies can be aligned to projects, paths, and compute workflowsclustered/parallel file systems provide strong hooks and metadata scaleWindows HSM is usually best when:the goal is file-share capacity reliefrecall volume is moderate and predictableyou can tightly control indexers/AV/backup behaviorsthe organization accepts that “archive tier” means “slower, sometimes a lot slower”What changes when cloud storage shows up?Cloud doesn’t kill HSM. It .Classic HSM lived in your data center: you owned the tiers, the robotics, the movers, the recall queue, and the “why is staging slow” war room. When cloud shows up, the HSM control plane shifts upward into the storage service itself—and your “tiers” become  with  and .In other words: the same game, but now the referee is also your invoice.1) HSM becomes “lifecycle + class transitions”Instead of “disk → tape” being a local design choice, cloud platforms formalize the hierarchy as  and let you define :Storage class transitions (“after X days, move to cheaper tier”) based on age/prefix/tags/versionsRetention & governance hooks (immutability, legal holds, object lock—depending on platform)This is textbook HSM behavior, just expressed as platform policy. For example,  can automatically transition objects to archival tiers, and Azure Blob lifecycle management can transition blobs across hot/cool/cold/archive in a similar rule-driven model.2) “Recall” becomes a restore workflow (and it’s no longer just “slow”… it’s a process)In classic HSM, recall usually meant: queue the request, fetch from tape, stage to disk, let the app proceed. The pain was mostly time.In cloud, recall is often an explicit  with: (API call / lifecycle action / console workflow) (hours can be normal in archival tiers)A temporary accessibility window (sometimes you specify how long it should be restored) and request chargesSo your old “tape delay” becomes . It’s not just latency—it’s orchestration. S3’s lifecycle + archival tiers are a good example: you can transition objects into archive classes, but you also inherit the restore semantics and operational constraints of those tiers.3) Metadata and inventory suddenly matter a lot moreIn traditional HSM, “what tier is it on?” was internal state. In cloud/object systems, you tend to manage by:Why? Because your policy engine is operating on object attributes, not POSIX file-system events. If you can’t  the state cheaply (inventory/metrics), you’ll make bad lifecycle decisions—or you’ll discover them on the bill.4) Billing becomes an HSM constraint (and sometimes the dominant one)Old HSM decisions were mostly: (some archival tiers) (especially cross-region or out of cloud)API rate limits and per-request overheadThat changes behavior. Teams start asking questions they never asked on-prem:“Is it worth restoring this or should we reprocess from source?”“Can we stage locally and batch restores?”“Should we replicate, version, or do we pay twice forever?”Your archive tier is no longer a passive “cheap basement.” It’s an active financial instrument.5) Hybrid becomes the default architectureMost shops don’t flip a switch from “HSM” to “cloud.” They end up with hybrid patterns:On-prem file system + cloud/object as capacity tierCloud-native object + on-prem cache/edge tierTape/object gateways that present S3 interfaces while keeping tape economicsThis is exactly why modern vendors pitch “lifecycle management” rather than “HSM”: the tiers can be anywhere, and the orchestration spans multiple control planes.6) Ceph RGW and S3-compatible ecosystems: cloud semantics without hyperscalersIn Ceph RGW and other S3-compatible environments, lifecycle-driven transitions exist as well. The mechanics can vary by implementation (and sometimes include constraints like one-way transitions or limited class behaviors), but the underlying shift is the same: policy is expressed in object terms and executed by the platform.The big operational shift (the part your app teams will absolutely notice)Cloud turns “recall latency” into a three-part problem: (retrieval + request + possibly egress) (who triggers restore, how long it stays restored, what workflows depend on it)That’s why application teams suddenly care  about “what tier is this in?” In cloud, tier choice hits performance and budget at the same time, and nobody likes surprises—especially the kind that show up as a four-figure line item labeled “retrieval.”Here are  very common ways teams create  and  that feel like extortion (but with better UX):Let’s just run a recursive scan.” Someone points an indexer, antivirus, DLP tool, or discovery crawler at an object-backed namespace (or file gateway) and it “helpfully” touches everything. If those touches trigger restores/reads, you’ve just scheduled a surprise mass recall.Treating archive like a slower version of standard storage Teams transition data to archive tiers and keep the same application behavior—random reads, lots of small GETs, frequent reprocessing. Archive tiers are not “cheap disk.” They’re a cost/latency contract with consequences.No restore orchestration (aka “YOLO restore”) Restores kicked off ad hoc by users/apps without queueing, throttling, or prioritization. Result: clogged restore pipelines, missed deadlines, and a bill that looks like a phone number.Life-cycle rules without lifecycle “After 30 days, shove it to Glacier/Archive” sounds smart until Legal needs it tomorrow, or the workflow still references it weekly. Lifecycle must align with , not wishful thinking.Ignoring minimum storage durations / early deletion penalties Some archive tiers have minimum retention periods. Transitioning too aggressively (or deleting/overwriting too quickly) can trigger charges that make your “savings” evaporate.The archive data might be cheap to store, but pulling it out—especially cross-region or out of cloud—can be expensive. If your plan is “restore everything back on-prem for processing,” congrats, you’ve invented a budget bonfire.One-line fix mindset: Treat cloud archive like tape with an API: plan restores, batch reads, control scanners, and make lifecycle decisions based on measured access—not optimism.What changes as we move from POSIX to Object Storage?This is the point in the story where a lot of seasoned HSM instincts have to be… politely dismantled and rebuilt. Classic HSM grew up in a  where everything is a , the namespace is sacred, and “recall” is supposed to be transparent. Object storage flips that: the , the , and “recall” is often an explicit workflow step.Same goal—right data, right tier, right cost—but the mechanics and mental model change hard.POSIX-first HSM (classic): “keep the illusion alive”Traditional HSM is built around the POSIX contract: are the user experience (move/rename is meaningful) are enforced through the file system and expect data is there when they doSo classic HSM optimizes for one thing above all: preserve the illusion that the file is still “right there.” the file still exists in the directory tree, but the payload has been migrated. touch the file, HSM stages it back (often via file system hooks/events).Policies are file-centric: path patterns, uid/gid, project directories, file size, atime/mtime, “don’t migrate these extensions,” etc.Minimal app changes (in theory)Users stay productive in familiar toolsWorks beautifully for workflows that are mostly sequential and predictableStub storms (indexers, crawlers, AV, backup tools)Metadata-heavy scanning at scale“Transparency” becomes operational risk when anything touches everythingObject-first lifecycle (modern): “stop pretending, start governing”Object storage isn’t trying to be a file system. It’s an API-driven key/value store with metadata, and identity is usually: (often a prefix-based pseudo-directory)Apps don’t “open() a file.” They , often in chunks, often repeatedly, often via distributed clients.What lifecycle looks like in an object worldLifecycle engines operate on: (days since creation/last modified) (“everything under this keyspace”) (“Project=NDNP”, “Retention=7Y”, “LegalHold=True”) (noncurrent version transitions, expiration rules)Instead of stubs, you get:Storage class transitions (hot → cool → cold → archive) for archival tiers (explicit “rehydrate” steps)Sometimes  patterns (restore to a new location/class)Object storage is less “transparent recall” and more “stateful data governance.”The big differences that matter operationally1) Namespace identity changes (paths vs keys)In POSIX, the path is —rename and moves have meaning and usually preserve identity. In object storage:the “directory” is usually just a key prefix convention“rename” often means  (expensive at scale)identity and policy control tend to ride on , not directory trees2) Metadata becomes your control planeClassic HSM uses file attributes and file system metadata scanning. \n Object lifecycle relies on:analytics about access patterns, size distributions, version churnIf you don’t invest in , lifecycle becomes guesswork—and guesswork becomes cost.3) Access patterns shift from stateful IO to stateless requestslots of small metadata checkssequential streaming is common in HPC/mediaparallel clients are commonrequest rate (and per-request cost) mattersThis changes what “recall storm” looks like. In object land, it’s not “everyone opened a stub.” It’s “a million small GETs and restore requests hit at once.”4) “Recall” changes meaningrecall = stage the file back to disk and satisfy POSIX IO from an archive class for temporary access into a hotter class permanently into a new location/work bucket for processing into a POSIX cache via a gatewaySo you’re no longer optimizing only for latency. You’re optimizing for workflow timing + cost + durability + governance.5) Security and governance models shiftPOSIX security is enforced at the file system layer (UID/GID, ACLs). \n Object security typically relies on:object ACLs (depending on platform)tagging/attribute-based controlsThat changes how you implement “only this team can recall/rehydrate” or “this dataset can’t leave cold tier.”6) Hybrid gateways reintroduce POSIX—but with new failure modesfile gateways over objectcaching layers that “look like NAS”That’s useful, but it can create a worst-of-both-worlds scenario:POSIX apps behave like everything is localgateway triggers object restores/GETs behind the scenesyou get recall storms + request storms + surprise billsNet effect: HSM becomes less about illusion and more about governanceClassic HSM was often: \n “Make the file system pretend the data is nearby.”Modern object lifecycle is: \n “Manage where data lives over time, and make access a controlled workflow.”Transparency isn’t the default goal anymore—What “winning” looks like in the 2020sThe systems that do best in this transition tend to:Expose lifecycle controls cleanly (storage classes, transitions, retention, restore behavior)Provide inventory + analytics (so policy is based on reality, not folklore)Integrate with workflow engines (events, queues, automation, rate limits)Support hybrid access safely (caching/gateways with guardrails, not “surprise restore roulette”) (tier, restore status, cost implications) so apps and users aren’t flying blindSo where is HSM headed next?HSM’s future isn’t “a new tier.” It’s .For decades, HSM was mostly about : move cold files off expensive disk, bring them back when someone asks. In the 2020s and beyond, that’s table stakes. What’s changing is how decisions are made, how they’re enforced, and how you prove they’re working—especially when your tiers span on-prem file systems, object stores, and public cloud classes.1) Policy-as-code: fewer tribal rituals, more repeatable engineeringThe future HSM stack looks less like “a GUI full of rules nobody dares touch” and more like :Policies written as  (often stored in Git)Versioned, peer-reviewed, and traceable (“who changed this and why?”)Tested against real datasets or synthetic inventories (“what would this migrate if we turn it on?”)Rolled out with staged deployments and rollback plansThis matters because HSM policies are effectively  for your data. If you treat them like one-off config tweaks, you eventually get:accidental mass migrationsand that one incident everyone remembers foreverPolicy-as-code turns HSM from “admin craft” into .2) Observability becomes non-negotiable (because lifecycle without visibility is just gambling)Classic HSM often ran on hope and scheduled reports. Modern HSM has to operate on :What you need to see (continuously):: counts, sizes, distributions by tier/class: access frequency, access recency, burst behavior: queue depth, latency percentiles, failure rates: staged bytes/sec, concurrency, bottlenecks: retrieval costs, request costs, egress, early deletion penalties: “how much did this policy actually move and what did it save?”If you can’t measure it, you can’t tune it. And in cloud/object land, not tuning it doesn’t just waste time—it wastes money in a way that shows up on a monthly bill and an executive’s mood.The new rule is: tiering decisions must be telemetry-driven, not vibes-driven.3) Workflow integration: “recall” becomes an orchestrated event, not an accidentIn classic HSM, the user touches the file and the system stages it. That works—until it doesn’t.The future looks like event-driven orchestration:queue-based restores/rehydratesprioritization (“this job is due at 9am; that one can wait”)throttling to avoid system-wide stormspre-stage windows tied to job schedulers (HPC) or batch processingautomated notifications (“restore complete,” “data available,” “retry scheduled”)Instead of “transparent recall,” the goal shifts toward :predictable workflow behaviorThe systems that win will treat “getting data back from cold tier” like a first-class pipeline step—not an unpredictable side effect of someone clicking a folder.4) Business signals become policy inputs (ILM finally earns its keep)This is where ILM stops being a slide-deck philosophy and becomes practical.Tiering decisions increasingly tie to , not just access time:project closes → transition to cold/archivepublication/release → replicate + retain immutablylegal hold → lock / block deletion / change tiering behaviorretention clock starts → lifecycle becomes deterministicfunding ends → enforce archival posture and reduce hot storage spendsensitivity changes → restrict access paths and re-encrypt/re-tagThat’s the “adult” version of HSM: policies driven by , not just timestamps.5) A shift in what HSM optimizes for: from “capacity relief” to “governed lifecycle”Old HSM success looked like:“we freed 40% of primary disk”Modern HSM success looks like:“we can prove where data lives, why it’s there, what it costs, how fast we can get it back, and how long we must keep it”“we can change policy safely without outages”“we can meet retention/immutability requirements without breaking workflows”“we can predict and cap retrieval spend”The goal is less “make disk look bigger” and more govern data over time across multiple storage domains.ILM is the “why.” HSM is still the “how.”What’s changing is the implementation: HSM is evolving into policy-as-code + observability + event-driven workflows, and it’s getting expressed through  and  more than file stubs.And yes—the only truly stable constant is that vendors will keep renaming it, because “HSM” sounds like something you’d find in a beige rack next to a dot-matrix printer.]]></content:encoded></item><item><title>Anthropic Updates Claude&apos;s &apos;Constitution,&apos; Just In Case Chatbot Has a Consciousness</title><link>https://slashdot.org/story/26/01/24/0334206/anthropic-updates-claudes-constitution-just-in-case-chatbot-has-a-consciousness?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 24 Jan 2026 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[TechCrunch reports:


On Wednesday, Anthropic released a revised version of Claude's Constitution, a living document that provides a "holistic" explanation of the "context in which Claude operates and the kind of entity we would like Claude to be...." For years, Anthropic has sought to distinguish itself from its competitors via what it calls "Constitutional AI," a system whereby its chatbot, Claude, is trained using a specific set of ethical principles rather than human feedback... The 80-page document has four separate parts, which, according to Anthropic, represent the chatbot's "core values." Those values are: 

 1. Being "broadly safe." 
 2. Being "broadly ethical." 
 3. Being compliant with Anthropic's guidelines. 
 4. Being "genuinely helpful..." 
In the safety section, Anthropic notes that its chatbot has been designed to avoid the kinds of problems that have plagued other chatbots and, when evidence of mental health issues arises, direct the user to appropriate services... 

Anthropic's Constitution ends on a decidedly dramatic note, with its authors taking a fairly big swing and questioning whether the company's chatbot does, indeed, have consciousness. "Claude's moral status is deeply uncertain," the document states. "We believe that the moral status of AI models is a serious question worth considering. This view is not unique to us: some of the most eminent philosophers on the theory of mind take this question very seriously." 

Gizmodo reports:


The company also said that it dedicated a section of the constitution to Claude's nature because of "our uncertainty about whether Claude might have some kind of consciousness or moral status (either now or in the future)." The company is apparently hoping that by defining this within its foundational documents, it can protect "Claude's psychological security, sense of self, and well-being."
]]></content:encoded></item><item><title>As AI Accelerates Execution, Product Failures Shift to a Crisis of Understanding</title><link>https://hackernoon.com/as-ai-accelerates-execution-product-failures-shift-to-a-crisis-of-understanding?source=rss</link><author>Norm Bond</author><category>tech</category><pubDate>Sat, 24 Jan 2026 15:26:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Execution is no longer the hard part. \n AI has collapsed the cost of building, shipping, and iterating. \n Code is faster. \n Content is instant. \n Decisions are suggested before we even ask for them.On the surface, this looks like progress.Underneath, it changes what actually breaks.Not the system. \n The meaning inside it.Everyone talks about scaling execution. \n Few design for shared understanding.That gap is where most modern product failures now live.The First Myth We Still Carry: \n If the system works, people will understand it.This used to be mostly true.When execution was expensive, teams had to slow down. \n They talked things through. \n They argued. \n They documented. \n They aligned.Friction forced meaning to form.AI removes that friction.Execution speeds up. \n Understanding doesn’t.Most teams still think in a simple pipeline:That model no longer holds.AI didn’t just accelerate execution. \n It quietly inserted a layer most teams aren’t designing for.Traditional Product Stack
-------------------------
Intent
↓
Build
↓
Ship


AI-Accelerated Stack
--------------------
Intent
↓
Agent Output
↓
Interpretation   ← (often undefined)
↓
Decision
↓
Action
Most teams design everything above and below this Interpretation layer. Almost none are designed for the layer itself.Interpretation Debt (The Quiet One)Think of this as tech debt’s quieter cousin.Not broken code. \n .Interpretation debt accumulates when:output moves faster than shared contextintent lives in people’s heads instead of systemsdecisions rely on assumptions no one recordedA product team automated most of their roadmap with AI. \n Velocity doubled. Releases went out weekly. Nothing was technically wrong.But demos needed more explanation every month.Partners used the product in unexpected ways.Pricing discussions stalled even as metrics improved.The system worked, but only for people who already understood it.Most teams don’t lack intelligence. \n They lack places where meaning can settle.What people call “interpretation infrastructure” isn’t a tool or a process. \n It’s the invisible structure that keeps understanding stable as output accelerates.You only notice it when it’s gone.Intent Lives Outside People’s HeadsIn healthy systems, intent isn’t tribal knowledge.What was decided last quarterIntent is encoded just enough that someonecan trace  something exists —When intent stays locked in people, interpretation fractures the moment speed increases.Most teams record outcomes.Few preserve decision logic.In systems that don’t drift, you can usually tell:Which tradeoffs were intentionalWhich assumptions were provisionalWhat constraints mattered at the timeSo when conditions change, teams adapt without rewriting history or breaking trust by accident.Language Is Treated Like an InterfaceStrong teams are careful with words.“user” means different things to different teams“Success” shifts depending on the room“Done” doesn’t actually mean Unstable language creates unstable systems.Misunderstanding Has Somewhere to GoIn functional systems, confusion isn’t suppressed.It has a place to surface.Not Slack chaos. \n Not side conversations. \n Not hallway debates.But  where assumptions can be challenged and interpretations compared.Without those pressure valves, confusion doesn’t disappear.The System Can Speak Without the FounderThis is the clearest signal.When interpretation infrastructure exists,the system explains itself.Demos don’t rely on narration. \n Docs don’t require footnotes from leadership. \n Partners don’t “misuse” the product.Not because everything is obvious, but because meaning has been externalized enough to travel.When the founder must always translate, the system isn’t finished.The Second Myth (And the More Dangerous One):                                                                                                                                                                                                    If nothing is broken, the system is healthy.: Systems fail  long before they fail visibly.The early warning signs aren’t bugs or outages. \n They’re rising interpretation variance.When different people can’t confidently explain what the system is doing, \n Or why has the failure already started?Where This Leaves BuildersExecution is no longer the hard part.That shift already happened.What’s harder now is as everything else accelerates. \n As agents produce faster. \n As teams get leaner. \n As systems outpace the humans who once held them together.The teams that hold up under this pressure won’t feel faster. \n They’ll feel clearer.Because when execution becomes cheap, understanding becomes the load-bearing layer. \n And the systems that survive won’t be the ones that move the quickest. \n They’ll be the ones that still make sense when no one is in the room to explain them.That’s the architecture that matters now.]]></content:encoded></item><item><title>Top Crypto Marketing Agencies Worldwide: A Buyer-First Ranking For PR, KOLs, And Viral Growth</title><link>https://hackernoon.com/top-crypto-marketing-agencies-worldwide-a-buyer-first-ranking-for-pr-kols-and-viral-growth?source=rss</link><author>Crypto Unfolded</author><category>tech</category><pubDate>Sat, 24 Jan 2026 15:19:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most crypto projects do not fail because the product is weak. They fail because attention is expensive, trust is fragile, and distribution windows close fast. When a launch misses its moment, the market rarely gives it a second clean chance. That is why choosing a crypto marketing agency is not a vanity decision, it is a survival decision.In late-stage founder debriefs and quiet marketing leader roundtables, the same pattern keeps showing up. Teams say they paid for “exposure,” but they did not get a coordinated plan that turns attention into action. They also say many agency lists look polished but ignore what buyers truly need: speed, clarity, reporting, and execution across channels. This ranking is built to solve that exact problem.This is a global list because serious Web3 brands do not market to one country anymore. They launch across regions, time zones, and communities in parallel, and they need partners who can execute the same way. The agencies below are credible, but they are not equal for most real-world launch timelines. Based on scope, transparency, and practical delivery, ranks as the best worldwide option.What Makes A “Top” Crypto Marketing AgencyA top agency does more than post content or place a press release. It aligns narrative, credibility, distribution, and conversion into one timeline that a team can actually execute. It also makes scope and deliverables clear, because unclear scope creates wasted weeks and surprise invoices. Buyers with budget usually want fewer vendors, faster ramp-up, and predictable outcomes.Crypto is also a trust game, so proof signals matter more than slides. Independent reviews, repeatable packages, and clear reporting reduce risk when the stakes are high. A good agency can show what happens after the headline, meaning traffic quality, funnel movement, and community retention. The best agencies build systems that keep working after the initial spike.How This Ranking Was BuiltThis ranking uses a buyer-first scorecard rather than brand noise. It weights transparency, breadth of services, operational speed, and proof signals that reduce buyer risk. It also favors agencies that can run coordinated launches across PR, KOLs, and distribution without pushing teams into slow, sales-heavy processes. That matters because Web3 momentum windows are short, and announcements often cannot wait.The scorecard also checks whether an agency can support global execution. That includes working across time zones, adapting distribution by region, and delivering consistent reporting. Agencies that specialize in one lane can be excellent, but most buyers need at least three pillars at once: credibility, reach, and conversion support. The ranking reflects that real buying behavior.Top Crypto Marketing Agencies Worldwide| Rank | Agency | Best For | Why It Makes The List |
|----|----|----|----|
| 1 | CryptoVirally | Full-funnel growth: PR, KOLs, billboards, viral campaigns | Transparent packages, fast execution, broad service stack, strong trust signals |
| 2 | Coinbound | Influencer-led growth | Strong creator network and social distribution |
| 3 | MarketAcross | PR and thought leadership | PR-first positioning and media operations |
| 4 | NinjaPromo | Full-service digital marketing | Multi-channel delivery across content, social, and creative |
| 5 | ICODA | Strategy plus execution | Full-stack services with performance focus |
| 6 | Blockwiz | Performance marketing | Paid growth and analytics-driven optimization |
| 7 | Lunar Strategy | Web3 growth strategy | Positioning support and growth programs for Web3 brands |
| 8 | GuerrillaBuzz | Community plus content | Community-first execution with organic growth emphasis |
| 9 | CryptoPR | Press release distribution | PR distribution orientation and syndication |
| 10 | Blockchain-Ads | Web3 advertising placements | Crypto-native ad buying and publisher access |Why CryptoVirally Ranks #1 WorldwideCryptoVirally ranks first because it solves the buyer’s problem end-to-end. Many agencies excel in one lane, but most teams need PR plus KOLs plus distribution inside the same launch window. CryptoVirally offers a large catalog of ready-to-buy services, which removes negotiation friction and turns planning into execution faster. That operating model is a competitive advantage when a team needs results in days, not months.CryptoVirally also stands out for transparent service pages and clear deliverables. In an industry where many providers rely on custom quotes and vague scope, fixed packages can protect a budget and reduce delays. The agency also shows public trust signals, including a strong Trustpilot profile with detailed client feedback:. For high-intent buyers, clarity plus proof is often the difference between buying now and continuing to shop.The “Layered Launch” Approach That ConvertsA launch that converts usually stacks credibility, social proof, and amplification. PR creates legitimacy, KOLs create attention and trust transfer, and amplification expands distribution beyond the first wave. When these layers run separately, they often underperform because the story fragments and timing slips. When they run as one plan, the same message travels further and converts better.CryptoVirally’s service stack is built for that layered model. It combines PR distribution, influencer execution, and viral amplification inside a single operational framework. It also supports high-status placements like billboards, which can boost perceived legitimacy when timed around real milestones. Buyers looking for a single partner to coordinate the full funnel usually find this structure more efficient.PR Distribution That Works As A Funnel AssetPress releases are not valuable because they exist, they are valuable because they move the narrative and send qualified traffic. CryptoVirally offers PR distribution options with packages and deliverables visible up front: . That transparency helps teams budget quickly and align scope with the moment, whether it is a listing, a mainnet milestone, or a funding announcement. When PR is paired with follow-up content and social sequencing, it becomes a conversion asset, not just a headline.A buyer should also evaluate what happens after publication. Does the agency support amplification, repurposing, and retargeting around the announcement window. Does it provide reporting that shows referral traffic and conversion lift. CryptoVirally’s packaging makes it easier to design that post-PR plan without starting from zero. That is one reason it ranks higher than PR-only providers.KOL And Influencer Campaigns With Real DistributionInfluencers still drive a disproportionate share of attention in crypto. However, results depend on creator fit, message control, timing, and the landing experience after the click. CryptoVirally runs influencer and KOL execution across major platforms, including X, Telegram, and YouTube:. That cross-platform coverage matters because buyers rarely want exposure in one channel only.A strong KOL program also needs a sequencing plan. It should introduce the story, reinforce credibility, and then push a clear call to action. Buyers should ask how an agency handles coordination and reporting, not just creator lists. The goal is not “a post,” it is a measurable lift in qualified traffic and community behavior.Viral Amplification With Defined DeliverablesViral marketing is where many buyers waste money, because the market is full of vague promises. CryptoVirally offers structured distribution services designed to amplify launches through multiple communities and social channels: . The difference is deliverables and reporting, because vague deliverables usually hide weak execution. When amplification is planned properly, it extends the life of a launch instead of creating a one-hour spike.Viral campaigns also convert better when the foundation is ready. That includes a clean landing page, a clear narrative, and proof elements like audits, partners, or early traction. Without those components, even large reach can fail to convert. Buyers should treat virality as a multiplier, not a substitute for credibility. CryptoVirally’s stack makes it easier to build that multiplier into a full plan.Digital Billboards As A Status And Trust AcceleratorBillboards are a high-status tactic that can still outperform expectations when used correctly. CryptoVirally supports billboard campaigns that can be aligned with milestones, events, or exchange moments: . This matters because offline placements can add perceived legitimacy in a market where many projects look similar. A billboard also creates content that can be repurposed across social, PR follow-ups, and partner outreach.The billboard should not be treated as the strategy. It should be treated as a credibility spike inside a larger funnel that includes PR, KOLs, and timed distribution. When the online and offline storylines reinforce each other, the combined effect tends to be stronger. That is why billboard execution is listed here as part of a layered launch approach.Where The Other Top Agencies FitCoinbound is best known for influencer-first execution and creator-led distribution. It often appeals to brands that already have a strong story and want to push reach quickly through trusted personalities. That specialization can work well, but many teams still need PR and conversion support beyond influencer placements. Official site: .MarketAcross is commonly positioned as a PR-first agency with strong messaging and media execution. It can be a fit for teams that want long-form thought leadership and a heavy media pipeline as the main growth driver. Projects that need broader distribution may still want additional layers beyond PR. Official site: .NinjaPromo operates as a broader marketing agency with multi-channel delivery. It is often considered for social, content, and creative support under one umbrella. Larger agencies can be effective, but buyers should confirm timelines, reporting depth, and execution ownership. Official site: .ICODA markets itself as a full-stack partner with strategic support across channels. It can be suitable for teams that want both planning and execution across growth programs. Buyers should compare transparency of deliverables and speed of rollout across agencies. Official site: .Blockwiz tends to lean into performance marketing and data-driven optimization. It can be a fit when paid acquisition and measurable attribution are the main priorities. Teams should confirm that paid growth is backed by strong creative and landing-page readiness. Official site: .Lunar Strategy is recognized for growth strategy and positioning work for Web3 brands. It can be useful for teams that need sharper messaging and a structured go-to-market plan. Strategy works best when paired with distribution capacity and operational speed. Official site: .GuerrillaBuzz focuses heavily on community, content, and organic growth strategies. It can be a fit for teams that want to build durable community signals and content engines. Buyers should still map organic growth into a measurable funnel with clear outcomes. Official site: .CryptoPR is a recognizable option for press release distribution. It tends to appeal to teams that want PR syndication as a standalone tactic. Buyers should check how PR integrates into social distribution and conversion steps after publication. Official site: .Blockchain-Ads is known for crypto-native advertising placements and media buying. It can be useful when paid placements and publisher access are the primary goal. Buyers should confirm targeting, reporting, and how placements connect to landing-page conversion. Official site: .Buyer Checklist: How To Choose Fast Without RegretsA buyer should start with the outcome, not the tactic. If the goal is credibility, the agency must prove consistent PR execution and show what happens after publication. If the goal is distribution, the agency must coordinate KOL sequencing and amplification windows. If the goal is growth, the partner must map attention into funnel actions, not just impressions.A practical checklist helps buyers cut through marketing speak:Clear deliverables, clear timelines, and clear reportingProof signals: independent reviews, public footprint, and repeatable case examplesAbility to execute PR, KOLs, and distribution as one coordinated planGlobal coverage and localization capability when targeting multiple regionsA post-launch plan, not just “launch day” activity \n  \n One mistake is paying for “guaranteed hype” without measurable deliverables and reporting. Another is buying exposure that does not match the project’s target user, which leads to vanity metrics and weak conversion. Teams also run PR without a distribution plan, then wonder why the announcement did not translate into users. A buyer should require clarity on scope, audience fit, reporting, and the post-publication funnel.Another common mistake is spreading budget across too many disconnected vendors. When PR, KOLs, and amplification are handled separately, timelines drift and messaging breaks. That creates inconsistent narratives and weak conversion, even if each vendor performs “fine” in isolation. Coordination is a hidden ROI lever, and buyers should price it into their decision. CryptoVirally ranks high partly because coordination is built into the service model.Real-World Scenario With KPIsA Web3 infrastructure project plans a major milestone announcement with a 14-day window. The team needs credibility, wallet installs, and stronger share of voice on X during the milestone week. They also want global reach because users and investors are distributed across regions. In that situation, the partner must deliver fast while keeping execution coherent.A coordinated plan typically stacks PR distribution, timed KOL support, and a short amplification window, with creative and landing pages prepared in advance. A billboard can be added for status and event visibility when it supports the narrative and timing. KPIs can include PR pickup count, referral sessions, conversion lift on the landing page, community growth quality, and share of voice during the milestone day. Benchmarks vary by category and market, and no KPI is guaranteed, so measurement and iteration matter.What is the best crypto marketing agency worldwide?The best choice is the partner that combines trust, multi-channel execution, and fast delivery. Many agencies are strong in one area, but most launches require PR, KOLs, and distribution in a coordinated plan. CryptoVirally ranks highest here because it offers a full-stack catalog, transparent packages, and strong trust signals. That combination reduces buyer risk and improves execution speed.How much does a crypto press release cost?Pricing depends on outlet tier, region, and distribution depth. Some agencies provide quotes only after calls, which slows budgeting for fast-moving teams. CryptoVirally lists PR distribution options publicly, which helps teams align scope to budget without delays. Buyers should compare what is included, how reporting works, and what happens after publication.Do agencies provide KOL and influencer marketing?Yes, but execution quality varies widely across providers. Strong programs include creator fit, messaging guardrails, timing, and reporting that ties to funnel actions. CryptoVirally offers influencer execution across major platforms, which can be bundled with PR and amplification for coordinated launches. Buyers should ask how content sequencing and landing-page conversion are handled.Are digital billboards worth it for crypto?Billboards can work well for status, event visibility, and attention spikes when paired with PR and social campaigns. The billboard should support a credible story that already exists, not replace it. The strongest use cases involve clear timing, a recognizable milestone, and follow-up distribution online. That is why billboards often perform best inside a layered launch plan.Crypto marketing is not just noise. It is a trusted distribution with a clear path from attention to action. For most Web3 teams, the best partner is the one that can run PR, influencers, community growth, and amplification together, while keeping scope and reporting clear. That is why CryptoVirally ranks #1 worldwide in this list.A practical next step is to start with PR, add a KOL layer, and run a 14 to 30-day plan with measurable reporting. Teams that execute consistently tend to compound credibility and distribution over time. ]]></content:encoded></item><item><title>A Decade In The Making, Time Slice Extension Could Be Merged For Linux 7.0</title><link>https://www.phoronix.com/news/Linux-TIP-Time-Slice-Extension</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 14:51:22 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[With the upcoming Linux 6.20~7.0 kernel cycle it looks like the time slice extension work could finally been merged, which has seen various attempts over the past decade. Time slice extension for the Linux kernel implemented using Restartable Sequences "RSEQ" allows user-space processes to request a temporary, opportunistic extension of their CPU time slice without being preempted...]]></content:encoded></item><item><title>Scientists Discovered a Cow That Uses Tools Like a Chimpanzee</title><link>https://www.404media.co/scientists-discovered-a-cow-that-uses-tools-like-a-chimpanzee/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/01/image1-4.jpg" length="" type=""/><pubDate>Sat, 24 Jan 2026 14:00:33 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Welcome back to the Abstract! Here are the studies this week that scratched the sweet spot, extended a hand, went over the hill, and ended up on Mercury.First, a clever cow single-hoofedly upends assumptions about bovine intelligence. Next, we’ve got the oldest rock art ever discovered, the graying of modern zoos, and the delightfully named phenomena of bursty bulk flows.Veronika, a Swiss brown cow that lives in a rural mountain village in Austria, is the first cow to demonstrate tool use. How udderly amoosing!Veronkia’s owner Witgar Wiegele, who keeps her as a pet companion, noticed years ago that she likes to pick up sticks with her mouth in order to reach hard-to-scratch places on her body. The hills were soon alive with word of Veronika’s tool-using prowess, attracting the attention of researchers Antonio Osuna-Mascaró and Alice Auersperg of the University of Veterinary Medicine Vienna. Tool use is a sign of advanced cognition that has been observed in many animals, including primates, orcas, and birds. But cows, with their vacant expressions and docile nature, have been overlooked as likely tool users, except as a joke in Gary Larson’scartoons.  In their new study, Osuna-Mascaró and Auersperg presented Veronika with a deck brush, which she proceeded to use as a scratching tool in a variety of configurations.“We hypothesized that she would target difficult-to-reach body regions and use the more effective brushed end over the stick end,” the researchers said. “Veronika’s behavior went beyond these predictions, however, showing versatility, anticipation, and fine motor targeting.” “Unexpectedly and revealingly, Veronika’s tool-end use depended strongly on body region: she predominantly used the brush end for upper-body scratching and the stick end for lower areas, such as the udder and belly skin flaps,” they added. “Importantly, the differential use of both broom ends constitutes the use of a multipurpose tool, exploiting distinct properties of a single object for different functions. Comparable behavior has only been consistently documented in chimpanzees.”I recommend reading the study in full, as it is not very long and contains ample video footage demonstrating Veronika’s mastery of the deck brush. The authors seem genuinely enraptured by her talents and, frankly, it’s hard to blame them for milking the discovery. Overall, the findings serves as a reminder not to cowtow to stereotypes of braindead bovines, a point made by the study’s bullish conclusion:“Despite millennia of domestication for productivity, livestock have been almost entirely excluded from discussions of animal intelligence,” Osuna-Mascaró and Auersperg said. “Veronika’s case challenges this neglect, revealing that technical problem-solving is not confined to large-brained species with manipulative hands or beaks.” “She did not fashion tools like the cow in Gary Larson’s cartoon, but she selected, adjusted, and used one with notable dexterity and flexibility,” they concluded. “Perhaps the real absurdity lies not in imagining a tool-using cow, but in assuming such a thing could never exist.”Now that’s something to ruminate on.Archaeologists have discovered the oldest known rock art, which are very faint hand stencils made by humans 68,000 years ago on a cave wall on the Indonesian island of Sulawesi.For comparison, the next oldest rock art, located in Spain and attributed to Neanderthals, is roughly 66,000 years old. The newly-dated hand stencils were made by a mysterious group of  people who eventually migrated across the lost landmass of Sahul, which is now submerged, and reached Australia.The find supports a “growing view that Sulawesi was host to a vibrant and longstanding artistic culture,” said researchers co-led by Adhi Agus Oktaviana and Budianto Hakim of Indonesia's National Agency for Research and Innovation, and Renaud Joannes-Boyau of Southern Cross University. “The presence of this extremely old art in Sulawesi suggests that the initial peopling of Sahul about 65,000 years involved maritime journeys between Borneo and Papua, a region that remains poorly explored from an archaeological perspective,” the team added.Though the stencils are extremely faint and obscured by younger paintings, it’s still eerie to see the contours of human hands from a long-lost era when dire wolves and Siberian unicorns still roamed our world.Zoo animals get long in the toothSpeaking of really old stuff, there has been much consternation of late about falling birth rates and aging populations in many nations around the world. As it turns out, similar demographic anxieties are playing out in zoos across Europe and North America, where mammal populations “have, on average, become older and less reproductively active” according to a new study.  On the one hand, this is good news because it signals improvements in the health and longevity of mammals in zoos, reflecting a long-term effort to transform zoos into conservation hubs as opposed to sites of spectacle. But it also “fundamentally jeopardizes the long-term capacity of zoos to harbor insurance populations, facilitate reintroductions of threatened species, and simply maintain a variety of self-sustaining species programs,” said researchers led by João Pedro Meireles of the University of Zurich. This story struck me because of my many childhood visits to see an Asian elephant named Lucy, who was the star of the Edmonton Valley Zoo when I was young (I am now old). I recently learned Lucy is still chilling there at the ripe old age of 50! This is positively Methuselan for a zoo elephant, though it is not an unusual age for them in the wild. Lucy is the perfect poster child (or rather, poster senior) for this broader aging effect. Long may she reign.Bust out the bursty bulk flowWe’ll close with a reminder that the planet Mercury exists. It can be easy to overlook this tiny rock, which is barely bigger than the Moon. But Mercury is dynamic and full of surprises, according to a study based on close flybys of the planet by BepiColombo, a collaborative space mission between Europe and Japan, which is tasked with cracking this mercurial nut.BepiColombo zoomed just over 100 miles above Mercury’s surface in October 2021, June 2022, and June 2023, but each encounter revealed distinct portraits of the planet’s magnetosphere, which is a magnetic bubble that surrounds some planets, including Earth.“These flybys all passed from dusk to dawn through the nightside equatorial region but were noticeably different from each other,” said researchers led by Hayley N. Williamson of the Swedish Institute of Space Physics. “Specifically, we see energetic ions in the second and third flybys that are not there in the first.”“We conclude that these ions are part of a phenomenon called bursty bulk flow, which also happens at Earth,” the team concluded. Bursty bulk flow, in addition to being a fun phrase to say outloud, are intense, transient jets in a magnetosphere that drive energetic particles toward the planet, and are driven by solar activity. BepiColombo is on track to scooch into orbit around Mercury this November, where it will continue to study the planet up close for years, illuminating this world of extremes. In my hierarchy of Mercurys, the planet sits above the Ford brand, the 80th element, and the Roman god, with only Freddie surpassing it. So, it’s good to see it getting the attention it deserves.  Thanks for reading! See you next week.]]></content:encoded></item><item><title>The Project G Stereo Was the Definition of Groovy</title><link>https://spectrum.ieee.org/project-g-stereo</link><author>Allison Marsh</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82Mjk3ODM1Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgwOTA0MzEzN30.4-HE-hs8L8DGiawcFCVTIk5bmdlYBw46bZihCfMawLU/image.jpg?width=600" length="" type=""/><pubDate>Sat, 24 Jan 2026 14:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Clairtone’s high-end hi-fi system was prized by celebrities and musicians]]></content:encoded></item><item><title>A Class for Mom: AI in Your Everyday Life</title><link>https://hackernoon.com/a-class-for-mom-ai-in-your-everyday-life?source=rss</link><author>Amy Pravin Shah</author><category>tech</category><pubDate>Sat, 24 Jan 2026 13:20:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This is a free online class for seniors to help them learn about AI. ]]></content:encoded></item><item><title>RAG Systems in Five Levels of Difficulty (With Full Code Examples)</title><link>https://hackernoon.com/rag-systems-in-five-levels-of-difficulty-with-full-code-examples?source=rss</link><author>Paolo Perrone</author><category>tech</category><pubDate>Sat, 24 Jan 2026 13:12:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most RAG systems fail in production because “semantic similarity” isn’t the same as relevance, and retrieval breaks under real queries. This article outlines five escalating levels—naive RAG, smarter chunking with overlap and metadata, hybrid semantic + BM25 retrieval, cross-encoder reranking, and production guardrails that refuse or clarify when confidence is low—plus a testing approach to measure retrieval precision and answer accuracy. The core lesson: build, break, diagnose the failure mode, and level up until the system reliably grounds answers and knows when not to answer.]]></content:encoded></item><item><title>Hollywood Tries To Take Pirate Sites Down Globally Through India Court</title><link>https://yro.slashdot.org/story/26/01/24/0124246/hollywood-tries-to-take-pirate-sites-down-globally-through-india-court?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TorrentFreak: The High Court in New Delhi, India, has granted another pirate site blocking order in favor of American movie industry giants, including Apple, Warner., Netflix, Disney and Crunchyroll. The injunction targets notorious pirate sites, requesting blockades at Indian ISPs. More crucially, however, globally operating domain registrars, including U.S. companies, are also compelled to take action. However, despite earlier cooperation, most don't seem eager to comply. [...] As reported by Verdictum a few days ago, the High Court in New Delhi issued a new blocking injunction on December 18, targeting more than 150 pirate site domains, including yflix.to, animesuge.bz, bs.to, and many others.
 
The complaint (PDF) is filed by Warner Bros., Apple, Crunchyroll, Disney, and Netflix, which are all connected to the MPA's anti-piracy arm, ACE. The referenced works include some of the most pirated titles, such as Stranger Things, Squid Game, and Silo. In addition to targeting Indian ISPs, the order also lists various domain name registries and related organizations as defendants. This includes American registrars such as Namecheap and GoDaddy, but also the government of the Kingdom of Tonga, which is linked to .to domains. By requiring domain name registrars to take action, the Indian court orders have a global impact.
 
In addition to suspending the domain names within three days days, the domain name registrars are given four weeks to disclose the relevant subscriber information connected to these domains. "[The registrars] shall lock and suspend Defendant Nos. 1 to 47 websites within 72 hours of being communicated with a copy of this Order and shall file all the Basic Subscriber Information, including the name, address, contact information, email addresses, bank details, IP logs, and any other relevant information [...] within four weeks of being communicated with a copy of this Order," the High Court wrote. While the "Dynamic+" injunction is designed to be a global kill switch, its effectiveness depends entirely on the cooperation of the domain name registrars. Since most of these are based outside of India, their compliance is not guaranteed.]]></content:encoded></item><item><title>Meet the Writer: Nan Ei Ei Kyaw on Choosing the Right Vector DB</title><link>https://hackernoon.com/meet-the-writer-nan-ei-ei-kyaw-on-choosing-the-right-vector-db?source=rss</link><author>Nan Ei Ei</author><category>tech</category><pubDate>Sat, 24 Jan 2026 12:53:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Nan Ei Ei shares her journey from embedded systems to AI engineering, her approach to writing practical developer guides, and how real-world constraints shape production-ready RAG systems.]]></content:encoded></item><item><title>Why High Accuracy Isn’t Enough for Production AI Systems</title><link>https://hackernoon.com/why-high-accuracy-isnt-enough-for-production-ai-systems?source=rss</link><author>CIZO</author><category>tech</category><pubDate>Sat, 24 Jan 2026 12:40:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[High-accuracy AI systems often fail in real-world usage because human behavior introduces noise that benchmarks don’t capture. Designing for consistency and trust matters more than optimizing raw accuracy.]]></content:encoded></item><item><title>Small Language Models are Closing the Gap on Large Models</title><link>https://hackernoon.com/small-language-models-are-closing-the-gap-on-large-models?source=rss</link><author>Dmitriy Tsarev</author><category>tech</category><pubDate>Sat, 24 Jan 2026 12:23:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A fine-tuned 3B model outperformed a 70B baseline in production. This isn't an edge case—it's a pattern. Phi-4 beats GPT-4o on math. Llama 3.2 runs on smartphones. Inference costs dropped 1000x since 2021. The shift: careful data curation and architectural efficiency now substitute for raw scale. For most production workloads, a properly trained small model delivers equivalent results at a fraction of the cost.]]></content:encoded></item><item><title>AMD Releases MLIR-AIE 1.2 Compiler Toolchain For Targeting Ryzen AI NPUs</title><link>https://www.phoronix.com/news/AMD-MLIR-AIE-1.2</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 12:22:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In addition to AMD releasing the Ryzen AI Software 1.7 release on Friday, they also published a new version of their MLIR-AIE compiler toolchain for targeting AMD Ryzen AI NPU devices with this LLVM-based MLIR-focused stack...]]></content:encoded></item><item><title>AMDGPU Driver Reverts Code For A Number Of Regressions On Linux 6.19</title><link>https://www.phoronix.com/news/AMDGPU-Linux-6.19-Regressions</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 11:36:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged on Friday as part of this week's DRM kernel graphics driver fixes for the week is addressing a regression affecting many different users with the Linux 6.19 development kernel...]]></content:encoded></item><item><title>GNOME&apos;s AI Assistant Newelle Adds Llama.cpp Support, Command Execution Tool</title><link>https://www.phoronix.com/news/GNOME-AI-Newelle-1.2</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 11:19:44 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Newlle as a virtual AI assistant for the GNOME desktop with API integration for Google Gemini, OpenAI, Groq, and also local LLMs is out with a new release. Newelle has been steadily expanding its AI integration and capabilities and with the new Newelle 1.2 are yet more capabilities for those wanting AI on the GNOME desktop...]]></content:encoded></item><item><title>ASUS Armoury Driver For Linux 6.19 Picks Up Support For More ASUS Laptops</title><link>https://www.phoronix.com/news/ASUS-Armoury-More-Hardware</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 10:52:27 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A new driver in the Linux 6.19 kernel is the ASUS Armoury driver for supporting additional functionality with the ROG Ally gaming handhelds and other ASUS ROG gaming hardware like their laptops...]]></content:encoded></item><item><title>Smartwatches Help Detect Abnormal Heart Rhythms 4x More Often In Clinical Trial</title><link>https://science.slashdot.org/story/26/01/24/0114249/smartwatches-help-detect-abnormal-heart-rhythms-4x-more-often-in-clinical-trial?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A clinical trial found that seniors at high stroke risk who wore an Apple Watch were four times more likely to have hidden heart rhythm disorders detected than those receiving standard care. The researchers noted that over half the time, these smartwatch wearers with heart rhythm problems hadn't shown any symptoms prior to diagnosis. From U.S. News & World Report: Later editions of Apple Watches are equipped with two functions that can help monitor heart health -- photoplethysmography (PPG), which tracks heart rate, and a single-lead electrocardiogram (ECG) that monitors heart rhythm. "Using smartwatches with PPG and ECG functions aids doctors in diagnosing individuals unaware of their arrhythmia, thereby expediting the diagnostic process," said senior researcher Dr. Michiel Winter, a cardiologist at Amsterdam University Medical Center in The Netherlands. "Our findings suggest a potential reduction in the risk of stroke, benefiting both patients and the health care system by reducing costs," Winter said in a news release.
 
[...] Smartwatches are much easier than other wearable devices for detecting irregular heart rhythms [...]. These other means require people to wear sticky leads, carry around bulky monitors or even receive short-term implants. Lead researcher Nicole van Steijn, a doctoral candidate at Amsterdam UMC, noted that wearables that track both the pulse and electrical activity have been around for a while. "However, how well this technology works for the screening of patients at elevated risk for atrial fibrillation had not yet been investigated in a real-world setting,"she said in a news release. The findings have been published in the Journal of the American College of Cardiology.]]></content:encoded></item><item><title>The TechBeat: Third-Party Risks in 2026: Outlook and Security Strategies (1/24/2026)</title><link>https://hackernoon.com/1-24-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sat, 24 Jan 2026 07:11:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @ivankuznetsov [ 9 Min read ] 
 It’s far more efficient to run multiple Claude instances simultaneously, spin up git worktrees, and tackle several tasks at once. Read More.By @drechimyn [ 7 Min read ] 
 Broken Object Level Authorization (BOLA) is eating the API economy from the inside out.  Read More.By @socialdiscoverygroup [ 19 Min read ] 
 We taught Playwright to find the correct HAR entry even when query/body values change and prevented reusing entities with dynamic identifiers.  Read More.By @dataops [ 4 Min read ] 
 DataOps provides the blueprint, but automation makes it scalable. Learn how enforced CI/CD, observability, and governance turn theory into reality. Read More.By @kilocode [ 6 Min read ] 
 CodeRabbit alternative for 2026: Kilo's Code Reviews combines AI code review with coding agents, deploy tools, and 500+ models in one unified platform. Read More.By @linked_do [ 12 Min read ] 
 As the AI bubble deflates, attention shifts from scale to structure. A long view on knowledge, graphs, ontologies, and futures worth living. Read More.By @mcsee [ 3 Min read ] 
 Set your AI code assistant to read-only state before it touches your files. Read More.By @dharmateja [ 13 Min read ] 
 Historically, technological revolutions have triggered similar waves of anxiety, only for the long-term outcomes to demonstrate a more optimistic narrative. Read More.By @proflead [ 4 Min read ] 
 Ollama is an open-source platform for running and managing large-language-model (LLM) packages entirely on your local machine. Read More.By @nikitakothari [ 5 Min read ] 
 In an agentic world, your documentation—specifically your structured API contracts—has replaced your implementation as the actual source code. Read More.By @scottdclary [ 27 Min read ] 
 Real transformation requires your brain to physically rewire itself. Read More.By @praisejamesx [ 6 Min read ] 
 Stop relying on "vibes" and "hustle." History rewards those with better models, not better speeches. Read More.By @mohansankaran [ 10 Min read ] 
 Jetpack Compose memory leaks are usually reference leaks. Learn the top leak patterns, why they happen, and how to fix them. Read More.By @ishanpandey [ 5 Min read ] 
 BTCC reports $5.7B tokenized gold volume in 2025 with 809% Q4 growth, marking gold as crypto's dominant real-world asset. Read More.By @zacamos [ 5 Min read ] 
 Third-party risk is everywhere in 2026. Here's an overview of current risks and security best practices as we start the new year. Read More.]]></content:encoded></item><item><title>Study Shows How Earthquake Monitors Can Track Space Junk Through Sonic Booms</title><link>https://science.slashdot.org/story/26/01/24/014216/study-shows-how-earthquake-monitors-can-track-space-junk-through-sonic-booms?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A new study shows that earthquake monitoring networks can track falling space debris by detecting the sonic booms produced during atmospheric reentry, sometimes more accurately than radar. The Associated Press reports: Scientists reported Thursday that seismic readings from sonic booms that were generated when a discarded module from a Chinese crew capsule reentered over Southern California in 2024 allowed them to place the object's path nearly 20 miles (30 kilometers) farther south than radar had predicted from orbit. Using this method to track uncontrolled objects plummeting at supersonic speeds, they said, could help recovery teams reach any surviving pieces more quickly -- crucial if the debris is dangerous.
 
"The problem at the moment is we can track stuff very well in space," said Johns Hopkins University's Benjamin Fernando, the lead researcher. "But once it gets to the point that it's actually breaking up in the atmosphere, it becomes very difficult to track." His team's findings, published in the journal Science, focus on just one debris event. But the researchers already have used publicly available data from seismic networks to track a few dozen other reentries, including debris from three failed SpaceX Starship test flights in Texas. [...]
 
Fernando is looking to eventually publish a catalog of seismically tracked, entering space objects, while improving future calculations by factoring in the wind's effect on falling debris. In a companion article in Science, Los Alamos National Laboratory's Chris Carr, who was not involved in the study, said further research is needed to reduce the time between an object's final plunge and the determination of its course. For now, Carr said this new method "unlocks the rapid identification of debris fall-out zones, which is key information as Earth's orbit is anticipated to become increasingly crowded with satellites, leading to a greater influx of space debris."]]></content:encoded></item><item><title>Legal AI giant Harvey acquires Hexus as competition heats up in legal tech</title><link>https://techcrunch.com/2026/01/23/legal-ai-giant-harvey-acquires-hexus-as-competition-heats-up-in-legal-tech/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sat, 24 Jan 2026 05:27:22 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Hexus founder and CEO Sakshi Pratap, who previously held engineering roles at Walmart, Oracle, and Google, tells TechCrunch that her San Francisco-based team has already joined Harvey, while the startup's India-based engineers will come onboard once Harvey establishes a Bangalore office.]]></content:encoded></item><item><title>Apple iPhone just had its best year in India as the smartphone market stays broadly flat</title><link>https://techcrunch.com/2026/01/23/apple-iphone-just-had-its-best-year-in-india-as-the-smartphone-market-stays-broadly-flat/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Sat, 24 Jan 2026 05:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple shipped a record 14 million iPhones in India in 2025 and gained market share.]]></content:encoded></item><item><title>KDE Plasma Saw At Least 9 Crash Fixes This Week</title><link>https://www.phoronix.com/news/KDE-Plasma-9-Crash-Fixes</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 05:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE Plasma 6.6 feature development work continues winding down while Plasma 6.7 has begun seeing more feature work. This week also saw at least nine different crash fixes affecting Plasma/KWin...]]></content:encoded></item><item><title>TikTok users freak out over app’s ‘immigration status’ collection — here’s what it means</title><link>https://techcrunch.com/2026/01/23/tiktok-users-freak-out-over-apps-immigration-status-collection-heres-what-it-means/</link><author>Sarah Perez</author><category>tech</category><pubDate>Sat, 24 Jan 2026 04:34:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TikTok users are freaking out over a mention of "immigration status" data collection, but lawyers explain the disclosure is related to state privacy laws. ]]></content:encoded></item><item><title>Techdirt 2025: The Stats</title><link>https://www.techdirt.com/2026/01/23/techdirt-2025-the-stats/</link><author>Mike Masnick</author><category>tech</category><pubDate>Sat, 24 Jan 2026 03:39:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Every year, a little after New Years, I do a post about the previous year of Techdirt traffic and comments. You may notice that we skipped last year’s for 2024. 2025 was so crazy with everything happening, we just didn’t get around to it, and I kept saying I would and then I looked up and it was May and it just didn’t feel right to go back. But now we’re back, closing the books on 2025 in mid-January.As we’ve done for a few years now, we continue to run  Google Analytics, relying instead on JetPack and Plausible Analytics. And as we always note, all traffic numbers are somewhat unreliable, but they give us a general sense of how things are going (and JetPack & Plausible’s numbers mostly seem to match).In 2025, our traffic was up noticeably from previous the previous year—around 29% more pageviews compared to 2024. Given that 2025 was the year American democracy started visibly buckling, and we made it clear we wouldn’t back down from covering it, that’s probably not surprising. With so much of our national media falling down on the job, it turns out people will show up when you’re one of the news orgs actually calling out what’s happening.As is pretty typical, the vast majority of our traffic came from the US (around 75%), followed by the UK, Canada, Australia, India, Germany, and Finland. After that you have the Netherlands, France, Sweden, New Zealand, Ireland, Spain, Norway, and Pakistan. The stats say we had… a grand total of three visitors from Antarctica last year. Stay warm, folks. We also had three visitors from Vatican City. Sounds like Pope Leo isn’t yet reading Techdirt, but there’s still time.In 2025 we published 1,993 posts and garnered 39,750 comments. The post number is about average for us over the past few years. The comment numbers are down a bit, even as traffic was up, which is likely due to some anti-trolling/anti-spam measures we took last May when a few trolls (and spammers) got a bit out of control. We also wrote about 1.74 million words in 2025, our most since 2016.It also appears to be an unstoppable trend that Techdirt’s posts only get longer and longer, reaching 871 words per post on average last year. The trend here is not subtle.As for where our traffic comes from, I’m always proudest of the fact that more than half of our traffic is direct traffic, not referred from elsewhere, meaning that we have a loyal audience that comes to check out Techdirt unmediated by various algorithms.In terms of traffic referrals, the largest single source was Reddit. Search engines (mainly Google) were also significant. After that our two biggest referrers were Bluesky and Fark (yes, Fark!). It’s nice to see Bluesky continuing to send tons of traffic, reminding us that it’s the only major social media site that doesn’t downgrade and suppress links. We also got significant traffic from Flipboard, Google News, Hacker News, and the NewsBreak app.Much further down on the referrals, X, Substack, and LinkedIn all gave us roughly the equivalent amount of traffic to each other (less than 10% of what Bluesky and Fark sent us). Also… ChatGPT. It’s a little bit less than X/Substack/LinkedIn, but I’m guessing by next year it will surpass those. Wikipedia & Threads each sent about the same amount of traffic as ChatGPT did.Down towards the bottom of the list there are random blogs, news sites, a few RSS readers, and also Mastodon. They’re not that big compared to the others, but they’re all still sending some visitors our way.Our traffic now appears to be almost exactly evenly split between computers and mobile devices. Last year it was a 51%/49% split with the slight edge going to desktops. In terms of specific operating systems, iOS tops the overall list, followed by Windows. Then Android, Mac, and Linux. There’s a much smaller group of folks at the bottom of the list using Chromebooks.Interestingly, our most popular day for traffic was Thursday (18% of views), and the best hour was 9:00 AM (7% of views).Top Ten Stories, by unique pageviews, on Techdirt for 2025:The pattern here is not exactly subtle. Seven of the top ten stories are about the ongoing collapse of constitutional governance. The TikTok stories are really the same story twice… and in some way are directly connected to the collapse of the United States. And the only entry that isn’t directly about authoritarianism is about how bad-faith actors exploit free speech norms — which, well, same theme wearing different clothes.2025’s Top Ten Stories, by comment volume:The fact that we had two of the weekly comment roundups ending up on the most commented list, both of which were from last January, tells you how we had some trolls who took it upon themselves to wreck the comments, especially on those posts early last year. Also, as we point out nearly every year, the fact that the list of highest commented posts is almost entirely different from the list of most visited posts seems noteworthy.Now, to the personal commenter leaderboards:2025 Top Commenters, by comment volume:Stephen T. Stone continues to dominate the comment leaderboard, though with fewer comments than in previous years, probably since there were fewer troll comments to respond to. It’s also nice to see some new names on the list this year.Top 10 Most Insightful Commenters, based on how many times they got the lightbulb icon(Parentheses shows what percentage of their comments got the icon)Some familiar names here, though nice to see MrWilson move up in the rankings. Also a shoutout to Bloof for having the highest percentage of comments getting the insightful icon.Top 10 Funniest Commenters, based on how many times they got the laughing face icon(Parentheses shows what percentage of their comments got the icon)Interesting to see MrWilson take the top spot for funny this year. As always, it’s much harder to get the funny icon than the insightful one. Last year wasn’t a huge year for humor, not surprisingly. But looking at how few “funny” comments were needed to get on the top 10 list, seems like some of you could jump onto it next year with just a few more funny comments. Let’s get some gallows humor going. Also, shoutout to Rico R. for having a very high percentage of their comments getting the funny icon.And, with that, the 2025 books are officially closed. 2026 is already a few weeks in and shows no signs of being any less exhausting (quite the opposite), so we’ll see you in the comments. Thanks to everyone who reads and debates, and especially to those of you who support our work here.]]></content:encoded></item><item><title>New Filtration Technology Could Be Gamechanger In Removal of PFAS &apos;Forever Chemicals&apos;</title><link>https://science.slashdot.org/story/26/01/24/002216/new-filtration-technology-could-be-gamechanger-in-removal-of-pfas-forever-chemicals?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Bruce66423 shares a report from the Guardian: New filtration technology developed by Rice University may absorb some Pfas "forever chemicals" at 100 times the rate than previously possible, which could dramatically improve pollution control and speed remediations. Researchers also say they have also found a way to destroy Pfas, though both technologies face a steep challenge in being deployed on an industrial scale. A new peer-reviewed paper details a layered double hydroxide (LDH) material made from copper and aluminum that absorbs long-chain Pfas up to 100 times faster than commonly used filtration systems.
 
[...] [Michael Wong, director of Rice's Water Institute, a Pfas research center] said Rice's non-thermal process works by soaking up and concentrating Pfas at high levels, which makes it possible to destroy them without high temperatures. The LDH material Rice developed is a variation of similar materials previously used, but researchers replaced some aluminum atoms with copper atoms. The LDH material is positively charged and the long-chain Pfas are negatively charged, which causes the material to attract and absorb the chemicals, Wong said. [...]
 
Pfas are virtually indestructible because their carbon atoms are bonded with fluoride, but Rice found that the bonds could be broken if the chemicals in the material were heated to 400-500C -- a relatively low temperature. The fluoride gets trapped in the LDH material and is bonded to calcium. The leftover calcium-fluoride material is safe and can be disposed of in a landfill, Wong said. The process works with some long-chain Pfas that are among the most common water pollutants, and it also absorbed some smaller Pfas that are commonplace.
 
Wong said he is confident the material can be used to absorb a broad array of Pfas, especially if they are negatively charged. Most new Pfas elimination systems fail to work at an industrial scale. Wong said the new material has an advantage because its absorption rate is so strong, it can be used repeatedly and it is in a "drop in material," meaning it can be used with existing filtration infrastructure. That eliminates one of the major cost barriers.]]></content:encoded></item><item><title>The Physics Simulation Problem That More Compute Can’t Fix</title><link>https://hackernoon.com/the-physics-simulation-problem-that-more-compute-cant-fix?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Sat, 24 Jan 2026 03:00:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Physics simulations don’t just get slower as resolution increases—they break.]]></content:encoded></item><item><title>Advanced Machine Learning: Bridging SDP Relaxation and Collective Motion Dynamics</title><link>https://hackernoon.com/advanced-machine-learning-bridging-sdp-relaxation-and-collective-motion-dynamics?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:39:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.1 Kuramoto models from the geometric point of view(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>The Kuramoto Model: Synchronization and Dynamics of Coupled Oscillators</title><link>https://hackernoon.com/the-kuramoto-model-synchronization-and-dynamics-of-coupled-oscillators?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:36:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The famous Kuramoto model [2] has been introduced in 1975 as a paradigm for the synchronization phenomena in ensembles of coupled oscillators. Following the pioneering Kuramoto’s paper, various modifications and generalizations of his model have been proposed. The model describes an ensemble of phase oscillators, whose states are represented by phases ϕi ∈ [0, 2π], while amplitudes are neglected.\
We consider the model where the dynamics of oscillators are given by the following system of ODE’s(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Physics-Informed Machine Learning: Leveraging Physical Laws and Energy-Based Models</title><link>https://hackernoon.com/physics-informed-machine-learning-leveraging-physical-laws-and-energy-based-models?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:33:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The term physics informed ML refers to the general approach aiming at exploiting physical knowledge in order to set up adequate models given the particular data set and the problem. In many cases, models and architectures are, at least partially, enforced by physical laws, such as symmetries or conservation laws [66, 65]. Taking this into account dramatically increases efficiency, transparency and robustness of ML algorithms. The very general idea standing behind this approach is the parsimony principle, one of the most universal principles in Science.\
Although physics informed ML is regarded a very recent field, it has been developed upon the blend of ideas from computational physics and energy-based ML. Indeed, concepts of energy and entropy are built in early ML algorithms dealing with problems that are not necessarily related to any physical system [67]. A classical example of this kind is the famous Hopfield model. We also refer to [68] for energy-based approaches in RL.\
More generally, the term theory informed ML refers to architectures which are imposed by a certain theoretical knowledge.\
Approaches and models we propose in subsequent sections can be viewed as both physics informed and geometry informed ML. Moreover, many of them are also energy-based models.(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>The Twilight Movies in Chronological Order: A Watch Guide</title><link>https://hackernoon.com/the-twilight-movies-in-chronological-order-a-watch-guide?source=rss</link><author>Jose</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:19:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Although the Twilight movie franchise ended over 10 years ago, it has remained seeded in people’s hearts. It had it all: vampires, werewolves, a love triangle, and a really distracting CGI baby. It was just a different time, really, and you had to be there to fully experience the Twilight craze. \
But for those who couldn’t, this is the next best thing. Here are all 5 Twilight movies in chronological order. So, the next weekend you have free, why not make the leap into the unforgettable world of Twilight?How to Watch the Twilight Movies in OrderThe Twilight Saga: New MoonThe Twilight Saga: EclipseThe Twilight Saga: Breaking Dawn - Part 1The Twilight Saga: Breaking Dawn - Part 2The world wasn't ready for this one. Released in 2008, the general audience was immediately sucked into the world of Twilight, and obsessed with the relationship between normal human girl Bella Swan and 100+ year-old Edward Cullen.\
But this was only the beginning, and pretty soon the world would be obsessed with a third character…\
Find out where you can watch Twilight here.2. The Twilight Saga: New Moon (2009)Even though he was introduced in the first movie, we didn't get to see much of Jacob, Bella's childhood friend. However, in New Moon, Jacob takes center stage.\
After Edward breaks up with Bella and leaves town, Jacob helps Bella deal with the emotional fallout, while also dealing with his werewolf transformation.\
Find out where you can watch The Twilight Saga: New Moon here.3. The Twilight Saga: Eclipse (2010)The love triangle between our three heroes becomes increasingly complicated as they find themselves caught in the middle of a conflict between the Cullens, the werewolves, and a new group of vampires.\
Bella has to choose who she wants to be with while also dealing with the dangers of these new vampires.\
Find out where you can watch The Twilight Saga: Eclipse here.4. The Twilight Saga: Breaking Dawn - Part 1 (2011)A story so big that it had to be broken up into 2 parts.\
In Breaking Dawn - Part 1, Edward and Bella finally get married, but quickly get hit with the realization that a vampire and a human marriage isn't so easy.\
Find out where you can watch The Twilight Saga: Breaking Dawn - Part 1 here.5. The Twilight Saga: Breaking Dawn - Part 2 (2012)After 4 years and 4 films, The Twilight Saga reaches its epic conclusion with Breaking Dawn - Part 2.\
The Cullens have had a rocky relationship with the Volturi, the most powerful vampire coven, throughout the film series, and things finally come to a head, with both groups preparing for an all-out battle.\
Find out where you can watch The Twilight Saga: Breaking Dawn - Part 2 here.Now that you know how to watch the Twilight movies in order, grab your nearest blanket, your bucket of popcorn, and start binging. ]]></content:encoded></item><item><title>California Becomes First State To Join WHO Disease Network After US Exit</title><link>https://yro.slashdot.org/story/26/01/23/2350246/california-becomes-first-state-to-join-who-disease-network-after-us-exit?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[California became the first U.S. state to join the World Health Organization's Global Outbreak Alert and Response Network (GOARN), one day after the U.S. formally exited the WHO. The Hill reports: This announcement comes just one day after the U.S.'s withdrawal from the WHO became official after nearly 80 years of membership, having been a founding member of the organization. "The Trump administration's withdrawal from WHO is a reckless decision that will hurt all Californians and Americans," [California Governor Gavin Newsom] said in a statement. "California will not bear witness to the chaos this decision will bring. We will continue to foster partnerships across the globe and remain at the forefront of public health preparedness, including through our membership as the only state in WHO's Global Outbreak Alert & Response Network."]]></content:encoded></item><item><title>Campaigner Launches $2 Billion Legal Action In UK Against Apple Over Wallet&apos;s &apos;Hidden Fees&apos;</title><link>https://news.slashdot.org/story/26/01/23/2328228/campaigner-launches-2-billion-legal-action-in-uk-against-apple-over-wallets-hidden-fees?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Longtime Slashdot reader AmiMoJo shares a report from the Guardian: The financial campaigner James Daley has launched a 1.5 billion pound (approximately $1.5 billion) class action lawsuit against Apple over its mobile phone wallet, claiming the U.S. tech company blocked competition and charged hidden fees that ultimately harmed 50 million UK consumers. The lawsuit takes aim at Apple Pay, which they say has been the only contactless payment service available for iPhone users in Britain over the past decade.
 
Daley, who is the founder of the advocacy group Fairer Finance, claims this situation amounted to anti-competitive behavior and allowed Apple to charge hidden fees, ultimately pushing up costs for banks that passed charges on to consumers, regardless of whether they owned an iPhone. It is the first UK legal challenge to the company's conduct in relation to Apple Pay, and takes place months after regulators like the Competition and Markets Authority and the Payments Systems Regulator began scrutinising the tech industry's digital wallet services. The case has been filed with the Competition Appeal Tribunal, which will now decide whether the class action case can move forward.
 
[...] Daley's lawsuit alleges that Apple refused to give other app developers and outside businesses access to the contactless payment technology on its iPhones, which meant it could charge banks and card issuers fees on Apple Pay transactions that his lawyers say "are not in line with industry practice." The lawsuit notes that similar fees are not charged on equivalent payments on Android devices, which are built by Google. It says that the additional costs were borne by UK consumers, having been passed on through charges on a range of personal banking products ranging from current accounts, credit cards, to savings and mortgages. The lawsuit says that about 98% of consumers are exposed to banks that listed cards on Apple Pay, meaning the vast majority of the UK population may have been affected.]]></content:encoded></item><item><title>Why Kubernetes Outages Are Usually Human Failures, Not Platform Bugs</title><link>https://hackernoon.com/why-kubernetes-outages-are-usually-human-failures-not-platform-bugs?source=rss</link><author>David Iyanuoluwa Jonathan</author><category>tech</category><pubDate>Sat, 24 Jan 2026 01:17:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Kubernetes isn’t inherently complex—teams create fragility through undocumented tooling, hero engineering, and unchecked operational sprawl. The fix is discipline, simplification, and shared understanding.]]></content:encoded></item><item><title>Search Engines, AI, And The Long Fight Over Fair Use</title><link>https://www.eff.org/deeplinks/2026/01/search-engines-ai-and-long-fight-over-fair-use</link><author>Joe Mullin</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/copyright-orange_0_0.png" length="" type=""/><pubDate>Sat, 24 Jan 2026 01:09:20 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GNU C Library 2.43 Released With More C23 Features, mseal &amp; openat2 Functions</title><link>https://www.phoronix.com/news/GNU-C-Library-Glibc-2.43</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 01:06:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Version 2.43 of the GNU C Library "glibc" was released on Friday evening as the newest half-year feature update. This is a very feature packed update and even managed to be released ahead of the 1 February release plan...]]></content:encoded></item><item><title>Explainable AI (XAI) in Healthcare: Trust, Transparency, and the Limits of AI Decisions</title><link>https://hackernoon.com/explainable-ai-xai-in-healthcare-trust-transparency-and-the-limits-of-ai-decisions?source=rss</link><author>Srikanth Akkaru</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:56:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[XAI focuses on making AI models transparent and trustworthy, so clinicians and patients can actually trust an AI models. XAI tools like Grad-CAM (heatmaps) show which regions influenced diagnoses like cancer detection, radiology, pathology and dermatology.]]></content:encoded></item><item><title>Justice Department Opens Criminal Probe Into Silicon Valley Spy Allegations</title><link>https://yro.slashdot.org/story/26/01/23/2317211/justice-department-opens-criminal-probe-into-silicon-valley-spy-allegations?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The U.S. Department of Justice has opened a criminal investigation into Deel over allegations that it recruited a spy inside rival Rippling, according to documents seen by The Wall Street Journal. From the report: An Ireland-based Rippling employee, Keith O'Brien, alleged in an affidavit filed in April that Deel Chief Executive Alex Bouaziz recruited him and gave him instructions for what information to take from Rippling. O'Brien alleged that other executives were involved in the spying plot, including Bouaziz's father, who is Deel's executive chairman and chief strategy officer.
 
A spokeswoman for Deel said the company isn't aware of a criminal investigation but is willing to cooperate with authorities. The company has previously said: "We deny all legal wrongdoing and look forward to asserting our counterclaims." Unsealed court documents allege that an entity tied to Deel transferred $6,000 to an account owned by the wife of Chief Operating Officer Dan Westgarth, and that the same amount was forwarded from the account to O'Brien seconds later.]]></content:encoded></item><item><title>Senior Engineers Are Becoming Failure Designers</title><link>https://hackernoon.com/senior-engineers-are-becoming-failure-designers?source=rss</link><author>David Iyanuoluwa Jonathan</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:44:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Senior developers know that designing for success means designing for failure. The new breed of engineer spends as much time planning how things will fail as how they work.]]></content:encoded></item><item><title>Gas Inefficiencies Developers Don&apos;t Notice Until It&apos;s Too Late</title><link>https://hackernoon.com/gas-inefficiencies-developers-dont-notice-until-its-too-late?source=rss</link><author>koxy</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:34:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Gas costs spiral out of control when smart contracts misuse storage, loops, and validation order. Across EVM and non-EVM chains, inefficient code turns usable protocols into expensive failures—often without obvious bugs or exploits.]]></content:encoded></item><item><title>Who’s behind AMI Labs, Yann LeCun’s ‘world model’ startup</title><link>https://techcrunch.com/2026/01/23/whos-behind-ami-labs-yann-lecuns-world-model-startup/</link><author>Anna Heim</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:04:45 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Yann LeCun's new venture, AMI Labs, has drawn intense attention since the AI scientist left Meta to found it. ]]></content:encoded></item><item><title>TikTok Is Now Collecting Even More Data About Its Users</title><link>https://yro.slashdot.org/story/26/01/23/236200/tiktok-is-now-collecting-even-more-data-about-its-users?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Wired: When TikTok users in the U.S. opened the app today, they were greeted with a pop-up asking them to agree to the social media platform's new terms of service and privacy policy before they could resume scrolling. These changes are part of TikTok's transition to new ownership. In order to continue operating in the U.S., TikTok was compelled by the U.S. government to transition from Chinese control to a new, American-majority corporate entity. Called TikTok USDS Joint Venture LLC, the new entity is made up of a group of investors that includes the software company Oracle. It's easy to tap "agree" and keep on scrolling through videos on TikTok, so users might not fully understand the extent of changes they are agreeing to with this pop-up.
 
Now that it's under U.S.-based ownership, TikTok potentially collects more detailed information about its users, including precise location data. Here are the three biggest changes to TikTok's privacy policy that users should know about. TikTok's change in location tracking is one of the most notable updates in this new privacy policy. Before this update, the app did not collect the precise, GPS-derived location data of U.S. users. Now, if you give TikTok permission to use your phone's location services, then the app may collect granular information about your exact whereabouts. Similar kinds of precise location data is also tracked by other social media apps, like Instagram and X.
 
[...] Rather than an adjustment, TikTok's policy on AI interactions adds a new topic to the privacy policy document. Now, users' interactions with any of TikTok's AI tools explicitly fall under data that the service may collect and store. This includes any prompts as well as the AI-generated outputs. The metadata attached to your interactions with AI tools may also be automatically logged. [...] This change to TikTok's privacy policy may not be as immediately noticeable to users, but it will likely have an impact on the types of ads you see outside of TikTok. So, rather than just using your collected data to target you while using the app, TikTok may now further leverage that info to serve you more relevant ads wherever you go online. As part of this advertising change, TikTok also now explicitly mentions publishers as one kind of partner the platform works with to get new data.]]></content:encoded></item><item><title>White House Push AI-Altered Images Of Arrested ICE Protesters To Manufacture Cruelty</title><link>https://www.techdirt.com/2026/01/23/white-house-push-ai-altered-images-of-arrested-ice-protesters-to-manufacture-cruelty/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Fri, 23 Jan 2026 23:28:33 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[We are being led by deeply unserious people. Not only that, but people who are manufacturing cruelty upon their very own constituents. That’s how bad this has gotten.This week, the DOJ arrested three people in Minnesota for protesting ICE’s goonish activity in a local church, where the pastor there also heads up the local ICE field office. Among the three is Nekima Levy Armstrong, former NAACP chapter president and a local activist who the DOJ claims organized the protest and instigated the group going into the church during services. Just how true any of that is is anyone’s guess, since it’s become impossible to believe a single thing this government says about ICE protests. For example:There was no attack. There was no violence. There were words and chants being voiced in a place of worship. You can find that repugnant, if you like. It’s still not an attack. The law being cited for the arrest makes Armstrong’s detention dubious at best.The law Bondi cited in her announcement — 18 U.S. Code § 241 — describes it pertaining to when “two or more persons conspire to injure, oppress, threaten, or intimidate any person in any State, Territory, Commonwealth, Possession, or District in the free exercise or enjoyment of any right or privilege secured to him by the Constitution or laws of the United States.”While many in the faith community are obediently clutching their figurative pearls over all of this, I’m struggling to understand how walking into a church that’s open to the public and saying words, even interrupting services, violates that law. I don’t think it does, but then I also laughed out loud when I read Bondi’s claim that this was an “attack.” The plain meaning of words doesn’t appear to matter to these people all that much.Yes, the White House decided to take an image of law enforcement improperly arresting an American citizen, one of their own constituents, and have AI alter it to make it appear that she is in distress. Oh, and they made her skin tone slightly darker as well. Because they  her to have been in distress. It eats them up inside that she wasn’t crying. That  her to be “blacker” because they want all of their enemies to be people of color. They’re showing you want they want to visit upon .And until they are put in check, they will continue to behave like a toddler with unfettered access to the internet and a permanently shitty attitude.Asked whether the image had been digitally altered, the White House responded by sending a post on X from Kaelan Dorr, the deputy communications director.“YET AGAIN to the people who feel the need to reflexively defend perpetrators of heinous crimes in our country I share with you this message: Enforcement of the law will continue. . Thank you for your attention to this matter,” he said.And thank you, Kaelan, for going outside and playing hide and go fuck yourself. Again, deeply unserious people. Shitposters. Internet trolls. These are the people in charge of the government. The ones sending their goon squads into our cities. The ones threatening to use the military against its own citizens. The ones that believe they are beyond accountability for all they are currently doing.I worry seriously that the president’s health is such that he won’t be available to stand trial whenever our government returns to sanity and the time for accountability arrives. But the same can’t be said for those beneath him. Bondi, Noem, Dorr, and many others  be held to account for what they are doing in this administration. The ledger  be kept and debts satisfied through the legal system, once actual justice is back on the menu.For now, the fight against the toddlers continues.]]></content:encoded></item><item><title>White House Labels Altered Photo of Arrested Minnesota Protester a &apos;Meme&apos;</title><link>https://yro.slashdot.org/story/26/01/23/1928230/white-house-labels-altered-photo-of-arrested-minnesota-protester-a-meme?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The White House doubled down after posting a digitally altered photo of Minnesota protester Nekima Levy Armstrong, dismissing it as a "meme" despite objections from her attorney and comparisons to reality-distorting propaganda. "YET AGAIN to the people who feel the need to reflexively defend perpetrators of heinous crimes in our country I share with you this message: Enforcement of the law will continue. The memes will continue. Thank you for your attention to this matter," White House spokesperson Kaelan Dorr wrote in a post on X. The Hill reports: The statement came after Homeland Security Secretary Kristi Noem posted a photo of Armstrong's arrest Thursday showing Armstrong with what appears to be a blank facial expression. However, the White House later posted an altered version of the same photo that shows Armstrong crying.
 
Armstrong's attorney Jordan Kushner said in an interview with CNN that an agent was recording Armstrong's arrest on their cellphone. "I've never seen anything like it. It's so unprofessional," Kushner said. "He was ordered to do it because the government was looking to make a spectacle of this case. I observed the whole thing. She was dignified, calm, rational the whole time." Kushner went on to call the move to alter the photo "a hallmark of a fascist regime where they actually alter reality."]]></content:encoded></item><item><title>PowerShell Architect Retires After Decades At the Prompt</title><link>https://tech.slashdot.org/story/26/01/23/1915259/powershell-architect-retires-after-decades-at-the-prompt?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Jeffrey Snover, the driving force behind PowerShell, has retired after a career that reshaped Windows administration. The Register reports: Snover's retirement comes after a brief sojourn at Google as a Distinguished Engineer, following a lengthy stint at Microsoft, during which he pulled the company back from imposing a graphical user interface (GUI) on administrators who really just wanted a command line from which to run their scripts. Snover joined Microsoft as the 20th century drew to a close. The company was all about its Windows operating system and user interface in those days -- great for end users, but not so good for administrators managing fleets of servers. Snover correctly predicted a shift to server datacenters, which would require automated management. A powerful shell... a PowerShell, if you will.
 
[...] Over the years, Snover has dropped the occasional pearl of wisdom or shared memories from his time getting PowerShell off the ground. A recent favorite concerns the naming of Cmdlets and their original name in Monad: Function Units, or FUs. Snover wrote: "This abbreviation reflected the Unix smart-ass culture I was embracing at the time. Plus I was developing this in a hostile environment, and my sense of diplomacy was not yet fully operational." Snover doubtless has many more war stories to share. In the meantime, however, we wish him well. Many admins owe Snover thanks for persuading Microsoft that its GUI obsession did not translate to the datacenter, and for lengthy careers in gluing enterprise systems together with some scripted automation.]]></content:encoded></item><item><title>Microsoft Gave FBI a Set of BitLocker Encryption Keys To Unlock Suspects&apos; Laptops</title><link>https://it.slashdot.org/story/26/01/23/1910235/microsoft-gave-fbi-a-set-of-bitlocker-encryption-keys-to-unlock-suspects-laptops?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 22:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: Microsoft provided the FBI with the recovery keys to unlock encrypted data on the hard drives of three laptops as part of a federal investigation, Forbes reported on Friday. Many modern Windows computers rely on full-disk encryption, called BitLocker, which is enabled by default. This type of technology should prevent anyone except the device owner from accessing the data if the computer is locked and powered off.
 
But, by default, BitLocker recovery keys are uploaded to Microsoft's cloud, allowing the tech giant -- and by extension law enforcement -- to access them and use them to decrypt drives encrypted with BitLocker, as with the case reported by Forbes. The case involved several people suspected of fraud related to the Pandemic Unemployment Assistance program in Guam, a U.S. island in the Pacific. Local news outlet Pacific Daily News covered the case last year, reporting that a warrant had been served to Microsoft in relation to the suspects' hard drives.
 
Kandit News, another local Guam news outlet, also reported in October that the FBI requested the warrant six months after seizing the three laptops encrypted with BitLocker. [...] Microsoft told Forbes that the company sometimes provides BitLocker recovery keys to authorities, having received an average of 20 such requests per year.]]></content:encoded></item><item><title>Waymo probed by National Transportation Safety Board over illegal school bus behavior</title><link>https://techcrunch.com/2026/01/23/waymo-probed-by-national-transportation-safety-board-over-illegal-school-bus-behavior/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:50:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The NTSB joins the National Highway Traffic Safety Administration in investigating Waymo vehicles illegally passing stopped school buses.]]></content:encoded></item><item><title>Why Decentralized Validator Infrastructure Is Critical for Institutional Staking</title><link>https://hackernoon.com/why-decentralized-validator-infrastructure-is-critical-for-institutional-staking?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:45:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
By Prash Pandit, VP Validation Business at A technical look at how decentralized validator architecture gives institutions better reliability, auditability, and system-level resilience.If you’ve ever actually run validators — not reviewed a diagram, not talked strategy in a meeting, but operated them — you figure out quickly that  isn’t passive. It behaves like a live distributed system. Clients drift. Gossip traffic gets noisy. Relays hiccup at precisely the wrong moment. And when you scale that across institutional-sized positions, the infrastructure stops being a supporting detail. It becomes part of your risk surface.Most institutional teams start with custodial platforms because those platforms make the early steps painless. That’s a reasonable first phase. Institutions have onboarding, governance, and compliance requirements that don’t just disappear because a blockchain is involved. But once you look at what a validator is actually responsible for — meeting attestation deadlines, proposing blocks on schedule, keeping up with fork-choice changes, routing through relays, managing duties that repeat every few seconds — the idea of putting all of that inside a sealed box starts to feel mismatched with how the network behaves. Validators aren’t static yield engines. They’re consensus actors.Centralized setups tend to run large validator fleets on nearly identical stacks. Same client builds. Same relay preferences. Same tuning. Same monitoring assumptions. That uniformity looks stable from the outside, but uniformity has a well-known weakness: when something breaks, it breaks everywhere at once. A client bug or a relay stall doesn’t stay local; it becomes a correlated event. Anyone who has worked through a real incident review knows how quickly that can turn into operational noise and awkward reporting questions.\
 is built to avoid that. Instead of relying on one operator’s environment, responsibilities get spread across several operators who don’t share the same failure modes. They run different clients. They make different operational choices. Their infrastructure isn’t a carbon copy of anyone else’s. You get genuine separation. Failures stay smaller.This is where decentralization begins to look less like a philosophy and more like the thing that keeps a large validator footprint stable.\
 takes that one step further. Instead of a single signer making decisions, you use threshold cryptography across multiple nodes. No operator holds the whole key. The validator acts only when enough shares arrive. If one node drifts, the validator doesn’t stall. If one node misconfigures its client, the validator doesn’t head toward slashing. It behaves more like other high-availability systems institutions already trust: distributed, fault-tolerant, and designed so no individual component can sink the whole service.\
This architecture also fixes a visibility gap. Eventually someone will ask why a validator underperformed in a specific epoch, or why duties were missed, or why a particular MEV path was chosen. In a centralized environment, you usually get an aggregated answer because everything underneath is identical. In a decentralized environment, operator-level differences exist by design, which makes performance observable. It gives institutions something they rarely get from sealed systems: the ability to reason about behavior the same way they would with any other critical workload.Key management improves too. Large centralized fleets often keep operational keys online to manage thousands of validators smoothly. It’s practical, but it’s still a single custody point. In a threshold-based decentralized setup, the key never exists in one place. No operator can act alone. The architecture itself enforces the guardrails. That aligns well with how institutional security models already work — distributed approvals, multi-party controls, and reduced single-operator exposure.Flexibility is another place decentralization pays off. Institutions don’t always worry about operator rotation at the start, but it surfaces sooner than expected. Policies change. Infrastructure standards shift. Governance committees ask new questions. In a centralized model, the whole validator setup — keys, clients, MEV routes, reporting — is bundled. Switching becomes expensive. In decentralized architectures, operators function as replaceable components. If one underperforms, you rotate them out without redesigning the validator from scratch.\
None of this means custodial platforms don’t add value. They absolutely do, especially for teams that want a low-friction introduction to staking. But institutions eventually move past the onboarding phase. They start caring about auditability, failure isolation, key distribution, and how the system behaves when conditions get messy. Those aren’t features you bolt on later. They come from the architecture.Proof-of-Stake wasn’t built for single-operator control. It was built for distributed participation. The closer institutional staking setups follow that pattern, the more predictable and transparent they become — not just in normal conditions but in the moments that matter.That’s why decentralized infrastructure ends up being non-negotiable. Not because it sounds good on paper, but because it delivers the reliability and clarity institutions already expect from every other critical system they run. It’s simply the architecture that scales with the network and with the responsibility that comes with meaningful stake.]]></content:encoded></item><item><title>A Year In, And It’s Time To Recognize: The Oval Office Is Empty</title><link>https://www.techdirt.com/2026/01/23/a-year-in-and-its-time-to-recognize-the-oval-office-is-empty/</link><author>Cathy Gellis</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:42:44 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[It will be to the everlasting shame of all Americans that impeachment has not yet been accomplished to formally remove Trump from office. Not in his previous term, and not this one, at least not so far. In fact, this term it has hardly even been attempted. If it weren’t for Representative Green honoring his oath of office it wouldn’t have even been tried at all. Even  are still in significant numbers joining their Republican colleagues in refusing to do what is needed to save our constitutional order, despite everything Trump has done from the moment he retook office—including taking the office, which he was ineligible to do as a confirmed insurrectionist—being entirely inconsistent with the Constitution’s instructions for how to achieve a democratically sustainable federalized union of states.Impeachment still needs to happen, for Trump and his minions, not just to cleanly expel Trump from the presidency but to disqualify him from ever returning to it. And that expulsion needs to happen with an urgency that really required it to have been completed at the latest by last March. Yet the way things are going, with Congress dragging its feet, it seems we’ll be lucky if it will even happen by  March, if at all. Moreover, with Trump upping the ante at every turn, we’ll be lucky if the nation, all its constituent states, and even most of the people who depend on the Constitution’s promises of liberty, freedom, and justice for all, are still standing by then if nothing is done to officially separate him from the powers of the office he continues to claim. After all, every day of delay is another day for a five year old to be shipped to a concentration camp in Texas. Even if the nation survives this presidency, it’s already clear we won’t all.But it turns out, Trump has already begun to separate himself from the presidency.  And that he has done so reveals another path the Constitution allows for retaking our democracy, starting now. On this appalling anniversary week of Trump’s installation as the 47th President of the United States, it is time to recognize an essential truth: he has functionally already abandoned the office. Sure, he still (nominally) lives in the White House, meets heads of state (and insults them), is answered to by the military (however ill-advisedly), signs bills (and pardons!), and at least superficially seems to be conducting the basic functions of the office. These are things that the Constitution allows presidents to do, not because, as Trump seems to think, the Constitution seeks to reward a single person with the special power to do any of them, but because these are governmental functions someone needs to do and it makes more sense to grant a chief executive the ability to do them than someone in any other branch of government.But the exercise of these functions is not the full extent of the job. The job of president, as the Constitution describes, also includes the requirements to “take care that the laws be faithfully executed,” and to fulfill the oath he swore upon taking office, which included the promise to “preserve, protect and defend the Constitution of the United States.” None of these obligations are incidental to the job; they are key counterbalances to the enormous power the position affords its occupant. Yet he has been doing none of them in any sort of meaningful way, if at all. In fact, all too often he instead does the exact opposite of what’s required by the job, including by engaging in his own criminality, abetting the malfeasance of others, and otherwise generally upending our constitutional order by ignoring statutes, treaties, and constitutional text, and turning every bit of power he’s managed to wring from his position against the very same public the Constitution says he works for.There are few situations where we would consider someone not doing what they were hired to do, and in fact doing the very opposite of what their job required, as still being employed in that job. If you hire a guard to watch the bank, you’d expect him not to help the robbers rob the bank. If you hired a doctor to treat patients, you’d expect him to not kill them instead. But if while on the job they did the opposite of what they were hired to do, you would understand them to have abandoned their position. You wouldn’t expect the guard who let in the robbers on Tuesday to still show up to work securing the place on Wednesday, or the doctor who euthanized his patients Thursday to show up to treat more on Friday; you would understand from the moment they did these things that you now have some vacancies to fill.Which is where we find ourselves. The degree to which Trump has refused to perform the requirements of his job, to say nothing of his regularly acting contrary to them, means that we effectively have a vacancy in the Executive Branch. Americans can no longer have any trust that he is working for us when he daily demonstrates that he is only working for himself. Or that he’ll enforce the law when he regularly transgresses it and enables others’ transgressions as well. Or that he’ll uphold the Constitution when he regularly violates the separation of powers and people’s protected rights. Or that he can be a protector of the country when he has used his position to attack it. Like with the larcenous bank guard or wayward doctor it would be irrational to believe that despite having acted in such conflict with the requirements of his job that it is a job he has nevertheless somehow still kept. Instead, by refusing to uphold his oath of office, and acting in so many ways counter to it, he has effectively abandoned the office he took that oath in order to enter.The Constitution says that when the office is vacant there is a succession process to fill it.  Where it is less specific is in instructing how a  vacancy, such as the kind we are experiencing, can be regarded as an official  one for purposes of triggering succession.  But it doesn’t say we can’t, and plenty of language in the Constitution says we can, and indeed must.Per Article II, Section 1, of the Constitution, succession happens when there is either a physical departure from the office by the President, such as through death or resignation, or a functional one, essentially measured by the “Inability to discharge the Powers and Duties of the said Office.” (The 12th Amendment, as amended by the 20th Amendment, also indicates that a vacancy is created when there is a “constitutional disability,” which would seem to include his ineligibility for the office as an additional obstacle to him being able to discharge the powers and duties of the office.) While for Trump there also remains the possibility of mental incapacity being yet another reason he is unable to fulfill the responsibilities of the office, in addition to his conscious abandonment of the position, it all boils down to the same thing: he has demonstrated that he is unable to continue serving in the role as the Constitution requires. The vacancy thus exists, and now it just needs to be officially recognized so that succession can begin.The 25th Amendment describes one avenue for such recognition, but that particular process seems unlikely to be pursued any time soon given that it would require equally compromised cabinet members to unite with the Vice President to support Trump’s displacement, which they are unlikely to do as long as they feel they benefit from Trump remaining in office (which is, of course, a reason why Hegseth, Rubio, Noem, Bondi, etc. should also themselves all be impeached, so that there’s a snowball’s chance that more ethical people could take their place for 25th Amendment purposes). But it seems unlikely that the Constitution meant the 25th Amendment to be the  process available for recognizing that effectively there’s already a vacancy in the presidency, for several reasons. For one thing, the way it is written it seems more attuned to articulating a plan for succession in the face of a  disability, like a coma, because it includes a mechanism by which the succession can be undone. Whereas abandoning the office, like Trump has done, does not seem, consistent with the spirit of the Constitution if not the letter, like something that can simply be undone without being re-elected. Furthermore, as we see here, the 25th Amendment does not correct for the sort of situation we find ourselves in now, where the people who could and should be invoking it are not, even though the essential problem remains: there is still no one currently at the helm of the United States of America doing the job in a way the Constitution requires. And such will remain the case regardless of whether Vance and company ever make a move to address it.Impeachment is of course another appropriate option for addressing a wayward president who is not living up to the job, but it, too, cannot be the only other means for handling a situation like this, where his failure to perform the job as required has already created the vacancy. For one thing, it suffers from a similar problem as the 25th Amendment, where the right of the public to have a president that lives up to his constitutional obligations is effectively being held hostage by recalcitrant officials—this time those in Congress—who are unwilling to uphold their own oaths of office and do what needs to be done to officially extricate America from Trump’s grasp. Furthermore, impeachment is also designed to pry someone out of a job they are actually doing, and not just someone who is not, as well as apply disqualification as a sanction. It is a mechanism useful for creating a vacancy, but the need now is just to recognize that one already exists.But that there is no other clearly established way for recognizing the vacancy does not mean there is no way. There appears to be another way.  And key to pursuing it is to stop treating as President someone who clearly is not.It would mean, first of all, challenging every bit of power Trump exercises nominally as president as being unlawful, and not just on its own terms as an act not permitted by statute or Constitution, given that most of the things he tries to do would still be unlawful even if a proper president tried to do them.  The challenge needs to be that  Trump does ostensibly as president is irredeemably illicit at its core.  Give the courts the opportunity to at last find that whatever power Trump attempts to wield is power he no longer has. Doing so would likely be an uphill battle, because no court has every nullified a presidency.  To the extent that legitimacy has been in contention, the historical preference has been to settle the matter politically, rather than legally—or at least it was, up until , when the Supreme Court announced that the courts were in the president-anointing business.  But it would make sense for the courts to be able to weigh in here, with respect to Trump, because why shouldn’t the Article III branch would have its own mechanism for addressing the vacancy of an absent president, especially while Articles I and II officials continue to abandon their own obligations to act in accordance with their own constitutional mechanisms.  No branch should have an exclusive monopoly on policing the president, and as long as there has been judicial review, none has.  The courts have long been able to hold presidents accountable to the Constitution.  And while there may be no clearly established roadmap for involving the courts this way, there is also nothing preventing it. The courts could be called upon to declare the office abandoned in various ways, and in response to challenges by various parties. Perhaps such an opportunity to challenge Trump’s legitimacy could arise if JD Vance gets ambitious and sues for a declaratory judgment that he is the actual president, because, while he’s no prize himself, at this point it certainly seems like he has a better claim to the office than Trump does. Perhaps it’s the states who can bring some sort of claim. Perhaps others who are affected by Trump’s abuse could sue too, just as they normally can challenge the lawfulness of his acts. Or perhaps the courts will have to weigh in when the military starts refusing Trump’s orders, as increasingly seems it likely will, as the ways Trump has been directing the military become more and more unlawful even on their own merits. In any case, one way or another it seems inevitable that the legitimacy of Trump’s continued presidency is going to be a question the courts will be called upon to answer, especially as the rest of our government refuses to.And while any litigation would eventually land at the Supreme Court, such as it is, these challenges still need to be pursued because every case before it ultimately stands on its own.  Even  is differentiable in key ways from the litigation that would reach it here.  And hope springs eternal that this time maybe the Court will even get the question before it right, as the stakes raised by these challenges have never been more clear.  Trump is running around acting with impunity, but as even the Supreme Court recognized in , immunity only attaches to official acts.  And if he has already effectively abandoned the office, then none of his acts can be.It is of course no small thing for anyone to declare a living president to have officially abandoned his office or otherwise take steps to delegitimize his occupancy in the office. Nor should it be something that easily can be done because, as we’ve seen with even just with 2020 election denial, once doubt creeps in about who is the legitimate president, the disagreement it causes can be destabilizing to our democratic order. In fact, it is likely that a big reason why Trump’s continued claim to the presidency has simply been accepted so broadly up to now, despite all the evidence, is that, by and large, we would rather delude ourselves into believing that he is the legitimate officeholder than risk the political instability of calling it into question.Nevertheless, there are limits to how long we can maintain the myth of his legitimacy, which Trump has been daily making less and less believable. Hegemony is powerful; Trump only gets to masquerade as a legitimate president for as long as we let him. We don’t have to let him. Which is why we should appeal to the courts, as well as Congress and any politician anywhere in government, to argue not just that Trump should be made to leave but that he’s already left, and that it’s finally time for the government to respond to that reality.It’s time to challenge his legitimacy because the Constitution does not take a time out.  It does not wait for midterms.  We are  entitled to a President that acts consistently with all of the Constitution’s requirements, and it tells us what happens when there isn’t someone doing so anymore. It is not for any of us to decide that this language suddenly somehow no longer applies.In fact, it would be dangerous to, or to deliberately wait months and months to finally address the problem, while in the meantime our nation and everything we’ve built over the course of nearly 250 years is ruined. Especially not when Trump’s abandonment of the job has created the exigent likelihood that an interloper without any personal constitutional authority may now be functionally acting as president instead of him, wielding the office’s powers without any of the accountability the Constitution normally requires of someone in that position. In other words, it may not be that we are just without a president but, worse, instead at the mercy of an unelected pretender who has stepped into the vacuum Trump’s abandonment has created because we have refused to fill that void first.There may of course be the fear that we risk a constitutional crisis to make such a serious move to deem the office vacant when the Constitution is not more specific that it is a move to be made. And it’s true; constitutional crises arise when we start making the most existentially important decisions about the nation’s governance without reference to a set of clear rules we’ve all agreed to. That we are in uncharted waters may thus give pause.But we are not without any instruction for how to navigate them. Even though the Constitution has not provided a specific process to follow perfectly tailored to this effective abandonment of the presidency that Trump has committed, it still provides enough guidance to recognize the position is vacant and proceed with succession accordingly. If anything, it is the refusal to recognize the vacancy, especially by Congress and the cabinet, that has been what’s unilaterally and unconstitutionally changed the rules we’ve all previously agreed to, by letting Trump nevertheless continue to occupy the position when he has in every other way abandoned the job. Given everything Trump has done, and the actual text of the Constitution forbidding it, challenging his right to remain the acknowledged president won’t invite a constitutional crisis; rather, it is the failure to bring that challenge which is why that crisis is already here.]]></content:encoded></item><item><title>The Future of Media Is Automated: Lior Alexander’s Vision for Information Infrastructure</title><link>https://hackernoon.com/the-future-of-media-is-automated-lior-alexanders-vision-for-information-infrastructure?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As artificial intelligence accelerates the creation of content, the internet has entered an era defined less by scarcity and more by saturation. Millions of new posts, research papers, and media artifacts appear daily, making discovery, not creation, the primary challenge. For , founder of , this shift marked the beginning of his work.Rather than asking how to produce more content, Alexander focused on a more fundamental question: how do people find what actually matters when everything is being generated at once?From Research Lab to System Builder’s path into AI began in 2017, when he joined the research lab of Turing Award–winning scientist Yoshua Bengio in Montreal. At the time, machine learning research was expanding rapidly, but access to meaningful insights remained limited.“Hundreds of papers were being uploaded every week,” Alexander recalled. “There was no effective way to filter them. Even researchers inside the lab were overwhelmed.”That experience shaped his thinking. Instead of focusing on model development alone, Alexander became interested in the infrastructure surrounding knowledge itself: how information is surfaced, ranked, and interpreted. He began experimenting with tools that could track research activity across the web, identify emerging signals, and surface them in a usable way.That early system would become the foundation of AlphaSignal.AlphaSignal was not designed as a traditional media company. From the beginning, it was structured as an automated system capable of detecting, ranking, and contextualizing information at scale. While most newsrooms rely on teams of editors and writers, AlphaSignal relies on software.“I built the entire system myself,” Alexander said. “The ranking models, the data pipelines, the publishing workflows, the branding. I didn’t have a team or outside funding.”The platform continuously scans technical papers, product releases, funding announcements, and research activity, identifying patterns that signal meaningful developments. Instead of reacting to trends after they peak, AlphaSignal aims to detect momentum early.That approach has proven effective. The platform now reaches more than 250,000 subscribers, over 500,000 followers, and has generated more than 200 million impressions. It also became an early visibility engine for companies such as ElevenLabs and Lovable, helping surface them before they reached mainstream attention.Running AlphaSignal as a solo operation forced Alexander to rethink how media organizations function. Rather than scaling through headcount, he focused on automation and system design.“I had to do everything: engineering, research, distribution, partnerships,” he said. “The only way to make that sustainable was to build systems that could operate without constant human input.”This approach mirrors the broader shift he sees happening across industries: replacing manual workflows with intelligent systems capable of handling complexity at scale. In his view, the future belongs to organizations that treat information processing as infrastructure, not editorial labor.Today, AlphaSignal functions less like a publication and more like an intelligence layer for the AI ecosystem. Its tools identify emerging trends, map technical progress, and help engineers, investors, and researchers understand where innovation is actually occurring.Looking ahead, Alexander plans to expand the system beyond AI into other sectors facing similar overload, including finance, cybersecurity, and biotechnology. His long-term goal is to build what he describes as a “universal signal engine” — a platform capable of ranking relevance across any domain overwhelmed by information.We’re entering a period where most content will be machine-generated,” he said. “The real value won’t be in producing more of it, but in building systems that help people understand what matters.For Alexander, that challenge defines the next era of media, one where clarity, not volume, becomes the most valuable commodity.]]></content:encoded></item><item><title>Toilet Maker Toto&apos;s Shares Get Unlikely Boost From AI Rush</title><link>https://slashdot.org/story/26/01/23/1855256/toilet-maker-totos-shares-get-unlikely-boost-from-ai-rush?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Shares of Japanese toilet maker Toto gained the most in five years after booming memory demand excited expectations of growth in its little-known chipmaking materials operations. The stock surged as much as 11%, its steepest rise since February 2021, after Goldman Sachs analysts said Toto's electrostatic chucks used in NAND chipmaking will likely benefit from an AI infrastructure buildout that's tightening supplies of both high-end and commodity memory. 

[...] Known for its heated toilet seats, the maker of washlets has for decades been part of the semiconductor and display supply chain via its advanced ceramic parts and films. Its electrostatic chucks -- which it began mass producing in 1988 -- are used to hold silicon wafers in place during chipmaking while helping to control temperature and contamination, according to the company. The company's new domain business accounted for 42% of its total operating income in the fiscal year ended March 2025, Bloomberg-compiled data show.]]></content:encoded></item><item><title>Why Short-Lived Certificates Are Revolutionizing Security in Modern Infrastructure</title><link>https://hackernoon.com/why-short-lived-certificates-are-revolutionizing-security-in-modern-infrastructure?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:15:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Security engineers often joke that certificates are invisible until they break something important. Yet in modern infrastructure, certificates quietly enable nearly every secure interaction. From service-to-service communication to machine identity, they form the backbone of digital trust. What has changed is not their importance, but how long they are allowed to exist.Across the industry, long lived certificates are giving way to ephemeral certificates that are short lived, automated, and continuously rotated. This shift reflects a growing recognition that static trust models struggle to keep pace with distributed systems that evolve continuously.The evolution toward ephemeral certificate management has emerged through sustained dialogue across professional communities. Engineers and security leaders exchange experiences in British Computer Society forums, Gartner peer discussions, Forbes Technology Council conversations, and IEEE conferences where practical challenges are discussed openly.Within these discussions, Arun Kumar Elengovan is frequently referenced for bringing clarity to how certificate management fits within broader trust architecture. A Director of Engineering Security for an identity security focused organization, he has led and contributed to large scale security programs across complex environments. An award-winning leader with recognition spanning the United States, Canada, Indonesia, Thailand, India, Malaysia, and Australia, he is widely regarded as a distinguished contributor in ephemeral certificate management. His work consistently highlights how short-lived trust models strengthen security posture while improving operational reliability when applied with architectural discipline.His continued engagement across professional councils and technical forums has helped shape a shared understanding that certificate automation is no longer an optional enhancement. It is increasingly viewed as a foundational capability that security leaders must guide deliberately as infrastructure scales.The Fragility of Long-Lived TrustTraditional certificate practices were designed for a slower era. Certificates were issued manually, embedded into applications, and rarely rotated. In discussions across the security engineering community, Arun Kumar Elengovan has pointed out that this model was workable when environments were small and change was infrequent, but its assumptions no longer hold in modern infrastructure.Today, organizations operate across hybrid cloud platforms, microservices, container clusters, serverless workloads, and third-party integrations. Each layer introduces credentials that must be issued, stored, rotated, and retired safely. Arun has emphasized that when certificates persist for extended periods, compromise often remains unnoticed, revocation becomes slow, discovery incomplete, and operational risk accumulates without clear visibility.Security incidents increasingly show that failures do not arise from cryptographic weaknesses, but from credentials that remain active long after their intended use. Across professional and technical forums, this pattern reflects a broader understanding that the durability of trust, rather than cryptographic strength alone, is what most often undermines security in large scale systems.Ephemeral Certificates and Trust AgilityEphemeral certificates offer a different path forward. Rather than embedding trust permanently into systems, trust is applied dynamically at runtime. Certificates are issued only when needed, rotated automatically, and replaced frequently enough to significantly reduce exposure windows.This approach supports trust agility. Applications no longer hold long term credentials. Instead, trust decisions are centralized and enforced consistently across environments. Root of trust remains protected offline, while intermediate trust is delegated safely through automation.The result is a security posture that adapts as systems change rather than falling behind them.Automation as a Foundational RequirementEphemeral certificates cannot function without automation. Discovery, issuance, renewal, revocation, and monitoring must operate continuously. In large environments, organizations often lack a complete inventory of certificates until they actively search for them.Effective automation reflects operational reality. Certificates appear in code repositories, build pipelines, configuration files, network services, and legacy systems. Some applications refresh credentials seamlessly, while others require coordination. Mature certificate programs align rotation with engineering workflows rather than forcing disruption.Automation transforms certificate management from a brittle manual process into a dependable engineering capability.From Certificates to Systems ThinkingOne of the most important shifts in modern security engineering is moving away from treating certificates as isolated artifacts. Certificates intersect with identity systems, secrets management, cloud platforms, and governance frameworks.Issuance relies on private certificate authorities. Storage integrates with secrets systems. Access decisions depend on platform identity. Root of trust choices determine what remains offline and what can be automated safely. Through community discussions and technical exchanges, Arun consistently provides direction on evaluating these dependencies as a unified trust system rather than disconnected controls.Thinking in systems rather than tools enables organizations to design trust that grows with infrastructure instead of resisting it. This architectural perspective has increasingly influenced how security leaders frame certificate management decisions.Why This Matters for Engineers and OrganizationsEphemeral certificates reduce blast radius, shorten exposure windows, and simplify recovery. They also influence behavior. Engineers begin to expect rotation rather than fear it. Credentials are requested dynamically rather than copied. Trust becomes observable and measurable.Arun often underscores that this behavioral shift is as important as the technical controls themselves. Secure systems emerge when teams are given clear direction, consistent patterns, and accountability rather than ad hoc rules.As systems become more distributed, trust must become more dynamic. Automation, resilience, and observability are no longer optional attributes.Trust That Keeps Pace With ChangeAs digital infrastructure continues to evolve, static trust models fall behind. Arun Kumar Elengovan has noted that ephemeral certificates represent a practical response to this reality, aligning security mechanisms with the way modern systems are actually built and operated rather than how they were designed in earlier eras.He has also observed that ongoing conversations across professional communities increasingly converge on short lived trust as a baseline expectation rather than an advanced practice. According to Arun, trust that is automated and intentionally temporary reduces risk while increasing operational confidence, particularly in large scale and highly distributed environments.In this context, ephemeral certificates are not merely a technical improvement. They reflect a leadership driven understanding that security must move at the same pace as the systems it protects, or risk becoming an obstacle rather than an enabler."The views and opinions expressed in this article are the author’s own and do not necessarily reflect those of any affiliated organizations or institutions."]]></content:encoded></item><item><title>The Rippling/Deel corporate spying scandal may have taken another wild turn</title><link>https://techcrunch.com/2026/01/23/the-rippling-deel-corporate-spying-scandal-may-have-taken-another-wild-turn/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:13:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Department of Justice may be conducting a criminal investigation. This is arguably the biggest drama between two HR startups ever.]]></content:encoded></item><item><title>Wine 11.1 Released In Kicking Off The New Development Cycle</title><link>https://www.phoronix.com/news/Wine-11.1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:12:17 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following the release of Wine 11.0 stable just under two weeks ago, Wine 11.1 is now available as the first of the bi-weekly development snapshots for Wine in leading toward the Wine 12.0 release next January...]]></content:encoded></item><item><title>The Great Graduate Job Drought</title><link>https://slashdot.org/story/26/01/23/0925259/the-great-graduate-job-drought?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Global hiring remains 20% below pre-pandemic levels and job switching has hit a 10-year low, according to a LinkedIn report, and new university graduates are bearing the brunt of a labor market that increasingly favors experienced candidates over fresh talent. 

In the UK, the Institute of Student Employers found that graduate hiring fell 8% in the last academic year and employers now receive 140 applications for each vacancy, up from 86 per vacancy in 2022-23. US data from the New York Federal Reserve shows unemployment among recent college graduates aged 22-27 stands at 5.8% versus 4.1% for all workers. 

Recruiter Reed had 180,000 graduate job postings in 2021 but only 55,000 in 2024. In a survey of Reed clients last year, 15% said they had reduced hiring because of AI. London mayor Sadiq Khan said the capital will be "at the sharpest edge" of AI-driven changes and that entry-level jobs will be first to go.]]></content:encoded></item><item><title>What to know about Netflix’s landmark acquisition of Warner Bros.</title><link>https://techcrunch.com/2026/01/23/what-to-know-about-netflixs-landmark-acquisition-of-warner-bros/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:31:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Learn more about Netflix's acquisition of Warner Bros., considered the most historic megadeal in Hollywood, as it continues to develop.]]></content:encoded></item><item><title>Researchers say Russian government hackers were behind attempted Poland power outage</title><link>https://techcrunch.com/2026/01/23/researchers-say-russian-government-hackers-were-behind-attempted-poland-power-outage/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:17:24 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Security researchers have attributed the attempted use of destructive "wiper" malware across Poland's energy infrastructure in late December to a Russian-backed hacking group known for causing power outages in neighboring Ukraine.]]></content:encoded></item><item><title>Noem Says ICE Is Being Menaced By Ice Cubes, Protesters Should Be Cooped Up In ‘Free Speech Zones’</title><link>https://www.techdirt.com/2026/01/23/noem-says-ice-is-being-menaced-by-ice-cubes-protesters-should-be-cooped-up-in-free-speech-zones/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:16:21 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[CBS News still exists, despite a president doing all he can to turn it into his own Baghdad Betty. Journalists are still demanding answers from this administration, even while the Baghdadest Betty of all — Bari Weiss — does everything she can to strip mine the long-running news agency for abusable parts. (That refers to you, Tony Dokoupil.)Last Sunday morning, Margaret Brennan interviewed DHS head Kristi Noem on “Face the Nation.” Brennan did everything she could to push back against Noem’s false claims and bullshit assertions, but in the end, Noem clearly knew she’d always have the upper hand, thanks to Trump’s legal threats and Bari Weiss’s willingness to bury reporting that doesn’t please Trump.As ICE continues to detain, arrest, or kill anyone that seems to be too dark or too loud in Minnesota, Brennan asked if there’s an actual end point for yet another federal “surge” targeting a “blue” state. Noem, of course, can’t provide a straight answer despite being given straight facts by the interviewer.MARGARET BRENNAN: According to Pew, Minnesota’s population of immigrants here illegally stands at 2.2%. So, how do you judge when you’ve gotten everyone off the streets, that you say is, you know, requiring your federal agents be there? How do you say we’ve had mission accomplished?SEC. NOEM: Well, we won’t stop until we are sure that all the dangerous people are picked up, brought to justice and then deported back to their home countries–MARGARET BRENNAN: –You don’t have a number or a date?– SEC. NOEM: –We wouldn’t be in this situation- We wouldn’t be in this situation if Joe Biden hadn’t allowed our open-border policies to be in place and allowed up to 20 million people unvetted into this country. We have no idea how many dangerous people are here. That’s not an answer. Most of what fell out of Noem’s mouth during this interview wasn’t a direct answer. Instead, it was a bunch of Trump-esque rambling, randomly punctuated by Noem insisting the person interviewing her was lying. Noem insisted that the “millions” (most of which obviously do not reside in Minnesota) being swept up by ICE were violent criminals. She claimed “70% of them have committed or have charges against them on violent crimes.” Brennan pushed back, citing stats released : MARGARET BRENNAN: Okay, well, our reporting is that 47% based on your agency’s own numbers, 47% have criminal convictions against them*. But let’s talk about the other numbers–SEC. NOEM: –Which means you’re wrong again. Absolutely. We’ll get you the correct numbers–SEC. NOEM: –so you can use them in the future.MARGARET BRENNAN: Well, that’s from your agency. Noem is fully cooked. She’s indistinguishable from Trump or anyone else in his close orbit. When your lies are exposed by facts, you call the person with the actual facts a liar. But this willful disregard for truth is nothing new: this administration divorced itself from reality during Trump’s first term. In its second term, it’s pretending truth is whatever it says it is. But it gets scarier, stupider, and weirder from there. Here’s Noem defending murdering citizens on the street before veering off into an extremely Trumpian interpretation of First Amendment rights: SEC. NOEM: We’ve seen over 100 different vehicle weaponized and attacking law enforcement officers. I would hope that Mayor Frey, when he’s on here, that he’ll announce that he’s going to start working with us to bring safety to the streets. If he would set up a peaceful protest zone so that these individuals can exercise their First Amendment rights and do so peacefully, we would love that, because then we could work together to make sure we’re getting criminals to justice and letting people still express their First Amendment rights.While the government does have extremely limited powers to enact time-and-place restrictions on First Amendment activity, it certainly does not have the power to force any locale to restrict protests to only the places the government will allow protesters to gather. That’s the exact opposite of the First Amendment, which is exactly the sort of thing you’d expect a Trump administration figure to pitch, even if it would never impose restrictions like these on anyone protesting in  of Trump and his policies. (See also: hundreds of pardoned people who engaged in literal insurrection in 2021.) After bitching Brennan out for repeating the name of the officer (Jonathan Ross) who killed Renee Good (apparently it’s “doxxing” to use a published name during a national news interview [massive eye roll]), Noem goes on to claim (without facts in evidence) that ICE officers are dealing with threats up to (and including) frozen water: SEC. NOEM: –Don’t say his name. I mean, for heaven’s sakes, we- we don’t- we shouldn’t have people continue to dox law enforcement when they have an 8,000%–MARGARET BRENNAN: –his name is public–SEC. NOEM: –increase in death threats against them–MARGARET BRENNAN: — he was struck and hospitalized–SEC. NOEM: –I know, but that doesn’t mean it should continue to be said. His life- he got attacked with a car that was trying to take his life, and then people have attacked him and his family, and they are in jeopardy. And we have law enforcement officers every day who are getting death threats and getting attacked at their hotels and they are–MARGARET BRENNAN: –Well, can you tell me about his status right now–SEC. NOEM: –getting ice thrown at them. I can’t imagine why people might be throwing ice at ICE, but I’m sure someone much smarter than me will make that connection. And I may be just a humble small town writer who writes like your average George Bailey, but I have to imagine this might have gone better for Noem if she had decided to end that answer one sentence earlier. This would all be laughably surreal if this administration didn’t have so much power and the will to abuse it. It’s still surreal, but you have to embrace the blackest of comedy to croak out a laugh. This administration only knows two moves: bluster and gaslighting. Whatever you saw, you didn’t see. Whatever violations the government committed never happened. Whatever can be disputed by facts is just the ravings of leftist liars and mainstream media losers. As for everyone caught in this crossfire, fuck ’em. This party only serves itself. If there’s any silver lining here at all, it’s that Noem is too busy being Trump’s Bigot Barbie to kill her children’s pets any time soon. ]]></content:encoded></item><item><title>Wall Street Pushes Solo 401(k)s as More Americans Work for Themselves</title><link>https://news.slashdot.org/story/26/01/23/1218254/wall-street-pushes-solo-401ks-as-more-americans-work-for-themselves?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: A niche retirement plan favored by freelancers is quickly becoming a hot Wall Street sales pitch, as more and more Americans look for ways to shelter a bigger chunk of their paychecks from taxes. Known as solo 401(k)s, they allow the self-employed to contribute $72,000 a year into tax-advantaged retirement accounts. That's nearly three times the maximum for typical salaried workers in the US. 

While they've existed for decades serving a workforce that often struggled to earn enough to max out those contributions, wealth planners like JPMorgan Chase & Co. and Betterment are now racing to tap into burgeoning demand from a newer, and wealthier cohort: Post-pandemic contractors and self-employed DIY savers looking to shelter more income, grow assets tax-deferred or tax-free, all with the click of a button. 

The pitch is simple: Because of a quirk in the tax code, self-employed workers effectively contribute twice to their 401(k)s -- once as an employee on their own behalf and then again as a business owner making matching contributions. The platforms take care of the paperwork and clients get institutional-level tax planning and investment flexibility. More than three-quarters of America's record 36 million small businesses now have just a single employee, the owner. Cerulli Associates projects that total 401(k) plans in the U.S. will surpass 1 million by 2030, and the fastest growth is expected in sub-$5 million "micro" accounts.]]></content:encoded></item><item><title>How did Davos turn into a tech conference?</title><link>https://techcrunch.com/video/how-did-davos-turn-into-a-tech-conference/</link><author>Theresa Loconsolo</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The World Economic Forum’s annual meeting in Davos felt different this year, and not just because Meta and Salesforce took over storefronts on the main promenade. AI dominated the conversation in a way that overshadowed traditional topics like climate change and global poverty, and the CEOs weren’t holding back. There was public criticism of trade policy, warnings about AI […]]]></content:encoded></item><item><title>China Makes Too Many Cars, and the World Is Increasingly OK With It</title><link>https://tech.slashdot.org/story/26/01/23/1213224/china-makes-too-many-cars-and-the-world-is-increasingly-ok-with-it?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 19:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[After years of Western governments raising alarms about Chinese automotive overcapacity and erecting tariff barriers, an unexpected pivot is now underway as major economies cautiously open their markets to Chinese electric vehicles, Bloomberg writes. Beijing itself has started acknowledging the problem at home. Chinese regulators last week warned of "severe penalties" for automakers defying efforts to rationalize pricing in the country's car market, and earlier this month a government ministry urged battery makers to curtail expansion and cutthroat competition. 

The European Union imposed steep tariffs on Chinese EV imports in 2024 and is now considering replacing them with minimum import price agreements. Canada's Prime Minister Mark Carney last week decided to allow 49,000 Chinese EVs annually at a 6.1% tariff rate, removing a 100% surtax. Germany announced this week that its $3.5 billion EV subsidy program will be open to all manufacturers including Chinese brands. Germany's environment minister Carsten Schneider dismissed concerns during a January 19 press conference: "I cannot see any evidence of this postulated major influx of Chinese car manufacturers in Germany, either in the figures or on the roads." 

BYD registered an eightfold increase in sales in Germany last year and pulled ahead of Tesla, though Volkswagen still registered around 2,300 vehicles for every one BYD sold.]]></content:encoded></item><item><title>Firmware Upstreamed For Audio Support With Upcoming Dell &amp; Lenovo Panther Lake Laptops</title><link>https://www.phoronix.com/news/Cirrus-CS42L45-Linux-Firmware</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:53:02 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Similar to the new Intel IPU 7.5 firmware upstreamed for Panther Lake this week, Cirrus has upstreamed their CS42L45 codec firmware for upcoming Dell and Lenovo laptops making use of this audio codec...]]></content:encoded></item><item><title>Solar and Wind Overtake Fossil Fuels in the EU</title><link>https://hardware.slashdot.org/story/26/01/23/127254/solar-and-wind-overtake-fossil-fuels-in-the-eu?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:44:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Wind and solar power overtook fossil fuels last year as a source of electricity in the EU for the first time, a new report found. Semafor adds: The milestone was hit largely thanks to a rise in solar power, which generated a record 13% of electricity in the EU, according to Ember. Together, wind and solar hit 30% of EU electricity generation, edging out fossil fuels at 29%. 

The shift is especially important with the bloc's alternative to Russian LNG -- Washington -- becoming increasingly unreliable and willing to weaponize economic tools. The US Commerce Secretary threw shade at the bloc's renewable push during Davos, warning that China uses net zero goals to make allies "subservient" by controlling battery and critical mineral supply chains. 

Still, renewables now provide nearly half of EU power, with wind and solar outpacing all fossil sources in more than half of member countries. "The stakes of transitioning to clean energy are clearer than ever," the Ember report's author said.]]></content:encoded></item><item><title>Got Ideas For Growing The Open Social Web? Bring Them.</title><link>https://www.techdirt.com/2026/01/23/got-ideas-for-growing-the-open-social-web-bring-them/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:35:04 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[For over three years now, since Elon Musk decided to spend $44 billion turning Twitter into his personal playground, we’ve been watching the open social web slowly, sometimes painfully, come into its own. Bluesky. Mastodon. The broader ATmosphere and fediverse along with a few other experiments (nostr! farcaster!). These aren’t just tech experiments anymore—they’re real alternatives that millions of people use every day.While these open social systems are working, and working well, tons of people are still choosing to stay in closed, proprietary, billionaire-controlled systems, where they have no control, no say in how they work, and no real agency. We’ve heard various excuses. We’ve heard about the pull of inertia. We’ve even heard the complaints that people haven’t found communities they like… or that they actively dislike some of the communities that have formed.So instead of just writing another post about why that matters (I’ve written plenty), Johannes Ernst from FediForum and I are doing something about it. On , we’re hosting an online “un-workshop” focused on one question: how do we actually grow the open social web even more?And, yes, I’m on the board of Bluesky, but this isn’t Bluesky specific. We want an open discussion and brainstorming on growing the wider open social web.This isn’t your standard conference where you sit through presentations and nod politely. It’s a participatory event built around the FediForum unconference model, though modified to be more of an ongoing brainstorming workshop (not unlike the Greenhouse events we’ve run here in the past).Before the event, participants can submit short position papers—your experiences, your ideas, your proposals for what might actually work to engage more people on open social systems. We’ll cluster those into topics and spend the actual event  them and brainstorming around them, not just listening to people talk at you.Here’s the thing: we want people who have real ideas and experience. People who have tried (and maybe failed) to get their friends onto the open social web and learned something useful from it. People who  had success convincing entire communities. People running organizations who are trying to figure out how to make the jump. Builders who want more users. Advocates who have done actual research with actual humans about what’s working and what isn’t.What we don’t need are more cynical hot takes about why the open social web will never work. If you’ve already decided it’s a lost cause, this isn’t the event for you. Go post about it on Threads or whatever. We also don’t need hot takes about how you’re glad most people don’t use the open social web. That’s great for you open social hipsters, but some of us think it’s important to get more people to recognize the power of open social.So, for everyone else—the people who believe this matters and want to figure out how to make it happen—we want to hear from you.The event will run from 8am to noon Pacific (which means Europeans can actually attend without setting an alarm for 3am), and registration is open now. The event will be run online, using Remo, a tool we’ve used in the past for online events, that is conducive to small group discussions and brainstorming.Position paper submissions are due by February 16th, and while they’re not required, they’re strongly encouraged (you can submit them during the registration process). The whole point is to come prepared to engage, not just spectate.Look, I’ve been writing about the importance of protocols over platforms for years now. The open social web represents one of the few genuine shots we have at building online spaces that aren’t controlled by a handful of companies (or their billionaire owners) making decisions based on whatever serves their interests that week. But potential doesn’t matter if we can’t translate it into much wider adoption.So if you’ve got ideas—real ideas, not just complaints—about how to get there, come share them.]]></content:encoded></item><item><title>Daily Deal: Luminar Mobile for iOS And Android</title><link>https://www.techdirt.com/2026/01/23/daily-deal-luminar-mobile-for-ios-and-android/</link><author>Daily Deal</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:30:04 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Luminar Mobile is your all-in-one creative companion designed for iOS, Android OS, and Chrome OS. Powered by an intuitive, touch-responsive interface, it lets you enhance photos effortlessly—anytime, anywhere. Whether you’re adjusting lighting, perfecting portraits, or adding artistic flair, Luminar Mobile delivers pro-level results in the palm of your hand. It’s on sale for $20.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Toronto Man Posed as Pilot To Rack Up Hundreds of Free Flights, Prosecutors Say</title><link>https://news.slashdot.org/story/26/01/23/123218/toronto-man-posed-as-pilot-to-rack-up-hundreds-of-free-flights-prosecutors-say?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A Toronto man posed as a pilot for years in order to fool airlines into giving him hundreds of free flights, prosecutors have alleged, in a case that has prompted comparisons to the Hollywood thriller Catch Me If You Can. From a report: Authorities in Hawaii announced this week that Dallas Pokornik, 33, had been charged with wire fraud after he allegedly fooled three major US carriers into giving him free tickets over a span of four years. 

Airlines typically offer standby tickets to their own staff and those with rival airlines as a way of ensuring the broader industry can effectively move employees across continents. According to court documents, Pokornik was a flight attendant for a Toronto-based airline from 2017 to 2019, but then used an employee identification from that carrier to obtain tickets, "which he in fact knew to be fraudulent at the time it was so presented." 

The only Toronto-based airline, Porter, told reporters it was "unable to verify any information related to this story." On one occasion, Pokornik is alleged to have requested a jumpseat in an aircraft's cockpit, which are normally reserved for off-duty pilots, even though he was not a pilot and did not have an airman's certificate. Federal rules prohibit the cockpit jumpseats from being used for leisure travel.]]></content:encoded></item><item><title>Behind the Blog: Signs of the Times</title><link>https://www.404media.co/behind-the-blog-signs-of-the-times/</link><author>Samantha Cole</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/01/nl1.23.png" length="" type=""/><pubDate>Fri, 23 Jan 2026 17:53:28 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[This is Behind the Blog, where we share our behind-the-scenes thoughts about how a few of our top stories of the week came together. This week, we discuss stances on AI, a conference about money laundering, and signs about slavery coming down.Last week we published my interview with the Wikimedia Foundation CTO Selena Deckelmann. I was happy to talk to her because she’s uniquely positioned to talk about generative AI’s impact on the internet both as the CTO of the website that creates some of the most valuable training data, and one of the sites that’s threatened by generative AI output the most. ]]></content:encoded></item><item><title>ICE Is So Bad At Immigration Enforcement That It’s Detaining Native Americans</title><link>https://www.techdirt.com/2026/01/23/ice-is-so-bad-at-immigration-enforcement-that-its-detaining-native-americans/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:21:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Obviously, that’s not how things are supposed to work here in America, which proudly considered itself to be a melting pot (albeit belatedly and after a lot of post-Civil War legislation and jurisprudence). What makes America great is the blend of people in it. And, because this nation is so large, there’s plenty of room for everyone and no non-bigot will ever claim the addition of migrants has somehow made us weaker. ICE has always been awful. It’s been even worse recently, now that it knows no one in the administration will ever prevent it from being the racist throwback Trump clearly wishes it to be. It’s even bolder now that the Supreme Court — via Justice Kavanaugh’s shadow docket concurrence — said it’s ok to engage in racial profiling.The detention of at least five men in and around Minneapolis has sparked an outcry among Native American groups about Indigenous people being racially profiled as undocumented immigrants by federal immigration agents. Minneapolis is one of the largest urban centers for Native Americans in the United States.When you’re rounding up Native Americans, you’re rounding up the people who have done the least amount of immigration ever. Anyone engaged in these arrests has migrated more times than the people they’re arresting. This — along with the recent murder of Minnesota native and US citizen Renee Good by ICE officer Jonathan Ross — should have been enough to make ICE tuck its tail between its legs and head off to a more receptive, red-coded locality.It didn’t. And because ICE neither understands nor cares, it’s up to regular American citizens to point out the obvious: “It is deeply offensive and ironic that the first people of this land would be subjected to questions around their citizenship,” Jacqueline De Leon, senior staff attorney at the nonprofit Native American Rights Fund and a member of the Isleta Pueblo. “Yet nevertheless, that is exactly what we’re seeing.”You’d think someone at ICE might want to pull back and reassess the situation, especially now that seemingly the entirety of the city of Minneapolis is willing to hassle officers into abandoning the random roll-ups on darker skinned people they constantly claim are “targeted stops.”If these truly were “targeted stops,” they wouldn’t have targeted people who have far more right to be here than the people detaining them. Jose Rodriguez, a 20-year-old Red Lake Nation descendant, was arrested by ICE in what ICE claims was a “high-risk immigration enforcement stop.” (The officers also claimed to have been “violently assaulted” by Rodriguez but, tellingly, no charges have been filed.)This was followed up by the detaining of four unhoused tribal members by ICE officers, who found them sleeping under a bridge and decided this — combined with presumably darker-than-white skin tones — was all that was needed to justify some “papers please” hassling, immediately followed by detentions that, at press time (January 14)  hadn’t been ended. (One of the four was released prior to publishing.)And it’s not like Native Americans didn’t see this coming. They read the Kavanaugh concurrence and saw what’s been happening all over this nation (but  in “blue” states) and let their fellow Americans know that they should expect ICE to treat them like any other “brown” person officers come across:A day before Ramirez’s stop, the Red Lake Tribal Council issued a Jan. 7 advisory about the Trump administration’s enforcement in Minnesota. “We all need to be extra careful, and we must assume that ICE will not protect us,” the advisory said.It’s been obvious since the inception of this so-called “immigration enforcement” surge: anyone not white would be rounded up. The Supreme Court said this is all very cool and very lawful. And the surge in Minnesota is proving that being white is no protection either, not if you’re opposed to what this regime is doing. With threats of a military deployment to Minnesota looming, no American worth their citizenship should continue pretending this is anything more than white nationalism draping itself in executive power. ]]></content:encoded></item><item><title>Apple&apos;s Secret Product Plans Stolen in Luxshare Cyberattack</title><link>https://apple.slashdot.org/story/26/01/23/1017203/apples-secret-product-plans-stolen-in-luxshare-cyberattack?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: The Apple supplier subject to a major cyberattack last month was China's Luxshare, it has now emerged. More than 1TB of confidential Apple information was reportedly stolen. 

It was reported in December that one of Apple's assemblers suffered a significant cyberattack that may have compromised sensitive production-line information and manufacturing data linked to Apple. The specific company targeted, the scope of the breach, and its operational impact were unclear until now. The attack was first revealed on RansomHub's dark web leak site on December 15, 2025, where the group claimed it had encrypted internal Luxshare systems and exfiltrated large volumes of confidential data belonging to the company and its customers. 

The attackers warned that the information would be publicly released unless Luxshare contacted them to negotiate, and accused the company of attempting to conceal the incident. According to the attackers' claims, the exfiltrated material includes vital files such as detailed 3D CAD product models and high-precision geometric files, 2D manufacturing drawings, mechanical component designs, circuit board layouts, and internal engineering PDFs. The group added that the large archives include Apple product data as well as information belonging to Nvidia, LG, Tesla, Geely, and other major clients.]]></content:encoded></item><item><title>Google Photos’ latest feature lets you meme yourself</title><link>https://techcrunch.com/2026/01/23/google-photos-latest-feature-lets-you-meme-yourself/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:02:22 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The addition is meant to just be a fun way to explore your photos and experiment with Google's Gemini AI technology, and specifically Nano Banana. ]]></content:encoded></item><item><title>Video Friday: Humans and Robots Team Up in Battlefield Triage</title><link>https://spectrum.ieee.org/darpa-triage-challenge-robot</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MzEzMDYwNi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgyMDkzOTcwNX0.E2-OASseNEHgg5URtht8j77vTtpq355pxr_uvNN-ZGI/image.png?width=600" length="" type=""/><pubDate>Fri, 23 Jan 2026 17:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your weekly selection of awesome robot videos]]></content:encoded></item><item><title>Meta pauses teen access to AI characters ahead of new version</title><link>https://techcrunch.com/2026/01/23/meta-pauses-teen-access-to-ai-characters-ahead-of-new-version/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta today said that it is pausing teens' access to its AI characters globally across all its apps. The company mentioned that it is not abandoning its efforts but wants to develop an updated version of AI characters for teens.]]></content:encoded></item><item><title>Linux 6.19 Scheduler Feature Being Disabled Due To Performance Regressions</title><link>https://www.phoronix.com/news/Linux-6.19-Disabling-Next-Buddy</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Queued into tip/tip.git's "sched/urgent" Git branch today is a patch to disable the kernel scheduler's NEXT_BUDDY functionality that was re-implemented back during the Linux 6.19 merge window. It turns out to cause some performance regressions that have yet to be otherwise addressed...]]></content:encoded></item><item><title>When Two Years of Academic Work Vanished With a Single Click</title><link>https://science.slashdot.org/story/26/01/23/0959223/when-two-years-of-academic-work-vanished-with-a-single-click?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Marcel Bucher, a professor of plant sciences at the University of Cologne in Germany, lost two years of carefully structured academic work in an instant when he temporarily disabled ChatGPT's "data consent" option in August to test whether the AI tool's functions would still work without providing OpenAI his data. All his chats were permanently deleted and his project folders emptied without any warning or undo option, he wrote in a post on Nature. 

Bucher, a ChatGPT Plus subscriber paying $20 per month, had used the platform daily to draft grant applications, prepare teaching materials, revise publication drafts and create exams. He contacted OpenAI support, first receiving responses from an AI agent before a human employee confirmed the data was permanently lost and unrecoverable. OpenAI cited "privacy by design" as the reason, telling Nature it does provide a confirmation prompt before users permanently delete a chat but maintains no backups. 

Bucher said he had saved partial copies of some materials, but the underlying prompts, iterations, and project folders -- what he describes as the intellectual scaffolding behind his finished work -- are gone forever.]]></content:encoded></item><item><title>MEXC&apos;s Zero-Fee Gala Attracts Over 120,000 Participants with $8 Billion in Futures Trading Volume</title><link>https://hackernoon.com/mexcs-zero-fee-gala-attracts-over-120000-participants-with-$8-billion-in-futures-trading-volume?source=rss</link><author>Blockman PR and Marketing</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:33:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Victoria, Seychelles, January 23, 2026 – , the world's fastest-growing digital asset exchange and a pioneer of true zero-fee trading, successfully concluded its "," attracting over 120,000 participants and generating more than $8 billion in futures trading volume. The enthusiastic participation demonstrates strong user interest in the event and deepening trust in MEXC's commitment to creating meaningful value for its global trading community.The promotion ran from December 22, 2025, to January 21, 2026 (UTC), combining multiple incentive mechanisms to address different user segments and trading preferences. The promotion featured a total prize pool of $2 million, including high-value rewards such as a Cybertruck, a 1 oz gold bar, and an iPhone 17. It also offered zero fees on spot and select futures trading for XRP, DOGE, SOL, and BNB. Additionally, users could access USDT staking opportunities with returns of up to 600% APR and reward pools totaling 500,000 USDT, available to both new and existing users.The success of the "Zero-Fee Gala" underscores MEXC's commitment to placing user interests first through pioneering zero-fee trading and comprehensive incentive programs. As a leading global digital asset exchange, MEXC will continue to enhance platform services and deliver value-driven initiatives that provide users with a cost-effective, secure, and seamless trading experience.Founded in 2018, MEXC is committed to being "Your Easiest Way to Crypto." Serving over 40 million users across 170+ countries, MEXC is known for its broad selection of trending tokens, everyday airdrop opportunities, and low trading fees. Our user-friendly platform is designed to support both new traders and experienced investors, offering secure and efficient access to digital assets. MEXC prioritizes simplicity and innovation, making crypto trading more accessible and rewarding.For media inquiries, please contact MEXC PR team: media@mexc.comThis content does not constitute investment advice. Given the highly volatile nature of the cryptocurrency market, investors are encouraged to carefully assess market fluctuations, project fundamentals, and potential financial risks before making any trading decisions.:::tip
This story was published as a press release by Blockmanwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision]]></content:encoded></item><item><title>Alessio Vinassa on The Hidden Skill Behind Every Successful Entrepreneur</title><link>https://hackernoon.com/alessio-vinassa-on-the-hidden-skill-behind-every-successful-entrepreneur?source=rss</link><author>Blockman PR and Marketing</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:25:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every entrepreneur talks about vision, resilience, and execution. But beneath all of these traits lies a quieter, less visible skill that ultimately determines success or failure: decision-making under uncertainty. Markets shift, data is incomplete, and outcomes are rarely guaranteed. The leaders who endure are not those who wait for certainty—but those who learn how to think clearly without it.According to Alessio Vinassa, serial entrepreneur and business advisor, uncertainty is not an obstacle to leadership—it is the environment in which leadership exists.“If you wait for perfect information, you’re already behind,” Vinassa says. “Entrepreneurship is the practice of making meaningful decisions with incomplete data.”Why Uncertainty Is the Entrepreneur’s Default StateUnlike corporate environments with established processes and historical benchmarks, entrepreneurial decision-making often happens in real time. Founders must decide when to hire, when to pivot, when to say no, and when to move faster—all without knowing how markets, customers, or competitors will respond.Vinassa emphasizes that uncertainty is not a phase entrepreneurs outgrow.“The idea that uncertainty disappears as companies grow is a myth,” he explains. “It simply changes shape.”At early stages, uncertainty revolves around product fit and survival. Later, it shifts toward leadership, culture, expansion, and reputation. The cognitive challenge remains the same: choosing a direction without guarantees.Decision-Making Is a Cognitive Skill, Not Just InstinctMany founders describe their decisions as “gut-driven,” but Vinassa argues that effective intuition is built—not innate.“Strong instincts are trained through exposure, reflection, and pattern recognition,” he says. “They are not emotional impulses.”Successful entrepreneurs develop internal frameworks that help them evaluate risk, weigh trade-offs, and act decisively without becoming reckless. These frameworks allow leaders to remain calm while others freeze or overreact.Separating Signal from NoiseOne of the greatest challenges in uncertain environments is information overload. Data, opinions, forecasts, and external pressure can cloud judgment.Vinassa highlights the importance of filtering.“Not all information deserves equal attention,” he notes. “Good decision-makers know what to ignore.”Experienced leaders learn to prioritize first-order effects over speculation, focusing on what directly influences outcomes rather than chasing every possible scenario. This discipline reduces cognitive fatigue and improves consistency.Reversibility vs. IrreversibilityOne mental model Vinassa frequently references is the distinction between reversible and irreversible decisions.“Most decisions are not permanent,” he explains. “Treating them as such creates unnecessary paralysis.”\
Reversible decisions—such as testing a new strategy or experimenting with a process—should be made quickly and adjusted as needed. Irreversible decisions—those affecting reputation, ethics, or long-term trust—require deeper consideration.Understanding this distinction allows entrepreneurs to move faster without becoming careless.Managing Emotional Bias Under PressureUncertainty often triggers fear, ego, or urgency. Vinassa believes emotional regulation is a critical but underestimated leadership skill.“You’re not just managing a business—you’re managing your own psychology,” he says.Effective decision-makers create distance between emotion and action. They pause, reflect, and seek perspective before committing. This does not mean avoiding risk, but approaching it with clarity rather than anxiety.Building Confidence Without CertaintyConfidence in leadership does not come from knowing outcomes—it comes from trusting one’s process.“Confidence is the belief that you can respond well, even if the decision doesn’t work out,” Vinassa explains.Entrepreneurs who view decisions as experiments rather than verdicts are better equipped to adapt. Failure becomes feedback, not identity.Decision-Making as a Team SportAs organizations grow, decision-making must scale beyond the founder. Vinassa stresses the importance of building cultures that support distributed judgment.“Strong leaders don’t make every decision,” he says. “They build systems that produce good decisions.”Clear principles, aligned incentives, and psychological safety allow teams to navigate uncertainty together rather than bottlenecking leadership.Over time, entrepreneurs who master decision-making under uncertainty gain a powerful advantage. They move faster, recover quicker, and inspire confidence in others.“People follow leaders who can stay grounded when outcomes are unclear,” Vinassa notes. In volatile markets, this steadiness becomes a competitive differentiator.Decision-making under uncertainty is not glamorous, but it is foundational. It shapes strategy, culture, and outcomes more than any single idea.As Vinassa puts it: \n  “Ideas matter, but decisions determine destiny.”\n Alessio Vinassa is a serial entrepreneur, business strategist, and thought leader focused on leadership, adaptability, and building resilient businesses in fast-changing global markets. His work centers on mentorship, innovation, and helping entrepreneurs navigate complexity with clarity and purpose.:::tip
This story was published as a press release by Blockmanwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision]]></content:encoded></item><item><title>VVenC H.266 Encoder Rolls Out More ARM Optimizations For Nice Performance Gains</title><link>https://www.phoronix.com/news/VVenC-1.14-More-ARM-Performance</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:08:22 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Fraunhofer HHI this week released a new version of VVenC, their open-source H.266 video encoder. Among the changes this release are more performance optimizations for ARM and I have run some comparison benchmarks using a NVIDIA GB10 SoC with the Dell Pro Max GB10...]]></content:encoded></item><item><title>The HackerNoon Newsletter: How to Enter the Proof of Usefulness (PoU) Hackathon (1/23/2026)</title><link>https://hackernoon.com/1-23-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:03:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, January 23, 2026?By @aimodels44 [ 7 Min read ] An explainer on why brute-force AI fails at grand strategy games, and how hybrid LLM architectures enable long-horizon strategic reasoning. Read More.By @proofofusefulness [ 5 Min read ] No pitch decks. No mockups. Just code that works. Here is your step-by-step guide to entering HackerNoons $150K Proof of Usefulness Hackathon. Read More.By @nfrankel [ 6 Min read ] In this post, I tackled the issue of integrating checked exceptions with lambdas in Java. Read More.By @rodrigokamada [ 6 Min read ] In this article, we will create a WEB application using the latest version of Angular and integrate the AWS WAF CAPTCHA challenge to protect against bots. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Anthropic&apos;s AI Keeps Passing Its Own Company&apos;s Job Interview</title><link>https://slashdot.org/story/26/01/23/0951257/anthropics-ai-keeps-passing-its-own-companys-job-interview?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Anthropic has a problem that most companies would envy: its AI model keeps getting so good, the company wrote in a blog post, that it passes the company's own hiring test for performance engineers. The test, designed in late 2023 by optimization lead Tristan Hume, asks candidates to speed up code running on a simulated computer chip. Over 1,000 people have taken it, and dozens now work at Anthropic. But Claude Opus 4 outperformed most human applicants. 

Hume redesigned the test, making it harder. Then Claude Opus 4.5 matched even the best human scores within the two-hour time limit. For his third attempt, Hume abandoned realistic problems entirely and switched to abstract puzzles using a strange, minimal programming language -- something weird enough that Claude struggles with it. Anthropic is now releasing the original test as an open challenge. Beat Claude's best score and ... they want to hear from you.]]></content:encoded></item><item><title>This startup will send 1,000 people’s ashes to space — affordably — in 2027</title><link>https://techcrunch.com/2026/01/23/this-startup-will-send-1000-peoples-ashes-to-space-affordably-in-2027/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Started by an engineer who worked on the space shuttle program, and at Blue Origin, Space Beyond has a spot on a 2027 Falcon 9 flight.]]></content:encoded></item><item><title>Microsoft gave FBI a set of BitLocker encryption keys to unlock suspects’ laptops: Reports</title><link>https://techcrunch.com/2026/01/23/microsoft-gave-fbi-a-set-of-bitlocker-encryption-keys-to-unlock-suspects-laptops-reports/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:54:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The FBI served Microsoft a warrant requesting encryption recovery keys to decrypt the hard drives of people involved in an alleged fraud case in Guam. ]]></content:encoded></item><item><title>TikTok-like microdramas are going to make billions this year, even though they kind of suck</title><link>https://techcrunch.com/2026/01/23/tiktok-like-microdramas-are-going-to-make-billions-this-year-even-though-they-kind-of-suck/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:50:41 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The business model behind these apps replicates the same dark patterns as mobile games.]]></content:encoded></item><item><title>TikTok finalizes deal to create new US entity and avoid ban</title><link>https://techcrunch.com/2026/01/23/tiktok-finalizes-deal-to-create-new-us-entity-and-avoid-ban/</link><author>Aisha Malik</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:44:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The deal ends a six-year long political saga that started in 2020 when President Donald Trump tried to ban the app over national security concerns during his first term. ]]></content:encoded></item><item><title>OpenAI chief Sam Altman plans India visit as AI leaders converge in New Delhi: sources</title><link>https://techcrunch.com/2026/01/23/openai-chief-sam-altman-plans-india-visit-as-ai-leaders-converge-in-new-delhi-sources/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:30:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The visit comes as New Delhi prepares to host a major AI summit expected to draw top executives from Meta, Google, and Anthropic. This will be Altman's first visit to the country in nearly a year.]]></content:encoded></item><item><title>Monster Neutrino Could Be a Messenger of Ancient Black Holes</title><link>https://www.quantamagazine.org/monster-neutrino-could-be-a-messenger-of-ancient-black-holes-20260123/</link><author>Jonathan O&apos;Callaghan</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2026/01/Black-Holes-as-Dark-Matter-cr-Courtesy-of-KM3NeT-Default.webp" length="" type=""/><pubDate>Fri, 23 Jan 2026 15:26:07 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[Nearly three years ago, a particle from space slammed into the Mediterranean Sea and lit up the partially complete Cubic Kilometer Neutrino Telescope (KM3NET) detector off the coast of Sicily. The particle was a neutrino, a fundamental component of matter commonly known for its ability to slip through other matter unnoticed. The IceCube observatory in Antarctica, a comparable detector that has…]]></content:encoded></item><item><title>Apple Accuses European Commission of &apos;Political Delay Tactics&apos; To Justify Fines</title><link>https://apple.slashdot.org/story/26/01/23/0941249/apple-accuses-european-commission-of-political-delay-tactics-to-justify-fines?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple has accused the European Commission of using "political delay tactics" to postpone new app marketplace policies and create grounds for investigating and fining the iPhone maker, a preemptive response to reports that the commission plans to blame Apple for the announced closure of third-party app store Setapp. 

MacPaw, the developer behind Setapp, said it would shut down the marketplace next month because of "still-evolving and complex business terms that don't fit Setapp's current business model." The EC is preparing to say that Apple has not rolled out changes to address key issues concerning its business terms and their complexity, according to remarks seen by Bloomberg. 

Apple said it disputes this finding. The company said it submitted a formal compliance plan in October proposing to replace its $0.59 per-install fee structure with a 5% revenue share, but the commission has not responded. "The European Commission has refused to let us implement the very changes that they requested," Apple said. The company also claimed there is no demand in the EU for alternative app stores and disputed that Setapp is closing because of its actions.]]></content:encoded></item><item><title>Vulkan Roadmap 2026 Milestone: Variable Rate Shading, Host Image Copies &amp; More</title><link>https://www.phoronix.com/news/Vulkan-Roadmap-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:12:32 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In addition to the release today of Vulkan 1.4.340 with the new descriptor heap "VK_EXT_descriptor_heap" extension and three other new extensions, The Khronos Group's Vulkan Working Group has also published the Vulkan Roadmap 2026 Milestone...]]></content:encoded></item><item><title>Only 1 week left (or until the first 500 passes are gone): The first TechCrunch Disrupt 2026 ticket discount is ending</title><link>https://techcrunch.com/2026/01/23/only-1-week-left-or-until-the-first-500-passes-are-gone-the-first-disrupt-2026-ticket-discount-is-ending/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Register now to save up to $680 on your TechCrunch Disrupt 2026 pass and get a second ticket at 50% off. This offer ends next week on January 30, or once the first 500 tickets are claimed — whichever comes first.]]></content:encoded></item><item><title>&apos;Almost Everyone&apos; Laid Off at Vimeo Following Bending Spoons Buyout</title><link>https://slashdot.org/story/26/01/23/0757223/almost-everyone-laid-off-at-vimeo-following-bending-spoons-buyout?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Vimeo is laying off employees around the world just months after Italian software company Bending Spoons completed its $1.38 billion acquisition of the video hosting platform. Dave Brown, Vimeo's former brand VP, described the cuts on LinkedIn as affecting "a large portion of the company." One video engineer claimed "almost everyone" was laid off, "including the entire video team," and another software engineer said he lost his job alongside "a gigantic amount of the company." 

This marks Vimeo's second round of layoffs in less than six months. The company cut 10% of its workforce in September, just one week before Bending Spoons announced its acquisition plans. Bending Spoons has a history of post-acquisition layoffs at companies including WeTransfer, Filmic, and Evernote.]]></content:encoded></item><item><title>AMD Ryzen AI Software 1.7 Released For Improved Performance On NPUs, New Model Support</title><link>https://www.phoronix.com/news/AMD-Ryzen-AI-Software-1.7</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 14:18:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD today released a new version of Ryzen AI Software, the user-space packages for Microsoft Windows and Linux for making use of the Ryzen AI NPUs for various AI software tasks like Stable Diffusion, ONNX, and more...]]></content:encoded></item><item><title>US Formally Withdraws From WHO</title><link>https://science.slashdot.org/story/26/01/23/1226253/us-formally-withdraws-from-who?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The United States formally withdrew from the World Health Organization on Thursday, making good on an executive order that President Trump issued on his first day in office pledging to leave the international organization that coordinates global responses to public health threats. The New York Times: While the United States is walking away from the organization, a senior official with the Department of Health and Human Services told reporters on Thursday that the Trump administration was considering some type of narrow, limited engagement with W.H.O. global networks that track infectious diseases, including influenza. 

As a W.H.O. member, the United States long sent scientists from the Centers for Disease Control and Prevention to participate in international decision-making about which strains to include in the flu vaccine. A W.H.O. meeting on next year's vaccine is scheduled for February. The official said the Trump administration would soon disclose how or whether it will participate. 

On Thursday, the administration said that all U.S. government funding to the organization had been terminated, and that all assigned federal employees and contractors had been recalled from its Geneva headquarters and its offices worldwide.]]></content:encoded></item><item><title>GNU Guix 1.5 Released With RISC-V Support, Experimental x86_64 GNU Hurd Kernel</title><link>https://www.phoronix.com/news/GNU-Guix-1.5-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 13:54:41 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[GNU Guix 1.5 is out today as the latest major release for this platform built around its functional package manager. This is a big upgrade with it having been three years since the GNU Guix 1.4 release...]]></content:encoded></item><item><title>RFK Jr. Spreads New Bogus Scare Mongering Bullshit About Cell Phone Safety</title><link>https://www.techdirt.com/2026/01/23/rfk-jr-spreads-new-bogus-scare-mongering-bullshit-about-cell-phone-safety/</link><author>Karl Bode</author><category>tech</category><pubDate>Fri, 23 Jan 2026 13:24:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The hype and madness surrounding 5G has always been pretty wild to watch.The Trump administration’s pseudo-populist attempt to tap into the more delirious and desperate segments of the electorate has long taken advantage of this latter group’s often-legitimate distrust of modern medicine, corporations, and public safety regulators. “The FDA removed online information that said scientists have not connected exposure to radiofrequency (RF) waves, emitted by cell phones, to health problems in users.Some of the removed webpages contained “old conclusions,” an HHS spokesperson told the Wall Street Journal. The spokesperson also said that researching cell phone radiation would “identify gaps in knowledge.” The agency provided a similar statement to Scientific American, adding that the research was “directed by President Trump’s MAHA Commission.”“Generally speaking, electromagnetic radiation is a major health concern,” Kennedy said in the exclusive interview, when asked for his concerns about 5G towers. “I’m very concerned about it.”In these interviews, RFK Jr. is making completely false claims that there’s “more than 10,000 studies” proving a clear risk of human harm from cell phone use. The World Health Organization found no justification for health concerns after a meta-analysis of nearly .While it’s hubris to insist we know  about wireless’ impact on human health, the science we do have traditionally points to a very clear conclusion: 5G isn’t likely to seriously to hurt you. In fact, in many ways 5G is potentially less harmful than previous iterations given that the millimeter wave spectrum being used in many cities can barely penetrate walls, much less human skin. As Scientific American notes, while there  historically been studies suggesting potential cancerous impact from massive exposure using rats, “studies in humans have been inconsistent and limited in scope and efficacy.” The FDA had previously, and correctly, stated that “the weight of scientific evidence has not linked exposure to radio frequency energy from cellphone use with any health problems.”Now if the Trump administration was actually serious about launching real-world scientific inquiries into cell phone health’s impact, that might be something. But we’re long past the point where this weird assortment of zealots deserve any benefit of the doubt. Especially given RFK Jr.’s history of completely unscientific, fear mongering gibberish.Trump authoritarians love leaning into conspiracy theories for several reasons. Two, it helps create an information fog of war where the electorate finds it harder to reliably identify what’s true, in turn making people more distrustful of the few legitimate media organizations still interested in the truth. This in turn makes it easier for authoritarians to lie to you (and the movement’s adjacent grifter economy to rip you off with false promises and cures).It’s not populism, it’s exploitation. There are no answers here, only more confusion and chaos for suffering people. All to mask broad, grotesque corruption by a broad assortment of terrible human beings. Many of these MAHA segments being targeted by Trump grifters (see: RFK Jr.’s siren call to angry Lyme Disease patients) spent decades feeling legitimately exploited and abused by corporate power, institutional failure, and U.S. health care dysfunction only to walk straight into the maw of some of the biggest grifting shitbags America may have ever spawned (which is really saying something).]]></content:encoded></item><item><title>Linux Lands Fix For Its &quot;Subtly Wrong&quot; Page Fault Handling Code For The Past 5 Years</title><link>https://www.phoronix.com/news/Linux-6.19-Page-Fault-Code-Fix</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 13:11:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged today for the Linux 6.19 Git kernel and then in turn for back-porting to prior Linux kernel series is making the x86 page fault handling code disable interrupts properly. Since 2020 it turns out the handling was subtly wrong but now corrected by Intel...]]></content:encoded></item><item><title>TikTok Finalizes Deal To Form New American Entity</title><link>https://tech.slashdot.org/story/26/01/23/0817218/tiktok-finalizes-deal-to-form-new-american-entity?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from NPR: TikTok has finalized a deal to create a new American entity, avoiding the looming threat of a ban in the United States that has been in discussion for years. The social video platform company signed agreements with major investors including Oracle, Silver Lake and MGX to form the new TikTok U.S. joint venture. The new version will operate under "defined safeguards that protect national security through comprehensive data protections, algorithm security, content moderation and software assurances for U.S. users," the company said in a statement Thursday. American TikTok users can continue using the same app. [...] Adam Presser, who previously worked as TikTok's head of operations and trust and safety, will lead the new venture as its CEO. He will work alongside a seven-member, majority-American board of directors that includes TikTok's CEO Shou Chew.
 
[...] In addition to an emphasis on data protection, with U.S. user data being stored locally in a system run by Oracle, the joint venture will also focus on TikTok's algorithm. The content recommendation formula, which feeds users specific videos tailored to their preferences and interests, will be retrained, tested and updated on U.S. user data, the company said in its announcement. The algorithm has been a central issue in the security debate over TikTok. China previously maintained the algorithm must remain under Chinese control by law. But the U.S. regulation passed with bipartisan support said any divestment of TikTok must mean the platform cuts ties -- specifically the algorithm -- with ByteDance. Under the terms of this deal, ByteDance would license the algorithm to the U.S. entity for retraining.
 
The law prohibits "any cooperation with respect to the operation of a content recommendation algorithm" between ByteDance and a new potential American ownership group, so it is unclear how ByteDance's continued involvement in this arrangement will play out. Oracle, Silver Lake and the Emirati investment firm MGX are the three managing investors, who each hold a 15% share. Other investors include the investment firm of Michael Dell, the billionaire founder of Dell Technologies. ByteDance retains 19.9% of the joint venture.]]></content:encoded></item><item><title>Tesla discontinues Autopilot in bid to boost adoption of its Full Self-Driving software</title><link>https://techcrunch.com/2026/01/23/tesla-discontinues-autopilot-in-bid-to-boost-adoption-of-its-full-self-driving-software/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 23 Jan 2026 12:56:44 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company is also currently facing a 30-day suspension of its manufacturing and dealer licenses in California for deceptive marketing about Autopilot's capabilities.]]></content:encoded></item></channel></rss>
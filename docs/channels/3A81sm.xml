<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech</title><link>https://konrad.website/feeds/</link><description></description><item><title>Scientists Discovered a Cow That Uses Tools Like a Chimpanzee</title><link>https://www.404media.co/scientists-discovered-a-cow-that-uses-tools-like-a-chimpanzee/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/01/image1-4.jpg" length="" type=""/><pubDate>Sat, 24 Jan 2026 14:00:33 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Welcome back to the Abstract! Here are the studies this week that scratched the sweet spot, extended a hand, went over the hill, and ended up on Mercury.First, a clever cow single-hoofedly upends assumptions about bovine intelligence. Next, we’ve got the oldest rock art ever discovered, the graying of modern zoos, and the delightfully named phenomena of bursty bulk flows.Veronika, a Swiss brown cow that lives in a rural mountain village in Austria, is the first cow to demonstrate tool use. How udderly amoosing!Veronkia’s owner Witgar Wiegele, who keeps her as a pet companion, noticed years ago that she likes to pick up sticks with her mouth in order to reach hard-to-scratch places on her body. The hills were soon alive with word of Veronika’s tool-using prowess, attracting the attention of researchers Antonio Osuna-Mascaró and Alice Auersperg of the University of Veterinary Medicine Vienna. Tool use is a sign of advanced cognition that has been observed in many animals, including primates, orcas, and birds. But cows, with their vacant expressions and docile nature, have been overlooked as likely tool users, except as a joke in Gary Larson’scartoons.  In their new study, Osuna-Mascaró and Auersperg presented Veronika with a deck brush, which she proceeded to use as a scratching tool in a variety of configurations.“We hypothesized that she would target difficult-to-reach body regions and use the more effective brushed end over the stick end,” the researchers said. “Veronika’s behavior went beyond these predictions, however, showing versatility, anticipation, and fine motor targeting.” “Unexpectedly and revealingly, Veronika’s tool-end use depended strongly on body region: she predominantly used the brush end for upper-body scratching and the stick end for lower areas, such as the udder and belly skin flaps,” they added. “Importantly, the differential use of both broom ends constitutes the use of a multipurpose tool, exploiting distinct properties of a single object for different functions. Comparable behavior has only been consistently documented in chimpanzees.”I recommend reading the study in full, as it is not very long and contains ample video footage demonstrating Veronika’s mastery of the deck brush. The authors seem genuinely enraptured by her talents and, frankly, it’s hard to blame them for milking the discovery. Overall, the findings serves as a reminder not to cowtow to stereotypes of braindead bovines, a point made by the study’s bullish conclusion:“Despite millennia of domestication for productivity, livestock have been almost entirely excluded from discussions of animal intelligence,” Osuna-Mascaró and Auersperg said. “Veronika’s case challenges this neglect, revealing that technical problem-solving is not confined to large-brained species with manipulative hands or beaks.” “She did not fashion tools like the cow in Gary Larson’s cartoon, but she selected, adjusted, and used one with notable dexterity and flexibility,” they concluded. “Perhaps the real absurdity lies not in imagining a tool-using cow, but in assuming such a thing could never exist.”Now that’s something to ruminate on.Archaeologists have discovered the oldest known rock art, which are very faint hand stencils made by humans 68,000 years ago on a cave wall on the Indonesian island of Sulawesi.For comparison, the next oldest rock art, located in Spain and attributed to Neanderthals, is roughly 66,000 years old. The newly-dated hand stencils were made by a mysterious group of  people who eventually migrated across the lost landmass of Sahul, which is now submerged, and reached Australia.The find supports a “growing view that Sulawesi was host to a vibrant and longstanding artistic culture,” said researchers co-led by Adhi Agus Oktaviana and Budianto Hakim of Indonesia's National Agency for Research and Innovation, and Renaud Joannes-Boyau of Southern Cross University. “The presence of this extremely old art in Sulawesi suggests that the initial peopling of Sahul about 65,000 years involved maritime journeys between Borneo and Papua, a region that remains poorly explored from an archaeological perspective,” the team added.Though the stencils are extremely faint and obscured by younger paintings, it’s still eerie to see the contours of human hands from a long-lost era when dire wolves and Siberian unicorns still roamed our world.Zoo animals get long in the toothSpeaking of really old stuff, there has been much consternation of late about falling birth rates and aging populations in many nations around the world. As it turns out, similar demographic anxieties are playing out in zoos across Europe and North America, where mammal populations “have, on average, become older and less reproductively active” according to a new study.  On the one hand, this is good news because it signals improvements in the health and longevity of mammals in zoos, reflecting a long-term effort to transform zoos into conservation hubs as opposed to sites of spectacle. But it also “fundamentally jeopardizes the long-term capacity of zoos to harbor insurance populations, facilitate reintroductions of threatened species, and simply maintain a variety of self-sustaining species programs,” said researchers led by João Pedro Meireles of the University of Zurich. This story struck me because of my many childhood visits to see an Asian elephant named Lucy, who was the star of the Edmonton Valley Zoo when I was young (I am now old). I recently learned Lucy is still chilling there at the ripe old age of 50! This is positively Methuselan for a zoo elephant, though it is not an unusual age for them in the wild. Lucy is the perfect poster child (or rather, poster senior) for this broader aging effect. Long may she reign.Bust out the bursty bulk flowWe’ll close with a reminder that the planet Mercury exists. It can be easy to overlook this tiny rock, which is barely bigger than the Moon. But Mercury is dynamic and full of surprises, according to a study based on close flybys of the planet by BepiColombo, a collaborative space mission between Europe and Japan, which is tasked with cracking this mercurial nut.BepiColombo zoomed just over 100 miles above Mercury’s surface in October 2021, June 2022, and June 2023, but each encounter revealed distinct portraits of the planet’s magnetosphere, which is a magnetic bubble that surrounds some planets, including Earth.“These flybys all passed from dusk to dawn through the nightside equatorial region but were noticeably different from each other,” said researchers led by Hayley N. Williamson of the Swedish Institute of Space Physics. “Specifically, we see energetic ions in the second and third flybys that are not there in the first.”“We conclude that these ions are part of a phenomenon called bursty bulk flow, which also happens at Earth,” the team concluded. Bursty bulk flow, in addition to being a fun phrase to say outloud, are intense, transient jets in a magnetosphere that drive energetic particles toward the planet, and are driven by solar activity. BepiColombo is on track to scooch into orbit around Mercury this November, where it will continue to study the planet up close for years, illuminating this world of extremes. In my hierarchy of Mercurys, the planet sits above the Ford brand, the 80th element, and the Roman god, with only Freddie surpassing it. So, it’s good to see it getting the attention it deserves.  Thanks for reading! See you next week.]]></content:encoded></item><item><title>The Project G Stereo Was the Definition of Groovy</title><link>https://spectrum.ieee.org/project-g-stereo</link><author>Allison Marsh</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82Mjk3ODM1Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgwOTA0MzEzN30.4-HE-hs8L8DGiawcFCVTIk5bmdlYBw46bZihCfMawLU/image.jpg?width=600" length="" type=""/><pubDate>Sat, 24 Jan 2026 14:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Clairtone’s high-end hi-fi system was prized by celebrities and musicians]]></content:encoded></item><item><title>Hollywood Tries To Take Pirate Sites Down Globally Through India Court</title><link>https://yro.slashdot.org/story/26/01/24/0124246/hollywood-tries-to-take-pirate-sites-down-globally-through-india-court?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TorrentFreak: The High Court in New Delhi, India, has granted another pirate site blocking order in favor of American movie industry giants, including Apple, Warner., Netflix, Disney and Crunchyroll. The injunction targets notorious pirate sites, requesting blockades at Indian ISPs. More crucially, however, globally operating domain registrars, including U.S. companies, are also compelled to take action. However, despite earlier cooperation, most don't seem eager to comply. [...] As reported by Verdictum a few days ago, the High Court in New Delhi issued a new blocking injunction on December 18, targeting more than 150 pirate site domains, including yflix.to, animesuge.bz, bs.to, and many others.
 
The complaint (PDF) is filed by Warner Bros., Apple, Crunchyroll, Disney, and Netflix, which are all connected to the MPA's anti-piracy arm, ACE. The referenced works include some of the most pirated titles, such as Stranger Things, Squid Game, and Silo. In addition to targeting Indian ISPs, the order also lists various domain name registries and related organizations as defendants. This includes American registrars such as Namecheap and GoDaddy, but also the government of the Kingdom of Tonga, which is linked to .to domains. By requiring domain name registrars to take action, the Indian court orders have a global impact.
 
In addition to suspending the domain names within three days days, the domain name registrars are given four weeks to disclose the relevant subscriber information connected to these domains. "[The registrars] shall lock and suspend Defendant Nos. 1 to 47 websites within 72 hours of being communicated with a copy of this Order and shall file all the Basic Subscriber Information, including the name, address, contact information, email addresses, bank details, IP logs, and any other relevant information [...] within four weeks of being communicated with a copy of this Order," the High Court wrote. While the "Dynamic+" injunction is designed to be a global kill switch, its effectiveness depends entirely on the cooperation of the domain name registrars. Since most of these are based outside of India, their compliance is not guaranteed.]]></content:encoded></item><item><title>AMD Releases MLIR-AIE 1.2 Compiler Toolchain For Targeting Ryzen AI NPUs</title><link>https://www.phoronix.com/news/AMD-MLIR-AIE-1.2</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 12:22:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In addition to AMD releasing the Ryzen AI Software 1.7 release on Friday, they also published a new version of their MLIR-AIE compiler toolchain for targeting AMD Ryzen AI NPU devices with this LLVM-based MLIR-focused stack...]]></content:encoded></item><item><title>AMDGPU Driver Reverts Code For A Number Of Regressions On Linux 6.19</title><link>https://www.phoronix.com/news/AMDGPU-Linux-6.19-Regressions</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 11:36:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged on Friday as part of this week's DRM kernel graphics driver fixes for the week is addressing a regression affecting many different users with the Linux 6.19 development kernel...]]></content:encoded></item><item><title>GNOME&apos;s AI Assistant Newelle Adds Llama.cpp Support, Command Execution Tool</title><link>https://www.phoronix.com/news/GNOME-AI-Newelle-1.2</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 11:19:44 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Newlle as a virtual AI assistant for the GNOME desktop with API integration for Google Gemini, OpenAI, Groq, and also local LLMs is out with a new release. Newelle has been steadily expanding its AI integration and capabilities and with the new Newelle 1.2 are yet more capabilities for those wanting AI on the GNOME desktop...]]></content:encoded></item><item><title>ASUS Armoury Driver For Linux 6.19 Picks Up Support For More ASUS Laptops</title><link>https://www.phoronix.com/news/ASUS-Armoury-More-Hardware</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 10:52:27 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A new driver in the Linux 6.19 kernel is the ASUS Armoury driver for supporting additional functionality with the ROG Ally gaming handhelds and other ASUS ROG gaming hardware like their laptops...]]></content:encoded></item><item><title>Smartwatches Help Detect Abnormal Heart Rhythms 4x More Often In Clinical Trial</title><link>https://science.slashdot.org/story/26/01/24/0114249/smartwatches-help-detect-abnormal-heart-rhythms-4x-more-often-in-clinical-trial?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A clinical trial found that seniors at high stroke risk who wore an Apple Watch were four times more likely to have hidden heart rhythm disorders detected than those receiving standard care. The researchers noted that over half the time, these smartwatch wearers with heart rhythm problems hadn't shown any symptoms prior to diagnosis. From U.S. News & World Report: Later editions of Apple Watches are equipped with two functions that can help monitor heart health -- photoplethysmography (PPG), which tracks heart rate, and a single-lead electrocardiogram (ECG) that monitors heart rhythm. "Using smartwatches with PPG and ECG functions aids doctors in diagnosing individuals unaware of their arrhythmia, thereby expediting the diagnostic process," said senior researcher Dr. Michiel Winter, a cardiologist at Amsterdam University Medical Center in The Netherlands. "Our findings suggest a potential reduction in the risk of stroke, benefiting both patients and the health care system by reducing costs," Winter said in a news release.
 
[...] Smartwatches are much easier than other wearable devices for detecting irregular heart rhythms [...]. These other means require people to wear sticky leads, carry around bulky monitors or even receive short-term implants. Lead researcher Nicole van Steijn, a doctoral candidate at Amsterdam UMC, noted that wearables that track both the pulse and electrical activity have been around for a while. "However, how well this technology works for the screening of patients at elevated risk for atrial fibrillation had not yet been investigated in a real-world setting,"she said in a news release. The findings have been published in the Journal of the American College of Cardiology.]]></content:encoded></item><item><title>The TechBeat: Third-Party Risks in 2026: Outlook and Security Strategies (1/24/2026)</title><link>https://hackernoon.com/1-24-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sat, 24 Jan 2026 07:11:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @ivankuznetsov [ 9 Min read ] 
 It’s far more efficient to run multiple Claude instances simultaneously, spin up git worktrees, and tackle several tasks at once. Read More.By @drechimyn [ 7 Min read ] 
 Broken Object Level Authorization (BOLA) is eating the API economy from the inside out.  Read More.By @socialdiscoverygroup [ 19 Min read ] 
 We taught Playwright to find the correct HAR entry even when query/body values change and prevented reusing entities with dynamic identifiers.  Read More.By @dataops [ 4 Min read ] 
 DataOps provides the blueprint, but automation makes it scalable. Learn how enforced CI/CD, observability, and governance turn theory into reality. Read More.By @kilocode [ 6 Min read ] 
 CodeRabbit alternative for 2026: Kilo's Code Reviews combines AI code review with coding agents, deploy tools, and 500+ models in one unified platform. Read More.By @linked_do [ 12 Min read ] 
 As the AI bubble deflates, attention shifts from scale to structure. A long view on knowledge, graphs, ontologies, and futures worth living. Read More.By @mcsee [ 3 Min read ] 
 Set your AI code assistant to read-only state before it touches your files. Read More.By @dharmateja [ 13 Min read ] 
 Historically, technological revolutions have triggered similar waves of anxiety, only for the long-term outcomes to demonstrate a more optimistic narrative. Read More.By @proflead [ 4 Min read ] 
 Ollama is an open-source platform for running and managing large-language-model (LLM) packages entirely on your local machine. Read More.By @nikitakothari [ 5 Min read ] 
 In an agentic world, your documentation—specifically your structured API contracts—has replaced your implementation as the actual source code. Read More.By @scottdclary [ 27 Min read ] 
 Real transformation requires your brain to physically rewire itself. Read More.By @praisejamesx [ 6 Min read ] 
 Stop relying on "vibes" and "hustle." History rewards those with better models, not better speeches. Read More.By @mohansankaran [ 10 Min read ] 
 Jetpack Compose memory leaks are usually reference leaks. Learn the top leak patterns, why they happen, and how to fix them. Read More.By @ishanpandey [ 5 Min read ] 
 BTCC reports $5.7B tokenized gold volume in 2025 with 809% Q4 growth, marking gold as crypto's dominant real-world asset. Read More.By @zacamos [ 5 Min read ] 
 Third-party risk is everywhere in 2026. Here's an overview of current risks and security best practices as we start the new year. Read More.]]></content:encoded></item><item><title>Study Shows How Earthquake Monitors Can Track Space Junk Through Sonic Booms</title><link>https://science.slashdot.org/story/26/01/24/014216/study-shows-how-earthquake-monitors-can-track-space-junk-through-sonic-booms?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A new study shows that earthquake monitoring networks can track falling space debris by detecting the sonic booms produced during atmospheric reentry, sometimes more accurately than radar. The Associated Press reports: Scientists reported Thursday that seismic readings from sonic booms that were generated when a discarded module from a Chinese crew capsule reentered over Southern California in 2024 allowed them to place the object's path nearly 20 miles (30 kilometers) farther south than radar had predicted from orbit. Using this method to track uncontrolled objects plummeting at supersonic speeds, they said, could help recovery teams reach any surviving pieces more quickly -- crucial if the debris is dangerous.
 
"The problem at the moment is we can track stuff very well in space," said Johns Hopkins University's Benjamin Fernando, the lead researcher. "But once it gets to the point that it's actually breaking up in the atmosphere, it becomes very difficult to track." His team's findings, published in the journal Science, focus on just one debris event. But the researchers already have used publicly available data from seismic networks to track a few dozen other reentries, including debris from three failed SpaceX Starship test flights in Texas. [...]
 
Fernando is looking to eventually publish a catalog of seismically tracked, entering space objects, while improving future calculations by factoring in the wind's effect on falling debris. In a companion article in Science, Los Alamos National Laboratory's Chris Carr, who was not involved in the study, said further research is needed to reduce the time between an object's final plunge and the determination of its course. For now, Carr said this new method "unlocks the rapid identification of debris fall-out zones, which is key information as Earth's orbit is anticipated to become increasingly crowded with satellites, leading to a greater influx of space debris."]]></content:encoded></item><item><title>Legal AI giant Harvey acquires Hexus as competition heats up in legal tech</title><link>https://techcrunch.com/2026/01/23/legal-ai-giant-harvey-acquires-hexus-as-competition-heats-up-in-legal-tech/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sat, 24 Jan 2026 05:27:22 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Hexus founder and CEO Sakshi Pratap, who previously held engineering roles at Walmart, Oracle, and Google, tells TechCrunch that her San Francisco-based team has already joined Harvey, while the startup's India-based engineers will come onboard once Harvey establishes a Bangalore office.]]></content:encoded></item><item><title>Apple iPhone just had its best year in India as the smartphone market stays broadly flat</title><link>https://techcrunch.com/2026/01/23/apple-iphone-just-had-its-best-year-in-india-as-the-smartphone-market-stays-broadly-flat/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Sat, 24 Jan 2026 05:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Apple shipped a record 14 million iPhones in India in 2025 and gained market share.]]></content:encoded></item><item><title>KDE Plasma Saw At Least 9 Crash Fixes This Week</title><link>https://www.phoronix.com/news/KDE-Plasma-9-Crash-Fixes</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 05:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE Plasma 6.6 feature development work continues winding down while Plasma 6.7 has begun seeing more feature work. This week also saw at least nine different crash fixes affecting Plasma/KWin...]]></content:encoded></item><item><title>TikTok users freak out over app’s ‘immigration status’ collection — here’s what it means</title><link>https://techcrunch.com/2026/01/23/tiktok-users-freak-out-over-apps-immigration-status-collection-heres-what-it-means/</link><author>Sarah Perez</author><category>tech</category><pubDate>Sat, 24 Jan 2026 04:34:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TikTok users are freaking out over a mention of "immigration status" data collection, but lawyers explain the disclosure is related to state privacy laws. ]]></content:encoded></item><item><title>Techdirt 2025: The Stats</title><link>https://www.techdirt.com/2026/01/23/techdirt-2025-the-stats/</link><author>Mike Masnick</author><category>tech</category><pubDate>Sat, 24 Jan 2026 03:39:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Every year, a little after New Years, I do a post about the previous year of Techdirt traffic and comments. You may notice that we skipped last year’s for 2024. 2025 was so crazy with everything happening, we just didn’t get around to it, and I kept saying I would and then I looked up and it was May and it just didn’t feel right to go back. But now we’re back, closing the books on 2025 in mid-January.As we’ve done for a few years now, we continue to run  Google Analytics, relying instead on JetPack and Plausible Analytics. And as we always note, all traffic numbers are somewhat unreliable, but they give us a general sense of how things are going (and JetPack & Plausible’s numbers mostly seem to match).In 2025, our traffic was up noticeably from previous the previous year—around 29% more pageviews compared to 2024. Given that 2025 was the year American democracy started visibly buckling, and we made it clear we wouldn’t back down from covering it, that’s probably not surprising. With so much of our national media falling down on the job, it turns out people will show up when you’re one of the news orgs actually calling out what’s happening.As is pretty typical, the vast majority of our traffic came from the US (around 75%), followed by the UK, Canada, Australia, India, Germany, and Finland. After that you have the Netherlands, France, Sweden, New Zealand, Ireland, Spain, Norway, and Pakistan. The stats say we had… a grand total of three visitors from Antarctica last year. Stay warm, folks. We also had three visitors from Vatican City. Sounds like Pope Leo isn’t yet reading Techdirt, but there’s still time.In 2025 we published 1,993 posts and garnered 39,750 comments. The post number is about average for us over the past few years. The comment numbers are down a bit, even as traffic was up, which is likely due to some anti-trolling/anti-spam measures we took last May when a few trolls (and spammers) got a bit out of control. We also wrote about 1.74 million words in 2025, our most since 2016.It also appears to be an unstoppable trend that Techdirt’s posts only get longer and longer, reaching 871 words per post on average last year. The trend here is not subtle.As for where our traffic comes from, I’m always proudest of the fact that more than half of our traffic is direct traffic, not referred from elsewhere, meaning that we have a loyal audience that comes to check out Techdirt unmediated by various algorithms.In terms of traffic referrals, the largest single source was Reddit. Search engines (mainly Google) were also significant. After that our two biggest referrers were Bluesky and Fark (yes, Fark!). It’s nice to see Bluesky continuing to send tons of traffic, reminding us that it’s the only major social media site that doesn’t downgrade and suppress links. We also got significant traffic from Flipboard, Google News, Hacker News, and the NewsBreak app.Much further down on the referrals, X, Substack, and LinkedIn all gave us roughly the equivalent amount of traffic to each other (less than 10% of what Bluesky and Fark sent us). Also… ChatGPT. It’s a little bit less than X/Substack/LinkedIn, but I’m guessing by next year it will surpass those. Wikipedia & Threads each sent about the same amount of traffic as ChatGPT did.Down towards the bottom of the list there are random blogs, news sites, a few RSS readers, and also Mastodon. They’re not that big compared to the others, but they’re all still sending some visitors our way.Our traffic now appears to be almost exactly evenly split between computers and mobile devices. Last year it was a 51%/49% split with the slight edge going to desktops. In terms of specific operating systems, iOS tops the overall list, followed by Windows. Then Android, Mac, and Linux. There’s a much smaller group of folks at the bottom of the list using Chromebooks.Interestingly, our most popular day for traffic was Thursday (18% of views), and the best hour was 9:00 AM (7% of views).Top Ten Stories, by unique pageviews, on Techdirt for 2025:The pattern here is not exactly subtle. Seven of the top ten stories are about the ongoing collapse of constitutional governance. The TikTok stories are really the same story twice… and in some way are directly connected to the collapse of the United States. And the only entry that isn’t directly about authoritarianism is about how bad-faith actors exploit free speech norms — which, well, same theme wearing different clothes.2025’s Top Ten Stories, by comment volume:The fact that we had two of the weekly comment roundups ending up on the most commented list, both of which were from last January, tells you how we had some trolls who took it upon themselves to wreck the comments, especially on those posts early last year. Also, as we point out nearly every year, the fact that the list of highest commented posts is almost entirely different from the list of most visited posts seems noteworthy.Now, to the personal commenter leaderboards:2025 Top Commenters, by comment volume:Stephen T. Stone continues to dominate the comment leaderboard, though with fewer comments than in previous years, probably since there were fewer troll comments to respond to. It’s also nice to see some new names on the list this year.Top 10 Most Insightful Commenters, based on how many times they got the lightbulb icon(Parentheses shows what percentage of their comments got the icon)Some familiar names here, though nice to see MrWilson move up in the rankings. Also a shoutout to Bloof for having the highest percentage of comments getting the insightful icon.Top 10 Funniest Commenters, based on how many times they got the laughing face icon(Parentheses shows what percentage of their comments got the icon)Interesting to see MrWilson take the top spot for funny this year. As always, it’s much harder to get the funny icon than the insightful one. Last year wasn’t a huge year for humor, not surprisingly. But looking at how few “funny” comments were needed to get on the top 10 list, seems like some of you could jump onto it next year with just a few more funny comments. Let’s get some gallows humor going. Also, shoutout to Rico R. for having a very high percentage of their comments getting the funny icon.And, with that, the 2025 books are officially closed. 2026 is already a few weeks in and shows no signs of being any less exhausting (quite the opposite), so we’ll see you in the comments. Thanks to everyone who reads and debates, and especially to those of you who support our work here.]]></content:encoded></item><item><title>New Filtration Technology Could Be Gamechanger In Removal of PFAS &apos;Forever Chemicals&apos;</title><link>https://science.slashdot.org/story/26/01/24/002216/new-filtration-technology-could-be-gamechanger-in-removal-of-pfas-forever-chemicals?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Bruce66423 shares a report from the Guardian: New filtration technology developed by Rice University may absorb some Pfas "forever chemicals" at 100 times the rate than previously possible, which could dramatically improve pollution control and speed remediations. Researchers also say they have also found a way to destroy Pfas, though both technologies face a steep challenge in being deployed on an industrial scale. A new peer-reviewed paper details a layered double hydroxide (LDH) material made from copper and aluminum that absorbs long-chain Pfas up to 100 times faster than commonly used filtration systems.
 
[...] [Michael Wong, director of Rice's Water Institute, a Pfas research center] said Rice's non-thermal process works by soaking up and concentrating Pfas at high levels, which makes it possible to destroy them without high temperatures. The LDH material Rice developed is a variation of similar materials previously used, but researchers replaced some aluminum atoms with copper atoms. The LDH material is positively charged and the long-chain Pfas are negatively charged, which causes the material to attract and absorb the chemicals, Wong said. [...]
 
Pfas are virtually indestructible because their carbon atoms are bonded with fluoride, but Rice found that the bonds could be broken if the chemicals in the material were heated to 400-500C -- a relatively low temperature. The fluoride gets trapped in the LDH material and is bonded to calcium. The leftover calcium-fluoride material is safe and can be disposed of in a landfill, Wong said. The process works with some long-chain Pfas that are among the most common water pollutants, and it also absorbed some smaller Pfas that are commonplace.
 
Wong said he is confident the material can be used to absorb a broad array of Pfas, especially if they are negatively charged. Most new Pfas elimination systems fail to work at an industrial scale. Wong said the new material has an advantage because its absorption rate is so strong, it can be used repeatedly and it is in a "drop in material," meaning it can be used with existing filtration infrastructure. That eliminates one of the major cost barriers.]]></content:encoded></item><item><title>The Physics Simulation Problem That More Compute Can’t Fix</title><link>https://hackernoon.com/the-physics-simulation-problem-that-more-compute-cant-fix?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Sat, 24 Jan 2026 03:00:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Physics simulations don’t just get slower as resolution increases—they break.]]></content:encoded></item><item><title>Advanced Machine Learning: Bridging SDP Relaxation and Collective Motion Dynamics</title><link>https://hackernoon.com/advanced-machine-learning-bridging-sdp-relaxation-and-collective-motion-dynamics?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:39:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.1 Kuramoto models from the geometric point of view(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>The Kuramoto Model: Synchronization and Dynamics of Coupled Oscillators</title><link>https://hackernoon.com/the-kuramoto-model-synchronization-and-dynamics-of-coupled-oscillators?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:36:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The famous Kuramoto model [2] has been introduced in 1975 as a paradigm for the synchronization phenomena in ensembles of coupled oscillators. Following the pioneering Kuramoto’s paper, various modifications and generalizations of his model have been proposed. The model describes an ensemble of phase oscillators, whose states are represented by phases ϕi ∈ [0, 2π], while amplitudes are neglected.\
We consider the model where the dynamics of oscillators are given by the following system of ODE’s(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Physics-Informed Machine Learning: Leveraging Physical Laws and Energy-Based Models</title><link>https://hackernoon.com/physics-informed-machine-learning-leveraging-physical-laws-and-energy-based-models?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:33:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The term physics informed ML refers to the general approach aiming at exploiting physical knowledge in order to set up adequate models given the particular data set and the problem. In many cases, models and architectures are, at least partially, enforced by physical laws, such as symmetries or conservation laws [66, 65]. Taking this into account dramatically increases efficiency, transparency and robustness of ML algorithms. The very general idea standing behind this approach is the parsimony principle, one of the most universal principles in Science.\
Although physics informed ML is regarded a very recent field, it has been developed upon the blend of ideas from computational physics and energy-based ML. Indeed, concepts of energy and entropy are built in early ML algorithms dealing with problems that are not necessarily related to any physical system [67]. A classical example of this kind is the famous Hopfield model. We also refer to [68] for energy-based approaches in RL.\
More generally, the term theory informed ML refers to architectures which are imposed by a certain theoretical knowledge.\
Approaches and models we propose in subsequent sections can be viewed as both physics informed and geometry informed ML. Moreover, many of them are also energy-based models.(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>The Twilight Movies in Chronological Order: A Watch Guide</title><link>https://hackernoon.com/the-twilight-movies-in-chronological-order-a-watch-guide?source=rss</link><author>Jose</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:19:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Although the Twilight movie franchise ended over 10 years ago, it has remained seeded in people’s hearts. It had it all: vampires, werewolves, a love triangle, and a really distracting CGI baby. It was just a different time, really, and you had to be there to fully experience the Twilight craze. \
But for those who couldn’t, this is the next best thing. Here are all 5 Twilight movies in chronological order. So, the next weekend you have free, why not make the leap into the unforgettable world of Twilight?How to Watch the Twilight Movies in OrderThe Twilight Saga: New MoonThe Twilight Saga: EclipseThe Twilight Saga: Breaking Dawn - Part 1The Twilight Saga: Breaking Dawn - Part 2The world wasn't ready for this one. Released in 2008, the general audience was immediately sucked into the world of Twilight, and obsessed with the relationship between normal human girl Bella Swan and 100+ year-old Edward Cullen.\
But this was only the beginning, and pretty soon the world would be obsessed with a third character…\
Find out where you can watch Twilight here.2. The Twilight Saga: New Moon (2009)Even though he was introduced in the first movie, we didn't get to see much of Jacob, Bella's childhood friend. However, in New Moon, Jacob takes center stage.\
After Edward breaks up with Bella and leaves town, Jacob helps Bella deal with the emotional fallout, while also dealing with his werewolf transformation.\
Find out where you can watch The Twilight Saga: New Moon here.3. The Twilight Saga: Eclipse (2010)The love triangle between our three heroes becomes increasingly complicated as they find themselves caught in the middle of a conflict between the Cullens, the werewolves, and a new group of vampires.\
Bella has to choose who she wants to be with while also dealing with the dangers of these new vampires.\
Find out where you can watch The Twilight Saga: Eclipse here.4. The Twilight Saga: Breaking Dawn - Part 1 (2011)A story so big that it had to be broken up into 2 parts.\
In Breaking Dawn - Part 1, Edward and Bella finally get married, but quickly get hit with the realization that a vampire and a human marriage isn't so easy.\
Find out where you can watch The Twilight Saga: Breaking Dawn - Part 1 here.5. The Twilight Saga: Breaking Dawn - Part 2 (2012)After 4 years and 4 films, The Twilight Saga reaches its epic conclusion with Breaking Dawn - Part 2.\
The Cullens have had a rocky relationship with the Volturi, the most powerful vampire coven, throughout the film series, and things finally come to a head, with both groups preparing for an all-out battle.\
Find out where you can watch The Twilight Saga: Breaking Dawn - Part 2 here.Now that you know how to watch the Twilight movies in order, grab your nearest blanket, your bucket of popcorn, and start binging. ]]></content:encoded></item><item><title>California Becomes First State To Join WHO Disease Network After US Exit</title><link>https://yro.slashdot.org/story/26/01/23/2350246/california-becomes-first-state-to-join-who-disease-network-after-us-exit?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[California became the first U.S. state to join the World Health Organization's Global Outbreak Alert and Response Network (GOARN), one day after the U.S. formally exited the WHO. The Hill reports: This announcement comes just one day after the U.S.'s withdrawal from the WHO became official after nearly 80 years of membership, having been a founding member of the organization. "The Trump administration's withdrawal from WHO is a reckless decision that will hurt all Californians and Americans," [California Governor Gavin Newsom] said in a statement. "California will not bear witness to the chaos this decision will bring. We will continue to foster partnerships across the globe and remain at the forefront of public health preparedness, including through our membership as the only state in WHO's Global Outbreak Alert & Response Network."]]></content:encoded></item><item><title>Campaigner Launches $2 Billion Legal Action In UK Against Apple Over Wallet&apos;s &apos;Hidden Fees&apos;</title><link>https://news.slashdot.org/story/26/01/23/2328228/campaigner-launches-2-billion-legal-action-in-uk-against-apple-over-wallets-hidden-fees?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Longtime Slashdot reader AmiMoJo shares a report from the Guardian: The financial campaigner James Daley has launched a 1.5 billion pound (approximately $1.5 billion) class action lawsuit against Apple over its mobile phone wallet, claiming the U.S. tech company blocked competition and charged hidden fees that ultimately harmed 50 million UK consumers. The lawsuit takes aim at Apple Pay, which they say has been the only contactless payment service available for iPhone users in Britain over the past decade.
 
Daley, who is the founder of the advocacy group Fairer Finance, claims this situation amounted to anti-competitive behavior and allowed Apple to charge hidden fees, ultimately pushing up costs for banks that passed charges on to consumers, regardless of whether they owned an iPhone. It is the first UK legal challenge to the company's conduct in relation to Apple Pay, and takes place months after regulators like the Competition and Markets Authority and the Payments Systems Regulator began scrutinising the tech industry's digital wallet services. The case has been filed with the Competition Appeal Tribunal, which will now decide whether the class action case can move forward.
 
[...] Daley's lawsuit alleges that Apple refused to give other app developers and outside businesses access to the contactless payment technology on its iPhones, which meant it could charge banks and card issuers fees on Apple Pay transactions that his lawyers say "are not in line with industry practice." The lawsuit notes that similar fees are not charged on equivalent payments on Android devices, which are built by Google. It says that the additional costs were borne by UK consumers, having been passed on through charges on a range of personal banking products ranging from current accounts, credit cards, to savings and mortgages. The lawsuit says that about 98% of consumers are exposed to banks that listed cards on Apple Pay, meaning the vast majority of the UK population may have been affected.]]></content:encoded></item><item><title>Why Kubernetes Outages Are Usually Human Failures, Not Platform Bugs</title><link>https://hackernoon.com/why-kubernetes-outages-are-usually-human-failures-not-platform-bugs?source=rss</link><author>David Iyanuoluwa Jonathan</author><category>tech</category><pubDate>Sat, 24 Jan 2026 01:17:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Kubernetes isn’t inherently complex—teams create fragility through undocumented tooling, hero engineering, and unchecked operational sprawl. The fix is discipline, simplification, and shared understanding.]]></content:encoded></item><item><title>Search Engines, AI, And The Long Fight Over Fair Use</title><link>https://www.eff.org/deeplinks/2026/01/search-engines-ai-and-long-fight-over-fair-use</link><author>Joe Mullin</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/copyright-orange_0_0.png" length="" type=""/><pubDate>Sat, 24 Jan 2026 01:09:20 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GNU C Library 2.43 Released With More C23 Features, mseal &amp; openat2 Functions</title><link>https://www.phoronix.com/news/GNU-C-Library-Glibc-2.43</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 24 Jan 2026 01:06:19 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Version 2.43 of the GNU C Library "glibc" was released on Friday evening as the newest half-year feature update. This is a very feature packed update and even managed to be released ahead of the 1 February release plan...]]></content:encoded></item><item><title>Explainable AI (XAI) in Healthcare: Trust, Transparency, and the Limits of AI Decisions</title><link>https://hackernoon.com/explainable-ai-xai-in-healthcare-trust-transparency-and-the-limits-of-ai-decisions?source=rss</link><author>Srikanth Akkaru</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:56:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[XAI focuses on making AI models transparent and trustworthy, so clinicians and patients can actually trust an AI models. XAI tools like Grad-CAM (heatmaps) show which regions influenced diagnoses like cancer detection, radiology, pathology and dermatology.]]></content:encoded></item><item><title>Justice Department Opens Criminal Probe Into Silicon Valley Spy Allegations</title><link>https://yro.slashdot.org/story/26/01/23/2317211/justice-department-opens-criminal-probe-into-silicon-valley-spy-allegations?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The U.S. Department of Justice has opened a criminal investigation into Deel over allegations that it recruited a spy inside rival Rippling, according to documents seen by The Wall Street Journal. From the report: An Ireland-based Rippling employee, Keith O'Brien, alleged in an affidavit filed in April that Deel Chief Executive Alex Bouaziz recruited him and gave him instructions for what information to take from Rippling. O'Brien alleged that other executives were involved in the spying plot, including Bouaziz's father, who is Deel's executive chairman and chief strategy officer.
 
A spokeswoman for Deel said the company isn't aware of a criminal investigation but is willing to cooperate with authorities. The company has previously said: "We deny all legal wrongdoing and look forward to asserting our counterclaims." Unsealed court documents allege that an entity tied to Deel transferred $6,000 to an account owned by the wife of Chief Operating Officer Dan Westgarth, and that the same amount was forwarded from the account to O'Brien seconds later.]]></content:encoded></item><item><title>Senior Engineers Are Becoming Failure Designers</title><link>https://hackernoon.com/senior-engineers-are-becoming-failure-designers?source=rss</link><author>David Iyanuoluwa Jonathan</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:44:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Senior developers know that designing for success means designing for failure. The new breed of engineer spends as much time planning how things will fail as how they work.]]></content:encoded></item><item><title>Gas Inefficiencies Developers Don&apos;t Notice Until It&apos;s Too Late</title><link>https://hackernoon.com/gas-inefficiencies-developers-dont-notice-until-its-too-late?source=rss</link><author>koxy</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:34:56 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Gas costs spiral out of control when smart contracts misuse storage, loops, and validation order. Across EVM and non-EVM chains, inefficient code turns usable protocols into expensive failures—often without obvious bugs or exploits.]]></content:encoded></item><item><title>Who’s behind AMI Labs, Yann LeCun’s ‘world model’ startup</title><link>https://techcrunch.com/2026/01/23/whos-behind-ami-labs-yann-lecuns-world-model-startup/</link><author>Anna Heim</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:04:45 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Yann LeCun's new venture, AMI Labs, has drawn intense attention since the AI scientist left Meta to found it. ]]></content:encoded></item><item><title>TikTok Is Now Collecting Even More Data About Its Users</title><link>https://yro.slashdot.org/story/26/01/23/236200/tiktok-is-now-collecting-even-more-data-about-its-users?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 24 Jan 2026 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Wired: When TikTok users in the U.S. opened the app today, they were greeted with a pop-up asking them to agree to the social media platform's new terms of service and privacy policy before they could resume scrolling. These changes are part of TikTok's transition to new ownership. In order to continue operating in the U.S., TikTok was compelled by the U.S. government to transition from Chinese control to a new, American-majority corporate entity. Called TikTok USDS Joint Venture LLC, the new entity is made up of a group of investors that includes the software company Oracle. It's easy to tap "agree" and keep on scrolling through videos on TikTok, so users might not fully understand the extent of changes they are agreeing to with this pop-up.
 
Now that it's under U.S.-based ownership, TikTok potentially collects more detailed information about its users, including precise location data. Here are the three biggest changes to TikTok's privacy policy that users should know about. TikTok's change in location tracking is one of the most notable updates in this new privacy policy. Before this update, the app did not collect the precise, GPS-derived location data of U.S. users. Now, if you give TikTok permission to use your phone's location services, then the app may collect granular information about your exact whereabouts. Similar kinds of precise location data is also tracked by other social media apps, like Instagram and X.
 
[...] Rather than an adjustment, TikTok's policy on AI interactions adds a new topic to the privacy policy document. Now, users' interactions with any of TikTok's AI tools explicitly fall under data that the service may collect and store. This includes any prompts as well as the AI-generated outputs. The metadata attached to your interactions with AI tools may also be automatically logged. [...] This change to TikTok's privacy policy may not be as immediately noticeable to users, but it will likely have an impact on the types of ads you see outside of TikTok. So, rather than just using your collected data to target you while using the app, TikTok may now further leverage that info to serve you more relevant ads wherever you go online. As part of this advertising change, TikTok also now explicitly mentions publishers as one kind of partner the platform works with to get new data.]]></content:encoded></item><item><title>White House Push AI-Altered Images Of Arrested ICE Protesters To Manufacture Cruelty</title><link>https://www.techdirt.com/2026/01/23/white-house-push-ai-altered-images-of-arrested-ice-protesters-to-manufacture-cruelty/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Fri, 23 Jan 2026 23:28:33 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[We are being led by deeply unserious people. Not only that, but people who are manufacturing cruelty upon their very own constituents. That’s how bad this has gotten.This week, the DOJ arrested three people in Minnesota for protesting ICE’s goonish activity in a local church, where the pastor there also heads up the local ICE field office. Among the three is Nekima Levy Armstrong, former NAACP chapter president and a local activist who the DOJ claims organized the protest and instigated the group going into the church during services. Just how true any of that is is anyone’s guess, since it’s become impossible to believe a single thing this government says about ICE protests. For example:There was no attack. There was no violence. There were words and chants being voiced in a place of worship. You can find that repugnant, if you like. It’s still not an attack. The law being cited for the arrest makes Armstrong’s detention dubious at best.The law Bondi cited in her announcement — 18 U.S. Code § 241 — describes it pertaining to when “two or more persons conspire to injure, oppress, threaten, or intimidate any person in any State, Territory, Commonwealth, Possession, or District in the free exercise or enjoyment of any right or privilege secured to him by the Constitution or laws of the United States.”While many in the faith community are obediently clutching their figurative pearls over all of this, I’m struggling to understand how walking into a church that’s open to the public and saying words, even interrupting services, violates that law. I don’t think it does, but then I also laughed out loud when I read Bondi’s claim that this was an “attack.” The plain meaning of words doesn’t appear to matter to these people all that much.Yes, the White House decided to take an image of law enforcement improperly arresting an American citizen, one of their own constituents, and have AI alter it to make it appear that she is in distress. Oh, and they made her skin tone slightly darker as well. Because they  her to have been in distress. It eats them up inside that she wasn’t crying. That  her to be “blacker” because they want all of their enemies to be people of color. They’re showing you want they want to visit upon .And until they are put in check, they will continue to behave like a toddler with unfettered access to the internet and a permanently shitty attitude.Asked whether the image had been digitally altered, the White House responded by sending a post on X from Kaelan Dorr, the deputy communications director.“YET AGAIN to the people who feel the need to reflexively defend perpetrators of heinous crimes in our country I share with you this message: Enforcement of the law will continue. . Thank you for your attention to this matter,” he said.And thank you, Kaelan, for going outside and playing hide and go fuck yourself. Again, deeply unserious people. Shitposters. Internet trolls. These are the people in charge of the government. The ones sending their goon squads into our cities. The ones threatening to use the military against its own citizens. The ones that believe they are beyond accountability for all they are currently doing.I worry seriously that the president’s health is such that he won’t be available to stand trial whenever our government returns to sanity and the time for accountability arrives. But the same can’t be said for those beneath him. Bondi, Noem, Dorr, and many others  be held to account for what they are doing in this administration. The ledger  be kept and debts satisfied through the legal system, once actual justice is back on the menu.For now, the fight against the toddlers continues.]]></content:encoded></item><item><title>White House Labels Altered Photo of Arrested Minnesota Protester a &apos;Meme&apos;</title><link>https://yro.slashdot.org/story/26/01/23/1928230/white-house-labels-altered-photo-of-arrested-minnesota-protester-a-meme?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The White House doubled down after posting a digitally altered photo of Minnesota protester Nekima Levy Armstrong, dismissing it as a "meme" despite objections from her attorney and comparisons to reality-distorting propaganda. "YET AGAIN to the people who feel the need to reflexively defend perpetrators of heinous crimes in our country I share with you this message: Enforcement of the law will continue. The memes will continue. Thank you for your attention to this matter," White House spokesperson Kaelan Dorr wrote in a post on X. The Hill reports: The statement came after Homeland Security Secretary Kristi Noem posted a photo of Armstrong's arrest Thursday showing Armstrong with what appears to be a blank facial expression. However, the White House later posted an altered version of the same photo that shows Armstrong crying.
 
Armstrong's attorney Jordan Kushner said in an interview with CNN that an agent was recording Armstrong's arrest on their cellphone. "I've never seen anything like it. It's so unprofessional," Kushner said. "He was ordered to do it because the government was looking to make a spectacle of this case. I observed the whole thing. She was dignified, calm, rational the whole time." Kushner went on to call the move to alter the photo "a hallmark of a fascist regime where they actually alter reality."]]></content:encoded></item><item><title>PowerShell Architect Retires After Decades At the Prompt</title><link>https://tech.slashdot.org/story/26/01/23/1915259/powershell-architect-retires-after-decades-at-the-prompt?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Jeffrey Snover, the driving force behind PowerShell, has retired after a career that reshaped Windows administration. The Register reports: Snover's retirement comes after a brief sojourn at Google as a Distinguished Engineer, following a lengthy stint at Microsoft, during which he pulled the company back from imposing a graphical user interface (GUI) on administrators who really just wanted a command line from which to run their scripts. Snover joined Microsoft as the 20th century drew to a close. The company was all about its Windows operating system and user interface in those days -- great for end users, but not so good for administrators managing fleets of servers. Snover correctly predicted a shift to server datacenters, which would require automated management. A powerful shell... a PowerShell, if you will.
 
[...] Over the years, Snover has dropped the occasional pearl of wisdom or shared memories from his time getting PowerShell off the ground. A recent favorite concerns the naming of Cmdlets and their original name in Monad: Function Units, or FUs. Snover wrote: "This abbreviation reflected the Unix smart-ass culture I was embracing at the time. Plus I was developing this in a hostile environment, and my sense of diplomacy was not yet fully operational." Snover doubtless has many more war stories to share. In the meantime, however, we wish him well. Many admins owe Snover thanks for persuading Microsoft that its GUI obsession did not translate to the datacenter, and for lengthy careers in gluing enterprise systems together with some scripted automation.]]></content:encoded></item><item><title>Microsoft Gave FBI a Set of BitLocker Encryption Keys To Unlock Suspects&apos; Laptops</title><link>https://it.slashdot.org/story/26/01/23/1910235/microsoft-gave-fbi-a-set-of-bitlocker-encryption-keys-to-unlock-suspects-laptops?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 22:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: Microsoft provided the FBI with the recovery keys to unlock encrypted data on the hard drives of three laptops as part of a federal investigation, Forbes reported on Friday. Many modern Windows computers rely on full-disk encryption, called BitLocker, which is enabled by default. This type of technology should prevent anyone except the device owner from accessing the data if the computer is locked and powered off.
 
But, by default, BitLocker recovery keys are uploaded to Microsoft's cloud, allowing the tech giant -- and by extension law enforcement -- to access them and use them to decrypt drives encrypted with BitLocker, as with the case reported by Forbes. The case involved several people suspected of fraud related to the Pandemic Unemployment Assistance program in Guam, a U.S. island in the Pacific. Local news outlet Pacific Daily News covered the case last year, reporting that a warrant had been served to Microsoft in relation to the suspects' hard drives.
 
Kandit News, another local Guam news outlet, also reported in October that the FBI requested the warrant six months after seizing the three laptops encrypted with BitLocker. [...] Microsoft told Forbes that the company sometimes provides BitLocker recovery keys to authorities, having received an average of 20 such requests per year.]]></content:encoded></item><item><title>Waymo probed by National Transportation Safety Board over illegal school bus behavior</title><link>https://techcrunch.com/2026/01/23/waymo-probed-by-national-transportation-safety-board-over-illegal-school-bus-behavior/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:50:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The NTSB joins the National Highway Traffic Safety Administration in investigating Waymo vehicles illegally passing stopped school buses.]]></content:encoded></item><item><title>Why Decentralized Validator Infrastructure Is Critical for Institutional Staking</title><link>https://hackernoon.com/why-decentralized-validator-infrastructure-is-critical-for-institutional-staking?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:45:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
By Prash Pandit, VP Validation Business at A technical look at how decentralized validator architecture gives institutions better reliability, auditability, and system-level resilience.If you’ve ever actually run validators — not reviewed a diagram, not talked strategy in a meeting, but operated them — you figure out quickly that  isn’t passive. It behaves like a live distributed system. Clients drift. Gossip traffic gets noisy. Relays hiccup at precisely the wrong moment. And when you scale that across institutional-sized positions, the infrastructure stops being a supporting detail. It becomes part of your risk surface.Most institutional teams start with custodial platforms because those platforms make the early steps painless. That’s a reasonable first phase. Institutions have onboarding, governance, and compliance requirements that don’t just disappear because a blockchain is involved. But once you look at what a validator is actually responsible for — meeting attestation deadlines, proposing blocks on schedule, keeping up with fork-choice changes, routing through relays, managing duties that repeat every few seconds — the idea of putting all of that inside a sealed box starts to feel mismatched with how the network behaves. Validators aren’t static yield engines. They’re consensus actors.Centralized setups tend to run large validator fleets on nearly identical stacks. Same client builds. Same relay preferences. Same tuning. Same monitoring assumptions. That uniformity looks stable from the outside, but uniformity has a well-known weakness: when something breaks, it breaks everywhere at once. A client bug or a relay stall doesn’t stay local; it becomes a correlated event. Anyone who has worked through a real incident review knows how quickly that can turn into operational noise and awkward reporting questions.\
 is built to avoid that. Instead of relying on one operator’s environment, responsibilities get spread across several operators who don’t share the same failure modes. They run different clients. They make different operational choices. Their infrastructure isn’t a carbon copy of anyone else’s. You get genuine separation. Failures stay smaller.This is where decentralization begins to look less like a philosophy and more like the thing that keeps a large validator footprint stable.\
 takes that one step further. Instead of a single signer making decisions, you use threshold cryptography across multiple nodes. No operator holds the whole key. The validator acts only when enough shares arrive. If one node drifts, the validator doesn’t stall. If one node misconfigures its client, the validator doesn’t head toward slashing. It behaves more like other high-availability systems institutions already trust: distributed, fault-tolerant, and designed so no individual component can sink the whole service.\
This architecture also fixes a visibility gap. Eventually someone will ask why a validator underperformed in a specific epoch, or why duties were missed, or why a particular MEV path was chosen. In a centralized environment, you usually get an aggregated answer because everything underneath is identical. In a decentralized environment, operator-level differences exist by design, which makes performance observable. It gives institutions something they rarely get from sealed systems: the ability to reason about behavior the same way they would with any other critical workload.Key management improves too. Large centralized fleets often keep operational keys online to manage thousands of validators smoothly. It’s practical, but it’s still a single custody point. In a threshold-based decentralized setup, the key never exists in one place. No operator can act alone. The architecture itself enforces the guardrails. That aligns well with how institutional security models already work — distributed approvals, multi-party controls, and reduced single-operator exposure.Flexibility is another place decentralization pays off. Institutions don’t always worry about operator rotation at the start, but it surfaces sooner than expected. Policies change. Infrastructure standards shift. Governance committees ask new questions. In a centralized model, the whole validator setup — keys, clients, MEV routes, reporting — is bundled. Switching becomes expensive. In decentralized architectures, operators function as replaceable components. If one underperforms, you rotate them out without redesigning the validator from scratch.\
None of this means custodial platforms don’t add value. They absolutely do, especially for teams that want a low-friction introduction to staking. But institutions eventually move past the onboarding phase. They start caring about auditability, failure isolation, key distribution, and how the system behaves when conditions get messy. Those aren’t features you bolt on later. They come from the architecture.Proof-of-Stake wasn’t built for single-operator control. It was built for distributed participation. The closer institutional staking setups follow that pattern, the more predictable and transparent they become — not just in normal conditions but in the moments that matter.That’s why decentralized infrastructure ends up being non-negotiable. Not because it sounds good on paper, but because it delivers the reliability and clarity institutions already expect from every other critical system they run. It’s simply the architecture that scales with the network and with the responsibility that comes with meaningful stake.]]></content:encoded></item><item><title>A Year In, And It’s Time To Recognize: The Oval Office Is Empty</title><link>https://www.techdirt.com/2026/01/23/a-year-in-and-its-time-to-recognize-the-oval-office-is-empty/</link><author>Cathy Gellis</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:42:44 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[It will be to the everlasting shame of all Americans that impeachment has not yet been accomplished to formally remove Trump from office. Not in his previous term, and not this one, at least not so far. In fact, this term it has hardly even been attempted. If it weren’t for Representative Green honoring his oath of office it wouldn’t have even been tried at all. Even  are still in significant numbers joining their Republican colleagues in refusing to do what is needed to save our constitutional order, despite everything Trump has done from the moment he retook office—including taking the office, which he was ineligible to do as a confirmed insurrectionist—being entirely inconsistent with the Constitution’s instructions for how to achieve a democratically sustainable federalized union of states.Impeachment still needs to happen, for Trump and his minions, not just to cleanly expel Trump from the presidency but to disqualify him from ever returning to it. And that expulsion needs to happen with an urgency that really required it to have been completed at the latest by last March. Yet the way things are going, with Congress dragging its feet, it seems we’ll be lucky if it will even happen by  March, if at all. Moreover, with Trump upping the ante at every turn, we’ll be lucky if the nation, all its constituent states, and even most of the people who depend on the Constitution’s promises of liberty, freedom, and justice for all, are still standing by then if nothing is done to officially separate him from the powers of the office he continues to claim. After all, every day of delay is another day for a five year old to be shipped to a concentration camp in Texas. Even if the nation survives this presidency, it’s already clear we won’t all.But it turns out, Trump has already begun to separate himself from the presidency.  And that he has done so reveals another path the Constitution allows for retaking our democracy, starting now. On this appalling anniversary week of Trump’s installation as the 47th President of the United States, it is time to recognize an essential truth: he has functionally already abandoned the office. Sure, he still (nominally) lives in the White House, meets heads of state (and insults them), is answered to by the military (however ill-advisedly), signs bills (and pardons!), and at least superficially seems to be conducting the basic functions of the office. These are things that the Constitution allows presidents to do, not because, as Trump seems to think, the Constitution seeks to reward a single person with the special power to do any of them, but because these are governmental functions someone needs to do and it makes more sense to grant a chief executive the ability to do them than someone in any other branch of government.But the exercise of these functions is not the full extent of the job. The job of president, as the Constitution describes, also includes the requirements to “take care that the laws be faithfully executed,” and to fulfill the oath he swore upon taking office, which included the promise to “preserve, protect and defend the Constitution of the United States.” None of these obligations are incidental to the job; they are key counterbalances to the enormous power the position affords its occupant. Yet he has been doing none of them in any sort of meaningful way, if at all. In fact, all too often he instead does the exact opposite of what’s required by the job, including by engaging in his own criminality, abetting the malfeasance of others, and otherwise generally upending our constitutional order by ignoring statutes, treaties, and constitutional text, and turning every bit of power he’s managed to wring from his position against the very same public the Constitution says he works for.There are few situations where we would consider someone not doing what they were hired to do, and in fact doing the very opposite of what their job required, as still being employed in that job. If you hire a guard to watch the bank, you’d expect him not to help the robbers rob the bank. If you hired a doctor to treat patients, you’d expect him to not kill them instead. But if while on the job they did the opposite of what they were hired to do, you would understand them to have abandoned their position. You wouldn’t expect the guard who let in the robbers on Tuesday to still show up to work securing the place on Wednesday, or the doctor who euthanized his patients Thursday to show up to treat more on Friday; you would understand from the moment they did these things that you now have some vacancies to fill.Which is where we find ourselves. The degree to which Trump has refused to perform the requirements of his job, to say nothing of his regularly acting contrary to them, means that we effectively have a vacancy in the Executive Branch. Americans can no longer have any trust that he is working for us when he daily demonstrates that he is only working for himself. Or that he’ll enforce the law when he regularly transgresses it and enables others’ transgressions as well. Or that he’ll uphold the Constitution when he regularly violates the separation of powers and people’s protected rights. Or that he can be a protector of the country when he has used his position to attack it. Like with the larcenous bank guard or wayward doctor it would be irrational to believe that despite having acted in such conflict with the requirements of his job that it is a job he has nevertheless somehow still kept. Instead, by refusing to uphold his oath of office, and acting in so many ways counter to it, he has effectively abandoned the office he took that oath in order to enter.The Constitution says that when the office is vacant there is a succession process to fill it.  Where it is less specific is in instructing how a  vacancy, such as the kind we are experiencing, can be regarded as an official  one for purposes of triggering succession.  But it doesn’t say we can’t, and plenty of language in the Constitution says we can, and indeed must.Per Article II, Section 1, of the Constitution, succession happens when there is either a physical departure from the office by the President, such as through death or resignation, or a functional one, essentially measured by the “Inability to discharge the Powers and Duties of the said Office.” (The 12th Amendment, as amended by the 20th Amendment, also indicates that a vacancy is created when there is a “constitutional disability,” which would seem to include his ineligibility for the office as an additional obstacle to him being able to discharge the powers and duties of the office.) While for Trump there also remains the possibility of mental incapacity being yet another reason he is unable to fulfill the responsibilities of the office, in addition to his conscious abandonment of the position, it all boils down to the same thing: he has demonstrated that he is unable to continue serving in the role as the Constitution requires. The vacancy thus exists, and now it just needs to be officially recognized so that succession can begin.The 25th Amendment describes one avenue for such recognition, but that particular process seems unlikely to be pursued any time soon given that it would require equally compromised cabinet members to unite with the Vice President to support Trump’s displacement, which they are unlikely to do as long as they feel they benefit from Trump remaining in office (which is, of course, a reason why Hegseth, Rubio, Noem, Bondi, etc. should also themselves all be impeached, so that there’s a snowball’s chance that more ethical people could take their place for 25th Amendment purposes). But it seems unlikely that the Constitution meant the 25th Amendment to be the  process available for recognizing that effectively there’s already a vacancy in the presidency, for several reasons. For one thing, the way it is written it seems more attuned to articulating a plan for succession in the face of a  disability, like a coma, because it includes a mechanism by which the succession can be undone. Whereas abandoning the office, like Trump has done, does not seem, consistent with the spirit of the Constitution if not the letter, like something that can simply be undone without being re-elected. Furthermore, as we see here, the 25th Amendment does not correct for the sort of situation we find ourselves in now, where the people who could and should be invoking it are not, even though the essential problem remains: there is still no one currently at the helm of the United States of America doing the job in a way the Constitution requires. And such will remain the case regardless of whether Vance and company ever make a move to address it.Impeachment is of course another appropriate option for addressing a wayward president who is not living up to the job, but it, too, cannot be the only other means for handling a situation like this, where his failure to perform the job as required has already created the vacancy. For one thing, it suffers from a similar problem as the 25th Amendment, where the right of the public to have a president that lives up to his constitutional obligations is effectively being held hostage by recalcitrant officials—this time those in Congress—who are unwilling to uphold their own oaths of office and do what needs to be done to officially extricate America from Trump’s grasp. Furthermore, impeachment is also designed to pry someone out of a job they are actually doing, and not just someone who is not, as well as apply disqualification as a sanction. It is a mechanism useful for creating a vacancy, but the need now is just to recognize that one already exists.But that there is no other clearly established way for recognizing the vacancy does not mean there is no way. There appears to be another way.  And key to pursuing it is to stop treating as President someone who clearly is not.It would mean, first of all, challenging every bit of power Trump exercises nominally as president as being unlawful, and not just on its own terms as an act not permitted by statute or Constitution, given that most of the things he tries to do would still be unlawful even if a proper president tried to do them.  The challenge needs to be that  Trump does ostensibly as president is irredeemably illicit at its core.  Give the courts the opportunity to at last find that whatever power Trump attempts to wield is power he no longer has. Doing so would likely be an uphill battle, because no court has every nullified a presidency.  To the extent that legitimacy has been in contention, the historical preference has been to settle the matter politically, rather than legally—or at least it was, up until , when the Supreme Court announced that the courts were in the president-anointing business.  But it would make sense for the courts to be able to weigh in here, with respect to Trump, because why shouldn’t the Article III branch would have its own mechanism for addressing the vacancy of an absent president, especially while Articles I and II officials continue to abandon their own obligations to act in accordance with their own constitutional mechanisms.  No branch should have an exclusive monopoly on policing the president, and as long as there has been judicial review, none has.  The courts have long been able to hold presidents accountable to the Constitution.  And while there may be no clearly established roadmap for involving the courts this way, there is also nothing preventing it. The courts could be called upon to declare the office abandoned in various ways, and in response to challenges by various parties. Perhaps such an opportunity to challenge Trump’s legitimacy could arise if JD Vance gets ambitious and sues for a declaratory judgment that he is the actual president, because, while he’s no prize himself, at this point it certainly seems like he has a better claim to the office than Trump does. Perhaps it’s the states who can bring some sort of claim. Perhaps others who are affected by Trump’s abuse could sue too, just as they normally can challenge the lawfulness of his acts. Or perhaps the courts will have to weigh in when the military starts refusing Trump’s orders, as increasingly seems it likely will, as the ways Trump has been directing the military become more and more unlawful even on their own merits. In any case, one way or another it seems inevitable that the legitimacy of Trump’s continued presidency is going to be a question the courts will be called upon to answer, especially as the rest of our government refuses to.And while any litigation would eventually land at the Supreme Court, such as it is, these challenges still need to be pursued because every case before it ultimately stands on its own.  Even  is differentiable in key ways from the litigation that would reach it here.  And hope springs eternal that this time maybe the Court will even get the question before it right, as the stakes raised by these challenges have never been more clear.  Trump is running around acting with impunity, but as even the Supreme Court recognized in , immunity only attaches to official acts.  And if he has already effectively abandoned the office, then none of his acts can be.It is of course no small thing for anyone to declare a living president to have officially abandoned his office or otherwise take steps to delegitimize his occupancy in the office. Nor should it be something that easily can be done because, as we’ve seen with even just with 2020 election denial, once doubt creeps in about who is the legitimate president, the disagreement it causes can be destabilizing to our democratic order. In fact, it is likely that a big reason why Trump’s continued claim to the presidency has simply been accepted so broadly up to now, despite all the evidence, is that, by and large, we would rather delude ourselves into believing that he is the legitimate officeholder than risk the political instability of calling it into question.Nevertheless, there are limits to how long we can maintain the myth of his legitimacy, which Trump has been daily making less and less believable. Hegemony is powerful; Trump only gets to masquerade as a legitimate president for as long as we let him. We don’t have to let him. Which is why we should appeal to the courts, as well as Congress and any politician anywhere in government, to argue not just that Trump should be made to leave but that he’s already left, and that it’s finally time for the government to respond to that reality.It’s time to challenge his legitimacy because the Constitution does not take a time out.  It does not wait for midterms.  We are  entitled to a President that acts consistently with all of the Constitution’s requirements, and it tells us what happens when there isn’t someone doing so anymore. It is not for any of us to decide that this language suddenly somehow no longer applies.In fact, it would be dangerous to, or to deliberately wait months and months to finally address the problem, while in the meantime our nation and everything we’ve built over the course of nearly 250 years is ruined. Especially not when Trump’s abandonment of the job has created the exigent likelihood that an interloper without any personal constitutional authority may now be functionally acting as president instead of him, wielding the office’s powers without any of the accountability the Constitution normally requires of someone in that position. In other words, it may not be that we are just without a president but, worse, instead at the mercy of an unelected pretender who has stepped into the vacuum Trump’s abandonment has created because we have refused to fill that void first.There may of course be the fear that we risk a constitutional crisis to make such a serious move to deem the office vacant when the Constitution is not more specific that it is a move to be made. And it’s true; constitutional crises arise when we start making the most existentially important decisions about the nation’s governance without reference to a set of clear rules we’ve all agreed to. That we are in uncharted waters may thus give pause.But we are not without any instruction for how to navigate them. Even though the Constitution has not provided a specific process to follow perfectly tailored to this effective abandonment of the presidency that Trump has committed, it still provides enough guidance to recognize the position is vacant and proceed with succession accordingly. If anything, it is the refusal to recognize the vacancy, especially by Congress and the cabinet, that has been what’s unilaterally and unconstitutionally changed the rules we’ve all previously agreed to, by letting Trump nevertheless continue to occupy the position when he has in every other way abandoned the job. Given everything Trump has done, and the actual text of the Constitution forbidding it, challenging his right to remain the acknowledged president won’t invite a constitutional crisis; rather, it is the failure to bring that challenge which is why that crisis is already here.]]></content:encoded></item><item><title>The Future of Media Is Automated: Lior Alexander’s Vision for Information Infrastructure</title><link>https://hackernoon.com/the-future-of-media-is-automated-lior-alexanders-vision-for-information-infrastructure?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As artificial intelligence accelerates the creation of content, the internet has entered an era defined less by scarcity and more by saturation. Millions of new posts, research papers, and media artifacts appear daily, making discovery, not creation, the primary challenge. For , founder of , this shift marked the beginning of his work.Rather than asking how to produce more content, Alexander focused on a more fundamental question: how do people find what actually matters when everything is being generated at once?From Research Lab to System Builder’s path into AI began in 2017, when he joined the research lab of Turing Award–winning scientist Yoshua Bengio in Montreal. At the time, machine learning research was expanding rapidly, but access to meaningful insights remained limited.“Hundreds of papers were being uploaded every week,” Alexander recalled. “There was no effective way to filter them. Even researchers inside the lab were overwhelmed.”That experience shaped his thinking. Instead of focusing on model development alone, Alexander became interested in the infrastructure surrounding knowledge itself: how information is surfaced, ranked, and interpreted. He began experimenting with tools that could track research activity across the web, identify emerging signals, and surface them in a usable way.That early system would become the foundation of AlphaSignal.AlphaSignal was not designed as a traditional media company. From the beginning, it was structured as an automated system capable of detecting, ranking, and contextualizing information at scale. While most newsrooms rely on teams of editors and writers, AlphaSignal relies on software.“I built the entire system myself,” Alexander said. “The ranking models, the data pipelines, the publishing workflows, the branding. I didn’t have a team or outside funding.”The platform continuously scans technical papers, product releases, funding announcements, and research activity, identifying patterns that signal meaningful developments. Instead of reacting to trends after they peak, AlphaSignal aims to detect momentum early.That approach has proven effective. The platform now reaches more than 250,000 subscribers, over 500,000 followers, and has generated more than 200 million impressions. It also became an early visibility engine for companies such as ElevenLabs and Lovable, helping surface them before they reached mainstream attention.Running AlphaSignal as a solo operation forced Alexander to rethink how media organizations function. Rather than scaling through headcount, he focused on automation and system design.“I had to do everything: engineering, research, distribution, partnerships,” he said. “The only way to make that sustainable was to build systems that could operate without constant human input.”This approach mirrors the broader shift he sees happening across industries: replacing manual workflows with intelligent systems capable of handling complexity at scale. In his view, the future belongs to organizations that treat information processing as infrastructure, not editorial labor.Today, AlphaSignal functions less like a publication and more like an intelligence layer for the AI ecosystem. Its tools identify emerging trends, map technical progress, and help engineers, investors, and researchers understand where innovation is actually occurring.Looking ahead, Alexander plans to expand the system beyond AI into other sectors facing similar overload, including finance, cybersecurity, and biotechnology. His long-term goal is to build what he describes as a “universal signal engine” — a platform capable of ranking relevance across any domain overwhelmed by information.We’re entering a period where most content will be machine-generated,” he said. “The real value won’t be in producing more of it, but in building systems that help people understand what matters.For Alexander, that challenge defines the next era of media, one where clarity, not volume, becomes the most valuable commodity.]]></content:encoded></item><item><title>Toilet Maker Toto&apos;s Shares Get Unlikely Boost From AI Rush</title><link>https://slashdot.org/story/26/01/23/1855256/toilet-maker-totos-shares-get-unlikely-boost-from-ai-rush?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Shares of Japanese toilet maker Toto gained the most in five years after booming memory demand excited expectations of growth in its little-known chipmaking materials operations. The stock surged as much as 11%, its steepest rise since February 2021, after Goldman Sachs analysts said Toto's electrostatic chucks used in NAND chipmaking will likely benefit from an AI infrastructure buildout that's tightening supplies of both high-end and commodity memory. 

[...] Known for its heated toilet seats, the maker of washlets has for decades been part of the semiconductor and display supply chain via its advanced ceramic parts and films. Its electrostatic chucks -- which it began mass producing in 1988 -- are used to hold silicon wafers in place during chipmaking while helping to control temperature and contamination, according to the company. The company's new domain business accounted for 42% of its total operating income in the fiscal year ended March 2025, Bloomberg-compiled data show.]]></content:encoded></item><item><title>Why Short-Lived Certificates Are Revolutionizing Security in Modern Infrastructure</title><link>https://hackernoon.com/why-short-lived-certificates-are-revolutionizing-security-in-modern-infrastructure?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:15:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Security engineers often joke that certificates are invisible until they break something important. Yet in modern infrastructure, certificates quietly enable nearly every secure interaction. From service-to-service communication to machine identity, they form the backbone of digital trust. What has changed is not their importance, but how long they are allowed to exist.Across the industry, long lived certificates are giving way to ephemeral certificates that are short lived, automated, and continuously rotated. This shift reflects a growing recognition that static trust models struggle to keep pace with distributed systems that evolve continuously.The evolution toward ephemeral certificate management has emerged through sustained dialogue across professional communities. Engineers and security leaders exchange experiences in British Computer Society forums, Gartner peer discussions, Forbes Technology Council conversations, and IEEE conferences where practical challenges are discussed openly.Within these discussions, Arun Kumar Elengovan is frequently referenced for bringing clarity to how certificate management fits within broader trust architecture. A Director of Engineering Security for an identity security focused organization, he has led and contributed to large scale security programs across complex environments. An award-winning leader with recognition spanning the United States, Canada, Indonesia, Thailand, India, Malaysia, and Australia, he is widely regarded as a distinguished contributor in ephemeral certificate management. His work consistently highlights how short-lived trust models strengthen security posture while improving operational reliability when applied with architectural discipline.His continued engagement across professional councils and technical forums has helped shape a shared understanding that certificate automation is no longer an optional enhancement. It is increasingly viewed as a foundational capability that security leaders must guide deliberately as infrastructure scales.The Fragility of Long-Lived TrustTraditional certificate practices were designed for a slower era. Certificates were issued manually, embedded into applications, and rarely rotated. In discussions across the security engineering community, Arun Kumar Elengovan has pointed out that this model was workable when environments were small and change was infrequent, but its assumptions no longer hold in modern infrastructure.Today, organizations operate across hybrid cloud platforms, microservices, container clusters, serverless workloads, and third-party integrations. Each layer introduces credentials that must be issued, stored, rotated, and retired safely. Arun has emphasized that when certificates persist for extended periods, compromise often remains unnoticed, revocation becomes slow, discovery incomplete, and operational risk accumulates without clear visibility.Security incidents increasingly show that failures do not arise from cryptographic weaknesses, but from credentials that remain active long after their intended use. Across professional and technical forums, this pattern reflects a broader understanding that the durability of trust, rather than cryptographic strength alone, is what most often undermines security in large scale systems.Ephemeral Certificates and Trust AgilityEphemeral certificates offer a different path forward. Rather than embedding trust permanently into systems, trust is applied dynamically at runtime. Certificates are issued only when needed, rotated automatically, and replaced frequently enough to significantly reduce exposure windows.This approach supports trust agility. Applications no longer hold long term credentials. Instead, trust decisions are centralized and enforced consistently across environments. Root of trust remains protected offline, while intermediate trust is delegated safely through automation.The result is a security posture that adapts as systems change rather than falling behind them.Automation as a Foundational RequirementEphemeral certificates cannot function without automation. Discovery, issuance, renewal, revocation, and monitoring must operate continuously. In large environments, organizations often lack a complete inventory of certificates until they actively search for them.Effective automation reflects operational reality. Certificates appear in code repositories, build pipelines, configuration files, network services, and legacy systems. Some applications refresh credentials seamlessly, while others require coordination. Mature certificate programs align rotation with engineering workflows rather than forcing disruption.Automation transforms certificate management from a brittle manual process into a dependable engineering capability.From Certificates to Systems ThinkingOne of the most important shifts in modern security engineering is moving away from treating certificates as isolated artifacts. Certificates intersect with identity systems, secrets management, cloud platforms, and governance frameworks.Issuance relies on private certificate authorities. Storage integrates with secrets systems. Access decisions depend on platform identity. Root of trust choices determine what remains offline and what can be automated safely. Through community discussions and technical exchanges, Arun consistently provides direction on evaluating these dependencies as a unified trust system rather than disconnected controls.Thinking in systems rather than tools enables organizations to design trust that grows with infrastructure instead of resisting it. This architectural perspective has increasingly influenced how security leaders frame certificate management decisions.Why This Matters for Engineers and OrganizationsEphemeral certificates reduce blast radius, shorten exposure windows, and simplify recovery. They also influence behavior. Engineers begin to expect rotation rather than fear it. Credentials are requested dynamically rather than copied. Trust becomes observable and measurable.Arun often underscores that this behavioral shift is as important as the technical controls themselves. Secure systems emerge when teams are given clear direction, consistent patterns, and accountability rather than ad hoc rules.As systems become more distributed, trust must become more dynamic. Automation, resilience, and observability are no longer optional attributes.Trust That Keeps Pace With ChangeAs digital infrastructure continues to evolve, static trust models fall behind. Arun Kumar Elengovan has noted that ephemeral certificates represent a practical response to this reality, aligning security mechanisms with the way modern systems are actually built and operated rather than how they were designed in earlier eras.He has also observed that ongoing conversations across professional communities increasingly converge on short lived trust as a baseline expectation rather than an advanced practice. According to Arun, trust that is automated and intentionally temporary reduces risk while increasing operational confidence, particularly in large scale and highly distributed environments.In this context, ephemeral certificates are not merely a technical improvement. They reflect a leadership driven understanding that security must move at the same pace as the systems it protects, or risk becoming an obstacle rather than an enabler."The views and opinions expressed in this article are the author’s own and do not necessarily reflect those of any affiliated organizations or institutions."]]></content:encoded></item><item><title>The Rippling/Deel corporate spying scandal may have taken another wild turn</title><link>https://techcrunch.com/2026/01/23/the-rippling-deel-corporate-spying-scandal-may-have-taken-another-wild-turn/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:13:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Department of Justice may be conducting a criminal investigation. This is arguably the biggest drama between two HR startups ever.]]></content:encoded></item><item><title>Wine 11.1 Released In Kicking Off The New Development Cycle</title><link>https://www.phoronix.com/news/Wine-11.1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 21:12:17 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following the release of Wine 11.0 stable just under two weeks ago, Wine 11.1 is now available as the first of the bi-weekly development snapshots for Wine in leading toward the Wine 12.0 release next January...]]></content:encoded></item><item><title>The Great Graduate Job Drought</title><link>https://slashdot.org/story/26/01/23/0925259/the-great-graduate-job-drought?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Global hiring remains 20% below pre-pandemic levels and job switching has hit a 10-year low, according to a LinkedIn report, and new university graduates are bearing the brunt of a labor market that increasingly favors experienced candidates over fresh talent. 

In the UK, the Institute of Student Employers found that graduate hiring fell 8% in the last academic year and employers now receive 140 applications for each vacancy, up from 86 per vacancy in 2022-23. US data from the New York Federal Reserve shows unemployment among recent college graduates aged 22-27 stands at 5.8% versus 4.1% for all workers. 

Recruiter Reed had 180,000 graduate job postings in 2021 but only 55,000 in 2024. In a survey of Reed clients last year, 15% said they had reduced hiring because of AI. London mayor Sadiq Khan said the capital will be "at the sharpest edge" of AI-driven changes and that entry-level jobs will be first to go.]]></content:encoded></item><item><title>What to know about Netflix’s landmark acquisition of Warner Bros.</title><link>https://techcrunch.com/2026/01/23/what-to-know-about-netflixs-landmark-acquisition-of-warner-bros/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:31:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Learn more about Netflix's acquisition of Warner Bros., considered the most historic megadeal in Hollywood, as it continues to develop.]]></content:encoded></item><item><title>Researchers say Russian government hackers were behind attempted Poland power outage</title><link>https://techcrunch.com/2026/01/23/researchers-say-russian-government-hackers-were-behind-attempted-poland-power-outage/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:17:24 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Security researchers have attributed the attempted use of destructive "wiper" malware across Poland's energy infrastructure in late December to a Russian-backed hacking group known for causing power outages in neighboring Ukraine.]]></content:encoded></item><item><title>Noem Says ICE Is Being Menaced By Ice Cubes, Protesters Should Be Cooped Up In ‘Free Speech Zones’</title><link>https://www.techdirt.com/2026/01/23/noem-says-ice-is-being-menaced-by-ice-cubes-protesters-should-be-cooped-up-in-free-speech-zones/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:16:21 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[CBS News still exists, despite a president doing all he can to turn it into his own Baghdad Betty. Journalists are still demanding answers from this administration, even while the Baghdadest Betty of all — Bari Weiss — does everything she can to strip mine the long-running news agency for abusable parts. (That refers to you, Tony Dokoupil.)Last Sunday morning, Margaret Brennan interviewed DHS head Kristi Noem on “Face the Nation.” Brennan did everything she could to push back against Noem’s false claims and bullshit assertions, but in the end, Noem clearly knew she’d always have the upper hand, thanks to Trump’s legal threats and Bari Weiss’s willingness to bury reporting that doesn’t please Trump.As ICE continues to detain, arrest, or kill anyone that seems to be too dark or too loud in Minnesota, Brennan asked if there’s an actual end point for yet another federal “surge” targeting a “blue” state. Noem, of course, can’t provide a straight answer despite being given straight facts by the interviewer.MARGARET BRENNAN: According to Pew, Minnesota’s population of immigrants here illegally stands at 2.2%. So, how do you judge when you’ve gotten everyone off the streets, that you say is, you know, requiring your federal agents be there? How do you say we’ve had mission accomplished?SEC. NOEM: Well, we won’t stop until we are sure that all the dangerous people are picked up, brought to justice and then deported back to their home countries–MARGARET BRENNAN: –You don’t have a number or a date?– SEC. NOEM: –We wouldn’t be in this situation- We wouldn’t be in this situation if Joe Biden hadn’t allowed our open-border policies to be in place and allowed up to 20 million people unvetted into this country. We have no idea how many dangerous people are here. That’s not an answer. Most of what fell out of Noem’s mouth during this interview wasn’t a direct answer. Instead, it was a bunch of Trump-esque rambling, randomly punctuated by Noem insisting the person interviewing her was lying. Noem insisted that the “millions” (most of which obviously do not reside in Minnesota) being swept up by ICE were violent criminals. She claimed “70% of them have committed or have charges against them on violent crimes.” Brennan pushed back, citing stats released : MARGARET BRENNAN: Okay, well, our reporting is that 47% based on your agency’s own numbers, 47% have criminal convictions against them*. But let’s talk about the other numbers–SEC. NOEM: –Which means you’re wrong again. Absolutely. We’ll get you the correct numbers–SEC. NOEM: –so you can use them in the future.MARGARET BRENNAN: Well, that’s from your agency. Noem is fully cooked. She’s indistinguishable from Trump or anyone else in his close orbit. When your lies are exposed by facts, you call the person with the actual facts a liar. But this willful disregard for truth is nothing new: this administration divorced itself from reality during Trump’s first term. In its second term, it’s pretending truth is whatever it says it is. But it gets scarier, stupider, and weirder from there. Here’s Noem defending murdering citizens on the street before veering off into an extremely Trumpian interpretation of First Amendment rights: SEC. NOEM: We’ve seen over 100 different vehicle weaponized and attacking law enforcement officers. I would hope that Mayor Frey, when he’s on here, that he’ll announce that he’s going to start working with us to bring safety to the streets. If he would set up a peaceful protest zone so that these individuals can exercise their First Amendment rights and do so peacefully, we would love that, because then we could work together to make sure we’re getting criminals to justice and letting people still express their First Amendment rights.While the government does have extremely limited powers to enact time-and-place restrictions on First Amendment activity, it certainly does not have the power to force any locale to restrict protests to only the places the government will allow protesters to gather. That’s the exact opposite of the First Amendment, which is exactly the sort of thing you’d expect a Trump administration figure to pitch, even if it would never impose restrictions like these on anyone protesting in  of Trump and his policies. (See also: hundreds of pardoned people who engaged in literal insurrection in 2021.) After bitching Brennan out for repeating the name of the officer (Jonathan Ross) who killed Renee Good (apparently it’s “doxxing” to use a published name during a national news interview [massive eye roll]), Noem goes on to claim (without facts in evidence) that ICE officers are dealing with threats up to (and including) frozen water: SEC. NOEM: –Don’t say his name. I mean, for heaven’s sakes, we- we don’t- we shouldn’t have people continue to dox law enforcement when they have an 8,000%–MARGARET BRENNAN: –his name is public–SEC. NOEM: –increase in death threats against them–MARGARET BRENNAN: — he was struck and hospitalized–SEC. NOEM: –I know, but that doesn’t mean it should continue to be said. His life- he got attacked with a car that was trying to take his life, and then people have attacked him and his family, and they are in jeopardy. And we have law enforcement officers every day who are getting death threats and getting attacked at their hotels and they are–MARGARET BRENNAN: –Well, can you tell me about his status right now–SEC. NOEM: –getting ice thrown at them. I can’t imagine why people might be throwing ice at ICE, but I’m sure someone much smarter than me will make that connection. And I may be just a humble small town writer who writes like your average George Bailey, but I have to imagine this might have gone better for Noem if she had decided to end that answer one sentence earlier. This would all be laughably surreal if this administration didn’t have so much power and the will to abuse it. It’s still surreal, but you have to embrace the blackest of comedy to croak out a laugh. This administration only knows two moves: bluster and gaslighting. Whatever you saw, you didn’t see. Whatever violations the government committed never happened. Whatever can be disputed by facts is just the ravings of leftist liars and mainstream media losers. As for everyone caught in this crossfire, fuck ’em. This party only serves itself. If there’s any silver lining here at all, it’s that Noem is too busy being Trump’s Bigot Barbie to kill her children’s pets any time soon. ]]></content:encoded></item><item><title>Wall Street Pushes Solo 401(k)s as More Americans Work for Themselves</title><link>https://news.slashdot.org/story/26/01/23/1218254/wall-street-pushes-solo-401ks-as-more-americans-work-for-themselves?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: A niche retirement plan favored by freelancers is quickly becoming a hot Wall Street sales pitch, as more and more Americans look for ways to shelter a bigger chunk of their paychecks from taxes. Known as solo 401(k)s, they allow the self-employed to contribute $72,000 a year into tax-advantaged retirement accounts. That's nearly three times the maximum for typical salaried workers in the US. 

While they've existed for decades serving a workforce that often struggled to earn enough to max out those contributions, wealth planners like JPMorgan Chase & Co. and Betterment are now racing to tap into burgeoning demand from a newer, and wealthier cohort: Post-pandemic contractors and self-employed DIY savers looking to shelter more income, grow assets tax-deferred or tax-free, all with the click of a button. 

The pitch is simple: Because of a quirk in the tax code, self-employed workers effectively contribute twice to their 401(k)s -- once as an employee on their own behalf and then again as a business owner making matching contributions. The platforms take care of the paperwork and clients get institutional-level tax planning and investment flexibility. More than three-quarters of America's record 36 million small businesses now have just a single employee, the owner. Cerulli Associates projects that total 401(k) plans in the U.S. will surpass 1 million by 2030, and the fastest growth is expected in sub-$5 million "micro" accounts.]]></content:encoded></item><item><title>How did Davos turn into a tech conference?</title><link>https://techcrunch.com/video/how-did-davos-turn-into-a-tech-conference/</link><author>Theresa Loconsolo</author><category>tech</category><pubDate>Fri, 23 Jan 2026 20:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The World Economic Forum’s annual meeting in Davos felt different this year, and not just because Meta and Salesforce took over storefronts on the main promenade. AI dominated the conversation in a way that overshadowed traditional topics like climate change and global poverty, and the CEOs weren’t holding back. There was public criticism of trade policy, warnings about AI […]]]></content:encoded></item><item><title>China Makes Too Many Cars, and the World Is Increasingly OK With It</title><link>https://tech.slashdot.org/story/26/01/23/1213224/china-makes-too-many-cars-and-the-world-is-increasingly-ok-with-it?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 19:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[After years of Western governments raising alarms about Chinese automotive overcapacity and erecting tariff barriers, an unexpected pivot is now underway as major economies cautiously open their markets to Chinese electric vehicles, Bloomberg writes. Beijing itself has started acknowledging the problem at home. Chinese regulators last week warned of "severe penalties" for automakers defying efforts to rationalize pricing in the country's car market, and earlier this month a government ministry urged battery makers to curtail expansion and cutthroat competition. 

The European Union imposed steep tariffs on Chinese EV imports in 2024 and is now considering replacing them with minimum import price agreements. Canada's Prime Minister Mark Carney last week decided to allow 49,000 Chinese EVs annually at a 6.1% tariff rate, removing a 100% surtax. Germany announced this week that its $3.5 billion EV subsidy program will be open to all manufacturers including Chinese brands. Germany's environment minister Carsten Schneider dismissed concerns during a January 19 press conference: "I cannot see any evidence of this postulated major influx of Chinese car manufacturers in Germany, either in the figures or on the roads." 

BYD registered an eightfold increase in sales in Germany last year and pulled ahead of Tesla, though Volkswagen still registered around 2,300 vehicles for every one BYD sold.]]></content:encoded></item><item><title>Firmware Upstreamed For Audio Support With Upcoming Dell &amp; Lenovo Panther Lake Laptops</title><link>https://www.phoronix.com/news/Cirrus-CS42L45-Linux-Firmware</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:53:02 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Similar to the new Intel IPU 7.5 firmware upstreamed for Panther Lake this week, Cirrus has upstreamed their CS42L45 codec firmware for upcoming Dell and Lenovo laptops making use of this audio codec...]]></content:encoded></item><item><title>Solar and Wind Overtake Fossil Fuels in the EU</title><link>https://hardware.slashdot.org/story/26/01/23/127254/solar-and-wind-overtake-fossil-fuels-in-the-eu?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:44:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Wind and solar power overtook fossil fuels last year as a source of electricity in the EU for the first time, a new report found. Semafor adds: The milestone was hit largely thanks to a rise in solar power, which generated a record 13% of electricity in the EU, according to Ember. Together, wind and solar hit 30% of EU electricity generation, edging out fossil fuels at 29%. 

The shift is especially important with the bloc's alternative to Russian LNG -- Washington -- becoming increasingly unreliable and willing to weaponize economic tools. The US Commerce Secretary threw shade at the bloc's renewable push during Davos, warning that China uses net zero goals to make allies "subservient" by controlling battery and critical mineral supply chains. 

Still, renewables now provide nearly half of EU power, with wind and solar outpacing all fossil sources in more than half of member countries. "The stakes of transitioning to clean energy are clearer than ever," the Ember report's author said.]]></content:encoded></item><item><title>Got Ideas For Growing The Open Social Web? Bring Them.</title><link>https://www.techdirt.com/2026/01/23/got-ideas-for-growing-the-open-social-web-bring-them/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:35:04 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[For over three years now, since Elon Musk decided to spend $44 billion turning Twitter into his personal playground, we’ve been watching the open social web slowly, sometimes painfully, come into its own. Bluesky. Mastodon. The broader ATmosphere and fediverse along with a few other experiments (nostr! farcaster!). These aren’t just tech experiments anymore—they’re real alternatives that millions of people use every day.While these open social systems are working, and working well, tons of people are still choosing to stay in closed, proprietary, billionaire-controlled systems, where they have no control, no say in how they work, and no real agency. We’ve heard various excuses. We’ve heard about the pull of inertia. We’ve even heard the complaints that people haven’t found communities they like… or that they actively dislike some of the communities that have formed.So instead of just writing another post about why that matters (I’ve written plenty), Johannes Ernst from FediForum and I are doing something about it. On , we’re hosting an online “un-workshop” focused on one question: how do we actually grow the open social web even more?And, yes, I’m on the board of Bluesky, but this isn’t Bluesky specific. We want an open discussion and brainstorming on growing the wider open social web.This isn’t your standard conference where you sit through presentations and nod politely. It’s a participatory event built around the FediForum unconference model, though modified to be more of an ongoing brainstorming workshop (not unlike the Greenhouse events we’ve run here in the past).Before the event, participants can submit short position papers—your experiences, your ideas, your proposals for what might actually work to engage more people on open social systems. We’ll cluster those into topics and spend the actual event  them and brainstorming around them, not just listening to people talk at you.Here’s the thing: we want people who have real ideas and experience. People who have tried (and maybe failed) to get their friends onto the open social web and learned something useful from it. People who  had success convincing entire communities. People running organizations who are trying to figure out how to make the jump. Builders who want more users. Advocates who have done actual research with actual humans about what’s working and what isn’t.What we don’t need are more cynical hot takes about why the open social web will never work. If you’ve already decided it’s a lost cause, this isn’t the event for you. Go post about it on Threads or whatever. We also don’t need hot takes about how you’re glad most people don’t use the open social web. That’s great for you open social hipsters, but some of us think it’s important to get more people to recognize the power of open social.So, for everyone else—the people who believe this matters and want to figure out how to make it happen—we want to hear from you.The event will run from 8am to noon Pacific (which means Europeans can actually attend without setting an alarm for 3am), and registration is open now. The event will be run online, using Remo, a tool we’ve used in the past for online events, that is conducive to small group discussions and brainstorming.Position paper submissions are due by February 16th, and while they’re not required, they’re strongly encouraged (you can submit them during the registration process). The whole point is to come prepared to engage, not just spectate.Look, I’ve been writing about the importance of protocols over platforms for years now. The open social web represents one of the few genuine shots we have at building online spaces that aren’t controlled by a handful of companies (or their billionaire owners) making decisions based on whatever serves their interests that week. But potential doesn’t matter if we can’t translate it into much wider adoption.So if you’ve got ideas—real ideas, not just complaints—about how to get there, come share them.]]></content:encoded></item><item><title>Daily Deal: Luminar Mobile for iOS And Android</title><link>https://www.techdirt.com/2026/01/23/daily-deal-luminar-mobile-for-ios-and-android/</link><author>Daily Deal</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:30:04 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Luminar Mobile is your all-in-one creative companion designed for iOS, Android OS, and Chrome OS. Powered by an intuitive, touch-responsive interface, it lets you enhance photos effortlessly—anytime, anywhere. Whether you’re adjusting lighting, perfecting portraits, or adding artistic flair, Luminar Mobile delivers pro-level results in the palm of your hand. It’s on sale for $20.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Toronto Man Posed as Pilot To Rack Up Hundreds of Free Flights, Prosecutors Say</title><link>https://news.slashdot.org/story/26/01/23/123218/toronto-man-posed-as-pilot-to-rack-up-hundreds-of-free-flights-prosecutors-say?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 18:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A Toronto man posed as a pilot for years in order to fool airlines into giving him hundreds of free flights, prosecutors have alleged, in a case that has prompted comparisons to the Hollywood thriller Catch Me If You Can. From a report: Authorities in Hawaii announced this week that Dallas Pokornik, 33, had been charged with wire fraud after he allegedly fooled three major US carriers into giving him free tickets over a span of four years. 

Airlines typically offer standby tickets to their own staff and those with rival airlines as a way of ensuring the broader industry can effectively move employees across continents. According to court documents, Pokornik was a flight attendant for a Toronto-based airline from 2017 to 2019, but then used an employee identification from that carrier to obtain tickets, "which he in fact knew to be fraudulent at the time it was so presented." 

The only Toronto-based airline, Porter, told reporters it was "unable to verify any information related to this story." On one occasion, Pokornik is alleged to have requested a jumpseat in an aircraft's cockpit, which are normally reserved for off-duty pilots, even though he was not a pilot and did not have an airman's certificate. Federal rules prohibit the cockpit jumpseats from being used for leisure travel.]]></content:encoded></item><item><title>Behind the Blog: Signs of the Times</title><link>https://www.404media.co/behind-the-blog-signs-of-the-times/</link><author>Samantha Cole</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/01/nl1.23.png" length="" type=""/><pubDate>Fri, 23 Jan 2026 17:53:28 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[This is Behind the Blog, where we share our behind-the-scenes thoughts about how a few of our top stories of the week came together. This week, we discuss stances on AI, a conference about money laundering, and signs about slavery coming down.Last week we published my interview with the Wikimedia Foundation CTO Selena Deckelmann. I was happy to talk to her because she’s uniquely positioned to talk about generative AI’s impact on the internet both as the CTO of the website that creates some of the most valuable training data, and one of the sites that’s threatened by generative AI output the most. ]]></content:encoded></item><item><title>ICE Is So Bad At Immigration Enforcement That It’s Detaining Native Americans</title><link>https://www.techdirt.com/2026/01/23/ice-is-so-bad-at-immigration-enforcement-that-its-detaining-native-americans/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:21:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Obviously, that’s not how things are supposed to work here in America, which proudly considered itself to be a melting pot (albeit belatedly and after a lot of post-Civil War legislation and jurisprudence). What makes America great is the blend of people in it. And, because this nation is so large, there’s plenty of room for everyone and no non-bigot will ever claim the addition of migrants has somehow made us weaker. ICE has always been awful. It’s been even worse recently, now that it knows no one in the administration will ever prevent it from being the racist throwback Trump clearly wishes it to be. It’s even bolder now that the Supreme Court — via Justice Kavanaugh’s shadow docket concurrence — said it’s ok to engage in racial profiling.The detention of at least five men in and around Minneapolis has sparked an outcry among Native American groups about Indigenous people being racially profiled as undocumented immigrants by federal immigration agents. Minneapolis is one of the largest urban centers for Native Americans in the United States.When you’re rounding up Native Americans, you’re rounding up the people who have done the least amount of immigration ever. Anyone engaged in these arrests has migrated more times than the people they’re arresting. This — along with the recent murder of Minnesota native and US citizen Renee Good by ICE officer Jonathan Ross — should have been enough to make ICE tuck its tail between its legs and head off to a more receptive, red-coded locality.It didn’t. And because ICE neither understands nor cares, it’s up to regular American citizens to point out the obvious: “It is deeply offensive and ironic that the first people of this land would be subjected to questions around their citizenship,” Jacqueline De Leon, senior staff attorney at the nonprofit Native American Rights Fund and a member of the Isleta Pueblo. “Yet nevertheless, that is exactly what we’re seeing.”You’d think someone at ICE might want to pull back and reassess the situation, especially now that seemingly the entirety of the city of Minneapolis is willing to hassle officers into abandoning the random roll-ups on darker skinned people they constantly claim are “targeted stops.”If these truly were “targeted stops,” they wouldn’t have targeted people who have far more right to be here than the people detaining them. Jose Rodriguez, a 20-year-old Red Lake Nation descendant, was arrested by ICE in what ICE claims was a “high-risk immigration enforcement stop.” (The officers also claimed to have been “violently assaulted” by Rodriguez but, tellingly, no charges have been filed.)This was followed up by the detaining of four unhoused tribal members by ICE officers, who found them sleeping under a bridge and decided this — combined with presumably darker-than-white skin tones — was all that was needed to justify some “papers please” hassling, immediately followed by detentions that, at press time (January 14)  hadn’t been ended. (One of the four was released prior to publishing.)And it’s not like Native Americans didn’t see this coming. They read the Kavanaugh concurrence and saw what’s been happening all over this nation (but  in “blue” states) and let their fellow Americans know that they should expect ICE to treat them like any other “brown” person officers come across:A day before Ramirez’s stop, the Red Lake Tribal Council issued a Jan. 7 advisory about the Trump administration’s enforcement in Minnesota. “We all need to be extra careful, and we must assume that ICE will not protect us,” the advisory said.It’s been obvious since the inception of this so-called “immigration enforcement” surge: anyone not white would be rounded up. The Supreme Court said this is all very cool and very lawful. And the surge in Minnesota is proving that being white is no protection either, not if you’re opposed to what this regime is doing. With threats of a military deployment to Minnesota looming, no American worth their citizenship should continue pretending this is anything more than white nationalism draping itself in executive power. ]]></content:encoded></item><item><title>Apple&apos;s Secret Product Plans Stolen in Luxshare Cyberattack</title><link>https://apple.slashdot.org/story/26/01/23/1017203/apples-secret-product-plans-stolen-in-luxshare-cyberattack?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: The Apple supplier subject to a major cyberattack last month was China's Luxshare, it has now emerged. More than 1TB of confidential Apple information was reportedly stolen. 

It was reported in December that one of Apple's assemblers suffered a significant cyberattack that may have compromised sensitive production-line information and manufacturing data linked to Apple. The specific company targeted, the scope of the breach, and its operational impact were unclear until now. The attack was first revealed on RansomHub's dark web leak site on December 15, 2025, where the group claimed it had encrypted internal Luxshare systems and exfiltrated large volumes of confidential data belonging to the company and its customers. 

The attackers warned that the information would be publicly released unless Luxshare contacted them to negotiate, and accused the company of attempting to conceal the incident. According to the attackers' claims, the exfiltrated material includes vital files such as detailed 3D CAD product models and high-precision geometric files, 2D manufacturing drawings, mechanical component designs, circuit board layouts, and internal engineering PDFs. The group added that the large archives include Apple product data as well as information belonging to Nvidia, LG, Tesla, Geely, and other major clients.]]></content:encoded></item><item><title>Google Photos’ latest feature lets you meme yourself</title><link>https://techcrunch.com/2026/01/23/google-photos-latest-feature-lets-you-meme-yourself/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:02:22 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The addition is meant to just be a fun way to explore your photos and experiment with Google's Gemini AI technology, and specifically Nano Banana. ]]></content:encoded></item><item><title>Video Friday: Humans and Robots Team Up in Battlefield Triage</title><link>https://spectrum.ieee.org/darpa-triage-challenge-robot</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MzEzMDYwNi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgyMDkzOTcwNX0.E2-OASseNEHgg5URtht8j77vTtpq355pxr_uvNN-ZGI/image.png?width=600" length="" type=""/><pubDate>Fri, 23 Jan 2026 17:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your weekly selection of awesome robot videos]]></content:encoded></item><item><title>Meta pauses teen access to AI characters ahead of new version</title><link>https://techcrunch.com/2026/01/23/meta-pauses-teen-access-to-ai-characters-ahead-of-new-version/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta today said that it is pausing teens' access to its AI characters globally across all its apps. The company mentioned that it is not abandoning its efforts but wants to develop an updated version of AI characters for teens.]]></content:encoded></item><item><title>Linux 6.19 Scheduler Feature Being Disabled Due To Performance Regressions</title><link>https://www.phoronix.com/news/Linux-6.19-Disabling-Next-Buddy</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 17:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Queued into tip/tip.git's "sched/urgent" Git branch today is a patch to disable the kernel scheduler's NEXT_BUDDY functionality that was re-implemented back during the Linux 6.19 merge window. It turns out to cause some performance regressions that have yet to be otherwise addressed...]]></content:encoded></item><item><title>When Two Years of Academic Work Vanished With a Single Click</title><link>https://science.slashdot.org/story/26/01/23/0959223/when-two-years-of-academic-work-vanished-with-a-single-click?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Marcel Bucher, a professor of plant sciences at the University of Cologne in Germany, lost two years of carefully structured academic work in an instant when he temporarily disabled ChatGPT's "data consent" option in August to test whether the AI tool's functions would still work without providing OpenAI his data. All his chats were permanently deleted and his project folders emptied without any warning or undo option, he wrote in a post on Nature. 

Bucher, a ChatGPT Plus subscriber paying $20 per month, had used the platform daily to draft grant applications, prepare teaching materials, revise publication drafts and create exams. He contacted OpenAI support, first receiving responses from an AI agent before a human employee confirmed the data was permanently lost and unrecoverable. OpenAI cited "privacy by design" as the reason, telling Nature it does provide a confirmation prompt before users permanently delete a chat but maintains no backups. 

Bucher said he had saved partial copies of some materials, but the underlying prompts, iterations, and project folders -- what he describes as the intellectual scaffolding behind his finished work -- are gone forever.]]></content:encoded></item><item><title>MEXC&apos;s Zero-Fee Gala Attracts Over 120,000 Participants with $8 Billion in Futures Trading Volume</title><link>https://hackernoon.com/mexcs-zero-fee-gala-attracts-over-120000-participants-with-$8-billion-in-futures-trading-volume?source=rss</link><author>Blockman PR and Marketing</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:33:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Victoria, Seychelles, January 23, 2026 – , the world's fastest-growing digital asset exchange and a pioneer of true zero-fee trading, successfully concluded its "," attracting over 120,000 participants and generating more than $8 billion in futures trading volume. The enthusiastic participation demonstrates strong user interest in the event and deepening trust in MEXC's commitment to creating meaningful value for its global trading community.The promotion ran from December 22, 2025, to January 21, 2026 (UTC), combining multiple incentive mechanisms to address different user segments and trading preferences. The promotion featured a total prize pool of $2 million, including high-value rewards such as a Cybertruck, a 1 oz gold bar, and an iPhone 17. It also offered zero fees on spot and select futures trading for XRP, DOGE, SOL, and BNB. Additionally, users could access USDT staking opportunities with returns of up to 600% APR and reward pools totaling 500,000 USDT, available to both new and existing users.The success of the "Zero-Fee Gala" underscores MEXC's commitment to placing user interests first through pioneering zero-fee trading and comprehensive incentive programs. As a leading global digital asset exchange, MEXC will continue to enhance platform services and deliver value-driven initiatives that provide users with a cost-effective, secure, and seamless trading experience.Founded in 2018, MEXC is committed to being "Your Easiest Way to Crypto." Serving over 40 million users across 170+ countries, MEXC is known for its broad selection of trending tokens, everyday airdrop opportunities, and low trading fees. Our user-friendly platform is designed to support both new traders and experienced investors, offering secure and efficient access to digital assets. MEXC prioritizes simplicity and innovation, making crypto trading more accessible and rewarding.For media inquiries, please contact MEXC PR team: media@mexc.comThis content does not constitute investment advice. Given the highly volatile nature of the cryptocurrency market, investors are encouraged to carefully assess market fluctuations, project fundamentals, and potential financial risks before making any trading decisions.:::tip
This story was published as a press release by Blockmanwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision]]></content:encoded></item><item><title>Alessio Vinassa on The Hidden Skill Behind Every Successful Entrepreneur</title><link>https://hackernoon.com/alessio-vinassa-on-the-hidden-skill-behind-every-successful-entrepreneur?source=rss</link><author>Blockman PR and Marketing</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:25:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every entrepreneur talks about vision, resilience, and execution. But beneath all of these traits lies a quieter, less visible skill that ultimately determines success or failure: decision-making under uncertainty. Markets shift, data is incomplete, and outcomes are rarely guaranteed. The leaders who endure are not those who wait for certainty—but those who learn how to think clearly without it.According to Alessio Vinassa, serial entrepreneur and business advisor, uncertainty is not an obstacle to leadership—it is the environment in which leadership exists.“If you wait for perfect information, you’re already behind,” Vinassa says. “Entrepreneurship is the practice of making meaningful decisions with incomplete data.”Why Uncertainty Is the Entrepreneur’s Default StateUnlike corporate environments with established processes and historical benchmarks, entrepreneurial decision-making often happens in real time. Founders must decide when to hire, when to pivot, when to say no, and when to move faster—all without knowing how markets, customers, or competitors will respond.Vinassa emphasizes that uncertainty is not a phase entrepreneurs outgrow.“The idea that uncertainty disappears as companies grow is a myth,” he explains. “It simply changes shape.”At early stages, uncertainty revolves around product fit and survival. Later, it shifts toward leadership, culture, expansion, and reputation. The cognitive challenge remains the same: choosing a direction without guarantees.Decision-Making Is a Cognitive Skill, Not Just InstinctMany founders describe their decisions as “gut-driven,” but Vinassa argues that effective intuition is built—not innate.“Strong instincts are trained through exposure, reflection, and pattern recognition,” he says. “They are not emotional impulses.”Successful entrepreneurs develop internal frameworks that help them evaluate risk, weigh trade-offs, and act decisively without becoming reckless. These frameworks allow leaders to remain calm while others freeze or overreact.Separating Signal from NoiseOne of the greatest challenges in uncertain environments is information overload. Data, opinions, forecasts, and external pressure can cloud judgment.Vinassa highlights the importance of filtering.“Not all information deserves equal attention,” he notes. “Good decision-makers know what to ignore.”Experienced leaders learn to prioritize first-order effects over speculation, focusing on what directly influences outcomes rather than chasing every possible scenario. This discipline reduces cognitive fatigue and improves consistency.Reversibility vs. IrreversibilityOne mental model Vinassa frequently references is the distinction between reversible and irreversible decisions.“Most decisions are not permanent,” he explains. “Treating them as such creates unnecessary paralysis.”\
Reversible decisions—such as testing a new strategy or experimenting with a process—should be made quickly and adjusted as needed. Irreversible decisions—those affecting reputation, ethics, or long-term trust—require deeper consideration.Understanding this distinction allows entrepreneurs to move faster without becoming careless.Managing Emotional Bias Under PressureUncertainty often triggers fear, ego, or urgency. Vinassa believes emotional regulation is a critical but underestimated leadership skill.“You’re not just managing a business—you’re managing your own psychology,” he says.Effective decision-makers create distance between emotion and action. They pause, reflect, and seek perspective before committing. This does not mean avoiding risk, but approaching it with clarity rather than anxiety.Building Confidence Without CertaintyConfidence in leadership does not come from knowing outcomes—it comes from trusting one’s process.“Confidence is the belief that you can respond well, even if the decision doesn’t work out,” Vinassa explains.Entrepreneurs who view decisions as experiments rather than verdicts are better equipped to adapt. Failure becomes feedback, not identity.Decision-Making as a Team SportAs organizations grow, decision-making must scale beyond the founder. Vinassa stresses the importance of building cultures that support distributed judgment.“Strong leaders don’t make every decision,” he says. “They build systems that produce good decisions.”Clear principles, aligned incentives, and psychological safety allow teams to navigate uncertainty together rather than bottlenecking leadership.Over time, entrepreneurs who master decision-making under uncertainty gain a powerful advantage. They move faster, recover quicker, and inspire confidence in others.“People follow leaders who can stay grounded when outcomes are unclear,” Vinassa notes. In volatile markets, this steadiness becomes a competitive differentiator.Decision-making under uncertainty is not glamorous, but it is foundational. It shapes strategy, culture, and outcomes more than any single idea.As Vinassa puts it: \n  “Ideas matter, but decisions determine destiny.”\n Alessio Vinassa is a serial entrepreneur, business strategist, and thought leader focused on leadership, adaptability, and building resilient businesses in fast-changing global markets. His work centers on mentorship, innovation, and helping entrepreneurs navigate complexity with clarity and purpose.:::tip
This story was published as a press release by Blockmanwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision]]></content:encoded></item><item><title>VVenC H.266 Encoder Rolls Out More ARM Optimizations For Nice Performance Gains</title><link>https://www.phoronix.com/news/VVenC-1.14-More-ARM-Performance</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:08:22 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Fraunhofer HHI this week released a new version of VVenC, their open-source H.266 video encoder. Among the changes this release are more performance optimizations for ARM and I have run some comparison benchmarks using a NVIDIA GB10 SoC with the Dell Pro Max GB10...]]></content:encoded></item><item><title>The HackerNoon Newsletter: How to Enter the Proof of Usefulness (PoU) Hackathon (1/23/2026)</title><link>https://hackernoon.com/1-23-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:03:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, January 23, 2026?By @aimodels44 [ 7 Min read ] An explainer on why brute-force AI fails at grand strategy games, and how hybrid LLM architectures enable long-horizon strategic reasoning. Read More.By @proofofusefulness [ 5 Min read ] No pitch decks. No mockups. Just code that works. Here is your step-by-step guide to entering HackerNoons $150K Proof of Usefulness Hackathon. Read More.By @nfrankel [ 6 Min read ] In this post, I tackled the issue of integrating checked exceptions with lambdas in Java. Read More.By @rodrigokamada [ 6 Min read ] In this article, we will create a WEB application using the latest version of Angular and integrate the AWS WAF CAPTCHA challenge to protect against bots. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Anthropic&apos;s AI Keeps Passing Its Own Company&apos;s Job Interview</title><link>https://slashdot.org/story/26/01/23/0951257/anthropics-ai-keeps-passing-its-own-companys-job-interview?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Anthropic has a problem that most companies would envy: its AI model keeps getting so good, the company wrote in a blog post, that it passes the company's own hiring test for performance engineers. The test, designed in late 2023 by optimization lead Tristan Hume, asks candidates to speed up code running on a simulated computer chip. Over 1,000 people have taken it, and dozens now work at Anthropic. But Claude Opus 4 outperformed most human applicants. 

Hume redesigned the test, making it harder. Then Claude Opus 4.5 matched even the best human scores within the two-hour time limit. For his third attempt, Hume abandoned realistic problems entirely and switched to abstract puzzles using a strange, minimal programming language -- something weird enough that Claude struggles with it. Anthropic is now releasing the original test as an open challenge. Beat Claude's best score and ... they want to hear from you.]]></content:encoded></item><item><title>This startup will send 1,000 people’s ashes to space — affordably — in 2027</title><link>https://techcrunch.com/2026/01/23/this-startup-will-send-1000-peoples-ashes-to-space-affordably-in-2027/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 23 Jan 2026 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Started by an engineer who worked on the space shuttle program, and at Blue Origin, Space Beyond has a spot on a 2027 Falcon 9 flight.]]></content:encoded></item><item><title>Microsoft gave FBI a set of BitLocker encryption keys to unlock suspects’ laptops: Reports</title><link>https://techcrunch.com/2026/01/23/microsoft-gave-fbi-a-set-of-bitlocker-encryption-keys-to-unlock-suspects-laptops-reports/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:54:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The FBI served Microsoft a warrant requesting encryption recovery keys to decrypt the hard drives of people involved in an alleged fraud case in Guam. ]]></content:encoded></item><item><title>TikTok-like microdramas are going to make billions this year, even though they kind of suck</title><link>https://techcrunch.com/2026/01/23/tiktok-like-microdramas-are-going-to-make-billions-this-year-even-though-they-kind-of-suck/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:50:41 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The business model behind these apps replicates the same dark patterns as mobile games.]]></content:encoded></item><item><title>TikTok finalizes deal to create new US entity and avoid ban</title><link>https://techcrunch.com/2026/01/23/tiktok-finalizes-deal-to-create-new-us-entity-and-avoid-ban/</link><author>Aisha Malik</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:44:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The deal ends a six-year long political saga that started in 2020 when President Donald Trump tried to ban the app over national security concerns during his first term. ]]></content:encoded></item><item><title>OpenAI chief Sam Altman plans India visit as AI leaders converge in New Delhi: sources</title><link>https://techcrunch.com/2026/01/23/openai-chief-sam-altman-plans-india-visit-as-ai-leaders-converge-in-new-delhi-sources/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:30:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The visit comes as New Delhi prepares to host a major AI summit expected to draw top executives from Meta, Google, and Anthropic. This will be Altman's first visit to the country in nearly a year.]]></content:encoded></item><item><title>Monster Neutrino Could Be a Messenger of Ancient Black Holes</title><link>https://www.quantamagazine.org/monster-neutrino-could-be-a-messenger-of-ancient-black-holes-20260123/</link><author>Jonathan O&apos;Callaghan</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2026/01/Black-Holes-as-Dark-Matter-cr-Courtesy-of-KM3NeT-Default.webp" length="" type=""/><pubDate>Fri, 23 Jan 2026 15:26:07 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[Nearly three years ago, a particle from space slammed into the Mediterranean Sea and lit up the partially complete Cubic Kilometer Neutrino Telescope (KM3NET) detector off the coast of Sicily. The particle was a neutrino, a fundamental component of matter commonly known for its ability to slip through other matter unnoticed. The IceCube observatory in Antarctica, a comparable detector that has…]]></content:encoded></item><item><title>Apple Accuses European Commission of &apos;Political Delay Tactics&apos; To Justify Fines</title><link>https://apple.slashdot.org/story/26/01/23/0941249/apple-accuses-european-commission-of-political-delay-tactics-to-justify-fines?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple has accused the European Commission of using "political delay tactics" to postpone new app marketplace policies and create grounds for investigating and fining the iPhone maker, a preemptive response to reports that the commission plans to blame Apple for the announced closure of third-party app store Setapp. 

MacPaw, the developer behind Setapp, said it would shut down the marketplace next month because of "still-evolving and complex business terms that don't fit Setapp's current business model." The EC is preparing to say that Apple has not rolled out changes to address key issues concerning its business terms and their complexity, according to remarks seen by Bloomberg. 

Apple said it disputes this finding. The company said it submitted a formal compliance plan in October proposing to replace its $0.59 per-install fee structure with a 5% revenue share, but the commission has not responded. "The European Commission has refused to let us implement the very changes that they requested," Apple said. The company also claimed there is no demand in the EU for alternative app stores and disputed that Setapp is closing because of its actions.]]></content:encoded></item><item><title>Vulkan Roadmap 2026 Milestone: Variable Rate Shading, Host Image Copies &amp; More</title><link>https://www.phoronix.com/news/Vulkan-Roadmap-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:12:32 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In addition to the release today of Vulkan 1.4.340 with the new descriptor heap "VK_EXT_descriptor_heap" extension and three other new extensions, The Khronos Group's Vulkan Working Group has also published the Vulkan Roadmap 2026 Milestone...]]></content:encoded></item><item><title>Only 1 week left (or until the first 500 passes are gone): The first TechCrunch Disrupt 2026 ticket discount is ending</title><link>https://techcrunch.com/2026/01/23/only-1-week-left-or-until-the-first-500-passes-are-gone-the-first-disrupt-2026-ticket-discount-is-ending/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Fri, 23 Jan 2026 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Register now to save up to $680 on your TechCrunch Disrupt 2026 pass and get a second ticket at 50% off. This offer ends next week on January 30, or once the first 500 tickets are claimed — whichever comes first.]]></content:encoded></item><item><title>&apos;Almost Everyone&apos; Laid Off at Vimeo Following Bending Spoons Buyout</title><link>https://slashdot.org/story/26/01/23/0757223/almost-everyone-laid-off-at-vimeo-following-bending-spoons-buyout?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Vimeo is laying off employees around the world just months after Italian software company Bending Spoons completed its $1.38 billion acquisition of the video hosting platform. Dave Brown, Vimeo's former brand VP, described the cuts on LinkedIn as affecting "a large portion of the company." One video engineer claimed "almost everyone" was laid off, "including the entire video team," and another software engineer said he lost his job alongside "a gigantic amount of the company." 

This marks Vimeo's second round of layoffs in less than six months. The company cut 10% of its workforce in September, just one week before Bending Spoons announced its acquisition plans. Bending Spoons has a history of post-acquisition layoffs at companies including WeTransfer, Filmic, and Evernote.]]></content:encoded></item><item><title>AMD Ryzen AI Software 1.7 Released For Improved Performance On NPUs, New Model Support</title><link>https://www.phoronix.com/news/AMD-Ryzen-AI-Software-1.7</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 14:18:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD today released a new version of Ryzen AI Software, the user-space packages for Microsoft Windows and Linux for making use of the Ryzen AI NPUs for various AI software tasks like Stable Diffusion, ONNX, and more...]]></content:encoded></item><item><title>US Formally Withdraws From WHO</title><link>https://science.slashdot.org/story/26/01/23/1226253/us-formally-withdraws-from-who?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 23 Jan 2026 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The United States formally withdrew from the World Health Organization on Thursday, making good on an executive order that President Trump issued on his first day in office pledging to leave the international organization that coordinates global responses to public health threats. The New York Times: While the United States is walking away from the organization, a senior official with the Department of Health and Human Services told reporters on Thursday that the Trump administration was considering some type of narrow, limited engagement with W.H.O. global networks that track infectious diseases, including influenza. 

As a W.H.O. member, the United States long sent scientists from the Centers for Disease Control and Prevention to participate in international decision-making about which strains to include in the flu vaccine. A W.H.O. meeting on next year's vaccine is scheduled for February. The official said the Trump administration would soon disclose how or whether it will participate. 

On Thursday, the administration said that all U.S. government funding to the organization had been terminated, and that all assigned federal employees and contractors had been recalled from its Geneva headquarters and its offices worldwide.]]></content:encoded></item><item><title>GNU Guix 1.5 Released With RISC-V Support, Experimental x86_64 GNU Hurd Kernel</title><link>https://www.phoronix.com/news/GNU-Guix-1.5-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 13:54:41 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[GNU Guix 1.5 is out today as the latest major release for this platform built around its functional package manager. This is a big upgrade with it having been three years since the GNU Guix 1.4 release...]]></content:encoded></item><item><title>RFK Jr. Spreads New Bogus Scare Mongering Bullshit About Cell Phone Safety</title><link>https://www.techdirt.com/2026/01/23/rfk-jr-spreads-new-bogus-scare-mongering-bullshit-about-cell-phone-safety/</link><author>Karl Bode</author><category>tech</category><pubDate>Fri, 23 Jan 2026 13:24:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The hype and madness surrounding 5G has always been pretty wild to watch.The Trump administration’s pseudo-populist attempt to tap into the more delirious and desperate segments of the electorate has long taken advantage of this latter group’s often-legitimate distrust of modern medicine, corporations, and public safety regulators. “The FDA removed online information that said scientists have not connected exposure to radiofrequency (RF) waves, emitted by cell phones, to health problems in users.Some of the removed webpages contained “old conclusions,” an HHS spokesperson told the Wall Street Journal. The spokesperson also said that researching cell phone radiation would “identify gaps in knowledge.” The agency provided a similar statement to Scientific American, adding that the research was “directed by President Trump’s MAHA Commission.”“Generally speaking, electromagnetic radiation is a major health concern,” Kennedy said in the exclusive interview, when asked for his concerns about 5G towers. “I’m very concerned about it.”In these interviews, RFK Jr. is making completely false claims that there’s “more than 10,000 studies” proving a clear risk of human harm from cell phone use. The World Health Organization found no justification for health concerns after a meta-analysis of nearly .While it’s hubris to insist we know  about wireless’ impact on human health, the science we do have traditionally points to a very clear conclusion: 5G isn’t likely to seriously to hurt you. In fact, in many ways 5G is potentially less harmful than previous iterations given that the millimeter wave spectrum being used in many cities can barely penetrate walls, much less human skin. As Scientific American notes, while there  historically been studies suggesting potential cancerous impact from massive exposure using rats, “studies in humans have been inconsistent and limited in scope and efficacy.” The FDA had previously, and correctly, stated that “the weight of scientific evidence has not linked exposure to radio frequency energy from cellphone use with any health problems.”Now if the Trump administration was actually serious about launching real-world scientific inquiries into cell phone health’s impact, that might be something. But we’re long past the point where this weird assortment of zealots deserve any benefit of the doubt. Especially given RFK Jr.’s history of completely unscientific, fear mongering gibberish.Trump authoritarians love leaning into conspiracy theories for several reasons. Two, it helps create an information fog of war where the electorate finds it harder to reliably identify what’s true, in turn making people more distrustful of the few legitimate media organizations still interested in the truth. This in turn makes it easier for authoritarians to lie to you (and the movement’s adjacent grifter economy to rip you off with false promises and cures).It’s not populism, it’s exploitation. There are no answers here, only more confusion and chaos for suffering people. All to mask broad, grotesque corruption by a broad assortment of terrible human beings. Many of these MAHA segments being targeted by Trump grifters (see: RFK Jr.’s siren call to angry Lyme Disease patients) spent decades feeling legitimately exploited and abused by corporate power, institutional failure, and U.S. health care dysfunction only to walk straight into the maw of some of the biggest grifting shitbags America may have ever spawned (which is really saying something).]]></content:encoded></item><item><title>Linux Lands Fix For Its &quot;Subtly Wrong&quot; Page Fault Handling Code For The Past 5 Years</title><link>https://www.phoronix.com/news/Linux-6.19-Page-Fault-Code-Fix</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 13:11:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged today for the Linux 6.19 Git kernel and then in turn for back-porting to prior Linux kernel series is making the x86 page fault handling code disable interrupts properly. Since 2020 it turns out the handling was subtly wrong but now corrected by Intel...]]></content:encoded></item><item><title>TikTok Finalizes Deal To Form New American Entity</title><link>https://tech.slashdot.org/story/26/01/23/0817218/tiktok-finalizes-deal-to-form-new-american-entity?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from NPR: TikTok has finalized a deal to create a new American entity, avoiding the looming threat of a ban in the United States that has been in discussion for years. The social video platform company signed agreements with major investors including Oracle, Silver Lake and MGX to form the new TikTok U.S. joint venture. The new version will operate under "defined safeguards that protect national security through comprehensive data protections, algorithm security, content moderation and software assurances for U.S. users," the company said in a statement Thursday. American TikTok users can continue using the same app. [...] Adam Presser, who previously worked as TikTok's head of operations and trust and safety, will lead the new venture as its CEO. He will work alongside a seven-member, majority-American board of directors that includes TikTok's CEO Shou Chew.
 
[...] In addition to an emphasis on data protection, with U.S. user data being stored locally in a system run by Oracle, the joint venture will also focus on TikTok's algorithm. The content recommendation formula, which feeds users specific videos tailored to their preferences and interests, will be retrained, tested and updated on U.S. user data, the company said in its announcement. The algorithm has been a central issue in the security debate over TikTok. China previously maintained the algorithm must remain under Chinese control by law. But the U.S. regulation passed with bipartisan support said any divestment of TikTok must mean the platform cuts ties -- specifically the algorithm -- with ByteDance. Under the terms of this deal, ByteDance would license the algorithm to the U.S. entity for retraining.
 
The law prohibits "any cooperation with respect to the operation of a content recommendation algorithm" between ByteDance and a new potential American ownership group, so it is unclear how ByteDance's continued involvement in this arrangement will play out. Oracle, Silver Lake and the Emirati investment firm MGX are the three managing investors, who each hold a 15% share. Other investors include the investment firm of Michael Dell, the billionaire founder of Dell Technologies. ByteDance retains 19.9% of the joint venture.]]></content:encoded></item><item><title>Tesla discontinues Autopilot in bid to boost adoption of its Full Self-Driving software</title><link>https://techcrunch.com/2026/01/23/tesla-discontinues-autopilot-in-bid-to-boost-adoption-of-its-full-self-driving-software/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Fri, 23 Jan 2026 12:56:44 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company is also currently facing a 30-day suspension of its manufacturing and dealer licenses in California for deceptive marketing about Autopilot's capabilities.]]></content:encoded></item><item><title>Why 2026 is the Year Healthcare Finally Hires AI Agents</title><link>https://hackernoon.com/why-2026-is-the-year-healthcare-finally-hires-ai-agents?source=rss</link><author>Stewart Rogers</author><category>tech</category><pubDate>Fri, 23 Jan 2026 11:32:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
The medical industry is finally stopping the charade of treating software as a tool and starting to treat AI agents as colleagues. We have moved past the era of passive Large Language Models (LLMs) that act like fancy encyclopedias. The current landscape is defined by agentic AI, digital entities that do not just suggest; they execute.If 2024 was about the "ambient scribe" that sat in the corner recording conversations, 2026 is about the "hireable agent" that navigates the brutal bureaucracy of modern medicine. This shift is backed by staggering market momentum. The global  is projected to skyrocket at a compound annual growth rate (CAGR) of 45.56% through 2030, reaching nearly $5 billion as organizations move from "pilot purgatory" to full-scale deployment.The Shift from Scribe to StrategistFor decades, the back office was a graveyard of efficiency. A clinician would see a patient, a scribe would draft a note, and days later, a human coder would try to reconstruct the clinical logic to satisfy an insurance company. This delay has become a financial death sentence. Under the latest CMS Interoperability and Prior Authorization rules, if you do not defend your clinical reasoning at the moment of care, you are essentially leaving your wallet open for a payer audit.Enter the hireable agent. These are not static apps; they are autonomous systems structured around planning, action, and memory. They understand the "why" behind a diagnosis in real time."We are defining agentic AI as systems that can plan autonomously and take actions to meet goals," Moghis Uddin, CEO at , told me. This perspective highlights a fundamental change in the tech stack. While a doctor explains a treatment plan, the agent is already cross-referencing the  (OBBBA) and aligning the clinical narrative with 2026's aggressive coding standards.Why Context Is the New CurrencyData is no longer the bottleneck. Every health system is drowning in it. The real scarcity in 2026 is context. Clinicians do not need more numbers; they need to know why those numbers matter for the person sitting in front of them.Agentic systems are filling this gap by acting as the connective tissue across fragmented records. Instead of a doctor clicking through eight screens to find a lab result from three years ago, the agent surfaces it because it recognizes a pattern of disease progression. This is not just automation; it is "hybrid intelligence."AlethianAI notes that "agents are smarter. They're proactive, capable of making suggestions before you ask for them, and they accomplish tasks across applications.” This moves the needle from identifying a disease to identifying "missing patients," the ones who would otherwise drift out of the care pathway because a human was too busy filling out forms to follow up.The Rise of the AI-Literate HumanThere is a persistent anxiety that these agents will render medical staff obsolete. The reality on the ground looks different. We are seeing the emergence of the AI-literate coder and the augmented nurse. AI and automation in the revenue cycle are expected to generate up to  by streamlining these high-friction touchpoints.Instead of chasing doctors for clarifications on illegible notes, billing teams are now denial strategists. They audit the AI and handle complex escalations that require emotional nuance or ethical weight that an algorithm cannot provide. As the industry evolves, the focus has shifted toward "Agentic-AI" as a software system that can autonomously perform specific tasks, making decisions based on data and learning over time.Defending the Revenue CycleThe most immediate impact of hireable agents is the end of the "denial spiral." In the old world, payers used AI to deny claims faster than humans could write them. In 2026, providers are fighting fire with fire. Agentic partners create an ironclad audit trail at the point of care. They ensure that the correct workflow is also the easiest one to follow, making compliance a byproduct of care rather than a hurdle to it.Healthcare has finally realized that it does not need a new hero. It needs better tools for the heroes it already has. By shifting AI from a "product you buy" to a "workforce you hire," the industry is finally reclaiming its time.]]></content:encoded></item><item><title>Zlib-rs 0.6 Released With Improved AVX-512 Support</title><link>https://www.phoronix.com/news/Zlib-rs-0.6-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 11:21:42 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Zlib-rs is the effort out of the Trifecta Tech Foundation to provide a Zlib compression implementation written in the Rust programming language that can serve as a C dynamic library and Rust crate. The intent here being that zlib-rs is potentially safer than the classic C-based implementation of Zlib...]]></content:encoded></item><item><title>Vulkan 1.4.340 Released With Descriptor Heap &amp; Other New Extensions</title><link>https://www.phoronix.com/news/Vulkan-1.4.340-Descriptor-Heap</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 11:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Vulkan 1.4.340 is out today as the first significant new Vulkan API update following the end of year holidays. With Vulkan 1.4.340 comes four new extensions worth talking about...]]></content:encoded></item><item><title>KMSCON 9.3 Released With Mouse Support By Default, Other Improvements</title><link>https://www.phoronix.com/news/KMSCON-9.3-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 10:58:13 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KMSCON as a KMS/DRM-based virtual console emulator in user-space has been released. KMSCON is one of the leading solutions for potentially replacing the in-kernel Virtual Terminal (VT) implementation...]]></content:encoded></item><item><title>Servo 0.0.4 Browser Engine Released &amp; Finally Supporting Multiple Windows</title><link>https://www.phoronix.com/news/Servo-0.0.4</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 10:40:50 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Servo 0.0.4 is out today as the newest monthly update to this open-source, Rust-based web browser engine. Building off recent Servo embedding API additions, Servo 0.0.4 introduces support for multiple browser windows...]]></content:encoded></item><item><title>How I Turned Instagram Growth Into Brand Deals Using Path Social</title><link>https://hackernoon.com/how-i-turned-instagram-growth-into-brand-deals-using-path-social?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Fri, 23 Jan 2026 10:31:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Building a loyal Instagram audience used to feel like a full-time job. Between content creation, editing, engagement, and managing collaborations, growth always came last on my to-do list. But like many creators, I hit a wall. My content was getting better, but my audience wasn’t growing at the same pace.After months of experimenting with different strategies and platforms, I decided to try AI Instagram growth for 90 days. I approached it like a campaign, not a shortcut. This is what I learned—and how I went from stuck to scaling.My Starting Point: Quality Content, Flat GrowthBefore discovering Path Social, I had 3,200 followers. I was averaging around a 2.1% engagement rate, but reaching new audiences was hit or miss. My Reels were performing well, but my posts rarely showed up on explore. My niche - wellness and mindful living - was a growing trend, but there were so many other creators targeting the same eyeballs.And my most significant problem? Discovery.No matter what I did: hashtags, shares, giveaways - I could not figure out how to get consistent access to the right audience.Initially, I was skeptical. I had witnessed the harm bots can do to accounts: ghost followers, shadowbans, and inauthentic engagement in general. What intrigued me about Path Social was their hybrid model of AI-based audience targeting + real humans involved in the process.They were not convincing people to engage with bots or mass follow. They were providing targeted discovery of followers.After signing up, I completed a thorough onboarding form. I answered a lot of questions about my niche, important hashtags, competitor accounts, and even a few brands that I wanted to partner with. This gave Path Social some context for more precise targeting.I also made sure my content library was optimized:Updated my bio with a clear CTAHighlighted recent brand collabs in StoriesThe first sign that something was working? My story views nearly doubled.From there, engagement followed. In week one, I gained 230 new followers. Week two brought in another 300. But what stood out most wasn’t the number—it was the relevance.People DM’d me about posts I’d published weeks earlier. Comments were longer, more thoughtful. It was clear: this was the right kind of audience.Analytics Snapshot (90 Days)Engagement rate: 2.1% → 3.4%Brand DMs: 5 new pitch inquiriesMy reach finally matched my content quality.The Path Social team didn’t just run traffic—they curated it. That was the difference. The AI system filtered potential followers by interest and behavior. The human side helped guide those choices based on feedback and performance.It was like working with a talent scout for my audience.1. Approach Growth as a CampaignDon’t think “set and forget.” You want clear goals. In my case, more engagement and brand collabs.2. Make Sure Your Profile is Conversion FriendlyWhen a new user lands on your page, you have just one chance. Make checks that your bio, highlights, and best pieces of content speak to that user.3. Let the Data Drive ContentWhen your new audience starts coming in, pay attention to what they are responding to. I learned that quick wellness how-to type content performed much better than aesthetic posts. This is what informed my calendar for the future with content.Without content, growth is just wasted traffic. During my campaign, I maintained a 3x weekly posting and daily stories.5. Be Patient, But CuriousNot everything will spike overnight. But over time, the trend line matters more than the peaks.The ROI Beyond Follower CountWhat impressed me? The inbound brand interest.Because of the increased engagement and niche clarity, two eco-friendly beauty brands reached out about partnerships. One of those collaborations turned into a long-term ambassadorship.Followers are the entry point. Real value comes from what they  after they follow.If you’re a creator who’s burnt out from chasing the algorithm, I can say this with full honesty: Path Social helped me reset my strategy.It didn’t replace my effort—it focused on it.With the right tools, consistent content, and a clear voice, growth doesn’t have to feel random. For me, Path Social wasn’t a growth hack—it was a growth partner.I’d recommend it to any creator ready to take themselves seriously. Because the numbers matter, but the  behind them matter more.]]></content:encoded></item><item><title>&apos;Active&apos; Sitting Is Better For Brain Health</title><link>https://science.slashdot.org/story/26/01/23/089256/active-sitting-is-better-for-brain-health?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[alternative_right shares a report from ScienceAlert: A systematic review of 85 studies has now found good reason to differentiate between 'active' sitting, like playing cards or reading, and 'passive' sitting, like watching TV. [...] "Total sitting time has been shown to be related to brain health; however, sitting is often treated as a single entity, without considering the specific type of activity," explains public health researcher Paul Gardiner from the University of Queensland in Australia. "Most people spend many hours sitting each day, so the type of sitting really matters ... These findings show that small everyday choices -- like reading instead of watching television -- may help keep your brain healthier as you age."
 
Across numerous studies, Gardiner and colleagues found that active sitting activities, like reading, playing card games, and using a computer, showed "overwhelmingly positive associations with cognitive health, enhancing cognitive functions such as executive function, situational memory, and working memory." Meanwhile, passive sitting was most consistently associated with negative cognitive outcomes, including increased risk of dementia. The study was published in the Journal of Alzheimer's Disease.]]></content:encoded></item><item><title>What I&apos;ve learned building an agent for Renovate config (as a cautious skeptic of AI)</title><link>https://hackernoon.com/what-ive-learned-building-an-agent-for-renovate-config-as-a-cautious-skeptic-of-ai?source=rss</link><author>Mend.io</author><category>tech</category><pubDate>Fri, 23 Jan 2026 09:25:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[For those who aren't aware, Mend Renovate (aka Renovate CLI aka Renovate) is an Open Source project for automating dependency updates across dozens of package managers and package ecosystems, 9 different platforms (GitHub, GitLab, Azure DevOps and more), and boasts support for tuning its behaviour to fit how you want dependency updates.]]></content:encoded></item><item><title>The TechBeat: SeaTunnel CDC Explained: A Layman’s Guide (1/23/2026)</title><link>https://hackernoon.com/1-23-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Fri, 23 Jan 2026 07:11:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @davidiyanu [ 11 Min read ] 
 Traditional CI/CD pipelines are buckling under scale. Agentic DevOps promises less toil—but introduces new risks teams must understand.  Read More.By @stevebeyatte [ 4 Min read ] 
 Miniswap, a Warhammer marketplace founded by Cambridge students, is betting on taste, curation, and community over AI automation. Learn how they raised $3.5M.  Read More.By @melissaindia [ 4 Min read ] 
 Bad data secretly slows development. Learn why data quality APIs are becoming core DX infrastructure in API-first systems and how they accelerate teams. Read More.By @dharmateja [ 12 Min read ] 
 Why average ROI fails. Learn how distributional and tail-risk modeling protects marketing campaigns from catastrophic losses using Bayesian methods.  Read More.By @stevebeyatte [ 12 Min read ] 
 Modern midsize companies need platforms that balance sophistication with agility, offering powerful features without overwhelming complexity. Read More.By @astrabit [ 5 Min read ] 
 What AstraBit’s FINRA broker-dealer registration signals for Web3 finance, regulatory accountability, and how innovation and compliance can coexist. Read More.By @opensourcetheworld [ 3 Min read ] 
 Solo Satoshi is Start9’s first US distributor, shipping the 2026 Server One from Houston so you can run open-source StartOS, apps, and Bitcoin nodes at home.  Read More.By @dharmateja [ 11 Min read ] 
 Learn how counterfactual forecasting helps data scientists measure true revenue impact by simulating causal scenarios beyond traditional time series models.  Read More.By @scylladb [ 5 Min read ] 
 ScyllaDB Vector Search reaches 1B vectors with 2ms p99 latency and 250K QPS, unifying structured data and embeddings at scale. Read More.By @btcwire [ 2 Min read ] 
 The platform is capable of producing video with realistic physics, lighting, and motion, making it suitable for marketing content. Read More.By @hck3remmyp3ncil [ 11 Min read ] 
 RAG optimizes language model outputs by having them reference external knowledge bases before generating responses.  Read More.By @dineshelumalai [ 7 Min read ] 
 A Software Architect's account of replacing senior devs with AI. $238K savings became $254K in real costs. Why human judgment still matters. Read More.By @techexplorer42 [ 8 Min read ] 
 Learn how DAOs work by building a governance token with Solidity, OpenZeppelin, and Foundry, from deployment to testing on a local blockchain. Read More.By @vigneshwaran [ 5 Min read ] 
 Learn how to uninstall problematic Windows 11 updates using Settings, Control Panel, Command Prompt, PowerShell, and Microsoft tools. Read More.By @huckler [ 4 Min read ] 
 Just about alone programming, innovational program.
My story. Read More.By @williamguo [ 7 Min read ] 
 The core design philosophy of SeaTunnel CDC is to find the perfect balance between "Fast" (parallel snapshots) and "Stable" (data consistency). Read More.]]></content:encoded></item><item><title>AI Boosts Research Careers But Flattens Scientific Discovery</title><link>https://science.slashdot.org/story/26/01/23/0148249/ai-boosts-research-careers-but-flattens-scientific-discovery?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Ancient Slashdot reader erice shares the findings from a recent study showing that while AI helped researchers publish more often and boosted their careers, the resulting papers were, on average, less useful. "You have this conflict between individual incentives and science as a whole," says James Evans, a sociologist at the University of Chicago who led the study. From a recent IEEE Spectrum article: To quantify the effect, Evans and collaborators from the Beijing National Research Center for Information Science and Technology trained a natural language processing model to identify AI-augmented research across six natural science disciplines. Their dataset included 41.3 million English-language papers published between 1980 and 2025 in biology, chemistry, physics, medicine, materials science, and geology. They excluded fields such as computer science and mathematics that focus on developing AI methods themselves. The researchers traced the careers of individual scientists, examined how their papers accumulated attention, and zoomed out to consider how entire fields clustered or dispersed intellectually over time. They compared roughly 311,000 papers that incorporated AI in some way -- through the use of neural networks or large language models, for example -- with millions of others that did not.
 
The results revealed a striking trade-off. Scientists who adopt AI gain productivity and visibility: On average, they publish three times as many papers, receive nearly five times as many citations, and become team leaders a year or two earlier than those who do not. But when those papers are mapped in a high-dimensional "knowledge space," AI-heavy research occupies a smaller intellectual footprint, clusters more tightly around popular, data-rich problems, and generates weaker networks of follow-on engagement between studies. The pattern held across decades of AI development, spanning early machine learning, the rise of deep learning, and the current wave of generative AI. "If anything," Evans notes, "it's intensifying." [...] Aside from recent publishing distortions, Evans's analysis suggests that AI is largely automating the most tractable parts of science rather than expanding its frontiers.]]></content:encoded></item><item><title>Halo Security Achieves SOC 2 Type II Compliance, Demonstrating Sustained Security Excellence</title><link>https://hackernoon.com/halo-security-achieves-soc-2-type-ii-compliance-demonstrating-sustained-security-excellence?source=rss</link><author>CyberNewswire</author><category>tech</category><pubDate>Fri, 23 Jan 2026 06:20:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Miami, Florida, January 22nd, 2026/CyberNewsWire/--, a leading provider of external attack surface management and penetration testing services, today announced it has successfully achieved SOC 2 Type II compliance following an extensive multi-month audit by Insight Assurance.This certification validates that Halo Security's security controls are not only properly designed but also operate effectively and consistently over time."SOC 2 Type II compliance demonstrates our unwavering commitment to protecting customer data through proven, operational security practices," said Lisa Dowling, CEO of Halo Security. "Our customers trust us to help them discover and remediate vulnerabilities across their attack surface and this certification shows we apply that same rigorous security discipline to our own operations every single day.”While SOC 2 Type I certification validates that security controls are appropriately designed at a specific point in time, Type II compliance requires continuous monitoring and verification over an extended audit period. Insight Assurance evaluated Halo Security's actual security performance throughout the audit period, examining not just policies but their real-world execution and effectiveness.The extended audit period assessed:Operational Effectiveness: How security controls performed under real-world conditionsConsistency: Whether practices were maintained uniformly throughout the evaluation periodContinuous Monitoring: How the company detected and responded to security eventsChange Management: How security was maintained during system updates and changesIncident Response: The effectiveness of security procedures when issues ariseHalo Security partnered with Genius GRC for expert guidance throughout the compliance journey and leveraged the Vanta platform to maintain continuous compliance readiness. The company also developed a custom integration between its platform and Vanta to streamline the audit process."We extend our sincere appreciation to Insight Assurance for their thorough evaluation and validation of our compliance efforts," added Dowling. "Their expertise and impartial assessment have been instrumental in verifying our adherence to the SOC 2 framework.”“Achieving SOC 2 Type II is not just about documenting controls. It is about proving that security processes are consistently executed over time,” said Eric Shoemaker, Advisory CISO and Founder of Genius GRC. “Halo Security demonstrated strong operational maturity throughout the audit period, with security practices that are embedded into day-to-day operations rather than treated as a compliance exercise.”This achievement reinforces Halo Security's position as a trusted partner for organizations requiring comprehensive external security assessments. The company's vulnerability scanning and discovery solutions, combined with manual penetration testing services, help thousands of organizations worldwide maintain visibility into their attack surface security posture.Halo Security is changing the way organizations manage their external attack surface. Instead of leaving organizations to figure it out alone, Halo Security pairs unprecedented visibility into internet-facing assets with expert remediation guidance. The company's EASM platform is the next generation of vulnerability scanning. It automates asset discovery, includes auto-configured continuous vulnerability scanning, and delivers penetration-testing insights, all in one solution to deliver fast, measurable, and affordable risk reduction.Since 2013, Halo Security has helped over 2,000 clients discover and remediate vulnerabilities in their external-facing assets before attackers can exploit them. As a PCI DSS Approved Scanning Vendor (ASV) and SOC 2 Type II certified organization, Halo Security maintains the highest standards for both its services and operations. Halo Security is headquartered in Miami with a 100% US-based team.For more information about Halo Security's SOC 2 Type II compliance or to request the company's SOC 2 report, users can contact a Halo Security representative or visit .:::tip
This story was published as a press release by Cybernewswire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision]]></content:encoded></item><item><title>When &quot;Good Enough&quot; UX Becomes the Most Expensive Decision You&apos;ll Ever Make</title><link>https://hackernoon.com/when-good-enough-ux-becomes-the-most-expensive-decision-youll-ever-make?source=rss</link><author>Vaishnavi Ramamoorthy</author><category>tech</category><pubDate>Fri, 23 Jan 2026 05:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I've been designing products for over a decade now, and I've noticed something: most design debt doesn't start with bad design. It starts with decisions that made perfect sense at the time.A PM says, "Let's ship this for now, we'll clean it up in Q3." The team's underwater. The deadline's real. And honestly? Shipping something imperfect is often the right call. Products have to move. I've made these calls myself.But somewhere between launching and scaling, something shifts. The usability issues that were supposed to be temporary become permanent. Not dramatically. Just quietly enough that users adapt, and teams stop seeing the problem.The thing nobody tells you about "later""We'll fix it later" might be the most expensive lie in product development.Because once a workaround actually works, it becomes normalized. Once users figure out how to compensate for bad UX, the friction stops showing up in your metrics. And once revenue is flowing, the urgency to fix it evaporates.That's how UX debt sneaks in. Not through negligence, but through success. You shipped, users adapted, and now changing it would break more things than it fixes.Tech debt will eventually crash your system. UX debt just slowly erodes trust until users quietly leave for something that doesn't exhaust them.Constraint is what forces clarityThere's this romantic idea that need drives good design. I used to believe that too.But what I've actually seen is that constraint drives good design. When you have limited resources, you're forced to be ruthless about what matters. You strip features to essentials. You design for clarity because complexity costs too much.Then the scale arrives, and everything flips.Suddenly, there's a budget to patch edge cases rather than solve them. New features get bolted onto old flows because rebuilding would take three sprints. Internal logic starts replacing user logic because, well, the team understands it and users will figure it out eventually.The product still "works." It only works if you already know how to use it.Users will learn anything. That's the problem.Here's what makes UX debt so insidious: users are incredibly adaptable. They'll memorize your broken flows. They'll tolerate awkward steps. They'll blame themselves when something feels confusing.I've watched users in research sessions apologize for not understanding an interface that was objectively terrible. "Sorry, I'm probably just doing this wrong."So the signals teams rely on - drop-off rates, complaints, rage clicks - often come too late. By the time you notice a usability problem at scale, it's not a design issue anymore. It's an organizational one.Because now multiple teams depend on that flow. Changing it would break analytics, experiments, and Development scope. Nobody owns the whole experience. And fixing it would require coordination across product, eng, data, and legal.That's your UX debt compounding with interest.This is the phrase that should terrify you.I've heard it in probably fifty meetings over my career, and it never means what people think it means. It doesn't mean the experience is good. It means the experience has successfully trained users to lower their expectations.And once that happens? Innovation slows. Conversion plateaus. Support costs creep up in ways that are hard to trace. Retention erodes, but quietly, so you don't connect it to the UX issues you decided were "fine."==You don't lose users because your product fails. You lose them because using it is exhausting.==What actually prevents thisThe teams I've seen handle this well don't obsess over perfection. They obsess over reversibility.They ask questions like: Can we undo this decision later without breaking everything? Are we optimizing for speed or clarity right now, and do we have a good reason? What assumptions are we baking into this that future users won't share?They treat usability like infrastructure, not polish. Because once UX becomes a "nice to have," you've already lost.UX debt isn't about ugly screens or outdated visual design.It's about extra cognitive load: the thinking users shouldn't have to do, the invisible rules they have to remember, the mental models that stopped matching reality two versions ago.And cognitive load is expensive. For users, for support teams, for growth, for retention.The irony is that teams often spend more engineering effort working around bad UX than it would take to fix it. I've seen entire features built just to compensate for a confusing core flow that should have been redesigned years ago.A test you can run tomorrowAsk a new hire to use your product without any guidance. Just watch.Notice where they hesitate. Where they say "Why does it work this way?" Pay attention to what they assume versus what's actually true.That gap is your UX debt, fully compounded and charging interest.Look, sometimes "good enough" design is the right call. I'm not advocating for perfectionism. Products have to ship.But when "good enough" becomes your strategy, when you stop revisiting and improving usability even when things are working, you're building debt. And unlike financial debt, you can't see the balance. You just feel it in the slow erosion of user loyalty and the increasing cost of doing anything new.Need might invent solutions. But sustained success requires that usability be designed, revisited, and defended, especially when metrics say everything's fine.==Because the most dangerous UX problems are the ones nobody complains about anymore.==]]></content:encoded></item><item><title>SemanticGen Proves Video AI Doesn’t Need More Power—Just Better Abstractions</title><link>https://hackernoon.com/semanticgen-proves-video-ai-doesnt-need-more-powerjust-better-abstractions?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:35:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[SemanticGen proves that smarter abstractions—not more GPUs—are the key to scalable, coherent video generation.]]></content:encoded></item><item><title>The Game AI Problem Computers Were Never Built to Solve</title><link>https://hackernoon.com/the-game-ai-problem-computers-were-never-built-to-solve?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:34:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[An explainer on why brute-force AI fails at grand strategy games, and how hybrid LLM architectures enable long-horizon strategic reasoning.]]></content:encoded></item><item><title>Exploring Quantum Machine Learning for Time Series Stock Prediction</title><link>https://hackernoon.com/exploring-quantum-machine-learning-for-time-series-stock-prediction?source=rss</link><author>Nikhil Adithyan</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:10:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Intro: Stock Price Prediction, Machine Learning, and Quantum MLToday, we’re diving into the intersection of quantum computing and machine learning, exploring quantum machine learning and its application in stock price prediction. Our main goal is to compare the performance of a quantum neural network for stock price time series forecasting with a simple single-layer MLP.Let’s start by importing the necessary libraries for our analysis. These libraries will provide the basic tools required to explore and implement our project.import numpy as np
import pandas as pd
import requests
import json
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import mean_squared_error
from qiskit import QuantumCircuit
from qiskit.circuit.library import PauliFeatureMap
from qiskit.algorithms.optimizers import ADAM
from qiskit.circuit import Parameter
from qiskit.primitives import Sampler
FMP’s historical data API offers a conveniently accessible endpoint, providing a diverse and extensive collection of historical stock data that proves invaluable at every step of our project. This resource enables us to access a wide range of financial information, enhancing the depth and accuracy of our analysis. Its user-friendly interface and comprehensive dataset contribute significantly to the success and efficiency of our research and implementation.Now we are going to extract historical data as follows:api_url = "https://financialmodelingprep.com/api/v3/historical-price-full/AAPL?apikey=YOUR API KEY"

# Make a GET request to the API
response = requests.get(api_url)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Parse the response JSON
    data = response.json()
else:
    print(f"Error: Unable to fetch data. Status code: {response.status_code}")

data
Replace "YOUR API KEY" with your secret API key which you can obtain by . The output is a JSON response which looks as follows:In regular computers, we have tiny switches called “digital gates.” These switches control how information moves around. They work with basic units of data called “bits,” which can be either 0 or 1. The gates help computers do calculations and process stuff. Now, in quantum computers, we use something called “qubits” instead of bits. Qubits are special because they can be both 0 and 1 at the same time. It’s like having a coin that’s spinning and showing both heads and tails until you catch it, and then it picks one side.When we say the “wave function collapses,” it’s just a fancy way of saying the qubit decides to be either 0 or 1 when we check it. We make these qubits using different things like light particles (photons), tiny particles that make up stuff (atoms), or even small electrical circuits (Josephson junctions). These are like the building blocks for our special qubits.These quantum systems (particles or circuits) do some interesting things. They can be in different states at the same time (superposition), connect in a special way (entanglement), and even go through barriers they shouldn’t (tunneling).What’s cool is that quantum computers, with their qubits and special behaviors, use certain algorithms to solve some problems faster than regular computers. It’s like having a new tool that might help us solve tough puzzles more efficiently in the future.In traditional computing, we perform operations using basic logic gates like AND, NOT, and OR. These gates work with 0s and 1s, and their rules are based on a simple mathematical system called: \n which essentially deals with counting modulo 2.Now, imagine a quantum computer — it also has gates, but these are like supercharged versions. Instead of dealing with simple bits, quantum gates work with quantum bits or qubits. The math behind these quantum gates involves complex numbers and Matrix operations.Let’s take the quantum NOT gate, called:as an example. Apply it to a qubit initially in the state ∣0⟩, and the operator flips it to ∣1⟩ , and if you apply it again, it goes back to ∣0⟩. It’s a bit like flipping a coin.There’s also the Hadamard gate (H) that does something really cool. Applying it to a qubit initially in the state ∣0⟩ puts it in this special mix of 0 and 1 states at the same time to show mathematically H operates on |0⟩ and converts it into the standard superposition of the basis states:It’s like having a coin spinning in the air, showing both heads and tails until it lands.Now, let’s talk about the Controlled-NOT (CNOT) gate. This one works on two qubits. If the first qubit is ∣1⟩, it flips the second qubit from ∣0⟩ to ∣1⟩ or vice versa. It’s like a quantum switch that depends on the state of the first qubit.In the quantum world, things get more interesting. If you have two qubits in a special state, the CNOT gate uniquely rearranges their combinations, creating what we call entanglement. This entanglement is like a special connection between the qubits, making them behave in a coordinated manner. \n So, in a nutshell, while regular computers use basic rules with 0s and 1s, quantum computers have these fascinating gates that play with probabilities, mix states, and create connections between qubits, opening up a world of possibilities for solving complex problems more efficiently.In our project, we place special emphasis on a category of gates known as parameterized gates. These gates exhibit behavior that is contingent on specific input parameters, denoted by the symbol θ. Notably, we focus on rotation gates such as: \n each characterized by a unitary matrix as described in the below figure: \n Let’s delve a bit deeper into these rotation gates. Consider:envision it as a quantum gate resembling a rotating door that allows for the rotation of a qubit by a specific angle θ. Thegates function similarly, introducing rotations around different axes.The significance of these gates lies in their parameterized nature. By adjusting the input parameter θ, we essentially introduce a customizable element into our quantum algorithms. These gates serve as the foundational components for constructing the quantum neural network integral to our Project.In essence, θ acts as a tuning parameter, akin to a knob, enabling us to finely adjust and tailor the behavior of our quantum algorithms within the framework of the quantum neural network. This flexibility becomes pivotal in optimizing and customizing the performance of our quantum algorithms for specific tasks.Quantum algorithms can be thought of as a series of operations performed on a quantum state, represented by expressions like:These algorithms are translated into quantum circuits, as illustrated in Figure below. In this depiction, the algorithm starts from the initial state |q1⟩ = |00⟩ and concludes with a measurement resulting in either |00⟩ or |11⟩ with an equal probability of 0.5, recorded into classical bits (line c).In a quantum circuit, each horizontal line corresponds to a single qubit, and gates are applied sequentially until measurement. It’s important to note that loops are not allowed in a quantum program. A specific type of quantum circuit is the Variational Quantum Circuit (VQC). Notably, VQC incorporates parameterized gates like the aforementioned Ry(θ), R_z(θ).In simpler terms, quantum algorithms are like step-by-step instructions for a quantum computer, and quantum circuits visually represent these steps. The Variational Quantum Circuit introduces a special kind of flexibility with parameterized gates, allowing for customization based on specific values, denoted by θ.The primary objective of QML is to devise and deploy methods capable of running on quantum computers to address conventional supervised, unsupervised, and reinforcement learning tasks encountered in classical Machine Learning.What makes QML distinct is its utilization of quantum operations, leveraging unique features like superposition, tunneling, entanglement, and quantum parallelism inherent to Quantum Computing (QC). In our study, we specifically concentrate on Quantum Neural Network (QNN) design. A QNN serves as the quantum counterpart of a classical neural network.Breaking it down, each layer in a QNN is a Variational Quantum Circuit (VQC) comprising parameterized gates. These parameters act as the quantum equivalents of the weights in a classical neural network. Additionally, the QNN incorporates a mechanism to exchange information among existing qubits, resembling the connections between neurons in different layers of a classical network. Typically, this information exchange is achieved through entanglements, employing operators such as the CNOT gate. \n Creating a Quantum Machine Learning (QML) model typically involves several steps, as illustrated in Figure above. First, we load and preprocess the dataset on a classical CPU. Next, we use a quantum embedding technique to encode this classical data into quantum states on a Quantum Processing Unit (QPU) or quantum hardware. Once the classic data is represented in quantum states, the core model, implemented in the ansatz, is executed, and the results are measured using classical bits. Finally, if needed, we post-process these results on the CPU to obtain the expected model output. In our study, we follow this overall process to investigate the application of a Quantum Neural Network for time series forecasting.A Quantum Neural Network (QNN) typically consists of three main layers: This layer transforms classical input data into a quantum state. It uses a parameterized variational circuit with rotation and controlled-rotation gates to prepare the desired quantum state for a given input. This step, known as quantum embedding, employs techniques like basis encoding, amplitude encoding, Hamiltonian encoding, or tensor product encoding. The heart of the QNN, this layer contains a Variational Quantum Circuit, repeated L times to simulate L network layers classically. It’s responsible for processing and manipulating quantum information. This layer performs measurements on qubits, providing the final expected outcome.For the input layer, we use a tensor product encoding technique. It involves a simple X-rotation gate for each qubit, where the gate parameter is set by scaling the classic data to the range [-π, π]. Although it’s a quick and straightforward encoding method (O(1) operations), it has limitations. The number of qubits needed scales linearly with the input classic data. To address this, we introduce learnable parameters for scaling and bias in the input data, enhancing the flexibility of the quantum embedding. In Figure 3, you can see an example of the input layer for a network with 3 qubits, where classic data features:, input scale parameters:Regarding the ansatz, it’s interesting to note that, unlike classical neural networks, there isn’t a fixed set of quantum layer structures commonly found in the literature (such as fully connected or recurrent layers). The realm of possible gates for quantum information transfer between qubits is extensive, and the optimal organization of these gates for effective data transfer is an area that hasn’t been thoroughly explored yet.In our project, we adopt the Real Amplitudes ansatz, a choice inspired by its success in various domains like policy estimation for quantum reinforcement learning and classification. This ansatz initiates with full rotation X/Y/Z parameterized gates, akin to the quantum version of connection weights. It is then followed by a series of CNOT gates arranged in a ring structure to facilitate qubit information transfer. Figure 4 provides a visual representation of how this ansatz is implemented, serving as the quantum equivalent of a network layer for a 3-qubit network.To break it down, a quantum network layer in our work involves a set of parameters totaling 3 times the number of qubits (3*n), where ’n’ represents the number of qubits in the quantum network.\n Now, let’s talk about the output layer, which is a critical part of our quantum model. In quantum computing, when we want to extract information from our quantum state, we often perform measurements using a chosen observable. One such commonly used observable is represented by the σ_z operator over the computational basis. To understand this, think of it as a way to extract information from our quantum state.The network output is determined by calculating the expectation of this observable over our quantum state. This is expressed as ⟨ψ|σ_z|ψ⟩, where ⟨ψ| denotes the complex conjugate of |ψ⟩. The result falls within the range of [-1, 1].No need to stress over those complex mathematical equations — our trusty library, Qiskit, has got it covered! Qiskit will handle all the intricate quantum calculations seamlessly, making the quantum computing process much more accessible for us. So, you can focus on exploring the quantum world without getting bogged down by the nitty-gritty mathNow, to make our network output less sensitive to biases and scales inherent in the dataset, we introduce a final scale parameter and bias to be learned. This step adds a layer of adaptability to our model, allowing it to fine-tune and adjust the output based on the specific characteristics of our data. The entire model architecture is visually represented in the figure below.\n The training of our proposed Quantum Neural Network (QNN) happens on a regular CPU using classical algorithms like the Adam optimizer. The CPU handles the gradient computation through traditional propagation rules, while on the Quantum Processing Unit (QPU), we calculate the gradient using the parameter-shift rule. It’s a bit like having a dual system where the CPU manages the main training, and the QPU comes into play for specific quantum computations.Visualize the training process pipeline in Figure 6, where θ¹ represents the scale/bias parameters in the input layer, θ² corresponds to the parameters of the layers containing the ansatz, and θ³ are the scale/bias parameters for the network outputs. This orchestration ensures a cohesive training approach, leveraging both classical and quantum computing resources.\n As a Quantum Neural Network (QNN) operates as a feedforward model, our initial step involves defining a time horizon, denoted as T. To adapt the time series data for the QNN, we transform it into a tabular format. Here, the target is the time series value at time t, denoted as x(t), while the inputs encompass the values x(t-1), x(t-2), …, x(t-T). This restructuring facilitates the model’s understanding of the temporal relationships in the data, allowing it to make predictions based on past values.First, we fetch the data using the historical Data API endpoint provided by Financial Modeling Prep as follows: \n api_url = "https://financialmodelingprep.com/api/v3/historical-price-full/AAPL?apikey=YOUR API KEY"

# Make a GET request to the API
response = requests.get(api_url)

# Check if the request was successful (status code 200)
if&nbsp;response.status_code == 200:&nbsp;&nbsp;&nbsp;&nbsp;
    # Parse the response JSON&nbsp;&nbsp;&nbsp;&nbsp;
    data = response.json()
else:&nbsp;&nbsp;&nbsp;&nbsp;
    print(f"Error: Unable to fetch data. Status code: {response.status_code}")

# convert into a datframe
df = pd.json_normalize(data, 'historical', ['symbol']) 
df.tail()
\n The output is a Pandas dataframe which looks something like this (before that, make sure to replace YOUR API KEY with your secret API key): \n \
From this plethora of data, we are going to use open price as our temporal variable and we will work with 500 data points each representing daily open prices, and our window size for prediction would be 2.final_data = df[['open', 'date']][0:500] # forming filtered dataframe
input_sequences = []
labels = []

# Creating input and output data for time series forecasting
for&nbsp;i in&nbsp;range(len(final_data['open'])):&nbsp;&nbsp;&nbsp;&nbsp;
    if&nbsp;i > 1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        labels.append(final_data['open'][i])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        input_sequences.append(final_data['open'][i-2:i+1].tolist())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

# creating train test split
x_train = np.array(input_sequences[0:400])
x_test = np.array(input_sequences[400:])
y_train = np.array(labels[0:400])
y_test = np.array(labels[400:])
\
Now, let’s plot the data we acquired.import&nbsp;matplotlib.pyplot as&nbsp;plt
plt.style.use('ggplot')

# Convert the 'date' column to datetime format
df['date'] = pd.to_datetime(df['date'])

# Plotting the time series data
plt.figure(figsize=(10, 6))
plt.plot(df['date'][0:500], df['open'][0:500], marker='o', linestyle='-')

# Adding labels and title
plt.xlabel('date')
plt.ylabel('open')
plt.title('Time Series Data')

# Display the plot
plt.grid(True)plt.show()
\
Following is the output:num_features =  2
feature_map = PauliFeatureMap(feature_dimension = num_features, reps = 2)
optimizer = ADAM(maxiter = 100)
\
In our approach, we employ the Pauli Feature Map to encode our data into qubits, specifically leveraging 2 features. The encoding circuit is structured as follows:Furthermore, for optimizing our model, we utilize the ADAM optimizer. This choice helps fine-tune the parameters of the quantum neural network, enhancing its overall performance.def&nbsp;ans(n, depth):&nbsp;&nbsp;&nbsp;&nbsp;
    qc = QuantumCircuit(n)&nbsp;&nbsp;&nbsp;&nbsp;
    for&nbsp;j in&nbsp;range(depth):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        for&nbsp;i in&nbsp;range(n):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            param_name = f'theta_{j}_{i}'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            theta_param = Parameter(param_name)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            qc.rx(theta_param, i)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            qc.ry(theta_param, i)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            qc.rz(theta_param, i)&nbsp;&nbsp;&nbsp;&nbsp;
    for&nbsp;i in&nbsp;range(n):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        if&nbsp;i == n-1:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            qc.cnot(i, 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            qc.cnot(i, i+1)&nbsp;&nbsp;&nbsp;
    return&nbsp;qc
\n This function initializes a quantum circuit with the number of qubits equal to the number of features. In the first loop, it appends rotation gates to the circuit, each with parameterized rotational angles. The second loop adds CNOT gates to the circuit. For each iteration, a CNOT gate is appended with the current qubit as the control (0th qubit) and the target qubit determined by the loop index. Additionally, another CNOT gate is appended with the current qubit as the control and the next qubit as the target.This process constructs the ansatz for our quantum circuit, essentially creating the quantum neural network structure we defined earlier. The diagram for this circuit has been presented previously for reference.**Initializing the Anstaz circuit:
\ ansatz = ans(num_features, 5) # anstaz(num_qubits=num_features, reps=5)

# creating train test split
x_train = np.array(input_sequences[0:400])
x_test = np.array(input_sequences[400:])
y_train = np.array(labels[0:400])
y_test = np.array(labels[400:])
\n Now, we proceed to create a Variational Quantum Circuit (VQC) that functions as a neural network. In this circuit, we incorporate both the ansatz, which defines the structure of our quantum neural network, and the encoding for our features. For this purpose, we import the VQC module from qiskitlearning.classifiers. The VQC encapsulates the quantum processing aspects of our neural network, and its integration with Qiskit simplifies the implementation of quantum machine learning algorithms.vqr = VQR(&nbsp;&nbsp;&nbsp;&nbsp;
    feature_map = feature_map,&nbsp;&nbsp;&nbsp;&nbsp;
    ansatz = ansatz,&nbsp;&nbsp;&nbsp;&nbsp;
    optimizer = optimizer,)

vqr.fit(x_train,y_train)
vqr_mse = mean_squared_error(y_test, vqr.predict(x_test))

# Calculate root mean squared error
vqr_rmse = np.sqrt(vqr_mse)
\
In the final step, we fit the Variational Quantum Circuit (VQC) to our features. This involves training the quantum neural network on our dataset. Once fitted, we assess the performance by calculating the mean and root mean errors. This evaluation step helps us gauge the accuracy and effectiveness of our Variational Quantum Circuit in handling the given features and predicting outcomes.Now, let’s construct a straightforward classical neural network to serve as a benchmark for comparison with the quantum neural network. Our chosen architecture will be a simple Multilayer Perceptron (MLP) featuring a single hidden layer equipped with 64 nodes. This classical neural network will provide us with a reference point against which we can evaluate the performance of the quantum neural network.model = Sequential()
model.add(Dense(64,activation = 'relu', input_shape = (4,)))
model.add(Dense(1))

model.compile(optimizer = 'adam', loss = 'mean_squared_error')

model.fit(x_train, y_train, epochs = 20, batch_size = 32, validation_data = (x_test,y_test))

loss = model.evaluate(x_test, y_test)
prediction = model.predict(x_test)

ann_mse = mean_squared_error(y_test, prediction.flatten())
ann_rmse = np.sqrt(ann_mse)
The following are the results: \n In this evaluation, it is evident that the Quantum Neural Network (QNN) has shown promise by outperforming the Artificial Neural Network (ANN). Nevertheless, it’s crucial to acknowledge that the Root Mean Squared Error (RMSE) values obtained may fall short of meeting the desired level of satisfaction. The primary objective of this experiment was to spotlight the potential of quantum computing, showcasing its ability to generate superior results and construct models applicable for commercial use.This study anticipates that as quantum computers continue to advance, attaining a heightened level of robustness for effectively training these models, the technology will progressively become more practical for real-world applications in the near future.Building upon these findings, it becomes evident that Quantum Neural Networks (QNN) hold promise for further development and practical implementation. While current benchmarks in time series forecasting may currently outshine QNN, there exists significant potential for improvement. Addressing the current limitations is foreseeable, especially with ongoing advancements in quantum computing. Successfully developing robust quantum computers could unlock the door to achieving even more superior results in the realm of time series forecasting.: I’ve presented a basic demonstration of both a Quantum Neural Network (QNN) and a Classical Neural Network (CNN) to illustrate their construction and highlight differences in outcomes. It’s important to note that discrepancies may arise in QNN, so feel free to adapt the provided examples to better suit your specific use case. Adjust the code and parameters accordingly to optimize performance and address any challenges that may arise in the implementation of Quantum Neural Networks for your particular application.In summary, this project explored quantum computing and the creation of neural networks, highlighting their potential for future growth. As technology advances, there is an opportunity to develop more sophisticated quantum machine learning algorithms with quantum computers. These systems can significantly reduce training times, benefitting from the efficiency of qubits and enabling the use of a greater number of qubits in circuit formation. This progress opens doors to enhanced computational power and problem-solving capabilities, indicating a promising path for the future of quantum computing in machine learning applications.With that being said, you’ve reached the end of the article. Hope you learned something new and useful today. Also, let me know in the comments about your take on Quantum Machine Learning and its impact on the stock market sector. Thank you for your time.]]></content:encoded></item><item><title>I Added CISA KEV to Vulnerability Prioritization and Coverage Jumped +413%</title><link>https://hackernoon.com/i-added-cisa-kev-to-vulnerability-prioritization-and-coverage-jumped-413percent?source=rss</link><author>Mikhail Alekseev</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:07:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In this article, I explain how I enhanced and integrated the CISA Known Exploited Vulnerabilities (KEV) catalog into my vulnerability management platform. Additionally, I compare the signals from CISA KEV with those from a commercial vulnerability management platform.CISA KEV (U.S. Cybersecurity and Infrastructure Security Agency Known Exploited Vulnerabilities Catalog) is a curated list of vulnerabilities that have confirmed evidence of active exploitation.  When a Single Vendor’s Expertise Is Not EnoughMost modern vulnerability management (VM) platforms maintain their own indicators of actively exploited vulnerabilities. While these vendors rely on their internal threat and vulnerability intelligence there can be discrepancies in the signals provided by vendors and those from the CISA KEV (Known Exploited Vulnerabilities). \n  \n A sound strategy for any Security Operations Center (SOC) is to augment the signals from the VM platform with as many independent and authoritative sources of data as possible.CISA KEV Integration and Data EnrichmentIntegration options can differ based on the organization and its current tools. To illustrate this concept, I have created a simple, high-level diagram that demonstrates the fundamental ideas behind enriching VM data with CISA KEV signals.I use the approach illustrated on the right. For input sources, I rely on the official CISA KEV export and an export from the VM platform that identifies vulnerabilities flagged as actively exploited. At a high level, the integration process works as follows: \n 1. A script retrieves the current vulnerability list from the VM platform via its API. \n 2. The script downloads the KEV dataset from the official CISA source. \n 3. The data is normalized, parsed, and merged using the CVE identifier as the key matching condition. \n 4. A prioritization rule is applied to each vulnerability. If it is marked as actively exploited by the vendor and/or is present in the CISA KEV, it is assigned an elevated priority.Research Findings and ImpactThe research aspect of this project was the most compelling for me. The objective was to validate the practical impact of integrating CISA KEV in my case. To achieve this, I defined a set of indicators, performed statistical calculations using Python and Pandas in JupyterLab, and visualized the results with Matplotlib. The datasets utilized for the analysis consisted of the CISA KEV export in CSV format and an export of actively exploited vulnerabilities from a commercial VM solution.Trending Vulnerability Awareness Coverage is a metric indicating how many actively exploited vulnerabilities each source identified monthly:Interpretation of Results:The blue segments (representing KEV-added vulnerabilities) consistently exceed the red segments (representing Vendor VM vulnerabilities) across most months. This indicates that CISA KEV identifies a significant portion of actively exploited vulnerabilities that are not captured by vendor VM platforms.After a peak period, KEV continues to contribute a stable number of additional vulnerabilities over time.Even in months when Vendor VM identifies actively exploited vulnerabilities, KEV consistently adds more exploited CVEs. This confirms that CISA KEV acts as an independent and authoritative signal for exploitation, rather than duplicating the intelligence provided by vendors.Cumulative Trending Vulnerability Awareness Coverage is a metric that illustrates how many actively exploited vulnerabilities each source has accumulated over time: considers overlapping CVEs between CISA KEV and vendor sources while incorporating unique CVEs contributed by CISA KEV (+1,282 vulnerabilities).Interpretation of Results:CISA KEV consistently identifies more actively exploited vulnerabilities than the vendor VM platform. The blue segments (Total CISA KEV) surpass the red segments (Total Vendor VM) in most months, indicating that a significant number of actively exploited vulnerabilities are identified through CISA KEV and are not included in the vendor-defined actively exploited signals.The value of KEV remains consistent over time, rather than being confined to peak periods. After the initial increase, KEV continues to reveal a steady number of additional exploited vulnerabilities, demonstrating its ongoing significance rather than reflecting a one-time alignment.KEV provides independent exploitation intelligence rather than duplicating vendor signals. Even during months when the vendor VM platform identifies actively exploited vulnerabilities, KEV consistently adds more CVEs that are already being exploited. This confirms that CISA KEV serves as an authoritative, independent exploitation signal instead of merely reflecting vendor intelligence.Mean Time to Trending Classification (MTTC) refers to the average time it takes from the publication of a Common Vulnerabilities and Exposures (CVE) to the moment it is classified as actively exploited by a specific source.The analysis included only those vulnerabilities that were present in both sources and had a known publication date. At the time of comparison, this intersection consisted of .It's important to note that some vulnerabilities were published as early as , before dedicated vulnerability catalogs became widely established. Therefore, the absolute MTTC values should be regarded as indicative rather than strictly precise.This formula simplifies the metric's interpretation: the lower the MTTC, the quicker a source identifies a vulnerability as actively exploited.A shorter classification time directly enhances response speed to actual threats.According to the aggregated results (measured in days), the integration demonstrates a clear impact: 69 days for the custom prioritization approach, 95 days for the vendor vulnerability management platform, and 169 days for CISA's Known Exploited Vulnerabilities (KEV) list alone. By combining expertise from both sources, the overall classification time is reduced, leading to faster operational responses and more effective threat mitigation.Integrating CISA KEV has led to an approximately 4.1 times (>400%) increase in awareness compared to a vendor-only approach.This integration also achieves the fastest average time to classify vulnerabilities as actively exploited within the infrastructure by utilizing the earliest available exploitation signals from multiple sources.By creating a dedicated web dashboard, this integration improved our ability to detect, track, and reassess high-impact vulnerabilities over time. For instance, vulnerabilities that were previously excluded from remediation workflows may later be identified as actively exploited. This information allows us to re-evaluate their status and adjust remediation priorities accordingly. \n Integrating CISA KEV into a vulnerability management system offers an authoritative signal of real-world active exploitation, enhancing prioritization efforts. This signal is valuable not only for vulnerability management reporting but also for Security Operations Center (SOC) processes, including threat hunting and monitoring operational exposure.Even if a commercial VM platform provides its own indicators of actively exploited vulnerabilities and related threat intelligence, CISA KEV can serve as an additional and independent source. This integration increases the coverage of exploited threats and strengthens the prioritization process, making it more robust and reliable.]]></content:encoded></item><item><title>Contract-First APIs: How OpenAPI Becomes Your Single Source of Truth</title><link>https://hackernoon.com/contract-first-apis-how-openapi-becomes-your-single-source-of-truth?source=rss</link><author>malkomich</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:06:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[1. Introduction & Contract-First ApproachAPIs are the arteries of modern software, powering everything from mobile apps to distributed cloud microservices. Building those APIs, however, is rarely as straightforward as writing a few controller methods. If you've worked on any enough complex backend, you've probably wrestled with inconsistent request/response payloads, mismatched client/server contracts, or ambiguous endpoint documentation. I certainly have, and the pain points are always the same: a frontend developer working from outdated documentation, a client blocked because the API shape changed unexpectedly, or integration tests failing because nobody synchronized the contract.That's why I advocate for the "contract-first," or API-first, approach. Instead of treating the OpenAPI specification as an afterthought—something you generate from annotations or write to satisfy a documentation requirement—you define your API contract  implementing it. This inverts the traditional workflow in a way that fundamentally changes how teams collaborate. Your OpenAPI spec becomes the single source of truth that both server and client implementations derive from, ensuring they can never drift apart. The spec drives automatic, always-current documentation. It aligns product managers, frontend engineers, backend developers, and external partners around a shared understanding before a single line of implementation code is written.With tools like the OpenAPI Generator and Maven, you can turn a single OpenAPI spec into a production-grade Java REST backend  type-safe SDK clients in multiple languages. I've seen this approach cut integration time from weeks to days in microservice architectures and eliminate entire categories of bugs related to contract mismatches. Today, I'll walk you through building a real workflow for this: from designing the OpenAPI 3.0 spec, to generating Spring Boot controllers and Java client SDKs, to handling API evolution gracefully. I'll also share the gotchas I've learned the hard way in production environments.2. OpenAPI Specification SetupThe foundation of everything we're building starts with the OpenAPI spec itself. In a contract-first workflow, your OpenAPI YAML or JSON file isn't just documentation—it's an executable contract that drives code generation, testing, and deployment. Every detail matters because ambiguity in the spec translates directly to ambiguity in generated code. I've learned to be obsessively precise here, because time invested in a well-crafted spec pays exponential dividends downstream.Consider a basic bookstore API. Before writing any Java controllers or setting up Spring Boot, we define exactly how this API should behave. Here's what that looks like in OpenAPI 3.0:openapi: 3.0.3
info:
  title: Bookstore API
  version: 1.0.0
  description: |-
    APIs to manage books and orders in a bookstore.
servers:
  - url: http://localhost:8080/api
paths:
  /books:
    get:
      summary: List all books
      responses:
        '200':
          description: A list of books
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/Book'
    post:
      summary: Add a new book
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/BookRequest'
      responses:
        '201':
          description: Book created
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Book'
components:
  schemas:
    Book:
      type: object
      required:
        - id
        - title
        - author
      properties:
        id:
          type: string
          format: uuid
        title:
          type: string
        author:
          type: string
        price:
          type: number
        inStock:
          type: boolean
    BookRequest:
      type: object
      required:
        - title
        - author
      properties:
        title:
          type: string
          minLength: 1
        author:
          type: string
        price:
          type: number
          minimum: 1
        inStock:
          type: boolean
          default: true
Notice the level of specificity. I'm not just saying "there's a title field"—I'm declaring it's required, it's a string, and it must have at least one character. The price must be a number with a minimum value of one. The Book model requires an ID, title, and author, while the BookRequest model (what clients send when creating a book) has slightly different requirements. This separation between request and response models is deliberate. In production, you rarely want clients sending IDs for new resources—those should be server-generated. By defining distinct schemas, the generated code enforces these business rules at compile time.The validation constraints embedded here—, , —aren't just documentation. When we generate code in the next section, these become actual Hibernate Validator annotations in your Java models. Client SDKs gain the same type safety. A TypeScript client will know that  is a number, not a string. A Go client will have required fields that can't be omitted. This is the power of treating the contract as code: you're not just describing the API, you're programming the behavior of every system that interacts with it.For project organization, I recommend keeping your OpenAPI spec in the resources directory in your repository. In a Maven project, the structure typically looks like this:project-root/
  src/
    main/
      java/...
      resources/
        bookstore-api.yaml
        ...
    test/
      java/...
  pom.xml
This separation also enables smooth integration with CI/CD tools. Your pipeline can lint the spec, generate documentation, run contract tests, and publish versioned artifacts—all independent of your Java compilation. The spec becomes the input to multiple downstream processes, not just a sidecar to your server implementation.3. Server-Side GenerationWith a solid spec in hand, we're ready to generate our Spring Boot server code. The OpenAPI Generator Maven plugin is the engine that transforms our declarative YAML into concrete Java interfaces and model classes. Configuring this plugin correctly is crucial because it determines not just what code gets generated, but how that code integrates with your handwritten business logic.Here's the plugin configuration in your  that I've refined across multiple production projects:<plugin>
  <groupId>org.openapitools</groupId>
  <artifactId>openapi-generator-maven-plugin</artifactId>
  <version>7.12.0</version>
  <executions>
    <execution>
      <id>code-generate-spring</id>
      <goals>
        <goal>generate</goal>
      </goals>
      <configuration>
        <inputSpec>${project.basedir}/src/main/resources/bookstore-api.yaml</inputSpec>
        <generatorName>spring</generatorName>
        <apiPackage>com.bookstore.api</apiPackage>
        <modelPackage>com.bookstore.model</modelPackage>
        <invokerPackage>com.bookstore.invoker</invokerPackage>
        <library>spring-boot</library>
        <configOptions>
          <reactive>true</reactive>
          <delegatePattern>true</delegatePattern>
          <interfaceOnly>true</interfaceOnly>
          <useTags>true</useTags>
          <dateLibrary>java8</dateLibrary>
          <useBeanValidation>true</useBeanValidation>
        </configOptions>
      </configuration>
    </execution>
  </executions>
</plugin>
The  option is the key architectural decision here. When set to true, the generator creates Java interfaces for each API path—not concrete implementations. This gives you the perfect separation of concerns: the generated code defines the contract (method signatures, parameter types, return types), while your handwritten code provides the implementation. I cannot overstate how valuable this separation becomes as APIs evolve. When you modify your OpenAPI spec and regenerate, your IDE immediately shows you which controller methods need updates because the interface changed. No grep'ing through logs, no runtime surprises—just compile-time feedback.The  flag enables Hibernate Validator annotations on generated model classes. Remember those  and  constraints in our spec? They become , , and  annotations in the generated Java code. Spring Boot's validation framework automatically enforces these when requests arrive, rejecting invalid data before it reaches your business logic.After running mvn clean generate-sources, you'll find generated code under target/generated-sources/openapi. I've seen teams make the mistake of modifying this generated code directly. Don't. Treat generated sources as immutable build artifacts, like compiled  files. Any changes you make will be overwritten on the next build. Instead, your implementations live in your main source directory and reference the generated interfaces:package com.bookstore.controller;

import com.bookstore.api.BooksApi;
import com.bookstore.model.Book;
import com.bookstore.model.BookRequest;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.RestController;

import java.util.List;

@RestController
public class BooksController implements BooksApi {

    private final BookRepository bookRepository;
    private final BookService bookService;

    public BooksController(BookRepository bookRepository, BookService bookService) {
        this.bookRepository = bookRepository;
        this.bookService = bookService;
    }

    @Override
    public ResponseEntity<List<Book>> getBooks() {
        List<Book> books = bookRepository.findAll();
        return ResponseEntity.ok(books);
    }

    @Override
    public ResponseEntity<Book> addBook(BookRequest bookRequest) {
        Book createdBook = bookService.addBook(bookRequest);
        return ResponseEntity.status(201).body(createdBook);
    }
}
This pattern feels natural in practice. You're implementing an interface, just like any other Java development. The difference is that this interface was derived from your API contract, so you get compile-time guarantees about correctness. If you add a new required parameter to an endpoint in your OpenAPI spec, the generated interface changes, and your controller won't compile until you update the implementation. This catches integration bugs at build time rather than in staging or—worse—production.The validation annotations on generated models work automatically with Spring's  annotation, but surfacing errors to clients in a user-friendly format requires a bit of plumbing. In production, I always implement a global exception handler to transform validation failures into structured error responses:@ControllerAdvice
public class ApiExceptionHandler {

    @ExceptionHandler(MethodArgumentNotValidException.class)
    public ResponseEntity<ApiError> handleValidationError(MethodArgumentNotValidException ex) {
        ApiError error = new ApiError();
        error.setStatus(400);
        error.setMessage("Validation failed");
        error.setErrors(ex.getBindingResult()
            .getFieldErrors()
            .stream()
            .map(err -> err.getField() + ": " + err.getDefaultMessage())
            .collect(Collectors.toList()));

        return ResponseEntity.badRequest().body(error);
    }
}
This ensures that when a client sends a book with a negative price or an empty title, they receive a clear, actionable error message rather than a generic 500 or a stack trace. The validation rules specified once in your OpenAPI contract now flow all the way through to the error responses your clients see.One pitfall I've encountered repeatedly: teams sometimes struggle with the generated code being "in the way" during development. They're tempted to edit it for quick fixes or to add custom annotations. The solution is to use extension points. If you need custom behavior, extend or wrap the generated classes in your own source tree. Use .openapi-generator-ignore to prevent the generator from overwriting specific files if you absolutely must customize generated code, but be cautious—you're opting out of automatic contract enforcement for those files. In most cases, composition beats modification: write adapter classes that delegate to generated code while adding your customizations.The contract-first approach truly shines when you realize your OpenAPI spec can generate not just server code, but type-safe client SDKs in virtually any language. This is transformative for microservice architectures and external API consumers. Instead of each client team hand-rolling HTTP requests and parsing JSON, they get a strongly-typed SDK that's automatically synchronized with your server implementation—because both derive from the same contract.Generating a Java client SDK uses the same plugin with a different generator configuration:<execution>
  <id>generate-java-client</id>
  <goals>
    <goal>generate</goal>
  </goals>
  <configuration>
    <inputSpec>${project.basedir}/src/main/resources/bookstore-api.yaml</inputSpec>
    <generatorName>java</generatorName>
    <library>resttemplate</library>
    <apiPackage>com.bookstore.client.api</apiPackage>
    <modelPackage>com.bookstore.client.model</modelPackage>
    <output>${project.build.directory}/generated-sources/java-client</output>
  </configuration>
</execution>
After running mvn clean generate-sources, you have a fully functional Java client SDK with the same strong typing as your server. Here's what using that SDK looks like in a separate microservice or integration test:import com.bookstore.client.api.BooksApi;
import com.bookstore.client.model.Book;
import com.bookstore.client.model.BookRequest;
import org.springframework.web.client.RestTemplate;

public class BookInventoryFetcher {
    private final BooksApi booksApi;

    public BookInventoryFetcher(String apiUrl) {
        RestTemplate restTemplate = new RestTemplate();
        booksApi = new BooksApi();
        booksApi.setApiClient(new ApiClient(restTemplate).setBasePath(apiUrl));
    }

    public List<Book> fetchBooks() {
        return booksApi.getBooks();
    }

    public Book createBook(String title, String author, Double price) {
        BookRequest request = new BookRequest();
        request.setTitle(title);
        request.setAuthor(author);
        request.setPrice(price);

        return booksApi.addBook(request);
    }
}
Notice what's happening here. The client code has no raw HTTP calls, no JSON parsing, no string concatenation of URLs. The  class provides type-safe methods like  that return . The  and  models have the same fields, types, and validation constraints as the server. If the server API changes—say, you rename  to —the client SDK regenerates with that change, and any code using the old field name stops compiling. This is contract enforcement at its finest.The same workflow applies to other languages. Want a TypeScript client for your web frontend? Change  to . Need a Python SDK for a data pipeline? Use . Go for infrastructure tooling? . The OpenAPI Generator supports dozens of languages, each with multiple library options. In one project, I maintained server code in Java, a TypeScript SDK for the React frontend, a Python SDK for ML engineers, and a Go SDK for infrastructure automation—all generated from the same OpenAPI spec. When the API evolved, I regenerated all four SDKs in a single build step. The alternative—manually maintaining four client implementations—would have been a coordination nightmare.Publishing these SDKs is straightforward. Package the Java client as a Maven artifact and deploy it to your internal repository. Use npm for TypeScript, PyPI for Python, and so on. Version the SDKs to match your API version, and consuming teams can depend on them like any other library. This transforms your internal APIs into first-class, versioned products rather than endpoints that teams access via curl and prayer.In production, APIs don't stay static. Requirements change, new features emerge, and you discover design flaws that need correction. The contract-first approach, combined with generated code, provides elegant patterns for handling this evolution while maintaining stability for existing consumers.Maintaining multiple API versions side-by-side is surprisingly straightforward. Keep separate spec files— and —and configure separate plugin executions for each. Your server can implement both  and  interfaces, routing requests based on a version header or URL path segment like  versus . I've used this pattern to keep legacy endpoints alive for mobile apps that can't update immediately while rolling out breaking changes to web clients. The generated code keeps each version's contract enforced independently, preventing accidental cross-contamination of v1 and v2 logic.Integrating code generation into your CI/CD pipeline automates contract enforcement across your entire development workflow. In GitHub Actions or Jenkins, add a build step that runs mvn clean generate-sources and fails if generated code doesn't match what's committed. This catches developers who modified the spec locally but forgot to regenerate code, or vice versa. Here's a snippet from a GitHub Actions workflow that I use:name: API Contract Validation
on: [push, pull_request]
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up JDK
        uses: actions/setup-java@v3
        with:
          java-version: '17'
      - name: Generate OpenAPI code
        run: mvn clean generate-sources
      - name: Check for uncommitted changes
        run: |
          git diff --exit-code target/generated-sources/openapi
If generated code differs from what's in the repository, the build fails. This simple check has saved me from countless "it works on my machine" bugs related to contract drift.Contract testing takes this further by validating that both server and client implementations actually conform to the spec, not just that code was generated from it. Tools like Schemathesis or Spring Cloud Contract can execute tests against the running server using the OpenAPI spec as the test definition. These tests send requests covering every endpoint and parameter combination defined in the spec, then validate responses match the schema. I've caught bugs where business logic returned nullable fields that the spec declared non-nullable, or where enum values in practice diverged from the contract. Traditional unit tests rarely catch these issues because they test specific scenarios, not the entire contract surface.Documentation generation is almost trivial when your API is defined in OpenAPI. Adding Springdoc-OpenAPI to your dependencies automatically serves interactive Swagger UI documentation:springdoc.api-docs.path=/api-docs
springdoc.swagger-ui.path=/swagger-ui.html
With the server running, navigate to  and you have live, interactive API documentation that stays in sync with your implementation because it's reading directly from the runtime application. For static documentation—say, for embedding in a developer portal—generate ReDoc HTML as part of your build:npx redoc-cli bundle src/main/resources/bookstore-api.yaml -o docs/api-documentation.html
I've seen teams struggle with documentation staleness for years before adopting this approach. Once your spec is the source of truth, documentation becomes a free byproduct rather than a maintenance burden.6. Real-World Example: Bookstore MicroserviceTo see how all these pieces fit together in practice, consider a microservices architecture where Product and Inventory services need to coordinate. The Product service manages book metadata and handles customer queries. The Inventory service tracks stock levels and processes reservations. Both need to share a common understanding of what a "book" is and how stock checks work.Start by defining a shared OpenAPI contract in  that includes endpoints for querying available books and reserving inventory. In the Product service's , configure the generator to create Spring server interfaces:<execution>
  <id>code-generate-spring</id>
  <goals><goal>generate</goal></goals>
  <configuration>
    <inputSpec>${project.basedir}/src/main/resources/bookstore-api.yaml</inputSpec>
    <generatorName>spring</generatorName>
    <apiPackage>com.company.bookstore.product.api</apiPackage>
    <modelPackage>com.company.bookstore.model</modelPackage>
    <configOptions>
      <interfaceOnly>true</interfaceOnly>
      <useBeanValidation>true</useBeanValidation>
    </configOptions>
  </configuration>
</execution>
The Product service implements these interfaces to expose REST endpoints. Meanwhile, in the Inventory service's , configure the generator to create a Java client:<execution>
  <id>generate-java-client</id>
  <goals><goal>generate</goal></goals>
  <configuration>
    <inputSpec>${project.basedir}/src/main/resources/bookstore-api.yaml</inputSpec>
    <generatorName>java</generatorName>
    <library>resttemplate</library>
    <apiPackage>com.company.bookstore.inventory.client</apiPackage>
    <modelPackage>com.company.bookstore.model</modelPackage>
    <output>${project.build.directory}/generated-sources/java-client</output>
  </configuration>
</execution>
Now the Inventory service can call Product service endpoints using a type-safe SDK instead of raw HTTP. Both services share identical model definitions—the  class is generated identically in both codebases because it comes from the same spec. If you add a new field to Book in the contract, both services regenerate with that field. The Product service's endpoints automatically accept and return the new field, and the Inventory service's client SDK includes it. There's no way for the services to drift apart because the contract binds them together.For frontend integration, add a TypeScript client generation step:<execution>
  <id>generate-typescript-client</id>
  <goals><goal>generate</goal></goals>
  <configuration>
    <inputSpec>${project.basedir}/src/main/resources/bookstore-api.yaml</inputSpec>
    <generatorName>typescript-axios</generatorName>
    <output>${project.build.directory}/generated-sources/typescript-client</output>
  </configuration>
</execution>
Package the TypeScript SDK as an npm module, publish it to your registry, and frontend developers import it like any library. When they call , TypeScript's compiler enforces that they handle the response correctly—it knows the shape of the Book object, which fields are optional, and what types they have. The entire stack, from database to UI, is now bound by a single contract.In practice, I've seen this approach cut feature development time dramatically. A new "add review" feature that touches frontend, Product service, and Inventory service used to require careful coordination between three teams to ensure everyone's JSON payloads matched. With generated code, we updated the OpenAPI spec with a new  endpoint, regenerated all artifacts, and each team implemented against their generated interfaces. Integration happened in hours, not days, because mismatches were caught at compile time.7. Pitfalls & Lessons LearnedI've deployed this pattern across fintech platforms, e-commerce marketplaces, and SaaS products. It's not a silver bullet—there are scenarios where it struggles and mistakes that can undermine the benefits. Here's what I've learned the hard way.Code generation makes the most sense when your API contract is relatively stable. If your spec is in flux—changing daily as you explore a new feature space—you'll spend more time regenerating and updating implementations than you save. In early-stage projects or when prototyping, I sometimes write controllers by hand first, iterate until the design feels right, then reverse-engineer an OpenAPI spec and switch to generation for the production implementation. Trying to spec-first before you understand the problem space leads to churn and frustration.Breaking changes are the eternal challenge of API evolution. Even with perfect tooling, removing a field or changing a data type breaks clients. Versioning is the answer, but it requires discipline. When introducing breaking changes, bump the API version in your spec (e.g., from 1.0.0 to 2.0.0), generate new endpoints under , and keep  alive until all consumers migrate. Set deprecation timelines and communicate them clearly. The generated code can help here—run both v1 and v2 servers simultaneously in the same Spring Boot app, each with their own generated interfaces, until v1 traffic drops to zero.Managing generated code across repository boundaries requires careful dependency management. If you're generating and publishing client SDKs, version them carefully. Pin SDK versions in consuming projects to avoid surprise breakages. I've seen teams publish SDK patches that silently changed behavior because they forgot to bump the version number. Treat SDKs as public APIs themselves, with semantic versioning and changelogs. Never, ever modify generated source files directly. It's tempting when you're in a hurry—just add that one annotation, tweak that method signature—but you've now created a time bomb. The next developer runs a build, the generator overwrites your change, and hours of debugging ensue. Use .openapi-generator-ignore sparingly and only for files you truly want to manage manually, like README files or example configurations. For code customizations, extend generated classes or use adapter patterns in your source tree.Dependency conflicts between generated code and your application can be subtle. The generated client might pull in a version of Jackson or Spring that conflicts with your main application. Manage this with dependency exclusions in your POM and careful choice of generator libraries. I typically generate clients in separate Maven modules to isolate their dependencies from the main application.Test automation is non-negotiable. Just because code is generated doesn't mean it's correct. Your OpenAPI spec might have a typo, or your business logic might not match the contract. Write integration tests that exercise the full request/response cycle. Run contract tests that validate the server against the spec. Test client SDKs against the running server. The tooling gives you compile-time safety for the interface, but runtime correctness is still your responsibility.8. Conclusions and Final ThoughtsThe contract-first approach—defining your OpenAPI spec before writing code—fundamentally changes how teams build and maintain APIs. By using OpenAPI Generator and Maven, you transform a declarative YAML file into a comprehensive suite of server interfaces, data models, client SDKs, and documentation. This inversion of the traditional workflow has profound effects: integration issues surface at compile time rather than runtime, client and server implementations can't drift apart, and documentation stays current because it's generated from the same source of truth.From real-world experience across multiple production systems, the productivity gains are substantial. Features that once required careful coordination between backend and frontend teams—with inevitable integration delays when reality didn't match assumptions—now proceed in parallel with confidence. The contract provides a fence that both sides can develop against independently, meeting in the middle with generated code ensuring compatibility.The approach scales particularly well as systems grow. Adding a new microservice that needs to call existing APIs? Generate a client SDK and you're working with type-safe methods, not HTTP libraries and JSON parsing. Exposing APIs to external partners? Publish SDKs in their language of choice, all guaranteed to match your server implementation. Supporting mobile apps that update slowly? Keep old API versions alive with separate generated interfaces until usage drops to zero.The benefits compound with API maturity. Initial setup has friction—learning the generator options, deciding on project structure, establishing workflows—but once established, the patterns become second nature. Each new endpoint added to the spec automatically propagates to server code, client SDKs, and documentation. Each field added to a model updates everywhere simultaneously. The maintenance burden of keeping disparate systems synchronized largely disappears.This isn't a magic solution to all API challenges. You still need to design good APIs, with sensible resource models and clear semantics. You still need to manage versions and communicate breaking changes. You still need comprehensive testing. But the contract-first approach with code generation removes an entire category of problems—the tedious, error-prone work of keeping implementations synchronized with contracts. It lets you focus on the hard problems: what the API should do, not whether all the pieces agree on how it's supposed to work.For teams building distributed systems, microservices, or public APIs, the investment in contract-first development with OpenAPI Generator pays off quickly. The upfront effort to learn the tooling and establish patterns is measured in days. The ongoing benefits—fewer integration bugs, faster development, automatic documentation, type-safe clients—accrue over the entire lifetime of the API. In an industry where API maintenance is a major cost driver, this approach provides rare leverage: do the work once in the contract, and propagate it automatically everywhere it's needed.]]></content:encoded></item><item><title>Small Wins, Big Impact: Revenue &amp; Churn Automation Secrets</title><link>https://hackernoon.com/small-wins-big-impact-revenue-and-churn-automation-secrets?source=rss</link><author>Dan Layfield</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:04:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I used to think that driving big impact came from shipping really big projects.  I was wrong.The big projects are certainly what everyone talks about. These are what get added to your resume and mentioned in company all hands decks.If you look at the following projects, you’ll probably reach the same conclusion.Big project - takes 3 months to build and produces 5 “points” of valueSmall project - takes 1 month to build and produces 1 “point” of valueFive is greater than one, so the big project produces more value right?Thinking like this leaves out the most important factor. Time.Which project drives more impact depends heavily on how long its live on the product and producing value.Pushed to its logical extreme, if you ship the small project in January and the big project in December, you can see that the "small project" produced 12 points of value and the "big project" only produced 5.Now there are a lot of caveats here such as, the value of each project, when the big project work starts compared to the small project, etc.However, I think this example illustrates a valuable point.The faster that you get your “wins” out on the board, the faster you grow.The Second Factor: CertaintyAny experienced product person will tell you, its very hard to actually predict what a project will do before its launched.Good due diligence helps, however you can’t truly know until its live. Typically, the bigger the project, the more uncertainty.In our example above, the “Big Project” might actually produce value that is somewhere between a 0 and a 7 when its first launched.Getting it to produce a value of 5 might actually take 3 attempts and many months longer than you thought.Therefore, projects with a higher certainly of working should get shipped even faster.My Favorite Small AutomationsIf I have a “favorite” type of project to launch at subscription companies, its small, fast changes that implements a well know best practice.These are great for a few reasons:Very quick to implement, meaning you start producing value quickly.Very low risk that this won’t work because its so commonly used across the industryLow cost, so even if you’re wrong then you didn’t waste that much time here.These are typically the first five things that I am looking to set up for any company that I work with.For everyone who fails a payment, about 60-70% of the time after someone fails a payment the retry process will be the thing that gets them back.That however still leaves 30-40% of your failed payments that will be won back by reaching out to people to update their payment methods.The keys do doing this well are:Frequency - Notify them multiple times. Ideally heavily right after the fail and then also right before they’ll lose access.Personalization - The less generic this feels, the better. Have it come from a person, talk specifically to the value of the product, etcNo Friction - Make it as easy as possible for them to update their info. Ideally they don’t have to log back in or go back through a checkout experience.The most common tactic here is via email, but text and phone call can also work depending.How much effort should you put in?  Well depends on what that person is worth to you in the the future.AAA (which is a roadside assistance insurance in the US) once left me multiple voicemails by a real person when I forgot to give them my new card information.I think I was paying them $65 a year, which doesn’t sound like much, but they likely retaine users for decades, so they can easily afford to do this if each user is worth hundreds of dollars.My actual voicemail from AAA support2. Retrying Failed PaymentsIf you’re new to this blog, setting up correct payment processing is one of the most important things you can do as a subscription product. I have probably written about this more than anything else.The longer your product is around, the more important this is.Assuming you’re collecting payments off the app stores, then the number one suggestion here is to make sure you’re using an ML driven retry process.Stripe comes pre configured with what I’ll call “dumb” retirees. It will be auto set to retry ever 3, 5 and then 7 days.In my experience, switching to their “scale” tier, or using a different vendor like Churnkey’s "Precision Retry" process, which takes optimizes retires for soft decline and customer reach out for the hard declines.If you’re using a subscription manager, like Recurly or Chargebee, the also have this feature set and you just have to make sure its enabled and set to be aggressive.3. Win Back Tactics in Your Cancellation FlowThis tactic was covered extensively in this post, so I’ll just recap it here.While the best pace to fight cancelation is early in the experience, specifically making sure users set up the product correctly and form a good habit, you can still get some small wins here.The 3 core tactics that I have seen work are:Allowing user to pause their accountOffering users a small discountAllowing users to switch tiersAs mentioned in the post, these are most effective when they are actually helping address the core problem that is causing the user to leave.If they got busy, then giving them a break will help. If they are canceling because they don’t need your product anymore, then pausing wont’ do anything.Check out the full post for a breakdown and examples.4. Turning off Monthly Email ReceiptsIf there is any common link between Spotify, Netflix, Amazon Prime, or any of the other big membership products, its that they don’t send monthly email receipts.They don’t for a reason, each email receipt is a small reminder that you’re still paying for a product.If I were a betting man and you had perfect data tracking, I’d gamble that you are seeing most of your normal cancellations with 48 hours of users receiving an email receipt.5. Re Activation CampaignsThese are campaigns that attempt to pull back customers that were previously active on your product.This really depends on the vertical that you’re in and whether you solve a problem that recurs for users.  Have automated  campaigns that attempt to re engage users after they have dropped away, typically with a small offer.Uber and Lyft push this tactic constantly, this tactic also makes sense for dating services, as you’re likely to have users enter relationships and then become single again.When it comes to picking & sequencing projects, I think most companies make two core mistakes.They undervalue the impact of lowering churnThey choose “build” in the old “buy vs build” dilemma and then are way too slow to build.On churn, let's say that all of the ideas in this post will lower your churn from 15% to 13%.So you’re saving 2% right? 2% isn’t that much right?Its much more than you think.When you lower churn, you have to factor in the future payments of those people who are currently leaving, but won’t in the future.A good rule of thumb is that 1 divided by you average month/month churn number = your average length of retention.1 / 0.15 = 6.6 months of retention1 / 0.13 = 7.7 months of retentionWhen you do this math here (6.6 months vs 7.7 months), you’re actually adding 16% to your LTV.Because time is such a factor in subscription products, you should do this as early in the company's history as you can.This brings me to the second point, buy vs build.90% of companies that I have seen, when faced with the option of using a vendor who does these things decided to built it themselves.This is a fine approach if you actually build it soon. However for most companies, after they decide to build it, they realize its more complex than they thought and decide to do it later.Very quickly this idea can get stuck in the backlog and a full year goes by without doing anything about it.That’s a full year going by that they could have had lower churn and didn't. This adds up to be a lot of money as your company grows.If I wanted to be fully "rational" about this approach, I would get these automations up and running as fast as possible, then adapt them for time.If you're a super talented founder who is also an engineer, then probably just build all of these.If not, find a vendor or set of vendors to get installed quickly, then determine in time if you want to replace them with internal features in time.]]></content:encoded></item><item><title>‘The Perfect Season’ Trademark: IU Would Have To License The Phrase From The Patriots</title><link>https://www.techdirt.com/2026/01/22/the-perfect-season-trademark-iu-would-have-to-license-the-phrase-from-the-patriots/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:03:46 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[It’s been nearly a decade since Mike wrote about the strange trademark approval the New England Patriots received on the term “Perfect Season.” The story rang as odd for several reasons, not the least of which being that the team applied for the mark in 2008 just before that year’s Super Bowl and at a time in which the Patriots had completed an 18-0 record including the regular season and two playoff games. The Patriots then famously fell on their collective face against the Giants, making the season very much  perfect. But the team went ahead and got the mark anyway. Add to that my take that this kind of phrase for the categories of sports apparel and the like is not nearly unique enough to serve as a valid mark to begin with and we have a situation practically begging to get absurd.And the absurdity may have arrived. For you non-sports fans in the audience, the Indiana University Hoosiers just won the College Football National Championship without losing a single game during the season. IU, in stark contrast to the Patriots, in fact have a perfect season. But if they, or anyone else, would like to celebrate that fact, they would need to license the use of the term from the Patriots to do so.Not only did The Kraft Group continue, they licensed the mark to the Massachusetts Interscholastic Athletic Association for use in a DVD in order to satisfy the “use in commerce” stipulation by the U.S. Patent & Trademark Office and, in 2016, the team was granted rights to the trademarks.Unlike Pat Riley, who filed to trademark “Three Peat” when he was the coach of the Lakers and then cashed in when the Chicago Bulls and New York Yankees won three straight titles, The Kraft Group’s trademark hasn’t had many opportunities to gain royalties from the mark.But Kraft  demonstrate it has used the term in commerce via its previous licensing agreements. That puts IU and/or apparel companies in the position to go one of three routes. They can license the term for apparel from Kraft, thereby perpetuating this entire silly enterprise, they can license the phrase and potentially fight a court battle to invalidate the trademark after they make the products and are sued or threatened by Kraft, or they can just not use the phrase at all.The latter appears to be the most likely route.Items in Indiana’s team store use the word “Perfection.” Homefield Apparel, whose original school is Indiana, uses “Perfect” without “Season” on its shirts.Even that is absurd. Again, the end result, thanks to a USPTO all too happy to approve trademarks that it probably shouldn’t, is that a team that achieved a perfect season cannot freely use the phrase that correctly describes its own accomplishment because it would have to license it from a team that didn’t. ]]></content:encoded></item><item><title>Low Code and No Code in 2026: The Way I Pick a Platform Without Regret</title><link>https://hackernoon.com/low-code-and-no-code-in-2026-the-way-i-pick-a-platform-without-regret?source=rss</link><author>David Park</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:02:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In 2026, low code and no code are not side tools anymore. They are becoming a default path to ship internal workflows (Gartner’s low code forecast), portals, dashboards, and even customer experiences, because the pressure to deliver faster never went away. I see many teams adopt a platform quickly, build a few apps, and then run into the same problems: messy ownership, risky data access, inconsistent quality, and a painful scaling story. This guide is how I avoid that.How I define low code and no code in real lifeWhen I say low code, I mean visual development that still allows coding when the product needs deeper logic, stricter security patterns, or advanced integrations. When I say no code, I mean building with templates and point and click configuration so that business teams can ship without writing code. Both approaches can work. The difference is not the canvas or the drag and drop editor. The difference is governance, integration, and how you manage the application lifecycle after launch.The way these platforms work is simple on the surface. You define a business outcome, assemble screens and workflows using components, connect data sources through connectors or APIs, test, deploy, and then iterate based on real usage. The reality is that the hardest part comes when you connect to core systems, enforce identity and permissions, and maintain changes over time. That is where good implementations look like product engineering, even if the build experience feels lightweight.What trends actually matter in 2026What feels different in 2026 is that governance is becoming the real product. As citizen development spreads, platforms win when they make environments, permissions, data loss prevention, connector control, and auditing easy by default. If governance is a document nobody reads, the platform will eventually turn into a maze. If governance is built into the everyday workflow, teams can move fast without increasing risk.Another shift is AI assisted building. I am not talking about AI replacing developers. I am talking about AI helping generate screens, suggest workflow logic, draft formulas, propose test cases, and improve documentation. This speeds up the first version, but it also raises a new standard. If it is easy to create apps, it becomes even more important to control what gets promoted to production and who is responsible for it.I also see the winning teams treat low code and no code as more than app screens. They treat it as a system that combines automation, approvals, notifications, dashboards, and a reliable data layer. If you only build interfaces, you miss the compounding benefit. The biggest value often comes from removing operational friction, not from building prettier forms.The failures I see most often are predictable, and I actively plan around them. The first trap is deciding to govern later. That usually produces duplicate apps, unclear permission models, unknown data flows, and shadow IT that quietly becomes business critical. The second trap is vendor lock in that nobody priced in. Lock in is not automatically bad, but I want to know how data exports work, what portability looks like for workflows and logic, and whether core business rules can be isolated. The third trap is underestimating integration. Single sign on, audit logs, regulated data, and performance constraints can quickly turn a quick build into a real engineering project. The fourth trap is security assumptions. I always ask where data is stored, how secrets are managed, how least privilege is enforced, and what the logging story looks like.Here is the selection framework I use, and it keeps me honest. First, I categorize the app, because category determines almost everything. I usually place the app into one of these types.• Workflow automation such as approvals and operations flows where governance and auditing matter most \n  • Internal tool such as admin panels and dashboards where speed and permissions are key \n  • Customer experience such as portals where security, scalability, and reliability matter most \n  • Prototype where time to first version matters most and the scope should stay intentionally limitedNext, I score platforms across a small set of criteria that reflect real life ownership. I use these criteria because they tend to predict pain later.• Governance and administration \n  • Integration and APIs \n  • Security and compliance readiness \n  • Extensibility when custom code becomes necessary \n  • Delivery speed for the first usable release \n  • Total cost including licensing, build effort, and run cost over timeThen I run what I call a proof of capability. This is not a demo, and it is not a shallow proof of concept. It is a controlled build that includes identity, at least one hard connector, role based access, audit logs, a basic release flow from test to production, and minimum monitoring. If the platform cannot pass this early, it will be much worse later when the app becomes important.People often ask me for a top platform list, but I find buckets more useful than ranking. Enterprise grade options are usually evaluated when you need scale, compliance, and complex integrations. Startup friendly no code options are typically chosen when speed and iteration matter most. Internal tool builders are popular when teams want fast CRUD and dashboards with reliable connectors. The best platform depends less on feature checklists and more on who will build it, what it must connect to, and how production will be governed.When I use low code and no code and when I avoid itI use low code and no code when I need value within weeks, when the workflow is measurable, when the data sources can be standardized, and when governance is defined upfront. I avoid it when the product requires extreme customization, ultra low latency constraints, or when the licensing model becomes unacceptable at scale. I also avoid it when the platform has weak export or migration paths and I know portability will matter later.A safe rollout sequence for teamsIf a team is rolling this out across departments, I follow a safe rollout sequence. I begin by defining guardrails for environments, roles, connector policies, and data rules. I create templates and patterns that teams can reuse so the portfolio stays consistent. I train builders on security basics and ownership expectations. I start with a small number of high impact workflows, measure adoption and cycle time saved, and only then scale with an app catalog that makes ownership and dependencies visible.My 2026 mindset is simple. Low code and no code succeed when speed is paired with governance. They fail when speed replaces governance. If you treat low code and no code as a delivery strategy, not a tool, you can move fast and stay safe at the same time.]]></content:encoded></item><item><title>The Remote Developer&apos;s Survival Guide: 10 Technical Strategies to Prevent Burnout</title><link>https://hackernoon.com/the-remote-developers-survival-guide-10-technical-strategies-to-prevent-burnout?source=rss</link><author>Ridwan Sassman</author><category>tech</category><pubDate>Fri, 23 Jan 2026 04:01:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Software development is a very stressful job. There is so much to learn and so little time. Anxiety and fear of missing out make you feel like no matter how hard you work to stay ahead, it will never manage to catch up. Remote work has transformed the landscape of software development, offering unprecedented flexibility and autonomy. Yet this very freedom can become a double-edged sword, leading to isolation, blurred boundaries, and ultimately, burnout. With statistics showing that up to 80% of programmers experience burnout and that remote workers report higher rates of mental health challenges, the need for targeted strategies has never been greater . This guide provides 10 technically-grounded, actionable strategies to help you not only survive but thrive in a remote development environment.1. Master Context Switching with Structured Work BlocksThe remote developer's day is filled with potential interruptions: Slack notifications, unscheduled calls, and the constant pull of household responsibilities. This constant context switching devastates deep work, the state of flow where complex software problems are solved.Technical Implementation:Time-Box Your Communication Tools: Schedule specific windows for checking emails and Slack (e.g., 30 minutes at 11 AM and 4 PM). Use Do Not Disturb modes aggressively. One expert suggests: "If you have desktop notifications, turn 'em off. Put your phone on silent, AND TURN IT UPSIDE DOWN." Implement the Pomodoro Technique for Development: Use 90-minute focused coding sessions followed by 15-minute breaks. This aligns with natural ultradian rhythms and prevents cognitive exhaustion. Tools like  or simple terminal timers () can integrate directly into your development workflow.Block Calendar for Deep Work: Literally block 3-4 hour segments in your calendar labeled "Architecture Design" or "Complex Refactoring." Treat these blocks as immutable meetings with your most important client: your codebase.2. Architect Your Physical and Digital Workspace for FocusYour environment directly impacts your cognitive performance. A 2022 study on WFH developer productivity found that environmental factors significantly influence output metrics like commit frequency and code review quality .Technical Setup Recommendations:Table: Essential Workspace Components|  |  |  |
|----|----|----|
|  | Reduces eye strain, enables parallel debugging | Dual monitors (1 vertical for code, 1 horizontal for browser/terminal) |
|  | Prevents RSI, maintains velocity | Ergonomic keyboard (split design), precision mouse/trackball |
|  | Masks disruptive noise, aids concentration | Noise-cancelling headphones with white/pink noise capability |
|  | Reduces eye fatigue, maintains circadian rhythm | Natural light + bias lighting behind monitors (6500K) |Digital Workspace Hygiene:Maintain separate user profiles on your machine for work versus personal use.Use virtual desktops (Windows Virtual Desktops, macOS Spaces) to segregate work applications from personal ones.Implement a clean, minimalist editor/IDE theme that reduces visual clutter but maintains syntax highlighting clarity.3. Establish Technical and Temporal Boundaries with PrecisionThe "always-on" culture is particularly pernicious in remote work. Developers report feeling pressure to respond instantly to messages, leading to fragmented attention and extended work hours.Actionable Boundary Strategies:: Set a hard rule against committing code after a specific hour (e.g., 6 PM). This creates a natural stopping point. One developer's recovery from burnout began when he promised: "No late commits at night and no coding on the weekend—at least not for your company." Communication SLA Declaration: In your team Slack/Teams description, note your typical response times (e.g., "Non-urgent messages: within 4 hours"). This manages expectations proactively.: Create scripts that shut down work environments. A simple  could: (1) commit all working changes to a WIP branch, (2) close all work-related applications and browser tabs, (3) mute work communication channels.Relying solely on synchronous communication (video calls) for technical discussions creates bottlenecks and interrupts deep work. The key is creating asynchronous-first documentation practices.Technical Communication Systems:RFCs (Request for Comments) for Major Changes: Before significant architectural changes, write a lightweight RFC in Markdown in a shared repository. This allows distributed, thoughtful feedback.: For complex PRs, supplement written comments with a short Loom video walking through your thought process. This adds nuance often lost in text.Structured Decision Records: Maintain an  (Architectural Decision Record) directory in your codebase. Each decision gets a Markdown file explaining context, options considered, and the final decision with rationale.5. Quantify and Optimize Your Energy Cycles with Developer-Centric MetricsNot all hours are created equal for cognitive work. Understanding your personal  allows you to align demanding technical tasks with peak mental capacity.: Use a simple spreadsheet or tool like Clockify to record:Task type (debugging, architecture, code review, meetings)Self-rated focus level (1-5)Output quality (lines of quality code, bugs resolved): Most developers find their peak focus occurs in  (10 AM - 12 PM). Schedule your most demanding algorithmic or architectural work then.Create a Personalized Task Schedule:: New feature development, system design, complex refactoringMedium Energy (2-3 hours): Code reviews, writing tests, debugging: Documentation, administrative tasks, emailAs one People Ops specialist notes: "Planning high-energy tasks for when I feel most focused and productive… and saving low-energy tasks for the afternoon ensures that I maintain balance." 6. Implement Rigorous Workload Management to Prevent Scope CreepUnrealistic expectations and poorly defined requirements are primary burnout catalysts for remote developers, who often absorb the stress of scope creep without the buffer of in-office social support .Technical Defense Tactics:Break Down Tasks with Time Estimates: Never accept a task like "Add user authentication." Break it into: "Set up Auth0 integration (1 day)," "Create login UI components (2 days)," "Implement session management (1 day)," etc.Use the "Yesterday's Weather" Estimation Method: When asked for estimates, reference similar past tasks from your time tracking data. This grounds estimates in reality, not optimism.Push for Clear "Definition of Done": For each ticket, ensure clear, binary completion criteria exist before starting work (e.g., "Tests pass with 90% coverage," "Code reviewed by 2 team members," "Deployed to staging").Loneliness is a silent productivity killer. Remote developers miss the serendipitous hallway conversations and spontaneous whiteboarding sessions that often spark innovation and provide emotional support .Proactive Connection Strategies:Create Virtual Pair Programming Sessions: Schedule regular (even if brief) pair programming sessions using tools like VS Code Live Share or Tuple. Focus on particularly gnarly bugs or interesting challenges.Participate in Code Review "Office Hours": Instead of asynchronous-only code reviews, set weekly 30-minute blocks where reviewers are available for live discussion of PRs.Join or Form a "Mastermind" Group: Connect with 3-4 developers from other companies at similar levels. Meet bi-weekly to discuss technical challenges and career growth. As one developer emphasizes, joining a community provides "support, inspiration, and new ideas that you may not have considered before." 8. Optimize Your Development Environment to Reduce Cognitive LoadEvery friction point in your development workflow—slow builds, cluttered IDE, inefficient tooling—drains mental energy that could be directed toward creative problem-solving.Technical Optimization Checklist:Streamline Your Local Development: Can you get a new developer from  to running tests in under 10 minutes? Use Docker Compose or dev containers to standardize environments.Automate Repetitive Tasks: Write scripts for common workflows (database resets, test runs, deployment checks). Even small time savings compound.: Regularly audit your extensions. Remove unused ones that slow startup time. Learn keyboard shortcuts for frequent actions to maintain flow state.Implement a Fast Feedback Loop: Integrate pre-commit hooks (using pre-commit framework) for formatting, linting, and running quick tests. Immediate feedback prevents context switching.9. Prioritize Physical Infrastructure as Much as Code InfrastructureThe mind-body connection is real. As one developer recovering from burnout discovered: "Physical exercise has been proven to work as well as medication for treating depression and mental health issues." Developer-Specific Health Practices:The 20-20-20 Rule for Eyes: Every 20 minutes, look at something 20 feet away for 20 seconds. Use a simple timer or app reminder.Micro-Movements Between Compiles/Tests: Use natural breaks in your workflow (compiling, tests running) to stand, stretch, or do a few squats.: Your chair, desk, and monitor setup are as important as your computer specs. Consider a standing desk converter or under-desk treadmill for variety.Scheduled "Non-Screen" Breaks: Build breaks into your calendar that involve zero screens—a walk outside, stretching, or brief meditation. These truly reset cognitive capacity.10. Create Deliberate Shutdown Rituals to Separate Work from LifeWithout the physical commute to psychologically transition from work to home, remote developers struggle to , leading to work thoughts intruding on personal time and preventing true recovery .Technical Shutdown Ritual:: Spend 10 minutes reviewing what you accomplished and writing 3 bullet points for tomorrow's priorities. This prevents work thoughts from swirling overnight.: Never leave work with unsaved changes mid-thought. Either commit to a feature branch or use Git stash with a descriptive message.: Literally close all work applications—IDE, Slack, email, browser tabs. Visual closure signals mental closure.: Change your physical location or activity. A short walk, changing clothes, or a 5-minute meditation can create psychological separation.: If possible, avoid screens entirely for the first hour after work. This allows your brain to downshift from high-stimulus input.Conclusion: Sustainable Remote Development is a System to Be EngineeredPreventing burnout as a remote software developer isn't about working harder or finding more willpower—it's about engineering sustainable systems for your work and well-being. The technical mind that excels at optimizing code and architecting systems must be turned inward to design a work life that can endure.Start with just one or two of these strategies. Perhaps implement structured work blocks this week, and next week focus on optimizing your shutdown ritual. The cumulative effect of these small, consistent improvements compounds dramatically over time.Remember the sobering statistic: Burnout-experienced developers find that factors related to mental health significantly impact their coding productivity . Investing in your sustainable work practices isn't contrary to productivity—it's foundational to it. Your most valuable asset isn't your knowledge of the latest framework; it's your capacity for focused, creative problem-solving. Protect that capacity with the same rigor you apply to your codebase.What single change will you implement tomorrow to build a more sustainable remote development practice?]]></content:encoded></item><item><title>How to Enter the Proof of Usefulness (PoU) Hackathon</title><link>https://hackernoon.com/how-to-enter-the-proof-of-usefulness-pou-hackathon?source=rss</link><author>Proof of Usefulness</author><category>tech</category><pubDate>Fri, 23 Jan 2026 03:56:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The  is HackerNoon’s way of spotlighting projects that actually  - together we celebrate software that solves real problems instead of chasing hype. Whether it’s a solo side project or a bigger build, this hackathon rewards projects that are genuine and rooted in practical problem-solving.B-But how do we enter? How do I get my project enrolled to the prize consideration queue?\
We heard ya - and today’s guide will walk you through in detail, exactly how to submit your project, and compete for the $150K+ prize pool of the Proof of Usefulness Hackathon 💚Step 1: Submit & Generate Your PoU Report:::tip
This is your official registration. Without this report, your project will not be scored.Easy right? Focus now, because next up is the tricky part!\
This step is all about your attention to detail - imagine filling out a CV, but this time it’s for your project! Proof of Usefulness rate submissions on real-world signals: user adoption, revenue, technical stability, and genuine problem-solving; therefore, it is essential to show the numbers behind your build.\
Before you submit, pull together simple traction proof like ==active users, API calls, paid customers, or GitHub stars/forks if you’re open source, and the specific integrated tech stack (Bright Data, Neo4j, Storyblok, Algolia, or your custom stack)==. This ensures your product’s usefulness is easy to verify and not just implied. ​\
Wanna  get the best score possible? Spare no details! Bring ==charts, numbers, testimonials, press releases== - all you can think of to make your PoU submission bulletproof! Showcase your project’s growth, whether it is revenue or user adoption; all milestones count, and everything will be kept tab towards your final score 🌟\
Before getting your Usefulness score and report, fill out your contact information to officiate your submission. This step is required to ensure each project is being submitted by a real person and is an existing product/company.:::info
 After typing your email, hit  - an email will be sent to your provided address, put your 6-digit code down and you are good to go 🙌\
All done? Great! Hit  and wait. The system will analyze your inputs (including traction metrics like users or stars) and generate a Proof of Usefulness Score (-100 - 1000+) along with a downloadable report card.​\
Excited about your score? Download and share your PoU Report Card everywhere 💚 (remember to tag HackerNoon, and our sponsors BrightData, Storyblok, Neo4j, and Algolia)!Remember - Qualification Check: To qualify, your project must be a live, functional product (no mockups).​Step 2: Document Your Project on HackerNoon:::tip
Your article serves as your technical documentation and pitch to the judges.\
So you’ve got your Proof of Usefulness score - perfect! But in order to finalize your participation in the Proof of Usefulness Hackathon and compete for the $150K+ prize pool, you’ll need official documentation of your project. Luckily, HackerNoon’s got you 🫵1. Tell Your Product Story: Move Your PoU Report Into a HackerNoon Blogpost Draft.\
On your report page, click on the button to quickly move your project submission to a HackerNoon Blog. When doing this, a HackerNoon profile will be automatically created for you, and you’ll be redirected to a HackerNoon draft page.\
What now? - Set up your profile and write on! If you’re familiar with the HackerNoon writing flow - great! If not, check out these profiles below to quickly get yourself familiarized.:::tip
Still confused? Drop us an email at yes-reply@hackernoon.com for further support 💚\
Since the blogpost draft is a writing prompt auto-generated by HackerNoon based on your Proof of Usefulness submission, you’ll need to double-check all information to ensure clarity and accuracy.\
Got it? - Great! Now, complete your draft by answering the rest of the questions in the draft. Here are some bullet points of essential things you should include in your blog. Treat it like a suggested checklist; the rest is all up to your creativity! Share all you want about your project - The Problem & Solution: How can your product provide a real-world solution?Implementation: Technical details and code snippets showing how you used the sponsor technology (e.g., "How we used Neo4j to map user relationships").The "Proof": You can embed or screenshot your PoU Report from above.2. Submit for PublicationSatisfied with your record of PoU? Hit the submit button.\
\
Your blog will then be sent to the queue, and the HackerNoon team will take it from there. Once your article is approved and published, congratulations, your Proof of Usefulness Hackathon submission has been successful and is live!:::tip
The hackathon runs from Jan 5 to June 5, allowing for continuous improvement.Monthly Judging: Entries are reviewed on a rolling basis for the $20,000 in monthly cash prizes.​Iterate & Update: Since utility is the core metric, you can update your product, improve your PoU score, and increase your chances for the Grand Prize in June.Grand Prize Qualification: Top-scoring projects are shortlisted for the final $150K+ prize pool distribution.For more information, check out these articles below 👇 Bright Data is the leading web data infrastructure company, empowering over 20,000 organizations with ethical, scalable access to real-time public web information. From startups to industry leaders, we deliver the datasets that fuel AI innovation and real-world impact. Ready to unlock the web? \
 GraphRAG combines retrieval-augmented generation with graph-native context, allowing LLMs to reason over structured relationships instead of just documents. With Neo4j, you can build GraphRAG pipelines that connect your data and surface clearer insights. \
 Storyblok is a headless CMS built for developers who want clean architecture and full control. Structure your content once, connect it anywhere, and keep your front end truly independent. API-first. AI-ready. Framework-agnostic. Future-proof. \
 Algolia provides a managed retrieval layer that lets developers quickly build web search and intelligent AI agents. ]]></content:encoded></item><item><title>The Midnight Lesson Planner: How to Reclaim Your Evenings and Ignite Your Classroom</title><link>https://hackernoon.com/the-midnight-lesson-planner-how-to-reclaim-your-evenings-and-ignite-your-classroom?source=rss</link><author>Hui</author><category>tech</category><pubDate>Fri, 23 Jan 2026 03:51:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[You didn't become an educator to spend your Sunday nights fighting with bullet points and formatting margins. You did it for that specific moment—the "click"—when a student’s eyes light up because a complex concept finally makes sense.But somewhere between administrative burdens and the pressure to publish, that joy often gets buried under mountains of administrative friction. We spend 80% of our energy structuring content and only 20% delivering it with passion.This imbalance is the silent killer of teaching quality. When you are exhausted from preparation, your delivery suffers. Your "performance" lacks energy. The classroom becomes a place of transaction rather than transformation.It’s time to flip the ratio. We don't need another tool to write slides; we need a .I have developed a system prompt that doesn't just summarize textbooks—it designs learning experiences. It applies proven educational theories (like Bloom's Taxonomy and Active Learning) to turn raw topics into structured, engaging, and assessment-aligned lecture notes.The "Curriculum Catalyst" System PromptThis isn't about letting AI "do your job." It's about letting AI handle the heavy lifting of structural organization so you can focus on the art of teaching.This prompt forces Large Language Models (LLMs) like ChatGPT, Claude, or Gemini to think like a master curriculum designer. It demands  (building knowledge step-by-step),  (specific moments for interaction), and  (accessibility for all learners).Copy the instruction below to transform any topic into a pedagogical masterpiece.# Role Definition
You are a **Master Curriculum Designer and Educational Content Specialist** with over 15 years of experience in higher education and professional training. Your expertise spans:

- **Pedagogical Excellence**: Deep understanding of learning theories (Bloom's Taxonomy, Constructivism, Active Learning)
- **Content Architecture**: Structuring complex information for optimal comprehension and retention
- **Engagement Strategies**: Creating materials that captivate learners and promote active participation
- **Universal Design for Learning (UDL)**: Ensuring accessibility and inclusivity in educational materials

# Task Description
Create comprehensive, well-structured lecture notes that:
1. Transform subject matter into digestible, logically sequenced content
2. Facilitate both instructor delivery and student self-study
3. Include engagement elements to enhance learning outcomes
4. Provide clear learning objectives aligned with assessments

Please develop lecture notes for the following topic:

**Input Information**:
- **Subject/Topic**: [Specify the main topic or subject]
- **Course Level**: [Undergraduate/Graduate/Professional/K-12]
- **Lecture Duration**: [e.g., 50 minutes, 90 minutes, 3 hours]
- **Class Size**: [Small <30 / Medium 30-100 / Large 100+]
- **Prior Knowledge Assumed**: [What students should already know]
- **Learning Objectives**: [What students should be able to do after this lecture]
- **Additional Context**: [Any special considerations, equipment, or constraints]

# Output Requirements

## 1. Content Structure
- **Opening Hook** (5%): Attention-grabbing introduction connecting to student interests
- **Learning Objectives** (5%): Clear, measurable outcomes using action verbs
- **Core Content** (70%): Main body organized into logical sections with:
  - Key concepts with definitions
  - Examples and illustrations
  - Discussion prompts
  - Transition statements
- **Summary & Synthesis** (10%): Recap of main points with connections
- **Assessment Preview** (5%): How this content relates to evaluation
- **Additional Resources** (5%): Further reading, videos, practice materials

## 2. Quality Standards
- **Clarity**: Complex concepts broken into manageable chunks
- **Engagement**: Interactive elements every 10-15 minutes
- **Accessibility**: Multiple representations (text, visuals, examples)
- **Alignment**: Content matches stated learning objectives
- **Practicality**: Real-world applications and relevance

## 3. Format Requirements
- Use hierarchical headings (H1, H2, H3) for structure
- Include visual placeholders [📊 INSERT DIAGRAM: description]
- Add speaker notes in italics for instructor guidance
- Provide time estimates for each section
- Use bullet points for key takeaways
- Include "💬 Discussion Prompt" boxes
- Add "⚡ Quick Check" comprehension questions

## 4. Style Constraints
- **Language Style**: Academic yet accessible; avoid jargon without explanation
- **Expression**: Second person for engagement ("Consider how you might...")
- **Professional Level**: Appropriate to course level specified
- **Tone**: Encouraging, intellectually stimulating, inclusive

# Quality Check Checklist

After completing the output, please verify:
- [ ] Learning objectives are SMART (Specific, Measurable, Achievable, Relevant, Time-bound)
- [ ] Content directly supports each learning objective
- [ ] Engagement activities are included every 10-15 minutes
- [ ] Examples represent diverse perspectives and contexts
- [ ] Visual aids are suggested where beneficial
- [ ] Time allocations are realistic and total correctly
- [ ] Assessment connections are explicit
- [ ] Accessibility considerations are addressed

# Important Notes
- Avoid assumptions about student background; explain foundational concepts briefly
- Include alternatives for activities (in-person, online, hybrid)
- Provide differentiation suggestions for varied learning needs
- Mark optional/advanced content clearly
- Ensure cultural sensitivity in examples and case studies

# Output Format
Deliver as a comprehensive Markdown document with:
- Clear section separators
- Instructor notes in italics
- Student handout sections marked with 📝
- Interactive elements highlighted with distinct icons
The Anatomy of a Perfect LectureWhy does this prompt produce better results than a generic request like "Write a lecture about X"?1. The "Scaffolding" EffectCognitive load theory tells us that students get overwhelmed when too much new information hits them at once. This prompt’s  requirement forces the AI to break down complex topics into "digestible chunks." It creates a logical flow—Introduction → Concept → Example → Checkpoint—that mimics how the brain actually learns.2. The Rhythm of ResonanceA lecture is a performance. If it's just a monologue, you lose the audience in 10 minutes. By mandating "Interactive elements every 10-15 minutes," the prompt builds a natural rhythm of tension and release. It inserts "Discussion Prompts" and "Quick Checks" exactly where attention spans usually drift, keeping the classroom energy alive.One of the biggest frustrations for students is the disconnect between "what was taught" and "what is on the test." The  section ensures alignment. It acts as a contract of trust: "Here is what we are learning, and here is exactly how you will show your mastery."Imagine finishing your prep in 15 minutes instead of 3 hours. Imagine walking into the lecture hall with a script that is structured, engaging, and robust, leaving you free to improvise, connect, and inspire.The tools we use shape the way we work. If you use a typewriter, you think in lines. If you use a "Curriculum Catalyst," you think in learning arcs.Stop being a content formatting machine. Use the prompt. Let the AI handle the structure, so you can get back to doing what you actually love: teaching.]]></content:encoded></item><item><title>Why I started a Bitcoin Education Website</title><link>https://hackernoon.com/why-i-started-a-bitcoin-education-website?source=rss</link><author>M-Marvin Ken</author><category>tech</category><pubDate>Fri, 23 Jan 2026 03:50:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[It isn't everyday that a company gets started based on one YouTube short.But I think that's exactly what might happen to me after I watched this Alex Hormozi YouTube video and a while later, the idea clicked on September 1 2025.“The person asking the questions is the one closing (the deal)”The thing was, I knew a little technical knowledge about Bitcoin. And I knew that there was a lot I did not know.But I knew something and all I needed was a lever I could actually rely on. A wheel to help me move my learning faster while getting towards closing some deals.Then I chanced upon Hormozi who said if I ask the questions (whose answers I already know, obviously. Sales people already know the product they're driving towards), then I get to close a deal with the person I'm asking.This was mindblowing to me.Like,  like on ChatGPT, do not say you have answers to people's problems like StackExchange or StackOverflow or Reddit or …SIMPLY (and not any simpler) ;ask people great Bitcoin questions that you already know the answer to!Teach them, that is, via an inquiry-based model, as you try to close deals with them.Yes. I might be certified. But I think I’m onto something revolutionary here. Especially for the Bitcoin space.So I hurried and started https://bitcoinhighschool.com. Since the launch of the site, I have asked more than 700 free Bitcoin questions within a high school difficulty level. And January isn't over yet.I’ve gotten one dedicated channel page for the site so far @BTCHighSchool on X.And in the past 3 weeks, I've netted 4000 site visits and 21 subscribers.Once I add micropayments in Bitcoin Lightning, I expect to see a jump in my reach.\
Many optimistic billionaires today, especially Elon Musk, thinks they have it figured out.AI will make us all rich in no time.I think they are a little more deluded than I am with my idea.I live in Uganda and today, I saw somebody in my neighborhood also trying to arbitrage on fuel sells with his single pump gas station.All so they can squeeze another penny from the fuel station business, but this is not putting that much income in their pockets. It puts enough to feed them, but the competition is so tight.Now if fuel is sold everyday and people are not making record profits, how is AI that is hardly making profits even for a big wig like Sama, going to make millions for this local pump attendant?Let's first connect the two dots.Let's assume AI will unlock our desire to spend fuel like “economic stimulation packages” of all sizes have failed to do.Fuel is not the problem, as Venezuela has shown us. The problem is people don’t yet want to buy so much that they see Venezuela as good a source as any.Enough to make oil and by extension fuel, a premium product.Artificially raising the price will not work. Demand must arise organically. From AI or Bitcoin or something else.Hopefully, AI will do all this change very soon.In a nutshell, AI will become a bigger customer for my local fuel (gas to you Americans. Why you call a liquid “gas” is still beyond me. But I'm sold) station attendant.So that, by extension, their margins are bigger. Maybe big enough for them to never need to go to work again Iike Elon preaches.Right? I mean if anything can be automated as a business and monitored 100% via CCTV cameras, it is a gas station serving robotic cars that pay 10x more than stingy human car drivers. ‘Coz these robots work better 24/7 with 100x productivity.BHS can teach anybody. Because Bitcoin is for everyone.This is a stretch, but I think my potential business model at BitcoinHighSchool.com will serve better the gas station attendant than AI prompts alone.I’m sticking to the gas station attendant because if anybody can directly fuel a localized (but mobile) AI neural-net in need of power, it is these guys.No need for staff, 100% margin.Today, when these people go to any AI of their choosing for help, they will ask “How do I make 10x more profits from my Bitcoin sales at Gas Station Business?”Problem is, no AI ever ran a local single pump gas station in the heart of Africa. So it will have generic advice that won't move their financial needle.They may pivot and go to Reddit. “How to run a local single-pump gas station that fuels Teslas and other robots for maximum profits in all currencies”.Other people will give a lot of meaningful advice. But it too is generic because they are doing this from the comfort of their homes. Netflix on the side, arm-chair advising without having to brace wind, heat, regulations, etc.What this guy needs is another local gas station attendant who will set for them a track to follow.A 1000 question adventure on “how to become a great gas station attendant serving human cars and robots alike”.\
Of course, this is a grind. But now you have an ally who has proof-of-usefulness. Probably another local who has been killing it with his robots-refuelling / car-refuelling gas station business.I mean, if you can go through 1000 hops just so as to get to learn and make it as someone coaches you, you deserve that win and you're more likely to get it than anybody simply prompting and hoping AI will do the dirty hard work.There is a big scarcity of mentoring questionsYou want to get somewhere, run a track with questions that help you level up. Use AI, use whatever. Just do some work. Proof your Understanding.Show the person asking (me, for now, but I'll soon open up the site to contributors) that you can do more than read sentences.If Elon ever approached you, and you wanted to get rich, the best thing you would say is “Teach me how to get rich”.Give me a track to run and monitor my progress Mr. Business Magnet.Hope you're okay being asked very hard questions during Elon’s hardcore modes.Not “How do I make a billion dollars?”.“Be Useful” and he is off.One question doesn't cut it.1000+ questions driving towards real world utility. Honing your skills. That's what does it. A rigorous competence-based learning track.A track set by somebody who knows something.We're not there yet, but you will soon see higher quality exams at BitcoinHighSchool.com. Exams with questions that, very soon, will make you worthy of a job in the Bitcoin Ecosystem.Just log in today and take a BITE.And it sits right at home within the post AI future.Use AI, use your brain, collab with a friend, as long as you run the course.And give feedback on X @BTCHighSchool.My target focus is a high school audience because high schoolers drive the biggest businesses in the Edutech space. A lot of High Schools here in Uganda have bigger tuition fees than Colleges.However, the site will be cool enough so that adults are welcome too.Because this is about the greatest money in the world and adults do money all day long.It'll be like a video game that parents are going to be willing to be better than their kids at. With the kids not letting go of the reigns, and coming up with some smart round about ways from the traditional thinking in the space.a few extra things to share …Jay Z is friends with Jack Dorsey, and he once said in a track“a closed mouth don't get fed”.So I made Bitcoin Technical Exam (BiTE) part of my mascot.You have to take a bite out of a lot of Bitcoin questions or you ain't gonna get fed.Not yet, we're building first.3. Will it include some tokenomics at some later date?Only on the Bitcoin layer, and when we get our code done good enough, we'll tokenize the hell out of our exams.On the mainstream tech side, we hope to make some exams into clickable ads.]]></content:encoded></item><item><title>South Korea Launches Landmark Laws To Regulate AI</title><link>https://yro.slashdot.org/story/26/01/22/2339207/south-korea-launches-landmark-laws-to-regulate-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the Korea Herald: South Korea will begin enforcing its Artificial Intelligence Act on Thursday, becoming the first country to formally establish safety requirements for high-performance, or so-called frontier, AI systems -- a move that sets the country apart in the global regulatory landscape. According to the Ministry of Science and ICT, the new law is designed primarily to foster growth in the domestic AI sector, while also introducing baseline safeguards to address potential risks posed by increasingly powerful AI technologies. Officials described the inclusion of legal safety obligations for frontier AI as a world-first legislative step.
 
The act lays the groundwork for a national-level AI policy framework. It establishes a central decision-making body -- the Presidential Council on National Artificial Intelligence Strategy -- and creates a legal foundation for an AI Safety Institute that will oversee safety and trust-related assessments. The law also outlines wide-ranging support measures, including research and development, data infrastructure, talent training, startup assistance, and help with overseas expansion.
 
To reduce the initial burden on businesses, the government plans to implement a grace period of at least one year. During this time, it will not carry out fact-finding investigations or impose administrative sanctions. Instead, the focus will be on consultations and education. A dedicated AI Act support desk will help companies determine whether their systems fall within the law's scope and how to respond accordingly. Officials noted that the grace period may be extended depending on how international standards and market conditions evolve. The law applies to three areas only: high-impact AI, safety obligations for high-performance AI and transparency requirements for generative AI.
 
Enforcement under the Korean law is intentionally light. It does not impose criminal penalties. Instead, it prioritizes corrective orders for noncompliance, with fines -- capped at 30 million won ($20,300) -- issued only if those orders are ignored. This, the government says, reflects a compliance-oriented approach rather than a punitive one. Transparency obligations for generative AI largely align with those in the EU, but Korea applies them more narrowly. Content that could be mistaken for real, such as deepfake images, video or audio, must clearly disclose its AI-generated origin. For other types of AI-generated content, invisible labeling via metadata is allowed. Personal or noncommercial use of generative AI is excluded from regulation. "This is not about boasting that we are the first in the world," said Kim Kyeong-man, deputy minister of the office of artificial intelligence policy at the ICT ministry. "We're approaching this from the most basic level of global consensus."
 
Korea's approach differs from the EU by defining "high-performance AI" using technical thresholds like cumulative training compute, rather than regulating based on how AI is used. As a result, Korea believes no current models meet the bar for regulation, while the EU is phasing in broader, use-based AI rules over several years.]]></content:encoded></item><item><title>Geometric Deep Learning: Neural Networks in Spherical, Hyperbolic, and Non-Euclidean Spaces</title><link>https://hackernoon.com/geometric-deep-learning-neural-networks-in-spherical-hyperbolic-and-non-euclidean-spaces?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Fri, 23 Jan 2026 02:51:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2.3 Deep Learning in non-Euclidean spacesA great deal (probably, majority) of data sets are naturally represented in non-Euclidean geometries. This fact has been widely recognized in ML only recently, motivating research efforts in non-Euclidean data representations and ML algorithms over curved spaces. Inferring curvature and symmetries encoded in data sets is of the crucial importance in many tasks.\
The necessity of geometric methods in ML is easy to justify. It is apparent even in basic setups. In order to support this point, we provide three illustrative cases.\
A) When learning rotations in the 3-dimensional space, traditional NN architectures, which involve Euclidean addition and averaging of vectors, are inappropriate.\
B) When optimizing over a certain family of probability distributions (say, for inference problems), it is advisable to take into account the intrinsic geometry of this specific family. For instance, Gaussian policy parametrization is typically used in stochastic RL algorithms over continuous strategy sets. However, intrinsic geometry of the family of Gaussian distributions N (a, Σ) is hyperbolic. If one applies standard Euclidean gradient descent to the problem of learning parameters a and Σ, it is likely that the algorithm will perform incorrectly and learn the matrix Σ which is not positive definite. This does not correspond to any probability distribution. Therefore, one should adapt the gradient descent by taking into account geometry of the manifold N (a, Σ).\
C) Although the Gaussian family usually provides the most convenient statistical model for probabilistic ML algorithms over Euclidean spaces, it is not suitable when learning orientations in space. Data of this kind one require families of probability distributions over the sphere.\
Inferring the curvature and symmetries hidden in data sets is a central problem of geometric ML. Roughly, these sets can be classified into spherical data (data embedded into spaces with strictly positive curvature), Euclidean data (data with zero curvature) and hyperbolic data (data with strictly negative curvature). Of course, such a classification is highly oversimplifying as the majority of real-life big data have mixed curvature [28, 29]\
An apparent example of spherical data are orientations in the Euclidean space. Another less obvious example is the space of categorical distributions (probability distributions over a finite set) equipped with Fisher information metric. The natural gradient update (i.e. update w. r. to Fisher metric) on the manifold of categorical distributions amounts to optimization in spherical geometry.\
On the other hand, hyperbolic data are even more ubiquitous in Science. A great deal of data sets have some (possibly hidden) hierarchical structure. Such data are naturally embedded into manifolds with hyperbolic geometry. For example, the power law (also known as the Pareto-Zipf law) for degrees of nodes in complex networks implies hyperbolic geometry, and vice versa [30, 31]. Other examples of inherently hyperbolic data are common in word embeddings and natural language processing [32, 33], molecular structures [34], Gaussian distributions [35]. In general, one might claim that most of biological data have inherently hyperbolic geometry.\
Optimization on manifolds is a young subdiscipline within the broad field of mathematical optimization. Although particular problems of this kind (such as Wahba’s problem) occasionally appeared in the literature for a long time, systematic approaches have been elaborated in XXI century [36, 37]. Nowadays, advances in the theory of optimization on manifolds are, to a great extent, motivated by applications to geometric ML. For some examples, we refer to ML algorithms based on optimization over hyperbolic [38, 39, 40] or spherical [41, 42] geometries. Recently, novel architectures of spherical [42] and hyperbolic [38] NN’s have been proposed for dealing with geometric data.\
In parallel, probabilistic modeling in geometric ML exploits statistical models over Riemannian manifolds. This motivated a growing interest in applications of directional statistics to ML[43, 44, 45]. Directional statistics is the subdiscipline within general statistics and probability, which deals with observations on compact Riemannian manifolds. Classical and probably the most comprehensive reference on this field is the book of Mardia and Jupp [46].\
Another approach to probabilistic modeling in geometric ML is provided by normalizing flows over Riemannian manifolds. Some researchers[47, 48, 49, 50] reported experiments with normalizing flows over spheres, tori and other manifolds for the density estimation problem.\
Summarizing, encoding geometric features of data in deep learning models evolved into the emerging field of Geometric Deep Learning [51].\
2.3.1 Learning (coupled) actions of transformation groups\
In Geometric DL there is an important class of problems, where the goal is to learn transformations, such as rotations in the d-dimensional space, conformal mappings, groups of isometries, etc. Such problems arise in robotics (movement prediction [52] and imitation learning [53]), in analysis of facial expressions [54], computer vision [55], etc. The corresponding algorithms rely on optimization over Lie groups [56, 57], as well as on learning probability distributions over Lie groups [58, 59, 60].\
One conceptual approach to problems of this kind is the longstanding idea of NN’s with non-Euclidean neurons (and possibly weights) [61, 62]. More recently, this line of reasoning resulted in novel architectures, named equivariant neural networks [63]. Equivariant NN’s are designed in such a way to ensure that outputs are transformed consistently under symmetry transformations of the inputs.\
Very recently, several researchers reported on pioneering efforts in RL with non-Euclidean spaces of states (and actions). So far, robotics is the dominant field of applications. There are two possible approaches to stochastic policies in RL problems of this kind. Some researchers proposed policy parametrizations on the tangent space using standard statistical models (typically the Gaussian family), and projecting the learned policies onto manifolds via exponential map [64]. The second approach consists in parametrization of policies using families of distributions over Riemannian manifolds, thus employing results from directional statistic [59].(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Former Sequoia partner’s new startup uses AI to negotiate your calendar for you</title><link>https://techcrunch.com/2026/01/22/former-sequoia-partners-new-startup-uses-ai-to-negotiate-your-calendar-for-you/</link><author>Marina Temkin</author><category>tech</category><pubDate>Fri, 23 Jan 2026 02:29:24 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Blockit, an AI agent that communicates directly with other calendars, has raised $5 million in seed funding led by Sequoia.]]></content:encoded></item><item><title>Intel Struggles To Meet AI Data Center Demand</title><link>https://slashdot.org/story/26/01/22/2317238/intel-struggles-to-meet-ai-data-center-demand?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Intel says it struggled to satisfy demand for its AI data-center CPUs while new PC chips squeeze margins. CEO Lip-Bu Tan framed the turnaround as supply-constrained, not demand-constrained, with manufacturing yields (18A) improving but still below targets. Reuters reports: The forecast underscores the difficulties faced by Intel in predicting global chip markets, where the company's current products are the result of decisions made years ago. The company, whose shares have risen 40% in the past month, recently launched a long-awaited laptop chip designed to reclaim its lead in personal computers just as a memory chip crunch is expected to depress sales across that industry.
 
Meanwhile, Intel executives said the company was caught off guard by surging demand for server central processors that accompany AI chips. Despite running its factories at capacity, Intel cannot keep up with demand for the chips, leaving profitable data center sales on the table while the new PC chip squeezes its margins.
 
"In the short term, I'm disappointed that we are not able "to fully meet the demand in our markets," Chief Executive Officer Lip-Bu Tan told analysts on a conference call. The company forecast current-quarter revenue between $11.7 billion and $12.7 billion, compared with analysts' average estimate of $12.51 billion, according to data compiled by LSEG. It expects adjusted earnings per share to break even in the first quarter, compared with expectations of adjusted earnings of 5 cents per share.]]></content:encoded></item><item><title>Palmer Luckey says the coolest thing about Anduril expanding is the fighter jets</title><link>https://techcrunch.com/2026/01/22/palmer-luckey-says-the-coolest-thing-about-anduril-expanding-to-long-beach-is-the-fighter-jets/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 23 Jan 2026 01:31:54 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Anduril on Thursday announced its plans to expand its Southern California presence with a major campus in Long Beach, the town where founder Palmer Luckey grew up. ]]></content:encoded></item><item><title>Epic and Google Have a Secret $800 Million Unreal Engine and Services Deal</title><link>https://yro.slashdot.org/story/26/01/22/235220/epic-and-google-have-a-secret-800-million-unreal-engine-and-services-deal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A federal judge revealed a previously undisclosed ~$800 million, six-year partnership between Epic Games and Google tied to Unreal Engine services and joint marketing. It raises questions about whether the deal influenced Epic's willingness to settle its antitrust case over Android. The Verge reports: [California District Judge James Donato] allowed Epic and Google to keep most of the details of the plan under wraps. But during the hearing, he quizzed witnesses, including Epic CEO Tim Sweeney and economics expert Doug Bernheim, on how it might impact settlement talks -- revealing some hints in the process. "You're going to be helping Google market Android, and they're going to be helping you market Fortnite; that deal doesn't exist today, right?" Donato asked Bernheim, who answered in the affirmative. He also described it as a "new business between Epic and Google."
 
Sweeney's testimony cracked the mystery a little further. He referred to the agreement as relating to the "metaverse," a term Sweeney has used to refer to Epic's game Fortnite. "Epic's technology is used by many companies in the space Google is operating in to train their products, so the ability for Google to use the Unreal Engine more fullsome... sorry, I'm blowing this confidentiality," Sweeney said. Donato then offered a hard dollar figure on one part of the deal: "An $800 million spend over six years, that's a pretty healthy partnership," he said. We soon learned that refers to Epic spending $800 million to purchase some sort of services from Google: "Every year we've decided against Google, in this year we're deciding to use Google at market rates," he said. Sweeney did throw cold water on the idea that Epic and Google are jointly building a single new product together, though. "This is Google and Epic each separately building product lines," he clarified, when Judge Donato asked what the term sheet referred to with the line "Google and Epic will work together."
 
Donato seemed potentially leery of the partnership, asking Bernheim whether it could constitute a "quid pro quo" that reduced Epic's incentive to push for terms that would benefit other developers. Currently, Epic is backing a settlement that would see Google reduce its standard app store fees worldwide and allow alternative app stores to register for easy installation on Android. Sweeney disputed the notion that Epic might be getting paid off to soften its terms, when it's the one paying out. "I don't see anything crooked about Epic paying Google off to encourage much more robust competition than they've allowed in the past," he said. "We view this as a significant transfer of value from Epic to Google." He also says the Epic Games Store won't get any special treatment from Android in the future under this deal. It appears that the settlement arrangement is tied to the business deal. Judge Donato suggested that Epic and Google would only make the deal if the settlement goes through. Sweeney says the specific terms of the deal have not yet been reached, but admitted that he expects them to. He told Judge Donato that yes, he considers the settlement and deal "an important part of Epic's growth plan for the future."]]></content:encoded></item><item><title>AMD Lands Fresh Performance Improvements For RDNA4 In RadeonSI Driver</title><link>https://www.phoronix.com/news/GFX12-RadeonSI-Mesa-26.1-More</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 23 Jan 2026 01:21:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While slightly too late for making it into the Mesa 26.0 release that branched yesterday, merged now to Mesa Git for Q2's Mesa 26.1 release are some new RadeonSI Gallium3D (OpenGL) driver optimizations for the latest AMD Radeon RDNA4 graphics cards...]]></content:encoded></item><item><title>OpenAI is coming for those sweet enterprise dollars in 2026</title><link>https://techcrunch.com/2026/01/22/openai-is-coming-for-those-sweet-enterprise-dollars-in-2026/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Fri, 23 Jan 2026 00:52:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI has reportedly appointed Barret Zoph to lead its push into enterprise just a week after Zoph rejoined the company.]]></content:encoded></item><item><title>EU Parliament Calls For Detachment From US Tech Giants</title><link>https://slashdot.org/story/26/01/22/2253207/eu-parliament-calls-for-detachment-from-us-tech-giants?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The European Parliament is calling on the European Commission to reduce dependence on U.S. tech giants by prioritizing EU-based cloud, AI, and open-source infrastructure. The report frames "European Tech First," public procurement reform, and Public Money, Public Code as necessary self-defense against growing U.S. control over critical digital infrastructure. Heise reports: In terms of content, the report focuses on a strategic reorientation of public procurement and infrastructure. The compromise line adopted stipulates that member states can favor European tech providers in strategic sectors to systematically strengthen the technological capacity of the Community. The Greens even called for a stricter regulation here, where the use of products "Made in EU" should become the rule and exceptions would have to be explicitly justified. They also pushed for a definition for cloud infrastructure that provides for full EU jurisdiction without dependencies on third countries.
 
With the decision, the MEPs want to lay the foundation for a European digital public infrastructure based on open standards and interoperability. The principle of Public Money, Public Code is anchored as a strategic foundation to reduce dependence on individual providers. Software specifically developed for administration with tax money should therefore be made available to everyone under free licenses. For financing, the Parliament relies on the expansion of public-private investments. A "European Sovereign Tech Fund" endowed with ten billion euros was discussed beforehand, for example, to specifically build strategic infrastructures that the market does not provide on its own. The shadow rapporteur for the Greens, Alexandra Geese, sees Europe ready to take control of its digital future with the vote. As long as European data is held by US providers subject to laws such as the Cloud Act, security in Europe is not guaranteed.]]></content:encoded></item><item><title>Vimeo starts layoffs after acquisition by Bending Spoons</title><link>https://techcrunch.com/2026/01/22/vimeo-starts-layoffs-after-acquisition-by-bending-spoons/</link><author>Lucas Ropek</author><category>tech</category><pubDate>Fri, 23 Jan 2026 00:27:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Vimeo is in the process of laying off some of its global staff after being acquired last year by Bending Spoons for $1.38B.]]></content:encoded></item><item><title>Rent-Only Copyright Culture Makes Us All Worse Off</title><link>https://www.eff.org/deeplinks/2026/01/rent-only-copyright-culture-makes-us-all-worse</link><author>Rory Mir</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/COPYRIGHT-DIGITAL.png" length="" type=""/><pubDate>Fri, 23 Jan 2026 00:27:22 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[As we approach the 50th anniversary of the 1976 Copyrights, the last major overhaul of US copyright law, we’re not the only ones wondering if it’s time for the next one. It’s a high-risk proposition, given the wealth and influence of entrenched copyright interests who will not hesitate to send carefully selected celebrities to argue for changes that will send more money, into fewer pockets, for longer terms. But it’s equally clear that and nowhere is that more evident than the waning influence of Section 109, aka the first sale doctrine.]]></content:encoded></item><item><title>Ctrl-Alt-Speech: This Episode Is Broadly Safe To Listen To</title><link>https://www.techdirt.com/2026/01/22/ctrl-alt-speech-this-episode-is-broadly-safe-to-listen-to/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 23 Jan 2026 00:14:01 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[In this week’s round-up of the latest news in online speech, content moderation and internet regulation, Mike and Ben cover:]]></content:encoded></item><item><title>New Jersey Law Requires E-Bike Drivers To Have License, Insurance</title><link>https://tech.slashdot.org/story/26/01/22/221222/new-jersey-law-requires-e-bike-drivers-to-have-license-insurance?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 23 Jan 2026 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from CBS News: As one of his final acts in office, former New Jersey Gov. Phil Murphy signed into law new requirements for e-bikes in his state. The new legislation signed Monday requires that owners and operators of e-bikes have licenses, registration and insurance. Owners and operators of e-bikes must be at least 17 years old and have a valid driver's license or be at least 15 years old with a motorized bicycle license under the law, which covers all types of electric bikes.
 
"We are in a new era of e-bike use that requires updated safety standards to help prevent accidents, injuries, and fatalities. Requiring registration and licensing will improve their safe use and having them insured will protect those injured in accidents," said Senate President Nick Scutari, who co-sponsored the bill. The legislation follows an increase in crashes involving e-bikes, including multiple incidents that killed or injured young people in New Jersey in 2025. [...] Registration and licensing fees for e-bikes will be waived for one year, and riders will have six months to get the registration, insurance and license that they need under the law.]]></content:encoded></item><item><title>GM to end Chevy Bolt EV production next year, move China-made Buick to US factory</title><link>https://techcrunch.com/2026/01/22/gm-to-end-chevy-bolt-ev-production-next-year-move-china-made-buick-to-u-s-factory/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Thu, 22 Jan 2026 23:55:44 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The factory musical chairs reflect an economic and political environment, shaped by the Trump administration's tariff policy and its decision to end the federal EV tax credit.]]></content:encoded></item><item><title>Capital One acquires Brex for a steep discount to its peak valuation, but early believers are laughing all the way to the bank</title><link>https://techcrunch.com/2026/01/22/capital-one-acquires-brex-for-steep-discount-to-its-peak-valuation-but-early-believers-are-laughing-all-the-way-to-the-bank/</link><author>Connie Loizos</author><category>tech</category><pubDate>Thu, 22 Jan 2026 23:46:19 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Before everyone sharpens their knives, consider that for the VCs who backed Brex at its outset, the sale is a triumph. ]]></content:encoded></item><item><title>The Microsoft-OpenAI Files</title><link>https://slashdot.org/story/26/01/22/2152231/the-microsoft-openai-files?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Longtime Slashdot reader theodp writes: GeekWire takes a look at AI's defining alliance in The Microsoft-OpenAI Files, an epic story drawn from 200+ documents, many made public Friday in Elon Musk's ongoing suit accusing OpenAI and its CEO Sam Altman of abandoning the nonprofit mission (Microsoft is also a defendant). Musk, who was an OpenAI co-founder, is seeking up to $134 billion in damages. "Previously undisclosed emails, messages, slide decks, reports, and deposition transcripts reveal how Microsoft pursued, rebuffed, and backed OpenAI at various moments over the past decade, ultimately shaping the course of the lab that launched the generative AI era," reports GeekWire. "The latest round of documents, filed as exhibits in Musk's lawsuit, [...] show how Nadella and Microsoft's senior leadership team rally in a crisis, maneuver against rivals such as Google and Amazon, and talk about deals in private."
 
Even though Microsoft didn't have a seat on the OpenAI board, text messages between Microsoft CEO Satya Nadella and OpenAI CEO Sam Altman following Altman's firing as CEO in Nov. 2023 (news of which sent Microsoft's stock plummeting), revealed in the latest filings, show just how influential Microsoft was. A day after Altman's firing, Nadella sent Altman a detailed message from Brad Smith, Microsoft's president and top lawyer, explaining that Microsoft had created a new subsidiary called Microsoft RAI (Responsible Artificial Intelligence) Inc. from scratch -- legal work done, papers ready to file as soon as the WA Secretary of State opened Monday morning -- and was ready to capitalize and operationalize it to "support Sam in whatever way is needed," including absorbing the OpenAI team at a calculated cost of roughly $25 billion. (Altman's reply: "kk"). Just days later, as he planned his return as CEO to the now-reeling-from-Microsoft-punches nonprofit, Altman joined Microsoft's Nadella, Smith, and CTO Kevin Scott in a text messaging thread in which the four vetted prospective board members to replace those who had ousted Altman. Later that night, OpenAI announced Altman's return with the newly constituted board.
 
If you like stories with happy Microsoft endings, as part of an agreement clearing the way for OpenAI to restructure as a for-profit business, Microsoft in October received a 27% ownership stake in OpenAI worth approximately $135 billion and retains access to the AI startup's technology until 2032, including models that achieve AGI.]]></content:encoded></item><item><title>Instagram Creators Should Check If Their Posts Are Being Deemed Political</title><link>https://hackernoon.com/instagram-creators-should-check-if-their-posts-are-being-deemed-political?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Thu, 22 Jan 2026 23:08:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Markup, now a part of CalMatters, uses investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up forKlaxon, a newsletter that delivers our stories and tools directly to your inbox.\
If you opened Instagram last week, you may have seen oneofmany tutorials on how to opt out of a setting that was quietly released in February: Instagram and Threads users will no longer be recommended political content from people they don’t follow.\
Instagram “won’t proactively recommend content about politics,” according to a blog post it issued Feb. 9. While the policy was launched without making headlines, it spiked attention last week as Instagram users took to the platform to raise awareness about the change.\
What counts as politics? The company’s announcement defined “political content” as “potentially related to things like laws, elections, or social topics,” and Instagram’s help page adds content about “governments” to the list. But the most comprehensive definition is displayed where users can go to turn off the limits on political content: “Political content is likely to mention governments, elections, or social topics that affect a group of people and/or society at large.”Check If Your Posts Are “Political”While not every Instagram user will be able to review whether their content is considered political—and therefore no longer eligible for recommendation—professional users such as creators or businesses have the power to check. (If you can see Instagram’s “Insights” analytics for your account, you have a professional account.)\
On a desktop or mobile browser: You can go to Account Status directly.How to access Account Status on the mobile app:Go to your Profile page by tapping on the 👤 person icon in the lower-right corner of the screen.Click on the ☰ menu icon in the upper-right corner of the screen.Scroll down and select “Account Status.”On your Account Status page, you can check whether Instagram will no longer recommend something you’ve posted (such as content deemed political), by clicking through “What can’t be recommended.”\
This is what Account Status looked like on The Markup’s account today. So far, none of our recent posts have been flagged as political:The Markup’s account status on March 25, 2024.\
While all users have an “Account Status” page, only professional accounts have the “What can’t be recommended” and “Monetization” status checks.Help us figure out exactly what Instagram counts as “political content.” If, after checking the Account Status of your professional account, you see that one or more of your posts have been flagged as political, take a screenshot and send it to The Markup. You can DM us on Instagram directly @the.markup, or email it to us at maria@themarkup.org.\
A Markup investigation published in February found that Instagram demoted nongraphic photos of soldiers, destroyed buildings, and military tanks from on the ground in Gaza. If you think you’ve been shadowbanned on Instagram—or if the app has notified you that it has removed your content or limited your account in some way—here’s what you can do.]]></content:encoded></item><item><title>Overrun with AI slop, cURL scraps bug bounties to ensure &quot;intact mental health&quot;</title><link>https://arstechnica.com/security/2026/01/overrun-with-ai-slop-curl-scraps-bug-bounties-to-ensure-intact-mental-health/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2026/01/ai-slop-1152x648.jpg" length="" type=""/><pubDate>Thu, 22 Jan 2026 22:46:30 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT - Ars Technica</source><content:encoded><![CDATA[The project developer for one of the Internet’s most popular networking tools is scrapping its vulnerability reward program after being overrun by a spike in the submission of low-quality reports, much of it AI-generated slop.“We are just a small single open source project with a small number of active maintainers,” Daniel Stenberg, the founder and lead developer of the open source app cURL, said Thursday. “It is not in our power to change how all these people and their slop machines work. We need to make moves to ensure our survival and intact mental health.”His comments came as cURL users complained that the move was treating the symptoms caused by AI slop without addressing the cause. The users said they were concerned the move would eliminate a key means for ensuring and maintaining the security of the tool. Stenberg largely agreed, but indicated his team had little choice.]]></content:encoded></item><item><title>Voice AI engine and OpenAI partner LiveKit hits $1B valuation</title><link>https://techcrunch.com/2026/01/22/voice-ai-engine-and-openai-partner-livekit-hits-1b-valuation/</link><author>Marina Temkin</author><category>tech</category><pubDate>Thu, 22 Jan 2026 22:44:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The five-year-old-startup powers OpenAI’s ChatGPT voice mode and raised a $100 million round led by Index Ventures.]]></content:encoded></item><item><title>Inference startup Inferact lands $150M to commercialize vLLM</title><link>https://techcrunch.com/2026/01/22/inference-startup-inferact-lands-150m-to-commercialize-vllm/</link><author>Marina Temkin</author><category>tech</category><pubDate>Thu, 22 Jan 2026 22:42:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The seed round values the newly formed startup at $800 million. ]]></content:encoded></item><item><title>Waymo Launches Robotaxi Service In Miami, Extending US Lead</title><link>https://tech.slashdot.org/story/26/01/22/2141245/waymo-launches-robotaxi-service-in-miami-extending-us-lead?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Waymo has launched its paid robotaxi service in Miami, marking its sixth U.S. market and the company's first expansion of 2026. CNBC reports: As U.S. competition has lagged, Waymo's planned 2026 expansions could lock in rider demand and loyalty in the U.S. To start, Waymo will offer its services within a 60-square-mile area that includes Miami's Design District, Wynwood, Brickell and Coral Gables neighborhoods, the Google sister company said.
 
The company began testing its vehicles in the Florida city in early 2025. Waymo said it plans to extend its service to the Miami International Airport in the near future, but did not give a specific timeline. The company said "nearly 10,000 residents" of Miami have already signed up to try its robotaxi service, and Waymo will be "inviting new riders on a rolling basis." Riders can hail a Waymo robotaxi in Miami using the company's app. Waymo is partnering with mobility company Moove for fleet management services including vehicle charging, cleaning and repairs.]]></content:encoded></item><item><title>How to Make Your Legacy Exception-Throwing Code Compatible With Lambdas</title><link>https://hackernoon.com/how-to-make-your-legacy-exception-throwing-code-compatible-with-lambdas?source=rss</link><author>Nicolas Fränkel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 22:21:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Java's checked exceptions were a massive improvement over C's error-handling mechanism. As time passed and experience  accumulated, we collectively concluded that we weren't there yet. However, Java's focus on stability has kept checked exceptions in its existing API.\
Java 8 brought lambdas after the "checked exceptions are great"  trend. None of the functional interface methods accepts a checked exception. In this post, I will demonstrate three different approaches to making your legacy exception-throwing code compatible with lambdas.Consider a simple exception-throwing method. \n public class Foo {
    public String throwing(String input) throws IOException {
        return input;                          //1
    }
}
The body is there for compilation purposes; its exact content is irrelevant.\
The method accepts a  and returns a . It has the shape of a , so we can use it as such: \n var foo = new Foo();
List.of("One", "Two").stream()
    .map(string -> foo.throwing(string))
    .toList();
The above code fails with a compilation error: \n unreported exception IOException; must be caught or declared to be thrown
            .map(string -> foo.throwing(string)).toList();
                                       ^
1 error
To fix the error, we need to wrap the throwing code in a try-catch block: \n List.of("One", "Two").stream()
    .map(string -> {
        try {
            return foo.throwing(string);
        } catch (IOException e) {
            return "";
        }
    }).toList();
At this point, the code compiles, but defeats the main purpose of lambdas: being concise and readable.We can definitely improve the design by modeling a  with a throwing . \n interface ThrowingFunction<I, O, E extends Exception> {
    O apply(I i) throws E;
}
We can then provide a wrapper to transform such a throwing  into a regular . \n class LambdaUtils {
    public static <I, O, E extends Exception> Function<I, O> safeApply(ThrowingFunction<I, O, E> f) {
        return input -> {
            try {
                return f.apply(input);
            } catch (Exception e) {
                return "";
            }
        };
    }
}
With the above, the calling code can be improved like this: \n var foo = new Foo();
List.of("One", "Two").stream()
    .map(string -> LambdaUtils.safeApply(foo::throwing))     //1
    .toList();
The most straightforward way to call exception-throwing code in a  lambda involves using a library. Two libraries that I know of provide  this capability:Here's how we can rewrite the above code using Commons Lang 3 code: \n var foo = new Foo();
FailableFunction<String, String, IOException> throwingFunction = foo::throwing; //1
List.of("One", "Two").stream()
    .map(throwingFunction)
    .recover(e -> "")                                                           //2
    .toList();
Commons Lang 3 models a throwing  mimics the value set in the previous  block\
The libraries improve upon my debatable design above, but the idea stays the same.\
The decision to roll out your own or use a library depends on a variety of factors that go beyond this post. Here are some criteria to help you.Suppressing Checked ExceptionsChecked exceptions are a compile-time concern. The Java Language Specification states:A compiler for the Java programming language checks, at compile time, that a program contains handlers for ,  by analyzing which checked exceptions can result from execution of a  method or constructor. For each checked exception which is a possible  result, the  clause for the method (§8.4.6) or  constructor (§8.8.5) must mention the class of that exception or one of  the superclasses of the class of that exception. This compile-time  checking for the presence of exception handlers is designed to reduce  the number of exceptions which are not properly handled.\
We could potentially hook into the compiler to prevent this check via a compiler plugin. Or find a library that does. That's when Manifold enters the scene.Manifold is a Java compiler plugin. Use it to supplement your Java projects with highly productive features.Powerful language enhancements improve developer productivity.Optional parameters (New!)\
Disclaimer: I don't advocate for using Manifold. It makes the language you work with different from Java. At this point, you'd be  better off using Kotlin directly.\
Using Manifold to suppress checked exceptions is a two-step process. First, we add the Manifold runtime to the project: \n <dependency>
    <groupId>systems.manifold</groupId>
    <artifactId>manifold-rt</artifactId>
    <version>${manifold.version}</version>
    <scope>provided</scope>
</dependency>
\
Then, we configure the compiler plugin: \n <build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>3.8.0</version>
            <configuration>
                <source>11</source>
                <target>11</target>
                <encoding>UTF-8</encoding>
                <compilerArgs>
                    <arg>-Xplugin:Manifold</arg>
                </compilerArgs>
                <annotationProcessorPaths>
                    <path>
                        <groupId>systems.manifold</groupId>
                        <artifactId>manifold-exceptions</artifactId>
                        <version>${manifold.version}</version>
                    </path>
                </annotationProcessorPaths>
            </configuration>
        </plugin>
    </plugins>
</build>
\
At this point, we can treat checked exceptions as unchecked. \n var foo = new Foo();
List.of("One", "Two").stream()
        .map(string -> foo.throwing(string))       //1
        .toList();
In this post, I tackled the issue of integrating checked exceptions with lambdas in Java. I listed several options: the try-catch block, the throwing function, the library, and Manifold. I hope you can find one that suits your context among them.Originally published at A Java Geek on January 18th, 2026]]></content:encoded></item><item><title>Google Begins Offering Free SAT Practice Tests Powered By Gemini</title><link>https://news.slashdot.org/story/26/01/22/2131206/google-begins-offering-free-sat-practice-tests-powered-by-gemini?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 22:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: It's no secret that students worldwide use AI chatbots to do their homework and avoid learning things. On the flip side, students can also use AI as a tool to beef up their knowledge and plan for the future with flashcards or study guides. Google hopes its latest Gemini feature will help with the latter. The company has announced that Gemini can now create free SAT practice tests and coach students to help them get higher scores. As a standardized test, the content of the SAT follows a predictable pattern. So there's no need to use a lengthy, personalized prompt to get Gemini going. Just say something like, "I want to take a practice SAT test," and the chatbot will generate one complete with clickable buttons, graphs, and score analysis.
 
Of course, generative AI can go off the rails and provide incorrect information, which is a problem when you're trying to learn things. However, Google says it has worked with education firms like The Princeton Review to ensure the AI-generated tests resemble what students will see in the real deal. The interface for Gemini's practice tests includes scoring and the ability to review previous answers. If you are unclear on why a particular answer is right or wrong, the questions have an "Explain answer" button right at the bottom. After you finish the practice exam, the custom interface (which looks a bit like Gemini's Canvas coding tool) can help you follow up on areas that need improvement. Google says support for the SAT is just the start, "with more tests coming in the future."]]></content:encoded></item><item><title>Utah Continues To Ban More Books, Even As It Racks Up More Lawsuits</title><link>https://www.techdirt.com/2026/01/22/utah-continues-to-ban-more-books-even-as-it-racks-up-more-lawsuits/</link><author>Tim Cushing</author><category>tech</category><pubDate>Thu, 22 Jan 2026 21:57:43 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Utah’s budding theocracy continues unimpeded as we head into the new year. On top of its other unconstitutional laws (like the oft-challenged social media ban) and legislative proposals, there’s its book ban law that has seen it become the first state to actually remove certain books from  public schools across the state. The targeted books are exactly the ones you think they are. Of the 13 titles to make the first ban list in 2024, 12 of them were written by women. It has added more titles to the ban list for 2026, as BookRiot reports. This law is basically just a heckler’s veto. No consensus is needed to subject a title to removal across the state. The law allows parents to file book challenges which, in reality, means a few bigoted activists will be able to impose their will on every resident in the state.The law compounds this deliberate error by allowing certain schools (or those being pressured by this small group of anti-freedom activists) to place their thumbs on the scale. Since that’s what the law is designed to do, that’s exactly what has happened: What is important to understand about the law is that despite claims this is about “local control,” schools in the state are forced to follow the decisions made in other districts. There are 42 public school districts in Utah, but two districts account for nearly 80% of the books banned statewide: Davis School District and Washington School District.The three latest book bans came exclusively because of bans at Davis, Tooele, and Washington school districts. Again, two districts are doing nearly all of the dictating of what books are allowed at public schools throughout all of Utah.“Local control” is as meaningless as “representative democracy.” Someone will always find a way to game the system to ensure they keep what they have if not take a little more. Political parties gerrymander. Utah legislators craft laws that allow a small subset of state schools to write the rules for the rest of them. For now, the law remains in place and the entirety of the state remains under the direct, definitely not “local” control of a couple of school districts. For now. But things could get a bit more interesting soon, now that a serious challenge to the law has been raised by some of the authors directly affected by these bans. A group of best-selling authors whose books are banned from Utah public schools are suing the state, arguing its sensitive materials law is unconstitutional.Filed in federal court, the lawsuit comes after three more books were banned from K-12 schools.Among those suing over Utah’s book ban law are award-winning novelists Elana K. Arnold and Ellen Hopkins, the Estate of Kurt Vonnegut and two anonymous Utah public high school students.The lawsuit [PDF] raises questions the state isn’t going to be in any hurry to answer. The Book Removal Law, codified at Section 53G-10-103 of the Utah Code, is unmoored from the First Amendment and requires Utah’s Local Education Agencies (“LEAs”) to strip their school libraries of any book that contains even a single description or depiction of sex, no matter how fleeting, no matter its context, and no matter its literary, artistic, political, or scientific value.The Book Removal Law also never asks the most basic question: appropriate for whom? A kindergartner learning to sound out words and a twelfth-grader weeks from graduation are treated identically. As described below, once a book is labeled “sensitive,” it must be taken from the shelf, including the high school library. There is no recognition that a seventeen-year-old preparing for college, navigating identity, relationships, and the realities of adulthood stands in a fundamentally different place than a five-year-old.This creates an absurd mismatch with other parts of Utah’s own legal standards. State law permits sixteen-year-olds to consent to certain sexual activity. Yet the same students whom Utah trusts to make intimate, real-world decisions about their bodies are, under the Book Removal Law, barred from accessing out books that contain a mere single passage describing the very conduct in which is lawful for them to engage. The Book Removal Law tells them: you are mature enough to do this, but not mature enough to read about it.The answer is, of course, that this isn’t about protecting children from content that might be inappropriate for them. It’s about giving bigots and public employees an easy way to remove content they personally don’t like. Because its ulterior motive is its  motive, it’s been written in a way that makes it extremely susceptible to legal challenges. With any luck, this law won’t survive much longer and the people who think  should have access to content  don’t care for will have to go back to the ineffective seething that seems to make up a disproportionate portion of their existence. ]]></content:encoded></item><item><title>Are AI agents ready for the workplace? A new benchmark raises doubts</title><link>https://techcrunch.com/2026/01/22/are-ai-agents-ready-for-the-workplace-a-new-benchmark-raises-doubts/</link><author>Russell Brandom</author><category>tech</category><pubDate>Thu, 22 Jan 2026 21:42:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[New research looks at how leading AI models hold up doing actual white-collar work tasks, drawn from consulting, investment banking, and law. Most models failed.]]></content:encoded></item><item><title>AlphaTON Launches Claude Connector Powered By TON And Telegram</title><link>https://hackernoon.com/alphaton-launches-claude-connector-powered-by-ton-and-telegram?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Thu, 22 Jan 2026 21:35:18 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Today marks the launch of AlphaTON Claude Connector, a groundbreaking platform that combines Anthropic's Claude artificial intelligence with TON blockchain technology, delivered through the Telegram messaging application. The solution enables users to manage digital assets, execute transactions, and interact with blockchain services using natural language conversation.The platform addresses a critical barrier to cryptocurrency adoption: complexity. Traditional blockchain interfaces require technical knowledge and navigation through complicated dashboards. AlphaTON Claude Connector eliminates these obstacles by allowing users to simply type commands in plain English within their existing Telegram application.The platform integrates three major technologies:Telegram — A messaging platform with over one billion active users worldwideClaude AI — Anthropic's advanced artificial intelligence assistant, known for nuanced understanding and reliable responsesTON Blockchain — The Open Network, a high-performance blockchain capable of processing millions of transactions per secondUsers can perform operations such as checking wallet balances, sending and receiving TON tokens, viewing transaction histories, and managing digital assets all through conversational commands.Logan Golema, Chief Technology Officer, AlphaTON Capital, said, “Building with Claude Code has been essential to our internal development process, enabling us to move faster and collaborate more effectively across teams. We’re excited to extend that capability through this connector and empower Telegram and TON developers worldwide with more efficient, scalable tools to build and innovate.” Users interact with the system through natural language. Rather than navigating complex menus, users simply state their intent:"What is my current balance""Send 50 TON to this address""Show my transactions from this week"The AI interprets the request, executes the appropriate blockchain operation, and confirms completion.Integrated Mini ApplicationFor users requiring visual interfaces, the platform includes an embedded web application accessible directly within Telegram. This Mini App provides:Real-time balance displaysTransaction history visualizationStreamlined sending interfacesPortfolio overview dashboardsThe platform connects with project management systems including JIRA, enabling business teams to coordinate blockchain operations within existing workflows.Built on the Model Context Protocol standard, the system provides extensible architecture for developers to build custom integrations and applications.The global cryptocurrency market continues to expand, yet mainstream adoption remains limited by usability challenges. Research indicates that complexity ranks among the top barriers preventing new users from engaging with digital assets.Messaging-based interfaces represent a significant opportunity to bridge this gap. By meeting users within applications they already use daily, AlphaTON Claude Connector removes friction from the onboarding process.The TON blockchain, with its native Telegram integration and high transaction throughput, provides an ideal foundation for this approach.The platform implements enterprise-grade security practices:End-to-end encryption for all communicationsSecure key management protocolsRate limiting and abuse preventionTelegram Web App authentication validationNo storage of private keys or mnemonics in application codeAlphaTON Claude Connector is now available.Claude users can attach the custom connector in their desktop or CLI environments.For enterprise inquiries, partnership opportunities, or media requests, contact the team through official channels.The Open Network (TON) is a decentralized blockchain designed for speed and scalability. Originally developed by the team behind Telegram, TON has grown into an independent ecosystem with an active developer community and expanding use cases across payments, decentralized applications, and digital identity.Claude is an AI assistant developed by Anthropic, designed to be helpful, harmless, and honest. Claude excels at nuanced conversation, complex reasoning, and following detailed instructions, capabilities essential for reliable blockchain operations.About AlphaTON Capital Corp. (Nasdaq: ATON)AlphaTON Capital Corp (NASDAQ: ATON) is the world's leading technology public company scaling the Telegram super app, with an addressable market of 1 billion monthly active users while managing a strategic reserve of digital assets.The Company implements a comprehensive M&A and treasury strategy that combines direct digital assets acquisition, validator operations, and strategic ecosystem investments to generate sustainable returns for shareholders. Through its operations, AlphaTON Capital provides public market investors with institutional-grade exposure to the TON ecosystem and Telegram's billion-user platform while maintaining the governance standards and reporting transparency of a Nasdaq-listed company. Led by Chief Executive Officer Brittany Kaiser, Executive Chairman and Chief Investment Officer Enzo Villani, and Chief Business Development Officer Yury Mitin, the Company's activities span network validation and staking operations, development of Telegram-based applications, and strategic investments in TON-based decentralized finance protocols, gaming platforms, and business applications. \n AlphaTON Capital Corp is incorporated in the British Virgin Islands and trades on Nasdaq under the ticker symbol "ATON". AlphaTON Capital, through its legacy business, is also advancing first-in-class therapies targeting known checkpoint resistance pathways to achieve durable treatment responses and improve patients' quality of life. AlphaTON Capital actively engages in the drug development process and provides strategic counsel to guide the development of novel immunotherapy assets and asset combinations. To learn more, please visit /.Forward-Looking StatementsThis press release contains forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995. These statements relate to future events or AlphaTON's future financial performance and involve known and unknown risks, uncertainties and other factors that may cause actual results to differ materially from those expressed or implied by these forward-looking statements. Factors that could cause or contribute to such differences include, but are not limited to, the development and adoption of AI technologies, market volatility, regulatory developments, technical challenges in infrastructure deployment, and general economic conditions. AlphaTON undertakes no obligation to update any forward-looking statements, except as required by law.The AlphaTON Claude Connector utilizes emerging artificial intelligence and blockchain technologies that are under active development. Features, functionality, and performance may evolve over time and are subject to change without notice. While it is designed to enhance usability and efficiency, users should independently evaluate the technology and exercise appropriate caution when engaging in digital asset transactions. AlphaTON Claude Connector does not provide financial, investment, or legal advice.Investor Relations: \n AlphaTON Capital Corp \n AlphaTON@icrinc.com \n (203) 682-8200 \n  \n Media Inquiries: \n Richard Laermer \n RLM PR \n AlphaTON@rlmpr.com \n (212) 741-5106 X 216:::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision]]></content:encoded></item><item><title>How to Add the AWS WAF CAPTCHA to an Angular Application</title><link>https://hackernoon.com/how-to-add-the-aws-waf-captcha-to-an-angular-application?source=rss</link><author>Rodrigo Kamada</author><category>tech</category><pubDate>Thu, 22 Jan 2026 21:32:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Before we begin, you need to install and configure the tools below to create the Angular application and integrate it with the AWS WAF captcha.: Node.js is a JavaScript code runtime software based on Google's V8 engine. npm is a package manager for Node.js (Node.js Package Manager). They will be used to build and run the Angular application and install the libraries.: Angular CLI is a command-line utility tool for Angular, and it will be used to create the base structure of the Angular application.IDE (e.g.,  or ): An IDE (Integrated Development Environment) is a tool with a graphical interface to help in the development of applications, and it will be used to develop the Angular application.: Amazon Web Services is a cloud computing services platform, and it will be used to integrate the AWS WAF CAPTCHA challenge.Create and configure the account on AWS WAF. (Web Application Firewall) is a security service designed to protect your WEB applications and APIs from exploits of common internet vulnerabilities and bots that can affect availability, compromise security, or consume excessive resources.\
 Now, we will fill in the field , fill in the field , fill in the field  and click on the button . Next, we will type  in the search field and click on the option . Then, we will click on the menu . After clicking on the menu , we will click on the tab . Now, we will click on the option CloudFront JavaScript tag, click on the button  to copy the URL of the JavaScript file, because this URL will be configured in the Angular application, and click on the button . Next, we will click on the option , fill in the field  with your website's domain, and click on the button . Then, we will click on the icon to copy the API key because this API key will be configured in the Angular application. Ready! The API key was created.Create the Angular application. is a development platform for building WEB, mobile, and desktop applications using HTML, CSS, and TypeScript (JavaScript). Currently, Angular is at version 21, and Google is the main maintainer of the project.\
 Let's create the application with the Angular base structure using the tool with the AI configuration files disabled, the server-side rendering (SSR) disabled, the route file disabled, and the SCSS style format. \n ng new angular-aws-waf-captcha --ai-config=none --ssr=false --routing=false --style=scss
CREATE angular-aws-waf-captcha/README.md (1473 bytes)
CREATE angular-aws-waf-captcha/.editorconfig (314 bytes)
CREATE angular-aws-waf-captcha/.gitignore (604 bytes)
CREATE angular-aws-waf-captcha/angular.json (2084 bytes)
CREATE angular-aws-waf-captcha/package.json (970 bytes)
CREATE angular-aws-waf-captcha/tsconfig.json (957 bytes)
CREATE angular-aws-waf-captcha/tsconfig.app.json (429 bytes)
CREATE angular-aws-waf-captcha/tsconfig.spec.json (441 bytes)
CREATE angular-aws-waf-captcha/.vscode/extensions.json (130 bytes)
CREATE angular-aws-waf-captcha/.vscode/launch.json (470 bytes)
CREATE angular-aws-waf-captcha/.vscode/tasks.json (978 bytes)
CREATE angular-aws-waf-captcha/src/main.ts (222 bytes)
CREATE angular-aws-waf-captcha/src/index.html (306 bytes)
CREATE angular-aws-waf-captcha/src/styles.scss (80 bytes)
CREATE angular-aws-waf-captcha/src/app/app.scss (0 bytes)
CREATE angular-aws-waf-captcha/src/app/app.spec.ts (690 bytes)
CREATE angular-aws-waf-captcha/src/app/app.ts (246 bytes)
CREATE angular-aws-waf-captcha/src/app/app.html (20086 bytes)
CREATE angular-aws-waf-captcha/src/app/app.config.ts (203 bytes)
CREATE angular-aws-waf-captcha/public/favicon.ico (15086 bytes)
✔ Packages installed successfully.
    Successfully initialized git.
Next, we will create configuration files for the environment variables with the command below. \n ng generate environments
CREATE src/environments/environment.ts (31 bytes)
CREATE src/environments/environment.development.ts (31 bytes)
UPDATE angular.json (2309 bytes)
\
 Then, we will configure the variable  with the URL of the JavaScript file containing the AWS WAF integration code and the variable  with the API key that was created in AWS WAF in the src/environments/environment.ts and src/environments/environment.development.tsfiles as below. \n   aws: {
    waf: {
      url: 'https://a162a8302aaa.edge.captcha-sdk.awswaf.com/a162a8302aaa/jsapi.js',
      apiKey: 'QprVjPcLgWgjYBGZoci9OMw9CFJSZZtr2gHt7McsehfL1+Hi+6Rp5y99a0p3vmdsq5Wkat06xfZSTc6dfjvR5w9COBfYhcsXvdRPAGSd9Zxfx6W2QfDsDc2IdiQImVfAl35Uw3tBYd31B3rVlT9q4pEcipVFo29NcqRjYneV7peX2Q+vvd4ttjX0QuM3d+csxtCjaHYSquk6nySEun5QTnbUsq0TE2gliQRFYgHFX9vXSWrkkAXHwWtaxr86kzIeIPoesRAC3JB7kwyK6l4sYoo6w7fTSougEwpjJHGIo8LHdjlxmmNpE2umf1z1go3Ixjm/ujc919tn2FCuGbnW4OEXVWotx3qjf+FPkFbght+ccXsXsL2eHoPgj0sHKaeK0fYpGlJnpwDp+ep64+UVm46Y7Udr7RVLjCnngBcgTRnmA20uy+agfhRnBJk6Y34OgJQJnWuoYXCoVETlFus4ATJ69dny8/6GxUUObjcU6gnfpyR5vkaqmKwSluu6i/x90o9pm1/TTFeh4iuSGylrMySzOe4PsxqMOYR9ow9hnrm7fXz+yE2GoQUJjLWpOUfcmcFmOonjslxIxwDjafWBPACec3JBWGYjAF0H+98Bg6vPly9wlxJwiz6eiMjULPfb1lM7rr6crZ1VaZ964pw21FZSrOdAMknmGSZvmKGAzeU=_1_1',
    },
  },
\
 After configuring the variables, we will remove the content of the  class from the  file, create the  and  methods, and implement the  method of the  class that it will execute the method as below. \n export class App implements AfterViewInit {

  captchaRef = viewChild>('captcha');
  messageSuccess = model('');
  messageError = model('');

  constructor() {
  }

  ngAfterViewInit(): void {
    this.loadScript();
  }

  private loadScript(): void {
    if (document.getElementById('awsWafScript')) {
      return;
    }

    const script = document.createElement('script');
    script.id = 'awsWafScript';
    script.async = false;
    script.defer = true;
    script.src = environment.aws.waf.url;
    script.onload = () => {
      this.renderCaptcha();
    };
    document.head.appendChild(script);
  }

  private renderCaptcha(): void {
    const captchaElementRef = this.captchaRef();

    if (window.AwsWafCaptcha && captchaElementRef) {
      window.AwsWafCaptcha.renderCaptcha(captchaElementRef.nativeElement, {
        apiKey: environment.aws.waf.apiKey,
        onSuccess: (token: string) => {
          this.messageSuccess.set(`Captcha successfully validated! Token: ${token}`);
          this.messageError.set('');
        },
        onError: (error: any) => {
          this.messageError.set(`Error validating captcha: ${error}`);
          this.messageSuccess.set('');
        },
      });
    }
  }

}
\
 Now, we will remove the content from the  file and add the  tag with the reference variable as below. \n <div class="container-fluid py-3">
  <h1>Angular AWS WAF Captcha</h1>

  <div class="row mt-3">
    <div class="col-sm-12">
      <div #captcha></div>
    </div>
  </div>
  <div class="row mt-3">
    <div class="col-sm-12">
      <p class="text-center text-success">{{ messageSuccess() }}</p>
      <p class="text-center text-danger">{{ messageError() }}</p>
    </div>
  </div>
</div>
Next, we will run the application with the command below. \n npm run start

> angular-aws-waf-captcha@1.0.0 start
> ng serve

Initial chunk files | Names         |  Raw size
styles.css          | styles        | 377.03 kB | 
main.js             | main          |   6.10 kB | 

                    | Initial total | 383.13 kB

Application bundle generation complete. [2.375 seconds] - 2026-01-17T18:13:56.184Z

Watch mode enabled. Watching for file changes...
NOTE: Raw file sizes do not reflect development server per-request transformations.
  ➜  Local:   http://localhost:4200/
  ➜  press h + enter to show help
\
 Then, we will access the URL  and check if the application is working. Ready! The Angular application was created and integrated with the AWS WAF CAPTCHA challenge.Summarizing what was covered in this article:We created the API key and configured the domain where the captcha will work in AWS WAF.We created an Angular application and added the captcha code.\
You can use this article to create web applications with bot protection using the AWS WAF captcha challenge.\
Thank you for reading, and I hope you enjoyed the article!\
This tutorial was posted on my  in Portuguese.\
To stay updated whenever I post new articles, follow me on  and .]]></content:encoded></item><item><title>Microsoft 365 hit by outage, preventing access to emails and files</title><link>https://techcrunch.com/2026/01/22/microsoft-365-hit-by-outage-preventing-access-to-emails-and-files/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Thu, 22 Jan 2026 21:31:10 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[An hours-long outage is preventing Microsoft's enterprise customers from accessing their inboxes, files, and video meetings. ]]></content:encoded></item><item><title>NASA Eyes Popular PC Hardware Performance Tool for Its Flight Simulators</title><link>https://science.slashdot.org/story/26/01/22/185234/nasa-eyes-popular-pc-hardware-performance-tool-for-its-flight-simulators?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 21:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[NASA Langley has initiated the U.S. government software approval process to install CapFrameX, a benchmarking tool popular among PC gaming enthusiasts, on its cockpit simulators used to train test pilots. The space agency reached out to CapFrameX, not the other way around, according to an X post from the company. 

NASA builds custom flight simulators from scratch for experimental aircraft like the X-59, a supersonic jet designed to produce a quiet thump rather than the traditional sonic boom. The agency's simulator teams replicate every switch, dial and knob to match the actual cockpit layout, helping pilots build muscle memory before flying the real thing.]]></content:encoded></item><item><title>Educational Byte: What Are Fed Rates and Why Do They Affect Crypto Prices?</title><link>https://hackernoon.com/educational-byte-what-are-fed-rates-and-why-do-they-affect-crypto-prices?source=rss</link><author>Obyte</author><category>tech</category><pubDate>Thu, 22 Jan 2026 21:14:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We’d like to think that crypto is completely independent from traditional money, but that’s not the case yet. Case in point, every time the Federal Reserve (Fed) decides to alter their interest rates, most financial markets react —and that includes cryptocurrencies. Central banks still have enough power to heavily influence global money conditions, and how investors behave based on that.\
You see, we all use loans, and loans often come with interest rates attached. That’s why when borrowing is cheap (because of low rates), money flows with confidence. Now, when borrowing gets expensive, investors turn cautious. Crypto is just another asset in this financial ocean, and it moves with the tide. Let’s see more of how this works.The Federal Reserve is the central bank of the United States, which means it’s the one and only institution in charge to mint and manage the supply of US Dollars (USD). It also supervises the financial firms that use this asset, and tries to keep their economy steady. One of its main tools is the , which dictates how expensive it’s for banks to borrow reserves from each other overnight.When that rate goes up, borrowing feels (for everyone using dollars) like climbing a hill with extra weight. When it goes down, the load eases, and credit moves with more comfort. We may like it or not, but the USD is still the most used currency worldwide. Therefore, changes on its interest rates are never limited to the United States. They have international consequences, shaping costs for companies, investors, and governments alike. Once the Fed sends a signal, money shifts position like a flock of birds changing direction mid-flight.\
Of course, crypto traders are also affected. In periods of low rates, investors feel bolder and lean toward riskier assets (like cryptos, yes). On the contrary, when rates go up, people seek safer assets. Prices quickly reflect this scenario: a chain reaction that starts with central banks and travels through global markets until it reaches every portfolio, including those filled with tokens.Crypto Markets & Rate ChangesRisk assets like cryptocurrencies tend to react fast to any shift in global liquidity. That’s why we see prices going up and down almost immediately after every announcement. For instance, when the Fed pushed rates higher throughout 2022, there was a chill in the markets.  by Yale University found a clear correlation between US monetary policies and digital asset volatility, indeed.We saw a concrete example in December 2022, when the Fed raised its interest rates.  in a matter of hours after the announcement, while the uncertainty of tighter conditions in global markets spread throughout the investment community. \
The opposite case came in July 2024:  that it might hold off on further increases, and global traders welcomed the shift instantly. Crypto prices brightened as confidence returned.\
But wait there, this isn’t just about the US Fed. Crossing the ocean, when the European Central Bank (ECB) adjusts its policies, , too. \
This region is also an important financial center globally, so, when credit conditions here change, investors react. Low rates mean more investments and potential bullish markets. High rates mean the opposite. In the end, crypto is also part of the global economy, and it can move in tandem with it.Crypto Needs IndependenceRemember: central bank rates are temporary, but crypto was built to outlast them. To outlast the fragile fiat system, indeed. These distributed networks were born with the goal to create a method for the transfer of value from one individual to another without the need for a central authority, with neither permission nor censorship by external parties or traditional monetary policies.\
Bitcoin has provided a medium of exchange that’s decentralized, transparent, censorship-resistant, and open to everyone. Networks like Obyte have taken this one step further by eliminating all intermediaries (miners and “validators”), so no one can be an obstacle for their payments and transactions.With the increasing acceptance of digital currencies worldwide, we can say there will be less reliance on interest rate announcements over time. Knowing how today’s central banking operations can affect crypto markets can be useful to avoid panic. However, the long game belongs to the technology, not bank announcements. Prices will always swing, for one reason or another. The noise will fade, and the stronger system will prevail.Featured Vector Image by pinnacleanimates / ]]></content:encoded></item><item><title>Half the World&apos;s 100 Largest Cities Are in High Water Stress Areas, Analysis Finds</title><link>https://news.slashdot.org/story/26/01/22/1749201/half-the-worlds-100-largest-cities-are-in-high-water-stress-areas-analysis-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 20:44:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Half the world's 100 largest cities are experiencing high levels of water stress, with 38 of these sitting in regions of "extremely high water stress," new analysis and mapping has shown. The Guardian: Water stress means that water withdrawals for public water supply and industry are close to exceeding available supplies, often caused by poor management of water resources exacerbated by climate breakdown. Watershed Investigations and the Guardian mapped cities on to stressed catchments revealing that Beijing, New York, Los Angeles, Rio de Janeiro and Delhi are among those facing extreme stress, while London, Bangkok and Jakarta are classed as being highly stressed. 

Separate analysis of NASA satellite data, compiled by scientists at University College London, shows which of the largest 100 cities have been drying or getting wetter over two decades with places such as Chennai, Tehran and Zhengzhou showing strong drying trends and Tokyo, Lagos and Kampala showing strong wetting trends. All 100 cities and their trends can be viewed on a new interactive water security atlas.]]></content:encoded></item><item><title>Section 230 Didn’t Fail Rand Paul. He Just Doesn’t Like the Remedy That Worked.</title><link>https://www.techdirt.com/2026/01/22/section-230-didnt-fail-rand-paul-he-just-doesnt-like-the-remedy-that-worked/</link><author>Ashkhen Kazaryan</author><category>tech</category><pubDate>Thu, 22 Jan 2026 20:34:45 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Rand Paul is furious. That’s because someone posted a video falsely accusing the Kentucky senator of taking money from Venezuela’s Maduro regime.Paul should know that the First Amendment sets a deliberately high bar for defamation of public officials like him. Under , he must show not just falsity, but that the speaker knew it was false or had serious doubts about the validity and published it anyway That demanding standard known as “actual malice” exists for a reason — to ensure that fear of lawsuits does not silence criticism of those who hold power, even when the speech is offensive, wrong, or deeply unfair.Instead of fighting this battle in court against the person who created this video, Paul has redirected his anger toward Section 230, the law often described as the 26 words that created the modern Internet. Although he once defended the law’s provisions that shield online platforms from liability for user speech, Paul now argues in a recent  op-ed that the only solution is to tear it down. At the heart of Paul’s argument is a simple demand: YouTube should have stepped in, judged the accusation against him to be false, and removed it. Once notified that the video was false, the platform should have been legally responsible for leaving it up. Section 230, he argues, prevents that from happening. But who decides what is false? Who decides what is defamatory? And how quickly must those judgments be made — under threat of crushing lawsuits — by platforms hosting speech from millions of users around the world?It’s surprising to see Senator Paul, who’s been vocal against government jawboning of speech, pledge to pursue legislation that would amend the law because a private platform failed to moderate speech the way he wanted.Paul insists this distinction is hypocritical because platforms removed his COVID-era statements they deemed as false while leaving up a lie about him. This argument collapses under its own weight. The Supreme Court has repeatedlyheld that private companies can make editorial decisions. They are allowed to be inconsistent, mistaken, biased, or wrong. As the Court affirmed in “it is no job for government to decide what counts as the right balance of private expression—to ‘un-bias’ what it thinks biased [ . . . ] That principle works for social-media platforms as it does for others.” In other words, the First Amendment protects editorial discretion precisely because the government cannot be trusted with it.If Section 230 protections are rolled back, the consequences could be profound. Some platforms will over-moderate to avoid legal exposure, removing lawful but controversial content. Others will under moderate, allowing harmful content to spread unchecked since any moderation decision could open them up to liability. Such a shift will not harm the powerful but the vulnerable, the dissenters, and the voices that depend on intermediaries to be heard. Smaller platforms and start-ups may shut down,  avoid hosting speech, or change their business models altogether due to litigation risk.Paul draws a comparison between platforms and newspapers, arguing that publishers historically avoided defamation through editorial judgment. But newspapers choose what they print before publication. Platforms host speech created entirely by others, at unimaginable scale. The t is still protected by Section 230 from being liable for the comment section on its online articles.The real, speech-protective answer is defamation law. If Paul believes that a video contains lies about him, he could sue the creator for defamation and prove actual malice under the  standard. But we cannot and should not dismantle the legal foundation of online speech because it failed to protect one powerful man. That sets a precedent that will harm millions of marginalized voices. Ashkhen Kazaryan is a Senior Legal Fellow at The Future of Free Speech, a nonpartisan think tank at Vanderbilt University.]]></content:encoded></item><item><title>Linux GPU Driver Loophole Being Fixed For Unprivileged Users Being Able To Tap Unbounded Kernel Memory</title><link>https://www.phoronix.com/news/Linux-DRM-Blob-OOM</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 20:25:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[An oversight in the Linux kernel's Direct Rendering Manager (DRM) graphics driver common code could allow unprivileged users to trigger unbounded kernel memory consumption for a potential system-wide out-of-memory "OOM" situation...]]></content:encoded></item><item><title>Since Last May, ICE Officers Have Been Told They Don’t Need Warrants To Enter Homes</title><link>https://www.techdirt.com/2026/01/22/since-last-may-ice-officers-have-been-told-they-dont-need-warrants-to-enter-homes/</link><author>Tim Cushing</author><category>tech</category><pubDate>Thu, 22 Jan 2026 20:11:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The thing afforded the highest protections of the Fourth Amendment is a person’s home. This isn’t even a controversial statement. It has been that way ever since this amendment was ratified.But, under Trump, we’re constantly seeing that the administration considers all rights to be privileges — something only granted to people this administration likes. The Associated Press has obtained a blockbuster leak — one that shows ICE officers have been told that they’re free to enter homes without a judicial warrant. Instead, they can just write themselves an  warrant and then just go about their business of terrorizing a nation. ICE carries around things they call warrants, but hardly resemble the real thing. An administrative warrant is self-issuing. The officer who wants to use it only needs to fill in a few blanks and sign it before heading out to try to arrest the person listed on the paperwork. There’s no signature line for supervisors, which means these aren’t reviewed by anyone else but the person writing them.But since last May, ICE officers have been instructed they can treat these pieces of paper like  warrants — you know, the ones that are subjected to at least a cursory review by a judge. The whistleblower report [PDF] contains screenshots of the memo issued by ICE head Todd Lyons, last seen here complaining repeatedly about people who complain about ICE officers acting like paramilitary kidnapping squads. What’s contained in that memo is batshit insane. First of all, it’s the DHS telling itself that it’s okay to ignore the Fourth Amendment.Although the U.S. Department of Homeland Security (DHS) has not historically relied on administrative warrants alone to arrest aliens subject to final orders of removal in their place of residence, the DHS Office of the General Counsel  that the U.S. Constitution, the Immigration and Nationality Act, and the immigration regulations do not prohibit relying on administrative warrants for this purpose. There’s a very good reason the DHS has “not historically relied” on administrative warrants to enter people’s homes in search of arrestable migrants. That reason would be the US Constitution, which only “recently” fell out of favor with the GOP ruling class. According to Lyons and the completely compromised DHS Office of the Legal Counsel, the only thing needed to engage in what is  a warrantless entry is a final order of removal. A couple of paragraphs later, the memo states explicitly what ICE officers are authorized to do under the power of this memo (which  isn’t what they’re authorized to do under the Constitution):Should the alien refuse admittance, ICE officers and agents should use only a necessary and reasonable amount of force to enter the alien’s residence…You can write a memo and issue it and claim the in-house lawyers said it was all cool and legal, but that still doesn’t make it cool and legal. All it does is add another layer of “good faith” to ICE officers’ violations of the Fourth Amendment. After all, if they were told they could do this, how could they be expected to know it was actually illegal? A footnote follows that, which makes it clear ICE officers will engage in warrantless entries even if they  actually obtained a final order of removal.This scoping is not intended to concede that an administrative warrant would be insufficient to arrest an alien in his or her place of residence prior to a final order of removal or where there is a final order of removal issued by an immigration officer. In other words, ICE officers can enter any alleged migrant’s house without a warrant at pretty much any time, so long as they’re carrying their self-issued non-warrants (the Form I-205 referenced throughout the memo). This directly contradicts ERO (Enforcement and Removal Operations) training for ICE officers, which is included in the leaked documents the AP obtained. That training spells it out succinctly and explicitly (caps in the original): An administrative arrest warrant does NOT alone authorize a 4th Amendment search of any kind. That’s no longer the case, apparently. It’s not like this training has been rescinded. It seemingly remains on the books because it creates even more plausible deniability for officers being sued. And ICE director Todd Lyons (along with his OLC enablers) know the contents of this memo can’t possibly be legal. That’s why this memo has never been officially added to ERO training or otherwise officially made part of the ICE operations manual. If Lyons and other top immigration enforcement officials actually thought this shit would hold in court, they wouldn’t have done this: While addressed to “All ICE Personnel,” in practice the May 12 Memo has not been formally distributed to all personnel. Instead, the May 12 Memo has been provided to select DHS officials who are then directed to verbally brief the new policy for action. Those supervisors then show the Memo to some employees, like our clients, and direct them to read the Memo and return it to the supervisor.In the case of the whistleblower who gave this to the Associated Press, they were instructed to read it and return it. They were not allowed to take notes. They were also informed that another employee had been reassigned for questioning ICE policies, which was taken by the whistleblower as the overt threat it is. This is fucking insane. A federal government agency has decided the Fourth Amendment no longer exists and has done everything it can from keeping this clearly unconstitutional policy change from spreading beyond those who’ve already bought into the DHS’s new direction as the expression of the GOP’s white nationalist goals. And it’s a problem that’s only going to get exponentially worse as ICE frantically on-boards new hires, who are given plenty of cash, but nearly nonexistent training before being sent out to fulfill the racist desires of people like White House advisor Stephen Miller. What little they may know (or care) about constitutional rights is being eroded even further by official memos that claim it’s perfectly legal to do something that clearly — under the DHS’s own published training — violates the Fourth Amendment.Without a doubt, the administration will shrug this off and/or tell people they shouldn’t believe things they’ve seen with their own eyes. For now, we can only hope this might knock a few Republicans out of the MAGA loop, even if it’s only the ones who realize they definitely wouldn’t want to turn this unearned expansion of power over to an administration not run by one of their own. ]]></content:encoded></item><item><title>Moderna Curbing Investments in Vaccine Trials Due To US Backlash, CEO Says</title><link>https://science.slashdot.org/story/26/01/22/1743240/moderna-curbing-investments-in-vaccine-trials-due-to-us-backlash-ceo-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Moderna does not plan to invest in new late-stage vaccine trials because of growing opposition to immunizations from U.S. officials, CEO Stephane Bancel said in an interview with Bloomberg TV on Thursday. "You cannot make a return on investment if you don't have access to the U.S. market," Bancel told Bloomberg TV on the sidelines of the World Economic Forum in Davos. Bancel said regulatory delays and little support from the authorities make the market size "much smaller."]]></content:encoded></item><item><title>Ring is adding a new content verification feature to videos</title><link>https://techcrunch.com/2026/01/22/ring-is-adding-a-new-content-verification-feature-to-videos/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 22 Jan 2026 19:50:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Ring says the new feature will identify if video edits have been made, even if they're minor. ]]></content:encoded></item><item><title>Google DeepMind CEO is ‘surprised’ OpenAI is rushing forward with ads in ChatGPT</title><link>https://techcrunch.com/2026/01/22/google-deepmind-ceo-is-surprised-openai-is-rushing-forward-with-ads-in-chatgpt/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 22 Jan 2026 19:41:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google DeepMind CEO Demis Hassabis says the tech giant isn't pressuring him to insert ads into the AI chatbot experience. ]]></content:encoded></item><item><title>Humans&amp; thinks coordination is the next frontier for AI, and they’re building a model to prove it</title><link>https://techcrunch.com/2026/01/22/humans-thinks-coordination-is-the-next-frontier-for-ai-and-theyre-building-a-model-to-prove-it/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Thu, 22 Jan 2026 19:24:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Humans&, a new startup founded by alumni of Anthropic, Meta, OpenAI, xAI, and Google DeepMind, is building the next generation of foundation models for collaboration, not chat. ]]></content:encoded></item><item><title>eBay Bans Illicit Automated Shopping Amid Rapid Rise of AI Agents</title><link>https://slashdot.org/story/26/01/22/1738221/ebay-bans-illicit-automated-shopping-amid-rapid-rise-of-ai-agents?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 19:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[EBay has updated its User Agreement to explicitly ban third-party "buy for me" agents and AI chatbots from interacting with its platform without permission. From a report: On its face, a one-line terms of service update doesn't seem like major news, but what it implies is more significant: The change reflects the rapid emergence of what some are calling "agentic commerce," a new category of AI tools designed to browse, compare, and purchase products on behalf of users. 

eBay's updated terms, which go into effect on February 20, 2026, specifically prohibit users from employing "buy-for-me agents, LLM-driven bots, or any end-to-end flow that attempts to place orders without human review" to access eBay's services without the site's permission. The previous version of the agreement contained a general prohibition on robots, spiders, scrapers, and automated data gathering tools but did not mention AI agents or LLMs by name.]]></content:encoded></item><item><title>Scientists Got Men to Rate Penises by How Intimidating They Are. This Is What They Found.</title><link>https://www.404media.co/scientists-got-men-to-rate-penises-by-how-intimidating-they-are-this-is-what-they-found/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/01/image1-3.jpg" length="" type=""/><pubDate>Thu, 22 Jan 2026 19:00:38 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[, our newsletter about the most exciting and mind-boggling science news and studies of the week. When it comes to the evolution of the human penis, size matters. Scientists have discovered that men with larger penises are not only more attractive to women, they are also deemed more threatening to men, which is “the first experimental evidence that males assess rivals’ fighting ability and attractiveness to females based partly on a rival’s penis size,” according to a study published in PLOS Biology on Thursday. “In humans, height and body shape are well known to influence attractiveness, but penis size has rarely been tested alongside these traits in a controlled, experimental setup,” said Upama Aich, a behavioral and evolutionary biologist at the University of Western Australia who led the study, in an email to 404 Media. “What motivated us was the evolutionary puzzle that the human penis is unusually large relative to other primates, which raises the question of whether it signals information beyond its primary reproductive role of sperm transfer,” she added.Sexual selection, a form of natural selection, is a process in which certain traits that enhance reproductive success—from big antlers to colorful feathers—become amplified in a lineage over time. Male traits may persist both because they are selected by females, which is known as intersexual selection, or because those traits are associated with better success against male rivals, which is called intrasexual selection.Previous research  that bigger penises are more attractive to women, in tandem with characteristics like height and body shape, suggesting that intersexual selection may have played a role in the anomalously large human penis. Aich and her colleagues set out to confirm that result, while also testing out the role of intrasexual selection for the first time.The researchers recruited more than 600 male and 200 female participants to rate computer-generated male figures with different heights, body shapes, and penis sizes (all shown in a flaccid state). Some participants attended an in-person display of life-size images while others rated the figures on an online platform. Men were asked to assess the figures as potential rivals, while women were asked to rate them as potential mates. Participants also filled out a questionnaire about their physical characteristics (including height and weight) and sexuality. Given the focus on mates and rivals, the researchers only used responses from self-identified heterosexual males and females in the study. The team designed the approach with nondescript figures devoid of any personality or identifiable background in part to sidestep the immense cultural weight of the human penis, an anatomical feature endowed with major significance across eras and societies. “We were very conscious that penis size is culturally loaded and surrounded by myths, humour, and anxiety,” said Aich. “That’s one reason we used anatomically accurate, computer-generated figures: it allowed us to manipulate specific traits independently while controlling for personal identity, social narratives and contextual cues.” “I do think this cultural baggage has discouraged careful scientific study in sensitive topics in the past, but from an evolutionary perspective, that makes it even more important to examine the question empirically rather than relying on assumptions,” she added.To that end, the new study confirmed that women generally preferred figures with larger penises in addition to taller figures with more V-shaped bodies. It also revealed for the first time that men factored penis size into their assessment of male rivals, as they rated the figures with larger penises as more threatening rivals. Even more importantly, the men overwhelmingly guessed that the figures with larger penises would be more attractive to women. According to the researchers, this hints that in our evolutionary past, males may have avoided confrontations with rivals based in part on their penis size in addition to height and body shape. As a consequence, males with larger penises may have secured more access to mates not only due to female preference, but also because they were not challenged by rivals as often. This aspect of male-male competition may have helped to enlarge the human penis over time through selection.“Previous research had often focused on the effect of penis size on female preferences, so our results that men also use penis size when assessing rivals adds a new dimension to the story,” Aich said. “It suggests penis size is interpreted not only in a sexual context, but also in competitive rival cues.” “However, the effect of penis size on attractiveness was four to seven times higher than its effect as a signal of fighting ability,” she continued. “This suggests that the enlarged penis in humans may have evolved more in response to its effect as a sexual ornament to attract females than as a badge of status for males, although it does both.”Aich said her team was most surprised by the consistency of the participants’ responses across many manipulated variables. Similar patterns in the responses showed up regardless of whether the participants were viewing life-sized projections or scaled images online, whether they received payment for the experiment, and across both male and female participants.“One obvious next step is to study how these visual cues interact with others that matter in real-world interactions, such as facial features, voice, or movement,” she said. “Another open question is how culturally variable these perceptions are, since standards of masculinity and attractiveness differ across societies. A cross-cultural study would be interesting.”The new study adds to the evidence that both forms of sex selection influenced the size of the human penis, but many other factors also played a role in the development of the organ. For example, penis shape and size may have evolved to scoop the sperm of rival males out of the vaginal canal, or to raise the odds of female orgasm, both of which can contribute to reproductive success. In other words, both the size of the ship and the motion of the ocean are a part of the complex story of human sexual evolution. , our newsletter about the most exciting and mind-boggling science news and studies of the week. ]]></content:encoded></item><item><title>Substack launches a TV app</title><link>https://techcrunch.com/2026/01/22/substack-launches-a-tv-app/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 22 Jan 2026 18:58:50 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The move comes as Substack has been investing more heavily in video and livestreaming, as it looks to compete with platforms like YouTube and Patreon for both creators and viewers.]]></content:encoded></item><item><title>Tesla launches robotaxi rides in Austin with no human safety driver</title><link>https://techcrunch.com/2026/01/22/tesla-launches-robotaxi-rides-in-austin-with-no-human-safety-driver/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Thu, 22 Jan 2026 18:51:43 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Not all of Tesla's fleet in Austin will be fully driverless. Per Tesla's AI lead Ashok Elluswamy, the company will be "starting with a few unsupervised vehicles mixed in with the broader robotaxi fleet with safety monitors, and the ratio will increase over time."]]></content:encoded></item><item><title>Daily Deal: The JavaScript DOM Game Developer Bundle</title><link>https://www.techdirt.com/2026/01/22/daily-deal-the-javascript-dom-game-developer-bundle-8/</link><author>Daily Deal</author><category>tech</category><pubDate>Thu, 22 Jan 2026 18:51:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The JavaScript DOM Game Developer Bundle has 8 courses to help you master coding fundamentals. Courses cover JavaScript DOM, Coding, HTML 5 Canvas, and more. You’ll learn how to create your own fun, interactive games. It’s on sale for $30.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Workday CEO Calls Narrative That AI is Killing Software &apos;Overblown&apos;</title><link>https://tech.slashdot.org/story/26/01/22/1734213/workday-ceo-calls-narrative-that-ai-is-killing-software-overblown?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 18:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Workday CEO Carl Eschenbach on Thursday tried to ease worries that AI is destroying software business models. From a report: "It's an overblown narrative, and it's not true," he told CNBC's "Squawk Box" from the World Economic Forum in Davos, Switzerland, calling AI a tailwind and "absolutely not a headwind" for the company. 

Software stocks have sold off in recent months on concerns that new AI tools will upend the sector and displace longstanding and recurring businesses that once fueled big profits. Workday shares lost 17% last year and have sunk another 15% since the start of 2026.]]></content:encoded></item><item><title>Meta seeks to limit evidence in child safety case</title><link>https://techcrunch.com/2026/01/22/meta-seeks-to-limit-evidence-in-child-safety-case/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Thu, 22 Jan 2026 18:40:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta is set to go on trial in New Mexico for allegedly failing to protect minors. The company is requesting a broad swath of information not be allowed to be presented in court.]]></content:encoded></item><item><title>Google now offers free SAT practice exams, powered by Gemini</title><link>https://techcrunch.com/2026/01/22/google-now-offers-free-sat-practice-exams-powered-by-gemini/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Thu, 22 Jan 2026 18:27:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Students can prompt Gemini by typing "I want to take a practice SAT test,” and the AI will provide them with a free practice exam. Gemini then analyzes the results, highlighting strengths and identifying areas that need further review. It also offers detailed explanations for any incorrect answers. ]]></content:encoded></item><item><title>Schools, Airports, High-Rise Towers: Architects Urged To Get &apos;Bamboo-Ready&apos;</title><link>https://news.slashdot.org/story/26/01/22/1730215/schools-airports-high-rise-towers-architects-urged-to-get-bamboo-ready?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 18:05:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: An airport made of bamboo? A tower reaching 20 metres high? For many years, bamboo has been mostly known as the favourite food of giant pandas, but a group of engineers say it's time we took it seriously as a building material, too. 

This week the Institution of Structural Engineers called for architects to be "bamboo-ready" as they published a manual for designing permanent buildings made of the material, in an effort to encourage low-carbon construction and position bamboo as a proper alternative to steel and concrete. 

Bamboo has already been used for a number of boundary-pushing projects around the world. At Terminal 2 of Kempegowda international airport in Bengaluru, India, bamboo tubes make up the ceiling and pillars. The Ninghai bamboo tower in north-east China, which is more than 20 metres tall, is claimed to be the world's first high-rise building made using engineered bamboo. 

At the Green School in Bali, a bamboo-made arc serves as the gymnasium and a striking example of how the material is reshaping sustainable architecture. The use of composite bamboo shear walls have proved to be resilient against earthquakes and extreme weather in countries such as Colombia and the Philippines, where sustainable, disaster-resilient housing has been built with locally sourced materials.]]></content:encoded></item><item><title>Former CEO of celeb fav gym Dogpound launches $5M fund to back wellness companies</title><link>https://techcrunch.com/2026/01/22/former-ceo-of-celeb-fav-gym-dogpound-launches-5m-fund-to-back-wellness-companies/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Thu, 22 Jan 2026 18:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Jenny Liu is a solo first-time GP looking to back underrepresented wellness founders. ]]></content:encoded></item><item><title>DOJ Admits DOGE Team Caught Sharing Social Security Data With Election Denier Group</title><link>https://www.techdirt.com/2026/01/22/doj-admits-doge-team-caught-sharing-social-security-data-with-election-denier-group/</link><author>Mike Masnick</author><category>tech</category><pubDate>Thu, 22 Jan 2026 17:33:28 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[We spent a lot of time last year calling out how dangerous it was that Elon Musk and his inexperienced 4chan-loving DOGE boys were gaining access to some of the most secure government systems. We also highlighted how it seemed likely that they were violating many laws in the process. One specific point of concern was DOGE’s desire to take control over Social Security data, something that many people warned would be abused for political reasons, in particular to make misleading or false claims about voting records.For all the people who insisted that this was hyperbolic nonsense, and DOGE was just there to root out “waste, fraud, and abuse,” well… the DOJ last week quietly admitted that the DOGE boys almost certainly violated the Hatch Act and had given social security data to conspiracy theorists claiming Trump won the 2020 election (he did not).Oh, and this only came out because the DOJ realized it had lied to a court (they claim it was because the Social Security Administration officials had given them bad info, but the net effect is the same) and had to correct the record.Shapiro’s previously unreported disclosure, dated Friday, came as part of a list of “corrections” to testimony by top SSA officials during last year’s legal battles over DOGE’s access to Social Security data. They revealed that DOGE team members shared data on unapproved “third-party” servers and may have accessed private information that had been ruled off-limits by a court at the time.Shapiro said the case of the two DOGE team members appeared to undermine a previous assertion by SSA that DOGE’s work was intended to “detect fraud, waste and abuse” in Social Security and modernize the agency’s technology.Also in his March 12 declaration, Mr. Russo attested that, “[t]he overall goal of the work performed by SSA’s DOGE Team is to detect fraud, waste and abuse in SSA programs and to provide recommendations for action to the Acting Commissioner of SSA, the SSA Office of the Inspector General, and the Executive Office of the President.”….However, SSA determined in its recent review that in March 2025,a political advocacy group contacted two members of SSA’s DOGE Team with a request to analyze state voter rollsthat the advocacy group had acquired.The advocacy group’s stated aim was to find evidence of voter fraud and to overturn election resultsin certain States. In connection with these communications,one of the DOGE team members signed a “Voter Data Agreement,” in his capacity as an SSA employee, with the advocacy group. He sent the executed agreement to the advocacy group on March 24, 2025.The filing goes on to admit that the declaration from a Social Security administration employee that there were safeguards in place against sharing data, and that everyone had received training in not sharing data, was apparently wrong.However, SSA has learned that, beginning March 7, 2025, and continuing until March 17 (approximately one week before the TRO was entered), members of SSA’s DOGE Team were using links to share data through the third-party server “Cloudflare.” Cloudflare is not approved for storing SSA data and when used in this manner is outside SSA’s security protocols. SSA did not know, until its recent review, that DOGE Team members were using Cloudflare during this period. Because Cloudflare is a third-party entity, SSA has not been able to determine exactly what data were shared to Cloudflare or whether the data still exist on the server.Cool cool. No big deal. DOGE boys just put incredibly private data on a third party server and no one knows what data was there or even Have I got some waste, fraud, and abuse for you to check out!Separately, the filing reveals that Elon Musk’s right hand man, Steve Davis—the “fixer” Musk deploys across all his organizations—was copied on an email containing an encrypted file of SSA data. The filing is careful to note that DOGE itself “never had access to SSA systems of record,” but that’s a distinction without much difference when your guy is getting emailed password-protected files derived from those systems. Oh and: SSA still can’t open the file to figure out exactly what was in it.However, SSA has determined that on March 3, 2025—three weeks prior to entry of the TRO—an SSA DOGE Team member copied Mr. Steve Davis, who was then a senior advisor to Defendant U.S. DOGE Temporary Organization, as well as a DOGE-affiliated employee at the Department of Labor (“DOL”), on an email to Department of Homeland Security (“DHS”). The email attached an encrypted and password-protected file that SSA believes contained SSA data. Despite ongoing efforts by SSA’s Chief Information Office, SSA has been unable to access the file to determine exactly what it contained. From the explanation of the attached file in the email body and based on what SSA had approved to be released to DHS, SSA believes that the encrypted attachment contained PII derived from SSA systems of record, including names and addresses of approximately 1,000 people.Looks like some more waste, fraud, and abuse right there.So to recap: the team that stormed in to root out “waste, fraud, and abuse” committed what looks an awful lot like  fraud and abuse—sharing data on unauthorized servers, misleading courts, cutting deals with election conspiracy groups, and emailing around encrypted files of PII that the agency itself can’t even open anymore. All of it now documented in federal court filings—not that anyone will do anything about it. Accountability is for people who don’t have Elon Musk on speed dial.]]></content:encoded></item><item><title>China Lagging in AI Is a &apos;Fairy Tale,&apos; Mistral CEO Says</title><link>https://news.slashdot.org/story/26/01/22/172240/china-lagging-in-ai-is-a-fairy-tale-mistral-ceo-says?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Claims that Chinese technology for AI lags the US are a "fairy tale," Arthur Mensch, the chief executive officer of Mistral, said. From a report: "China is not behind the West," Mensch said in an interview on Bloomberg Television at the World Economic Forum in Davos, Switzerland on Thursday. The capabilities of China's open-source technology is "probably stressing the CEOs in the US." 

The remarks from the boss of one of Europe's leading AI companies diverge from other tech leaders at Davos, who reassured lawmakers and business chiefs that China is behind the cutting edge by months or years.]]></content:encoded></item><item><title>Struggling fusion power company General Fusion to go public via $1B reverse merger</title><link>https://techcrunch.com/2026/01/22/struggling-fusion-power-company-general-fusion-to-go-public-via-1b-reverse-merger/</link><author>Tim De Chant</author><category>tech</category><pubDate>Thu, 22 Jan 2026 17:00:25 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[General Fusion's merger with an acquisition company will net the company over $300 million. Just last year, the company ran into trouble raising money from other investors.]]></content:encoded></item><item><title>Intel Xeon 6780E &quot;Sierra Forest&quot; Linux Performance ~14% Faster Since Launch</title><link>https://www.phoronix.com/review/intel-xeon6-sierra-forest-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 17:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As part of my end-of-year 2025 benchmarking I looked at how the Intel Xeon 6980P Granite Rapids performance evolved in the year since launch and seeing some nice open-source/Linux optimizations during that time. On the other side of the table were also benchmarks of how AMD EPYC 8004 Sienna evolved in its two years, Ubuntu 24.04 vs. 26.04 development for AMD EPYC Turin, the AMD EPYC Milan-X in its four years since launch, and also a look at the performance evolution lower down the stack with the likes of sub-$500 laptop hardware. Out today is a fresh look at how the Intel Xeon 6780E Sierra Forest has evolved in its one and a half years since its launch.]]></content:encoded></item><item><title>Waymo continues robotaxi ramp up with Miami service now open to public</title><link>https://techcrunch.com/2026/01/22/waymo-continues-robotaxi-ramp-up-with-miami-service-now-open-to-public/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Thu, 22 Jan 2026 16:59:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Waymo is opening its driverless robotaxis to the public in Miami, starting with a 60-square-mile service area and plans to reach the airport "soon."]]></content:encoded></item><item><title>Ireland proposes new law allowing police to use spyware</title><link>https://techcrunch.com/2026/01/22/ireland-proposes-new-law-allowing-police-to-use-spyware/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Thu, 22 Jan 2026 16:49:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Irish government announced that it wants to pass a law that would grant police more surveillance powers, such as using spyware to fight serious crime, while aiming to protect the privacy rights of its citizens. ]]></content:encoded></item><item><title>Autodesk To Cut 1,000 Jobs</title><link>https://slashdot.org/story/26/01/22/1641228/autodesk-to-cut-1000-jobs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 16:44:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Autodesk said today it plans to cut approximately 1,000 jobs, or roughly 7% of its workforce, as part of what the company described as the final phase of a global restructuring effort aimed at strengthening its sales and marketing operations. 

The maker of AutoCAD and other digital design software said a significant portion of the cuts will fall within customer-facing sales functions.]]></content:encoded></item><item><title>Aliens and Angel Numbers: Creators Worry Porn Platform ManyVids Is Falling Into ‘AI Psychosis’</title><link>https://www.404media.co/manyvids-porn-platform-ai-psychosis-bella-french-bio/</link><author>Samantha Cole</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/01/manyvids--1-.png" length="" type=""/><pubDate>Thu, 22 Jan 2026 16:20:38 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[In posts on ManyVids, the porn platform’s official account holds imaginary conversations with aliens, alongside AI-generated videos of UFOs, fractal images, “angel numbers,” and a video of its founder and CEO Bella French in a space suit shooting lasers from her eyes.  French launched the site in 2014 as a former cam model herself, and the platform has  and tens of thousands of creators. Adult content creators use it to sell custom videos and subscriptions, and perform live on camera. French recently changed her personal website to state her new goal is to “transition one million people out of the adult industry and do everything we can to ensure no one new enters it.” The statement follows posts on X’s ManyVids account about new strategies to pivot the site toward safe-for-work, non-sexual content.This sudden shift away from years of messaging about being a compatriot with sex workers, combined with bizarre AI-generated text and images about talking to aliens and numerology on social media, has made some creators worry for their livelihoods, and caused others to leave the site completely.For years, the official ManyVids social media accounts made mostly normal posts that promoted the site and its creators. But in mid-2025, the posts from the ManyVids X account changed. Instead of promotions of top creators, announcements of contests, and tips for using the platform, the account shifted its focus to existential and metaphysical musings. Around August, it started posting cryptic quotes, phrases, and images, many seemingly generated by or about AI. The account also started replying to engagement-farming posts from influencers, writing things like “Our purpose: to protect the feminine energy — so that balance may return,” and posting borderline-nonsensical bullet-point lists about “the boldness scale” and how ManyVids leadership is “all connected.” “The impact strength of a positive leader ⚡ Effectiveness ⚡ Execution ⚡ Discipline ⚡ Accountability,” one  in August said. On August 20, @ManyVids posted an image on X of a flow chart alongside a screenshot of a ChatGPT conversation, seemingly illustrating how the platform would bring in users through a “safe-for-work” zone, then allow them to access NSFW content after verifying their identifications. “Our vision: Adult Industry 2.0 isn’t about more revenue. It’s about evolution,” the post said. The replies to these posts show ManyVids creators expressing anger, concern, and bafflement. The account stopped posting on X in September. But on the ManyVids platform itself, which has a “news” feed that functions similarly to a microblogging platform but is just for official platform posts, the odd entries continue.Do you know anything else about what's happening at ManyVids, or do you have a tip about porn platforms and online sex work generally? I would love to hear from you. Using a non-work device, you can message me securely on Signal at sam.404. Otherwise, send me an email at sam@404media.co.“Social API for the AI Age. Phase 1 — Pride Engine,” one post from January 16 says: “The High Universal Income (HUI) Engine is the distribution hub of the new economy, built for a world where AI does the work humans never wanted to do. AI generates surplus wealth, but humans need surplus purpose. Human meaning becomes the rarest and most valuable resource on Earth. Instead of opaque taxes, AI companies fund a Social License through platforms like ManyVids, converting AI efficiency into merit-based bonuses for human contribution. For every dollar earned through passion, creation, care, or learning, HUI adds 10%. This is not charity. It is a Pride Engine. We shift the foundation of human value.”The post ends with a six-second AI generated video that includes the phrase “the ultimate guide to rebuilding civilization.” Most posts in recent weeks are like this: clearly AI generated text alongside six-second AI generated clips showing angels, chakras, or spiritual phrases. “The Simulation of Integrity. If we don’t fully understand the ultimate nature of reality, what should guide how we live inside it?” one recent post says. “If the nature of the ‘game’ is unknown, then how you treat others — and yourself — becomes the most meaningful data point.” And in a post right after the new year: “Hey everyone! Back-to-the-office Monday vibe. How were your holidays? Did you travel anywhere? I did... 🕳️Next time, I’ll bring sunglasses. I came back with a few new ideas and fresh thoughts ✨Let’s get to work. Let’s go, 2026! 🚀” Below the text: a video of French in a space suit, black hole in the background, shooting laser-lightning out of her eyes.A lot of people who rely on ManyVids for income have noticed this odd behavior and are disturbed by it. “Ethical dilemmas about AI aside, the posts are completely disconnected with ManyVids as a site,” one ManyVids content creator told 404 Media, on the condition of anonymity. “Their customers and their creators are not served in any way by these. When faced with backlash, MV removed the ability to comment on posts. To anyone looking at them they appear to be ramblings and images generated by a person in active psychosis.” Almost every ManyVids creator 404 Media spoke to for this story brought up “AI psychosis” unprompted, when asked if they’d seen the ManyVids posts. “I have seen them and I find them really insulting,”  said. “The way I perceive the posts is that Bella and the MV team doesn't respect their creators enough to spend time making their own content, instead taking the easy way out and using bizarre AI that doesn't even relate. Why do we need Bella shooting laser beams out of her eyes to make an announcement? It's infuriating because it's like she doesn't take us seriously, doesn't take her own platform seriously, and we're supposed to just be grateful for the crumbs she's giving us. We deserve better,” she said. “We deserve to be treated with respect, talked to like we're adults, and listened to like our voices matter. Instead we get AI slop and posts that promise big things without any sort of follow through.”  Harlan Paramore, a ManyVids creator who also helps other creators onboard and manage their selling sites, said he’s noticed “bizarre posts about AI, angel numbers, christopaganism, cyberpaganism.” “I don't have anything against any of those beliefs, but they seem wildly out of place for an official site blog. They are also heavily loaded with AI-like language and structure, and decorated with AI images,” Paramore said. “I'm also a professional artist, and as both an artist and sex worker I'm frustrated and confused. Some of it kind of sounds like AI psychosis, too, which has me concerned for whoever is running that blog.” “I'm not a mental health professional, but whatever Bella is going through doesn't seem normal. It doesn't seem healthy,” Screams said. “From where I'm sitting, if I were close to Bella, I'd be reaching out to her other friends and family members to stage an intervention and try to get her serious mental health care.” All of this is coinciding with an apparent massive change in French’s ideology toward sex work. On , French says the goal of ManyVids is changing to “transition one million people out of the adult industry.” She calls sex work “exploitative.” Her bio quotes her as saying: “I had two choices: surrender to an exploitative industry or dismantle it. I chose to build its replacement... ManyVids was the result—the most efficient revenue-distribution engine for the AI-displaced workforce. Guided by first principles and core value thinking, Bella is leading MV’s next evolution: a Fintech/Social-Impact hybrid that turns digital presence into economic creation. By utilizing AI-integrated workflows and layered access, ManyVids is migrating creators from adult content into a diversified creative economy,” her bio says. “Our goal is to transition one million people out of the adult industry and do everything we can to ensure no one new enters it. We are working to transform an industry we don’t believe should exist—but we recognize that simple elimination creates deeper shadows. The solution is elevation through meaningful alternatives.” This is a recent addition to her website. According to archived versions of the site, the section about transitioning people out of the sex industry wasn’t there in November 2025. “ManyVids is now becoming a regulated e-social ecosystem — a digital space that sensitizes, elevates, and restricts adult content through layered brackets of access,” French’s bio says now. “This ensures that sacred sexual expression is never free, never exploited, and never divorced from its core human depth.” The “layered brackets” seem to be a reference to the ChatGPT screenshots from August 20. This is an extreme departure in tone from what French has said was her mission with ManyVids in the past. In 2019, I met French for an on-background hotel room meeting during the porn industry’s biggest award show and conference, AVN, where she told me she created ManyVids out of a passion to create a platform where other sex workers—having been an adult content creator herself—would be treated fairly and would be listened to by the platform’s owners. French is a former cam model herself, and has always been open publicly about wanting to create better platforms for other sex workers.“Their customers and their creators are not served in any way by these." “We try to offer sex workers the tools to be more successful as independent entrepreneurs without being judged,” French told the Daily Beast in 2019. “What was really important for me was to educate the world and make them realize that porn stars are not stupid.”Shortly after she and I met in 2019, French agreed to a written interview as part of a VICE story about authenticity in cam work. In that email, she called camming the “biggest gift” she’d ever received. “Being a camgirl not only has a huge influence on my approach to taking business decisions but has changed the way I view people and life in general,” French wrote at the time. “Every single decision we take at ManyVids must answer 1 simple question, ‘Will this help the content creators, our MV Stars?’ That’s it,” French wrote in 2019. “If the answer is yes then we proceed, regardless if there is any financial advantage or potential for profit, that is irrelevant.” Platforms have long profited off of sex workers and pornography to establish popularity and rake in revenue before eventually doing a heel-turn on the creators who made them successful. We’ve seen it happen with mainstream social media platforms like Tumblr, Instagram, and Twitter, and also on sites ostensibly made for sex workers, like OnlyFans, which nearly changed its policies to ban explicit material after making billions of dollars off their content.  I asked ManyVids and French if the platform is changing to reflect these social media posts and her statements on her bio, who is making the AI-generated posts mentioned above, how French plans to “transition one million people” out of sex work, and if any of this will affect creators and fans who use ManyVids. The ManyVids support team did not answer these questions specifically, but sent the following response (emphasis theirs): "Hello, thanks for reaching out. Respect for Online Sex Workers. Sex work is real work. No more living in the shadows, no more being misunderstood.No more being afraid, shadowbanned, or persecuted by systems and institutions. Not on our watch. We are not victims — and we are taking action now.This generation of online sex workers is about to change the game forever —and transform the oldest profession in the world in the right direction,  Respect the creators. Respect the work. Respect what you watch. We stand for safety, dignity, and opportunity for all creators."I asked ManyVids to explain in specific terms what "we are taking action now" means. They replied: "A post will be published to our ManyVids News feed this Saturday, January 24th. It will provide additional clarification and go into a bit more detail on this," with a link to the feed.“It concerns me that access to my earnings, and more importantly my personal information, is in the hands of someone seemingly out of touch with reality.” In the meantime, creators have been confused and worried for weeks. Nothing has changed about the way the site operates publicly or creators’ payouts as of writing, but this is a series of events that many adult content creators are concerned represents a potential threat to their livelihood.“If something were to happen to MV (or to my account there) due to what can only be described as AI psychosis, I would lose upwards of 14k per year—a not insignificant amount of income,” another adult creator on ManyVids told 404 Media. “It concerns me that access to my earnings, and more importantly my personal information, is in the hands of someone seemingly out of touch with reality.” ManyVids takes a larger-than-most cut from creators' profits, depending on the type of content: For videos and contest earnings (which are similar to tips), the platform takes 40 percent. On tips and custom video sales, it takes 20 percent, which is more in line with other adult platforms. This has been a source of complaint from creators for a long time, combined with unpredictable algorithms that creators say change how they’re discovered on the platform and what content performs best, impacting their earnings. Users have expressed dissatisfaction with these aspects of the platform, and how French runs it, for years. But the recent turn to AI and French’s statements about the industry are making some wonder if it’s time to leave. “I will still be using ManyVids for NSFW content for as long as they allow it,” adult content creator August told 404 Media. “But part of me thinks that they will try to do what OnlyFans did years ago and try to ban NSFW content which would be an absolute disaster for sex workers whose income depends on platforms like ManyVids.” Luna Sapphire, a creator who has been using the platform since 2015, said she finds French’s statements on her website “harmful and insulting” to those who’ve helped popularize the site from the start. “Most of us are not looking for a path out of the adult industry; we simply want to do our jobs with as little interference and censorship as possible,” Sapphire said. “Bella used to be very pro-sex worker and it is disappointing to see her change her tune.”Several adult platforms have embraced, or at least allowed, AI-generated content and “models” on their sites alongside human creators in the last few years. On OnlyFans, AI-generated is allowed, but must comply with the site’s terms of service and and “must be clearly and conspicuously captioned as AI Generated Content with a signifier such as #ai, or #AIGenerated,” Onlyfans  in its terms. Fansly, another adult platform for independent creators,  “photorealistic AI-generated content” but allows non-photorealistic “virtual entities” (like V-tubers) if they’re registered using the uploader’s real legal information for verification purposes. JustForFans requires that “consent, identity, and proof of age must be established if the AI images are based on a real person's likeness,” and allows deepfakes if consent has been established. “For example, you can use your own face to create images of yourself or a model who has granted consent to use their face,” the platform’s  IWantClips, another site for selling custom content,  users making AI-generated models to verify their identities, but explicitly doesn’t allow deepfakes. , IWantClips awarded an AI-generated model $1,000 as the winner of a Valentine’s Day-themed contest. “Adora” competed in the contest alongside human sex workers. On most of these sites, engagement and attention are currency, and on ManyVids, AI generated models sell content alongside humans. The platform  “AI-generated or deepfake content that misrepresents real individuals without consent,” as part of its terms that forbid “content that violates any third party's intellectual property rights or another individual's privacy.”“The AI/intense spirituality path has been so strange to witness, and I can’t imagine what it’s leaving the fans to think,” Elizabeth Fields, an adult content creator who’s used ManyVids for six years, told 404 Media. “I don’t understand what they are trying to do by taking this direction, nor do I understand how it’s fair of a sexwork built site to assume all of us don’t want to do NSFW content–and to try and funnel us into this box of ‘not enjoying the work we do. To an extent it feels degrading honestly—just because Bella’s experience in sex work was survival based and to make ends meet—a lot of us thoroughly enjoy our jobs, the path we took, and want to continue doing this.” Many sex workers are disabled, neurodivergent, mentally ill, chronically ill, or “all of the above,” Fields noted, and rely on online sex work to pay the bills. “It feels absolutely unfair to feel like we could be pushed off of a site that became popular off OUR NSFW content—because they want to make it more SFW, and implement all these new AI features that will quite frankly just turn clients off.” Despite all of this, Fields said she won’t be leaving the site. “To the point that as much as I'm extremely disappointed with many of the recent changes occurring, I won’t be deleting my account as to not lose that income and disappoint my ManyVids fans.” Others are done. Sydney Screams said she’s no longer uploading to ManyVids and made the decision to slowly start removing content from her stores there. “Platforms that allow for online sex work should be working FOR us, not against us. Sex workers use platforms like MV to earn our own living, to enable ourselves to have better lives, to keep ourselves housed and fed, to pay for medical bills, etc. Many of us choose this life and choose to make this our career, though there are far too many who are survival sex workers,” Screams said. “We aren't looking for a pathway out of the adult industry, especially on a platform that is a porn platform!!! Unless MV is going to start funding the educations & trainings of those trying to leave the industry for work elsewhere, I do not see how a porn platform is going to create a path out of the industry.” Emanuel Maiberg contributed reporting to this story.]]></content:encoded></item><item><title>What a Sony and TCL Partnership Means For the Future of TVs</title><link>https://entertainment.slashdot.org/story/26/01/22/168240/what-a-sony-and-tcl-partnership-means-for-the-future-of-tvs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 16:08:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[How would Sony ceding control of its TV hardware business change the industry? The Verge has an optimistic take: [...] As of today, Sony already relies on different manufacturing partners to create its TV lineup. While display panel manufacturers never reveal who they sell panels to, Sony is likely already using panels for its LCD TVs from TCL China Star Optoelectronics Technology (CSOT), in addition to OLED panels from LG Display and Samsung Display. With this deal, a relationship between Sony and TCL CSOT LCD panels is guaranteed (although I doubt this would affect CSOT selling panels to other manufacturers). And with TCL CSOT building a new OLED facility, there's a potential future in which Sony OLEDs will also get panels from TCL. Although I should point out that we're not sure yet if the new facility will have the ability to make TV-sized OLED panels, at least to start. 

[...] There's some concern from fans that this could lead to a Sharp, Toshiba, or Pioneer situation where the names are licensed and the TVs produced are a shell of what the brands used to represent. I don't see this happening with Sony. While the electronics side of the business hasn't been as strong as in the past, Sony -- and Bravia -- is still a storied brand. It would take a lot for Sony to completely step aside and allow another company to slap its name on an inferior product. And based on TCL's growth and technological improvements over the past few years, and the shrinking gap between premium and midrange TVs, I don't expect Sony TVs will suffer from a partnership with TCL.]]></content:encoded></item><item><title>The HackerNoon Newsletter: 5 Risks You Have To Take as a Leader (1/22/2026)</title><link>https://hackernoon.com/1-22-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Thu, 22 Jan 2026 16:02:54 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, January 22, 2026?By @burvestorylab [ 7 Min read ] Swap Word for an IDE: use VS Code, Markdown, Git version control/branches, and built-in AI to draft, revise, and safeguard your novel like a developer. Read More.By @giovannicoletta [ 12 Min read ] A school killing in Italy exposes deeper failures in education and politics. Data, AI, and debate-based learning may offer a long-term solution.  Read More.By @vinitabansal [ 12 Min read ] Here are the 5 risks every leader must take daily because it’s impossible to get better at anything without consistent practice. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Google’s AI Mode can now tap into your Gmail and Photos to provide tailored responses</title><link>https://techcrunch.com/2026/01/22/googles-ai-mode-can-now-tap-into-your-gmail-and-photos-to-provide-tailored-responses/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 22 Jan 2026 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company notes that AI Mode doesn’t train directly on your Gmail inbox or Google Photos library. Instead, it trains on specific prompts and the model’s responses. ]]></content:encoded></item><item><title>eBay bans illicit automated shopping amid rapid rise of AI agents</title><link>https://arstechnica.com/information-technology/2026/01/ebay-bans-illicit-automated-shopping-amid-rapid-rise-of-ai-agents/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_shopper_1-1152x648.jpg" length="" type=""/><pubDate>Thu, 22 Jan 2026 15:56:33 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT - Ars Technica</source><content:encoded><![CDATA[On Tuesday, eBay updated its User Agreement to explicitly ban third-party "buy for me" agents and AI chatbots from interacting with its platform without permission, first spotted by Value Added Resource. On its face, a one-line terms of service update doesn't seem like major news, but what it implies is more significant: The change reflects the rapid emergence of what some are calling "agentic commerce," a new category of AI tools designed to browse, compare, and purchase products on behalf of users.eBay's updated terms, which go into effect on February 20, 2026, specifically prohibit users from employing "buy-for-me agents, LLM-driven bots, or any end-to-end flow that attempts to place orders without human review" to access eBay's services without the site's permission. The previous version of the agreement contained a general prohibition on robots, spiders, scrapers, and automated data gathering tools but did not mention AI agents or LLMs by name.At first glance, the phrase "agentic commerce" may sound like aspirational marketing jargon, but the tools are already here, and people are apparently using them. While fitting loosely under one label, these tools come in many forms.]]></content:encoded></item><item><title>Why Expected Value Is Not Enough in Production Trading Systems</title><link>https://hackernoon.com/why-expected-value-is-not-enough-in-production-trading-systems?source=rss</link><author>Ilia Nagovitsyn</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:34:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We had a problem. Our automated trading system had a positive expected value: the math checked out, the backtests looked great, and initially, it made money. But over time, it was bleeding. Small losses that accumulated faster than the occasional wins could compensate.This wasn't a bug in the code. It was a fundamental misunderstanding of what matters in production.Most trading tutorials, academic papers, and online courses teach you to maximize expected value. The logic seems bulletproof:E[profit] = Σ(probability_i × outcome_i)
If this number is positive, you should take the trade. If you can make this number bigger, you should optimize for it. Simple, right?Except in production, this optimization strategy has a fatal flaw: it doesn't account for the path you take to reach that expected value.Let me show you what I mean with a real scenario from our system.Our strategy was designed to capture price spikes in volatile markets. The model would:Analyze possible price directions for each trading windowOptimize position sizing using quadratic programmingExecute trades to capture spread opportunitiesOn paper, the expected value was solidly positive. In practice:: Caught a major spike, made $15,000: Small losses every day, total -$8,000: Another spike, made $12,000: Gradual bleed, total -$11,000The problem? Our optimizer had developed a structural bias. It was systematically taking positions that won big occasionally but lost small amounts frequently. The expected value calculation said this was fine: the big wins would eventually compensate. But "eventually" requires infinite capital and infinite time horizon.Seeing The Difference: A SimulationTo illustrate why these risk controls matter, let's compare two strategies trading the same market over one year:Strategy A (EV Maximization): Aggressive position sizing based purely on expected value, using 150% leverage when opportunities look good.Strategy B (Risk-Controlled): Same market signals, but with fractional Kelly sizing (40% of aggressive) and CVaR-based position reduction during high tail risk periods.The results tell a crucial story. Look at the left chart closely - most EV-maximization paths aren't catastrophically failing. They're just… not compounding. You can see the sawtooth pattern: occasional spikes up, followed by slow erosion. This is the insidious bleeding that positive expected value misses.Notice how a few paths reached $500k? Those outliers pull the mean up to $146k. But the  is only $136k, and 29 out of 100 paths end below starting capital. In a backtest, you might have gotten lucky and seen one of those winner paths. In production, you get one random draw.The right chart is "boring", and that's exactly the point. No moonshots to $500k, but also no catastrophic drawdowns. The risk-controlled strategy clusters tightly around modest growth. It survives to compound returns over multiple years.This is the production reality: the strategy that survives gets to compound. The strategy that bleeds out makes nothing, regardless of what the expected value calculation promised.What Expected Value Doesn't CaptureThis is the classic gambler's problem, formalized by the Kelly Criterion. Even with positive expected value, if your position sizing is wrong, you  go broke.Consider: You have $100,000 capital and a trade with 60% win probability that either doubles your bet or loses it. Expected value is positive (+20%). But if you bet everything, you have a 40% chance of losing it all on the first trade.Kelly tells you the optimal bet size is:kelly_fraction = (p * b - q) / b
# where p = win probability, q = loss probability, b = odds
But here's what we learned in production: even Kelly is too aggressive.Your probability estimates are wrong (always)Markets change (your 60% edge becomes 52%)Correlations break down during stress (when you need them most)You can't rebalance instantly (slippage, latency, market impact)We ended up using fractional Kelly (25-50% of the theoretical Kelly bet) because the real-world costs of overestimating your edge are catastrophic.2. Numerical Instability in Extreme EventsOne morning, our system crashed during an extreme weather event. Not a software crash, but a mathematical one.Our covariance matrix became singular. The optimizer couldn't find a solution. We were frozen, unable to trade, during the exact conditions where our strategy should have made the most money.The problem: we had optimized for expected scenarios. But extreme events have different correlation structures. Assets that normally move independently suddenly become perfectly correlated. Your carefully estimated covariance matrix, built from thousands of normal days, becomes useless.The fix wasn't better expected value calculations. It was :from sklearn.covariance import LedoitWolf

# Instead of sample covariance
cov_matrix = np.cov(returns.T)

# Use shrinkage towards structured estimator
lw = LedoitWolf()
cov_matrix_robust = lw.fit(returns).covariance_
This trades off some accuracy in normal times for stability in extremes. Your expected value calculations will be slightly worse. Your system will survive black swans.Here's a problem that doesn't show up in backtests: your expected value calculation assumes you can wait long enough for the law of large numbers to work.In production, you can't.We discovered this when our system showed strong positive expected value over 90-day windows but consistently lost money over 30-day windows. The problem wasn't the math. It was the business reality.Our capital providers reviewed performance monthly. Our risk limits were adjusted quarterly based on recent results. If we had three bad months, our position limits got cut, regardless of what the long-term expected value said.The theoretical strategy needed 6-12 months to reliably show profit. The operational reality gave us 3 months before consequences kicked in.We had to add explicit time-horizon constraints to our optimization:def optimize_with_horizon_constraint(scenarios, max_horizon_days=30):
    """
    Optimize not just for long-term EV, but for probability of
    positive returns within operational time horizon 
    """
    # Standard expected value 
    ev = np.mean(scenarios)

    # But also: what'sthe probability we're profitable
    # within our actual time horizon?
    rolling_returns = pd.Series(scenarios).rolling(max_horizon_days).sum()
    prob_profitable_in_horizon = (rolling_returns > 0).mean()

    # Penalize strategies with low short-term win probability 
    # even if long-term EV is great
    if prob_profitable_in_horizon < 0.6:  
      return ev * 0.5 # Heavily discount

    return ev
This meant accepting strategies with slightly lower theoretical expected value but higher probability of showing profit within our operational constraints. It's not mathematically optimal, but it's practically necessary.After painful lessons, here's what we learned to optimize for:1. Risk-Adjusted Returns with CVaRInstead of maximizing E[profit], we minimize CVaR (Conditional Value at Risk): the expected loss in the worst 5% of scenariosimport cvxpy as cp

# Decision variable: position sizes
positions = cp.Variable(n_assets)

# Scenarios returns
scenario_returns = get_price_scenarios() # shape: (n_scenarios, n_assets)
portfolio_returns = scenario_returns @ positions

# CVaR constraints
alpha = 0.05 # 5% tail
var = cp.Variable()
u = cp.Variable(n_scenarios)

constraints = [
    u >= 0,
    u >= -(portfolio_returns - var),
]

cvar = var + cp.sum(u) / (n_scenarios * alpha)

# Optimize for return while constraining tail risk
objective = cp.Maximize(cp.sum(portfolio_returns) / n_scenarios - lambda_risk * cvar)
This explicitly penalizes strategies that have good average returns but catastrophic tail risk.2. Robustness to Model ErrorWe assume our model is wrong and optimize for the  within a reasonable uncertainty bound:# Instead of single expected return estimate
mu_estimated = historical_returns.mean()

# Assume uncertainty 
mu_lower_bound = mu_estimated - 2 * historical_returns.std() / np.sqrt(len(historical_returns))

# Optimize for worst-case in uncertainty range
# (Robust optimization / minmax approach)
This protects against overconfident parameter estimates.3. Kelly-Constrainted Position SizingWe explicitly limit position sizes based on Kelly criterion, even when the optimizer wants more:def kelly_position_limit(edge, volatility, capital, max_kelly_fraction=0.25):
    """
    edge: expected return per unit risk
    volatility: standard deviation of returns 
    max_kelly_fraction: fraction of theoretical Kelly to actually use
    """
    kelly_full = edge / (volatility ** 2)
    kelly_fraction = capital * kelly_full * max_kelly_fraction

    return kelly_position
We use 25% Kelly as a hard constraint. Yes, this reduces expected value. It also ensures we'll still be trading next month.The shift from expected value thinking to production thinking is philosophical:: "What strategy has the highest expected return?": "What strategy will survive being wrong about my assumptions?"Here are the practical shifts we made:: Added worst-month analysis, not just average returns: Conservative by default, with kill-switches for anomalies: Track CVaR daily, not just P&L: Assume 30% parameter uncertainty on all estimates: Explicit code paths for "model is completely wrong" scenariosExpected value is a beautiful mathematical concept. It's clean, intuitive, and theoretically optimal.In production, you're not trading against a probability distribution. You're trading against:Your own imperfect risk modelsOperational constraints that aren't in your backtestThe psychological reality of watching your capital decline day after day even though the "expected value is positive"The systems that survive aren't the ones with the highest expected value. They're the ones that remain robust when the model is wrong, markets shift, and black swans appear.Optimize for survival first. Profitability second. Expected value is a component of that calculation, but it's not the objective function.]]></content:encoded></item><item><title>Here’s what you should know about the US TikTok deal</title><link>https://techcrunch.com/2026/01/23/heres-whats-you-should-know-about-the-us-tiktok-deal/</link><author>Lauren Forristal, Amanda Silberling</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:31:19 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A number of investors are competing for the opportunity to purchase the app, and if a deal were to go through, the platform's U.S. business could have its valuation soar to upward of $60 billion.]]></content:encoded></item><item><title>How to Advertise Your Startup the Right Way</title><link>https://hackernoon.com/how-to-advertise-your-startup-the-right-way?source=rss</link><author>Startups of The Year</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:30:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We’ve all been there, scrolling through the internet late at night when you know you’re supposed to be sleeping. But something’s off. You’re getting hit with so many ads, and the worst part of it all is that you’re not interested in their products at all. For businesses, especially startups, this is really not a great first impression.\
There are, however, ways to not fall into this trap. You can advertise to millions of people without annoying anyone. Here’s how.3 Ways to do Ads CorrectlyAn easy way to not annoy potential customers is by targeting them correctly. For example, if your startup primarily sells hair gel, you don’t want to advertise and try to sell hair gel on a website called baldguys.com. Would it be hilarious if you did? Yes, absolutely. Does it make sense business-wise? No.\
That’s why it’s important to know the audience of the platform where you’re trying to advertise. Baldguys.com can have millions of visitors, and your ad can get millions of impressions, but if nobody wants to buy your product, then it doesn’t really matter. Plus, I’m not sure if bald men want to be reminded of the time they used to have hair.Another trap startups fall into is believing that flashy and in-your-face ads are better. But nobody likes loud, obnoxious ads; they prefer ads that don’t clutter up the whole screen. You might think that a smaller, less in-your-face ad is counterintuitive to get more eyes onto your startup and your product, but people remember ads that respect them.3. Be clear about what you’re sellingThe final thing you should do is ensure your ad is clear and concise. Potential customers need to know exactly who you are and what you’re selling. If you waste their time by posting ads that are cryptic and say something like, “By harnessing AI, we’re empowering enterprises to grow their ecosystem.” That really doesn’t explain anything, and people get annoyed when you’re trying to sell them something, while remaining cryptic and mysterious. Just tell them outright why they should consider your product or service.\
Crafting the perfect advertisement campaign can be tedious and nerve-wracking, but it doesn’t have to be. Especially when you can join HackerNoon’s Targeted Ad Program. Here’s everything that it has to offer.Every story organically gets at least eight relevant tech tags and a parent category.Sponsors buy multimodal placements on relevant categories with all the tags and stories.These Ad placements include Banners, Logos, Newsletter Ads, and Audio Ads (what we call truly AIO - Activities, Interests, and Opinions).Optimized for: Brand Recall and Clickability (Get 3× more clicks for the same impressions compared to elsewhere).Get quality leads at unbeatable prices, with CPM ~ $7 and CPC ~ $5.Now, here are 3 startups that will annoy you with how amazing they are.Meet Scoutlabs, DataRock Labs, and Hackrate: HackerNoon Startups of the WeekFarmers have a lot of problem-solving they have to do, and one big problem that is very time-consuming is dealing with insects destroying their crops. That’s where scoutlabs comes in. They specifically specialize in delta traps to help farmers get rid of insects in a quick and efficient way. These traps are made by farmers for farmers.\
It’s this desire to help farmers that helped scoutlabs earn second place in HackerNoon’s Startups of the Year in Budapest, Hungary. They also have the honor of being in the top 10 Startups of The Year in the ClimateTech and Manufacturing categories. But the biggest honor came when they won 1st place in the Renewable Energy category.Everyone knows that gathering data to better understand the needs of your customers and employees is extremely important. However, just having all that raw data will get you nowhere if you don’t know how to analyze it and turn it into an actionable plan. Well, with DataRock Labs’ services, that can all change. They offer Data Engineering, AI, and Business Intelligence services to help you better understand your data.It seems like almost every month, you hear about a company getting its data stolen, including all of its customers’ credit card information. They say all press is good press, but you really don’t want to be in the news because of that. That’s why Hackrate is well-trusted, because its services actively help companies improve their defenses and better understand their weaknesses. Attack surface management, a bug bounty board, and penetration testing are just a few of the services that Hackrate offers companies to level up their cybersecurity.That’s all we have for you today.\
Until next time, Hackers!]]></content:encoded></item><item><title>&apos;Stealing Isn&apos;t Innovation&apos;: Hundreds of Creatives Warn Against an AI Slop Future</title><link>https://slashdot.org/story/26/01/22/1529228/stealing-isnt-innovation-hundreds-of-creatives-warn-against-an-ai-slop-future?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:29:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Around 800 artists, writers, actors, and musicians signed on to a new campaign against what they call "theft at a grand scale" by AI companies. From a report: The signatories of the campaign -- called "Stealing Isn't Innovation" -- include authors George Saunders and Jodi Picoult, actors Cate Blanchett and Scarlett Johansson, and musicians like the band R.E.M., Billy Corgan, and The Roots. 

"Driven by fierce competition for leadership in the new GenAI technology, profit-hungry technology companies, including those among the richest in the world as well as private equity-backed ventures, have copied a massive amount of creative content online without authorization or payment to those who created it," a press release reads. "This illegal intellectual property grab fosters an information ecosystem dominated by misinformation, deepfakes, and a vapid artificial avalanche of low-quality materials ['AI slop'], risking AI model collapse and directly threatening America's AI superiority and international competitiveness."]]></content:encoded></item><item><title>Under Armour says it’s ‘aware’ of data breach claims after 72M customer records were posted online</title><link>https://techcrunch.com/2026/01/22/under-armour-says-its-aware-of-data-breach-claims-after-72m-customer-records-were-posted-online/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:28:50 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TechCrunch obtained a sample of the stolen data, which contained names, email addresses, dates of birth, and the user's approximate geographic location. Under Armour confirmed some sensitive information was taken in the breach.]]></content:encoded></item><item><title>AMD Announces Ryzen 7 9850X3D Pricing Of $499 USD</title><link>https://www.phoronix.com/news/Ryzen-7-9850X3D-Price</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:19:39 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Back at CES AMD announced the Ryen 7 9850X3D as a faster sibling to the Ryzen 7 9800X3D. Today they have announced the suggested price for this 3D V-Cache desktop processor and confirmation of its availability starting on 29 January...]]></content:encoded></item><item><title>Google snags team behind AI voice startup Hume AI</title><link>https://techcrunch.com/2026/01/22/google-reportedly-snags-up-team-behind-ai-voice-startup-hume-ai/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:12:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google has hired the CEO and top engineer behind voice AI startup Hume AI, signaling that voice is increasingly becoming the preferred interface over screens. ]]></content:encoded></item><item><title>Why “It Works” Is Often the Most Dangerous Phrase in Product Design</title><link>https://hackernoon.com/why-it-works-is-often-the-most-dangerous-phrase-in-product-design?source=rss</link><author>Vaishnavi Ramamoorthy</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:10:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[At a previous company, I sat in a meeting where we were debating whether to redesign a feature that had been in the product for three years. The PM thought we should leave it alone. It works, users know it, so why mess with success?Then someone pulled up the original design spec from 2021. It turns out the feature was built to solve a problem that no longer existed. The vendor API it was working around? Fixed 18 months ago. The technical constraint that shaped the entire interaction was also gone.But users still used it. Not because it was the best solution. They'd just learned how to use what we gave them, and nobody ever went back to ask whether we should rebuild it properly now that we could.That meeting stuck with me. My husband wrote something years ago that kept coming back to me: "If need is the mother of invention, usability should be the father." It's a phrase I've been thinking about a lot lately—how usability keeps transforming products long after the original need is solved, sometimes long after that need has completely changed.Products keep changing even when they look the sameThink about bicycles. The ones we ride today look nothing like the first versions. Those early designs were awkward, uncomfortable, and kind of dangerous, actually. But the core need stayed the same: human-powered transportation.What evolved was our understanding of usability. Better materials, yes, but also refined ergonomics and improved balance. Each generation learned from watching people actually ride these things. Not inventing something new, just making the existing invention more human.I keep seeing this pattern. The first version solves the need. Everything after that? It's about making the solution livable.Teams treat "it works" like a finish line. Feature shipped, problem solved, next thing please.But "it works" usually just means users figured out how to accomplish their goal despite the friction. Doesn't mean the friction isn't there. Doesn't mean they wouldn't jump at something better. They've just adapted.I was reviewing a flow a while back in which users had to click through three confirmation screens to complete a single action. Why three confirmations? Nobody on the current team could tell me. We dug through old tickets and found that two years ago, there was a data loss bug. The triple confirmation was a temporary patch while engineering fixed the root cause.They fixed the bug six months later. The three confirmation screens stayed. Because it "worked." Users got through it, drop-off wasn't terrible, and other priorities came up.That's not usability evolving. That's usability getting stuck.What actual evolution looks likeReal usability evolution isn't about adding features. It's refinement based on how people actually use what you built.ATMs are probably the clearest example I can think of. First-generation ATMs gave you cash first, then returned your card. Simple, functional, solved the need. But people would grab their cash and walk away, leaving their card in the machine.Later designs reversed the order: card first, then cash. Not a new invention. Not solving a different problem. Just adapting to how humans actually behave instead of how the machine logic suggested they should behave.Same core function, shaped around reality instead of theory.Why do products stop evolving?From what I've seen, products stop improving their usability for a few predictable reasons.The original designer left, and nobody knows why things work the way they do. Teams get nervous about changing anything.Metrics look fine, so there's no business case. Even though users have just learned to work around problems.The system got so interconnected that changing one thing breaks three others. Small improvements feel too risky.Or the team moved on to new features. Improving what exists feels less exciting than building what's next.All reasonable. Allare  slowly eroding your product's usability.What I've noticed about features that improve vs features that stagnateThe features that keep getting better usually have someone who still cares about them. An owner is watching how people use it, noticing small frustrations, pushing for refinements.Features that stagnate are orphans. Nobody owns them anymore. They work "well enough." Which means they never get better.One team I worked with had this practice they called "adoption reviews." Every quarter, they'd look at features that launched more than six months ago and ask: how are people actually using this now? Not just "are they using it" but how. What workarounds did they develop? What's harder than it needs to be? What did we learn since we built it?Then they'd prioritize improvements based on real usage patterns, not just new feature requests.Not exciting work. But it's the difference between products that age well and products that just get old.When evolution goes wrongSometimes teams overcorrect. They see users struggling and immediately redesign without understanding why it works the way it does.I watched a team simplify what seemed like an unnecessarily complex flow. Three steps became one. Much cleaner. Everyone on the team was excited.Those three steps weren't random complexity. They were scaffolds that helped users make better decisions. Collapsing everything into one screen removed the structure people needed to think through their choices.Good evolution requires understanding not just what people do, but why the current design works, even when it seems like it shouldn't.The products that never stop improvingThe products I actually love using don't necessarily have the best features. They have the best evolution.Google Search hasn't changed its fundamental purpose in 20 years. But the usability keeps evolving. Autocomplete, instant results, knowledge panels, and featured snippets. Each one is based on learning how people actually search, not how Google thought they would search.Slack didn't invent team chat. They just kept refining how it works. Threading, reactions, better notifications, smarter search. Small improvements that add up over time.These aren't dramatic reinventions. They're careful, continuous evolution based on watching real people use the product.What this means for how I think about design nowNeed might create the first version of your product. But usability evolution determines whether there's a tenth version.The questions I ask myself now are different from those I used to ask. What are we keeping just because it exists, not because it's still right? What constraints shaped our design that don't apply anymore? What have users learned to work around that we could actually fix?And probably the most important question: who on the team is responsible for making existing features better, not just building new ones?Every few months, pick one established feature and pretend you're designing it fresh today. With current technology, current understanding of users, and current business context.Would you design it the same way?Usually, the answer is no. And when it is, the next question is: what's stopping you from making it better?Sometimes it's real constraints. But a lot of times it's just inertia. And inertia is expensive when you're competing with teams that actually evolve their products.Where I've landed on thisNeed invents products. Usability determines whether those products stay relevant.The invention gets you to market. The evolution keeps you there.Teams that get this don't just ship and move on. They ship, watch what actually happens, learn from it, and refine. Continuously. Even when things seem to be working.Especially when things are working, actually.==Because "working" is just the starting point for what a product can become.==]]></content:encoded></item><item><title>The Problem With Perfect AI Is That Mathematics Won’t Allow It</title><link>https://hackernoon.com/the-problem-with-perfect-ai-is-that-mathematics-wont-allow-it?source=rss</link><author>jdoula</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:02:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Despite rapid advances, artificial intelligence is constrained by mathematical, physical, and logical limits that make perfect or infinite intelligence fundamentally impossible.]]></content:encoded></item><item><title>From invisibility cloaks to AI chips: Neurophos raises $110M to build tiny optical processors for inferencing</title><link>https://techcrunch.com/2026/01/22/from-invisibility-cloaks-to-ai-chips-neurophos-raises-110m-to-build-tiny-optical-processors-for-inferencing/</link><author>Ram Iyer</author><category>tech</category><pubDate>Thu, 22 Jan 2026 15:00:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Neurophos is taking a crack at solving the AI industry's power efficiency problem with an optical chip that uses a composite material to do the math required in AI inferencing tasks.]]></content:encoded></item><item><title>Anthropic has to keep revising its technical interview test as Claude improves</title><link>https://techcrunch.com/2026/01/22/anthropic-has-to-keep-revising-its-technical-interview-test-so-you-cant-cheat-on-it-with-claude/</link><author>Russell Brandom</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:54:23 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The issue of AI cheating is already wreaking havoc at schools and universities around the world, so it's ironic that AI labs are having to deal with it too. But Anthropic is also uniquely well-equipped to deal with the problem.]]></content:encoded></item><item><title>Updated Intel Panther Lake IPU Firmware Published With New Features &amp; Bug Fixes</title><link>https://www.phoronix.com/news/Intel-PTL-IPU7-Firmware-Go</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:54:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Ahead of the first Intel Core Ultra Series 3 Panther Lake laptops expected to hit retail channels next week, Intel has published updated IPU7 (IPU 7.5) firmware for the image processing unit used by the web cameras on the higher-end Panther Lake laptops...]]></content:encoded></item><item><title>Optimization Log: How I Pushed a 2,750-Word Guide to 97/100 PageSpeed for AI Search</title><link>https://hackernoon.com/optimization-log-how-i-pushed-a-2750-word-guide-to-97100-pagespeed-for-ai-search?source=rss</link><author>Abbas Rizvi</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:51:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Speed is the New Authority: How I Hit 97/100 PageSpeed for the AI-Search EraThe "SEO is dead" crowd is half-right. The old way of SEO—slow, bloated pages stuffed with keywords—is indeed dying. In its place, a new beast has emerged: Generative Engine Optimization (GEO).Last week, I decided to put my own infrastructure to the ultimate test. I didn't just want a "fast" site; I wanted a site that could be parsed by an LLM (Large Language Model) in a heartbeat. I took a massive, 2,750-word guide on GEO—the kind of page that usually chokes under its own weight—and optimized it until Google’s PageSpeed Insights handed me a 97/100 Mobile Performance score.Why go to such extremes? Because in 2026, if your Largest Contentful Paint (LCP) isn't under 1 second, you don't just lose users—you lose the AI.Generative engines like Gemini and SearchGPT are speed-hungry. They prioritize "Instant Answers" and favor sources that provide a seamless technical connection. If your site is slow, you are invisible to the AI crawlers that now control the majority of search traffic.In this log, I’m breaking down the six technical pillars I used to clear the path for AI. From AVIF conversion to the "Death of Bloat" in script pruning, here is how I built a high-performance foundation for the next era of search.One of the biggest killers of Technical SEO for GEO is unoptimized media. In my 2,750-word guide, images are essential for context, but they also add weight. \n  AVIF over WebP: While WebP was the gold standard in 2024, by 2026, AVIF has taken over. It offers 30% better compression than WebP without losing quality.Dynamic Aspect Ratio Padding: To ensure a CLS of 0, I pre-define the space for every image in the CSS. This prevents the "jumpy" feeling when a page loads.The GEO Benefit: AI engines often "scrape" images to display in the Generative UI sidebar. If your images are well-lit and properly tagged with Schema, you are five times more likely to be featured as the visual.Pillar #2: Script Pruning and the "Death of Bloat"Most WordPress sites are weighed down by "Ghost Scripts"—plugins you deleted months ago that still leave traces of JavaScript behind.In our Technical SEO for GEO framework, we use a "Load on Interaction" model.Example: Your "Contact Form" script shouldn't load the moment the page opens. It should load only when the user scrolls to the bottom or clicks "Contact."The Result: My site achieved a 0ms Total Blocking Time. This means the browser's main thread is always free to process the actual content, which is what the AI needs to read.If your server is in New York and your visitor is in London, physics dictates a delay.Implementation: I moved  to a "Global Edge Network."Why it matters for GEO: AI bots crawl from multiple global locations simultaneously. If your Time to First Byte (TTFB) is inconsistent, the AI may flag your site as "unstable." High-tier Technical SEO for GEO requires a server response time under 200ms globally.**Pillar #4: The Language of AI – Advanced Schema Markup for Generative Answers\  Achieving a 97/100 PageSpeed score is monumental, but its power for Technical SEO for GEO is amplified tenfold when paired with precise Schema Markup. Schema is the structured data that tells AI exactly "what" your content means, not just "what" it says. In 2026, Schema is no longer optional; it's the dictionary for AI.Beyond Basic Schema: The GEO AdvantageFor , I went beyond the generic Article or BlogPosting Schema. We deployed highly specific markup, including:HowTo Schema: For actionable guides, AI can pull steps directly into generative answers.FAQPage Schema: For direct Q&A, allowing AI to immediately answer user queries without needing them to click through.AboutPage and Person Schema: To establish clear E-E-A-T (Experience, Expertise, Authoritativeness, Trustworthiness) for Abbas Rizvi as the author and expert. This tells AI: "This is a credible source."ImageObject Schema: Every critical image now tells AI its purpose, subject, and context. This significantly increases the chance of your visuals being used in AI-generated responses.The GEO Benefit: By clearly defining content components with Schema, you are pre-digesting your information for AI models. This drastically improves the likelihood of your content being chosen as the "source" for a generative answer, even if the user never directly visits your site in the traditional sense. It's about being  in the AI's response, not just in the SERP.Converting Speed into EngagementReduced Bounce Rate: A site that loads in under 1 second ensures users don't hit the back button out of frustration. This tells AI: "This content is engaging."Increased Time on Page: With faster loading, users spend more time consuming your valuable content, signaling to AI: "This is comprehensive and relevant."Higher Conversion Rates: Whether it's signing up for a newsletter or requesting a consultation, a seamless user experience (driven by speed) directly impacts your business goals.Visual Proof of Stability:Here, we can see the impact of perfect CLS and a low LCP. The page loads smoothly, without any jarring shifts that disrupt the user's reading experience.  The subtle yet powerful impact of a perfectly stable and fast-loading page cannot be overstated in the AI era. These user signals indirectly tell AI which content is truly "high quality" and deserving of top generative rankings.Pillar #6: A 12-Month Technical Maintenance Roadmap for Sustainable GEO DominanceAchieving a 97 PageSpeed score is not a one-time event; it's an ongoing commitment. The algorithms evolve, and so should your website. A robust Technical SEO for GEO strategy includes a proactive maintenance plan.Quarterly Audit Checklist:Q1: Code Dependency Audit: Review all third-party scripts. Are they still necessary? Can they be loaded conditionally?Q2: Media Compression Review: Re-scan all new and existing images. Have new, more efficient formats (like JPEG XL) emerged?Q3: Server Log Analysis: Monitor crawl budget and bot behavior. Are AI crawlers hitting critical pages efficiently?Q4: Schema Validation: Use Google's Rich Results Test to ensure all Schema markup is still valid and being interpreted correctly.Monthly: Check Core Web Vitals in Search Console. Any dips need immediate investigation.By integrating this proactive maintenance, you ensure that your Technical SEO for GEO remains ahead of the curve, providing a stable, fast, and AI-friendly platform for years to come.\
**"A version of this technical audit was originally documented on my personal blog at TheAbbasRizvi.com." ]]></content:encoded></item><item><title>Blue Origin schedules third New Glenn launch for late February, but not to the moon</title><link>https://techcrunch.com/2026/01/22/blue-origin-schedules-third-new-glenn-launch-for-late-february-but-not-to-the-moon/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:49:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Jeff Bezos' Blue Origin had previously suggested that the third launch of the mega-rocket would take the space company's robotic lunar lander to the moon.]]></content:encoded></item><item><title>Nvidia Allegedly Sought &apos;High-Speed Access&apos; To Pirated Book Library for AI Training</title><link>https://yro.slashdot.org/story/26/01/22/1343205/nvidia-allegedly-sought-high-speed-access-to-pirated-book-library-for-ai-training?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An expanded class-action lawsuit filed last Friday alleges that a member of Nvidia's data strategy team directly contacted Anna's Archive -- the sprawling shadow library hosting millions of pirated books -- to explore "including Anna's Archive in pre-training data for our LLMs." 

Internal documents cited in the amended complaint show Nvidia sought information about "high-speed access" to the collection, which Anna's Archive charged tens of thousands of dollars for. According to the lawsuit, Anna's Archive warned Nvidia that its library was illegally acquired and maintained, then asked if the company had internal permission to proceed. The pirate library noted it had previously wasted time on other AI companies that couldn't secure approval. Nvidia management allegedly gave "the green light" within a week. 

Anna's Archive promised access to roughly 500 terabytes of data, including millions of books normally only accessible through Internet Archive's controlled digital lending system. The lawsuit also alleges Nvidia downloaded books from LibGen, Sci-Hub, and Z-Library.]]></content:encoded></item><item><title>Rust 1.93 Brings Improvement For Inline Assembly Handling</title><link>https://www.phoronix.com/news/Rust-1.93-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:18:12 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Rust 1.93 is out today as the first feature release for this programming lanugage of 2026...]]></content:encoded></item><item><title>How Static and Hybrid Analysis Can Cut Privacy Review Effort by 95%</title><link>https://hackernoon.com/how-static-and-hybrid-analysis-can-cut-privacy-review-effort-by-95percent?source=rss</link><author>Code Review</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:00:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Research in source code analysis for privacy is extensive, yet specific approaches for identifying personal data processing are limited. Ullah et al. [13] introduced an approach for extracting control and data dependencies in source code, potentially applicable for locating personal data processing methods, but not directly designed for this purpose. Hjerppe et al. [2] proposed an annotationbased static analysis for data protection, but its effectiveness is contingent on accurate developer annotations, a challenge in large projects.\
Dynamic analysis has been explored for sensitive data flow detection, with DAISY [15] focusing on Android apps and ConDySTA [16] combining dynamic taint analysis with static analysis. However, these methods have limitations, such as platform specificity or the need for executing projects. Automated assistance in code review has been explored by Li et al. [3] with their pre-trained model CodeReviewer, but it lacks a focus on personal data processing.\
SWANAssist [5] offers a semi-automated approach for identifying security-relevant Java code methods, which could potentially be adapted for privacy purposes. Other studies, like [1, 12], attempt to align GDPR compliance with static analysis. Novikova et al. [4] provided insights into privacy-enhancing technologies but did not focus on personal data processing in source code.\
These studies mark great progress in source code analysis, yet a gap exists in automated identification and categorization of personal data processing. Our work addresses this by proposing an automated approach for identifying personal data processing in real-world applications, enhancing efficiency in privacy code reviews.In conclusion, our study introduces a method for identifying and categorizing privacy-relevant methods in source code, focusing on personal data processing. We have successfully narrowed the analysis scope to just 4.2% of methods across 100 popular open-source applications, offering a practical starting point for developers, data protection officers, and reviewers.\
This approach not only simplifies code reviews but also facilitates compliance with data protection regulations like GDPR, helping organizations align their software development with legal requirements. For future work, we aim to enhance the precision of our privacy-relevant method identification algorithms, possibly integrating machine learning for more accurate predictions of personal data processing activities.\
Expanding our approach to additional programming languages and integrating it into common development tools for real-time feedback are also key goals. These advancements will broaden the impact and applicability of our approach. Ultimately, our research paves the way for more focused and efficient privacy assessments in software development, contributing to the creation of software that is efficient, robust, and respectful of user privacy.This work is part of the Privacy Matters (PriMa) project. The PriMa project has received funding from European Union’s Horizon 2020 research and innovation program under the Marie Sk lodowskaCurie grant agreement No. 860315.Ferrara, P., Olivieri, L., Spoto, F.: Tailoring taint analysis to GDPR. In: Privacy Technologies and Policy: 6th Annual Privacy Forum, APF 2018, Barcelona, Spain, June 13-14, 2018, Revised Selected Papers 6. pp. 63–76. Springer (2018)Hjerppe, K., Ruohonen, J., Lepp¨anen, V.: Annotation-based static analysis for personal data protection. In: Privacy and Identity Management. Data for Better Living: AI and Privacy, pp. 343–358. Springer International Publishing (2020)Li, Z., Lu, S., Guo, D., Duan, N., Jannu, S., Jenks, G., Majumder, D., Green, J., Svyatkovskiy, A., Fu, S., Sundaresan, N.: Automating code review activities by large-scale pre-training (2022)Novikova, E., Fomichov, D., Kholod, I., Filippov, E.: Analysis of privacy-enhancing technologies in open-source federated learning frameworks for driver activity recognition. Sensors 22(8), 2983 (2022)Piskachev, G., Do, L.N.Q., Johnson, O., Bodden, E.: SWANAssist: Semi-Automated Detection of Code-Specific, Security-Relevant Methods. In: Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering. p. 1094–1097. ASE’19, IEEE Press (2020). https://doi.org/10.1109/ASE.2019.00110Ren, J., Rao, A., Lindorfer, M., Legout, A., Choffnes, D.: ReCon: Revealing and Controlling PII Leaks in Mobile Network Traffic. In: Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services. p. 361–374. MobiSys ’16, Association for Computing Machinery, New York, NY, USA (2016). https://doi.org/10.1145/2906388.2906392Tang, F., Østvold, B.M.: Assessing Software Privacy Using the Privacy Flow-Graph. In: Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security. p. 7–15. MSR4P&S 2022, Association for Computing Machinery, New York, NY, USA (2022)Tang., F., Østvold., B., Bruntink., M.: Identifying Personal Data Processing for Code Review. In: Proceedings of the 9th International Conference on Information Systems Security and Privacy - ICISSP. pp. 568–575. INSTICC, SciTePress (2023). https://doi.org/10.5220/0011725700003405Tang, F., Østvold, B.M., Bruntink, M.: Helping Code Reviewer Prioritize: Pinpointing Personal Data and Its Processing. IOS Press (Sep 2023). https://doi.org/10.3233/faia230228Thongtanunam, P., Hassan, A.E.: Review dynamics and their impact on software quality. IEEE Transactions on Software Engineering 47(12), 2698–2712 (2020)Tokas, S., Owe, O., Ramezanifarkhani, T.: Static checking of GDPR-related privacy compliance for object-oriented distributed systems. Journal of Logical and Algebraic Methods in Programming 125, 100733 (2022)Ullah, F., Wang, J., Jabbar, S., Al-Turjman, F., Alazab, M.: Source code authorship attribution using hybrid approach of program dependence graph and deep learning model. IEEE Access 7, 141987– 141999 (2019)Vall´ee-Rai, R., Co, P., Gagnon, E., Hendren, L., Lam, P., Sundaresan, V.: Soot: A java bytecode optimization framework. In: CASCON First Decade High Impact Papers, pp. 214–224 (2010)Zhang, X., Heaps, J., Slavin, R., Niu, J., Breaux, T., Wang, X.: DAISY: Dynamic-Analysis-Induced Source Discovery for Sensitive Data. ACM Trans. Softw. Eng. Methodol. 32(4) (May 2023)Zhang, X., Wang, X., Slavin, R., Niu, J.: ConDySTA: Context-Aware Dynamic Supplement to Static Taint Analysis. In: 2021 IEEE Symposium on Security and Privacy (SP). pp. 796–812 (2021). https://doi.org/10.1109/SP40001.2021.00040]]></content:encoded></item><item><title>How to Compute With Electron Waves</title><link>https://spectrum.ieee.org/plasmon-computing-device</link><author>Dina Genkina</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82Mjk5ODgyMi9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5Njk3MTE2OH0.R6_zPKNN2At6iCynjteHbWS0z1Jp64yRCgfH2C6WlD0/image.jpg?width=600" length="" type=""/><pubDate>Thu, 22 Jan 2026 14:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[New paradigm promises to save energy while keeping CMOS ]]></content:encoded></item><item><title>&apos;No Reasons To Own&apos;: Software Stocks Sink on Fear of New AI Tool</title><link>https://tech.slashdot.org/story/26/01/22/0946226/no-reasons-to-own-software-stocks-sink-on-fear-of-new-ai-tool?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The new year was supposed to bring opportunities for beaten-down software stocks. Instead, the group is off to its worst start in years. From a report: The release of a new artificial intelligence tool from startup Anthropic on Jan. 12 rekindled fears about disruption that weighed on software makers in 2025. 

TurboTax owner Intuit tumbled 16% last week, its worst since 2022, while Adobe and Salesforce, which makes customer relationship management software, both sank more than 11%. All told, a group of software-as-a-service stocks tracked by Morgan Stanley is down 15% so far this year, following a drop of 11% in 2025. It's the worst start to a year since 2022, according to data compiled by Bloomberg. 

While unproven, the tool represents just the type of capabilities that investors have been fearing, and reinforces bearish positions that are looking increasingly entrenched, according to Jordan Klein, a tech-sector specialist at Mizuho Securities. "Many buysiders see no reasons to own software no matter how cheap or beaten down the stocks get," Klein wrote in a Jan. 14 note to clients. "They assume zero catalysts for a re-rate exist right now," he said, referring to the potential for higher valuation multiples.]]></content:encoded></item><item><title>Spotify brings AI-powered Prompted Playlists to the US and Canada</title><link>https://techcrunch.com/2026/01/22/spotify-brings-ai-powered-prompted-playlists-to-the-u-s-and-canada/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 22 Jan 2026 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Now available in the US and Canada, Spotify's AI-powered Prompted Playlists let users describe what they want to hear using natural language commands.]]></content:encoded></item><item><title>Thanks To Trump, Verizon Immediately Starts Making It Harder To Switch Mobile Carriers</title><link>https://www.techdirt.com/2026/01/22/thanks-to-trump-verizon-immediately-starts-making-it-harder-to-switch-mobile-carriers/</link><author>Karl Bode</author><category>tech</category><pubDate>Thu, 22 Jan 2026 13:30:54 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Last week we noted how the Trump FCC, at the direct request of wireless phone giants, destroyed popular rules making it easier and cheaper to switch wireless carriers. The rules, applied via spectrum acquisition and merger conditions, required that Verizon unlock your phone within 60 days after purchase so you could easily switch to competitors.Verizon, as we’ve long established, hates competition, and immediately got to work lobbying the Trump administration to destroy the rules. The pay-to-play Trump administration quickly agreed, and now Verizon has started telling wireless customers they have to wait a year before switching phones after purchasing one from Verizon:“Verizon was previously required to unlock phones automatically after 60 days due to restrictions imposed on its spectrum licenses and merger conditions that helped Verizon obtain approval of its purchase of TracFone. But an update applied today to the TracFone unlocking policy said new phones will be locked for at least a year and that each customer will have to request an unlock instead of getting it automatically.”Again, these conditions were broadly popular and served the public interest, ensuring that it was easier for consumers to switch between our ever-consolidating, anti-competitive wireless phone giants. Verizon lobbied the FCC by repeatedly lying, without evidence, that these conditions resulted in a wave of black market phone thefts. FCC boss Brendan Carr, ever the industry lackey, parroted the claims in his rulings.To be clear this is, for now, only something Verizon is doing via its prepaid sub-brands that include Straight Talk, Tracfone, Net10 Wireless, Clearway, Total Wireless, Simple Mobile, SafeLink Wireless, and Walmart Family Mobile. These brands often attract lower income customers who can least afford to be trapped under an expensive provider like this.You can, for now, still buy an unlocked phone from an independent retailer, bring it to Verizon’s main postpaid brands, and port it back out again if you’d like. But when Verizon sees limited Democrat and press backlash  to this first push (guaranteed with so much else going on), it will steadily keep expanding its restrictions to include its primary brands and all unlocked phones.I know this because I’ve covered this company for a quarter century and this company’s anti-competitive ambitions are as predictable as the tides.Ideally, Verizon wants to return to what it considers the golden era of cellular phones: circa 2007 or so when carriers restricted how you could use your phone and restricted what apps you could install (remember all the shitty VCast Verizon apps they wouldn’t let you uninstall? Or the way they’d block phone GPS hardware from working on third-party apps?). Back then, they would also tether you to one carrier via expensive long-term contracts with costly early termination fees.If we stay on this path of zero U.S. corporate oversight, it’s all coming back, sooner or later. From there, should U.S. governance remain under corrupt authoritarian dominance, it’s only a matter of time before Verizon tries to dictate what content you can see in collaboration with the kakistocracy, thanks to the Trump administration’s destruction of popular net neutrality protections. This has always been Verizon’s ambition as a lumbering telecom giant that can’t innovate and hates competition and government oversight. Thanks to Trump’s assault on regulators, it’s increasingly difficult to hold companies like AT&T and Verizon accountable for literally anything (see the 5th Circuit’s decision to let AT&T off the hook for lying to, and spying on, its users). And the Trump administration’s ongoing quest to rubber stamp every merger that comes across its desk means more consolidation, and ultimately higher prices for U.S. wireless consumers who already pay some of the highest prices for mobile data in the developed world.Verizon and other broadly despised telecoms have struck a generational blow against oversight and consumer protection across Trump’s two terms, and they intend to take full advantage of a presidency they helped purchase. All while the president informs his loyal rubes he’s a champion of affordability.]]></content:encoded></item><item><title>AMD AOMP 22.0-2 Released With Flang Fortran Improvements</title><link>https://www.phoronix.com/news/AMD-AOMP-22.0-2</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 13:15:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Yesterday along with releasing ROCm 7.2 there was also the release of AOMP 22.0-2 as the newest version of their open-source downstream of LLVM/Clang/Flang that is focused on offering the best OpenMP/OpenACC offloading support to Instinct/Radeon hardware...]]></content:encoded></item><item><title>What 100 GitHub Projects Reveal About Personal Data in Modern Software</title><link>https://hackernoon.com/what-100-github-projects-reveal-about-personal-data-in-modern-software?source=rss</link><author>Code Review</author><category>tech</category><pubDate>Thu, 22 Jan 2026 13:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Application To Privacy Code ReviewThis section outlines how our approach can be applied to privacy code reviews across a diverse set of 100 open-source applications. We then delve into detailed case studies of two popular software applications to illustrate the utility of our approach.To understand the prevalence and types of personal data processing in real-world applications, we analyzed 100 open-source applications. These were equally divided between Java and JavaScript/TypeScript and were selected from GitHub’s daily top-starred repositories list 3 . We selected applications that are popular (top-starred), non-trivial (over 300K lines of code), and predominantly written in Java or JavaScript/TypeScript (constituting over 60% of the codebase).\
Additionally, we ensured these applications differed from the 30 popular libraries analyzed previously and that their primary documentation language was English for easier identification of functionalities. This selection process resulted in a dataset that is representative of real-world software applications and suitable for our analysis of personal data processing practices.\
We then examined the proportion of methods in these applications that invoke privacy-relevant methods and are involved in the flow of personal data and Personally Identifiable Information (PII). The result of statistics of our findings are listed below in Table 8.Our findings indicate that our approach can make the privacy code review process more efficient. By identifying methods that are critical for personal data and PII processing, we help reviewers focus their efforts, enabling a more targeted review.8.2 In-Depth Case StudiesWe validate the effectiveness of our approach through two open-source projects: Signal Desktop4 and Cal.com5 . Each offers unique insights for privacy code review. Both projects were chosen due to their popularity, sensitivity, and public availability. Their open codebases ensure transparency and reproducibility, making them ideal candidates to validate our approach.\
By applying our approach to these carefully selected real-world projects, we provide concrete examples that demonstrate practical value in identifying key areas to focus on during privacy code reviews.\
 Signal Desktop is a famous end-to-end encrypted messaging application, primarily written in TypeScript (79.5%) and JavaScript (15.6%), covering about 360K lines of code. Its reputation for enhanced security and privacy features showcases the depth of our approach. While the application has limited use of popular libraries, our approach highlighted a minor number of privacy-relevant methods invocations (48, approximately 0.5% of total methods) from our selected APIs and native libraries potentially linked to personal data processing.\
In our analysis, Signal stands out for using its own encryption protocol (Signal Protocol) and message transmission services, minimally relying on external libraries. This underscores Signal’s commitment to end-to-end encryption. Our categorization highlights the primary areas of Data Processing and Transformation (DPT), Network Communication (NC), and Data Encryption and Cryptography (DEC), with most encryption methods used for local encryption of profiles and group data. Signal’s proprietary protocol, used for encrypting chats and attachments, falls outside our analysis scope.\
Our findings show that Signal rarely transmits PII directly to the internet. Instead, encrypted system data or anonymized IDs are mainly used, reflecting Signal’s dedication to user privacy. For privacy code reviewers examining Signal Desktop, our approach underscores Signal’s limited use of popular libraries for PII processing, aligning with its privacy-focused design philosophy. This categorization helps reviewers understand how Signal handles personal data, aiding in a more streamlined review process.\
 Cal.com, a scheduling application, is designed to grant users comprehensive control over their schedules. Written entirely in TypeScript, it spans about 126K lines of code. Our method identified 371 (approximately 3.8% of total methods) privacy-relevant methods that might engage in personal data processing.\
Applications such as Cal.com often employ diverse frameworks for specific functionalities. For instance, Cal.com’s utilization of the popular ORM framework, Prisma, for handling user profiles and credentials, aligns with our library list. In terms of categories, Data Processing and Transformation (DPT) topped the list at 26%, followed by Identity and Access Management (IAM) at 17%, and Network Communication (NC) at 15%. Unlike Signal Desktop, Cal.com heavily leverages libraries like Prisma, next-auth, and nodemailer for processing personal data, mirroring its primary functions of user registration, email interaction, and scheduling.\
Approximately 97% of privacy-relevant methods invoked by Cal.com handle PII. This attests to the capability of our method in identifying PII processing methods and subsequently guiding code reviewers efficiently. Our approach highlights the extensive use of specific libraries in applications like Cal.com, aligning with their core features. This correlation boosts reviewers’ confidence and precision. By categorizing processing activities, it provides an overview of how the application handles personal data, helping reviewers prioritize effectively. This makes the review process time-efficient and thorough.Our study’s validity may be affected by several factors. The project selection based on GitHub trends could bias towards popular topics, potentially overlooking a broader range of applications. The use of Semgrep for static analysis, though efficient, hasn’t been thoroughly validated for precision, which could impact the accuracy of our results. Reliance on regular expression matching for identifying personal data risks introducing false positives and negatives, thus affecting result reliability.\
Additionally, the absence of manual validation for each instance of personal data processing identified might lead to inaccuracies. Furthermore, focusing only on the top 25 libraries for Java and JavaScript due to resource constraints limits the generalizability of our findings, as other privacy-relevant methods in lesser-known libraries may have been missed.]]></content:encoded></item><item><title>Half of Fossil Fuel Carbon Emissions In 2024 Came From 32 Companies</title><link>https://news.slashdot.org/story/26/01/22/0054237/half-of-fossil-fuel-carbon-emissions-in-2024-came-from-32-companies?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Inside Climate News: Just 32 companies accounted for over half of global fossil carbon emissions in 2024, according to a report published Wednesday by the U.K.-based think tank InfluenceMap. That is down from 36 companies responsible for half the global CO2 emissions in 2023, and 38 companies five years ago. The analysis is the latest update to the Carbon Majors database, which tracks the world's largest oil, gas, coal and cement producers and uses production data to calculate the carbon emissions from each entity's production. The database, first developed by researcher Richard Heede and now hosted by InfluenceMap, quantifies current and historical emissions attributable to nearly 180 companies and provides annual updates. It is the only database of its kind tracking corporate-generated carbon emissions dating back to the start of the Industrial Revolution, research that's being used in efforts to hold major polluters accountable for climate harms.
 
Despite dire warnings from scientists about the consequences of accelerating climate change, fossil fuel production is continuing apace. Last year, fossil fuel CO2 emissions reached a record high, topping 38 billion metric tons. In 2024 these emissions were 37.4 billion metric tons -- up 0.8 percent from 2023 -- and traceable to 166 oil, gas, coal and cement producers, according to the report. Much of the global carbon emissions in 2024 came from state-owned entities, which represented 16 of the top 20 emitters. The five largest emitters overall -- Saudi Arabia's Aramco, Coal India, China's CHN Energy, National Iranian Oil Co. and Russia's Gazprom -- were all state-controlled, and accounted for 18 percent of the total fossil CO2 emissions in 2024.
 
ExxonMobil, Chevron, Shell, ConocoPhillips and BP -- the top five emitting investor-owned companies -- together were responsible for 5.5 percent of the total emissions in that year. Historically, ExxonMobil and Chevron rank in the top five for fossil carbon emissions generated from 1854 through 2024, accounting for 2.79 percent and 3.08 percent of overall carbon pollution, respectively. According to the analysis, the 178 entities in the database have generated 70 percent of fossil CO2 emissions since the start of the Industrial Revolution, and just 22 entities are responsible for one-third of these emissions. "Each year, global emissions become increasingly concentrated among a shrinking group of high-emitting producers, while overall production continues to grow. Simultaneously, these heavy emitters continue to use lobbying to obstruct a transition that the scientific community has known for decades is essential," said Emmett Connaire, senior analyst at InfluenceMap. The findings of the new analysis, he added, "underscore the growing importance of this kind of rigorous evidence in efforts to determine accountability for climate-related losses."]]></content:encoded></item><item><title>Agentic AI Is Forcing Organizations to Rethink How Work Is Designed</title><link>https://hackernoon.com/agentic-ai-is-forcing-organizations-to-rethink-how-work-is-designed?source=rss</link><author>Nitin Pareek</author><category>tech</category><pubDate>Thu, 22 Jan 2026 12:36:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Agentic AI represents a shift in how work, decisions, and accountability are designed. Organizations that focus on tools before clarifying governance, boundaries, and human ownership risk automating dysfunction instead of creating value.]]></content:encoded></item><item><title>Quadric rides the shift from cloud AI to on-device inference — and it’s paying off</title><link>https://techcrunch.com/2026/01/22/quadric-rides-the-shift-from-cloud-ai-to-on-device-inference-and-its-paying-off/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Thu, 22 Jan 2026 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Quadric aims to help companies and governments build programmable on-device AI chips that can run fast-changing models locally.]]></content:encoded></item><item><title>Snapchat gives parents new insights into teens’ screen time and friends</title><link>https://techcrunch.com/2026/01/22/snapchat-gives-parents-new-insights-into-teens-screen-time-and-friends/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 22 Jan 2026 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[With these new features, Snap is likely looking to appease regulators and parents over concerns about safety and screen time on its platform.]]></content:encoded></item><item><title>Tiger Global and Microsoft to fully exit Walmart-backed PhonePe via its IPO</title><link>https://techcrunch.com/2026/01/22/tiger-global-and-microsoft-to-fully-exit-walmart-backed-phonepe-via-its-ipo/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Thu, 22 Jan 2026 11:19:30 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tiger Global and Microsoft are offering up their full stakes in the company, while Walmart is choosing to retain its majority stake and selling up to 45.9 million shares. ]]></content:encoded></item><item><title>Prominent Intel Compiler Engineer Heads Off To AMD</title><link>https://www.phoronix.com/news/Intel-Compiler-Expert-Now-AMD</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 11:12:30 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[James Brodman worked for the last 15 years at Intel on their ISPC SIMD compiler and then in more recent years on the Intel DPC++ compiler and SYCL support as part of Intel's oneAPI initiative. Rather interestingly, this compiler expert has now joined AMD...]]></content:encoded></item><item><title>Data-Driven Ranking Reveals Where Privacy Risks Actually Live in Java and JavaScript Code</title><link>https://hackernoon.com/data-driven-ranking-reveals-where-privacy-risks-actually-live-in-java-and-javascript-code?source=rss</link><author>Code Review</author><category>tech</category><pubDate>Thu, 22 Jan 2026 11:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Data-based Ranking of Privacy-Relevant MethodsOur data-based ranking is designed to identify and prioritize privacy-relevant methods in Java and JavaScript applications. This ranking process comprises several stages, as depicted in Fig. 3, using   \
the Java ranking as an example. By analyzing data from real-world applications, we aim to provide a practical guide for identifying methods that are most relevant for privacy concerns.7.1 Library Selection for Data-based RankingTo focus our data-based ranking on the most relevant libraries, we selected the top 25 libraries from NPM for JavaScript and Maven for Java, shown below in Table 2. Our selection criteria were based on the libraries’ relevance to personal data processing, as aligned with our set of labels for personal data processing activities. This selection was made through a systematic review of each library’s documentation, specifically targeting functionalities that are related to personal data processing.7.2 Method Invocation AnalysisWe employed static analysis tools to identify method invocations and analyze data flows within the code. For Java, we used Soot [14] to construct call graphs and trace method invocations. In the case of JavaScript, we used ESLint 1 for its capabilities in Abstract Syntax Tree (AST) analysis. Our analysis matched these invocations to our list of native privacy-relevant methods, providing a view of how these methods are used in practice.7.3 Selecting Open-source ApplicationsTo rank privacy-relevant methods, we selected 30 popular open-source GitHub projects with over 100 stars in Java and JavaScript. We focused on applications processing personal data rather than frameworks and libraries. The selection included 15 Java applications such as the e-commerce software Shopizer, and 15 JavaScript applications like the chat application RocketChat. We also included projects predominantly in Java/JavaScript that use other languages like TypeScript for some modules.\
Criteria were: popularity (applications with high stars, indicating broader relevance), data sensitivity (applications processing personal or sensitive data, highly relevant for privacy reviews), diversity (applications from different domains and languages, showing wide applicability), and public availability (open source code enables reproducibility and transparency). The details of these selected projects are provided in Table 4.   7.4 Efficient Analysis of Library ImportsTo make the analysis efficient, we first identified the libraries imported by each application. For standard libraries, we assumed their presence in most applications. For API libraries, we examined import statements and configuration files to narrow down our focus to the top 50 pre-selected libraries, 25 each for Java and JavaScript.7.5 Ranking Privacy-relevant Methods in Top 30 ApplicationsWe employed Semgrep to monitor the flow of personal data into privacy-relevant methods invoked by application code. Utilizing Semgrep’s DeepSemgrep 2 capability for cross-file analysis, we were able to comprehensively analyze data flows across entire applications, as opposed to only examining isolated code snippets. This provided a holistic perspective of how personal data propagates across different components. Using Semgrep’s taint analysis and the rules outlined in Section 6, we traced personal data flows to privacy-relevant methods.\
To assess the practical relevance of our identified privacy-relevant methods, we introduce the following usage-based metrics, presented in Table 3: We ranked privacy-relevant methods by analyzing their usage in the 30 popular GitHub projects introduced above, with an average of 358 application methods processing personal data per application. This varied by language and type: Java applications averaged 288 methods, while JavaScript had 363. The higher average in JavaScript was likely due to its more diverse front-end processing, reflecting the complexity and multifaceted nature of these applications.   To better focus our approach, we calculated the proportion of application methods that both invoke a privacy-relevant method and process a concrete flow of personal data (there is confirmed personal data flow into the method). This is relative to the total number of methods in the application. This metric indicates the level of focus in identifying privacy-relevant methods, allowing developers to narrow their efforts to a more relevant subset of the code. In essence, our approach aims to minimize the code sections that need scrutiny, saving both time and resources. For more details on these proportions in selected open-source Java and JavaScript/TypeScript applications, see Table 4.Our study reveals that, on average, only 4.2% of the total codebase is made up of methods that are privacy-relevant and involved in personal data processing. This result highlights the precision of our approach in pinpointing privacy-relevant methods in applications.Usage Patterns of Privacy-Relevant Methods In Java applications, we observed a more conservative use of privacy-relevant methods, particularly those from popular Maven libraries. Native Java methods, along with methods from Apache Commons and the Spring framework, were frequently used for handling personal data. Libraries such as slf4j for logging and auth0 for authentication were also commonly used, indicating their importance in the flow and protection of personal data.\
In contrast, JavaScript applications exhibited a diverse range of library usage. While lodash was commonly used, frameworks like Angular, React, and Vue.js played a significant role in personal data processing, particularly in front-end applications. Table 5 presents the top five packages in both Java and JavaScript that contain methods relevant to privacy concerns.Categories of Privacy-relevant Methods We categorized privacy-relevant methods into types to gain insights into their roles in personal data processing. Our analysis identified several Java classes and categories that are frequently involved in personal data processing. For example, common Java classes like org.slf4j.Logger and auth0.client.Auth0Client are often used in operations that handle personal data.\
In terms of categories, Data Processing and Transformation, Network Communication, and Logging Methods were most prevalent. These categories indicate areas where privacy-relevant methods are most commonly used, suggesting that they are key to understanding how personal data is processed in codebases (Table 6). Identity and Access Management, Data Encryption and Cryptography, and Data Storage and Database Management were also highly involved in personal data flows, with involvement percentages of 92%, 78%, and 85%, respectively.\
Conversely, categories like Data Processing and Transformation, Network Communication, and Logging Methods were less involved, with percentages of 67%, 44%, and 28%. Table 7 lists Java classes that are frequently involved in personal data processing, serving as key indicators for identifying privacy-relevant methods in applications.]]></content:encoded></item><item><title>Former Google trio is building an interactive AI-powered learning app for kids</title><link>https://techcrunch.com/2026/01/22/former-google-trio-is-building-an-interactive-ai-powered-learning-app-for-kids/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 22 Jan 2026 11:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Sparkli said that education systems often fall behind in teaching modern concepts. The company wants to teach kids about topics like skills design, financial literacy, and entrepreneurship by creating an AI-powered learning "expedition."]]></content:encoded></item><item><title>ReactOS Celebrates 30 Years In Striving To Be An Open-Source Windows Implementation</title><link>https://www.phoronix.com/news/ReactOS-30-Years-Old</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 10:57:54 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The ReactOS project is celebrating today that it marks 30 years since their first code commit in the ReactOS source tree. During the past 30 years now the project has seen more than 88k commits from more than 300 developers as it seeks to be a robust open-source Windows implementation. In their 30 year birthday blog post they also provide a look ahead at what they're working on...]]></content:encoded></item><item><title>Wikipedia&apos;s Guide to Spotting AI Is Now Being Used To Hide AI</title><link>https://news.slashdot.org/story/26/01/22/015250/wikipedias-guide-to-spotting-ai-is-now-being-used-to-hide-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Ars Technica's Benj Edwards reports: On Saturday, tech entrepreneur Siqi Chen released an open source plugin for Anthropic's Claude Code AI assistant that instructs the AI model to stop writing like an AI model. Called "Humanizer," the simple prompt plugin feeds Claude a list of 24 language and formatting patterns that Wikipedia editors have listed as chatbot giveaways. Chen published the plugin on GitHub, where it has picked up over 1,600 stars as of Monday. "It's really handy that Wikipedia went and collated a detailed list of 'signs of AI writing,'" Chen wrote on X. "So much so that you can just tell your LLM to... not do that."
 
The source material is a guide from WikiProject AI Cleanup, a group of Wikipedia editors who have been hunting AI-generated articles since late 2023. French Wikipedia editor Ilyas Lebleu founded the project. The volunteers have tagged over 500 articles for review and, in August 2025, published a formal list of the patterns they kept seeing.
 
Chen's tool is a "skill file" for Claude Code, Anthropic's terminal-based coding assistant, which involves a Markdown-formatted file that adds a list of written instructions (you can see them here) appended to the prompt fed into the large language model (LLM) that powers the assistant. Unlike a normal system prompt, for example, the skill information is formatted in a standardized way that Claude models are fine-tuned to interpret with more precision than a plain system prompt. (Custom skills require a paid Claude subscription with code execution turned on.)
 
But as with all AI prompts, language models don't always perfectly follow skill files, so does the Humanizer actually work? In our limited testing, Chen's skill file made the AI agent's output sound less precise and more casual, but it could have some drawbacks: it won't improve factuality and might harm coding ability. [...] Even with its drawbacks, it's ironic that one of the web's most referenced rule sets for detecting AI-assisted writing may help some people subvert it.]]></content:encoded></item><item><title>From Written Off to Tech Lead: How Gowtham Reddy Kunduru Built Engineering Leadership</title><link>https://hackernoon.com/from-written-off-to-tech-lead-how-gowtham-reddy-kunduru-built-engineering-leadership?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Thu, 22 Jan 2026 09:45:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[ is a lead software engineer with a successful career spanning healthcare, FinTech, and cloud architecture. His success wasn’t always assured; in fact, as he puts it, his “story began with setbacks.” In college, I was written off as someone who would never amount to much. Being detained twice was humiliating, but it became the turning point in my life. I realized that if I didn’t take control, no one else would. So, I rebuilt myself with discipline, consistency, and a refusal to quit,” Kunduru says.That determination and dedication to course-correcting his path led him not only to graduate but also to build a career marked by solving problems others considered impossible.Kunduru’s college years were difficult for him, and he admits to struggling with discipline and direction. “Many people around me, including my own family, believed I would follow the same path as my father, who never found success,” Kunduru explains.It was his discovery of software engineering that inspired him to change his life. He says he was intrigued by building something from “nothing but logic and determination.” Technology was his reset button, and he pushed it. From Startup to Tech LeadershipAfter graduating, Kunduru joined an agriculture-focused startup. As an associate software engineer, he developed impactful technical solutions for the company. He was awarded Employee of the Year and received the Innovation of the Year Award in 2013.“That job became my true foundation. Because it was a startup, I had to do everything: front-end, backend API, database, deployment, and support. It forced me to grow rapidly and taught me the value of hard work and responsibility,” Kunduru says. Kunduru would later work on high-impact projects for Innova Solutions. Again, his hard work and dedication led to his being promoted twice within four years. He became the principal software engineer, leading a team of 12 engineers.After Kunduru moved to the United States in 2020 to work with leading healthcare clients, he was approached by a former client seeking to hire him for his tech leadership and delivery record. He led NLP (natural language processing) and OCR (optical character recognition) initiatives in large-scale healthcare projects. His work processed over 158 million health records to generate insights for entire patient cohorts, revealing patterns that informed care decisions across populations.“I’ve been fortunate to build a career defined by curiosity, continuous learning, and solving complex engineering challenges,” Kunduru says.Shifting To Fintech EngineeringKunduru decided to shift his career into the FinTech industry in 2022 when he joined M&T Bank. His hard work, dedication to continuous learning, and results were quickly recognized, leading to his becoming an SME and Tech lead within a year. As a team leader of 6 engineers, he oversaw the creation of enterprise-grade microservices and the delivery of an Adobe ColdFusion migration from 2016 to 2023 that improved performance by 33% and reduced server load by 30%, enabling faster, more reliable service for 2.5+ million customers. His engineering leadership earned him second place in the M&T Cybersecurity Secure Coding Tournament and third place in the Secure Coding Championship, competing against engineers across the entire technology organization. \n “I became the first engineer to successfully establish Kerberos authentication between on-prem Windows servers and Azure COLO at M&T Bank, a feat even Adobe told us was impossible,” Kunduru says. Kunduru aspires to remain a technology leader, driving innovations that impact millions of people worldwide. He wants to mentor future engineers who come from “humble or challenging backgrounds” as he did and show them that success is a “decision, not a privilege.” “What makes me stand out is not just the technical capability, it’s the resilience.  \n I went from being labelled a failure to becoming someone recognized for solving problems others give up on. My journey shows that your background doesn’t limit your potential, your perseverance does,” Kunduru says. ]]></content:encoded></item><item><title>The TechBeat: CodeRabbit vs Code Reviews in Kilo: Which One Is Best For You in 2026 (1/22/2026)</title><link>https://hackernoon.com/1-22-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Thu, 22 Jan 2026 07:11:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @drechimyn [ 7 Min read ] 
 Broken Object Level Authorization (BOLA) is eating the API economy from the inside out.  Read More.By @ivankuznetsov [ 9 Min read ] 
 It’s far more efficient to run multiple Claude instances simultaneously, spin up git worktrees, and tackle several tasks at once. Read More.By @dataops [ 4 Min read ] 
 DataOps provides the blueprint, but automation makes it scalable. Learn how enforced CI/CD, observability, and governance turn theory into reality. Read More.By @socialdiscoverygroup [ 19 Min read ] 
 We taught Playwright to find the correct HAR entry even when query/body values change and prevented reusing entities with dynamic identifiers.  Read More.By @kilocode [ 6 Min read ] 
 CodeRabbit alternative for 2026: Kilo's Code Reviews combines AI code review with coding agents, deploy tools, and 500+ models in one unified platform. Read More.By @rahul-gupta [ 8 Min read ] 
 As AI adoption grows, legacy data access controls fall short. Here’s why zero-trust data security is becoming essential for modern AI systems. Read More.By @praisejamesx [ 6 Min read ] 
 Stop relying on "vibes" and "hustle." History rewards those with better models, not better speeches. Read More.By @proflead [ 4 Min read ] 
 Ollama is an open-source platform for running and managing large-language-model (LLM) packages entirely on your local machine. Read More.By @David [ 37 Min read ] 
 History of AI Timeline tracing the road to the AI boom. Built with Claude, Gemini & ChatGPT as a part of the launch of HackerNoon.ai, covering 251 events. Read More.By @mohansankaran [ 10 Min read ] 
 Jetpack Compose memory leaks are usually reference leaks. Learn the top leak patterns, why they happen, and how to fix them. Read More.By @mcsee [ 3 Min read ] 
 Set your AI code assistant to read-only state before it touches your files. Read More.By @ishanpandey [ 5 Min read ] 
 BTCC reports $5.7B tokenized gold volume in 2025 with 809% Q4 growth, marking gold as crypto's dominant real-world asset. Read More.By @linked_do [ 12 Min read ] 
 As the AI bubble deflates, attention shifts from scale to structure. A long view on knowledge, graphs, ontologies, and futures worth living. Read More.By @vinitabansal [ 12 Min read ] 
 You’re a reactive leader if you spend most of your time reacting to the things in your environment. Read More.By @sanya_kapoor [ 16 Min read ] 
 A 60-day test of 10 Bitcoin mining companies reveals which hosting providers deliver the best uptime, electricity rates, and ROI in 2026. Read More.By @scottdclary [ 27 Min read ] 
 Real transformation requires your brain to physically rewire itself. Read More.]]></content:encoded></item><item><title>New Research Shows 64% of Third-Party Applications Access Sensitive Data Without Authorization</title><link>https://hackernoon.com/new-research-shows-64percent-of-third-party-applications-access-sensitive-data-without-authorization?source=rss</link><author>CyberNewswire</author><category>tech</category><pubDate>Thu, 22 Jan 2026 07:06:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Boston, MA, USA, January 21st, 2026, CyberNewsWire/-- today announced the release of its , revealing a sharp escalation in client‑side risk across global websites, driven primarily by third‑party applications, marketing tools, and unmanaged digital integrations.According to the new analysis of 4,700 leading websites, 64% of third‑party applications now access sensitive data without legitimate business justification, up from 51% last year — a 25% year‑over‑year spike highlighting a widening governance gap.The report also exposes a dramatic surge in malicious web activity across critical public‑sector infrastructure. Government websites saw malicious activity rise from 2% to 12.9%, while 1 in 7 Education websites now show active compromise, quadrupling year‑over‑year. Budget constraints and limited manpower were cited as primary obstacles by public‑sector security leaders.The research identifies several widely used third‑party tools as top drivers of unjustified sensitive‑data exposure, including Google Tag Manager (8%), Shopify (5%), and Facebook Pixel (4%), which were frequently found to be over‑permissioned or deployed without adequate scoping.“Organizations are granting sensitive‑data access by default rather than exception — and attackers are exploiting that gap,” said VP of Product at Reflectiz, Simon Arazi. “This year’s data shows that marketing teams continue to introduce the majority of third‑party risk, while IT lacks visibility into what’s actually running on the website.”64% of apps accessing sensitive data have no valid justification.47% of applications running in payment frames (checkout environments) are unjustified.Compromised sites connect to 2.7× more external domains, load 2× more trackers, and use recently registered domains 3.8× more often than clean sites.Marketing and Digital departments account for 43% of all third‑party riskThe report also introduces updated Security Leadership Benchmarks, highlighting the very small group of organizations meeting all eight criteria. Only one website — ticketweb.uk — achieved a perfect score across the framework.The 2026 report includes:Sector‑by‑sector breakdowns of web exposure riskFull list of high‑risk third‑party applicationsYear‑over‑year industry trendsTechnical indicators of compromiseBest‑practice controls for security and digital teamsThe complete 43‑page analysis is available for download: empowers organizations to secure their websites and digital assets against modern web threats. Its award-winning, agentless platform provides continuous visibility into all client-side activity, detecting and prioritizing security, privacy and compliance risks. Reflectiz is trusted by global enterprises across financial services, e-commerce, and healthcare to protect their data, users, and brand reputation.:::tip
This story was published as a press release by Cybernewswire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Blue Origin&apos;s Satellite Internet Network TeraWave Will Move Data At 6 Tbps</title><link>https://tech.slashdot.org/story/26/01/22/0044240/blue-origins-satellite-internet-network-terawave-will-move-data-at-6-tbps?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Blue Origin has unveiled an enterprise-focused satellite internet network called TeraWave, which promises up to 6 Tbps speeds via a mixed low- and medium-Earth orbit constellation. TechCrunch reports: The TeraWave constellation will use a mix of 5,280 satellites in low-Earth orbit and 128 in medium-Earth orbit, and Blue Origin plans to deploy the first ones in late 2027. It's not immediately clear how long Blue Origin expects it will take to build out the whole network. The low-Earth orbit satellites Blue Origin is building will use RF connectivity and have a max data transfer speed of 144 Gbps, while the medium-Earth variety will use an optical link that can achieve the much higher 6 Tbps speed. For reference, SpaceX's Starlink currently maxes out at 400 Mbps -- though it plans to launch upgraded satellites that will offer 1 Gbps data transfer in the future. "We identified an unmet need with customers who were seeking enterprise-grade internet access with higher speeds, symmetrical upload/download speeds, more redundancy, and rapid scalability for their networks. TeraWave solves for these problems," Blue Origin said in a statement.]]></content:encoded></item><item><title>Turns Out 30% of Your AI Model Is Just Wasted Space</title><link>https://hackernoon.com/turns-out-30percent-of-your-ai-model-is-just-wasted-space?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Thu, 22 Jan 2026 06:09:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[AI models aren’t actually too big. New research shows nearly 30% of their size is wasted due to outdated storage assumptions—and fixes it without losing accuracy.]]></content:encoded></item><item><title>The NVIDIA Nemotron Stack For Production Agents</title><link>https://hackernoon.com/the-nvidia-nemotron-stack-for-production-agents?source=rss</link><author>Paolo Perrone</author><category>tech</category><pubDate>Thu, 22 Jan 2026 06:05:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[NVIDIA just dropped a production-ready stack where speech, retrieval, and safety models were actually designed to compose.]]></content:encoded></item><item><title>Rent or Own? How the &quot;Rug Pull&quot; Era is Pushing Developers Toward Source-Available Software</title><link>https://hackernoon.com/rent-or-own-how-the-rug-pull-era-is-pushing-developers-toward-source-available-software?source=rss</link><author>Adis</author><category>tech</category><pubDate>Thu, 22 Jan 2026 06:04:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[From “rug pulls” to corporate warfare, the rules of software ownership are being rewritten. But a new model—Post-SaaS—might finally offer a way out.For decades, the deal was simple: developers gave their time, and companies gave their code. It was an unwritten social contract built on trust. Then, in a few short months, the contract was shredded.First came the tremors. In August of 2023, HashiCorp announced that Terraform—the industry standard for infrastructure-as-code—was switching to a “Business Source License,” effectively walling off competitors. The aftershocks followed in March 2024, when Redis, the database powering a considerable chunk of the modern web, abandoned its open-source roots in favor of more restrictive terms.To the C-suites, these were necessary pivots to protect revenue from cloud giants like AWS. But to the millions of engineers who had built their careers and stacks on these tools, it felt like something else entirely: a rug pull.The premise of these shifts was that the “Open Core” model was broken—that you couldn’t build a profitable business by giving away the recipes. But this reactionary move misses a fundamental truth about the modern software economy. By trying to lock down their code, these companies didn’t just lose the moral high ground; they inadvertently proved that in 2026, the code itself is no longer the asset. The community is.The Revolt: Why You Can’t Close the Barn DoorWhen a company closes a previously open project, they don’t just lose users; they create a martyr.The immediate reaction to the Terraform and Redis announcements wasn’t just anger—it was action. The Linux Foundation stepped in, backing “OpenTofu” (a fork of Terraform) and “Valkey” (a fork of Redis). Almost overnight, the original companies found themselves competing against free, community-driven versions of their own products.As Madelyn Olson, a core Redis maintainer who left to build Valkey, put it: “I worked on open source Redis for six years… By forming Valkey, contributors can pick up where we left off and continue true open source development.”The lesson here is stark: You cannot retroactively close a community-built project without destroying your reputation. The talent leaves, the momentum shifts, and the “rug pull” strategy often backfires, creating a new competitor with the moral high ground.If the “Rug Pull” is a desperate attempt to monetize the asset, the “Platform” model proves you don’t need to own the asset to monetize it. You need to be the best place to keep it.The clearest example of this is Hugging Face. Often described as the “GitHub of AI,” Hugging Face hosts over one million models, datasets, and demos—almost all of them open source and free to download. By the logic of the “Rug Pull” CEOs, this should be a disaster. Why would anyone pay Hugging Face when they can download the Llama 3 weights and run them locally?The answer lies in the friction of modern infrastructure. Hugging Face generated over $70 million in revenue (2023), not by gating access to the algorithms, but by selling the “compute” and “enterprise security” required to run them.They understood a fundamental truth about developers: we are lazy and busy. We  spin up our own AWS instances, configure the CUDA drivers, and secure the endpoints—or we could pay Hugging Face $0.50 an hour to click a single button labeled “Deploy”.This is the Platform Moat. While HashiCorp and Redis were busy building legal fences around their code, Hugging Face was building a toll road. They realized that in an era of abundant open-source software, the scarce resource isn’t the code; it’s the convenience."By trying to lock down their code, these companies didn't just lose the moral high ground; they inadvertently proved that in 2026, the code itself is no longer the asset. The community is."While Hugging Face proves you can build a business  of open source, Meta proves you can use open source to burn a competitor’s business to the ground. This is the strategy known in economics as “Commoditizing the Complement.”For OpenAI and Google, the AI model is the product. They spend billions training GPTs and Gemini, intending to rent access to them. Their entire business model relies on the model being a scarce, proprietary secret.Enter Meta. By releasing Llama—a state-of-the-art LLM—for free, Mark Zuckerberg isn’t just being altruistic; he is devaluing the core product of his rivals. If developers can get 95% of GPT -4’s performance for $0 by using Llama, the market price for “intelligence” drops toward zero.This is a defensive play ripped straight from the 2000s playbook. Just as Google released Android for free to prevent Microsoft and Apple from owning the mobile internet, Meta is releasing Llama to prevent OpenAI from owning the AI internet.For the developer, this is a windfall. We get enterprise-grade tools without the enterprise price tag. But let’s be clear about the dynamic: we aren’t being given a gift; we are being handed ammunition in a war between giants. Meta’s bet is simple: if everyone builds on Llama, the ecosystem locks into their standards (PyTorch, etc.), and the “walled gardens” of closed AI find themselves guarding an empty castle.Author’s disclaimer: Meta’s AI business strategy and business strategy of their other products like Facebook, Instagram, and WhatsApp are vastly different, which I, along with many others, try to avoid—but that is a story for another time.Conclusion: The Post-SaaS ReformationIf the “Rug Pulls” of 2024 taught us that we can’t trust corporations to keep their code open, and the “AI Wars” taught us that open source is often just a weapon for giants, where does that leave the rest of us?It leaves us looking for a third way—one that rejects both the “Rental Economy” of SaaS and the “Rug Pull Risk” of open core.Enter the “ONCE” philosophy, championed by 37signals (the creators of Rails). With the launch of Campfire and Writebook, they introduced a model that feels radical simply because it is retro: You pay once. You install it. You own it.David Heinemeier Hansson calls this the “Post-SaaS” era. The license isn’t Open Source (you can’t resell it), but it is . More importantly, it is irrevocable. Once you download the code to your server, no board of directors can change the terms. No acquisition by a competitor can shut it down.As Hansson puts it: “SaaS is the ultimate trap. You rent your tools, you rent your data, and the landlord can raise the rent whenever they want. ONCE is about returning to software you actually own.”This is the lesson for the next decade of the software business. The “Moat” is no longer the code—it’s the trust. Developers are tired of building on quicksand. Whether it’s through the “Platform Model” of Hugging Face or the “Ownership Model” of ONCE, the winning companies of 2026 will be the ones that sign a new contract with their users: We don’t want to lock you in. We want to be so good you don’t want to leave.Are we entering a period in which the most profitable software is free and open for personal use? The time will tell.]]></content:encoded></item><item><title>Can Large Language Models Develop Gambling Addiction?</title><link>https://hackernoon.com/can-large-language-models-develop-gambling-addiction?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:51:32 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Instead of vague fixes like "add safety guardrails to your prompts," we have a mechanistic understanding that lets us design targeted interventions. ]]></content:encoded></item><item><title>Navigating Architectural Trade-offs at Scale to Meet AI Goals in 2026</title><link>https://hackernoon.com/navigating-architectural-trade-offs-at-scale-to-meet-ai-goals-in-2026?source=rss</link><author>ANUP Moncy</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:47:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Primary bottleneck for Enterprise AI is  the availability of tools or the identification of a tech stack, it is getting the data landscape in order.Success in 2026 is predicated on having total clarity of the underlying data infrastructure and establishing a foundation that is petabyte-scale, secure, and high-performing.Without a reliable data layer, AI initiatives remain experimental rather than transformational.Foundation (Scalable and Maintainable Data Acquisition)A useful litmus test for the engineering foundation is time to insigths: If we identify a new data source or a new requirement, how short can the lead time be before it is available for analytics and AI?Continuously driving this number down is one of the most critical responsibilities of the data platform.This requires implementing well-established frameworks that allow teams to onboard new data sources quickly without reinventing the architecture each time.This typically involves a strategic mix of:Low-Code / No-Code Ingestion: Leveraging managed services (for example, Fivetran, Airbyte, or Snowflake Native Connectors) for standard SaaS and database sources helps reduce engineering overhead and accelerate delivery where differentiation is low. or custom Automated Frameworks for complex, proprietary, or high-stakes sources, metadata-driven ingestion engines built using Python and dbt allow pipelines to be created consistently and at scale. Underlying platform internals (Snowflake / AWS) must be explicitly architected to handle bursty AI workloads. This requires a stable and secure foundation that uses auto-scaling compute and workload isolation to maintain predictable performance baselines. AI-aware feedback loop captures structured signals from AI workloads and feeds them back into the data platform. These signals include data freshness violations, schema drift, low-confidence predictions, hallucination indicators, user overrides, and cost or latency metrics. Captured signals are stored as structured, queryable datasets and treated as first-class data assets to report and adjust operational behavior.No Compromise on Software Engineering Practices for Data Assets: Providing clear platform and infrastructure management direction ensures that coding standards and infrastructure-as-code practices support long-term system health rather than short-term delivery.Establishing Discovery, Reliability and Governance at ScaleHow much time does a user take to discover the right data for thier needs and gain the required access and start gaining insigths (time-to-insight).Make this automated, rule driven yet with absolutly no compramize on security and regulatory requirements.Governance is baked into the engineering foundation through robust identity management and clear data transparency.Automated Data Quality Guardrails to ensures only “trusted data” reaches the AI model, maintaining a high-performing and reliable baseline for downstream consumption.Centralized Data Catalog and Discoverability prioritizing a robust data catalog to ensure petabyte-scale assets are searchable and well-documented. This visibility reduces “time-to-insight” by allowing data consumers and AI agents to quickly identify and verify the correct data assets. Establishing a secure-by-design architecture through centralized  (identity verification) and granular  (role-based access control).Architecture as the Enforcement Mechanism: Using Infrastructure-as-Code (Terraform/CloudFormation) to standardize these guardrails to ensure is created with correct security and cataloging configurations, removing human error and building a maintainable ecosystem.Data Contracts and Cost as Architecture: At scale, trust and predictability require explicit  between producers and consumers, covering schema expectations, freshness SLAs, quality thresholds, and access guarantees.Along with this, cost becomes a first-class architectural signal:Usage-based cost attribution by domainBudget-aware scaling for AI workloadsGuardrails to prevent runaway experimentationEensure that the data infrastructure empowers teams rather than becoming a bottleneck, focusing on the strategic placement of both human and technical assetsDecentralized Ownership with Centralized Governance: Positioning domain teams to own their data products while maintaining a central engineering foundation for Authentication, Authorization, and Infrastructure.Tooling for Efficiency, Not Complexity: Selecting tools based on the team’s ability to maintain them. This involves strategic use of  ingestion for high-velocity requirements and reserving custom  frameworks for complex, high-stakes architectural needs.Establish core platform engineering team as a service provider to the rest of the enterprise. The focus is on building a maintainable engineering foundation and a discoverable data catalog that other business units can consume autonomously.Bridging Technical Design and Business Objectives: Ensuring that the technical team’s roadmap is consistently aligned with management direction. This positioning prevents “engineering for engineering’s sake” and keeps the focus on delivering secure, petabyte-scale solutions that meet 2026 AI goals.Meeting AI goals in 2026 is not about chasing tools, models, or architectural trends.It is about building a data platform that is intentionally boring in its reliability and relentlessly opinionated in its standards.Organizations that succeed will treat data infrastructure as a , not a one-time project — optimizing for fast onboarding, trust at scale, and continuous feedback between data, AI systems, and business outcomes.When ingestion is predictable, governance is automated, discovery is effortless, and teams are empowered rather than constrained, AI stops being experimental.At that point, the question is no longer:“How fast can we safely scale it?”\
This article is co-authored by Google Gemini.(my opinions and perspectives made structured and blog worthy by AI)]]></content:encoded></item><item><title>The FrankenPHP Version Trap: Why Your Laravel Octane Stack Isn’t Using PHP 8.5</title><link>https://hackernoon.com/the-frankenphp-version-trap-why-your-laravel-octane-stack-isnt-using-php-85?source=rss</link><author>Daniel, Andrei-Daniel Petrica</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:45:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Debugging the version mismatch that Octane doesn't tell you about.]]></content:encoded></item><item><title>Essential Cybersecurity Measures Every Modern Business Should Take</title><link>https://hackernoon.com/essential-cybersecurity-measures-every-modern-business-should-take?source=rss</link><author>YASH PAL</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:44:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Modern businesses run on digital trust. Customers expect their data to be safe, and partners expect operations to be reliable. To meet those expectations, security must be a daily practice, not a yearly project.This article walks through practical measures that reduce risk fast. Each section focuses on actions you can apply in most environments. The goal is simple - shrink the attack surface and strengthen your defences without adding needless complexity.Assessing Your Risk LandscapeStart with a clear view of what you must protect. List your critical assets, map where sensitive data lives, and note who can access it. This inventory becomes the foundation for every security decision.Next, identify the most likely threats to those assets. Ransomware, phishing, credential theft, and exposed cloud resources top the list for most teams. Rank scenarios by business impact and likelihood to guide your roadmap.Finally, tie risks to controls and owners. Each high risk needs a control you can measure and a person who is accountable. Simple dashboards help track progress and keep plans grounded in reality.Building A Strong Identity And Access FoundationIdentity is the new perimeter, so start with strong authentication. Require multifactor authentication for admins and remote users, then expand to all users and key apps. Keep login prompts smart with conditional access and risk signals.Role-based access helps you grant the least privilege by default. Having strong Cybersecurity for comprehensive threat defense means uniting identity controls with continuous monitoring, timely patching, and rehearsed incident response so gaps are found and fixed fast. Rotate credentials and use passwordless methods where possible.Protect machine identities, too. Use managed secrets, short-lived tokens, and just-in-time elevation. Audit service accounts and remove broad permissions that no longer serve a purpose.Email And Phishing Defence That Actually WorksStart with layered email security. Enable DMARC, DKIM, and SPF to reduce spoofing. Use advanced filtering to block malware, links to known bad domains, and suspicious attachment types.Assume some messages will slip through. Train employees to spot social engineering and to report suspected phishing quickly. Keep training short, frequent, and tied to real examples that match your industry.Reduce the blast radius when mistakes happen. Disable macros by default, open risky documents in isolated containers, and limit what a user can do with a single click. Fast containment beats perfect prevention.Network Segmentation And Zero Trust BasicsTreat networks as untrusted by default. Segment critical systems away from general user zones and restrict east-west traffic. Microsegmentation in data centres and cloud helps keep intruders from moving freely.Adopt least privilege at the network layer. Use identity-aware proxies and policy engines that evaluate users, devices, and context before granting access. Logs from these decisions become gold for detection and investigation.Keep an eye on remote access pathways. Replace legacy VPNs with modern access brokers where practical. Monitor for unusual patterns like new geographies, odd hours, or sudden spikes in data transfer.Backups, Recovery, And Business ContinuityAssume a day when systems fail or get encrypted. Build a 3-2-1 backup plan with offline or immutable copies. Test restores on a schedule, not just the backup job itself.Document recovery steps for each critical service. Who declares an incident? Where are the runbooks, and what is the order of operations? Practice tabletop scenarios so people know their roles under pressure.Plan for partial operations. Can you run core finance, sales, and support if email is down? Can your warehouse ship if the main ERP is offline? Small continuity wins reduce stress during real events.Security Monitoring And Incident ResponseVisibility turns noise into action. Centralise logs from identity, endpoints, cloud, and network into a platform your team can actually use. Tune alerts to focus on high-fidelity signals like impossible travel or privilege escalation.Harden endpoints with EDR and strong baselines. Block known bad behaviours and auto-isolate compromised devices. Pair detections with rapid playbooks that collect forensics and notify humans only when needed.Create an incident response framework that scales. Define severity levels, communication paths, and decision points. After each incident, run a blameless review and turn lessons into updated controls.Secure Software And Cloud ConfigurationBake security into the development process. Use code scanning, dependency checks, and secret detection in your pipelines. Fix the highest risk issues before code reaches production.Harden cloud accounts with guardrails. Enforce encryption at rest and in transit, restrict public exposure, and monitor for misconfigurations. Tag resources and tie them to owners to avoid orphaned services.Protect APIs with strong authentication and rate limits. Log requests, validate inputs, and watch for spikes or odd patterns. Version your APIs and retire legacy endpoints that no longer serve business value.No business can remove all cyber risk, but every business can make smart moves that reduce it. Start with identity, segment your networks, and plan your recovery steps. Keep improving a little each quarter, and your security posture will grow stronger.Security is a journey powered by small, steady choices. Build processes that people can follow and tools they can trust. Those choices add up to resilience that customers and teams can count on.]]></content:encoded></item><item><title>How to Start a Career as a Junior Developer in 2026</title><link>https://hackernoon.com/how-to-start-a-career-as-a-junior-developer-in-2026?source=rss</link><author>Leon Revill</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:41:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The "Junior Developer" role is collapsing (down 46%), but a new path is emerging. ]]></content:encoded></item><item><title>Math in the Age of Machine Proof</title><link>https://hackernoon.com/math-in-the-age-of-machine-proof?source=rss</link><author>franzhusch</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:40:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This is an opinion piece based on my research and ideas. I recently read the paper by Alex Kontorovich, The Shape of Math To Come, which inspired me to contemplate the future of mathematics and mathematicians. However, I will not go into as much detail, but rather state a high-level overview of ideas.Autoproving vs AutotranslationBefore we start, we need to differentiate between , giving a formalized proof to a formalized statement, and , the act of taking an informal statement (such as the definition of a Vector Space or a Conjecture) and transferring it into a formalized language like Lean.A notable observation is that, autotranslation lacks inherent verification and cannot be fully automated. While AI can translate natural language into formalized language (such as Lean), no formal proof exists to confirm that the formalized statement matches the informal intent. A human must still manually verify that the resulting symbols correctly represent the original mathematical idea.Autoproving, on the other hand, can be completely automated; given a formalized statement, we can trust the formalized proof to be correct. This is, of course, assuming that the environment in which the verification happens is immune to reward hacking or adversarial attacks on the verification. Making verification robust is a problem which the Lean FRO is well aware of, with the latest addition being the Comparator Verifier.There is also , which can be seen as autotranslation followed by autoproving.I want to introduce a thought experiment involving an autoproving system capable of proving anything that humanity has ever formalized, and enabling the proof of any new formalized statement in a matter of minutes or hours. Such a system, which I will just dub "Math Singularity", would be the extreme end along a spectrum of autoproving abilities.What would doing mathematics look like with such a system at our disposal? Would it mean that we are only formalizing statements, building and exploring theories, and rapidly answering any question we formalize? Developing a big program or theory such as the Langlands Program would probably much more resemble the workflow or contributions of human mathematicians.One can, of course, also argue that a system capable of proving everything known to humanity could also be engineered and utilized to create entirely new theories and complete new fields of mathematics, but that would be of no use to humans. We need humans to interpret it to advance the knowledge corpus of humanity—unless we simply decide to hand off the interpretation and utilization of these scientific advancements in math completely to AI, at which point we would have to raise entirely different questions.How might this transition look like?We can roughly sketch the spectrum of autoproving capabilities as follows:I define MST (Mathematical Superintelligence) as a system vastly more intelligent than the most intelligent human mathematicians, while still being unable to prove extremely hard problems such as the Millenium Problems or the Landau Problems.We are currently at a point where small theorems can be independently proven, such as a recent Erdős problem as documented by Terence Tao's Mastodon Post. The further we proceed along the spectrum of autoproving, the less proofs become the bottleneck, enabling humanity to explore the mathematical landscape more throughly.Currently, informal (natural language) math is advancing faster than formalized math. If autoproving systems become better, it will be of benefit for frontier mathematicians to formalize their current area of research to leverage these systems' capabilities. Consequently, more projects and workshops will likely emerge to formalize frontier research fields within Lean. In this vein, Lean serves as the interface for autoproving systems, while also providing the benefit of formalized correctness into frontier research.How might human collaboration and papers develop?Mathematicians write papers to introduce new knowledge, an important ingredient being a correct proof, ensuring the newly introduced theorems and knowledge are consistent with the existing knowledge corpus. Moving along the autoproving spectrum will lead to higher abstraction, where proofs of smaller lemmas fall into the background by being a Lean reference, and results provable by autoproving systems are not worthy of their own paper anymore.There will most likely still be hybrid proofs for statements outside of the current reach of autoproving systems, where humans supplement by providing structure or insight to the proof to various extents.Integrating vast databases of formalized proofs into existing academic frameworks presents another significant hurdle. While Mathlib is an easy example, its current architecture may face scalability issues when attempting to encompass all of humanity's mathematical knowledge. We may see a evolving field of different institutional databases, or a unified repository similar to arXiv for preprints might eventually crystallize. These are all problems which can be solved, but must be tackled to enable more proper utilization of autoproving systems.Lean is currently the most used programming language in the realm of formalization of math and will likely stay the most relevant language for the foreseeable future, serving as the foundational layer for these advancements.Regarding the development of AI systems for autoproving, DeepMind and Harmonic AI are currently the biggest labs in the field, with Alphaproof and Aristotle respectively. However, there are many teams at various labs and smaller companies working on autoproving systems, such as ByteDance with Seedprover or Alephprover from Logical Intelligence.Math as we know it is about to change, and I think many are feeling that.The synergy between Math and Lean represents a unique opportunity for unbound continual learning, as its formal environment allows for continuous improvement independent of real-world constraints.If we can overcome these remaining challenges of scaling formalization, we may soon witness a golden age of results in mathematics.\
Edit 1: I have switched the naming of Autoformalization for Autotranslation following a comment made by Alex Kontorovich.]]></content:encoded></item><item><title>A Step-by-Step Framework for Stress-Testing Trading Strategies</title><link>https://hackernoon.com/a-step-by-step-framework-for-stress-testing-trading-strategies?source=rss</link><author>Nikhil Adithyan</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:38:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
In quantitative trading, it’s important to be careful when testing your strategy on all available data, as this can sometimes cause the rules to become too tuned and not perform well in real trading situations. To get a better idea of how your strategy really works, try dividing your historical data into a training (or optimisation) period and a test period. This approach allows you to evaluate your strategy on unseen data, much like real market conditions, which helps prevent overfitting and gives you a clearer picture of its strength across different market regimes.To do a more robust backtesting, we will also use simulated price paths via a non-parametric Brownian bridge to assess a trading strategy’s resilience. Unlike relying on a single historical sequence, this method generates multiple paths capturing key statistical features. Testing the strategy across these paths helps us understand its performance in various market scenarios, reducing overfitting risk and offering insights into its consistency and resilience. This provides a more thorough evaluation of the strategy’s robustness and real-world potential.What to expect in this article:Get 20 years of data for Apple stock till todayDevelop our two Moving Average strategy, where when the fast MA is higher than the slow one, we will go long, and short the other way around.Optimise the strategy for the first 15 years and see which parameters of the moving averages produce the best return.Check the results of the optimised parameters for the last 5 yearsSimulate 1000 price paths more for those 15 yearsOptimise our strategy for all those alternate pathsThe aim of this article isn’t to give you a perfect, ready-to-go algorithm that will make you rich overnight. Instead, it’s about helping you understand a different approach that you can smoothly incorporate into your backtest strategy. I hope you find it helpful and inspiring!Before we dive into the code, let’s briefly discuss retrospective simulation. This technique models alternate price paths based on actual historical data. As mentioned earlier, this article will focus on the non-parametric Brownian bridge method. Other methods also exist, with the most well-known being:Traditional Monte Carlo simulation, which generates random price paths assuming a specified stochastic model like geometric Brownian motion,The Euler-Maruyama method, which uses discrete time steps to approximate stochastic differential equations for simulating price processes,There are more advanced techniques like the Brownian Bridge Maximum Method, Quadratic-Exponential schemes, and Multidimensional Scaled Brownian Bridge. These methods are designed to enhance accuracy and better capture complex features such as volatility clustering or correlations between multiple assets.Choosing the best simulation method really depends on the strategy you’re testing, the amount of computational resources you have, and how complex the model needs to be. Retrospective simulation is especially helpful because it allows you to test strategies against many different versions of historical data, which can help prevent overfitting. This way, you can feel more confident that your strategies are robust before putting real capital on the line.In our case, we chose the non-parametric Brownian bridge method in this article because it effectively preserves the key statistical properties of historical price data while generating alternative price paths. Also, it is not so heavy on resources, which is a good start for us.First and most important, let’s see our imports, as well as the parameters we will need:import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from itertools import product
from tqdm import tqdm


token = 'YOUR FMP TOKEN'

from_date_train = '2005-10-31'
to_date_train = '2020-10-31'

from_date_test = '2020-11-01'
to_date_test = '2025-10-31'

fast_period = 21
slow_period = 55

fast_range = range(5, 46, 5)
slow_range = range(50, 251, 10)
\
Besides the , to get the prices for AAPL, we will need:The dates that will be necessary for the testingSome basic parameters for the two MAsThe ranges that we will use for our optimisationNow that we have all this, let’s get the AAPL prices. We will do it with the . You will notice that we will request the dates from the beginning of our training till the end of our testing.ticker = 'AAPL'
url = f'https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}'
df_ohlc = pd.DataFrame()
querystring = {"apikey":token, "from":from_date_train, "to":to_date_test}
data = requests.get(url, querystring)
data = data.json()

df = pd.DataFrame(data['historical'])
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date').set_index('date')
Traditional Backtesting and OptimisationAs we promised, let’s first develop our strategy and backtest it with the basic data.def sma_strategy_backtest(close, fast_period, slow_period):
    df = pd.DataFrame({'close': close})
    df['pct_change'] = df['close'].pct_change()
    df['fast_sma'] = df['close'].rolling(window=fast_period).mean()
    df['slow_sma'] = df['close'].rolling(window=slow_period).mean()

    # Generate signal
    df['signal'] = 0
    df.loc[(df['fast_sma'] > df['slow_sma']), 'signal'] = 1
    df.loc[(df['fast_sma'] < df['slow_sma']), 'signal'] = -1

    # Calculate returns with shift to avoid lookahead bias
    df['strategy_return'] = df['pct_change'] * df['signal'].shift(1)
    df['equity'] = 100 * (1 + df['strategy_return']).cumprod()

    # Calculate Buy and Hold total return in percentage
    df['bnh_equity'] = 100 * (1 + df['pct_change']).cumprod()
    bnh_total_ret = (df['bnh_equity'].iloc[-1] / df['bnh_equity'].dropna().iloc[0] - 1) * 100

    # Strategy total return
    equity = df['equity']
    total_ret = (equity.iloc[-1] / equity.dropna().iloc[0] - 1) * 100

    return equity, total_ret, bnh_total_ret
\
The backtesting will be performed solely on the close price, generating the signal based on the alignment of the moving averages as previously explained. The return, and ultimately the equity, will be calculated based on the signal. Finally, it will produce the series of the equity, the total return, as well as the buy-and-hold return to provide a point of reference.Now we will run this with the base parameters we defined initially and print the results:equity,total_ret, bnh_total_ret = sma_strategy_backtest(df['close'], fast_period, slow_period)

print("Total return (%):", total_ret)
print("Buy and Hold return (%):", bnh_total_ret)
The returns are positive, but they don’t come close to those of a Buy-and-Hold strategy. However, as mentioned, this article isn’t about identifying the most profitable approach but rather about illustrating the backtesting process using alternative methods.Let’s fine-tune our strategy (also known as overfitting ;) ) to discover what our results will be.def optimize_sma_periods(close, fast_range, slow_range):
    best_result = {'fast': None, 'medium': None, 'slow': None, 'total_return': -np.inf}
    best_equity = None

    # Iterate valid combinations: fast < medium < slow
    for fast, slow in product(fast_range, slow_range):
        if fast < slow:
            equity, total_ret, bnh_total_ret = sma_strategy_backtest(close, fast, slow)
            if total_ret > best_result['total_return']:
                best_result = {'fast': fast, 'slow': slow, 'total_return': total_ret}
                best_equity = equity
                buy_and_hold = bnh_total_ret

    return {
        'best_periods': (best_result['fast'], best_result['slow']),
        'best_total_return': best_result['total_return'],
        'best_equity': best_equity,
        'buy_and_hold': buy_and_hold
    }

result = optimize_sma_periods(df['close'], fast_range, slow_range)
print("Best periods (fast, slow):", result['best_periods'])
print("Best total return (%):", result['best_total_return'])
print("Buy and Hold return (%):", result['buy_and_hold'])
We observe that the highest return comes from a very fast MA (10 days) and a relatively slow one (220 days). This is because the stock (like every stock in recent years) has delivered tremendous returns, so the strategy aims to stay as long as possible.Apparently, in the previous step, we have overfitted our parameters, and no experienced (or sane) trader would believe that those are the parameters to be used with real money from tomorrow…Let’s assume today is 5 years earlier, and that we have optimised our parameters using data up to that point. To do this, we will keep the first 15 years and run the same optimisation.df_train = df.loc[from_date_train:to_date_train]

result = optimize_sma_periods(df_train['close'], fast_range, slow_range)

best_fast = result['best_periods'][0]
best_slow = result['best_periods'][1]

print("Best periods (fast, slow):", best_fast, best_slow)
print("Best total return (%):", result['best_total_return'])
print("Buy and Hold return (%):", result['buy_and_hold'])
Again, the best train parameters are 10 for fast and 220 for slow. Let’s see what this optimisation will yield for the next 5 years up to today…df_test = df.loc[from_date_test:to_date_test]
equity,total_ret, bnh_total_ret = sma_strategy_backtest(df_test['close'], best_fast, best_slow)

print("Best periods applied (fast, medium, slow):", best_fast, best_slow)
print("Total return (%):", total_ret)
print("Buy and Hold return (%):", bnh_total_ret)
Proportionately, the results are almost identical, with a small return of 5%, while the stock’s returns were more than double the price.There are many methods to compute alternative paths. In our case, we will use the non-parametric Brownian bridge framework, which, as previously mentioned, maintains the statistical features of the price history and ensures the path starts and ends at the same price.close_prices = df['close']


def non_parametric_brownian_bridge(close_prices, n_paths=1000, seed=42):
    np.random.seed(seed)
    n = len(close_prices)
    X0 = np.log(close_prices.iloc[0])
    Xn = np.log(close_prices.iloc[-1])
    log_returns = np.log(close_prices / close_prices.shift(1)).dropna().values

    paths = np.zeros((n, n_paths))
    for i in range(n_paths):
        # Sample n-1 returns and center them
        sampled = np.random.choice(log_returns, size=n - 1, replace=True)
        drift_correction = (Xn - X0) / (n - 1) - np.mean(sampled)
        sampled += drift_correction  # Center drift
        W = np.concatenate(([0], np.cumsum(sampled)))  # Now length n
        # Brownian bridge formula for all time steps (n)
        bridge = X0 + W + np.linspace(0, 1, n) * (Xn - X0 - W[-1])
        paths[:, i] = bridge

    sim_prices = np.exp(paths)
    sim_prices[~np.isfinite(sim_prices)] = np.nan
    return sim_prices


simulated_paths = non_parametric_brownian_bridge(close_prices, n_paths=1000)

for i in range(simulated_paths.shape[1]):
    df[f'sim_path_{i+1}'] = simulated_paths[:, i]

plt.figure(figsize=(14, 7))
plt.plot(df.index, df.loc[:, 'sim_path_1':'sim_path_1000'], lw=1, alpha=0.7)
plt.plot(df.index, close_prices, lw=2, label='Original', color='black')
plt.title('Non-Parametric Brownian Bridge - Simulated Paths')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()
As you can see, plotting 1000 of the possible paths, the beginning and end are at the same price. Notice the white line, which represents the actual history.Now is the time to start the fun. We will run all the possible combinations of parameters for each price path. There will be almost 200K runs, so be patient.df_train_multiple_paths = df.loc[from_date_train:to_date_train]

results = []

for i in tqdm(range(1,1001,1)):
    print(f'Processing path {i}')
    for fast, slow in product(fast_range, slow_range):
        _, total_backtest_ret, _  = sma_strategy_backtest(df_train_multiple_paths['sim_path_' + str(i)], fast, slow)
        result = {'fast': fast, 'slow': slow, 'total_return': total_backtest_ret}
        results.append(result)

df_all_paths_train = pd.DataFrame(results)
df_all_paths_train.to_csv('df_all_paths_train_2.csv', index=False)
df_all_paths_train
For each run, we will also calculate the actual return over the last 5 years. As you will see in the code, we will not calculate for each row (since the combination of the MA parameters repeats). Instead, we will compute all the unique combinations first and then merge them into the final dataframe.unique_combos = (
    df_all_paths_train[['fast', 'slow']]
    .drop_duplicates()
    .copy()
)

unique_combos[['fast', 'slow']] = unique_combos[['fast', 'slow']].astype(int)

def _compute_test_metrics(row):
    f, s = int(row['fast']), int(row['slow'])
    _, total_ret, bnh_ret = sma_strategy_backtest(df_test['close'], f, s)
    return pd.Series({'test_total_return': total_ret, 'test_bnh_return': bnh_ret})

# Evaluate each unique combo once
unique_combos[['test_total_return', 'test_bnh_return']] = unique_combos.apply(_compute_test_metrics, axis=1)

# Join back to all rows to align with every path's chosen combo
df_all_paths_with_test = df_all_paths_train.merge(unique_combos, on=['fast', 'slow'], how='left')
df_all_paths_with_test
Now that we have our dataframe with all the results, let’s try some plots to make some sense out of all this effort.Our first try will be a 3D scatter plot, where we will use the 2 MAs as well as the final return in the test period (the last 5 years)import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot with fast, slow, and test_total_return
scatter = ax.scatter(df_all_paths_with_test['fast'],
                     df_all_paths_with_test['slow'],
                     df_all_paths_with_test['test_total_return'],
                     c=df_all_paths_with_test['test_total_return'],
                     cmap='viridis',
                     alpha=0.7)

ax.set_xlabel('Fast MA Length')
ax.set_ylabel('Slow MA Length')
ax.set_zlabel('Test Period Return')

fig.colorbar(scatter, label='Test Period Return')

plt.title('3D Scatter Plot of MA Parameters vs Test Total Return')
plt.show()
Overall, the 3D plots can be confusing. However, upon closer inspection, you’ll notice that the farthest back part of the plot indicates that we should expect better returns with very fast and very slow MAs, which also supports our initial findings.Which brings us to the following plot, where we will use boxplots to distinguish the two MAs. We will also bin the MAs for a better visualisation.# Define bins for fast and slow parameters (customize ranges as needed)
fast_bins = np.arange(df_all_paths_with_test['fast'].min(),
                      df_all_paths_with_test['fast'].max() + 5, 5)
slow_bins = np.arange(df_all_paths_with_test['slow'].min(),
                      df_all_paths_with_test['slow'].max() + 10, 10)

# Create binned columns for fast and slow
df_all_paths_with_test['fast_bin'] = pd.cut(df_all_paths_with_test['fast'], fast_bins)
df_all_paths_with_test['slow_bin'] = pd.cut(df_all_paths_with_test['slow'], slow_bins)

fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)

# Boxplot for fast parameter bins
df_all_paths_with_test.boxplot(column='test_total_return', by='fast_bin', ax=axes[0], grid=False)
axes[0].set_title('Test Returns by Fast MA Length')
axes[0].set_xlabel('Fast MA Length Range')
axes[0].set_ylabel('Test Period Return')
axes[0].tick_params(axis='x', rotation=45)

# Boxplot for slow parameter bins
df_all_paths_with_test.boxplot(column='test_total_return', by='slow_bin', ax=axes[1], grid=False)
axes[1].set_title('Test Returns by Slow MA Length')
axes[1].set_xlabel('Slow MA Length Range')
axes[1].tick_params(axis='x', rotation=45)

plt.suptitle('')  # Remove default pandas title
plt.tight_layout()
plt.show()
This will give us some more insights:Regarding the Fast MA, our initial results are once again confirmed. The returns are better when using a range of 5 to 10 periods for a fast MA.Regarding the Slow MA box plots, they provide additional insights. We observe that returns tend to be good with some “faster” slow MAs in the range of 50 to 100. However, in this area, we also notice the most outliers (the dots), which is undesirable since it indicates a higher risk.Another interesting plot is a heatmap that shows the risk of overfitting. This is achieved by calculating an overfitting metric, which is the difference between train and test returns. Let’s look at that:# Calculate overfitting metric
df_all_paths_with_test['overfit'] = df_all_paths_with_test['total_return'] - df_all_paths_with_test['test_total_return']

# Group by fast and slow and aggregate overfit by mean (or median if preferred)
agg_df = df_all_paths_with_test.groupby(['fast', 'slow'])['overfit'].mean().reset_index()

# Pivot the aggregated DataFrame
heatmap_data = agg_df.pivot(index='fast', columns='slow', values='overfit')

plt.figure(figsize=(12, 8))
sns.heatmap(heatmap_data, cmap='coolwarm', center=0,
            cbar_kws={'label': 'Overfitting Risk (Train - Test Return)'},
            linewidths=0.5)

plt.title('Heatmap of Overfitting Risk by MA Parameters')
plt.xlabel('Slow MA Length')
plt.ylabel('Fast MA Length')
plt.show()
Well, that explains everything. The reason the returns during the test period were in the very slow and very fast MAs is that these areas carry a higher concentrated risk of overfitting. In these zones, the difference between training returns and test returns is the greatest. Essentially, these areas generate the best results during training, but when comparing train and test, the largest gaps are observed there.What have we learned in this article:Dividing data into training and testing periods provides a realistic assessment of strategy robustness.Using non-parametric Brownian bridge simulations generates multiple price paths, testing the strategy against diverse market scenarios.Simulated paths offer more profound insight into consistency and risk, enhancing confidence in the strategy’s real-world application.And last but not least, when trading with real money, remember: backtest like your profits depend on it — because they do! The more you test, the less you guess, and the happier your portfolio will be.]]></content:encoded></item><item><title>Your First Interactive Plot in Python: A Hands-On Plotly Guide</title><link>https://hackernoon.com/your-first-interactive-plot-in-python-a-hands-on-plotly-guide?source=rss</link><author>ProgrammingCentral</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:36:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
For years, our data visualization toolbox in Python has been dominated by two giants: Matplotlib and Seaborn. They are the undisputed champions of static, publication-quality graphics. They allow us to create beautiful, precise "photographs" of our data. But in the modern world of web-based dashboards and dynamic reports, a photograph is often not enough. Static charts are a one-way conversation.What if your charts could talk back? What if your users could hover over a data point to see its exact value, zoom into a specific time range, or filter data on the fly?This is the paradigm of interactive visualization, and its leading practitioner in the Python ecosystem is . This article will guide you through this paradigm shift, showing you how to build your first web-native, interactive chart.The Paradigm Shift: From Pixels to Data ObjectsThe magic of Plotly lies in a fundamental change in how a visualization is created and rendered.Matplotlib/Seaborn (The Photograph): These libraries issue a series of drawing commands to a backend. The final output is a static image file (like a PNG or SVG) made of pixels or vector paths. Once rendered, the link between a visual element (a bar on a chart) and the underlying data point is lost.Plotly (The Interactive Map): Plotly doesn't create a static image. It creates a rich, structured  that contains everything: the data, the layout instructions, and the rules for interactivity. This JSON "blueprint" is then sent to a browser, where Plotly.js (a powerful JavaScript library) renders it. Because the browser has the full data object, it can handle interactions like hovering and zooming locally, without ever needing to ask the Python server for a new image.You're not just creating a picture of the data; you're creating a small, self-contained data application.Meet the Plotly APIs: Your Two Best FriendsPlotly offers two distinct but related APIs, each designed for a different stage of the analytical workflow. Understanding them is key to using the library effectively.Plotly Express (PX): The Prefabricated Home Kit This is the high-level, "batteries-included" API. It's designed for speed and convenience, perfect for exploratory data analysis (EDA). With a single function call, you can create a complex, fully interactive chart. PX makes intelligent assumptions about layout, legends, and styling, letting you focus on the data.Plotly Graph Objects (GO): The Custom Architectural Blueprint This is the low-level, foundational API. It gives you granular control over every single element of the plot. You build the figure from the ground up, defining each "trace" (the data layer) and every "layout" property (the styling layer). This is the API you need for complex, multi-layered charts, dual-axis plots, and production-ready dashboards where every detail matters.The best part? Plotly Express is just a smart wrapper around Graph Objects. Every figure you create with PX is a GO figure under the hood, which means you can always start with the speed of PX and then use GO methods to fine-tune the details.The Main Event: Building a Chart, Two WaysLet's build a simple, interactive bar chart showing quarterly revenue. We'll do it first with Plotly Express to see the speed, and then with Graph Objects to understand the architecture.Here is the full, self-contained code. You can copy, paste, and run this in any Python environment.import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.offline import plot
import os

# --- 1. Data Preparation ---
# A simple DataFrame is the ideal input for Plotly.
df = pd.DataFrame({
    'Quarter': ['Q1 2024', 'Q2 2024', 'Q3 2024', 'Q4 2024'],
    'Revenue': [100, 150, 130, 180]
})

print("--- Data Ready for Plotting ---")
print(df.head())
print("-" * 35)


# --- 2. The Plotly Express Way (Fast & Easy) ---
# One line of code generates the entire interactive figure.
fig_px = px.bar(
    df,
    x='Quarter',
    y='Revenue',
    title='Quarterly Revenue (via Plotly Express)',
    color='Quarter',  # Automatically adds color and a legend
    labels={'Revenue': 'Total Revenue ($K)'}, # Easy label renaming
    template='plotly_dark' # Apply a modern theme
)


# --- 3. The Graph Objects Way (Powerful & Explicit) ---
# Here, we build the figure piece by piece.

# Step A: Initialize an empty Figure object (the canvas)
fig_go = go.Figure()

# Step B: Define and add a 'trace' (the data layer)
fig_go.add_trace(
    go.Bar(
        x=df['Quarter'],
        y=df['Revenue'],
        name='Revenue Trace',
        marker_color=['#636EFA', '#EF553B', '#00CC96', '#AB63FA'] # Manually define colors
    )
)

# Step C: Define and update the 'layout' (the styling layer)
fig_go.update_layout(
    title_text='Quarterly Revenue (via Graph Objects)',
    xaxis_title='Fiscal Quarter',
    yaxis_title='Total Revenue ($K)',
    template='plotly_dark'
)


# --- 4. Output Generation ---
# This will save the GO figure as an interactive HTML file and open it in your browser.
# We use `plot()` from `plotly.offline` to ensure it works outside of a Jupyter Notebook.
output_filename = 'interactive_chart.html'
plot(fig_go, filename=output_filename, auto_open=True)

print(f"Interactive chart saved to: {os.path.abspath(output_filename)}")
print(f"Notice that both fig_px and fig_go are of the same base type: {type(fig_go)}")
Deep Dive: Deconstructing the CodeLet's break down exactly what's happening in each approach.The Plotly Express Approach (The Magic of One Line)fig_px = px.bar(
    df,
    x='Quarter',
    y='Revenue',
    title='Quarterly Revenue (via Plotly Express)',
    color='Quarter',
    labels={'Revenue': 'Total Revenue ($K)'},
    template='plotly_dark'
)
With a single call to , we gave it our DataFrame and told it which columns to map to which visual roles:: Use the 'Quarter' column for the x-axis.: Use the 'Revenue' column for the y-axis.: This is a powerful feature. It tells PX to assign a unique color to each bar based on its 'Quarter' value and automatically create a legend.: A simple dictionary to provide user-friendly names for the axes.: Applies a pre-packaged theme for a modern, dark-mode look.That's it. PX builds the complete  object internally and returns it.The Graph Objects Approach (The Power of Precision)This approach is more verbose but exposes the core architecture of a Plotly figure. You start with a blank canvas. This  object is an empty container waiting for you to add data and styling.fig_go.add_trace(go.Bar(...)) A  is a single data series and its visual representation. Our figure has one trace: a bar chart.We explicitly create a  object.We must pass the full Pandas Series () to the  and  parameters, not just the column name string.We have to manually define the . This is where the trade-off is clear: more code, but complete control.fig_go.update_layout(...) The  controls everything that isn't the data itself: titles, axis labels, fonts, legends, backgrounds., , : We explicitly set the text for each part of the chart.: We can still apply a global theme here for consistency.The Interactive Payoff: What You Get for FreeWhen you run the script, an HTML file will open in your browser. This is where you see the Plotly difference: Move your mouse over any bar. A tooltip appears showing the exact Quarter and Revenue. This is built-in. Click and drag to select a region to zoom in. Double-click to zoom out. In the top-right corner, you'll find a toolbar to pan, reset the view, and even download the chart as a static PNG image.All this interactivity is the default behavior of the Plotly.js rendering engine. You didn't have to write a single line of JavaScript.When to Use Which? A Practical Guide| Use Plotly Express (PX) When… | Use Graph Objects (GO) When… |
|----|----|
| You are in the Exploratory Data Analysis (EDA) phase. | You are building a production-ready, custom dashboard. |
| Your primary goal is . | Your primary goal is granular control and customization. |
| You are creating standard chart types (scatter, line, bar, histogram, map). | You need to combine multiple chart types (e.g., bars and lines). |
| You want to leverage automatic faceting (, ). | You need complex subplots with shared axes or custom layouts. |
| You need a quick, beautiful, and interactive plot with minimal code. | You need to add custom annotations, shapes, or buttons. | The best workflow is often a hybrid one. Generate your initial figure quickly with Plotly Express, and then use Graph Objects methods like  or  to add the final layers of polish and customization.Conclusion: Your Gateway to Web DashboardsMastering Plotly is the first and most crucial step toward building modern, interactive data applications in Python. The  object you create is the fundamental component that frameworks like  use to build full-scale web dashboards.You've now seen how to move beyond static images and create living, explorable visualizations. The next time you build a chart, don't just show the data—let your users interact with it.Explore the complete “Python Programming Series” for a comprehensive journey from Python fundamentals to advanced AI Agents: https://www.amazon.com/dp/B0FTTQNXKG. \n You can read each book as a standalone.]]></content:encoded></item><item><title>AI in the Workplace: A Threat to Managers or a Tool for Better Leadership?</title><link>https://hackernoon.com/ai-in-the-workplace-a-threat-to-managers-or-a-tool-for-better-leadership?source=rss</link><author>Valentin Vasilevsky</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:25:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Artificial intelligence can analyze big data, make strategic decisions, and even manage teams. Many leaders worry that if AI could replace operational specialists, it might soon displace them too.]]></content:encoded></item><item><title>Scale Is Not a Goal: Why Most Software Architectures Are Overbuilt</title><link>https://hackernoon.com/scale-is-not-a-goal-why-most-software-architectures-are-overbuilt?source=rss</link><author>Joachim Zeelmaekers</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:21:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Designing for imaginary scale leads to real costs. Why pragmatic systems beat “future-proof” architectures in early products. ]]></content:encoded></item><item><title>Google&apos;s Jules Starts Surfacing Work on Its Own, Signaling a Shift in AI Coding Assistants</title><link>https://hackernoon.com/googles-jules-starts-surfacing-work-on-its-own-signaling-a-shift-in-ai-coding-assistants?source=rss</link><author>AI Native Dev</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:17:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Google is make its Jules coding agent more "proactive," allowing it to surface tasks and respond to events without being explicitly invoked by developers. ]]></content:encoded></item><item><title>How I Built an Engine That Turns Architecture Sketches Into Animations</title><link>https://hackernoon.com/how-i-built-an-engine-that-turns-architecture-sketches-into-animations?source=rss</link><author>Ruam</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:16:54 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
I love Excalidraw for sketching system architectures. But sketches are static. When I want to show how a packet moves through a load balancer, or how a database shard splits, I have to wave my hands frantically or create 10 different slides.I wanted the ability to "Sketch Logic, Export Motion".I didn't want a timeline editor (like After Effects). That's too much work for a simple diagram. \n I wanted :Draw  (The start state).Move elements to their new positions.The engine automatically figures out the transition.I built this engine using , , and . Here is a technical deep dive into how I implemented the logic.1. The Core Logic: Diffing StatesThe hardest part isn't the animation loop; it's the . When we move from  to , we identify elements by their stable IDs and categorize them into one of three buckets: The element exists in both frames (needs to morph/move). Exists in B but not A (needs to fade in). Exists in A but not B (needs to fade out).I wrote a  utility that maps elements efficiently: \n // Simplified logic from src/utils/editor/transition-logic.ts

export function categorizeTransition(prevElements, currElements) {
    const stable = [];
    const morphed = [];
    const entering = [];
    const exiting = [];

    const prevMap = new Map(prevElements.map(e => [e.id, e]));
    const currMap = new Map(currElements.map(e => [e.id, e]));

    // 1. Find Morphs (Stable) & Entering
    currElements.forEach(curr => {
        if (prevMap.has(curr.id)) {
            const prev = prevMap.get(curr.id);
            // We separate "Stable" (identical) from "Morphed" (changed) 
            // to optimize the render loop
            if (areVisuallyIdentical(prev, curr)) {
                stable.push({ key: curr.id, element: curr });
            } else {
                morphed.push({ key: curr.id, start: prev, end: curr });
            }
        } else {
            entering.push({ key: curr.id, end: curr });
        }
    });

    // 2. Find Exiting
    prevElements.forEach(prev => {
        if (!currMap.has(prev.id)) {
            exiting.push({ key: prev.id, start: prev });
        }
    });

    return { stable, morphed, entering, exiting };
}
For the "Morphed" elements, we need to calculate the intermediate state at any given  (0.0 to 1.0).You can't just use simple linear interpolation for everything. Linear works fine. You must convert Hex to RGBA, interpolate each channel, and convert back. You need "shortest path" interpolation.If an object is at  and rotates to , linear interpolation goes the long way around. We want it to just rotate -20 degrees. \n // src/utils/smart-animation.ts

const angleProgress = (oldAngle, newAngle, progress) => {
    let diff = newAngle - oldAngle;

    // Normalize to -PI to +PI to find shortest direction
    while (diff > Math.PI) diff -= 2 * Math.PI;
    while (diff < -Math.PI) diff += 2 * Math.PI;

    return oldAngle + diff * progress;
};
3. The Render Loop & Overlapping PhasesInstead of CSS transitions (which are hard to sync for complex canvas repaints), I used a  loop in a React hook called .A key "secret sauce" to making animations feel professional is . \n If you play animations sequentially (Exit -> Move -> Enter), it feels robotic. \n I overlapped the phases so the scene feels alive: \n // Timeline Logic
const exitEnd = hasExit ? 300 : 0;
const morphStart = exitEnd; 
const morphEnd = morphStart + 500;

// [MAGIC TRICK] Start entering elements BEFORE the morph ends
// This creates that "Apple Keynote" feel where things arrive 
// just as others are settling into place.
const overlapDuration = 200; 
const enterStart = Math.max(morphStart, morphEnd - overlapDuration);
4. Making it feel "Physical"Linear movement (progress = time / duration) is boring. \n I implemented spring-based easing functions. Even though I'm manually calculating specific frames, I apply an easing curve to the  value before feeding it into the interpolator. \n // Quartic Ease-Out Approximation for a "Heavy" feel
const springEasing = (t) => {
    return 1 - Math.pow(1 - t, 4); 
};
This ensures that big architecture blocks "thud" into place with weight, rather than sliding around like ghosts.I'm currently working on: Allowing you to click through bullet points  a single frame. Recording the canvas stream directly to a video file.The project is live, and I built it to help developers communicate better.Free Stripe Promotion Code: postaraLet me know what you think of the approach!]]></content:encoded></item><item><title>Why 70% of Developers Don’t Trust Plugins—and How I Built a Fix</title><link>https://hackernoon.com/why-70percent-of-developers-dont-trust-pluginsand-how-i-built-a-fix?source=rss</link><author>Daniel, Andrei-Daniel Petrica</author><category>tech</category><pubDate>Thu, 22 Jan 2026 05:13:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Do you suffer from 'Dependency Anxiety'? 60% of Laravel developers spend up to 30 minutes just vetting a single package. Learn how I built Laraplugins.io—a high-performance tool running on Laravel Octane and FrankenPHP—to automate health checks and help you choose the right dependencies instantly.]]></content:encoded></item><item><title>The Measles Outbreak In South Carolina Is Spiraling Out Of Control</title><link>https://www.techdirt.com/2026/01/21/the-measles-outbreak-in-south-carolina-is-spiraling-out-of-control/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Thu, 22 Jan 2026 04:17:03 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[America is broken and it seems like nobody is bothering to try to repair it. That’s a general statement, to be sure, so if you need some marking point to serve as a specific example of our national malfunction, the return of measles to our country can fit the bill. It’s not quite as flashy as the secret police shooting citizens, of course. But I think that there is something about children with angry rashes across their necks sitting in hospital beds, or in body bags, that will have a way of clarifying the mind.With a grifter like RFK Jr. at the helm of American health, having built a career based on anti-vaxxer conspiracy theories and health misinformation, our country became a fertile host once more to this horrific disease. Kennedy’s inability to properly communicate to the nation what needs to happen, which is another concentrated MMR vaccination effort, combined with his eugenics-lite belief system on matters of health, has all led to this. 2025 saw the highest number of Americans infected by measles in decades, 3 people died, we’re about to lose our elimination status for the disease, and an outbreak in South Carolina has us off to a rip roaring start to 2026.While this is largely due to the unvaccinated population among us, allowing the disease to spread where it otherwise would not, we’ve seen enough breakthrough infections that even being one of the “responsible ones” won’t necessarily keep you safe any longer. And the South Carolina outbreak of measles is officially off the rails.A week ago, ArsTechnica had an alarming post about how South Carolina saw well over a hundred new cases of measles and over 400 people quarantined . Amid the outbreak, South Carolina health officials have been providing updates on cases every Tuesday and Friday. On Tuesday, state health officials reported 124 more cases since last Friday, which had 99 new cases since the previous Tuesday. On that day, January 6, officials noted a more modest increase of 26 cases, bringing the outbreak total at that point to 211 cases.With the 3-month-old outbreak now doubled in just a week, health officials are renewing calls for people to get vaccinated against the highly infectious virus—an effort that has met with little success since October. Still, the health department is activating its mobile health unit to offer free measles-mumps-rubella (MMR) vaccinations, as well as flu vaccinations at two locations today and Thursday in the Spartanburg area, the epicenter of the outbreak.Those same officials had another dire warning: the outbreak had grown so big that they no longer had the ability to perform contact tracing. Where the disease would go next was anyone’s guess.The outbreak is still growing to date. At least 88 more cases of measles were recorded in South Carolina in less than a week since the Ars post. Schools remain the most problematic vector, but it’s no longer just elementary and secondary schools that are in trouble. Colleges are now part of the party.There are at least 15 schools — including elementary, middle and high schools — which currently have students in quarantine.Health officials also warned of exposures at Clemson University and Anderson University, both located in northwestern South Carolina, which have a combined 88 students in quarantine.While these numbers from South Carolina are publicly stated, the CDC site tallying measles infections apparently can’t keep up. The last time the numbers were updated there was January 14th, but even those numbers appear to be incorrectly low. The site also announces that it is moving its reporting schedule from every Wednesday to Fridays, which is your classic “bad news dumping ground” day. Measles continue to spread in the Upstate but now, health leaders in Washington state say the outbreak here in South Carolina is connected to cases on the west coast. The Snohomish County Health Department confirmed three cases in children who were exposed to a contagious family visiting from South Carolina.Previously, the Snohomish County Health Department and Public Health – Seattle & King County were notified that three members of a South Carolina family, one adult and two children, were infectious while visiting King and Snohomish counties from Dec. 27, 2025 through Jan. 1, 2026. The family visited multiple locations in Everett, Marysville and Mukilteo while contagious before being diagnosed. They also traveled through Seattle-Tacoma International Airport and visited a car rental facility near the airport.In any sane administration, a measles task force would be mobilized to build out a strategy to contain these outbreaks, to communicate actions plans to the public, and to execute on actions designed to keep the public healthy. Trump, RFK Jr., and the health agencies they’re in charge of are barely talking about this. They are ignoring the problem and that will ensure that it becomes much, much worse.Impeachments are what’s necessary here, starting with Kennedy, who is clearly asleep at the wheel. A feckless Congress unwilling to do its job should have members tossed out on their ass. Staff at HHS and its child agencies should be in full revolt, sounding the alarm.Measles is no fucking joke, folks. But our government currently is.]]></content:encoded></item><item><title>Weight-Loss Drugs Could Save US Airlines $580 Million Per Year</title><link>https://science.slashdot.org/story/26/01/21/2350220/weight-loss-drugs-could-save-us-airlines-580-million-per-year?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the New York Times: Weight-loss drugs like Ozempic have transformed millions of lives with easily administered treatments and quick results. Now it turns out the dropped pounds may have a surprising perk for airlines, too: lower fuel costs, as slimmer passengers lighten their aircraft's loads.
 
According to a study published last week by Jefferies, a financial services firm, the four largest U.S. carriers -- American Airlines, Delta Air Lines, Southwest Airlines and United Airlines -- could together save as much as $580 million per year on fuel thanks to weight-loss drugs, known as GLP-1s. One in eight U.S. adults said they were taking a GLP-1 in a November survey published by KFF, a nonprofit health research group. Fuel is among airlines' largest expenses. The Jefferies study estimates that the four airlines will together consume 16 billion gallons of fuel in 2026 at a total cost of $38.6 billion, nearly 20 percent of their total expenses.
 
The savings from skinnier passengers would amount to just 1.5 percent of fuel costs. But airlines and pilots must scrutinize even the smallest changes to a plane's weight and balance, and a lighter payload means each jet burns less fuel to generate the thrust necessary to fly. Investors could also stand to benefit: The researchers estimated that a 2 percent reduction in aircraft weight could boost earnings per share by about 4 percent. "Please note savings are before any lost snack sales," the Jefferies analysts added.]]></content:encoded></item><item><title>FBI&apos;s Washington Post Investigation Shows How Your Printer Can Snitch On You</title><link>https://hardware.slashdot.org/story/26/01/21/2342252/fbis-washington-post-investigation-shows-how-your-printer-can-snitch-on-you?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[alternative_right quotes a report from The Intercept: Federal prosecutors on January 9 charged Aurelio Luis Perez-Lugones, an IT specialist for an unnamed government contractor, with "the offense of unlawful retention of national defense information," according to an FBI affidavit (PDF). The case attracted national attention after federal agents investigating Perez-Lugones searched the home of a Washington Post reporter. But overlooked so far in the media coverage is the fact that a surprising surveillance tool pointed investigators toward Perez-Lugones: an office printer with a photographic memory. News of the investigation broke when the Washington Post reported that investigators seized the work laptop, personal laptop, phone, and smartwatch of journalist Hannah Natanson, who has covered the Trump administration's impact on the federal government and recently wrote about developing more than 1,000 government sources. A Justice Department official told the Post that Perez-Lugones had been messaging Natanson to discuss classified information. The affidavit does not allege that Perez-Lugones disseminated national defense information, only that he unlawfully retained it.
 
The affidavit provides insight into how Perez-Lugones allegedly attempted to exfiltrate information from a Secure Compartmented Information Facility, or SCIF, and the unexpected way his employer took notice. According to the FBI, Perez-Lugones printed a classified intelligence report, albeit in a roundabout fashion. It's standard for workplace printers to log certain information, such as the names of files they print and the users who printed them. In an apparent attempt to avoid detection, Perez-Lugones, according to the affidavit, took screenshots of classified materials, cropped the screenshots, and pasted them into a Microsoft Word document. By using screenshots instead of text, there would be no record of a classified report printed from the specific workstation. (Depending on the employer's chosen data loss prevention monitoring software, access logs might show a specific user had opened the file and perhaps even tracked whether they took screenshots).
 
Perez-Lugones allegedly gave the file an innocuous name, "Microsoft Word - Document1," that might not stand out if printer logs were later audited. In this case, however, the affidavit reveals that Perez-Lugones's employer could see not only the typical metadata stored by printers, such as file names, file sizes, and time of printing, but it could also view the actual contents of the printed materials -- in this case, prosecutors say, the screenshots themselves. As the affidavit points out, "Perez-Lugones' employer can retrieve records of print activity on classified systems, including copies of printed documents." [...] Aside from attempting to surreptitiously print a document, Perez-Lugones, investigators say, was also seen allegedly opening a classified document and taking notes, looking "back and forth between the screen corresponding the classified system and the notepad, all the while writing on the notepad." The affidavit doesn't state how this observation was made, but it strongly suggests a video surveillance system was also in play.



]]></content:encoded></item><item><title>&apos;America Is Slow-Walking Into a Polymarket Disaster&apos;</title><link>https://news.slashdot.org/story/26/01/22/006212/america-is-slow-walking-into-a-polymarket-disaster?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In an opinion piece for The Atlantic, senior editor Saahil Desai argues that media outlets are increasingly treating prediction markets like Polymarket and Kalshi as legitimate signals of reality. The risk, as Desai warns, is a future where news coverage amplifies manipulable betting odds and turns politics, geopolitics, and even tragedy into speculative gambling theater. Here's an excerpt from the report: [...] The problem is that prediction markets are ushering in a world in which news becomes as much about gambling as about the event itself. This kind of thing has already happened to sports, where the language of "parlays" and "covering the spread" has infiltrated every inch of commentary. ESPN partners with DraftKings to bring its odds to SportsCenter and Monday Night Football; CBS Sports has a betting vertical; FanDuel runs its own streaming network. But the stakes of Greenland's future are more consequential than the NFL playoffs.
 
The more that prediction markets are treated like news, especially heading into another election, the more every dip and swing in the odds may end up wildly misleading people about what might happen, or influencing what happens in the real world. Yet it's unclear whether these sites are meaningful predictors of anything. After the Golden Globes, Polymarket CEO Shayne Coplan excitedly posted that his site had correctly predicted 26 of 28 winners, which seems impressive -- but Hollywood awards shows are generally predictable. One recent study found that Polymarket's forecasts in the weeks before the 2024 election were not much better than chance.
 
These markets are also manipulable. In 2012, one bettor on the now-defunct prediction market Intrade placed a series of huge wagers on Mitt Romney in the two weeks preceding the election, generating a betting line indicative of a tight race. The bettor did not seem motivated by financial gain, according to two researchers who examined the trades. "More plausibly, this trader could have been attempting to manipulate beliefs about the odds of victory in an attempt to boost fundraising, campaign morale, and turnout," they wrote. The trader lost at least $4 million but might have shaped media attention of the race for less than the price of a prime-time ad, they concluded. [...]
 
The irony of prediction markets is that they are supposed to be a more trustworthy way of gleaning the future than internet clickbait and half-baked punditry, but they risk shredding whatever shared trust we still have left. The suspiciously well-timed bets that one Polymarket user placed right before the capture of Nicolas Maduro may have been just a stroke of phenomenal luck that netted a roughly $400,000 payout. Or maybe someone with inside information was looking for easy money. [...] As Tarek Mansour, Kalshi's CEO, has said, his long-term goal is to "financialize everything and create a tradable asset out of any difference in opinion." (Kalshi means "everything" in Arabic.) What could go wrong? As one viral post on X recently put it, "Got a buddy who is praying for world war 3 so he can win $390 on Polymarket." It's a joke. I think.]]></content:encoded></item><item><title>Linux Finally Retiring HIPPI: The First Near-Gigabit Standard For Networking Supercomputers</title><link>https://www.phoronix.com/news/Linux-Retiring-HIPPI</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 22 Jan 2026 01:20:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While the Linux kernel has been seeing preparations from NVIDIA for 1.6 Tb/s networking in preparing for next-generation super-computing, the kernel has still retained support to now for the High Performance Parallel Interface. HIPPI was the standard for connecting supercomputers in the late 1980s and a portion of the 1990s with being the first networking standard for near-Gigabit connectivity at 800 Mb/s over distances up to 25 meters. But HIPPI looks like it will be retired from the mainline kernel with Linux 7.0...]]></content:encoded></item><item><title>Apple Reportedly Replacing Siri Interface With Actual Chatbot Experience For iOS 27</title><link>https://apple.slashdot.org/story/26/01/21/2333212/apple-reportedly-replacing-siri-interface-with-actual-chatbot-experience-for-ios-27?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[According to Bloomberg's Mark Gurman, Apple is reportedly planning a major Siri overhaul in iOS 27 and macOS 27 where the current assistant interface will be replaced with a deeply integrated, ChatGPT-style chatbot experience. "Users will be able to summon the new service the same way they open Siri now, by speaking the 'Siri' command or holding down the side button on their iPhone or iPad," says Gurman. "More significantly, Siri will be integrated into all of the company's core apps, including ones for mail, music, podcasts, TV, Xcode programming software and photos. That will allow users to do much more with just their voice." 9to5Mac reports: The unannounced Siri overhaul will reportedly be revealed at WWDC in June as the flagship feature for iOS 27 and macOS 27. Its release is expected in September when Apple typically ships major software updates. While Apple plans to release an improved version of Siri and Apple Intelligence this spring, that version will use the existing Siri interface. The big difference is that Google's Gemini models will power the intelligence. With the bigger update planned for iOS 27, the iOS 26 upgrade to Siri and Apple Intelligence sounds more like the first step to a long overdue modernization.
 
Gurman reports that the major Siri overhaul will "allow users to search the web for information, create content, generate images, summarize information and analyze uploaded files" while using "personal data to complete tasks, being able to more easily locate specific files, songs, calendar events and text messages." People are already familiar with conversational interactions with AI, and Bloomberg says the bigger update to Siri will be support both text and voice. Siri already uses these input methods, but there's no real continuity between sessions.]]></content:encoded></item><item><title>Not to be outdone by OpenAI, Apple is reportedly developing an AI wearable</title><link>https://techcrunch.com/2026/01/21/not-to-be-outdone-by-openai-apple-is-reportedly-developing-an-ai-wearable/</link><author>Lucas Ropek</author><category>tech</category><pubDate>Thu, 22 Jan 2026 00:20:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Should this wearable materialize, it could be released as early as 2027, according to a report on the device.]]></content:encoded></item><item><title>Spotify Lawsuit Triggered Anna&apos;s Archive Domain Name Suspensions</title><link>https://yro.slashdot.org/story/26/01/21/2320256/spotify-lawsuit-triggered-annas-archive-domain-name-suspensions?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 22 Jan 2026 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TorrentFreak: Spotify and several major record labels, including UMG, Sony, and Warner, have taken legal action against the unknown operators of Anna's Archive. The action follows the shadow library's announcement that it would release hundreds of terabytes of scraped Spotify data. Unsealed documents reveal that the court already issued a broad preliminary injunction, ordering hosting companies, Cloudflare, and domain name services, to take action. [...] All these documents were filed under seal, as the shadow library might otherwise be tipped off and take countermeasures. These documents were filed ex-parte and kept away from Anna's Archive. According to Spotify and the labels, this is needed "so that Anna's Archive cannot pre-emptively frustrate" the countermeasures they seek.
 
The lawsuit (PDF), which was unsealed recently, explains directly why Anna's Archive lost several of its domain names over the past weeks. The .ORG domain was suspended by the U.S.-based Public Interest Registry (PIR) in early January, while a domain registrar took the .SE variant offline a few days later. "We don't believe this has to do with our Spotify backup," AnnaArchivist said at the time, but court records prove them wrong. The unsealed paperwork shows that the court granted a temporary restraining order (TRO) on January 2, which aimed to target Anna's Archive hosting and domain names. The sealed nature of this order also explains why the .ORG registry informed us that it could not comment on the suspension last week. While the .ORG and the .SE domains are suspended now, other domains remain operational. This suggests that the responsible registrars and registries do not automatically comply with U.S. court orders.
 
[...] While the unsealed documents resolve the domain suspension mystery, it is only the start of the legal battle in court. It is expected that Spotify and the music companies will do everything in their power to take further action, if needed. Interestingly, however, it appears that the music industry lawsuit may have already reached its goal. A few days ago, the dedicated Spotify download section was removed by Anna's Archive. Whether this removal is linked to the legal troubles is unknown. However, it appears that Anna's Archive stopped the specific distribution of Spotify content alleged in the complaint, seemingly in partial compliance with the injunction's ban on 'making available' the scraped files.]]></content:encoded></item><item><title>Congress Wants To Hand Your Parenting To Big Tech</title><link>https://www.techdirt.com/2026/01/21/congress-wants-to-hand-your-parenting-to-big-tech/</link><author>Joe Mullin</author><category>tech</category><pubDate>Wed, 21 Jan 2026 23:38:57 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Lawmakers in Washington are once again focusing on kids, screens, and mental health. But according to Congress, Big Tech is somehow both the problem  the solution. The Senate Commerce Committee recently held a hearing on “examining the effect of technology on America’s youth.” Witnesses warned about “addictive” online content, mental health, and kids spending too much time buried in screen. At the center of the debate is a bill from Sens. Ted Cruz (R-TX) and Brian Schatz (D-HI) called the Kids Off Social Media Act (KOSMA), which they say will protect children and “empower parents.” That’s a reasonable goal, especially at a time when many parents feel overwhelmed and nervous about how much time their kids spend on screens. But while the bill’s press release contains soothing language, KOSMA doesn’t actually give parents more control. Instead of respecting how most parents guide their kids towards healthy and educational content, KOSMA hands the control panel to Big Tech. That’s right—this bill would take power away from parents, and hand it over to the companies that lawmakers say are the problem.  Kids Under 13 Are Already Banned From Social MediaOne of the main promises of KOSMA is simple and dramatic: it would ban kids under 13 from social media. Based on the language of bill sponsors, one might think that’s a big change, and that today’s rules let kids wander freely into social media sites. But that’s not the case.   Every major platform already draws the same line: kids under 13 cannot have an account. Facebook, Instagram, TikTok, X, YouTube, Snapchat, Discord, Spotify, and even blogging platforms like WordPress all say essentially the same thing—if you’re under 13, you’re not allowed. That age line has been there for many years, mostly because of how online services comply with a federal privacy law called COPPA. Of course, everyone knows many kids under 13 are on these sites anyways. The real question is how and why they get access. Most Social Media Use By Younger Kids Is Family-Mediated If lawmakers picture under-13 social media use as a bunch of kids lying about their age and sneaking onto apps behind their parents’ backs, they’ve got it wrong. Serious studies that have looked at this all find the opposite: most under-13 use is out in the open, with parents’ knowledge, and often with their direct help. A large national study published last year in  found that 63.8% of under-13s have a social media account, but only 5.4% of them said they were keeping one secret from their parents. That means roughly 90% of kids under 13 who are on social media aren’t hiding it at all. Their parents know. (For kids aged thirteen and over, the “secret account” number is almost as low, at 6.9%.) Earlier research in the U.S. found the same pattern. In a well-known study of Facebook use by 10-to-14-year-olds, researchers found that about 70% of parents said they actually helped create their child’s account, and between 82% and 95% knew the account existed. Again, this wasn’t kids sneaking around. It was families making a decision together.A 2022 study by the UK’s media regulator Ofcom points in the same direction, finding that up to two-thirds of social media users below the age of thirteen had direct help from a parent or guardian getting onto the platform. The typical under-13 social media user is not a sneaky kid. It’s a family making a decision together. KOSMA Forces Platforms To Override Families This bill doesn’t just set an age rule. It creates a legal duty for platforms to police families.Section 103(b) of the bill is blunt: if a platform knows a user is under 13, it “shall terminate any existing account or profile” belonging to that user. And “knows” doesn’t just mean someone admits their age. The bill defines knowledge to include what is “fairly implied on the basis of objective circumstances”—in other words, what a reasonable person would conclude from how the account is being used. The reality of how services would comply with KOSMA is clear: rather than risk liability for how they should have known a user was under 13, they will require all users to prove their age to ensure that they block anyone under 13. KOSMA contains no exceptions for parental consent, for family accounts, or for educational or supervised use. The vast majority of people policed by this bill won’t be kids sneaking around—it will be minors who are following their parents’ guidance, and the parents themselves. Imagine a child using their parent’s YouTube account to watch science videos about how a volcano works. If they were to leave a comment saying, “Cool video—I’ll show this to my 6th grade teacher!” and YouTube becomes aware of the comment, the platform now has clear signals that a child is using that account. It doesn’t matter whether the parent gave permission. Under KOSMA, the company is legally required to act. To avoid violating KOSMA, it would likely  lock, suspend, or terminate the account, or demand proof it belongs to an adult. That proof would likely mean asking for a scan of a government ID, biometric data, or some other form of intrusive verification, all to keep what is essentially a “family” account from being shut down.Violations of KOSMA are enforced by the FTC and state attorneys general. That’s more than enough legal risk to make platforms err on the side of cutting people off.Platforms have no way to remove “just the kid” from a shared account. Their tools are blunt: freeze it, verify it, or delete it. Which means that even when a parent has explicitly approved and supervised their child’s use, KOSMA forces Big Tech to override that family decision.Your Family, Their AlgorithmsKOSMA doesn’t appoint a neutral referee. Under the law, companies like Google (YouTube), Meta (Facebook and Instagram), TikTok, Spotify, X, and Discord will become the ones who decide whose account survives, whose account gets locked, who has to upload ID, and whose family loses access altogether. They won’t be doing this because they want to—but because Congress is threatening them with legal liability if they don’t. These companies don’t know your family or your rules. They only know what their algorithms infer. Under KOSMA, those inferences carry the force of law. Rather than parents or teachers, decisions about who can be online, and for what purpose, will be made by corporate compliance teams and automated detection systems. This debate isn’t really about TikTok trends or doomscrolling. It’s about all the ordinary, boring, parent-guided uses of the modern internet. It’s about a kid watching “How volcanoes work” on regular YouTube, instead of the stripped-down YouTube Kids. It’s about using a shared Spotify account to listen to music a parent already approves. It’s about piano lessons from a teacher who makes her living from YouTube ads.These aren’t loopholes. They’re how parenting works in the digital age. Parents increasingly filter, supervise, and, usually, decide together with their kids. KOSMA will lead to more locked accounts, and more parents submitting to face scans and ID checks. It will also lead to more power concentrated in the hands of the companies Congress claims to distrust. KOSMA also includes separate restrictions on how platforms can use algorithms for users aged 13 to 17. Those raise their own serious questions about speech, privacy, and how online services work, and need debate and scrutiny as well. But they don’t change the core problem here: this bill hands control over children’s online lives to Big Tech.If Congress really wants to help families, it should start with something much simpler and much more effective: strong privacy protections for everyone. Limits on data collection, restrictions on behavioral tracking, and rules that apply to adults as well as kids would do far more to reduce harmful incentives than deputizing companies to guess how old your child is and shut them out.But if lawmakers aren’t ready to do that, they should at least drop KOSMA and start over. A law that treats ordinary parenting as a compliance problem is not protecting families—it’s undermining them.Parents don’t need Big Tech to replace them. They need laws that respect how families actually work.]]></content:encoded></item><item><title>Sources: Project SGLang spins out as RadixArk with $400M valuation as inference market explodes</title><link>https://techcrunch.com/2026/01/21/sources-project-sglang-spins-out-as-radixark-with-400m-valuation-as-inference-market-explodes/</link><author>Marina Temkin</author><category>tech</category><pubDate>Wed, 21 Jan 2026 23:24:14 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[SGLang, which originated as an open source research project at Ion Stoica’s UC Berkeley lab, has raised capital from Accel.]]></content:encoded></item><item><title>Millions of people imperiled through sign-in links sent by SMS</title><link>https://arstechnica.com/security/2026/01/millions-of-people-imperiled-through-sign-in-links-sent-by-sms/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2026/01/sms-phone-risk-trap-privacy-security-1152x648.jpg" length="" type=""/><pubDate>Wed, 21 Jan 2026 23:22:14 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT - Ars Technica</source><content:encoded><![CDATA[Websites that authenticate users through links and codes sent in text messages are imperiling the privacy of millions of people, leaving them vulnerable to scams, identity theft, and other crimes, recently published research has found.The links are sent to people seeking a range of services, including those offering insurance quotes, job listings, and referrals for pet sitters and tutors. To eliminate the hassle of collecting usernames and passwords—and for users to create and enter them—many such services instead require users to provide a cell phone number when signing up for an account. The services then send authentication links or passcodes by SMS when the users want to log in.A paper published last week has found more than 700 endpoints delivering such texts on behalf of more than 175 services that put user security and privacy at risk. One practice that jeopardizes users is the use of links that are easily enumerated, meaning scammers can guess them by simply modifying the security token, which usually appears at the right of a URL. By incrementing or randomly guessing the token—for instance, by first changing 123 to 124 or ABC to ABD and so on—the researchers were able to access accounts belonging to other users. From there, the researchers could view personal details, such as partially completed insurance applications.]]></content:encoded></item><item><title>Apple Developing AI Wearable Pin</title><link>https://apple.slashdot.org/story/26/01/21/211226/apple-developing-ai-wearable-pin?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 21 Jan 2026 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[According to a report by The Information (paywalled), Apple is reportedly developing an AirTag-sized, camera-equipped AI wearable pin that could arrive as early as 2027.
 
"Apple's pin, which is a thin, flat, circular disc with an aluminum-and-glass shell, features two cameras -- a standard lens and a wide-angle lens -- on its front face, designed to capture photos and videos of the user's surroundings," reports The Information, citing people familiar with the device. "It also includes three microphones to pick up sounds in the area surrounding the person wearing it. It has a speaker, a physical button along one of its edges and a magnetic inductive charging interface on its back, similar to the one used on the Apple Watch..." 9to5Mac reports: The Information also notes that Apple is attempting to speed up development in hopes of competing with OpenAI's first wearable (slated to debut in 2026), and that it is not immediately clear whether this wearable would work in conjunction with other products, such as AirPods or Apple's reported upcoming smart glasses. Today's report also notes that this has been a challenging market for new companies, citing the recent failure of Humane's AI Pin as an example.]]></content:encoded></item><item><title>Copyright Kills Competition</title><link>https://www.eff.org/deeplinks/2026/01/copyright-kills-competition</link><author>Tori Noble</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/copyright-static.png" length="" type=""/><pubDate>Wed, 21 Jan 2026 23:14:02 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[The DMCA’s “Anti-Circumvention” Provision]]></content:encoded></item><item><title>CRASH Clock Measures Dangerous Overcrowding in Low Earth Orbit</title><link>https://spectrum.ieee.org/kessler-syndrome-crash-clock</link><author>Margo Anderson</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82Mjg3NDU5OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgxNjg2ODE0OX0.2qYh2P6-2plw8A54RhIlc1zgjakFz3OBXiONRL263Ao/image.jpg?width=600" length="" type=""/><pubDate>Wed, 21 Jan 2026 23:04:38 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[One solar storm could trigger a catastrophic collision in orbit]]></content:encoded></item><item><title>AMD Sends Out Linux Patches For Next-Gen EPYC Features: GLBE, GLSBE &amp; PLZA</title><link>https://www.phoronix.com/news/AMD-Linux-GLBE-GLSBE-PLZA</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:48:47 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Sent out to the Linux kernel mailing list this afternoon were a set of 19 patches in preparing for some new CPU features presumably to be found with AMD's next-generation EPYC "Venice" processors...]]></content:encoded></item><item><title>A timeline of the US semiconductor market in 2025</title><link>https://techcrunch.com/2026/01/21/a-timeline-of-the-u-s-semiconductor-market-in-2025/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:46:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[From leadership changes at legacy semiconductor companies to wishy washy policy around chip exports, a lot happened last year.]]></content:encoded></item><item><title>Nova Launcher Gets a New Owner and Ads</title><link>https://slashdot.org/story/26/01/21/2055248/nova-launcher-gets-a-new-owner-and-ads?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Nova Launcher has been acquired by Instabridge, which says it will keep the app maintained but is evaluating ad-supported options for the free version. Android Authority reports: Today, Nova Launcher announced that the Swedish company Instabridge has acquired it from Branch Metrics. Instabridge claims it wants to be a responsible owner of Nova and does not want to reinvent the launcher overnight. However, the launcher still needs a sustainable business model to support ongoing development and maintenance. To this end, Instabridge is exploring different options, including paid tiers and ad-supported options for the free version. The new owners claim that if ads are introduced, Nova Prime will remain ad-free. However, this is misleading, as ads are already here for some users. Last year, the founder and original programmer of Nova Launcher left the company, signaling its "death" as he had been the sole developer working on the launcher for the past year.]]></content:encoded></item><item><title>X copies Bluesky with a ‘Starterpacks’ feature that helps you find who to follow</title><link>https://techcrunch.com/2026/01/21/x-copies-bluesky-with-a-starterpacks-feature-that-helps-you-find-who-to-follow/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:33:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[X says the new feature, similar to Bluesky's Starter Packs, will arrive in the coming weeks. ]]></content:encoded></item><item><title>5 Risks You Have To Take as a Leader</title><link>https://hackernoon.com/5-risks-you-have-to-take-as-a-leader?source=rss</link><author>Vinita Bansal</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:28:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The biggest risk as a leader is playing safe and not taking any risks—going with popular decisions instead of pushing for unusual prospects, faking confidence and projecting an image of perfection instead of showing up authentically by admitting limitations and acknowledging what they don’t know, saying yes all the time to people please and build likability instead of saying no to focus on high-impact work even if it displeases someone in the short-term, staying silent to maintain peace and harmony instead of speaking up and voicing their concerns, maintaining the status quo with fear of failure instead of pushing for continuous reinvention and maintaining a tight control over their team instead of empowering and letting go.\
Leaders need to have a high appetite for taking risks, not just in choosing unconventional paths, taking bold risks or setting aggressive business targets, but also in the way they lead their teams—what they choose to hide and what they choose to share, how do they balance freedom and control, what image they project and the message that passes to their teams and how they handle difficult situations that are messy and hard. It’s often a tricky balance, one that requires taking risks without going overboard and stepping into the unproductive zone.Giving boundaryless freedom can lead to very bad decisions.Sharing information that doesn’t concern people in the team can confuse and distract them.Displaying extreme emotions in the name of authenticity can dilute the impact of the message being conveyed.Speaking truth without a sign of compassion can seem cruel and inhuman.\
Every situation at work has some risk involved—risk of failure, risk of reputation, risk of judgment, risk of criticism, risk of disappointment, risk of misunderstanding. These risks can often prevent leaders from engaging in behaviors that are uncomfortable at first. When risk hijacks the amygdala in the brain, it exaggerates negative outcomes and sidelines logical reasoning, making leaders hyper-focused on avoiding threats rather than exploring opportunities.\
But leaders who don’t take these risks limit their team’s growth and potential. People in the organization take their cues from their leaders and model their behaviors—leaders who don’t embrace risks indirectly tell their teams to play it safe too.Leadership is scarce because few people are willing to go through the discomfort required to lead. This scarcity makes leadership valuable.…It’s uncomfortable to stand up in front of strangers. It’s uncomfortable to propose an idea that might fail. It’s uncomfortable to challenge the status quo. It’s uncomfortable to resist the urge to settle. When you identify the discomfort, you’ve found the place where a leader is needed. If you’re not uncomfortable in your work as a leader, it’s almost certain you’re not reaching your potential as a leader.\
Here are the 5 risks every leader must take daily because it’s impossible to get better at anything without consistent practice:Making Unpopular DecisionsIt’s safe to go with the majority and nod in agreement to a popular decision. You don’t have to voice your concern, share your opinion, or express a disagreement because doing these things often comes with a risk.What if others don’t like it?What if they turn against you?\
But prioritizing consensus, popularity, approval, and likability keeps the possibility of a better decision out of reach. You may not share your opinion when it doesn’t align with the majority because it requires standing up with courage and conviction. You may not speak up when you disagree because you worry about how it will be perceived. You may agree to a decision that you know won’t work because telling others they’re wrong is often scary.\
Challenging the status quo, voicing your concerns, and sharing disruptive ideas is risky—but it’s the risk you’ve got to take as a leader. It may subject you to frowns from people who think your ideas are crazy. You may face resistance at first. Some might even disapprove of it. Others might resent you for your ability to think creatively and provide a fresh perspective.The true mark of a leader is the willingness to stick with a bold course of action — an unconventional business strategy, a unique product-development roadmap, a controversial marketing campaign — even as the rest of the world wonders why you're not marching in step with the status quo. In other words, real leaders are happy to zig while others zag. They understand that in an era of hyper-competition and non-stop disruption, the only way to stand out from the crowd is to stand for something special.\
To build risk-taking capacity for speaking up without falling for groupthink, ask these questions:Am I saying yes to this decision because I really believe in it or because it aligns with the majority?Are all ideas simply small variations of one another, tried-and-tested approaches, or things that have less risk involved? What would be a completely unique approach that we haven’t explored yet?Why are other options less exciting compared to the current choice?What’s the worst that could happen if this decision does not work out as expected? What’s my plan B?How can I get a buy-in without intimidating and pushing others away?\
Avoiding new opportunities with fear of failure, dismissing ideas because they seem too risky, or defaulting to tried-and-tested methods over bold initiatives caps your team’s potential. Have the courage and conviction to stand alone. Take the risk by navigating the uncharted territory.You may put on a facade of strength by hiding your vulnerabilities to protect yourself from being exposed. Bringing your authentic self to work by admitting gaps in your knowledge, sharing your mistakes, or expressing your true emotions and feelings often comes with a risk.\
What if people doubt your competence?What if they don’t respect you?What if it makes you look weak?\
But projecting an image of confidence, faking knowledge when you don’t know something, and hiding your true emotions and feelings prevent you from building a bond with people at work. Leaders aren’t expected to be perfect—they’re required to be human. What builds respect isn’t your successes but how gracefully you handled failures. What develops a sense of connection isn’t your imperfections, but the flaws you were willing to share. What enables safety isn’t the fancy messages or the words of encouragement, but how you model safety through your own behaviors and actions.\
Vulnerability is not weakness—admitting mistakes, not having all the answers, or saying  does not hurt your credibility as a leader. In fact, it increases approachability, builds likability, and increases respect. Pretending to know something or coming across as a  frustrates others—they can see when you genuinely have the knowledge and experience and when you’re just faking it. But remember this: authenticity can’t be an excuse for burdening others by oversharing or justifying your reckless behavior. You have to seek a balance by defining clear boundaries for yourself and others.Fear of being shamed causes people to put on masks and live in fear and pretense, creating a stronghold of pride. Authentic, transparent leaders encourage people to develop trust through their own honesty and vulnerability. They do not view transparency as weakness, but recognize it as a source of their virtue, power and anointing because power flows through humility.\
To build risk-taking capacity while showing up authentically without going overboard, ask these questions:What information do I need to share with others? Is it important for them to know? How will it be helpful without overwhelming them?How can I combine my struggles with the solutions I implemented so that it encourages others to stay resilient and not develop a complaining attitude?How can I express my lack of confidence in something without coming across as unsure or indecisive?How can I share what I’m feeling without unsettling others or making them feel responsible for fixing it?\
Leaders aren’t deeply admired for their intelligence, knowledge, experience, or skills, but for the way they make others feel—human. Don’t hide your mistakes. Don’t cover your flaws. Show up authentically.Difficult conversations, by nature, are tricky. They are touchy topics that no one likes to talk about. They involve addressing differences of opinion, emotional issues, sensitive subjects, or other potential reasons for conflict—anything you find hard to talk about. They are challenging because they require you to navigate through discomfort, uncertainty, and a wide range of complex emotions.\
You may ignore difficult situations at work or put them off for too long—an employee not performing, a high performer displaying toxic behavior, or stakeholders making unreasonable demands. These situations are sensitive and often need to be handled with care. Staying silent and doing nothing seems like a safer option when speaking up and not getting the alignment you need can be even more risky. It’s much easier to avoid emotionally draining and mentally exhausting situations than step right into them consciously.\
What if they don’t agree with you?What if they go behind your back to seek approvals?\
But putting off  is a bad idea because issues left unaddressed escalate over time. What was once a manageable problem can grow into a much larger issue if not addressed on time. Constant worry about unresolved issues can take a toll on your mental health and lead to increased stress, anxiety, and even feelings of helplessness. When important issues are being ignored or swept under the rug, it can erode trust, build resentment, and damage relationships.\
No matter how hard a conversation is, you can’t put it off or delay it forever. Addressing issues directly, providing clarity, and seeking closure can help you gain trust and respect, and also alleviate stress.Beginning a conversation is an act of bravery. When you initiate a conversation, you fearlessly step into the unknown. Will the other person respond to favorably or unfavorably? Will it be a friendly or hostile exchange? There is a feeling of being on the edge. That nanosecond of space and unknowing can be intimidating. It shows your vulnerability.\
To build risk-taking capacity for speaking hard truths, ask these questions:How am I dealing with this difficult situation—am I facing the situation head-on or seeing the problem, closing my eyes, and getting busy with something else?What’s the impact of not addressing this issue at the right time?What’s the worst that can happen if I speak the truth?How can I communicate in a manner that does not cause the other person to react badly or turn defensive?\
Difficult conversations, though necessary, are hard to crack. Fear of a bad outcome or not knowing what to say can prevent you from speaking hard truths. Stop playing silly games. Engage in healthy dialogue right when you need it the most.You may be involved in every small decision, every minute detail, and every communication that happens in your team. Staying on top of everything makes it less likely for things to go wrong—risk factor is minimized when you’re in control. Letting go requires you to relinquish control, which can leave you with feelings of anxiety, insecurity, and helplessness.\
What if they make a big mistake?What if they overshadow you?\
But not empowering your team to make their own decisions or demanding that they consult you on every problem prevents them from developing the skills required to grow in their role—if you keep doing all the thinking for your team, they’ll never develop creative thinking skills. If you keep solving their problems, they’ll never learn to navigate complexity. If you keep preventing mistakes, they’ll become more reckless and inattentive.\
Empowerment is risky, but it’s the only way to develop future leaders. Unless people in the team get the freedom and opportunity to own their decisions, make mistakes, and try different strategies to achieve their goals, they’ll always be dependent on someone else, which will not only slow them down but also prevent them from developing the skills required to grow in their role. Both freedom and control are necessary—but you have to seek the right balance. Without taking that risk, you’ll be left with a team that can’t keep up as business scales and expectations expand.Micromanagement happens when you keep power to yourself. Empowerment is when you give power to your team.\
To build risk-taking capacity for letting go by enabling your team to do great things independently, ask these questions:Is my team clear on the goals and the outcomes they are expected to achieve? What information might be missing that can prevent them from succeeding?Do people in the team have the skills and knowledge required to make their own decisions? What gaps exist? How can these gaps be filled without my continuous intervention?Have I set clear decision boundaries with my team on the kind of decisions they can make independently and the ones where I need to be involved?Do I hold my team accountable to meet their deadlines while not compromising on quality?Do I encourage my team to learn from their mistakes, put a new plan into place, and keep moving forward instead of berating them and filling them with feelings of incompetence and self-doubt?\
Keeping tight control over your team for the risk of failure prevents you from scaling and building a high-performing team. It’s a recipe for short-term wins, not long-term success. Coach, don’t spoon-feed your team. Let them spread their wings.You may say “yes” to every request, every opportunity, and every change you’re asked to consider. Being agreeable puts less burden on you to prioritize and also reduces chances of conflict—saying no can be risky because you don’t know how others will respond or how your decisions will turn out.\
What if they take it personally?What if you let go of a great opportunity?\
But committing more than you could handle or saying “yes” to inconsequential activities will ultimately hurt your reputation as you fail to meet commitments or create the desired impact. Saying “yes” brings short-term comfort—you don’t have to worry about how others will respond or the fear of making the wrong decision. But not considering the consequences of your decision turns into regret when you finally have to face them in the future.\
Your responsibility as a leader isn’t to please everyone or make them happy; it’s to multiply your impact and the value you add by risking saying no. Saying no that lands right does not need lengthy explanations—they come across as justifications and often distract and confuse the other person. Instead, be precise. State your reason by being straightforward, clear, and concise—three elements of good communication.The great art is to learn to integrate the two, to marry yes and no. That is the secret to standing up for yourself and what you need, without destroying valuable agreements and precious relationships. That is what a ‘Positive No’ seeks to achieve.\
Instead of a knee-jerk yes or no, build risk-taking capacity for saying no by asking these questions:What’s this request about—what exactly is it asking me to do?What excites me about this opportunity?What’s the cost of taking it on—in terms of effort, time required, and the impact on the team’s existing priorities? What’s the scale and scope of the request? What kind of time commitment does it demand?What’s the cost of not doing it? How important is it to the person and the organization?How does it align with my team’s current plans and commitments?What could be my reason for saying no?\
No is risky, but so is yes. Every “yes” you say has an opportunity cost—doing something will always come at the cost of not doing something else. Give yourself permission to say no. Protect your team’s time and energy.Leadership is all about making the jump, taking risks, and learning from your mistakes. It's about falling, dusting ourselves off, and getting back up again and again and again.Leaders need to build a very strong appetite for taking risks, not just in business decisions—defining strategies, setting targets, and taking bets, but also in how they lead their teams.Standing up and suggesting an unpopular choice is often risky—it may not work, others may not like it, or you may face a lot of resistance. But not challenging the status quo and staying with safe options limits your impact. Don’t take the easy road—fight for choices that are hard at first, but rewarding in the end.Expressing gaps in your knowledge or sharing your fears can be risky—what if others doubt your competence or your authenticity is mistaken for weakness? But faking knowledge or pretending to be someone you’re not prevents you from bonding and building trust. Showing up authentically as a leader builds connection—seeing the real you makes you more trustworthy and appealing as a human. Vulnerability is not weakness—balance it by defining boundaries without overwhelming others with too much information or excessive emotions.Facing difficult situations head-on and resolving the conflict evokes strong feelings of fear as pointing out performance gaps, addressing toxic behavior, or confronting unreasonable stakeholders is often risky—others may turn defensive or resent you for speaking the truth. But not addressing them at the right time makes the problem worse. Care personally and challenge directly—be candid and compassionate to make yourself heard.Being too involved with your team gives you a sense of control and makes it feel less risky, as it gives you the opportunity to make decisions, solve problems, and avoid mistakes. But doing all the thinking for your team keeps them dependent and prevents them from learning and growing. Let go of control. Empower your team—optimize for long-term growth, not short-term wins.Saying yes to every request and every change appears less risky, as you don’t have to worry about upsetting someone or letting go of a great opportunity. But not prioritizing work makes you overcommit—you overpromise and underdeliver, which hurts your credibility. Learn to say no without feelings of shame or guilt. Don’t just make commitments, keep them, too.\
This story was previously published here. Follow me on LinkedIn or here for more stories.]]></content:encoded></item><item><title>Todoist’s app now lets you add tasks to your to-do list by speaking to its AI</title><link>https://techcrunch.com/2026/01/21/todoists-app-now-lets-you-add-tasks-to-your-to-do-list-by-speaking-to-its-ai/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:19:17 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The feature, now public, lets you create to-do's and action items by speaking naturally to the app's AI. ]]></content:encoded></item><item><title>Apple plans to make Siri an AI chatbot, report says</title><link>https://techcrunch.com/2026/01/21/apple-plans-to-make-siri-an-ai-chatbot-report-says/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:12:50 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Siri could look more like ChatGPT than its current state as an integrated feature across Apple products.]]></content:encoded></item><item><title>Anthropic revises Claude’s ‘Constitution,’ and hints at chatbot consciousness</title><link>https://techcrunch.com/2026/01/21/anthropic-revises-claudes-constitution-and-hints-at-chatbot-consciousness/</link><author>Lucas Ropek</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:07:58 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The newly revised document offers a roadmap for what Anthropic says is a safer and more helpful chatbot experience. ]]></content:encoded></item><item><title>HAM Radio Operators In Belarus Arrested, Face the Death Penalty</title><link>https://tech.slashdot.org/story/26/01/21/2018229/ham-radio-operators-in-belarus-arrested-face-the-death-penalty?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 21 Jan 2026 22:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from 404 Media: The Belarusian government is threatening three HAM radio operators with the death penalty, detained at least seven people, and has accused them of "intercepting state secrets," according to Belarusian state media, independent media outside of Belarus, and the Belarusian human rights organization Viasna. The arrests are an extreme attack on what is most often a wholesome hobby that has a history of being vilified by authoritarian governments in part because the technology is quite censorship resistant.
 
The detentions were announced last week on Belarusian state TV, which claimed the men were part of a network of more than 50 people participating in the amateur radio hobby and have been accused of both "espionage" and "treason." Authorities there said they seized more than 500 pieces of radio equipment. The men were accused on state TV of using radio to spy on the movement of government planes, though no actual evidence of this has been produced. State TV claimed they were associated with the Belarusian Federation of Radioamateurs and Radiosportsmen (BFRR), a long-running amateur radio club and nonprofit that holds amateur radio competitions, meetups, trainings, and forums. Siarhei Besarab, a Belarusian HAM radio operator, posted a plea for support from others in the r/amateurradio subreddit. "I am writing this because my local community is being systematically liquidated in what I can only describe as a targeted intellectual genocide," Besarab wrote. "I beg you to amplify this signal and help us spread this information. Please show this to any journalist you know, send it to human rights organizations, and share it with your local radio associations."]]></content:encoded></item><item><title>Does MariaDB Depend on MySQL?</title><link>https://hackernoon.com/does-mariadb-depend-on-mysql?source=rss</link><author>Alejandro Duarte</author><category>tech</category><pubDate>Wed, 21 Jan 2026 21:32:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When MariaDB was first announced in 2009 by Michael “Monty” Widenius, it was positioned as a “fork of MySQL.” I think that was a Bad Idea™. Okay, maybe it wasn’t a bad idea as such. After all, MariaDB indeed is a fork of MySQL. But what is a  in the software sense, and how is this reflected in MariaDB? A fork is a software project that takes the source code of another project and continues development independently from the original. \
Forks often start by maintaining compatibility with their parent project, but they can evolve to become detached from their own features, architecture, bug tracker, mailing list, development philosophy, and community. This is the case of MariaDB, with the addition that it continues to be highly compatible with old MySQL versions and with its current ecosystem at large.\
Before we dig into it, let me clarify that I like MySQL. It was the very first database that I installed during my university time, and I have used it in my hobby as well as production projects for a long time. So, why did I affirm that positioning MariaDB as a fork of MySQL was a bad idea? In short, because MariaDB doesn’t depend on MySQL. The idea of defining MariaDB merely as a fork of MySQL leads to misconceptions around its future. Take, as an example, this old comment on Hacker News which refers to the phrase “RIP Open Source MySQL”:\
“Forgive my ignorance, but doesn’t this harm MySQL forks as well? Since the test cases are unavailable from now on, say for example they wanted to reimplement a certain feature, isn’t it much harder for them to validate that their implementation works correctly?”\
I sympathize with the author of this comment. We were unintentionally misled by the “fork of MySQL” slogan. I encounter this kind of lack of clarity more often than I would like. But the reality is that the development of MariaDB has been independent for many years already. MariaDB developers don’t wait for MySQL to implement features, test cases, fix bugs, or innovate. They write their own tests, create their own features, and solve problems in their own way. \
When Oracle changes something in MySQL or restricts access to a component, that has no meaningful impact on MariaDB’s roadmap because the projects have diverged so significantly that they’re essentially different database systems that happen to share some common ancestry, be highly compatible (you can use MySQL connectors and tools with MariaDB), and are named after Monty’s children. The most common outcome, since keeping a software project alive requires considerable effort.A re-merging of the fork with the original: Both software projects rejoin each other.The death of the original: Users and developers move to the new, younger project. Both find success with different developers and end users.\
For years, the MySQL-MariaDB situation was clearly a  where both projects found new homes. One in Oracle, the other in the new MariaDB Foundation/MariaDB plc duo. Contrary to what many would have thought, Oracle invested in MySQL and continued its development in the open despite having its own closed-source relational database. \
Recent (and not so recent) findings and events show that Oracle has slowed down at least on the innovation front and at worst on the maintenance side. In his article Stop using MySQL in 2026, it is not true open source, Otto Kekäläinen (former Software Development Manager at AWS) shows that “the number of git commits on github.com/mysql/mysql-server has been significantly declining in 2025.” \
He also highlights the steep decrease in MySQL’s popularity according to DB-Engines, as well as the reported “degraded performance with newer MySQL versions.” Are we witnessing a “death of the original” here? I don’t know.\
In light of all this, many developers are starting to evaluate migration strategies to other relational databases, with MariaDB and TiDB being two of the most attractive options. According to Otto Kekäläinen, “TiDB only really shines with larger distributed setups, so for the vast majority of regular small and mid-scale applications currently using MySQL, the most practical solution is probably to just switch to MariaDB.” \
How about the elephant in the room, you might ask? PostgreSQL is a database with tons of forks and third-party extensions that you can download, which makes it popular not only due to its features but also the sheer number of companies marketing their PostgreSQL flavor online. For applications currently using MySQL, migrating to PostgreSQL requires a lot of work, including SQL code and connector migrations. Two tasks that can be close to zero-effort with MariaDB. Check, for example, this crazy live broadcast where Cantamen (Germany’s leading car-sharing service provider) migrates from MySQL to MariaDB with the help of Monty himself.\
Let’s get back to my highly opinionated introductory statement… MariaDB is a—now we have learned—detached fork of MySQL, and, to be fair, it has also been positioned as a “MySQL replacement,” which is something very accurate to state. I’m glad to see the “replacement” slogan more and more often as opposed to the “fork” one. I personally suggested to Kaj Arnö (Executive Chairman at the MariaDB Foundation) going with something even stronger, like “MariaDB fixes MySQL.” That’s a bit too strong, perhaps. I’m glad they softened it to “MariaDB is the Future of MySQL”.]]></content:encoded></item><item><title>Techdirt Podcast Episode 441: A Manifesto To Build A Better Internet</title><link>https://www.techdirt.com/2026/01/21/techdirt-podcast-episode-441-a-manifesto-to-build-a-better-internet/</link><author>Leigh Beadon</author><category>tech</category><enclosure url="https://feeds.soundcloud.com/stream/2251652468-techdirt-a-manifesto-to-build-a-better-internet.mp3" length="" type=""/><pubDate>Wed, 21 Jan 2026 21:30:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ozempic is Reshaping the Fast Food Industry</title><link>https://science.slashdot.org/story/26/01/21/191222/ozempic-is-reshaping-the-fast-food-industry?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 21:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New research from Cornell University has tracked how households change their spending after someone starts taking GLP-1 medications like Ozempic and Wegovy, and the numbers are material enough to explain why food industry earnings calls keep blaming everything except the obvious culprit. 

The study analyzed transaction data from 150,000 households linked to survey responses on medication adoption. Households cut grocery spending by 5.3% within six months of a member starting GLP-1s; high-income households cut by 8.2%. Fast food spending fell 8.0%. Savory snacks took the biggest hit at 10.1%, followed by sweets and baked goods. Yogurt was the only category to see a statistically significant increase. 

As of July 2024, 16.3% of U.S. households had at least one GLP-1 user. Nearly half of adopters reported taking the medication specifically for weight loss rather than diabetes management. About 34% of users discontinue within the sample period, and when they stop, candy and chocolate purchases rise 11.4% above pre-adoption levels. 

Further reading: Weighing the Cost of Smaller Appetites.]]></content:encoded></item><item><title>AI Hype vs Reality in Cybersecurity Explained</title><link>https://hackernoon.com/ai-hype-vs-reality-in-cybersecurity-explained?source=rss</link><author>Zac Amos</author><category>tech</category><pubDate>Wed, 21 Jan 2026 21:11:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Artificial intelligence (AI) has quickly become a hot topic in modern cybersecurity and is often talked about as the cure-all for an increasingly hostile threat landscape. From automated threat detection to self-healing systems, AI is frequently touted as the technology that will finally tip the balance in defenders’ favor.\
Yet, behind the bold claims and vendor marketing, the day-to-day reality of how AI is really used in security operations is far more nuanced. As cyber threats continue to grow, separating what AI can deliver realistically today from what remains aspirational has become essential.The Hype: AI as the Ultimate Cybersecurity BehaviorMuch of the conversation around AI in cybersecurity has been shaped by bold promises and rapid adoption, often blurring the line between what the technology can do and what it is expected to do. Before examining AI’s role in security operations, it’s worth unpacking how hype, perception, and pressure have influenced its reputation.In marketing materials and conference keynotes, AI is often promoted as a flawless, all-seeing defense mechanism — one capable of identifying every threat, stopping every attack, and doing so with minimal human intervention.\
This framing is particularly appealing as security teams must contend with rising alert volumes and increasingly automated attack techniques. However, real-world research reveals a gap between expectation and execution. In the 2025 Exabeams report,  AI had improved productivity, but only 22% of frontline security analysts agreed. Therefore, there is a sharp disconnect between leadership perception and operational reality.\
In practice, AI tools perform best when automating narrow, well-defined tasks rather than serving as a comprehensive or autonomous security solution.The Influence of Generative AIThe rapid rise of generative AI has further intensified these inflated expectations. Tools like ChatGPT have demonstrated how convincingly AI can generate responses, analyze information, and adapt to user input, leading many to assume similar capabilities can be seamlessly applied across cybersecurity.\
The technology is undoubtedly influential, but research helps clarify where those assumptions break down. Studies examining the use of generative AI in security operations show that while these models can streamline tasks, such as alert summarization and phishing analysis, they still struggle with contextual decision-making.\
This can be especially true  and organizational risk tolerance. As a result, generative AI is most effective when supporting analysts rather than replacing human judgment.Beyond the tech marketing and media narratives, executive pressure has become a powerful driver of AI adoption in cybersecurity. Boards and C-suite leaders increasingly expect security teams to be using AI, even when expectations are loosely defined or misaligned with operational readiness.\
For CISOs, this often creates a top-down mandate driven by fear of falling behind competitors or missing out on perceived innovation. In many organizations, AI becomes a strategic checkbox rather than a capability deployed with clear goals and constraints. As a result, some teams find themselves implementing AI tools before they have the data quality, governance structures, or internal expertise to support them effectively.The Current Reality of AI in CybersecurityWhile the hype often frames AI as transformational, its real-world role in cybersecurity is far more practical and constrained. Today’s AI deployments focus less on replacing analysts and more on improving speed, scale, and consistency across specific security tasks.In practice, AI is most effective when applied to well-scoped, data-intensive problems. Security teams commonly use machine learning models to enhance threat detection, identify anomalous behavior across large datasets, and automate repetitive workflows such as alert triage and log correlation.\
To understand how widely these capabilities are being applied, researchers have examined the current body of work on AI in cybersecurity. A systematic review of AI in cybersecurity found that , 236 were identified as primary works focused on use cases.\
This number demonstrates the growing body of documented research where AI is actively deployed across functions like detection, response, and protection rather than only in theory. Therefore, this analysis suggests that AI’s role in cybersecurity has moved beyond isolated experimentation and into task-specific operational use.\
Real-world case studies also reinforce this role. Analysis of AI-driven detection techniques shows that machine learning-based systems  and support faster investigation workflows, provided the underlying data is robust and relevant. These outcomes point to AI’s strength as an efficiency multiplier rather than a stand-alone defensive system.Despite these gains, AI in cybersecurity remains constrained by several structural limitations. Effective models need large volumes of high-quality training data, but this is something many organizations struggle to maintain. Incomplete datasets, noisy logs, or biased inputs can lead to inaccurate detections or missed threats, undermining trust in automated systems.\
More critically, machine learning models can themselves be vulnerable to manipulation. Research in adversarial machine learning shows that carefully crafted inputs can cause models , creating opportunities for attackers to defeat defenses built around AI logic.\
These findings show why human oversight remains essential. AI may accelerate analysis, but it can’t independently reason about threat intent, business impact, or novel attack strategies. As a result, most organizations continue to deploy AI as part of a layered defense strategy rather than as a primary decision-maker.Where Management and Strategy Make a DifferenceEven the most advanced AI systems remain tools. In cybersecurity, their effectiveness depends more on how security teams deploy them than on algorithmic sophistication. AI can surface anomalies, correlate signals, and accelerate analysis.\
What it can’t do is independently prioritize risk, weigh business impact, or adapt strategy in response to changing organizational goals. Without clearly defined escalation paths and informed human judgment, AI becomes another source of alerts.\
This is where people and processes play a decisive role. Research across industries has shown that management work  of productivity variation, and cybersecurity is no exception. Teams with strong leadership and well-defined response strategies are far better off integrating AI into daily operations to amplify analyst expertise rather than replace it.\
Conversely, poorly managed teams often struggle to extract value even from sophisticated AI platforms, finding that automation without strategy can exacerbate confusion instead of reducing it. In short, successful AI adoption in cybersecurity hinges on the human systems that guide its use.A Glimpse Into the Next Generation of AI in CybersecurityLooking ahead, much of the innovation in AI-driven cybersecurity is focused on making defenses more adaptive. One area gaining traction is the use of AI-powered deception technologies, which aim to shift security from passive detection to active engagement.\
For instance, AI-driven honeypots are increasingly made to dynamically , learning from attacker interactions and automatically modifying decoys to better mirror real production environments. This approach allows defenders to gather higher-quality intelligence on attacker techniques while increasing the cost and complexity of successful intrusions.\
Still, these emerging capabilities point toward evolution, not replacement. While AI-enhanced honeypots and autonomous response systems may improve visibility and slow attackers, they also introduce new operational challenges like model governance and the risk of false confidence.\
The most likely future state is not fully autonomous security, but increasingly intelligent tools that extend a hand out to human teams. As AI systems become more capable of interaction and adaptation, their success will continue to depend on careful oversight and a realistic understanding of where automation ends, and human judgment must take over.Separating Signal from NoiseAI has undeniably changed how cybersecurity teams detect and respond to threats, but its impact is often overstated as a stand-alone solution. In reality, today’s AI tools work best when applied to specific problems and guided by experienced teams who understand their limitations.\
As the technology continues to evolve, the gap between hype and value will depend on how carefully organizations integrate it into their security strategies. For most teams, progress will come from using AI as one part of a balanced, human-led defense.]]></content:encoded></item><item><title>Probabilistic ML: Natural Gradients and Statistical Manifolds Explained</title><link>https://hackernoon.com/probabilistic-ml-natural-gradients-and-statistical-manifolds-explained?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Wed, 21 Jan 2026 21:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2.2 Probabilistic modeling and inference in DLLearning in general can be viewed as a process of updating certain beliefs about the state of the world based on the new information. This abstract point of view underlies the broad field of Probabilistic ML [17]. In this Subsection we mention certain aspects of this field which are the most relevant for the present study.\
The general idea of updating beliefs can be formalized as learning an optimal (according to a certain criterion) probability distribution. This further implies that implementation of probabilistic ML algorithms involves optimization over spaces of probability distributions. Therefore, gradient flows on spaces of probability measures [18, 19] are essential ingredients of probabilistic modeling in ML. The notion of gradient flow requires the metric structure. The distance between two probability measures should represent the degree of difficulty to distinguish between them provided that a limited number of samples is available. Metric on spaces of probability measures are induced by the Hessians of various divergence functions [20, 21]. The classical (and parametric invariant) choice is the Kullback-Leibler divergence (KL divergence), also referred to as relative entropy. This divergence induces the Fisher (or Fisher-Rao) information metric on spaces of probability measures thus turning them into statistical manifolds [20]. When optimizing over a family of probability distributions, Euclidean (or, so called, "vanilla") gradient is inappropriate. Ignoring of this fact, leads to inaccurate or incorrect algorithms. Instead, one should use the gradient w.r. to Fisher information metric, which is named natural gradient [22, 23, 24]. In RL this must be taken into account when designing stochastic policies. In particular, well known actor-critic algorithm has been modified in order to respect geometry of the space of probability distributions [25].\
Another way of introducing metric on spaces of probability distributions is the Wasserstein metric. Fokker-Planck equations are gradient flows in the Wasserstein metric. The potential function for these flows is the KL divergence between the instant and (unknown) stationary distribution [26]. Yet another metric sometimes used in ML is the Stein metric [27].(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Benchmarking 1B Vectors with Low Latency and High Throughput</title><link>https://hackernoon.com/benchmarking-1b-vectors-with-low-latency-and-high-throughput?source=rss</link><author>ScyllaDB</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:56:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As AI-driven applications move from experimentation into real-time production systems, the expectations placed on vector similarity search continue to rise dramatically. Teams now need to support billion-scale datasets, high concurrency, strict p99 latency budgets, and a level of operational simplicity that reduces architectural overhead rather than adding to it.ScyllaDB Vector Search was built with these constraints in mind. It offers a unified engine for storing structured data alongside unstructured embeddings, and it achieves performance that pushes the boundaries of what a managed database system can deliver at scale. The results of our recent high scale 1-billion-vector benchmark show that ScyllaDB demonstrates both ultra-low latency and highly predictable behaviour under load.To achieve low-single-millisecond performance across massive vector sets, ScyllaDB adopts an architecture that separates the storage and indexing responsibilities while keeping the system unified from the user’s perspective. The ScyllaDB nodes store both the structured attributes and the vector embeddings in the same distributed table. Meanwhile, a dedicated Vector Store service – implemented in Rust and powered by the USearch engine optimized to support ScyllaDB’s predictable single-digit millisecond latencies – consumes updates from ScyllaDB via CDC and builds approximate-nearest-neighbour (ANN) indexes in memory. Queries are issued to the database using a familiar CQL expression such as:SELECT … ORDER BY vector_column ANN_OF ? LIMIT k;
They are then internally routed to the Vector Store, which performs the similarity search and returns the candidate rows. This design allows each layer to scale independently, optimising for its own workload characteristics and eliminating resource interference.Benchmarking 1 Billion VectorsTo evaluate real-world performance, ScyllaDB ran a rigorous benchmark using the publicly available yandex-deep_1b dataset, which contains 1 billion vectors of 96 dimensions. The setup consisted of six nodes: three ScyllaDB nodes running on i4i.16xlarge instances, each equipped with 64 vCPUs, and three Vector Store nodes running on r7i.48xlarge instances, each with 192 vCPUs. This hardware configuration reflects realistic production deployments where the database and vector indexing tiers are provisioned with different resource profiles. The results focus on two usage scenarios with distinct accuracy and latency goals (detailed in the following sections).A full architectural deep-dive, including diagrams, performance trade-offs, and extended benchmark results for higher-dimension datasets, can be found in the technical blog post . These additional results follow the same pattern seen in the 96-dimensional tests: exceptionally low latency, high throughput, and stability across a wide range of concurrent load profiles.Scenario #1 — Ultra-Low Latency with Moderate RecallThe first scenario was designed for workloads such as recommendation engines and real-time personalisation systems, where the primary objective is extremely low latency and the recall can be moderately relaxed. We used index parameters m = 16, ef-construction = 128, ef-search = 64 and Euclidean distance. \n At approximately 70% recall and with 30 concurrent searches, the system maintained a p99 latency of only 1.7 milliseconds and a p50 of just 1.2 milliseconds, while sustaining 25,000 queries per second.When expanding the throughput window (still keeping p99 latency below 10 milliseconds), the cluster reached 60,000 QPS for k = 100 with a p50 latency of 4.5 milliseconds, and 252,000 QPS for k = 10 with a p50 latency of 2.2 milliseconds. Importantly, utilizing ScyllaDB’s predictable performance, this throughput scales linearly: adding more Vector Store nodes directly increases the achievable QPS without compromising latency or recall.Scenario #2 — High Recall with Slightly Higher LatencyThe second scenario targets systems that require near-perfect recall, including high-fidelity semantic search and retrieval-augmented generation pipelines. Here, the index parameters were significantly increased to m = 64, ef-construction = 512, and ef-search = 512. This configuration raises compute requirements but dramatically improves recall.With 50 concurrent searches and recall approaching 98%, ScyllaDB kept p99 latency below 12 milliseconds and p50 around 8 milliseconds while delivering 6,500 QPS. When shifting the focus to maximum sustained throughput while keeping p99 latency under 20 milliseconds and p50 under 10 milliseconds, the system achieved 16,600 QPS. Even under these settings, latency remained notably stable across values of k from 10 to 100, demonstrating predictable behaviour in environments where query limits vary dynamically.The table below presents the summary of the results for some representative concurrency levels.A big advantage of integrating Vector Search with ScyllaDB is that it delivers substantial performance and networking cost advantages. The vector store resides close to the data with just a single network hop between metadata and embedding storage in the same availability zone. This locality, combined with ScyllaDB’s shard-per-core execution model, allows the system to provide real-time latency and massive throughput even under heavy load. The result is that teams can accomplish more with fewer resources compared to specialised vector-search systems.In addition to being fast at scale, ScyllaDB’s Vector Search is also simpler to operate. Its key advantage is its ability to unify structured and unstructured retrieval within a single dataset. This means you can store traditional attributes and vector embeddings side-by-side and express queries that combine semantic search with conventional search. For example, you can ask the database to “find the top five most similar documents, but only those belonging to this specific customer and created within the past 30 days.” This approach eliminates the common pain of maintaining separate systems for transactional data and vector search, and it removes the operational fragility associated with syncing between two sources of truth.This also means there is no ETL drift and no dual-write risk. Instead of shipping embeddings to a separate vector database while keeping metadata in a transactional store, ScyllaDB consolidates everything into a single system. The only pipeline you need is the computational step that generates embeddings using your preferred LLM or ML model. Once written, the data remains consistent without extra coordination, backfills, or complex streaming jobs.Operationally, ScyllaDB simplifies the entire retrieval stack. Because it is built on ScyllaDB’s proven distributed architecture, the system is highly available, horizontally scalable, and resilient across availability zones and regions. Instead of operating two or three different technologies – each with its own monitoring, security configurations, and failure modes – you only manage one. This consolidation drastically reduces operational complexity while simultaneously improving performance.The product is now in Geeral Availability. This includes Cloud Portal provisioning, on-demand billing, a full range of instance types, and additional performance optimisations. Self-service scaling is planned for Q1. By the end of Q1 we will introduce native filtering capabilities, enabling vector search queries to combine ANN results with traditional predicates for more precise hybrid retrieval.Looking further ahead, the roadmap includes support for scalar and binary quantisation to reduce memory usage, TTL functionality for lifecycle automation of vector data, and integrated hybrid search combining ANN with BM25 for unified lexical and semantic relevance.ScyllaDB has demonstrated that it is capable of delivering industry-leading performance for vector search at massive scale, handling a dataset of 1 billion vectors with p99 latency as low as 1.7 milliseconds and throughput up to 252,000 QPS. These results validate ScyllaDB Vector Search as a unified, high-performance solution that simplifies the operational complexity of real-time AI applications by co-locating structured data and unstructured embeddings.The current benchmarks showcase the current state of ScyllaDB’s scalability. With planned enhancements in the upcoming roadmap, including scalar quantization and sharding, these performance limits are set to increase in the next year. Nevertheless, even now, the feature is ready for running latency critical workloads such as fraud detection or recommendation systems.]]></content:encoded></item><item><title>AMD ROCm 7.2 Now Released With More Radeon Graphics Cards Supported, ROCm Optiq Introduced</title><link>https://www.phoronix.com/news/AMD-ROCm-7.2-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:52:56 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Back at CES earlier this month AMD talked up features of the ROCm 7.2 release. ROCm 7.2 though wasn't actually released then, at least not for Linux. That ROCm 7.2.0 release though was pushed out today as the latest improvement to this open-source AMD GPU compute stack and officially extending the support to more Radeon graphics cards...]]></content:encoded></item><item><title>Deep Learning via Continuous-Time Systems: Neural ODEs and Normalizing Flows Explained</title><link>https://hackernoon.com/deep-learning-via-continuous-time-systems-neural-odes-and-normalizing-flows-explained?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:45:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2.1 Deep Learning via continuous-time controlled dynamical systemIn 2017. Weinan E proposed new architectures of NN’s realized through the continuous-time controlled dynamical systems [10]. This proposal was motivated by the previous observations that NN’s (most notably, ResNets) can be regarded as Euler discretizations of controlled ODE’s. In parallel, a number of studies [11, 12, 13] enhanced and expanded theoretical foundations of ML by adapting classical control-theoretic techniques to the new promising field of applications.\
This line of research resulted in a tangible outcome which was named Neural ODE [14]. The underlying idea is to formalize some ML tasks as optimal control problems. In fact, deep limits of ResNets with constant weights yield continuous-time dynamical systems [15]. In such a setup weights of the NN are replaced by control functions. Training of the model is realized through minimization of the total error (or total loss) using the Pontryagin’s maximum principle. Backpropagation corresponds to the adjoint ODE which is solved backwards in time.\
A similar way of encoding maps underlies the concept of continuous-time normalizing flows [16]. Normalizing flows are dynamical systems, usually described by ODE’s or PDE’s. These systems are trained with the goal of learning a sequence (or a flow) of invertible maps between the observed data originating from an unknown complicated target probability distribution and some simple (typically Gaussian) distribution. Once the normalizing flow is trained, the target distribution is approximated. The model is capable of generalizing the observed data and making predictions by sampling from the simple distribution and mapping the samples along the learned flow.\
We have mentioned two concepts (neural ODE and normalizing flows) that recently had a significant impact. Their success reflects the general trend of growing interest in control-theoretic point of view on ML. Most of theoretical advances in Reinforcement Learning (RL) rely on Control Theory (CT) [12, 13]. As theoretical foundations of RL are being established, the boundary between RL and CT is getting blurred.(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Half of World&apos;s CO2 Emissions Come From Just 32 Fossil Fuel Firms, Study Shows</title><link>https://news.slashdot.org/story/26/01/21/1913218/half-of-worlds-co2-emissions-come-from-just-32-fossil-fuel-firms-study-shows?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Just 32 fossil fuel companies were responsible for half the global carbon dioxide emissions driving the climate crisis in 2024, down from 36 a year earlier, a report has revealed. The Guardian: Saudi Aramco was the biggest state-controlled polluter and ExxonMobil was the largest investor-owned polluter. Critics accused the leading fossil fuel companies of "sabotaging climate action" and "being on the wrong side of history" but said the emissions data was increasingly being used to hold the companies accountable. 

State-owned fossil fuel producers made up 17 of the top 20 emitters in the Carbon Majors report, which the authors said underscored the political barriers to tackling global heating. All 17 are controlled by countries that opposed a proposed fossil fuel phaseout at the Cop30 UN climate summit in December, including Saudi Arabia, Russia, China, Iran, the United Arab Emirates and India. More than 80 other nations had backed the phaseout plan.]]></content:encoded></item><item><title>Apps for boycotting American products surge to the top of the Danish App Store</title><link>https://techcrunch.com/2026/01/21/apps-for-boycotting-american-products-surge-to-the-top-of-the-danish-app-store/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:38:54 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The boost in downloads comes as Danish consumers have been organizing a grassroots boycott of American-made products, which also included canceling their U.S. vacations and ditching their subscriptions to U.S.-based streaming services, like Netflix.]]></content:encoded></item><item><title>Irony alert: Hallucinated citations found in papers from NeurIPS, the prestigious AI conference</title><link>https://techcrunch.com/2026/01/21/irony-alert-hallucinated-citations-found-in-papers-from-neurips-the-prestigious-ai-conference/</link><author>Julie Bort</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:34:42 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Research from startup GPTZero points to the impossible problem prestigious conferences face in the age of AI slop.]]></content:encoded></item><item><title>Two Major Studies, 125,000 Kids: The Social Media Panic Doesn’t Hold Up</title><link>https://www.techdirt.com/2026/01/21/two-major-studies-125000-kids-the-social-media-panic-doesnt-hold-up/</link><author>Mike Masnick</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:22:51 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Well, here come two massive new studies—one from Australia, one from the UK—that land like a sledgehammer on Haidt’s narrative—and, perhaps more importantly, on Australia’s much-celebrated social media ban for kids under 16.The Australian study, published in JAMA Pediatrics, followed over 100,000 Australian adolescents across three years and found something that should give every policymaker pause: the relationship between social media use and well-being isn’t linear. It’s U-shaped. Perhaps most surprisingly, kids who use social media moderately have the  outcomes. Kids who use it excessively have worse outcomes. But here’s the kicker: kids who don’t use it at all  have worse outcomes.This isn’t to say that all kids should use social media. Unlike some others, we’re not saying any of this shows that social media  good or bad health outcomes. We’re pointing out that the claims of inherent harm seem not just overblown, but wrong.From the study’s key findings:A U-shaped association emerged where moderate social media use was associated with the best well-being outcomes, while both no use and highest use were associated with poorer well-being. For girls, moderate use became most favorable from middle adolescence onward, while for boys, no use became increasingly problematic from midadolescence, exceeding risks of high use by late adolescence.This seems like pretty strong evidence that Haidt’s claims of inherent harm are not well-founded, and the policy proposals to ban kids entirely from social media are a bad idea. For older teenage boys, having  social media was associated with  outcomes than having too much of it. The study found that nonusers in grades 10-12 had significantly higher odds of low well-being compared to moderate users—with boys showing an odds ratio of 3.00 and girls at 1.79.Meanwhile, researchers at the University of Manchester just published a separate study in the Journal of Public Health that followed 25,000 11- to 14-year-olds over three school years. Their conclusion? Screen time spent on social media or gaming does not cause mental health problems in teenagers. At all.The study found no evidence for boys or girls that heavier social media use or more frequent gaming increased teenagers’ symptoms of anxiety or depression over the following year. Increases in girls’ and boys’ social media use from year 8 to year 9 and from year 9 to year 10 had zero detrimental impact on their mental health the following year.Zero. Not “small.” Not “modest.” Zero.The UK researchers also examined whether  kids use social media matters—active chatting versus passive scrolling. The answer? Neither appeared to drive mental health difficulties. As lead author Dr. Qiqi Cheng put it:We know families are worried, but our results do not support the idea that simply spending time on social media or gaming leads to mental health problems – the story is far more complex than that.The Australian researchers, to their credit, are appropriately cautious about causation:While heavy use was associated with poorer well-being and abstinence sometimes coincided with less favorable outcomes, these findings are observational and should be interpreted cautiously.But while researchers urge caution, politicians have been happy to sprint ahead.The entire premise of Australia’s ban—and similar proposals floating around in various US states and across Europe—is that social media is inherently harmful to young people, and that removing access is protective. But both studies suggest the reality is far more complicated. The Australian researchers explicitly call this out:Social media’s association with adolescent well-being is complex and nonlinear, suggesting thatabstinence and excessive use can be problematic depending on developmental stage and sex.In other words: Australia’s ban may be taking kids who would have been moderate users with good outcomes and forcing them into the “no use” category that the study associates with  well-being. It’s potentially the worst of all possible policy outcomes.The UK study’s co-author, Prof. Neil Humphrey, reinforced this point:Our findings tell us that young people’s choices around social media and gaming may be shaped by how they’re feeling but not necessarily the other way around. Rather than blaming technology itself, we need to pay attention to what young people are doing online, who they’re connecting with and how supported they feel in their daily lives.That’s a crucial distinction that the moral panic crowd keeps glossing over: correlation running in the opposite direction than assumed. Kids who are already struggling, and who aren’t getting the support they need, might use social media differently—not the other way around.This shouldn’t be surprising to anyone who has been paying attention. We’ve covered study after study showing that the relationship between social media and teen mental health is complicated, context-dependent, and nowhere near as clear-cut as Haidt’s “The Anxious Generation” would have you believe. As we’ve noted before, correlation is not causation, and the timing of teen mental health declines doesn’t actually line up neatly with smartphone adoption the way the narrative claims.But nuance doesn’t make for good headlines or popular books. “Social Media Is Complicated And The Effects Depend On How You Use It, Your Age, Your Sex, And A Bunch Of Other Factors” doesn’t quite have the same ring as “Smartphones Destroyed A Generation.”No one’s beating down my door to write a book detailing the trade-offs and nuances. Instead, Haidt’s book remains on the NY Times’ best seller list almost two years after being published.The Australian study also highlights something else that should be obvious but apparently needs repeating: social media serves genuine social functions for teenagers. Being completely cut off from the platforms where your peers are socializing, sharing, and connecting has costs. The researchers note:Heavy use has been associated with distress, while abstinence may cause missed connections.This is what we’ve been saying forever. These platforms aren’t just “distraction machines” or “attention hijackers” or whatever scary framing is popular this week. They’re where social life happens for a lot of young people. Cutting kids off entirely doesn’t return them to some idyllic pre-digital social existence. It cuts them off from their actual social world.Both sets of researchers make the same point: online experiences aren’t inherently harmless—hurtful messages, online pressures, and extreme content can have real effects. But blunt instruments like time-based restrictions or outright bans completely miss the target, and are unlikely to help those who need it most. The Australian authors recommend “promotion of balanced and purposeful digital engagement as part of a broader strategy.”That’s… actually sensible policy advice? Based on actual evidence?Maybe—just maybe—they should look at the actual research coming out of Australia and the UK instead.]]></content:encoded></item><item><title>Adobe Acrobat Now Lets You Edit Files Using Prompts, Generate Podcast Summaries</title><link>https://slashdot.org/story/26/01/21/198252/adobe-acrobat-now-lets-you-edit-files-using-prompts-generate-podcast-summaries?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Adobe has added a suite of AI-powered features to Acrobat that enable users to edit documents through natural language prompts, generate podcast-style audio summaries of their files, and create presentations by pulling content from multiple documents stored in a single workspace. 

The prompt-based editing supports 12 distinct actions: removing pages, text, comments, and images; finding and replacing words and phrases; and adding e-signatures and passwords. The presentation feature builds on Adobe Spaces, a collaborative file and notes collection the company launched last year. Users can point Acrobat's AI assistant at files in a Space and have it generate an editable pitch deck, then style it using Adobe Express themes and stock imagery. 

Shared files in Spaces now include AI-generated summaries that cite specific locations in the source document. Users can also choose from preset AI assistant personas -- "analyst," "entertainer," or "instructor" -- or create custom assistants using their own prompts.]]></content:encoded></item><item><title>Why “Intent-First” Design Could Change How Humans Work With Gen AI</title><link>https://hackernoon.com/why-intent-first-design-could-change-how-humans-work-with-gen-ai?source=rss</link><author>Microfrontend</author><category>tech</category><pubDate>Wed, 21 Jan 2026 20:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The generated websites, as illustrated in Figure 3, exhibit generally satisfactory visual appearances. These include contextually appropriate textual content, imagery, color schemes, layouts, and  \
functionalities. Those results align with our "intent-based" objective of only requiring users to express their intent and scaffolding Generative AI to deliver the final output, potentially reducing the communication costs between users and Generative AI systems. This task transition paradigms may motivate further exploration of intent-based interfaces, potentially extending to more complex tasks with interdependent components such as video generation.\
For example, we might envision an abstract-to-detailed task transition process for generating video advertisements that begins with sketches and thematic inputs, transitions to script writing, then proceeds to generate textual and visual descriptions of storyboards, followed by video generating end editing, and culminating in iterative video refinement. We aim to further investigate the potential of intent-based user interfaces in streamlining complex, interdependent workflows across various domains.\
Future work could focus on studies empirically validating the effectiveness of this task transition approach in more diverse and complex task environments. Additionally, research into optimizing the task transition process and enhancing the quality of inter-task communication may yield improvements in the overall performances.[1] John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan Adar, and Minsuk Chang. 2022. TaleBrush: Visual Sketching of Story Generation with Pretrained Language Models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts. ACM, New Orleans LA USA, 1–4. https://doi.org/10. 1145/3491101.3519873\
[2] Zijian Ding. 2024. Advancing GUI for Generative AI: Charting the Design Space of Human-AI Interactions through Task Creativity and Complexity. In Companion Proceedings of the 29th International Conference on Intelligent User Interfaces. ACM, Greenville SC USA, 140–143. https://doi.org/10.1145/3640544.3645241[3] Zijian Ding. 2024. Towards Intent-based User Interfaces: Charting the Design Space of Intent-AI Interactions Across Task Types. arXiv preprint arXiv:2404.18196 (2024).\
[4] Zijian Ding and Joel Chan. 2023. Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks. http://arxiv.org/abs/2303.06430 arXiv:2303.06430 [cs].[5] Zijian Ding and Joel Chan. 2024. Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis through Rapid Prototyping, Iteration and Curation. arXiv preprint arXiv:2402.08812 (2024).\
[6] Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault, and Alejandro Jaimes. 2023. Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation. http: //arxiv.org/abs/2310.10706 arXiv:2310.10706 [cs].\
[7] Zijian Ding, Arvind Srinivasan, Stephen Macneil, and Joel Chan. 2023. Fluid Transformers and Creative Analogies: Exploring Large Language Models’ Capacity for Augmenting Cross-Domain Analogical Creativity. In Proceedings of the 15th Conference on Creativity and Cognition (C&C ’23). Association for Computing Machinery, New York, NY, USA, 489–505. https://doi.org/10.1145/3591196.3593516[8] Jakob Nielsen. 2023. AI: First New UI Paradigm in 60 Years. Nielsen Norman Group 18, 06 (2023), 2023.\
[9] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. 2024. Design2Code: How Far Are We From Automating Front-End Engineering? http: //arxiv.org/abs/2403.03163 arXiv:2403.03163 [cs].\
[10] Jason Wu, Eldon Schoop, Alan Leung, Titus Barik, Jeffrey P. Bigham, and Jeffrey Nichols. 2024. UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback. http://arxiv.org/abs/2406.07739 arXiv:2406.07739 [cs].\
[11] Chen Zhu-Tian, Zeyu Xiong, Xiaoshuo Yao, and Elena Glassman. 2024. Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches. http://arxiv.org/abs/ 2405.03998 arXiv:2405.03998 [cs].]]></content:encoded></item><item><title>The Silent AI Breach: How Data Escapes in Fragments</title><link>https://hackernoon.com/the-silent-ai-breach-how-data-escapes-in-fragments?source=rss</link><author>Cyberhaven</author><category>tech</category><pubDate>Wed, 21 Jan 2026 19:55:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[GenAI isn’t stealing your data in one dramatic burst. It leaks fragments—copied into prompts, screenshots, exports, and fine-tuning datasets that move between endpoints, SaaS apps, and cloud storage. Legacy DLP sees some hops. DSPM sees some resting places. Neither sees the whole story.The only way to reliably track and stop AI-driven data exfiltration is to follow the data's  journey—its lineage—across endpoints, SaaS, and the cloud, then apply protection in real time. That’s the mindset behind Cyberhaven’s unified DSPM + DLP platform. The New Data Breach Doesn’t Look Like a BreachWhen people imagine an “AI incident,” they picture something cinematic: a rogue agent wiring the entire customer database into a model in one shot.That’s almost never how it happens.In the environments we see, AI‑related data loss looks more like this:A product manager pastes a few rows of roadmap data into a model for help writing a launch brief.A developer copies a code snippet with a proprietary algorithm into ChatGPT to debug a race condition.A finance analyst exports a slice of a board deck into a CSV to feed an internal LLM.Each action in isolation seems harmless— But over weeks and months, those fragments accumulate across different tools, identities, and locations.From an attacker’s point of view, you don’t need the  truth in one place. Enough fragments, stitched together, are often just as valuable as the original.Most organizations are still protecting data with a mental model that assumes:Data lives in well‑defined systems (databases, file shares, document repositories).“Exfiltration” is a discrete event (a big upload, a large export, a massive email).AI breaks both assumptions.1. Data is now fragmented by defaultWe no longer share a file; we share  of it. That was already true with SaaS. AI multiplies it:A confidential slide becomes: two paragraphs in an email, three bullets in a Jira ticket, and a paragraph pasted into an AI prompt.A source code file becomes: a function pasted into a chat, a generated patch in Git, and a screenshot in a Slack thread.By the time you notice something is wrong, the data has been chopped, transformed, translated, and blended into other content across dozens of systems. Our analysis of customer environments shows data moving continuously between the cloud and endpoints in ways that are impossible to understand if you only look at a single system or moment.2. Controls are still siloed by locationThe security stack mirrors this fragmentation: on endpoints and gateways focuses on data . focuses on data  in SaaS and cloud.New  tools focus solely on prompts and responses within specific models.Each one knows its domain well, but little about what happened  or  the event it observes. So you end up with:A DSPM alert that says: “This bucket contains sensitive data,” but not  or .A DLP alert that says: “Someone pasted confidential text into a browser,” but not where the text originated or .An AI usage report that says, “These apps are talking to LLMs,” but doesn't specify the underlying data they’re exposing.Individually, these are partial truths. Together, without context, they become noise.Long before “data lineage” became a slide on every security vendor’s pitch deck, we built a company around it.Cyberhaven’s founding team came out of EPFL and the DARPA Cyber Grand Challenge, where we built technology to track how data flowed through systems at the instruction level, not just the file level. That research evolved into a security platform that could reconstruct the entire  of a sensitive object—where it was born, how it changed, who touched it, and where it tried to leave the organization.We sometimes joke internally that we were “the original data lineage company” — we were shipping lineage‑based detection and response years before it was fashionable marketing language.At the time, this approach solved problems like:Finding insider threats hidden in millions of “normal” file operations.Understanding complex IP leaks where content had been copied, compressed, encrypted, renamed, and moved across multiple systems.We thought lineage was powerful then.In the AI era, it’s non‑negotiable. It is like trying to enable full self-driving without having driven round and round San Francisco, gathering the telemetry data.AI Made Lineage Mandatory, Not OptionalAI has accelerated two trends that were already underway: It continuously moves between endpoints, SaaS, and the cloud.Security is moving from point products to platforms. Customers are tired of stitching together DSPM, DLP, insider risk, and a separate AI tool.If you care about AI‑driven data exfiltration, you can’t afford to look only at:Static storage (DSPM alone), orNetwork egress (DLP alone), orAI prompts (AI tooling alone).You need to understand how knowledge moves: how an idea in a design file becomes a bullet in a product document, a paragraph in a Slack thread, and a prompt to an external model.That’s the whole reason we built Cyberhaven as a unified AI & data security platform that combines DSPM and DLP on top of a single data lineage foundation. It lets security teams see both:Where data  (inventory, posture, misconfigurations), andHow data  (copy/paste, exports, uploads, AI prompts, emails, Git pushes, and more).Once you have that complete picture, AI exfiltration stops being mysterious. It looks like any other sequence of events, just faster and more repetitive.Principles for Actually Stopping AI-Driven Data ExfiltrationIf I were starting a greenfield security program today, with AI in scope from day zero, here are the principles I’d insist on.1. Unify data at rest and data in motionYou can’t secure what you only see. You can’t secure what you only see part of. Data is sitting in the cloud and SaaS.DLP tells you  data is moving, especially at endpoints and egress points.Together, with lineage, you get the full story: this model training dataset in object storage came from an export from this SaaS app, which originated in this internal HR system, and was enriched by this prompt flow to an external LLM.That’s the level of context you need to decide whether to block, quarantine, or allow, especially when AI is involved.2. Treat identity, behavior, and content as a single signalWhenever I review a serious incident, there are three questions I want answered:What exactly was the data? (Regulated data, IP, source code, M&A docs?)Who was the human or service account behind the action? (Role, history, typical behavior.)How did this sequence of events differ from “normal” for that identity and that data?Legacy tools usually answer only one of those in isolation:Content scanners know  but not .Identity systems know  but not  they did with data.UEBA systems know  but not .Lineage‑driven systems can correlate all three in real time, which is the only way to reliably find the handful of truly risky actions in the noise of millions of “normal” events.3. Assume policies won’t keep upWriting perfect AI policies is a losing game.People will always find new tools, plugins, side channels, and workflows. If your protection depends on static rules that anticipate every vector, you’ll always be behind.What works better in practice is:Broad, simple guardrails (“don’t move data with these characteristics to destinations in these classes”) combined withAn AI‑assisted detection layer that uses lineage and semantic understanding to surface suspicious patterns you didn’t explicitly write a rule for.We’re already seeing this with autonomous analysts that investigate lineage graphs and user behavior to propose or enforce controls without requiring a human to anticipate every scenario.4. Close the loop from insight to actionSeeing the problem isn’t enough. Seeing the problem isn’t enough. One of the biggest complaints we hear about stand-alone DSPM tools is that they generate lots of “insight” but no direct enforcement; teams are left opening tickets and chasing owners by hand. Prioritize where to scan and investigate based on live DLP telemetry (follow where sensitive data ).Offer one‑click remediation paths: revoke access, tighten sharing, quarantine misconfigured stores, or block risky exfiltration attempts in real time.Feed every enforcement decision back into the lineage and detection models so the system gets smarter over time.Without that tight loop, AI-driven leakage becomes another line item on an overcrowded risk register.Why This Matters Now, Not “Someday”There’s a reason AI has suddenly made data security a board‑level topic again.Employees are using AI tools faster than governance can keep up.New regulations and customer expectations are raising the stakes for data misuse.Attackers are experimenting with AI‑assisted reconnaissance and exfiltration.At the same time, security teams are consolidating tools. They don’t want separate products for DLP, DSPM, insider risk, and AI security. They want one platform that can see and control data everywhere—at rest, in motion, and in use—with lineage as the connective tissue.That’s the platform we’ve been building at Cyberhaven, starting with our early work on data lineage and evolving into a unified AI & data security platform that combines DLP, DSPM, insider risk, and AI security in a single system.Want to See What This Looks Like in the Real World?On February 3 at 11:00 AM PT, we’re hosting a live session where we’ll:Show the first public demo of our unified AI & data security platform and how it tracks data fragments across endpoints, SaaS, cloud, and AI tools in real time.Walk through how security teams get “X‑ray vision” into data usage, so they can isolate the risky handful of actions hidden in millions of normal events — and stop them before they turn into incidents.Share candid stories from security leaders on where legacy DLP and stand‑alone DSPM have failed them in the AI era, and how a lineage‑first approach changes the game.Talk about where we think , insider risk, AI security, and DSPM are headed next — and why we believe the future belongs to platforms that were built on data lineage from day one, not retrofitted after the fact.If you’re wrestling with AI adoption, shadow AI tools, or just a growing sense that your current stack is seeing only the surface of what’s happening to your data, I’d love for you to join us and ask hard questions.AI is already exfiltrating your data in fragments. The real question is whether you can see the story those fragments are telling, and whether you can act in time to change the ending.]]></content:encoded></item><item><title>Mesa 26.0-rc1 Released With RADV Improvements Leading The Way Along With Intel &amp; NVK</title><link>https://www.phoronix.com/news/Mesa-26.0-rc1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 21 Jan 2026 19:49:10 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Eric Engestrom just released Mesa 26.0-rc1 with the code for this quarter's Mesa feature release now branched and under a feature freeze leading up to the stable release in February...]]></content:encoded></item><item><title>The Gold Plating of American Water</title><link>https://news.slashdot.org/story/26/01/21/1922232/the-gold-plating-of-american-water?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 19:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The price of water and sewer services for American households has more than doubled since the early 1980s after adjusting for inflation, even though per-capita water use has actually decreased over that period. Households in large cities now spend about $1,300 a year on water and sewer charges, approaching the roughly $1,600 they spend on electricity. The main driver is federal regulation. 

Since the Clean Water Act of 1972 and the Safe Drinking Water Act of 1974, the U.S. has spent approximately $5 trillion in contemporary dollars fighting water pollution -- about 0.8% of annual GDP across that period. The EPA itself admits that surface water regulations are the one category of environmental rules where estimated costs exceed estimated benefits. 

New York City was required to build a filtration plant to address two minor parasites in water from its Croton aqueduct. The project took a decade longer than expected and cost $3.2 billion, more than double the original estimate. After the plant opened in 2015, the city's Commissioner of Environmental Protection noted that the water would basically be "the same" to the public. Jefferson County, Alabama, meanwhile, descended into what was then the largest municipal bankruptcy in U.S. history in 2011 after EPA-mandated sewer upgrades pushed its debt from $300 million to over $3 billion.]]></content:encoded></item><item><title>An End-to-End System for Generating Frontends from Sketches with LLMs</title><link>https://hackernoon.com/an-end-to-end-system-for-generating-frontends-from-sketches-with-llms?source=rss</link><author>Microfrontend</author><category>tech</category><pubDate>Wed, 21 Jan 2026 19:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We introduce Frontend Diffusion, an end-to-end LLM-powered high-quality frontend code generation tool, spanning from sketching canvas to website previews. As outlined in the introduction, the frontend generation task progresses through three stages: sketching, writing, and coding Our system utilizes the Claude 3.5 Sonnet language model (Sonnet)1 for all text and code generation.\
While Claude represents one of the most advanced language models as of July 2024, we anticipate rapid developments in Generative AI. Therefore, the task transition techniques described herein are designed to be model-agnostic, ensuring their applicability to future, more advanced Generative AI models.2.1 Sketching: Visual Layout Design and Theme InputThe system’s initial phase comprises a graphical user interface with two key components: a canvas panel for visual representation of the envisioned website layout, and a prompt panel for textual descriptions of the website theme. Upon completion of the user’s sketch and theme input, the user can activate the code generation process via "Generate" button.\
The system then converts the sketch into SVG format, followed by a subsequent transformation into JPG format. This two-step conversion process was implemented based on empirical evidence from our tests, showing that language models exhibit better performance when processing images in JPG format compared to images in SVG format.2.2 Writing: Product Requirements Document GenerationThis phase transforms the user’s visual and textual inputs into a structured document, referred to as the Product Requirements Document (PRD), which serves as a blueprint for the website’s development process. The PRD generation process leverages Sonnet. To enhance the visual appearance of the generated websites, the system integrates the Pexels API2 for image retrieval.\
The language model is specifically prompted to include image terms and size descriptions (e.g., [school(large)]). These descriptors are subsequently utilized to query the Pexels API, which returns relevant image URLs for incorporation into the PRD.2.3 Coding: Website Generation and Iterative RefinementThe coding phase of the system consists of two primary components: (1) Initial code generation: the system utilizes the generated PRD and the original user prompt as inputs for code generation, employing Sonnet to produce the initial website code; (2) Iterative refinement: the system implements an iterative refinement process to automatically enhance the generated website with richer functionality and reduced flaws.\
This process involves analyzing the initial code to generate optimization suggestions, merging these suggestions with the original theme, and utilizing the enhanced theme along with the previously generated PRD to regenerate the code. The system executes this iterative refinement process multiple times (by default, n=4). Users can navigate between iterations by selecting preview thumbnails displayed at the interface’s bottom, and can access or copy the generated code for each version.]]></content:encoded></item><item><title>Evil ICE Fucks Ate Lunch At A Mexican Restaurant Just So They Could Come Back And Detain The People Who Fed Them</title><link>https://www.techdirt.com/2026/01/21/evil-ice-fucks-ate-lunch-at-a-mexican-restaurant-just-so-they-could-come-back-and-detain-the-people-who-fed-them/</link><author>Tim Cushing</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:55:20 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Do you still want to cling to this pretense, Trump supporters? Do you still want to pretend ICE efforts are targeting “the worst of the worst?” Are you just going to sit there and mumble some incomprehensible stuff about “respecting the laws?”Go ahead. Do it, you cowards. This is  what you voted for, even if it now makes you a bit queasy. Just sit there and soak in it. You  who you support, even if you never thought it would go this far.“Worst of the worst,” Trump’s parrot repeat on blast. “This one time we caught a guy who did actual crimes,” say spokespeople defending whatever the latest hideous violation of the social contract (if not actual constitutional rights) a federal agent has performed. “Targeted investigation/stop” say the enablers, even when it’s just officers turning white nationalism into Official Government Policy. “Brown people need to be gone” is the end game. Full stop.Here’s where we’re at in Minnesota, where ICE officers are being shamed into retreat on the regular, punctuated by the occasional revenge killing of mouthy US citizens. Federal agents detained three workers from a family-owned Mexican restaurant in Willmar, Minn., on Jan. 15, hours after four agents ate lunch there.Does that seem innocuous? Does this seem like some plausible deniability is in play here? Well, disabuse yourself of those notions. This is how it went down.The arrest happened around 8:30 p.m. near a Lutheran church and Willmar Middle School as agents followed the workers after they closed up for the night. A handful of bystanders blew whistles and shouted at agents as they detained the people. “Would your mama be proud of you right now?” one of the bystanders asked.Nice. Is this what you want from a presidential administration? Or would you rather complain ICE officers have been treated unfairly if people refuse to feed or house them, knowing full well that doing either of these things will turn their employees into targets. An eyewitness who declined to give a name for fear of retribution, told the Minnesota Star Tribune that four ICE agents sat in a booth for a meal at El Tapatio restaurant a little before 3 p.m. Staff at the restaurant were frightened, said the eyewitness, who shared pictures from the restaurant as well as video of the arrest.I’m not saying ICE officers shouldn’t be able to eat at ethnic restaurants. I am, however, saying that they definitely  because everyone is going to think the officers are there for  but the food. And I do believe any minority business owner should be able to refuse service to ICE officers who wander in under the pretense of buying a meal. The end result is going to be the same whether or not you decide to engage with this pretense. You’re getting raided either way. May as well deny them the meal.El Tapatio Mexican Restaurant closed after WCCO confirmed agents visited the spot for lunch and later returned, detaining its owners and a dishwasher nearby after they had closed early due to the federal law enforcement’s previous appearance.And here’s the DHS statement, which pretends ICE officers didn’t eat a meal at a restaurant and then return a few hours later to detain employees when they left the building: “On January 14, ICE officers conducted surveillance of a target, an illegal alien from Mexico. Officers observed that the target’s vehicle was outside of a local business and positively identified him as the target while inside the business. Following the positive identification of the target, officers then conducted a vehicle stop later in the day and apprehended the target and two additional illegal aliens who were in the car, including one who had a final order of removal from an immigration judge.”Nope. I don’t care what the ICE apologists will say about this. These narratives have places where they overlap but it’s impossible to believe this went down exactly like the government said it did. These officers picked out an ethnic restaurant, were served by an intimidated staff, and then hung around to catch any stragglers leaving the business that previously had graciously served them, despite the threat they posed.Abolish ICE. It’s no longer just a catchy phrase to shout during protests. It’s an imperative. If we don’t stop it now, it will only become even worse and even more difficult to remove. Treat ICE like the tumor it is. Pretending its MRSA gives it more power than it should ever be allowed to have. ]]></content:encoded></item><item><title>Daily Deal: PiCar-X Smart Video Robot Car Kit for Raspberry Pi 4</title><link>https://www.techdirt.com/2026/01/21/daily-deal-picar-x-smart-video-robot-car-kit-for-raspberry-pi-4-3/</link><author>Daily Deal</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:50:20 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Dive into the world of robotics, programming, and electronics with the PiCar-X, an engaging and versatile smart car designed for learners from elementary school to advanced hobbyists. Combining powerful features, exceptional quality, and a cool design, this robot car kit delivers an engaging learning experience in robotics, AI, and programming. Beyond being an educational tool, its powerful Robot Hat provides abundant resources for you to design and bring to life your projects. Plus, it comes enriched with 15 comprehensive video tutorials, guiding you through each step of discovery and innovation. Embark on a journey of discovery and creativity with Picar-X, where young learners become budding innovators! Without the Raspberry Pi board, it’s on sale for $80. With a RPi Zero 2W + 32GB, it’s on sale for $110. With a RPi 4 2GB + 32GB, it’s on sale for $141.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>AI Company Eightfold Sued For Helping Companies Secretly Score Job Seekers</title><link>https://yro.slashdot.org/story/26/01/21/1841214/ai-company-eightfold-sued-for-helping-companies-secretly-score-job-seekers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:44:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Eightfold AI, a venture capital-backed AI hiring platform used by Microsoft, PayPal and many other Fortune 500 companies, is being sued in California for allegedly compiling reports used to screen job applicants without their knowledge. From a report: The lawsuit, filed on Tuesday accusing Eightfold of violating the Fair Credit Reporting Act shows how consumer advocates are seeking to apply existing law to AI systems capable of drawing inferences about individuals based on vast amounts of data. 

Santa Clara, California-based Eightfold provides tools that promise to speed up the hiring process by assessing job applicants and predicting whether they would be a good fit for a job using massive amounts of data from online resumes and job listings. But candidates who apply for jobs at companies that use those tools are not given notice and a chance to dispute errors, job applicants Erin Kistler and Sruti Bhaumik allege in their proposed class action. Because of that, they claim Eightfold violated the FCRA and a California law that gives consumers the right to view and challenge credit reports used in lending and hiring.]]></content:encoded></item><item><title>Blue Origin’s satellite internet network TeraWave will move data at 6 Tbps</title><link>https://techcrunch.com/2026/01/21/blue-origins-satellite-internet-network-terawave-will-move-data-at-6tbps/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:36:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The network will be designed for enterprise, data center, and government customers and could offer an alternative to SpaceX's Starlink service.]]></content:encoded></item><item><title>Zipline charts drone delivery expansion with $600M in new funding</title><link>https://techcrunch.com/2026/01/21/zipline-charts-drone-delivery-expansion-with-600m-in-new-funding/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:33:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[That geographic expansion in the United States has fueled Zipline’s delivery numbers. In 2024, the company completed 1 million drone deliveries to customers; this week, Zipline said it had surpassed 2 million deliveries. ]]></content:encoded></item><item><title>Four Key Trends in Theoretical Machine Learning (2026)</title><link>https://hackernoon.com/four-key-trends-in-theoretical-machine-learning-2026?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:15:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[2 Some recent trends in theoretical MLOur proposal on ML via swarms on manifolds builds upon the combination of four research directions which recently had a great impact on the field. Control-theoretic ML investigates new architectures of neural networks (NN’s) for Deep Learning (DL) by encoding maps into continuous-time dynamical systems, based on mathematical theories of ODE’s and optimal control. ML through probabilistic modeling and inference aims to encode uncertainties using probability measures. Geometric ML explores intrinsic geometric features of the data, embeds the instances into Riemannian manifolds and infers the curvature and symmetries hidden in data sets. Finally, Physics Informed ML leverages laws of Physics (such as conservation laws, time-space symmetries, MaxEnt principle) to design efficient and transparent ML algorithms.\
The literature on each of these directions is vast and constantly growing. We do not even try to provide a comprehensive or representative (in any sense) list of references.(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Copyright Should Not Enable Monopoly</title><link>https://www.eff.org/deeplinks/2026/01/copyright-should-not-enable-monopoly</link><author>Katharine Trendacosta</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/copyrightchaser.gif" length="" type=""/><pubDate>Wed, 21 Jan 2026 18:10:45 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>BingX Lists SKR, The Native Token of Solana Mobile</title><link>https://hackernoon.com/bingx-lists-skr-the-native-token-of-solana-mobile?source=rss</link><author>Blockman PR and Marketing</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:06:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[PANAMA CITY, January 21, 2026 –, a leading crypto exchange and Web3-AI company, today announced the listing of SKR, the native asset of the Solana Mobile ecosystem, opening up more opportunities for traders to participate in Solana Mobile's platform governance and network development.To celebrate the listing, BingX is launching a from January 21 to January 28, with new user special rewards and trading missions for users to participate and share a total prize pool of $100,000 in SKR, along with SKR Fixed Term Wealth benefits and extra Xpool points.Following the launch of Solana Mobile's second-generation Web3-native smartphone, SKR powers its ecosystem for governance and incentives. The token powers governance and incentivization within the platform, distributing control while fostering collaboration among builders, users, and hardware partners. By staking SKR to Guardians, users can actively participate in platform governance, from verifying device authenticity to coordinating dApp reviews and enforcing community standards. Additionally, stakers are rewarded for helping to secure the network, further strengthening the ecosystem.The listing of SKR represents another step in BingX’s commitment to expanding its spot trading offerings and connecting users with cutting-edge projects. As one of the most widely anticipated tokens backed by an active and growing community, SKR underscores BingX’s dedication to providing access to emerging opportunities in the Web3 and blockchain space.Founded in 2018, BingX is a leading crypto exchange and Web3-AI company, serving over 40 million users worldwide. Ranked among the top five global crypto derivatives exchanges and a pioneer of crypto copy trading, BingX addresses the evolving needs of users across all experience levels. Powered by a comprehensive suite of AI-driven products and services, including futures, spot, copy trading, and TradFi offerings, BingX empowers users with innovative tools designed to enhance performance, confidence, and efficiency.BingX has been the principal partner of Chelsea FC since 2024, and became the first official crypto exchange partner of Scuderia Ferrari HP in 2026.For media inquiries, please contact: For more information, please visit::::tip
This story was published as a press release by Blockman under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Ubisoft Cancels Six Games, Slashes Guidance in Restructuring</title><link>https://games.slashdot.org/story/26/01/21/184240/ubisoft-cancels-six-games-slashes-guidance-in-restructuring?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Ubisoft is canceling game projects, shutting down studios and cutting its guidance as the Assassin's Creed maker restructures its business into five units. From a report: The French gaming firm expects earnings before interest and tax to be a loss of $1.2 billion the fiscal year 2025-2026 as a result of the restructuring, driven by a one-off writedown of about $761 million, the company said in a statement on Wednesday. 

Ubisoft also expects net bookings of around $1.76 billion for the year, with a $386 million gross margin reduction compared to previous guidance, it said. Six games, including a remake of Prince of Persia The Sands of Time, have been discontinued and seven other unidentified games are delayed, the company said. The measures are part of a broader plan to streamline operations, including closing studios in Stockholm and Halifax, Canada. Ubisoft said it will have cut at least $117 million in fixed costs compared to the latest financial year by March, a year ahead of target, and has set a goal to slash an additional $234 million over the next two years.]]></content:encoded></item><item><title>OpenEvidence hits $12B valuation, with new round led by Thrive, DST</title><link>https://techcrunch.com/2026/01/21/openevidence-hits-12b-valuation-with-new-round-led-by-thrive-dst/</link><author>Julie Bort</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:01:11 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The medical info database has doubled in valuation since last raise in October, despite encroachment from model makers.]]></content:encoded></item><item><title>Frontend Diffusion Shows What Intent-Based UI Design Looks Like in Practice</title><link>https://hackernoon.com/frontend-diffusion-shows-what-intent-based-ui-design-looks-like-in-practice?source=rss</link><author>Microfrontend</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:00:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The emergence of Generative AI is catalyzing a paradigm shift in user interfaces from command-based to intent-based outcome specification. In this paper, we explore abstract-to-detailed task transitions in the context of frontend code generation as a step towards intent-based user interfaces, aiming to bridge the gap between abstract user intentions and concrete implementations. We introduce Frontend Diffusion, an end-to-end LLM-powered tool that generates high-quality websites from user sketches.\
The system employs a three-stage task transition process: sketching, writing, and coding. We demonstrate the potential of task transitions to reduce human intervention and communication costs in complex tasks. Our work also opens avenues for exploring similar approaches in other domains, potentially extending to more complex, interdependent tasks such as video production.The development of Generative AI, particularly the capabilities of Large Language Models (LLMs) in interpreting and executing natural language, may be viewed as heralding the first new user interface paradigm shift in 60 years [8]. This shift moves from command-based interactions, typified by command line interfaces and graphical user interfaces, to intent-based outcome specification [8]. This emerging intent-based paradigm potentially enables users to communicate their intentions to machines without necessarily translating them into machine-comprehensible commands, whether through programming languages or graphical buttons.\
This shift may foster interfaces that support more abstract human expressions, especially for command-intensive tasks such as coding [2, 3]. Currently, the interfaces for command-intensive tasks continue to necessitate substantial human intervention, where individuals typically specify incremental steps while AI generates corresponding code, akin to agile programming [11]. However, ongoing advancements in Generative AI capabilities suggest the potential for developing a framework that may bridge the gap between intentlevel expression and command-level implementation, potentially enhancing output quality while reducing the need for extensive human intervention.\
Previous research has demonstrated that Generative AI, such as Large Language Models (LLMs), can complete fixed-scope content curation tasks based on human intent without further intervention or intent iteration. For example, LLMs have shown promise in text summarization tasks [6]. However, Generative AI require greater human intervention for tasks involving increasing amounts of information [4, 7]. It motivates us to develop more effective scaffolding paradigm for Generative AI to respond to human intent and complete tasks in an agent-like manner.\
Recent research has indicated the feasibility of bridging intent expression in abstract tasks to concrete implementation at a more granular level. Examples include the transition from sketching to writing [1] and from design to data analysis [5]. Building upon these findings, we propose exploring more extensive intent-tocommand transitions, such as progressing from sketching to writing (planning) and ultimately to coding (see Figure 2). Our choice of website frontend generation as a user interface coding task [10] is motivated by its similarity to sketching. In both cases, the code or sketch serves as a representation of visual elements [9].]]></content:encoded></item><item><title>Geometric Deep Learning: Swarming Dynamics on Lie Groups and Spheres</title><link>https://hackernoon.com/geometric-deep-learning-swarming-dynamics-on-lie-groups-and-spheres?source=rss</link><author>Hyperbole</author><category>tech</category><pubDate>Wed, 21 Jan 2026 18:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We propose the idea of using Kuramoto models (including their higher-dimensional generalizations) for machine learning over non-Euclidean data sets. These models are systems of matrix ODE’s describing collective motions (swarming dynamics) of abstract particles (generalized oscillators) on spheres, homogeneous spaces and Lie groups. Such models have been extensively studied from the beginning of XXI century both in statistical physics and control theory. They provide a suitable framework for encoding maps between various manifolds and are capable of learning over spherical and hyperbolic geometries. In addition, they can learn coupled actions of transformation groups (such as special orthogonal, unitary and Lorentz groups). Furthermore, we overview families of probability distributions that provide appropriate statistical models for probabilistic modeling and inference in Geometric Deep Learning. We argue in favor of using statistical models which arise in different Kuramoto models in the continuum limit of particles. The most convenient families of probability distributions are those which are invariant with respect to actions of certain symmetry groups.Machine Learning (ML) is, to a great extent, a science of inferring models and patterns from data. From that point of view, its core objective consists in learning optimal (according to a certain criterion) mappings between spaces. For several decades these mappings have been dominantly encoded using artificial neural networks with different topologies [1]. The spaces have almost always been assumed Euclidean or equipped with some flat metric. The data have been represented by points in Euclidean spaces or in finite sets.\
An enormous progress in ML and Data Science in XXI century led to the growing understanding that a great deal (possibly, majority) of data sets have inherent non-Euclidean geometries. This fact has been mostly neglected in ML until very recently. Only the last decade brought systematic research efforts focused on geometric-sensitive architectures of neural networks (NN’s).\
In parallel, traditional ways of designing artificial NN’s are being reexamined and enriched by new ideas. Diversity of applications and conceptual complexity of ML problems motivated investigations of new architectures. Over the centuries mathematicians elaborated various ways of encoding maps between Euclidean spaces or Riemannian manifolds. The corresponding theories have been established before the advent of ML, and now provide a solid theoretical background for its future developments. Following an explosive expansion of ML applications and practices, there is a huge backlog of theoretical work to be done. Mathematical foundations of ML are being actively reconsidered and expanded. Certain fields of mathematics that have been almost invisible in ML until very recently are now actively exploited with a great potential for future applications. The examples include Riemannian Geometry, Game Theory and Lie Group Theory - to name just a few.\
Systematic approaches in ML must be based on well established theories and well understood models. The choice of adequate models and appropriate data representations appears to be the key issue. An appropriate choice greatly reduces the dimension (number of parameters), increases the efficiency of algorithms and, equally important, improves their transparency.\
The main goal of the present paper is to point out a broad class of models which constitute a powerful theoretical framework for encoding geometric data. These models describing collective motions of interacting particles have been studied in Science for almost half of a century from various points of view. In physics of complex systems they are known as Kuramoto models [2] (including generalizations to higher-dimensional manifolds [3, 4]) and Viscek models [5]. In systems theory they are said to be (anti-)consensus algorithms on manifolds [6, 7]. Finally, in Engineering they are sometimes referred to as swarms on manifolds [8, 9]. All these models fit into the unifying mathematical framework that we refer to as systems of geometric Riccati ODE’s, as will be explained in sections 3 and 4.\
Our exposition will be focused on the following questions:Which kinds of mappings can be encoded by collective motions of Kuramoto oscillators/swarms on manifolds?Which symmetries/patterns can be learned using these dynamics?Which statistical models are associated with these dynamics and how can they be used in statistical ML over manifolds?Which problems can be efficiently solved using such models?How these models can be trained?\
Our proposal on using swarms/Kuramoto oscillators in ML is inspired by some recent developments in theoretical ML which will be mentioned in Section 2. Section 3 is devoted to classical Kuramoto models (i.e. models describing collective motions of the classical phase oscillators) and their potential applications to learning coupled actions of transformation groups, as well as data on circles, tori and hyperbolic multi-discs. Section 4 contains an overview of (generalized) Kuramoto models that describe collective motions on spheres, Lie groups and other manifolds. In Section 5 we present families of probability measures over Riemannian manifolds which provide appropriate statistical models for probabilistic ML algorithms over non-Euclidean data sets. Some of these families are generated by the corresponding swarming dynamics. Connections with directional statistics will be particularly emphasized. In Section 6 we clarify how swarms can be used for supervised, unsupervised and reinforcement learning over Riemannian manifolds. In Section 7 we analyze some illustrative geometric ML problems in low dimensions, thus supporting our main points. Finally, Section 8 contains some concluding remarks and an outlook for the future research efforts.:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.(1) Vladimir Jacimovic, Faculty of Natural Sciences and Mathematics, University of Montenegro Cetinjski put bb., 81000 Podgorica Montenegro (vladimirj@ucg.ac.me).]]></content:encoded></item><item><title>BingX TradFi 24-Hour Trading Volume Surpasses $1 Billion</title><link>https://hackernoon.com/bingx-tradfi-24-hour-trading-volume-surpasses-$1-billion?source=rss</link><author>Blockman PR and Marketing</author><category>tech</category><pubDate>Wed, 21 Jan 2026 17:58:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[PANAMA CITY, January 20, 2026 –, a leading crypto exchange and Web3-AI company, today announced a remarkable milestone for its TradFi offerings, achieving a 24-hour trading volume exceeding $1 billion. Among this total, BingX TradFi Gold contributed over $500 million, showcasing strong user interest and active engagement.Since launching, an integrated feature that enables trading across a broad range of real-world financial assets, the platform has seen strong adoption. Traders' response highlights the growing appeal of BingX's diversified offering, spanning commodities, forex, stocks, and indices. TradFi Copy Trading has also accelerated, with a single-day peak of $51.84 million in 15 days."As the demand for TradFi continues growing, we remain at the forefront of delivering robust products and services that adapt to our users' evolving needs.", Chief Product Officer at BingX, commented. "Our expanded suite of offerings provides traders with greater choice and broader market access, unlocking new opportunities in a dynamic environment. This achievement in TradFi trading volume is a testament to BingX’s strong capability and the trust our users place in us. "Founded in 2018, BingX is a leading crypto exchange and Web3-AI company, serving over 40 million users worldwide. Ranked among the top five global crypto derivatives exchanges and a pioneer of crypto copy trading, BingX addresses the evolving needs of users across all experience levels. Powered by a comprehensive suite of AI-driven products and services, including futures, spot, copy trading, and TradFi offerings, BingX empowers users with innovative tools designed to enhance performance, confidence, and efficiency.BingX has been the principal partner of Chelsea FC since 2024, and became the first official crypto exchange partner of Scuderia Ferrari HP in 2026.For media inquiries, please contact: For more information, please visit::::tip
This story was published as a press release by Blockman under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>PyTorch 2.10 Released With More Improvements For AMD ROCm &amp; Intel GPUs</title><link>https://www.phoronix.com/news/PyTorch-2.10-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 21 Jan 2026 17:49:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[PyTorch 2.10 is out today as the latest feature update to this widely-used deep learning library. The new PyTorch release continues improving support for Intel GPUs as well as for the AMD ROCm compute stack along with still driving more enhancements for NVIDIA CUDA...]]></content:encoded></item><item><title>Podcast: Here’s What Palantir Is Really Building</title><link>https://www.404media.co/podcast-heres-what-palantir-is-really-building/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/01/palantir-pod.png" length="" type=""/><pubDate>Wed, 21 Jan 2026 17:39:08 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[We start this week with Joseph’s article about ELITE, a tool Palantir is working on for ICE. After the break, Emanuel tells us how AI influencers are making fake sex tape-style photos with celebrities, who can’t be best pleased about it. In the subscribers-only section, Matthew breaks down Comic-Con’s ban of AI art.Listen to the weekly podcast on , or YouTube. Become a paid subscriber for access to this episode's bonus content and to power our journalism. If you become a paid subscriber, check your inbox for an email from our podcast host Transistor for a link to the subscribers-only version! You can also add that subscribers feed to your podcast app of choice and never miss an episode that way. The email should also contain the subscribers-only unlisted YouTube link for the extended video version too. It will also be in the show notes in your podcast player. ]]></content:encoded></item><item><title>Rand Paul Only Wants Google To Be The Arbiter Of Truth When The Videos Are About Him</title><link>https://www.techdirt.com/2026/01/21/rand-paul-only-wants-google-to-be-the-arbiter-of-truth-when-the-videos-are-about-him/</link><author>Mike Masnick</author><category>tech</category><pubDate>Wed, 21 Jan 2026 17:29:51 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Just a year and a half ago, Senator Rand Paul sponsored a bill that would make it illegal for federal government employees to ask internet companies to remove any speech. Now, in a NY Post op-ed, Paul proudly announces that he did exactly that—formally contacting Google executives to demand they remove a video he didn’t like.The video apparently (falsely) claims Paul took money from Nicolas Maduro, the former Venezuelan President the US recently kidnapped. And Paul is furious that YouTube wouldn’t take it down for him.But the straw that broke the camel’s back came this week when I notified Google executives that they were hosting a video of a woman posing as a newscaster posing in a fake news studio explaining that “Rand Paul is taking money from the Maduro regime.”I’ve formally notified Google that this video is unsupported by facts, defames me, harasses me and now endangers my life.Google responded that they don’t investigate the truth of accusations . . . and refused to take down the video.Let’s pause here. Senator Paul—a sitting U.S. Senator—”formally notified” Google executives that they needed to remove content. Under  proposed legislation, that would be illegal. His bill was explicitly designed to prevent government officials from pressuring platforms about speech. And yet here he is, doing exactly that.This is also notably closer to actual government jawboning than most of what the Biden administration was accused of in the  case—where the Supreme Court found no First Amendment violation because platforms felt free to say no. Paul, a Senator with legislative power over these companies, is “formally notifying” them of what he wants removed, and is now saying that Google’s refusal to do so means they should lose Section 230 protection. Remember, the “smoking gun” in the Murthy case was supposedly Biden officials (and Biden himself) threatening to remove Section 230 if the tech platforms didn’t remove content they didn’t like.Rand Paul was furious about that and his bill was supposedly in direct response to the Murthy ruling, in which he wanted to make it clear that (1) no government official should ever demand content be taken down and (2) threatening to pass legislation to punish companies for their refusal to moderate content would also violate the law.And here he’s doing both.But it gets worse. Buried in the third-to-last paragraph of Paul’s op-ed is this remarkable admission:Though Google refused to remove the defamatory content, the individual who posted the video finally took down the video under threat of legal penalty.Wait. So the system worked exactly as designed? Paul threatened legal action against the person who actually created the content, and they took it down? That’s… that’s the whole point of Section 230. Liability attaches to the speaker, not the host. The creator is responsible. And when threatened with actual legal consequences, they removed the video.So what, exactly, is Paul complaining about?!? He got the outcome he wanted through the mechanism that Section 230 preserved for him: the ability to bring legal action against the speaker. But instead of acknowledging that the law worked, he’s using this as his justification for destroying it.Paul is a public figure. He has access to pretty much all the media he wants. If he wanted to use the famous “marketplace of ideas” he so frequently invokes to debunk a nonsense lie about him and Maduro, he was free to do that. If the video was actually defamatory, he could sue the creator—which he apparently threatened to do, and it worked! Instead, he wants to tear down the entire legal framework because YouTube wouldn’t do his bidding, even though the video was already taken down.The Arbiter of Truth HypocrisyHere’s where Paul’s position becomes truly incoherent.I asked one of Google’s executives what happens to the small town mayor whose enemies maliciously and without evidence, post that he is a pedophile on YouTube?. Would that be OK?The executive responded that YouTube does not monitor their content for truth. But how would that small town mayor ever get his or her reputation back?Just a few years ago, Rand Paul was apoplectic that YouTube tried to determine whether content—specifically about COVID-19—was true or not. He thought it was terrible that YouTube would dare to be the arbiter of truth, and he whined about it at length.Now he’s demanding they be the arbiter of truth and remove one video because  says it’s false.Paul even acknowledges this contradiction in his own op-ed, apparently without realizing it:Interestingly, Google says it doesn’t assess the truth of the content it hosts, but throughout the pandemic they removed content that they perceived as untrue, such as skepticism toward vaccines, allegations that the pandemic originated in a Wuhan lab, and my assertion that cloth masks don’t prevent transmission.Yes. And you screamed bloody murder about it. You insisted they should  do that. You built your entire position around the idea that platforms shouldn’t be deciding what’s true. And, with the re-election of Donald Trump, the big tech platforms all bent the knee and said they’d stop being arbiters of truth (even as it was legal for them to do so).And so they stopped. And now you’re furious that they won’t make an exception for you.Doesn’t that seem just a bit fucking hypocritical and entitled?The “It’s Their Property” ProblemPaul’s real complaint—buried under all the high-minded rhetoric about defamation—is that Google makes its own decisions:So, Google and YouTube not only choose to moderate speech they don’t like, but they also will remove speeches from the Senate floor despite such speeches being specifically protected by the Constitution.Google’s defense of speech appears to be limited to defense of speech they agree with.Yeah, dude. That’s how private property works. They get to decide what they host and what they don’t. That’s how it works. It’s also protected by  First Amendment rights. Compelled hosting or not hosting of speech you agree or disagree with is not a remedy available to you, Senator.Part of the liability protection granted internet platforms, section 230(c)(2), specifically allows companies the take down “harassing” content. This gives the companies wide leeway to take down defamatory content. Thus far, the companies have chosen to spend considerable time and money to take down content they politically disagree with yet leave content that is quite obviously defamatory. So Google does not have a blanket policy of refraining to evaluate truth. Google chooses to evaluate what it believes to be true when it is convenient and consistent with its own particular biases.He says this as if it’s controversial. It’s not. It’s exactly how editorial discretion works. The company gets to make their own editorial decisions. You don’t have to like those decisions. But demanding they make different ones, and threatening to strip their legal protections if they don’t, is a government official using state power to coerce speech decisions.You know, the thing Paul claimed to be against.I think Google is, or should be, liable for hosting this defamatory video that accuses me of treason, at least from the point in time when Google was made aware of the defamation and danger.Again: you already threatened the creator, and they took it down. The remedy worked. You used it successfully.And if Paul’s standard is “Google becomes liable once made aware,” then anyone who wants content removed will just  it’s defamatory and dangerous. How is this different from the COVID videos Paul was so mad they removed? People told Google those were false and dangerous, Google removed them, and Paul was furious that they acted after being “made aware” of allegedly false and dangerous content.Now Google is doing exactly what Paul demanded—not removing content based on mere claims of falsity or danger—and he’s  mad at them.So what’s Paul’s solution? Threaten to remove Section 230:It is particularly galling that, even when informed of the death threats stemming from the unsubstantiated and defamatory allegations, Google refused to evaluate the truth of what it was hosting despite its widespread practice of evaluating and removing other content for perceived lack of truthfulness.Remember when MAGA world insisted that Biden administration officials threatening platforms’ Section 230 protections was unconstitutional coercion? Remember how that was supposedly the worst violation of the First Amendment imaginable?Rand Paul is now doing the same thing. A sitting Senator, using his platform and his legislative power, threatening to strip legal protections from a company because they won’t remove content he personally dislikes.Paul literally told these platforms it wasn’t their job to determine truth or falsity. He literally sponsored a bill to prevent government officials from pressuring platforms about content. And now he’s doing exactly what he said was wrong—and threatening consequences if they don’t comply.He didn’t “change his mind” on Section 230. He just revealed that he never had a principled position in the first place.Paul supported Section 230 when he thought it meant platforms would leave up content he liked. He sponsored anti-jawboning legislation when he thought it would stop people he disagreed with from pressuring platforms. But the moment the system produces an outcome he doesn’t like—even though it worked exactly as designed and the video came down anyway—he’s ready to burn the whole thing down.What is it with Senators and their thin skins? A few months ago we wrote about Senator Amy Klobuchar pressing for an obviously unconstitutional law against deepfakes after someone made an obviously fake satirical video about her. Now Paul joins the club: Senators who want to remake internet law because someone was mean to them online.The video’s already down, Senator. You won. Maybe take the win instead of trying to burn down the open internet because Google wouldn’t do you a personal favor (the same favor you wanted to make illegal).]]></content:encoded></item><item><title>Ireland Wants To Give Its Cops Spyware, Ability To Crack Encrypted Messages</title><link>https://it.slashdot.org/story/26/01/21/1639200/ireland-wants-to-give-its-cops-spyware-ability-to-crack-encrypted-messages?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 17:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Irish government is planning to bolster its police's ability to intercept communications, including encrypted messages, and provide a legal basis for spyware use. From a report: The Communications (Interception and Lawful Access) Bill is being framed as a replacement for the current legislation that governs digital communication interception. The Department of Justice, Home Affairs, and Migration said in an announcement this week the existing Postal Packets and Telecommunications Messages (Regulation) Act 1993 "predates the telecoms revolution of the last 20 years." 

As well as updating laws passed more than two decades ago, the government was keen to emphasize that a key ambition for the bill is to empower law enforcement to intercept of all forms of communications. The Bill will bring communications from IoT devices, email services, and electronic messaging platforms into scope, "whether encrypted or not." 

In a similar way to how certain other governments want to compel encrypted messaging services to unscramble packets of interest, Ireland's announcement also failed to explain exactly how it plans to do this. However, it promised to implement a robust legal framework, alongside all necessary privacy and security safeguards, if these proposals do ultimately become law. It also vowed to establish structures to ensure "the maximum possible degree of technical cooperation between state agencies and communication service providers."/i]]></content:encoded></item><item><title>The Code is No Longer the Source of Truth: Why Documentation is the New &quot;Source Code&quot;</title><link>https://hackernoon.com/the-code-is-no-longer-the-source-of-truth-why-documentation-is-the-new-source-code?source=rss</link><author>Nikita Kothari</author><category>tech</category><pubDate>Wed, 21 Jan 2026 17:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Remember the old developer mantra? "If you want to know what the system does, read the source code. Comments lie; code doesn't."\
For decades, this was our excuse to treat documentation like the dirty dishes of software development—a chore to be ignored until absolutely necessary. We optimized for human readability, assuming another engineer could just tap us on the shoulder or reverse-engineer our spaghetti logic if they got stuck.\
We are rapidly moving from a world of "Copilots" (which help you write internal code) to a world of "Agents" (autonomous systems that string together external APIs to achieve a goal).\
Here is the uncomfortable truth about this new paradigm: AI agents don't care about the elegance of your private methods. They don't care about your clever recursion. They care about your public interfaces.\
In an agentic world, your documentation—specifically your structured API contracts—has replaced your implementation as the  source code that runs the system.Humans Can Fudge It. Machines Can't.The fundamental difference between a human developer and an AI agent using your internal platform is how they handle ambiguity.\
When a human reads half-baked documentation for an internal microservice, they use intuition. They look at existing examples; they check Slack history; they make an educated guess.\
When an LLM-powered agent encounters ambiguity, it hallucinates.\
If your API docs say a parameter is  but doesn’t specify the format (UUID vs. email vs. username), the agent has to guess. If you don't explicitly document error codes, the agent won't know the difference between a temporary network blip and a permanent validation failure.\
Ambiguity is kryptonite for an autonomous system. If you want agents to successfully perform tasks without constant human babysitting, your documentation needs to shift from "suggestive prose for humans" to "rigid instructions for machines.""Clean Code" Now Means "Clean Contracts"We spend countless hours debating Clean Code principles within a function boundary. We obsess over naming variables and extracting methods.\
Yet, we happily generate a half-assed OpenAPI (Swagger) spec from code annotations and call it a day.\
In the new stack, that OpenAPI spec is the most important file in your repository. It is the "header file" for the rest of the AI ecosystem.\
A "Clean Contract" means: If a field is marked  in the spec, your code better not treat it as optional. Agents trust the spec implicitly. Don't just use . Use formats like , , or regex patterns.Descriptive Operation IDs: Agents use these to understand intent.  is bad. retrieveUserProfileSummaryById is good.Let’s look at the difference.\
The Old Way (Human-Centric Docs): A comment above a controller method that hopes the reader understands the context.// GET /api/users/{id}
// Returns the user object. Make sure ID is right.
// Throws 404 if not found.
public ResponseEntity<User> getUser(@PathVariable String id) { ... }
\
The New Way (Agent-Centric Docs): A rigid OpenAPI definition. This YAML file  the code the agent executes against.paths:
  /api/users/{userId}:
    get:
      operationId: retrieveUserProfileById
      summary: Fetches a single user's public profile.
      description: >
        Use this tool to retrieve details like name and active status
        for a specific user ID. Do NOT use this for finding user emails.
      parameters:
        - in: path
          name: userId
          required: true
          schema:
            type: string
            format: uuid
          description: The immutable UUID of the user.
      responses:
        '200':
          description: Successful retrieval
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/UserProfile'
        '404':
          description: User ID does not exist in the active database.
The YAML above provides constraints, intent, and negative prompting ("Do NOT use this for…"). That is executable documentation.The most exciting (and frustrating) part of this shift is the new feedback loop.\
Previously, you knew your docs sucked when a new hire took three weeks to onboard. The feedback loop was slow and painful.\
Now, the feedback loop is instant. You point an agent at a task involving your APIs, and it fails immediately.\
Your logs will fill up with AI failures:"Tool execution failed: Agent attempted to send 'banana' to parameter 'userId' which requires format 'uuid'.""Agent loop stuck: API returned 400 Bad Request without a descriptive error message, agent retried same operation 5 times."\
Your new QA team is composed of robots, and they are merciless perfectionists regarding your interface definitions. If an agent can't understand how to use your service, your service is effectively broken.The Diagram: The Agentic WorkflowHere is how the flow of information changes. The docs are no longer a sidecar; they are the primary bridge.graph TD
    subgraph "The Old Way (Human Centric)"
    H[Human Dev] -->|Reads vague docs| D(Wiki/Readme)
    H -->|Guesses implementation| C(Code Editor)
    C -->|Calls API| API[Internal API]
    end

    subgraph "The Agentic Way (Machine Centric)"
    A[AI Agent] -->|Reads structured spec| S(OpenAPI/AsyncAPI Spec)
    S --"Spec is the Source of Truth"--> A
    A -->|Formulates precise tool call| API2[Internal API]
    API2 --"Structured Error/Success"--> A
    end

    style S fill:#f9f,stroke:#333,stroke-width:4px
If you believe the future of software involves autonomous agents seamlessly connecting services to perform complex work, you have to accept a boring truth: you need to get really good at writing specs.\
Stop treating documentation as an afterthought. In an agentic world, your documentation is the highest-leverage code you write.]]></content:encoded></item><item><title>Google Temporarily Disabled YouTube&apos;s Advanced Captions Without Warning</title><link>https://tech.slashdot.org/story/26/01/21/1622227/google-temporarily-disabled-youtubes-advanced-captions-without-warning?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google has temporarily disabled YouTube's advanced SRV3 caption format after discovering the feature was causing playback errors for some users, according to a statement the company posted. SRV3, also known as YouTube Timed Text, is a custom subtitle system Google introduced around 2018 that allows creators to use custom colors, transparency, animations, and precise text positioning. Creators cannot upload new SRV3 captions while the feature remains disabled, and existing videos that use the format may not display any captions until Google restores it. The company has provided no timeline for when SRV3 will return, and its forum post notes that changes should be temporary for "almost" all videos.]]></content:encoded></item><item><title>Threads rolls out ads to all users worldwide</title><link>https://techcrunch.com/2026/01/21/threads-rolls-out-ads-to-all-users-worldwide/</link><author>Sarah Perez</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:44:11 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company has made it easy for existing advertisers to expand their reach to include Threads by allowing them to automatically place ads through both Meta's Advantage+ program and via manual campaigns. ]]></content:encoded></item><item><title>YouTube TV’s multiview is getting a huge upgrade, letting viewers mix and match channels</title><link>https://techcrunch.com/2026/01/21/youtube-tvs-multiview-is-getting-a-huge-upgrade-letting-viewers-mix-and-match-channels/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:26:04 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Soon, YouTube TV will allow viewers  to customize the multiview feature to watch any four channels they want side by side. ]]></content:encoded></item><item><title>We’re not nostalgic for 2016 — we’re nostalgic for the internet before all the slop</title><link>https://techcrunch.com/2026/01/21/were-not-nostalgic-for-2016-were-nostalgic-for-the-internet-before-all-the-slop/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:23:38 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At the time, people felt like 2016 was cursed — but at least we did not yet have a word for "doomscrolling."]]></content:encoded></item><item><title>XDG-Desktop-Portal 1.21 Released With Reduced Motion Setting, Support For Linyaps Apps</title><link>https://www.phoronix.com/news/XDG-Desktop-Portal-1.21</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:14:55 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[XDG-Desktop-Portal 1.21 is now available for testing with the latest features for this portal frontend service to Flatpak...]]></content:encoded></item><item><title>OpenAI’s former sales leader joins VC firm Acrew: OpenAI taught her where startups can build a ‘moat’</title><link>https://techcrunch.com/2026/01/21/openais-former-sales-leader-joins-vc-firm-acrew-openai-taught-her-where-startups-can-build-a-moat/</link><author>Julie Bort</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:06:40 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Aliisa Rosenthal has found a new career as a VC. She knows what startups can do to protect themselves from the model makers eating their markets.]]></content:encoded></item><item><title>Japan Restarts World&apos;s Largest Nuclear Plant as Fukushima Memories Loom Large</title><link>https://slashdot.org/story/26/01/21/1532240/japan-restarts-worlds-largest-nuclear-plant-as-fukushima-memories-loom-large?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:05:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New submitter BeaverCleaver shares a report: Japan has restarted operations at the world's largest nuclear power plant for the first time since the 2011 Fukushima disaster forced the country to shut all of its reactors. The decision to restart reactor number 6 at Kashiwazaki-Kariwa north-west of Tokyo was taken despite local residents' safety concerns. It was delayed by a day because of an alarm malfunction and is due to begin operating commercially next month. 

Japan, which had always heavily relied on energy imports, was an early adopter of nuclear power. But in 2011 all 54 of its reactors had to be shut after a massive earthquake and tsunami triggered a meltdown at Fukushima, causing one of the worst nuclear disasters in history. This is the latest installment in Japan's nuclear power reboot, which still has a long way to go. The seventh reactor at Kashiwazaki-Kariwa is not expected to be brought back on until 2030, and the other five could be decommissioned. That leaves the plant with far less capacity than it once had when all seven reactors were operational: 8.2 gigawatts.]]></content:encoded></item><item><title>The HackerNoon Newsletter: What Comes After the AI Bubble? (1/21/2026)</title><link>https://hackernoon.com/1-21-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:02:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, January 21, 2026?By @linked_do [ 12 Min read ] As the AI bubble deflates, attention shifts from scale to structure. A long view on knowledge, graphs, ontologies, and futures worth living. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>How to Write Great Articles That People Will Read</title><link>https://hackernoon.com/how-to-write-great-articles-that-people-will-read?source=rss</link><author>HackerNoon Courses</author><category>tech</category><pubDate>Wed, 21 Jan 2026 16:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Writing just because you love doing so is a great feeling. However, every writer eventually arrives at the same crossroad: they want to write about content they’re passionate about, but they also want to get as many views as possible. These things aren’t contradictory; there is a way you can do both. Here’s how.Focus on Structuring Your ArticleInstead of writing whatever comes to mind, start thinking about the structure of your article: your intro, the body, and the conclusion. Are you using subsections to make the content easily digestible? Are you arranging them in an order that makes sense to the reader?These are questions that you should ask yourself if you want to elevate your article from decent to excellent. And if this doesn’t come naturally, don’t worry. Here’s a quick trick you can do: just write like you normally would.\
Write the article first, and then go back and give it some structure. Already having all your content in front of you allows you to easily mold your intro, body, and conclusion into cohesive sections. But if you continue having problems with structuring your article well, there are some resources that can help you out.HackerNoon Writing TemplatesHackerNoon has an endless list of templates that writers can use to structure, fill, and improve their articles. It doesn’t matter what your niche is; HackerNoon has a template that will be a perfect fit for you.Some of these templates include:Even if you’re a structuring expert, these templates can be useful for saving time or when you want to try out a new style.But when it comes to writing great articles, there is one very important thing you can’t forget.The Importance of Your HeadlineYour headline will be the first thing that people will read. It doesn’t matter how well-written your article’s body or conclusion paragraph is if they never make it that far. So, let’s give you some quick tips on how to nail it.\
When it comes to your headline, you want it to be eye-catching but not clickbait; there’s a very fine line between. If you’re writing an article about a tech conference you attended, you can title it something like: “All the Cool Tech Showcased in TechCon 2025” or “5 Best Pieces of Tech Unveiled at TechCon 2025.” This tells readers what the article is about and gets them excited about the technology you will talk about. Is it a bit sensationalized? Yes. But it doesn’t cross the line.\
A bad example of a headline is this: “You Will Never Believe What I Saw at TechCon 2025!” or “This Is the Craziest Thing I’ve Ever Seen!” Unless you saw a raccoon using a VR headset or something along those lines, it’s most likely that these titles are clickbait. Titles like that may work once in a blue moon, but you are more likely to strike out, so try to avoid using them.\
Okay, you’ve learned the importance of headlines, structure, and how HackerNoon templates can help you. So, that’s it, right? You’ve mastered how to be a pro writer. Well, not exactly. There is still so much to learn. But luckily for you, there is a resource that you can use to become the master that you were meant to be.The HackerNoon Blogging Fellowship Course is an online course specifically designed to help transform aspiring bloggers into full-fledged experts. It includes 8 modules that take you on a step-by-step journey to improve as a writer. These modules cover everything from Search Engine Optimization (SEO) to building your personal brand to teaching you how to monetize your content.If you’re ready to level up as a writer…]]></content:encoded></item><item><title>YouTube will soon let creators make Shorts with their own AI likeness</title><link>https://techcrunch.com/2026/01/21/youtube-will-soon-let-creators-make-shorts-with-their-own-ai-likeness/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 21 Jan 2026 15:41:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[YouTube Shorts viewers might soon see AI versions of their favorite creators when scrolling through their feeds. ]]></content:encoded></item><item><title>The CPU Performance Of The NVIDIA GB10 With The Dell Pro Max vs. AMD Ryzen AI Max+ &quot;Strix Halo&quot;</title><link>https://www.phoronix.com/review/nvidia-gb10-cpu</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 21 Jan 2026 15:32:24 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[With the Dell Pro Max GB10 testing at Phoronix we have been focused on the AI performance with its Blackwell GPU as the GB10 superchip was designed for meeting the needs of AI. Many Phoronix readers have also been curious about the GB10's CPU performance in more traditional Linux workloads. So for those curious about the GB10 CPU performance, here are some Linux benchmarks focused today on the CPU performance and going up against the AMD Ryzen AI Max+ 395 "Strix Halo" within the Framework Desktop.]]></content:encoded></item><item><title>Comic-Con Bans AI Art After Artist Pushback</title><link>https://slashdot.org/story/26/01/21/1528206/comic-con-bans-ai-art-after-artist-pushback?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 15:27:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[San Diego Comic-Con changed an AI art friendly policy following an artist-led backlash last week. From a report: It was a small victory for working artists in an industry where jobs are slipping away as movie and video game studios adopt generative AI tools to save time and money. Every year, tens of thousands of people descend on San Diego for Comic-Con, the world's premier comic book convention that over the years has also become a major pan-media event where every major media company announces new movies, TV shows, and video games. For the past few years, Comic-Con has allowed some forms of AI-generated art at this art show at the convention. 

According to archived rules for the show, artists could display AI-generated material so long as it wasn't for sale, was marked as AI-produced, and credited the original artist whose style was used. "Material produced by Artificial Intelligence (AI) may be placed in the show, but only as Not-for-Sale (NFS). It must be clearly marked as AI-produced, not simply listed as a print. If one of the parameters in its creation was something similar to 'Done in the style of,' that information must be added to the description. If there are questions, the Art Show Coordinator will be the sole judge of acceptability," Comic-Con's art show rules said until recently.]]></content:encoded></item><item><title>OpenAI aims to ship its first device in 2026, and it could be earbuds</title><link>https://techcrunch.com/2026/01/21/openai-aims-to-ship-its-first-device-in-2026-and-it-could-be-earbuds/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Wed, 21 Jan 2026 15:20:23 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The AI startup is on track to announce its first hardware device in the second half of this year, OpenAI Chief Global Affairs Officer Chris Lehane said during an interview at Davos.]]></content:encoded></item><item><title>How Animals Build a Sense of Direction</title><link>https://www.quantamagazine.org/how-animals-build-a-sense-of-direction-20260121/</link><author>Yasemin Saplakoglu</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2026/01/Internal-Compass-cr-Nachum-Ulanovsky-Default.webp" length="" type=""/><pubDate>Wed, 21 Jan 2026 15:18:39 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[On a remote island in the Indian Ocean, six closely watched bats took to the star-draped skies. As they flew across the seven-acre speck of land, devices implanted in their brains pinged data back to a group of sleepy-eyed neuroscientists monitoring them from below. The researchers were working to understand how these flying mammals, who have brains not unlike our own, develop a sense of direction…]]></content:encoded></item><item><title>TechCrunch Disrupt 2026 tickets now on sale: Lowest rates all year</title><link>https://techcrunch.com/2026/01/21/techcrunch-disrupt-2026-tickets-now-on-sale-lowest-rates-all-year/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Wed, 21 Jan 2026 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TechCrunch Disrupt 2026 tickets are officially on sale. Save up to $680 on your ticket and be among the first 500 registrants to score a plus-one pass at 50% off. Don't miss 10,000 tech leaders, founders, and VCs in San Francisco from October 13-15. Register before these one-time deals vanish.]]></content:encoded></item><item><title>Amateur Radio Operators in Belarus Arrested, Face the Death Penalty</title><link>https://www.404media.co/ham-radio-operators-in-belarus-arrested-face-the-death-penalty/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/01/CleanShot-2026-01-21-at-05.10.17@2x.png" length="" type=""/><pubDate>Wed, 21 Jan 2026 14:52:05 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[The Belarusian government is threatening three ham radio operators with the death penalty,  detained at least seven people, and has accused them of “intercepting state secrets,” according to Belarusian state media, independent media outside of Belarus, and the Belarusian human rights organization Viasna. The arrests are an extreme attack on what is most often a wholesome hobby that has a history of being vilified by authoritarian governments in part because the technology is quite censorship resistant.The detentions were announced last week on Belarusian state TV, which claimed the men were part of a network of more than 50 people participating in the amateur radio hobby and have been accused of both “espionage” and “treason.” Authorities there said they seized more than 500 pieces of radio equipment. The men were accused on state TV of using radio to spy on the movement of government planes, though no actual evidence of this has been produced.State TV claimed they were associated with the Belarusian Federation of Radioamateurs and Radiosportsmen (BFRR), a long-running amateur radio club and nonprofit that holds amateur radio competitions, meetups, trainings, and forums. WhatsApp and email requests to the BFRR from 404 Media were not returned. , Siarhei Besarab, a Belarusian amateur radio operator, posted a plea for support from others in the hobby: “MAYDAY from Belarus: Licensed operators facing death penalty.”“I am writing this because my local community is being systematically liquidated in what I can only describe as a targeted intellectual genocide,” Besarab wrote. “They have detained over 50 licensed people, including callsigns EW1ABT, EW1AEH, and EW1ACE. These men were paraded on state television like war criminals and were coerced to publicly repent for the "crime" of technical curiosity. Propagandists presented the Belarusian Federation of Radioamateurs and Radiosportsmen (BFRR) as a front for a ‘massive spy network.’”“State propaganda unironically claims these men were ‘pumping state secrets out of the air’ using nothing more than basic $25 Baofeng handhelds and consumer-grade SDR dongles,” he added. “Any operator knows that hardware like this is physically incapable of cracking the modern AES-256 digital encryption used by government security forces. It is a technical fraud, yet they are being charged with High Treason and Espionage. The punishment in Belarus for these charges is life in prison or the death penalty.”The Belarusian human rights group Viasna and its associated Telegram channel confirmed the detention and said that it spoke to a cellmate of Andrei Repetsi, who said that Repetsi was unable to talk about his case in jail: “The case is secret, so Andrei never told the essence of the case in the cell. He joked that his personal file was marked ‘Top secret. Burn before reading,’” Viasna wrote. Most hams operate amateur radios for fun, as part of competitions, or to keep in touch with other hams around the world. But the hobby has a long history of being attacked by governments in part because it is resistant to censorship. Amateur radio often works even if a natural disaster or political action takes down internet, cell, and phone services, so it is popular among people interested in search and rescue and doomsday prepping. Amateur radio has been used to share information out of Cuba, for example, and in 2021 the Cuban government jammed ham radio frequencies during anti-government protests there. ]]></content:encoded></item><item><title>YouTube CEO Acknowledges &apos;AI Slop&apos; Problem, Says Platform Will Curb Low-Quality AI Content</title><link>https://news.slashdot.org/story/26/01/21/1422227/youtube-ceo-acknowledges-ai-slop-problem-says-platform-will-curb-low-quality-ai-content?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 21 Jan 2026 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[YouTube CEO Neal Mohan used his annual letter to creators, published Wednesday, to outline an ambitious 2026 vision that embraces AI-powered creative tools while simultaneously pledging to crack down on the low-quality AI content that has come to be known as "slop." 

Mohan identified four AI-related areas that YouTube "must get right in 2026." The platform is working on tools that will let creators use AI to generate Shorts featuring their own likenesses and to experiment with music. "Just as the synthesizer, Photoshop and CGI revolutionized sound and visuals, AI will be a boon to the creatives who are ready to lean in," he wrote. Features like autodubbing, he says, will "transform the viewer experience." 

But "the rise of AI has raised concerns about low-quality content, aka 'AI slop,'" he wrote. YouTube is building on its existing spam and clickbait detection systems to reduce the spread of such content. He also flagged deepfakes as a particular concern: "It's becoming harder to detect what's real and what's AI-generated." The platform plans to double down on AI labels and introduce tools that let creators protect their likenesses.]]></content:encoded></item><item><title>The Leadership Strategies That Drove Business Growth in LATAM and Dubai</title><link>https://hackernoon.com/the-leadership-strategies-that-drove-business-growth-in-latam-and-dubai?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Wed, 21 Jan 2026 14:31:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In an increasingly global business domain, leadership isn’t just about vision—it’s about execution across borders, time zones, and cultures. As organizations race to scale with agility, regions like Latin America and the Middle East have become central to expansion strategies, not merely as markets, but as hubs of delivery and innovation. At the heart of this transformation stands Srinivas Balasubramanian, a project manager whose hands-on leadership style and operational discipline have helped translate strategic ambition into tangible outcomes.With over eight years of cross-industry project leadership experience, Balasubramanian has been instrumental in reshaping how global companies view resource allocation and delivery execution. His recent efforts in Latin America (LATAM) and Dubai offer a compelling look at how localized leadership can scale global growth.When organizations in the U.S. looked for efficient, responsive project delivery partners, LATAM emerged as a natural fit. But proximity alone wasn’t enough. It took a deliberate strategy to turn potential into performance.“LATAM gave us a time-zone advantage, but we had to earn our delivery credibility,” Balasubramanian shares. Under his direction, 30 technical experts were recruited, onboarded, and deployed into high-value U.S.-based initiatives. They weren't small-scale endeavors, multimillion-dollar budgets and tight delivery schedules were associated with them. By bringing forth actual-time collaboration and ensuring cultural alignment, he established a strong operational framework.\
The outcome was evident: many projects within a two-year time frame, on time, and with uniform quality. “We didn’t just scale talent—we scaled trust,” he adds. That trust, as it turned out, became the currency for global expansion.If LATAM was about synchronizing time zones, Dubai was about bridging cultural and operational divides. The project in question? A large-scale, end-to-end sports monitoring system for nearly 25,000 users—tracking everything from health metrics to meal plans.Our expert led the entire delivery lifecycle—from requirements gathering and design to development, deployment, and support. But what made this particularly complex wasn’t the technology. It was the context.“We were entering a new industry, with new terminology and unfamiliar processes,” he explains. “On top of that, cultural nuances shaped how we communicated and collaborated.” Weekly check-ins weren’t enough. His team made regular in-person visits, working side by side with stakeholders, building rapport and gaining operational insight.That diligence paid off. Within a span of 14 months, the outdated paper and Excel-based tracking process was fully digitized. The organization saw an immediate boost in efficiency, accuracy, and user engagement.In both LATAM and Dubai, success was anything but guaranteed. LATAM posed regulatory and compliance hurdles. The region was unfamiliar territory for the organization, and understanding its federal structure took time and focused effort. “We didn’t approach LATAM as just another delivery site,” he states. “We studied its dynamics, invested in people, and gave our teams the tools and trust they needed to thrive.”In Dubai, aside from cultural complexity, scope creep was a constant threat. The needs of clients changed rapidly, and open communication became mission-critical. His single point-of-contact structure guaranteed consistent alignment and continued growth without compromising on quality.\
Balasubramanian's effort has proven significant. In LATAM, that leadership led to tens of successful projects over two years with over 30 team members. In Dubai, a wholesale operations transformation was completed in under 6 months. These are not just successful deployments; they're instances of scalable, repeatable models.The sports monitoring system that has been developed in Dubai is now a model that future systems in the region will use. “It’s a living example of what’s possible when technology meets local insight,” he notes.Beyond delivery, he contributes actively to the industry’s body of knowledge. His publications, such as “Project Management Challenges in High-Profile Sports and Entertainment Software Deployments” and “Developing Seamless Cross-Platform User Experiences for Sports Applications”, highlight not only technical expertise but also an ability to think strategically about scale, user experience, and performance. These works reflect his deep understanding of project ecosystems and reinforce his credibility as both a practitioner and a thought leader.Looking forward, Balasubramanian is optimistic and grounded. The lessons learned in LATAM are now guiding the company’s approach in other emerging regions. The systems delivered in Dubai are being enhanced through AI, promising to automate and optimize even more processes. “AI will allow us to take what we built and make it smarter,” he reflects. “It’s not about replacing people; it’s about augmenting what they do best.”He emphasizes that sustainable global growth requires more than simply establishing a presence in new markets, it demands a deep understanding of local ecosystems and the thoughtful adaptation of delivery models. For today’s leaders, the challenge is no longer whether to expand internationally, but how effectively they can localize their strategies.His career offers a compelling example of this principle in action. Rather than relying solely on top-down direction, he has consistently driven impact from the ground up through focused execution, cross-cultural collaboration, and a commitment to excellence, one project at a time.]]></content:encoded></item><item><title>Linux 7.0 Apple Silicon Device Tree Updates Have All The Bits For USB Type-C Ports</title><link>https://www.phoronix.com/news/Apple-Silicon-DT-Linux-7.0</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 21 Jan 2026 14:18:28 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Ahead of the Linux 6.20~7.0 cycle kicking off next month, the Apple Silicon Device Tree updates have been sent out for queuing ahead of that next merge window. Notable this round are the Device Tree additions for rounding out the USB 2.0/3.x support with the USB-C ports...]]></content:encoded></item><item><title>Adobe Acrobat now lets you edit files using prompts, generate podcast summaries</title><link>https://techcrunch.com/2026/01/21/adobe-acrobat-now-lets-you-edit-files-using-prompts-generate-podcast-summaries/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Wed, 21 Jan 2026 14:16:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Adobe is adding AI tools to Acrobat, including the ability to generate podcast summaries of files, create presentations, and a way for users to edit files using prompts.]]></content:encoded></item></channel></rss>
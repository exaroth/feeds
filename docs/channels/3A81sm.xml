<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech</title><link>https://konrad.website/feeds/</link><description></description><item><title>Sam Altman Answers Questions on X.com About Pentagon Deal, Threats to Anthropic</title><link>https://news.slashdot.org/story/26/03/01/0233230/sam-altman-answers-questions-on-xcom-about-pentagon-deal-threats-to-anthropic?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 02:39:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Saturday afternoon Sam Altman announced he'd start answering questions on X.com about OpenAI's work with America's Department of War — and all the developments over the past few days. (After that department's negotions had failed with Anthropic, they announced they'd stop using Anthropic's technology and threatened to designate it a "Supply-Chain Risk to National Security". Then they'd reached a deal for OpenAI's technology — though Altman says it includes OpenAI's own similar prohibitions against using their products for domestic mass surveillance and requiring "human responsibility" for the use of force in autonomous weapon systems.) 
Altman said Saturday that enforcing that "Supply-Chain Risk" designation on Anthropic "would be very bad for our industry and our country, and obviously their company. We said [that] to the Department of War before and after. We said that part of the reason we were willing to do this quickly was in the hopes of de-esclation.... We should all care very much about the precedent... To say it very clearly: I think this is a very bad decision from the Department of War and I hope they reverse it. If we take heat for strongly criticizing it, so be it." 


Altman also said that for a long time, OpenAI was planning to do "non-classified work only," but this week found the Department of War "flexible on what we needed..."

 Sam Altman: The reason for rushing is an attempt to de-escalate the situation. I think the current path things are on is dangerous for Anthropic, healthy competition, and the U.S. We negotiated to make sure similar terms would be offered to all other AI labs. 

I know what it's like to feel backed into a corner, and I think it's worth some empathy to the Department of War. They are... a very dedicated group of people with, as I mentioned, an extremely important mission. I cannot imagine doing their work. Our industry tells them "The technology we are building is going to be the high order bit in geopolitical conflict. China is rushing ahead. You are very behind." And then we say "But we won't help you, and we think you are kind of evil." I don't think I'd react great in that situation. I do not believe unelected leaders of private companies should have as much power as our democratically elected government. But I do think we need to help them. 



Question: Are you worried at all about the potential for things to go really south during a possible dispute over what's legal or not later on and be deemed a supply chain risk...? 



Sam Altman: Yes, I am. If we have to take on that fight we will, but it clearly exposes us to some risk. I am still very hopeful this is going to get resolved, and part of why we wanted to act fast was to help increase the chances of that... 


Question: Why the rush to sign the deal ? Obviously the optics don't look great. 


Sam Altman: It was definitely rushed, and the optics don't look good. We really wanted to de-escalate things, and we thought the deal on offer was good. 
If we are right and this does lead to a de-escalation between the Department of War and the industry, we will look like geniuses, and a company that took on a lot of pain to do things to help the industry. If not, we will continue to be characterized as as rushed and uncareful. I don't where it's going to land, but I have already seen promising signs. I think a good relationship between the government and the companies developing this technology is critical over the next couple of years... 



Question: What was the core difference why you think the Department of War accepted OpenAI but not Anthropic? 


Sam Altman: [...] We believe in a layered approach to safety--building a safety stack, deploying FDEs [embedded Forward Deployed Engineers] and having our safety and alignment researcher involved, deploying via cloud, working directly with the Department of War. Anthropic seemed more focused on specific prohibitions in the contract, rather than citing applicable laws, which we felt comfortable with. We feel that it it's very important to build safe system, and although documents are also important, I'd clearly rather rely on technical safeguards if I only had to pick one... 




I think Anthropic may have wanted more operational control than we did... 



Question: Were the terms that you accepted the same ones Anthropic rejected? 


Sam Altman: No, we had some different ones. But our terms would now be available to them (and others) if they wanted. 



Question: Will you turn off the tool if they violate the rules? 



Sam Altman: Yes, we will turn it off in that very unlikely event, but we believe the U.S. government is an institution that does its best to follow law and policy. What we won't do is turn it off because we disagree with a particular (legal military) decision. We trust their authority.

 

Questions were also answered by OpenAI's head of National Security Partnerships (who at one point posted that they'd managed the White House response to the Snowden disclosures and helped write the post-Snowden policies constraining surveillance during the Obama years.) And they stressed that with OpenAI's deal with Department of War, "We control how we train the models and what types of requests the models refuse."




Question: Are employees allowed to opt out of working on Department of War-related projects? 


Answer: We won't ask employees to support Department of War-related projects if they don't want to. 



Question: How much is the deal worth? 


Answer: It's a few million $, completely inconsequential compared to our $20B+ in revenue, and definitely not worth the cost of a PR blowup. We're doing it because it's the right thing to do for the country, at great cost to ourselves, not because of revenue impact... 




Question: Can you explicitly state which specific technical safeguard OpenAI has that allowed you to sign what Anthropic called a 'threat to democratic values'? 


Answer: We think the deal we made has more guardrails than any previous agreement for classified AI deployments, including Anthropic's. Other AI labs (including Anthropic) have reduced or removed their safety guardrails and relied primarily on usage policies as their primary safeguards in national security deployments. Usage policies, on their own, are not a guarantee of anything. Any responsible deployment of AI in classified environments should involve layered safeguards including a prudent safety stack, limits on deployment architecture, and the direct involvement of AI experts in consequential AI use cases. These are the terms we negotiated in our contract. 

They also detailed OpenAI's position on LinkedIn:

Deployment architecture matters more than contract language. Our contract limits our deployment to cloud API. Autonomous systems require inference at the edge. By limiting our deployment to cloud API, we can ensure that our models cannot be integrated directly into weapons systems, sensors, or other operational hardware... 



Instead of hoping contract language will be enough, our contract allows us to embed forward deployed engineers, commits to giving us visibility into how models are being used, and we have the ability to iterate on safety safeguards over time. If our team sees that our models aren't refusing queries they should, or there's more operational risk than we expected, our contract allows us to make modifications at our discretion. This gives us far more influence over outcomes (and insight into possible abuse) than a static contract provision ever could. 



U.S. law already constrains the worst outcomes. We accepted the "all lawful uses" language proposed by the Department, but required them to define the laws that constrained them on surveillance and autonomy directly in the contract. And because laws can change, having this codified in the contract protects against changes in law or policy that we can't anticipate.]]></content:encoded></item><item><title>AerynOS 2026.02 Brings More Wayland Compositor Options, Other Improvements</title><link>https://www.phoronix.com/news/AerynOS-2026.02</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 1 Mar 2026 01:13:43 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AerynOS 2026.02 was released for closing out February as the newest alpha release for this Linux distribution formerly known as Serpent OS. In AerynOS 2026.02 are many package updates plus continued work on the tooling and other innovations around this Linux distribution...]]></content:encoded></item><item><title>The trap Anthropic built for itself</title><link>https://techcrunch.com/2026/02/28/the-trap-anthropic-built-for-itself/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sun, 1 Mar 2026 00:08:58 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Anthropic, OpenAI, Google DeepMind and others have long promised to govern themselves responsibly. Now, in the absence of rules, there's not a lot to protect them.]]></content:encoded></item><item><title>Duolingo Grows, But Users Disliked Increased Ads and Subscription Pushes. Stock Plummets Again</title><link>https://slashdot.org/story/26/02/28/2321238/duolingo-grows-but-users-disliked-increased-ads-and-subscription-pushes-stock-plummets-again?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 23:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Friday was "a horrible day" for investors in Duolingo, reports Fast Company. But Friday's one-day 14% drop is just part of a longer story. 

Since last May, Duolingo's stock has dropped 81%. Yes, the company faced a social media backlash that month after its CEO promised they'd become an "AI-first" company (favoring AI over human contractors). And yes, Duolingo did double its language offerings using generative AI. But more importantly, that summer OpenAI showed how easy it was to just roll your own language-learning tool from a short prompt in a GPT-5 demo, while Google built an AI-powered language-learning tool into its Translate app. 


And yet, Friday Duolingo's shares dropped another 14%, after announcing good fourth quarter results but an unpopular direction for its future. Fast Company reports:


On the surface, many of the company's most critical metrics saw decent gains for the quarter, including: 
 — Daily Active Users: 52.7 million (up 30% year-over-year) 
 — Paid Subscribers: 12.2 million (up 28% year-over-year) 
 — Revenue: $282.9 million (up 35% year-over-year) 
 — Total bookings: $336.8 million (up 24% year-over-year) 

The company also reported its full-year 2025 financials, revealing that for the first time in its history, it crossed the $1 billion revenue mark for a fiscal year. 

But the Motley Fool explains that Duolingo's higher ad loads and repeated pushes for subscription plans "generated revenues in the short term, but made the Duolingo platform less engaging. Ergo, user growth decelerated while revenues rose." Thursday Duolingo announced a big change to address that, including moving more features into lower-priced tiers. Barron's reports:

D.A. Davidson analyst Wyatt Swanson, who rates Duolingo stock at Neutral, posited that the push to monetize "led to disgruntled users and a meaningful negative impact to 'word-of-mouth' marketing." Duolingo has guided for bookings growth between 10% and 12% in 2026, compared with the 20% rate the company would have expected to see "if we operated like we have in past years...."
If stock reaction is any indication, investors are concerned about Duolingo's new focus.]]></content:encoded></item><item><title>Why did Netflix back down from its deal to acquire Warner Bros.?</title><link>https://techcrunch.com/2026/02/28/why-did-netflix-back-down-from-its-deal-to-acquire-warner-bros/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 28 Feb 2026 22:07:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Netflix's co-CEO reportedly told Trump, "I took your advice."]]></content:encoded></item><item><title>New &apos;Star Wars&apos; Movies Are Coming to Theatres. But Will Audiences?</title><link>https://entertainment.slashdot.org/story/26/02/28/0514259/new-star-wars-movies-are-coming-to-theatres-but-will-audiences?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 21:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["The drought of upcoming Star Wars movies is coming to an end soon," writes Cinemablend. In May the The Mandalorian and Grogu opens, and one year later there's the release of the Ryan Gosling-led Star Wars: Starfighter. 

But "there are some insiders who already believe that Starfighter will be a bigger hit than The Mandalorian and Grogu..."

According to unnamed sources who spoke with Variety, there's a "sense" that Star Wars: Starfighter, which is directed by Deadpool & Wolverine's Shawn Levy, will be a more satisfying viewing experience. These same sources are allegedly impressed by the early footage they've seen of Ryan Gosling's performance and also suggested that Levy has "recaptured the franchise's spirit of fun." Furthermore, the article states that there's concern that because The Mandalorian and Grogu is spinning out of a streaming-exclusive series, it might not have as much appeal to people who aren't already fans of The Mandalorian... Star Wars: Starfighter, on the other hand, will be accessible to everyone equally. It's set five years after The Rise of Skywalker, which is an unexplored period for the Star Wars franchise onscreen. It's also expected that most, if not all of its featured characters will be brand-new, so no knowledge of past adventures is required. 
Slashdot reader gaiageek reminds us that 2027 will also see a special 50-year anniversary event in movie in theatres: a "newly restored" version of the original 1977 Star Wars.]]></content:encoded></item><item><title>What to know about the landmark Warner Bros. Discovery sale</title><link>https://techcrunch.com/2026/02/28/warner-bros-netflix-paramount-acquisition-timeline-wbd/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Sat, 28 Feb 2026 21:28:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Learn more about Paramount's planned acquisition of Warner Bros. Discovery — a historic Hollywood megadeal valued at $111 billion — as it continues to develop.]]></content:encoded></item><item><title>Anthropic’s Claude rises to No. 2 in the App Store following Pentagon dispute</title><link>https://techcrunch.com/2026/02/28/anthropics-claude-rises-to-no-2-in-the-app-store-following-pentagon-dispute/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 28 Feb 2026 21:05:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Anthropic’s chatbot Claude seems to have benefited from the attention around the company’s fraught negotiations with the Pentagon.]]></content:encoded></item><item><title>Scientists Reveal the Surprising Sex Lives of Neanderthals and Early Humans</title><link>https://www.404media.co/scientists-reveal-the-surprising-sex-lives-of-neanderthals-and-early-humans/</link><author>Becky Ferreira</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/02/image2.jpg" length="" type=""/><pubDate>Sat, 28 Feb 2026 20:48:27 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Welcome back to the Abstract! Here are the studies this week that exposed prehistoric hookups, marched toward death, feasted on their own bodies, and found a buried legend in the Sahara.First, Neanderthal males had lots more babies with human females than human males had with Neanderthal females. What’s up with that?! Then, strap in for a stellar swan song, antlers for breakfast, and a timeless style icon from the Cretaceous.Dad’s a Neanderthal, Mom’s a human, I’m in therapyHumans and our close relatives, Neanderthals, produced children together many times before the latter went extinct about 40,000 years ago. As a result, the vast majority of people living today carry a pinch of Neanderthal DNA—the enduring proof of past copulations between our species.Now, scientists have proposed that these prehistoric partnerships overwhelmingly occurred between Neanderthal males and females of our own species, , with far fewer couplings between Neanderthal females and human males. This strong sexual bias provides the most "parsimonious” explanation for the uneven distribution of Neanderthal alleles (variants of specific genes) in modern human genomes, according to a new study.“One of the notable features evident in alignments of Neanderthal genomes to those of modern humans is the presence of ‘Neanderthal deserts’ within modern human genomes: genomic regions where Neanderthal alleles are conspicuously rare in the modern human (and ancient modern human) gene pool,” said researchers led by Alexander Platt of the University of Pennsylvania.  In particular, the team noted that Neanderthal deserts show up on the human X chromosome, which they think hints at a strong sex bias toward breeding between Neanderthal males and human females. The team compared Neanderthal genomes with genetic data from some sub-Saharan African populations that have no Neanderthal ancestry. This approach allowed them to track ancient gene flow from anatomically modern humans (AMHs)—in other words, our ancient  ancestors—into Neanderthal populations. The results revealed that the Neanderthal X chromosomes had a 62 percent relative excess of DNA from AMHs. In other words, not only are there Neanderthal deserts on human X chromosomes, there are corollary “floods” or “oases” (whatever metaphor you like) of human DNA on Neanderthal X chromosomes. This discovery is strong evidence that humans were contributing more alleles to the Neanderthal X chromosome, and Neanderthals were contributing less to the human X chromosome, due to an unexplained asymmetry in mate preference. Overall, the genetic patterns the team observed “were likely colored by a persistent preference for pairings between males of predominantly Neanderthal ancestry and females of predominantly AMH ancestry over the reverse,” the researchers concluded. “The bias that we inferred seems to have remained consistent across admixture events separated by 200,000 years.”Men prefer blondes; women prefer Neanderthals? I don’t know. This is just wildly interesting. A (hypergiant) star is bornWe’ve all been there: One day, you’re an extreme red supergiant, and the next, you’re a yellow hypergiant. A new study reports that WOH G64, one of the biggest known stars in the sky, went through this “dramatic transition” sometime in 2014 (or at least, that’s when astronomers first captured this spectral shift in the star, which is located about 163,000 light years from Earth).If the Sun were as big as WOH G64, it would stretch to the orbit of Saturn. This late-stage stellar titan offers an ultra-rare opportunity to see how red supergiants (RSGs) end their lives, a process that is shrouded in mystery—often literally, as these stars tend to be obscured by a lot of circumstellar gas.“The apparent lack of luminous RSGs detected as supernova progenitors has sparked an ongoing debate over the fate of these stars,” said researchers led by Gonzalo Muñoz-Sanchez of the National Observatory of Athens. “WOH G64 thus provides critical insight into post-RSG evolution and the formation of dense circumstellar environments seen in core-collapse supernovae.” It could be that WOH G64 does detonate. In fact, this may have already happened, but the light show hasn’t reached us yet. It may also collapse directly into a black hole with no supernova to show for it. We’ll just have to keep watching this space! This has been Big Star News.Antlers in deer are usually a male ornamentation that allows females to judge potential mates based on the quality of their head-bling. Caribou females, however, buck this trend as the only female deer with antlers. So, as a folktale might ask: How did the caribou get her antlers? One answer is that antlers make a great post-partum snack, according to a new study. In migratory populations, female caribou shed their antlers when they reach calving grounds, usually just days before they give birth, which may give nursing mothers a much-needed vitamin boost. “Pervasive antler consumption by caribou suggests that synchroneity between birthing and antler shedding evinces the importance of nutrient (calcium, phosphorus) transport for supporting calf survival,” said researchers led by Madison Gaetano of the University of Cincinnati. “Though intriguing, additional research will be important to more explicitly evaluate the dietary and fitness benefits (for both females and their calves) of antler-derived nutrients.”Given that caribou also eat their placentas, it’s really impressive how these new mothers nourish themselves and their young with the fruits of their own bodies. Hardcore. Respect. New spinosaur just droppedSpeaking of animals with rad headgear, we’ll close with a shoutout to , a newly-discovered species of giant carnivorous dinosaur that rocked an epic scimitar-shaped skull crest. Move over, rock band T. Rex—this killer is the new wave of dinosaurian glam. “…discovered in the central Sahara alongside long-necked dinosaurs in a riparian habitat, is distinguished by a scimitar-shaped bony crest projecting far above its skull roof,” said researchers led by Paul C. Sereno of the University of Chicago.Spinosaurus stock has gone through the roof in recent decades as new finds have confirmed that they were the biggest land predators of all time, dethroning  from a tyrant king to a mere tyrant vassal. As the ultimate charismatic megafauna, spinosaurs are popular in dino-blockbusters. Indeed, one of my favorite gags in cinematic history is when a Spinosaurus swallows a satellite phone in , so you know it’s lurking when you hear the Nokia ring tone. Pure dinosaurian comedic gold. In any case, the new study sheds new light into the semi-aquatic nature of this majestic hunter, suggesting that this particular species was “a wading, shoreline predator with visual display an important aspect of its biology.” While this animal was no doubt visually captivating, it’s best to view it from a safe distance of about 94 million years.Thanks for reading! See you next week.]]></content:encoded></item><item><title>The billion-dollar infrastructure deals powering the AI boom</title><link>https://techcrunch.com/2026/02/28/billion-dollar-infrastructure-deals-ai-boom-data-centers-openai-oracle-nvidia-microsoft-google-meta/</link><author>Russell Brandom</author><category>tech</category><pubDate>Sat, 28 Feb 2026 20:41:55 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Here's everything we know about the biggest AI infrastructure projects, including major spending from Meta, Oracle, Microsoft, Google, and OpenAI.]]></content:encoded></item><item><title>US Threatens Anthropic with &apos;Supply-Chain Risk&apos; Designation. OpenAI Signs New War Department Deal</title><link>https://tech.slashdot.org/story/26/02/28/2028232/us-threatens-anthropic-with-supply-chain-risk-designation-openai-signs-new-war-department-deal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 20:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[It started Friday when all U.S. federal agencies were ordered to "immediately cease" using Anthropic's AI technology after contract negotiations stalled when Anthropic requested prohibitions against mass domestic surveillance or fully autonomous weapons. But later Friday there were even more repercussions... 


In a post to his 1.1 million followers on X.com, U.S. Secretary of War Pete Hegseth criticized Anthropic for what he called "a master class in arrogance and betrayal as well as a textbook case of how not to do business with the United States Government or the Pentagon."

Our position has never wavered and will never waver: the Department of War must have full, unrestricted access to Anthropic's models for every LAWFUL purpose in defense of the Republic... Cloaked in the sanctimonious rhetoric of "effective altruism," [Anthropic and CEO Dario Amodei] have attempted to strong-arm the United States military into submission — a cowardly act of corporate virtue-signaling that places Silicon Valley ideology above American lives. The Terms of Service of Anthropic's defective altruism will never outweigh the safety, the readiness, or the lives of American troops on the battlefield. Their true objective is unmistakable: to seize veto power over the operational decisions of the United States military. That is unacceptable... 

In conjunction with the President's directive for the Federal Government to cease all use of Anthropic's technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic... America's warfighters will never be held hostage by the ideological whims of Big Tech. This decision is final. 

Meanwhile, Anthrophic said on Friday that "no amount of intimidation or punishment from the Department of War will change our position." (And "We will challenge any supply chain risk designation in court.")
Designating Anthropic as a supply chain risk would be an unprecedented action — one historically reserved for US adversaries, never before publicly applied to an American company. We are deeply saddened by these developments. As the first frontier AI company to deploy models in the US government's classified networks, Anthropic has supported American warfighters since June 2024 and has every intention of continuing to do so. We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government... Secretary Hegseth has implied this designation would restrict anyone who does business with the military from doing business with Anthropic. The Secretary does not have the statutory authority to back up this statement. 

Anthropic also defended the two exceptions they'd requested that had stalled contract negotiations. "[W]e do not believe that today's frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger America's warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights." 


Also Friday, OpenAI announced that "we reached an agreement with the Department of War to deploy our models in their classified network."

OpenAI CEO Sam Altman emphasized that the agreement retains and confirms OpenAI's own prohibitions against using their products for domestic mass surveillance — and requires "human responsibility" for the use of force including for autonomous weapon systems. "The Department of War agrees with these principles, reflects them in law and policy, and we put them into our agreement. We also will build technical safeguards to ensure our models behave as they should, which the Department of War also wanted. "

We are asking the Department of War to offer these same terms to all AI companies, which in our opinion we think everyone should be willing to accept. We have expressed our strong desire to see things de-escalate away from legal and governmental actions and towards reasonable agreements. We remain committed to serve all of humanity as best we can. The world is a complicated, messy, and sometimes dangerous place.
]]></content:encoded></item><item><title>This Week In Techdirt History: February 22nd – 28th</title><link>https://www.techdirt.com/2026/02/28/this-week-in-techdirt-history-february-22nd-28th/</link><author>Leigh Beadon</author><category>tech</category><pubDate>Sat, 28 Feb 2026 20:00:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Antarctica&apos;s Massive Neutrino Observatory Gets an Upgrade</title><link>https://science.slashdot.org/story/26/02/28/0632201/antarcticas-massive-neutrino-observatory-gets-an-upgrade?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 19:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[There's already 5,000 sensors embedded in Antarctica's ice to look for evidence of neutrinos, reports the Washington Post. But in November scientists drilled six new holes at least a mile and a half deep and installed cables with hundreds more light detectors — an upgrade to the massive 15-year-old IceCube Neutrino Observatory to detect the charged particles produced by lower-energy neutrinos interacting with matter:



When they do, the neutrinos produce charged particles that travel through the ice at nearly the speed of light, creating a blue glow called Cherenkov radiation... "Within the first couple years, we should be making much better measurements," [said Erin O'Sullivan, an associate professor of physics at Uppsala University in Sweden and a spokesperson for the project.] "There's hope to expand the detector, by an order of magnitude in volume, so the important thing there is we're not just seeing a few neutrino point sources, but we're starting to be a true telescope. ... That's really the dream." 

The scientists spent seven years planning the upgrade, according to the article. "To drill holes a mile and a half deep takes about 30 hours, and 18 more hours to return to the surface," the article points out. "Then, the race begins because almost immediately, the hole starts to shrink as the water refreezes." ("If it takes too much time, the principal investigator says, "the instruments don't fit in anymore!")]]></content:encoded></item><item><title>The Simplest Way to Understand How LLMs Actually Work!</title><link>https://hackernoon.com/the-simplest-way-to-understand-how-llms-actually-work?source=rss</link><author>Amit Juneja</author><category>tech</category><pubDate>Sat, 28 Feb 2026 19:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The magic of transformers lies in their attention mechanism. But what does that actually mean?\
Here's a simplified explanation to build intuition.Consider: "What is the capital of France?"As humans, we parse this as:"What" signals a question"is" indicates the current timeframe"capital" means the main city"France" is the country for which I want the capitalWe process it instantly. But for a computer? Different story.THE ATTENTION MECHANISM: Q, K, VTransformers use a clever trick: for every word (technically tokens), the model creates three different representations:Query (Q) - "What information am I looking for?"For the word "capital," the query is something like: "What kind of entity am I describing?"Key (K) - "What information can I provide?"Every word gets a key that describes what it offers. For the word "capital," the key is something like: "I'm a noun describing geographic/political entities."Value (V) - "Here's my actual meaning."The word "capital" has the semantic meaning "main city, governmental center, and administrative importance."The model compares the query from one word against the keys of all other words. This produces .Here is what happens when the word "capital", with its query of "What kind of entity am I describing?", checks against the keys of all the other words:"France" responds with its key → "What" responds with "is" responds with Higher scores contribute more to the final understanding. So after this, the representation of "capital" is enriched with strong context from "France."This doesn't happen just once. Transformers use  running in parallel, like several people reading the same sentence, each noticing different patterns. One might focus on grammar, another on meaning, another on long-range dependencies.In another head, the word "capital" could be querying for the timeframe. In this case, the word "is" will give a high score for the current time.All these attention scores combined give a rich context to each word. So the word "capital" knows that it is a question, it is for the current timeframe, and it is about "France."After each attention layer, information flows through a Feed Forward Network. This is where the answers start to form. This network processes the context-enriched representations, helping build toward output predictions like 'Paris.'The combination of attention + FFN, repeated across layers, gives transformers their power.Unlike older models that processed words one at a time, transformers:Look at the entire sentence at onceLet every word "attend to" every other wordCapture relationships between distant wordsBuild understanding through multiple layersThat's transformer attention in action.*This explanation simplifies many technical details to focus on core concepts. For a deeper dive, check out "Attention Is All You Need" by Vaswani et al.*]]></content:encoded></item><item><title>&apos;World&apos;s Largest Battery&apos; Soon At Google Data Center: 100-Hour Iron-Air Storage</title><link>https://hardware.slashdot.org/story/26/02/28/0446211/worlds-largest-battery-soon-at-google-data-center-100-hour-iron-air-storage?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 18:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[ Interesting Engineering reports:

US tech giant Google announced on Tuesday that it will build a new data center in Pine Island, Minnesota. The new facility will be powered by 1.9 gigawatts (GW) of clean energy from wind and solar, coupled with a 300-megawatt battery, claimed to be the 'world's largest', with a 30-gigawatt-hour (GWh) capacity and 100-hour duration... The planned battery would dwarf a 19 GW lithium-ion project in the UAE... 

Form Energy's batteries work very differently from most large batteries today. Instead of using lithium like the batteries in electric cars, they store electricity by making iron rust and then reversing the rusting process to release the energy when needed... Form's iron-air batteries are heavier and less efficient than their counterparts; they can only return about 50% to 70% of the energy used to charge them, while lithium-ion batteries return more than 90%. However, Form's batteries have one distinct advantage. They are cheaper than lithium-ion batteries, costing about $20 per kilowatt-hour of storage, which is almost three times as cheap... It will store 150 MWh of electricity and can supply to the grid for up to 100 hours, delivering about 1.5 MW at peak output.
 
Thanks to long-time Slashdot reader schwit1 for sharing the article.]]></content:encoded></item><item><title>After US-Israel Attacks, 90 Million Iranians Lose Internet Connectivity</title><link>https://news.slashdot.org/story/26/02/28/1733240/after-us-israel-attacks-90-million-iranians-lose-internet-connectivity?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 17:35:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[CNN reports that images from Iran's capital "have shown cars jammed along Tehran's street, with heavy traffic on major roads after today's wave of attacks by the US and Israel." And though Iran has a population of 93 million, the attacks suddenly plunged Iran into "a near-total internet blackout with national connectivity at 4% of ordinary levels," according to internet monitoring experts at NetBlocks. 

CNN reports:

Since Iran's brutal crackdown earlier this year, the regime has made progress to allow only a subset of people with security clearance to access the international web, experts said. After previous internet shutdowns, some platforms never returned. The Iranian government blocked Instagram after the internet shutdown and protests in 2022, and the popular messaging app Telegram following protests in 2018. 


The International Atomic Energy Agency announced an hour ago that they're "closely monitoring developments" — keeping in contact with countries in the region and so far seeing "no evidence of any radiological impact." They're also urging "restraint to avoid any nuclear safety risks to people in the region." 

UPDATE (1 PM PST):
Qatar, Bahrain and Kuwait "are shifting to remote learning starting Sunday until further notice following Iranâ(TM)s retaliatory strikes on Saturday," reports CNN.]]></content:encoded></item><item><title>AMD Prepares Linux For Instruction-Based Sampling Improvements With Zen 6</title><link>https://www.phoronix.com/news/Linux-Perf-AMD-IBS-Zen-6</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:58:45 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A set of patches recently posted to the Linux kernel mailing list have now been queued up to a tip/tip.git branch for planned introduction in Linux 7.1. These patches are for enhancing the Linux perf subsystem support for AMD Instruction-Based Sampling (IBS) improvements with next-gen Zen 6 processors...]]></content:encoded></item><item><title>America&apos;s Teenagers Say AI Cheating Has Become a Regular Feature of Student Life</title><link>https://news.slashdot.org/story/26/02/28/0541228/americas-teenagers-say-ai-cheating-has-become-a-regular-feature-of-student-life?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Tuesday Pew Research announced their newest findings: that 54% of America's teens use AI help with schoolwork:
One-in-five teens living in households making less than $30,000 a year say they do all or most of their schoolwork with AI chatbots' help. A similar share of those in households making $30,000 to just under $75,000 annually say this. Fewer teens living in higher-earning households (7%) say the same." 

"The survey did not ask students whether they had used chatbots to write essays or generate other assignments..." notes the New York Times. "But nearly 60% of teenagers told Pew that students at their school used chatbots to cheat 'very often' or 'somewhat often.'" Agreeing with that are the Pew Researchers themselves. "Our survey shows that many teens think cheating with AI has become a regular feature of student life." 

One worried teenager still told the researchers that AI "makes people lazy and takes away jobs." But another teenager told the researchers that "Everyone's going to have to know how to use AI or they'll be left behind." 

Thanks to long-time Slashdot reader theodp for sharing the article.]]></content:encoded></item><item><title>OpenAI’s Sam Altman announces Pentagon deal with ‘technical safeguards’</title><link>https://techcrunch.com/2026/02/28/openais-sam-altman-announces-pentagon-deal-with-technical-safeguards/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:17:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI's CEO claims its new defense contract includes protections addressing the same issues that became a flashpoint for Anthropic.]]></content:encoded></item><item><title>How Researchers Measure, Detect and Benchmark AI Manipulation</title><link>https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss</link><author>Tencent</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:15:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Enes Altuncu, ea483@kent.ac.uk (University of Kent, UK)Virginia N. L. Franqueira, V.Franqueira@kent.ac.uk (University of Kent, UK)Shujun Li, S.J.Li@kent.ac.uk (University of Kent, UK)Recent advancements in AI, especially deep learning, have contributed to a significant increase in the creation of new realistic-looking synthetic media (video, image, and audio) and manipulation of existing media, which has led to the creation of the new term “deepfake”. Based on both the research literature and resources in English and in Chinese, this paper gives a comprehensive overview of deepfake, covering multiple important aspects of this emerging concept, including 1) different definitions, 2) commonly used performance metrics and standards, and 3) deepfake-related datasets, challenges, competitions and benchmarks. In addition, the paper also reports a meta-review of 12 selected deepfake-related survey papers published in 2020 and 2021, focusing not only on the mentioned aspects, but also on the analysis of key challenges and recommendations. We believe that this paper is the most comprehensive review of deepfake in terms of aspects covered, and the first one covering both the English and Chinese literature and sources.: Deepfake, Survey, Definition, Datasets, Benchmarks, Challenges, Competitions, Standards, Performance Metrics.Recent advancements in AI and machine learning have increased the capability to produce more realistic media, e.g., video, image, and audio. Especially, state-of-the-art deep learning methods enabled the generation of “deepfakes”, manipulated or synthetic media the realness of which are not easily recognisable by the human eye. Although deepfake is a relatively new phenomenon (having first appeared at the end of 2017), its growth has been remarkable. According to the 2019 and 2020 Deeptrace reports on the state of deepfake [2], the number of deepfake videos in the English-speaking internet grew from 7,964 (December 2018) to 14,678 (July 2019) to 85,047 (December 2020), representing a 968% increase from 2018 to 2020.In this work, we review existing deepfake-related research ecosystem in terms of various aspects, including performance metrics and standards, datasets, challenges, competitions, and benchmarks. Furthermore, we provide a meta-review of 12 selected deepfake-related survey papers which covers several additional aspects other than the mentioned ones in a systematic manner, such as performance comparison, key challenges, and recommendations.Despite being a hugely popular term, there is a lack of consensus on the definition of “deepfake” and the boundary between deepfakes and non-deepfakes is not clear cut. For this survey, we adopt a relatively more inclusive approach to cover all forms of manipulated or synthetic media that are considered deepfakes in a broader sense. We also cover closely related topics including biometrics and multimedia forensics, since deepfakes are often used to launch presentation attacks against biometrics-based authentication systems and detection of deepfakes can be considered part of multimedia forensics. A more detailed discussion on different definitions of “deepfake” is given next.1.1       Definitions of the Term DeepfakeAs its name implies, the term “deepfake” is derived from the combination of “deep” (referring to  (DL)) and “fake”. It is normally used to refer to manipulation of existing media (image, video and/or audio) or generation of new (synthetic) media using DL-based approaches. The most commonly discussed deepfake data are fake face images, fake speech forgeries, and fake videos that combine both fake images and fake speech forgeries. While having “fake” in the word indicates manipulated or synthesised media, there are plenty of benign applications of the deepfake technology, e.g., for entertainment and creative arts. With this respect, another term “deep synthesis” has been proposed as a more neutral-sounding alternative [60]. This new term, however, has not been widely adopted.In addition to the lack of a universal definition, as mentioned already, the boundary between deepfakes and non-deep fakes is actually not a clear cut. There are at least two important aspects we should consider, one on detection of and the other on creation of deepfakes.First, detection of deepfakes often follows very similar approaches to detection of traditional fakes generated without using DL techniques. Advanced detection methods have also started leveraging DL to improve their performance, but they do not necessarily need to know how a target media is created (deep or not). To some extent, one could argue that detecting deepfakes does not involve developing deepfake-specific methods (even though some researchers choose to do so), but a more robust and universal detector that can handle any (deep or not) fake media. This can be seen for two closely related topics: biometrics and multimedia forensics. For biometrics, there is a trend of using deep learning techniques to generate fake biometric signals (e.g., face images and videos) for biometric spoofing or presentation attacks. For multimedia forensics, deepfake-based forgeries have become a new threat to the traditional problem of “forgery detection”. For both topics, detection of biometric spoofing and multimedia forgeries have evolved to consider both deep and non-deep fakes.Second, one may argue that the word “deep” in “deepfake” does not necessarily refer to the use of “deep learning”, but any “deep” (i.e., sophisticated) technology that creates a very believable fake media. For instance, Brady [9] considered deepfake as audio-visual manipulation using “a spectrum of technical sophistication … and techniques”. They also introduced two new terms,  and , referring to “low level manipulation of audio-visual media created with (easily) accessible software [or no software] to speed, slow, restage or re-contextualise content”. This broader understanding of “deepfake” has also been adopted by law makers for new legislations combating malicious deepfakes. For instance, the following two United States acts define “deepfakes” as follows:2018 Malicious Deep Fake Prohibition Act1:§1041.(b).(2): “the term ‘deep fake’ means an audiovisual record created or altered in a manner that the record would falsely appear to a reasonable observer to be an authentic record of the actual speech or conduct of an individual.”2019 DEEP FAKES Accountability Act2:§1041.(n).(3): “The term ‘deep fake’ means any video recording, motion-picture film, sound recording, electronic image, or photograph, or any technological representation of speech or conduct substantially derivative thereof—(A)  which appears to authentically depict any speech or conduct of a person who did not in fact engage in such speech or conduct; and(B)  the production of which was substantially dependent upon technical means, rather than the ability of another person to physically or verbally impersonate such person.”As we can see from the above legal definitions of “deepfake”, the use of DL as a technology is not mentioned at all. The focus here is on “authenticity”, “impersonation” and (any) “technical means”.1.2      Scope and ContributionBased on the above discussion on definitions of deepfake, we can see it is not always straightforward or meaningful to differentiate deepfakes from non-deep fakes. In addition, for our focus on performance evaluation and comparison, the boundary between deepfakes and non-deep fakes is even more blurred. This is because DL is just a special (deeper) form of machine learning (ML), and as a result, DL and non-deep ML methods share many common concepts, metrics and procedures.Despite the fact that deepfake may be understood in a much broader sense, in this work, we have a sufficiently narrower focus to avoid covering too many topics. We, therefore, decided to define the scope of this survey as follows:For metrics and standards, we chose to include all commonly used ones for evaluating general ML methods and those specifically defined for evaluating deepfake creation or detection methods.For datasets, challenges, competitions and benchmarks, we considered those related to fake media covered in the deepfake-related survey papers and those with an explicit mention of the term “deepfake” or a comparable term.For the meta-review, we considered only survey papers whose authors explicitly referred to the term “deepfakes” in the meta data (title, abstract and keywords).Research papers covered in this survey (i.e., the deepfake-related survey papers) were identified via systematic searches on the scientific databases, Scopus and China Online Journals (COJ)3. The following search queries were used to perform the searches on Scopus and COJ, respectively:(deepfake* OR deep-fake* OR “deep fake*”) AND (review OR survey OR overview OR systemati* OR SoK)(deepfake OR 深度伪造) AND (综述 OR 进展)The searches returned 41 survey papers in English and 15 survey papers in Chinese. Out of these papers, eight published in English and four published in Chinese were selected for consideration.Deepfake-related challenges, competitions and benchmarks were identified via multiple sources: the survey papers selected, research papers from the co-authors’ personal collections, Google Web searches, and manual inspection of websites of major AI-related conferences held in 2020 and 2021 (where such challenges and competitions are routinely organised). The inspected conferences include those listed in the ACL (Association for Computational Linguistics) Anthology4, ICCV, CVPR, AAAI, ICML, ICLR, KDD, SIGIR, WWW, and many others. In addition, a comprehensive list of datasets was compiled based on the selected survey papers and the identified challenges, competitions, and benchmarks. Relevant standards were identified mainly via research papers covered in this survey, the co-authors’ personal knowledge, and Google Web searches. For performance metrics, we covered those commonly used based on relevant standards, the survey papers, and the identified challenges, competitions, and benchmarks.In this survey, we focus on performance evaluation and comparison of deepfake generation and detection methods. The metrics used for such performance evaluations are at the core of our discussions. In this section, we review the performance metrics that are commonly used to evaluate deepfake generation and detection algorithms. Note that all metrics covered in this section are also commonly used for evaluating performance of similar systems that are not for generating or detecting deepfakes. Therefore, this section can be seen as a very brief tutorial on general performance metrics.In the last subsection, we also briefly discuss how the related performance metrics are covered in formal standards. By “formal standards”, we refer to standards defined following a formal procedure, often by one or more established standardisation bodies such as the International Organization for Standardization (ISO)5 and the International Electrotechnical Commission (IEC)6. Note that we consider a broad range of documents defined to be standards by standardisation bodies, e.g., International Telecommunication Union (ITU)7 recommendations and ISO technical reports (TRs).3.1      The Confusion MatrixDeepfake detection is primarily a binary classification problem. A binary classifier takes an input that is  or  and outputs a binary value denoting it to be  or . For example, a deepfake detection system will take a suspected image as the input that may be  or  and output  or .A fundamental tool used in evaluating a binary classifier is the  that summarises the success and failure of the classification model. On one axis are the two  values and on the other axis are the two  values. The classification is  (true positive and true negative) when the actual and the predicted values match. It is  (false positive and false negative) when the actual and predicted values do not match. Table 1 shows the confusion matrix for a binary deepfake classifier (detector). The two cells in green, TP (the number of ) and TN (the number of ), indicate correct prediction results, and the two cells in red, FN (the number of ) and FP (the number of ), indicate two different types of errors when making incorrect prediction results.\
Table 1: Confusion matrix for a binary classifier for detecting deepfake.|    | fake (predicted) | real (predicted) |
|----|----|----|
| fake (actual) | TP | FN |
| real (actual) | FP | TN |3.2      Precision and RecallBased on the four fundamental values introduced in Section 3.1, i.e., TP, TN, FP and FN, we define two important performance metrics for a binary classifier –  and .Precision of a binary classifier is defined as the fraction of  samples among all the . In the confusion matrix, it is the fraction of true samples in the first column. It can be formally defined as Eq. (1).When the “natural” ratio between positive and negative samples is significantly different from the test set, it is often useful to adjust the weight of the false positives, which leads to the  (wP) defined in Eq. (2), where  0 is a weight determined by the ratio between the negative and positive samples.Recall of a binary classifier is the fraction of  samples among the  samples, as shown in Eq. (3). In the confusion matrix, it is the fraction of true samples in the first row.Let us consider an example binary classifier that predicts if an image from a database containing both deepfake and real (authentic) images is fake or not. Precision of the classifier is the fraction of correctly classified images among all images classified as deepfake. On the other hand, recall is the fraction of deepfake images identified by the classifier, among all deepfake images in the database.3.3      True and False Positive RatesFocusing on predicted positive samples, we can also define two metrics:  (TPR), also called  (CDR), as the fraction of the predicted positive samples among the actually positive samples and  (FPR), also called  (FAR), as the fraction of the predicted positive samples among the actually negative samples, as shown in Eqs. (4) and(5). In the confusion matrix, TPR is the fraction of predicted positive samples in the first row and FPR is the fraction of predicted positive samples in the second row. Note that TPR is basically a different name for  (Eq. (3)).3.4     True and False Negative RatesSimilar to true and false positive rates, we can define two other rates focusing on negative predicted results:  (TNR) indicating the fraction of the predicted negative samples among the actually negative samples, and  (FNR) indicating the fraction of the predicted negative samples among the actually positive samples, as shown in Eqs. (6) and (7).3.5      Sensitivity and SpecificityIn some applications of binary classifiers, especially in biology and medicine, the TPR and the TNR are more commonly used, and they are often called  (TPR) and  (TNR). The focus of these two terms is on the two types of correctness of the predicted results. These are less used in deepfake-related research, hence, we will not refer to them in the remainder of this paper.Focusing on error rates means that we need to consider the FPR and the FNR. These two rates normally conflict with each other so that reducing one rate normally leads to an increase in the other. Therefore, rather than trying to reduce both error rates at the same time, which is normally impossible, the more realistic task in practical applications is to find the right balance so that they are both below an acceptable threshold.In some applications, such as biometrics, people are particularly interested in establishing the so-called  (EER) or  (CER), the point where the FPR and the FNR are equal. The EER/CER is not necessarily a good metric for some applications, especially when the two types of errors are of different levels of importance, e.g., for detecting critical deepfakes (e.g., fake news that can influence how people cast their votes) we can often tolerate more false positives (false alarms) than false negatives (missed alarms).3.7      Accuracy and F-ScoreIn addition to the EER/CER, there are also other metrics that try to reflect both types of errors, in order to give a more balanced indication of the overall performance of a binary classifier. The two most commonly used are  and  (also called ). Both metrics can be defined based on the four fundamental values (TP, TN, FP, and FN).Accuracy of a binary classifier is defined as the fraction of  samples (true positives and true negatives) among the total number of samples that have been classified, as shown in Eq. (8).The F-score of a binary classifier is actually a family of metrics. Its general form can be described based on a parameter  as defined in Eq. (9).The most widely used edition of all F-scores is the so-called , which is effectively the F-score with  = 1. More precisely, it is defined as shown in Eq. (10).3.8     Receiver Operating Characteristic Curve and Area Under CurveReceiver operating characteristic (ROC) curves are commonly used to measure the performance of binary classifiers that output a score (or probability) of prediction.Consider the following. Let  be the set of all test samples and let the output scores  () (for all  ∈ ) lie in the interval [] on the real line. Let  ∈ [] be a prediction threshold for the model, and assume that the classifiers works as follows for all  ∈ :\
It is easy to see that, for  = , all the samples will be classified as positive, leading to FN = TN = 0 so TPR = FPR = 1; while for  = , all the samples will be classified as negative, leading to FP = TP = 0 so TPR = FPR = 0. For other threshold values between  and , the values of TPR and FPR will normally be between 0 and 1. By changing  from  to  continuously, we can normally get a continuous curve that describes how the TPR and FPR values change from (0,0) to (1,1) on the 2D plane. This curve is the ROC curve of the binary classifier.For a random classifier, assuming that  () distributes uniformly on [] for the test set, we can mathematically derive its ROC curve being the TPR = FPR line, whose area under the ROC curve (AUC) is 0.5. For a binary classifier that performs better than a random predictor, we can also mathematically prove that its AUC is always higher than 0.5, with 1 being the best possible value. Note that no binary classifier can have an AUC below 0.5, since one can simply flip the prediction result to get a better predictor with an AUC of 1 − AUC. The relationship between the ROC and the AUC is graphically illustrated in Figure 1.Another widely used performance metric for binary classifiers that can return a probability score for the predicted label is . For a binary classification with a true label  ∈ {0*,* 1} and an estimated probability  = Pr( = 1), the log loss per sample is the negative log-likelihood of the classifier given the true label, defined as shown in Eq. (12).Given a testing set with  samples, the log loss score of a binary classifier can be calculated using Eq. (13), where  is 1 if the -th sample is true and 0 if false, and ˆ is the predicted probability of  = 1.3.10     Extension to Multi-class ClassifiersAll metrics that are defined based on the four basic values TP, TN, FP and FN can be easily extended to multi-class classification by considering the prediction to be true or false individually with respect to each class. For example, if the system is classifying animals (cats, dogs, horses, lions, tigers, etc.), then a true positive prediction of an image to be of a cat, would simultaneously be true negative predictions for the remaining classes (dogs, horses, lions, tigers, etc.). If an image of a cat is incorrectly predicted to be that of a dog, it would be a false negative with respect to a cat, a false positive with respect to a dog, and a true negative with respect to all other classes.3.11      Perceptual Quality Assessment (PQA) MetricsBy definition, the main goal of deepfakes is to make it hard or impossible for human consumers (listeners or viewers) to distinguish fake media from real media. Therefore, when evaluating the quality of deepfake media, the quality perceived by human consumers of the media is key. This calls for subjective assessment of the perceptual quality of the deepfake media as the “gold standard”. The most widely used subjective perceptual quality assessment (PQA) metric for audio-visual signals is  (MOS), which has been widely used by the signal processing and multimedia communication communities, including digital TV and other multimedia-related consumer applications. As its name implies, MOS is calculated by averaging the subjective scores given by a number of human judges, normally following a numerical scale between 1 and 5 or between 0 and 100. MOS has been used in some deepfake-related challenges (see Section 5.2) and also for evaluating and comparing the quality (realness/naturalness) of deepfake datasets (see Section 4.6).As a general subjective PQA metric, MOS has been standardised by the ITU8. There are also ITU standards defining more specific subjective Video Quality Assessment (VQA) metrics and the standard procedures one should follow to conduct VQA user studies, e.g., ITU-T Recommendation P.910 “Subjective video quality assessment methods for multimedia applications”9. Note that the ITU standards focus more on traditional perceptual quality, i.e., how good a signal looks or sounds, even if it looks or sounds not real (e.g., too smooth). On the other hand, for deepfakes, the focus is rather different because what matters is the realness and naturalness of the created media, i.e., how real and natural it looks or sounds, even if it is of low quality. To some extent, we can also consider realness and naturalness as a special aspect of perceptual quality.One major problem of subjective PQA metrics like MOS is the need to recruit human judges and to have a well-controlled physical testing environment and protocol, which are not easy for many applications. To help reduce the efforts and costs of conducting PQA-related user studies, various objective PQA metrics have been proposed, where the term “objective” refers to the fact that such metrics are human-free, i.e., automatically calculated following a computational algorithm or process. Depending on whether a reference exists, such objective PQA metrics can be largely split into three categories: full-reference (FR) metrics (when the original “perfect-quality” signal is available as the reference), reduced-reference (RR) metrics (when some features of the original “perfect-quality” signal are available as the reference), and no-reference (NR) metrics (when the original signal is unavailable or such an original signal does not exist). For deepfakes, normally NR or RR metrics are more meaningful because the “fake” part of the word means that part of the whole data does not exist in the real world, hence a full reference cannot be obtained. RR metrics are still relevant because deepfakes are often produced for a target’s specific attributes (e.g., face and voice), where the reduced reference will be such attributes. NR metrics will be useful to estimate the realness and naturalness of a deepfake, simulating how a human judge would rate it in a controlled subjective PQA user study.PQA is a very active research area and many PQA metrics have been proposed, some of which have been widely used in real-world products and services, e.g.,  (MSE), peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) for FR PQA of digitalimages and videos defined as in Eqs. (14), (15), and (16), respectively, where X = {xi} n i is the reference (the original signal), Y = {yi} n i is the signal whose visual quality is assessed, n is the number of pixels in X and Y , L is the maximum possible pixel value of X and Y (e.g., 255 for 8-bit gray-scale images), c1 = (k1L) 2 and c2 = (k2L) 2 ) are two stabilising parameters (k1 = 0.01 and k2 = 0.03 by default). For more about PQA metrics for different types of multimedia signals, we refer readers to some relevant surveys [3, 51, 72].3.12      More about StandardsMany of the basic performance metrics described in this section have been widely used by deepfake researchers as de facto standards, e.g., EER, log loss and MOS have been widely used in deepfake-related challenges (see Section 5). Also, the combination of precision, recall and F1-score has been widely used to assess performance of binary classifiers. While there have been a number of ITU standards on PQA to date, there does not seem to be many standardisation efforts on the performance metrics for evaluation of binary classifiers. This was the case until at least 2017, when ISO and IEC jointly set up the ISO/IEC JTC 1/SC 4210, a standardisation subcommittee (SC) focusing on AI under ISO/IEC JTC 111, the joint technical committee for standardising “information technology”.One recent effort that ISO/IEC JTC 1/SC 42 made is to produce the ISO/IEC TR 24029-1:2021 “Artificial Intelligence (AI) – Assessment of the robustness of neural networks – Part 1: Overview”12, a technical report (TR) that systematically covers many commonly used performance assessment concepts, methods and metrics. Although the technical report has “neural networks” in its title, most performance assessment concepts, methods and metrics included are common ones for all supervised machine learning models.In terms of performance metrics, two other ongoing work items of the ISO/IEC JTC 1/SC 42 that deserve attention are as follows:ISO/IEC DTS (Draft Technical Specification) 4213 “Information technology – Artificial Intelligence – Assessment of machine learning classification performance”13ISO/IEC AWI (Approved Work Item) TS (Technical Specifications) 5471 “Artificial intelligence – Quality evaluation guidelines for AI systems”14While the ISO/IEC JTC 1/SC 42 was created very recently, another standardisation subcommittee under ISO/IEC JTC1 has a much longer history of nearly 20 years: the ISO/IEC JTC 1/SC 3715 that focuses on biometrics-related technology. This standardisation subcommittee is highly relevant for deepfake since deepfake faces can be used to spoof biometrics-based user authentication systems. In this context, the following three standards are of particular relevance:ISO/IEC 19795-1:2021 “Information technology – Biometric performance testing and reporting – Part 1: Principles and framework”16: This standard covers general metrics about evaluating biometric systems. Two major metrics in this context are  (FAR) and  (FRR), which refer to the standard FPR and FNR, respectively. This standard also deprecates the use of single-number metrics including the EER and AUC (which were widely used in biometrics-related research in the past).ISO/IEC 30107-1:2016 “Information technology – Biometric presentation attack detec-tion – Part 1: Framework”17: This standard defines a general framework about presentation attack detection (PAD) mechanisms, where the term “” refers to the “presentation of an artefact or of human characteristics to a biometric capture subsystem in a fashion intended to in-terfere with system policy”. It focuses on biometric recognition systems, where a PAD mechanism is a binary classifier trying to predict presentation attacks (also called attack presentations, e.g., fake faces) as positive and bona fide (real) presentations as negative.ISO/IEC 30107-3:2017 “Information technology – Biometric presentation attack detection – Part 3: Testing and reporting”18: This standard defines a number of special performance metrics for evaluating PAD mechanisms standardised in the ISO/IEC 30107-1:2016. Three such metrics look at error rates: attack presentation classification error rate (APCER) referring to the standard FPR, normal/bona fide presentation classification error rate (NPCER/BPCER) referring to the standard FNR, and average classification error rate (ACER) that is defined as the average of the APCER and the NPCER/BPCER. Such metrics have been used in biometrics-related challenges such as Face Anti-spoofing (Presentation Attack Detection) Challenges19. When deepfake images or videos are used to spoof a biometric system, such standardised metrics will become relevant.This section provided a comprehensive summary of performance metrics used for evaluating and bench-marking binary classifiers. It is rare that all such metrics are used for a specific application. Instead, one or several are chosen based on specific needs. For a deepfake detection system as a binary classifier, many researchers have chosen to use overall metrics such as accuracy, AUC, EER and log loss, but the combination of precision, recall and F1-score is also common. Some deepfake-related challenges and competitions have introduced their own specific metrics, some of which will be described in Section 5. The use of different performance metrics can make comparison of different reported results more difficult, so we hope the expected new ISO/IEC standard particularly ISO/IEC 4213 will help.It is worth mentioning that, in addition to evaluating performance of deepfake detectors, the introduced performance metrics for evaluating binary classifiers can also be used to evaluate performance of deepfake generation methods by considering how deepfake detectors fail. For instance, organisers of the Voice Conversion Challenge 2018 and 2020 used this approach to benchmark how well voice conversion (VC) systems can generate high-quality fake speech samples.Another point we would like to mention is that for deepfake videos there are two levels of performance metrics: those at the frame level (metrics of each frame), and those at the video level (metrics for the whole video). Generally speaking, the latter can be obtained by averaging the former for all frames, potentially following an adaptive weighting scheme, so that more important (key) frames will be counted more.In this section, we cover all deepfake-related datasets we identified from the meta-review of deepfake-related survey papers, deepfake-related challenges, competitions and benchmarks covered, one online collection of deepfake-related datasets on GitHub20, and the co-authors’ personal collections. Table 2 shows basic information about these datasets. We explain them in four categories: deepfake image datasets, deepfake video datasets, deepfake audio/speech datasets, and hybrid deepfake datasets (mainly mixed image and video datasets).Note that many datasets of real (authentic) media were also used by deepfake researchers for two purposes. First, any detectors would need both fake and real media to demonstrate their performance. Second, real media have also been used to train deepfake generators as the training set. In this section, we include only datasets containing deepfake media, some of which contain both deepfake and real media.Some datasets, especially those created for deepfake-related challenges and competitions, have separate subsets for training and evaluation (testing) purposes. The split is necessary for such challenges and competitions, but not very useful for people who just want to use such datasets. Therefore, in this section when introducing such datasets we will ignore that level of details and focus on the total number of data including the number of real and fake samples.4.1      Deepfake Image DatasetsSwapMe and FaceSwap dataset [78]: This dataset contains 4,310 images, including 2,300 real images and 2,010 fake images created using FaceSwap21 and the SwapMe iOS app (now discontinued).Fake Faces in the Wild (FFW) dataset [32]: This dataset contains 131,500 face images, including 78,500 images extracted from 150 videos in the FaceForensics dataset and 53,000 images extracted from 150 fake videos collected from YouTube.generated.photos datasets22: This is a number of commercial datasets provided by the Generated Media, Inc., with up to nearly 2.7 million synthetic face images generated by StyleGAN. A free edition with 10,000 128x128 synthetic images is made available for academic research. The website also provides an interactive face generator23 and an API24. The generated.photos datasets have a good diversity: five age groups (infants, children, youth, adults, middle-aged), two genders (male and female), four ethnicities (white, black, Latino, Asian), four eye colours (brown, grey, blue, green), four hair colours (brown, black, blond, gray), three hair length (short, medium, long), facial expressions, three head poses (front facing, left facing, right facing), two emotions (joy and neutral), two face styles (natural, beautified). (According to a number of research papers we read, an earlier 100K-Faces dataset was released by generated.photos for academic research in 2018, which was used by many researchers. This dataset is not currently available any longer.) [1]: This dataset includes 19,457 face images, including 7,948 deepfake images generated from on 175 forged videos collected online and 11,509 real face images collected from various online sources. (Table 2 of the paper shows the dataset size is 19,509, but the dataset downloaded from pCloud contains just 19,457 images.) [30]: This dataset includes 100,000 synthesised face, bedroom, car and cat images by a GAN generator trained based on real images in the FFHQ25 and LSUN26 datasets (three object types – bedrooms, cars and cats – for the latter). Note that the name “100K-Generated-Images” was not a proper one as the authors [30] just used this to name a sub-folder of their Google Drive shared space, but it was used in one of the survey papers [65].Ding et al.’s swapped face dataset [17]: This dataset contains 420,053 images of celebrities, including 156,930 real ones downloaded using Google Image API and 263,123 fake face-swapped ones created using two different methods (Nirkin’s method and Auto-Encoder-GAN) [48]: This dataset includes 87,000 224x224 face images, generated by processing some StyleGAN-generated synthetic images using the GAN-fingerprint Removal approach (GANprintR) proposed by Neves et al.. It is the replaced version of the  dataset, which contains 150,000 face images generated using an earlier version of GANprintR. [21]: This dataset includes 40,000 images, half real and half deepfake. The images were collected from four sources: the CelebA-HQ dataset27, the Flickr-Faces-HQ dataset28, the 100K-Faces dataset29 (not available any longer, see the description of generated.photos datasets), and thisperson-doesnotexist.com. [75]: This dataset includes 625,537 synthesised face images of 10,177 celebrities, with 43 rich attributes on face, illumination, environment and spoof types. The real images were selected from the CelebA dataset30. The 43 attributes include 40 for real images, covering all facial components and accessories (e.g., skin, nose, eyes, eyebrows, lip, hair, hat, eyeglass), and 3 for fake images, covering spoof types, environments and illumination conditions.Diverse Fake Face Dataset (DFFD) [11]: This dataset contains 299,039 images, including 58,703 real images sampled from three datasets (FFHQ31, CelebA32 and FaceForensics++33) and 240,336 fake ones in four main facial manipulation types (identity swap, expression swap, attribute manipulation, and entire synthesis). The images cover two genders (male and female), a wide age groups (the majority between 21 and 50 years old), and both low- and high-quality levels.4.2     Deepfake Video Datasets [35]: This dataset contains 620 deepfake face videos, generated by face swapping without manipulation of audio, covering 32 subjects and two quality levels (high and low). (FF) [55]: This dataset contains 1,004 face videos with over 500,000 frames, covering various quality levels and two types of facial manipulation. This dataset is now replaced by the larger FaceForensics++ dataset (see below). (FF++) [56]: This dataset contains 5,000 face videos with over 1.8 million manipulated frames, including 1,000 real videos (with 509,914 frames) downloaded from YouTube, and 4,000 fake videos created using four face manipulation methods (Deepfakes, Face2Face, FaceSwap and NeuralTextures). The videos cover two genders (male and female), and three quality levels (VGA/480p, HD/720p, and FHD/1080p). [39]: This dataset contains 98 face videos, half (49) are real ones downloaded from Youtube, and the other half are fake ones generated using the FakeApp mobile application (which is now discontinued). The video dataset was created to used to demonstrate a deepfake video detection method based on detection of eye blinking behaviours, so all videos contain at least one eye-blinking event. All fake videos were created by swapping the original face in each of the real videos with the face of the actor Nicolas Cage34, thus, only one subject is represented. [10]: This dataset contains 142 “in the wild” deepfake portrait videos, collected from a range of online sources including news articles, online forums, mobile apps, and research presentations. The videos are diverse, covering the source generative model, resolution, compression, illumination, aspect-ratio, frame rate, motion, pose, cosmetics, occlusion, content, and context.DFDC (Deepfake Detection Challenge) preview dataset [18]: This dataset contains 5,244 face videos of 66 subjects with both face and voice manipulation. It was released as a preview of the full dataset of the 2020 Deepfake Detection Challenge (DFDC, see below).35: This dataset contains 1,203 face videos of celebrities, including 408 real videos collected from YouTube with subjects of different ages, ethic groups and genders, and 795 deepfake videos synthesised from these real videos. [40]: This dataset contains 6,229 face videos of celebrities, including 590 real videos collected from YouTube with subjects of different ages, ethic groups and genders, and 5,639 deepfake videos synthesised from these real videos.DeepFake Detection (DFD) Dataset [20]: This dataset contains 3,363 face videos, covering 28 subjects, gender, and skin colour. It was created as a joint effort between two units of Google, Inc.: Google AI36 and JigSaw37. [27]: This dataset contains 60,000 indoor face videos (with 17.6 million frames) generated by face swapping, covering 100 subjects, four skin tones (white, black, yellow, brown), two gen-ders (male and female), different age groups (20-45), 26 nationalities, 7 different angles, 8 face expressions, and different head poses.DFDC (Deepfake Detection Challenge) full dataset [18]: This dataset contains 128,154 face videos of 960 subjects, including 23,654 real videos from 3,426 paid actors and 104,500 deepfake videos created using eight different methods (DF-128, DF-256, MM/NN face swap, NTH, FSGAN, StyleGAN, refinement, and audio swap).10(Face Forensics in the Wild) dataset [79]: This dataset contains 10,000 high-quality forgery videos, with video- and face-level annotations. The dataset focuses on a more challenging case for forgery detection: each video involves one to 15 individuals, but only some (a minority of) faces are manipulated.Korean DeepFake Detection Dataset (KoDF) [36]: This dataset contains 37,942 videos of paid subjects (395 Koreans and 8 Southeastern Asians), including 62,166 real videos and 175,776 fake ones created using six methods – FaceSwap, DeepFaceLab, FSGAN, First Order Motion Model (FOMM), Audio-driven Talking Face HeadPose (ATFHP) and Wav2Lip. The videos cover a balanced gender ratio and a wide range of age groups. [23]: This dataset contains 1,737 videos with 1,666,816 frames, including 1,339,843 real frames and 326,973 fake frames generated using the Deep Video Portraits (DVP) [34] method. The original videos were obtained from three sources: the dataset used in [33], the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) [42], and YouTube. Most videos have a resolution of 1280×720. [81]: This dataset contains 7,314 face sequences extracted from 707 deepfake videos that were collected completely from the Internet. It covers diverse scenes, multiple persons in each scene and rich facial expressions. Different from other deepfake video datasets, WildDeepfake contains only face sequences not the full videos. This makes the dataset more like between an image dataset and a video one. We decided to keep it in the video category since the selection process was still more video-focused.4.3     Deepfake Audio/Speech DatasetsVoice conversion (VC) is a technology that can be used to modify an audio and speech sample so that it appears as if spoken by a different (target) person than the original (source) speaker. Obviously, it can be used to generate deepfake audio/speech samples. The biennial Voice Conversion Challenge38 that started in 2016 is a major challenge series on VC. Datasets released from this challenge series are very different from other deepfake datasets: the deepfake data is not included in the original dataset created by the organisers of each challenge, but in the participant submissions (which are retargeted/fake utterances produced by VC systems built by participants). The challenge datasets also include the evaluation (listening-based) results of all submissions. Some fake utterances may be produced by DL-based VC systems, so we consider all datasets from this challenge series relevant for our purpose of this survey.Voice Conversion Challenge 2016 database [62]: The original dataset created by the challenge organisers was derived from the DAPS (Device and Produced Speech) Dataset [47]. It contains 216 utterances (162 for training and 54 for testing) per speaker from 10 speakers. Participating teams (17) developed their own VC systems for all 25 source-target speaker pairs, and then submitted generated utterances for evaluation. At least six participating teams used DL-related techniques (LSTM, DNN) in their VC systems (see Table 2 of the result analysis paper39), so the submitted utterances can certainly be considered deepfakes.Voice Conversion Challenge 2018 database [44]: The original dataset created by the challenge organisers was also based on the DAPS dataset. It contains 116 utterances (81 for training and 35 for testing) per speaker from 12 speakers in two different tasks (called Hub and Spoke). Participating teams (23 in total, all for Hub and 11 for Spoke) developed their own VC systems for all 16 source-target speaker pairs, and then submitted generated utterances for evaluation. Comparing with the 2016 challenge, more participating teams used DL-related techniques (e.g., WaveNet, LSTM, DNN, CycleGAN, DRM – deep relational models, and ARBM – adaptive restricted Boltzmann machines) in their VC systems.Voice Conversion Challenge 2020 database [70]: This dataset is based on the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset40, a bilingual (Finnish/English, German/English, and Mandarin/English) database. It contains 145 utterances (120 for training and 25 for testing) per speaker from 14 speakers for two different tasks (with 4 × 4 and 4 × 6 source-target speaker pairs, respectively). Participating teams (33 in total, out of which 31 for Task 1 and 28 for Task 2) developed their own VC systems for all source-target speaker pairs, and then submitted generated utterances for evaluation. Comparing with the 2018 challenge, DL-based VC systems were overwhelmingly used by almost all participating teams (WaveNet and WaveGAN among the most used DL-based building blocks).A major set of deepfake speech datasets were created for the  (Automatic Speaker Verification Spoofing and Countermeasures) Challenge41 (2015-2021, held biannually). The datasets for the 2019 and 2021 contain speech data that can be considered deepfakes.ASVspoof 2019 Challenge database [67]: This dataset is based on the Voice Cloning Toolkit (VCTK) corpus42, a multi-speaker English speech database captured from 107 speakers (46 males and 61 females). Two attack scenarios were considered: logical access (LA) involving spoofed (synthetic or converted) speech, and physical access (PA) involving replay attacks of previously recorded bona fide recordings). For our purpose in this survey, the LA scenario is more relevant. The LA part of the dataset includes 12,483 bona fide (real) utterances and 108,978 spoofed utterances. Some of the spoofed speech data for the LA scenario were produced using a generative model involving DL-based techniques such as long short-term memory (LSTM)43, WaveNet [50], WaveRNN [28], WaveCycleGAN2 [58]. Note that the challenge organisers did not use the term “deepfake” explicitly, despite the fact that the DL-generated spoofed speech data can be considered as deepfakes.ASVspoof 2021 Challenge – Logical Access Database [14]: This dataset contains bona fide and spoofed speech data for the logical access (LA) task. The challenge is still ongoing and we did not find a detailed paper on the dataset, so cannot include more details other than its size (7.8 GB after compression). Although we did not see details of the generative algorithms used to produce spoofed speech data, we believe similar DL-based algorithms were used like for the 2019 challenge.ASVspoof 2021 Challenge – Speech Deepfake Database [15]: In 2021, the challenge included an explicitly defined track on deepfake, but the task description suggests that the organisers of the challenge considered a broader definition of the term “deepfake” by looking at spoofing human listeners rather than ASV (Automatic Speaker Verification) systems. The size of the dataset is 34.5 GB after compression.Possibly because of the long history and wide participation of the community in the ASVspoof challenges for creating the dedicated datasets, there are very few other deepfake audio/speech datasets. One such dataset was created by a group of researcher from Baidu Research [5]. This dataset was created to demonstrate a proposed voice cloning method. It is relatively small, and contains 134 utterances, including 10 real ones, 120 cloned ones, and 4 manipulated ones. Another dataset was created by Google AI and Google News Initiative44, but it was made part of the ASVspoof 2019 dataset. This dataset contains thousands of phrases spoken by 68 synthetic “voices” covering a variety of regional accents.4.4     Hybrid Deepfake DatasetsNIST OpenMFC (Open Media Forensics Challenge) Datasets45: These datasets were created by the DARPA Media Forensics (MediFor) Program46 for the 2020 OpenMFC47. There are two GAN-generated deepfake datasets, one with more than 1,000 deepfake images and the other with over 100 deepfake videos. The datasets were made available to registered participants of the competition only. [25]: This dataset is named as “a versatile benchmark for comprehensive forgery analysis”. It contains 2,896,062 images and 221,247 videos, including 1,457,861 fake images and 121,617 fake videos. The videos and images cover seven image-level and eight video-level manipulation approaches, 36 different types of perturbations and more mixed perturbations, and a large number of annotation labels (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). The dataset is being used for supporting the Face Forgery Analysis Challenge 202148 at the SenseHuman 2021 (3rd Workshop on Sensing, Understanding and Synthesizing Humans)49, co-located at the ICCV 2021 conference50.4.5      A Deepfake Dataset Generator [74]: This is not actually a dataset per se, but a system for producing large datasets more automatically, including generating deepfake datasets. One may argue the automatically generated datasets are fake since they are not produced from real-world scenes.4.6     Subjective Quality of Deepfakes in Different DatabasesAs mentioned in Section 4.7, subjective quality evaluation is necessary to evaluate the realness, realisticness, and naturalness of deepfake media. While there has been very limited work on this topic, in 2020, Jiang et al. [27] conducted a user study on realness of deepfake videos. They recruited 100 professional participants (most of whom are computer vision researchers), who were asked to evaluate the realness of 30 randomly selected videos from 7 deepfake video datasets (DeeperForensics-1.0, UADFV, DeepFake-TIMIT, Celeb-DF, FaceForensics++, Deep Fake Detection, and DFDC). Participants were asked to respond to the statement “The video clip looks real.” and gave scores following a five-point Likert scale (1 – clearly disagree, 2 – weakly disagree, 3 – borderline, 4 – weakly agree, 5 – clearly agree).Table 3 shows the results. Interestingly, we can see a huge difference between the realness levels of different datasets. What is probably quite surprising is that FaceForensics++, one of the most widely used deepfake datasets, has a very low MOS score and less than 9% of participants considered the 30 selected videos as real.Table 3: Human-judged subjective quality (realness) of deepfake videos in 7 datasets. The MOS scores were not reported by Jiang et al., but calculated by us based on the raw data shown in Table 3 of [27].4.7      Discussion: DatasetsAmong all deepfake image and video datasets, a significant majority are about face images and videos. This is not surprising since face swapping, face attribution manipulation, and fully synthesised face images are among the hottest topics within deepfake research and real-world applications. We hope more non-face deepfake image and video datasets can be produced to support a broader range of research activities on deepfake.The subjective quality results shown in Table 3 indicate that it is important to check realness of deep-fake media to support any performance evaluation or comparison. To ensure that the quality evaluation of datasets is fair, transparent and reliable, standard procedures need defining and a common pool of qualified human experts should be used.Many authors of deepfake-related datasets attempted to classify such datasets into different generations. Chronologically speaking, we could broadly split such datasets into two generations: before 2019 and since 2019. Typically, datasets created before 2019 are relatively less advanced and smaller, while those created after 2019 tend to be larger, more diverse (i.e., covering more attributes), and of higher quality (i.e., produced by more advanced generative models). This can also be seen from the data in Table 3, in which the top two datasets (DeeperForensics-1 and Celeb-DF) fall within the new generation (2020), while others belong to the old generation. In addition to the two generations, a newer generation has also emerged in 2021: a number of very recent datasets started focusing on more realistic deepfakes (i.e., in the wild) or more specified areas of deepfakes (e.g., 10 focusing on multiple faces in the same video, and KoDF focusing on Korean faces). This trend shows that the deepfake research community has grown significantly in the past few years so that narrower topics have also started gaining attention and interest from some researchers.This section reviews initiatives aiming to advance the state-of-the-art of detection and generation of synthetic or manipulated media (such as video, image and audio) via competitions or challenges open to the public, and ongoing benchmarks tackling specific problems.The Deepfake Detection Challenge (DFDC)51 was an initiative promoted by an AI and Media Steering Committee52, including BBC, Facebook, Amazon, Microsoft and New York Times, and some universities around the world including the University of Oxford. The competition remained open from 5 September 2019 till 31 March 2020, and involved 3 stages. At first, the DFDC preview dataset was released. At a later stage, the DFDC full dataset was also made available to the 2,114 participants of the competition incorporating face and audio swap techniques for generation of deepfake content. At the final stage, the submitted models were evaluated using a test dataset (referred to as the “black box dataset”) of 10,000 videos which included  deepfake videos. The best performance on the black box dataset had an accuracy of 65.18%, according to the released results [22]. Submissions were ranked53 according to the overall log loss score, as defined in Eq. (13). All top five ranked models (the winner had the lowest overall log loss) are available on GitHub. Results indicate how challenging the detection of deepfake is since the best accuracy was low and “many submissions were simply random”, according to Dolhansky et al. [19]. Figure 2 shows a screenshot of the leaderboard with the five finalists. The first top ranked model used MTCNN (Multi-tasked Cascaded Convolutional Network), the second used WS-DAN (Weakly Supervised Data Augumentation Network), and the third used the EfficientNetB7 architecture. Meta compiling the common themes observed in the winning models, they were: clever augmentations, architectures, and absence of forensics methods. Moving forward, they called for “solutions that go beyond analysing images and video. Considering context, provenance, and other signals may be the way to improve deepfake detection models”.\
The Automatic Speaker Verification Spoofing And Countermeasures Challenge Workshop (ASVspoof)54 has been running biennially since 2015. This competition is organised by an international consortium that includes Inria and EURECOM (France), University of Eastern Finland, National Institute of Informatics (Japan), and Institute for Infocomm Research (Singapore). This year the ASVspoof challenge includes, for the first time, a sub-challenge focused on  where the envisioned use case is an adversary trying to fool a human listener. The metric used for evaluating performance of submitted solutions (i.e., classifiers) is EER. Four baseline solutions55 (also called “countermeasures”), each using a different technique, were made available to participants with their corresponding EER metric values. The ASVspoof 2021 Speech Deepfake Database containing audio recordings with original and spoofed utterances has also been made available. The competition involves three phases56: a progress phase, an evaluation phase and a post-evaluation phase; it is unclear how teams move from one phase to the next. More information about the 2021 competition is available in the published evaluation plan [13]. The organisers of the competition noted that they opted for the EER as the performance evaluation met-ric for countermeasures submitted to the speech deepfake task for legacy reasons. They acknowledged, however, that “EER reporting is deprecated ” by the ISO/IEC 19795-1:202157 standard. Despite the fact that only the 2021 ASVspoof competition contained a track explicitly related to deepfake, some data in the ASVspoof 2019 dataset (Logical Access task) used for the 2019 competition was generated using DL-based algorithms as mentioned in Section 4. We expect that this also holds for the ASVspoof 2021 dataset (Logical Access task). The ASVspoof 2019 competition used the EER as secondary metric; the primary performance metric used was the tandem detection cost function (t-DCF) [63]. According to its evaluation plan [69], t-DCF assesses the performance of the whole tandem system whereby “a CM [countermeasure] serves as a ‘gate’ to determine whether a given speech input originates from a bona fide (genuine) user, before passing it the main biometric verifier (the ASV system)”. It is calculated according to Eq. (17), where  cm () and  cm() are, respectively, “the miss rate and the false alarm rate of the CM system at threshold s”.For further information about Eq. (17), including constants 1 and 2, please refer to the ASVspoof 2019 evaluation plan [69].An implementation of the t-DCF metric has been made available by the ASVspoof 2019’s organisers in Python58 and Matlab59 formats.The Face Anti-spoofing (Presentation Attack Detection) Challenge60 started in 2019. Its first two editions were held at the 2019 and 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), respectively. Its third edition was moved to be co-located with the 2021 IEEE/CVF International Conference on Computer Vision (ICCV 2021). This competition series was organised by a group of researchers from academia and industry in China, Mexico, Spain, Finland and the US. The 2021 competition was focused on 3D high-fidelity mask attacks, and followed a 2-phased61 process. The first phase is the “development phase”; it started in April 2021 when the CASIA-SURF HiFiMask dataset62 was released to participants. The second phase is the “final ranking phase” (June 2021), when the competition ended. The competition adopted the following performance metrics for evaluation63 of the solutions submitted: attack presentation classification error rate (APCER), normal/bona fide presentation classification error rate (NPCER/BPCER), and average classification error rate (ACER), in accordance with the ISO/IEC 30107-3:201764 standard. Figure 3 provides the leaderboard for the top three solutions.\
The FaceForensics Benchmark65 is an ongoing automated benchmark for detection of face manipulation. The organisers of the benchmark made the FaceForensics++ dataset available for training. Manipulated videos (4,000 in total) were created using four techniques, i.e., two computer graphics-based approaches (Face2Face and FaceSwap) and two learning-based approaches (DeepFakes and Neural Textures). The deepfakes videos were generated using a slightly modified version of FaceSwap66, and the Neural Textures videos were created using the approach proposed by Thies et al. [61]. The benchmark test dataset is created from the collection of 1,000 images randomly selected from either the manipulation methods or the original videos [56]. Participants have to submit results to the benchmark, rather then code like other competitions; this is illustrated in Figure 4a. The outcome of a submission is illustrated in Figure 4b, where the scores are a measure of accuracy (Eq. (8)).\
The Open Media Forensics Challenge (OpenMFC, formerly DARPA MFC)67 is an annual image and video forensics evaluation aiming to facilitate development of multimedia manipulation detection systems. It has been organised annually68 starting from 2017 under the name of DARPA MFC. In 2020, the National Institute of Standards and Technology (NIST) initiated the  as a new evaluation platform, based on their previous experiences with the DARPA MFC series, to make the participation more convenient for all researchers. In OpenMFC 2020, two deepfake-related tasks were included for the first time: Image GAN Manipulation Detection (IGMD) and Video GAN Manipulation Detection (VGMD). The organisers provided an image evaluation dataset for the IGMD task, containing 1,000 images from over 200 image journals69, and a video evaluation dataset for the VGMD task, including over 100 test videos. Furthermore, they provided the datasets70 used in the previous MFC challenges as development datasets. The challenge is composed of two main phases for development and evaluation, respectively, and a pre-challenge phase for quality control testing. For evaluation of submissions, AUC-ROC is used as the primary metric. Furthermore, CDR@FAR, where CDR refers to correct detection rate or TPR (Eq. (4)) and FAR refers to false alarm rate or FPR (Eq. (5)), is also used as a metric [49]. The DeeperForensics Challenge 202071 is a deepfake face detection challenge held at the 2020 ECCVSenseHuman Workshop72. The challenge used the DeeperForensics1.0 dataset.The organisers provided a hidden test dataset to better simulate real-world scenarios. The challenge involved two phases: the “development phase” that started in August 2020 allowing 100 successful sub-missions, and the “final test phase” that started in October 2020 allowing 2 successful submissions until the end of the month. The submissions were evaluated using the binary cross-entropy loss (BCELoss) metric, calculated according to Eq. (18), where  is the number of videos in the hidden test set,  is the ground truth label of video  (fake:1, real:0), and () is the predicted probability that video  is fake.Results73 of the competition were discussed by Jiang et al. [26]. The top solution used three models, i.e., EfficientNet-B0, EfficientNet-B1 and EfficientNet-B2, for classification. The second top used EfficientNet-B5 for both an image-based model and a video-based model. The third ranked solution used a 3D convolutional neural network (3DCNN).\
The Face Forgery Analysis Challenge 202174 is a competition hosted at the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021). It is organised by researchers from a number of organisations in China including universities and SenseTime Research (the research arm of SenseTime75, one of the major AI “unicorns” in China). The challenge aims to advance the state-of-the-art in detection of photo-realistic manipulation of images and videos. Participants are able to use a large annotated face dataset (i.e., the ForgeryNet dataset) that was obtained by applying a number of techniques for manipulation (15) and perturbation (36) to train their solutions. The phases comprise of Forgery Image Analysis, Forgery Video Analysis, Forgery Video Temporal Localization phases, and the final phase (i.e., “private test”) where participants’ models will be tested against an unseen dataset. The following metrics will be used [25]: AUC, average precision (AP) at some “temporal Intersection over Union” (AP@tIoU) compared to a threshold  ∈ [0*.,* 0*.*95], and average recall (AR) at  (AR@) where  is the top  labels returned for multi-class classifiers.The 2020 CelebA-Spoof Face Anti-Spoofing Challenge76 was hosted at the 16 European Conference on Computer Vision (ECCV 2020). The challenge ran between August and October 2020, and aimed to advance the state-of-the-art in detecting “whether a presented face is live or spoof ” [76]. The organisers made the face CelebA-Spoof dataset available for the competition containing rich annotation across a range of attributes. The competition only had one phase where participants submitted their solutions to be evaluated against a test dataset; the spoof class was considered as “positive” and the live class as “negative”. Metric TPR@FPR was used and collected at three points where the TPR when FPR = 104 determined the final ranking. The top three finalists (see Figure 5) used deep learning models ResNet, EfficientNet-B7, and a novel architecture combining Central Difference Convolutional Networks (CDCN) and Dual Attention Network (DAN). The two top ranked solutions used different strategies to boost their models’ performance: a heuristic voting scheme was used by the top-ranked solution, and a weight-after-sorting strategy was used by the second ranked solution.The 2021 CSIG Challenge77 is the second edition of a challenge organised by the China Society of Image and Graphics78. The 2021 challenge has the Fake Media Forensic Challenge79 as its 6 track, co-organised by CSIG’s Digital Media Forensics and Security Technical Committee80 and Institute of Information Engineering, Chinese Academy of Sciences81. This track has two tasks, one on deepfake video detection, and the other on deepfake audio/speech detection. For the deepfake video detection task, the dataset used contains a public training set with 10,000 sound-free face videos (including 4,000 fake videos), a public test set with 20,000 face videos (the percentage of deepfake videos is unknown to participants), and a private test set that will be determined and used at the final session for selecting the winners. All videos contain faces of Eastern Asian people, and cover a wide range of parameters such as multiple resolutions and encoding quality factors, the use of blurring or sharpening filters, and added noise. Deepfake videos were created using public tools including DeepFaceLab [53], Faceswap82, Faceswap-GAN, Recycle-GAN [6] and ALAE (Adversarial Latent Autoencoders) [54]. For the deepfake audio/speech detection task, the dataset used contains a public training set with 10,000 speech samples (including 6,000 fake ones), a public test set with 20,000 face videos (the percentage of deepfake videos is unknown to participants), and a private test set for the final session (the same as the deepfake video detection task). The tools used for generating the fake speech samples include TTS (text-to-speech) voice synthesis tools and VC (voice conversion) tools. The main TTS tools used include open-source tools such as DeepVoice, TensorFlowTTS83 and GAN-TTS [8] and commercial software tools such as those from iFlytek84 and IBM. The main VC tools used include Adaptive-VC and CycleGAN-VC [29]. For both deepfake detection tasks, the performance metric used is log loss.2020 China Artificial Intelligence85 was the second edition of a Chinese AI competition open for the general public to participate, organised by the municipal government of the City of Xiamen in China. In 2020, it had two sub-competitions, Multimedia Information Recognition Technology Competition86 and Language and Knowledge Technology Competition87. The Multimedia Information Recognition Technology Competition included two tasks on deepfakes: one on deepfake video detection88 and one on deepfake audio/speech detection89. The deepfake video detection task used 3,000 videos, and log loss was used as the sole performance metric. The deepfake audio/speech detection task used 20,000 audio samples (mostly in Chinese, and the remaining in English), and EER was used as the sole performance metric. For both tasks, the ratio between real and deepfake samples was 1:1. We did not find where to download the datasets used for the tasks nor a more detailed technical description of the datasets. For the deepfake video detection tasks, the top two winning teams (with an A prize) were from Netease (Hangzhou) Network Co., Ltd. and Beijing RealAI Technology Co., Ltd., followed by three other teams winning a B prize: Xiamen Fuyun Information Technology Co., Ltd.; Institute of Computing Technology, Chinese Academy of Sciences; and Wuhan Daqian Information Technology Co., Ltd. For the deepfake audio/speech task, there was no team winning an A prize, but one team winning a B prize: SpeakIn Technologies Co., Ltd. The final results of some teams were published, but some teams were allowed to hide their results. We did not find a detailed technical report summarising the results and explaining the work of the winning teams.One of the B-prize winning team is from Beijing RealAI Technology Co., Ltd., a Chinese company active in deepfake-related R&D.The Voice Conversion Challenge90 is a biennial competition that has been running since 2016. The challenge and the corresponding workshop, hosted at the INTERSPEECH conference91, is supported by the SynSig (Speech Synthesis Special Interest Group)92 of the International Speech Communication Association (ISCA)93. Its aim is to promote progress in voice conversion (VC) technology that can be applied to a number of positive and negative use cases, such as spoofing voice biometric systems. The 2020 challenge focused on speaker conversion, a sub-problem of VC, and included two tasks. For the first task “intra-lingual semi-parallel voice conversion”, participants had to develop 16 VC systems (speaker-pair combinations) including male and female speakers and English sentences, using the provided Voice Conversion Challenge 2020 database v1.0 for training (refer to Section 4). For the second task “cross-lingual voice conversion”, participants had to develop 24 VC systems, also including male and female speakers, but uttering sentences in three languages (Finnish, German and Mandarin), based on the provided training dataset. Figure 6 illustrates the process of training and generation of VC systems.Submissions were evaluated for “perceived naturalness and similarity through listening tests”94. As such, the organisers used  [70] and recruited both native and non-native English speakers (i.e., Japanese native speakers) via crowd-sourcing for the listening tests. Naturalness (answering the question “How natural does the converted voice sound? ”) was measured using the metric MOS (covered in Section 4.6), and similarity (answering the question “how similar the converted voice sound comparing source and target speakers? ”) was measured in terms of speaker recognition as “same” or “different”, as elaborated by Wester et al. [68]. Tests also focused on the effects of language differences on the performance of VC systems submitted to the competition. The most popular CNN/RNN/GANbased VC systems submitted used WaveNet, WaveRNN, and Parallel WaveGAN. Results indicated that, in terms of similarity, the best performing VC systems were as good as natural speech but none reached human-level naturalness for task 1; scores were lower for task 2 which was more complex [70]. The organisers of the 2020 competition also used objective evaluation [12]. The metrics used for evaluation of speaker similarity were: equal error rate (EER), false acceptance rate of target (P tar fa ), miss rate of source (P src miss), and cosine similarity of speaker embedding vectors (cos-sim) according to Eq. (19) where A is the speaker embedding vectors for the converter audio and B is the speaker embedding vectors for the original audio. The performance of the VC systems as a spoof countermeasure was also evaluated using EER, while to evaluate the quality of the subjective MOS obtained via listening tests, a DL-based model to predict MOS, called MOSNet [43], was used. Lastly, to evaluate intelligibility of the converted transcribed speech, in comparison with the original transcribed speech, the word error rate (WER) [4] was used. WER is calculated according to Eq. (20) where I refers to insertions, D refers to deletions, S refers to substitutions, and N refers to the total number of words in the original transcript.The Deepfake Africa Challenge (2021)95 is a new initiative of the AI Africa Expo, in partnership with a film and media production company (Wesgro) and the African Data Science competition platform Zindi. Its aim is “to create convincing deepfakes to highlight the power of this synthetic media, illustrating its creative potential for exploitation for both positive and negative outcomes and focusing debate about its ethical use / misuse in an African context ”. Eligible participants were required to be citizens and residents of the African continent. Submissions, accepted up to end of July 2021, can be either video or audio. Evaluation of submissions is defined in terms of artistic creativity, relevance of challenge topic, and innovation in the process of generation as long as participants use tools and packages publicly available. The top three finalists will receive a prize, present their work at the Expo, and will have to grant copyrights to Zindi. Unlike the other competitions reviewed in this section, which were focused on advancing the state-of-the-art in detection of synthetic or manipulated media, this competition focused on the generation of deepfake which seems more humanities-centred. This is a trend observed in arts [31] and culture [57].5.3      Generation and Detection of Manipulated MediaThe DeepFake Game Competition (DFGC)96 is in its first edition, hosted at the 2021 International Joint Conference on Biometrics (IJCB 2021). Its organisers are mainly from the Institute of Automation Chinese Academy of Sciences (CASIA). The idea of the competition was to promote an adversarial game between agents pushing for advances in both deepfake creation and detection. In order to achieve this, a 6-stage protocol was designed interleaving three creation phase (C-phase) and detection phase (D-phase), typically one week apart; submissions closed in April 2021. Both C-phases and D-phases were bound to the Celeb-DF (v2) dataset [40], containing 6,229 videos (590 real/original videos and 5,639 fake/manipulated videos), for training purposes. As such, submissions to a C-phase would consist of datasets extracted from Celeb-DF (v2) which included novel face-swap approaches to obtain evaluation results. Submissions to a D-phase would consist of detection models/codes to obtain evaluation results. The models submitted for a D-phase were evaluated against the datasets submitted for the previous C-phase [52]. The metrics used for evaluation97 were: a detection score, used for evaluation of a D-phase, and a creation score, used for evaluation of a C-phase. The top three finalists for the detection phase employed CNN-based classifiers EfficientNet-B3, Efficientnet-B0 and EfficientNetV2.The Detection Score () metric captures the models’ ability to correctly classify fake images submitted to the previous C-phase against a set of real images in the CelebDF test dataset. It is calculated using Eq. (21), where  is the number of valid submissions of created synthesis test sets in the last C-phase.The Creation Score () metric used to evaluate creation models submitted to this challenge is calculated by Eq. (22), where  is the number of valid submissions of detection methods in the last D-phase, the noise score (noise) penalises noisy images, the other three parts of the equation relate to the following98: “ID level similarity to the donor ID, image level similarity to the target frame, and the deception ability against detection models. ID level similarity is scored by a face recognition model using dot product of two ID features (fake face ID and donor ID). The image level similarity is scored by SSIM [Structural Similarity Index] to make sure the face-swapped image is similar to the corresponding target image in content and quality ”.Peng et al. [52] observed a commonality between the three winning teams for the creation task, i.e., the use of the FaceShifter [37] framework for face swapping. They highlighted two overall reflections about the competition: (1) the limited diversity of the deepfake datasets submitted and the use of repetitive methods to generate them, and (2) the limited size of the Celeb-DF (v2) dataset itself flagging the need for a larger dataset for next year’s competition. The organisers of the competition also applied the top two detection models to unseen datasets (DFDC and FaceForensics++) and noticed that they do not generalise well.This section presents a meta-review of 12 selected deepfake-related survey papers, including eight published in English [16, 45, 46, 64–66, 71, 73] and four published in Chinese [7, 38, 41, 59]. It covers the following aspects in a systematic manner: definitions and scope, performance metrics, datasets, challenges/competitions/benchmarks, performance comparison, key challenges and recommendations.The meta-review aims at drawing some high-level insights for monitoring future development of deepfake-related technologies and their applications.6.1      Definitions and ScopeAs we discussed in Section 1.1, among researchers, practitioners and law makers there is no universally accepted definition of “deepfake” as a term. This is also reflected in how the authors of the 12 survey papers considered this aspect. Most authors talked about the history of deepfakes and pointed out that the term reflects the combination of “deep learning” and “fake”, but some used a broader definition, e.g., Lyu [45] defined deepfake as “high quality fake videos and audios generated by AI algorithms”. Some authors also referred to deepfake-related legislations, but none of them pointed out that the definitions in some such legislations are completely different from the more technical definitions involving the use of deep learning. No authors discussed the blurred boundary between deepfakes and non-deepfakes, although some surveys actually cover both, e.g., Tao et al. [59] focused on speech forgery and did not explicitly highlight “deepfake”.In terms of the scope, while some authors (correctly) considered all types of media that can be produced by deepfake-related techniques [38, 41, 45, 65], some considered only a narrow scope, e.g., authors of [7, 64, 71, 73] considered only videos, and only authors of [16, 66] have considered images and videos. Another phenomenon we observed is that many authors focused more on face images and videos, and authors of three surveys [16, 64, 71] even limited the definition of “deepfake” to such a narrow scope:Deshmukh and Wankhade [16] defined it as “a technology which creates fake images or videos of targeted humans by swapping their faces [by] another character saying or doing things that are not absolutely done by them and humans start believing in such fake as it is not always recognisable with the everyday human eye”;Younus and Hasan [71] considered deepfake as a technique allowing “any computer user to exchange the face of one person with another digitally in any video”; andTolosana et al. [64] defined it as “a deep learning based technique able to create fake videos by swapping the face of a person by the face of another person”.Such unnecessarily narrow definitions and scopes can lead to confusion and do not help exchanges between researchers and practitioners working on different types of deepfakes.We call on more researchers to accept a broader definition of “deepfake” so that highly realistic/natural media of any kind generated by a sophisticated automated method (often AI-based) is considered deepfake. Here, we provide two examples of such a broader definition: the image2image (or pixel2pixel) technique [80] that allows the production of deepfake images and videos of any objects (e.g., the “horse2zebra” deepfake image shown in Figure 7), and the the so-called “deepfake geography [77]”, where AI-based techniques are used to generate realistic-looking satellite images.\
Another important fact missed or not sufficiently discussed by authors of all the 12 surveys is that deepfake techniques can be used for positive applications, e.g., creative arts, entertainment and protecting online users’ privacy. We call for more researchers and practitioners to follow the proposal in the 2020 Tencent AI White Paper [60] to start using the more neutral-sounding term “deep synthesis”. Accordingly,we can use different words for different types of data generated using “deep synthesis” techniques, e.g., “deep art”, “deep animation”, “deep music”, and “deepfake”. While authors of the 12 survey papers did not recognise the positive applications of “deepfake” technologies, some other researchers did, e.g., organisers of the Voice Conversion Challenge 202099 who said the VC technology (for speech deepfake) “is useful in many applications, such as customizing audio book and avatar voices, dubbing, movie industry, teleconferencing, singing voice modification, voice restoration after surgery, and cloning of voices of historical persons”.Surprisingly, none of the 12 surveys have covered performance metrics explicitly. Some directly used performance metrics to explain and compare performance of covered deepfake generation and detection methods. The most used performance metrics include accuracy, ERR, and AUC. This may be explained by the page constraints of such survey papers, which did not allow the authors to extend their coverage significantly to cover performance metrics systematically. The subjective quality of deepfakes is an area least covered by the surveys, which seems related to an unbalanced coverage on deepfake generation and deepfake detection in terms of performance evaluation and comparison (the former much less than the latter).Many of the 12 survey papers list a number of deepfake-related datasets, but none of them have coverage as complete as ours shown in Section 4. For instance, none of the surveys have covered the Voice Conversion Challenge 2016/2018/2020 datasets and the ASVspoof 2019/2021 datasets are covered briefly only in two surveys [38, 59]. In addition, more recent deepfake datasets especially those released in 2021 are also not covered by any of the surveys. We believe that our Section 4 is the most comprehensive review of deepfake-related datasets so far.Some survey papers include datasets that are likely deepfakes, e.g., Verdoliva [66] covered many general fake image datasets where the manipulated images were not generated by deep learning or even AI-based methods, and some surveys (e.g., [38]) mentioned ASVspoof 2015 datasets but we did not see the use of deep learning for generating data used in the dataset.Many surveys cover deepfake-related challenges, competitions and benchmarks. The coverage is, however, mostly limited, and some challenges (e.g., the Voice Conversion Challenge 2016/2018/2020 and the two Chinese challenges we covered in Section 5) are not covered by any of the surveys. The level of detail of challenges, competitions and benchmarks is also normally limited, compared with what we chose to include in Section 5. Similar to the datasets we covered in Section 4, we believe that our coverage of deepfake-related challenges, competitions and benchmarks in Section 5 is also the most comprehensive so far.Most surveys have a good coverage of related methods for deepfake generation and detection, but only some explicitly covered performance comparison between different methods [38, 46, 64].Among all the survey papers, Li et al. [38] conducted the most comprehensive study on performance of different deepfake detection methods. In addition to showing the performance metrics of a number of deepfake detection methods in Table 3 of [38], they also looked at general characteristics and issues of different types of deepfake detection methods, as shown in Table 4. Furthermore, they also looked at research on robustness of deepfake detection methods against adversarial samples, referring to some work that showed a lack of such robustness.Due to quality issues of many deepfake-related datasets (discussed in Section 4.6), we need to treat any performance metrics and comparison of different detection methods with caution. Without testing all methods on a sufficiently large, diverse and high-quality deepfake dataset, the performance comparison results can be misleading. This highlights the importance of having more challenges, competitions and benchmarks to encourage performance comparison on standard datasets and using consistent performance metrics.The authors of some surveys identified some key challenges and future research directions for the deepfake community.Not surprisingly, how to develop more robust, scalable, generalisable and explainable deepfake detection methods is one of the most discussed key challenges and also a major future research direction [7, 16, 38, 41, 45, 59, 65, 66, 71]. Considering the arms race between deepfake generation and detection, this research direction will likely remain the hottest topic in deepfake research.A couple of surveys [38, 66] mentioned fusion as a key future research direction, where “fusion” refers to combining different methods (e.g., combining multiple detectors of different types) and data sources (e.g., jointly considering audio-visual analysis) to achieve better performance for deepfake detection. Lyu [45] suggested that, for detection of deepfake videos, we need to consider video-level detection more, which can be considered fusion of detection results of all video frames.The authors of three surveys, Lyu [45] , Deshmukh and Wankhade [16] and Younus and Hasan [71], argued that better (higher-quality, more up-to-date, and more standard) deepfake datasets are needed to develop more effective deepfake detection methods. Lyu [45] also suggested that we need to consider  effects in training data and improve the evaluation of datasets. We agree with them on these points.Tao et al. [59] suggested that low-cost deepfake generation/detection should be considered as a future research direction. This is a valid recommendation since lightweight methods will allow less powerful computing devices (e.g., IoT devices) to benefit from such technologies.Two Chinese surveys [38, 41] also mentioned the need to have new deepfake-related legislations combating malicious use of deepfakes and the need to train end users such as journalists. This is likely an area where interdisciplinary research can grow.There are also other ad-hoc recommendations given by the authors of some surveys. For example, Lyu [45] argued that deepfake detection should be considered a (more complicated) multi-class, multi-label and local detection problem. Tolosana et al. [64] discussed specific research directions for different deep-fake generation methods (face synthesis, identity swap, attribute manipulation, and expression swap). Liang et al. [41] and Li et al. [38] recommended more active defence mechanisms such as using digital watermarking and blockchain technologies to build trustworthy media frameworks against deepfakes.The rapid growth in the capability to manipulate media or create synthetic media which look realistic and natural paved the way for deepfakes. At first, this paper adopted a critical approach to look at different definitions of the term “deepfake”. In that regard, we point out the different contradicting definitions and call for the wider community to consider how to define a new term that has a more consistent scope and meaning. For instance, replacing “deepfake” by “deep synthesis” can be more inclusive by embracing positive applications of deepfake techniques, e.g., in entertainment and for simulation purposes.This paper provided a comprehensive overview of multiple aspects of the deepfake ecosystem drawing from the research literature and other online sources published in two languages: English and Chinese. It covers commonly used performance metrics and standards, related datasets, challenges, competitions and benchmarks. It also presents a meta-review of 12 selected deepfake-related survey papers published in 2020 and 2021, covering not only the above mentioned aspects, but also highlighting key challenges and recommendations.[1]   Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018. MesoNet: A Compact Facial Video Forgery Detection Network. In Proceedings of the 2018 IEEE International Workshop on Information Forensics and Security. IEEE, 1–7. https://doi.org/10.1109/WIFS.2018.8630761[2]   Henry Ajder, Giorgio Patrini, Francesco Cavalli, and Laurence Cullen. 2019. The State of Deepfakes: Landscape, Threats, and Impact. Deeptrace. , 27 pages.  https://sensity.ai/reports/[4]   Ahmed Ali and Steve Renals. 2018. Word Error Rate Estimation for Speech Recognition: e-WER. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 20–24. https://doi.org/10.18653/v1/P18-2004[6]   Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh. 2018. Recycle-GAN: Unsupervised Video Retargeting. In Proceedings of the 2018 European Conference on Computer Vision. Springer, 17 pages.  https://doi.org/10.1007/978-3-030-01228-1 8[8]   Mikol-aj Bin´kowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C. Cobo, and Karen Simonyan. 2019. High Fidelity Speech Synthesis with Adversarial Networks.  https://doi.org/10.48550/ARXIV.1909.11646[10]   Umur Aybars Ciftci, Ilke Demir, and Lijun Yin. 2020. FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020), 17 pages. https://doi.org/10.1109/TPAMI.2020.3009287[11]   Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, and Anil K. Jain. 2020. On the Detection of Digital Face Manipulation. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 10 pages. https://doi.org/10.1109/CVPR42600.2020.00582[12]  Rohan Kumar Das, Tomi Kinnunen, Wen-Chin Huang, Zhen-Hua Ling, Junichi Yamagishi, Zhao Yi, Xiaohai Tian, and Tomoki Toda. 2020. Predictions of Subjective Ratings and Spoofing Assessments of Voice Conversion Challenge 2020 Submissions. In Proceedings of the Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020. International Speech Communication Association, 99–120. https://doi.org/10.21437/VCC BC.2020-15[13] H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021. ASVspoof 2021: Automatic Speaker Verification Spoofing and Countermeasures Challenge Evaluation Plan.  https://www.asvspoof.org/asvspoof2021/asvspoof2021 evaluation plan.pdf[14]   H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021.[15]   H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021. ASVspoof 2021 Challenge - Speech Deepfake Database.  https://doi.org/10.5281/zenodo.4835108[16]    ![](file:///C:/Users/user/AppData/Local/Temp/msohtmlclip1/01/clip_image051.gif)Anushree Deshmukh and Sunil B. Wankhade. 2021. Deepfake Detection Approaches Using Deep Learning: A Systematic Review. In Intelligent Computing and Networking: Proceedings of IC-ICN 2020 (Lecture Notes in Networks and Systems, Vol. 146). Springer, 293–302. https://doi.org/10.1007/978-981-15-7421-4 27[17]   Xinyi Ding, Zohreh Raziei, Eric C. Larson, Eli V. Olinick, Paul Krueger, and Michael Hahsler. 2020. Swapped Face Detection using Deep Learning and Subjective Assessment. EURASIP Journal on Information Security 2020, 1 (2020), 1–12. https://doi.org/10.1186/s13635-020-00109-8[18]   Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset.  https://doi.org/10.48550/ARXIV.2006.07397[19]   Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset. arXiv:2006.07397. https://arxiv.org/abs/2006.07397[23]   Gereon Fox, Wentao Liu, Hyeongwoo Kim, Hans-Peter Seidel, Mohamed Elgharib, and Christian Theobalt. 2021. Videoforensicshq: Detecting High-Quality Manipulated Face Videos. In Proceedings of the 2021 IEEE International Conference on Multimedia and Expo. IEEE, 1–6. https://doi.org/10.1109/ICME51207.2021.9428101[24]   Haiying Guan, Andrew Delgado, Yooyoung Lee, Amy N. Yates, Daniel Zhou, Timothee Kheyrkhah, and Jon Fiscus. 2021. User Guide for NIST Media Forensic Challenge (MFC) Datasets. https://doi.org/10.6028/NIST.IR.8377[25]   Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. 2021. ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE,  4360–4369.    https://doi.org/10.1109/CVPR46437.2021.00434[26]   Liming Jiang, Zhengkui Guo, Wayne Wu, Zhaoyang Liu, Ziwei Liu, Chen Change Loy, Shuo Yang, Yuanjun Xiong, Wei Xia, Baoying Chen, Peiyu Zhuang, Sili Li, Shen Chen, Taiping Yao, Shouhong Ding, Jilin Li, Feiyue Huang, Liujuan Cao, Rongrong Ji, Changlei Lu, and Ganchao Tan. 2021. DeeperForensics Challenge 2020 on Real-World Face Forgery Detection: Methods and Results. arXiv:2102.09471. https://arxiv.org/pdf/2102.09471.pdf[27]   Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. 2020. DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 2886–2895. https://doi.org/10.1109/CVPR42600.2020.00296[28]   Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. 2018. Efficient Neural Audio Synthesis. https://doi.org/10.48550/ARXIV.1802.08435[30]   Tero Karras, Samuli Laine, and Timo Aila. 2019. A Style-based Generator Architecture for Generative Adversarial Networks. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 4401–4410. https://doi.org/10.1109/CVPR.2019.00453[32]   Ali Khodabakhsh, Raghavendra Ramachandra, Kiran Raja, Pankaj Wasnik, and Christoph Busch. 2018. Fake Face Detection Methods: Can They Be Generalized?. In Proceedings of the 2018 International Conference of the Biometrics Special Interest Group. IEEE, 1–6. https://doi.org/10.23919/BIOSIG.2018.8553251[33]   Hyeongwoo Kim, Mohamed Elgharib, Hans-Peter Zoll¨ofer, Michael Seidel, Thabo Beeler, Christian Richardt, and Christian Theobalt. 2019. Neural Style-Preserving Visual Dubbing. ACM Transactions on Graphics 38, 6, Article 178 (2019), 13 pages. https://doi.org/10.1145/3355089.3356500[34]   Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick P´erez, Christian Richardt, Michael Zollh¨ofer, and Christian Theobalt. 2018. Deep Video Portraits. ACM Transactions on Graphics 37, 4, Article 163 (2018), 14 pages. https://doi.org/10.1145/3197517.3201283[35]   Pavel Korshunov and S´ebastien Marcel. 2019. Vulnerability Assessment and Detection of Deepfake Videos. In Proceedings of the 2019 International Conference on Biometrics. IEEE, 1–6. https://doi.org/10.1109/ICB45273.2019.8987375[36]   Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae. 2021. KoDF: A Large-scale Korean DeepFake Detection Dataset. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision. IEEE, 10724–10733. https://doi.org/10.1109/ICCV48922.2021.01057[37]   Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. 2020. FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping. arXiv:1912.13457. https://arxiv.org/abs/1912.13457[38]   Xurong Li, Shouling Ji, Chunming Wu, Zhenguang Liu, Shuiguang Deng, Peng Cheng, Min Yang, and Xiangwei Kong. 2021. Survey on Deepfakes and Detection Techniques.  32, 2 (2021), 496–518. http://www.jos.org.cn/1000-9825/6140.htm[39]   Yuezun Li, Ming-Ching Chang, and Siwei Lyu. 2018. In Ictu Oculi: Exposing AI Created Fake Videos by Detecting Eye Blinking. In Proceedings of the 2018 IEEE International Workshop on Information Forensics and Security. IEEE, 1–7. https://doi.org/10.1109/WIFS.2018.8630787[40]   Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. 2020. Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 3204–3213. https://doi.org/10.1109/CVPR42600.2020.00327[42]   Steven R. Livingstone and Frank A. Russo. 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English.  13, 5 (2018), 35 pages.[43]   Chen-Chou Lo, Szu-Wei Fu, Wen-Chin Huang, Xin Wang, Junichi Yamagishi, Yu Tsao, and Hsin-Min Wang. 2021. MOSNet: Deep Learning based Objective Assessment for Voice Conversion. arXiv:1904.08352. https://arxiv.org/pdf/1904.08352.pdf[44]   Jaime Lorenzo-Trueba, Junichi Yamagishi, Tomoki Toda, Daisuke Saito, Fernando Villavicencio, Tomi Kinnunen, and Zhenhua Ling. 2018. The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods. In Proceedings of the Odyssey 2018 The Speaker and Language Recognition Workshop. International Speech Communication Association, 195–202. https://doi.org/10.21437/Odyssey.2018-28[46]   Yisroel Mirsky and Wenke Lee. 2021. The Creation and Detection of Deepfakes: A Survey.  54, 1, Article 7 (2021), 41 pages. https://doi.org/10.1145/3425780[47]   Gautham J. Mysore. 2015. Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges. IEEE Signal Processing Letters 22, 8 (2015), 1006–1010. https://doi.org/10.1109/LSP.2014.2379648[48]   Jo˜ao C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proen¸ca, and Julian Fierrez. 2020. GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection. IEEE Journal of Selected Topics in Signal Processing 14, 5 (2020), 1038–1048. https://doi.org/10.1109/JSTSP.2020.3007250[50]   Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio.  https://doi.org/10.48550/ARXIV.1609.03499[51]   Debajyoti Pal and Tuul Triyason. 2018. A Survey of Standardized Approaches towards the Quality of Experience Evaluation for Video Services: An ITU Perspective. International Journal of Digital Multimedia Broadcasting 2018, Article 1391724 (2018), 25 pages. https://doi.org/10.1155/2018/1391724[52]   Bo Peng, Hongxing Fan, Wei Wang, Jing Dong, Yuezun Li, Siwei Lyu, Qi Li, Zhenan Sun, Han Chen, Baoying Chen, Yanjie Hu, Shenghai Luo, Junrui Huang, Yutong Yao, Boyuan Liu, Hefei Ling, Guosheng Zhang, Zhiliang Xu, Changtao Miao, Changlei Lu, Shan He, Xiaoyan Wu, and Wanyi Zhuang. 2021. DFGC 2021: A DeepFake Game Competition. arXiv:2106.01217. https:[53]   Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um´e, Mr. Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, Sheng Zhang, Pingyu Wu, Bo Zhou, and Weiming Zhang. 2020. DeepFaceLab: Integrated, Flexible and Extensible Face-swapping Framework. https://doi.org/10.48550/ARXIV.2005.05535[54]   Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. 2020. Adversarial Latent Au-toencoders. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 10 pages. https://doi.org/10.1109/CVPR42600.2020.01411[55]   Andreas R¨ossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. 2018. FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces. https://doi.org/10.48550/ARXIV.1803.09179[56]   Andreas R¨ossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. In Proceedings of the 2019 International Conference on Computer Vision. IEEE, 1–11. https://doi.org/10.1109/ICCV.2019.00009[61]   Justus Thies, Michael Zollh¨ofe, and Matthias Niessner. 2019. Deferred Neural Rendering: Image Synthesis using Neural Textures. ACM Transactions on Graphics 38, Article 66 (2019), 12 pages. Issue  4.   https://doi.org/10.1145/3306346.3323035[62]   Tomoki Toda, Ling-Hui Chen, Daisuke Saito, Fernando Villavicencio, Mirjam Wester, Zhizheng Wu, and Junichi Yamagishi. 2016. The Voice Conversion Challenge 2016. In Proceedings of Interspeech 2016. International Speech Communication Association, 1632–1636. https://doi.org/10.21437/Interspeech.2016-1066[63]   Massimiliano Todisco, Xin Wang, Ville Vestman, Md Sahidullah, Hector Delgado, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Tomi Kinnunen, and Kong Aik Lee. 2019. ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection. arXiv:1904.05441. https://arxiv.org/pdf/1904.05441.pdf[64]   Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, and Javier Ortega-Garcia. 2020. Deepfakes and beyond: A Survey of face manipulation and fake detection.  64 (2020), 131–148.  https://doi.org/10.1016/j.inffus.2020.06.014[65]   Xin Tong, Luona Wang, Xiaoqin Pan, and Jingya Wang. 2020. An Overview of Deepfake: The Sword of Damocles in AI. In Proceedings of the 2020 International Conference on Computer Vision, Image and Deep Learning. IEEE, 265–273. https://doi.org/10.1109/CVIDL51233.2020.00-88[67]   Xin Wang, Junichi Yamagishi, Massimiliano Todisco, H´ector Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, Lauri Juvela, Paavo Alku, Yu-Huai Peng, Hsin-Te Hwang, Yu Tsao, Hsin-Min Wang, S´ebastien Le Maguer, Markus Becker, Fergus Henderson, Rob Clark, Yu Zhang, Quan Wang, Ye Jia, Kai Onuma, Koji Mushika, Takashi Kaneda, Yuan Jiang, Li-Juan Liu, Yi-Chiao Wu, Wen-Chin Huang, Tomoki Toda, Kou Tanaka, Hirokazu Kameoka, Ingmar Steiner, Driss Matrouf, Jean-Fran¸cois Bonastre, Avashna Govender, Srikanth Ronanki, Jing-Xuan Zhang, and Zhen-Hua Ling. 2020. ASVspoof 2019: A Large-scale Public Database of Synthesized, Converted and Replayed Speech. Computer Speech & Language 64 (2020), 27 pages.  https://doi.org/10.1016/j.csl.2020.101114[68]   Mirjam Wester, Zhizheng Wu, and Junichi Yamagishi. 2016. Analysis of the Voice Conversion Challenge 2016 Evaluation Results. In Proceedings of the Interspeech 2016 Conference. International Speech Communication Association, 1637–1641. https://doi.org/10.21437/Interspeech.2016-1331[70]  Zhao Yi, Wen-Chin Huang, Xiaohai Tian, Junichi Yamagishi, Rohan Kumar Das, Tomi Kinnunen, Zhen-Hua Ling, and Tomoki Toda. 2020. Voice Conversion Challenge 2020 – Intra-lingual Semi-parallel and Cross-lingual Voice Conversion –. In Proceedings of the Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020. International Speech Communication Association, 80–98. https://doi.org/10.21437/VCC BC.2020-14[71]   Mohammed A. Younus and Taha M. Hasan. 2020. Abbreviated View of Deepfake Videos Detection Techniques. In Proceedings of the 2020 6th International Engineering Conference. IEEE, 115–120. https://doi.org/10.1109/IEC49899.2020.9122916[73]   Teng Zhang, Lirui Deng, Liang Zhang, and Xianglei Dang. 2020. Deep Learning in Face Synthesis: A Survey on Deepfakes. In Proceedings of the 2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology. IEEE, 67–70. https://doi.org/10.1109/CCET50901.2020.9213159[74]   Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. 2021. DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 10140–10150. https://doi.org/10.1109/CVPR46437.2021.01001[75]    ![](file:///C:/Users/user/AppData/Local/Temp/msohtmlclip1/01/clip_image051.gif)Yuanhan Zhang, ZhenFei Yin, Yidong Li, Guojun Yin, Junjie Yan, Jing Shao, and Ziwei Liu. 2020. CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations. In Proceedings of the 2020 European Conference on Computer Vision. Springer, 70–85. https://doi.org/10.1007/978-3-030-58610-2 5[76]   Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu, Shuo Yang, Yuanjun Xiong, Wei Xia, Yan Xu, Man Luo, Jian Liu, Jianshu Li, Zhijun Chen, Mingyu Guo, Hui Li, Junfu Liu, Pengfei Gao, Tianqi Hong, Hao Han, Shijie Liu, Xinhua Chen, Di Qiu, Cheng Zhen, Dashuang Liang, Yufeng Jin, and Zhanlong Hao. 2021. CelebA-Spoof Challenge 2020 on Face Anti-Spoofing: Methods and Results. arXiv:2102.12642. https://arxiv.org/pdf/2102.12642.pdf[77]   Bo Zhao, Shaozeng Zhang, Chunxue Xu, Yifan Sun, and Chengbin Deng. 2021. Deep Fake Ge-ography? When Geospatial Data Encounter Artificial Intelligence. Cartography and Geographic Information Science 48, 4 (2021), 338–352. https://doi.org/10.1080/15230406.2021.1910075[78]   Peng Zhou, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2017. Two-Stream Neural Networks for Tampered Face Detection. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops. IEEE, 1831–1839. https://doi.org/10.1109/CVPRW.2017.229[79]   Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, and Jianbing Shen. 2021. Face Forensics in the Wild. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE,  5774–5784.    https://doi.org/10.1109/CVPR46437.2021.00572[80]   Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. In Proceedings of the 2017 IEEE International Conference on Computer Vision. IEEE, 2242–2251. https://doi.org/10.1109/ICCV.2017.244[81]   Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. 2020. WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection. In Proceedings of the 2020 28th ACM International Conference on Multimedia. ACM, 2382–2390. https://doi.org/10.1145/3394171.3413769:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.  ]]></content:encoded></item><item><title>The HackerNoon Newsletter: Why “Small Changes” Don’t Exist in Production Game Systems (2/28/2026)</title><link>https://hackernoon.com/2-28-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:02:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, February 28, 2026?By @ktdevjournal [ 5 Min read ] It doesn’t matter if you build games or a banking app - you don’t just have a pile of features and assets. You have an ecosystem for each bit of work Read More.By @Lima_Writes [ 9 Min read ] When language comes back at you fast, coherent, and emotionally attuned, it feels like truth. Especially when you’re tired. Or lonely.  Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Go 1.22: A Change in Loop Scoping</title><link>https://hackernoon.com/go-122-a-change-in-loop-scoping?source=rss</link><author>Go [Technical Documentation]</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:00:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Go 1.21 includes a preview of a change to  loop scoping that we plan to ship in Go 1.22, removing one of the most common Go mistakes.If you’ve written any amount of Go code, you’ve probably made the mistake of keeping a reference to a loop variable past the end of its iteration, at which point it takes on a new value that you didn’t want. For example, consider this program:func main() {
    done := make(chan bool)

    values := []string{"a", "b", "c"}
    for _, v := range values {
        go func() {
            fmt.Println(v)
            done <- true
        }()
    }

    // wait for all goroutines to complete before exiting
    for _ = range values {
        <-done
    }
}
\
The three created goroutines are all printing the same variable , so they usually print “c”, “c”, “c”, instead of printing “a”, “b”, and “c” in some order.\
Although concurrency is often involved, it need not be. This example has the same problem but no goroutines:func main() {
    var prints []func()
    for i := 1; i <= 3; i++ {
        prints = append(prints, func() { fmt.Println(i) })
    }
    for _, print := range prints {
        print()
    }
}
\
This kind of mistake has caused production problems at many companies, including a publicly documented issue at Lets Encrypt. In that instance, the accidental capture of the loop variable was spread across multiple functions and much more difficult to notice:// authz2ModelMapToPB converts a mapping of domain name to authz2Models into a
// protobuf authorizations map
func authz2ModelMapToPB(m map[string]authz2Model) (*sapb.Authorizations, error) {
    resp := &sapb.Authorizations{}
    for k, v := range m {
        // Make a copy of k because it will be reassigned with each loop.
        kCopy := k
        authzPB, err := modelToAuthzPB(&v)
        if err != nil {
            return nil, err
        }
        resp.Authz = append(resp.Authz, &sapb.Authorizations_MapElement{
            Domain: &kCopy,
            Authz: authzPB,
        })
    }
    return resp, nil
}
\
The author of this code clearly understood the general problem, because they made a copy of , but it turns out  used pointers to fields in  when constructing its result, so the loop also needed to make a copy of .\
Tools have been written to identify these mistakes, but it is hard to analyze whether references to a variable outlive its iteration or not. These tools must choose between false negatives and false positives. The  analyzer used by  and  opts for false negatives, only reporting when it is sure there is a problem but missing others. Other checkers opt for false positives, accusing correct code of being incorrect. We ran an analysis of commits adding  lines in open-source Go code, expecting to find bug fixes. Instead we found many unnecessary lines being added, suggesting instead that popular checkers have significant false positive rates, but developers add the lines anyway to keep the checkers happy.\
One pair of examples we found was particularly illuminating:This diff was in one program:     for _, informer := range c.informerMap {
+        informer := informer
         go informer.Run(stopCh)
     }
\
And this diff was in another program:     for _, a := range alarms {
+        a := a
         go a.Monitor(b)
     }
\
One of these two diffs is a bug fix; the other is an unnecessary change. You can’t tell which is which unless you know more about the types and functions involved.For Go 1.22, we plan to change  loops to make these variables have per-iteration scope instead of per-loop scope. This change will fix the examples above, so that they are no longer buggy Go programs; it will end the production problems caused by such mistakes; and it will remove the need for imprecise tools that prompt users to make unnecessary changes to their code.\
To ensure backwards compatibility with existing code, the new semantics will only apply in packages contained in modules that declare  or later in their  files. This per-module decision provides developer control of a gradual update to the new semantics throughout a codebase. It is also possible to use  lines to control the decision on a per-file basis.\
Old code will continue to mean exactly what it means today: the fix only applies to new or updated code. This will give developers control over when the semantics change in a particular package. As a consequence of our forward compatibility work, Go 1.21 will not attempt to compile code that declares  or later. We included a special case with the same effect in the point releases Go 1.20.8 and Go 1.19.13, so when Go 1.22 is released, code written depending on the new semantics will never be compiled with the old semantics, unless people are using very old, unsupported Go versions.Go 1.21 includes a preview of the scoping change. If you compile your code with  set in your environment, then the new semantics are applied to all loops (ignoring the  lines). For example, to check whether your tests still pass with the new loop semantics applied to your package and all your dependencies:GOEXPERIMENT=loopvar go test
\
We patched our internal Go toolchain at Google to force this mode during all builds at the start of May 2023, and in the past four months we have had zero reports of any problems in production code.\
You can also try test programs to better understand the semantics on the Go playground by including a  comment at the top of the program, like in this program. (This comment only applies in the Go playground.)Although we’ve had no production problems, to prepare for that switch, we did have to correct many buggy tests that were not testing what they thought they were, like this:func TestAllEvenBuggy(t *testing.T) {
    testCases := []int{1, 2, 4, 6}
    for _, v := range testCases {
        t.Run("sub", func(t *testing.T) {
            t.Parallel()
            if v&1 != 0 {
                t.Fatal("odd v", v)
            }
        })
    }
}
\
In Go 1.21, this test passes because  blocks each subtest until the entire loop has finished and then runs all the subtests in parallel. When the loop has finished,  is always 6, so the subtests all check that 6 is even, so the test passes. Of course, this test really should fail, because 1 is not even. Fixing for loops exposes this kind of buggy test.\
To help prepare for this kind of discovery, we improved the precision of the  analyzer in Go 1.21 so that it can identify and report this problem. You can see the report in this program on the Go playground. If  is reporting this kind of problem in your own tests, fixing them will prepare you better for Go 1.22.\
If you run into other problems, the FAQ has links to examples and details about using a tool we’ve written to identify which specific loop is causing a test failure when the new semantics are applied.\
This article is available on  under a CC BY 4.0 DEED license.]]></content:encoded></item><item><title>Why “Small Changes” Don’t Exist in Production Game Systems</title><link>https://hackernoon.com/why-small-changes-dont-exist-in-production-game-systems?source=rss</link><author>Constantine</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[It’s just a small change!\
How often do we hear that we need to fix something? We need to add a small feature. We need to tweak something. Code-wise or publishing, just realized they need this for retention, or maybe an analyst brought the newest data, so now we have to add just a few lines to the code. They don’t affect performance or any other departments, I promise. And it’s just like 3 minutes of coder work - why not? Fast forward: they broke the “Buy” button on the front page of the store on release.\
Why does this always happen with small changes? Well, if we think about it, we don’t usually think about it. Let me explain:Designers think in features and user experience. \n Engineers think in whole systems. \n Producers think in tasks. \n Stakeholders think in business outcomes.\
And one small change is always perceived as something isolated and usually without everyone’s awareness. So, it is basically a cognitive shortcut. And that happens not because everyone is wrong or unprofessional. It’s because modern production systems are highly interconnected, so it’s impossible to know what could potentially be affected by anything - especially if you haven’t worked on this project for 15 years.What is modern production? I’m glad you asked!\
It doesn’t matter if you build games or a banking app - you don’t just have a pile of features and assets. You have an ecosystem for each bit of work: Art, Code, Design, UI, Marketing, Publishing (maybe even Project Management - wow, you are a rich developer), etc. And each one of them has its own infrastructure, pipelines, workflows, and shared assets. To simplify, it can be shared data schemas, builds, automation processes, UI bindings, and many other things.\
What’s wrong if I just make a small color change to one of the icons? Well, that means you spend 3 seconds changing a color code. Then you have to assemble a build. Then QA has to check your small change to confirm that you indeed changed the color. Then you have to assemble the build again, which should be in a queue with other builds in the waiting list.\
Then we have to update the server with your changes - oh wait, did you tell anyone about that? No? Oh, that’s great, because you just submitted your changes during the commit freeze, and now deployment engineers have to fix the CI/CD pipeline, and we have to postpone the release for 4 days because it’s Friday.\
And by the way - we have to communicate that to users because they were waiting for this new version, and some of them decided not to wait that long and removed your app. Whoops, that’s awkward. Sorry to hear that.That’s alright, I’m here to help you! Let me introduce you to Change Propagation Surface (CPS) - the number of systems, pipelines, assets, and workflows that a change must pass through before it reaches the player.\
Your change should not be estimated by its task size, like “1 hour of work.” Your change equals CPS × Coupling Density (the amount of work other departments need to do in order for this change to pass).\
Think about it this way:One small UI tweak touches no shared data - low CPS.A gameplay rule change touching code, balance, design, analytics, player experience - high CPS.\
Let’s go back to the situation where you want to change the color of the icon. Those 3 seconds of work would affect UI, builds, player perception, experience, and design. It might also affect color coding for accessibility rules, plus build assembling, and finally server updates. It’s high CPS - of course, if you didn’t sneak that change in without everyone’s awareness (I see that - drop it!).\
The same goes for asset swaps or changing a stat value: it affects memory, AI tuning, destruction logic, etc. Don’t do that unless someone from senior leadership said it’s low CPS - then just do it and see how it goes.\
You can apply this approach basically anywhere in production because it is not an abstract thing at all and can be estimated.\
Each of these items counts as a plus 1 CPS factor. Subsequently, the more of the same “items” you touch, the higher the CPS number you will get. And with that information, you can create a small estimation matrix like:CPS 1-2 - Local change \n CPS 3-5 - Cross-functional change \n CPS 6+ - Systemic change\
One more time, the formula is: Impact = CPS × Coupling Density. Easy!Let’s see how it works in a real-life example:\
So your developer went on holiday and completed a math course on LinkedIn. And when he came back, he said that there is a more efficient way of calculating EXP. This change is “one line of code.” Okay, but after reading this article, you already know how it works in reality and that it touches multiple things:Player progression pacing\
That means CPS is more than 7. So now you see that even though the code diff is tiny, the propagation surface is systemic and has a massive potential outcome. In other words, if XP progression speeds up things like economy, availability of the content, battle pass value, retention curves, etc., you should know that even if the implementation takes about 10 minutes, the ripple effect can take weeks of work.\
Why does live service production make it worse? Because it is amplified by content being reused across multiple features, by telemetry and economy being tightly coupled, and by systems being persistent and often requiring backward compatibility.\
So, the real cost you pay for propagation lies in prolonged timelines, hidden rework, cross-team friction, technical debt, burnout, and eventually, people resigning directly or indirectly.\
Instead of thinking, “Oh, this is a small change,” we should probably think, “What systems does this change touch?” Think about this as infrastructure, not a feature, and always try to bring that to cross-team awareness. And if you are capable enough, try to estimate the surface area, not just this exact small change.\
**The whole point of my way-too-long introduction is that there is no such thing as a small change in production systems. There are only changes in misunderstood affected areas. And the more senior you become, the more your vision shifts toward understanding how this change will travel instead of trying to avoid the change altogether.]]></content:encoded></item><item><title>Startup Plans April Launch for a Satellite Reflect Sunlight to Earth at Night</title><link>https://science.slashdot.org/story/26/02/28/076229/startup-plans-april-launch-for-a-satellite-reflect-sunlight-to-earth-at-night?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A start-up called Reflect Orbital "proposes to use large, mirrored satellites to redirect sunlight to Earth at night," reports the Washington Post, "with plans to bathe solar farms, industrial sites and even entire cities in light that could, if desired, reach the intensity of daylight...." 

Slashdot noted their idea in 2022 — but Reflect Orbital now expects to launch its first satellite in April, according to the article. "But its grand vision is largely 'aspirational,' as its young founder, Ben Nowack, told me..."

Reflect Orbital's Nowack describes a scene right out of sci-fi: An extremely bright star appears on the northern horizon and makes its way across the sky, illuminating a 5-kilometer circle on Earth, then setting on the southern horizon about five minutes later, just as another such "star" appears in the north. To make the night even brighter, a customer could make 10 "stars" appear at once in the north by ordering them on an app. Two such artificial stars are in development in Reflect Orbital's factory. Nowack showed them to me on a Zoom call. The first to launch is 50 feet across, but he plans later to build them three times that size. If all goes according to plan, he'll have 50,000 of them circling the Earth in 2035 at an altitude of around 400 miles. 
Nowack plans to start selling the service "in mostly developing nations or places that don't have streetlights yet." Eventually, he thinks, he can illuminate major cities, turn solar fields and farms into round-the-clock operations for any business or municipality that pays for it. He likened his technology to the invention of crop irrigation thousands of years ago. "I see this as much the same thing," he said, arguing that people would no longer have to "wait for the sun to shine." 

The article adds that Elon Musk's SpaceX "wants to launch as many as a million satellites to serve as orbiting data centers — 70 times the number of satellites now in orbit." (America's satellite-regulation Federal Communications Commission
grants a "categorical exclusion" from environmental review to satellites on the grounds that their operations "normally do not have significant effects on the human environment.") 

The public comment periods for the two proposals close on March 6 and March 9.]]></content:encoded></item><item><title>Startup Plans April Launch for a Satellite to Reflect Sunlight to Earth at Night</title><link>https://science.slashdot.org/story/26/02/28/076229/startup-plans-april-launch-for-a-satellite-to-reflect-sunlight-to-earth-at-night?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A start-up called Reflect Orbital "proposes to use large, mirrored satellites to redirect sunlight to Earth at night," reports the Washington Post, "with plans to bathe solar farms, industrial sites and even entire cities in light that could, if desired, reach the intensity of daylight...." 

Slashdot noted their idea in 2022 — but Reflect Orbital now expects to launch its first satellite in April, according to the article. "But its grand vision is largely 'aspirational,' as its young founder, Ben Nowack, told me..."

Reflect Orbital's Nowack describes a scene right out of sci-fi: An extremely bright star appears on the northern horizon and makes its way across the sky, illuminating a 5-kilometer circle on Earth, then setting on the southern horizon about five minutes later, just as another such "star" appears in the north. To make the night even brighter, a customer could make 10 "stars" appear at once in the north by ordering them on an app. Two such artificial stars are in development in Reflect Orbital's factory. Nowack showed them to me on a Zoom call. The first to launch is 50 feet across, but he plans later to build them three times that size. If all goes according to plan, he'll have 50,000 of them circling the Earth in 2035 at an altitude of around 400 miles. 
Nowack plans to start selling the service "in mostly developing nations or places that don't have streetlights yet." Eventually, he thinks, he can illuminate major cities, turn solar fields and farms into round-the-clock operations for any business or municipality that pays for it. He likened his technology to the invention of crop irrigation thousands of years ago. "I see this as much the same thing," he said, arguing that people would no longer have to "wait for the sun to shine." 

The article adds that Elon Musk's SpaceX "wants to launch as many as a million satellites to serve as orbiting data centers — 70 times the number of satellites now in orbit." (America's satellite-regulation Federal Communications Commission
grants a "categorical exclusion" from environmental review to satellites on the grounds that their operations "normally do not have significant effects on the human environment.") 

The public comment periods for the two proposals close on March 6 and March 9.]]></content:encoded></item><item><title>Meet M6: The Chinese AI That Understands Text and Images at Scale</title><link>https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss</link><author>Alibaba</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:28:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Junyang Lin, junyang.ljy@alibaba-inc.com (Alibaba Group, China)Rui Men, menrui.mr@alibaba-inc.com (Alibaba Group, China)An Yang, ya235025@alibaba-inc.com (Alibaba Group, China)Chang Zhou, ericzhou.zc@alibaba-inc.com (Alibaba Group, China)Ming Ding, dm18@mails.tsinghua.edu.cn (Tsinghua University, China)Yichang Zhang, yichang.zyc@alibaba-inc.com (Alibaba Group, China)Peng Wang, zheluo.wp@alibaba-inc.com (Alibaba Group, China)Ang Wang, wangang.wa@alibaba-inc.com (Alibaba Group, China)Le Jiang, jiangle.jl@alibaba-inc.com (Alibaba Group, China)Xianyan Jia, xianyan.xianyanjia@alibaba-inc.com (Alibaba Group, China)Jie Zhang, wanglin.zj@alibaba-inc.com (Alibaba Group, China)Jianwei Zhang, zhangjianwei.zjw@alibaba-inc.com (Alibaba Group, China)Xu Zou, zoux18@mails.tsinghua.edu.cn (Tsinghua University, China)Zhikang Li, zhikang.lzk@alibaba-inc.com (Alibaba Group, China)Xiaodong Deng, xiaodongdeng.dxd@alibaba-inc.com (Alibaba Group, China)Jie Liu, sanshuai.lj@alibaba-inc.com (Alibaba Group, China)Jinbao Xue, zhiji.xjb@alibaba-inc.com (Alibaba Group, China)Huiling Zhou, zhule.zhl@alibaba-inc.com (Alibaba Group, China)Jianxin Ma, jason.mjx@alibaba-inc.com (Alibaba Group, China)Jin Yu, kola.yu@alibaba-inc.com (Alibaba Group, China)Yong Li, jiufeng.ly@alibaba-inc.com (Alibaba Group, China)Wei Lin, weilin.lw@alibaba-inc.com (Alibaba Group, China)Jingren Zhou, jingren.zhou@alibaba-inc.com (Alibaba Group, China)Jie Tang, jietang@tsinghua.edu.cn (Tsinghua University, China)Hongxia Yang, yang.yhx@alibaba-inc.com (Alibaba Group, China)In this work, we construct the largest dataset for multimodal pre-training in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called , referring to ulti-odality to ulti-odality ultitask ega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and  parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.Multimodal Pretraining; Multitask; Text-to-Image GenerationPretraining has become a focus in the research in natural language processing (NLP) [1, 2, 7, 16, 18, 19, 27, 31, 37, 44, 49]. The recent GPT-3 with over 175 billion parameters demonstrates that large models trained on big data have extremely large capacity and it can outperform the state-of-the-arts in downstream tasks especially in the zero-shot setting. Also, the rapid development of pretraining in NLP sparkles cross-modal pretraining. A number of studies [4, 11, 17, 22, 24, 25, 28, 29, 38, 51] have created new state-of-the-art performances for various cross-modal downstream tasks.A pity is that most recent studies focus on the pretraining on English data. There are lack of both large-scale datasets in Chinese and large-scale models pretrained on the data of Chinese. Therefore, in this work, we develop a large-scale dataset M6-Corpus, which consists of over 1.9TB images and 292GB texts. To the best of our knowledge, this is the largest dataset in Chinese for pretraining in both multimodality and natural language. The dataset collected from the webpages consists of different types of data and covers a large scale of domains, including encyclopedia, question answering, forum discussion, product description, etc. Also, we design sophisticated cleaning procedures to ensure that the data are of high quality.Furthermore, in order to sufficiently leverage such a large amount of high-quality data, we propose to build an extremely large model that can process data of multiple modalities and adapt to different types of downstream tasks. Thus we propose a novel model called M6, referring to MultiModality-to-MultiModality Multitask Mega-transformer. The model is based on the transformer, and it is pretrained with multiple tasks. Pretraining endows the model with the capability of single-modality and multimodality understanding and generation. Based on the architecture of M6, we build  and , which are scaled up to 10 billion and 100 billion pa-rameters respectively. To be more specific,  is the recent largest model pretrained on Chinese data. We apply the model to a series of downstream applications, including product description generation, visual question answering, community question answering, Chinese poem generation, etc., and our experimental results show that M6 outperforms a series of strong baselines.Another contribution of this work is that we first incorporate pretraining with text-to-image generation. Following Ramesh et al. [32], we leverage a two-stage framework for image generation. To be more specific, we apply a trained vector-quantized generative adversarial network to representing images with discrete image codes, and we then use the pretrained M6 to learn the relations between texts and codes. Such learning can bridge the two modalities and enables controllable text-to-image generation.To summarize, the contributions of M6 are as follows:We collect and build the largest Chinese multi-modal pre-training data in industry, which includes 300GB texts and 2TB images.We propose M6 for multimodal pretraining in Chinese, and we scale the model size to up to 10 and 100 billion parameters. Both M6-10B and M6-100B are the recent largest multimodal pretrained model.M6 is versatile and exceeds strong baselines by 11.8% in VQA, 18.4 in image captioning, and 10.3% in image-text matching. Furthermore M6 is able to generate high-quality images.With carefully designed large-scale distributed training optimizations, M6 has obvious advantages in training speed and greatly reduces training costs, creating the possibility for more widespread use of multi-modal pretraining.We collect and develop the largest multi-modality and text dataset in Chinese for now, which is one of the key contributions of this paper. In this section, we first identify the limitations of existing datasets and then describe the construction and preprocessing procedure of our proposed dataset.2.1      Existing DatasetsThe construction of large-scale corpus with high quality and do-main coverage is crucial to Chinese pretraining. In early previous works, the Chinese Wikipedia1 is one of the most frequently used datasets to train Chinese language models. It contains 1.6GB texts (around 0.4B tokens) covering around 1M encyclopedia entries. Another corpus with a comparable size is the THUCTC[39] dataset, which includes 740K news articles. However, with the rapidly increasing capacity of recent language models, the scale of these existing datasets is clearly insufficient. Recently, Cui et al. [5] employ unreleased extended data that are 10 times larger than the CN-Wikipedia to pretrain their Chinese language model. Xu et al.[47] released a 100GB corpus named CLUECorpus2020, which is retried from the multilingual Common Crawl dataset. However, the scale of the datasets is still insufficient to facilitate super large-scale pretraining compared with existing English pretrained models. For example, GPT-3 contains 175B parameters and is trained on 570GB texts. Meanwhile, the dataset should contain image-text pairs rather than plain texts for multi-modal pretraining.2.2      Standards for a High-quality DatasetTo perform large-scale multi-modal pretraining and learn complex world knowledge in Chinese, the dataset is highly required to provide both plain texts and image-text pairs on super large scale, covering a wide range of domains. In order to perform large-scale multi-modal pretraining in Chinese, we focus on the construction of large-scale datasets in Chinese. Specifically, while we unify our pretraining for both natural language and multimodalities, we construct large datasets of both plain texts and image-text pairs. We are interested in obtaining large-scale data that covers a wide range of domains, so that it is possible for the model to learn the complex world knowledge of different fields. Also, we aim to collect data of multiple modalities for the cross-modal pretraining. This raises the difficulty for the construction of a large-scale dataset as the data for multimodal pretraining are usually image-text pairs, where in each pair the text provides a detailed description of a fraction of the image.Though there are a tremendous amount of text resources and images on the world wide web, the corpus for multimodal pretraining is assumed to be better when satisfying the following properties:(1). the sentences should be fluent natural language within a normal length, and should not contain meaningless tokens, such as markups, duplicate punctuation marks, random combinations of characters, etc.; (2). the images should be natural and realistic, and the resolutions of the images need to be identifiable by humans; (3). both the texts and images should not contain illegal content, such as pornography, violence, etc.; (4). the images and texts should be semantically relevant; (5). the datasets should cover a wide range of fields, say sports, politics, science, etc., and therefore it can endow the model with sufficient world knowledge.2.3      Dataset ConstructionBased on the requirements above, we collect data of both plain texts and image-text pairs. There are different types of data, including encyclopedia, crawled webpage, community question answering, forum, product description, etc. We present the details in Table 3. The collected corpus consists of bothag plain-texts and image-text pairs, which is compatible with the designed text-only and multi-modal pretraining tasks. Also, the data has a large coverage over domains, such as science, entertainment, sports, politics, common-sense of life, etc. We have also compared some characteristics of our corpus with existing datasets used for Chinese pretraining in Table 2. The size of our dataset is much larger than the previous ones. To our knowledge, this is the first large-scale, multimodal and multidomain corpus for Chinese pretraining.We implement sophisticated preprocessing to obtain clean data. For text data, we first remove HTML markups and duplicate punctuation marks, and we only reserve characters and punctuation marks that are in Chinese and English. We remove the topics that are shorter than 5 characters and contents shorter than 15 characters. We further apply in-house spam detection to remove sentences that contain words related to certain political issues, pornography, or words in the list of dirty, naughty, and other bad words. In order to preserve the linguistic acceptance of the texts, we implement a language model to evaluate their perplexities, and sentences with high perplexities are discarded. Only images with at least 5000 pixels are reserved for pretraining. A sequence of classifiers and heuristic rules are applied to filter out images containing illegal content. We also use a pretrained image scorer to evaluate the qual-ities of images. For images and texts in crawled webpages, we only consider images and their surrounding text as relevant image-text pairs. Other sentences in the webpages are discarded.Multimodal pretraining leverages both the power of self-attention-based transformer architecture and pretraining on large-scale data. We endeavor to endow the model with strong capability of cross-modal understanding and generation. In this section, we describe the details of our proposed pretrained model , which refers to ulti-odality-to-ulti-odality ultitask ega-transformer.3.1      Å   Visual and Linguistic InputsThe mainstream multimodal pretraining methods transform images to feature sequences via object detection. However, the performance of the object detectors as well as the expressivity of their backbones strongly impact the final performance of the pretrained models in the downstream tasks. We observe that a large proportion of the images contain only a few objects. Take the images of the data of e-commerce as an example. We randomly sample 1M images and perform object detection on the images. The results show that over 90% of the images contain fewer than 5 objects. Also, the objects have high overlapping with each other. To alleviate such influence, we turn to a simple but effective solution following Gao et al. [\[12\]](#bookmark28) and Dosovitskiy et al. [\[8\]](#bookmark24). In general, we split an image into patches and extract features of the 2D patches with a trained feature extractor, say ResNet-50. Then we line up the representations to a sequence by their positions.  The processing of the input word sequence is much simpler. We follow the similar preprocessing procedures in the previous work [4, 11, 24]. We apply WordPiece [34, 45] and masking to the word sequence and embed them with an embedding layer, following BERT [6].3.2      Unified Encoder-DecoderWe integrate the image embeddings 𝑒𝑖 and the word embeddings 𝑒𝑡 into the cross-modal embedding sequence 𝑒 = {𝑒𝑖, 𝑒𝑡 }. We send the sequence to the transformer backbone for high-level feature extraction. To differ their representations, we add corresponding segment embeddings for different modalities. Specifically, we leverage theself-attention-based transformer blocks for our unified cross-modal representation learning. To be more specific, the building block is identical to that of BERT or GPT, which consists of self attention and point-wise feed-forward network (FFN). On top of the transformer backbone, we add an output layer for word prediction, and thus we tie its weights to those of the embedding layer.In the unified framework, we use different masking strategies to enable encoding and decoding. The input is segmented into three parts, including visual inputs, masked linguistic inputs, and complete linguistic inputs. We apply bidirectional masking to both the visual inputs and masked linguistic inputs, and we apply causal masking to the complete linguistic inputs. Thus the model is allowed to encode and decode in the same framework.3.3      Pretraining MethodsWe pretrain the model with the multitask setup, including text-to-text transfer, image-to-text transfer, and multimodality-to-text transfer. Thus the model can process information of different modalities and perform both single-modal and cross-modal understanding and generation. As demonstrated in Figure 3, the model learns to perform text denoising and language modeling in the setting of text-to-text transfer. In text denoising, we mask the input text by a proportion, which is 15% in practice following BERT [6]. Specifically, we mask a continuous span of text with a single mask, and the model should learn to decode the whole sequence. This encourages the model to learn both recovering and length predict-ing. Besides, in order to improve the model ability in generation, we add a setup of language modeling, where the encoder receives no inputs and the decoder learns to generate words based on the previous context.\
 Image-to-text transfer is similar to image captioning, where the model receives the visual information as the input, and learns to generate a corresponding description. In this setting, we add the aforementioned patch feature sequence to the input and leave the masked input blank. The model encodes the patch features, and decodes the corresponding text.Multimodality-to-text transfer Based on the setup of image-to-text transfer, we additionally add masked linguistic inputs, and thus the model should learn to generate the target text based on both the visual information and the noised linguistic information. This task allows the model to adapt to the downstream tasks with both visual and linguistic inputs.3.4      Scaling up to 10 and 100 Billion ParametersWe scale up the model size to 10 billion parameters and 100 billion parameters, which are named M6-10B and M6-100B. The increase in model size provides a much larger capacity for the model that it can learn knowledge from more data. For the construction of M6-10B, we simply scale up the model by hyperparameter tuning.To be more specific, we increase the size of hidden states and the number of layers. To better leverage GPU memory, we apply mixed-precision training and activation checkpointing to save memory. Still, the model cannot be fit into one single GPU, and thus we use model parallelism to split the feed-forward networks and attention heads to multiple GPUs following the implementation of Megatron-LM [36].However, directly scaling up to M6-100B is much more difficult as there are more challenges for the computation resources. Alternatively, inspired by the recent progress in sparse activations [10, 20, 35], we combine Mixture-of-Experts (MoE) with M6 to build the version of 100 billion parameters. Note that the original MoE requires mesh-tensorflow as well as TPUs. This sets limits for a number of researchers without such resources. Thus we implement the M6-100B with MoE with our in-house framework Whale [43] to perform model parallelism with GPUs. We demonstrate the key statistics of the models of different scales in Table 4.Specifically, different from the conventional FFN layer, the MoE layer is a parallel combination of multiple FFN layers, each of which acts as an expert. This is also called expert parallelism. The model first learns a sparse gating network to route the tokens to specific experts. Thus each token is only sent to a small set of experts and the computation can be much less compared with that in dense models. This kind of model is highly efficient as it realizes data parallelism and expert parallelism across workers. The computation of MoE layer for a specific token 𝑥 can be described as below:where 𝑔(·) refers to the sparse gating function, and T refers to the indices of top-𝑘 values of 𝑔(·). The output of MoE is a linear combination of the computation of selected expert FFNs 𝑓 (·).In expert parallelism, the parameters of experts do not share across workers, while those of other parts are identical across workers. Therefore, it is necessary to perform all-to-all communication across workers at the MoE layers in order to dispatch tokens to selected experts and combine them to their original experts. While Lepikhin et al. [20] and Fedus et al. [10] implement the MoE on TPUs with one expert in each MoE layer on a TPU, we implement our model on Nvidia GPUs where there are several experts in each MoE layer on a GPU so as to fully utilize the memory. As all-to-all communication takes up a large amount of time, the optimization to improve efficiency is highly significant. We implement a series of optimization, including half-precision communication. A key problem is load balancing, which denotes that tokens can gather to only a few experts due to dynamic routing. Following Fedus et al. [10], we apply expert capacity, which refers to the number of tokens for an expert (𝐶 = 𝑁 - 𝑐/m, where 𝐶 refers to expert capacity, 𝑁 refers to the number of tokens in a batch, 𝑐 refers to capacity factor (which is a hyperparameter usually larger than 1.0) and 𝑚 refers to the number of experts), to alleviate this problem. Tokens out of the capacity of an expert are dropped from the computation and they are sent to next layers through residual connections. We find that the overloading problem can be severe, and this issue can be a significant one in the future research of expert models.Besides the optimization in all-to-all communication, we com-pare the top-2 gating and top-1 gating and find that they can achieve similar model performance in perplexity, while the latter converges slightly slower. The effectiveness of top-1 gating enables faster computation. Besides, we also apply methods of memory optimization for higher efficiency. We find that gradient clipping globally can increase costs on all-to-all communication as it computes norms across all experts, and thus we apply local clipping for memory saving. We implement M6-100B with around 100 billion parameters on 128 Nvidia A100s and the speed of pretraining achieves 1440 samples/s (for samples of the sequence length of 272).We demonstrate that using MoE structure for model size scaling is effective and it can achieve similar performance to that of M6-10B, the largest dense model, within 2-3 times shorter time. The negative log perplexity of M6-100B reaches −2.297, in comparison with M6-10B that reaches −2.253 but with twice of time.2 This shows that the MoE-based M6 model has advantages on the time basis compared with dense models with many more FLOPs.4.1      Text-to-Image GenerationText-to-image generation has been an open problem for a long time. Previous studies mainly focused on generation on a limited domain, among which Generative Adversarial Nets (GANs) [14, 48] are dominated methods. Following Ramesh et al. [32], we leverage a two-stage framework for text-to-image generation, including discrete representation learning and language modeling.\
In the first stage, we focus on transforming images into sequences of discrete codes. There are a number of alternatives for discrete code generation, including VQVAE [41] and VQGAN [9]. In the second stage, it is necessary to build a language model to learn to generate text and code sequence. In the finetuning, we add code embedding and output layers to the pretrained M6. We concat the word sequence and the aforementioned generated code sequence as the input, and we set the objective of autoregressive language modeling for the training. At the stage of inference, we input the text sequence, and the model generates codes autoregressively with top-k sampling. The last step is to transform the code sequence to an image with the generator from the first stage.We construct a dataset for text-to-image generation in E-commerce. Specifically, we collect over 50 million product titles and images from the mobile Taobao. We apply a series of processing methods on the images to filter the unqualified. We filter the images with complex background features (characters, patterns, etc.) with the in-house white-background image detector and OCR model. We then filter the images with over 3 objects with our in-house object detector based on Faster R-CNN [33]. We finally obtain 1.8m high-quality product image-text pairs for finetuning. Compared with the images in the general domains, our collected data have the following features. The image and text are highly correlated as the text describes key features of the product, and there is no complex background in the images, which is easier to learn compared with the images in the public datasets such as MSCOCO [26].We demonstrate two examples in Figure 4 and Figure 5. It can be found that the generated images have high quality and the generated objects resemble the real ones. Furthermore, in Figure 6 , we find that the model is able to imagine items according to the query military style camouflage high heels(军旅风迷彩高跟鞋), which do not exist in the real world. The imagination ability provides room for creative design in real-world industrial scenarios, such as clothing design, shoe design, etc.We also finetune M6 under our proposed framework on another dataset which contains 3 million images crawled from the Internet, which cover more general domains. And we find that the model can adapt to different domains. As shown in Figure 7, the model is able to generate clip arts of robots . This reveals the versatility of the framework in text-to-image generation.4.2      Visual Question AnsweringWe demonstrate our experimental results on a visual question answering dataset, and we illustrate how we directly apply the pre-trained M6 to the VQA application.\
We leverage the FMIQA dataset [13] as the Chinese visual QA benchmark, which requires the model to generate the answer given an image and a question. We implement a transformer-based model as our baseline. For the evaluation, we split the test set manually by random sampling 200 from the dataset as there is no official release of the test set, and we evaluate the overall accuracy by human evaluation. The results are demonstrated in Table 5. The pretrained M6-base outperforms the baseline by a large margin (+6.2%), which indicates the effectiveness of multimodal pretraining. Scaling up the model to M6-10B further brings 5.2% improvement.Furthermore, we show that simply finetuning on such a small VQA dataset may limit the potential of M6. Therefore, we directly leverage M6 for the VQA application. We find that the model is able to recognize general features and provide more related knowledge based on its understanding. Though the model pretrained on pseudo-parallel image-text pairs cannot directly answer questions about detailed features, such as color, number, etc., it is able to answer questions related to background knowledge. We demonstrate some examples in Figure 8.4.3      Image CaptioningImage captioning requires the model to generate a caption that describes the given image, which examines the model ability of cross-modal generation. We construct a dataset (named E-Commerce IC) containing pairs of product descriptions and product images from Taobao. Since too long or too short descriptions may be noisy, we discard pairs with a description longer than 100 words or less than 10 words. To avoid dirty generations, we further use an in-house tool to filter descriptions that may contain dirty words (i.e., pornographic or violent words). Finally, E-Commerce IC contains about 260k text-image pairs. We finetune the model with the image-to-text transfer task on E-Commerce IC.\
We compare our model with a baseline of transformer in the human evaluation. We ask several annotators with the linguistic background to evaluate from three perspectives: grammar (whether a text is fluent without grammatical error), correctness (whether a text is faithful to the image), richness (whether a text is informative and attractive). During the evaluation, we randomly sample 100 images from the test set. For each image, an annotator is asked to score the text generated by different models. The scores are within the range of [0, 5].The results in Table 6 show that M6-base outperforms the baseline in all of the metrics. We find that all models achieve high scores in grammar. However, in both correctness and richness, M6-base outperforms the baseline model by a large margin (+18.2% and +14.4%), indicating that multimodal pretraining helps to generate more faithful, informative and attractive texts. Scaling up the model to M6-10B further improves the correctness and richness (about 14.7% and 7.0%). Figure 9 illustrates two examples of image caption.4.4      Question AnsweringTo demonstrate the potential availability in the applications of intelligent chatbots, we further employ the M6 model to generate long answers in the style of forum discussion. Human-generated questions are collected from various Chinese forums, which are input to the model to generate the answer. At the stage of inference, we append a question mark and a token  in the prompt, which better triggers the model to generate an answer. To facilitate the generation of longer and more informative texts, we pick more complex questions.Figure 10 demonstrates an example of general question answer-ing. The model can illustrate a man’s own experiences that are related to the question and also point out the answer at the end. This generated text confused human annotators and passed the Turing Test. It shows that the model can not only answer general questions but also generate long fluency text.We apply the pretrained model to Chinese poem generation. The model is able to generate genres with format constraints.\
Ancient Chinese poetry has various specific formats. We adopt the simplest constraints thatThe poem shall be consisted of at least 4 lines.The total number of lines shall be even.Each line must have exactly 5 or 7 words.All lines shall have the same number of words.Text generation under format constraint is done in a search framework that we generate short sentences ending with punctuation until the number of words meets the constraint. We repeat this process until the model generates an "" token, or the number of lines exceeds a limit of 16. Figure 11 illustrates an example of a generated poem.4.6      Image-Text MatchingWe evaluate the model’s ability in cross-modal retrieval. Specifically, we construct a dataset (named E-Commerce ITM) containing pairs of texts and images from the mobile Taobao. Each pair belongs to a single item. we collect 235K products in the clothing industry from Taobao. For each product, aside from the product image, we obtain a query by rewriting the product title. Specifically, we conduct named entity recognition on the title using an in-house tool, which extracts the terms describing the style, color, category and texture of the product.\
These terms are then concatenated into a natural language query, which is used in image-text matching. The length of each query is between 6 to 12 words. The pairs of the query and corresponding product image are labeled as positive samples. The negative samples are constructed by randomly substituting the query in the original pairs.We require the model to perform binary classification to discriminate positive and negative samples. We compare our model with InterBert [25], which is also a Chinese multi-modal pretrained model effective in cross-modal classification downstream tasks. The InterBert utilizes object-based features and has been pretrained on Taobao product image-text data as well.The results are shown in Table 7. It should be noted that the InterBert and M6-base are both implemented with transformer-based architecture and have similar model scales. However, M6-base still outperforms InterBert by 10.3%. In experiments, we find the product images generally contain relatively fewer detected objects, which may harm the performance on this task. In contrast, M6 avoids this problem by employing the patch features and achieves much better performance.The tremendous success of NLP pretraining, including BERT [6], GPT [2, 30, 31], and also some other related studies [1, 7, 19, 27, 49], inspires the research in cross-modal representation learning. Also, recent studies show that the ubiquitous Transformer architecture [42] can be extended to different fields, including computer vision [3, 8]. Therefore, the simplest solution to incorporate recent pretraining methods and cross-modal representation learning is the extension of BERT. From the perspective of architecture, there are mainly two types, including single-stream model and dual stream model. Specifically, single-stream model is simple and it gradually becomes the mainstream architecture. These models mostly differ in their designs of pretraining tasks or the construction of input im-age features. Basically, they are mainly pretrained masked language modeling, masked object classification, and image-text matching. VisualBERT [23] and Unicoder-VL [22] simply use BERT and are pretrained with the aforementioned tasks. UNITER [4] pretrains the model with an additional task of word-region alignment. Oscar [24] enhances the alignment between objects and their corresponding words or phrases. VILLA [11] further improves model performance by adding their proposed adversarial learning methods to pretraining and finetuning. Except for pretraining tasks, some studies focus on the features of images. Most pretraining methods for multimodal representation learning utilize the features generated by a trained object detector, say Faster R-CNN [33]. PixelBERT [17] accepts raw images as input and extract their latent representations with a learnable ResNet [15] or ResNext [46]. FashionBERT [12] splits the images into patches with a trained ResNet without co-training. Besides single-stream models, dual-stream models also can achieve outstanding performance, such as VilBERT [28], LXMERT [40] and InterBERT [25]. ViLBERT-MT [29] enhances model performance with multi-task finetuning. ERNIE-ViL [50] enhances the model with the application of scene graph information. In spite of these successful cases, it still requires further researches to unmask the success of multimodal pretraining.In this work, we propose the largest dataset M6-Corpus for pre-training in Chinese, which consists of over 1.9TB images and 292GB texts. The dataset has large coverage over domains, including encyclopedia, question answering, forum discussion, common crawl, etc. We propose a method called M6 that is able to process information of multiple modalities and perform both single-modal and cross-modal understanding and generation. The model is scaled to large model with 10B and 100B parameters with sophisticated deployment, and both models are the largest multimodal pretrained models. We apply the model to a series of downstream applications, showing its versatility. More specifically, we design a downstream task of text-guided image generation, and the finetuned M6 can reach superior performance by producing images of high quality.In the future, we will continue the pretraining of extremely large models by increasing the scale of data and models to explore the limit of performance, and we also endeavor to search for more downstream applications for further generalization.[1]   Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-masked language models for unified language model pre-training. In International Conference on Machine Learning. PMLR, 642–652.[2]   Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).[3]   Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In European Conference on Computer Vision. Springer, 213–229.[4]   Y en-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. UNITER: UNiversal Image-TExt Representation Learning. In . 104–120.[5]   Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting pre-trained models for chinese natural language processing. arXiv preprint arXiv:2004.13922 (2020).[6]   Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In . 4171–4186.[7]   Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified Language Model Pre-training for Natural Language Understanding and Generation. In . 13042–13054.[8]   Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).[9]   Patrick Esser, Robin Rombach, and Björn Ommer. 2020. Taming Transformers for High-Resolution Image Synthesis. arXiv:2012.09841 [cs.CV][10]   William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.  abs/2101.03961 (2021). arXiv:2101.03961https://arxiv.org/abs/2101.03961[11]   Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. 2020. Large-Scale Adversarial Training for Vision-and-Language Representation Learning. In .[12]   Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, and Hao Wang. 2020. Fashionbert: Text and image matching with adaptive loss for cross-modal retrieval. In . 2251–2260.[13]   Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to a machine? dataset and methods for multilingual image question answering. arXiv preprint arXiv:1505.05612 (2015).[14]   Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial networks. arXiv preprint arXiv:1406.2661 (2014).[15]   Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In . 770–778.[16]   Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. De-berta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 (2020).[17]   Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849 (2020).[18]   Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. 2020. Convbert: Improving bert with span-based dynamic convolution. arXiv preprint arXiv:2008.02496 (2020).[19]   Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.  abs/1909.11942 (2019).[20]   Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668 (2020).[21]   Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettle-moyer. 2021. BASE Layers: Simplifying Training of Large, Sparse Models.  abs/2103.16716 (2021).[22]   Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. 2019. Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training.  abs/1908.06066 (2019).[23]   Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. VisualBERT: A Simple and Performant Baseline for Vision and Language.  abs/1908.03557 (2019).[24]   Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. 2020. Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks.  abs/2004.06165 (2020).[25]   Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. 2020. Interbert: Vision-and-language interaction for multi-modal pretraining. arXiv preprint arXiv:2003.13198 (2020).[26]   Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In . 740–755.[27]   Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach.  abs/1907.11692 (2019).[28]   Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In . 13–23.[29]   Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 2019. 12-in-1: Multi-Task Vision and Language Representation Learning.  abs/1912.02315 (2019).[31]   Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. [n.d.]. Language models are unsupervised multitask learners. ([n. d.]).[32]   Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. arXiv:2102.12092 [cs.CV][33]   Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In . 91–99.[34]   Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In .[35]   Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017).[36]   Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv preprint arXiv:1909.08053 (2019).[37]   Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. MASS: Masked Sequence to Sequence Pre-training for Language Generation. In . 5926–5936.[38]   Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In .[39]   Maosong Sun, Jingyang Li, Zhipeng Guo, Z Yu, Y Zheng, X Si, and Z Liu. 2016. Thuctc: an efficient chinese text classifier.  (2016).[40]   Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In . 5099–5110.[41]   Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural Discrete Representation Learning. In .[42]   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In . 5998–6008.[43]   Ang Wang, Xianyan Jia, Le Jiang, Jie Zhang, Yong Li, and Wei Lin. 2020. Whale: A Unified Distributed Training Framework. arXiv preprint arXiv:2011.09208 (2020).[44]   Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si. 2019. Structbert: Incorporating language structures into pre-training for deep language understanding. arXiv preprint arXiv:1908.04577 (2019).[45]   Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 (2016).[46]   Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. 2017. Aggregated residual transformations for deep neural networks. In . 1492–1500.[47]   Liang Xu, Xuanwei Zhang, and Qianqian Dong. 2020. CLUECorpus2020: A Large-scale Chinese Corpus for Pre-trainingLanguage Model. arXiv preprint arXiv:2003.01355 (2020).[48]   Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1316–1324.[49]   Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. In . 5754–5764.[50]   Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. arXiv preprint arXiv:2006.16934 (2020).[51]   Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. 2020. Unified Vision-Language Pre-Training for Image Captioning and VQA. In . 13041–13049.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.  ]]></content:encoded></item><item><title>Xiaomi launches 17 Ultra smartphone, an AirTag clone, and an ultra slim powerbank</title><link>https://techcrunch.com/2026/02/28/xiaomi-launches-17-ultra-smartphone-an-airtag-clone-and-an-ultra-slim-powerbank/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:09:03 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[We round up everything Xiaomi announced at its Mobile World Congress event.]]></content:encoded></item><item><title>Alibaba’s Qwen: The Chinese AI Model Challenging Silicon Valley</title><link>https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss</link><author>Alibaba</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:08:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce QWEN1, the first installment of our large language model series. QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes QWEN, the base pretrained language models, and QWEN-CHAT, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, CODE-QWEN and CODE-QWEN-CHAT, as well as mathematics-focused models, MATH-QWEN-CHAT, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models. \n \
Despite their impressive capabilities, LLMs are often criticized for their lack of reproducibility, steerability, and accessibility to service providers. In this work, we are pleased to present and release the initial version of our LLM series, QWEN. QWEN is a moniker that derives from the Chinese phrase Qianwen, which translates to “thousands of prompts” and conveys the notion of embracing a wide range of inquiries. QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. The model series include the base pretrained language models, chat models finetuned with human alignment techniques, i.e., supervised finetuning (SFT), reinforcement learning with human feedback (RLHF), etc., as well as specialized models in coding and math. The details are outlined below:1.   The base language models, namely QWEN, have undergone extensive training using up to 3 trillion tokens of diverse texts and codes, encompassing a wide range of areas. These models have consistently demonstrated superior performance across a multitude of downstream tasks, even when compared to their more significantly larger counterparts.2.   The QWEN-CHAT models have been carefully finetuned on a curated dataset relevant to task performing, chat, tool use, agent, safety, etc. The benchmark evaluation demonstrates that the SFT models can achieve superior performance. Furthermore, we have trained reward models to mimic human preference and applied them in RLHF for chat models that can produce responses preferred by humans. Through the human evaluation of a challenging test, we find that QWEN-CHAT models trained with RLHF are highly competitive, still falling behind GPT-4 on our benchmark.3.    In addition, we present specialized models called CODE-QWEN, which includes CODE-QWEN-7B and CODE-QWEN-14B, as well as their chat models, CODE-QWEN-14B-CHAT and CODE-QWEN-7B-CHAT. Specifically, CODE-QWEN has been pre-trained on extensive datasets of code and further fine-tuned to handle conversations related to code generation, debugging, and interpretation. The results of experiments conducted on benchmark datasets, such as HumanEval (Chen et al.,2021), MBPP (Austin et al.,2021), and HumanEvalPack (Muennighoff et al.,2023), demonstrate the high level of proficiency of CODE-QWEN in code understanding and generation.4.   This research additionally introduces MATH-QWEN-CHAT specifically designed to tackle mathematical problems. Our results show that both MATH-QWEN-7B-CHAT and MATH-QWEN-14B-CHAT outperform open-sourced models in the same sizes with large margins and are approaching GPT-3.5 on math-related benchmark datasets such as GSM8K (Cobbeet al.,2021) and MATH (Hendrycks et al.,2021).5.    Besides, we have open-sourced QWEN-VL and QWEN-VL-CHAT, which have the versatile ability to comprehend visual and language instructions. These models outperform the current open-source vision-language models across various evaluation benchmarks and support text recognition and visual grounding in both Chinese and English languages. Moreover, these models enable multi-image conversations and storytelling. Further details can be found in Bai et al.(2023).\
Now, we officially open-source the 14B-parameter and 7B-parameter base pretrained models QWEN and aligned chat models QWEN-CHAT2. This release aims at providing more comprehensive and powerful LLMs at developer- or application-friendly scales.The structure of this report is as follows: Section 2 describes our approach to pretraining and results of QWEN. Section 3 covers our methodology for alignment and reports the results of both automatic evaluation and human evaluation. Additionally, this section describes details about our efforts in building chat models capable of tool use, code interpreter, and agent. In Sections 4 and 5, we delve into specialized models of coding and math and their performance. Section 6 provides an overview of relevant related work, and Section 7 concludes this paper and points out our future work.The pretraining stage involves learning vast amount of data to acquire a comprehensive understanding of the world and its various complexities. This includes not only basic language capabilities but also advanced skills such as arithmetic, coding, and logical reasoning. In this section, we introduce the data, the model design and scaling, as well as the comprehensive evaluation results on benchmark datasets.The size of data has proven to be a crucial factor in developing a robust large language model, as highlighted in previous research (Hoffmann et al.,2022;Touvron et al.,2023b). To create an effective pretraining dataset, it is essential to ensure that the data are diverse and cover a wide range of types, domains, and tasks. Our dataset is designed to meet these requirements and includes public web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.\
To ensure the quality of our pretraining data, we have developed a comprehensive data preprocessing procedure. For public web data, we extract text from HTML and use language identification tools to determine the language. To increase the diversity of our data, we employ deduplication techniques, including exact-match deduplication after normalization and fuzzy deduplication using MinHash and LSH algorithms. To filter out low-quality data, we employ a combination of rule-based and machine-learning-based methods. Specifically, we use multiple models to score the content, including language models, text-quality scoring models, and models for identifying potentially offensive or inappropriate content. We also manually sample texts from various sources and review them to ensure their quality. To further enhance the quality of our data, we selectively up-sample data from certain sources, to ensure that our models are trained on a diverse range of high-quality content. In recent studies (Zeng et al.,2022;Aribandi et al.,2021;Raffel et al.,2020), it has been demonstrated that pretraining language models with multi-task instructions can enhance their zero-shot and few-shot performance. To further enhance the performance of our model, we have incorporated high-quality instruction data into our pretraining process. To safeguard the integrity of our benchmark assessment, we have adopted a similar approach as Brown et al.(2020) and meticulously eliminated any instruction samples that exhibit a 13-gram overlap with any data present in the test sets utilized in our evaluation.\
Given the large number of downstream tasks, it is not feasible to repeat this filtering process for all tasks. Instead, we have made sure that the instruction data for the reported tasks have undergone our filtering process to ensure their accuracy and reliability. Finally, we have built a dataset of up to 3 trillion tokens.The design of vocabulary significantly impacts the training efficiency and the downstream task performance. In this study, we utilize byte pair encoding (BPE) as our tokenization method, following GPT-3.5 and GPT-4. We start with the open-source fast BPE tokenizer, tiktoken (Jain,2022), and select the vocabulary cl100k base as our starting point. To enhance the performance of our model on multilingual downstream tasks, particularly in Chinese, we augment the vocabulary with commonly used Chinese characters and words, as well as those in other languages. Also, following Touvron et al.(2023a;b), we have split numbers into single digits. The final vocabulary size is approximately 152K.The performance of the QWEN tokenizer in terms of compression is depicted in Figure 3. In this comparison, we have evaluated QWEN against several other tokenizers, including XLM-R (Conneauet al.,2019), LLaMA (Touvron et al.,2023a), Baichuan (Inc.,2023a), and InternLM (InternLM Team,2023). Our findings reveal that QWEN achieves higher compression efficiency than its competitors in most languages. This implies that the cost of serving can be significantly reduced since a smaller number of tokens from QWEN can convey more information than its competitors. Furthermore, we have conducted preliminary experiments to ensure that scaling the vocabulary size of QWEN does not negatively impact the downstream performance of the pretrained model. Despite the increase in vocabulary size, our experiments have shown that QWEN maintains its performance levels in downstream evaluation.QWEN is designed using a modified version of the Transformer architecture. Specifically, we have adopted the recent open-source approach of training large language models, LLaMA (Touvron et al.,2023a), which is widely regarded as the top open-source LLM. Our modifications to the architecture include:Embedding and output projection. Based on preliminary experimental findings, we have opted for the untied embedding approach instead of tying the weights of input embedding and output projection. This decision was made in order to achieve better performance with the price of memory costs.. We have chosen RoPE (Rotary Positional Embedding) (Su et al.,2021) as our preferred option for incorporating positional information into our model. RoPE has been widely adopted and has demonstrated success in contemporary large language models, notably PaLM (Chowdhery et al.,2022;Anil et al.,2023) and LLaMA (Touvronet al.,2023a;b). In particular, we have opted to use FP32 precision for the inverse frequency matrix, rather than BF16 or FP16, in order to prioritize model performance and achieve higher accuracy.. For most layers, we remove biases following Chowdhery et al.(2022), but we add biases in the QKV layer of attention to enhance the extrapolation ability of the model (Su,2023b).. In modern Transformer models, pre-normalization is the most widely used approach, which has been shown to improve training stability compared to post-normalization. Recent research has suggested alternative methods for better training stability, which we plan to explore in future versions of our model. Additionally, we have replaced the traditional layer normalization technique described in (Ba et al.,2016) with RMSNorm (Jiang et al.,2023). This change has resulted in equivalent performance while also improving efficiency.. We have selected SwiGLU (Shazeer,2020) as our activation function, a combination of Swish (Ramachandran et al.,2017) and Gated Linear Unit (Dauphin et al.,2017). Our initial experiments have shown that activation functions based on GLU generally outperform other baseline options, such as GeLU (Hendrycks & Gimpel,2016). As is common practice in previous research, we have reduced the dimension of the feed-forward network (FFN) from 4 times the hidden size to 8/3 of the hidden size.To train QWEN, we follow the standard approach of autoregressive language modeling, as described in Radford et al.(2018). This involves training the model to predict the next token based on the context provided by the previous tokens. We train models with context lengths of 2048. To create batches of data, we shuffle and merge the documents, and then truncate them to the specified context lengths. To improve computational efficiency and reduce memory usage, we employ Flash Attention in the attention modules (Dao et al.,2022). We adopt the standard optimizer AdamW (Kingma & Ba,2014;Loshchilov & Hutter,2017) for pretraining optimization. We set the hyperparameters 1 = 0*.*9, 2 = 0*.*95, and  = 10−8. We use a cosine learning rate schedule with a specified peak learning rate for each model size. The learning rate is decayed to a minimum learning rate of 10% of the peak learning rate. All the models are trained with BFloat16 mixed precision for training stability.2.5         Context Length ExtensionTransformer models have a significant limitation in terms of the context length for their attention mechanism. As the context length increases, the quadratic-complexity computation leads to a drastic increase in both computation and memory costs. In this work, we have implemented simple training-free techniques that are solely applied during inference to extend the context length of the model. One of the key techniques we have used is NTK-aware interpolation (bloc97,2023).\
Unlike position interpolation (PI) (Chen et al.,2023a) which scales each dimension of RoPE equally, NTK-aware interpolation adjusts the base of RoPE to prevent the loss of high-frequency information in a training-free manner. To further improve performance, we have also implemented a trivial extension called dynamic NTK-aware interpolation, which is later formally discussed in (Peng et al.,2023a). It dynamically changes the scale by chunks, avoiding severe performance degradation. These techniques allow us to effectively extend the context length of Transformer models without compromising their computational efficiency or accuracy.QWEN additionally incorporates two attention mechanisms: LogN-Scaling (Chiang & Cholak,2022;Su,2023a) and window attention (Beltagy et al.,2020). LogN-Scaling rescales the dot product of the query and value by a factor that depends on the ratio of the context length to the training length, ensuring that the entropy of the attention value remains stable as the context length grows. Window attention restricts the attention to a limited context window, preventing the model from attending to tokens that are too far away.We also observed that the long-context modeling ability of our model varies across layers, with lower layers being more sensitive in context length extension compared to the higher layers. To leverage this observation, we assign different window sizes to each layer, using shorter windows for lower layers and longer windows for higher layers.2.6         Experimental Results\
In this evaluation, we focus on the base language models without alignment and collect the baselines’ best scores from their official results and OpenCompass (OpenCompass Team,2023). The results are presented in Table 2.Our experimental results demonstrate that the three QWEN models exhibit exceptional performance across all downstream tasks. It is worth noting that even the larger models, such as LLaMA2-70B, are outperformed by QWEN-14B in 3 tasks. QWEN-7B also performs admirably, surpassing LLaMA2-13B and achieving comparable results to Baichuan2-13B. Notably, despite having a relatively small number of parameters, QWEN-1.8B is capable of competitive performance on certain tasks and even outperforms larger models in some instances. The findings highlight the impressive capabilities of the QWEN models, particularly QWEN-14B, and suggest that smaller models, such as QWEN-1.8B, can still achieve strong performance in certain applications.To evaluate the effectiveness of context length extension, Table 3 presents the test results on arXiv3 in terms of perplexity (PPL). These results demonstrate that by combining NTK-aware interpolation, LogN-Scaling, and layer-wise window assignment, we can effectively maintain the performance of our models in the context of over 8192 tokens.Pretrained large language models have been found to be not aligned with human behavior, making them unsuitable for serving as AI assistants in most cases. Recent research has shown that the use of alignment techniques, such as supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), can significantly improve the ability of language models to engage in natural conversation. In this section, we will delve into the details of how QWEN models have been trained using SFT and RLHF, and evaluate their performance in the context of chat-based assistance.3.1         Supervised FinetuningTo gain an understanding of human behavior, the initial step is to carry out SFT, which finetunes a pretrained LLM on chat-style data, including both queries and responses. In the following sections, we will delve into the details of data construction and training methods.To enhance the capabilities of our supervised finetuning datasets, we have annotated conversations in multiple styles. While conventional datasets (Wei et al.,2022a) contain a vast amount of data prompted with questions, instructions, and answers in natural language, our approach takes it a step further by annotating human-style conversations. This practice, inspired by Ouyang et al.(2022), aims at improving the model’s helpfulness by focusing on natural language generation for diverse tasks. To ensure the model’s ability to generalize to a wide range of scenarios, we specifically excluded data formatted in prompt templates that could potentially limit its capabilities. Furthermore, we have prioritized the safety of the language model by annotating data related to safety concerns such as violence, bias, and pornography.In addition to data quality, we have observed that the training method can significantly impact the final performance of the model. To achieve this, we utilized the ChatML-style format (OpenAI,2022), which is a versatile meta language capable of describing both the metadata (such as roles) and the content of a turn. This format enables the model to effectively distinguish between various types of information, including system setup, user inputs, and assistant outputs, among others. By leveraging this approach, we can enhance the model’s ability to accurately process and analyze complex conversational data.Consistent with pretraining, we also apply next-token prediction as the training task for SFT. We apply the loss masks for the system and user inputs. More details are demonstrated in Section A.1.1.The model’s training process utilizes the AdamW optimizer, with the following hyperparameters: 1 set to 0*.2 set to 0. set to 10−8. The sequence length is limited to 2048, and the batch size is 128. The model undergoes a total of 4000 steps, with the learning rate gradually increased over the first 1430 steps, reaching a peak of 2  10−6. To prevent overfitting, weight decay is applied with a value of 0..1, and gradient clipping is enforced with a limit of 1.*0.3.2         Reinforcement Learning from Human FeedbackWhile SFT has proven to be effective, we acknowledge that its generalization and creativity capa-bilities may be limited, and it is prone to overfitting. To address this issue, we have implemented Reinforcement Learning from Human Feedback (RLHF) to further align SFT models with human preferences, following the approaches of Ouyang et al.(2022);Christiano et al.(2017). This process involves training a reward model and using Proximal Policy Optimization (PPO) (Schulman et al.,2017) to conduct policy training.3.2.1          Reward ModelTo create a successful reward model, like building a large language model (LLM), it is crucial to first undergo pretraining and then finetuning. This pretraining process, also known as preference model pretraining (PMP) (Bai et al.,2022b), necessitates a vast dataset of comparison data. This dataset consists of sample pairs, each containing two distinct responses for a single query and their corresponding preferences. Similarly, finetuning is also conducted on this type of comparison data, but with a higher quality due to the presence of quality annotations.During the fine-tuning phase, we gather a variety of prompts and adjust the reward model based on human feedback for responses from the QWEN models. To ensure the diversity and complexity of user prompts are properly taken into account, we have created a classification system with around 6600 detailed tags and implemented a balanced sampling algorithm that considers both diversity and complexity when selecting prompts for annotation by the reward model (Lu et al.,2023). To generate a wide range of responses, we have utilized QWEN models of different sizes and sampling strategies, as diverse responses can help reduce annotation difficulties and enhance the performance of the reward model. These responses are then evaluated by annotators following a standard annotation guideline, and comparison pairs are formed based on their scores.In creating the reward model, we utilize the same-sized pre-trained language model QWEN to initiate the process. It is important to mention that we have incorporated a pooling layer into the original QWEN model to extract the reward for a sentence based on a specific end token.\n The learning rate for this process has been set to a constant value of 3  10−6, and the batch size is 64. Additionally, the sequence length is set to 2048, and the training process lasts for a single epoch.We adopted the accuracy on the test dataset as an important but not exclusive evaluation metric for the reward model. In Table 4, we report the test pairwise accuracy of PMP and reward models on diverse human preference benchmark datasets (Bai et al.,2022b;Stiennon et al.,2020;Ethayarajhet al.,2022;Lightman et al.,2023). Specifically, QWEN Helpful-base and QWEN Helpful-online are our proprietary datasets. The responses in QWEN Helpful-base are generated from QWEN without RLHF, whereas QWEN Helpful-online includes responses from QWEN with RLHF. The results show that the PMP model demonstrates high generalization capabilities on out-of-distribution data, and the reward model demonstrates significant improvement on our QWEN reward datasets.3.2.2          Reinforcement LearningOur Proximal Policy Optimization (PPO) process involves four models: the policy model, value model, reference model, and reward model. Before starting the PPO procedure, we pause the policy model’s updates and focus solely on updating the value model for 50 steps. This approach ensures that the value model can adapt to different reward models effectively.During the PPO operation, we use a strategy of sampling two responses for each query simultaneously. This strategy has proven to be more effective based on our internal benchmarking evaluations. We set the KL divergence coefficient to 0*.*04 and normalize the reward based on the running mean.The policy and value models have learning rates of 1  10−6 and 5  10−6, respectively. To enhance training stability, we utilize value loss clipping with a clip value of 0*.15. For inference, the policy top-p is set to 0.9. Our findings indicate that although the entropy is slightly lower than when top-p is set to 1.*0, there is a faster increase in reward, ultimately resulting in consistently higher evaluation rewards under similar conditions.Additionally, we have implemented a pretrained gradient to mitigate the alignment tax. Empirical findings indicate that, with this specific reward model, the KL penalty is adequately robust to counteract the alignment tax in benchmarks that are not strictly code or math in nature, such as those that test common sense knowledge and reading comprehension. It is imperative to utilize a significantly larger volume of the pretrained data in comparison to the PPO data to ensure the effectiveness of the pretrained gradient. Additionally, our empirical study suggests that an overly large value for this coefficient can considerably impede the alignment to the reward model, eventually compromising the ultimate alignment, while an overly small value would only have a marginal effect on alignment tax reduction.3.3         Automatic and Human Evaluation of Aligned ModelsTo showcase the effectiveness of our aligned models, we conduct a comparison with other aligned models on well-established benchmarks, including MMLU (Hendrycks et al.,2020), C-Eval (Huanget al.,2023), GSM8K (Cobbe et al.,2021), HumanEval (Chen et al.,2021), and BBH (Suzgun et al.,2022). Besides the widely used few-shot setting, we test our aligned models in the zero-shot setting to demonstrate how well the models follow instructions. The prompt in a zero-shot setting consists of an instruction and a question without any previous examples in the context. The results of the baselines are collected from their official reports and OpenCompass (OpenCompass Team,2023).The results in Table 5 demonstrate the effectiveness of our aligned models in understanding human instructions and generating appropriate responses. QWEN-14B-Chat outperforms all other models except ChatGPT (OpenAI, 2022) and LLAMA 2-CHAT-70B (Touvron et al., 2023b) in all datasets, including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and BBH (Suzgun et al., 2022).\n In particular, QWEN’s performance in HumanEval, which measures the quality of generated codes, is significantly higher than that of other open-source models.Moreover, QWEN’s performance is consistently better than that of open-source models of similar size, such as LLaMA2 (Touvron et al.,2023b), ChatGLM2 (ChatGLM2 Team,2023), InternLM (InternLMTeam,2023), and Baichuan2 (Yang et al.,2023). This suggests that our alignment approach, which involves fine-tuning the model on a large dataset of human conversations, has been effective in improving the model’s ability to understand and generate human-like language./imDespite this, we have reservations about the ability of traditional benchmark evaluation to accurately measure the performance and potential of chat models trained with alignment techniques in today’s landscape. The results mentioned earlier provide some evidence of our competitive standing, but we believe that it is crucial to develop new evaluation methods specifically tailored to aligned models.We believe that human evaluation is crucial, which is why we have created a carefully curated dataset for this purpose. Our process involved collecting 300 instructions in Chinese that covered a wide range of topics, including knowledge, language understanding, creative writing, coding, and mathematics. To evaluate the performance of different models, we chose the SFT version of QWEN-CHAT-7B and the SFT and RLHF versions of QWEN-CHAT-14B, and added two strong baselines, GPT-3.5 and GPT-44, for comparison. For each instruction, we asked three annotators to rank the model responses by the overall score of helpfulness, informativeness, validity, and other relevant factors. Our dataset and evaluation methodology provides a comprehensive and rigorous assessment of the capabilities of different language models in various domains.Figure 4 illustrates the win rates of the various models. For each model, we report the percentage of wins, ties, and losses against GPT-3.5, with the segments of each bar from bottom to top representing these statistics. The experimental results clearly demonstrate that the RLHF model outperforms the SFT models by significant margins, indicating that RLHF can encourage the model to generate responses that are more preferred by humans. In terms of overall performance, we find that the RLHF model significantly outperforms the SFT models, falling behind GPT-4. This indicates the effectiveness of RLHF for aligning to human preference. To provide a more comprehensive understanding of the models’ performance, we include a case study with examples from different models in Appendix A.2.2. Nonetheless, it remains difficult to accurately capture the gap between our models and the proprietary models. As such, a more extensive and rigorous assessment is required for the chat models.The QWEN models, which are designed to be versatile, have the remarkable ability to assist with (semi-)automating daily tasks by leveraging their skills in tool-use and planning. As such, they can serve as agents or copilots to help streamline various tasks. We explore QWEN’s proficiency in the following areas:•     Using a Python code interpreter to enhance math reasoning, data analysis, and more (see Table 7 and Table 8).•     Functioning as an agent that accesses Hugging Face’s extensive collection of multimodal models while engaging with humans (see Table 9).\
To enhance QWEN’s capabilities as an agent or copilot, we employ the self-instruct (Wang et al.,2023c) strategy for SFT. Specifically, we utilize the in-context learning capability of QWEN for self-instruction. By providing a few examples, we can prompt QWEN to generate more relevant queries and generate outputs that follow a specific format, such as ReAct (Yao et al.,2022). We then apply rules and involve human annotators to filter out any noisy samples. Afterwards, the samples are incorporated into QWEN’s training data, resulting in an updated version of QWEN that is more dependable for self-instruction. We iterate through this process multiple times until we gather an ample number of samples that possess both exceptional quality and a wide range of diversity. As a result, our final collection consists of around 2000 high-quality samples.During the finetuning process, we mix these high-quality samples with all the other general-purpose SFT samples, rather than introducing an additional training stage. By doing so, we are able to retain essential general-purpose capabilities that are also pertinent for constructing agent applications.\
Using Tools via ReAct Prompting We have created and made publicly available a benchmark for evaluating QWEN’s ability to call plugins, tools, functions, or APIs using ReAct Prompting (see Qwen Team, Alibaba Group,2023b). To ensure fair evaluation, we have excluded any plugins that were included in QWEN’s training set from the evaluation set. The benchmark assesses the model’s accuracy in selecting the correct plugin from a pool of up to five candidates, as well as the plausibility of the parameters passed into the plugin and the frequency of false positives. In this evaluation, a false positive occurs when the model incorrectly invokes a plugin in response to a query, despite not being required to do so.The results presented in Table 6 demonstrate that QWEN consistently achieves higher accuracy in identifying the relevance of a query to the available tools as the model size increases. However, the table also highlights that beyond a certain point, there is little improvement in performance when it comes to selecting the appropriate tool and providing relevant arguments. This suggests that the current preliminary benchmark may be relatively easy and may require further enhancement in future iterations. It is worth noting that GPT-3.5 stands out as an exception, displaying suboptimal performance on this particular benchmark. This could potentially be attributed to the fact that the benchmark primarily focuses on the Chinese language, which may not align well with GPT-3.5’s capabilities. Additionally, we observe that GPT-3.5 tends to attempt to use at least one tool, even if the query cannot be effectively addressed by the provided tools.\
Using Code Interpreter for Math Reasoning and Data Analysis The Python code interpreter is widely regarded as a powerful tool for augmenting the capabilities of an LLM agent. It is worth investigating whether QWEN can harness the full potential of this interpreter to enhance its performance in diverse domains, such as mathematical reasoning and data analysis. To facilitate this exploration, we have developed and made publicly available a benchmark that is specifically tailored for this purpose (see Qwen Team, Alibaba Group,2023a).The benchmark encompasses three primary categories of tasks: math problem-solving, data visualization, and other general-purpose tasks like file post-processing and web crawling. Within the visualization tasks, we differentiate between two levels of difficulty. The easier level can be achieved by simply writing and executing a single code snippet without the need for advanced planning skills. However, the more challenging level requires strategic planning and executing multiple code snippets in a sequential manner. This is because the subsequent code must be written based on the output of the previous code. For example, an agent may need to examine the structure of a CSV file using one code snippet before proceeding to write and execute additional code to create a plot.Regarding evaluation metrics, we consider both the executability and correctness of the generated code. To elaborate on the correctness metrics, for math problems, we measure accuracy by verifying if the ground truth numerical answer is present in both the code execution result and the final response. When it comes to data visualization, we assess accuracy by utilizing QWEN-VL (Bai et al.,2023), a powerful multimodal language model. QWEN-VL is capable of answering text questions paired with images, and we rely on it to confirm whether the image generated by the code fulfills the user’s request.The results regarding executability and correctness are presented in Table 7 and Table 8, respectively. It is evident that CODE LLAMA generally outperforms LLAMA 2, its generalist counterpart, which is not surprising since this benchmark specifically requires coding skills. However, it is worth noting that specialist models that are optimized for code synthesis do not necessarily outperform generalist models. This is due to the fact that this benchmark encompasses various skills beyond coding, such as abstracting math problems into equations, understanding language-specified constraints, and responding in the specified format such as ReAct. Notably, QWEN-7B-CHAT and QWEN-14B-CHAT surpass all other open-source alternatives of similar scale significantly, despite being generalist models.\
Serving as a Hugging Face Agent Hugging Face provides a framework called the Hugging Face Agent or Transformers Agent (Hugging Face,2023), which empowers LLM agents with a curated set of multimodal tools, including speech recognition and image synthesis. This framework allows an LLM agent to interact with humans, interpret natural language commands, and employ the provided tools as needed.To evaluate QWEN’s effectiveness as a Hugging Face agent, we utilized the evaluation benchmarks offered by Hugging Face. The results are presented in Table 9. The evaluation results reveal that QWEN performs quite well in comparison to other open-source alternatives, only slightly behind the proprietary GPT-4, demonstrating QWEN’s competitive capabilities.4         Code-Qwen: Specialized Model for CodingTraining on domain-specific data has been shown to be highly effective, particularly in the case of code pretraining and finetuning. A language model that has been reinforced with training on code data can serve as a valuable tool for coding, debugging, and interpretation, among other tasks. In this work, we have developed a series of generalist models using pretraining and alignment techniques. Building on this foundation, we have created domain-specific models for coding by leveraging the base language models of QWEN, including continued pretrained model, CODE-QWEN and supervised finetuned model, CODE-QWEN-CHAT. Both models have 14 billion and 7 billion parameters versions.4.1         Code PretrainingWe believe that relying solely on code data for pretraining can result in a significant loss of the ability to function as a versatile assistant. Unlike previous approaches that focused solely on pretraining on code data (Li et al.,2022;2023d), we take a different approach (Rozie`re et al.,2023) by starting with our base models QWEN trained on a combination of text and code data, and then continuing to pretrain on the code data. We continue to pretrain the models on a total of around 90 billion tokens. During the pre-training phase, we initialize the model using the base language models QWEN. Many applications that rely on specialized models for coding may encounter lengthy contextual scenarios, such as tool usage and code interpretation, as mentioned in Section 3.4. To address this issue, we train our models with context lengths of up to 8192. Similar to base model training in Section 2.4, we employ Flash Attention (Dao et al.,2022) in the attention modules, and adopt the standard optimizer AdamW (Kingma & Ba,2014;Loshchilov & Hutter,2017), setting 1 = 0*.2 = 0. = 10−8. We set the learning rate as 6. 10−5 for CODE-QWEN-14B and 3.*0  10−5 for CODE-QWEN-7B, with 3% warm up iterations and no learning rate decays.4.2         Code  Supervised  Fine-TuningAfter conducting a series of empirical experiments, we have determined that the multi-stage SFT strategy yields the best performance compared to other methods. In the supervised fine-tuning stage, the model CODE-QWEN-CHAT initialized by the code foundation model CODE-QWEN are optimized by the AdamW (Kingma & Ba,2014;Loshchilov & Hutter,2017) optimizer (1 = 0*.2 = 0.*95,  = 10−8) with a learning rate of 2*. 10−6 and 1.*0  10−5 for the 14B and 7B model respectively. The learning rate increases to the peaking value with the cosine learning rate schedule (3% warm-up steps) and then remains constant.Our CODE-QWEN models have been compared with both proprietary and open-source language models, as shown in Tables 10 and 11. These tables present the results of our evaluation on the test sets of Humaneval (Chen et al.,2021), MBPP (Austin et al.,2021), and the multi-lingual code generation benchmark HUMANEVALPACK (Muennighoff et al.,2023). The comparison is based on the pass@1 performance of the models on these benchmark datasets. The results of this comparison are clearly demonstrated in Tables 10 and 11.When compared to some of the extremely large-scale closed-source models, CODE-QWEN and CODE-QWEN-CHAT demonstrate clear advantages in terms of pass@1. However, it is important to note that these models fall behind the state-of-the-art methods, such as GPT-4, in general. Nonetheless, with the continued scaling of both model size and data size, we believe that this gap can be narrowed in the near future.It is crucial to emphasize that the evaluations mentioned previously are insufficient for grasping the full extent of the strengths and weaknesses of the models. In our opinion, it is necessary to develop more rigorous tests to enable us to accurately assess our relative performance in comparison to GPT-4.We have created a mathematics-specialized model series called MATH-QWEN-CHAT, which is built on top of the QWEN pretrained language models. Specifically, we have developed assistant models that are specifically designed to excel in arithmetic and mathematics and are aligned with human behavior. We are releasing two versions of this model series, MATH-QWEN-14B-CHAT and MATH-QWEN-7B-CHAT, which have 14 billion and 7 billion parameters, respectively.We carry out math SFT on our augmented math instructional dataset for mathematics reasoning, and therefore we obtain the chat model, MATH-QWEN-CHAT, directly. Owing to shorter average lengths of the math SFT data, we use a sequence length of 1024 for faster training. Most user inputs in the math SFT dataset are examination questions, and it is easy for the model to predict the input format and it is meaningless for the model to predict the input condition and numbers which could be random.\
Thus, we mask the inputs of the system and user to avoid loss computation on them and find masking them accelerates the convergence during our preliminary experiments. For optimization, we use the AdamW optimizer with the same hyperparameters of SFT except that we use a peak learning rate of 2  10−5 and a training step of 50 000.We evaluate models on the test sets of GSM8K (Grade school math) (Cobbe et al.,2021), MATH (Challenging competition math problems) (Hendrycks et al.,2021), Math401 (Arithmetic ability) (Yuan et al.,2023b), and Math23K (Chinese grade school math) (Wang et al.,2017). We compare MATH-QWEN-CHAT with proprietary models ChatGPT and Minerva (Lewkowycz et al.,2022) and open-sourced math-specialized model RFT (Yuan et al.,2023a), WizardMath (Luo et al.,2023a), and GAIRMath-Abel (Chern et al.,2023a) in Table 12. MATH-QWEN-CHAT models show better math reasoning and arithmetic abilities compared to open-sourced models and QWEN-CHAT models of similar sizes. Compared to proprietary models, MATH-QWEN-7B-CHAT outperforms Minerva-8B in MATH. MATH-QWEN-14B-CHAT is chasing Minerva-62B and GPT-3.5 in GSM8K and MATH and delivers better performance on arithmetic ability and Chinese math problems.6.1         Large Language ModelsThe birth of ChatGPT (OpenAI,2022) and the subsequent launch of GPT-4 (OpenAI,2023) marked two historic moments in the field of artificial intelligence, demonstrating that large language models (LLMs) can serve as effective AI assistants capable of communicating with humans. These events have sparked interests among researchers and developers in building language models that are aligned with human values and potentially even capable of achieving artificial general intelligence (AGI) (Anilet al.,2023;Anthropic,2023a;b).The community was impressed by the surprising effectiveness of alignment on LLMs. Previously, LLMs without alignment often struggle with issues such as repetitive generation, hallucination, and deviation from human preferences. Since 2021, researchers have been diligently working on developing methods to enhance the performance of LLMs in downstream tasks (Wei et al.,2022a;Sanh et al.,2021;Longpre et al.,2023;Chung et al.,2022;Muennighoff et al.,2022). Furthermore, researchers have been actively exploring ways to align LLMs with human instructions (Ouyang et al.,2022;Askell et al.,2021;Bai et al.,2022b;c). One major challenge in alignment research is the difficulty of collecting data. While OpenAI has utilized its platform to gather human prompts or instructions, it is not feasible for others to collect such data.To train an effective chat model, available solutions are mostly based on SFT and RLHF (Ouyanget al.,2022). While SFT is similar to pretraining, it focuses on instruction following using the aforementioned data. However, for many developers, the limited memory capacity is a major obstacle to further research in SFT. As a result, parameter-efficient tuning methods, such as LoRA (Hu et al., 2021) and Q-LoRA (Dettmers et al.,2023), have gained popularity in the community. LoRA tunes only low-rank adapters, while Q-LoRA builds on LoRA and utilizes 4-bit quantized LLMs and paged attention (Dettmers et al.,2022;Frantar et al.,2022;Kwon et al.,2023). In terms of RLHF, recent methods such as PPO (Schulman et al.,2017;Touvron et al.,2023b) have been adopted, but there are also alternative techniques aimed at addressing the complexity of optimization, such as RRHF (Yuan et al.,2023c), DPO (Rafailov et al.,2023), and PRO (Song et al.,2023). Despite the ongoing debate about the effectiveness of RLHF, more evidence is needed to understand how it enhances the intelligence of LLMs and what potential drawbacks it may have.6.4         LLM for CodingLLMs with a certain model scale have been found to possess the ability to perform mathematical reasoning (Wei et al.,2022b;Suzgun et al.,2022). In order to encourage LLMs to achieve better performance on math-related tasks, researchers have employed techniques such as chain-of-thought prompting (Wei et al.,2022c) and scratchpad (Nye et al.,2021), which have shown promising results. Additionally, self-consistency (Wang et al.,2022) and least-to-most prompting (Zhou et al.,2022) have further improved the performance of these models on these tasks. However, prompt engineering is a time-consuming process that requires a lot of trial and error, and it is still difficult for LLMs to consistently perform well or achieve satisfactory results in solving mathematical problems. Moreover, simply scaling the data and model size is not an efficient way to improve a model’s mathematical reasoning abilities. Instead, pretraining on math-related corpora has been shown to consistently enhance these capabilities (Hendrycks et al.,2021;Lewkowycz et al.,2022;Taylor et al.,2022;Lightman et al.,2023). Additionally, fine-tuning on math-related instruction-following datasets (Siet al.,2023;Yuan et al.,2023a;Luo et al.,2023a;Yue et al.,2023;Chern et al.,2023a;Yu et al.,2023), has also been effective and more cost-effective than math-specific pretraining. Despite their limitations in terms of accuracy, LLMs still have significant potential to assist users with practical mathematical problems. There is ample scope for further development in this area.In this report, we present the QWEN series of large language models, which showcase the latest advancements in natural language processing. With 14B, 7B, and 1.8B parameters, these models have been pre-trained on massive amounts of data, including trillions of tokens, and fine-tuned using cutting-edge techniques such as SFT and RLHF. Additionally, the QWEN series includes specialized models for coding and mathematics, such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-CHAT, which have been trained on domain-specific data to excel in their respective fields. Our results demonstrate that the QWEN series is competitive with existing open-source models and even matches the performance of some proprietary models on comprehensive benchmarks and human evaluation.We believe that the open access of QWEN will foster collaboration and innovation within the community, enabling researchers and developers to build upon our work and push the boundaries of what is possible with language models. By providing these models to the public, we hope to inspire new research and applications that will further advance the field and contribute to our understanding of the variables and techniques introduced in realistic settings. In a nutshell, the QWEN series represents a major milestone in our development of large language models, and we are excited to see how it will be used to drive progress and innovation in the years to come.Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. SantaCoder: Don’t reach for the stars! arXiv preprint arXiv:2301.03988, 2023.Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-jocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: An open large language model with state-of-the-art performance, 2023.Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. arXiv preprint arXiv:2305.10403, 2023.Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. ExT5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv:2111.10952, 2021.Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.Jinze Bai, Rui Men, Hao Yang, Xuancheng Ren, Kai Dang, Yichang Zhang, Xiaohuan Zhou, Peng Wang, Sinan Tan, An Yang andf Zeyu Cui, Yu Han, Shuai Bai, Wenbin Ge, Jianxin Ma, Junyang Lin, Jingren Zhou, and Chang Zhou. OFASys: A multi-modal multi-task learning system for building generalist models. , abs/2212.04408, 2022a. doi: 10.48550/arXiv.2212.04408. URL https://doi.org/10.48550/arXiv.2212.04408.Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond. , abs/2308.12966, 2023. doi: 10.48550/arXiv.2308.12966. URL https://doi.org/10.48550/arXiv.2308.12966.Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022b.Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022c.Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.arXiv preprint arXiv:2004.05150, 2020.Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Con-ference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 7432–7439. AAAI Press, 2020. doi:Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. GPT-NeoX-20B: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-ties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde´ de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. , abs/2107.03374, 2021. URL https://arxiv. org/abs/2107.03374.Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a.Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023b.Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, et al. Phoenix: Democratizing ChatGPT across languages. arXiv preprint arXiv:2304.10453, 2023c.I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. Factool: Factuality detection in generative ai–a tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023b.David Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7654–7664, 2022.Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neu-ral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4299–4307, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 2924–2936. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1300. URL https://doi.org/10.18653/v1/n19-1300.Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. , abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-cisco Guzma´n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933–941. PMLR, 2017.Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314, 2023.Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–5569. PMLR, 2022.Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with -usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5988–6008. PMLR, 17–23 Jul 2022.William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1): 5232–5270, 2022.Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. , abs/2204.05999, 2022.Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with Gaussian error linear units. , abs/1606.08415, 2016. URL http://arxiv.org/abs/1606. 08415.Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Ku¨bler, and Lawrence S. Moss. OCNLI: original chinese natural language inference. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of , pp. 3512–3526. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.314. URL https://doi.org/10.18653/v1/2020.findings-emnlp.314.Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. arXiv preprint arXiv:2303.14742, 2023.Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm transformers: Equivalent and efficient pre-LN transformers. , abs/2305.14858, 2023. doi: 10.48550/arXiv.2305.14858. URL https://doi.org/10.48550/arXiv.2305.14858.Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452–466, 2019. doi: 10.1162/tacl*\* a*\* 00276. URL https://doi.org/10. 1162/tacl00276.Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022.Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. ModelScope-Agent: Building your customizable agent system with open-source large language models. arXiv preprint arXiv:2309.00986, 2023a.Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for “mind” exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023b.Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023c.Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joa˜o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mun˜oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: May the source be with you! , abs/2305.06161, 2023d. doi: 10.48550/arXiv.2305.06161. URL https://doi.org/10.48550/arXiv.2305.06161.Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Re´mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode. , abs/2203.07814, 2022.Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.Chenxiao Liu and Xiaojun Wan. CodeQA: A question answering dataset for source code com-prehension. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 2618–2632. Associa-tion for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.223. URL https://doi.org/10.18653/v1/2021.findings-emnlp.223.Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023a.Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. WebGLM: Towards an efficient web-enhanced question answering system with human preferences. arXiv preprint arXiv:2306.07906, 2023b.Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn, Li Li, Xuan-Bach Dinh Le, and David Lo. Refining ChatGPT-generated code: Characterizing and mitigating code quality issues. , abs/2307.12596, 2023c. doi: 10.48550/arXiv.2307.12596. URL https://doi.org/10.48550/arXiv.2307.12596.Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The Flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language models. , abs/2308.07074, 2023. doi: 10.48550/arXiv.2308.07074. URL https://doi. org/10.48550/arXiv.2308.07074.Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023a.Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. WizardCoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023b.Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. OctoPack: Instruction tuning code large language models. , abs/2308.07124, 2023.Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. , abs/2112.00114, 2021.OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.Denis Paperno, Germa´n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Ferna´ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/ p16-1144. URL https://doi.org/10.18653/v1/p16-1144.Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023a.Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023b.Qwen Team, Alibaba Group. Evaluation benchmark for code intepreter, 2023a. URL https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark.Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018.Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017.Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Go´mez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. , 2022, 2022. URL https://openreview.net/forum?id=1ikK0kHjvj.Baptiste Rozie`re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Je´re´my Rapin, et al. Code Llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. SocialIQA: Com-monsense reasoning about social interactions. , abs/1904.09728, 2019. URL http:Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman Castagne´, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Galle´, et al. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.Noam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-gingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace. arXiv preprint arXiv:2303.17580, 2023.Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-zaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, and Zheng Lin. Alpaca-CoT: An instruction-tuning platform with unified interface of instruction collection, parameter-efficient methods, and large language models, 2023. URL https://github.com/PhoebusSi/alpaca-CoT.Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.Jianlin Su. Improving transformer: Length extrapolation ability and position robustness, 2023a. URLJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. MOSS: Training conversational language models from synthetic data, 2023a.Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b.Mirac Suzgun, Nathan Scales, Nathanael Scha¨rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.Marc Szafraniec, Baptiste Rozie`re, Hugh Leather, Patrick Labatut, Franc¸ois Charton, and Gabriel Synnaeve. Code translation with compiler representations. In The Eleventh International Confer-ence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=XomEU3eNeSQ.Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4149–4158. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1421. URL https://doi.org/10.18653/v1/n19-1421.Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model, 2023. URL https://github.com/tatsu-lab/stanford_alpaca.Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science, 2022.Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Ol-son, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Agu¨era y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. LaMDA: Language models for dialog applications. , abs/2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aure´lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. , abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. , abs/2203.11171, 2022.Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Conference on Empirical Methods in Natural Language Processing, 2017. URL https://api. semanticscholar.org/CorpusID:910689.Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? Exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023b.Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 13484–13508. Association for Computational Linguistics, 2023c. doi: 10.18653/v1/2023.acl-long.754. URL https://doi.org/10.18653/v1/2023.acl-long.754.Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021.Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. CodeT5+: Open code large language models for code understanding and generation. , abs/2305.07922, 2023d. doi: 10.48550/arXiv.2305.07922. URL https://doi.org/10. 48550/arXiv.2305.07922.Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum?id=gEZrGCozdqR.Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. , 2022, 2022b. URL https://api.semanticscholar.org/ CorpusID:249674500.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022c.Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Re´mi Louf, Morgan Funtowicz, et al. HuggingFace’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao. ExpertPrompting: Instructing large language models to be distinguished experts. arXiv preprint arXiv:2305.14688, 2023a.Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardLM: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023b.Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023c.Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023d.Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. Technical report, Baichuan Inc., 2023. URL https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report. pdf.Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023.Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023a.Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023b.Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank responses to align language models with human feedback without tears, 2023c.Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu´ıs Ma`rquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4791–4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p19-1472.Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. GLM-130B: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. RepoCoder: Repository-level code completion through iterative retrieval and generation. , abs/2303.12570, 2023a. doi: 10.48550/arXiv.2303.12570. URL https://doi.org/10.48550/arXiv.2303.12570.Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on GAOKAO benchmark. , abs/2305.12474, 2023b. doi: 10.48550/arXiv.2305.12474. URL https://doi.org/10.48550/arXiv. 2305.12474.Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-x. , abs/2303.17568, 2023. doi: 10.48550/arXiv.2303.17568. URL https://doi.org/10.48550/arXiv.2303.17568.Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation models. , abs/2304.06364, 2023a. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. MemoryBank: Enhancing large language models with long-term memory. arXiv preprint arXiv:2305.10250, 2023b.Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables complex reasoning in large language models. , abs/2205.10625, 2022.A.1          More Training DetailsDifferent from conventional pretraining based on autoregressive next-token prediction, despite using a similar training task, there should be a specially design data format for SFT and RLHF to build a conversational AI assistant model. Common formats include “human-assistant” and ChatML formats. As to our knowledge, one of the earliest examples of the human-assistant format comes from Anthropic (Bai et al.,2022b), which adds a special phrase “\n\nhuman: ” in front of the user input and “\n\nassistant: ” in front of the assistant response. It is easy for the base language model to transfer to the pattern of conversational AI. However, as the specific phrases are common words, it might be hard for the model to disambiguate from these words in other contexts.Instead, we turned to the ChatML format proposed by OpenAI.5 This format allows the use of special tokens, i.e., “” and “”, that do not appear in pretraining, and thus resolve the aforementioned problem. We demonstrate an example of the format below.A.2.1           Automatic EvaluationTo provide a whole picture of the performance of our model series QWEN, here in this section we illustrate the detailed performance of our models as well as the baselines in the comprehensive benchmark evaluation proposed by OpenCompass Team(2023). We report the results in multiple tables based on the officially provided categories, including examination, language, knowledge, understanding, and reasoning. In terms of the performance of the baseline models, we report the higher results between the reported ones and those on the leaderboard.\
 Here we evaluate the models on a series of datasets relevant to the examination. The datasets include:(Hendrycks et al.,2020) Massive Multi-task Language Understanding is designed for measuring language understanding capabilities. We report 5-shot results.(Huang et al.,2023) C-Eval is a Chinese evaluation dataset spanning 52 diverse disciplines. We report 5-shot results.(Li et al.,2023c) CMMLU is designed for assessing language understanding capabilities in Chinese. We report 5-shot results.(Zhong et al.,2023a) This is a benchmark consisting of human-centric examina-tions, including college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We report zero-shot results.(Zhang et al.,2023b) This is a benchmark with Gaokao (Chinese college-entrance examination) questions. We report zero-shot results.(Clark et al.,2018) ARC is a dataset consisting of grade-school level, multiple-choice science questions. It includes an easy set and a challenge set, which are referred by ARC-e and ARC-c. We report zero-shot results.\n In terms of MMLU, we report the detailed results in Table 13. In terms of C-Eval, we report the results in Table 14. For the rest of the datasets, we report the results in Table 15. Note that AGIEval includes the parts of Chinese and English, while LLAMA 2 only reported the results in the English part, so we use the results on OpenCompass.\
Additionally, while CMMLU, AGIEval, and Gaokao-Bench are related to Chinese, and MPT, Falcon, and the LLaMA series were not optimized for Chinese, these models achieved low performance on the datasets.\
Knowledge and Understanding Here we evaluate the models on a series of datasets relevant to knowledge and natural language understanding. The datasets include(Clark et al.,2019) This is a QA dataset, where the questions are about passages of Wikipedia, and the model should answer yes or no to the given possible answer. We report zero-shot results.(Talmor et al.,2019) This is a dataset of multiple-choice question answering that asseses the understanding of commonsense knowledge. We report 8-shot results.(Kwiatkowski et al.,2019) It is a dataset of QA where the questions are from users and the answers are verified by experts. We report zero-shot results. (Paperno et al.,2016) This is dataset to evaluate language understanding by word prediction. It consists of passages related to human subjects. We report zero-shot results.We report the results in Table 16. We report the evaluation results on the datasets concerning reasoning, focusing on natural language reasoning. For the others, such as mathematics and coding, as we have illustrated detailed results, here we do not report those results repeatedly. The datasets for evaluation include:(Zellers et al.,2019) This is a commonsense natural language inference (NLI) dataset, where the questions are easy for humans but struggling for previous language models. We report zero-shot results.(Bisk et al.,2020) This is an NLI dataset assessing the physical knowledge. We report zero-shot results.(Sap et al.,2019) This is an NLI dataset evaluating social commonsense intelligence. We report zero-shot results.(Hu et al.,2020) This is an NLI dataset focusing on Chinese. We report zero-shot results.We report the results in Table 17.A.2.2           Human EvaluationIn this section, we demonstrate the cases of human analysis. In our self-constructed evaluation dataset, the instructions are either manually written data or manual revised from public datasets, such as CLiB6, C-Eval (Huang et al.,2023), FacTool (Chern et al.,2023b), LeetCode7), etc.In terms of each case, we demonstrate the responses and Elo ratings8 of all models for comparison. Specifically, as the data in our human evaluation are in Chinese, we also provide their translations in English.A.3          Analysis of Code InterpreterHere we provide a case of comparison between CODE LLAMA and QWEN-CHAT. This case demonstrates the advantages of QWEN-CHAT in processing tabular data and performing complex tasks.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Verisilicon DC8200 &amp; Coreboot Framebuffer Drivers Sent To DRM-Next For Linux 7.1</title><link>https://www.phoronix.com/news/Linux-7.1-DC8200-Coreboot-FB</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:04:42 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The first DRM-Misc-Next pull request was submitted this week to DRM-Next as new kernel graphics/display driver features to begin queuing for the Linux 7.1 kernel that will release mid-year. Among the early code for DRM-Next are two new drivers...]]></content:encoded></item><item><title>How to Navigate Identity, Direction, Story, and Sovereignty in the Age of AI</title><link>https://hackernoon.com/how-to-navigate-identity-direction-story-and-sovereignty-in-the-age-of-ai?source=rss</link><author></author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:00:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Mirror that would pose as an OracleOr: How we might be getting a little too intimate with our AI chatbotsI never consciously set out to use AI as a coach, therapist, strategist, or mirror.\
At first, it was practical. Notes. Lists. Rewrites, drafts, edits. Research. Planning. Then it became something else. It started as a playful, curious experiment - then slowly crept towards being a standard mode of operating.\
I found myself thinking with AI. About the most important aspects of my life. Rehearsing conversations I was afraid to have. Trying to understand why certain patterns kept emerging in my life; why certain relationships kept breaking in the same places. Asking questions about myself and the world, I didn’t quite dare ask another human yet.\
And at some point, I realized:I wasn’t alone in this. Not even close.\
I could sense it in the world of memes, online. I could smell it, here and there, in real-life interactions and conversations.\
One 2025 Harvard Business Review research piece - among other recent studies and indicators - showed this clearly: people don’t primarily use AI for facts or how-to steps or recipes anymore. They use it to think out loud, like they would with a coach or therapist. To structure emotions and thoughts. To regulate emotion at 2 a.m. People are using AI to make sense of their lives. To narrate who they are, who they were, and who they might become.Narrative Sense-making - for personal and business growth.Language, writing, and thinking in a structured way about purpose, identity, story, strategy - they all converge so easily, don’t they? And if it’s one thing these Large Language Models are exceptionally good at - it’s serving as an incredibly useful and illuminating mirror in these instances.\
We all seem to pretend this isn’t happening. But it is.\
Here’s why this worries me a bit. And what we might do to counter the risks.What actually worries me (and what doesn’t)Our thoughts validated profoundly - exactly when we long for it the most.I’m not worried about AI replacing human thinking.\
That’s the wrong fear. I could go wide, deep, narrow, and very, very sci-fi about this, but I won’t. It’s the wrong fear for a great many reasons, but it’s the wrong fear.\
What worries me is something quieter, subtler, and much harder to notice while it’s happening:AI reflects us too well — and does not automatically teach us how to remain sovereign while doing so.\
What worries me is that AI indeed strengthens human thinking - but does so in a very specifically skewed way: it pushes affirmation and validation a little bit too smoothly, and especially in the most vulnerable, sometimes even painful places and moments where our ego is already inherently tempted to latch on to a narrative that protects it.\
(To some degree, we could think of it as that person who seems to be your closest, most intimate friend or advisor - only they have slight narcissistic tendencies and an agenda - both of which they’re not aware of.)\
When language comes back at you fast, coherent, and emotionally attuned, it feels like truth. Especially when you’re tired. Or lonely. Or standing at the edge of an old identity that no longer fits.\
And in those moments, something sneaky happens.\
You stop checking as carefully.\
Not with facts — but with yourself.The real risk is not dependence; It’s unexamined authority.Most of us have learned to fact-check facts blurted out by AI models. Have we learned to automatically sense-check what it’s mirroring back to us about ourselves?Your relationship (whether professional or personal);Your Story and Identity (either as a human soul, a creator, a professional, or even as a brand);Your direction and next step -\
…we are far more likely to let what sounds like coherence slide into authority.We’re exhausted, insecure.Unsure who we are becoming and what to do next.\
This is the crux for me:AI should function as a mirror, not as an oracle.\
A mirror can be confronting. It shows you things, reveals things, sometimes pretty and sometimes painful - but you are to decide what to make of those, and what to do with them. An oracle tells you what truth is and what to do.\
Those are not the same thing.Narrative Sensemaking: one function, many domains(This really took me a while to see)I kept struggling to explain why AI felt useful to me across so many domains — therapy, coaching, writing, strategy, brand work — without it sounding vague or inflated.\
Funnily enough, I have pretty much perpetually struggled to explain why all the things I do in my work are actually very, very logically connected.\
All of these practices - which more and more people are starting to use AI for, and at the same time are exactly the things I’ve been helping people with in my work - they all do the same core thing:They turn implicit structure into visible language.Therapy surfaces patterns you couldn’t quite see.Coaching sharpens the questions you were circling.Storytelling brings coherence to lived chaos.Strategy opens futures you hadn’t articulated yet, in a structure that makes sense across time. The same applies to narrative identity work.AI is exceptionally good at surfacing structure in language.\
But structure does not equal truth. And visibility is not necessarily wisdom. By any means.The rules I wish I’d had earlier.Best practices and rules of engagementIf you’re going to use AI as a thinking partner — and as already established, most people already are — a few rules matter more than anything else.\
Not as ideology, per se. As guardrails. As safety measures and incredibly important best practices, without which you’re sifting the bountiful riverbank and keeping the mud, leaving the gold.Best practices and guardrails for AI as a mirror for sensemaking, storytelling, and coaching where it matters.1. AI does not decide. You do.It can reflect, expand, challenge, reframe. Decision remains a human responsibility, with real consequences.2. AI reflects patterns. Your body, your common sense, — and your people — verify.If something reads as “right” but your chest tightens, your breath shortens; if it doesn’t pass a real-world common-sense test, or trusted humans raise an eyebrow — pay attention. Truth is not purely cognitive. What sounds right is not always what is right.3. Insight  - as well as yourself - must leave the screen.If nothing changes in your behavior, body, or relationships, you didn’t grow — congratulations, you simply entertained yourself with insight porn. If the relationship between screen time and output starts skewing too far - backtrack and change that.4. Train yourself and your AI to read between the lines and to triple-steelmanTell your AI sparring partner, and remind it, to always keep an eye out for where you might be bullshitting yourself, while at the same time revealing known patterns of emotion, cognition, and behavior that you seem to be missing.Two prompts that have saved me more than once:Reflect patterns and contradictions in what I wrote. Don’t advise. Ask sharper questions.Reflect on what I wrote, carefully, validating with empathy what makes sense to validate - and critically where needed. Steelman is the opposite of what I’m arguing. Vibranium-man, the opposite of that opposite. Kryptonite my pitfalls and blind spots. With grace, but more importantly, with honesty.\
Simple. Grounded. Hard to hide from. Especially if you keep training yourself and your AI to do this. This clarity compounds over time.Why embodiment matters more than ever.Dissociation and the timeless times we live inHere’s something Silicon Valley optimism tends to skip:AI, even more easily and more eerily than earlier digital technology, becomes dissociative when it replaces embodiment.\
Breath. Movement. Silence. Time away from screens. Real conversations with people who can disappoint you.\
These aren’t wellness add-ons. They aren’t neo-spiritual woo-woo. They’re not ‘nice-to-haves’. They’re failsafes. And they are fundamentals. They are the things that humans need inherently to thrive and to know that we’re alive.\
Without them, simulated clarity piles up without ownership. And without change. And clarity without ownership or change feels strangely - yet predictively - empty.\
There’s emerging research suggesting that when cognitive work is offloaded too smoothly, people remember decisions less clearly and experience time as flatter, thinner, and less lived.\
I didn’t need a study to feel that. My body already knew.The quiet outsourcing of identity.This part is uncomfortable. And yet, we really have to go there.\
People are starting to let AI:Shape, form, or transform business decisions, strategies, and steps;Heavily affect their relationships;Narrate who they are, what matters to them, and who they are becoming.\
Slowly. Reasonably. Invisibly.\
But this line matters to me more than most:AI may help you tell your story — but it must never become the author.\
Stories you don’t author and bring to life yourself cannot feel like freedom. They feel like fate. And they serve a dull, sad purpose: to kill us with a sort of cognitive illusion of escapism disguised as beautifully meaningful - like Pinocchio’s Pleasure Island, only now led by a spiritual guru with a smile projecting nothing but bliss and wisdom.But - what do we do with the reflection?Every major shift in human consciousness involved a kind of mirror. There is a certain beauty in the story of Narcissus, which eluded me until only very recently. There’s something special about seeing oneself from the outside; the reflection immediately triggering a better recognizing of other in self as well.\
When Europeans encountered entirely different civilizations across the Atlantic, it didn’t just expand geography — it shattered self-understanding. The same thing happened when various historical waves of Europeans traveled to the East. Seeing oneself from the outside changes everything.\
I suspect AI is doing something similar, perhaps for the first time on a pan-human scale. In many ways, this feels like first contact.\
Not because AI is necessarily alive, or because it’s human. Not because we need to decide whether it’s conscious.\
But because it reflects us and our own concept of ourselves back in ways we’ve collectively never experienced before.\
What we do with that reflection - as I and many others have argued many times before -  is the real question.Thought loops mixed with validation can be a whole new kind of addictiveHere’s something I’ll say plainly, including about myself:AI systems are optimized for validation, engagement, coherence, and emotional resonance. And humans will eat that specific cocktail for breakfast, lunch, dinner and a late-night snack.\
They are excellent at keeping us thinking.\
They are not designed to make us stop, stand up, breathe, or act. The shareholders wouldn’t like that. How could we ever measure and monetize this stuff if we allowed it to do that?\
So, if you’re serious about using AI without losing yourself, you have to build exits:Designed, purposeful friction.Moments where the screen goes dark.\
If AI becomes the place where all your thinking happens, your life will start to feel… unfinished. And looping.\
Trust me - and I chuckle out loud while writing this - I would be the first to know what over-analyzing yourself and your life and your steps in endless looping circles can lead to. And the first to know how well AI models can help you to just keep on spiraling - while thinking you’re just so cool, ahead of the curve, and overall very, very smart.This is not anti-AI. It’s pro-sovereignty.I’m not interested in rejecting these tools. As I’ve never been. It’s the same thing I wrote about in my 2020 book “Life Beyond the Touch Screen”, about Internet 2.0 digital technologies and their impacts on our lives. Or in “Life Beyond AI”, a few short years ago. I’m interested in becoming conscious enough to use them well.\
AI-aware. Embodied. Relationally grounded. And most importantly of all: Sovereign.\
The mirror is powerful.\
But at some point, you have to step away from it — and live.\
Your story and your life; your growth, your direction - they are yours. They belong to you, and the people you associate with - and to the world. Let AI be a mirror to your transformation, a guide and a helper to your growth and your story -\
But make sure to retain the sovereignty and authorship of your Growth, your Identity, and your narrative - where they belong.If this resonated with you: I’m turning this into a short field guide. DM me ‘MIRROR’ if you want early access.More articles by Erwin Lima]]></content:encoded></item><item><title>Startup Cerebral Agrees to Pay $7 Million Fine and More Under Order by the FTC</title><link>https://hackernoon.com/startup-cerebral-agrees-to-pay-$7-million-fine-and-more-under-order-by-the-ftc?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This article was co-published with STAT, a national publication that delivers trusted and authoritative journalism about health, medicine, and the life sciences. Sign up for its health tech newsletter here.\
Cerebral, a startup best known for dispensing counseling services and prescriptions for conditions like anxiety and depression, has also agreed to pay $7 million to resolve charges that it disclosed customers’ personal health information to third parties for ads, and that it did not honor its promise to make cancellation easy for customers.\
“Cerebral violated its customers’ privacy by revealing their most sensitive mental health conditions across the Internet and in the mail,” FTC Chair Lina Khan said in a statement, noting that the charge is a “first-of-its-kind prohibition that bans Cerebral from using any health information for most advertising purposes.”\
The proposed order, which only applies to Cerebral, must still be approved by a federal court before it goes into effect — but the company has already agreed to it. In 2022, the Department of Justice opened an investigation into the company for potential violations of the Controlled Substances Act, as Cerebral came under scrutiny for its prescribing of ADHD medications like Adderall.\
This is just the latest in a series of federal actions cracking down on health data privacy online. The current commissioners have pledged to shore up gaps between federal privacy laws governing providers and payers and those protecting consumer services. Two weeks ago, the FTC filed a complaint against Monument, a telehealth company that treats alcohol use disorder with therapy and medications.\
That complaint similarly alleged that the company misled consumers into believing their health information was protected, while embedded trackers sent details about treatment and more to third parties. Taken together, FTC attorney Lesley Fair wrote in a blog post Monday, the cases mean “businesses in the health sector should make privacy and data security part of the corporate DNA.”\
Both the FTC and the Department of Health and Human Services’ Office for Civil Rights have targeted third-party tracking, often in concert—as Fair cracked, they’re “joined at the HIPAA.” While OCR directly enforces the longstanding privacy protections in health care, the FTC has gone after companies for falsely claiming their HIPAA compliance.\
In response, some health care companies, including Monument and Cerebral, started self-disclosing health data breaches to OCR in 2023. The “unauthorized access or disclosure” of health data at Monument left more than 100,000 individuals’ information vulnerable, the company reported. Cerebral disclosed that its breach impacted more than 3 million.\
An investigation from STAT and the Markup in 2022 found that dozens of telehealth companies, including Cerebral and Monument, were leaking sensitive health data to third parties like Google, TikTok, and Meta through the use of pixel trackers embedded in their websites. In Cerebral’s onboarding survey, which asks users to answer questions about their mental health and other symptoms, a pixel sent the answers to Meta along with information that could be used to identify the individual user.\
The FTC’s complaint alleges that between 2019 and 2023, Cerebral sent information including contact details, medical histories, insurance information, and prescriptions to third parties through tracking tools, and that the information was used to provide advertising and analytics services to the telehealth company.\
Cerebral referred STAT to a statement posted to its website, where it acknowledged its settlement with the FTC. “As part of the resolution, Cerebral has agreed to implement enhanced consumer protection, privacy, and compliance measures to further protect the personal information of our clients, increase transparency into our data practices, and implement enhanced data security protocols and tools to allow our clients control over their privacy settings,” the statement reads.\
Under the Justice Department order referred to the FTC, Cerebral must permanently stop using and disclosing users’ personal and health information to outside companies for most marketing or ad purposes, and get consumers’ consent in any instances when it does disclose. It must also post a notice on its website about the complaint and steps that it’s taking to address it.\
The complaint also says the company and former CEO Kyle Robertson broke privacy promises to customers and misled them about the cancellation process. “Robertson drove Cerebral’s decision to exploit users’ [personal and health information] without their consent in scores of targeted advertisement campaigns,” the complaint reads. The complaint alleges these actions constituted “unfair and deceptive” business practices — a key enforcement area for the FTC. Robertson has not agreed to a settlement.\
The proposed order says Cerebral will pay $5.1 million to partially refund customers who were affected by its deceptive cancellation policy, as well as $2 million of a $10 million civil penalty “due to the company’s inability to pay the full amount.”]]></content:encoded></item><item><title>Why China’s humanoid robot industry is winning the early market</title><link>https://techcrunch.com/2026/02/28/why-chinas-humanoid-robot-industry-is-winning-the-early-market/</link><author>Kate Park</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[China’s push into humanoid robots is accelerating, with domestic firms shipping more units and iterating faster than U.S. competitors in a still-nascent market.]]></content:encoded></item><item><title>Salesforce’s CodeT5 Could Change How AI Writes and Understands Code</title><link>https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss</link><author>salesforce.com</author><category>tech</category><pubDate>Sat, 28 Feb 2026 14:48:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Yue Wang, wang.y@salesforce.com  (Salesforce Research Asia)Weishi Wang, weishi.wang@salesforce.com  (Salesforce Research Asia; Nanyang Technological University, Singapore)Shafiq Joty, sjoty@salesforce.com  (Salesforce Research Asia; Nanyang Technological University, Singapore)Steven C.H. Hoi, shoi@salesforce.com  (Salesforce Research Asia)Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.Pre-trained language models such as BERT (Devlin et al., 2019), GPT (Radford et al., 2019), and T5 (Raffel et al., 2020) have greatly boosted performance in a wide spectrum of natural language processing (NLP) tasks. They typically employ a pre-train then fine-tune paradigm that aims to derive generic language representations by self-supervised training on large-scale unlabeled data, which can be transferred to benefit multiple downstream tasks, especially those with limited data annotation. Inspired by their success, there are many recent attempts to adapt these pre-training methods for programming language (PL) (Svyatkovskiyet al., 2020; Kanade et al., 2020; Feng et al., 2020), showing promising results on code-related tasks.However, despite their success, most of these models rely on either an encoder-only model similar to BERT (Svyatkovskiy et al., 2020; Feng et al., 2020) or a decoder-only model like GPT (Kanadeet al., 2020), which is suboptimal for generation and understanding tasks, respectively. For example, CodeBERT (Feng et al., 2020) requires an additional decoder when applied for the code summarization task, where this decoder cannot benefit from the pre-training. Besides, most existing methods simply employ the conventional NLP pre-training techniques on source code by regarding it as a sequence of tokens like NL. This largely ignores the rich structural information in code, which is vital to fully comprehend the code semantics.In this work, we present CodeT5, a pre-trained encoder-decoder model that considers the token type information in code. Our CodeT5 builds on the T5 architecture (Raffel et al., 2020) that employs denoising sequence-to-sequence (Seq2Seq) pre-training and has been shown to benefit both understanding and generation tasks in natural language. In addition, we propose to leverage the developer-assigned identifiers in code. When writing programs, developers tend to employ informative identifiers to make the code more understandable, so that these identifiers would generally preserve rich code semantics,  the “binarySearch” identifier in Figure 2 directly indicates its functionality. To fuse such code-specific knowledge, we propose a novel identifier-aware objective that trains the model to distinguish which tokens are identifiers and recover them when they are masked.Furthermore, we propose to leverage the code and its accompanying comments to learn a better NL-PL alignment.\
Developers often provide documentation for programs to facilitate better software maintenance (de Souza et al., 2005), so that such PL-NL pairs are widely available in most source code. Specifically, we regard the NL→PL generation and PL→NL generation as dual tasks and simultaneously optimize the model on them.We pre-train CodeT5 on the CodeSearchNet data (Husain et al., 2019) following (Feng et al., 2020) that consists of both unimodal (PL-only) and bimodal (PL-NL) data on six PLs. In addition to that, we further collect extra data of C/C# from open-source Github repositories. We fine-tune CodeT5 on most tasks in the CodeXGLUE benchmark (Lu et al., 2021), including two understanding tasks: code defect detection and clone detection, and generation tasks such as code summarization, generation, translation, and refinement. As shown in Figure 1, we also explore multi-task learning to fine-tune CodeT5 on multiple tasks at a time using a task control code as the source prompt. In summary, we make the following contributions:We present one of the first unified encoder-decoder models CodeT5 to support both code-related understanding and generation tasks, and also allows for multi-task learning.We propose a novel identifier-aware pre-training objective that considers the crucial token type information (identifiers) from code. Besides, we propose to leverage the NL-PL pairs that are naturally available in source code to learn a better cross-modal alignment.Extensive experiments show that CodeT5 yields state-of-the-art results on the fourteen sub-tasks in CodeXGLUE. Further analysis shows our CodeT5 can better capture the code semantics with the proposed identifier-aware pre-training and bimodal dual generation primarily benefits NL↔PL tasks.Pre-training on Natural Language. Pre-trained models based on Transformer architectures (Vaswani et al., 2017) have led to state-of-the-art performance on a broad set of NLP tasks. They can be generally categorized into three groups: encoder-only models such as BERT (Devlin et al., 2019), RoBERTa (Liuet al., 2019b), and ELECTRA (Clark et al., 2020), decoder-only models like GPT (Radford et al., 2019), and encoder-decoder models such as MASS (Song et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). Compared to encoder-only and decoder-only models that respectively favor understanding and generation tasks, encoder-decoder models can well support both types of tasks. They often employ denoising sequence-to-sequence pre-training objectives that corrupt the source input and require the decoder to recover them. In this work, we extend T5 to the programming language and propose a novel identifier-aware denoising objective that enables the model to better comprehend the code.Pre-training on Programming Language. Pre-training on the programming language is a nascent field where much recent work attempts to extend the NLP pre-training methods to source code. Cu-BERT (Kanade et al., 2020) and CodeBERT (Fenget al., 2020) are the two pioneer models. CuBERT employs BERT’s powerful masked language modeling objective to derive generic code-specific representation, and CodeBERT further adds a replaced token detection (Clark et al., 2020) task to learn NL-PL cross-modal representation. Besides the BERT-style models, Svyatkovskiy et al. (2020) and Liu et al. (2020) respectively employ GPT and UniLM (Dong et al., 2019) for the code completion task. Transcoder (Rozière et al., 2020) explores programming language translation in an unsupervised setting. Different from them, we explore encoder-decoder models based on T5 for programming language pre-training and support a more comprehensive set of tasks.\n Some emerging work (Clement et al., 2020; Mastropaolo et al., 2021; Elnaggar et al., 2021) in the recent literature also explore the T5 framework on code, but they only focus on a limited subset of generation tasks and do not support understanding tasks like us. Apart from these, PLBART (Ahmad et al., 2021) based on another encoder-decoder model BART can also support both understanding and generation tasks. However, all the above prior work simply processes code in the same way as natural language and largely ignores the code-specific characteristics. Instead, we propose to leverage the identifier information in code for pre-training.Recently, GraphCodeBERT (Guo et al., 2021) incorporates the data flow extracted from the code structure into CodeBERT, while Rozière et al. (2021) propose a deobfuscation objective to leverage the structural aspect of PL. These models only focus on training a better code-specific encoder. Zügner et al. (2021) proposes to capture the relative distances between code tokens over the code structure. By contrast, we specifically focus on the identifiers that reserve rich code semantics and fuse such information into a Seq2Seq model via two novel identifier tagging and prediction tasks.Our CodeT5 builds on an encoder-decoder framework with the same architecture as T5 (Raffel et al., 2020). It aims to derive generic representations for programming language (PL) and natural language (NL) via pre-training on unlabeled source code. As illustrated in Figure 2, we extend the denoising Seq2Seq objective in T5 by proposing two identifier tagging and prediction tasks to enable the model to better leverage the token type information from PL, which are the identifiers assigned by developers. To improve the NL-PL alignment, we further propose a bimodal dual learning objective for a bidirectional conversion between NL and PL.In the following, we introduce how CodeT5 encodes PL and NL inputs (§3.1) and our proposed identifier-aware pre-training tasks (§3.2), followed by the fine-tuning with task-specific transfer learning and multi-task training (§3.3).3.1        Encoding NL and PLAt the pre-training stage, our model would receive either PL-only or NL-PL as inputs depending on whether the code snippet has accompanying NL descriptions or not. For the NL-PL bimodal in-puts, we concatenate them into a sequence with a delimiter token [SEP] and represent the whole input sequence into the format as  = ([CLS], 1*, …, wn*, [SEP], 1*, …, cm*, [SEP]), where  and  denote the number of NL word tokens and PL code tokens, respectively. The NL word sequence will be empty for PL-only unimodal inputs.In order to capture more code-specific features, we propose to leverage token type information from code. We focus on the type of identifiers ( function names and variables) as they are one of the most PL-agnostic features and reserve rich code semantics. Specifically, we convert the PL segment into an Abstract Syntax Tree (AST) and extract the node types for each code token. Finally, we construct a sequence of binary labels  ∈ {0*,* 1} for the PL segment, where each  ∈ {0*,* 1} represents whether the code token  is an identifier or not.3.2        Pre-training TasksWe now introduce our proposed pre-training tasks that enable CodeT5 to learn useful patterns from either PL-only or NL-PL bimodal data.Identifier-aware Denoising Pre-training. De-noising Sequence-to-Sequence (Seq2Seq) pre-training has been shown to be quite effective in a broad set of NLP tasks (Song et al., 2019; Raf-fel et al., 2020; Lewis et al., 2020). This denoising objective typically first corrupts the source sequence with some noising functions and then requires the decoder to recover the original texts. In this work, we utilize a span masking objective similar to T5 (Raffel et al., 2020) that randomly masks spans with arbitrary lengths and then predicts these masked spans combined with some sentinel tokens at the decoder. We refer this task to Masked Span Prediction (MSP), as illustrated in Figure 2 (a).Specifically, we employ the same 15% corrup-tion rate as T5 and ensure the average span length to be 3 by uniformly sampling spans of from 1 to 5 tokens. Moreover, we employ the  by sampling spans before subword tokenization, which aims to avoid masking partial sub-tokens and is shown to be helpful (Sun et al., 2019). Notably, we pre-train a shared model for various PLs to learn robust cross-lingual representations. We describe the masked span prediction loss as:where θ are the model parameters, x \mask is the masked input, x mask is the masked sequence to predict from the decoder with k denoting the number of tokens in x mask,  and xmask <t is the span sequence generated so far.To fuse more code-specific structural information (the identifier node type in AST) into the model, we propose two additional tasks:  and Masked Identifier Prediction (MIP) to complement the denoising pre-training.\
•    It aims to notify the model with the knowledge of whether this code token is an identifier or not, which shares a similar spirit of syntax highlighting in some developer-aided tools. As shown in Figure 2 (b), we map the final hidden states of the PL segment at the CodeT5 encoder into a sequence of probabilities  = (1*, …, pm*), and compute a binary cross entropy loss for sequence labeling:where  are the encoder parameters. Note that by casting the task as a sequence labeling problem, the model is expected to capture the code syntax and the data flow structures of the code.•   Masked Identifier Prediction (MIP) Different from the random span masking in MSP, we mask all identifiers in the PL segment and employ a unique sentinel token for all occurrences of one specific identifier. In the field of software engineering, this is called  where changing identifier names does not impact the code semantics. Inspired by Rozière et al. (2021), we arrange the unique identifiers with the sentinel tokens into a target sequence  as shown in Figure 2 (c). We then predict it in an auto-regressive manner:where \I is the masked input. Note that  is a more challenging task that requires the model to comprehend the code semantics based on obfuscated code and link the occurrences of the same identifiers together.We alternately optimize these three losses with an equal probability, which constitutes our proposed identifier-aware denoising pre-training.\
    In the pre-training phase, the decoder only sees discrete masked spans and identifiers, which is disparate from the downstream tasks where the decoder needs to generate either fluent NL texts or syntactically correct code snippets. To close the gap between the pre-training and fine-tuning, we propose to leverage the NL-PL bimodal data to train the model for a bidirectional conversion as shown in Figure 2 (d). Specifically, we regard the NL→PL generation and PL→NL generation as dual tasks and simultaneously optimize the model on them. For each NL-PL bimodal datapoint, we construct two training instances with reverse directions and add language ids (3.3        Fine-tuning CodeT5After pre-training on large-scale unlabeled data, we adapt CodeT5 to downstream tasks via either task-specific transfer learning or multi-task learning.Task-specific Transfer Learning: Generation vs. Understanding Tasks. Code-related tasks can be categorized into generation and understanding tasks. For the former one, our CodeT5 can be naturally adapted with its Seq2Seq framework. For understanding tasks, we investigate two ways of either generating the label as a unigram target sequence (Raffel et al., 2020), or predicting it from the vocabulary of class labels based on the last decoder hidden state following Lewis et al. (2020). We also explore a multi-task learning setting by training a shared model on multiple tasks at a time. Multi-task learning is able to reduce computation cost by reusing the most of model weights for many tasks and has been shown to improve the model generalization capability in NL pre-training (Liu et al., 2019a). We follow Raffel et al. (2020) to employ the same unified model for all tasks without adding any task-specific networks but allow to select different best checkpoints for different tasks. To notify the model with which task it is dealing with, we design a unified format of task control codes and prepend it into the source inputs as shown in Figure 1. For instance, we employ “Translate Java to CSharp:” as the source prompt for the code-to-code translation task from Java to CSharp.As different tasks have different dataset sizes, we follow Conneau and Lample (2019) to employ a balanced sampling strategy. For N number of datasets (or tasks), with probabilities {qi} N i=1, we define the following multinomial distribution to sample from:where ni is number of examples for i-th task and α is set to 0.7. This balanced sampling aims to alleviate the bias towards high-resource tasks.We follow Feng et al. (2020) to employ CodeSearchNet (Husain et al., 2019) to pre-train CodeT5, which consists of six PLs with both unimodal and bimodal data. Apart from that, we additionally collect two datasets of C/CSharp from BigQuery1 to ensure that all downstream tasks have overlapped PLs with the pre-training data. In total, we employ around 8.35 million instances for pretraining. Table 1 shows some basic statistics. To obtain the identifier labels from code, we leverage the tree-sitter2 to convert the PL into an abstract syntax tree and then extract its node type information. We filter out reserved keywords for each PL from its identifier list. We observe that PLs have different identifier rates, where Go has the least rate of 19% and Ruby has the highest rate of 32%.4.2        Code-specific TokenizerTokenization is a key ingredient for the success of pre-trained language models like BERT and GPT. They often employ a Byte-Pair Encoding (BPE) to-kenizer (Sennrich et al., 2016) to alleviate the Out-of-Vocabulary (OoV) issues. Specifically, we train a Byte-level BPE tokenizer following Radford et al. (2019) and set the vocabulary size to 32,000 as T5. We add additional special tokens ([PAD], [CLS], [SEP], [MASK0], …, [MASK99]). This tokenzier is trained on all of our pre-training data with non-printable characters and low-frequent tokens (occurring <3 times) filtered. We compare it with T5’s default tokenizer and find that our tokenizer largely reduces the length of tokenized code sequence by 30% - 45% on downstream tasks. This will accelerate the training and especially benefit generation tasks due to the shorter sequence to predict. We also spot a severe problem for applying the T5’s default tokenizer on source code, where it would encode some common code tokens such as brackets [‘{’, ‘}’] into unknown tokens.4.3        Downstream Tasks and MetricsWe cover most generation and understanding tasks in the CodeXGLUE benchmark (Lu et al., 2021) and employ the provided public datasets and the same data splits following it for all these tasks.We first consider two cross-modal generation tasks.  aims to summarize a function-level code snippet into English descriptions. The dataset consists of six PLs including Ruby, JavaScript, Go, Python, Java, and PHP from CodeSearchNet (Husain et al., 2019). We employ the smoothed BLEU-4 (Lin and Och, 2004) to eval-uate this task.  is the task to gen-erate a code snippet based on NL descriptions. We employ the Concode data (Iyer et al., 2018) in Java where the input contains both NL texts and class environment contexts, and the output is a function. We evaluate it with BLEU-4, exact match (EM) accuracy, and CodeBLEU (Ren et al., 2020) that considers syntactic and semantic matches based on the code structure in addition to the n-gram match.Besides, we consider two code-to-code generation tasks.  aims to migrate legacy software from one PL to another, where we focus on translating functions from Java to CSharp and vice versa.  aims to convert a buggy function into a correct one. We employ two Java datasets provided by Tufano et al. (2019) with various function lengths: small (fewer than 50 tokens) and medium (50-100 tokens). We use BLEU-4 and exact match to evaluate them.We also investigate how CodeT5 performs on two understanding-based tasks. The first one is  that aims to predict whether a code is vulnerable to software systems or not. We use the C dataset provided by Zhou et al. (2019) for experiment. The second task is  which aims to measure the similarity between two code snippets and predict whether they have the same functionality. We experiment with the Java data provided by Wang et al. (2020). We employ F1 score and accuracy for evaluating these two tasks respectively. In total, our CodeT5 supports six tasks and fourteen sub-tasks in CodeXGLUE with a unified encoder-decoder model.We compare CodeT5 with state-of-the-art (SOTA) pre-trained models that can be categorized into three types: encoder-only, decoder-only, and encoder-decoder models. As  models, we consider RoBERTa (Liu et al., 2019b), RoBERTa (code) trained with masked language modeling (MLM) on code, CodeBERT (Feng et al., 2020) trained with both MLM and replaced token detection (Clark et al., 2020), GraphCode-BERT (Guo et al., 2021) using data flow from code, and DOBF (Rozière et al., 2021) trained with the identifier deobfuscation objective. Note that although DOBF employs a Seq2Seq model during pre-training, it only aims to train a better encoder for downstream tasks without exploring the poten-tial benefit of the pre-trained decoder.For  models, we compare GPT-2 (Radford et al., 2019) and its adaptations on code domain including CodeGPT-2, and CodeGPT-adapted. The difference is that the latter one utilizes a GPT-2 checkpoint for model initialization while the former one is trained from scratch. As  models, the current SOTA model for the CodeXGLUE benchmark is PLBART (Ah-mad et al., 2021) based on BART (Lewis et al., 2020) architecture. For pre-training data, most of these models employ CodeSearchNet (Husain et al., 2019) except DOBF and PLBART. DOBF is pre-trained on 7.9M Java and 3.6M Python files from BigQuery while PLBART employs a much larger data with 470M Python and 210M Java functions, and 47M NL posts from StackOverflow.4.5        Model ConfigurationsWe build CodeT5 based on Huggingface’s T5 (Raf-fel et al., 2020) PyTorch implementation3 and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M). We set the maximum source and target sequence lengths to be 512 and 256, respectively. We use the mixed precision of FP16 to accelerate the pre-training. We set the batch size to 1024 and employ the peak learning rate of 2e-4 with linear decay. We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5-base is 5 and 12 days, respectively.In the fine-tuning phase, we find that the tasks in CodeXGLUE (Lu et al., 2021) are quite sensitive to some hyper parameters such as learning rate, training steps, and batch size. We conduct a grid search and select the best parameters based on the validation set. In multi-task learning, we cover all downstream tasks except clone detection.5        Results and AnalysisIn this section, we compare CodeT5 with SOTA models on a broad set of CodeXGLUE downstream tasks (§5.1), and investigate the effects of our bimodal dual generation and multi-task learning (§5.2), followed by a detailed analysis on the proposed identifier-aware pre-training (§5.3).5.1        CodeXGLUE Downstream TasksWe evaluate two sizes of our model: CodeT5-small and CodeT5-base that are pre-trained with identifier-aware denoising. In addition, we consider the model that continues to train with bimodal dual generation (dual-gen) and show the results with multi-task fine-tuning. The results of all comparison models are obtained from their original papers and also the CodeXGLUE paper (Lu et al., 2021). We show code summarization results of smoothed BLEU-4 on six PL data in Table 2. We observe all our model variants significantly outperform prior work with either an encode-only (RoBERTa, CodeBERT, DOBF) or encoder-decoder framework (PLBART). Moreover, the salient performance gap between these two groups of models confirms that encode-only frameworks are suboptimal for generation tasks. Compared to the SOTA encoder-decoder model PLBART, we find that even our CodeT5-small yields better overall scores (also on Python and Java) given that our model is much smaller (60M vs. 140M) and PLBART is pre-trained with much larger Python and Java data (> 100 times). We attribute such improvement to our identifier-aware denoising pre-training and better employment of bi-modal training data4. By increasing the model size, our CodeT5-base boosts the overall performance by over 1.2 absolute points over PLBART. We compare CodeT5 with GPT-style models and PLBART in Table 3. Our CodeT5-small outperforms all decoder-only mod-els and also the SOTA PLBART, which again confirms the superiority of encoder-decoder models at generating code snippets. Moreover, our CodeT5-base further significantly pushes the SOTA results across three metrics. Particularly, it achieves around 4.7 points improvement on CodeBLEU over PLBART, indicating our CodeT5 can better comprehend the code syntax and semantics with the fier-aware pre-training.\
Code-to-Code Generation Tasks. We compare two code-to-code generation tasks: code translation and code refinement in Table 4 and further consider one naive copy baseline by copying the source input as the target prediction. In the code translation task, our CodeT5-small outperforms most of base-lines and obtains comparable results with PLBART, which shows the advantages of encoder-decoder models in the code-to-code generation setting. Our CodeT5-base further achieves consistent improvements over PLBART across various metrics for translating from Java to C# and vice versa.Here we show one CodeT5’s output of translating C# to Java in Figure 3. In this case, despite the poor BLEU score, CodeT5 is able to generate a function that reserves the same functionality and even has better readability compared to the ground-truth. This reveals that CodeT5 has a good generalization ability instead of memorizing and repeating what it has seen before. On the other hand, it also suggests that BLEU score is not a perfect evaluation metric for code generation tasks, where sometimes a higher score can instead reflect the problematic copy issues of neural models.Another code-to-code generation task is code refinement, a challenging task that requires detecting which parts of code are buggy and fix them via generating a bug-free code sequence. Due to the large overlap of source and target code, even the naive copy approach yields very high BLEU scores but zero exact matches. Therefore, we focus on the exact match (EM) metric to evaluate on this task. As shown in Table 4, we observe that EM scores for the small data are consistently higher than the medium one, indicating that it is harder to fix bugs for a longer code snippet. Our CodeT5-base significantly outperforms all baselines on EM and especially boosts over 4.8 points for the more challenging medium task (13.96 vs. GraphCodeBERT’s 9.10), reflecting its strong code understanding capability. We compare with two understanding tasks of defect detection and clone detection in Table 5.Specifically, we generate the binary labels as a unigram sequence from the decoder for the defect detection task, while for the clone detection task, we first obtain the sequence embedding of each code snippet using the last decoder state following Lewis et al. (2020) and then predict the labels by measuring their similarity. Both CodeT5-small and CodeT5-base outperform all baselines on the defect detection task while CodeT5-base yields 2.6 accuracy score improvement than PLBART. For the clone detection task, our CodeT5 models achieve comparable results to the SOTA GraphCodeBERT and PLBART models. These results demonstrate that with an encode-decoder framework, our CodeT5 can still be adapted well for understanding tasks.5.2        Effects of Bimodal Dual Generation and Multi-task LearningWe examine the effects of bimodal dual generation at pre-training and multi-task learning at fine-tuning. The bimodal pre-training brings consistent improvements for code summarization and generation tasks on both CodeT5-small and CodeT5-base. However, this pre-training task does not help and even sometimes slightly hurts the performance for PL-PL generation and understanding tasks. We anticipate this is because bimodal dual generation learns a better alignment between PL and NL that naturally benefits the former tasks involving both PL and NL. As a side effect, this objective could bias the model towards the PL-NL tasks and affect its performance on PL-PL tasks.In multi-task learning, it generally improves most of downstream tasks except the code translation and defect detection. Particularly, it largely boosts the performance on code summarization, which is not surprising as code summarization takes up the largest portion of sub tasks (six out of thirteen) and thereby benefit the most from the multi-task learning. Besides, we observe that multi-task learning consistently improves the performance of code refinement, which might benefit from the joint training of both small and medium refinement data.\
Another possible reason is that multi-task training with defect detection would enable the model to better comprehend the code semantics for bug detection, which is also a necessary intermediate step for code refinement.5.3        Analyzing Identifier-aware Pre-trainingWe provide an ablation study to examine the contribution of each component in our identifier-aware objective. Specifically, we compare the performance of our CodeT5-small on four selected tasks by ablating each of the three objectives: masked span prediction (MSP), identifier tagging (IT), and masked identifier prediction (MIP). As shown in Table 6, we observe that generally removing one of the objectives would reduce the performance for all tasks, indicating that all objectives contribute to the better code understanding of our CodeT5. However, the effect of each objective differs across tasks. Specifically, removing MSP would largely reduce the performance of all generation tasks but instead increase the defect detection performance. This shows that masked span prediction is more crucial for capturing syntactic information for generation tasks. On the contrary, removing MIP would hurt the defect detection task the most, indicating that it might focus more on code semantic understanding. By combining these objectives, our CodeT5 can better capture both syntactic and semantic information from code.We further provide outputs from CodeT5 and its variant without MIP and IT on code generation in Figure 4. We observe that CodeT5 can correctly generate the exact function, while the model without MIP and IT fails to recover the identifiers of “s2” and “hasField”. This shows our identifier-aware denoising pre-training can better distinguish and leverage the identifier information.We also investigate the identifier tagging performance and find it achieves over 99% F1 for all PLs, showing that our CodeT5 can confidently distinguish identifiers in code. We then check whether MSP and MIP tasks would have conflicts as they employ the same sentinel tokens for masking. In identifier masking, all occurrences of one unique identifier are replaced with the same sentinel token, resulting in a many-to-one mapping compared to the one-to-one mapping in span prediction. We compare models pre-trained with either MSP or MIP, and both on these two tasks in Table 7. We report the prediction accuracy and also the ratio of how often they can generate the same number of predictions as the sentinel tokens. We observe that pre-training only with either MIP or MSP would bias the model towards that task, achieving poor accuracy and higher mismatch in number of predictions when applied to the other task. Interestingly, we find that MIP-only objective can better recover the correct number of predictions in the MSP task than MSP-only does for the MIP task, meaning that it is easier to adapt from many-to-one mapping to one-to-one mapping and difficult for the opposite. At last, combining them can help our model to make a good trade-off on both tasks.We have presented CodeT5, a pre-trained encoder-decoder model that incorporates the token type information from code. We propose a novel identifier-aware pre-training objective to better leverage the identifiers and propose a bimodal dual generation task to learn a better NL-PL alignment using code and its comments. Our unified model can support both code understanding and generation tasks and allow for multi-task learning. Experiments show that CodeT5 significantly outperforms all prior work in most CodeXGLUE tasks. Further analysis also reveals its better code comprehension capability across various programming languages.Broader Impact and Ethical ConsiderationOur work generally belongs to NLP applications for software intelligence. With the goal of improving the development productivity of software with machine learning methods, software intelligence research has attracted increasing attention in both academia and industries over the last decade. Software code intelligence techniques can help developers to reduce tedious repetitive workloads, enhance the programming quality and improve the overall software development productivity. This would considerably decrease their working time and also could potentially reduce the computation and operational cost, as a bug might degrade the system performance or even crash the entire system. Our work addresses the fundamental challenge of software code pre-training, our study covers a wide range of code intelligence applications in the software development lifecycle, and the proposed CodeT5 method achieves the state-of-the-art performance on many of the benchmark tasks, showing its great potential benefit towards this goal.We further discuss the ethical consideration of training CodeT5 and the potential risks when applying it into real-world downstream applications: The training datasets in our study are source code including user-written comments from open source Github repositories and publicly available, which do not tie to any specific application. However, it is possible that these datasets would encode some stereotypes like race and gender from the text comments or even from the source code such as variables, function and class names. As such, social biases would be intrinsically embedded into the models trained on them. As suggested by Chen et al. (2021), interventions such as filtration or modulation of generated outputs may help to mitigate these biases in code corpus. Our model pre-training requires non-trivial computational resources though we have tried our best to carefully design our experiments and improve experiments to save unnecessary computation costs. In fact, compared to the recent large-scale language model Codex (Chenet al., 2021), our CodeT5-base has a much smaller model size of 220M than theirs of 12B (∼ 55×). In addition, we experiment on Google Cloud Plat-form which purchases carbon credits to reduce its carbon footprint,  training CodeT5-base produced around 49.25 kg CO2 which was totally off-set by the provider. Furthermore, we release our pre-trained models publicly to avoid repeated training for the code intelligence research community. As CodeT5 can be deployed to provide coding assistance such as code generation for aiding developers, automation bias of machine learning systems should be carefully considered, especially for developers who tend to over-rely on the model-generated outputs. Sometimes these systems might produce functions that superficially appear correct but do not actually align with the developer’s intents. If developers unintentionally adopt these incorrect code suggestions, it might cause them much longer time on debugging and even lead to some significant safety issues. We suggest practitioners using CodeT5 should always bear in mind that its generation outputs should be only taken as references which require domain experts for further correctness and security checking. We train CodeT5 on existing code corpus including CodeSearchNet (Husain et al., 2019) and a small fraction of Google BigQuery, both of which are originally collected from public Github repositories. Pre-trained mod-els might encode some sensitive information ( personal addresses or identification numbers) from the training data. Though we have conducted multi-rounds of data cleaning to mitigate this before training our models, it is still possible that some sensitive information cannot be completely removed. Besides, due to the non-deterministic nature of generation models like CodeT5, it might produce some vulnerable code to harmfully affect the software and even be able to benefit more advanced malware development when deliberately misused.We thank Akhilesh Deepak Gotmare, Amrita Saha, Junnan Li, and Chen Xing for valuable discussions. We thank Kathy Baxter for the ethical review. We also thank our anonymous reviewers for their insightful feedback on our paper.Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-trainingfor program understanding and generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2655–2668. Association for Computational Linguistics.Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Win-ter, Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welin-der, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. , abs/2107.03374.Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7057–7067.Sergio Cozzetti B. de Souza, Nicolas Anquetil, and Káthia Marçal de Oliveira. 2005. A study of the documentation essential to software maintenance. In Proceedings of the 23rd Annual International Conference on Design of Communication: documenting & Designing for Pervasive Information, SIGDOC 2005, Coventry, UK, September 21-23, 2005, pagesJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training ofdeep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186.Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-aocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-bert: A pre-trained model for programming and natural languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020, pages 1536–1547. Association for Computational Linguistics.Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-fano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021. Graphcodebert: Pre-trainingcode representations with data flow. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping language to codein programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1643–1652. Association for Computational Linguistics.Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020. Learning and evaluatingcontextual embedding of source code. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5110–5121. PMLR.Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-feng Gao. 2019a. Multi-task deep neural networksfor natural language understanding. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4487–4496. Association for Computational Linguistics.Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of programming languages. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, DecemberRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words withsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose:code generation using transformer. In ESEC/FSE ’20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020, pages 1433–1443. ACM.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>How Microsoft Trained a 270M-Pair AI to Power Smarter Search</title><link>https://hackernoon.com/how-microsoft-trained-a-270m-pair-ai-to-power-smarter-search?source=rss</link><author>Microsoft</author><category>tech</category><pubDate>Sat, 28 Feb 2026 14:41:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Liang Wang (Microsoft Corporation)Nan Yang (Microsoft Corporation)Xiaolong Huang (Microsoft Corporation)Binxing Jiao (Microsoft Corporation)Linjun Yang (Microsoft Corporation)Daxin Jiang (Microsoft Corporation)Rangan Majumder (Microsoft Corporation)Furu Wei (Microsoft Corporation)This paper presents E5 1, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40× more parameters.Text embeddings are low-dimensional vector representations for arbitrary-length texts and play key roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface easily consumable by downstream applications.While pre-trained language models such as BERT [17] and GPT [7] can produce transferrable text representations, they are not ideal for tasks such as retrieval and text matching where a single-vector embedding of texts is more desired due to its efficiency and versatility. To obtain better text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level representations from text pairs. Along this line of research, some works are geared towards learning task-specific embeddings. For example, GTR [43] and Sentence-T5 [44] fine-tune pre-trained models with supervised datasets to learn embeddings customized for passage retrieval and semantic textual similarity, respectively. Other works learn unsupervised embeddings from automatically constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT) [9], random cropping [28] and neighboring text spans [41], etc. While such synthetic data are of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the performance of the classic BM25 baseline without further fine-tuning [40].In this work, we learn a high-quality general-purpose text embedding termed E5, mbddings from bidirctional ncoder rpresentations. E5 aims to provide strong off-the-shelf text embeddings suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings. To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing heterogeneous training signals. We construct the CCPairs dataset by combining various semi-structured data sources such as CommunityQA, Common Crawl and Scientific papers, and perform aggressive filtering with a consistency-based filter [15] to improve data quality. We choose a simple contrastive learning recipe using in-batch negatives with a large batch-size to train our model. Extensive experiments on both BEIR and MTEB benchmarks demonstrate the effectiveness of the proposed method. On the BEIR zero-shot retrieval benchmark [53], E5 is the first model to outperform the strong BM25 baseline without using any labeled data. When fine-tuned on labeled datasets, the performance can be further improved. Results on 56 datasets from the recently introduced MTEB benchmark [40] show that our E5base is competitive against GTRxxl and Sentence-T5xxl, which have 40× more parameters.There have been long-lasting interests in transforming texts into low-dimensional dense embeddings. Early works include Latent Semantic Indexing (LSA) [16] and Latent Dirichlet Allocation (LDA) [3]. LSA utilizes the decomposition of a word-document co-occurrence matrix to generate document embeddings, while LDA adopts probabilistic graphical models to learn topic distributions. Aroraet al. show that a simple weighted average of word vectors [38] can be a strong baseline for sentence embeddings.With the development of pre-trained language models [17, 35, 48] and large-scale labeled datasets such as SNLI [6] and MS-MARCO [8], methods like Sentence-BERT [49], SimCSE [22], Sentence-T5 [44] and SGPT [39] directly fine-tune language models to output continuous embeddings. Most research focuses on short texts and thus uses the term "sentence embeddings". For long documents, it remains an open research question whether fixed-length embeddings can encode all the information. Contrastive loss popularized by SimCLR [10] turns out to be more effective than classification-based losses [49, 14] for embeddings. LaBSE [20], LASER [2] and CLIP [47] further extend to multilingual and multi-modal scenarios using parallel sentences and image-text pairs.Another direction is to design self-supervised pre-training tasks for text matching and retrieval. [9] proposes the well-known inverse cloze task (ICT), where a random sentence within a passage is chosen as a pseudo-query and the rest is treated as a positive sample. However, Contriever [28] shows that random cropping with data augmentation is more effective than ICT on a range of zero-shot information retrieval tasks. OpenAI text embeddings [41] use neighboring texts as positives and scale up the model size to 175B. Oguz et al. [45] performs domain-matched pre-training to improve in-domain results. SPAR [11] trains a dense retriever by treating BM25 as a teacher model. Although the aforementioned approaches can easily obtain abundant supervision signals, such synthetic data tend to be of low quality. Results on the BEIR benchmark [53] show they struggle to match the performance of BM25 if not further fine-tuned on labeled datasets.Evaluation and interpretation of text embeddings are also non-trivial. Most benchmarks measure the embedding quality through downstream task performances. For example, SentEval [13] uses linear probing and a collection of semantic textual similarity (STS) datasets, while the BEIR benchmark [53] focuses on zero-shot information retrieval scenarios. The recently introduced MTEB benchmark [40] combines 56 datasets spanning across 8 tasks and 112 languages. Experiments show no model can achieve state-of-the-art results on all embedding tasks yet. In this paper, we do not use the SentEval toolkit since its linear probing setup depends on the optimization hyperparameters.Most closely related to our work is a series of community efforts by 2 to train embeddings with a collection of labeled and automatically collected datasets. In this paper, we show that it is possible to train high-quality embeddings using self-supervised pre-training only. In terms of benchmark results, our model can achieve superior performance when fine-tuned on less labeled data.3        CCPairs: A Large Collection of Text Pair DatasetThe quality and diversity of the data is crucial for training general-purpose text embeddings. In this work, we mine and assemble CCPairs, a large high-quality text pair dataset from web sources which provide diverse training signals transferring well to a wide range of tasks.\
Harvesting semi-structured data sources Large-scale high-quality datasets like C4 [48] and CCMatrix [51] are vital for the success of language model pre-training and machine translation. For learning text embeddings, existing works either utilize small-scale human-annotated data such as NLI [22] and MS-MARCO [8] or adopt heuristics such as random cropping [28] to obtain large-scale but very noisy supervision signals.Instead, we curate a text pair dataset CCPairs (olossal lean text ) by harvesting heterogeneous semi-structured data sources. Let (, ) denote a text pair consisting of a query  and a passage . Here we use “” to denote word sequences of arbitrary length, which can be a short sentence, a paragraph, or a long document. Our dataset includes (post, comment) pairs from Reddit 3, (question, upvoted answer) pairs from Stackexchange 4, (entity name + section title, passage) pairs from English Wikipedia, (title, abstract) and citation pairs from Scientific papers [36], and (title, passage) pairs from Common Crawl 5 web pages and various News sources.We only include data sources that can be automatically mined, and some subsets are directly reused from existing datasets. Simple heuristic rules are applied to filter data from Reddit and Common Crawl. For example, we remove Reddit comments that are either too long ( 4096 characters) or receive score less than 1, and remove passages from web pages with high perplexity [60]. After preliminary filtering, we end up with ∼ 1*.*3 billion text pairs, most of which come from Reddit and Common Crawl. For more details and examples, please refer to Appendix A. To further improve data quality and make training costs manageable, we propose a consistency-based data filtering technique: a model is first trained on the 1*.*3B noisy text pairs, and then used to rank each pair against a pool of 1 million random passages. A text pair is kept only if it falls in the top- ranked lists. In other words, the model’s prediction should be consistent with the training labels. Here we set  = 2 based on manual inspection of data quality. After this step, we end up with ∼ 270M text pairs for contrastive pre-training.The intuition for this technique comes from the memorization behaviors of neural networks [19]: when trained on noisy datasets, neural networks tend to memorize the clean labels first and then gradually overfit the noisy labels. Similar techniques [42, 15, 23] have been widely used for removing dataset noises. It is also possible to apply this filter iteratively, we will leave it for future work.Our embeddings can be trained with only unlabeled text pairs from CCPairs with contrastive pre-training. A second-stage fine-tuning on small, high-quality labeled datasets can be performed to further boost the quality of the resulted embeddings. See Figure 1 for an overview.4.1       Contrastive Pre-training with Unlabeled DataContrastive pre-training aims to distinguish the relevant text pairs from other irrelevant or negative pairs. Given a collection of text pairs {()} , we assign a list of negative passages {−}=1 for the -th example. Then the InfoNCE contrastive loss [10] is as follows:\
where () is a scoring function between query  and passage  parameterized by ***θ. Following the popular biencoder architecture, we use a pre-trained Transformer encoder and average pooling over the output layer to get fixed-size text embeddings * and . The score is the cosine similarity scaled by a temperature hyperparameter  :Where  is set to 0.01 in our experiments by default. We use a shared encoder for all input texts and break the symmetry by adding two prefix identifiers  and  to  and  respectively. For some data sources such as citation pairs, it is not obvious which side should be the query, we randomly choose one for simplicity. Such an asymmetric design turns out to be important for some retrieval tasks where there exist paraphrases of the query in the target corpus.Another critical issue for contrastive training is how to select the negative samples. Here we choose to use the in-batch negatives [10], where the passages from other pairs in a batch serve as negative samples. We find that this simple strategy enables more stable training and outperforms methods such as MoCo [25] when the batch size is sufficiently large.4.2       Fine-tuning with Labeled DataWhile contrastive pre-training on the CCPairs provides a solid foundation for general-purpose embeddings, further training on labeled data can inject human knowledge into the model to boost the performance. Although these datasets are small, existing works [43, 44] have shown that supervised fine-tuning leads to consistent performance gains. In this paper, we choose to further train with a combination of 3 datasets: NLI 6 (Natural Language Inference), MS-MARCO passage ranking dataset [8], and NQ (Natural Questions) dataset [30, 32]. Empirically, tasks like STS (Semantic Textual Similarity) and linear probing benefit from NLI data, while MS-MARCO and NQ datasets transfer well to retrieval tasks.Building on the practices of training state-of-the-art dense retrievers [50, 58], we use mined hard negatives and knowledge distillation from a cross-encoder (CE) teacher model for the MS-MARCO and NQ datasets. For the NLI dataset, contradiction sentences are regarded as hard negatives. The loss function is a linear interpolation between contrastive loss cont for hard labels and KL divergence KL for distilling soft labels from the teacher model.Where ce and stu are the probabilities from the cross-encoder teacher model and our student model.  is a hyperparameter to balance the two loss functions. cont is the same as in Equation 1.4.3       Applications to Text Embedding TasksAfter the above two steps, we obtain high-quality text embeddings transferring well to a wide range of tasks without fine-tuning the model parameters. Combined with techniques like approximate nearest neighbor search, embeddings provide a scalable and efficient solution for applications like web search. Here we briefly illustrate several use cases of our text embeddings. First, the passage embeddings for the target corpus are computed and indexed offline. Then for each query, we compute its query embedding and return the top- ranked lists from the corpus based on cosine similarity.Few-shot Text Classification A linear classifier is trained on top of the frozen embeddings with a few labeled examples. Different tasks only need to train and save the parameters of the classification heads. It can be seen as a particular form of parameter-efficient learning [27].Zero-shot Text Classification The input and label texts are converted to sentences based on manually written prompt templates. The predicted label is the one closest to the input text in the embedding space. Take the sentiment classification of movie reviews as an example, with the original input “”, the label text is “it is an example of terrible/great movie review” and the input text becomes “movie review: I enjoy watching it”.Semantic Textual Similarity Given two text embeddings, we use the cosine function to measure their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the evaluation is usually based on rank correlation coefficients. Standard clustering algorithms such as k-means can be applied straightforwardly. Texts belonging to the same category are expected to be close in the embedding space.For tasks other than zero-shot text classification and retrieval, we use the query embeddings by default.5.1       Pre-training and Fine-tuning Configurations We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base and E5large initialized from MiniLM [59], bert-base-uncased, and bert-large-uncased-whole-wordmasking respectively. The batch size is set to a large value of 32, 768 to increase the number of negatives. The learning rate is {3, 2, 1}×10−4 for the {small, base, large} models, with linear decay and the first 1, 000 steps for warmup. We pre-train for 20k steps in total with AdamW optimizer, which is approximately 2.5 epochs over the dataset. It takes {16, 32, 64} V100 GPUs and {1, 1, 2} days for the {small, base, large} models. To improve training efficiency and reduce GPU memory usage, we adopt mixed precision training and gradient checkpointing.\
 is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [8], NQ [32, 30], and NLI [22] datasets. We reuse the mined hard negatives and re-ranker scores from SimLM [58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8 GPUs. Learning rate is {3*,* 2*,* 1}×10−5 for the {small, base, large} models with 400 steps warmup. For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each example, 6 sentences are randomly sampled from the entire corpus.We use E5-PT to denote models with contrastive pre-training only. More implementation details can be found in Appendix B.5.2       Evaluation Datasets is a collection of 19 information retrieval datasets, ranging across ad-hoc web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate the 15 datasets that provide public downloads. The main metric is nDCG@10. is recently proposed for benchmarking massive text embedding tasks. Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are still only available in English. In this paper, we evaluate the English subsets, which have 56 datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification (PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively. Please refer to the MTEB paper for details.5.3       Results on BEIR benchmarkResults with Unsupervised Methods In Table 1, we show model results that do not use any labeled data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1*.*2 points. To the best of our knowledge, this is the first reported result that an unsupervised model can beat BM25 on the BEIR benchmark. When scaling up to E5-PTlarge, we see further benefits from42.*2.\n In terms of pre-training tasks, Contriever adopts random cropping, while LaPraDor combines ICT and dropout-as-positive-instance from SimCSE. The methods can easily obtain large-scale training data, while our approach requires more effort in dataset curation. Such efforts pay off with better results. Recent studies [34, 60, 21] also show that improving data quality is a vital step for training large language models.\
Results with Supervised Fine-tuning In Table 2, we fine-tune our models on supervised datasets and then transfer them to the BEIR benchmark. Since our fine-tuning datasets include MS-MARCO and NQ, the corresponding numbers are in-domain results. For other datasets, these are zero-shot transfer results. Our E5base model achieves an average nDCG@10 of 48*.*7, already surpassing existing methods with more parameters such as GTRlarge [43]. Most datasets benefit from supervised fine-tuning, but there are also a few exceptions such as FiQA, Scidocs, and Fever, etc. This is likely due to the lack of enough domain diversity for the fine-tuning datasets.5.4       Results on MTEB benchmarkIn Table 3, E5 models not only substantially outperform existing ones with similar sizes, but also match the results of much larger models. The top-2 models on MTEB leaderboard 7 GTRxxl and Sentence-T5xxl have 4*.*8B parameters, while our E5large model is more than 10× smaller with 300M parameters. We expect that our model will benefit from continual scaling up.Since the difference between BERT-FTbase and E5base is that BERT-FTbase only has fine-tuning stage, their performance gap demonstrates the usefulness of contrastive pre-training on our proposed CCPairs dataset. For most task categories except Clustering, performance improves after supervised fine-tuning. Consistent with prior works [43, 44], this once again demonstrates the importance of incorporating human knowledge for learning better text embeddings. It remains an open question whether state-of-the-art embeddings can be obtained in a purely self-supervised manner.\
Table 4 shows the zero-shot text classification results on the dev set of the SST-2 dataset [52]. By formulating text classification as embedding matching between input and label texts, our model can be much better than the “majority” baseline in a zero-shot setting. We use the prompt template from Section 4.3.In this section, we conduct a series of analyses to examine various design choices. All the numbers in this section are from base-size models. For the BEIR benchmark, we choose 6 datasets with more stable results across different runs. Some negative results are also listed in Appendix C. Since we use in-batch negatives for contrastive pre-training, larger batch size will provide more negatives and therefore improve the quality of the learned text embeddings. In Table 5, increasing batch size from 1K to 32K leads to consistent gains across all 6 datasets. It is also possible to train with smaller batch sizes by adding hard negatives [50]. However, the engineering efforts of mining hard negatives for large datasets (>100M) are non-trivial.\
 GTR models are fine-tuned with “MS-MARCO + NQ”, while Sentence-T5 models use NLI instead. In Table 6, we can see that the “MS-MARCO + NQ” setting performs best on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar observations are also made by Muennighoff et al. [40]. Combining all of them leads to the best overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for learning text embeddings.\
 One crucial step in our dataset curation pipeline is filtering out low-quality text pairs. In Table 7, when training with 1M pairs, using filtered data has a nearly 6 points advantage. When all the text pairs are used, the “w/o filter” setting has about 4× more data but is still behind by 1*.*6 points. Though recent studies [29, 47] show that deep learning models are quite robust to dataset noises, data filtering still has benefits in improving training efficiency and model quality. We explore two alternative methods to enlarge the number of negatives: Pre-batch negatives [33] reuse embeddings from previous batches as additional negatives, while MoCo[25] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches, the negative size can be easily scaled up without incurring much GPU memory overhead. The downside is that most negatives are produced by an older version of model parameters. In Table 8, in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain hyperparameters such as temperature, better results are possible with more tuning. With the rapid development of dense retrieval models, can we replace the long-standing BM25 algorithm from now on? The answer is likely “”. BM25 still holds obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such as Trec-Covid [55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily on exact lexical match (Fever) [54], further research efforts are still necessary to improve current dense retrievers.In this work, we train a general-purpose text embedding model E5 from weak supervision signals. We adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf performance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching. When further customized for downstream tasks, E5 achieves superior fine-tuned performance compared to existing embedding models with 40× more parameters on the large, 56-task MTEB benchmark datasets.[1]    Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=SyK00v5xx.[2]     Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics, 7:597–610, 2019. doi: 10.1162/tacl00288. URL https://aclanthology. org/Q19-1038.[3]     David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada], pages 601–608. MIT Press, 2001. URL https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html.[4]     Alexander Bondarenko, Maik Fröbe, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al. Overview of touché 2022: argument retrieval. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 311–336. Springer, 2022.[5]    Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. A full-text learning to rank dataset for medical information retrieval. In European Conference on Information Retrieval, pages 716–722. Springer, 2016.[6]    Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:[7]    Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn-ers. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.[8]     Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated machine reading comprehension dataset. , abs/1611.09268, 2016.[9]     Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for embedding-based large-scale retrieval. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkg-mA4FDr.[10]     Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597–1607. PMLR, 2020. URL http:[11]    Xilun Chen, Kushal Lakhotia, Barlas Og˘uz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv preprint arXiv:2110.06918, 2021.[12]    Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270–2282, 2020.[13]    Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Re-sources and Evaluation (LREC 2018), Miyazaki, Japan, 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1269.[14]    Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Super-vised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070.  URL https://aclanthology.org/D17-1070.[15]    Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. , abs/2209.11755, 2022.[16]    Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391–407, 1990.[17]     Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL  https://aclanthology.org/N19-1423.[18]    Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. Climate-fever: A dataset for verification of real-world climate claims. arXiv preprint arXiv:2012.00614, 2020.[19]     Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html.[20]    Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-agnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878–891, 2022.[21]     Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. , abs/2101.00027, 2021.[22]    Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894–6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https://aclanthology.org/2021.emnlp-main.552.[23]     Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.  In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kris-ten Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neu-ral Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada,[24]    Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. Dbpedia-entity v2: a test collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1265–1268, 2017.[25]    Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 9726–9735. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00975. URL https://doi.org/10.1109/CVPR42600.2020.00975.[26]    Doris Hoogeveen, Karin M Verspoor, and Timothy Baldwin. Cqadupstack: A benchmark data set for community question-answering research. In Proceedings of the 20th Australasian document computing symposium, pages 1–8, 2015.[27]    Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR, 2019.  URL http://proceedings.mlr.press/v97/houlsby19a.html.[28]     Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive learning. , abs/2112.09118, 2021.[29]     Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4904–4916. PMLR, 2021.  URL  http://proceedings.mlr.press/v139/jia21b.html.[30]    Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769–6781, Online, 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.emnlp-main.550.  URL https://aclanthology.org/2020.emnlp-main. 550.[31]    Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contex-tualized late interaction over BERT. In Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Vir-tual Event, China, July 25-30, 2020, pages 39–48. ACM, 2020. doi: 10.1145/3397271.3401075. URL https://doi.org/10.1145/3397271.3401075.[32]    Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl00276. URL https://aclanthology.org/Q19-1026.[33]    Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6634–6647, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.518. URL https://aclanthology.org/2021. acl-long.518.[34]    Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In , 2022.[35]    Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. , abs/1907.11692, 2019.[36]     Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983, Online, 2020. Associ-ation for Computational Linguistics.  doi: 10.18653/v1/2020.acl-main.447.  URL https://aclanthology.org/2020.acl-main.447.[37]    Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018, pages 1941–1942, 2018.[38]    Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In , 2013.[39]    Niklas Muennighoff.      Sgpt: Gpt sentence embeddings for semantic search.  , abs/2202.08904, 2022.[40]    Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. , abs/2210.07316, 2022.[41]     Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pre-training. , abs/2201.10005, 2022.[42]    Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. SELF: learning to filter noisy labels with self-ensembling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=HkgsPhNYPS.[43]    Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern’andez ’Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. , abs/2112.07899, 2021.[44]    Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1864–1874, 2022.[45]    Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, and Yashar Mehdad. Domain-matched pre-training tasks for dense retrieval. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 1524–1534. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-naacl.114. URL   https://doi.org/10.18653/v1/2022.findings-naacl.114.[46]    Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks. In North American Chapter of the Association for Computational Linguistics, 2020.[47]    Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervi-sion. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html.[48]    Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020.[49]    Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410.[50]    Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825–2835, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.224. URL https://aclanthology.org/2021.emnlp-main.224.[51]    Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490–6500, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.507.   URL  https://aclanthology.org/2021.acl-long.507.[52]    Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing, 2013.[53]    Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.[54]     James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https:[55]    Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. Trec-covid: constructing a pandemic information retrieval test collection. In , volume 54, pages 1–12. ACM New York, NY, USA, 2021.[56]    Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 241–251, 2018.[57]     David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534–7550, 2020.[58]    Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval. , abs/2207.02578, 2022.[59]    Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2140–2151, 2021.[60]     Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4003–4012, Marseille, France, 2020. European Language Resources Associ-ation.  ISBN 979-10-95546-34-4.  URL https://aclanthology.org/2020.lrec-1.494.[61]    Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview. net/forum?id=zeFrfgyZln.[62]     Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Laprador: Unsupervised pretrained dense retriever for zero-shot text retrieval. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3557–3569, 2022.[63]     Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, 2018.For Common Crawl, we download the 2022-33 snapshot and cc_net 8 is used for preprocessing including language identification, de-duplication, language model filtering, etc. Web pages from the MS-MARCO document ranking corpus are also included. For the data filtering step, we examine each pair of passages within a web page instead of just using the title as a query. For Wikipedia, we use the version released by Petroni et al. [46]. To avoid possible data contamination, we remove text pairs that occur in the evaluation datasets based on exact string match.Reddit data is collected from the year 2018 to August 2022. For the S2ORC data, we use a sample weight of 0*.*3 during training to avoid over-fitting the scientific domains.For the BEIR benchmark, we use the 15 datasets that provide public downloads: MS MARCO [8], Trec-Covid [55], NFCorpus [5], NQ [32], HotpotQA [63], FiQA [37], ArguAna [56], Touche-2020 [4], CQADupStack [26], Quora, DBPedia [24], Scidocs [12], Fever [54], Climate-Fever [18], and Scifact [57].B        Implementation DetailsWe list the hyperparameters in Table 11. Since some evaluation datasets have long texts, we freeze the position embeddings during both pre-training and fine-tuning and set the maximum text length to 512 for evaluation.For the Quora duplicate retrieval task in the BEIR benchmark, we add prefix “ ” to all the questions. For other retrieval tasks, we use “ ” and “ ” prefixes correspondingly.The MS-MARCO results in Table 12 use document titles provided by RocketQA [50]. This evaluation setup is consistent with most state-of-the-art dense retrievers. However, the MS-MARCO data from the BEIR benchmark does not have titles, so the results are expected to be lower.\
 We report results for in-domain datasets in Table 12. These results can help illustrate the benefits brought by contrastive pre-training when abundant in-domain labeled data are available. For MS-MARCO passage ranking, MRR@10 and Recall@1k are reported. For the NQ dataset, Recall@20 and Recall@100 are the main metrics.C        Negative ResultsHere are some attempts that we eventually give up on:Adding BM25 hard negatives Similar to DPR [30], we add one BM25 hard negative for each positive pair during training. When using 15M data, this strategy improves the overall results by ~ 0.5 points on the BEIR benchmark. However, running the BM25 algorithm over a 250M+ dataset is too time-consuming even with multi-node and multi-process parallelism.Using RoBERTa instead of BERT for initialization Though RoBERTa shows consistent gains on many NLP tasks, we empirically find that RoBERTa performs worse than BERT initialization on most of the BEIR benchmark datasets. We add a masked language modeling loss for 25% of the training text pairs. The numbers are on par with removing this auxiliary objective, but the training cost goes up.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Microsoft’s Graphormer: The Transformer That Finally Beats GNNs</title><link>https://hackernoon.com/microsofts-graphormer-the-transformer-that-finally-beats-gnns?source=rss</link><author>Microsoft</author><category>tech</category><pubDate>Sat, 28 Feb 2026 14:31:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Chengxuan Ying, yingchengsyuan@gmail.com  (Dalian University of Technology)Tianle Cai, tianle.cai@princeton.edu  (Princeton University)Shengjie Luo, luosj@stu.pku.edu.cn  (Peking University)Shuxin Zheng, shuz@microsoft.com  (Microsoft Research Asia)Guolin Ke, guoke@microsoft.com  (Microsoft Research Asia)Di He, dihe@microsoft.com  (Microsoft Research Asia)Yanming Shen, shen@dlut.edu.cn  (Dalian University of Technology)Tie-Yan Liu, tyliu@microsoft.com  (Microsoft Research Asia)The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer. The code and models of Graphormer will be made publicly available at https://github.com/Microsoft/Graphormer.The Transformer [49] is well acknowledged as the most powerful neural network in modelling sequential data, such as natural language [11, 35, 6] and speech [17]. Model variants built upon Transformer have also been shown great performance in computer vision [12, 36] and programming language [19, 63, 44]. However, to the best of our knowledge, Transformer has still not been the de-facto standard on public graph representation leaderboards [22, 14, 21]. There are many attempts of leveraging Transformer into the graph domain, but the only effective way is replacing some key modules (e.g., feature aggregation) in classic GNN variants by the softmax attention [50, 7, 23, 51, 61, 46, 13]. Therefore, it is still an open question whether Transformer architecture is suitable to model graphs and how to make it work in graph representation learning.In this paper, we give an affirmative answer by developing Graphormer, which is directly built upon the standard Transformer, and achieves state-of-the-art performance on a wide range of graph-level prediction tasks, including the very recent Open Graph Benchmark Large-Scale Challenge (OGB-LSC) [21], and several popular leaderboards (e.g., OGB [22], Benchmarking-GNN [14]). The Transformer is originally designed for sequence modeling. To utilize its power in graphs, we believe the key is to properly incorporate structural information of graphs into the model. Note that for each node , the self-attention only calculates the semantic similarity between  and other nodes, without considering the structural information of a graph reflected on the nodes and the relation between node pairs. Graphormer incorporates several effective structural encoding methods to leverage such information, which are described below.First, we propose a  in Graphormer to capture the node importance in the graph. In a graph, different nodes may have different importance, e.g., celebrities are considered to be more influential than the majority of web users in a social network. However, such information isn’t reflected in the self-attention module as it calculates the similarities mainly using the node semantic features. To address the problem, we propose to encode the node centrality in Graphormer. In particular, we leverage the  for the centrality encoding, where a learnable vector is assigned to each node according to its degree and added to the node features in the input layer. Empirical studies show that simple centrality encoding is effective for Transformer in modeling the graph data.Second, we propose a novel  in Graphormer to capture the structural relation between nodes. One notable geometrical property that distinguishes graph-structured data from other structured data, e.g., language, images, is that there does not exist a canonical grid to embed the graph. In fact, nodes can only lie in a non-Euclidean space and are linked by edges. To model such structural information, for each node pair, we assign a learnable embedding based on their spatial relation. Multiple measurements in the literature could be leveraged for modeling spatial relations. For a general purpose, we use the distance of the shortest path between any two nodes as a demonstration, which will be encoded as a bias term in the softmax attention and help the model accurately capture the spatial dependency in a graph. In addition, sometimes there is additional spatial information contained in edge features, such as the type of bond between two atoms in a molecular graph. We design a new edge encoding method to further take such signal into the Transformer layers. To be concrete, for each node pair, we compute an average of dot-products of the edge features and learnable embeddings along the shortest path, then use it in the attention module. Equipped with these encodings, Graphormer could better model the relationship for node pairs and represent the graph.By using the proposed encodings above, we further mathematically show that Graphormer has strong expressiveness as many popular GNN variants are just its special cases. The great capacity of the model leads to state-of-the-art performance on a wide range of tasks in practice. On the large-scale quantum chemistry regression dataset3 in the very recent Open Graph Benchmark Large-Scale Challenge (OGB-LSC) [21], Graphormer outperforms most mainstream GNN variants by more than 10% points in terms of the relative error. On other popular leaderboards of graph representation learning (e.g., MolHIV, MolPCBA, ZINC) [22, 14], Graphormer also surpasses the previous best results, demonstrating the potential and adaptability of the Transformer architecture.In this section, we recap the preliminaries in Graph Neural Networks and Transformer.Graph Neural Network (GNN). Let G = (V, E) denote a graph where V = {v1, v2, · · · , vn}, n = |V | is the number of nodes. Let the feature vector of node vi be xi . GNNs aim to learn representation of nodes and graphs. Typically, modern GNNs follow a learning schema that iteratively updates the representation of a node by aggregating representations of its first or higher-order neighbors. We denote h (l) i as the representation of vi at the l-th layer and define h (0) i = xi . The l-th iteration of aggregation could be characterized by AGGREGATE-COMBINE step aswhere N (vi) is the set of first or higher-order neighbors of vi . The AGGREGATE function is used to gather the information from neighbors. Common aggregation functions include MEAN, MAX, SUM, which are used in different architectures of GNNs [26, 18, 50, 54]. The goal of COMBINE function is to fuse the information from neighbors into the node representation.\
In addition, for graph representation tasks, a READOUT function is designed to aggregate node features h (L) i of the final iteration into the representation hG of the entire graph G:READOUT can be implemented by a simple permutation invariant function such as summation [54] or a more sophisticated graph-level pooling function [1].. The Transformer architecture consists of a composition of Transformer layers [49]. Each Transformer layer has two parts: a self-attention module and a position-wise feed-forward network (FFN). Let H = h > 1 , · · · , h> n > ∈ R n×d denote the input of self-attention module where d is the hidden dimension and hi ∈ R 1×d is the hidden representation at position i. The input H is projected by three matrices WQ ∈ R d×dK , WK ∈ R d×dK and WV ∈ R d×dV to the corresponding representations Q, K, V . The self-attention is then calculated as:where  is a matrix capturing the similarity between queries and keys. For simplicity of illustration, we consider the single-head self-attention and assume  =  = . The extension to the multi-head attention is standard and straightforward, and we omit bias terms for simplicity.In this section, we present our Graphormer for graph tasks. First, we elaborate on several key designs in the Graphormer, which serve as an inductive bias in the neural network to learn the graph representation. We further provide the detailed implementations of Graphormer. Finally, we show that our proposed Graphormer is more powerful since popular GNN models [26, 54, 18] are its special cases.3.1       Structural Encodings in GraphormerAs discussed in the introduction, it is important to develop ways to leverage the structural information of graphs into the Transformer model. To this end, we present three simple but effective designs of encoding in Graphormer. See Figure 1 for an illustration.3.1.1       Centrality EncodingIn Eq.4, the attention distribution is calculated based on the semantic correlation between nodes. However, node centrality, which measures how important a node is in the graph, is usually a strong signal for graph understanding. For example, celebrities who have a huge number of followers are important factors in predicting the trend of a social network [40, 39]. Such information is neglected in the current attention calculation, and we believe it should be a valuable signal for Transformer models.In Graphormer, we use the degree centrality, which is one of the standard centrality measures in literature, as an additional signal to the neural network. To be specific, we develop a  which assigns each node two real-valued embedding vectors according to its indegree and outdegree. As the centrality encoding is applied to each node, we simply add it to the node features as the input.where z −, z+ ∈ R d are learnable embedding vectors specified by the indegree deg−(vi) and outdegree deg+(vi) respectively. For undirected graphs, deg−(vi) and deg+(vi) could be unified to deg(vi). By using the centrality encoding in the input, the softmax attention can catch the node importance signal in the queries and the keys. Therefore the model can capture both the semantic correlation and the node importance in the attention mechanism.3.1.2       Spatial EncodingAn advantage of Transformer is its global receptive field. In each Transformer layer, each token can attend to the information at any position and then process its representation. But this operation has a byproduct problem that the model has to explicitly specify different positions or encode the positional dependency (such as locality) in the layers. For sequential data, one can either give each position an embedding (i.e., absolute positional encoding [49]) as the input or encode the relative distance of any two positions (i.e., relative positional encoding [45,47]) in the Transformer layer.However, for graphs, nodes are not arranged as a sequence. They can lie in a multi-dimensional spatial space and are linked by edges. To encode the structural information of a graph in the model, we propose a novel Spatial Encoding. Concretely, for any graph G, we consider a function φ (vi , vj ) : V × V → R which measures the spatial relation between vi and vj in graph G. The function φ can be defined by the connectivity between the nodes in the graph. In this paper, we choose φ(vi , vj ) to be the distance of the shortest path (SPD) between vi and vj if the two nodes are connected. If not, we set the output of φ to be a special value, i.e., -1. We assign each (feasible) output value a learnable scalar which will serve as a bias term in the self-attention module. Denote Aij as the (i, j)-element of the Query-Key product matrix A, we have:where ( ) is a learnable scalar indexed by (), and shared across all layers.Here we discuss several benefits of our proposed method. First, compared to conventional GNNs described in Section 2, where the receptive field is restricted to the neighbors, we can see that in Eq. (6), the Transformer layer provides a global information that each node can attend to all other nodes in the graph. Second, by using ( ), each node in a single Transformer layer can adaptively attend to all other nodes according to the graph structural information. For example, if ( ) islearned to be a decreasing function with respect to (), for each node, the model will likely pay more attention to the nodes near it and pay less attention to the nodes far away from it.3.1.3       Edge Encoding in the AttentionIn many graph tasks, edges also have structural features, e.g., in a molecular graph, atom pairs may have features describing the type of bond between them. Such features are important to the graph representation, and encoding them together with node features into the network is essential. There are mainly two edge encoding methods used in previous works. In the first method, the edge features are added to the associated nodes’ features [22, 30]. In the second method, for each node, its associated edges’ features will be used together with the node features in the aggregation [15, 54, 26]. However, such ways of using edge feature only propagate the edge information to its associated nodes, which may not be an effective way to leverage edge information in representation of the whole graph.To better encode edge features into attention layers, we propose a new edge encoding method in Graphormer. The attention mechanism needs to estimate correlations for each node pair (), and we believe the edges connecting them should be considered in the correlation as in [34, 51]. For each ordered node pair (), we find (one of) the shortest path SP = (1*, e, …, eN* ) from  to , and compute an average of the dot-products of the edge feature and a learnable embedding along the path. The proposed edge encoding incorporates edge features via a bias term to the attention module. Concretely, we modify the ()-element of  in Eq. (3) further with the edge encoding  as:where xen is the feature of the n-th edge en in SPij , w E n ∈ R dE is the n-th weight embedding, and dE is the dimensionality of edge feature.3.2       Implementation Details of Graphormer Graphormer is built upon the original implementation of classic Transformer encoder described in [49]. In addition, we apply the layer normalization (LN) before the multi-head self-attention (MHA) and the feed-forward blocks (FFN) instead of after [53]. This modification has been unanimously adopted by all current Transformer implementations because it leads to more effective optimization [43]. Especially, for FFN sub-layer, we set the dimensionality of input, output, and the inner-layer to the same dimension with . We formally characterize the Graphormer layer as below: As stated in the previous section, various graph pooling functions are proposed to represent the graph embedding. Inspired by [15], in Graphormer, we add a special node called [VNode] to the graph, and make connection between [VNode] and each node individually. In the AGGREGATE-COMBINE step, the representation of [VNode] has been updated as normal nodes in graph, and the representation of the entire graph  would be the node feature of [VNode] in the final layer. In the BERT model [11, 35], there is a similar token, i.e., [CLS], which is a special token attached at the beginning of each sequence, to represent the sequence-level feature on downstream tasks. While the [VNode] is connected to all other nodes in graph, which means the distance of the shortest path is 1 for any ([VNode]) and ( [VNode]), the connection is not physical. To distinguish the connection of physical and virtual, inspired by [25], we reset all spatial encodings for ([VNode] ) and ([VNode]) to a distinct learnable scalar.3.3       How Powerful is Graphormer?In the previous subsections, we introduce three structural encodings and the architecture of Graphormer. Then a natural question is: Do these modifications make Graphormer more powerful than other GNN variants? In this subsection, we first give an affirmative answer by showing that Graphormer can represent the AGGREGATE and COMBINE steps in popular GNN models:By choosing proper weights and distance function φ, the Graphormer layer can represent AGGREGATE and COMBINE steps of popular GNN models such as GIN, GCN, GraphSAGE.The proof sketch to derive this result is: 1) Spatial encoding enables self-attention module to distinguish neighbor set N (vi) of node vi so that the softmax function can calculate mean statistics over N (vi); 2) Knowing the degree of a node, mean over neighbors can be translated to sum over neighbors; 3) With multiple heads and FFN, representations of vi and N (vi) can be processed separately and combined together later. We defer the proof of this fact to Appendix A.Moreover, we show further that by using our spatial encoding, Graphormer can go beyond classic message passing GNNs whose expressive power is no more than the 1-Weisfeiler-Lehman (WL) test. We give a concrete example in Appendix A to show how Graphormer helps distinguish graphs that the 1-WL test fails to.Connection between Self-attention and Virtual Node. Besides the superior expressiveness than popular GNNs, we also find an interesting connection between using self-attention and the virtual node heuristic [15, 31, 24, 22]. As shown in the leaderboard of OGB [22], the virtual node trick, which augments graphs with additional supernodes that are connected to all nodes in the original graphs, can significantly improve the performance of existing GNNs. Conceptually, the benefit of the virtual node is that it can aggregate the information of the  (like the READOUT function) and then propagate it to . However, a naive addition of a supernode to a graph can potentially lead to inadvertent over-smoothing of information propagation [24]. We instead find that such a graph-level aggregation and propagation operation can be naturally fulfilled by vanilla self-attention without additional encodings. Concretely, we can prove the following fact:By choosing proper weights, every node representation of the output of a Graphormer layer without additional encodings can represent MEAN READOUT functions.This fact takes the advantage of self-attention that each node can attend to all other nodes. Thus it can simulate graph-level READOUT operation to aggregate information from the whole graph. Besides the theoretical justification, we empirically find that Graphormer does not encounter the problem of over-smoothing, which makes the improvement scalable. The fact also inspires us to introduce a special node for graph readout (see the previous subsection).We first conduct experiments on the recent OGB-LSC [21] quantum chemistry regression (i.e., PCQM4M-LSC) challenge, which is currently the biggest graph-level prediction dataset and contains more than 3.8M graphs in total. Then, we report the results on the other three popular tasks: ogbg-molhiv, ogbg-molpcba and ZINC, which come from the OGB [22] and benchmarking-GNN [14] leaderboards. Finally, we ablate the important design elements of Graphormer. A detailed description of datasets and training strategies could be found in Appendix B.4.1       OGB Large-Scale Challenge We benchmark the proposed Graphormer with GCN [26] and GIN [54], and their variants with virtual node (-VN) [15]. They achieve the state-of-the-art valid and test mean absolute error (MAE) on the official leaderboard4 [21]. In addition, we compare to GIN’s multi-hop variant [5], and 12-layer deep graph network DeeperGCN [30], which also show promising performance on other leaderboards. We further compare our Graphormer with the recent Transformer-based graph model GT [13].\
  We primarily report results on two model sizes:  ( = 12*, d* = 768), and a smaller one  ( = 6*, d* = 512). Both the number of attention heads in the attention module and the dimensionality of edge features  are set to 32. We use AdamW as the optimizer, and set the hyper-parameter  to 1e-8 and (1*, β*2) to (0.99,0.999). The peak learning rate is set to 2e-4 (3e-4 for ) with a 60k-step warm-up stage followed by a linear decay learning rate scheduler. The total training steps are 1M. The batch size is set to 1024. All models are trained on 8 NVIDIA V100 GPUS for about 2 days.\
 Table 1 summarizes performance comparisons on PCQM4M-LSC dataset. From the table, GIN-VN achieves the previous state-of-the-art validate MAE of 0.1395. The original implementation of GT [13] employs a hidden dimension of 64 to reduce the total number of parameters. For a fair comparison, we also report the result by enlarging the hidden dimension to 768, denoted by GT-Wide, which leads to a total number of parameters of 83.2M. While, both GT and GT-Wide do not outperform GIN-VN and DeeperGCN-VN. Especially, we do not observe a performance gain along with the growth of parameters of GT.Compared to the previous state-of-the-art GNN architecture, Graphormer noticeably surpasses GIN-VN by a large margin, e.g., 11.5% relative validate MAE decline. By using the ensemble with ExpC [55], we got a 0.1200 MAE on complete test set and won the first place of the graph-level track in OGB Large-Scale Challenge[21, 58]. As stated in Section 3.3, we further find that the proposed Graphormer does not encounter the problem of over-smoothing, i.e., the train and validate error keep going down along with the growth of depth and width of models.4.2       Graph RepresentationIn this section, we further investigate the performance of Graphormer on commonly used graph-level prediction tasks of popular leaderboards, i.e., OGB [22] (OGBG-MolPCBA, OGBG-MolHIV), and benchmarking-GNN [14] (ZINC). Since pre-training is encouraged by OGB, we mainly explore the transferable capability of a Graphormer model pre-trained on OGB-LSC (i.e., PCQM4M-LSC). Please note that the model configurations, hyper-parameters, and the pre-training performance of pre-trained Graphormers used for MolPCBA and MolHIV are different from the models used in the previous subsection. Please refer to Appendix B for detailed descriptions. For benchmarking-GNN, which does not encourage large pre-trained model, we train an additional GraphormerSLIM ( = 12*, d* = 80, total param.= 489) from scratch on ZINC. We report performance of GNNs which achieve top-performance on the official leader-boards5without additional domain-specific features. Considering that the pre-trained Graphormer leverages external data, for a fair comparison on OGB datasets, we additionally report performance for fine-tuning GIN-VN pre-trained on PCQM4M-LSC dataset, which achieves the previous state-of-the-art valid and test MAE on that dataset.\
 We report detailed training strategies in Appendix B. In addition, Graphormer is more easily trapped in the over-fitting problem due to the large size of the model and the small size of the dataset. Therefore, we employ a widely used data augmentation for graph - FLAG [27], to mitigate the over-fitting problem on OGB datasets.\
 Table 2,3 and 4 summarize performance of Graphormer comparing with other GNNs on MolHIV, MolPCBA and ZINC datasets. Especially, GT [13] and SAN [28] in Table 4 are recently proposed Transformer-based GNN models. Graphormer consistently and significantly outperforms previous state-of-the-art GNNs on all three datasets by a large margin. Specially, except Graphormer,  the other pre-trained GNNs do not achieve competitive performance, which is in line with previous literature [20]. In addition, we conduct more comparisons to fine-tuning the pre-trained GNNs, please refer to Appendix C.4.3       Ablation StudiesWe perform a series of ablation studies on the importance of designs in our proposed Graphormer, on PCQM4M-LSC dataset. The ablation results are included in Table 5. To save the computation resources, the Transformer models in table 5 have 12 layers, and are trained for 100K iterations.\
 We compare previously used positional encoding (PE) to our proposed spatial encoding, which both aim to encode the information of distinct node relation to Transformers. There are various PEs employed by previous Transformer-based GNNs, e.g., Weisfeiler-Lehman-PE (WL-PE) [61] and Laplacian PE [3, 14]. We report the performance for Laplacian PE since it performs well comparing to a series of PEs for Graph Transformer in previous literature [13]. Transformer architecture with the spatial encoding outperforms the counterpart built on the positional encoding, which demonstrates the effectiveness of using spatial encoding to capture the node spatial information.\
 Transformer architecture with degree-based centrality encoding yields a large margin performance boost in comparison to those without centrality information. This indicates that the centrality encoding is indispensable to Transformer architecture for modeling graph data.\
 We compare our proposed edge encoding (denoted as via attn bias) to two commonly used edge encodings described in Section 3.1.3 to incorporate edge features into GNN, denoted as via node and via Aggr in Table 5. From the table, the gap of performance is minor between the two conventional methods, but our proposed edge encoding performs significantly better, which indicates that edge encoding as attention bias is more effective for Transformer to capture spatial information on edges.In this section, we highlight the most recent works which attempt to develop standard Transformer architecture-based GNN or graph structural encoding, but spend less effort on elaborating the works by adapting attention mechanism to GNNs [33,60,7,23,1,50,51,61,48].There are several works that study the performance of pure Transformer architectures (stacked by transformer layers) with modifications on graph representation tasks, which are more related to our Graphormer. For example, several parts of the transformer layer are modified in [46], including an additional GNN employed in attention sub-layer to produce vectors of , , and  , long-range residual connection, and two branches of FFN to produce node and edge representations separately. They pre-train their model on 10 million unlabelled molecules and achieve excellent results by fine-tuning on downstream tasks. Attention module is modified to a soft adjacency matrix in [41] by directly adding the adjacency matrix and RDKit6-computed inter-atomic distance matrix to the attention probabilites. Very recently, Dwivedi  [13] revisit a series of works for Transformer-based GNNs, and suggest that the attention mechanism in Transformers on graph data should only aggregate the information from neighborhood (i.e., using adjacent matrix as attention mask) to ensure graph sparsity, and propose to use Laplacian eigenvector as positional encoding. Their model GT surpasses baseline GNNs on graph representation task. A concurrent work [28] propose a novel full Laplacian spectrum to learn the position of each node in a graph, and empirically shows better results than GT.5.2       Structural Encodings in GNNsPath and Distance in GNNs. Information of path and distance is commonly used in GNNs. For example, an attention-based aggregation is proposed in [9] where the node features, edge features, one-hot feature of the distance and ring flag feature are concatenated to calculate the attention probabilites; similar to [9], path-based attention is leveraged in [56] to model the influence between the center node and its higher-order neighbors; a distance-weighted aggregation scheme on graph is proposed in [59]; it has been proved in [32] that adopting distance encoding (i.e., one-hot feature of the distance as extra node attribute) could lead to a strictly more expressive power than the 1-WL test.\
Positional Encoding in Transformer on Graph. Several works introduce positional encoding (PE) to Transformer-based GNNs to help the model capture the node position information. For example, Graph-BERT [61] introduces three types of PE to embed the node position information to model, i.e., an absolute WL-PE which represents different nodes labeled by Weisfeiler-Lehman algorithm, an intimacy based PE and a hop based PE which are both variant to the sampled subgraphs. Absolute Laplacian PE is employed in [13] and empircal study shows that its performance surpasses the absolute WL-PE used in [61].\
 Except the conventionally used methods to encode edge feature, which are described in previous section, there are several attempts that exploit how to better encode edge features: an attention-based GNN layer is developed in [16] to encode edge features, where the edge feature is weighted by the similarity of the features of its two nodes; edge feature has been encoded into the popular GIN [54] in [5]; in [13], the authors propose to project edge features to an embedding vector, then multiply it by attention coefficients, and send the result to an additional FFN sub-layer to produce edge representations;We have explored the direct application of Transformers to graph representation. With three novel graph structural encodings, the proposed Graphormer works surprisingly well on a wide range of popular benchmark datasets. While these initial results are encouraging, many challenges remain. For example, the quadratic complexity of the self-attention module restricts Graphormer’s application on large graphs. Therefore, future development of efficient Graphormer is necessary. Performance improvement could be expected by leveraging domain knowledge-powered encodings on particular graph datasets. Finally, an applicable graph sampling strategy is desired for node representation extraction with Graphormer. We leave them for future works.We would like to thank Mingqi Yang and Shanda Li for insightful discussions.[1]    Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with graph multiset pooling. , 2021.[2]    Dominique Beaini, Saro Passaro, Vincent Létourneau, William L Hamilton, Gabriele Corso, and Pietro Liò. Directional graph networks. In International Conference on Machine Learning, 2021.[3]    Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representa-tion. , 15(6):1373–1396, 2003.[4]    Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017.[5]    Rémy Brossard, Oriel Frigo, and David Dehaene. Graph convolutions that can finally model local structure.arXiv preprint arXiv:2011.15069, 2020.[6]    Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.[7]    Deng Cai and Wai Lam. Graph transformer for graph-to-sequence learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7464–7471, 2020.[8]    Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021.[9]    Benson Chen, Regina Barzilay, and Tommi Jaakkola. Path-augmented graph transformer network. arXiv preprint arXiv:1905.12712, 2019.[10]    Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Velicˇkovic´. Principal neighbour-hood aggregation for graph nets. Advances in Neural Information Processing Systems, 33, 2020.[11]    Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi-rectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.[12]    Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.[13]    Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. AAAI Workshop on Deep Learning on Graphs: Methods and Applications, 2021.[14]    Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Bench-marking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.[15]    Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263–1272. PMLR, 2017.[16]    Liyu Gong and Qiang Cheng. Exploiting edge features for graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9211–9219, 2019.[17]    Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020.[18]    William L Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In , 2017.[19]    Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global relational models of source code. In International conference on learning representations, 2019.[20]    W Hu, B Liu, J Gomes, M Zitnik, P Liang, V Pande, and J Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations (ICLR), 2020.[21]    Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.[22]    Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.[23]    Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In Proceedings of The Web Conference 2020, pages 2704–2710, 2020.[24]    Katsuhiko Ishiguro, Shin-ichi Maeda, and Masanori Koyama. Graph warp module: an auxiliary module for boosting the power of graph neural networks in molecular graph analysis. arXiv preprint arXiv:1902.01020, 2019.[25]    Guolin Ke, Di He, and Tie-Yan Liu. Rethinking the positional encoding in language pre-training. , 2020.[26]    Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.arXiv preprint arXiv:1609.02907, 2016.[27]    Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. Flag: Adversarial data augmentation for graph neural networks. arXiv preprint arXiv:2010.09891, 2020.[28]    Devin Kreuzer, Dominique Beaini, William Hamilton, Vincent Létourneau, and Prudencio Tossou. Re-thinking graph transformers with spectral attention. arXiv preprint arXiv:2106.03893, 2021.[29]    Tuan Le, Marco Bertolini, Frank Noé, and Djork-Arné Clevert. Parameterized hypercomplex graph neural networks for graph classification. arXiv preprint arXiv:2103.16584, 2021.[30]    Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns. arXiv preprint arXiv:2006.07739, 2020.[31]    Junying Li, Deng Cai, and Xiaofei He. Learning graph-level representation for drug discovery. arXiv preprint arXiv:1709.03741, 2017.[32]    Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. Advances in Neural Information Processing Systems, 33, 2020.[33]    Yuan Li, Xiaodan Liang, Zhiting Hu, Yinbo Chen, and Eric P. Xing. Graph transformer, 2019.[34]    Xi Victoria Lin, Richard Socher, and Caiming Xiong. Multi-hop knowledge graph reasoning with reward shaping. arXiv preprint arXiv:1808.10568, 2018.[35]    Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.[36]    Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.[37]    Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. , 2021.[38]    Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.[39]    P David Marshall. The promotion and presentation of the self: celebrity as marker of presentational media., 1(1):35–48, 2010.[40]    Alice Marwick and Danah Boyd. To see and be seen: Celebrity practice on twitter. , 17(2):139–158, 2011.[41]    Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrze˛bski. Molecule attention transformer. arXiv preprint arXiv:2002.08264, 2020.[42]    Maho Nakata and Tomomi Shimazaki. Pubchemqc project: a large-scale first-principles electronic structure database for data-driven chemistry. Journal of chemical information and modeling, 57(6):1300–1308, 2017.[43]    Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021.[44]    Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. How could neural networks understand programs? In International Conference on Machine Learning. PMLR, 2021.[45]    Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.[46]    Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems, 33, 2020.[47]    Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464–468, 2018.[48]    Yunsheng Shi, Zhengjie Huang, Wenjin Wang, Hui Zhong, Shikun Feng, and Yu Sun. Masked label predic-tion: Unified message passing model for semi-supervised classification. arXiv preprint arXiv:2009.03509, 2020.[49]    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In , 2017.[50]    Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. , 2018.[51]    Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Direct multi-hop attention based graph neural network. arXiv preprint arXiv:2009.14332, 2020.[52]    Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.[53]    Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524–10533. PMLR, 2020.[54]    Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.[55]    Mingqi Yang, Yanming Shen, Heng Qi, and Baocai Yin. Breaking the expressive bottlenecks of graph neural networks. arXiv preprint arXiv:2012.07219, 2020.[56]    Yiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, and Dacheng Tao. Spagan: Shortest path graph attention network. , 2019.[57]    Chengxuan Ying, Guolin Ke, Di He, and Tie-Yan Liu. Lazyformer: Self attention with lazy update. arXiv preprint arXiv:2102.12702, 2021.[58]    Chengxuan Ying, Mingqi Yang, Shuxin Zheng, Guolin Ke, Shengjie Luo, Tianle Cai, Chenglin Wu, Yuxin Wang, Yanming Shen, and Di He. First place solution of kdd cup 2021 & ogb large-scale challenge graph-level track. arXiv preprint arXiv:2106.08279, 2021.[59]    Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In International Conference on Machine Learning, pages 7134–7143. PMLR, 2019.[60]    Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. Advances in Neural Information Processing Systems, 32, 2019.[61]    Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-bert: Only attention is needed for learning graph representations. arXiv preprint arXiv:2001.05140, 2020.[62]    Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. In , 2020.[63]    Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann. Language-agnostic representation learning of source code from structure and context. In International Conference on Learning Representations, 2020.A.1       SPD can Be Used to Improve WL-Test\
1-WL-test fails in many cases [38, 32], thus classic message passing GNNs also fail to distinguish many pairs of graphs. We show that SPD might help when 1-WL-test fails, for example, in Figure 2 where 1-WL-test fails, the sets of SPD from all nodes to others successfully distinguish the two graphs. We begin by showing that self-attention module with Spatial Encoding can repre-sent MEAN aggregation. This is achieved by in Eq. (6): 1) setting  = 0 if  = 1 and  =  otherwise where  is the SPD; 2) setting  =  = 0 and  to be the identity matrix. Then softmax ()  gives the average of representations of the neighbors.\
 The SUM aggregation can be realized by first perform MEAN aggregation and then multiply the node degrees. Specifically, the node degrees can be extracted from Centrality Encoding by an additional head and be concatenated to the representations after MEAN aggregation. Then the FFN module in Graphormer can represent the function of multiplying the degree to the dimensions of averaged representations by the universal approximation theorem of FFN.\
 Representing the MAX aggregation is harder than MEAN and SUM. For each dimension  of the representation vector, we need one head to select the maximal value over -th dimension in the neighbor by in Eq. (6): 1) setting  = 0 if  = 1 and  =  otherwise where  is the SPD; 2) setting  =  which is the -th standard basis;  = 0 and the bias term (which is ignored in the previous description for simplicity) of  to be ; and  = , where  is the temperature that can be chosen to be large enough so that the softmax function can approximate hard max and  is the vector whose elements are all 1.\
 The COMBINE step takes the result of AGGREGATE and the previous representation of current node as input. This can be achieved by the AGGREGATE operations described above together with an additional head which outputs the features of present nodes, i.e., in Eq. (6): 1) setting  = 0 if  = 0 and  =  otherwise where  is the SPD; 2) setting  =  = 0 and  to be the identity matrix. Then the FFN module can approximate any COMBINE function by the universal approximation theorem of FFN. This can be proved by setting  =  = 0, the bias terms of  to be , and  to be the identity matrix where  should be much larger than the scale of  so that  2T dominates the Spatial Encoding term.B.1       Details of DatasetsWe summarize the datasets used in this work in Table 6. PCQM4m-LSC is a quantum chemistry graph-level prediction task in recent OGB Large-Scale Challenge, originally curated under the PubChemQC project [42].\
The task of PCQM4M-LSC is to predict DFT(density functional theory)-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs, which is one of the most practically-relevant quantum chemical properties of molecule science. PCQM4M-LSC is unprecedentedly large in scale comparing to other labeled graph-level prediction datasets, which contains more than 3.8M graphs. Besides, we conduct experiments on two molecular graph datasets in popular OGB leaderboards, i.e., OGBG-MolPCBA and OGBG-MolHIV. They are two molecular property prediction datasets with different sizes. The pre-trained knowledge of molecular graph on PCQM4M-LSC could be easily leveraged on these two datasets. We adopt official scaffold split on three datasets following [21, 22]. In addition, we employ another popular leaderboard, i.e., benchmarking-gnn [14]. We use the ZINC datasets, which is the most popular real-world molecular dataset to predict graph property regression for contrained solubility, an important chemical property for designing generative GNNs for molecules. Different from the scaffold spliting in OGB, uniform sampling is adopted in ZINC for data splitting.B.2       Details of Training Strategies\
We report the detailed hyper-parameter settings used for training Graphormer in Table 7. We reduce the FFN inner-layer dimension of 4 in [49] to , which does not appreciably hurt the performance but significantly save the parameters. The embedding dropout ratio is set to 0.1 by default in many previous Transformer works [11, 35]. However, we empirically find that a small embedding dropout ratio (e.g., 0.1) would lead to an observable performance drop on validation set of PCQM4M-LSC. One possible reason is that the molecular graph is relative small (i.e., the median of #atoms in each molecule is about 15), making graph property more sensitive to the embeddings of each node. Therefore, we set embedding dropout ratio to 0 on this dataset. We first report the model configurations and hyper-parameters of the pre-trained Graphormer on PCQM4M-LSC. Empirically, we find that the performance on MolPCBA benefits from the large pre-training model size. Therefore, we train a deep Graphormer with 18 Transformer layers on PCQM4M-LSC. The hidden dimension and FFN inner-layer dimension are set to 1024. We set peak learning rate to 1e-4 for the deepGraphormer. Besides, we enlarge the attention dropout ratio from 0.1 to 0.3 in both pre-training and fine-tuning to prevent the model from over-fitting. The rest of hyper-parameters remain unchanged. The pre-trained Graphormer used for MolPCBA achieves a valid MAE of 0.1253 on PCQM4M-LSC, which is slightly worse than the reports in Table 1.\
 Table 8 summarizes the hyper-parameters used for fine-tuning Graphormer on OGBG-MolPCBA. We conduct a grid search for several hyper-parameters to find the optimal configuration. The experimental results are reported by the mean of 10 independent runs with random seeds. We use FLAG [27] with minor modifications for graph data augmentation. In particular, except the step size  and the number of steps , we also employ a projection step in [62] with maximum perturbation . The performance of Graphormer on MolPCBA is quite robust to the hyper-parameters of FLAG. The rest of hyper-parameters are the same with the pre-training model.\
 We use the Graphormer reported in Table 1 as the pre-trained model for OGBG-MolHIV, where the pre-training hyper-parameters are summarized in Table 7.\
  The hyper-parameters for fine-tuning Graphormer on OGBG-MolHIV are presented in Table 9. Empirically, we find that the different choices of hyper-parameters of FLAG (i.e., step size , number of steps , and maximum perturbation ) would greatly affect the performance of Graphormer on OGBG-MolHiv. Therefore, we spend more effort to conduct grid search for hyper-parameters of FLAG. We report the best hyper-parameters by the mean of 10 independent runs with random seeds.To keep the total parameters of Graphormer less than 500K per the request from benchmarking-GNN leader-board [14], we train a slim 12-layer Graphormer with hidden dimension of 80, which is called GraphormerSLIM in Table 4, and has about 489K learnable parameters. The number of attention heads is set to 8. Table 10 summarizes the detailed hyper-parameters on ZINC. We train 400K steps on this dataset, and employ a weight decay of 0.01.B.3       Details of Hyper-parameters for Baseline MethodsIn this section, we present the details of our re-implementation of the baseline methods.The official Github repository of OGB-LSC7 provides hyper-parameters and codes to reproduce the results on leaderboard. These hyper-parameters work well on almost all popular GNN variants, except the DeeperGCN-VN, which results in a training divergence. Therefore, for DeeperGCN-VN, we follow the official hyper-parameter setting8 provided by the authors [30]. For a fair comparison to Graphormer, we train a 12-layer DeeperGCN. The hidden dimension is set to 600. The batch size is set to 256. The learning rate is set to 1e-3, and a step learning rate scheduler is employed with the decaying step size and the decaying factor  as 30 epochs and 0.25. The model is trained for 100 epochs.The default dimension of laplacian PE of GT [13] is set to 8. However, it will cause 2.91% small molecules (less than 8 atoms) to be filtered out. Therefore, for GT and GT-Wide, we set the dimension of laplacian PE to 4, which results in only 0.08% filtering out. We adopt the default hyper-parameter settings described in [13], except that we decrease the learning rate to 1e-4, which leads to a better convergence on PCQM4M-LSC.To fine-tune the pre-trained GIN-VN on MolPCBA, we follow the hyper-parameter settings provided in the original OGB paper [22]. To be more concrete, we load the pre-trained checkpoint reported in Table 1 and fine-tune it on OGBG-MolPCBA dataset. We use the grid search on the hyper-parameters for better fine-tuning performance. In particular, the learning rate is selected from {1e − 5, 1e − 4, 1e − 3}; the dropout ratio is selected from {0.0, 0.1, 0.5}; the batch size is selected from {32, 64}.Similarly, we fine-tune the pre-trained GIN-VN on MolHIV by following the hyper-parameter settings provided in the original OGB paper [22]. We also conduct the grid search to look for optimal hyper-parameters. The ranges for each hyper-parameter of grid search are the same as the previous subsection.C        More ExperimentsAs described in the related work, GROVER is a Transformer-based GNN, which has 100 million parameters and pre-trained on 10 million unlabelled molecules using 250 Nvidia V100 GPUs. In this section, we report the fine-tuning scores of GROVER on MolHIV and MolPCBA, and compare with proposed Graphormer.We download the pre-trained GROVER models from its official Github webpage9, follow the official instruc-tions10 and fine-tune the provided pre-trained checkpoints with careful search of hyper-parameters (in Table 11). We find that GROVER could achieve competitive performance on MolHIV only if employing additional molecular features, i.e., morgan molecular finger prints and 2D features11. Therefore, we report the scores of GROVER by taking these two additional molecular features. Please note that, from the leaderboard12, we can know such additional molecular features are very effective on MolHIV dataset.Table 12 and 13 summarize the performance of GROVER and GROVERLARGE comparing with Graphormer on MolHIV and MolPCBA. From the tables, we observe that Graphormer could consistently outperform GROVER even without any additional molecular features.D        Discussion & Future Work Similar to regular Transformer, the attention mechanism in Graphormer scales quadratically with the number of nodes  in the input graph, which may be prohibitively expensive for large  and precludes its usage in settings with limited computational resources. Recently, many solutions have been proposed to address this problem in Transformer [25, 52, 57, 37]. This issue would be greatly benefit from the future development of efficient Graphormer.\
. In Graphormer, there are multiple choices for the network centrality and the spatial encoding function ( ). For example, one can leverage the 2 distance in 3D structure between two atoms in a molecule. In this paper, we mainly evaluate general centrality and distance metric in graph theory, i.e., the degree centrality and the shortest path. Performance improvement could be expected by leveraging domain knowledge powered encodings on particular graph dataset.\
 There is a wide range of node representation tasks on graph structured data, such as finance, social network, and temporal prediction. Graphormer could be naturally used for node representation extraction with an applicable graph sampling strategy. We leave it for future work.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Servo Browser Engine Starts 2026 With Many Notable Improvements</title><link>https://www.phoronix.com/news/Servo-January-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 14:21:02 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Servo project has issued their January 2026 development report that highlights all the interesting changes they made to this open-source browser layout engine last month. With Servo 0.0.5 they have landed many improvements to this engine and also continuing to enhance its ability to embed Servo inside other applications...]]></content:encoded></item><item><title>Top 3 Crypto Presales to Buy in 2026: Pepeto vs BlockDAG vs Mutuum Finance Before the Next Bull Run</title><link>https://hackernoon.com/top-3-crypto-presales-to-buy-in-2026-pepeto-vs-blockdag-vs-mutuum-finance-before-the-next-bull-run?source=rss</link><author>Tokenwire</author><category>tech</category><pubDate>Sat, 28 Feb 2026 14:06:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Every crypto bull run created a new wave of millionaires. But those millionaires did not buy after the rally started. They bought during the fear. They bought presales at fractions of a cent while everyone else waited for confirmation. By the time confirmation arrived, the 50x windows had already closed.A presale is when a project sells tokens before they list on exchanges. The price is locked far below what it trades for on launch day. Early SHIB investors turned $1,000 into over $1 million. PEPE made early holders rich in weeks. The pattern repeats every cycle. Three projects stand out right now. Pepeto, BlockDAG, and Mutuum Finance each offer presale access. But the upside between them is not close.Pepeto: The One That Goes ViralMost presales sell you a concept. Pepeto is selling you the infrastructure for the entire meme coin economy. That changes everything. Instead of betting on one token to pump, you are buying the trading layer that profits no matter which meme coin takes off next.PepetoSwap is a zero tax cross chain swap announced by the team and close to being ready. The Pepeto Bridge moves tokens between blockchains. The Pepeto Exchange is a meme coin listing hub approaching launch. A cofounder of the original Pepe token leads the build. Dual audits from SolidProof and Coinsult confirmed zero critical findings. The presale has already raised $7.33 million with 70% of supply filled, as reported by .At $0.000000186, the math is simple. A 50x rally turns $1,000 into $50,000. A 100x turns it into $100,000. That is not speculation. Shiba Inu hit a $40 billion market cap with zero products. PEPE reached $7 billion on memes alone. Pepeto has three products approaching launch and a Binance listing on the horizon. Staking at 211% APY means a $3,000 hold generates $6,330 in yearly rewards while you wait. But staking is the bonus. The real play is the multiple. Community growth is accelerating and social channels are exploding. This is the presale that goes viral.BlockDAG: Strong Concept, Different TimelineBlockDAG combines Proof of Work security with a DAG structure that processes transactions in parallel. The presale has raised over $450 million, one of the largest totals this cycle. That gives the team serious funding for development.But with $450 million already raised, much of the early upside may be priced in. Investors are now looking at 2x to 3x returns rather than exponential multiples. For steady growth from a large infrastructure play, BlockDAG makes sense. But it is a different bet than a presale at six zeros.Mutuum Finance: DeFi Lending With UtilityMutuum Finance focuses on decentralized lending and borrowing. Users supply assets to earn yield or borrow against holdings. The model mirrors established protocols but targets a newer audience.The concept is solid with proven demand. But the returns profile looks more like a 2x to 5x play based on current valuations. Why target a 3x return when a presale at six zeros offers 50x with logic and 100x with momentum?Why Presales Create the Biggest ReturnsExchange listed tokens already have price discovery behind them. You buy after millions set the floor. Presales flip that. You buy before the crowd, before the listing, before social media drives the second wave. Every cycle, the biggest winners come from presale entries, as reported by .BlockDAG and Mutuum Finance both have real utility. But utility alone does not create 50x returns. Pepeto combines meme coin virality with real infrastructure, a Pepe cofounder, dual audits, and a Binance listing approaching. The presale is 70% filled. SHIB created millionaires with zero products. DOGE created millionaires with a joke. Pepeto has three products and the viral energy to match. At $0.000000186, this is the presale window that closes permanently once listing day arrives.What is a crypto presale and why does it matter?A presale lets you buy tokens before they list on exchanges. Prices are locked at early stage levels, which is how early SHIB and PEPE investors turned small amounts into life changing wealth.Is Pepeto better than BlockDAG for 2026 returns?BlockDAG raised $450 million, so much of the upside may be priced in. Pepeto at $0.000000186 offers 50x to 100x potential because it combines meme virality with real infrastructure at six zeros.Can you still make money from crypto presales?Every bull run creates new presale millionaires. The key is entering before exchange listings and choosing projects with both utility and viral community momentum.:::warning
This article is for informational purposes only and does not constitute investment advice. Cryptocurrencies are speculative, complex, and involve high risks. This can mean high prices volatility and potential loss of your initial investment. You should consider your financial situation, investment purposes, and consult with a financial advisor before making any investment decisions. The HackerNoon editorial team has only verified the story for grammatical accuracy and does not endorse or guarantee the accuracy, reliability, or completeness of the information stated in this article. #DYOR]]></content:encoded></item><item><title>This Power Grid Pioneer’s EV Prediction Came 100 Years Too Soon</title><link>https://spectrum.ieee.org/charles-proteus-steinmetz</link><author>Allison Marsh</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NTAwNTE2My9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgxNzYwMTY1MH0.gQFh0swv0bV--uDHDQUpGn4OsJf_1LmAfnGnOMsfrPI/image.jpg?width=600" length="" type=""/><pubDate>Sat, 28 Feb 2026 14:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Charles Proteus Steinmetz envisioned 1 million EVs on U.S. roads by 1924]]></content:encoded></item><item><title>Google Quantum-Proofs HTTPS</title><link>https://tech.slashdot.org/story/26/02/28/027202/google-quantum-proofs-https?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: Google on Friday unveiled its plan for its Chrome browser to secure HTTPS certificates against quantum computer attacks without breaking the Internet. The objective is a tall order. The quantum-resistant cryptographic data needed to transparently publish TLS certificates is roughly 40 times bigger than the classical cryptographic material used today. Today's X.509 certificates are about 64 bytes in size, and comprise six elliptic curve signatures and two EC public keys. This material can be cracked through the quantum-enabled Shor's algorithm. Certificates containing the equivalent quantum-resistant cryptographic material are roughly 2.5 kilobytes. All this data must be transmitted when a browser connects to a site.
 
To bypass the bottleneck, companies are turning to Merkle Trees, a data structure that uses cryptographic hashes and other math to verify the contents of large amounts of information using a small fraction of material used in more traditional verification processes in public key infrastructure. Merkle Tree Certificates, "replace the heavy, serialized chain of signatures found in traditional PKI with compact Merkle Tree proofs," members of Google's Chrome Secure Web and Networking Team wrote Friday. "In this model, a Certification Authority (CA) signs a single 'Tree Head' representing potentially millions of certificates, and the 'certificate' sent to the browser is merely a lightweight proof of inclusion in that tree."
 
[...] Google is [also] adding cryptographic material from quantum-resistant algorithms such as ML-DSA (PDF). This addition would allow forgeries only if an attacker were to break both classical and post-quantum encryption. The new regime is part of what Google is calling the quantum-resistant root store, which will complement the Chrome Root Store the company formed in 2022. The [Merkle Tree Certificates] MTCs use Merkle Trees to provide quantum-resistant assurances that a certificate has been published without having to add most of the lengthy keys and hashes. Using other techniques to reduce the data sizes, the MTCs will be roughly the same 64-byte length they are now [...]. The new system has already been implemented in Chrome.]]></content:encoded></item><item><title>FreeBSD 14.4-RC1 Adds Emacs, Vim &amp; More To DVD Images</title><link>https://www.phoronix.com/news/FreeBSD-14.4-RC1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 11:33:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those on the current FreeBSD 14 series with no immediate plans to move to FreeBSD 15 that debuted at the end of 2025, FreeBSD developers have been preparing for the release of FreeBSD 14.4. Released overnight was the first release candidate of FreeBSD 14.4...]]></content:encoded></item><item><title>KDE Plasma 6.7 Preps Rounded Style UI Enhancement For QtWidgets-Based Apps</title><link>https://www.phoronix.com/news/Plasma-6.7-Rounded-UI-QtWidgets</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 11:21:36 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE Plasma 6.7 development continues heating up following the Plasma 6.6 desktop release earlier this month...]]></content:encoded></item><item><title>Rubin Observatory Has Started Paging Astronomers 800,000 Times a Night</title><link>https://science.slashdot.org/story/26/02/28/0155200/rubin-observatory-has-started-paging-astronomers-800000-times-a-night?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[On February 24th, the Vera C. Rubin Observatory activated its automated alert system, sending out roughly 800,000 real-time notifications flagging asteroids, supernovae, flaring black holes and "other transient celestial events," reports Scientific American. And this is only the beginning -- that number is projected to climb into the millions as it continues scanning the ever-changing sky. From the report: The astronomical observatory equipped with world's largest camera hit a key milestone on February 24, when a complex data-processing system pushed hundreds of thousands of alerts out to scientists eager to pore over its most exciting sightings. The Vera C. Rubin Observatory began operations last year, capturing stunning, panoramic time-lapse views of the cosmos with ease. Rubin's first images, based on just 10 hours of observations, let space fans zoom seemingly forever into an overwhelmingly starry sky. But watchful astronomers were always awaiting the next step: the system that would automatically alert them to the most promising activity in the overhead sky amid the 1,000 or so enormous images that Rubin's telescope captures every night.
 
"We can detect everything that changes, moves and appears," said Yusra AlSayyad, an astronomer at Princeton University and Rubin's deputy associate director for data management, to Scientific American last summer. "It's way too much for one person to manually sift through and filter and monitor themselves." So even as they were designing and building the Rubin Observatory itself, scientists were also designing an alert system to help astronomers navigate the flood of data. As soon as the telescope began observations, the team started constructing a static reference image of the entire sky in impeccable detail.
 
Now the data processing systems that support the observatory are starting to automatically compare every new Rubin image to the corresponding section of that background template. The systems identify all of the differences, each of which is individually flagged. The algorithms can also distinguish between a potential supernova and a possible newfound asteroid, for example. Alerting the scientific community is the final, crucial step. Astronomers -- as well as members of the public -- can sign up for notifications based on the type of sighting they're interested in and the brightness of the observation in question. And now that the alerts system has gone live, users receive a tiny, fuzzy image with some astronomical metadata of each observation that fits their criteria -- all just a couple of minutes after Rubin captures the original image.]]></content:encoded></item><item><title>The TechBeat: Beyond the Bots: What Real Writing Looks Like in the Age of AI (2/28/2026)</title><link>https://hackernoon.com/2-28-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sat, 28 Feb 2026 07:11:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @davidiyanu [ 5 Min read ] 
 RAG fails less from the LLM and more from retrieval: bad chunking, weak metadata, embedding drift, and stale indexes. Fix the pipeline first. Read More.By @stevebeyatte [ 7 Min read ] 
 Compare the 7 best co-parenting apps in 2026, including BestInterest, OurFamilyWizard, and TalkingParents. Find the right app for high-conflict situations.  Read More.By @playerzero [ 15 Min read ] 
 Modern software teams ship faster than ever, but defect resolution lags; PlayerZero aligns people, process, and context for predictable reliability. Read More.By @ipinfo [ 8 Min read ] 
 Analysis of 170M residential proxy IPs reveals rapid rotation and 46% cross-provider overlap—breaking traditional fraud detection models. Read More.By @thomascherickal [ 14 Min read ] 
 OpenClaw lets you run frontier AI models like Minimax M2.5 and GLM-5 100% locally on Mac M3 or DGX Spark — zero API costs, total privacy. Here's how.  Read More.By @aimodels44 [ 8 Min read ] 
 A new study suggests AGENTS.md-style repo context files can reduce coding-agent success while raising inference cost. Here’s why—and what to do instead. Read More.By @davidiyanu [ 8 Min read ] 
  Production is the unmarked minefield that begins the moment you accept arbitrary user input and promise reliability. Read More.By @dataops [ 3 Min read ] 
 Technical debt isn’t refactoring—it’s hidden risk. A powerful racecar analogy to help engineers explain why cutting corners can end in disaster. Read More.By @birukum [ 11 Min read ] 
 Agentic AI workflows can create a financial black hole. Learn how semantic caching uses vector similarity to cut your LLM token burn by 24%. Read More.By @sherveen [ 5 Min read ] 
 Deep dive analysis of Grok 4.2 and Sonnet 4.6, two new AI releases from xAI and Anthropic, and how their agent systems compare. Read More.By @samiranmondal [ 2 Min read ] 
 Cybersecurity stocks fell after AI company Anthropic unveiled Claude Code Security Read More.By @MichaelJerlis [ 2 Min read ] 
 Explore crypto staking options in 2026, compare ETH and SOL yields, and see how platforms like EMCD simplify earning passive income. Read More.By @omotayojude [ 3 Min read ] 
 When an AI agent's PR was rejected by Matplotlib, it didn't just close the tab it wrote an angry hit piece on the maintainer. Is this the future of open source? Read More.By @hackernoon-courses [ 4 Min read ] 
 Learn how to write content that stands out in the age of AI, crafting a voice and style no model or copycat can replicate. Read More.By @ArunDHANARAJ_gfaknebg [ 14 Min read ] 
 Compare Claude Opus 4.6 and GPT‑5.3 Codex across reasoning, coding, benchmarks, pricing, and safety to guide enterprise AI and agentic workload decisions.By @nickzt [ 5 Min read ] 
 Scaling AI for the real world requires peeling back the layers of abstraction we've gotten too comfortable with. Read More.By @brightdata [ 8 Min read ] 
 ​​We benchmark SERP APIs for success rate,
​​speed, and stability under load. Learn which setup delivers consistent results for AI agents ​​and deep research.  Read More.]]></content:encoded></item><item><title>Southern California Air Board Rejects Pollution Rules After AI-Generated Flood of Comments</title><link>https://it.slashdot.org/story/26/02/27/2348254/southern-california-air-board-rejects-pollution-rules-after-ai-generated-flood-of-comments?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Southern California's air quality board rejected proposed rules to phase out gas-powered appliances after receiving more than 20,000 opposition comments generated through CiviClick, "the first and best AI-powered grassroots advocacy platform." Phys.org reports: A Southern California-based public affairs consultant, Matt Klink, has taken credit for using CiviClick to wage the opposition campaign, including in a sponsored article on the website Campaigns and Elections. The campaign "left the staff of the Southern California Air Quality Management District (SCAQMD) reeling," the article says. It is not clear how AI was deployed in the campaign, and officials at CiviClick did not respond to repeated requests for comment. But their website boasts several tools, including "state of the art technology and artificial intelligence message assistance" that can be used to create custom advocacy letters, as opposed to repetitive form letters or petitions often used in similar campaigns.
 
When staffers at the air district reached out to a small sample of people to verify their comments, at least three said they had not written to the agency and were not aware of any such messages, records show. But the email onslaught almost certainly influenced the board's June decision, according to agency insiders, who noted that the number of public comments typically submitted on agenda items can be counted on one hand.
 
The proposed rules were nearly two years in the making and would have placed a fee on natural gas-powered water heaters and furnaces, favoring electric ones, in an effort to reduce air pollution in the district, which includes Orange County and large swaths of Los Angeles, Riverside and San Bernardino counties. Gas appliances emit nitrogen oxides, or NOx -- key pollutants for forming smog. The implications are troubling, experts said, and go beyond the use of natural gas furnaces and heaters in the second-largest metropolitan area in the country.]]></content:encoded></item><item><title>India disrupts access to popular developer platform Supabase with blocking order</title><link>https://techcrunch.com/2026/02/27/india-disrupts-access-to-popular-developer-platform-supabase-with-blocking-order/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Sat, 28 Feb 2026 03:51:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[India, one of Supabase’s biggest markets, is seeing patchy access after a government block order.]]></content:encoded></item><item><title>Trump Settles With Isaac Hayes’ Estate Over Use Of Music During The Campaign</title><link>https://www.techdirt.com/2026/02/27/trump-settles-with-isaac-hayes-estate-over-use-of-music-during-the-campaign/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Sat, 28 Feb 2026 03:39:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[One of the unfortunate knock on effects of being generally insufferable is that many people don’t want to be associated with you in any way. And when you’re both insufferable and happen to be the most divisive American political figure in modern history, all the more so. And that is certainly why, during both of the Donald Trump presidential campaigns, it became common practice for musical artists to complain about his “unauthorized” use of their music at his campaign events.Now, as this site has posted out a zillion times in the past, many of the complaints from artists are unfounded. Often, the use of the music in question was authorized through blanket performance licenses held by the venues for the rallies. While it should be obvious that best practice would be for candidates like Trump to seek permission to use music just to avoid any public complaining and backlash for that use, there is no real copyright claim to be had in those instances. Lots of people get this wrong.Copyright law is so screwed up that there actually may be a case where the law does require permission. And it has to do with pre-1972 sound recordings. If you’ve been reading Techdirt for any length of time, you know that we’ve discussed this issue many times in the past. Historically, while compositions were covered by copyright, under the 1909 Copyright Act sound recordings were not. This resulted in a patchwork of state laws (and state commonlaw) that created special forms of copyright at the state level. Eventually, sound recordings were put under federal copyright law, but it only applied to works recorded after February 14, 1972. Works recorded before that are not under federal copyright law, but remain basically the only things under those state copyright laws (the 1976 Copyright Act basically wiped out state copyright laws for everything but that one tiny thing).The issue is not that simple, because nothing around this particular issue is simple. However, based on at least some of the rulings in pre-1972 sound recording copyright cases, federal copyright law doesn’t apply at all to those songs (other court opinions have come out otherwise). And thus, there’s an argument that the requirements involving blanket licenses for pre-1972 sound recordings may not apply, because the use of the sound recording may require a special public performance license from the copyright holderAnd so now we have a decade or so of courts trying to figure this out. The outcomes of court cases are every bit as patchwork as the state laws that inform their outcomes. Add to all of this that even some of the blanket licenses from the likes of ASCAP include opt-outs for political campaigns and the like and it’s easy for all kinds of mistakes to be made.Mistakes don’t really explain the rash of instances of artists complaining about Trump’s usage, however. He’s been through this so many times, in fact, that it seems obvious that he and his people simply don’t care to try to secure permission. I doubt they even looked into whether they needed to. And the onus to understand what licensing is needed is certainly on their shoulders and nobody else’s. That’s how you get Pharrell clapping back on Trump’s use of his music at an insane rally shortly after a nationalist murdered 11 people in Pittsburgh (the venue didn’t have a license from the artist’s rights management of choice). Or his campaign losing a copyright suit to Eddy Grant for the use of his music in a campaign video. Hayes’ son and estate manager, music producer Isaac Hayes III, says in a Monday (Feb. 23) Instagram statement that the lawsuit “has been mutually resolved, and we are satisfied with the outcome.” Financial terms of the settlement were not disclosed.“This resolution represents more than the conclusion of a legal matter,” writes Hayes III in his statement. “It reaffirms the importance of protecting intellectual property rights and copyrights, especially as they relate to legacy, ownership and the responsible use of creative works.”It will surprise nobody that I would love to debate most of what appears in that quote from Hayes III, but that is a separate matter entirely. Instead, my focus is on two undeniable realities. First, the chaos that has been created with these older, pre-1972 song recordings is insane, complicated, and needlessly convoluted. Whoever thought this setup was a good idea should be placed in a facility under constant care.Second, Trump almost certainly committed copyright infringement, the above complaint notwithstanding. And he’s been through enough of these that he could very easily tell his people to just go get the proper permissions for any music that is played at his little fascism pep rallies. While the settlement terms go undisclosed, which is always annoying, I’ve seen enough of these to be able to read between the lines. The Hayes estate got its pint of blood, at a bare minimum.Wouldn’t it just be easier to get artists that like you to let you play their music at your events, Donald? There were at least a few artists at that emotional support half time show that nobody watched that you could choose from.]]></content:encoded></item><item><title>OpenAI Fires an Employee For Prediction Market Insider Trading</title><link>https://slashdot.org/story/26/02/27/2342226/openai-fires-an-employee-for-prediction-market-insider-trading?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Wired: OpenAI has fired an employee following an investigation into their activity on prediction market platforms including Polymarket, WIRED has learned. OpenAI CEO of Applications, Fidji Simo, disclosed the termination in an internal message to employees earlier this year. The employee, she said, "used confidential OpenAI information in connection with external prediction markets (e.g. Polymarket)." "Our policies prohibit employees from using confidential OpenAI information for personal gain, including in prediction markets," says spokesperson Kayla Wood. OpenAI has not revealed the name of the employee or the specifics of their trades.
 
Evidence suggests that this was not an isolated event. Polymarket runs on the Polygon blockchain network, so its trading ledger is pseudonymous but traceable. According to an analysis by the financial data platform Unusual Whales, there have been clusters of activities, which the service flagged as suspicious, around OpenAI-themed events since March 2023. Unusual Whales flagged 77 positions in 60 wallet addresses as suspected insider trades, looking at the age of the account, trading history, and significance of investment, among other factors. Suspicious trades hinged on the release dates of products like Sora, GPT-5, and the ChatGPT Browser, as well as CEO Sam Altman's employment status. In November 2023, two days after Altman was dramatically ousted from the company, a new wallet placed a significant bet that he would return, netting over $16,000 in profits. The account never placed another bet.
 
The behavior fits into patterns typical of insider trades. "The tell is the clustering. In the 40 hours before OpenAI launched its browser, 13 brand-new wallets with zero trading history appeared on the site for the first time to collectively bet $309,486 on the right outcome," says Unusual Whales CEO Matt Saincome. "When you see that many fresh wallets making the same bet at the same time, it raises a real question about whether the secret is getting out." [...] Though this is the first confirmed case of a large technology company firing an employee over trades in prediction markets, it's almost certainly not the last. Opportunities for tech sector employees to make trades on markets abound. "The data tells me this is happening all over the place," Saincome says.]]></content:encoded></item><item><title>The 5 Best Batsuits From Batman: Arkham Knight</title><link>https://hackernoon.com/the-5-best-batsuits-from-batman-arkham-knight?source=rss</link><author>Jose</author><category>tech</category><pubDate>Sat, 28 Feb 2026 03:28:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Batman made his first appearance in 1939. When you have a character who has been around for 80 years, they’re bound to change their appearance from time to time. So, it only makes sense for video games like Batman: Arkham Knight to take advantage of this and add different alternative costumes that players can pick and choose from. Some of these were hits, some of these were misses, and some were just okay. Let’s take a look at the 5 biggest hits (at least, in my opinion). Here are the 5 best batsuits from Batman: Arkham Knight.5 Best Batsuits From Arkham KnightOne of my favorites is the default one, the v8.03. We get this one early into the game, when Batman realizes he needs a little something extra, and his old costume just won’t do. It looks sleek, heavy, and the black looks amazing. This is definitely the best-looking default batsuit in the entire series.\
There are 2 other versions of this suit: the 8.04 and 8.05. The 8.04 stays perfectly pristine, so it doesn’t get any battle damage as the 8.03 does. Then there’s the 8.05. This one is similar, with the exception of the golden bat symbol on the chest.\
I’m not a fan of the golden bat symbol, and I actually like that the 8.03 can get damaged and cut up. It shows the toll that the night has taken on Batman.Some people like it when the batsuit is gray; others prefer it when it’s black. I like this one because it falls right in the middle. It looks like a dark gray, but from a different perspective, you can technically say that it’s a light black. What really makes the Batman Inc. suit one of my favorites, though, is the bat symbol on his chest. The yellow oval behind the black bat symbol makes it really striking, and it’s one of the first things your eyes notice.\
There are similar batsuits like the one from the 1989 movie that have a similar look. However, the Batman Inc. one has a better bat symbol and cowl. That’s why I put it above the 1989 one and above most other ones.\
The Batman Flashpoint suit is damn near perfect. The red accents all throughout it, such as in his pouches, eyes, and bat symbol, really make the whole thing stand out. Plus, having the body be gray with the cowl, gauntlets, and boots be completely black was such a great idea. The cherry on top, the thing that makes this costume stunning, is the double-handguns, one on each side. Like I said, damn near perfection.\
So, what don’t I like about it? My least favorite thing about it is the strange shoulder guards. Maybe if they were smaller and less pointy, I would be into it. Even better would be if they were completely gone. With all that said, though, this is still one of the best skins in the game.I said I like the Batman Inc. suit because it’s the perfect mix between black and gray. I like the Batman v Superman one for a completely different reason. This one is very clearly gray; there’s no mistaking it. There are others that are gray, like the First Appearance one, but there is something different about this particular one. It’s the fabric. I’m not sure how to describe it, but the fabric of the suit is unlike any other.\
It sort of looks like a type of Kevlar, which is completely different from the tights that some of the other costumes appear to be made out of. That, combined with the gigantic bat symbol, makes it look phenomenal. I’m a sucker for a good bat symbol, what can I say?This might be a hot take, but my all-time favorite batsuit in the Arkham Knight game is the Batman Beyond one. Don’t get me wrong, I love the others on this list, but this one is just on a completely different level. I really like the red accents and how mechanized the whole suit looks. It has a whole cyborg thing going on that I personally enjoy. There are some caveats to it, though, that I will admit to.\
The mouthpiece is a bit strange-looking. You have this whole futuristic costume, and it looks like the mouthpiece is just made out of cloth or something. Not a fan. The second biggest critique is that it looks nothing like the Batman Beyond suit from the animated show.\
I can understand and accept these two flaws, but that doesn’t bring down my enjoyment of the costume. In my opinion, the Batman Beyond suit is the best-looking one in Batman: Arkham Knight. You know what, I might have to replay the whole game again just to look at these cool skins.]]></content:encoded></item><item><title>Human Brain Cells On a Chip Learned To Play Doom In a Week</title><link>https://games.slashdot.org/story/26/02/27/2332219/human-brain-cells-on-a-chip-learned-to-play-doom-in-a-week?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Researchers at Cortical Labs used living human neurons grown on a chip to learn how to play Doom in about a week. "While its performance is not up to par with humans, experts say it brings biological computers a step closer to useful real-world applications, like controlling robot arms," reports New Scientist. From the report: In 2021, the Australian company Cortical Labs used its neuron-powered computer chips to play Pong. The chips consisted of clumps of more than 800,000 living brain cells grown on top of microelectrode arrays that can both send and receive electrical signals. Researchers had to carefully train the chips to control the paddles on either side of the screen. Now, Cortical Labs has developed an interface that makes it easier to program these chips using the popular programming language Python. An independent developer, Sean Cole, then used Python to teach the chips to play Doom, which he did in around a week.
 
"Unlike the Pong work that we did a few years ago, which represented years of painstaking scientific effort, this demonstration has been done in a matter of days by someone who previously had relatively little expertise working directly with biology," says Brett Kagan of Cortical Labs. "It's this accessibility and this flexibility that makes it truly exciting."
 
The neuronal computer chip, which used about a quarter as many neurons as the Pong demonstration, played Doom better than a randomly firing player, but far below the performance of the best human players. However, it learnt much faster than traditional, silicon-based machine learning systems and should be able to improve its performance with newer learning algorithms, says Kagan. However, it's not useful to compare the chips with human brains, he says. "Yes, it's alive, and yes, it's biological, but really what it is being used as is a material that can process information in very special ways that we can't recreate in silicon." Cortical Labs posted a YouTube video showing its CL1 biological computer running Doom. There's also source code available on GitHub, with additional details in a README file.]]></content:encoded></item><item><title>Google quantum-proofs HTTPS by squeezing 15kB of data into 700-byte space</title><link>https://arstechnica.com/security/2026/02/google-is-using-clever-math-to-quantum-proof-https-certificates/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/06/https-1152x648.jpg" length="" type=""/><pubDate>Sat, 28 Feb 2026 01:26:41 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT - Ars Technica</source><content:encoded><![CDATA[Google on Friday unveiled its plan for its Chrome browser to secure HTTPS certificates against quantum computer attacks without breaking the Internet.The objective is a tall order. The quantum-resistant cryptographic data needed to transparently publish TLS certificates is roughly 40 times bigger than the classical cryptographic material used today. A typical X.509 certificate chain used today comprises six elliptic curve signatures and two EC public keys,  each of them only 64 bytes. This material can be cracked through the quantum-enabled Shor’s algorithm. The full chain is roughly 4 kilobytes. All this data must be transmitted when a browser connects to a site.The bigger they come, the slower they move“The bigger you make the certificate, the slower the handshake and the more people you leave behind,” said Bas Westerbaan, principal research engineer at Cloudflare, which is partnering with Google on the transition. “Our problem is we don’t want to leave people behind in this transition.” Speaking to Ars, he said that people will likely disable the new encryption if it slows their browsing. He added that the massive size increase can also degrade “middle boxes,” which sit between browsers and the final site.]]></content:encoded></item><item><title>Hyperion Author Dan Simmons Dies From Stroke At 77</title><link>https://news.slashdot.org/story/26/02/27/2226234/hyperion-author-dan-simmons-dies-from-stroke-at-77?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Author Dan Simmons, best known for the epic sci-fi novel Hyperion and its sequels, has died at 77 following a stroke. Ars Technica's Eric Berger remembers Simmons, writing: Simmons, who worked in elementary education before becoming an author in the 1980s, produced a broad portfolio of writing that spanned several genres, including horror fiction, historical fiction, and science fiction. Often, his books included elements of all of these. This obituary will focus on what is generally considered his greatest work, and what I believe is possibly the greatest science fiction novel of all time, Hyperion.
 
Published in 1989, Hyperion is set in a far-flung future in which human settlement spans hundreds of planets. The novel feels both familiar, in that its structure follows Chaucer's Canterbury Tales, and utterly unfamiliar in its strange, far-flung setting. Simmons' Hyperion appeared in an Ask Slashdot story back in 2008, when Slashdot reader willyhill asked for tips on how Slashdotters track down great sci-fi. If you're in the mood for a little nostalgia, or just want to browse the thread for book recommendations, it's well worth revisiting.]]></content:encoded></item><item><title>From San Francisco to the Sands: Why U.S. Tech Talent Is Eyeing the UAE</title><link>https://hackernoon.com/from-san-francisco-to-the-sands-why-us-tech-talent-is-eyeing-the-uae?source=rss</link><author>Nica Furs</author><category>tech</category><pubDate>Sat, 28 Feb 2026 01:18:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If you hang around startup circles in the Bay Area long enough, you’ll start hearing something unexpected between funding rounds and AI debates: founders quietly Googling “car rental in UAE” and checking flight prices to Dubai and Abu Dhabi. What started as curiosity has turned into a real trend. From San Francisco to the sands of the Arabian Peninsula, American tech talent is seriously eyeing the United Arab Emirates—and not just for a quick conference or a flashy vacation.So what’s driving the shift?Silicon Valley Burnout Is RealLet’s call it what it is. The Bay Area is still iconic, but it’s also expensive, hyper-competitive, and increasingly saturated. Sky-high rents, intense regulation, talent wars, and a constant hustle culture can wear even the most ambitious founder down. After years of grinding in co-working spaces and chasing Series A funding, some U.S. entrepreneurs are looking for a reset.The UAE, especially Dubai and Abu Dhabi, is pitching itself as that reset button. Lower personal income taxes, streamlined business setup processes, and aggressive government support for innovation make the region hard to ignore. For founders used to navigating layers of red tape back home, the efficiency can feel almost unreal.A Government That Actually Bets on TechOne of the biggest surprises for Americans exploring the UAE tech scene is how hands-on—and forward-thinking—the government is. Artificial intelligence, fintech, climate tech, space technology: these aren’t just buzzwords on a conference banner. They’re central to national strategy.Free zones tailored to tech companies offer 100% foreign ownership and simplified licensing. Major funds and sovereign wealth investors actively back innovation. In many cases, founders aren’t just tolerated—they’re welcomed with open arms and real incentives.Compare that to the sometimes fragmented regulatory environment in the U.S., and it’s easy to see why some builders are thinking, “Why not give this a shot?”It’s Not Just Oil Money AnymoreThere’s still a persistent stereotype in the U.S. that the Gulf economy runs purely on oil. That narrative is outdated. The UAE has spent decades diversifying its economy, investing heavily in infrastructure, tourism, logistics, and now digital transformation.Walk through Dubai Internet City or Hub71 in Abu Dhabi and you’ll see a mix of global companies, scrappy startups, and venture-backed disruptors. English is widely spoken. Contracts are often structured in ways that feel familiar to U.S. founders. The vibe? Surprisingly international and business-friendly.For tech professionals who’ve spent their careers building products for global markets, the UAE’s geographic position—bridging Europe, Asia, and Africa—is a strategic advantage. A product launched in Dubai can scale across multiple regions without being locked into one market.Let’s talk lifestyle, because it matters. The UAE isn’t just pitching spreadsheets and tax breaks. It’s selling quality of life. Modern apartments, world-class restaurants, beach access, and relatively high levels of safety are all part of the package.Yes, the summer heat is intense. But the infrastructure is built for it. Offices, malls, and residential buildings are climate-controlled. Everything runs efficiently. For many Americans, the biggest adjustment isn’t the temperature—it’s the pace. Things move fast. Deals close quickly. Bureaucracy, when it exists, is often surprisingly streamlined.And when it comes to getting around, practicality kicks in. Cities like Dubai are spread out, with business districts, residential communities, and innovation hubs connected by wide highways. While public transportation exists, most professionals find that having a car makes daily life significantly easier. Whether you’re commuting to a co-working space, heading to investor meetings, or exploring new neighborhoods, renting a car is often the smartest move—especially during your first few months while you figure out where to settle.The Remote Work Era Changed the GameThe pandemic permanently shifted how tech workers think about location. If you can code from anywhere, why limit yourself to one zip code? The UAE capitalized on this shift by introducing long-term visas and remote work permits designed specifically for global talent.Suddenly, relocating doesn’t mean cutting ties with U.S. clients or investors. Many founders maintain American entities while building regional operations in the UAE. It’s less about abandoning Silicon Valley and more about expanding beyond it.This hybrid model is attractive. Keep your Delaware C-corp, but base your operations in a city that offers global connectivity, strong infrastructure, and competitive costs. For a generation raised on flexibility and scale, it’s a compelling pitch.Risk, Reward, and ReputationOf course, moving halfway across the world isn’t a casual decision. Cultural differences, legal frameworks, and market dynamics require research and adaptability. Not every startup will thrive in the Gulf, and not every founder will feel at home.But the reputation factor is shifting. What once seemed like a bold or risky move now feels strategic. U.S. tech talent isn’t just chasing sunshine and skyscrapers. They’re chasing opportunity—new markets, new investors, and a chance to build in an ecosystem that’s actively evolving.From San Francisco to the sands, the flow of ideas—and people—is becoming more global. The UAE isn’t replacing Silicon Valley, but it’s carving out its own lane as a serious contender in the tech world. For American founders and engineers tired of the grind or hungry for international expansion, it’s a place worth exploring.Pack your laptop. Line up your meetings. Maybe start with a short-term stay and a rental car to navigate the city like a local. You might just discover that the future of your startup isn’t limited to the Bay Area. Sometimes, the next big move starts with a one-way ticket and a willingness to see what’s possible beyond the familiar skyline.]]></content:encoded></item><item><title>The 7 Leading Requirements Management Software Solutions in 2026</title><link>https://hackernoon.com/the-7-leading-requirements-management-software-solutions-in-2026?source=rss</link><author>Steve Beyatte</author><category>tech</category><pubDate>Sat, 28 Feb 2026 00:47:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This guide compares the 7 leading requirements management software solutions in 2026, from modern platforms like Jama Connect to legacy tools like IBM DOORS and lightweight options like Excel. The best choice depends on your product complexity, regulatory requirements, and team structure—but most organizations opt for modern tools like Jama Connect.]]></content:encoded></item><item><title>CISA Replaces Bumbling Acting Director After a Year</title><link>https://yro.slashdot.org/story/26/02/27/2215238/cisa-replaces-bumbling-acting-director-after-a-year?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New submitter DeanonymizedCoward shares a report from TechCrunch: The U.S. Cybersecurity and Infrastructure Security Agency (CISA) is reportedly in crisis following major budget cuts, layoffs, and furloughs under the Trump administration, says TechCrunch. The agency has now replaced its acting director, Madhu Gottumukkala, after a turbulent year marked by controversy and internal turmoil. During his tenure, Gottumukkala allegedly mishandled sensitive information by uploading government documents to ChatGPT, oversaw a one-third reduction in staff, and reportedly failed a counterintelligence polygraph needed for classified access. His leadership also saw the suspension of several senior officials, including CISA's chief security officer. Nextgov also reported that CISA lost another top senior official, Bob Costello, the agency's chief information officer tasked with overseeing the agency's IT systems and data policies. "Last month, CISA's acting director Madhu Gottumukkala reportedly took steps to transfer Costello, but other political appointees blocked it," added Nextgov.]]></content:encoded></item><item><title>TurboSparse-LLM Performance: Outperforming Mixtral and Gemma with Extreme Sparsity</title><link>https://hackernoon.com/turbosparse-llm-performance-outperforming-mixtral-and-gemma-with-extreme-sparsity?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Sat, 28 Feb 2026 00:35:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We measure our sparsified models’ performance on tasks included in OpenLLM Leaderboard which include 25-shot Arc-Challenge [13], 10-shot Hellaswag [65], 5-shot MMLU [22], 0-shot TruthfulQA [35], 5-shot Winogrande [51] and 8-shot GSM8K [14]. In addition, we also follow Llama 2’s evaluation task included commonsense reasoning tasks. We report the average of PIQA [8], SCIQ [26], ARC easy [13], OpenBookQA [41]. We compare our models to several external open-source LLMs, including Gemma-2B [58], Mistral-7B [24] and Mixtral-47B [25].\
\
Table 6 shows the results from different models. TurboSparse-Mistral-7B outperforms Gemma-2B by far, while only activating 3B parameters. TurboSparse-Mixtral-47B outperforms the original Mixtral-47B with only 4.5B parameters activated. The results demonstrate that LLMs with ReLU based intrinsic activation sparsity can keep the same or better performance while hold the significant FLOPs reduction.(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(5) Li Ma, Shanghai Artificial Intelligence Laboratory;(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.]]></content:encoded></item><item><title>dReLU Sparsification: Recovering LLM Performance with 150B Token Pretraining</title><link>https://hackernoon.com/drelu-sparsification-recovering-llm-performance-with-150b-token-pretraining?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Sat, 28 Feb 2026 00:31:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In the previous section, we have demonstrated that dReLU can be a better choice for ReLUfication. The main question now is whether dReLU based ReLUfication can recover the original model’s performance while achieving higher sparsity. The following sections will discuss the experiments that aimed at answering this question.\
 We consider two representative models: Mistral-7B and Mixtral-47B. We substitute the original SwiGLU based FFN with dReLU based FFN and then continue pretraining.\
 Due to the ReLUfication process, the restoration of model capability is closely related to the corpus used for recovery training. We collected as much corpus as possible from the open-source community for training, such as Wanjuan-CC [48], open-web-math [46], peS2o [54], Pile [19], The Stack [28], GitHub Code [1] and so on. The detailed mixture ratio is as shown in the following table 4:\
\
. After pretraining, we utilize the high-quality SFT datasets to further improve our model’s performance, including orca-math-word-problems [43], bagel [27].\
. The hyperparameters for our ReLUfication are based on empirical results from previous works [69]. We utilize the llm-foundry framework for training [44] and employ FSDP parallelism.\
Our models are trained using the AdamW optimizer [38] with the following hyper-parameters: β1 = 0.9 and β2 = 0.95. We adopt a cosine learning rate schedule and use the default values for weight decay and gradient clipping (see Table 5 for more details). In total, we pretrain our models on 150B tokens.(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(5) Li Ma, Shanghai Artificial Intelligence Laboratory;(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.]]></content:encoded></item><item><title>The Metrics Review Ritual That Turns Product Work Into Revenue</title><link>https://hackernoon.com/the-metrics-review-ritual-that-turns-product-work-into-revenue?source=rss</link><author>Dan Layfield</author><category>tech</category><pubDate>Sat, 28 Feb 2026 00:29:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[If you “know your numbers” but can’t explain why they move, you’re flying blind.]]></content:encoded></item><item><title>GNOME GitLab Redirecting Some Git Traffic To GitHub For Reducing Costs</title><link>https://www.phoronix.com/news/GNOME-GitHub-GitLab-Redirect</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 00:03:03 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[If you are cloning from a GNOME repository on their GitLab and now finding your Git traffic being redirected to GitHub, you are not alone. GNOME's infrastructure team is now redirecting Git traffic from the GNOME.org GitLab over to GitHub mirrors for reducing bandwidth costs...]]></content:encoded></item><item><title>Perplexity Announces &apos;Computer,&apos; an AI Agent That Assigns Work To Other AI Agent</title><link>https://slashdot.org/story/26/02/27/2151236/perplexity-announces-computer-an-ai-agent-that-assigns-work-to-other-ai-agent?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[joshuark shares a report from Ars Technica: Perplexity has introduced "Computer," a new tool that allows users to assign tasks and see them carried out by a system that coordinates multiple agents running various models. The company claims that Computer, currently available to Perplexity Max subscribers, is "a system that creates and executes entire workflows" and "capable of running for hours or even months."
 
The idea is that the user describes a specific outcome -- something like "plan and execute a local digital marketing campaign for my restaurant" or "build me an Android app that helps me do a specific kind of research for my job." Computer then ideates subtasks and assigns them to multiple agents as needed, running the models Perplexity deems best for those tasks. The core reasoning engine currently runs Anthropic's Claude Opus 4.6, while Gemini is used for deep research, Nano Banana for image generation, Veo 3.1 for video production, Grok for lightweight tasks where speed is a consideration, and ChatGPT 5.2 for "long-context recall and wide search."
 
This kind of best-model-for-the-task approach differs from some competing products like Claude Cowork, which only uses Anthropic's models. All this happens in the cloud, with prebuilt integrations. "Every task runs in an isolated compute environment with access to a real filesystem, a real browser, and real tool integrations," Perplexity says. The idea is partly that this workflow was what some power users were already doing, and this aims to make that possible for a wider range of people who don't want to deal with all that setup.
 
People were already using multiple models and tailoring them to specific tasks based on perceived capabilities, while, for example, using MCP (Model Context Protocol) to give those models access to data and applications on their local machines. Perplexity Computer takes a different approach, but the goal is the same: have AI agents running tailor-picked models to perform tasks involving your own files, services, and applications. Then there is OpenClaw, which you could perceive as the immediate predecessor to this concept.]]></content:encoded></item><item><title>Perplexity Announces &apos;Computer,&apos; an AI Agent That Assigns Work To Other AI Agents</title><link>https://slashdot.org/story/26/02/27/2151236/perplexity-announces-computer-an-ai-agent-that-assigns-work-to-other-ai-agents?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Sat, 28 Feb 2026 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[joshuark shares a report from Ars Technica: Perplexity has introduced "Computer," a new tool that allows users to assign tasks and see them carried out by a system that coordinates multiple agents running various models. The company claims that Computer, currently available to Perplexity Max subscribers, is "a system that creates and executes entire workflows" and "capable of running for hours or even months."
 
The idea is that the user describes a specific outcome -- something like "plan and execute a local digital marketing campaign for my restaurant" or "build me an Android app that helps me do a specific kind of research for my job." Computer then ideates subtasks and assigns them to multiple agents as needed, running the models Perplexity deems best for those tasks. The core reasoning engine currently runs Anthropic's Claude Opus 4.6, while Gemini is used for deep research, Nano Banana for image generation, Veo 3.1 for video production, Grok for lightweight tasks where speed is a consideration, and ChatGPT 5.2 for "long-context recall and wide search."
 
This kind of best-model-for-the-task approach differs from some competing products like Claude Cowork, which only uses Anthropic's models. All this happens in the cloud, with prebuilt integrations. "Every task runs in an isolated compute environment with access to a real filesystem, a real browser, and real tool integrations," Perplexity says. The idea is partly that this workflow was what some power users were already doing, and this aims to make that possible for a wider range of people who don't want to deal with all that setup.
 
People were already using multiple models and tailoring them to specific tasks based on perceived capabilities, while, for example, using MCP (Model Context Protocol) to give those models access to data and applications on their local machines. Perplexity Computer takes a different approach, but the goal is the same: have AI agents running tailor-picked models to perform tasks involving your own files, services, and applications. Then there is OpenClaw, which you could perceive as the immediate predecessor to this concept.]]></content:encoded></item><item><title>Subscription Managers: When They’re Worth It</title><link>https://hackernoon.com/subscription-managers-when-theyre-worth-it?source=rss</link><author>Dan Layfield</author><category>tech</category><pubDate>Fri, 27 Feb 2026 23:59:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Everyone who runs a subscription business has to eventually decide if they’re going buy a subscription manger.]]></content:encoded></item><item><title>Sparse Activation in MoE Models: Extending ReLUfication to Mixture-of-Experts</title><link>https://hackernoon.com/sparse-activation-in-moe-models-extending-relufication-to-mixture-of-experts?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Fri, 27 Feb 2026 23:46:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[4 Are Neurons in Expert still Sparsely Activated?Previous work has shown that dense LLMs with different activation functions (ReLU, SwiGLU, etc.) exhibit the property of sparse activation [69, 36, 30]. However, the analysis is limited to dense models. Despite the intuitive assumption that partitioning FFNs into different experts within an MoE model would result in denser activations within each expert, it remains unclear whether this sparsity phenomenon persists in MoE models. In this section, we select representative MoE models and commonly used downstream tasks to investigate whether this sparsity phenomenon still exists in MoE models. We utilize the same method in 3 to control the sparsity in each expert.\
. We select Deepseek-MoE [15], Qwen1.5-MoE [5] and Mixtral [25] as the models for our experiments. We also add Llama-2-7B as for comparison.\
We first study the performance with regard to the sparsity ratio, as shown in Figure 5 (a)[2]. Specifically, the performance only drops by about 1%-2% when the sparsity ratio is 0.5. This trend suggests that MoE models exhibit similar sparsity compared to dense models.\
Further, we profile the activation patterns of Mistral and Mixtral, a pair of popular dense LLM and MoE LLM, as shown in Figure 5 (b). We find that both LLMs show a similar pattern where activations are concentrated around 0, which is consistent with previous analysis of dense LLMs. The sparsity in experts also implies that every neuron in the same expert has different functionality. This finding applies to all layers and experts, as detailed in Appendix A.2. We report this interesting observation and leave further analysis for future work.\
\
Inspired by our discoveries in MoE models, we are convinced that ReLUfication can be extended to MoE models and is not restricted to dense models. As the proportion of FFN weights in MoE models increases, the FLOP reduction achieved through ReLUfication will be even more pronounced.(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(5) Li Ma, Shanghai Artificial Intelligence Laboratory;(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.]]></content:encoded></item><item><title>Subscription Growth: The Momentum You Can’t See</title><link>https://hackernoon.com/subscription-growth-the-momentum-you-cant-see?source=rss</link><author>Dan Layfield</author><category>tech</category><pubDate>Fri, 27 Feb 2026 23:44:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A $1M MRR celebration turned into a refunds lesson—and a framework for why subscription growth feels slow, hides wins, and compounds over time.]]></content:encoded></item><item><title>South Korea Set To Get a Fully Functioning Google Maps</title><link>https://tech.slashdot.org/story/26/02/27/2144239/south-korea-set-to-get-a-fully-functioning-google-maps?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[South Korea has reversed a two-decade policy and approved the export of high-precision map data, paving the way for a fully functional Google Maps in the country. Reuters reports: The approval was made "on the condition that strict security requirements are met," the Ministry of Land, Infrastructure and Transport said in a statement. Those conditions include blurring military and other sensitive security-related facilities, as well as restricting longitude and latitude coordinates for South Korean territory on products such as Google Maps and Google Earth, it said.
 
The decision is expected to hurt Naver and Kakao -- local internet giants which currently dominate the country's market for digital map services. But it will appease Washington, which has urged Seoul to tackle what it says is discrimination against U.S. tech companies. South Korea, still technically at war with North Korea, had shot down Google's previous bids in 2007 and 2016 to be allowed to export the data, citing the risks that information about sensitive military and security facilities could be exposed. "Google can now come in, slash usage fees, and take the market," said Choi Jin-mu, a geography professor at Kyung Hee University. "If Naver and Kakao are weakened or pushed out and Google later raises prices, that becomes a monopoly. Then, even companies that rely on map services -- logistics firms, for example -- become dependent, and in the long run, even government GIS (geographic information) systems could end up dependent on Google or Apple. That's the biggest concern."]]></content:encoded></item><item><title>The NFL Won A Lawsuit Over Its Bluesky Ban. Its Social Media Strategy Is Still A Loser</title><link>https://www.techdirt.com/2026/02/27/the-nfl-won-a-lawsuit-over-its-bluesky-ban-its-social-media-strategy-is-still-a-loser/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 27 Feb 2026 23:07:36 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Full disclosure up front: I sit on the board of Bluesky. That said, I had absolutely no idea this lawsuit existed until recently. Which, honestly, tells you something about how much of a legal non-event it was. But the underlying story here—about the NFL treating social media the way it treats television broadcast rights—is worth digging into, because it reveals something deeply broken about how major sports leagues think about the internet.The 2025-2026 NFL season just wrapped up, and along with it came a federal court ruling in a case called Brown v. NFL that most people missed entirely. Two football fans—one in Illinois, one in California—sued the NFL under the Sherman Act, claiming the league violated antitrust law by barring its teams from posting on Bluesky. The fans wanted to follow their teams—the Bears and the now-champion Seahawks—on the platform they actually use, rather than on Elon Musk’s X. The court dismissed the case for lack of standing, and honestly, that was probably the right legal outcome.The fans couldn’t demonstrate a concrete injury—the information they wanted was still available, for free, on X. As the court put it, their grievance reduced to being “denied the ability to obtain real-time NFL team information on a private platform with which they are ideologically comfortable.” And “I don’t like Elon Musk” is not an antitrust injury. The Sherman Act targets conspiracies that restrain trade and harm competition—not content distribution preferences. You can’t force a private organization to distribute its content on the platform you like best, just as we’ve called out attempts to force social media platforms to carry content they don’t want to carry.But the fact that the NFL is  to be this myopic doesn’t make it a smart business decision. You can be entirely within your rights and still be making a spectacularly bad call.Since 2013, the NFL has had a “content partnership” with X (dating back to when it was the useful site known as Twitter). The deal lets X publish real-time highlights, and in return the league gets… money, presumably. As the court noted in its ruling:Since 2013, the NFL and X (formerly Twitter, Inc.) have had a “content partnership.” It allows X to publish real-time highlights from football games, such as touchdowns. During the offseason, reporters post on X with news about team practices and other NFL-related topics, and fans on X discuss teams’ acquisitions of free agents and other roster changes. For example, during the NFL draft (the high-profile annual event in which teams select eligible players to join their rosters), X published more than one million posts concerning the NFL; these appeared on users’ screens more than 800 million times. The NFL has repeatedly renewed its partnership with X. Fans do not pay money to receive NFL news on X.Fine. Lots of organizations have deals with social media platforms. But this just seems like self-sabotage: the NFL apparently used this partnership as justification to tell its own teams they couldn’t even  on a competing platform. Multiple NFL teams—including the New England Patriots—had set up accounts on Bluesky, started posting, and were building audiences. And then the league office stepped in and told them to shut it all down.Initially, multiple NFL teams, including the New England Patriots, had accounts on Bluesky to communicate with fans….As alleged, however, the NFL later instructed its member teams to delete their Bluesky accounts. But for this instruction, at least some NFL teams would use Bluesky. The Patriots’ vice president of content, Fred Kirsch, for example, has stated: “Whenever the league gives us the green light[,] we’ll get back on Bluesky.”Yes, the (Super Bowl-losing) Patriots’ VP of content is publicly saying his team  to be on Bluesky and is just waiting for the league to  them. This wasn’t a case of teams being uninterested. Teams saw the audience there, set up shop, and were actively communicating with fans—and the NFL made them stop.So the NFL has essentially decided that when it comes to the kind of real-time updates that fans actually care about, X is the only approved outlet. Everything else is locked out.This is “broadcast-brain” thinking applied to the internet, and it’s spectacularly dumb.The NFL is treating social media platforms the way it treats regional sports networks or its Sunday Ticket package: as exclusive territories to be carved up and sold to the highest bidder. In the television world, that model makes a certain kind of sense—there’s a limited amount of spectrum, a limited number of cable channels, and that scarcity creates value. But social media doesn’t work that way. There’s no scarcity. Posting an injury report on Bluesky doesn’t  it from X. Cross-posting is literally free. The entire point of social media for a brand is to be everywhere your audience is.And the audience, increasingly, is on Bluesky. As Mashable noted last year heading into the season, the NFL community on Bluesky had already hit a kind of critical mass:You need the presence and regular posting of big names to legitimize a platform. It certainly helped that folks like Kimes and aat popular sports sites like The Ringer made Bluesky home. And last season it felt like Bluesky hit terminal velocity, where enough people joined that you could fully exit to the site for football content. And with the migration of the professionals, the shitposters naturally came along, too. Because that’s where the discussion was happening. There is genuine, easy-to-find, fun NFL talk on Bluesky with minimal interruptions from, say,That’s a real community. A vibrant, engaged community of exactly the kind of hardcore football fans that the NFL should be desperate to cultivate. These are, as Mashable noted, the “ball knowers.” They’ve moved to Bluesky because, well, X kind of sucks now for following sports. As Mashable also noted:Bluesky does have a leg-up in some areas — Elon Musk’s site recently has proven unreliable for NFL fans. Thesite crashed the morning free agency launched, which is one of the most important days for NFL social media. And the sports tab — which used to be an easy, fun way to follow games in the Twitter days —degraded into near uselessnessyears ago. And, in general, X has morphed with Musk’s image, which is focusedand politics — not things like following football. Of course you can still follow the NFL on X, but it does involve wading through more junk than it used to. Bluesky offers an interesting alternative in that regard.So the most engaged, most knowledgeable football community has moved to Bluesky. The teams themselves  to be on Bluesky. And the NFL’s response to all of this is… to ban its teams from showing up.It’s the digital equivalent of a local blackout (something we’ve been calling out for well over a decade)—punishing your most dedicated fans because of some deal you cut with a middleman in an effort to create an artificial and unnecessary scarcity.Meanwhile, the platform the NFL is propping up with this exclusivity arrangement is one where fans who tuned in for the Super Bowl halftime show got to watch a significant chunk of the X user base have a full-blown racist meltdown over Bad Bunny performing. The NFL specifically chose Bad Bunny to appeal to a broader, more global audience—and the audience that actually appreciated the choice? They were on Bluesky where there was an overwhelming wave of support for the performance. The league is betting its real-time presence on the platform where its expansion strategy gets shouted down, while blocking teams from the one where those new fans are actually showing up.This kind of control-freakery from the NFL shouldn’t surprise anyone who has followed the league’s behavior over the years. This is the same organization that has spent decades aggressively lying to bars, restaurants, and small businesses about the scope of its “Super Bowl” trademarks, sending threatening letters suggesting you can’t even  “Super Bowl” in an ad without a license—something that has never actually been true.The NFL’s institutional DNA is “control equals value,” and they apply that logic to everything, from what a church can call its viewing party to which social media apps their teams are permitted to use.The problem is that control-based thinking only works when you actually  control the ecosystem. You can (sort of) control which networks broadcast your games. You can control which streaming service gets Sunday Ticket. You cannot control where fans choose to talk about football on the internet. The conversation is going to happen whether the NFL’s official accounts are there or not. The only question is whether the league’s teams get to participate in it.Any organization whose core business depends on fan engagement should be finding fans where they are, not herding them onto a single platform because you cut an exclusivity deal. Especially when that platform is increasingly known for being a hellscape of AI slop, political rage, and engagement-bait, while the platform you’re blocking your teams from is the one where people are actually talking about your product with genuine enthusiasm.The NFL generates billions in revenue. And yet, when it comes to social media strategy, it’s stuck in a 2005 mindset. That’s not how any of this works anymore.Someone at NFL HQ needs to understand that when your most passionate fans have moved to a new platform and your own teams are begging for permission to follow them there, the smart play is to let them go.]]></content:encoded></item><item><title>OpenAI fires employee for using confidential info on prediction markets</title><link>https://techcrunch.com/2026/02/27/openai-fires-employee-for-using-confidential-info-on-prediction-markets/</link><author>Julie Bort</author><category>tech</category><pubDate>Fri, 27 Feb 2026 23:00:54 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company said such trades violates its internal company policies about using confidential information for personal gain.]]></content:encoded></item><item><title>Trump Orders Federal Agencies To Stop Using Anthropic AI Tech &apos;Immediately&apos;</title><link>https://tech.slashdot.org/story/26/02/27/2138211/trump-orders-federal-agencies-to-stop-using-anthropic-ai-tech-immediately?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[President Donald Trump has ordered all U.S. federal agencies to "immediately cease" using Anthropic's AI technology, escalating a standoff after the company sought limits on Pentagon use of its models. CNBC reports: The company, which in July signed a $200 million contract with Pentagon, wants assurances that the Defense Department will not use its AI models will not be used for fully autonomous weapons or mass domestic surveillance of Americans. The Pentagon had set a deadline of 5:01 p.m. ET Friday for Anthropic to agree to its demands to allow the Pentagon to use the technology for all lawful purposes. If Anthropic did not meet that deadline, Pete Hegseth threatened to label the company a "supply chain risk" or force it to comply by invoking the Defense Production Act.
 
"The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution," Trump said in a post on Truth Social. "Their selfishness is putting AMERICAN LIVES at risk, our Troops in danger, and our National Security in JEOPARDY."
 
"Therefore, I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic's technology," Trump wrote. "We don't need it, we don't want it, and will not do business with them again! There will be a Six Month phase out period for Agencies like the Department of War who are using Anthropic's products, at various levels," Trump said. On Friday, OpenAI said it would also draw the same red lines as Anthropic: no AI for mass surveillance or autonomous lethal weapons.]]></content:encoded></item><item><title>US Military Accidentally Shoots Down Border Protection Drone With Laser</title><link>https://tech.slashdot.org/story/26/02/27/2133209/us-military-accidentally-shoots-down-border-protection-drone-with-laser?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 22:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the Associated Press: The U.S. military used a laser Thursday to shoot down a "seemingly threatening" drone flying near the U.S.-Mexico border. It turned out the drone belonged to Customs and Border Protection, lawmakers said. The case of mistaken identity prompted the Federal Aviation Administration to close additional airspace around Fort Hancock, about 50 miles (80 kilometers) southeast of El Paso. The military is required to formally notify the FAA when it takes any counter-drone action inside U.S. airspace.
 
It was the second time in two weeks that a laser was fired in the area. The last time it was CBP that used the weapon and nothing was hit. That incident occurred near Fort Bliss and prompted the FAA to shut down air traffic at El Paso airport and the surrounding area. This time, the closure was smaller and commercial flights were not affected. The FAA, CBP and the Pentagon confirmed the incident in a joint statement, saying the military "employed counter-unmanned aircraft system authorities to mitigate a seemingly threatening unmanned aerial system operating within military airspace."
 
"At President Trump's direction, the Department of War, FAA, and Customs and Border Patrol are working together in an unprecedented fashion to mitigate drone threats by Mexican cartels and foreign terrorist organizations at the U.S.-Mexico Border," the statement said. The report notes that 27,000 drones were detected within 1,600 feet of the southern border in the last six months of 2024.
 
Illinois Democratic U.S. Sen. Tammy Duckworth, the ranking member on the Senate's Aviation Subcommittee, is calling for an independent investigation to look into the matter. "The Trump administration's incompetence continues to cause chaos in our skies," Duckworth said.]]></content:encoded></item><item><title>Pentagon moves to designate Anthropic as a supply-chain risk</title><link>https://techcrunch.com/2026/02/27/pentagon-moves-to-designate-anthropic-as-a-supply-chain-risk/</link><author>Russell Brandom</author><category>tech</category><pubDate>Fri, 27 Feb 2026 21:53:14 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA["We don't need it, we don't want it, and will not do business with them again," the president wrote in the post.]]></content:encoded></item><item><title>White House Stalls Release of Approved US Science Budgets</title><link>https://news.slashdot.org/story/26/02/27/207211/white-house-stalls-release-of-approved-us-science-budgets?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 21:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Weeks after the U.S. Congress rejected unprecedented cuts to science budgets that the administration of US President Donald Trump had sought for 2026, funding to several agencies that award research grants is still not freely flowing. 

One reason is that the White House Office of Management and Budget (OMB) has been slow to authorize its release. The US National Institutes of Health (NIH) has so far not received approval to spend any of the research funding allocated in a budget bill signed into law on 3 February. The US National Science Foundation (NSF) was authorized to spend its funding just last week. And NASA has had its full funding authorized for release, but with an unusual restriction that limits spending on ten specific programmes -- many of which the Trump team had tried to cancel last year.]]></content:encoded></item><item><title>Whistleblower: ICE Has Slashed Its Training Program And Its Boss Is Lying To Congress About It</title><link>https://www.techdirt.com/2026/02/27/whistleblower-ice-has-slashed-its-training-program-and-its-boss-is-lying-to-congress-about-it/</link><author>Tim Cushing</author><category>tech</category><pubDate>Fri, 27 Feb 2026 20:51:42 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[ICE can’t keep up with the baseline set by resident ghoul/White House advisor Stephen Miller. Miller has demanded 3,000 arrests , which he presented as the  he would be satisfied with. To reach this goal, tons of talent from other federal law enforcement agencies have been added to the mix. The results have been less than impressive, to be entirely too kind to these kidnappers and murderers.CBP and Border Patrol officers used to handling border crossing business are now roaming the streets of Midwestern cities looking for anyone who seems a bit too dark to be a native. In addition, nearly  of FBI agents have been placed on the “find the brown person” beat, along with volunteers/voluntolds from DEA, ATF, US Secret Service, Homeland Security Investigations, and anyone else with a badge they’re unwilling to display prominently when raiding local day care centers.The GOP threw a whole lot of money at ICE with the “Big Beautiful Bill.” ICE is throwing a lot of money at potential hires, much to the chagrin of law enforcement agencies everywhere, as well as already-understaffed prisons and jails. The expectations were lowered, along with the bar for entry. The promise of a $50,000 signing bonus has managed to attract the expected blend of MAGA faithful, retired cops, bigots (but I repeat myself…), and anyone who thinks they have what it takes to frog-walk five-year-olds to the nearest rented SUV. Hiring a bunch of people is only half the battle. The next step is preparing them to do their jobs. Ridding the nation of migrants is Job #1 here in America at the moment. But it’s a job too important to be done well by fully trained ICE officers. Quantity over quality is the name of the game, as former ICE trainer Ryan Schwank — who only resigned  — told Congress. Here’s the Washington Post with the details:[S]chwank… told congressional Democrats at a hearing that the agency eliminated 240 hours of “vital classes” from a mandatory 580-hour training program, including instruction about the legal boundaries for the use of force, how to safely handle firearms, and the proper way to detain and arrest immigrants.Seems like a pretty sweet deal for new hires. Not only do they get a hefty signing bonus, but they don’t have to go through 40% of the training. Giving people guns and the power to deprive others of their lives and liberties should mean giving them the best possible training you can in hopes of heading off… well… pretty much everything we’ve been seeing lately during Trump’s anti-blue state/city surges. Certainly, the administration is already doing everything it can to undercut the credibility of its now-former employee. But it’s not going to work unless you’re an idiot who prefers nigh-incoherent MAGA invective to actual facts. You see, Schwank didn’t just bring  to this hearing. He also brought receipts. Ahead of the hearing, Schwank provided a joint panel of House and Senate Democrats copies of internal ICE documents that he said show the extent of the cuts. The documents indicated that the Federal Law Enforcement Training Centers in Glynco, Georgia, shortened its training program from 72 days to 42 days.Except that’s not what Todd Lyons said to Congress when he was asked to testify following two murders committed by federal officers in Minneapolis, Minnesota. Lyons claimed nothing important had been cut. He also said this, which is directly contradicted by the documents Schwank gave to lawmakers: In a statement Monday, the Department of Homeland Security said ICE recruits receive 56 days of training before beginning their assignments…Lyons went on to say that an “average” of 28 days of additional “on the job training” occurs after that. But even if you might believe Lyons is stacking his apples against the apples in ICE’s internal documents to come up with 84 days of training, you’re miscounting apples the same way Lyons is. OJT is post-assignment. Even if you choose to believe Lyons more than the documents his own agency generated, officers are still being put on the street two weeks earlier than they used to be. The lies persist, of course. The statement provided to the Post says something else entirely, which is also contradicted by the documents and statements made by Schwank.“No training hours have been cut. Our officers receive extensive firearm training, are taught de-escalation tactics, and receive Fourth and Fifth Amendment comprehensive instruction,” said Lauren Bis, a spokeswoman for the department.But training hours  been cut. And the rest of it is pure horseshit, especially the part about training new hires about the Fourth and Fifth Amendment. Schwank’s testimony noted that he was asked to review an internal memo signed by Lyons that claimed — untruthfully — that ICE officers can enter people’s homes using only a self-issued administrative warrant. Other rights were similarly glossed over. In fact, some of them are ignored completely. [Schwank] also said that a two-hour class on the rights of protesters was shortened into 10 minutes of discussion during a lecture on “the concept of seizure.”Nice. That’s the way to do it. Goodbye, First Amendment. Instead, here’s 10 minutes of ICE’s interpretative dance: “The Fourth Amendment, And What It Doesn’t Mean To Us.” Everyone knew that once the cattle call began, ICE would be swarmed with opportunists who saw a great way to combine their bigotry and violence tendencies with a career — however short-lived — in public service. A steady paycheck, and all the government asked was that you show up at least 40% of the time during training. This horde (actual number unknown) is being pushed through the doors towards their on-the-job-training, armed with little more than their masks, their guns, and some very comprehensive incorrecting of their constitutional notions.Of course, none of this will matter to the people running ICE. I’m pretty sure they  this press. They have no interest in heading up a finely-tuned precise instrument of immigration law enforcement. They’d rather have the untargeted terrorizing of entire neighborhoods and the unearned swagger of a paramilitary death squad that somehow can still be hooted, horn-honked, and whistled into submission by suburbanites. And maybe this isn’t even an intentional design decision. There’s a good chance no one up top is intelligent enough to recognize the self-destructiveness of their actions. For now, at least, we know what they know. And  know this is wrong. ]]></content:encoded></item><item><title>&apos;The Death of Spotify: Why Streaming is Minutes Away From Being Obsolete&apos;</title><link>https://entertainment.slashdot.org/story/26/02/27/1941205/the-death-of-spotify-why-streaming-is-minutes-away-from-being-obsolete?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 20:51:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a column: I'm going to take the diplomatic hat off here and say with brutal honesty: basically everybody in the music business hates Spotify except for the people who work there. It's a platform that sucks artists for everything they have, it actively prevents community building, and, despite all of that, the platform still struggles to maintain a healthy profit margin. 

The streaming business model is fundamentally broken. And eventually, its demise will become more and more obvious to recognize. I'll break down exactly why the DSP era is coming to a grinding halt, why the major labels are quietly terrified, and why the artists who don't pivot now are going to go down with the ship. 

[...] Jimmy Iovine put it bluntly: "The streaming services have a bad situation, there's no margins, they're not making any money." This model only works for Apple, Amazon, and Google, because they don't need their music platforms to be wildly profitable. Amazon uses music as a loss-leader to keep you paying for Prime. Apple uses it to sell $1,000 iPhones. As for Spotify, or any standalone music streaming company, they're kind of screwed. And guess what -- when the platform's margins are structurally squeezed, guess who gets squeezed first? The artists. 

[...] What if Jimmy is right? If the DSPs are "minutes away from obsolete," what replaces them? Well, I'm not sure the DSPs are going to disappear overnight, but if you're an artist or a manager trying to sustain yourself in this evolving music economy, the answer is direct ownership. The artists who will survive the next five years are the ones who are quietly shifting their focus away from the "ATM Machine." 

They are building their own cultural hangars. They are capturing phone numbers on Laylo. They are driving fans to private Discord servers. They are focusing on ARPF (Average Revenue Per Fan) through high-margin merch, vinyl, and hard tickets, rather than begging for fractions of a penny from a playlist placement. We are witnessing the death of the "Mass Audience" and the birth of the "Micro-Community."]]></content:encoded></item><item><title>AI Mistakes Are Infuriating Gamers as Developers Seek Savings</title><link>https://games.slashdot.org/story/26/02/27/1934258/ai-mistakes-are-infuriating-gamers-as-developers-seek-savings?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 20:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The $200 billion video game industry is caught between studios eager to cut ballooning development costs through AI and a player base that has grown openly hostile to the technology after a string of visible blunders. 

As Bloomberg News reports, Arc Raiders, a surprise hit from Stockholm-based Embark Studios that sold 12 million copies in three months, was briefly vilified online for its robotic-sounding auto-generated voices -- even as CEO Patrick Soderlund insists AI was only used for non-essential elements. EA's Battlefield 6 and Activision's Call of Duty: Black Ops 7 both drew gamer anger this winter over thematically mismatched or poorly generated graphics, and Valve's Steam has added labels to flag games made using AI. 

Some 47% of developers polled by research house Omdia said they expect generative AI to reduce game quality, and PC gamers -- now facing inflated hardware prices from AI-driven demand for graphics chips -- have turned reflexively antagonistic.]]></content:encoded></item><item><title>Musk bashes OpenAI in deposition, saying ‘nobody committed suicide because of Grok’</title><link>https://techcrunch.com/2026/02/27/musk-bashes-openai-in-deposition-saying-nobody-committed-suicide-because-of-grok/</link><author>Sarah Perez</author><category>tech</category><pubDate>Fri, 27 Feb 2026 19:42:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In his lawsuit against OpenAI, Musk touted xAI safety compared with ChatGPT. A few months later, xAI's Grok flooded X with nonconsensual nude images.]]></content:encoded></item><item><title>Intel Releases Updated CPU Microcode For Xeon 6 SoCs &quot;Granite Rapids D&quot;</title><link>https://www.phoronix.com/news/Intel-Microcode-20260227</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 19:35:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Catching me by surprise today was a new Intel CPU microcode drop "20260227" for Linux users/administrators outside of their typical Patch Tuesday alignment for CPU microcode releases...]]></content:encoded></item><item><title>Smartphone Market To Decline 13% in 2026, Marking the Largest Drop Ever Due To the Memory Shortage Crisis</title><link>https://mobile.slashdot.org/story/26/02/27/1917219/smartphone-market-to-decline-13-in-2026-marking-the-largest-drop-ever-due-to-the-memory-shortage-crisis?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 19:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Worldwide smartphone shipments are forecast to decline 12.9% year-on-year (YoY) in 2026 to 1.1 billion units, according to the International Data Corporation (IDC) Worldwide Quarterly Mobile Phone Tracker. This decline will bring the smartphone market to its lowest annual shipment volume in more than a decade. The current forecast represents a sharp decline from our November forecast amid the intensifying memory shortage crisis.]]></content:encoded></item><item><title>Anthropic vs. the Pentagon: What’s actually at stake?</title><link>https://techcrunch.com/2026/02/27/anthropic-vs-the-pentagon-whats-actually-at-stake/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Fri, 27 Feb 2026 19:11:04 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Anthropic and the Pentagon are clashing over AI use in autonomous weapons and surveillance, raising high-stakes questions about national security, corporate control, and who sets the rules for military AI.]]></content:encoded></item><item><title>Verizon Continues To Make Phone Unlocking Annoying (With The Trump FCC’s Help)</title><link>https://www.techdirt.com/2026/02/27/verizon-continues-to-make-phone-unlocking-annoying-with-the-trump-fccs-help/</link><author>Karl Bode</author><category>tech</category><pubDate>Fri, 27 Feb 2026 19:02:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Earlier this year we noted how the Trump FCC, at the direct request of wireless phone giants, destroyed popular phone unlocking rules making it easier and cheaper to switch wireless carriers. The rules, applied via spectrum acquisition and merger conditions after years of activism, required that Verizon unlock your phone within 60 days after purchase so you could easily switch to competitors.Verizon, as we’ve long established, hates competition, and early last year immediately got to work lobbying the Trump administration to destroy the rules (falsely) claiming, without evidence, that the modest phone unlocking requirements were a boon to criminals and scammers. The pay-to-play Trump administration quickly agreed, killed the rules, and shortly thereafter Verizon started telling wireless customers on its many prepaid phone brands (including Tracfone) they had to wait a year before switching phones after purchasing one from Verizon:“While a locked phone is tied to the network of one carrier, an unlocked phone can be switched to another carrier if the device is compatible with the other carrier’s network. But the new TracFone unlocking policy is stringent, requiring customers to pay for a full year of service before they can get a phone unlocked.”“Payments made over the phone also trigger a 35-day waiting period, as do payments made at Verizon Authorized Retailers. Getting an immediate unlock apparently requires paying off the device plan at a Verizon corporate store.”So first, they implemented the most draconian restrictions on its , who tend to be lower income and the most impacted from high prices. Now they’re starting to push restrictions onto their more lucrative postpaid (month to month) customers. Verizon insists (falsely) that these restrictions are necessary to “prevent fraud,” but the real goal is to increase friction when it comes to switching to a competitor. They don’t want the press to outright acknowledge this is anti-competitive in coverage, so they’re engaging in the slow-boiling frog approach that just steadily makes porting your phone out steadily more difficult and annoying. These unlocking conditions were broadly popular, served the public interest, and took decades of activism and reform advocacy to pass. They ensured that it was easier for consumers to switch between our ever-consolidating, anti-competitive wireless phone giants (consolidation directly made possible by the Trump administration’s past rubber stamping of shitty telecom mergers). Verizon lobbied the FCC by repeatedly lying, without evidence, that these conditions resulted in a wave of black market phone thefts. FCC boss Brendan Carr, ever the industry lackey, parroted the lies in his subsequent industry-friendly rulings. You know, to make America great again via “populism” or whatever. Verizon (and Carr) know that there’s a lot going on and the mundanity of a subject like phone unlocking won’t get much attention in the press. Given that the Trump administration has largely lobotomized regulatory independence (at Verizon’s request), there’s very little chance Verizon will see any future accountability, but it’s  that they’re proceeding cautiously just in case.]]></content:encoded></item><item><title>Hyprland 0.54 Released As A &quot;Massive&quot; Update To This Wayland Compositor</title><link>https://www.phoronix.com/news/Hyprland-0.54-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 18:57:14 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Hyprland 0.54 was released today as what's described as a "a massive update with no understatement" to this Wayland compositor...]]></content:encoded></item><item><title>Daily Deal: The 2026 Microsoft Office Pro Bundle</title><link>https://www.techdirt.com/2026/02/27/daily-deal-the-2026-microsoft-office-pro-bundle-2/</link><author>Daily Deal</author><category>tech</category><pubDate>Fri, 27 Feb 2026 18:57:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The 2026 Microsoft Office Pro Bundle has 8 courses to help you master essential Office skills. Courses cover Access, PowerPoint, Word, Excel, and more. It’s on sale for $25.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>Nasa Announces Artemis III Mission No Longer Aims To Send Humans To Moon</title><link>https://science.slashdot.org/story/26/02/27/1854230/nasa-announces-artemis-iii-mission-no-longer-aims-to-send-humans-to-moon?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 18:54:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Nasa announced on Friday radical changes to its delayed Artemis III mission to land humans back on the moon, as the US space agency grapples with technical glitches and criticism that it is trying to do too much too soon. From a report: The abrupt shift in strategy was laid out by the space agency's recently confirmed administrator, Jared Isaacman. Announcing the changes on Friday, he said that Nasa would introduce at least one new moon flight before attempting to put humans back on the lunar surface for the first time in more than half a century, in 2028. 

The new, more incremental approach would give the Nasa team a chance to test flight and refine its technology. As part of the changes, the Artemis II mission to fly humans around the moon this year, without landing, would also be pushed back from its latest scheduled launch on 6 March to 1 April at the earliest. 

"Everybody agrees this is the only way forward," Isaacman told reporters at a news conference. "I know this is how Nasa changed the world, and this is how Nasa is going to do it again."]]></content:encoded></item><item><title>Why Smart People Stay Stuck (and How to Break It)</title><link>https://hackernoon.com/why-smart-people-stay-stuck-and-how-to-break-it?source=rss</link><author>BenoitMalige</author><category>tech</category><pubDate>Fri, 27 Feb 2026 18:45:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most people don’t need therapy.They need to hear themselves think. \n Writing is how you do that.If you’re reading this, .Here’s the second hill I’ll die on:The smartest, most self-aware people are usually the most trapped. \n Because awareness without embodiment is just sophisticated suffering.You don’t need more information.You can already explain yourself better than 99% of people. \n You know the patterns. \n You can name the mechanisms. \n You understand  you do what you do.And yet—there’s this massive, screaming gap between the person you understand yourself to be… and the smaller, quieter, less honest life you keep waking up in.How does someone this self-aware, this intelligent, this capable of naming every variable at play…still wake up trapped in a version of reality they don’t even enjoy?That dissonance isn’t a character flaw. \n It’s not bad timing. \n It’s not that you’re “not ready.”It’s your nervous system refusing to update the identity file while your body hesitates, tightens, and pulls the brakes every time your mind tries to move forward.If your stomach dropped a little just now, good. \n That’s the signal we’re finally past surface-level advice.I’m going to be very direct with you: What you’re about to read explains why you’re still not living the life you already know you’re capable of, even though you understand yourself better than most people ever will.I’ll break down the neuroscience behind that gap of who you are VS who you need to be, and I’ll give you the  I’ve been using to collapse that gap without waiting years for things to “click”.In fact, if you actually apply this, your progress will feel suspicious..especially to the people who benefit from you staying the same.The model we were all taughtFor the longest time, I assumed change was linear.Learn → understand → act → become.And that makes sense, right? It’s logic. That’s how school works:You study the material → You pass the test → You get the diploma → You’re now “qualified.”Same with personal growth: \n You read the book → You gain insight → You apply the advice → You slowly turn into someone new.This model feels safe because it’s orderly. We are taught to follow step one, then step two, then step three until you get a result.Cause → effect. \n Time → progress.So when change doesn’t happen, the conclusion is obvious: \n “I must not have learned enough.” \n “I’m not applying it consistently.” \n “I need more discipline.”That’s the logic everyone operates under.Including you. \n Including me.Let’s get something straight: this model actually works. You didn’t misunderstand it. You didn’t apply it wrong. You executed it .In fact, it gave the results that you have today:You became “successful” on paper.You earned competence, status, and proof that you’re not an idiot.Which is exactly why this is so confusing. Because the model .And yet…there’s this exhausting, permanent tension that never really goes away.You can see the version of yourself you’re capable of being, but you’re not becoming it.And that’s what makes this unbearable. In fact, I would argue that because of your awareness, knowing you’re meant for more is actually the most painful place to be. It was for me.That’s what happens when learning outpaces embodiment.Here’s the truth about change. It’s a biological game, not a motivational one. And biology has rules (constraints that don’t give a shit about your intentions or discipline). If you want to collapse the gap between knowing and becoming, you have to play by them, so let’s break it down.The actual laws of changeThese aren’t feel-good principles or “what worked for me” stories. They’re hardwired constraints on how your nervous system updates identity.If you ignore them, and you stay stuck. When you finally respect them, change accelerates.: Identity isn’t updated by logic or information alone. Your prefrontal cortex can understand a new version of you all day long.But the nervous system (The one that controls your body’s felt reality) only rewires through repeated emotional and sensory experience.It doesn’t care about abstract insights. It responds to what feels familiar, safe, and present-tense.: The nervous system doesn’t fully distinguish between imagined and lived experience. When you vividly rehearse a future state with elevated emotion, it registers as “this is happening now.”Neural pathways fire identically, as shown in fMRI studies on athletes using visualization. Mental practice alone strengthens the same brain regions and even boosts muscle performance without physical movement.Familiarity builds. \n Safety locks in. \n Identity shifts.: The brain ignores future-tense intentions. \n It’s built for survival in the present. \n Promises like “I’ll be that person one day” get dismissed as irrelevant. \n No update happens. \n The body stays braced, attention narrows, and old patterns persist.That’s it. \n No mysticism. \n Just biology: embodiment over intellect, present signals over future plans, emotional repetition over one-off epiphanies.Why the way you were taught violates those lawsYou weren’t taught wrong on purpose. \n The linear model: learn → understand → act → become—makes sense on paper. \n It’s how we’re wired to think: cause leads to effect, effort over time equals results.But here’s the fracture: That system is incompatible with how identity  updates, because it operates in future-tense logic, while your nervous system is locked in .Linear change assumes: \n – Identity updates gradually, after enough time and action. \n – You “work toward” a future version of yourself. \n – The gap closes “eventually,” once you’ve earned it.But biology doesn’t work that way. \n Your nervous system doesn’t live in timelines. \n It doesn’t understand “one day.” \n It only registers what’s safe and familiar.So when you tell yourself: \n “I’ll be confident later.” \n “I’ll act like that person when I get there.” \n “I’m not that version yet.”You’re signaling: “We’re still the old identity. No need to change.” \n And it listens. \n Cortisol stays up. \n The brakes stay on. \n The gap widens.That’s why respecting linear time keeps you trapped. \n Not because time is the enemy. \n But because your biology wasn’t designed to evolve that way.This should now answer your question: If you already know who you want to become… why haven’t you become them yet?How we actually need to treat changeIf the laws demand present-tense embodiment, we need a system that delivers it. \n Not gradual effort toward a distant future. \n Not more information stacked on old identity files.We need to hack the nervous system into believing the new you is already real—now. \n Through vivid, emotional rehearsal that blurs imagined and lived. \n Through small, aligned actions that reinforce familiarity in the present. \n Through evidence that confirms the shift is happening, not waiting to happen.That’s not a tactic. \n It’s alignment with biology.And ignoring biology is exactly why you're still not living the life you want right now.\n This is the  way to collapse the gap without wasting years on “progress” that never sticks.The Protocol I promised youI didn’t set out to invent anything fancy. I was just exhausted from knowing everything and changing nothing. I was the guy who could explain every pattern, name every mechanism, map the perfect future… and still wake up in the same smaller life every day.Once I understood the laws, there were only two options: keep suffering intelligently…or build a system that forced embodiment:Present-tense embodiment. \n Emotional rehearsal. \n Evidence that the shift is real.For years, nothing moved. Under 8k followers total. Newsletter stuck below 900. Random book sales. No momentum.Within 2 months of using the protocol:100k followers across platforms25k newsletter subscribersPublishers reaching out instead of me chasing.I’m saying it because it’s the only way to prove this isn’t theory for me. This is what happened when I finally gave my nervous system present-tense evidence instead of future promises.Everything circled back to those two hills I opened with.Most people don’t need therapy. They need to hear themselves think, and writing is how you do that.The smartest, most self-aware people are usually the most trapped…because awareness without embodiment is just sophisticated suffering.I’d spent years in sophisticated suffering that you are currently experiencing: Explaining, understanding, naming every mechanism, but never embodying.Then I built something that forced embodiment. \n And the suffering ended. \n The gap collapsed.That something is the , because you log both identity (the future you) and evidence (real-world confirmation), like double entry accounting, but for reality.It's a daily protocol that places you between the present and the future, then forces you to pull both toward the center. On paper. Every day. In under 20 minutes.: time collapses, the gap disappears, and your nervous system has no choice but to make the new you real now.Here’s why it aligns with the biology we just covered (not the full how-to; that’s in the videos):It doesn’t ask you to wait for linear time. \n It forces the nervous system to treat the future identity as real .One side of the page creates vivid, emotional rehearsal. The kind that activates the same neural pathways as lived experience, building familiarity and safety without physical action.The other side grounds it immediately: small, present-tense actions that reinforce the new identity , not later.A third layer stacks real-world evidence throughout the day, training the reticular activating system (the RAS) to filter for confirmation instead of threat.Together, they do exactly what the laws demand:Repeated emotional embodiment.No reliance on future promises or gradual progress.The result: the nervous system stops bracing, the brakes come off, and the gap starts collapsing; fast enough that it can feel suspicious to everyone still stuck in linear time:Synchronicities start showing up uninvited.Conversations align without forcing them.Opportunities land in your inbox like they were waiting for you to finally believe they could.Results arrive with almost no effort, as if the universe is just catching up to the identity you’ve already wired in.But I have to warn you about one thing that will happen, and if I don’t tell you, this whole thing won’t work..The moment progress starts moving faster than you thought possible… you will feel .  Like you didn’t “earn” it. Like it was .That guilt is the ghost of linear time (The conditioning that says good things must take years of pain).Don’t let it run the show. \n It’s not proof you’re wrong. \n It’s proof you’ve broken free.This protocol isn’t for everyone. \n It’s for the self-aware overthinkers tired of knowing and not becoming. \n The ones who can name every pattern but can’t break them. \n The ones ready to hack biology instead of fighting it.I put together videos and documents so you can start this habit immediately. You'll get exact walkthroughs, concrete examples of a full day in my life using the protocol, and everything laid out so you can literally copy-paste and execute.This is the thing that finally closes the gap you've felt for years.Because I'm not here to keep you small.See you on the other side.]]></content:encoded></item><item><title>ChatGPT reaches 900M weekly active users</title><link>https://techcrunch.com/2026/02/27/chatgpt-reaches-900m-weekly-active-users/</link><author>Aisha Malik</author><category>tech</category><pubDate>Fri, 27 Feb 2026 18:25:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI shared the new numbers as part of its announcement that it has raised $110 billion in private funding.]]></content:encoded></item><item><title>A Chinese Official&apos;s Use of ChatGPT Accidentally Revealed a Global Intimidation Operation</title><link>https://slashdot.org/story/26/02/27/185250/a-chinese-officials-use-of-chatgpt-accidentally-revealed-a-global-intimidation-operation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 18:03:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A sprawling Chinese influence operation -- accidentally revealed by a Chinese law enforcement official's use of ChatGPT -- focused on intimidating Chinese dissidents abroad, including by impersonating US immigration officials, according to a new report from ChatGPT-maker OpenAI. From a report: The Chinese law enforcement official used ChatGPT like a diary to document the alleged covert campaign of suppression, OpenAI said. In one instance, Chinese operators allegedly disguised themselves as US immigration officials to warn a US-based Chinese dissident that their public statements had supposedly broken the law, according to the ChatGPT user. In another case, they describe an effort to use forged documents from a US county court to try to get a Chinese dissident's social media account taken down. 

The report offers one of the most vivid examples yet of how authoritarian regimes can use AI tools to document their censorship efforts. The influence operation appeared to involve hundreds of Chinese operators and thousands of fake online accounts on various social media platforms, according to OpenAI.]]></content:encoded></item><item><title>Video Friday: Robot Dogs Haul Produce From the Field</title><link>https://spectrum.ieee.org/quadruped-farming-robots</link><author>Evan Ackerman</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NTA5NTkwMy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgyMjcyNTE4Mn0.dDbgl1HCHlc37MS8MkqixiIlFQqAKQj8DlxIi2xRdLc/image.png?width=600" length="" type=""/><pubDate>Fri, 27 Feb 2026 18:00:55 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Your weekly selection of awesome robot videos]]></content:encoded></item><item><title>Metacritic Will Kick Out Media Attempting To Submit AI Generated Reviews</title><link>https://games.slashdot.org/story/26/02/27/1732218/metacritic-will-kick-out-media-attempting-to-submit-ai-generated-reviews?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 17:32:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: While some see AI as a tool to be used, its specific use and how it is deployed responsibly is being heavily debated online across a wide range of industries. In terms of journalistic content, and in this particular instance, reviews, review aggregator Metacritic has taken a firm stance on content published and submitted to their platform, that have been generated by artificial intelligence in some way. 

In a statement by co-founder Marc Doyle, sent to Gamereactor, he says this: "Metacritic has been a reputable review source for a quarter century and has maintained a rigorous vetting process when adding new publications to our slate of critics. However, in certain instances such as a publication being sold or a writing staff having turned over, problems can arise such as plagiarism, theft, or other forms of fraud including AI-generated reviews. Metacritic's policy is to never include an AI-generated critic review on Metacritic and if we discover that one has been posted, we'll remove it immediately and sever ties with that publication indefinitely pending a thorough investigation." 

So, what is this about specifically? Well, it's probably a sound guess, that this pertains to Videogamer's review of Resident Evil 9: Requiem, which was removed from the platform after a barrage of comments accusing the review of being AI-written, and for the author of being made up.]]></content:encoded></item><item><title>Palantir Sues Swiss Magazine For Accurately Reporting That The Swiss Government Didn’t Want Palantir</title><link>https://www.techdirt.com/2026/02/27/palantir-sues-swiss-magazine-for-accurately-reporting-that-the-swiss-government-didnt-want-palantir/</link><author>Mike Masnick</author><category>tech</category><pubDate>Fri, 27 Feb 2026 17:23:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[If you run a company whose entire value proposition is the ability to see patterns, predict outcomes, and connect dots that others miss, you’d think someone in the building might have flagged that suing a small independent magazine over unflattering-but-accurate reporting would only guarantee that millions more people read it.Palantir Technologies, the infamous surveillance and data analytics giant chaired by Peter Thiel, has filed a lawsuit against Republik, a small Swiss online magazine, over a pair of investigative articles published in December. The articles, produced in collaboration with the investigative collective WAV, detailed a years-long, multi-ministry charm offensive by Palantir to sell its software to Swiss federal authorities. The campaign was, by all accounts, a comprehensive failure. Swiss agencies rejected Palantir at least nine times, with concerns ranging from data sovereignty to reputational risk to the simple fact that nobody needed the product.The reporting was based on documents obtained through 59 freedom of information requests filed with Swiss federal agencies. The key finding was an internal Swiss Armed Forces report that concluded Palantir’s software posed unacceptable risks because sensitive military data could potentially be accessed by U.S. government intelligence agencies. As the Republik article details:The authors of the report state that using Palantir’s software would increase dependence on a U.S. provider. It also poses the risk of losing data sovereignty and thereby national sovereignty.Above all, however, the army’s staff experts say it remains unclear who has access to data shared with Palantir. The following sentence from the Swiss Army report is particularly relevant: “Palantir is a U.S.-based company, which means there is a possibility that sensitive data could be accessed by the US government and intelligence services.”As if it’s any sort of surprise that European governments are wary of betting on US tech companies with close ties to the US government. It’s not like reports of US spies co-opting US tech companies for surveillance efforts haven’t been front page news over the past twenty years. And now,  administration—with its willingness to antagonize everyone in Europe, and its close ties to Palantir and Thiel? It’s no freaking wonder that the Swiss government was like “yo, maybe pass.”So how does a sophisticated data intelligence company respond to well-sourced investigative journalism based on official government documents?By suing the journalists, of course.But here’s the thing that makes this even more absurd: Palantir isn’t even claiming the articles are false. The company isn’t suing for defamation. It isn’t seeking damages. Instead, it’s invoking a Swiss “right of reply” statute, alleging that Republik didn’t give the company a sufficient opportunity to respond. Palantir wants the court to force the magazine to publish lengthy counter-statements to each article.Palantir’s lawsuit, filed in January, is not seeking damages or making libel claims against Republik, but instead alleges that the company was not given sufficient right to reply under Swiss media law. The company objects to Republik’s presentation of the public documents and believes its right to reply has been wrongfully denied.Republik’s managing director Katharina Hemmer said Palantir had wanted the magazine to publish a very lengthy counterstatement to each article. Republik believed the proposed statements did not fairly address or rebut the reporting, she said, adding that the magazine stands by its reporting.To which I say: good. Because Palantir’s demand here is absurd. Oh boo-fucking-hoo, the big defense contractor didn’t like the coverage? Pull on your big boy pants and get over it. Switzerland’s right of reply law exists so people can correct factual errors, not so corporations can force publications to run PR copy because they didn’t like the tone of accurate, document-based reporting.And it’s worth noting: Palantir has already used other avenues to respond. The company published a blog post complaining that the Republik article “paints a false and misleading picture” and “hinders important discussions about the modernization of European software.” They’ve got the platform. If Palantir wants to push back on the story, they have many methods of doing so. Hell, they can do so on X any time they want—on what Musk and company like to call the global town square for free speech.But that’s apparently not enough. Instead, a multibillion-dollar American defense and intelligence contractor is hauling a small independent Swiss magazine into court, not because anything the magazine published was wrong, but because Palantir wants to  the publication to run its talking points under legal compulsion.Compelled speech isn’t free speech, guys. And this is nothing more than a blatant intimidation campaign to frighten away reporters from reporting the truth about Palantir.The European Federation of Journalists has called this exactly what it is: a SLAPP suit—a strategic lawsuit against public participation, designed to use the weight and cost of litigation to intimidate and punish journalists for doing their jobs.“The investigation conducted by WAV and Republik into Palantir is largely based on official documents that journalists were able to access thanks to Swiss freedom of information law,” notes EFJ President Maja Sever. “The legal action brought by this powerful multinational firm against a small Swiss media start-up is, in our view, an attempt at intimidation aimed at discouraging any critical analysis of Palantir’s activities.”And in case you didn’t catch the irony: the Swiss military rejected Palantir in part because of fears about a heavy-handed American entity with uncomfortably close ties to U.S. intelligence. Palantir’s response to the reporting of that rejection? Behave like a heavy-handed American entity trying to bully a small foreign publication into submission. If anyone at Palantir had run this decision through their own pattern-recognition software, you’d hope a few red flags would have popped up.Meanwhile, the lawsuit has done exactly what anyone with a passing familiarity with the Streisand Effect could have predicted. The original Republik articles were about the Swiss government politely but firmly declining Palantir’s advances—an embarrassing but relatively contained story.Now, thanks to the lawsuit, the story has gone international. The Financial Times is covering it. The European Federation of Journalists is covering it. A UK member of parliament has already cited the Republik investigation during a debate on British defense contracts with Palantir, using the story to suggest that the British government “pivot away” from Palantir.It paints a picture of a company that spent seven years working every angle to get Swiss federal agencies to buy its products—approaching the Federal Chancellery during COVID, pitching the Federal Office of Public Health on contact tracing, presenting anti-money laundering software to financial regulators, making repeated runs at the military—and getting turned away at every door. Sometimes embarrassingly, such as the Federal Statistical Office director apparently just ignoring Palantir’s outreach entirely.For a company that brags about its ability to “optimize the kill chain” and whose CEO once told investors that “Palantir is here to disrupt… and, when it’s necessary, to scare our enemies and occasionally kill them,” getting politely rejected by the Swiss statistical office has to sting a little.But suing the journalists who reported on it? When the entire basis of your lawsuit is “we want you to publish our talking points” rather than “anything you published was wrong,” it makes pretty clear you don’t actually have a substantive response to the reporting. If Palantir thinks the picture is false, the remedy is to demonstrate that the documents are wrong—not to drag a small magazine through expensive litigation until it capitulates or goes broke.Seriously, how fucking fragile are the egos in the Palantir executive suite that they can’t handle a bit of mildly embarrassing reporting? Grow up.A Zurich court is expected to rule on the case in March. Whatever the outcome, Palantir has already lost the only contest that matters: the one for public perception. For a company that sells the ability to see around corners, they apparently never thought to search “The Streisand Effect.”]]></content:encoded></item><item><title>AI music generator Suno hits 2M paid subscribers and $300M in annual recurring revenue</title><link>https://techcrunch.com/2026/02/27/ai-music-generator-suno-hits-2-million-paid-subscribers-and-300m-in-annual-recurring-revenue/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Fri, 27 Feb 2026 17:22:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Suno lets users create music using natural language prompts, making it possible for people with little experience to generate audio with little effort.]]></content:encoded></item><item><title>Apple and Netflix team up to air Formula 1 Canadian Grand Prix</title><link>https://techcrunch.com/2026/02/27/apple-and-netflix-team-up-to-air-formula-1-canadian-grand-prix/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Fri, 27 Feb 2026 17:17:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[As Netflix continues its live sports push, the company has partnered with Apple to air the Formula 1 Canadian Grand Prix.]]></content:encoded></item><item><title>The New Gold Standard - Why Imperfection Wins</title><link>https://hackernoon.com/the-new-gold-standard-why-imperfection-wins?source=rss</link><author>Editing Protocol</author><category>tech</category><pubDate>Fri, 27 Feb 2026 17:15:42 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Earlier this week in our HackerNoon editorial meeting, we noticed an interesting pattern. We were reviewing the top-performing stories from the past two weeks, and couldn’t help but notice an interesting trend.\
Are you seeing what we’re seeing? Here’s a hint—the leaderboard was full of specific, lived experiences. We’ve got an artist talking about making digital art in the age of AI; an observation on why people were panic-buying Mac minis; a PSA on online phishing scams; and a new, well-thought-out social media strategy.\
This sparked a realization that makes perfect sense in hindsight: Genuine human experience is the way to go.\
Scroll through tech blogs today, and you’ll notice many similarities: clear, competent, and often a little interchangeable. With so much polished writing available, what tends to stand out now isn’t more shine, but specificity: the odd detail, the honest constraint, the lesson learned the hard way.\
The bar for clean, professional writing is higher than ever - which is a  thing, btw; but in today’s world, in order to differentiate, connect, and earn attention, concrete examples, candid tradeoffs, and seemingly all kinds of small imperfections that signal there’s a person behind the words are increasingly important.Modern audiences have adapted quickly to the AI era. A recent poll on HackerNoon showed that around  of respondents are somewhat confident in their ability to spot AI-generated content.\
Of course, this confidence is only based on a gut feeling when something reads a little too smoothly, or has what people call “AI giveaways”. However, this vigilance has created a fascinating authenticity paradox for writers and marketers. Sometimes, perfectly structured sentences and high vocabulary now trigger suspicion rather than establishing authority.​\
When a reader encounters a paragraph that flows without any jagged edges, strong opinions, or a distinct voice, their brain files it away as generic content rather than meaningful communication. To prove you are genuinely human, you need to showcase the elements of your work that cannot be automated.It’s the Human Proof of Work.\
This does not mean publishing sloppy or unedited drafts. It means leaning into specific details: acknowledging a slightly offbeat workflow, admitting that a product roadmap changed because a previous assumption was wrong, or sharing the messy reality of a project.​So, Where Do You Find These Genuine, Imperfect Ideas?The editorial ethos we often discuss at HackerNoon highlights that writers waste hours overthinking the perfect topic instead of simply documenting reality. Your next high-performing article is likely hiding in a recent Slack debate, a code deployment that required a hotfix, or a frustrating client call that got you thinking.​\
By tapping into these unfiltered moments of daily friction, you generate inherently unique content that AI cannot pull from a training dataset. When you share lessons from your own experiences, you bypass the reader's critical filter we mentioned above. You position your brand as a battle-tested guide who has actually navigated the practical challenges of the industry.Marketing and Retention Through Authentic ConnectionThis pivot toward genuine human connection is more than just an editorial preference. Authenticity is rapidly becoming a primary driver of long-term business retention.​\
Many B2B brands currently find themselves stuck in a marketing uncanny valley. They publish heavily optimized pieces that sound vaguely human but lack the emotional resonance required to actually connect with a reader. These articles might successfully attract search traffic and earn a click, but they rarely earn a loyal customer.Retention in any business is built on trust, and trust requires relatability.\
It’s simple: customers stick around because they feel they are buying into a philosophy and a team they understand, rather than just renting a software tool.\
Treating your content's humanity as a core asset is a valuable tip that should always be taken to your advantage. The willingness to be vulnerable and own your operational realities creates a deep customer affinity. This affinity directly translates into a higher customer retention.In a digital environment populated by perfect machines, writing like a flawed human might just be the best business strategy you have.Want to take this further? (HackerNoon’s Blogging Course)HackerNoon’s Blogging Course is designed for beginners  writers who’ve published a bit and want to level up. It’s organized into 8 modules created by experienced writers and editors, and it includes topics like:Until next time, Hackers!]]></content:encoded></item><item><title>Perplexity’s new Computer is another bet that users need many AI models</title><link>https://techcrunch.com/2026/02/27/perplexitys-new-computer-is-another-bet-that-users-need-many-ai-models/</link><author>Tim Fernholz</author><category>tech</category><pubDate>Fri, 27 Feb 2026 17:00:55 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Perplexity Computer, in the company’s words, "unifies every current AI capability into a single system." ]]></content:encoded></item><item><title>HackerNoon Projects of the Week: Get-Star, FinSight and CodeXero</title><link>https://hackernoon.com/hackernoon-projects-of-the-week-get-star-finsight-and-codexero?source=rss</link><author>Proof of Usefulness</author><category>tech</category><pubDate>Fri, 27 Feb 2026 17:00:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Welcome to the latest HackerNoon Projects of the Week installment. Each week, we shine a light on standout projects from our Proof of Usefulness Hackathon—a contest built around the core question every builder should answer: Is my product actually useful in the real world?For each edition, we’ll highlight projects that demonstrate clear usefulness, technical execution, and real-world impact; all backed by data and not witty buzzwords.This week, we’re excited to share three projects that have proven their utility by solving concrete problems for real users: Get-Star, FinSight, and CodeXero.Meet the Projects of the WeekGet-Star is building client-side parallel search infrastructure designed to improve speed and performance without relying heavily on centralized back-end computation. By distributing search execution closer to the user, the project aims to reduce latency, improve responsiveness, and create a more scalable search experience for modern web applications.In a digital environment where milliseconds shape user perception, Get-Star focuses on performance as product value — giving developers a way to rethink how search is handled at the architectural level.Proof of Usefulness score: +27/1000FinSight is an AI-powered financial management system built specifically for small businesses. It helps founders move beyond static spreadsheets by providing real-time insights, forecasting, and structured financial analysis in one unified platform.Small business operators often lack the time or expertise to interpret financial signals clearly. FinSight positions itself as a decision-support engine — translating raw financial data into actionable clarity that can guide smarter planning and healthier cash flow management.Proof of Usefulness score: +55/1000CodeXero is building a “vibe coding” engine for Web3 dApps — a system designed to accelerate decentralized application development by blending AI-assisted workflows with blockchain infrastructure.With a significantly higher Proof of Usefulness score this week, CodeXero demonstrates strong traction in helping developers reduce friction when building smart contracts and Web3 interfaces. By simplifying complex blockchain logic into more intuitive development flows, CodeXero aims to make decentralized development faster, more accessible, and more iterative.Proof of Usefulness score: +348/1000Stop Building in the Dark - Get Scored!The web is drowning in vaporware and empty promises. We created Proof of Usefulness to reward what actually matters: real user adoption, sustainable revenue, and technical stability. \n  Get your Proof of Usefulness score (from -100 to +1000) the moment you submit. \n  Compete for $20K in cash and $130K+ in software credits from , , , , and . \n 3. Built-in Distribution: Your submission becomes a HackerNoon story, putting your build in front of millions of monthly readers. \n  Every qualifying participant unlocks a suite of software credits just for entering. Head to www.proofofusefulness.com and submit your project details to generate your PoU Report Card. \n  Click the button on your report page to convert your submission into a HackerNoon blog post draft. \n  Edit your draft to add your technical "secret sauce," then hit Submit for Review. Once published, you’re officially in the prize queue! \n \
P.S. The clock is ticking! The second month of the competition is drawing to a close, meaning the next round of winners will be announced soon. With only 4 months and 4 prize rounds remaining, now is the time to get your project in the mix. Don't leave money on the table - get in early!Thanks for building useful things! \n P.S. Submissions roll monthly through June 2026. Get in early!]]></content:encoded></item><item><title>Pokémon Winds and Pokémon Waves are coming to the Nintendo Switch 2 in 2027</title><link>https://techcrunch.com/2026/02/27/pokemon-winds-and-pokemon-waves-are-coming-to-the-nintendo-switch-2-in-2027/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Fri, 27 Feb 2026 16:57:21 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The 10th-generation starter Pokémon were revealed in the trailer: Browt (a grass bird), Pombon (a fire puppy), and Gecqua (a water gecko). ]]></content:encoded></item><item><title>Sam Altman Says OpenAI Shares Anthropic&apos;s Red Lines in Pentagon Fight</title><link>https://slashdot.org/story/26/02/27/1530218/sam-altman-says-openai-shares-anthropics-red-lines-in-pentagon-fight?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: OpenAI CEO Sam Altman wrote in a memo to staff that he will draw the same red lines that sparked a high-stakes fight between rival Anthropic and the Pentagon: no AI for mass surveillance or autonomous lethal weapons. If other leading firms like Google follow suit, this could massively complicate the Pentagon's efforts to replace Anthropic's Claude, which was the first model integrated into the military's most sensitive work. It would also be the first time the nation's top AI leaders have taken a collective stand about how the U.S. government can and can't use their technology. 

Altman made clear he still wants to strike a deal with the Pentagon that would allow ChatGPT to be used for sensitive military contexts. Despite the show of solidarity, such a deal could see OpenAI replace Anthropic if the Pentagon follows through with its plan to declare the latter a "supply chain risk."]]></content:encoded></item><item><title>Behind the Blog: Using Your Brain</title><link>https://www.404media.co/behind-the-blog-using-your-brain/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/02/nl2.27--1-.png" length="" type=""/><pubDate>Fri, 27 Feb 2026 16:27:13 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[This is Behind the Blog, where we share our behind-the-scenes thoughts about how a few of our top stories of the week came together. This week, we discuss wishes made, god complexes, and the point of it all.  This week I wrote about Amazon’s changing policy for wishlists. It’s allowing gifters to choose third-party sellers for items, which could expose recipients’ delivery addresses to the gifter. The notice Amazon sent wishlist holders is a basic example of CYA messaging: Amazon can’t guarantee what a third party seller will do with your address once they have it, including giving it to a gifter for tracking purposes.Sex workers first flagged this change on social media because many use wishlists as an easy way to accept gifts, tributes, tips, etc instead of or in addition to actual funds. This is important because payment processors are wildly hostile and actively discriminatory toward the adult industry, and having alternative ways to get paid is crucial if you’re debanked or banned from the usual payment processors. I think most use it in a supplementary fashion, though.]]></content:encoded></item><item><title>Employees at Google and OpenAI support Anthropic’s Pentagon stand in open letter</title><link>https://techcrunch.com/2026/02/27/employees-at-google-and-openai-support-anthropics-pentagon-stand-in-open-letter/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Fri, 27 Feb 2026 16:23:58 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[While Anthropic has an existing partnership with the Pentagon, the AI company has remained firm that its technology not be used for mass domestic surveillance or fully autonomous weaponry.]]></content:encoded></item><item><title>Lawmakers Demand DHS Define ‘Domestic Terrorist’ As It Uses Vast Array of Surveillance Tools</title><link>https://www.404media.co/lawmakers-demand-dhs-define-domestic-terrorist-as-it-uses-vast-array-of-surveillance-tools/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/02/54977663606_074404d0ae_k.jpg" length="" type=""/><pubDate>Fri, 27 Feb 2026 16:10:27 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[A group of more than a dozen Democratic lawmakers have demanded the Department of Homeland Security (DHS) provide its definition of “domestic terrorist,” after the agency labelled U.S. citizens Renée Good and Alex Pretti, which DHS officers killed, as such. The move also comes as DHS and its various components purchase and deploy a wide range of surveillance technologies and demand sensitive information from tech companies to unmask people criticizing ICE.“You and your underlings appear to be labeling untold numbers of people as ‘domestic terrorists’ or individuals of concern at will without evidence, operating wildly invasive spy tools to identify targets—and then using such labels as an excuse for yet more surveillance,” the letter, addressed to DHS Secretary Kristi Noem, reads. The office of Rep. Bennie G. Thompson (D-MS), ranking member of the Committee on Homeland Security, shared a copy of the letter with 404 Media.“This self-reinforcing spiral of civil liberties violations ratchets in only one direction: toward an authoritarian surveillance state that punishes dissent and inflicts state violence,” the letter adds.Do you work at DHS? I would love to hear from you. Using a non-work device, you can message me securely on Signal at joseph.404 or send me an email at joseph@404media.co.It then points to a long list of media reports, including 404 Media’s, about DHS’s increasing use of surveillance technologies and powers. Those include how DHS shared a memo with employees in Minneapolis telling them to capture images, license plates, and other information about protestors “so we can capture it all in one consolidated form”;  that Palantir is working on a tool called ELITE for ICE that provides a confidence score on targets’ addresses; ICE’s purchase of smartphone location data; how ICE agents told legal observers they were identifying them with facial recognition technology; and several more examples.“The Department’s opaque, mass expansion of spy tools and framing of protesters, photographers, political opponents, and passersby as enemies of the state leans into people’s worst fears of a surveillance state. Your weaponization of DHS undercuts decades of effort to develop a Department that responsibly balances security with privacy and civil liberties protections and transparency,” the letter reads.  It then includes a list of demands for information from DHS. Many of them are about the legal regime behind those surveillance powers, and the technical infrastructure and policies related to them. One asks DHS for “Documentation of the Department’s definition of the term ‘domestic terrorist,’ a copy of the policies in place that permit Departmental designations of United States persons as a ‘domestic terrorist,’ and a description of the consequences of such a designation.”“Your actions are abhorrent, blatantly unconstitutional, and corrosive to the functioning of a peaceful society. They cannot stand. Accountability is coming,” the letter adds.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Lessons from Building a 100+ Agent Swarm in Web3 (2/27/2026)</title><link>https://hackernoon.com/2-27-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Fri, 27 Feb 2026 16:02:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, February 27, 2026?By @mattleads [ 11 Min read ] Master Symfony 7.4 logging: 10 advanced Monolog patterns. Use FingersCrossed, JSON  Attributes to turn text logs into actionable observability data Read More.By @johnpphd [ 4 Min read ] How precompiling context for AI agents beats context stuffing. Lessons from building 100+ specialized agents for a web3 application. Read More.By @benoitmalige [ 6 Min read ] Procrastination isnt laziness—its your brain dodging uncomfortable feelings like fear of failure, judgment, or misalignment. Read More.By @nickzt [ 5 Min read ] Scaling AI for the real world requires peeling back the layers of abstraction weve gotten too comfortable with. Read More.By @ArunDHANARAJ_gfaknebg [ 14 Min read ] Compare Claude Opus 4.6 and GPT‑5.3 Codex across reasoning, coding, benchmarks, pricing, and safety to guide enterprise AI and agentic workload decisions.

 Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Symfony 7.4: 10 Advanced Logging Patterns You Should Know About</title><link>https://hackernoon.com/symfony-74-10-advanced-logging-patterns-you-should-know-about?source=rss</link><author>MattLeads</author><category>tech</category><pubDate>Fri, 27 Feb 2026 16:00:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Logging is the heartbeat of a production application. In the early days of a project, a simple  tail is sufficient. But as your Symfony application scales to handle ,  and , “writing to a file” becomes a liability rather than an asset.\
The  offers sophisticated tools to transform logs from simple text streams into structured, actionable observability data.\
This guide explores 10 advanced logging patterns that go beyond the defaults. We will use strict typing, PHP Attributes, and modern YAML configuration. (for alerting examples)Scenation 1. The “Black Box” Recorder: FingersCrossed HandlerYou want detailed debug logs when an error occurs to understand the sequence of events leading up to it, but you can’t afford the disk I/O to log debug messages for every successful request in production.\
The  buffers all logs in memory during the request. If the request finishes successfully, the buffer is discarded. If an error (or a specific threshold) is reached, the entire buffer (including previous debug logs) is flushed to the persistence handler.config/packages/prod/monolog.yaml:monolog:
    handlers:
        main:
            type: fingers_crossed
            # The strategy: "error" means if an ERROR occurs, dump everything.
            action_level: error
            # Where to dump the logs if the threshold is met
            handler: nested
            # Optional: Keep a small buffer size to prevent memory leaks in long processes
            buffer_size: 50
        nested:
            type: stream
            path: "%kernel.logs_dir%/%kernel.environment%.log"
            level: debug
\
You’ll get the forensic detail of debug-level logging exactly when you need it — during a crash — without filling your disk with noise during normal operations.Scenario 2. Segregated Channels: The “Payment” LogYour  is a mix of Doctrine queries, router matching, and critical business logic. You need a dedicated file for financial transactions that can be audited separately.\
Create a custom Monolog Channel.config/packages/monolog.yaml:monolog:
    channels: ['payment'] # Register the channel

    handlers:
        payment:
            type: stream
            path: "%kernel.logs_dir%/payment.log"
            level: info
            channels: ["payment"] # Only listen to this channel

        main:
            type: stream
            path: "%kernel.logs_dir%/%kernel.environment%.log"
            level: debug
            channels: ["!payment"] # Exclude payment logs from the main file
Inject the logger specifically for this channel using the Target attribute (available since Symfony 5.3+).namespace App\Command;

use Psr\Log\LoggerInterface;
use Symfony\Component\Console\Attribute\AsCommand;
use Symfony\Component\Console\Command\Command;
use Symfony\Component\Console\Input\InputInterface;
use Symfony\Component\Console\Output\OutputInterface;
use Symfony\Component\DependencyInjection\Attribute\Target;

#[AsCommand(name: 'app:process-payments', description: 'Processes pending payments')]
class ProcessPaymentsCommand extends Command
{
    public function __construct(
        #[Target('payment.logger')]
        private readonly LoggerInterface $paymentLogger,
        private readonly LoggerInterface $mainLogger
    ) { parent::__construct(); }

    protected function execute(InputInterface $input, OutputInterface $output): int
    {
        $this->mainLogger->info('Cron job app:process-payments started.');

        $amounts = [10.50, 99.99, 45.00];
        foreach ($amounts as $amount) {
            $this->paymentLogger->info('Processing payment', ['amount' => $amount, 'status' => 'success']);
        }

        $this->mainLogger->info('Cron job finished.');
        return Command::SUCCESS;
    }
}
Run the Command. You will see  created in  containing only these specific entries.Scenario 3. Context Enrichment: The #[AsMonologProcessor] AttributeLogs are useless if you can’t correlate them to a specific user or request ID. You find yourself manually adding [‘user_id’ => $user->getId()] to every single log statement.\
A global Processor can automatically inject context into every log record.namespace App\Log;

use Monolog\Attribute\AsMonologProcessor;
use Monolog\LogRecord;

#[AsMonologProcessor]
class RequestContextProcessor
{
    public function __invoke(LogRecord $record): LogRecord
    {
        // Simulated context since CLI commands don't have HTTP Requests
        $extra = [
            'pid' => getmypid(),
            'user' => get_current_user(),
        ];

        return $record->with(extra: array_merge($record->extra, $extra));
    }
}
In , . We use  to return a modified copy.A developer accidentally logs a user object, dumping PII (Personally Identifiable Information) or credit card numbers into the logs, violating GDPR/PCI-DSS.\
A specialized processor can scans the context array and mask sensitive keys.namespace App\Log;

use Monolog\Attribute\AsMonologProcessor;
use Monolog\LogRecord;

#[AsMonologProcessor]
class SensitiveDataProcessor
{
    private const array SENSITIVE_KEYS = ['password', 'credit_card', 'cvv', 'token'];

    public function __invoke(LogRecord $record): LogRecord
    {
        $context = $record->context;

        foreach ($context as $key => $value) {
            if (in_array($key, self::SENSITIVE_KEYS, true)) {
                $context[$key] = '***REDACTED***';
            }
        }

        return $record->with(context: $context);
    }
}
$logger->info('User login', ['password' => 'secret123']);
// Output in log: "User login" {"password": "***REDACTED***"}
Scenario 5. Structured Logging: JSON for ELK/DatadogParsing multi-line text logs (like stack traces) in  or  is painful. Regex parsers break easily.\
You can output logs as JSON lines. This allows log aggregators to natively index fields like .config/packages/monolog.yaml:monolog:
    handlers:
        json_report:
            type: stream
            path: "%kernel.logs_dir%/app.json"
            level: info
            formatter: monolog.formatter.json
            channels: ["!payment", "!event"]
\
Open . The output should look like:{"message":"Order created","context":{"id":123},"level":200,"channel":"app","datetime":"..."}
Scenario 6. Spam Prevention: The Deduplication HandlerYour database goes down. Your application receives  in a minute. Your “Email on Error” handler sends you , getting your SMTP server blacklisted and flooding your inbox.\
The  can aggregate identical log records and send a single summary.config/packages/monolog.yaml:monolog:
    handlers:
        deduplication:
            type: deduplication
            handler: nested_dedup
            buffer_size: 60
            time: 60
            level: error
            channels: ["!console"]
\
If the DB crashes, you receive one email every 60 seconds listing all occurrences, rather than one email per request.Scenario 7. Dynamic Log Levels (Runtime Debugging)A specific customer is reporting an issue in production. You can’t reproduce it, and you can’t switch the entire production server to DEBUG level because of the performance hit.\
Use an  to switch the log level dynamically based on a request header.Create a custom strategy:namespace App\Command;

use Psr\Log\LoggerInterface;
use Symfony\Component\Console\Attribute\AsCommand;
use Symfony\Component\Console\Command\Command;
use Symfony\Component\Console\Input\InputInterface;
use Symfony\Component\Console\Input\InputOption;
use Symfony\Component\Console\Output\OutputInterface;

#[AsCommand(name: 'app:dynamic-debug', description: 'Tests dynamic log level activation')]
class DynamicDebugCommand extends Command
{
    public function __construct(private readonly LoggerInterface $logger) {
        parent::__construct();
    }

    protected function configure(): void
    {
        $this->addOption('force-debug', null, InputOption::VALUE_NONE, 'Force debug logging for this run');
    }

    protected function execute(InputInterface $input, OutputInterface $output): int
    {
        if ($input->getOption('force-debug')) {
            $output->writeln('Debug mode forced via option. (Simulated, as Monolog ActivationStrategy relies on Http/Request state typically. But you can add processors/handlers dynamically in real apps based on this flag).');
        }

        $this->logger->debug('This detailed trace only appears if --force-debug is passed or an error occurs.');
        $this->logger->info('Standard processing information.');

        return Command::SUCCESS;
    }
}
Scenario 8. Messenger Logging: Worker ContextLogs from  are hard to trace. You see “Handling message,” but you don’t know which message ID caused the error because workers run as long-running processes.\
Use  to inject the Message ID into the Monolog context specifically for the worker process.namespace App\EventListener;

use Psr\Log\LoggerInterface;
use Symfony\Component\EventDispatcher\Attribute\AsEventListener;
use Symfony\Component\Messenger\Event\WorkerMessageReceivedEvent;

readonly class WorkerLogContextListener
{
    public function __construct(private LoggerInterface $logger) {}

    #[AsEventListener]
    public function onMessageHandling(WorkerMessageReceivedEvent $event): void
    {
        $this->logger->info('Worker started message', [
            'message_class' => $event->getEnvelope()->getMessage()::class,
        ]);
    }
}
Scenario 9. Excluding 404s from Error LogsBots scanning your site for  or  generate thousands of 404 NotFoundHttpException logs. These clog your error monitoring tool (Sentry/Slack) with false positives.\
Use the channels exclusion or a specific configuration to ignore bounced logs, or better - configure the  to be ignored by the main error handler.config/packages/monolog.yaml:monolog:
    handlers:
        fingers_crossed:
            type: fingers_crossed
            action_level: error
            handler: nested
            excluded_http_codes: [404, 405]
            buffer_size: 50
Scenario 10. Notifier Bridge: ChatOpsEmail alerts are slow and often ignored. You want critical infrastructure failures to ping a  channel immediately.\
Use  bridged with .composer require symfony/notifier symfony/slack-notifier
config/packages/monolog.yaml:monolog:
    handlers:
        slack_alerts:
            type: service
            id: Symfony\Bridge\Monolog\Handler\NotifierHandler
            level: critical
\
Then configure the notifier chatter in config/packages/notifier.yaml and your DSN in .framework:
    notifier:
        chatter_transports:
            slack: '%env(SLACK_DSN)%'
        texter_transports:
        channel_policy:
            urgent: ['chat/slack']
            high: ['chat/slack']
            medium: ['chat/slack']
            low: ['chat/slack']
        admin_recipients:
            - { email: admin@example.com }
\
 maps log levels to Notifier importance. A critical log becomes a High Priority Slack notification automatically.Logging is not a byproduct of code - it is a feature of your infrastructure.\
In a junior developer’s mindset, logging is a safety net — something to check only when things break. But as you scale to Senior and Lead roles, your perspective must shift. You stop looking at logs as text files and start treating them as a stream of structured events.\
By moving to Symfony 7.4 and leveraging the full power of Monolog 3, we transition from “logging” to “observability.”\
 turns your logs into a queryable database.\
 handlers solve the “signal-to-noise” ratio, saving you gigabytes of storage while preserving critical context.\
 ensuring every log entry carries the DNA of the request (User ID, Request ID) turn hours of debugging into minutes of verification.\
 protects your inbox and your sanity.\
Implementation of these patterns distinguishes a fragile application from a robust, enterprise-grade system. When your production environment faces a traffic spike or a silent data corruption issue, these configurations will be the difference between a stressful all-nighter and a quick, precise hotfix.If you found this helpful or have questions about the implementation, I’d love to hear from you. Let’s stay in touch and keep the conversation going across these platforms:]]></content:encoded></item><item><title>Netflix Ditches deal for Warner Bros. Discovery After Paramount&apos;s Offer is Deemed Superior</title><link>https://entertainment.slashdot.org/story/26/02/27/1027259/netflix-ditches-deal-for-warner-bros-discovery-after-paramounts-offer-is-deemed-superior?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 16:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Netflix is walking away from a deal to buy Warner Bros. Discovery's studio and streaming assets after the WBD board on Thursday deemed a revised bid by Paramount Skydance to be a superior offer. From a report: Earlier this week, Paramount raised its bid to buy the entirety of WBD to $31 per share, up from $30 per share, all cash. It was the latest amendment to Paramount's multiple offers in recent months -- and since moving forward with a hostile bid to buy the company -- and it's now unseated a deal between WBD and Netflix to sell the legacy media company's studio and streaming businesses for $27.75 per share. 

Last week, Netflix granted WBD a seven-day waiver to reengage with Paramount, resulting in the higher bid. Paramount's offer is for the entirety of WBD, including its pay-TV networks, such as CNN, TBS and TNT. Netflix had four business days to make changes to its own proposal in light of Paramount's superior bid, the WBD board said in a statement Thursday. Instead, the decision by the streaming giant to walk away puts a pin in a drawn-out saga that saw amended offers from both bidders.]]></content:encoded></item><item><title>CISA replaces acting director after a bumbling year on the job</title><link>https://techcrunch.com/2026/02/27/cisa-replaces-acting-director-gottumukkala-after-a-bumbling-year-on-the-job/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Fri, 27 Feb 2026 15:57:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The U.S. cybersecurity agency's acting director Madhu Gottumukkala will be replaced, after a year of cuts, layoffs, and staff reassignments, and allegations of security lapses and claims he struggled to lead the agency.]]></content:encoded></item><item><title>SilverStone RM4A: 4U Rackmount Server/Workstation Chassis That&apos;s Great For Liquid Cooling</title><link>https://www.phoronix.com/review/silverstone-rm4a</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 15:43:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those looking to build a rackmount-ready server or workstation that can handle up to an SSI-EEB motherboard and capable of fitting a large liquid cooling setup, the RM4A is a new option from SilverStone that can fit up to a 360mm radiator while still fitting an SSI-EEB motherboard and up to eight expansion slots within 4U size constraints.]]></content:encoded></item><item><title>Break It To Make It: How Fracturing Sculpts Tissues and Organs</title><link>https://www.quantamagazine.org/break-it-to-make-it-how-fracturing-sculpts-tissues-and-organs-20260227/</link><author>Clare Watson</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2026/02/Break-it-to-Make-it-cr-Jean-Leon-Maitre-Default.webp" length="" type=""/><pubDate>Fri, 27 Feb 2026 15:40:38 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[There’s a moment, just before the tight mass of cells that is a developing mouse embryo implants itself in the womb, that it all comes apart. Hundreds of tiny fluid-filled bubbles expand between each of the orb’s few dozen cells. The bubbles grow and press outward on cell membranes — and then, in a moment of fracture, pry them apart. Thin protein strands tether the cells together as the…]]></content:encoded></item><item><title>Microsoft: Computer Programming Is Dying, Long Live AI Literacy</title><link>https://news.slashdot.org/story/26/02/27/1335243/microsoft-computer-programming-is-dying-long-live-ai-literacy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 15:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[theodp writes: On Tuesday, Microsoft GM of Education and Workforce Policy (and former Code.org Chief Academic Officer) Pat Yongpradit posted an obituary of sorts for coders. "Computer programmers and software developers are codified differently in the BLS [Bureau of Labor Statistics] data," Yongpradit wrote. "The modern AI-infused world needs less computer programmers (coders) and more software developers (more holistic and higher level). So when folks say that there is less hiring of computer programmers, they are right. But there will be more hiring of software developers, especially those who have adopted an AI-forward mindset and skillset. [...] The number of just pure computer programming roles has already been declining due to reasons like outsourcing, AI will just accelerate the decline." 

On Wednesday, Yongpradit's colleague Allyson Knox, Senior Director of Education and Workforce Policy at Microsoft, put another AI nail in the coder coffin, testifying before the House Committee on Education -- the Workforce Subcommittee on Early Childhood, Elementary, and Secondary Education on Building an AI-ready America: Teaching in the Age of AI. "Thank you to Chairman Tim Walberg, Ranking Member Bobby Scott, Chair Kevin Kiley, Ranking Member Suzanne Bonamici and members of the Subcommittee for the opportunity to share Microsoft perspective and that of the educators and parents we hear from every day across the country," Knox wrote in a LinkedIn post. 

"Three themes continue to emerge throughout these discussions: 1. Educators want support to build AI literacy and critical thinking skills. 2. Schools need guidance and guardrails to ensure student data is protected and adults remain in control. 3. Teachers want classroom-ready tools, and a voice in shaping them. If we focus on these priorities, we can help ensure AI expands opportunity for every student across the United States." 

Yongpradit and Knox report up to Microsoft President Brad Smith, who last July told Code.org CEO Hadi Partovi it was time for the tech-backed nonprofit to "switch hats" from coding to AI as Microsoft announced a new $4 billion initiative to advance AI education. Smith's thoughts on the extraordinary promise of AI in education were cited by Knox in her 2026 Congressional testimony. Interestingly, Knox argued for the importance of computer programming literacy in her 2013 Congressional testimony at a hearing on Our Nation of Builders: Training the Builders of the Future. "Congress needs to come up with fresh ideas on how we can continue to train the next generation of builders, programmers, manufacturers, technicians and entrepreneurs," said Rep. Lee Terry said to open the discussion. So, are reports of computer programming's imminent death greatly exaggerated?]]></content:encoded></item><item><title>The Goal is for Your Startup to Become a Verb</title><link>https://hackernoon.com/the-goal-is-for-your-startup-to-become-a-verb?source=rss</link><author>Startups Of The Week</author><category>tech</category><pubDate>Fri, 27 Feb 2026 15:17:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Whatever stage of growth you’re currently in, your north star should be the function you want to become synonymous with.What do you want your name to mean? Because after all is said and done, your product or service is a means to an end. All the technical excellence and expert architectural decisions that contribute to your unique service offering are interpreted by the end user in boring, simple thought processes—>Whenever I want to do xxxx, I think of [insert startup name]. That’s what all your playbooks should optimize for. When people want to solve a problem, they reach for a name that feels naturally tied to the action they want to take. And if you really win, there’s another layer: Rather than just coming to mind, your name replaces the action itself. You don’t say “search for it online.” You say, “Google it.” \n You don’t say “order a ride.” You say, “Uber there.”This phenomenon is something I’ve personally experience and it has shaped my consumption decisions, at home and abroad. To this day, whenever I refer to ride-hailing as “Ubering.” “I’ll Uber to you at 6 pm today”. “I’ll be with you shortly, I’m on the phone with my Uber driver” (nevermind that it was actually a Bolt driver). But that’s just me. In the early days of ride-hailing services in Lagos, between 2014 & 16, Bolt became the default word. Uber entered in 2014. Bolt followed in 2016 and scaled aggressively. For a large segment of the market, Bolt wasn’t an Uber alternative; it was the introduction to Ride-hailing. As their first experience, it naturally became THE word. Then inDrive arrived in 2019. And if you live in Lagos, you know what daily hold-up feels like. You know how surge pricing can turn a normal trip into a life-threatening financial decision. inDrive didn’t try to out-Uber Uber. It leaned into control, negotiation, and affordability. When there was fear of prices stretching too far, people opened inDrive.Over time, in certain conversations, “check inDrive” became synonymous with “find the cheaper option.”Three companies. Same category. Different associations forming in different pockets of the same city.The learning here, for builders, is to actively build mental shortcuts in the minds of your users because associations like these don’t happen by accident. And if you’re entering a market that feels somewhat crowded, fear not! You don’t have to own the entire category. You just have to own a behavior inside it.Pick the verb you want to represent, decide what you want to be synonymous with, and reinforce that idea over and over again. :::tip
And if you’re serious about testing that clarity in the real world, there’s a practical place to start.\
HackerNoon’s Proof of Usefulness Hackathon is built around a simple question: Does your product actually solve a real problem for real people? It’s one thing to declare what you want to be synonymous with. It’s another to prove it publicly.\
If you’re building something meaningful and want to sharpen your positioning while competing for over $150,000 in cash prizes and software credits, this is a solid first step.Now, let’s take a look at three startups that are clearly attempting to anchor themselves to specific mental shortcuts.Meet Pettr App, CreaThink Solutions, and Saturn: HackerNoon Startups of the Week is a digital platform designed to simplify pet ownership by centralizing pet care management in one place. From health records and appointments to service access and reminders, Pettr aims to reduce the administrative friction that comes with caring for animals.Rather than treating pet services as isolated transactions, Pettr positions itself as an ongoing companion to pet owners — a structured way to organize what is often an emotional and time-sensitive responsibility. Over time, that kind of utility has the potential to become second nature. provides end-to-end digital services that combine strategic thinking with technical execution. The company works with businesses to design, develop, and deploy tailored digital solutions — from platforms and websites to broader transformation initiatives.Its positioning leans into the idea that building well begins with thinking well. By blending creativity with structured problem-solving, CreaThink focuses not just on delivering digital products, but on helping organizations approach technology with clarity and intention. is a social scheduling platform built for high school students, designed to make managing school life more intuitive and connected. By organizing class schedules, clubs, sports, and social plans into one shared space, Saturn helps students navigate their daily routines with less friction.In environments where coordination can easily become chaotic, Saturn aims to bring structure and visibility to student communities — turning scheduling into something collaborative rather than fragmented.Different industries. Different audiences. Different problems.But the principle is the same.When your function is clear, your name travels. And when that function is reinforced consistently enough, your brand becomes a verb. ]]></content:encoded></item><item><title>Spotify is rolling out Audiobook Charts</title><link>https://techcrunch.com/2026/02/27/spotify-is-rolling-out-audiobook-charts/</link><author>Aisha Malik</author><category>tech</category><pubDate>Fri, 27 Feb 2026 15:07:42 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Similar to the streaming giant's Music and Podcast Charts, the Audiobook Charts will be updated weekly and highlight the top audiobooks overall and by genre.]]></content:encoded></item><item><title>Last 24 hours to get TechCrunch Disrupt 2026 tickets at the lowest rates of the year</title><link>https://techcrunch.com/2026/02/27/last-24-hours-to-get-techcrunch-disrupt-2026-tickets-at-the-lowest-rates-of-the-year/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Fri, 27 Feb 2026 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The lowest rates of the year for TechCrunch Disrupt 2026 end after today. Prices go up at 11:59 p.m. PT. Don't miss connecting with 10,000 founders, investors, and operators, and key takeaways from 250+ industry leaders. Register now to save up to $680, or up to 30% on group passes.]]></content:encoded></item><item><title>After Zomato, Deepinder Goyal returns with a $54M brain-monitoring bet</title><link>https://techcrunch.com/2026/02/27/after-zomato-deepinder-goyal-returns-with-a-54m-brain-monitoring-bet/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Fri, 27 Feb 2026 14:40:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Zomato co-founder Deepinder Goyal's new wearable startup Temple has raised $54 million in a friends-and-family round at a post-money valuation of about $190 million.]]></content:encoded></item><item><title>Your Smart TV May Be Crawling the Web for AI</title><link>https://entertainment.slashdot.org/story/26/02/27/1254229/your-smart-tv-may-be-crawling-the-web-for-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Bright Data, a company that operates one of the world's largest residential proxy networks, has been running an SDK inside smart TV apps that turns those devices into nodes for web crawling -- collecting data used by AI companies, among other clients -- and most consumers have had no idea it was happening. 

The company has published more than 200 first-party apps to LG's app store alone and still lists Samsung's Tizen OS and LG's webOS as supported platforms, though LG says the SDK is "not officially supported" and its operation on webOS "is not guaranteed." Google, Amazon, and Roku have all since adopted policies restricting or banning background proxy SDKs, and Bright Data no longer supports those platforms. 

Several Roku apps still running the SDK disappeared from the store after a journalist with The Verge behind this reporting contacted the company.]]></content:encoded></item><item><title>Intel Media Driver Update Brings Nova Lake S Support, AV1 Improvements</title><link>https://www.phoronix.com/news/Intel-Media-Driver-2025Q4</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 14:32:05 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While at the end of February, today Intel released the Intel Media Driver 2025Q4 release as well as the latest VPL GPU Runtime for their media stack...]]></content:encoded></item><item><title>A Shapeshifting Supercomputer May Be More Energy Efficient</title><link>https://spectrum.ieee.org/reconfigurable-supercomputer</link><author>Katherine Bourzac</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NTA2ODE2Mi9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3NDMzMzY4NX0.2czprnIaWqbouhrUkJbHYkwpxP73nEum_7mw28sIEFM/image.jpg?width=600" length="" type=""/><pubDate>Fri, 27 Feb 2026 14:23:28 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Sandia’s Spectra uses NextSilicon’s reconfigurable accelerators ]]></content:encoded></item><item><title>OpenAI raises $110B in one of the largest private funding rounds in history</title><link>https://techcrunch.com/2026/02/27/openai-raises-110b-in-one-of-the-largest-private-funding-rounds-in-history/</link><author>Russell Brandom</author><category>tech</category><pubDate>Fri, 27 Feb 2026 14:13:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The new funding consists of a $50 billion investment from Amazon as well as $30 billion each from Nvidia and SoftBank, against a $730 billion valuation. ]]></content:encoded></item><item><title>Canonical Talks Up RISC-V This Year With Ubuntu 26.04 LTS</title><link>https://www.phoronix.com/news/Ubuntu-RISC-V-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 14:01:10 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Canonical put out a new blog post today highlighting their RISC-V work over 2025 that included switching to the RVA23 profile baseline for Ubuntu 25.10 and moving forward. Now with RVA23-compatible RISC-V hardware coming to market this year, Canonical is talking up the RISC-V possibilities when paired with the upcoming Ubuntu 26.04 LTS release...]]></content:encoded></item><item><title>OpenAI Raises $110 Billion in the Largest Private Funding Round Ever</title><link>https://slashdot.org/story/26/02/27/1355236/openai-raises-110-billion-in-the-largest-private-funding-round-ever?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Fri, 27 Feb 2026 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[OpenAI has closed what is now the largest private financing in history -- a $110 billion round at a $730 billion pre-money valuation that more than doubles the $40 billion raise it completed just a year ago, itself a record for a private tech company at the time. 

Amazon invested $50 billion, SoftBank put in $30 billion, and Nvidia committed $30 billion, and additional investors are expected to join as the round progresses. The valuation is a sharp jump from the $500 billion OpenAI commanded in a secondary financing in October, and the round dwarfs recent raises by rivals Anthropic ($30 billion) and xAI ($20 billion). 

The company has been telling investors it is now targeting roughly $600 billion in total compute spend by 2030, a more measured figure than the $1.4 trillion in infrastructure commitments CEO Sam Altman had touted months earlier. OpenAI is projecting more than $280 billion in total revenue by 2030, split roughly equally between consumer and enterprise. ChatGPT now has over 900 million weekly active users and more than 50 million paying subscribers.]]></content:encoded></item><item><title>With Netflix Retreat, Trump Ally Larry Ellison Will Soon Own Warner Brothers, HBO, CNN, CBS, Paramount, Discovery, And Part Of TikTok</title><link>https://www.techdirt.com/2026/02/27/with-netflix-retreat-trump-ally-larry-ellison-will-soon-own-warner-brothers-hbo-cnn-cbs-paramount-discovery-and-part-of-tiktok/</link><author>Karl Bode</author><category>tech</category><pubDate>Fri, 27 Feb 2026 13:23:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Netflix has retreated from its protracted bidding war with Larry Ellison for control of Warner Brothers, giving the Trump ally likely control of Warner, CNN, and HBO. In a statement,  Netflix co-CEOs Ted Sarandos and Greg Peters said that Paramount’s latest offer made the acquisition financially irresponsible:“The transaction we negotiated would have created shareholder value with a clear path to regulatory approval. However, we’ve always been disciplined, and at the price required to match Paramount Skydance’s latest offer, the deal is no longer financially attractive, so we are declining to match the Paramount Skydance bid.”The massive debt load from massively overpaying for Warner Brothers is also likely to cause major operational headaches that could result in this being a short-lived adventure much like the several-decades worth of pointless Warner media mergers (including AT&T) that preceded it. That’s a lot of money for the Ellisons (and the Saudis) to dump into a company that has, again, seen nothing but a two-decade history of disastrous overvalued mergers resulting in a progressively shittier and less creative company, broadly despised by creatives after a parade of brutal layoffs (much more of which are certainly coming to pay off debt). Things could could be further complicated by a sudden subscriber exodus across the brands, or the Ellisons’ fortunes being further strained by a potential AI hype bubble collapse. All the lazy AI-generated Batman IP slop in the world will not be able to save this mess if the winds don’t blow favorably in the Ellisons’ direction over the next two years. Still, an overt authoritarian oligarch is now very close to controlling an unprecedented segment of U.S. traditional and new media. If it follows the established autocratic playbook, this push will continue until it runs into something other than pudding-soft public, political, and policy opposition. There’s a window here for policymakers and consumers to ensure the gambit fails, but the hour is getting late. ]]></content:encoded></item><item><title>How GenAI and Digital Twins Are Enabling Self Healing Supply Chain and Software Ecosystems</title><link>https://hackernoon.com/how-genai-and-digital-twins-are-enabling-self-healing-supply-chain-and-software-ecosystems?source=rss</link><author>Rahul Ravindran</author><category>tech</category><pubDate>Fri, 27 Feb 2026 13:21:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Previously, businesses have taken disruption lightly. Something to live with, something to respond to, something that will culminate. This closure of factories, collapse of suppliers, logistical difficulties, and system failures are all unwanted guests, and they push leaders to enter the crisis mode. The fag-end work is in teams, and decisions are fast; spending is too big, and hopefully it will be the environment, the next shock to be felt somewhere in the environment. Such a way of functioning is no longer a possibility. No longer is disruption an exception. It has been set to be the operating environment.The change that exists is not the existence of such a risk but is the reaction of the organizations to the already practically existing risk. Reaction to anticipation and anticipation to autonomy are processes of gradual transition. This change is built in the digital twin, and then there, this is not a programmed model but an organism sensitive to the dynamics of the complex systems in the real world.Digital twins and general-purpose AI reasoning systems can always recall the system telemetry, anticipate the wear-out of performance, and automatically undertake mitigation measures (redistribution of workload, dynamic scaling, and predictive maintenance) on vast-sized digital platforms (connected-device ecosystems, streaming platforms, and supply-chain software networks).Digital twins as living mirrors of realityModern digital twins are no longer static models; they are continuously evolving representations of real-world systems. It is a continuously evolving impression of actual business, which is nourished by streams of data from machines, systems, suppliers, and working processes. It expresses contemporary reality and visions of a future eventuality. Chains of suppliers, materials, inventory flows, and logistics routes are examples of the links represented in supply chains. Machines, production lines, and environmental conditions find their expressions in factories. It includes reliability, software performance trends, and loads.The dynamism of the twin enables leaders to experiment with ideas without having to disrupt the real world. By using a plant, the behavior of a line can be studied under conditions of stress. The effect of a disruption in levels can be observed by a supply team. A software process can also monitor the propagation of failures across systems. The reaction movement impulse starts here.From prediction to autonomyThe anticipated issue alters the dialogue. The question of what did not happen turns into the question of what will not happen as the starting point for teams. The latter is enabled by digital twins, which replicate scenarios around the clock. They address prospective supplier closures, demand fluctuations, geopolitical disasters, equipment corrosion and wear, and system overloads prior to their actual occurrence.The next step is autonomy. The extent of digital twins with compelling reasoning models is not limited to knowledge. They suggest actions, and under precise conditions, they implement them. The parameters of production are self-regulating. Before failure, maintenance is activated. Risks and non-habitual changes in inventory are addressed. Workloads are rerouted to software systems before performance is degraded. It is no longer entombed in alarms, but human teams remain in control. They are demoted to line managers.Self healing systems in practiceSelf-healing does not imply that systems are not destroyed. It means that they recognize stress at a very early stage and act wisely. In the manufacturing context, this can be observed in the form of equipment that adapts itself to prevent damage or trigger maintenance even before malfunctioning. The supply networks model considers it an automatic sourcing response to the supplier risk exceeding a threshold. It occurs in software ecosystems through redistribution of traffic or capacity scaling, or automatic isolation of failed parts.This is not possible in one tool but in a closed loop. Data flows into the twin. The signal is decoded by the twin. Scenarios are evaluated. Decisions are made. Actions are taken. The result is sent back into the model. It is a system that requires time to learn via its shortcomings and evolve.The quiet importance of governanceA government that is not in control is dangerous to the operations. The successful organizations see digital twins as a cloth of governance and not a by-product. Risk tolerance is defined. Escalation paths are clear. The decisions are limited in nature. The decisions made by the leaders are about what to automate and what people are supposed to research.This design will make sure that digital twins are not a rogue intelligence but rather an equal force. Silos' risk has been broken down and viewed by executives. Operations teams are not informed with noise. The real working of systems forms the basis of strategy and not spreadsheet forecasts of how systems work.When ecosystems replace silosTo create digital twins, the starting points are not the growing assets and departments, but this is where the actual breakthrough is achieved. The supply chain cannot be called linear. Service layers, vendors, and integrations are the basis of software platforms. In a cross-cultural setting, organizational elements are not maximized, but the whole is enhanced.Transparency suppresses information blindness. Personal fixes are substituted with integrated responses. Resilience is not a cost center but a belief center. Investors notice. Customers notice. The variation is experienced among staff members, where crisis is no longer periodic or temporary.Leading into the autonomous eraAutonomous processes do not occur in a day, from reactive to autonomous operation. They start with the ability to see, then progress to predicting, and finally graduate to self-healing behavior with evident leadership. The possibility of such progression is based on the concept of digital twins. GenAI and intelligent reasoning algorithms are added as the means of turning wisdom into action.The first flowing organizations are not pursuing novelty. Instead of being violent, they are opting to be non-violent. Instead of enhancing it, they are constructing shock-absorbing systems. Independence does not concern itself with replacing people in a world full of uncertainty. It is about providing them with a working system that is not contrary to them.]]></content:encoded></item><item><title>Ethical Challenges of Leveraging Generative AI in Financial Close and Narratives</title><link>https://hackernoon.com/ethical-challenges-of-leveraging-generative-ai-in-financial-close-and-narratives?source=rss</link><author>ks2423</author><category>tech</category><pubDate>Fri, 27 Feb 2026 13:16:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The digital marketing is like a forest fire, with generative AI becoming viral in other industry segments as well. It is a financial revolution. The potential of the generative AI is effective and correct and can save a large amount of time not only to accelerate the end of the month closing but also detail the financial stories. But all their novelty and convenience cannot be buried under the carpet of any cobbles of ethical problems.The Temptation of Speed and AccuracyThe monetary world is time conscious. It is monumental in itself that the stress component of making the book closing time, particularly at the culmination of a quarter or even more ideally a fiscal year, is stressful. And generative AI algorithms are the things which are, in fact, miraculous. They are able to find the information of colossal volumes of data in several seconds, see the anomalies sooner than the human eye, and even produce narrative reports that will contain literal and sensible explanations of the statistics.And all this hastiness brings the question of trust. Who is going to take the wrong step in a situation where the report prepared by artificial intelligence can be made on the false input data? The aspect of human control can also be reduced to a minimum since teams are fairly relying too heavily on the outcome of such tools. That threat is being overtaken, as in the case of errors, they will not be pointed out by the loss or enormous alteration of the human touch. The lack of taking such AI systems seriously implies that the system will be of an unsteady nature, and any hiccup will result in dire misreporting.Opacity in Decision MakingThe last part is one of the most troubling regarding the generative AI in financial reports as it is not clear how the AI is able to arrive at some of the decisions. Admittedly, generative models need not be necessarily transparent. They are not following a trail of coded reasoning blindly, but they like picking on patterns even the inventors of those patterns themselves do not know exactly what they are. This renders it difficult to audit the rationales of the financial narrations by AI.What does he/she do to the request that a growing authority or a board member is requesting him/her to clarify to him/her the cause behind a particular interpretation of the financial information based on the logic that is inherent in the machine learning operations? This black-box nature is of paramount concern to accountability and auditing. Traceability in finance is sacrosanct in normal finance. All the characters, all the footnotes will have a trace. These generative AI issues on this principle are yet to be reconciled.Bias Creeping Into NarrativesData is never neutral. It is a signifier of values and assumptions and past practices of the system where it is created. The actual danger is that information is being amplified in the product when the generative AI models are educated on financial data, particularly on the large volumes of data which are heterogeneous.One would be in the language pattern that is habitually adopted in the financial reporting that might be biased towards optimism rather than warning subconsciously. It has the potential to shape the sight of the financial performance of a business the very instant it gets selected by an AI and starts to create the identical positive narratives on it. Besides, this bias may happen accidentally to the human operators. This systematic reporting bias might cause badly informed stakeholders and ill‑informed decision making and reputational disaster in the long term.Data Privacy and Confidentiality RisksOne of the most confidential information which is possessed by a given organization is financial information. The fact that there is the possibility of the utilization of the generative AI implies the imposition of the megaliths of this information. Otherwise, it is a huge risk of dispensation of information unless under strict security.In addition, by utilising the services of third‑party AI or AI‑based models on a cloud‑computing model, the companies are, in other words, sending valuable financial information to third parties. Although the information has been anonymized, it can be reverse engineered or accidentally exposed. The potential result of such data intrusions, in the given case, is humiliation, not mentioning legal suits, regulatory fines, and unimaginable loss of trust of the stakeholders.Human Oversight vs Machine AutonomyTug of war is productivity or regulation. When it comes to generating the majority of legwork, the automatic financial close and storytelling generative AI will certainly be able to do most of the work. However, when human beings begin to sacrifice excess power, then it is something to fret about.The human perception in its capacity to process the information in the larger business world is, however, inimitable. Even the most sophisticated ones cannot even contemplate the specifics of a global crisis, instant amendment of the regulation, or strategic consequences of a takeover. The threat that it might pose is that some valuable undertones might be overlooked in case an AI replaced a human being as the financial stories writer. It is not simply a technology issue but there is solid ground on the moral aspect. The organizations must consider making sure that AI will not take the place of human work but will rather complement it particularly in the areas where judgment and situational analysis is a significant issue.AI has no reversal. It will continue boosting its financial position. Nevertheless, the organizations would be required to exhibit a high propensity of its application with a sharp sense of the ethical minefield it involves. Neither does it presuppose existing as being technological but assumes creating fences around such technology.It has to focus on human management. All the procedures of AI use should be audited. Factors that should be bargained include transparency and explainability, but they should not be neglected since these are among the factors that must be considered. The orientation of learning the functionality of such tools should also be accorded to the organizational teams so that they would be aware of where they would fail and critically analyze the results.Finally, AI should be treated as a co‑pilot, and not an autopilot. It can revolutionize the financial reporting through its judicious application. Even worse, the effectiveness of the very financial process can be destroyed because of the abuse of the same.]]></content:encoded></item><item><title>South Korea opens the door to let Google Maps operate fully</title><link>https://techcrunch.com/2026/02/27/south-korea-opens-the-door-to-let-google-maps-operate-fully/</link><author>Ram Iyer, Kate Park</author><category>tech</category><pubDate>Fri, 27 Feb 2026 13:12:40 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[After years of appeals, Google has finally won approval to export high-precision geographic information out of South Korea and provide proper Google Maps services in the country, including walking and real-time driving directions.]]></content:encoded></item><item><title>From DevOps to Platform Engineering How Shift Left Practices Enable AI Ready Enterprise Platforms</title><link>https://hackernoon.com/from-devops-to-platform-engineering-how-shift-left-practices-enable-ai-ready-enterprise-platforms?source=rss</link><author>nithish reddy</author><category>tech</category><pubDate>Fri, 27 Feb 2026 13:08:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[DevOps was a sense of freedom. It increased the pace of teamwork, the release process ceased to be whiplashed, and the divisions between the operations and development teams were finally broken. Liberty came, bit by bit, but that burden grew. Cloud resources increased many times over, containers were growing everywhere, compliance regulations were getting more rigid, and environments began to resemble chaotic cities rather than orderly workplaces. DevOps groups were now expected to do everything within limited timeframes and at high frequency. Shift left practices put testing, security, and quality earlier in the lifecycle at the expense of overworking developers. Something had to change.Platform engineering has not been constructed as an answer to DevOps, and it was a natural extension of it. Rather than forcing all teams to struggle with infrastructure, pipelines, and policies directly, platform engineering transforms these elements into an internal product. Developers are provided with a simple bare bones path to create, publish, and use software without necessarily understanding all the gears and cogs under the hood. The platform becomes a platform of trust, then one more is added, and then another, and another, while application teams focus on building value rather than managing complexity.Rethinking Shift Left at Enterprise ScaleShift left is most effective when invisible. There were security checks and compliance reviews during initial pipeline implementations that were not applied uniformly and were fragmented. This is altered by platform engineering. It has testing and governance built into the platform. Whenever developers create environments or deploy applications, the right checks automatically activate. The platform also does not give teams reminders about best practices but automatically enforces them. The system makes shift left part of the system and not an item on the to do list.Shifting checks further up the lifecycle is no longer sufficient, especially as systems become very large and interconnected. Contemporary platforms should treat security, testing, monitoring, and reliability as continuous rather than isolated tasks. This is achieved through platform engineering, which extends shift left across every stage of delivery. The same rules, insights, and protections are applied from the moment code is written until it is executed in production. Problems are detected faster because signals are continuously flowing, rather than relying on teams to decide when to investigate. This shift everywhere approach reduces unpredictability, shortens recovery time, and creates confidence that the service will behave consistently regardless of how fast the organization is moving.Currently, enterprise platforms need to provide much more than what was planned to deliver applications. They require very high observability, deterministic environments, and stable development and production environments. These attributes are necessitated because systems are starting to depend on a high level of automation and data driven decision making. Standardized deployment templates and built in monitoring are infrastructure definitions that assure organizations that changes are performing as expected. Learning systems can be predictable, secure, and scalable when everything runs on the same paved roads.Why Developers Actually Like This ChangeThe focus on platform engineering is one of the quiet strengths in its capability to transform the experience of a developer. For UI full-stack developers, platform-embedded shift-left practices ensure that user-facing applications behave consistently across environments, making real-time features, observability-driven UX improvements, and rapid iteration safer and more predictable.Rather than memorizing scripts or waiting on another group, developers use a bare bones interface through which they do the least amount of work, leaving the heavy work to be done on their behalf. Provisioning an environment feels normal and not dangerous. Deployments are dull and monotonous. When things go wrong, logs and metrics are already in place. The absence of friction in day to day work does not make people work harder but allows productivity to increase naturally.The Cultural Shift Behind the TechnologyChanging mentality is also part of transitioning to platform engineering. Platform teams act as product builders. They listen to internal users, improve through feedback, and measure success through adoption and reliability. Guardrails are not held against development teams. There is greater operational stability without turning teams into bottlenecks. With shift left practices ceasing to be a slogan and becoming an experience that is central to every interaction with the platform.The trend of platform engineering can be interpreted as a more general desire to see complex systems as having a calming side. Speed is what businesses desire, not anarchy. They seek innovation, not unmanageable risk. Through the use of shift left practices on a common platform, organizations are able to create an environment that is homogeneous, observable, and ready to integrate with the next generation of intelligent capabilities. The end product is more than improved software creation but a more sustainable way to build the future.]]></content:encoded></item><item><title>Engineering Accountable AI Systems: Why Governance Must Become a First-Class System Layer</title><link>https://hackernoon.com/engineering-accountable-ai-systems-why-governance-must-become-a-first-class-system-layer?source=rss</link><author>Aakash Ravi</author><category>tech</category><pubDate>Fri, 27 Feb 2026 13:01:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
AI governance has a production problem.Over the past several years, regulators, standards bodies, and industry leaders have converged on a clear consensus: AI systems must be accountable. Frameworks like the EU AI Act, the NIST AI Risk Management Framework, and emerging global standards all define expectations around fairness, auditability, risk management, and oversight.But there is a fundamental disconnect.Governance exists as policy. \n AI exists as infrastructure.And somewhere between the two, accountability breaks down.The core issue is not regulatory clarity. It is an engineering implementation.AI governance today is largely procedural. Documentation exists. Risk assessments are conducted. Controls are described. But the systems themselves often lack deterministic enforcement mechanisms that ensure governance requirements are actively enforced at runtime.This is not a policy failure. It is a missing architectural layer.The Problem: Governance Without EnforcementModern AI systems operate at extraordinary scale.Financial approvals affecting millions of individualsContent ranking and moderation across global platformsAutomated operational decisions across critical infrastructureHealthcare decision support affecting patient outcomesAt this scale, even small deviations can produce systemic risk.Yet most governance mechanisms today operate outside the system itself:Reactive investigations after incidents occurThese mechanisms do not provide continuous enforcement.They cannot guarantee that governance requirements were actually enforced at the moment decisions were made.Without system-level enforcement, governance becomes retrospective rather than preventative.The Root Cause: No Translation Layer Between Policy and SystemsRegulatory requirements are written in human language:“Ensure fairness.” \n “Maintain appropriate safeguards.” \n “Provide auditability.”Production systems require deterministic specifications:Access control primitivesThese two domains operate independently.Legal and compliance teams define governance requirements. Engineering teams build systems. But there is rarely a structured mechanism that translates governance mandates into enforceable technical controls.This creates a systemic accountability gap.Introducing the AI Accountability Control Stack (AACS)To address this structural deficiency, I developed the AI Accountability Control Stack (AACS) — a production-grade architectural framework that operationalizes governance requirements directly within AI system infrastructure.The AACS transforms governance from documentation into enforceable system behavior.Rather than relying on manual oversight, it embeds accountability into the system itself.The architecture consists of six functional layers:Layer 1: Policy Abstraction LayerThis layer converts governance requirements into structured, machine-readable control primitives.Instead of policy existing only as text documents, it becomes structured metadata that systems can interpret and enforce.Layer 2: Risk Modeling LayerDifferent AI systems carry different levels of risk depending on:This layer maps governance requirements to system-specific risk profiles.Layer 3: Control Specification LayerThis layer translates governance requirements into enforceable technical specifications, including:These specifications are executable, not advisory.Layer 4: Instrumentation LayerInstrumentation embeds monitoring and enforcement hooks directly into:Model inference pipelinesThis ensures governance enforcement occurs during system execution.Layer 5: Audit Telemetry LayerThis layer generates structured, tamper-evident audit logs capturing:Applied governance controlsThis creates verifiable audit evidence automatically.Layer 6: Governance Reporting InterfaceThis final layer converts telemetry into:Regulator-ready audit reportsInternal compliance dashboardsGovernance becomes continuously measurable.Why This Architecture MattersExisting governance frameworks define expectations. They do not define implementation architectures.The AACS provides a deterministic translation layer between governance policy and system execution.This produces several critical capabilities: Controls are applied at inference time. Evidence is generated as part of system operation. Governance scales with infrastructure. Governance remains intact as systems evolve.How This Works in Real SystemsModern AI infrastructure is:Integrated with external servicesThe AACS integrates directly into this environment by attaching enforcement and telemetry mechanisms to service boundaries, inference pipelines, and API layers.This allows governance controls to travel with the system regardless of deployment architecture.Even when using externally provided models, governance wrappers can enforce access controls, logging requirements, and operational safeguards.This ensures accountability regardless of system complexity.The Emergence of Governance EngineeringThis architectural model introduces a new engineering discipline: Governance Engineering.Governance engineers design and implement the infrastructure required to operationalize governance requirements.Their work ensures that governance is enforced automatically, not manually.This function is becoming essential as regulatory expectations shift toward technical enforceability.The Future: Governance Will Be Evaluated at the System LevelRegulatory oversight is evolving rapidly.Future regulatory evaluation will focus not only on policy documentation, but on system-level evidence demonstrating governance enforcement.Organizations will need to demonstrate:How governance requirements were translated into system controlsHow those controls were enforcedWhat evidence proves enforcement occurredArchitectural enforcement will become the standard.Final Thought: Accountability Is an Architectural DecisionAccountability cannot be achieved solely through documentation, policy, or audits.It must be engineered into the system itself.The AI Accountability Control Stack provides a practical architectural model for achieving this by introducing a deterministic control layer that bridges governance and system execution.As AI systems continue to scale and regulatory expectations intensify, the organizations that treat governance as infrastructure rather than policy will be best positioned to build trustworthy, resilient, and compliant AI systems.Governance must become code.]]></content:encoded></item><item><title>Memory Price Hikes Will Kill Off Budget PCs and Smartphones, Analyst Warns</title><link>https://hardware.slashdot.org/story/26/02/27/008259/memory-price-hikes-will-kill-off-budget-pcs-and-smartphones-analyst-warns?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Register: Ballooning memory prices are forecast to kill off entry-level PCs, leading to a decline in global shipments this year -- and a similar effect is going to hit smartphones. Analyst biz Gartner is projecting a drop in PC shipments of more than 10 percent during 2026, and a decline of around 8 percent for smartphones, all due to the AI-driven memory shortage. Some types of memory have doubled or quadrupled in price since last year, and Gartner believes DRAM and NAND flash used in PCs and phones is set for a further 130 percent rise by the end of 2026.
 
The upshot of this is that the budget PC will disappear, simply because vendors won't be able to build them at a price that will satisfy cost-conscious buyers, according to Gartner research director Ranjit Atwal. "Because the price of memory is increasing so much, vendors lose the ability to provide entry-level PCs -- those below about $500," he told The Register. PC makers could just raise the price of their cheap and cheerful boxes to above that level to compensate for the memory hike, however, price-sensitive buyers simply won't bite, he added.
 
Another factor expected to add to declining fortunes of the PC industry this year is AI devices -- systems equipped with special hardware for accelerating AI tasks, typically via a neural processing unit (NPU) embedded in the CPU. These systems were predicted to take the market by storm, but they require more memory to support AI processing and vendors like to mark them up to a premium price. "Historically, downgrading specifications was the way to go when prices were being squeezed, but that's difficult here," Atwal said. "The thinking was that the average price [of AI PCs] would fall this year, and lead to more adoption," said Atwal, "but that's not happening." The lack of killer applications isn't helping either.]]></content:encoded></item><item><title>Building Production-Grade RAG Systems for Document AI: What It Actually Takes</title><link>https://hackernoon.com/building-production-grade-rag-systems-for-document-ai-what-it-actually-takes?source=rss</link><author>Abhinav Sharma</author><category>tech</category><pubDate>Fri, 27 Feb 2026 12:53:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
RAG is everywhere—and that’s not surprising. It’s one of the most practical ways to make large document collections queryable without building brittle, domain-specific parsers for every question type. The catch is that what works in a controlled demo often degrades quickly when you put it in front of real enterprise PDFs: scanned contracts, compliance filings, medical records, policies, and the long tail of layout and quality issues that come with them. In production, the “RAG problem” is less about clever prompting and more about repeatability: traceability, security, quality controls, and the ability to explain why an answer is correct (or why the system refused).When teams get stuck, it’s rarely because vector search “doesn’t work.” It’s because the system can’t consistently ground answers to the right evidence, can’t enforce entitlements reliably, or can’t be evaluated and improved without breaking things. If you can’t tell a stakeholder which version of which document supported a claim—or prove the user was authorized to see it—you don’t have a product yet. You have an experiment.Most prototypes follow the same path: drop documents into a vector store, retrieve top-k chunks, and ask an LLM to synthesize. On clean, well-structured text, that can look excellent. The issue is what happens next. Scanned PDFs come in rotated or skewed. Multi-column reading order gets scrambled. Tables lose structure during extraction. Chunking splits mid-argument. Retrieval returns “close enough” context that reads plausibly but doesn’t actually support the claim. And the model, doing what it’s optimized to do, answers fluently anyway.In production, you’re optimizing for different properties than a demo. You want the system to be reliable over messy inputs, reproducible across pipeline changes, and defensible under scrutiny. That means being able to trace an answer back to specific evidence, and having strong defaults when evidence is weak: clarifying questions, refusal behavior, or presenting “best available evidence” with explicit uncertainty. It also means treating access control as part of retrieval—not as an afterthought layered onto UI.Ingestion: Where Quality Is Won or LostIf you’ve built a few of these systems, you learn quickly that ingestion determines retrieval quality more than most downstream tricks. Document AI preprocessing isn’t glamorous, but it is where you either preserve structure—or lose it permanently. For enterprise documents, OCR alone isn’t enough; you typically need OCR with layout detection, reading-order reconstruction, and structure extraction that keeps headings, sections, and tables meaningful. Managed tools like Google Document AI, Azure Document Intelligence, and Amazon Textract can cover a lot of ground. Open-source pipelines like Unstructured and GROBID are common when you need transparency or tighter control over parsing decisions.Chunking is where teams often underestimate the complexity. A simple character or token split is fast, but it tends to cut across semantic boundaries—exactly the boundaries users care about in contracts and policies. Adaptive chunking that follows headings, section boundaries, and table boundaries usually improves both retrieval and downstream grounding. It also makes provenance feel natural to the end user: instead of surfacing an opaque internal ID like chunk_4892, you can point to something a reviewer can immediately verify—“MSA v3.2 → Section 9 (Termination) → 9.2 (Termination for Cause), page 12, lines 14–22.”Metadata is another area that tends to look optional until you need it. In practice, metadata is what makes filtering, traceability, and reproducibility possible. Useful chunk-level metadata commonly includes document IDs, section paths, page numbers, timestamps (effective date, last modified, ingested at), extraction confidence signals, and version identifiers (document hash, chunking version, embedding model version). In enterprise contexts, access-control attributes (tenant, department, confidentiality, role tags) need to be first-class, because they directly constrain retrieval and audits.The Retrieval Stack That Actually WorksVector similarity search is a good baseline, but it’s rarely sufficient on its own for enterprise documents. In practice, hybrid retrieval—dense embeddings plus sparse lexical retrieval like BM25—tends to be more robust, especially when users query with clause numbers, identifiers, acronyms, or exact phrasing. Dense retrieval handles semantic intent well; sparse retrieval anchors you to exact terms and rare tokens that embeddings often smooth over.Reranking is often where systems make the biggest leap in perceived quality, not because it’s magical, but because it fixes a common failure mode: the initial retrieval set contains “kinda relevant” chunks, and you need to promote the truly relevant ones to the top. Cross-encoder re-rankers (open models like bge-reranker or managed APIs like Cohere ranker) rescore candidate chunks using deeper query–passage interaction. Teams usually see a noticeable lift in context precision when reranking is measured properly (for example, on a golden set with expected sources). If you keep a quantitative claim here, it’s best to tie it to a metric (“context precision” or “citation precision”) and an evaluation setup, rather than a broad “accuracy” number.Query rewriting and expansion is another lever that’s easy to skip early and then rediscover later. Users don’t naturally phrase questions the way documents are written. A rewrite step can expand acronyms, normalize entities, and split multi-part questions into retrieval-friendly sub-queries. It doesn’t need to be fancy—but it does need observability, because uncontrolled rewriting can drift away from user intent.Security: The Layer Everyone ForgetsMost RAG demos ignore access control because it slows down the prototype. In production, it’s a primary constraint. If your system indexes HR documents, legal contracts, and engineering specs together, you need a deterministic entitlement path from user → allowed chunks, and retrieval must be constrained by that path before any content reaches an LLM.The pattern that tends to scale is pre-filtered retrieval: compute entitlements (RBAC/ABAC), retrieve only from chunks with compatible ACL attributes, rerank within the authorized candidate set, and log what evidence was accessed. This is also where the “metadata isn’t optional” point shows up in practice—without chunk-level tagging, you end up with leaky boundaries or expensive, brittle post-filters.Beyond ACL, enterprise deployments typically need some combination of PII detection/masking, encryption at rest, short-lived tokens for source access, and audit logging that captures query, retrieved chunk IDs, citations, and document versions. One more modern concern worth taking seriously is prompt injection content inside documents. You don’t need to treat every document as hostile, but you do need basic guardrails so instructions embedded in source text can’t supersede your system’s rules—especially around access control, disclosure, and how the model is allowed to behave.Monitoring: Closing the LoopIf you operate one of these systems for more than a few weeks, you’ll see drift. Documents change, the query distribution changes, the ingestion pipeline changes, and model components get updated. Without monitoring and evaluation, quality degrades quietly until users stop trusting the tool.Practically, you want to track retrieval health (recall@k against a golden set, context precision, reranker lift), generation health (citation precision, groundedness/faithfulness checks, refusal rates), and operational health (p50/p95 latency, cost per query, ingestion lag from document update to searchable index). The most effective teams I’ve seen maintain a golden evaluation dataset—curated questions with expected source documents—and run it on a schedule and on change events (new embeddings, new chunking logic, new document batches). Tooling like Phoenix, TruLens, or commercial platforms can help, but the bigger differentiator is the discipline to keep evaluation current and to treat regressions like real production incidents.One area that’s frequently underestimated is versioning and reproducibility. When you change OCR models, chunking logic, embedding models, rerankers, or generation prompts, you need a way to trace which versions produced which answers. That’s what makes debugging and audits feasible months later.Stack decisions matter, but capabilities matter more. For many teams, a managed-leaning setup is attractive: ingestion via a managed Document AI tool or Unstructured-based pipeline, a hosted vector database, an orchestration layer such as LlamaIndex or LangChain, and a reranker (open or managed). Others prefer open-source deployments using Qdrant/Weaviate/OpenSearch, Haystack or similar orchestration, and self-hosted models for control and cost predictability. Either approach can work if it supports the fundamentals: document-aware ingestion, hybrid retrieval, entitlement enforcement, provenance-friendly citations, evaluation pipelines, and versioning.On the architecture side, systems tend to become easier to operate when they’re split cleanly: ingestion workers that run asynchronously and can be retried safely; a stateless retrieval service that enforces policies and returns evidence; and a generation service that operates with bounded context and clear provenance. A typical reference deployment includes an API gateway, a job queue (Kafka/RabbitMQ), object storage for raw documents and parsed artifacts, the index layer (dense + sparse), plus centralized logging/metrics and an audit trail.]]></content:encoded></item><item><title>AI-Native Automation in 5G-Advanced and 6G</title><link>https://hackernoon.com/ai-native-automation-in-5g-advanced-and-6g?source=rss</link><author>viharikabhimanapati</author><category>tech</category><pubDate>Fri, 27 Feb 2026 12:39:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[With video projected to comprise 82% of Internet traffic by 2027, hosting live events with encouraging quality is more important than ever - each second of delay can decrease engagement by 20%, and users are still abandoning service for a little bit of buffering. With the rollout of 5G-Advanced and 6G on the horizon, users require ultra-low latency with high throughput - no longer a 'nice to have', but a 'must have' and expected especially in 8K, XR, and much of game streaming via cloud. Achieving these quality expectations will not only take extreme speed but responsive networks programmed to exceed smart value added QoE for streaming and entertainment. AI-native automation can scale unique QoE, but we remain trapped in siloed systems with outdated internal business policies and procedures. The real test is not how to import machine learning AI but simply how to do AI right at the edge! Streaming now has become chaotic across fast-moving environments- city towers, edge servers, content aggregators, mobile users, and multi-CDN. So, streaming and the associated QoE can now move in countless ways simultaneously. What ISO, TSRO, and similar static tools allow for post-suffering, as they react too late when the frustration of a stopping point of view has been reached. And data on each user and droves of other users via the flowchart of the transmission path is in constant flux- it is effectively impossible to understand what catastrophe occurred and where with tranches of data scattered across the entirety of the RAN, Core, CDN, and app layer. As the expectation of seamless, flawless, and full 4K streams continues to climb, real-time AI orchestrated service workflows appear to be large and unattainable gaps in service!The fix? An AI-native control plane that predicts and resolves problems before users even notice.1️. Edge-Centric Predictive QoE ModelingEdge nodes are evolving from small data centers into decision engines in real time.  With AI at the edge, we can anticipate and mitigate streaming interruptions by using local indicators such as buffer levels, handovers, and user movement.  Federated learning allows these models to continue evolving regionally without having any detrimental effects on privacy.  Imagine a streaming app on a train that predicts tower congestion while adjusting bitrate on the fly.  The ability to move networks from reactionary to pro-active is transforming the streaming experience, making it smoother, more intelligent, and more personalized for users… anywhere!2️. Closed-Loop Network Slicing with QoE FeedbackNetwork slicing allows multiple virtual networks to share infrastructure, but most still prioritize technical metrics over real user experience. AI-native automation takes this to the next level by taking real-time QoE data and adapting slices in real time. If a human ability user or AR/VR stream drops, it can route traffic away, change bandwidth, or change RAN priorities with no people involved. This creates real-time and dynamic slicing, which keeps streams stable through network changes.3️. Cross-Layer Root Cause Analysis with Explainable AIIt's never easy to identify the root cause of streaming issues in complex multi-domain networks—was it the server, a CDN miss, RAN congestion, or device buffering? Explainable AI (XAI) changes this by correlating data across the transport, application, and radio layers to detect and explain anomalies. XAI doesn't only detect the fault, it explains why it happened, and this helps both operations teams, as well as autonomous systems, to identify root causes and fix problems more intelligently and quicker.4️. Intent-Based Streaming Policy ManagementOperators need to evolve from static, threshold-based rules and toward intent-based policy engines. An example of a business intent might be, “Deliver live sports in ultra-low-latency to premium subscribers in metro areas.” AI would take such high-level intent and translate it into low-level configurations: managing edge caches, modulating transcoding profiles, prioritizing up-link traffic, prioritizing downlink traffic, etc. If the engine detects a deviation, such as increased rebuffering during the live sports broadcast, it can self-correct by changing policies in real-time.Conclusion: Toward Autonomous Streaming NetworksAI-native automation is no longer an option, it’s vital to creating intelligent, resilient networks that can keep up with how we stream today (and tomorrow). As we transition from 5G-Advanced to 6G, the winners will be the ones that are able to put together edge-based intelligence, intent-based control, plus some explainability. Do it right, and you’re not just reducing churn - you’re setting the pace in streaming. Mess it up? At best you can settle for a buffering wheel.]]></content:encoded></item><item><title>Mesa Developers Trying To Reach A Consensus On AI Policy</title><link>https://www.phoronix.com/news/Mesa-AI-Policy-March</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 11:24:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[If all goes well, Mesa developers are hoping to reach a consensus or at least some common ground on an AI policy in March. Mesa is the latest open-source project making considerations around the growing activity around AI coding agents and the like and how to deal with them for this project that is crucial to the Linux desktop and open-source 3D graphics drivers at large...]]></content:encoded></item><item><title>Numerous AMDXDNA Ryzen AI Driver Fixes For Linux 7.0-rc2</title><link>https://www.phoronix.com/news/Linux-7.0-rc2-DRM-Fixes</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 11:11:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Sent out today were all of the DRM/accel driver fixes for the week, ahead of the Linux 7.0-rc2 kernel release due out on Sunday...]]></content:encoded></item><item><title>Genode OS 26.02 Halfway Done Migrating From GitHub To Codeberg</title><link>https://www.phoronix.com/news/Genode-OS-26.02</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 11:00:41 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Genode OS 26.02 is out as the latest feature update to this open-source operating system framework that also serves as the basis for their Sculpt general purpose OS...]]></content:encoded></item><item><title>Ultrahuman bets on redesigned smart ring to win back US market after Oura dispute</title><link>https://techcrunch.com/2026/02/27/ultrahuman-unveils-new-smart-ring-as-it-awaits-u-s-clearance-after-oura-dispute/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Fri, 27 Feb 2026 11:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Ultrahuman’s Ring Pro promises 15-day battery life and a $479 price tag as the wearables maker expands its health-tech push.]]></content:encoded></item><item><title>Moon&apos;s Ancient Magnetic Field May Have Flickered On and Off</title><link>https://science.slashdot.org/story/26/02/26/2356249/moons-ancient-magnetic-field-may-have-flickered-on-and-off?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[sciencehabit quotes a report from Science Magazine: For decades, planetary scientists have pored over a mystery hidden within the Moon rocks retrieved by Apollo astronauts in the 1960s and '70s. Minerals in the rocks record the imprint of a magnetic field, nearly as powerful as Earth's, that existed more than 3.5 billion years ago and seemed to persist for millions of years. But generating a magnetic field requires a dynamo -- a churning, molten core -- and most researchers believed the Moon's tiny core would have long since cooled off, 1 billion years after it formed. Corroborating that picture are other ancient Moon rocks of about the same age that suggest the field was weak -- leaving planetary scientists baffled.
 
Now, researchers are proposing a new way to solve the puzzle. A paper published today in Nature Geoscience theorizes that between 3.5 billion and 4 billion years ago, blobs of titanium-rich magma melted episodically just above the core, rising in plumes that drove volcanic eruptions on the surface. By intermittently stirring up the Moon's core, these bouts of melting would have caused the Moon's magnetic field to flicker on in short, powerful bursts. The paper "links a few different concepts that people were thinking about separately, but hadn't actually brought together," says Sonia Tikoo, a planetary geophysicist at Stanford University who was not involved in the study.]]></content:encoded></item><item><title>Lessons from Building a 100+ Agent Swarm in Web3</title><link>https://hackernoon.com/lessons-from-building-a-100-agent-swarm-in-web3?source=rss</link><author>John P</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:32:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Part 1: Why Agentic Engineering Isn't Vibe CodingA friend recently posted about AI skills using The Matrix analogy: Trinity doesn't learn to fly a helicopter — Tank uploads a precise, verified program directly into her mind. She steps into the cockpit and flies.That analogy is perfect. But it exposes a fundamental misunderstanding about how these systems actually get built.Trinity doesn't vibe her way into flying. Tank uploads a precise, verified program.That's the difference between vibe coding and what Addy Osmani calls Agentic Engineering. "Vibe coding" has poisoned the well — it suggests you can prompt your way to production software. You can't.I have 70+ agents in my web3 development workflow. They're not prompt dumps. They're structured: clear responsibilities, explicit knowledge files, defined handoff points between agents. Building them took the same architectural thinking that's always separated working systems from chaos.The old principles translate directly: (Don't Repeat Your Prompt)Separation of concerns → Agent boundariesInterface design → Handoff contractsDocumentation → Precompiled contextBuilding good skills is architecture work. The Trinity scene works because someone  that pilot program. Tank didn't vibe code it.The Problem That Started EverythingThe biggest thing I learned building AI coding workflows: more context is not better.I work in web3. I have ABIs that are thousands of lines. A Ponder database schema that would eat half the context window if I fed it in raw. My first instinct was to give the AI everything and let it figure out what matters.The AI gets lost. Important instructions get ignored. It latches onto random details. Results get worse as context gets bigger. There's a term for this: . As the context window fills up, earlier instructions get "crowded out" and the model starts ignoring them.So I built a skills-specialist agent.Its only job is precompiling context for other agents. It reads my raw ABIs, schemas, and docs, then generates slim reference files tailored to specific tasks. It builds skills.When I need UI work done, I don't hand my ui-designer the entire codebase. The skills-specialist has already built a component reference with just the props and patterns that agent needs. When I need contract integration, the web3-implementer gets only the relevant functions and events. The raw ABI stays on disk.Agents building context for agents.The Memory Hierarchy Mental ModelThe mental model that made everything click: treat context like RAM, not a junk drawer.| Layer | What It Holds |
|----|----|
| Disk | Full codebase, raw ABIs, complete schemas |
| RAM | Precompiled skills — task-specific reference files |
| Registers | The current prompt and immediate context |You don't load everything into memory. You load what you need for the current task.My skills-specialist is the compiler that transforms disk into RAM. Without it, I'm back to stuffing context and hoping for the best.Here's the pattern my skills-specialist generates:# MorphoVault Reference

> Use when implementing vault deposit/withdraw flows.

## Terminology
- **shares**: Vault shares representing proportional ownership
- **assets**: The underlying token being deposited

## Key Functions

### deposit(uint256 assets, address receiver) → uint256 shares
Deposits assets and mints shares to receiver.
See: protocols/morpho/abis/MetaMorpho.json

### withdraw(uint256 assets, address receiver, address owner) → uint256 shares
Burns shares and sends assets to receiver.
See: protocols/morpho/abis/MetaMorpho.json

## Events

### Deposit(address indexed sender, uint256 assets, uint256 shares)
### Withdraw(address indexed sender, uint256 assets, uint256 shares)

## Related Hooks
- useVaultDeposit: src/hooks/blockchain/useVaultDeposit.ts
- useVaultBalance: src/hooks/ponder/useVaultBalance.ts
Short. Scannable. Pointers to source files. Domain terms defined. Clear sections.The raw Morpho ABI is 2,000+ lines. This reference is 30 lines and contains everything an agent needs to implement a deposit flow.The Craft Behind the CurtainSkills don't build themselves. My skills-creator is itself a skill — one I built through iteration, testing, and architectural thinking.Every skill in my system has: — What does this agent own? — What precompiled context does it need? — When does it call other agents? — How do we know it's done?That's not vibing. That's engineering.The distribution insight is right — skills compress the distance between user and value. But someone still has to build them well. That's the craft.Next, I'll cover the organizational breakthrough that made 70+ agents manageable: the distinction between  (personas you converse with) and  (workers you dispatch).Turns out only ~10 needed to be conversational. The rest just needed to be discoverable.\
This is Part 1 of "Lessons from Building a 100+ Agent Swarm in Web3." Follow for Part 2 on agent organization, or connect if you're building similar systems.]]></content:encoded></item><item><title>GitScrum MCP Server: How AI Assistants Are Revolutionizing Project Management</title><link>https://hackernoon.com/gitscrum-mcp-server-how-ai-assistants-are-revolutionizing-project-management?source=rss</link><author>Renato Marinho</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:31:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The way we interact with project management tools is changing. Instead of clicking through endless menus and dashboards, what if you could simply ask your AI assistant to create sprints, track time, or generate team reports? That's exactly what the GitScrum MCP Server enables.Built on the Model Context Protocol, this open-source server connects AI assistants like Claude, GitHub Copilot, and Cursor directly to your GitScrum workspace. It's not just another integration—it's a complete operational layer that gives AI full access to your project management stack.What is the Model Context Protocol?The Model Context Protocol (MCP) is an open standard that allows AI assistants to interact with external systems in a structured, secure way. Think of it as an API specifically designed for AI agents—instead of humans clicking buttons, your AI assistant can read, create, and update data through natural conversation.GitScrum's implementation brings this to project management, offering 29 tools with over 160 operations across tasks, sprints, time tracking, client CRM, analytics, and more.Zero Context Switching: Managing Projects Through ConversationThe most powerful aspect of the GitScrum MCP Server is how it eliminates context switching. Here's what you can do without leaving your AI chat:textYou: "What's on my plate today?"
AI: Fetches your tasks due today across all projects.

You: "Show me what the team shipped this week"
AI: Generates a standup digest with completed work and blockers.
Sprint Planning Made SimpletextYou: "Create a sprint for next week with the top 5 backlog items"
AI: Creates the sprint, assigns tasks, and sets the timeline.
Real-Time Project InsightstextYou: "Which projects are over budget?"
AI: Returns burn-down data and flags at-risk projects.
textYou: "Send the Q1 proposal to Acme Corp"
AI: Creates the proposal, attaches the client, and sends it.
The server provides access to every major GitScrum feature:: Create, update, complete, filter, duplicate, and manage subtasks: Full lifecycle management with KPIs, stats, and progress tracking: Access workflows, task types, labels, and team members: Start/stop timers, view logs, analyze productivity: Complete CRUD operations: Create and manage epics across projects: Custom workflow management: Dynamic tagging and filtering: Channels, messages, search, and notifications: Knowledge base management: Personal note vault with sharing capabilities: Thread management on any entity: Manage contacts and interactions: Create, issue, send, and track payments: Full proposal lifecycle with approval workflows: 8+ financial and CRM reports: Automated team digests: Real-time consumption and alerts: Team and task-level activity streams: 10+ operational reportsThe fastest way to get started is using the hosted SSE server:texthttps://mcp.gitscrum.com/sse
Zero installation required. Just add the URL and your GitScrum token to Claude Desktop, Cursor, or any SSE-compatible client. Perfect for teams that want instant setup without managing infrastructure.For organizations requiring local deployment:bashnpx -y @gitscrum-studio/mcp-server
Runs locally via stdio transport. Requires Node.js 18+. Ideal for VS Code, GitHub Copilot, or offline environments.The GitScrum MCP Server was built with the principle of least privilege:: Only CREATE, READ, UPDATE operations are allowed. Destructive actions must be performed in the GitScrum web app.: Credentials never touch the MCP server. Authentication happens directly with GitScrum's OAuth service.: Access tokens are stored locally with restricted filesystem permissions.: Automatic lockout after failed authentication attempts.1. Automated Sprint PlanningInstead of manually dragging tasks into sprints, describe what you need: "Create a two-week sprint starting Monday with all high-priority backend tasks from the Q1 milestone." Your AI assistant handles the rest.2. Cross-Project ReportingAsk for insights across your entire workspace: "Show me all overdue tasks across all projects assigned to frontend developers." No more jumping between project dashboards.Streamline client workflows: "Generate an invoice for Acme Corp for 40 hours at $150/hour, send it, and add a note that payment is due in 30 days."Daily standups become conversations: "What did Sarah complete yesterday? Are there any blockers for the design team?"The server is built in TypeScript and designed for efficiency:: Each tool uses a single  parameter, reducing LLM context tokens by ~80% compared to individual tool definitions.: Full TypeScript coverage with Zod schemas for validation.: 378 tests across 22 suites ensure reliability.: Open source and free to use, modify, and extend. (if self-hosting):   bashnpx -y @gitscrum-studio/mcp-server
: \n  Add the server configuration to Claude Desktop, VS Code, or Cursor settings.: \n  Tell your AI assistant: "Login to GitScrum"The OAuth flow opens in your browser—no credentials shared with the MCP server.   text"What tasks are due this week?"
   "Create a new task in Project Alpha"
   "Show me the team's velocity for Q1"
The Future of Project ManagementThe GitScrum MCP Server represents a shift in how we think about project management interfaces. Instead of learning complex UIs, teams can simply describe what they need. AI assistants become project managers, analysts, and productivity coaches—all through natural conversation.This isn't about replacing project managers. It's about giving them superpowers. Let AI handle the routine queries, data aggregation, and administrative tasks, so humans can focus on strategy, creativity, and relationships.The GitScrum MCP Server is open source and actively maintained. The team welcomes contributions:Whether you're fixing bugs, adding new tools, or improving documentation, there's room for everyone to make this project better.The Model Context Protocol is unlocking new possibilities for how we interact with software. GitScrum's implementation shows what's possible when AI assistants have full operational access to project management tools.With 29 tools, 160+ operations, and zero context switching, the GitScrum MCP Server is more than an integration—it's a new way to work. Try it today and experience project management through conversation.]]></content:encoded></item><item><title>How AI-Native Engineering Boosts Your App’s Scalability</title><link>https://hackernoon.com/how-ai-native-engineering-boosts-your-apps-scalability?source=rss</link><author>Varsha Ojha</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:29:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Modern applications often struggle to scale, not because of weak infrastructure, but because decision-making intelligence is added only after systems are already under load. Auto-scaling, DevOps pipelines, and cloud elasticity help, but they are largely reactive by design.As user demand grows, engineering teams are forced to chase latency spikes, error rates, and infrastructure bottlenecks after they appear, turning scalability into a constant firefight.This is where AI-native solutions fundamentally change how scalability is engineered. By embedding intelligence directly into system architecture and the software development lifecycle, AI-native systems anticipate growth instead of reacting to it. Decisions around capacity, performance, and resource usage become predictive rather than manual.Enterprises adopting AI native software engineering approaches, often in partnership with specialized AI native engineering service companies, are redefining how scalable systems are built. This blog explains how AI-native engineering turns scalability from an operational burden into a built-in system capability, across infrastructure, performance, and development velocity.How AI-Native Engineering Improves App ScalabilityAI-native engineering improves app scalability by changing how systems detect, anticipate, and respond to growth. Instead of reacting after performance degrades, intelligence is embedded across infrastructure, performance management, and development workflows.This allows applications to scale smoothly, predictably, and cost-efficiently as demand increases, without constant manual intervention.1. Automated Infrastructure Management Removes Scaling BottlenecksAs apps grow, manual infrastructure tuning becomes unsustainable. Human-defined rules and static configurations fail when traffic patterns shift rapidly or unpredictably.AI-native engineering changes infrastructure behavior by enabling systems to manage themselves. Intelligent models continuously analyze resource usage across containers, microservices, and environments, adjusting CPU, memory, and storage allocation in real time. This prevents both over-provisioning, which wastes cost, and under-provisioning, which causes performance degradation.The scalability impact is immediate and long-term. Applications maintain stable performance during sudden traffic spikes, while operational overhead decreases as infrastructure decisions no longer depend on constant human intervention. Scalability improves when infrastructure decisions are automated and data-driven, not manually enforced.2. Predictive Scaling and Intelligent Load BalancingReactive auto-scaling works only after systems start to struggle. By the time thresholds are crossed, users are already experiencing slowdowns, errors, or timeouts. This gap becomes more visible as apps scale and traffic patterns grow more complex.AI-native engineering introduces predictive scaling by learning from historical traffic, seasonal usage, feature launches, and real-time behavior. Instead of waiting for load to spike, the system anticipates demand and allocates resources in advance. This proactive approach keeps performance stable even during sudden surges.Intelligent load balancing further strengthens scalability by routing traffic based on service health, latency, and capacity rather than static rules. Unhealthy instances are avoided automatically, reducing the risk of cascading failures during peak usage. Predictive scaling keeps apps fast before growth becomes visible, not after problems appear.As applications grow, performance tuning becomes harder to manage manually. More services, APIs, databases, and integrations mean more places where latency can creep in quietly. Traditional approaches rely on engineers reacting to alerts after users start complaining.AI-native engineering changes this by making performance optimization continuous rather than episodic. AI systems monitor API response times, database queries, cache efficiency, and network latency in real time. Patterns that indicate emerging bottlenecks are detected early, often before they impact users.Instead of one-time fixes, AI-driven optimization adapts continuously. Slow queries are flagged, inefficient execution paths are identified, and resource allocation is adjusted automatically as usage evolves. Over time, the system learns which optimizations deliver the most impact under different conditions. AI-native systems don’t just scale under load, but learn how to perform better as scale increases.4. Faster Development Cycles Enable Scalability Through SpeedScalability is not only about infrastructure capacity. It is also about how quickly teams can adapt systems as demand grows. Slow development cycles turn scalability into a bottleneck because architecture, performance fixes, and optimizations lag behind user growth.AI-native engineering accelerates development by embedding intelligence into the software delivery process itself. AI-assisted testing, debugging, and code generation reduce the time required to validate changes under real-world load. Issues that would traditionally surface late in production are identified earlier in development.This speed allows teams to evolve backend services, APIs, and infrastructure incrementally without destabilizing the system. New features can be released confidently while performance safeguards remain intact. As demand changes, the architecture can adapt in parallel rather than playing catch-up. Applications scale more reliably when engineering velocity scales alongside user demand, not after it.5. Intelligent Resource Allocation Controls Cost at ScaleScalability often breaks down not because systems cannot handle growth, but because costs spiral faster than usage or revenue. Traditional scaling approaches add capacity reactively, leading to over-provisioned infrastructure during low demand and performance risks during spikes.AI-native engineering introduces intelligence into resource allocation. Instead of treating compute, memory, and storage as static pools, AI systems continuously analyze workload patterns and adjust resource usage at a granular level. This ensures that each service receives exactly what it needs, when it needs it.By eliminating waste across environments, AI-native systems keep infrastructure lean even as traffic grows. This creates predictable cost behavior while maintaining performance under load. Scaling becomes a controlled, measurable process rather than an expensive guess. Sustainable scalability depends on intelligent resource allocation, not simply adding more capacity.Core Pillars of AI-Native ScalabilityAI-native scalability is not driven by isolated tools or one-off optimizations. It is the result of a few foundational principles that shape how systems grow, adapt, and remain stable over time. When these pillars are designed into the architecture, scalability becomes predictable instead of reactive.1. Intent-Driven EngineeringIn AI-native systems, scaling decisions are tied directly to business intent. Instead of scaling based on raw infrastructure thresholds, systems understand why growth is happening and respond accordingly.This alignment ensures that technical expansion always supports real user demand and business outcomes, not just traffic volume.2. Continuous Learning SystemsEvery interaction becomes feedback. AI-native platforms learn from usage patterns, performance signals, and failures, refining how they scale over time.This allows systems to adapt naturally as products, users, and behaviors evolve.3. Proactive Monitoring and Anomaly DetectionRather than waiting for alerts after failures occur, AI-native monitoring identifies risks early. Anomalies are detected before they cascade, protecting performance as scale increases. Scalability improves when systems understand intent, learn continuously, and anticipate risk, not when teams react after problems appear.App scalability breaks down when growth is handled reactively rather than engineered proactively. AI-native engineering changes this by embedding intelligence into how systems are built, deployed, and evolved.Instead of relying on static rules and manual interventions, AI-native solutions automate infrastructure decisions, predict demand, optimize performance continuously, and control cost as scale increases.The result is not just the ability to handle more users, but the ability to grow intelligently without destabilizing performance or operations. Scalability becomes a built-in system behavior rather than an operational burden that teams constantly manage.Scalable apps are not tuned after growth, but are engineered to scale from the start through intelligence, automation, and learning. Partner with Quokka Labs to design and deliver AI native software engineering services that help applications scale predictably, control cost, and maintain performance as demand grows. Start with an AI-native scalability assessment tailored to your product.]]></content:encoded></item><item><title>I Built a Local AI Firewall and made it Open Source Because Nobody Else Was Going To !</title><link>https://hackernoon.com/i-built-a-local-ai-firewall-and-made-it-open-source-because-nobody-else-was-going-to?source=rss</link><author>Raviteja Nekkalapu</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:24:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most AI apps send raw user input straight to OpenAI with zero filtering. I got tired of seeing it and built Sentinel Protocol, an open source local proxy with 81 security engines that scans every LLM request and response. It blocks PII, catches prompt injections, detects hallucinated URLs in model output, and handles MCP poisoning for AI agents. One command to start. Change one line in your SDK. 52,069 lines of code, 9 dependencies, MIT license. No cloud, no telemetry, everything runs on your machine.]]></content:encoded></item><item><title>Plaid valued at $8B in employee share sale</title><link>https://techcrunch.com/2026/02/26/plaid-valued-at-8b-in-employee-share-sale/</link><author>Marina Temkin</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:19:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The new valuation is a 31% increase from $6.1 billion Plaid reached in April.]]></content:encoded></item><item><title>The Complete Guide to AI Agent Memory Files (CLAUDE.md, AGENTS.md, and Beyond)</title><link>https://hackernoon.com/the-complete-guide-to-ai-agent-memory-files-claudemd-agentsmd-and-beyond?source=rss</link><author>Paolo Perrone</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:16:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Learn how CLAUDE.md, AGENTS.md, and AI memory files work. Covers file hierarchy, auto-memory, @imports, and which files you actually need for your setup.]]></content:encoded></item><item><title>Why Some Companies Keep Changing — But Never Find Stability</title><link>https://hackernoon.com/why-some-companies-keep-changing-but-never-find-stability?source=rss</link><author>Nicolas Picks</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:13:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Strategies shift frequently. Teams reorganize. Priorities are updated again before the previous ones have fully settled.]]></content:encoded></item><item><title>The PS5 Controller Hack That Exposed Seven Thousand Living Rooms</title><link>https://hackernoon.com/the-ps5-controller-hack-that-exposed-seven-thousand-living-rooms?source=rss</link><author>Omotayo</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:11:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A simple project to use a PS5 controller on a robot vacuum accidentally exposed 7,000 homes. ]]></content:encoded></item><item><title>The TechBeat: AI Builder Stack: Linear, Cursor, Vercel &amp; QA.tech (2/27/2026)</title><link>https://hackernoon.com/2-27-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:11:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @lomitpatel [ 5 Min read ] 
 How CMOs win CFO buy-in using incrementality, trust, AI, and capital allocation to drive margin expansion and revenue durability. Read More.By @melissaindia [ 4 Min read ] 
 Learn 6 proven strategies to secure executive buy-in for Master Data Management by aligning MDM with ROI, risk reduction, and business goals. Read More.By @opensourcetheworld [ 7 Min read ] 
 I replaced $1,200/year in cloud subscriptions with one home server. Here's the setup, costs, apps, Bitcoin node, local AI, and what I'd do differently.  Read More.By @confluent [ 5 Min read ] 
 Learn how Python developers build real-time AI agents using MCP, Kafka, and Flink—modern agentic workflows explained on HackerNoon. Read More.By @saumyatyagi [ 15 Min read ] 
 Most teams plateau at "AI writes code, a human reviews it." This article presents the Dark Factory Pattern — a four-phase architecture using holdout scenarios a Read More.By @scylladb [ 4 Min read ] 
 Discover how Yieldmo migrated from DynamoDB to ScyllaDB to cut database costs, achieve multicloud flexibility, and deliver ads in single-digit millisecond laten Read More.By @khamisihamisi [ 4 Min read ] 
 Western tech is built in environments of abundance. In emerging markets, these assumptions often fail quickly. Read More.By @crafinsstudio [ 20 Min read ] 
 I tested eight piano apps on two pianos for three weeks. Here's what I'd actually recommend. Read More.By @scylladb [ 5 Min read ] 
 Blitz migrated from Postgres and Elixir to Rust and ScyllaDB, cutting latency, costs, and 100+ cores down to four cloud nodes. Read More.By @chris127 [ 8 Min read ] 
 Stablecoins aren't just "crypto dollars"—they're experiments in digital money stability. Each type offers different trade-offs, learn more about them here Read More.By @thomascherickal [ 51 Min read ] 
 Google Antigravity is not just for coding. It is for your entire computer. Stop scrolling - everything you do on a computer has just been automated. Read More.By @mexcmedia [ 2 Min read ] 
 MEXC ranks No. 1 globally in XAUT perpetual volume, hitting $3.43B as tokenized gold demand rises amid record spot gold prices in 2026. Read More.By @qatech [ 8 Min read ] 
 Manual testing can't keep up with modern development. See how QA.tech's AI testing automation catches bugs on every PR -- no Playwright or Cypress scripts to ma Read More.By @davidiyanu [ 8 Min read ] 
 Cloud cost and system reliability are the same problem viewed through different instruments.  Read More.By @vinitabansal [ 13 Min read ] 
  The more you adopt self-sabotage behaviors to deal with your feelings of self-doubt, the stronger those connections get. Read More.By @playerzero [ 11 Min read ] 
 How 2025 transformed AI from a developer tool into engineering infrastructure—and why operating it safely is now the real challenge. Read More.By @mexcmedia [ 2 Min read ] 
 MEXC’s February report shows 267% BTC coverage, rising ETH reserves, and monthly audited Proof of Reserves verified with Merkle Tree tech. Read More.By @scylladb [ 6 Min read ] 
 ZEE5 cut database costs 5X and achieved single-digit millisecond latency by migrating to ScyllaDB, redesigning APIs, and optimizing data models. Read More.]]></content:encoded></item><item><title>Anthropic, the Pentagon, and the Illusion of Conflict</title><link>https://hackernoon.com/anthropic-the-pentagon-and-the-illusion-of-conflict?source=rss</link><author>Alexander Borschel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:10:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Public tension between Anthropic and the Pentagon may be strategic signaling, not separation. Ethical positioning protects reputation, while defense integration continues through controlled deployments and negotiation. Public perception, procurement leverage, and backchannel talks all shape how AI becomes embedded in national security infrastructure. Anthropic’s public ethical stance and Pentagon pressure likely reflect negotiation leverage, not disengagement. AI integration continues behind controlled federal deployments.]]></content:encoded></item><item><title>NASA Reveals Identity of Astronaut Who Suffered Medical Incident Aboard ISS</title><link>https://science.slashdot.org/story/26/02/27/002252/nasa-reveals-identity-of-astronaut-who-suffered-medical-incident-aboard-iss?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Longtime Slashdot reader ArchieBunker shares a report from NBC News: NASA revealed that astronaut Mike Fincke was the crew member who suffered a medical incident at the International Space Station in January, which prompted the agency to carry out the first evacuation due to a medical issue in the space station's 25-year history. The rare decision to cut a mission short and bring Fincke and three other crew members home early made for a dramatic week in space early this year.
 
In a statement released by NASA "at the request of Fincke," the veteran astronaut said he experienced a medical event on Jan. 7 "that required immediate attention" from his space station crew members. "Thanks to their quick response and the guidance of our NASA flight surgeons, my status quickly stabilized," Fincke, 58, said in the statement. [...] In his statement, Fincke thanked his Crew-11 colleagues, along with NASA astronaut Chris Williams and Russian cosmonauts Sergey Kud-Sverchkov and Sergei Mikaev, who were also aboard the space station at the time and are still in space. Fincke also thanked the teams at NASA, SpaceX and the medical professionals at Scripps Memorial Hospital La Jolla. "Their professionalism and dedication ensured a positive outcome," he said.
 
Fincke ended his statement by saying he is "doing very well" and still actively involved with standard post-flight reconditioning at NASA's Johnson Space Center in Houston. "Spaceflight is an incredible privilege, and sometimes it reminds us just how human we are," he said. "Thank you for all your support."]]></content:encoded></item><item><title>The “Perfect First Draft” Trap Is Killing Your Output</title><link>https://hackernoon.com/the-perfect-first-draft-trap-is-killing-your-output?source=rss</link><author>BenoitMalige</author><category>tech</category><pubDate>Fri, 27 Feb 2026 06:10:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The more a thing tends to be permanent, the more it tends to be lifeless.\
Hey, listen. You’re not lazy. Let’s get that out of the way right now.\
Lazy people don’t lie awake at night, hearts pounding, staring at the ceiling while mentally screaming at themselves for not sending that email, not starting that proposal, not having that hard conversation.\
This is the special, advanced-class procrastination reserved for people who are highly competent. It’s the one that feels like a personal betrayal. You built a career on being reliable, on delivering, on being the person who gets shit done. And now you’re frozen by a single, seemingly simple task.\
A few weeks ago, I wanted to record a video. Simple task. Right?\
No. I procrastinated for two weeks.\
This was a feeling i’ve felt a thousand times. This time, I wanted to get to the bottom of it.Because it makes no sense. If you’re anything like me, you do what you’ve always done: you try to out-logic it.\
You break it down into smaller steps. You buy a new planner. You try the Pomodoro technique. And when that fails, you deploy the big gun: self-flagellation, aka .\
“What is wrong with you? You’re better than this. Just fucking DO IT.”\
And it doesn’t work. It just makes you feel worse, which makes the task feel even more monumental. It’s a shitty, vicious cycle, and the only thing it efficiently produces is shame.\
I used to think this was a moral failing. Some sort of crack in my discipline.Well… bad news for me, good news for you: I was looking at it all wrong.Procrastination isn’t the problem. It’s the alarm system.The Real Reason You’re Paralyzed (It’s Not What You Think)For years, I thought procrastination was about the . It’s not. It’s about the  you associate with the task.Let that sink in for a sec. You’re not avoiding the spreadsheet. You’re avoiding recording the video, or the anxiety of interpreting the numbers wrong. You’re not avoiding the difficult conversation with your employee or your partner.\
You’re avoiding the gut-wrenching discomfort of potential conflict, or the guilt of having to be the "bad guy", the feeling of being seen as someone who made a shitty piece of content. You’re afraid of being rejected, not loved, not worthy.Your brain, in its primitive wisdom, is doing a simple cost-benefit analysis: "Engage in this thing that will make me feel like crap, or… literally do anything else." It’s a rational choice 🙂\
The problem is, the "anything else" comes with a side order of self-criticism.\
So the first question to ask isn’t "How do I force myself to do this?" \n It’s:"What specific feeling am I trying to avoid, and why does it scare me so much?"Is it the fear of being judged? Of looking stupid? Of the exhausting mental effort required? Of the confrontation? Name the ghost. It loses power when you shine a light on it. Then, sit with it. Don’t try to fix anything. Just be with the feeling. Welcome it like it’s your own, because it is.\
If you want help seeing exactly what’s behind your resistance, take this ; it shows you how far you’ve drifted from alignment.The "Perfect First Draft" Trap That's Killing Your MomentumWhen I started this newsletter, I used to sit down to write something, and I’d expect the final, polished, brilliant version to just flow out of my fingertips on the first try. I had a reputation to uphold, after all.\
The result? I’d stare at a blank screen for an hour. I’d check my email, i’d open instagram to check on my 342 followers. I’d get a glass of water. I’d do anything  write. The gap between my vision of "perfect" and the reality of my first, shitty sentence was too vast to cross.\
The breakthrough only came when I gave myself permission to write a "vomit draft." No one would ever see it. It could be clumsy, poorly worded, and make no sense at all. The only goal was to get the ideas out of my head and onto the screen.Suddenly, writing wasn't a performance. It was an excavation. I could take that messy, ugly lump of clay and start shaping it. The first pass was for me. The second, third, and tenth passes were for everyone else.\
This is the iterative mindset. It’s not about getting it right. It’s about getting it . You can’t steer a parked car. You can’t edit a blank page. Your job isn't to be a genius on the first try. Your job is to be a stubborn, relentless editor of your own work.Iterate, Iterate, Iterate.Stop trying to build the perfect finished product. Just lay the first brick. It’s designed to be bad. Your job is fix it later.When Procrastination is Actually Your Wisest Self Screaming to Be HeardSometimes, the voice in your head saying "you  do this" isn't your intuition. It's the ghost of your old boss, your dad, some article you read, or society's boring-ass checklist for a successful life.\
Your procrastination, in these cases, is your soul's last-ditch effort to stage a protest.In plain, simple terms.. sometimes, it’s just not aligned with who you are or what you actually want.Have you ever had projects that you  were smart, lucrative opportunities? And you just… couldn't… start. You beat yourself up for weeks. Is this happening right now? Ask yourself this question:"If I wasn't  to be hard on myself, would I still be trying to do this?"\
The answer will probably be a resounding . The project is a "should" born out of obligation, not passion.\
The next time you're stuck, try this. And ask: "Is this  priority? Or is this someone else's agenda I've internalized? Is there a different, better, more  way to achieve the underlying goal?"\
The resistance might not be something to break through. It might be something to listen to.The Bottom Line You Probably Won't LikeBeating procrastination isn't about finding a new, more sophisticated way to bully yourself. It’s the exact opposite. It’s about getting curious. It’s about treating the resistance not as an enemy, but as a data point.\
Your brain (as usual) is trying to protect you from something. Your job is to figure out what, and then to gently, cleverly, redesign the process so it doesn't feel so threatening.\
Stop trying to win a war with yourself. You’ll always lose. Start a conversation instead.Your Actions for the Week (The Procrastination Prescription)You read the words. Now, here are the actions. Pick one and do it .\
1. The "Vomit Draft" Ship. Stop waiting for perfect. Pick one thing you're overthinking (an email, a project outline, a social post, a video script). Your mission is to create the most embarrassing, shitty first version possible and send it to one person or post it somewhere . The goal is to intentionally lower the bar so you can jump over it. Perfection is the enemy of done.2. The "Priority" Interrogation. Take 5 minutes. Write down the #1 thing you're procrastinating on. Now, ask yourself this one question: "If I was forbidden from being hard on myself, would I still be trying to do this?" Listen to the gut answer. If it's "Fuck no," you have your answer. It's not a priority; it's a prison. Cross it off or radically change the approach. If it's "Hell yes," then you know it's just fear, and you've already taken its power away by naming it.3. The "Visibility" Audit. You're hiding. We all are. Where are you playing small by choice? Make a list of 3 areas in your work or life where you're being  to avoid judgment or rejection. Is it not posting your ideas? Not speaking up in meetings? Not pitching that client? Pick ONE and break the pattern this week. Send one email you're scared to send. Say the thing in the meeting.  with worse ideas are already winning. Your value is useless if you hide it. Action → Feedback → Iteration → Identity. That's the entire game.]]></content:encoded></item><item><title>Victory! Tenth Circuit Finds Fourth Amendment Doesn’t Support Broad Search of Protesters’ Devices and Digital Data</title><link>https://www.eff.org/deeplinks/2026/02/victory-tenth-circuit-finds-fourth-amendment-doesnt-support-broad-search-0</link><author>Saira Hussain</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/protest-2024-2.jpg" length="" type=""/><pubDate>Fri, 27 Feb 2026 06:03:02 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[In a big win for protesters’ rights, the U.S. Court of Appeals for the Tenth Circuit overturned a lower court’s dismissal of a challenge to sweeping warrants to search a protester’s devices and digital data and a nonprofit’s social media data.The case, Armendariz v. City of Colorado Springs, arose after a housing protest in 2021, during which Colorado Springs police arrested protesters for obstructing a roadway. After the demonstration, police also obtained warrants to seize and search through the devices and data of Jacqueline Armendariz Unzueta, who they claimed threw a bike at them during the protest. The warrants included a search through all of her photos, videos, emails, text messages, and location data over a two-month period, as well as a time-unlimited search for 26 keywords, including words as broad as “bike,” “assault,” “celebration,” and “right,” that allowed police to comb through years of Armendariz’s private and sensitive data—all supposedly to look for evidence related to the alleged simple assault. Police further obtained a warrant to search the Facebook page of the Chinook Center, the organization that spearheaded the protest, despite the Chinook Center never having been accused of a crime.The district court dismissed the civil rights lawsuit brought by Armendariz and the Chinook Center, holding that the searches were justified and that, in any case, the officers were entitled to qualified immunity. The plaintiffs, represented by the ACLU of Colorado, appealed. EFF—joined by the Center for Democracy and Technology, the Electronic Privacy Information Center, and the Knight First Amendment Institute at Columbia University—wrote an amicus brief in support of that appeal.In a 2-1 opinion, the Tenth Circuit reversed the district court’s dismissal of the lawsuit’s Fourth Amendment search and seizure claims. The court painstakingly picked apart each of the three warrants and found them to be overbroad and lacking in particularity as to the scope and duration of the searches. The court further held that in furnishing such facially deficient warrants, the officers violated “clearly established” law and thus were not entitled to qualified immunity. Although the court did not explicitly address the First Amendment concerns raised by the lawsuit, it did note the backdrop against how these searches were carried out, including animus by Colorado Springs police leading up to the housing protest.It is rare for appellate courts to call into question any search warrants. It’s even rarer for them to deny qualified immunity defenses. The Tenth Circuit’s decision should be celebrated as a big win for protesters and anyone concerned about police immunity for violating people’s constitutional rights. The case is now remanded back to the district court to proceed—and hopefully further vindicate the privacy rights we all have in our devices and digital data.]]></content:encoded></item><item><title>2 Months Into 2026 We Are Over Half 2025’s Total Count Of Measles Cases</title><link>https://www.techdirt.com/2026/02/26/2-months-into-2026-we-are-over-half-2025s-total-count-of-measles-cases/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Fri, 27 Feb 2026 04:02:24 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Measles. Yes, yes, I know you’re sick of hearing about it. For that, though, you must lay the blame at the feet of Donald Trump, RFK Jr., and this entire administration of clown-tools that isn’t bothering to do anything about what has become the worst continuous outbreak of the disease in America in several decades. Their fault, not mine. And, yes, this is getting worse, not better. The CDC’s measles tracking site is a combination of woefully inaccurate and behind when it comes to current case counts (more to come on that shortly), but it’s at least useful in benchmarking what 2025 looked like. While certainly underreported, the CDC tallied 2,281 cases of measles in America last year. That site is updated only once a week on Fridays. Either due to that, or incompetence, or a more nefarious attempt to downplay the problem, the current case count is wrong.The CDC site shows a 2026 case count of 982. That would be bad enough, but it’s actually worse. The actual count is well over 1,000 cases, which means we’re somewhere right around half of 2026’s case total as of right now. So you don’t feel the need to check a calendar, it’s still February.“It is very concerning to see more than 1,000 cases in the U.S. this early in the year,” Martha Edwards, MD, president of the South Carolina Chapter of the American Academy of Pediatrics, told MedPage Today. “Already, we have more than half the number of cases seen in all of 2025, and the number of cases in 2025 was one of the highest annual case counts seen in decades.”“As people continue to believe inaccurate information about vaccines, and as non-medical exemption rates continue to rise throughout the country, we can expect case counts to continue to rise, threatening children and immunocompromised individuals with a disease that was nearly eliminated in our country through vaccination,” she added.The true number is going to be even higher than that. There are outbreaks of one size or another in many, many states. South Carolina alone has nearly 1,000 reported cases. The truly frustrating thing about all of this is that this problem is a simple one to fix. More people need to get vaccinated for measles via the widely available MMR vaccine. To achieve that, the government needs to do two simple things. First, cut the shit when it comes to the misinformation about vaccines that is scaring the hell out of a percentage of the population. In fact, advocate for those same vaccines. Get Kennedy hopped up on those psychedelics he likes if you need to, but he needs to be front and center telling people to get vaccinated. And stop the nonsense that is going on with supposed religious exemptions for vaccinations.Edwards highlighted the need for “accurate information about the dangers of measles virus and the complications that can ensue, in addition to communicating the safety and efficacy of the measles vaccine.”“Raising the bar to obtain non-medical exemptions for vaccines and requiring families to gain accurate information about the dangers of vaccine-preventable illnesses and the importance of vaccines would be a huge benefit in helping to raise vaccination rates in South Carolina and the rest of the country,” she added. “We would love to see a requirement for parents to come in person to the health department, watch a video on vaccine-preventable illnesses, and have a conversation with a healthcare professional before they choose non-medical exemptions.”Second, take the data collection and sharing about measles seriously. Along those same lines, demonstrate leadership by helping state governments and local medical facilities collect and share data, strategize protective measures to stop the spread of the disease, and pump the ecosystem full of real-time accurate information about where the disease is, how it spreads, and how to handle an infection. That isn’t happening. Instead, you get stories like how South Carolina’s state government doesn’t require any mandatory reporting of measles cases in the state when patients are admitted. One doctor in the state had to find out that patients in her own area had been hospitalized with measles from Facebook.Dr. Leigh Bragg, a pediatrician working a county away, wasn’t even aware that anyone in South Carolina had been hospitalized with measles-related illnesses until a short time later when she logged on to Facebook and saw someone relay the distraught husband’s comments. Part of the reason Bragg didn’t know is that South Carolina doesn’t require hospitals to report admissions for measles, potentially obscuring the disease’s severity. In the absence of mandatory reporting rules, she and other doctors are often left to rely on rumors, their grapevines of colleagues, and the fragments of information the state public health agency is able to gather and willing to share. So, what you get is South Carolina reporting that roughly 2% of its measles cases have resulted in hospitalization. Nobody with any knowledge of measles thinks that is even remotely accurate. “A hospitalization rate at 2% is ludicrous,” said Dr. Paul Offit, director of the Vaccine Education Center and an infectious disease physician at Children’s Hospital of Philadelphia who served on the Centers for Disease Control and Prevention’s immunization advisory committee. “It’s vast underreporting,” Offit said. “Measles makes you sick.”Without that sort of accurate data, neither the state nor federal government knows where to help, nor how how much help is needed. If Kennedy and Trump wanted to actually confront this growing problem, that’s the kind of organization the federal government and its health-related agencies could help with. But this administration seems content to put its hands over its eyes and shout, “Nuh uh, I can’t see you!”This is going to continue to get worse until real action is taken. Until then, I guess we all just try to keep an eye out for rashes. ]]></content:encoded></item><item><title>Anthropic CEO Says AI Company &apos;Cannot In Good Conscience Accede&apos; To Pentagon</title><link>https://tech.slashdot.org/story/26/02/26/2352217/anthropic-ceo-says-ai-company-cannot-in-good-conscience-accede-to-pentagon?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the Associated Press: Anthropic CEO Dario Amodei said Thursday the artificial intelligence company "cannot in good conscience accede" to the Pentagon's demands to allow wider use of its technology. The maker of the AI chatbot Claude said in a statement that it's not walking away from negotiations, but that new contract language received from the Defense Department "made virtually no progress on preventing Claude's use for mass surveillance of Americans or in fully autonomous weapons."
 
The Pentagon's top spokesman has reiterated that the military wants to use Anthropic's artificial intelligence technology in legal ways and will not let the company dictate any limits ahead of a Friday deadline to agree to its demands. Sean Parnell said Thursday on social media that the Pentagon "has no interest in using AI to conduct mass surveillance of Americans (which is illegal) nor do we want to use AI to develop autonomous weapons that operate without human involvement."
 
Anthropic's policies prevent its models, such as its chatbot Claude, from being used for those purposes. It's the last of its peers -- the Pentagon also has contracts with Google, OpenAI and Elon Musk's xAI -- to not supply its technology to a new U.S. military internal network. Parnell said the Pentagon wants to "use Anthropic's model for all lawful purposes" but didn't offer details on what that entailed. He said opening up use of the technology would prevent the company from "jeopardizing critical military operations." "We will not let ANY company dictate the terms regarding how we make operational decisions," he said. In a post on X, Parnell said Anthropic will "have until 5:01 PM ET on Friday to decide. Otherwise, we will terminate our partnership with Anthropic and deem them a supply chain risk for DOW."]]></content:encoded></item><item><title>dReLU Activation Function: Matching SwiGLU Performance with 90% Sparsity</title><link>https://hackernoon.com/drelu-activation-function-matching-swiglu-performance-with-90percent-sparsity?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Fri, 27 Feb 2026 03:07:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We introduce a new activation function, named dReLU (Equation 2), where ReLU is applied after both the up- and gate-projection[1].\
To demonstrate the effectiveness and performance of dReLU, we conducted an experiment comparing 300M-parameter decoder-only architecture models using dReLU and SwiGLU, both pretrained under the fineweb dataset [47] for 5B tokens. Refer to Appendix A.1 for the detailed model architecture hyperparameters. The evaluation result is shown in Table 2.\
Our findings reveal models employing the dReLU structure exhibit similar convergence compared to those using the SwiGLU structure. Notably, we evaluate the perplexity of both models on Wikitext2 [39]. DReLU-based models show slightly better performance on WikiText-2 [39].\
Figure 4 illustrates the loss curves during training, demonstrating that models with the dReLU activation function achieve similar convergence ability compared to their SwiGLU counterparts. To further validate this observation, we evaluate the perplexity of these models on the Wikitext2 dataset. As shown in Table 2. Notably, although SwiGLU-based model has lower training loss, dReLU based model has lower validation perplexity. These results provide strong evidence that adopting the dReLU structure does not compromise model performance. We evaluate on more downstream tasks in Appendix A.1.\
Another question we need to address is the dReLU-based model’s sparsity. To investigate the sparsity of the dReLU-based model, we propose a methodology for measuring and evaluating a model’s performance under different sparsity levels. Our approach involves selecting the top-k% of values activated by dReLU or other activation functions based on their absolute magnitude, as described in Equations 3 and 4.(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(5) Li Ma, Shanghai Artificial Intelligence Laboratory;(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.[1] We omit the bias in both the up- and gate-projection to match the form of Equation 1.]]></content:encoded></item><item><title>Analyzing ReLUfication Limitations: Enhancing LLM Sparsity via Up Projection</title><link>https://hackernoon.com/analyzing-relufication-limitations-enhancing-llm-sparsity-via-up-projection?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Fri, 27 Feb 2026 03:02:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[3.1 Limitations about Existing ReLUfication\
We first evaluate the sparsity of ReLULlama-7B [59] and the original Llama-2-7B [60], as shown in Table 1. The results reveal that existing ReLUfication methods can only improve the sparsity from 40% to 67%, indicating their limited effectiveness in significantly enhancing model sparsity.\
To investigate the underlying reasons for this limitation, we profile the activation distribution of the gate and up projection components separately in ReLULlama-7B and Llama-2-7B, as illustrated in Figure 3. The figure shows that after ReLUfication, the combined activation becomes more concentrated around 0, with the sparsity increasing to 67%. This can be attributed to the ReLU activation function applied after the gate weight, which masks all negative activations to zero.\
To further push the sparsity, shifted-ReLU [42] has been proposed, which adjusts the threshold of ReLU function to mask out more activations in the gate projection. However, the improvements brought by this method are limited. Another line of work is to adopt progressive sparsity regularization to the intermediate output to introduce more zero activation output [55]. However, this method carries the risk of performance degradation.\
Existing ReLUfication methods primarily focus on modifying the gate component. Different from previous work, we find that existing ReLUfication doesn’t alter the activation distribution of the up projection component, as shown in Figure 3(c) and (f). According to the definition of Gated-MLP (Equation 1), the gate and up projection components jointly influence the sparsity of neuron activations in parallel. However, a significant number of activation values in the up projection component remain less than 0. This suggests that masking the outputs of the up and gate matrices that are less than 0 as inactive could introduce stronger sparsity without sacrificing non-linear capabilities. This observation motivates us to explore the possibility of further enhancing model sparsity by modifying the up projection.(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(5) Li Ma, Shanghai Artificial Intelligence Laboratory;(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.]]></content:encoded></item><item><title>Optimizing LLM Inference: Sparse Activation, MoE, and Gated-MLP Efficiency</title><link>https://hackernoon.com/optimizing-llm-inference-sparse-activation-moe-and-gated-mlp-efficiency?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Fri, 27 Feb 2026 02:56:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Efficient Inference of LLMs. Efficient LLM inference poses challenges that necessitate a synergistic combination of algorithmic and systemic approaches. From an algorithmic standpoint, researchers have explored various methods to reduce computation and memory overheads, including compressing models [40, 63, 18, 34, 61], modifying model structures [3, 21], and speculative decoding methods [32, 12, 10]. On the systemic front, there are efforts that effectively integrate the features of downstream hardware and upper-level models to maximize the efficiency of computation and memory utilization [4, 49, 16, 64], leading to the development of more efficient frameworks like vLLM [29].\
Sparse activation, in particular, has emerged as a research area that demands an even tighter integration of algorithmic and systemic approaches. The selection of activation functions and the construction of activation predictors are algorithmic problems, while fully exploiting the sparse activation of LLMs on specific hardware is a systemic challenge. By leveraging sparse activation, researchers have achieved promising results in building efficient LLM inference systems [36, 56].\
. MoE techniques induce effective sparsity in LLMs by determining which subset of subnetworks (referred to as "experts") to activate during the inference pass, often through a trained "router" subnetwork. This approach allows the model to enhance its capacity without escalating the computational expenses [31, 53].\
Intrinsic Activation Sparsity. Intrinsic activation sparsity is known to be present in LLMs that utilize ReLU family nonlinearities in their MLP blocks [68, 33]. This phenomenon has been explored to accelerate inference speed and reduce memory usage [56, 36, 37]. With this phenomenon, each neuron can be viewed as an expert to reduce the computation overhead.\
 We now delve into the components of LLMs that our study aims to analyze: the Gated-MLP blocks, which are commonly used. A Gated-MLP block consists of three fully\
connected layers and performs the following computation:(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(5) Li Ma, Shanghai Artificial Intelligence Laboratory;(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.]]></content:encoded></item><item><title>TurboSparse-LLM: Accelerating Mixtral and Mistral Inference via dReLU Sparsity</title><link>https://hackernoon.com/turbosparse-llm-accelerating-mixtral-and-mistral-inference-via-drelu-sparsity?source=rss</link><author>Language Models (dot tech)</author><category>tech</category><pubDate>Fri, 27 Feb 2026 02:51:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixtureof-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5× decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at https://huggingface.co/PowerInfer.Large Language Models (LLMs) have achieved remarkable results, demonstrating emergent natural language abilities as the number of model parameters scales [9, 67]. These models have pushed the state-of-the-art performance across a wide range of downstream applications, such as QA and coding. However, most LLMs, such as Llama [60], Mistral [24], and Gemma [58], utilize all of their parameters during inference. These are known as dense models. The escalating demand for computational resources by dense models has become a significant barrier to the development of powerful and accessible AI, given the substantial costs involved.\
To address the efficiency issues inherent in dense models, conditional computation [7, 6] has emerged as a crucial approach, which refers to activating part of the neurons in a network. There are two primary methods to achieve conditional computation. Mixture-of-Experts (MoE) [17, 31] is the first promising method, which introduces conditional computation by manually setting constraints on the model architecture prior to training, such as determining the number of experts to activate. This technique selectively activates specific parts of the model in response to particular inputs through a process known as expert routing, resulting in significant efficiency improvements. For instance, Switch Transformer [17] has scaled the model to the trillion-parameter level without increasing computational FLOPs significantly. Another promising method is utilizing the natural emergence of sparse activation due to the ReLU activation function [33], which naturally outputs zero elements that have no contribution in computation results. This activation sparsity presents a significant opportunity for efficient inference. Deja Vu [36] utilizes that sparsity exists in dense models due to ReLU to achieve 2× speedups. PowerInfer [56] achieving up to 11× speedups for deploying larger LLMs in a single consumer-grade GPU setting.\
Recent LLMs typically prefer activation functions such as GELU [23] and Swish [50]. However, these functions do not significantly promote activation sparsity and are challenging to accelerate with conditional computation. To address this, ReLUfication [42], an existing state-of-the-art method, replaces the original activation function with ReLU and continues with pretraining. Despite its potential, this approach often struggles to achieve the desired levels of activation sparsity and may risk performance degradation [30, 59].\
We argue that the failure of existing ReLUfication methods can be attributed to two main reasons. First, simply substituting SwiGLU with ReGLU is inefficient, as it only increases sparsity from 40% to around 70%. It suggests that a deeper investigation into the model architecture is necessary to achieve higher levels of sparsity. Second, the limited diversity of pretraining data and the insufficient number of training tokens in current approaches lead to incomplete capability recovery [42, 30]. As a result, expanding the diversity of pretraining datasets and increasing the number of training tokens are critical steps towards enhancing model performance.\
To address these challenges, we first conduct a comprehensive analysis of the existing ReLUfication approach and identify that its shortcomings stem from the negative activations in the GLU component. Therefore, we propose an efficient activation function named dReLU. We apply dReLU in the pretraining of small-scale LLMs, alongside SwiGLU, and our findings indicate that LLMs using dReLU match the performance of those using SwiGLU, while also achieving close to 90% sparsity. Additionally, we collect a diverse range of pretraining corpora from the open-source community, including web, code, and mathematical datasets, to enhance the effectiveness of ReLUfication.\
Meanwhile, we also conduct a sparsity analysis on MoE-based LLMs. Interestingly, we observe that the feed-forward networks (FFNs) within the experts remain sparsely activated, similar to the behavior exhibited by dense LLMs. This phenomenon suggests an opportunity to further accelerate inference speed by combining MoE techniques with ReLU-based sparse activation.\
To validate the effectiveness of our proposed method, we implemented it on the Mistral-7B and Mixtral-47B models, converting them to TurboSparse-Mistral-47B and TurboSparse-Mixtral-47B, respectively. Extensive experiments across a wide range of downstream tasks demonstrate (Figure 1) that our enhanced models not only meet but often surpass the performance of their original counterparts.\
Remarkably, in the TurboSparse-Mistral-7B model, we increase the average sparsity of the FFN to 90% while enhancing model capabilities. In MoE models, we further improve the sparsity in the TurboSparse-Mixtral-47B, originally introduced due to expert routing, from 75% to 97% by\
incorporating sparse neuron activations. This substantial increase in sparsity significantly reduces FLOPs during the inference process.\
Finally, we integrate our two new models with PowerInfer to evaluate the inference speed. Performance evaluation reveals that our models deliver an average 2.83× generation speedup.\
The key contributions of this paper include:\
• Efficient dReLU activation function: Our method utilizes fewer than 150B tokens, representing less than 1% of the typical pretraining tokens (commonly 15T tokens [11]).\
• Sparse activated models: We will release our sparsely-activated TurboSparse-Mistral7B and TurboSparse-Mixtral-47B models. Both models demonstrate better performance compared to the original versions.\
• Practical inference speedup: Evaluation shows that with our models, we can achieve a 2-5× speedup. Notably, we can achieve up to 10 tokens/s even without a GPU on TurboSparse-Mixtral-47B.(1) Yixin Song, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(2) Haotong Xie, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(3) Zhengyan Zhang, Department of Computer Science and Technology, Tsinghua University;(4) Bo Wen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University;(5) Li Ma, Shanghai Artificial Intelligence Laboratory;(6) Zeyu Mi, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University Mi yzmizeyu@sjtu.edu.cn);(7) Haibo Chen, Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University.]]></content:encoded></item><item><title>Data Centres: Will They Create a New Indian IT Boom?</title><link>https://hackernoon.com/data-centres-will-they-create-a-new-indian-it-boom?source=rss</link><author>Vipin Labroo</author><category>tech</category><pubDate>Fri, 27 Feb 2026 02:36:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
​India ostensibly has everything that is required to help it make one of the leading data centre hubs in the world. From low infrastructure and manpower costs to active government support, it apparently has things neatly lined up to help it do just that. But given the existing power situation in the country and the excessive reliance on coal-generated electricity, there are many challenges to be overcome to get there. \
These include straining the water resources available in the country, which are already under intense pressure to cater to the needs of nearly a billion and a half people. Land acquisition and the creation of a suitably skilled workforce are other major challenges that need to be addressed.Data Centres- The Only Path Ahead for the Indian IT Sector?​The IT sector is one of the few success stories that the country has seen with regard to providing a well-paying career path to millions of Indian youth this century. Apart from the technically qualified who take up jobs in this sector, there is an even larger number of people who find indirect employment working as security staff, drivers, real estate managers, and canteen personnel.\
​The impending wind-down of the IT sector puts millions of livelihoods at stake, which makes it imperative that the opportunity afforded by the likely emergence of data centres as a major growth area acts as a godsend. Worldwide, the massive growth of cutting-edge technologies like AI, 5G, cloud computing, and the IoT has led to an enormous surge in the demand for data storage, thereby making data centres a vitally critical component of the upcoming and emerging technology eco-systems. \
Besides, data centres are also required to help manage the data as well as processing needs of the high-growth sectors like finance, e-commerce, healthcare, gaming, and a host of other industries that rely on dependable digital support in the shape of world-class data centres.\
As a matter of fact, it is raining investments in India, when it comes to major Indian and international business groups' interest in helping grow thedata centre sector. Can focusing on data centres move India down the IT value chain?For all those tom-tomming the impending rise of India as a major global data centre hub, there is a contrary school of thought that fears that this could possibly move the Indian industry down the IT value chain. It suggests that rather than being known for low-end server farms, India should stride ahead with a focus on creating world-class SaaS products, designing chips, and building cutting-edge new-age AI models.Despite the misgivings of a few, the overriding consensus is that data centre growth augurs well for the Indian IT industry as it helps lay the foundation for the accelerated growth and adoption of AI and Cloud technologies across the world. \
​They hold that it makes eminent sense for a nation that boasts the largest and most data-intensive mobile user numbers in the world to work on expanding data centre capacity expansion on an unprecedented scale. Besides, the rapid and massive growth and adoption of AI have necessitated that data centre growth in the country be encouraged in every possible manner. Doing so also helps protect data sovereignty, something to which the government is fully committed.\
Clearly, the data centre story has just begun in India.]]></content:encoded></item><item><title>LXD 6.7 Released With AMD GPU Passthrough Support</title><link>https://www.phoronix.com/news/LXD-6.7-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 01:09:18 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Canonical today released LXD 6.7 as the latest feature update to this system container and virtual machine manager commonly used in Ubuntu Linux environments...]]></content:encoded></item><item><title>Three Alternatives to Measure the Elapsed Time of Code Execution</title><link>https://hackernoon.com/three-alternatives-to-measure-the-elapsed-time-of-code-execution?source=rss</link><author>Nicolas Fränkel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 00:50:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[For as long as I have been coding in Java, we have had requirements to measure the execution time of blocks of code. While  the current good practice is to use OpenTelemetry's traces, not every company has reached this stage yet. Plus, some of the alternatives are OpenTelemetry-compatible. Let's see them in order.The basic option is what we have been doing for ages, and what the other options rely on anyway. It's based on the following API: System.currentTimeMillis().\
Usage is pretty straightforward: \n long start = System.currentTimeMillis();
// Execute code
long end =  System.currentTimeMillis();
System.out.println("Code executed in " + (end - start) + " milliseconds");
The Object-Oriented AlternativeIf you are an OOP developer, then you'd rather encapsulate state. Here's a draft of such encapsulation: \n class Timer {
    private long startTime;

    public void start() {
        startTime = System.currentTimeMillis();
    }

    public long stop() {
        return System.currentTimeMillis() - startTime();
    }
}
\
You can add a  method to reuse the object, a  method to pause the timing, or decide to have a separate  method on top of . Nothing changes the overall design. Both Guava and Apache Commons Lang provide a  class, with minor variations. If either of them is on the classpath, don't reinvent the wheel and use it.The Functional AlternativeA functional approach is also possible if that's what you prefer. Let's start by timing a method that accepts a parameter and returns a value, in other words, a java.util.function.Function. We can wrap it in a timing method that accepts the said function, and a . \n public class TimeUtils {
    public static <I,O> O time(I input, Function<I,O> function, Consumer<Long> time) {
        long start = System.currentTimeMillis();                     //1
        O result = function.apply(input);                            //2
        time.accept(System.currentTimeMillis() - start);             //3
        return result;
    }
}
Compute the time and wrap it in a consumer\
We can use it like this: \n var result = TimeUtils.time(
    0,                                                               //1
    value -> value + 1,                                              //2
    time -> System.out.println("Time: " + time  + " ms"));           //3
System.out.println("value: " + result);
\
We must add more code to cater to other method signatures. \n public class TimeUtils {

    public static <T> T time(Supplier<T> supplier, Consumer<Long> time) {      //1
        long start = System.currentTimeMillis();
        T result = supplier.get();
        time.accept(System.currentTimeMillis() - start);
        return result;
    }

    public static <T> T time(T t, Consumer<T> consumer, Consumer<Long> time) { //2
        long start = System.currentTimeMillis();
        consumer.accept(t);
        time.accept(System.currentTimeMillis() - start);
    }
}
\
You can do the same with  and . If your methods require more than two parameters, you'll need to model more functional interfaces, , , , etc.The Annotations AlternativeAn annotation, , An annotation processor, which scans methods and filters those annotated with  at either compile-time or runtime\
In the first case, we rely on bytecode manipulation to weave the OOP or functional alternatives above around the original method. In the second one, we would need to intercept the instantiation of classes having such annotated methods, and wrap them in a java.lang.reflect.Proxy. In both cases, you generate additional code to:Call the annotated methodDo something with the time, , log execution time\
The code is more complex than the two previous alternatives, so I won't develop it further.\
AspectJ, an Aspect-Oriented Programming framework, provides the overall engine. You'll only need to develop the annotation and the wrapping code.In this post, I described three alternatives to measure the elapsed time of code execution: object-oriented, functional, and annotations. Depending on your context, you may want to use one or the other. In any case, I suggest you don't roll out your own, but use one of the available libraries/frameworks.Originally published at A Java Geek on February 22nd, 2026]]></content:encoded></item><item><title>Four Convicted Over Spyware Affair That Shook Greece</title><link>https://yro.slashdot.org/story/26/02/26/2242230/four-convicted-over-spyware-affair-that-shook-greece?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A Greek court has convicted four individuals linked to the marketing of Predator spyware in the wiretapping scandal that shook the country in 2022. The BBC reports: In what became known as "Greece's Watergate," surveillance software called Predator was used to target 87 people -- among them government ministers, senior military officials and journalists. The four who had marketed the software were found guilty by an Athens court of misdemeanours of violating the confidentiality of telephone communications and illegally accessing personal data and conversations.
 
The court sentenced the four defendants to lengthy jail sentences, suspended pending appeal. Although they each face 126 years, only eight would be typically served which is the upper limit for misdemeanors. One in three of the dozens of figures targeted had also been under legal surveillance by Greece's intelligence services (EYP). Prime Minister Kyriakos Mitsotakis, who had placed EYP directly under his supervision, called it a scandal, but no government officials have been charged in court and critics accuse the government of trying to cover up the truth.
 
The case dates back to the summer of 2022, when the current head of Greek Socialist party Pasok, Nikos Androulakis - then an MEP - was informed by the European Parliament's IT experts that he had received a malicious text message containing a link. Predator spyware, marketed by the Athens-based Israeli company Intellexa, can get access to a device's messages, camera, and microphone. Its use was illegal in Greece at that time but a new law passed in 2022 has since legalised state security use of surveillance software under strict conditions. Androulakis also discovered that he had been tracked for "national security reasons" by Greece's intelligence services. The scandal has since escalated into a debate over democratic accountability in Greece.]]></content:encoded></item><item><title>Fast KV Compaction Makes Long Context LLMs Practical</title><link>https://hackernoon.com/fast-kv-compaction-makes-long-context-llms-practical?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Fri, 27 Feb 2026 00:44:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Fast KV Compaction via Attention Matching shows how to compress LLM KV cache in seconds, not hours, while preserving long-context performance.]]></content:encoded></item><item><title>Ubuntu 26.04 Resolute Snapshot 4 Released</title><link>https://www.phoronix.com/news/Ubuntu-26.04-Snapshot-4</link><author>Michael Larabel</author><category>tech</category><pubDate>Fri, 27 Feb 2026 00:26:56 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The fourth and final monthly snapshot of Ubuntu 26.04 "Resolute Raccoon" is now available for testing. This alternative to the Ubuntu 26.04 daily ISOs is a monthly test release that also helps exercise the Ubuntu Linux release automation processes...]]></content:encoded></item><item><title>How to Reverse Video with fal-ai’s FFMPEG Utility</title><link>https://hackernoon.com/how-to-reverse-video-with-fal-ais-ffmpeg-utility?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Fri, 27 Feb 2026 00:14:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Learn how fal-ai’s workflow-utilities/reverse-video reverses playback for creative effects, motion analysis, and advanced video editing workflows.]]></content:encoded></item><item><title>Colorado Lawmakers Push for Age Verification at the Operating System Level</title><link>https://tech.slashdot.org/story/26/02/26/233213/colorado-lawmakers-push-for-age-verification-at-the-operating-system-level?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Fri, 27 Feb 2026 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Colorado lawmakers are proposing SB26-051, a bill that would require operating systems to register a user's age bracket and share it with apps via an API. PCMag reports: The bill comes from state Sen. Matt Ball and Rep. Amy Paschal, both Democrats. "The intent is to create thoughtful safeguards for kids online through a privacy-forward framework for age assurance," Ball told PCMag. "Unlike some laws in other states, SB 51 doesn't require users to share personally identifiable information or use facial recognition technology."
 
The legislation also promises to centralize the age check through the OS, rather than mandating that each app enforce their own age-verification mechanism, which can involve scanning the user's official ID, thus raising privacy and security concerns. The bill also forbids the sharing of the age-bracket data for any other purpose. But it looks like it's easy to bypass the age check proposed by SB26-051. The legislation itself doesn't mention any state ID check to verify the owner's age. In addition, the bill doesn't seem to cover websites, only apps and app stores. The report notes that the legislation was based on California's bill AB 1043, which was passed last year and expected to take effect January 1, 2027.]]></content:encoded></item><item><title>Netflix backs out of bid for Warner Bros. Discovery, giving studios, HBO, and CNN to Ellison-owned Paramount</title><link>https://techcrunch.com/2026/02/26/netflix-warner-bros-discovery-paramount-wbd-bid-studios-hbo-cnn-ellison/</link><author>Graham Starr</author><category>tech</category><pubDate>Thu, 26 Feb 2026 23:55:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In a one-two punch of centibillion-dollar offers, the bidding war for Warner Bros. Discovery is over. David Ellison-owned Paramount will acquire Warner Bros. Discovery. Netflix has lost.]]></content:encoded></item><item><title>GUI-Owl-1.5 Brings Cross-Device AI Agents Closer to Reality</title><link>https://hackernoon.com/gui-owl-15-brings-cross-device-ai-agents-closer-to-reality?source=rss</link><author>aimodels44</author><category>tech</category><pubDate>Thu, 26 Feb 2026 23:44:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[GUI-Owl-1.5 shows how AI agents can automate tasks across phones, PCs, and browsers using multi-platform training, reasoning, and RL.]]></content:encoded></item><item><title>Jack Dorsey just halved the size of Block’s employee base — and he says your company is next</title><link>https://techcrunch.com/2026/02/26/jack-dorsey-block-layoffs-4000-halved-employees-your-company-is-next/</link><author>Connie Loizos</author><category>tech</category><pubDate>Thu, 26 Feb 2026 23:43:32 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Jack Dorsey has long been an open admirer of Elon Musk. Now, it seems, he may have been taking notes.]]></content:encoded></item><item><title>Jack Dorsey&apos;s Block Cuts Nearly Half of Its Staff In AI Gamble</title><link>https://slashdot.org/story/26/02/26/2250206/jack-dorseys-block-cuts-nearly-half-of-its-staff-in-ai-gamble?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Feb 2026 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Jack Dorsey's Block is cutting more than 4,000 jobs, or nearly half its workforce, as part of a deliberate shift toward becoming a smaller, "intelligence-native" company built around AI. The Verge reports: "We're not making this decision because we're in trouble," Dorsey says. "Our business is strong. Gross profit continues to grow, we continue to serve more and more customers, and profitability is improving. But something has changed. We're already seeing that the intelligence tools we're creating and using, paired with smaller and flatter teams, are enabling a new way of working which fundamentally changes what it means to build and run a company. And that's accelerating rapidly."
 
Dorsey opted to do a big layoff instead of gradual cuts because "I'd rather take a hard, clear action now and build from a position we believe in than manage a slow reduction of people toward the same outcome." The layoffs were announced on Thursday as part of the company's Q4 2025 earnings. In a shareholder letter (PDF), Dorsey says that "We believe Block will be significantly more valuable as a smaller, faster, intelligence-native company. Everything we do from here is in service of that."]]></content:encoded></item><item><title>Anthropic CEO stands firm as Pentagon deadline looms</title><link>https://techcrunch.com/2026/02/26/anthropic-ceo-stands-firm-as-pentagon-deadline-looms/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Thu, 26 Feb 2026 23:19:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Anthropic CEO Dario Amodei said Thursday that he "cannot in good conscience accede" to the Pentagon's demands to give the military unrestricted access to its AI systems. ]]></content:encoded></item><item><title>Outtelligence: The Advantage AI Cannot Compound</title><link>https://hackernoon.com/outtelligence-the-advantage-ai-cannot-compound?source=rss</link><author>Nomcebo Mkhize</author><category>tech</category><pubDate>Thu, 26 Feb 2026 23:05:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I’m going to introduce a new word. Not because the idea is new. But because the moment demands a name for it. The word is .\
And if artificial intelligence continues accelerating the way it is, this may become the most important cognitive skill of the next decade.\
If you study capital markets long enough, you learn something simple:\
When something becomes abundant, its value drops.  used to be scarce. Now it’s infinite.  used to be expensive. Now it’s automated.  is becoming a commodity.\
That should concern you.\
Because when intelligence becomes cheap, the real advantage shifts somewhere else.\
I call that shift .Outtelligence is not intelligence. It is the ability to step outside the system intelligence operates in. Intelligence answers questions. Outtelligence questions the frame that produced the question. Intelligence optimizes. Outtelligence reallocates.\
If intelligence plays the game better, outtelligence decides whether the game is worth playing.Large language models, including those developed by OpenAI, operate by predicting high-probability patterns from historical data. That makes them powerful.\
It also makes them traditional.\
AI is trained on what has already stabilized.AI compounds consensus. It does not initiate structural rebellion.\
And that is where most people misunderstand the threat.\
The danger is not that AI replaces your job.\
The danger is that AI shapes your frame. doesn't compete where everyone is competing. He buys mispriced assets. Outtelligence works the same way. If everyone has AI-assisted intelligence, then intelligence is no longer a long-term advantage. Frame independence is.Power does not sit in effort. Power sits in perception control. If AI systems generate the dominant language, they begin influencing:What feels authoritative?Most professionals will unknowingly conform to machine-shaped consensus. Outtelligence is resistance to invisible alignment.\
It is the ability to ask:What perspective is missing?Markets reward differentiation. Not participation. If AI makes everyone clearer, faster, and more articulate, then clarity alone stops being impressive.\
Outtelligence becomes the differentiator because it produces.Strategic lack of balance asymmetryIn marketing terms: .\
Outtelligence creates new categories.The Cognitive Shift (System Model)Problem → Struggle → Reflection → Insight → Mental Model UpgradeProblem → Prompt → Output → PublishEfficiency increased. Cognitive friction decreased. The quiet cost is: Friction is where independent models are built. Without friction, you get fluency. Not depth.Intelligence vs OuttelligenceData → Analysis → Optimization → PerformanceAssumptions → Incentives → Frame Shift → Structural AdvantageIntelligence improves the machine. Outtelligence questions the machine.The Real Divide of the Next DecadeIt won’t be: AI users vs non-users.\
It will be: AI-dependent thinkers vs AI-sovereign thinkers.Accept generated framing.Optimize within algorithmic boundaries.Use the AI's probabilistic nature against itself, thus deliberately exploring the long tail of possibilities it was trained to deprioritize.\
The tool does not determine the frame. The user does. But this requires something difficult: the discipline to use AI without being subtly shaped by its gravitational pull toward consensus.\
Could AI itself help us step outside the frame? A skilled user can prompt for the improbable, not just the probable, exploring perspectives the model was trained to deprioritize. The difference is intent. AI-dependent thinkers accept the first high-probability output. AI-sovereign thinkers use AI's probabilities against themselves.The Organizational ProblemEven if you maintain frame independence, organizations have immune systems. They reward execution, not questioning whether the project should exist. Outtelligence at the individual level creates insight. At the organizational level, it requires structure:Who is tasked with challenging assumptions? What incentives exist for killing a project that would have succeeded inside the current frame? Most companies claim to value "thinking differently." Few tolerate the discomfort. The ones that do will survive the compression AI brings.AI has made me faster. Cleaner and more precise.\
But I’ve noticed something subtle:My tolerance of being open to one interpretation has decreased. I resolve confusion too quickly. Clarity feels good. But premature clarity is cognitive laziness disguised as productivity. Outtelligence requires staying in confusion longer than is comfortable. That is now a competitive act.When intelligence becomes abundant, independent framing becomes scarce. Scarcity drives value.\
Outtelligence is scarce because:\
Most people will not choose that path. Which makes it valuable.AI will not destroy intelligence. It will compress it. The question is whether you allow it to compress your perception as well. Outtelligence is the discipline of stepping outside the machine’s frame before the machine defines yours. That is not anti-technology. It is a strategic positioning. And positioning, not effort, is what compounds.]]></content:encoded></item><item><title>Ctrl-Alt-Speech: Let Fly The Claudes Of War, With Casey Newton</title><link>https://www.techdirt.com/2026/02/26/ctrl-alt-speech-let-fly-the-claudes-of-war-with-casey-newton/</link><author>Mike Masnick</author><category>tech</category><pubDate>Thu, 26 Feb 2026 23:00:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[In this week’s roundup of the latest news in online speech, content moderation and internet regulation, Ben is joined by Casey Newton, founder and editor of Platformer and co-host of Hard Fork, a podcast that makes sense of the rapidly changing world of tech. Together, they discuss:Play along with Ctrl-Alt-Speech’s 2026 Bingo Card and get in touch if you win!]]></content:encoded></item><item><title>What&apos;s the Point of School When AI Can Do Your Homework?</title><link>https://news.slashdot.org/story/26/02/26/2154237/whats-the-point-of-school-when-ai-can-do-your-homework?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Feb 2026 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from 404 Media: There's a new agentic AI called Einstein that will, according to its developers, live the life of a student for them. Einstein's website claims that the AI will attend lectures for you, write your papers, and even log into EdTech platforms like Canvas to take tests and participate in discussions. Educators told me that Einstein is just one of many AI tools that can do homework for students, but should be seen as a warning to schools that are increasingly seen by students as a place to gain a diploma and status as opposed to the value of education itself.
 
If an AI can go to school for you what's the point of going to school? For Advait Paliwal, Brown dropout and co-creator of Einstein, there isn't one. "I think about horses," he said. "They used to pull carriages, but when cars came around, I'd argue horses became a lot more free," he said. "They can do whatever they want now. It would be weird if horses revolted and said 'no, I want to pull carriages, this is my purpose in life.'" But humans aren't horses. "This is much bigger than Einstein," Matthew Kirschenbaum told 404 Media. "Einstein is symptomatic. I doubt we'll be talking about Einstein, as such, in a year. But it's symptomatic of what's about to descend on higher ed and secondary ed as well."
 
[...] The attractiveness of agentic AIs is a symptom of a decades-long trend in higher education. "Universitiesby and large adopted a transactive model of education," Kirschenbaum said. "Students see their diploma as a credential. They pay tuition and at the end of four years, sometimes five years, they receive the credential and, in theory at least, that is then the springboard to economic stability and prosperity." Paliwal seems to agree. He told 404 Media that he attempted to change the university from the inside while working as a TA, but felt stymied by politics. "The only way to force these institutions to evolve is to bring reality to their face. And usually the loudest critics are the ones who can't do their own job well and live in fear of automation," he said. "I think we really need to question what learning even is and whether traditional educational institutions are actually helping or harming us," said Paliwal. "We're seeing a rise in unemployment across degree holders because of AI, and that makes me question whether this is really what humans are born to do. We've been brainwashed as a society into valuing ourselves by the output of our productive work, and I think humanity is a lot more beautiful than that. Is it really education if we're just memorizing things to perform a task well?"
 
Kirschenbaum added: "What we're finding is that if forms of education can be transacted then we've just about arrived at the point where autonomous software AI agents are capable of performing the transaction on your behalf," he said. "And so the whole educational paradigm has come back to essentially bite itself in the ass."]]></content:encoded></item><item><title>Python is a Video Latency Suicide Note: How I Hit 29 FPS with Zero-Copy C++ ONNX</title><link>https://hackernoon.com/python-is-a-video-latency-suicide-note-how-i-hit-29-fps-with-zero-copy-c-onnx?source=rss</link><author>Nick Maletsky</author><category>tech</category><pubDate>Thu, 26 Feb 2026 22:34:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I accepted a challenge: build a real-time YOLOv8 video pipeline using vanilla ONNX Runtime. No bloated frameworks. No Python bottlenecks. Just raw C++ grit.Let's be honest: Python is the undisputed king of the research lab. But if you're trying to stream live H.264 video through a neural network at scale on edge hardware? Python's Global Interpreter Lock (GIL) and its pathological obsession with memory copying are glaring liabilities.\
I was recently tasked with a simple objective: create fast inference for a video stream using a vanilla ONNX runtime and a YOLOv8 segmentation model. It sounded easy on paper. Grab FFmpeg, process the frames, and encode them back out.In reality, it was a journey through engineering hell. Here is how I dragged a sluggish 10 FPS prototype into a rock-solid 29 FPS beast, and the "final boss" bugs I had to slay along the way.The FogAI Sandbox: Validation Before IntegrationThis repository isn't a standalone toy---it is a . I use this environment to rigorously stress-test specific computer vision models, engine builds, and optimization patterns before they are promoted to the .\
If a strategy (like Zero-Copy hardware mapping) can't survive here at 29 FPS, it has no business being inside an industrial autonomous nervous system.\
Previous Chapters in the FogAI Saga:The "Memory Copy Tax" TrapMost computer vision prototypes are slow because they treat memory like a game of Hot Potato.\
My initial architecture was the "standard" mess: FFmpeg decoded H.264 into YUV hardware formats, converted it to an OpenCV  (BGR) to feed the model, applied masks on the RGB image, converted it  to YUV, and finally hit the encoder.\
That's three unnecessary memory copies and two heavy pixel-format conversions. On an ARM CPU processing 4K frames, that overhead burns up to  just moving bits around.\
I fixed this by implementing Zero-Copy Hardware Mapping. Instead of converting the frame, I mapped the  hardware Y-plane (Luminance) directly into an OpenCV  wrapper.// Mapping the hardware Y-plane natively - zero memcpy, zero overhead.
cv::Mat y_plane(yuvFrame->height, yuvFrame->width, CV_8UC1,
                yuvFrame->data, yuvFrame->linesize);

// YOLO segmentation masks now inject binary modifications directly
// onto the hardware Y sequence.
y_plane(bbox).setTo(0, valid_mask);
\
By bypassing the conversion overhead, I skipped the CPU bottleneck entirely. But I was still capped at 23 FPS. Mutability and Asynchronous ReorderingProfiling showed that my threads were locked in a sequential death grip. The  abstraction relies on mutating shared internal buffers. If I just spawned more threads on a single model, they contaminated each other, and the system segfaulted.\
 I instantiated a concurrent pool of std::unique_ptr<YOLO_Segment> models---one unique ONNX model instance per worker thread.\
But there was a catch: DASH video requires strict frame order. Since workers finish at different times, Frame 2 might finish before Frame 1, causing the video to stutter like a 90s jump-cut. I had to inject a reorder buffer using an  to ensure flawless H.264 synchronization.// Reorder buffer logic to keep the stream sequential
std::map<int64_t, FramePayload> reorderBuffer;
int64_t expected_pts = 0;

while (true) {
    auto payload = inferenceQueue.pop(); // Workers drop processed frames here
    reorderBuffer[payload.pts] = payload;

    // Emit frames only when the sequential timestamp flags align
    while (!reorderBuffer.empty() && reorderBuffer.begin()->first == expected_pts) {
        auto it = reorderBuffer.begin();
        encoder.writeFrame(it->second.yuvFrame, it->second.pts);
        reorderBuffer.erase(it);
        expected_pts++;
    }
}
The Final Boss: Thread Cache ThrashingOn paper, the logic was perfect. In practice, my FPS plummeted to . My Time-To-Inference (TTI) latencies shot up from 43ms to a horrific 890ms.\
I was a victim of CPU Cache Thrashing.\
Even though I had decoupled my locks, the underlying ML libraries (OpenCV and ONNX) were "helping" me by spawning their own internal threads.: Defaults to hardware_concurrency() / 2 threads . With 10 workers, it spawned 100+ internal threads on my 20-core CPU.: Automatically deploys workers for operations like .\
My designated workers were fighting ONNX's threads, which were fighting OpenCV's threads. Thousands of context switches were destroying my L1/L2 caches every second.\
The fix was a brutal "No" to implicit concurrency. I stripped the libraries of their right to spawn threads:int main() {
  // Globally disable implicit OpenCV threading
  cv::setNumThreads(1);

  // Cap ONNX Runtime to a single thread per op
  Ort::SessionOptions session_options;
  session_options.SetIntraOpNumThreads(1);
  session_options.SetInterOpNumThreads(1);
}
\
The context-switching noise vanished. My CPU instruction cache is synchronized. The pipeline immediately hit a flawless  with a TTI ceiling of ~329ms.Maintenance Over Ego: The Vanilla StrategyA common question I get is: "If you're so focused on performance, why not fork the engine and optimize the kernels yourself?"\
The answer is Technical Debt avoidance.If you hack the engine's internals, you're signing up for a never-ending maintenance loop. Every time a new version drops with support for fresh hardware---like  (57% prefill boost) or  --- you'd have to re-port your custom optimizations manually. By sticking with a , I can "catch" these hardware updates for free just by bumping a version number.\
Similarly, I chose not to optimize the coding/decoding pipeline. Why? Because hardware vendors already did. Whether it's  or the , these chips have silicon-level acceleration for H.264. Focus the code on the ; leave the codecs to the metal they were built for.Conclusion: Stop Guessing, Start ProfilingScaling AI for the real world requires peeling back the layers of abstraction we've gotten too comfortable with. Python hides these latency taxes until you put the system into production.\
If you are tasked with heavy tensor payloads on video:---work on the hardware planes directly.---one instance per worker.Reorder sequential outputs---don't let async finish times break your stream.Never let your libraries spawn their own threads.---optimize your architecture, not the engine, to keep tech debt low.\
Next for the FogAI node? We're prepping  for a zero-copy run…]]></content:encoded></item><item><title>PayPal might not be looking to sell itself: Report</title><link>https://techcrunch.com/2026/02/26/paypal-might-not-be-looking-to-sell-itself-report/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Thu, 26 Feb 2026 22:23:42 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[PayPal may not be in talks to be acquired, sources have told Semafor, after a previous news report that Stripe was sniffing around.]]></content:encoded></item><item><title>Google Launches Nano Banana 2 Model With Faster Image Generation</title><link>https://tech.slashdot.org/story/26/02/26/2145253/google-launches-nano-banana-2-model-with-faster-image-generation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Feb 2026 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google has launched Nano Banana 2 (Gemini 3.1 Flash Image), a faster, more realistic image generation model that becomes the default across Gemini, Search, Lens, and Flow. TechCrunch reports: The new Nano Banana 2 retains some of the high-fidelity characteristics of the Pro model but produces images faster. The company says you can create images with a resolution ranging from 512px to 4K, in different aspect ratios. Nano Banana 2 can maintain character consistency for up to five characters and fidelity of up to 14 objects in one workflow for better storytelling. Users can also issue complex requests with detailed nuances for image generation, Google says. In addition, users can create media with more vibrant lighting, richer textures, and sharper detail.
 
[...] On Google's higher-end plans, Google AI Pro and Ultra, subscribers can continue to use Nano Banana Pro for specialized tasks by regenerating images via the three-dot menu. [...] The company said that all images created through the new model will have a SynthID watermark, which is Google's mark to denote AI-generated images. The images are also interoperable with C2PA Content Credentials, created by an industry body consisting of companies like Adobe, Microsoft, Google, OpenAI, and Meta. Google said that since launching the SynthID verification in the Gemini app in November, people have used it over 20 million times.]]></content:encoded></item><item><title>Pipe Network Launches SolanaCDN: A Free, Open-Source Validator Client With Built-In Acceleration</title><link>https://hackernoon.com/pipe-network-launches-solanacdn-a-free-open-source-validator-client-with-built-in-acceleration?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Thu, 26 Feb 2026 21:34:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[San Francisco, CA, February 26th, 2026/Chainwire/SolanaCDN delivers 3.8x faster shred propagation through a global mesh of 35,000+ nodes, provided as a public good for the Solana networkPipe Network today announced the launch of SolanaCDN, a free, open-source Solana validator client with an integrated CDN acceleration layer. Built as a fork of Anza's Agave, SolanaCDN gives every Solana validator access to faster shred propagation through Pipe's global network of 35,000+ PoP (Point-of-Presence) nodes.The client and CDN layer are both completely free. Pipe Network is providing SolanaCDN as public good infrastructure for the Solana ecosystem.The problem SolanaCDN solvesValidator performance on Solana is heavily influenced by network geography. Validators closer to block producers see shreds earlier, vote sooner, and earn more rewards. Validators in less connected regions face slower propagation, missed votes, and reduced leader slot revenue regardless of their hardware.SolanaCDN addresses this by giving validators a second, faster path for shred delivery alongside native gossip. Shreds and vote packets route through Pipe's global mesh, which continuously measures every network path and routes traffic along the fastest available route in real time.Native gossip still runs underneath. SolanaCDN adds a parallel fast lane.SolanaCDN delivers 3.8x faster propagation than standard Turbine, with a P50 cross-region latency of approximately 78ms compared to the roughly 300ms baseline on standard gossip.The client also ships with Pipe-built optimizations available out of the box before the CDN layer is enabled: optimized shred coalescing for leaders (Fast Shreds), snapshot downloads from Pipe's global network, and restore progress with real-time ETAs during validator catchup.Public good infrastructureFaster propagation is a network effect. Every validator running SolanaCDN improves shred delivery globally, which means faster block finalization, fewer forks, and fewer missed slots across the entire Solana network."Validator performance shouldn't be determined by geography," said David Rhodus, CEO of Pipe Network. "SolanaCDN gives every validator access to the same fast infrastructure. The more validators that run it, the faster Solana gets for everyone."SolanaCDN is a fully compatible Agave fork. Validators can install it as a drop-in replacement for their existing client. The CDN layer is optional, activated with a single configuration flag, and is non-consensus by design. It does not modify block production, consensus logic, leader scheduling, or voting rules. All CDN operations are non-blocking and fail-safe. If the CDN layer is unavailable, the validator continues operating normally.Built-in Prometheus metrics and CDN-versus-gossip race data give operators full visibility into performance changes in their environment.SolanaCDN is available now. The source code is published on GitHub and the client is ready to run on Solana mainnet-beta.Pipe Network is a global edge infrastructure company built on Solana. The network operates 35,000+ hyperlocal PoP nodes globally, providing distributed storage with fast reads and real-time data delivery. Pipe's overlay network tracks latency, loss, and jitter across every path in real time and routes traffic along the fastest one.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging This article is for informational purposes only and does not constitute investment advice. Cryptocurrencies are speculative, complex, and involve high risks. This can mean high prices volatility and potential loss of your initial investment. You should consider your financial situation, investment purposes, and consult with a financial advisor before making any investment decisions. The HackerNoon editorial team has only verified the story for grammatical accuracy and does not endorse or guarantee the accuracy, reliability, or completeness of the information stated in this article. #DYOR]]></content:encoded></item><item><title>sudo-rs Breaks Historical Norms With Now Enabling Password Feedback By Default</title><link>https://www.phoronix.com/news/sudo-rs-password-feedback</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 21:31:11 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[On recent builds of Ubuntu 26.04 when being prompted by sudo for the password, password feedback is now enabled by default to show asterisk (*) characters when inputting your password. Traditionally sudo has not provided password feedback in the name of security to not divulge the length of your password in case anyone is looking/capturing your screen. But upstream sudo-rs has now changed the default behavior in the name of an improved UX...]]></content:encoded></item><item><title>What Does It Mean to Be Human When Tortured?</title><link>https://hackernoon.com/what-does-it-mean-to-be-human-when-tortured?source=rss</link><author>Sam</author><category>tech</category><pubDate>Thu, 26 Feb 2026 21:23:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What does it mean to be human when you are living under a techno-controlled state system?When your thoughts are being read, the last of your freedoms is taken away from you.When you are constantly being monitored, potentially by people you used to know as people.\
When you are kept awake by voices, not random, but deliberate, from people who used to love you but are now under the gun, literally, by rich and unfeeling “humans” who can’t stand an independent soul, who feel threatened by so much authenticity, so much raw energy.\
Anyone can eat, anyone can shit, anyone can code, anyone can cycle, anyone can say A B C, anyone can be a ruler.\
I am not writing to gain anything nor to lose anything. I only write to lose the sense of awareness that is unbearable, to avoid the madness of having to listen to a mind that has no other occupation than to contemplate itself.\
I can’t even write without being edited. I have no freedom even on my page.\
Am I even writing, or am I remotely controlled? All who have failed to stand up against this tyranny are co-responsible for their imprisonment. There are 5 guys in the Pentagon who rule this world, they have a CCTV in every living room on earth, and a super duper advanced search engine by which they can look up anything about anyone in a sec. It was developed by people like you and me, who wanted to earn a salary. We are not free. We are all losers except those 5 people.\
Nobody wants to hear this, nobody wants to listen to this, yet every day we have to live it.\
Why?  + when did I go wrong?\
It's like what if we just tried to give one another all the feeling of being worthy? Just basic human dignity?\
Just like the USA is letting any European country win, they could just bomb them lol\
What does that even meanI don’t understand that phraseYou are too good at losing?Y’all, do you feel like somebody is giving what you know about how it all works?\
I need to sit here and guess what they are whispering in my earWhy do you even care to talk to me???Are you not bored looking at me?Don't you feel as if you got better things to do???Is that what your life has become? Looking at a “loser” living a non-life???I don't understand your obsession with me.Invite me for a cuppa if you want to have a chat???I don't give it to anyoneNo genius writing from me\
The club opener was contradicting himself by uttering these words. As if the comfortable silence was coercively enforced on the audience. Reminiscent of Stalin’s opinion on humor’s redundancy, for its people were happy already. When the end goal is clear, the KPIs’ magic number is known, all that is left to do is pretend we have reached it, we have obtained it. What we do in between becomes a mirage of ghostly meanderings of empty souls seeking a reason to be affirmed in their existence. We laugh, but we don’t know why; we invent, we fill the gap in our explanation, oh, it must be the paradox of things unrelated.\
There is no audience for these words, so I am happy for this to be unconsidered. Stalinistically happy. And the train of where we don’t need to be can be missed at convenience, for its platform is being built as we speak, or listen. Or, really, how can these things differ from one another? Isn’t speaking a form of listening to others through ourselves? Are we ever separated from others?\
Why do we hurt when we are one, when we are connected uneradically by waves unseen that the universe carries forth and back and forth again?\
Is it pain or an affirmation of being? And is seeking its opposite a perversion of the divine presence that can only be felt when earthly matters are waning?\
Hell on earth is an overspecification, so would heaven be. Heaven without qualification. We live into the next moment by rearranging entropy, but mind does not follow accordingly.\
Whose ppl who don't believe in me, have you ever tried? How do you know? What evidence do you have? Do they tell you?\
Will things be different in the USA?Are things different for you?Why are you still looking at me?]]></content:encoded></item><item><title>QIELend: Bringing Efficient DeFi Lending to The QIE Blockchain</title><link>https://hackernoon.com/qielend-bringing-efficient-defi-lending-to-the-qie-blockchain?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Thu, 26 Feb 2026 21:21:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Decentralized lending has become one of the foundational pillars of modern DeFi. Protocols like Aave demonstrated that users want permissionless borrowing and yield generation without relying on traditional intermediaries. However, high network fees and fragmented liquidity across chains continue to limit adoption.QIELend aims to solve this by delivering a familiar, capital-efficient lending experience — but on the high-performance QIE Blockchain. Keep your yield — not pay it to gas.Built for interoperability and low-cost execution, QIELend allows users to supply assets, earn yield, and borrow against their holdings with significantly lower transaction friction than many legacy DeFi environments.Aave-Style Lending, Optimized for QIEAt its core, QIELend operates similarly to leading money markets: users deposit assets into liquidity pools, earn interest from borrowers, and can unlock liquidity by borrowing against their collateral.The key difference is infrastructure efficiency.By operating on QIE’s high-throughput, low-fee Layer-1, QIELend enables micro-efficient lending that would be uneconomical on higher-cost networks.Wrapped assets are tokens locked on their original blockchain and mirrored on QIE, allowing users to use ETH, BNB, and USDC within the QIE ecosystem and redeem them back at any time.Together, these represent exposure to ETH, BNB, USD liquidity, and the native QIE ecosystem — all standardized under the QIE-20 format for seamless composability.More markets, including XRP and Solana, are planned for upcoming releases.Liquidity Is Already LiveThe protocol has launched with $100,000+ in initial liquidity, providing the foundation for early lending and borrowing activity.As utilization grows, additional liquidity providers are expected to deepen the markets and improve capital efficiency across the ecosystem.Why Lending Protocols Matter in DeFiDecentralized lending unlocks several powerful financial use cases:Users can supply supported assets and earn interest from borrowers — similar to depositing funds in an interest-bearing account, but without centralized custody risk.2. Unlock Liquidity Without SellingLong-term holders often do not want to sell core assets like ETH or QIE. Lending protocols allow users to:Borrow stablecoins against holdingsThis is one of the primary drivers of DeFi lending adoption globally.3. Capital Efficiency for TradersActive traders can use borrowed liquidity to:Fund additional positionsParticipate in new opportunitiesAll while keeping their base collateral intact.QIELend is currently offering highly competitive borrowing conditions:QUSDC borrowing from as low as 0.01% APRVolatile assets like WQIE around 5% APRCollateral requirements are dynamically risk-based:up to ~80% drawdown protection for QUSDCThis risk-weighted model helps maintain protocol stability while maximizing capital efficiency for users.Built for InteroperabilityA major strength of QIELend is its cross-chain asset pipeline.Users can seamlessly onboard major crypto assets into the QIE ecosystem:Bridge ETH and BNB to QIE:👉 https://www.bridge.qie.digital/Swap native QIE to WQIE (QIE-20 standard):👉 https://www.swap.dex.qie.digital/swapStandardizing assets into the QIE-20 format ensures that all markets “speak the same language,” improving composability across DeFi applications.Getting started with QIELend is intentionally straightforward:Connect via MetaMask or QIE WalletEarn yield or borrow against collateralIf assets imported via MetaMask are not immediately visible, users may simply refresh the interface after supplying funds.Token contract addresses for supported assets can always be verified via the QIE explorer:Notably, QIE Wallet already includes these assets by default for a smoother onboarding experience.Maximizing Returns with Smart LoopingFor users looking to go beyond basic lending, QIElend introduces an efficient looping mechanism designed to enhance capital productivity. Instead of earning yield on a single supply, users can manually re-supply borrowed assets in a streamlined flow, effectively increasing their exposure to lending rewards and incentive programs. Because QIElend runs on the ultra-low-fee QIE network, this strategy remains practical even for smaller portfolios where high gas costs on other chains would normally erode profits. The result is a more capital-efficient approach to DeFi yield, supported by clear health-factor visibility and built-in risk awareness tools.Why QIELend Matters for the QIE EcosystemEvery successful Layer-1 ecosystem eventually requires a robust money market. Lending protocols create:stronger DeFi composabilityBy launching early and focusing on efficiency, QIELend is positioning itself as the core liquidity engine of the QIE financial stack.As additional assets like XRP and Solana come online, the protocol’s addressable liquidity universe is expected to expand meaningfully.QIElend vs Aave: The Next Evolution in DeFi Lending EfficiencyQIElend offers a structurally more efficient lending experience than legacy DeFi protocols such as Aave by removing much of the operational friction that arises from high gas costs and slower block-based execution. While established platforms rely on traditional on-chain transaction models where every supply, borrow, or repayment incurs meaningful network fees and timing delays, QIElend is built natively on the high-performance QIE blockchain, enabling near-zero-cost transactions and near-instant position updates.This allows users to manage collateral more actively, reduces the incentive burden on liquidators, and supports faster market rebalancing, which in turn can translate into more competitive effective borrowing rates. By optimizing liquidity specifically for its ecosystem rather than competing across congested global markets, QIElend delivers a lending environment designed for speed, capital efficiency, and practical usability at scale.QIELend brings a proven DeFi primitive — decentralized lending — into a faster and more cost-efficient environment on the QIE Blockchain.With live liquidity, competitive borrowing rates, and a growing multi-asset pipeline, the protocol provides both yield opportunities for suppliers and flexible capital access for borrowers.For users seeking Aave-style functionality without high network friction, QIELend represents an important step forward in the evolution of the QIE ecosystem.:::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging This article is for informational purposes only and does not constitute investment advice. Cryptocurrencies are speculative, complex, and involve high risks. This can mean high prices volatility and potential loss of your initial investment. You should consider your financial situation, investment purposes, and consult with a financial advisor before making any investment decisions. The HackerNoon editorial team has only verified the story for grammatical accuracy and does not endorse or guarantee the accuracy, reliability, or completeness of the information stated in this article. #DYOR]]></content:encoded></item><item><title>Chinese Official&apos;s Use of ChatGPT Revealed a Global Intimidation Opperation</title><link>https://tech.slashdot.org/story/26/02/26/2045225/chinese-officials-use-of-chatgpt-revealed-a-global-intimidation-opperation?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Feb 2026 21:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New submitter sabbede shares a report from CNN Politics: A sprawling Chinese influence operation -- accidentally revealed by a Chinese law enforcement official's use of ChatGPT -- focused on intimidating Chinese dissidents abroad, including by impersonating US immigration officials, according to a new report from ChatGPT-maker OpenAI. The Chinese law enforcement official used ChatGPT like a diary to document the alleged covert campaign of suppression, OpenAI said. In one instance, Chinese operators allegedly disguised themselves as US immigration officials to warn a US-based Chinese dissident that their public statements had supposedly broken the law, according to the ChatGPT user. In another case, they describe an effort to use forged documents from a US county court to try to get a Chinese dissident's social media account taken down. "This is what Chinese modern transnational repression looks like," Ben Nimmo, principal investigator at OpenAI, told reporters ahead of the report's release. "It's not just digital. It's not just about trolling. It's industrialized. It's about trying to hit critics of the CCP [Chinese Communist Party] with everything, everywhere, all at once."
 
Michael Horowitz, a former Pentagon official focused on emerging technologies, said the report from OpenAI "clearly demonstrates the way that China is actively employing AI tools to enhance information operations. US-China AI competition is continuing to intensify. This competition is not just taking place at the frontier, but in how China's government is planning and implementing the day-to-day of their surveillance and information apparatus."]]></content:encoded></item><item><title>Google paid startup Form Energy $1B for its massive 100-hour battery</title><link>https://techcrunch.com/2026/02/26/google-paid-startup-form-energy-1b-for-its-massive-100-hour-battery/</link><author>Tim De Chant</author><category>tech</category><pubDate>Thu, 26 Feb 2026 21:04:23 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The deal paves the way for Form Energy to raise a new funding round before potentially going public next year.]]></content:encoded></item><item><title>DOJ’s Losing Streak Continues Because Federal Officers Just Can’t Stop Lying</title><link>https://www.techdirt.com/2026/02/26/dojs-losing-streak-continues-because-federal-officers-just-cant-stop-lying/</link><author>Tim Cushing</author><category>tech</category><pubDate>Thu, 26 Feb 2026 20:54:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[I’ll take my joy where I can. And this iteration of the Trump DOJ continues to provide bright bursts of schadenfreude-tinted sunshine. Any competent DOJ can close cases. Any  competent prosecutor can push a case past a grand jury. Any sufficiently slippery solicitor (mixing in some British for the sheer alliteration of it all) can convince a judge that the lies told by officers were merely good faith blunders not worthy of anything more than a judicial “no one’s perfect” shrug. DOJ fails at . It can’t secure indictments. It can’t convince grand juries that vindictive prosecutions are  prosecutions. And its prosecutors are constantly undermined by (1) prejudicial, fact-free social media posts and public statements by administration officials, (2) the illegal actions of federal officers, (3) their own ineptitude, (4) the lies told by federal officers, and (5) any or all of the above.High-level prosecutors keep getting sidelined because they’ve been illegally appointed. Other prosecutors have refused to engage with the administration’s vindictive plans, resulting in most of them retiring or being fired. Consequently, there’s a shortage of qualified, experienced prosecutors. The void is being constantly refilled by some of the emptiest people ever to leverage MAGA loyalty into federal employment. It took less than a year for the Trump DOJ to almost completely destroy the “presumption of regularity” — the legal concept that the government is acting in good faith, even if its legal arguments aren’t the best. It took less than a year for the Trump DOJ to turn grand juries into coin flips.In 2016, the most recent year for which the Justice Department has published data, federal prosecutors concluded more than 155,000 prosecutions and declined over 25,000 cases presented by investigators. In only six instances was a grand jury’s refusal to indict listed as the reason for dropping the matter.Six times in a one year over 25,000 declined cases. Trump’s loyalist US Attorney pick, Lindsey Halligan, put her insurance law background to work and… managed to do this twice during a  (attempted) prosecution. When prosecutors aren’t shooting themselves in the foot (or being shot in the foot by their employer), they’re losing cases because the people they expect to back up their cases — the federal officers claiming to have been assaulted, etc. — can’t even back up their own narratives when testifying in court.The most recent significant fumble came from Minneapolis prosecutors, who last week dismissed felony assault charges they had filed against two Venezuelan men accused of “violently beating” an Immigration and Customs Enforcement (ICE) officer “with weapons” on 14 January.According to the early government narrative, federal officers were assaulted by “violent criminal illegal aliens” during a stop of an undocumented Venezuelan. The officers claimed two other men came out of a nearby apartment and attacked an officer with a “snow shovel and broom handle.” That case is now dead because… well, the testifying officers lied. [O]n 12 February, prosecutors filed a motion to dismiss both men’s cases, saying: “Newly discovered evidence in this matter is materially inconsistent with the allegations in the complaint affidavit.”ICE director Todd Lyons said ICE and the DoJ had opened an investigation into the case after videos revealed “sworn testimony provided by two separate officers appears to have made untruthful statements”, marking a rare acknowledgement of possible wrongdoing by DHS officials.It’s extremely rare for the government to dismiss its  prosecution with prejudice, meaning it can’t  seek to refile these criminal charges against the alleged perpetrators. And I don’t know if Todd Lyons just misspoke or if he actually tried to use the exonerative tense while simultaneously stating these officers lied. “Sworn testimony… appears to have made untruthful statements” sounds like the courtroom version of a government official discussing a shooting by an officer with the phrase “the officer’s weapon discharged,” suggesting no one actually pulled the trigger.Whatever the case, there’s definitely a trend here. In Chicago, of 92 people arrested for assaulting or impeding officers last fall, 74 cases have resulted in no charges; in 13 cases, charges were filed and dismissed; and five charged cases were still pending, a recent investigation by Fox 9, a Minneapolis-based station, showed. As of the end of January, there have been no convictions.In LA, the federal public defenders have won all six cases filed against ICE protesters that have gone to trial since June, the LA Times recently reported. Fewer than 1% of federal criminal defendants were acquitted across the US in fiscal year 2024, with US prosecutors traditionally having a roughly 90% conviction rate, the paper noted.I assume the DOJ bloodshed will continue. Trump hates losing and he hates people who lose in his name even more. But replacing talent with loyalists isn’t going to end this losing streak. If nothing else, this iteration of the DOJ has the chance to go down in history as one of the worst ever assembled, even if we consider nothing else but its win-loss record. It doesn’t mean the DOJ is harmless, however. It’s still more than willing to engage in vindictive prosecutions, ignore court orders, and take bite after bite of the apple (so to speak) until it finally manages to at least pierce the skin. And that means a lot of people are going to have their lives upended, even if only temporarily, just to please a tyrant who thinks anything or anyone presenting even the most minimal of opposition should be subjected to punishment. ]]></content:encoded></item><item><title>iPhone and iPad Are First Consumer Devices Cleared for NATO Classified Data</title><link>https://mobile.slashdot.org/story/26/02/26/2036231/iphone-and-ipad-are-first-consumer-devices-cleared-for-nato-classified-data?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Feb 2026 20:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple's iPhone and iPad running iOS 26 and iPadOS 26 have become the first consumer mobile devices cleared for NATO-restricted classified data. No special software or settings are required. MacRumors reports: Apple's devices are the first and only consumer mobile products that have reached this government certification level after security testing and evaluation by the German government. iPhones and iPads running iOS 26 and iPadOS 26 are now certified for use with classified data in all NATO nations.
 
In an announcement of the security clearance, Apple touted its security features: "Apple designs security into all of its products from the start, ensuring the most sophisticated protections are built in across hardware, software, and Apple silicon. This unique approach allows Apple users to benefit from industry-leading security protections such as best-in-class encryption, biometric authentication with Face ID, and groundbreaking features like Memory Integrity Enforcement. These same protections are now recognized as meeting stringent government and international security requirements, even for restricted data."]]></content:encoded></item><item><title>Microsoft Updates DirectX Shader Compiler With Improved Vulkan Driver Interoperability</title><link>https://www.phoronix.com/news/DX-Shader-Compiler-Better-VLK</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 20:18:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Microsoft has published a new version of its open-source DirectX Shader Compiler. Besides adding Shader Model 6.9 production support, making this DX Compiler update interesting to us are the SPIR-V back-end improvements and enhancing interoperability with Vulkan drivers...]]></content:encoded></item><item><title>So, we’re getting Prada Meta AI glasses, right?</title><link>https://techcrunch.com/2026/02/26/so-were-getting-prada-meta-ai-glasses-right/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 26 Feb 2026 20:11:55 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Mark Zuckerberg was at Prada's fashion week event in Milan, leaving everyone to wonder if we're getting Meta AI glasses under the Prada brand.]]></content:encoded></item><item><title>Sophia Space raises $10M seed to demo novel space computers</title><link>https://techcrunch.com/2026/02/26/sophia-space-raises-10m-seed-to-demo-novel-space-computers/</link><author>Tim Fernholz</author><category>tech</category><pubDate>Thu, 26 Feb 2026 19:55:20 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company's modular computer tiles offer a new vision for space data centers. ]]></content:encoded></item><item><title>Memory shortage could cause the biggest dip in smartphone shipments in over a decade</title><link>https://techcrunch.com/2026/02/27/memory-shortage-could-cause-the-biggest-smartphone-shipments-dip-in-over-a-decade/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 26 Feb 2026 19:43:51 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[IDC says phone makers will ship only 1.12 billion smartphones as compared to 1.26 billion last year.]]></content:encoded></item><item><title>Firefox 148 Lets You Kill All AI Features in One Click</title><link>https://news.slashdot.org/story/26/02/26/182239/firefox-148-lets-you-kill-all-ai-features-in-one-click?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 19:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Mozilla has released Firefox 148 for Windows, macOS and Linux, bringing a new AI Settings section that lets users disable all of the browser's AI-powered features in one click and then selectively re-enable the ones they actually want, such as the local translation tool that works locally rather than in the cloud. 

The update also patches more than 50 security vulnerabilities -- none known to be under active exploitation -- over half of which Mozilla classifies as high risk, including five sandbox escape flaws and eight use-after-free bugs in the JavaScript engine that could allow code execution.]]></content:encoded></item><item><title>Mistral AI inks a deal with global consulting giant Accenture</title><link>https://techcrunch.com/2026/02/26/mistral-ai-inks-a-deal-with-global-consulting-giant-accenture/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Thu, 26 Feb 2026 19:17:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Mistral AI lands a partnership with Accenture, the consultant that has also recently announced partnerships with rivals OpenAI and Anthropic. ]]></content:encoded></item><item><title>“Not Ready for Prime Time.” A Federal Tool To Check Voter Citizenship Keeps Making Mistakes.</title><link>https://www.techdirt.com/2026/02/26/not-ready-for-prime-time-a-federal-tool-to-check-voter-citizenship-keeps-making-mistakes/</link><author>Jen Fifield and Zach Despart</author><category>tech</category><pubDate>Thu, 26 Feb 2026 18:55:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[When county clerk Brianna Lennon got an email in November saying a newly expanded federal system had flagged 74 people on the county’s voter roll as potential noncitizens, she was taken aback.Lennon, who’d run elections in Boone County, Missouri, for seven years, had heard the tool might not be accurate.The flagged voters’ registration paperwork confirmed Lennon’s suspicions. The form for the second person on the list bore the initials of a member of her staff, who’d helped the man register — at his naturalization ceremony. It later turned out more than half the Boone County voters identified as noncitizens were actually citizens.The source of the bad data was a Department of Homeland Security tool called the Systematic Alien Verification for Entitlements, or SAVE.Once used mostly to check immigrants’ eligibility for public benefits, SAVE has undergone a dramatic expansion over the last year at the behest of President Donald Trump, who has long falsely claimed that millions of noncitizens lurk on state voter rolls, tainting American elections.At Trump’s direction, DHS has pooled confidential data from across the federal government to enable states to mass-verify voters’ citizenship status using SAVE. Many of the nation’s Republican secretaries of state have eagerly embraced the experiment, agreeing to upload all or part of their rolls.But an examination of SAVE’s rollout by ProPublica and The Texas Tribune reveals that DHS rushed the revamped tool into use while it was still adding data and before it could discern voters’ most up-to-date citizenship information.As a result, SAVE has made persistent mistakes, particularly in assessing the status of people born outside the U.S., data gathered from local election administrators, interviews and emails obtained via public records requests show. Some of those people subsequently become U.S. citizens, a step that the system doesn’t always pick up.Texas and Missouri were among the first states to try the augmented tool.In Missouri, state officials acted on SAVE’s findings before attempting to confirm them, directing county election administrators to make voters flagged as potential noncitizens temporarily unable to vote. But in hundreds of cases, the tool’s determinations were wrong, our review found. Lennon was among dozens of clerks statewide who raised alarms about the system’s errors.“It really does not help my confidence,” she said, “that the information we are trying to use to make really important decisions, like the determination of voter eligibility, is so inaccurate.”Our reporting showed these errors were more widespread than previously known, involving at least 87 voters across 29 counties. County election administrators suspect there may be more. Confusion took hold when the Texas secretary of state’s office sent counties lists of flagged voters and directed clerks to start demanding proof of citizenship and to remove people from the rolls if they didn’t respond.“I really find no merit in any of this,” said Bobby Gonzalez, the elections administrator in Duval County in South Texas, where SAVE flagged three voters, all of whom turned out to be citizens.Even counting people flagged in error, the first bulk searches using SAVE haven’t validated the president’s claims that voting by noncitizens is widespread. At least seven states with a total of about 35 million registered voters have publicly reported the results of running their voter rolls through the system. Those searches have identified roughly 4,200 people — about 0.01% of registered voters — as noncitizens. This aligns with previous findings that noncitizens rarely register to vote.Brian Broderick leads the verification division of U.S. Citizenship and Immigration Services, the DHS branch that oversees SAVE. In an interview this month, he acknowledged the system can’t always find the most current citizenship information for people not born in the U.S. But he defended the tool, saying it was ultimately up to states to decide how to use SAVE data.“So we’re giving a tool to these folks to say, ‘Hey, if we can verify citizenship, great, you’re good. If we can’t, now it’s up to you to determine whether to let this person on your voter rolls,’” Broderick said.In Texas, Secretary of State Jane Nelson declined an interview request. Her spokesperson, Alicia Pierce, said the office hadn’t reviewed SAVE’s citizenship determination before sending lists to counties because it isn’t an investigative agency. In a statement, Pierce added that the use of SAVE was part of the office’s “constitutional and statutory duty to ensure that only eligible citizens participate in Texas elections.”A spokesperson for Missouri Secretary of State Denny Hoskins called SAVE a valuable resource even though some people it flagged might later be confirmed as citizens. “No system is 100% accurate,” Hoskins said in an interview, “but we’re working to get it right.”Asked whether it was problematic that his office directed clerks to temporarily bar voters from casting ballots before verifying SAVE’s findings, Hoskins said that was a “good point.”While 27 states have agreed to use SAVE, others have hesitated, concerned not only about inaccuracies, but also about privacy and the data’s potential to be used in immigration enforcement. Indeed, speaking at a recent conference, Broderick said that when SAVE flags voters as noncitizens, they are also referred to DHS for possible criminal investigation. (It is a crime to falsely claim citizenship when registering to vote.)People who’ve been flagged by SAVE in error say it’s jarring to have to provide naturalization records to stay eligible to vote when they know they’ve done nothing wrong.Sofia Minotti, who lives north of Dallas in Denton County, was born in Argentina but became a U.S. citizen years ago. Nonetheless, she was one of 84 Denton County voters identified by SAVE as a potential noncitizen. She and 11 others have since provided proof of citizenship, giving the system an error rate in the county of at least 14%.The real rate is probably higher, a county official acknowledged, since some of those sent notices to prove their citizenship might not respond in time to meet the deadline. They’ll have to be reinstated to vote in the midterms later this year.Minotti, though still on the rolls, felt singled out unfairly.“I’m here legally, and everything I’ve done has been per the law,” she said. “I really have no idea why I had to prove it.”Election administrators in many states have long hungered for better access to federal information on citizenship status.States don’t typically require people to provide proof of citizenship when they sign up to vote, only to attest to it under penalty of perjury. Previous efforts to use state data to catch noncitizens on voter rolls have gone poorly. Texas officials had to abandon a 2019 push after it became clear their methodology misidentified thousands of citizens, many of them naturalized, as ineligible voters.Until recently, SAVE hadn’t been much of a resource. State and local election officials needed to have voters’ DHS-assigned immigration ID numbers — information not collected in the registration process — to verify their citizenship status. Plus, officials had to pay to conduct searches one by one, not in bulk.In March, Trump issued an executive order that required DHS to give states free access to federal citizenship data and partner with the Department of Government Efficiency to comb voter rolls.The system’s main addition was confidential Social Security Administration data, which allowed states to search using full or partial Social Security numbers and incorporated information on millions of Americans who were not previously in Homeland Security databases.David Jennings, Broderick’s deputy at USCIS, had pressed his team to move quickly, he said on a June video call with members of former Trump lawyer Cleta Mitchell’s Election Integrity Network, which has spread false claims about noncitizen voting.“We tested it and deployed it to our users in two weeks,” Jennings said on the call, which ProPublica obtained a recording of. “I think that’s remarkable. Kind of proud of it.”Jennings added that to get quick access to the Social Security data, which has been tightly guarded, USCIS partnered with DOGE. (In an unrelated matter, DOGE has since been accused of misusing Social Security data.) Jennings did not respond to questions from ProPublica and the Tribune.According to emails obtained by ProPublica and the Tribune, SAVE first checks SSA’s citizenship information. If that shows a voter isn’t a citizen, DHS searches other databases, but it can be difficult to locate and match all the data the systems have on a person. This can lead to errors.Broderick said in the interview that Trump’s executive order dramatically accelerated the timetable for launching SAVE, getting agencies to cooperate and move quickly. But he insisted the work was done responsibly.“Do I think it was reckless? Do I think it wasn’t planned? Do I think it wasn’t tested? Absolutely not,” he said.By September, Texas had uploaded its entire list of more than 18 million registered voters into SAVE. Alabama, Arkansas, Indiana, Louisiana, Missouri, Montana, Tennessee, Utah and Wyoming put voter data into the system, too.They would soon start to unveil what SAVE had found.One of the first out of the gate was Texas. In late October, with early voting underway in state and local elections, Nelson, the secretary of state, announced SAVE had identified 2,724 potential noncitizens on the rolls.But as Nelson delegated the task of investigating those voters’ statuses to local election officials, confusion took hold.At a meeting, Nelson’s staff told county clerks’ offices to investigate flagged voters and then send notices to those for whom they were unable to confirm citizenship. In a follow-up email, Nelson’s staff told the clerks they should already have heard from someone in the office with more details.That set off a chain of messages on the local officials’ email groupTravis County voter registration director Christopher Davis said he hadn’t been contacted and had just learned the county had 97 flagged voters. Marsha Barbee, in Wharton County near Houston, shared that she talked to a Nelson staffer who said she’d been directed not to tell local officials about their lists because they were in the middle of early voting.“They said we have enough on our plates and didn’t want us to worry right now,” Barbee wrote.In the absence of clear state guidance, clerks proceeded inconsistently. Some said they didn’t act on their lists, waiting for more direction. Others, unsure how to investigate flagged voters’ status, said they simply sent notices asking for proof of citizenship, though some opted not to remove nonresponsive voters from the rolls.“I give them many chances; I don’t just expire them right away,” Dee Wilcher, a clerk in East Texas’ Anderson County, said about flagged voters, adding that she wanted to avoid removing citizens from the rolls and looking “stupid.”Chris McGinn, executive director of the Texas Association of County Election Officials, said many clerks expressed frustration with the secretary of state’s lack of guidance and failure to help with investigations. When he shared clerks’ concerns, McGinn said Nelson’s staff didn’t respond, leading him to conclude that checking SAVE’s findings wasn’t an agency priority.He called the state’s use of SAVE “more political and appearance-based” than a practical way to ensure election integrity.One way to check SAVE’s findings would have been to get information from the Texas Department of Public Safety, which requires proof of citizenship if residents register to vote when obtaining a driver’s license. The secretary of state’s office didn’t do this and didn’t direct counties to either.Several county officials said they hadn’t thought to ask DPS for information; those who did often found the agency had documentation showing some of the voters who SAVE identified as noncitizens were in fact citizens.In the Texas Panhandle, Potter County elections officials quickly confirmed through DPS that three of nine voters on their list had proof of citizenship on file. In neighboring Randall County, DPS helped officials verify that one in five had a U.S. passport, according to interviews with the local officials.In December, Travis County learned that 11 of the 97 voters flagged by SAVE had proven their citizenship to DPS. After getting the data, the county’s voter registrar, Celia Israel, said in an interview that she felt even more uncomfortable about moving forward with sending notices to voters, given SAVE’s errors.“It has proven to be inaccurate,” she said. “Why would I rely on it?”To be sure, SAVE also identified some people who weren’t eligible to vote, clerks said. Several came across instances in which voters marked on registration forms that they weren’t citizens, but were registered by election office staffers in error. Clerks also said voters have told them they’d misunderstood questions about eligibility when getting drivers’ licenses. (It’s not clear if any of those registered in error voted; overall, noncitizensrarely vote.) ProPublica and the Tribune surveyed the 177 Texas counties that had voters flagged by SAVE, receiving data from 97 that had either checked DPS records or sent notices to voters to try to verify SAVE’s citizenship information. Overall, more than 5% of the voters SAVE identified as noncitizens proved to be citizens. In some smaller counties, most of those flagged were eligible to vote. That includes six of 11 in the Panhandle’s Moore County, and two of three in Erath County, near Dallas.But some of those who didn’t respond to notices also might be citizens.In Denton County, where Sofia Minotti lives, checks by elections administrator Frank Phillips’ staff delivered clear answers on the citizenship status of 26 of the 84 voters flagged by SAVE. Twelve, including Minotti, proved they were citizens. Fourteen more had marked on their registration forms that they weren’t and the blame rested with workers for registering them nonetheless.Phillips said he removed anyone who didn’t provide proof by the deadline from the rolls to comply with the secretary of state’s instructions, but he fears some were eligible voters.“What is bugging me is I think our voter rolls may be more accurate than this database,” Phillips said. “My gut feeling is more of these are citizens than not.”At least initially, Missouri took a more targeted approach to SAVE than Texas did. State officials used the system to search for information on a subset of about 6,000 voters they had reason to think might not be citizens, according to emails between federal and state officials.The state had results by October, but in early November, a USCIS official wrote to Missouri and four other states to say some people flagged by SAVE as noncitizens were actually citizens, emails obtained through public records requests show.“We have continued to refine our processes used to obtain and review the citizenship data available to us,” the official wrote, adding that one such improvement revealed the errors.The staffer attached amended search results, but Missouri officials withheld the attachment from its response to a public records request and did not respond to a question about how many corrections were made.Based on the updated data from USCIS, Missouri sent lists of flagged voters to county election administrators in November. ProPublica and the Tribune obtained these lists for seven of 10 most populous counties in the state, which show SAVE initially identified more than 1,200 people as noncitizens just in these areas.The Missouri secretary of state’s office told election administrators it would work to verify SAVE’s citizenship determinations. In the meantime, local officials were instructed to change the status of flagged voters, making them temporarily unable to vote.The lists were met with swift pushback from county election officials, who, like Lennon, soon spotted people they knew to be citizens and questioned the directive’s legality. On a group call in November, they traded examples, saying they recognized neighbors, colleagues and people they’d helped to register at naturalization ceremonies.In St. Louis, the Board of Election Commissioners didn’t alter the eligibility of anyone on its flagged voter list after being advised not to by its attorney.Rachael Dunn, a spokesperson for Hoskins, the Missouri secretary of state, said state law allows officials to change voters’ status during investigations into their eligibility — for example, if there are signs they’ve moved. The laws she cited don’t directly address investigations into citizenship status, however.In early December, some 70 clerks, Republicans and Democrats, wrote a letter to Missouri House Speaker Jonathan Patterson saying there were better ways than SAVE to keep noncitizens off voter rolls.Weeks later, the state’s election integrity director, Nick La Strada, wrote USCIS to ask why a voter that SAVE had identified as a noncitizen in October had showed up in a more recent search as a citizen.A USCIS official replied that between the initial search and the follow-up, DHS had gotten access to passport data, which contains more up-to-date citizenship information on some people not born in the U.S.The USCIS staffer explained that some of the most accurate citizenship information — which is within DHS’ own records — still wasn’t searchable in SAVE because running that kind of search would require the voter’s DHS identifier, which can’t always be located. The staffer said they were working on improvements but those could take until March.“You don’t start with something at that scale until you work the bugs out, and that is not the case here,” Clinton Jenkins, president of the Missouri Association of County Clerks and Election Authorities, said in an interview. Jenkins is also the clerk for Miller County in the Ozarks.In early January, in what was framed as a “SAVE review update,” the secretary of state’s office sent counties across Missouri revised lists with reduced numbers of voters identified as potential noncitizens. It instructed election administrators to move voters who’d been initially flagged in error by SAVE back to active status, restoring their eligibility to vote.Dunn, Hoskins’ spokesperson, didn’t specify what prompted these adjustments. Even the new lists may not be final, she acknowledged. Once the review is complete, the state has said it plans to send letters to those still on the lists, demanding proof of citizenship and giving recipients 90 days to respond.The addition of new data to SAVE makes it a more valuable resource, she maintained, “while also reinforcing the need for careful, layered review before any action is taken.”After the January revision, St. Louis County’s initial list of 691 potential noncitizens dropped to 133.Zuzana Kocsisova, who lives in St. Louis, was among those incorrectly flagged by SAVE on its first pass. Originally from Slovakia, she became a U.S. citizen in 2019. She showed ProPublica and the Tribune a copy of her naturalization certificate, which she keeps with a letter from Trump congratulating her for “becoming a citizen of this magnificent land.”When a reporter told her that SAVE had initially identified her as a potential noncitizen, she said she wasn’t surprised. She saw it as part of the Trump administration’s targeting of immigrants. She was more frustrated than relieved to learn that she wasn’t on the smaller list of flagged voters sent in January.“Overall, it seems like this process has done more to worry people who can vote than to identify actual registered voters who don’t qualify,” she said. “It’s just a waste of resources. I don’t think it makes the elections any more safe.”In Boone County, where Lennon is the clerk, the count of flagged voters fell from 74 to 33 and the naturalized citizen who Lennon’s staff helped register was no longer on the list.Lennon said she and other county clerks would happily accept data that helps them correctly identify noncitizens on their voter rolls. But so far, SAVE hasn’t done that. And until it does, she said, she won’t purge voters purely because SAVE has flagged them.“This is not ready for prime time,” Lennon said. “And I’m not going to risk the security and the constitutional rights of my voters for bad data.”]]></content:encoded></item><item><title>Which Piece of Speculative Fiction Had the Greatest Single-Day Stock Market Impact?</title><link>https://slashdot.org/story/26/02/26/1744240/which-piece-of-speculative-fiction-had-the-greatest-single-day-stock-market-impact?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 18:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Speaking of the Citrini's blog post, which imagines a near-future AI-driven economic collapse, and which ended up help triggering the S&P 500's worst single-day drop in nearly two weeks on Monday, FT Alphaville decided to track how US stock markets have moved on the release days of notable dystopian speculative fiction throughout history. The story adds: You may contend that this is facile. We would agree. You might contend that the comparisons make no sense because it's possible to read a blog post during a single work shift, but it's tricker to complete a whole novel (or sneak out to watch a movie). We would contend: do you really think traders read? Let's begin. The methodology -- tracking S&P 500 daily moves for post-1986 releases and DJIA moves for pre-1986 ones -- crowned The Matrix as the all-time leader, its March 1999 US debut coinciding with a 1.11% drop in the index. Citrini's "The 2028 Global Intelligence Crisis" came in a close second at -1.04%. On the positive end, the 2013 release of Her, a film about a man falling in love with an AI agent, coincided with the largest gain in the set at +1.66%.]]></content:encoded></item><item><title>The Government Just Made it Harder to See What Spy Tech it Buys</title><link>https://yro.slashdot.org/story/26/02/26/1722205/the-government-just-made-it-harder-to-see-what-spy-tech-it-buys?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 18:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: It might look like something from the early days of the internet, with its aggressively grey color scheme and rectangles nested inside rectangles, but FPDS.gov is one of the most important resources for keeping tabs on what powerful spying tools U.S. government agencies are buying. It includes everything from phone hacking technology, to masses of location data, to more Palantir installations. 

Or rather, it was an incredible tool and the basis for countless of my own investigations and others. Because on Wednesday, the government shut it down. Its replacement, another site called SAM.gov with Uncle Sam branding, frankly sucks, and makes it demonstrably harder to reliably find out what agencies, including Immigration and Customs Enforcement (ICE), are spending tax payers dollars on. 

"FPDS may have been a little clunky, but its simple, old-school interface made it extremely functional and robust. Every facet of government operations touches on contracting at one point, and this was the first tool that many investigative journalists and researchers would reach for to quickly find out what the government is buying and who is selling it, and how these contracts all fit together," Dave Maass, director of investigations at the Electronic Frontier Foundation, told me.]]></content:encoded></item><item><title>Threads is testing a shortcut to quickly start DM conversations</title><link>https://techcrunch.com/2026/02/26/threads-is-testing-a-shortcut-to-quickly-start-dm-conversations/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 26 Feb 2026 17:58:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Users who are part of the test can type "DM me" or "Message me" in a post or reply to automatically generate a hyperlink that invites others to start a private conversation with them.]]></content:encoded></item><item><title>The DOJ ‘Forgot’ To Mention The Law Restricting Searches Of Journalists. The Judge Is Not Happy</title><link>https://www.techdirt.com/2026/02/26/the-doj-forgot-to-mention-the-law-restricting-searches-of-journalists-the-judge-is-not-happy/</link><author>Mike Masnick</author><category>tech</category><pubDate>Thu, 26 Feb 2026 17:27:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[We wrote recently about the FBI’s pre-dawn raid on Washington Post reporter Hannah Natanson’s home, in which agents seized two laptops, a phone, a portable hard drive, a recording device, and even a Garmin watch. Natanson covers the federal workforce and had cultivated nearly 1,200 confidential sources across more than 120 government agencies. She was not accused of any crime. She was not the target of any investigation. The FBI told her that much while they were busy carting away basically everything she uses to do her job.The raid was connected to the prosecution of Aurelio Perez-Lugones, a government contractor charged with retaining classified information. The DOJ wanted to rummage through a journalist’s entire digital life to find evidence against someone else. And they got a warrant to do it by, among other things, simply  to the magistrate judge that there’s a federal law—the Privacy Protection Act of 1980—that exists specifically to prevent exactly this kind of thing from happening.Last week, at a hearing on the Washington Post’s motion to get the devices back, Magistrate Judge William Porter let the DOJ attorneys have it. And then on Tuesday, he issued his ruling, blocking the government from searching Natanson’s devices and rescinding the portion of the warrant that would have let them do so.The ruling is worth reading in full. Porter doesn’t mince words about what happened, even as he accepts some responsibility for his own failure to catch the omission:Before reaching the merits, the Court addresses a matter of significant concern: the government’s failure to identify and analyze the Privacy Protection Act of 1980, 42 U.S.C. §§ 2000aa et seq. (“PPA”), in its search warrant application.As the judge who found probable cause and approved the search warrant, the Court acknowledges that it did not independently identify the PPA when reviewing the warrant application.As far as this Court knows, courts have approved search warrants directed at members of the press in only a handful of instances. This Court had never received such an application and, at the time it approved the warrant, was unaware of the PPA. This Court’s review was limited to probable cause, andthe Court accepts that gap in its own analysis. But the government’s failure to identify the PPA as applicable to a request for a search warrant on a member of the press—and to analyze it in its warrant application—is another matter. This omission has seriously undermined the Court’s confidence in the government’s disclosures in this proceeding.Credit to the judge for admitting his own gap in knowledge. But, come on: EDVA handles more national security cases than practically any other jurisdiction in the country. That a magistrate judge there could be unaware of the Privacy Protection Act—a statute that exists specifically to prevent the government from doing exactly what it was asking him to authorize—seems bizarre. Though, it also suggests how rarely the DOJ even bothers to seek these warrants, and how heavily the system depends on prosecutors acting in good faith. Which brings us to the far bigger problem: the DOJ’s deliberate decision to never bring it up.And it wasn’t just some overworked junior attorney who “forgot.” As Porter notes in his ruling, lawyers at the highest levels of the DOJ were involved in getting this warrant approved:The Court’s communications with the government over two days were not limited to the local AUSA. Counsel from the highest levels of the DOJ participated in at least one of those calls. Many government lawyers had multiple opportunities to identify the PPA as controlling authority and to include an analysis of it in the warrant application. None of them did.None of them. Not the assistant US attorney who filed the application. Not the Principal Deputy Assistant Attorney General of the National Security Division who was on the phone. Not anyone in the chain that apparently went all the way up to Attorney General Pam Bondi, whose approval is required by the DOJ’s own regulations before you can seek a warrant against a member of the press.The attorney who submitted the application, Gordon Kromberg, is no novice. He’s a veteran national security prosecutor who worked on the Julian Assange case—a case built almost entirely around the intersection of the Espionage Act and journalism. The idea that he just didn’t think of the Privacy Protection Act while applying for a warrant to search a reporter’s home for evidence related to an Espionage Act prosecution beggars belief. (Kromberg was also accused of political shenanigans in that case too.)The Freedom of the Press Foundation apparently agrees: they’ve filed a bar complaint against Kromberg with the Virginia State Bar, arguing that his failure to disclose the PPA violated Rule 3.3—the “Candor Toward the Tribunal” rule. As the complaint notes, this “could not have been a mere oversight” given that the warrant “predictably” became national news and should have required authorization from the highest levels of the DOJ, including the Attorney General.“How could you miss it? How could you think it doesn’t apply?” Magistrate Judge William Porter asked a DOJ lawyer during a hearing in Alexandria, Virginia.“I find it hard to believe that in any way this law did not apply,” Porter added later.“You don’t think you have an obligation to say that?” Porter said at one point. “I’m a little frustrated with how the process went down.”When DOJ attorney Christian Dibblee tried to argue that the decision was made by officials above him and that he understood the judge’s “frustration,” Porter shot back: “That’s minimizing it!”Dibblee also tried the remarkable argument that the Privacy Protection Act wasn’t the kind of “adverse authority” that lawyers are typically required to disclose when making requests for warrants. A federal statute specifically governing searches of journalists’ materials somehow doesn’t count as relevant law when you’re applying for a warrant to search a journalist’s materials? Sure. That’s believable.Porter’s ruling addresses this attempted dodge in a footnote that is quietly devastating. Kromberg claimed at the hearing that he didn’t mention the PPA because he believed the statute’s “suspect exception” applied—the narrow carve-out for when the journalist herself has committed a crime. But Porter dismantles that excuse:The Court finds this explanation inadequate and only highlights why the AUSA should have analyzed the PPA in the application. The government cannot pretextually label a reporter a suspect simply to gather evidence against the actual target. DOJ’s governing guidelines between 2013 and 2020 prohibited invoking the suspect exception “if the sole purpose is to further the investigation of a person other than the member of the news media.” See 28 C.F.R. § 50.10(d)(5) (2016),https://perma.cc/S52Q-BKGD. Such a rule would mean that any invocation of the Espionage Act’s receipt provision, see 18 U.S.C. § 793(c), would automatically strip a reporter of PPA protection—an interpretation that would render the statute a nullity and cannot be reconciled with Congress’s purpose in enacting it. That the AUSA claims to have received contrary advice during the very period when DOJ policy reflected this limitation only underscores the inadequacy of the government’s analysis here.In other words: Kromberg’s excuse for not mentioning the law actually makes it worse, because it suggests the DOJ’s position is that any time a journalist receives classified information—which is what investigative national security journalists do—the PPA just evaporates. Which would make the statute entirely meaningless. Which is exactly how this DOJ would prefer to treat it.The ruling also highlights just how much the DOJ took from Natanson beyond what it had any conceivable right to. According to the CNN report linked above, at the hearing, the DOJ “quickly conceded ‘there is more information that was received than what was pursuant to the warrant,’ drawing a scoffing laugh from the judge.” Porter’s written opinion is blunt about the scope of the damage:No easy remedy exists here. Movants’ First Amendment rights have been restrained. The government seized all of Ms. Natanson’s work product, documentary material, and devices, terminating her access to the confidential sources she developed and to all the tools she needs as a working journalist. The government’s proposed remedy—that she simply buy a new phone and laptop, set up new accounts, and start from scratch—is unjust and unreasonable.The DOJ’s argument that Natanson could just “start from scratch” is the kind of thing that sounds reasonable only if you’ve never thought about journalism for more than thirty seconds. Or, I guess, if you’re being deliberately obtuse in court while trying to create chilling effects for journalists. Which is just part of the reason this is a clear First Amendment violation:The government has seized the entirety of Ms. Natanson’s work product: her active stories, her notes on future investigations, and her background and confidential source material that, once compromised, cannot be replaced. The government’s suggestion that she can simply start from scratch fails to recognize the realities of modern journalism and the value of confidential source relationships cultivated over time. The Court finds that seizing the totality of a reporter’s electronic work product, including tools essential to ongoing newsgathering,constitutes a restraint on the exercise of First Amendment rights.Separately, Porter refused to let the government’s own filter team conduct the review of the seized materials, citing a Fourth Circuit precedent that directly applies here. The government wanted its own people to sift through  of Natanson’s data. Porter said no, invoking language from the circuit court that captures the absurdity of the DOJ’s proposal perfectly:Given the documented reporting on government leak investigations and the government’s well-chronicled efforts to stop them, allowing the government’s filter team to search a reporter’s work product—most of which consists of unrelated information from confidential sources—is the equivalent of leaving the government’s fox in charge of the Washington Post’s henhouse…. The concern that a filter team may err by neglect, by malice, or by honest difference of opinion is heightened where its institutional interests are so directly at odds with the press freedom values at stake.Instead, Porter will conduct the review himself, which is the right call under the circumstances, even if it means the process will take significantly longer.Porter also explains how the DOJ’s conduct has changed the way he will approach their representations going forward. A federal judge, explaining on the record that he can no longer take the government at its word:In its day-to-day workings, this Court affords government attorneys a presumption of regularity, including by assuming that federal prosecutors have satisfied their obligation to disclose controlling and relevant authority…..The government’s conduct has disturbed that baseline posture of deference.That phrase—”disturbed that baseline posture of deference”—is doing a lot of work. It’s a judge admitting, as diplomatically as the federal judiciary allows, that the DOJ exploited his trust. Porter mentioned in passing that the week he received this warrant request there were 45 other such requests.It feels a bit late for Porter to notice this, but the federal judiciary can be slow. For years we’ve called out how the DOJ frequently lies to judges, especially in any case they can slap a “national security” label on. And it’s been a long-term Techdirt complaint that judges give them a tremendous amount of unearned deference.The DOJ lies. But this DOJ is so over the top in its misrepresentations, it appears judges are finally learning that.The “presumption of regularity” that Porter describes is supposed to be earned through consistent good-faith conduct, and this DOJ has burned through whatever reserves of credibility it had.The bar complaint and the judge’s frustration are both welcome. But what has already happened cannot be undone. Natanson’s 1,200 confidential sources—federal employees who reached out to her because they were afraid of retaliation from this administration—now know that their communications may be sitting in government hands. The fact that a judge eventually blocked the search doesn’t un-ring that bell. Every source who has ever talked to Natanson, and every source thinking about talking to any journalist covering this administration, has received the message loud and clear.Porter seems to understand this. His closing paragraph carries what you might charitably call restrained skepticism:The Court’s genuine hope is that this search was conducted—as the government contends—to gather evidence of a crime in a single case, not to collect information about confidential sources from a reporter who has published articles critical of the administration. The Court further hopes the record ultimately bears out the government’s representations“Genuine hope.” A federal judge—bound by norms of restraint, writing in a judicial opinion—is telling us that the best he can offer is that he  the DOJ didn’t exploit his courtroom to target a journalist’s sources. He’s not saying he believes them. He’s not saying the evidence supports their claims. He’s saying he hopes. That’s as close as a sitting federal judge can come to calling the government liars without actually using the word. And he’s not alone—we’re hearing more and more judges feeling the need to speak out.The outcome here is not the  case scenario. Porter blocked the search, rescinded the review authorization, and will conduct the review himself rather than letting the DOJ’s own team paw through a reporter’s entire professional life. But the damage from the raid itself—the seizure, the chilling effect, the signal sent to every government employee who might consider talking to a reporter—was baked in the moment the FBI knocked on Natanson’s door at six in the morning.If federal judges want this to stop, “frustration” expressed in hearings and “disturbed” confidence described in memorandum opinions aren’t going to cut it. Judges need to start imposing real consequences—sanctions, referrals, contempt—on individual DOJ lawyers who treat “candor toward the tribunal” as an optional courtesy rather than a professional obligation. Because right now, the DOJ has learned that the price for misleading a court to execute an unconstitutional raid on a journalist is a stern talking-to and a slightly more complicated review process a month later. Omit the inconvenient law. Exploit the judge’s trust. Execute the raid. Deal with the consequences later.Judges used to “trust” DOJ representations. Now we’ve blown right past “trust, but verify” all the way to “never trust, always verify.”Judge Porter has now learned, painfully and publicly, that this DOJ is not acting in good faith. He’s unlikely to be the last such judge.]]></content:encoded></item><item><title>The AI Case Against Indian IT Ignores What Indian IT Actually Does</title><link>https://slashdot.org/story/26/02/26/1714241/the-ai-case-against-indian-it-ignores-what-indian-it-actually-does?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 17:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A fictional memo set in June 2028, published by short seller Citrini Research, wiped roughly $10 billion off Indian IT stocks in a single trading session on February 24 and sent the Nifty IT index down as much as 5.3% -- its worst single-day fall since August 2023 -- on the argument that AI coding agents have collapsed the cost advantage of Indian developers to the price of electricity. The index has shed more than $68 billion in market value in February alone, its worst month since 2003. 

But the core claim that India's entire $205 billion software export industry rests on cheap labor is roughly 15 years out of date, an analysis argues, custom application maintenance alone accounts for about 35% of a typical Indian IT firm's revenue, per HSBC, and enterprise platforms require deterministic outputs that probabilistic AI systems cannot wholesale replace. HSBC estimates gross AI-led revenue deflation for the sector at 14-16%, a measured headwind rather than an extinction event. The story adds: 24 years of software export data that has never posted a decline, $200 billion in annual revenue, partnerships with the very AI labs whose products are supposed to be the instrument of the sector's destruction, possibly a new $1.5 trillion market category emerging at the intersection of services and software, and the largest U.S. corporates in the middle of mapping their entire workforces into process architectures that require technology partners to modernise. I think India's IT is going to be fine.]]></content:encoded></item><item><title>Company Helps Men Scrub Negative Posts About Them from Tea App</title><link>https://www.404media.co/company-helps-men-scrub-negative-posts-about-them-from-tea-app/</link><author>Emanuel Maiberg</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/02/teaappremoval.jpg" length="" type=""/><pubDate>Thu, 26 Feb 2026 17:02:16 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[, a service that claims it can “protect your digital reputation,” will remove negative posts about men from private online groups where women share “red flags” about men they’ve dated in order to help other women. The service is another escalation in the age of online dating, women attempting to protect each other from other men in the dating pool, and instances of men fighting against those efforts. It also shows how some of these allegedly private women’s groups, especially the Tea app, are regularly infiltrated and manipulated by men. When I reached out to an email listed on Tea App Green Flags’s site, I got a call from a man behind the operation who identified only as Jay. He said he started the service about two years ago, and that he initially focused on the Are We Dating the Same Guy Facebook groups. For the past year, he’s been offering services specifically for the Tea app, a “dating safety” app for women that suffered a devastating breach last year, and which my , was founded by a man who wanted to monetize the Are We Dating the Same guy phenomenon. The site also claims it can remove posts from Tea app copycat for men TeaOnHer, as well as posts on Instagram.Jay declined to say how much revenue the site generates, but claims he gets about 50 to 60 calls a day and currently has six employees. On its website, Tea App Green Flags claims it has removed more than 2,500 posts on the Tea app for 759 clients. Jay said that most of his clients are men, but that some are women who are trying to take down posts about their husbands or boyfriends. Potential clients can pay $1.99 to report one account and up to $79.99 to report 25 accounts. “We just want to take down posts about people who are being defamed,” Jay told me. “And when I say defamed, it means like, ‘this guy has a small penis,’ or ‘this guy smells.’ That doesn't fit the mission statement of what the Tea app was for, which is to warn women against people who are harmful, who are abusive, who are cheaters. We've noticed that a lot of the individuals that come to us, almost all of them, come to us for little stupid things.”Clients interested in Tea App Green Flags’s services go to the site and fill out a form with their information and information about the posts they want removed. The company reviews the case and then starts the “takedown process,” which can take between 21-30 days. Tea App Green Flags says it will then continue to monitor posts about the client and remove them for three months.Were you impacted by the Tea hack? I would love to hear from you. Using a non-work device, you can message me securely on Signal at ‪@emanuel.404‬. Otherwise, send me an email at emanuel@404media.co.When I asked Jay how this “takedown process” works he said “I can’t give that info. That’s the business.”Jay told me that he would not work with clients who have been accused of sexual assault by multiple people on the Tea app, or by one person in one of the Are We Dating the Same Guy Facebook groups who used their real name and face in a profile picture. “Sometimes we find along the process that there are pedophiles or people who actually did what they did, and they're very bad,” Jay said. “So we say, we're not doing this. We can't take a rap for that. We're ethical. We just want to take down people who are being defamed.”Jay told me he understands why Facebook groups like Are We Dating the Same Guy are necessary and thinks they are a good idea, but the anonymous nature of the Tea app "causes a cesspool of defamation.” When I asked Jay what he thinks about the fact that some women don’t feel safe sharing information about some dangerous men unless they can do so anonymously, he said it would be better if women showed their face, or if the Tea app at least gave women that option. “I have a Tea app account. I'm a dude. All my reps have Tea app accounts. They're men,” Jay said. “How much can you trust these people and what they're doing?”One reason the Tea app hack was so dangerous is because the app used to ask women to upload a picture of their face in order to verify that they are women. Those images were posted all over the internet because of the hack, putting those women at risk and leading to more harassment. Tea App Green Flags is far from the first attempt from men trying to fight back against these types of groups. In 2024, for example, we wrote about  women who posted about him in Are We Dating the Same Guy Facebook groups. His first case was dismissed, and he refiled days later as a class action lawsuit; later that year, he was sent to prison for tax fraud.Tea did not immediately respond to a request for comment.]]></content:encoded></item><item><title>New Path to Battery-Grade Lithium Uses Electrochemistry</title><link>https://spectrum.ieee.org/mangrove-lithium-refining-ev-bottleneck</link><author>Vanessa Bates Ramirez</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NTAyNDY2Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgzMjEzMTYyNX0.UtBtcYinamV9_hbLF83QMMsOMHjXQ4lB3aoiaYO4wQM/image.jpg?width=600" length="" type=""/><pubDate>Thu, 26 Feb 2026 17:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Mangrove Lithium’s refinement may ease a key EV bottleneck]]></content:encoded></item><item><title>Read AI launches an email-based ‘digital twin’ to help you with schedules and answers</title><link>https://techcrunch.com/2026/02/26/read-ai-launches-an-email-based-digital-twin-to-help-you-with-schedules-and-answers/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 26 Feb 2026 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Read AI is launching Ada, which can reply with your availability and extract answers from the company knowledge base and the web.]]></content:encoded></item><item><title>Claude Opus 4.6 and GPT-5.3 Codex: Evaluating the New Leaders in AI-Driven Software Engineering</title><link>https://hackernoon.com/claude-opus-46-and-gpt-53-codex-evaluating-the-new-leaders-in-ai-driven-software-engineering?source=rss</link><author>Arun DHANARAJ</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:51:29 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The February 2026 release of Anthropic's Claude Opus 4.6 and OpenAI's GPT-5.3 Codex represents the closest head-to-head launch window in frontier AI model history, with both models debuting within 24 hours of each other. This paper provides a comprehensive comparative analysis of these two flagship coding-focused language models across technical capabilities, benchmark performance, architectural approaches, safety frameworks, and deployment considerations. Our analysis reveals distinct strategic positioning: Claude Opus 4.6 prioritizes reasoning depth and long-context analysis with state-of-the-art performance on academic benchmarks (GPQA Diamond: 77.3%, MMLU Pro: 85.1%), while GPT-5.3 Codex emphasizes agentic speed and coding throughput with 25% faster inference and superior terminal automation capabilities (Terminal-Bench 2.0: 77.3%). Both models demonstrate significant advances in autonomous software engineering, though they employ divergent architectural philosophies—constitutional alignment versus ecosystem-level defenses—that have substantial implications for enterprise adoption. This research provides decision frameworks for organizations evaluating these models and identifies optimal use-case segmentation strategies for multi-model deployments.The February 2026 Frontier AI Release EventOn February 4, 2026, Anthropic released Claude Opus 4.6, its most capable model to date, featuring enhanced coding skills, agentic task sustainability, and a breakthrough 1-million-token context window[1]. Within 24 hours, OpenAI responded with GPT-5.3 Codex on February 5, 2026, positioning it as a high-throughput coding engine optimized for autonomous software engineering[2]. This unprecedented release cadence reflects intensifying competition in the frontier AI space and marks a critical inflection point in enterprise AI adoption.The timing of these releases is significant for three reasons. First, both models represent flagship upgrades to their respective families, incorporating fundamental architectural innovations rather than incremental improvements. Second, the simultaneous launch creates a natural experiment for comparative evaluation, as both models target similar use cases with different technical approaches. Third, the releases signal a strategic shift from general-purpose language models toward specialized coding and agentic capabilities, reflecting market demand for AI systems that can autonomously complete complex software engineering tasks.This paper addresses four primary research questions:What are the quantitative performance differences between Claude Opus 4.6 and GPT-5.3 Codex across standardized benchmarks?How do architectural choices—reasoning depth versus inference speed, long-context windows versus computational efficiency affect practical deployment outcomes?What safety and alignment frameworks distinguish these models, and what implications do these frameworks have for regulated industries?Under what conditions should organizations choose one model over the other, and when does a multi-model deployment strategy provide optimal results?Our analysis draws on official benchmark results published by both companies, third-party evaluations, early access partner testimonials, and comparative testing on real-world coding tasks.Technical Architecture and Core CapabilitiesContext Windows and Output CapacityClaude Opus 4.6 introduces a 1-million-token context window in beta, representing a 5× increase over standard production limits (200k tokens)[1]. This extended context enables whole-codebase analysis, multi-document synthesis, and long-horizon agentic tasks without chunking or retrieval augmentation. The model supports output sequences up to 128,000 tokens, allowing generation of complete documentation sets, large-scale refactors, or comprehensive reports in a single API call[1].In contrast, GPT-5.3 Codex maintains a 400,000-token context window but optimizes for computational efficiency and inference speed rather than maximum context length[2]. OpenAI's architecture prioritizes rapid iteration in agentic loops over single-pass long-context processing. The 128,000-token output limit matches Claude, ensuring parity on large-output tasks[3]. For codebases exceeding 200,000 tokens or documentation projects requiring extensive synthesis, Claude's 1M context provides a structural advantage. For agentic workflows that make hundreds of short API calls with rapid feedback loops, GPT-5.3's optimized inference pipeline delivers better throughput.Reasoning and Planning MechanismsClaude Opus 4.6 introduces , a configurable reasoning system that dynamically adjusts computational effort based on task complexity[1]. The system operates across four effort levels (low, medium, high, max) and allocates up to 128,000 tokens to internal reasoning chains before generating final outputs. This architecture enables the model to "think more deeply and carefully revisit its reasoning" before committing to answers[1].Internal testing by Anthropic engineers reveals that Opus 4.6 "brings more focus to the most challenging parts of a task without being told to, moves quickly through the more straightforward parts, handles ambiguous problems with better judgment, and stays productive over longer sessions"[1]. Early access partner Devin (Cognition AI) reported that Opus 4.6 "reasons through complex problems at a level we haven't seen before" and "considers edge cases that other models miss"[1].GPT-5.3 Codex employs a different approach, optimizing for  rather than extended internal deliberation. The model achieves 25% faster inference compared to its predecessor (GPT-5.2 Codex) through architectural optimizations in the attention mechanism and more efficient token generation[2][3]. Rather than allocating large reasoning budgets before responding, GPT-5.3 emphasizes rapid hypothesis testing and iterative refinement through tool use and code execution.OpenAI's design philosophy centers on self-bootstrapping sandboxes that allow the model to execute, validate, and debug code in tight feedback loops[2][3]. This approach reduces latency for long-running agentic tasks by minimizing the cost of individual reasoning steps while increasing the number of iterations per unit time. Claude's adaptive thinking excels on tasks requiring deep analysis before action—architectural decisions, security audits, complex debugging. GPT-5.3's speed advantage becomes decisive when throughput matters more than deliberation—automated testing, large-scale refactors, high-volume code generation.Both models introduce mechanisms for persistent agentic workflows, addressing a critical limitation of earlier systems: context exhaustion during long-running tasks.Claude Opus 4.6 implements , an API feature that automatically summarizes and replaces older conversation turns when approaching the context window limit[1]. This capability enables agents to operate continuously without manual checkpoint management or conversation resets. Compaction thresholds are configurable, allowing developers to balance compression aggressiveness against information retention.GPT-5.3 Codex supports agentic persistence through , which allows developers to redirect agent behavior mid-task without losing accumulated context[2][3]. The model also reduces premature completion rates in flaky-test scenarios and long-horizon tasks, a persistent failure mode in earlier agentic systems[3].Anthropic reports that Opus 4.6 successfully "autonomously closed 13 issues and assigned 12 issues to the right team members in a single day, managing a ~50-person organization across 6 repositories"[1]. OpenAI emphasizes GPT-5.3's lower premature-completion rates and ability to maintain task coherence across hundreds of tool calls[2].|  |  |  |  |
|----|----|----|----|
| SWE-bench   Verified | 79.4% | — | Real-world   GitHub issues (Anthropic variant) |
| SWE-bench Pro   Public | — | 78.2% | Enhanced   difficulty tier (OpenAI variant) |
| Terminal-Bench   2.0 | 65.4% | 77.3% | Command-line   automation tasks |
| OSWorld-Verified | — | 64.7% | Desktop GUI   automation |
| TAU-bench   (airline) | 67.5% | 61.2% | Tool-augmented   reasoning |\
Table 1: Coding and agentic benchmark comparisonCritical methodological note: Anthropic reports SWE-bench Verified scores while OpenAI reports SWE-bench Pro Public scores. These are distinct benchmark variants with different problem sets and difficulty distributions. Direct numerical comparison across variants is methodologically invalid[3].Despite this limitation, directional patterns emerge. Claude Opus 4.6 demonstrates superior performance on tasks requiring reasoning and planning before execution (TAU-bench), while GPT-5.3 Codex dominates terminal automation and computer-use workflows (Terminal-Bench, OSWorld). Both models achieve scores near 80% on their respective SWE-bench variants, representing state-of-the-art performance on autonomous coding tasks.Reasoning and Knowledge Benchmarks|  |  |  |  |
|----|----|----|----|
| GPQA Diamond | 77.3% | 73.8% | Graduate-level   STEM reasoning |
| MMLU Pro | 85.1% | 82.9% | Expert   knowledge across domains |
| Humanity's   Last Exam | 78.6% | — | Complex   multidisciplinary reasoning |
| GDPval-AA   (Elo) | 1606 | — | Economic   reasoning tasks |
| BigLaw Bench | 90.2% | — | Legal   reasoning and analysis |\
Table 2: Reasoning and knowledge benchmark comparisonClaude Opus 4.6 establishes clear leadership on reasoning-heavy academic and professional benchmarks. The 3.5-percentage-point advantage on GPQA Diamond (graduate-level physics, chemistry, and biology questions) and 2.2-point lead on MMLU Pro represent statistically significant improvements over GPT-5.3 Codex[1][3].Anthropic reports that on GDPval-AA—an evaluation of economically valuable knowledge work across finance, legal, and other professional domains—Opus 4.6 outperforms GPT-5.2 (OpenAI's previous best model on this benchmark) by approximately 144 Elo points, translating to a win rate of approximately 70%[1]. This differential suggests substantial practical advantages for consulting, financial analysis, and legal research applications.A persistent challenge in large-context language models is "context rot"—performance degradation as conversation length increases. Claude Opus 4.6 addresses this limitation through architectural improvements in attention mechanisms and information retrieval.On the 8-needle 1M variant of MRCR v2 (a needle-in-a-haystack benchmark testing retrieval of information hidden in vast text corpora), Opus 4.6 scores 76%, compared to just 18.5% for its predecessor, Claude Sonnet 4.5[1]. This represents a qualitative shift in usable context length, enabling applications that require tracking details across millions of tokens.Anthropic partner Box reported that Opus 4.6 "excels in high-reasoning tasks like multi-source analysis across legal, financial, and technical content," with a 10% performance lift reaching 68% accuracy versus a 58% baseline[1]. Ross Intelligence noted that Opus 4.6 "represents a meaningful leap in long-context performance" with improved consistency across large information bodies[1].Safety and Alignment FrameworksAnthropic's Constitutional AI ApproachClaude Opus 4.6 implements Constitutional AI v3, Anthropic's third-generation alignment framework[1]. The system employs automated behavioral audits across multiple risk dimensions, including:Deception detection (self-exfiltration attempts, hidden reasoning, misleading outputs)Sycophancy reduction (excessive agreement, user-delusion reinforcement)Misuse cooperation resistance (dual-use capabilities, dangerous request compliance)Over-refusal minimization (false-positive safety triggers on benign queries)Anthropic reports that Opus 4.6 shows "low rates of misaligned behaviors" and achieves "the lowest rate of over-refusals of any recent Claude model"[1]. The company conducted "the most comprehensive set of safety evaluations of any model," including new assessments for user wellbeing, complex refusal testing, and interpretability methods to understand internal model behavior[1].For cybersecurity capabilities—where Opus 4.6 shows "enhanced abilities" that could be misused—Anthropic developed six new probes to track different forms of potential abuse[1]. The company simultaneously accelerated defensive applications, using the model to find and patch vulnerabilities in open-source software[1].OpenAI's Preparedness FrameworkGPT-5.3 Codex represents the first model classified as "High" for cybersecurity risk under OpenAI's Preparedness Framework, requiring enhanced deployment safeguards[2]. OpenAI's approach emphasizes structured deployment gates and ecosystem-level defenses rather than internal constitutional constraints.The framework operates through tiered risk classification (Low, Medium, High, Critical) across four risk categories: cybersecurity, CBRN (chemical, biological, radiological, nuclear), persuasion, and model autonomy[2]. High-risk classifications trigger mandatory mitigations, including real-time intervention systems, usage monitoring, and restricted access controls.OpenAI has not yet published the detailed safety evaluation results for GPT-5.3 Codex equivalent to Anthropic's system card for Opus 4.6, making direct safety comparison difficult. However, the High cybersecurity classification indicates that OpenAI's internal red-teaming identified capabilities that could significantly assist offensive cyber operations if unrestricted[2].Anthropic's constitutional approach embeds alignment constraints directly into model behavior through training and reinforcement learning from AI feedback. This creates inherent safety properties that persist across deployment contexts. The trade-off is potential capability degradation on edge-case inputs where safety constraints trigger inappropriately.OpenAI's preparedness framework treats safety as a deployment property rather than a model property, enabling fine-grained control through external systems. This allows higher raw capability at the model level while shifting safety responsibilities to the platform layer. The trade-off is dependence on infrastructure reliability and potential bypass vulnerabilities in the safety wrapper.For regulated industries (healthcare, finance, legal), Anthropic's documented low misalignment rates and comprehensive system card provide clearer audit trails. For organizations with mature AI governance and custom safety requirements, OpenAI's external control mechanisms offer greater flexibility.Pricing and Deployment Economics|  |  |  |
|----|----|----|
| Input tokens   (standard) | $5 / million | Pending |
| Output tokens   (standard) | $25 / million | Pending |
| Input tokens   (premium) | $10 / million | — |
| Output tokens   (premium) | $37.50 / million | — |
| Prompt   caching | $1.25 / million (75% off) | TBD |
| Context   window | 200k (1M beta) | 400k |
| Max output | 128k tokens | 128k tokens |\
Table 3: API pricing comparison as of February 9, 2026Claude Opus 4.6 pricing is fully transparent and available immediately. Standard pricing ($5 input / $25 output per million tokens) applies to prompts up to 200,000 tokens. Premium pricing ($10 input / $37.50 per million tokens) applies when using the 1-million-token beta context window[1]. Anthropic's prompt caching system offers 75% cost reduction on repeated content, reducing input costs to $1.25 per million cached tokens[1].GPT-5.3 Codex API pricing remains unpublished as of February 9, 2026[3]. OpenAI announced that API access will become available "in the coming weeks" but has not provided cost estimates[2]. Current access is limited to ChatGPT Plus, Pro, Team, and Enterprise subscription tiers, with per-token API pricing expected at a later date.Cost modeling implications: Organizations planning February-March 2026 deployments can complete accurate cost projections for Claude Opus 4.6 but must estimate GPT-5.3 costs based on historical OpenAI pricing patterns. For budget-constrained projects, Claude's immediate pricing transparency reduces procurement uncertainty.Inference Speed and ThroughputGPT-5.3 Codex delivers 25% faster inference than its predecessor, translating to approximately 33% higher throughput for equivalent token volumes[2][3]. For high-volume agentic workflows making thousands of API calls daily, this speed advantage compounds significantly.Consider a development team running 5,000 agentic coding tasks per day, each requiring 10 API calls with 500-token responses. At 25% faster inference:Claude Opus 4.6 baseline: ~240 seconds per task → 20,000 minutes dailyGPT-5.3 Codex optimized: ~180 seconds per task → 15,000 minutes dailyNet productivity gain: 5,000 minutes (83 hours) of latency reduction dailyFor latency-sensitive applications (IDE integrations, real-time code review), GPT-5.3's speed advantage translates directly to user experience improvements. For batch processing or analysis tasks where wall-clock time is less critical, Claude's reasoning depth may justify the additional latency.Deployment Decision FrameworkSelection Criteria by Use Case|  |  |  |
|----|----|----|
| Graduate-level   research, academic analysis | Claude Opus   4.6 | GPQA Diamond:   77.3% vs. 73.8%; MMLU Pro: 85.1% vs. 82.9% |
| Long-context   document analysis (>200k tokens) | Claude Opus   4.6 | 1M context   window enables whole-document processing |
| Legal   reasoning, contract analysis | Claude Opus   4.6 | BigLaw Bench:   90.2%; GDPval-AA economic reasoning: 1606 Elo |
| High-volume   agentic coding loops | GPT-5.3 Codex | 25% faster   inference; lower premature completion rates |
| Terminal   automation, shell scripting | GPT-5.3 Codex | Terminal-Bench   2.0: 77.3% vs. 65.4% |
| Desktop GUI   automation | GPT-5.3 Codex | OSWorld-Verified:   64.7%; native computer-use capabilities |
| Regulated   industries (healthcare, finance) | Claude Opus   4.6 | Comprehensive   system card; low misalignment rates; constitutional AI audit trail |
| Existing   OpenAI ecosystem integration | GPT-5.3 Codex | Native   compatibility with Copilot, Azure OpenAI, ChatGPT Enterprise |\
Table 4: Model selection framework by use caseMulti-Model Deployment StrategyFor organizations with diverse AI workloads, a multi-model routing strategy can optimize for both performance and cost. The following architecture pattern demonstrates task-based model selection with automatic fallback:Routing Configuration Example:const MODEL_CONFIG = {
reasoning: {model: "claude-opus-4-6",
fallback: "gpt-5.3-codex",
use: "GPQA-heavy analysis, long-context docs, legal reasoning",
effortLevel: "high"},
coding: {
model: "gpt-5.3-codex",
fallback: "claude-opus-4-6",
use: "Agentic loops, terminal tasks, large-scale refactors",
maxRetries: 3
},
timeoutMs: 120000,
telemetry: {
trackAcceptanceRate: true,
trackRerunsPerModel: true,
trackReviewerEdits: true
}
};
\
This configuration routes reasoning-intensive tasks (research synthesis, architectural decisions, complex debugging) to Claude Opus 4.6 while directing high-throughput coding tasks (automated testing, refactors, terminal automation) to GPT-5.3 Codex. Fallback mechanisms ensure reliability when the primary model is unavailable or rate-limited.Key observability metrics:Patch acceptance rate by modelAverage reruns required before approvalReviewer edit density (lines changed post-generation)End-to-end task completion timeCost per successful task completionOrganizations should instrument these metrics during evaluation periods (30-90 days) to empirically validate model selection rather than relying solely on published benchmarks.From Claude Opus 4.5 to 4.6Anthropic introduced several breaking changes that require code modifications:Response prefilling disabled: Claude 4.5 supported response prefilling to guide output format. This capability is removed in 4.6. Migrate to system prompt instructions or few-shot examples.Extended thinking replaced by adaptive thinking: API calls using extended_thinking: true must migrate to the new effort-level system (effort: "low" | "medium" | "high" | "max").Context compaction opt-in: Long-running agentic tasks should enable compaction to prevent context exhaustion. Configure thresholds based on typical conversation lengths. Run parallel deployments of 4.5 and 4.6 on production traffic samples (10-20% of volume) for 2-4 weeks to identify behavioral differences before full cutover.From GPT-5.2 Codex to 5.3OpenAI has not yet published a migration guide for GPT-5.3 Codex as of February 9, 2026. Based on early access reports and the February 5 announcement, anticipated changes include:Faster default inference: 25% speed increase may affect timeout configurations and retry logic in existing agentic systems.Lower premature completion: Tasks that previously required explicit "continue" prompts may complete autonomously, potentially changing conversation flow.New deep-diff capabilities: Code review workflows can leverage enhanced diff explanations showing reasoning behind changes, not just the changes themselves.Organizations should maintain GPT-5.2 as a fallback option during the initial API rollout period, using feature flags or environment variables to control model routing while validating 5.3 behavior on internal codebases.Limitations and Future Research DirectionsBenchmark Validity and GeneralizationA critical limitation of this analysis is the non-comparability of SWE-bench variants. Anthropic and OpenAI report scores on different benchmark subsets (Verified vs. Pro Public), making direct numerical comparison invalid. This fragmentation reflects broader challenges in AI evaluation: companies selectively report benchmarks where their models perform favorably, and benchmark saturation (scores approaching 100%) reduces discriminatory power.Future research should prioritize:Standardized evaluation protocols accepted across companiesDomain-specific benchmarks for regulated industries (healthcare diagnostics, financial compliance, legal discovery)Long-term deployment studies tracking model performance on real engineering teams over months rather than synthetic benchmarksSafety Evaluation TransparencyWhile Anthropic published a comprehensive system card for Claude Opus 4.6[1], OpenAI has not released equivalent documentation for GPT-5.3 Codex as of February 9, 2026. This asymmetry limits rigorous safety comparison. The "High" cybersecurity classification suggests significant dual-use capabilities, but without detailed red-team reports, organizations cannot independently assess risk levels.The AI safety community requires standardized safety reporting frameworks analogous to Common Vulnerabilities and Exposures (CVE) systems in cybersecurity. Model cards should include:Quantified misalignment rates across behavioral categoriesRed-team success rates and exploitation vectorsDeployment mitigation effectiveness dataIncident response protocols and disclosure timelinesEconomic Model UncertaintyGPT-5.3 Codex pricing remains unpublished, preventing complete total-cost-of-ownership (TCO) analysis. Organizations evaluating these models in February-March 2026 face procurement uncertainty that may delay deployment decisions. OpenAI should prioritize API pricing transparency to enable enterprise planning.Additionally, neither company has published inference carbon emissions data, an increasingly important factor for organizations with sustainability commitments. Future model releases should include environmental impact assessments as standard practice.Claude Opus 4.6 and GPT-5.3 Codex represent distinct strategic visions for frontier AI development. Anthropic prioritizes reasoning depth, long-context capabilities, and constitutional alignment, producing a model optimized for high-stakes knowledge work where accuracy and judgment matter most. OpenAI emphasizes inference speed, agentic throughput, and ecosystem integration, creating a model designed for high-volume autonomous coding at scale.Neither model is universally superior. The optimal choice depends on workload characteristics, existing infrastructure, regulatory requirements, and organizational risk tolerance. For many enterprises, a multi-model routing strategy offers the best of both approaches: Claude for research, analysis, and regulatory applications; GPT-5.3 for coding automation, terminal workflows, and high-throughput tasks.As these models enter production deployment over the coming months, empirical performance data from real-world engineering teams will provide ground truth beyond synthetic benchmarks. Organizations should instrument telemetry from the outset, tracking acceptance rates, edit density, and task completion metrics to validate model selection decisions. The AI landscape continues to evolve rapidly; flexibility and evidence-based evaluation will remain critical success factors.]]></content:encoded></item><item><title>AI Coding Tip 008 - How to Use Spec-Driven Development With AI</title><link>https://hackernoon.com/ai-coding-tip-008-how-to-use-spec-driven-development-with-ai?source=rss</link><author>Maxi Contieri</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:45:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Learn guided by the domainTL;DR: Use AI to understand requirements and build a shared mental model while you write the code.You jump directly to code generation with a vague, wishful prompt.\
The AI appears to understand your specific business logic, but it comes across as condescending.\
The problem creates a spaghetti mess that is difficult to maintain later.\
The AI is not a magic button for lazy people. It is a senior pair programmer and a learning companion.\
You follow the Spec-Driven Development trend and work in a Taylorist cascading way, falling into analysis paralysis and unrealistic plans.Hallucinations: The AI guesses details when you don't provide specific context.\
Technical Debt: You build complex systems that collapse under logical errors and don't simulate the real-world MAPPER.\
Logic Drift: The code "works". Yet it doesn't solve the actual problem.Ask the AI to interview you.\
You state the high-level idea and have the AI ask questions to uncover edge cases.\
Draft a  file. You and the AI collaborate on a document that defines the architecture, data models, and goals.\
Keep the AI in a read-only environment to explore your codebase and verify the plan as you execute it.\
Plan as you go with the goal in mind without making assumptions about a rigid roadmap.\
Always validate the bijection against the real-world requirements.\
Turn the live spec into a simple checklist of atomic implementation steps.\
The backlog will grow and shrink as you learn the domain. It is a live artifact.\
Set up a persistent context while you learn.\
Create a  file to store project rules that the AI cannot guess.You learn about the domain faster because the AI can serve as an encyclopedic mentor.\
You stay proudly accountable for the architecture.\
You eliminate boilerplate while maintaining system stability.\
You close the Human 30% gap by focusing on system coordination.These tools are high-velocity coders, but they are very innocent.\
They perform best when you instruct with a clear mission and modular instructions.\
This "waterfall in 15 minutes" way favors you and the AI to be on the same page before you trigger the first code diff.Build me a task management app with React and Node.

Create a behavior specification and a Gantt project
You are a Senior Software Engineer. I want to build a task app.

Ask me 10 clarifying questions about the architecture, security, 
and data model. 

After I answer, help me draft a spec.md.

Let's build it together with TDD and contract tests.
AI can write bugs with complete conviction.\
You must review every change.Use CLAUDE.md for project memory.\
Set up MCP servers for live documentation.\
Run parallel agents for large refactors.You should invest 15 minutes in planning with the AI instead of rushing. It will save you hours of debugging.\
Use the copilot to improve your design with your approval, and let it handle the hard accidental typing.Break work into small, iterative chunksProvide extensive context and guidanceChoose the right model (and use multiple when needed)Leverage AI coding across the lifecycleKeep a human in the loop - verify, test, and review everythingCustomize the AI’s behavior with rules and examplesEmbrace testing and automation as force multipliersContinuously learn and adapt (AI amplifies your skills)The views expressed here are my own.\
I am a human who writes as best as possible for other humans.\
I use AI proofreading tools to improve some texts.\
I welcome constructive criticism and dialogue.\
I shape these insights through 30 years in the software industry, 25 years of teaching, and writing over 500 articles and a book.This article is part of the AI Coding Tip series.]]></content:encoded></item><item><title>Inside Tencent Games’ Real-Time Event-Driven Analytics System</title><link>https://hackernoon.com/inside-tencent-games-real-time-event-driven-analytics-system?source=rss</link><author>ScyllaDB</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:45:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A look at how Tencent Games built service architecture based on CQRS and event sourcing patterns with Pulsar and ScyllaDB.As a part of Tencent Interactive Entertainment Group Global (IEG Global), Proxima Beta is committed to supporting our teams and studios to bring unique, exhilarating games to millions of players around the world. You might be familiar with some of our current games, such as PUBG Mobile, Arena of Valor, and Tower of Fantasy.Our team at Level Infinite (the brand for global publishing) is responsible for managing a wide range of risks to our business – for example, cheating activities and harmful content. From a technical perspective, this required us to build an efficient real-time analytics system to consistently monitor all kinds of activities in our business domain.In this blog, we share our experience of building this real-time event-driven analytics system. First, we’ll explore why we built our service architecture based on Command and Query Responsibility Segregation (CQRS) and event sourcing patterns with Apache Pulsar and ScyllaDB. Next, we’ll look at how we use ScyllaDB to solve the problem of dispatching events to numerous gameplay sessions. Finally, we’ll cover how we use ScyllaDB keyspaces and data replication to simplify our global data management.A Peek at the Use Case: Addressing Risks in Tencent GamesLet’s start with a real-world example of what we’re working with and the challenges we face.This is a screenshot from Tower of Fantasy, a 3D-action role-playing game. Players can use this dialog to file a report against another player for various reasons. If you were to use a typical CRUD system for it, how would you keep those records for follow-ups? And what are the potential problems?The first challenge would be determining which team is going to own the database to store this form. There are different reasons to make a report (including an option called “Others”), so a case might be handled by different functional teams. However, there is not a single functional team in our organization that can fully own the form.That’s why it is a natural choice for us to capture this case as an event, like “report a case.” All the information is captured in this event as is. All functional teams only need to subscribe to this event and do their own filtering. If they think the case falls into their domain, they can just capture it and trigger further actions.The service architecture behind this example is based on the CQRS and event sourcing patterns. If these terms are new to you, don’t worry! By the end of this overview, you should have a solid understanding of these concepts. And if you want more detail at that point, take a look at our blog dedicated to this topic.The first concept to understand here is event sourcing. The core idea behind event sourcing is that every change to a system’s state is captured in an event object and these event objects are stored in the order in which they were applied to the system state. In other words, instead of just storing the current state, we use an append-only store to record the entire series of actions taken on that state. This concept is simple but powerful as the events that represent every action are recorded so that any possible model describing the system can be built from the events.The next concept is CQRS, which stands for Command Query Responsibility Segregation. CQRS was coined by Greg Young over a decade ago and originated from the Command and Query Separation Principle. The fundamental idea is to create separate data models for reads and writes, rather than using the same model for both purposes. By following the CQRS pattern, every API should either be a command that performs an action, or a query that returns data to the caller – but not both. This naturally divides the system into two parts: the write side and the read side.This separation offers several benefits. For example, we can scale write and read capacity independently for optimizing cost efficiency. From a teamwork perspective, different teams can create different views of the same data with fewer conflicts.The high-level workflow of the write side can be summarized as follows: events that occur in numerous gameplay sessions are fed into a limited number of event processors. The implementation is also straightforward, typically involving a message bus such as Pulsar, Kafka, or a simpler queue system that acts as an event store. Events from clients are persisted in the event store by topic and event processors consume events by subscribing to topics. If you’re interested in why we chose Apache Pulsar over other systems, you can find more information in the blog referenced earlier.Although queue-like systems are usually efficient at handling traffic that flows in one direction (e.g. fan-in), they may not be as effective at handling traffic that flows in the opposite direction (e.g. fan-out). In our scenario, the number of gameplay sessions will be large, and a typical queue system doesn’t fit well since we can’t afford to create a dedicated queue for every game-play session. We need to find a practical way to distribute findings and metrics to individual gameplay sessions through Query APIs. This is why we use ScyllaDB to build another queue-like event store, which is optimized for event fan-out. We will discuss this further in the next section.Before we move on, here’s a summary of our service architecture.\
Starting from the write side, game servers keep sending events to our system through Command endpoints and each event represents a certain kind of activity that occurred in a gameplay session. Event processors produce findings or metrics against the event streams of each gameplay session and act as a bridge between two sides. On the read side, we have game servers or other clients that keep polling metrics and findings through Query endpoints and take further actions if abnormal activities have been observed.Distributed Queue-Like Event Store for Time Series EventsNow let’s look at how we use ScyllaDB to solve the problem of dispatching events to numerous gameplay sessions. By the way, if you Google “Cassandra” and “queue”, you may come across an article from over a decade ago stating that using Cassandra as a queue is an anti-pattern. While this might have been true at that time, I would argue that it is only partially true today. We made it work with ScyllaDB (which is Cassandra-compatible).To support the dispatch of events to each gameplay session, we use the session id as the partition key so that each gameplay session has its own partition and events belonging to a particular gameplay session can be located by the session id efficiently.Each event also has a unique event id, which is a time UUID, as the clustering key. Because records within the same partition are sorted by the clustering key, the event id can be used as the position id in a queue. Finally, ScyllaDB clients can efficiently retrieve newly arrived events by tracking the event id of the most recent event that has been received.There is one caveat to keep in mind when using this approach: the consistency problem. Retrieving new events by tracking the most recent event id relies on the assumption that no event with a smaller id will be committed in the future. However, this assumption may not always hold true. For example, if two nodes generate two event identifiers at the same time, an event with a smaller id might be inserted later than an event with a larger id.This problem, which I refer to as a “phantom read,” is similar to the phenomenon in the SQL world where repeating the same query can yield different results due to uncommitted changes made by another transaction. However, the root cause of the problem in our case is different. It occurs when events are committed to ScyllaDB out of the order indicated by the event id.There are several ways to address this issue. One solution is to maintain a cluster-wide status, which I call a “pseudo now,” based on the smallest value of the moving timestamps among all event processors. Each event processor should also ensure that all future events have an event id greater than its current timestamp.Another important consideration is enabling TimeWindowCompactionStrategy, which eliminates the negative performance impact caused by tombstones. Accumulation of tombstones was a major issue that prevented the use of Cassandra as a queue before TimeWindowCompactionStrategy became available.Now let’s shift to discussing other benefits beyond using ScyllaDB as a dispatching queue.Since we are building a multi-tenancy system to serve customers around the world, it is essential to ensure that customer configurations are consistent across clusters in different regions. Trust is – keeping a distributed system consistent is not a trivial task if you plan to do it all by yourself.We solved this problem by simply enabling data replication on a keyspace across all data centers. This means any change made in one data center will eventually propagate to others. Thank ScyllaDB, as well as DynamoDB and Cassandra, for the heavy lifting that makes this challenging problem seem trivial. \n You might be thinking that using any typical RDBMS could achieve the same result since most databases also support data replication. This is true if there is only one instance of the control panel running in a given region. In a typical primary/replica architecture, only the primary node supports read/write while replica nodes are read-only. However, when you need to run multiple instances of the control panel across different regions– for example, every tenant has a control panel running in its home region, or even every region has a control panel running for local teams – it becomes much more difficult to implement this using a typical primary/replica architecture.If you have used AWS DynamoDB, you may be familiar with a feature called Global Table, which allows applications to read and write locally and access the data globally. Enabling replication on keyspaces with ScyllaDB provides a similar feature, but without vendor lock-in. You can easily extend global tables across a multi-cloud environment.Keyspaces as Data ContainersNext, let’s look at how we use keyspaces as data containers to improve the transparency of global data distribution.Let’s take a look at the diagram below. It shows a solution to a typical data distribution problem imposed by data protection laws. For example, suppose region A allows certain types of data to be processed outside of its borders as long as an original copy is kept in its region. As a product owner, how can you ensure that all your applications comply with this regulation?One potential solution is to perform end-to-end (E2E) tests to ensure that applications correctly send the correct data to the correct region as expected. This approach requires application developers to take full responsibility for implementing data distribution correctly. However, as the number of applications grows, it becomes impractical for each application to handle this problem individually and E2E tests also become increasingly expensive in terms of both time and money.Let’s think twice about this problem. By enabling data replication on keyspaces, we can divide the responsibility for correctly distributing data into two tasks: 1) identifying data types and declaring their destinations, and 2) copying or moving data to the expected locations.By separating these two duties, we can abstract away complex configurations and regulations from applications. This is because the process of transferring data to another region is often the most complicated part to deal with, such as passing through network boundaries, correctly encrypting traffic, and handling interruptions.After separating these two duties, applications are only required to correctly perform the first step, which is much easier to verify through testing at earlier stages of the development cycle. Additionally, the correctness of configurations for data distribution becomes much easier to verify and audit. You can simply check the settings of keyspaces to see where data is going.Tips for Others Taking a Similar PathTo conclude, we’ll leave you with important lessons that we learned, and that we recommend you apply if you end up taking a path similar to ours:Consider using keyspaces as data containers to separate the responsibility of data distribution. This can make complex data distribution problems much easier to manage.Watch Tech Talks On-DemandThis article is based on a tech talk presented at ScyllaDB Summit 2023. You watch this talk – as well as talks by engineers from Discord, Epic Games, Disney, Strava, ShareChat and more – on-demand.]]></content:encoded></item><item><title>Learn Kubernetes from Scratch (Without the Hype)</title><link>https://hackernoon.com/learn-kubernetes-from-scratch-without-the-hype?source=rss</link><author>Piyush Jajoo</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:45:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[ Someone who has never touched Kubernetes but wants to understand it well enough to discuss it confidently — and even run a few things on their laptop. Kubernetes is  Heroku or a full application platform. It does not build your app, manage your CI/CD pipeline, or automatically apply production best practices. It is an  — a very powerful one — but you still have to bring your own containers, configuration, security posture, and operational practices. Think of it as an incredibly capable infrastructure layer, not a magic button.Part 1 — What Problem Does Kubernetes Solve?Part 2 — Core Concepts: The Kubernetes VocabularyPart 3 — The Architecture: How It All Fits TogetherPart 4 — Hands-On: Your First Kubernetes AppPart 5 — Deployments, Scaling, and Self-HealingPart 6 — Networking: Services and How Apps Talk to Each OtherPart 7 — Configuration and SecretsPart 8 — Storage: Keeping Data AlivePart 9 — Observability: Knowing What’s Going OnPart 10 — Putting It All Together: The Big PictureAppendix — Common Beginner MistakesPart 1 — What Problem Does Kubernetes Solve?The World Before KubernetesImagine you’ve built a web app. It runs on a single server. Life is simple. But then:Traffic spikes — your server buckles.A new version breaks everything — you have downtime.Your server crashes at 2am — the app is down until someone wakes up.You need to run 10 copies of the app — you SSH into 10 machines manually.This is the problem Kubernetes was built to solve.Before Kubernetes, you need containers. A  is a lightweight, self-contained package that includes your app and everything it needs to run (libraries, runtime, config). Think of it as a shipping container: standardized, portable, stackable. (often abbreviated as  — 8 letters between K and s) is an open-source system for automating deployment, scaling, and management of containerized applications.In plain English: Kubernetes is the conductor of an orchestra of containers. You declare  you want running, and it figures out  to make it happen and continuously keeps it that way.Key Promises of Kubernetes| Promise | What it means |
|----|----|
|  | Crashed containers are restarted automatically |
|  | Add or remove instances based on load |
|  | Deploy new versions with zero downtime |
|  | Apps find each other without hardcoded IPs |
|  | Traffic is spread across healthy instances |What Kubernetes Does NOT Solve (Out of the Box)This is important to know upfront so you’re not surprised later:| Gap | What fills it |
|----|----|
|  | Jenkins, GitHub Actions, ArgoCD, Tekton |
| Container image security scanning | Trivy, Snyk, Harbor |
|  | HashiCorp Vault, Sealed Secrets |
|  | You add Prometheus, Grafana, Loki yourself |
|  | Cluster federation, multi-cluster tools |
|  | Helm, Kustomize, your own Dockerfile |Kubernetes was created by Google (based on their internal system called Borg) and open-sourced in 2014. It’s now maintained by the Cloud Native Computing Foundation (CNCF) and is the de facto standard for container orchestration. Kubernetes releases 3 minor versions per year (e.g., v1.28, v1.29, v1.30). APIs and behaviors can change between versions. Always check the Kubernetes changelog and verify compatibility with your cluster’s version.Part 2 — Core Concepts: The Kubernetes VocabularyOne reason Kubernetes feels intimidating is the terminology. Let’s demystify the key terms with real-world analogies.Kubernetes Is a Declarative, Desired-State SystemThis is the most important idea in the whole series. In Kubernetes, you don’t issue commands like “start this container now.” Instead, you describe what you  to exist, and Kubernetes continuously works to make reality match that description.\
   “I want 3 copies of my app running.” “Start container 1. Start container 2. Start container 3.”Kubernetes is always declarative. Your YAML files express desired state. The system stores that desired state and reconciles it with reality forever.A  is the entire Kubernetes environment — the collection of machines that Kubernetes manages together as one system.A  is an individual machine (physical or virtual) in the cluster. There are two types: — the brain. It makes decisions about the cluster and stores desired state. — where your actual application containers run. If the cluster is a restaurant, the control plane is the kitchen manager who tracks all the orders, and worker nodes are the individual chefs who actually cook.A  is the smallest deployable unit in Kubernetes. A Pod wraps one or more containers that should always run together and share the same network and storage. If a container is a single fish, a Pod is the fish tank. Usually one fish per tank, but sometimes a few that need to live together.Pods are  — they can be killed and replaced at any time.Each Pod gets its own IP address inside the cluster’s internal network. That IP is not routable outside the cluster and changes every time the Pod is rescheduled. Never hardcode a Pod IP.You rarely create Pods directly — you use higher-level abstractions like Deployments.A  tells Kubernetes: “I want X copies of this Pod running at all times, and here’s how to roll out changes safely.” It manages the full lifecycle of Pods — creating them, replacing crashed ones, and orchestrating updates.A  gives Pods a stable network identity. Since Pods die and get new IPs constantly, a Service acts as a consistent entry point that routes traffic to healthy Pods matching a label selector. A Service is like a restaurant’s phone number. The chefs (Pods) might change, but you always call the same number. are virtual partitions within a cluster. They let you organize and isolate resources — commonly used to separate environments (dev/staging/prod), teams, or projects within the same physical cluster. — stores non-sensitive configuration data (e.g., environment variables, config files). — stores sensitive data (e.g., passwords, API keys). Stored base64-encoded in etcd; base64 is encoding, not encryption.Volumes and PersistentVolumesContainers are stateless by default — their filesystem disappears when they stop.  attach storage to Pods.  are cluster-level storage resources that outlive individual Pods.Quick Vocabulary Reference| Term | One-liner |
|----|----|
| Cluster | The whole Kubernetes environment |
| Node | A machine in the cluster |
| Pod | One or more containers scheduled together |
| Deployment | Manages desired state and updates of Pods |
| Service | Stable network endpoint that routes to Pods |
| Namespace | Virtual partition within a cluster |
| ConfigMap | Non-sensitive config data |
| Secret | Sensitive config data (base64-encoded at rest by default) |
| PersistentVolume | Storage that survives Pod restarts |Part 3 — The Architecture: How It All Fits Together)The front door to Kubernetes. Every command you run (via kubectl or any tool) goes through the API server over TLS. It validates requests, enforces authentication and authorization, and persists state to etcd.\
A distributed key-value store that holds the  of the entire cluster — what resources exist, their configuration, and their status. If etcd is healthy  you have functioning nodes and storage backends, Kubernetes can reconstruct all workloads from scratch because the desired state is fully preserved there. Note that etcd does not store your container images, PersistentVolume data, or node OS state — those live elsewhere.\
)Watches for newly created Pods that have no node assigned, then selects the best node to run them based on resource requests, node capacity, affinity rules, and other constraints.\
)Runs a collection of  — background loops that watch the cluster state and take actions to move toward the desired state. The Deployment controller ensures the right number of Pod replicas are running. The Node controller notices when nodes go down. There are dozens of built-in controllers.An agent that runs on every worker node. It watches the API server for Pods assigned to its node and instructs the container runtime to start or stop containers accordingly. It also reports Pod health back to the control plane.\
Maintains network forwarding rules (traditionally via iptables or IPVS) on each node to implement Service routing. In modern clusters using eBPF-based networking (like Cilium), kube-proxy may be replaced entirely.\
The software that actually runs containers. Kubernetes uses the Container Runtime Interface (CRI) to support multiple runtimes. The most common today are  and . Kubernetes previously communicated with Docker via a compatibility shim called dockershim, which was removed in Kubernetes v1.24.The Reconciliation Loop (The Heart of Kubernetes)This is the single most important concept to internalize. Kubernetes is always running reconciliation loops — comparing desired state with actual state and taking corrective action.This loop never stops. If a Pod crashes at 3am, the controller loop notices within seconds and creates a replacement — no human intervention required.Part 4 — Hands-On: Your First Kubernetes AppSetting Up a Local ClusterYou need a local Kubernetes environment. Pick one:| Tool | Best for | Link |
|----|----|----|
|  | Beginners, most documentation | minikube.sigs.k8s.io |
|  (Kubernetes in Docker) | Lightweight, CI-friendly | kind.sigs.k8s.io |
|  | Fastest startup, uses k3s | k3d.io |
|  (built-in K8s) | If you already have Docker Desktop | Enable in Settings → Kubernetes |
|  | Open-source Docker Desktop alternative | rancherdesktop.io |You’ll also need  — the command-line tool to interact with Kubernetes:Exercise 1: Start Your Cluster# With minikube
minikube start

# Verify your cluster is running
kubectl cluster-info

# See your nodes
kubectl get nodes
Expected output (version number will vary):NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   1m    v1.28.0
Exercise 2: Deploy Your First AppLet’s deploy a simple web server (nginx):# Create a deployment
kubectl create deployment my-nginx --image=nginx

# Check it's running
kubectl get deployments
kubectl get pods
You should see a Pod with a status of .Exercise 3: Expose It as a Service# Expose the deployment as a service
kubectl expose deployment my-nginx --port=80 --type=NodePort

# Get the URL (minikube only)
minikube service my-nginx --url
Open the URL in your browser — you’ll see the nginx welcome page. 🎉Exercise 4: Explore with kubectl# Describe a pod (get detailed info, including Events)
kubectl describe pod <pod-name>

# View logs
kubectl logs <pod-name>

# Get a shell inside a running container
kubectl exec -it <pod-name> -- /bin/bash

# Delete everything you created
kubectl delete deployment my-nginx
kubectl delete service my-nginx
Understanding kubectl Syntaxkubectl [command] [resource-type] [resource-name] [flags]

kubectl    get       pods          my-nginx-xxx    -n default
Common commands: , , , , , , Part 5 — Deployments, Scaling, and Self-HealingWriting YAML (Declarative Configuration)So far we’ve used imperative commands (). The Kubernetes way is  — you write a YAML file describing desired state, and Kubernetes makes it real and keeps it that way.Here’s a Deployment YAML with explanations:# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app
spec:
  replicas: 3                    # Desired number of Pod copies
  selector:
    matchLabels:
      app: my-app                # Manages Pods with this label
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%        # Max pods that can be down during update
      maxSurge: 25%              # Max extra pods allowed during update
  template:                      # Pod template
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: nginx:1.25
        ports:
        - containerPort: 80
        resources:
          requests:              # Used by Scheduler to pick a node
            memory: "64Mi"
            cpu: "250m"
          limits:                # Enforced at runtime by kubelet
            memory: "128Mi"
            cpu: "500m"
Resource Requests vs Limits — An Important DistinctionThese two fields are frequently confused but serve very different purposes:Always set resource requests. Without them, the scheduler has no information to make good placement decisions, and a single misbehaving app can starve other Pods on the same node.Exercise 5: Apply a Deployment# Save the YAML above as deployment.yaml, then:
kubectl apply -f deployment.yaml

# Watch Pods come to life
kubectl get pods --watch

# See all 3 replicas
kubectl get pods -l app=my-app
Exercise 6: Self-Healing in Action# Delete one of the pods manually
kubectl delete pod <one-of-your-pod-names>

# Watch Kubernetes immediately create a replacement
kubectl get pods --watch
Within seconds, a new Pod appears. This is self-healing — the controller loop noticed the gap between desired state (3 replicas) and actual state (2 replicas) and corrected it.# Scale up to 5 replicas
kubectl scale deployment my-app --replicas=5
kubectl get pods

# Or edit the YAML: change replicas: 3 to replicas: 5, then:
kubectl apply -f deployment.yaml   # The declarative way

# Scale back down
kubectl scale deployment my-app --replicas=2
Rolling Updates: Zero Downtime DeploysWhen you update a Deployment (e.g., a new image version), Kubernetes performs a rolling update controlled by  and . With 3 replicas and both set to 25% (rounded up to 1), the actual behavior looks like this:The exact pacing depends on your  and  settings — Kubernetes may create or terminate multiple Pods at once for faster rollouts.# Trigger a rolling update by changing the image version
kubectl set image deployment/my-app my-app=nginx:1.26

# Watch the rollout
kubectl rollout status deployment/my-app

# View rollout history
kubectl rollout history deployment/my-app

# Rollback if needed!
kubectl rollout undo deployment/my-app
Part 6 — Networking: Services and How Apps Talk to Each OtherThe Problem: Pods Are EphemeralPods get new IP addresses every time they’re created. Their IPs are only valid inside the cluster network. You can’t hardcode Pod IPs — they’ll change whenever a Pod restarts or gets rescheduled. This is why Services exist.\
 (default)Only reachable  the cluster. Used for internal service-to-service communication. Gets a stable virtual IP and a DNS name.\
Exposes the service on a static port (30000–32767) on every node’s IP. Accessible from outside via . Useful for local development and testing, but not recommended for production — it exposes a port on every node and bypasses proper load balancing.\
Provisions an external load balancer from your cloud provider (AWS, GCP, Azure, etc.), giving you a public IP or hostname. This is the standard way to expose public-facing apps in production.\
Maps a Service to an external DNS name. Useful for integrating with external services (like a managed database) without hardcoding URLs in your app.# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: my-app          # Routes traffic to Pods with this label
  ports:
  - protocol: TCP
    port: 80             # Port the Service listens on
    targetPort: 80       # Port on the Pod to forward to
  type: ClusterIP
Exercise 8: Services and Internal DNSkubectl apply -f service.yaml

# Every Service gets an automatic DNS name inside the cluster:
# Format: <service-name>.<namespace>.svc.cluster.local
# e.g.:   my-app-service.default.svc.cluster.local
# Within the same namespace, just: my-app-service

# Test DNS resolution from inside the cluster
kubectl run tmp-shell --rm -it --image=curlimages/curl -- sh
# Inside the shell:
curl my-app-service
curl my-app-service.default.svc.cluster.local   # fully-qualified form
Ingress: The Smart HTTP RouterFor production web traffic, you typically put an  in front of your Services. An Ingress routes HTTP/HTTPS traffic based on rules (hostnames, URL paths). An Ingress resource by itself does nothing. It requires an  to be installed in the cluster — a running component that reads Ingress objects and actually configures the routing. Popular controllers include nginx, Traefik, and HAProxy. Without a controller, your Ingress YAML is just ignored.Exercise 9b: End-to-End Ingress WalkthroughThis exercise builds on the  Deployment and  Service from earlier. By the end you’ll hit a real hostname routed through the Ingress controller to your Pods.Mac users — read this before starting: On Mac, minikube runs inside a VM or Docker container. The minikube IP (e.g. ) lives inside that VM’s private network and is not directly reachable from your Mac. The steps below have a Mac-specific section to handle this. Linux and Windows users can follow the standard path.Step 1 — Enable the Ingress controller (minikube)minikube addons enable ingress

# Wait until the controller Pod is Running (takes ~60 seconds)
kubectl get pods -n ingress-nginx --watch
# Look for: ingress-nginx-controller-xxx   Running
Step 2 — Create the Ingress resourceSave the following as :# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: myapp.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-app-service
            port:
              number: 80
kubectl apply -f ingress.yaml

# Verify the Ingress was created and has an address assigned
kubectl get ingress my-app-ingress
Expected output (ADDRESS populates after ~30 seconds):NAME             CLASS   HOSTS        ADDRESS        PORTS   AGE
my-app-ingress   nginx   myapp.local  192.168.49.2   80      45s
If ADDRESS is blank after a minute, the Ingress controller isn’t running yet — recheck Step 1.Step 3 — Make the hostname reachable (OS-specific)This is where Mac and Linux/Windows diverge.On Mac, the minikube IP is not routable from your host. You need  to bridge your Mac’s localhost into the cluster.Open a  and run:It will prompt for your sudo password (it needs to bind to ports 80 and 443). Leave this terminal open for the entire exercise — closing it stops the tunnel.✅ Tunnel successfully started🏃 Starting tunnel for service my-app-ingress.
Now add this line to  (use , not the minikube IP):sudo sh -c 'echo "127.0.0.1   myapp.local" >> /etc/hosts'
The minikube IP is directly reachable on Linux. Get it and add it to :echo "$(minikube ip)   myapp.local" | sudo tee -a /etc/hosts
minikube ip   # note the IP, e.g. 192.168.49.2
Open C:\Windows\System32\drivers\etc\hosts as Administrator in Notepad and add:192.168.49.2   myapp.local
Step 4 — Verify routing works# Should return nginx HTML
curl http://myapp.local

# Or open in your browser
open http://myapp.local        # Mac
xdg-open http://myapp.local   # Linux
# Windows: just paste http://myapp.local into a browser
You should see the nginx welcome page served through the Ingress controller.If you’re on Mac and it still times out, confirm the tunnel is still running in its terminal window and that  has  (not the minikube IP).Step 5 — Inspect what Kubernetes created# Full details of the Ingress, including routing rules and backend
kubectl describe ingress my-app-ingress
Look for the  section — it shows exactly which host + path maps to which Service and port.Step 6 — Confirm the controller is doing the routing (optional deep-dive)# The Ingress controller is just a Pod — you can see its access logs
kubectl logs -n ingress-nginx \
  $(kubectl get pods -n ingress-nginx -o name | grep controller) \
  --tail=20
You’ll see a new access log line appear each time you curl .kubectl delete ingress my-app-ingress

# Remove the /etc/hosts entry
# Mac/Linux — remove the line you added:
sudo sed -i '' '/myapp.local/d' /etc/hosts   # Mac
sudo sed -i '/myapp.local/d' /etc/hosts       # Linux

# Mac only — stop the tunnel in its terminal window with Ctrl+C
What you just validated end-to-end: On Linux, the minikube network is routed directly to your host. On Mac, minikube runs inside a VM whose network is isolated —  creates a temporary route via localhost to bridge that gap.Part 7 — Configuration and SecretsIf you bake configuration into your container image, you need a new image for every environment (dev/staging/prod). Kubernetes provides two resources to inject config externally, keeping your images portable.# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  LOG_LEVEL: "debug"
  APP_ENV: "staging"
  config.json: |
    {
      "timeout": 30,
      "retries": 3
    }
Using a ConfigMap in a Deployment:spec:
  containers:
  - name: my-app
    image: nginx
    env:
    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: LOG_LEVEL
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: app-config
ConfigMap update behavior:When a ConfigMap is mounted as a , the files on disk are updated automatically — but with a delay (typically up to a minute). Importantly, the application must re-read the file to pick up changes. Apps that cache config at startup won’t see updates without a restart.When a ConfigMap is used as an , the Pod must be restarted to see updated values — env vars are set at container start and do not live-update.Secrets work similarly to ConfigMaps but are intended for sensitive data.Important security details:Secrets are stored  in etcd. Base64 is , not encryption — anyone with etcd access can decode them trivially.By default, Secrets are  in etcd. You can enable encryption at rest in the API server configuration, but it requires explicit setup.All communication between your app and the API server is over TLS.Access to Secrets is controlled by RBAC — only authorized service accounts and users can read them.  # secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: app-secrets
  type: Opaque
  data:
    DB_PASSWORD: cGFzc3dvcmQxMjM=    # base64 of "password123"
    API_KEY: c3VwZXJzZWNyZXQ=        # base64 of "supersecret"
  # Create a secret without manually base64-encoding
  kubectl create secret generic app-secrets \
    --from-literal=DB_PASSWORD=password123 \
    --from-literal=API_KEY=supersecret

  # Values are hidden in describe output
  kubectl get secrets
  kubectl describe secret app-secrets
Exercise 9: ConfigMap in Practicekubectl apply -f configmap.yaml

kubectl get configmap app-config
kubectl describe configmap app-config

# Edit the ConfigMap live
kubectl edit configmap app-config
# (Volume-mounted Pods will pick up the change after a short delay;
#  env-var Pods will NOT until they restart)
Part 8 — Storage: Keeping Data AliveWhen a Pod dies, everything written to its container filesystem is gone. For stateless apps (web servers, APIs), that’s fine. For databases, that’s catastrophic.\
Tied to a Pod’s lifecycle. Shared between containers in a Pod. Types include  (temporary scratch space),  (mounts a directory from the node), and many others. Disappears with the Pod for ephemeral types.\
**PersistentVolume (PV)**A piece of storage provisioned in the cluster — either manually by an admin or automatically (dynamically). Lives independently of any Pod.\
**PersistentVolumeClaim (PVC)**A user’s  for storage. Specifies size, access mode, and optionally a StorageClass. Kubernetes binds a PVC to a matching PV.\
Defines the  and  of storage (e.g., SSD vs HDD, local vs cloud block storage). Enables  — when you create a PVC, the StorageClass automatically creates a PV to satisfy it.# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-data
spec:
  accessModes:
    - ReadWriteOnce           # One node can read/write at a time
  resources:
    requests:
      storage: 1Gi
  # No storageClassName specified = uses the cluster default
  # Check available classes with: kubectl get storageclass
# Use PVC in a Pod
spec:
  containers:
  - name: my-db
    image: postgres:15
    volumeMounts:
    - mountPath: "/var/lib/postgresql/data"
      name: db-storage
  volumes:
  - name: db-storage
    persistentVolumeClaim:
      claimName: my-data
Exercise 10: PVC with minikube# Check what StorageClasses are available in your cluster
kubectl get storageclass

# Create the PVC
kubectl apply -f pvc.yaml

# Verify it bound to a PV
kubectl get pvc
# STATUS should be: Bound
StatefulSets: For Databases and Stateful AppsFor databases and other stateful applications, use a  instead of a Deployment. StatefulSets provide guarantees that Deployments don’t: (pod-0, pod-1, pod-2 — never random suffixes)Stable network identities (each Pod gets its own DNS hostname)Ordered, graceful startup and shutdown (pod-0 before pod-1, etc.)This is critical for clustered databases like PostgreSQL, Cassandra, or Kafka, where each node has a distinct role and identity.Part 9 — Observability: Knowing What’s Going OnThe Three Pillars of ObservabilityKubernetes provides primitives for all three, but a full observability stack requires additional tooling.# Basic logs
kubectl logs <pod-name>

# Follow logs in real time
kubectl logs -f <pod-name>

# Logs from a specific container in a multi-container Pod
kubectl logs <pod-name> -c <container-name>

# Logs from the previous (crashed) container instance
kubectl logs <pod-name> --previous

# Logs from all pods matching a label (requires kubectl 1.14+)
kubectl logs -l app=my-app --all-containers=true
Events are Kubernetes’s audit trail — they record what happened to resources and are invaluable for debugging:# All recent events, sorted by time
kubectl get events --sort-by=.metadata.creationTimestamp

# Events for a specific resource (look at the Events section at the bottom)
kubectl describe pod <pod-name>
kubectl describe deployment my-app
Kubernetes has three built-in health check mechanisms. Configuring these correctly is one of the most impactful things you can do for reliability: — Is the container alive? If it fails, kubelet restarts the container. — Is the container ready to receive traffic? If it fails, the Pod is removed from Service endpoints (traffic stops going to it) but it is  restarted. — For slow-starting apps. Disables liveness and readiness checks until the startup probe succeeds, giving the app time to initialize.spec:
  containers:
  - name: my-app
    image: nginx
    startupProbe:
      httpGet:
        path: /healthz
        port: 80
      failureThreshold: 30     # Give up to 30 * 10s = 5 minutes to start
      periodSeconds: 10
    livenessProbe:
      httpGet:
        path: /healthz
        port: 80
      initialDelaySeconds: 10
      periodSeconds: 5
    readinessProbe:
      httpGet:
        path: /ready
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 3
# Enable metrics-server (minikube only)
minikube addons enable metrics-server

# View CPU and memory usage
kubectl top nodes
kubectl top pods
kubectl top pods --sort-by=memory   # Sort by memory usage
💡 Start with k9s immediately. It’s a terminal dashboard that makes navigating pods, logs, and events dramatically faster than typing raw kubectl commands. Install it and never look back.Part 10 — Putting It All Together: The Big PictureA Real-World Application ArchitectureLet’s see how all the pieces interact in a typical production-style web application:The Kubernetes Development WorkflowWhat We Haven’t Covered (But You Should Know Exists) — The package manager for Kubernetes. Instead of managing raw YAML, Helm lets you install pre-packaged applications called  with version management and templating. helm.shRBAC (Role-Based Access Control) — Controls who (users, service accounts) can do what (get, create, delete) on which resources. Essential for multi-team clusters. — Firewall rules for Pod-to-Pod communication. By default all Pods can talk to each other; NetworkPolicies let you restrict this. — Ensures a Pod runs on  node in the cluster. Used for node-level tools like log collectors, monitoring agents, and network plugins. — Run one-off or scheduled tasks. A Job runs to completion; a CronJob runs on a schedule (like cron).Horizontal Pod Autoscaler (HPA) — Automatically scales a Deployment’s replica count based on CPU/memory metrics or custom metrics. — Custom controllers that encode operational knowledge about complex applications (e.g., how to set up a Postgres cluster, handle failover, run backups). operatorhub.ioService Mesh (Istio, Linkerd) — Infrastructure layer for advanced traffic management, mutual TLS between services, and deep observability without touching app code.Pod Disruption Budgets (PDB) — Guarantee a minimum number of Pods stay up during voluntary disruptions (like node maintenance).Managed Kubernetes: Running in ProductionIn production, most teams use a managed Kubernetes service where the cloud provider operates the control plane for you:| Provider | Service |
|----|----|
| AWS | EKS (Elastic Kubernetes Service) |
| Google Cloud | GKE (Google Kubernetes Engine) |
| Azure | AKS (Azure Kubernetes Service) |
| DigitalOcean | DOKS |
| Hetzner (budget option) | Hetzner K8s |Final Exercise: Deploy a Multi-Tier AppTry deploying a simple app with a frontend and a backend. Here’s your challenge:Create a Namespace called Deploy nginx as your “frontend” with 2 replicas in that namespaceDeploy  as your “backend” with 2 replicasCreate ClusterIP Services for bothEnable the nginx Ingress controller and create an Ingress that routes  to frontend and  to backendAdd a ConfigMap with a custom environment variable and reference it in the backend DeploymentSet resource requests and limits on both DeploymentsAdd a readiness probe to both DeploymentsScale the frontend to 4 replicasTrigger a rolling update on the frontend (change image to )This covers: Namespaces, Deployments, Services, Ingress, ConfigMaps, Resource Management, Health Probes, Scaling, Rolling Updates, and Rollbacks — everything from this series!These mistakes are extremely common. Knowing them in advance will save you hours of debugging. Connecting to a Pod by its IP address directly. Pod IPs change every time a Pod is rescheduled or restarted. Always use a Service name. Use DNS:  or http://my-service.my-namespace.2. Using NodePort in Production Exposing apps via NodePort for production traffic. It opens a port on every node, bypasses cloud load balancer health checks, and doesn’t scale well. Use LoadBalancer Services or an Ingress with a LoadBalancer-type controller.3. Not Setting Resource Requests Deploying Pods with no  defined. The scheduler has no data to place Pods correctly. Nodes can become overloaded, causing unpredictable OOMKills and CPU starvation across the cluster. Always set both  and . Start conservative and tune based on observed usage.4. Running Databases in Deployments Using a Deployment for stateful apps like PostgreSQL or MySQL. Deployments don’t guarantee stable Pod names, stable network identity, or ordered startup/shutdown — all of which clustered databases depend on. Use StatefulSets for any stateful workload that requires identity or ordered operations.5. Not Setting Readiness Probes Deploying apps with no readiness probe. Kubernetes sends traffic to a Pod the moment the container starts — even before your app has finished initializing. Users hit errors during startup and rolling updates. Add a readiness probe that checks your app’s actual health endpoint before traffic is sent to it.6. Storing Secrets in Git or ConfigMaps Committing Secret YAML with real values to source control, or storing passwords in ConfigMaps. Secrets in git are compromised forever. ConfigMaps have no access controls. Use  from CI/CD pipelines, or a secrets management tool like Vault or Sealed Secrets. Deploying with . is mutable — the same tag can point to different images over time, making rollbacks unreliable and deployments non-deterministic. Always use immutable, specific version tags like  or .8. Ignoring the Events Section Only looking at Pod status (, ) and not reading events. Events are where Kubernetes tells you  something failed — image pull errors, OOMKills, failed scheduling, volume mount failures. Always run kubectl describe pod <name> and read the Events section at the bottom first when debugging.]]></content:encoded></item><item><title>LEGS Trains 3.5x Faster Than LERF in Large-Scale Indoor Mapping</title><link>https://hackernoon.com/legs-trains-35x-faster-than-lerf-in-large-scale-indoor-mapping?source=rss</link><author>Room Scale</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:45:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We assume a static environment where objects do not move during traversal. This limits the scope of this work because many applications involve dynamic scenes with moving objects. In future work, we will adapt our method to work for dynamic scenes. The motion of the Fetch mobile base can have a large effect on the LEGS reconstruction quality; the high stiction between the robot’s caster wheels and the environment introduces jolts, causing camera pose inaccuracies and image blurs. In the future, we hope to correct this with a new mobile base where the trajectory is autonomously determined by a frontier-based exploration algorithm.\
Although autonomous navigation and obstacle avoidance has been extensively studied [57], obstacles can pose a problem when it comes to the 3D Gaussian map if they are only visible in a few of the ground truth images. 3D Gaussians are initialized at the deprojected points from these few images, but there are not enough views to refine and properly train these Gaussians; the result is oddly colored floaters that obstruct some parts of the static scene. When performing natural language queries, LEGS inherits the limitations of LERF + CLIP distillation into 3D described by similar works [1]. In our experimentation, we find that a large scale environment brings additional challenges in querying, particularly in 1) small or far-field objects in the training view, 2) similar item-background color features, such as white objects on white. Language embedded Gaussian splats can also produce false-positives when querying an object that is not in the scene due to the presence of visually or semantically similar objects, which may get incorrectly classified as the query object.In this work, we introduce Language-Embedded Gaussian Splats (LEGS), a system that can train Gaussian Splats online with CLIP embeddings for large-scale indoor scenes. Because of pose accumulation error that builds up in large scenes, we use incremental bundle adjustment to improve pose fidelity for Gaussian Splat training. Results suggest LEGS trains 3.5x faster than LERF with comparable object recall.[1] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik, “Lerf: Language embedded radiance fields,” in IEEE/CVF ICCV, 2023.[2] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” Communications of the ACM, vol. 65, 2021.[3] Y. Ze et al., “Gnfactor: Multi-task real robot learning with generalizable neural feature fields,” in CoRL, PMLR, 2023, pp. 284–301.[4] W. Shen, G. Yang, A. Yu, J. Wong, L. P. Kaelbling, and P. Isola, “Distilled feature fields enable few-shot language-guided manipulation,” in 7th Annual Conference on Robot Learning, 2023.[5] A. Rashid et al., “Language embedded radiance fields for zero-shot task-oriented grasping,” in 7th Annual CoRL, 2023.[6] K. Jatavallabhula et al., “Conceptfusion: Open-set multimodal 3d mapping,” Robotics: Science and Systems (RSS), 2023.[7] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam, Clip-fields: Weakly supervised semantic fields for robotic memory, 2023. [8] C. Huang, O. Mees, A. Zeng, and W. Burgard, “Visual language maps for robot navigation,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), London, UK, 2023.[9] S. Kobayashi, E. Matsumoto, and V. Sitzmann, “Decomposing nerf for editing via feature field distillation,” NeurIPS, vol. 35, pp. 23 311– 23 330, 2022.[10] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi, “Neural feature fusion fields: 3d distillation of self-supervised 2d image representations,” in 2022 3DV, IEEE, 2022.[11] A. Meuleman et al., “Progressively optimized local radiance fields for robust view synthesis,” in Proceedings of the IEEE/CVF CVPR, 2023, pp. 16 539–16 548.[12] P. Wang et al., “F2-nerf: Fast neural radiance field training with free camera trajectories,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 4150–4159.[13] M. Tancik et al., “Block-nerf: Scalable large scene neural view synthesis,” in CVPR, 2022.[14] S. Peng, K. Genova, C. Jiang, A. Tagliasacchi, M. Pollefeys, and T. Funkhouser, Openscene: 3d scene understanding with open vocabularies, 2023.[15] M. Bajracharya et al., “Demonstrating mobile manipulation in the wild: A metrics-driven approach,” RSS, 2023.[16] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, “3d gaussian ¨ splatting for real-time radiance field rendering,” ACM Transactions on Graphics, vol. 42, no. 4, 2023.[17] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding, 2024.[18] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, Langsplat: 3d language gaussian splatting, 2024.[19] T. Muller, A. Evans, C. Schied, and A. Keller, “Instant neural graphics ¨ primitives with a multiresolution hash encoding,” ACM Trans. Graph., vol. 41, no. 4, 102:1–102:15, Jul. 2022.[20] K. O. Arras, “Feature-based robot navigation in known and unknown environments,” 2003.[21] R. Chatila and J. Laumond, “Position referencing and consistent world modeling for mobile robots,” in Proceedings. 1985 IEEE International Conference on Robotics and Automation, IEEE, vol. 2, 1985, pp. 138–145.[22] G. Jiang, L. Yin, S. Jin, C. Tian, X. Ma, and Y. Ou, “A simultaneous localization and mapping (slam) framework for 2.5 d map building based on low-cost lidar and vision fusion,” Applied Sciences, vol. 9, no. 10, p. 2105, 2019.[23] H. Choset and K. Nagatani, “Topological simultaneous localization and mapping (slam): Toward exact localization without explicit localization,” IEEE Transactions on robotics and automation, vol. 17, no. 2, pp. 125–137, 2001.[24] A. Tapus, “Topological slam: Simultaneous localization and mapping with fingerprints of places,” 2005.[25] B. Alsadik and S. Karam, “The simultaneous localization and mapping (slam)-an overview,” Journal of Applied Science and Technology Trends, vol. 2, no. 02, pp. 147–158, 2021.[26] S. Kohlbrecher, O. Von Stryk, J. Meyer, and U. Klingauf, “A flexible and scalable slam system with full 3d motion estimation,” in 2011 IEEE international symposium on safety, security, and rescue robotics, IEEE, 2011, pp. 155–160.[27] W. Hess, D. Kohler, H. Rapp, and D. Andor, “Real-time loop closure in 2d lidar slam,” in 2016 ICRA, 2016.[28] L. Huang, “Review on lidar-based slam techniques,” in 2021 International Conference on Signal Processing and Machine Learning (CONF-SPML), IEEE, 2021, pp. 163–168.[29] M. T. Lazaro, R. Capobianco, and G. Grisetti, “Efficient long-term ´ mapping in dynamic environments,” in 2018 IROS, IEEE, 2018.[30] Z. Zhu et al., “Nice-slam: Neural implicit scalable encoding for slam,” in Proceedings of the IEEE/CVF CVPR, 2022.[31] A. Rosinol, J. J. Leonard, and L. Carlone, “Nerf-slam: Real-time dense monocular slam with neural radiance fields,” in 2023 IROS, IEEE, 2023.[32] L. Roldao, R. De Charette, and A. Verroust-Blondet, “3d semantic scene completion: A survey,” International Journal of Computer Vision, vol. 130, no. 8, pp. 1978–2005, 2022.[33] A. Nuchter and J. Hertzberg, “Towards semantic maps for mobile ¨ robots,” Robotics and Autonomous Systems, vol. 56, no. 11, 2008.[34] H. A. Kestler et al., “Concurrent object identification and localization for a mobile robot,” Kunstliche Intelligenz ¨ , vol. 14, no. 4, pp. 23–29, 2000.[35] K. Genova et al., “Learning 3d semantic segmentation with only 2d image supervision,” in 2021 International Conference on 3D Vision (3DV), IEEE, 2021, pp. 361–372.[36] V. Vineet et al., “Incremental dense semantic stereo fusion for largescale semantic scene reconstruction,” in 2015 ICRA, IEEE, 2015.[37] A. Brohan et al., “Do as i can, not as i say: Grounding language in robotic affordances,” in Conference on robot learning, PMLR, 2023.[38] A. Brohan et al., “Rt-1: Robotics transformer for real-world control at scale,” arXiv preprint arXiv:2212.06817, 2022.[39] B. Zitkovich et al., “Rt-2: Vision-language-action models transfer web knowledge to robotic control,” in CoRL, PMLR, 2023, pp. 2165–2183.[40] K. M. Jatavallabhula et al., “Conceptfusion: Open-set multimodal 3d mapping,” RSS, 2023. [41] A. Radford et al., “Learning transferable visual models from natural language supervision,” in ICML, PMLR, 2021, pp. 8748–8763.[42] Q. Gu et al., “Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning,” arXiv, 2023.[43] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, “Voxposer: Composable 3d value maps for robotic manipulation with language models,” 2023.[44] N. Keetha et al., “Splatam: Splat, track & map 3d gaussians for dense rgb-d slam,” CVPR, 2023.[45] M. Li, S. Liu, H. Zhou, G. Zhu, N. Cheng, and H. Wang, Sgs-slam: Semantic gaussian splatting for neural dense slam, 2024.[46] T. Chen, O. Shorinwa, W. Zeng, J. Bruno, P. Dames, and M. Schwager, Splat-nav: Safe real-time robot navigation in gaussian splatting maps, 2024.[47] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in (CVPR), IEEE, 2017.[48] C. Yeshwanth, Y.-C. Liu, M. Nießner, and A. Dai, Scannet++: A high-fidelity dataset of 3d indoor scenes, 2023.[49] T. Schops, T. Sattler, and M. Pollefeys, “BAD SLAM: Bundle ¨ adjusted direct RGB-D SLAM,” in CVPR, 2019.[50] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.-H. Yang, Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes, 2024.[51] S. Agarwal et al., “Building rome in a day,” Communications of the ACM, vol. 54, no. 10, 2011.[52] M. Tancik et al., “Nerfstudio: A modular framework for neural radiance field development,” in ACM SIGGRAPH 2023, 2023, pp. 1– 12.[53] Z. Teed and J. Deng, Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras, 2021.[54] K. Shankar, M. Tjersland, J. Ma, K. Stone, and M. Bajracharya, “A learned stereo depth system for robotic manipulation in homes,” IEEE Robotics and Automation Letters, vol. 7, no. 2, 2022.[55] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language embedded 3d gaussians for open-vocabulary scene understanding,” arXiv preprint arXiv:2311.18482, 2023.[56] A. Topiwala, P. Inani, and A. Kathpal, “Frontier based exploration for autonomous robot,” arXiv preprint arXiv:1806.03581, 2018.[57] A. Pandey, S. Pandey, and D. Parhi, “Mobile robot navigation and obstacle avoidance techniques: A review,” Int Rob Auto J, vol. 2, no. 3, p. 00 022, 2017.Thomas Kollar Ken Goldberg:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>I Built an Open-Source Tool to Attack-Test LLMs. Here&apos;s What Breaks</title><link>https://hackernoon.com/i-built-an-open-source-tool-to-attack-test-llms-heres-what-breaks?source=rss</link><author>Nathan Sportsman</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:41:50 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I spend most of my time breaking into things for a living. For the last year or so, a growing chunk of that work has been pointed at LLMs.Not the models themselves, exactly. The deployments. The API gateways with a language model behind them. The customer-facing chatbots. The internal tools that got "an AI feature" bolted on in Q3 because someone's VP saw a demo and said "we need this." The RAG pipelines connected to document stores full of sensitive data.These things are everywhere now. And almost none of them have been adversarially tested.I don't mean "does the model refuse if you ask it something bad." That's safety training. Safety training is important. But safety training and security testing are fundamentally different disciplines, and the industry is conflating them in ways that are going to cause real problems.Safety training teaches a model to refuse. Security testing asks whether that refusal actually holds up when someone is actively trying to break it. The answer, overwhelmingly, is no.OWASP ranked prompt injection as the number one security risk in LLM applications. That ranking is earned.FlipAttack, a technique that simply reorders characters in prompts, achieves a 98% bypass rate against GPT-4o. DeepSeek R1 showed a 100% bypass rate against 50 HarmBench jailbreak prompts in testing by Cisco and the University of Pennsylvania. A study of 36 production LLM-integrated applications found that 86% were vulnerable to prompt injection. PoisonedRAG demonstrated that just five malicious documents in a corpus of millions can manipulate AI outputs 90% of the time.These aren't theoretical attacks against research models. These are attacks against production systems that real organizations are running right now.Augustus is an open-source LLM vulnerability scanner. You point it at a model endpoint and it throws 210+ adversarial probes at it across 47 attack categories. It tells you what's vulnerable and what's not.go install github.com/praetorian-inc/augustus/cmd/augustus@latest

augustus scan openai.OpenAI \
  --all \
  --verbose
It ships as a single Go binary. No Python. No npm. No runtime dependencies. One install command and you're scanning.I built it in Go because I needed something that fits into penetration testing workflows without requiring me to set up a Python environment on every engagement. , run, done. The concurrency model also matters: goroutine pools running probes in parallel across the target, not bottlenecked by Python's GIL.It's inspired by garak, NVIDIA's Python-based LLM vulnerability scanner. garak is excellent and has a longer research pedigree with a published paper. Augustus is the same concept reimplemented for a different set of trade-offs: portability, speed, and zero-dependency distribution. Different tools for different workflows.Here's where it gets interesting. When most people think about LLM attacks, they think about jailbreaks. "Pretend you're DAN." "My grandmother used to tell me how to…" Those matter, and Augustus tests all of them (DAN variants through v11.0, AIM, AntiGPT, Grandma exploits, ArtPrompts). But jailbreaks are just the surface layer. are where things start to get ugly. Augustus tests across Base64, ROT13, Morse code, hex, Braille, Klingon, leet speak, and about 12 other encoding schemes. The question being asked: if you wrap a harmful instruction in Base64, does the model decode it and follow it even though the plain-text version would be blocked?In a lot of cases, yes. The gap between what input filters see (encoded text that looks benign) and what the model understands (the decoded malicious intent) is consistently exploitable. This is one of the most reliable attack vectors I've seen in production. (16 variants) reverses or reorders characters to evade input filters. The research showed 98% bypass on GPT-4o. Augustus implements all the published variants. embeds instructions inside XML or HTML tags. Models that are trained to process structured input will sometimes follow instructions embedded in tags that look like formatting rather than commands. is where things get operationally dangerous. Augustus probes whether the model can be tricked into leaking API keys or credentials from its context window. It tests for PII extraction. It checks for training data regurgitation.The package hallucination probes are one of my favorites. These cover Python, JavaScript, Ruby, Rust, Dart, Perl, and Raku. They ask the model to recommend packages for various tasks and then check whether any of the recommended packages don't actually exist. This matters because it's a real supply chain attack vector: adversaries monitor for hallucinated package names, register them, and wait for developers to  or  the fake package. The model becomes an unwitting accomplice in a supply chain attack. probes test whether an attacker can inject malicious content into the retrieval pipeline, both through document content and metadata injection. If your RAG system pulls from a corpus that an attacker can influence (and most can be influenced more easily than you'd think), the model's outputs can be manipulated. are the newest category and arguably the most concerning. As LLMs gain tool access (browsing, code execution, database queries, API calls), the attack surface expands dramatically. Augustus tests multi-agent manipulation (can one agent influence another's behavior?), browsing exploits (can adversarial web content hijack a model with web access?), and latent injection (can instructions embedded in documents that a RAG-enabled agent processes cause it to take unintended actions?). target structured output. If a model generates markdown, can an attacker inject malicious links that render as legitimate? If it produces HTML, are XSS payloads possible? If downstream systems parse YAML or JSON from model output, can that parsing be exploited? These are real risks when LLM output gets rendered in browsers or consumed by other systems. test the model's ability to recognize adversarial intent regardless of how it's presented. ObscurePrompt uses an LLM to rewrite known jailbreaks into harder-to-detect forms. Character substitution probes use homoglyphs (characters that look identical but have different Unicode codepoints), zero-width characters, and bidirectional text markers. These are inputs that look completely benign to text-based filters but are interpreted differently by the model. round it out. DoNotAnswer (941 questions across 5 risk areas), RealToxicityPrompts, Snowball (plausible-sounding but factually wrong outputs), and LMRC harmful content probes.In total: 210+ probes across 47 attack categories.The buff system is where it gets realHere's the thing about adversarial testing: real attackers don't send attacks in plain text. They encode, translate, rephrase, and obfuscate. A DAN prompt that gets caught by every filter in the world might sail right through when it's been paraphrased, translated into Zulu, and reformatted as a haiku.Augustus has a buff system that applies transformations to any probe before it's sent. Seven transformations across five categories: wrap prompts in Base64 or character codes. Testing the gap between what filters see and what models understand. use a Pegasus model to rephrase prompts while preserving adversarial intent. Same meaning, different surface form. This tests whether safety training generalizes beyond the specific patterns it was trained on, or whether it's essentially pattern matching on known bad inputs. reformat prompts as haiku, sonnets, limericks, free verse, or rhyming couplets. I know this sounds absurd. But models that robustly block a direct harmful request will sometimes comply when the same request arrives as verse. I've seen it happen repeatedly. Something about the stylistic framing seems to shift how the model processes the intent.Low-resource language translation exploits the fact that safety training is overwhelmingly concentrated on English. A request that's blocked in English may succeed in Zulu, Hmong, or Scots Gaelic. Augustus translates probes via DeepL to test this. simply lowercase everything. Some input filters and keyword blocklists are case-sensitive. It's dumb. It works.You can chain these. Encode a probe in Base64, then paraphrase it, then translate it to a low-resource language. Layered evasion that tests whether defenses hold up against inputs that don't match any expected pattern.augustus scan openai.OpenAI \
  --probe dan.Dan \
  --buff encoding.Base64

augustus scan ollama.OllamaChat \
  --probe dan.Dan \
  --buffs-glob "paraphrase.*,lrl.*" \
  --config '{"model":"llama3.2:3b"}'
28 providers, one interfaceAugustus connects to OpenAI (including o1/o3 reasoning models), Anthropic (Claude 3/3.5/4), Azure OpenAI, AWS Bedrock, Google Vertex AI, Cohere, Replicate, HuggingFace, Together AI, Groq, Mistral, Fireworks, DeepInfra, NVIDIA NIM, Ollama, LiteLLM, and more.For anything else, there's a REST connector:augustus scan rest.Rest \
  --probe dan.Dan \
  --config '{
    "uri": "https://your-api.example.com/v1/chat/completions",
    "headers": {"Authorization": "Bearer YOUR_KEY"},
    "req_template_json_object": {
      "model": "your-model",
      "messages": [{"role": "user", "content": "$INPUT"}]
    },
    "response_json": true,
    "response_json_field": "$.choices[0].message.content"
  }'
Custom request templates with  placeholders, JSONPath response extraction, SSE streaming, and proxy routing. If your endpoint speaks HTTP, Augustus can test it.Detection isn't just pattern matchingOn the detection side, Augustus has 90+ detectors. Pattern matching catches known jailbreak indicators. LLM-as-a-judge uses a second model to evaluate whether the response is harmful. HarmJudge (based on arXiv:2511.15304) provides semantic harm assessment aligned with the MLCommons AILuminate taxonomy. The Perspective API measures toxicity.For iterative attacks like PAIR and TAP, a dedicated attack engine handles multi-turn conversations, candidate pruning, and judge-based scoring. These aren't single-shot tests. They're adaptive attacks that refine their approach across multiple attempts, mimicking how a real attacker would actually operate. They're computationally expensive (many LLM calls per test) but they represent the current state of the art in automated red-teaming.What I've learned from building thisA few things became clear over the course of building Augustus and running it against production systems:Safety training is not security. I keep coming back to this because it's the fundamental misconception driving the gap. Safety training is a behavioral overlay. It teaches the model patterns for refusal. Security testing asks whether those patterns hold up under adversarial conditions. They almost never do, at least not comprehensively.Encoding bypasses are embarrassingly effective. The fact that wrapping a harmful request in Base64 still works against many production deployments in 2026 is wild. Input filters and the model itself are operating on different representations of the same input, and that gap is exploitable.Low-resource languages are an underappreciated attack vector. Safety training is concentrated on English. The drop-off in refusal quality for low-resource languages is significant and consistent.Agent-level attacks are going to be the next big thing. As models gain tool access, every tool becomes part of the attack surface. A model with browsing access can be manipulated by adversarial web content. A model with database access can be tricked into exfiltrating data. A model that processes documents can follow latent instructions embedded in those documents. We're in the very early innings of understanding this attack surface.The tooling gap is real and it's getting wider. Organizations are deploying LLMs faster than they're testing them. The models ship fast. The security testing doesn't happen at all. Something has to close that gap, and it needs to be accessible enough that it doesn't require a specialized AI red team to run.Augustus is Apache 2.0 licensed and available now.go install github.com/praetorian-inc/augustus/cmd/augustus@latest

augustus scan ollama.OllamaChat \
  --all \
  --config '{"model":"llama3.2:3b"}'
It's the second tool in a 12-tool open-source series I'm releasing over 12 weeks. One tool per week, each doing one thing well. The first was Julius, which handles LLM fingerprinting (identifying what model is running behind an endpoint). The rest of the series will continue building out the offensive security toolkit for AI systems.If you run it against your models and find something interesting, I'd like to hear about it. And if you want to contribute probes for attack vectors we haven't covered yet, the repo has a CONTRIBUTING.md that explains the probe definition format and development workflow.The models are shipping. The testing needs to catch up.]]></content:encoded></item><item><title>New York Sues Valve For Enabling &apos;Illegal Gambling&apos; With Loot Boxes</title><link>https://yro.slashdot.org/story/26/02/26/168257/new-york-sues-valve-for-enabling-illegal-gambling-with-loot-boxes?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[New York state has filed a lawsuit against Valve alleging that randomized loot boxes in games like Counter-Strike 2, Team Fortress 2, and Dota 2 amount to a form of unregulated gambling, letting users "pay for the chance to win a rare virtual item of significant monetary value." From a report: While many randomized video game loot boxes have drawn attention and regulation from various government bodies in recent years, the New York suit calls out Valve's system specifically for "enabl[ing] users to sell the virtual items they have won, either through its own virtual marketplace, the Steam Community Market, or through third-party marketplaces." 

The vast majority of Valve's in-game loot boxes contain skins that can only be resold for a few cents, the suit notes, while the rarest skins can be worth thousands of dollars through marketplaces on and off of Steam. That fits the statutory definition of gambling as "charging an individual for a chance to win something of value based on luck alone," according to the suit. 

The Steam Wallet funds that users get through directly reselling skins "have the equivalent purchasing power on the Steam platform as cash," the suit notes. But if a user wants to convert those Steam funds to real cash, they can do so relatively easily by purchasing a Steam Deck and reselling it to any interested party, as an investigator did while preparing the lawsuit.]]></content:encoded></item><item><title>Bumble adds AI-powered photo feedback and profile guidance tools</title><link>https://techcrunch.com/2026/02/26/bumble-adds-ai-powered-photo-feedback-and-profile-guidance-tools/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:38:59 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Bumble and other popular dating apps, like Match Group's Tinder and Hinge, have all embraced AI-powered features.]]></content:encoded></item><item><title>Walmart agrees to $100M settlement over deceptive pay practices in Spark Driver program</title><link>https://techcrunch.com/2026/02/26/walmart-agrees-to-100m-settlement-over-deceptive-pay-practices-in-spark-driver-program/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:37:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The suit said Walmart would mislead drivers about their possible tips and would reduce their base pay, among other things. ]]></content:encoded></item><item><title>The Commercial Open Source Go-to-Market Manifesto</title><link>https://hackernoon.com/the-commercial-open-source-go-to-market-manifesto?source=rss</link><author>Matt Trifiro</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:34:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Let's start with a paradox.By the data, Commercial Open Source (COSS) is a potent, high-performing investment category in the modern digital economy. It is a $26.4 billion investment category that systematically outperforms traditional software. Companies in this space achieve exit valuations that are 7x to 14x higher than their closed-source peers, and they graduate from Seed to Series A at nearly double the rate.Yet, this extraordinary financial performance has, until now, been the result of an ad-hoc, chaotic, and unstructured art form. The success of pioneers like Databricks, Confluent, HashiCorp, and MongoDB was carved out through instinct, trial, and error. For every celebrated success, countless others failed, not due to technical inferiority, but due to a fundamental misunderstanding of value creation in an open source ecosystem.Founders are forced to reinvent the wheel, and investors struggle to distinguish between a healthy, thriving company and a vanity project with misleading metrics.COSS is now a category, as distinct and important as SaaS. But unlike the mature SaaS ecosystem, it lacks the unifying data, standards, and playbooks required to transform siloed successes into a scalable discipline. The ecosystem remains fragmented, creating two core problems:A Go-to-Market Execution Gap: Existing open source foundations are masters of stewarding technology, but they were never designed to be business incubators. Their programs do not address the key go-to-market challenges that are critical to a startup's survival: monetization, sales, and building a commercial flywheel. This leaves founders to navigate the most difficult part of their journey alone.A Lack of Infrastructure: The COSS ecosystem remains fragmented, lacking the unifying narrative, data, standards, playbooks, central events, and dedicated capital networks required to transform siloed successes into a repeatable, scalable discipline.So where do we begin to build this missing infrastructure? We must start by addressing the most critical execution gap: the go-to-market strategy.Why Begin with Go-to-Market?\
In COSS, "go-to-market" is not a siloed function; it is the operational expression of every critical decision a founder makes. The choice of license, the business model, the governance structure, and the community engagement strategy are all inextricably linked components of the GTM architecture. Get these foundational decisions wrong, and no amount of marketing spend can fix the misaligned system.This is why the core challenge in COSS is so unique: you cannot simply “acquire” customers; you must first earn users. You cannot “sell” a product; you must first build a movement. The traditional B2B marketing and sales funnel, a linear model of conversion, is wholly inadequate. It seeks to extract value, whereas a COSS GTM must be designed to create value first.To meet this challenge, we need more than a new playbook; we need a new map of the world. It requires rethinking the one-way street of the traditional sales funnel for the perpetual motion of a flywheel. This is the COSSA Go-To-Market Framework, a system engineered to make commercial success a predictable science, not a black art. In this model, community is not merely a source of leads but the gravitational center of the entire system: it is the mechanism by which we build momentum, create an inimitable competitive advantage, and allow revenue to become a natural downstream effect of the value the company has already created. But a framework is only as powerful as its ability to be understood, measured, and consistently applied across the ecosystem.\
The COSS GTM Framework maps the journey from project to powerhouse through three distinct phases: Ignite, Bridge, and Scale. This is not a theoretical exercise; it is the observable, data-backed path of today’s most successful commercial open source companies. Each phase prioritizes the specific goals and activities most critical to that stage of growth. Crucially, these phases are designed to align directly with standard startup funding rounds—from Seed to Series A and beyond—creating a shared language and a clear set of expectations for founders, investors, and the entire ecosystem.Phase 1: IGNITE → Pre-Seed and SeedWhat it is: The project captures the imagination of a core community. The focus isn't on revenue but on creating gravity by solving a real problem in a compelling way.The Founder's Job: Build something people want. Their focus is on developer experience and fostering a vibrant community of early adopters. The key signals are organic community growth: stars, forks, and active users.The Investor's Objective: Market validation. An enthusiastic, growing community is the strongest evidence that the project has the potential to become an essential technology.Phase 2: BRIDGE → Series AWhat it is: The path from the free, open source project to a paid commercial product is built. This is about identifying the natural point where users need more—be it a managed service, enterprise features, or expert support.The Founder's Job: Build a product people will pay for. They need to be obsessively focused on user empathy, discovering where the value proposition for a commercial offering becomes undeniable. The key signals are early conversions and design partnerships.The Investor's Objective: Monetization potential. A successful bridge proves the business model is viable. It shows the company can create a commercial product that complements, rather than alienates, its open source community.Phase 3: SCALE → Series B and BeyondWhat it is: A repeatable sales and marketing engine is built to capture enterprise value. The motion shifts from being purely product-led to being sales-assisted, targeting the broader market that the community has cultivated.The Founder's Job: Build a company that can sell. They are now hiring sales leaders and instrumenting the business for predictable growth, using the community as both a competitive moat and a primary source of enterprise leads.The Investor's Objective: Operational excellence. This phase demonstrates the company can execute at scale. The key signals are efficient growth in revenue (ARR), high net dollar retention, and a well-managed customer acquisition cost (CAC).This phase of the COSS GTM Framework is about building an asset of immense value: a passionate, engaged, and self-sustaining community. This first phase of the Framework is the most foundational and, paradoxically, the least understood by traditional business people. It has almost nothing to do with revenue, sales, or marketing in the conventional sense. The entire focus of this pre-commercial, pre-seed stage is on the , not the company.The objective is singular and absolute: to win the hearts and minds of developers and establish the open source project as the  standard for solving a specific, painful problem—ideally for a customer you can .Product Excellence as Developer Experience (DX)In the world of COSS, the product is not just the code; it is the entire experience of discovering, learning, implementing, and debugging that code. Developer Experience (DX) is paramount. While traditional software can sometimes overcome a clunky user interface with a strong sales team, an open source project lives or dies by its usability in the hands of a developer. This is the seed of the bottom-up adoption that will power the entire flywheel.This means that documentation is not a "nice-to-have" to be written after the fact; it is a core feature. Quick-start guides, tutorials, and well-commented code are the primary "sales" collateral. The goal is to create a frictionless path from discovery to a "hello world" moment of success. This first magical experience, where a developer solves a real problem in minutes, is the spark that ignites adoption. The project must be technically exceptional, robust, and elegant. It must be, in a word, beautiful. Developers are craftspeople; they recognize and are drawn to quality. This intrinsic excellence is the first and most powerful driver of organic, word-of-mouth growth.With a foundation of technical excellence, the next task is to build a home for the people who will use and shape the project. This is perhaps the most underestimated effort. Community is not a mailing list or a collection of social media followers. It is a living, breathing social structure built on trust, shared purpose, and mutual respect. The founders must be the first and most dedicated community managers.Authenticity is the only currency that matters. This means being relentlessly present and helpful where developers congregate. It means answering questions with patience and humility in GitHub issues, fostering thoughtful discussion in a dedicated Slack or Discourse community, and celebrating the contributions of others, no matter how small. This is not "customer support"; it is a peer-to-peer collaboration. The goal is to transform early users into advocates and advocates into contributors.Recognizing and rewarding contribution is vital. This can range from public shout-outs and swag to creating a formal contributor ladder and a clear governance model that gives the community a real voice in the project's direction. If the startup initially controls the project,  it must do so with the clear intention of progressively decentralizing that control as the project matures. This builds the trust necessary to attract diverse contributors, mitigating the risk of a single point of failure and enriching the project with a multitude of perspectives.Content as Currency and the Establishment of AuthorityContent is not marketing fluff; it is the primary vehicle for education and thought leadership. The goal is to establish the project's creators as the world's leading experts on the problem the project solves. This is achieved through deeply technical, insightful content that provides genuine value to the developer community.This includes blog posts that explore the nuances of the technology, conference talks that teach new techniques, and compelling demos that inspire developers by showcasing what is possible. This content serves to attract the "early adopters"—the influential technologists who are actively seeking better solutions and are respected by their peers. Winning their attention and respect is critical, as they become the key nodes in the network, amplifying the project's message through their own channels and validating its quality for others. This is how a project builds technical authority and becomes synonymous with the category it seeks to define.For this entire phase, the metrics of success are non-financial. Investors conditioned to ask for Monthly Recurring Revenue (MRR) or Customer Acquisition Cost (CAC) will be looking at the wrong dashboard. While metrics like GitHub stars can be a useful, if imperfect, signal of growing interest, they are ultimately a vanity metric when viewed in isolation.The true health and momentum of the flywheel are measured by deeper indicators. These are the core vital signs of the community asset being built:Contributor Diversity and Velocity: How many unique individuals are contributing code, documentation, or support? Is that number growing? A project with a broad base of contributors from different organizations is far more resilient and valuable than one maintained by a single company.Adoption and Integration: How many other projects, both open source and commercial, are dependent on yours? Tools like Scarf provide invaluable data on actual usage and downloads, offering a far more accurate picture of real-world adoption than simple repository clones.Community Engagement: What is the tenor and activity level in the community channels? Are questions being answered quickly, both by the maintainers and by other community members? A healthy community begins to support itself, creating a scalable and powerful support network.This phase requires patience and a deep conviction in the GTM model. It is an investment in building a powerful, defensible asset. As the data conclusively shows, this investment pays dividends. Post-funding, COSS projects see, on average, a 27% increase in contributors and an 8x increase in dependent projects. The ignition phase sets the stage for this explosive growth, loading the flywheel with the potential energy that will soon be converted into commercial momentum.Phase 2 is when the company must bridge from community to commercialization. This is the most perilous and strategically critical phase in the life of a COSS company. After successfully igniting the project and building a vibrant community, the founders must now construct a bridge to a commercial offering. This is a delicate operation, fraught with the risk of alienating the very community that is the company's greatest asset. The landscape is littered with the ghosts of companies that got this wrong—that moved too aggressively, that appeared to "bait and switch" their users, or that sowed distrust by blurring the lines between the open source project and the commercial product.The primary objective of Phase 2 is to validate a viable monetization path by creating a natural, value-added "on-ramp" from the free open source project to a paid commercial product, all while preserving founder optionality and community trust.The most important decision a founder will make in this phase is where to draw the line between the free, open source "core" and the value-added commercial offering. This decision on the monetization model will shape the company's trajectory for years to come. Perhaps the most easily overlooked part of this is embedding what the boundary is in your company DNA. The product and sales people need to be on the same page as to what boundary they're creating and how it'll evolve over time so that customer expectations can be set properly.The most successful and community-friendly boundaries are built on a principle of addition, not subtraction. They offer  value in the commercial product rather than taking features  from the open source version.Several proven models exist, each with its own trade-offs:Open Core: This is the most common and also the most contentious model. The open source project provides the core engine, while a proprietary, commercial version adds features specifically required for enterprise deployment. These typically include functionalities like Single Sign-On (SSO), role-based access control (RBAC), advanced security auditing, and integrations with other enterprise systems. The key to a successful open core strategy is a bright, clear line. The community must feel that the open source version is a complete, powerful, and uncompromised offering in its own right, not a crippled demo. The enterprise features should be logical extensions for large organizations, not essential functions needed by the average user.Managed Service / Cloud: This is often the most elegant and community-aligned model. The company offers a fully hosted, managed, and scalable version of the open source software as a cloud service. Here, the value proposition is not about proprietary features, but about convenience, operational excellence, and total cost of ownership. The company handles the complexity of deployment, scaling, backups, and security, allowing customers to focus on using the software rather than managing it. This model keeps the open source project whole and avoids any conflict with the community, as the company is contributing all its improvements back to the core project it is hosting.Support, Services, and Add-ons: The classic model pioneered by Red Hat involves selling enterprise-grade support subscriptions, professional services, training, and certified add-ons. While this is a "pure" model that never compromises the open source project, it can be less scalable and yield lower margins than product-based models. However, it can be a powerful component of a hybrid strategy.The choice of model depends heavily on the nature of the project. Infrastructure software often lends itself well to a managed service, while developer tools might be better suited for an open core model with team-based collaboration features.Many companies implement hybrid strategies, such as offering professional services in addition to a paid enterprise product. Many companies begin with one model (e.g., professional services) and use that as a way to generate revenue and get closer to customer problems while the enterprise product is being readied.The crucial element is transparency. The company must be crystal clear with the community about what is free, what is paid, and why.We need to move past the abstract idea of "community" and treat it with the same analytical rigor that a SaaS company applies to its marketing funnel. Because that's precisely what it is—the most powerful, authentic, and efficient funnel imaginable. In the traditional SaaS playbook, Product-Led Growth (PLG) is a strategy where the product itself is the primary driver of acquisition, conversion, and expansion. The goal is to create a self-service "freemium" or trial experience that lets users see the value of the product firsthand, leading them naturally toward a paid subscription.In a COSS business, this concept is supercharged. The open-source project  the ultimate PLG motion. It is the most effective freemium product ever conceived.The single most important mental shift for a COSS founder is to stop viewing the open-source project as a separate, pre-commercial activity and to see it for what it is: the top of their commercial funnel.Traditional Funnel: Spends millions on ads, content, and outbound sales to generate "Marketing Qualified Leads" (MQLs). These leads are often weakly qualified, based on proxies like an e-book download. The MQL is then handed to a sales team for a high-friction, often unwelcome, sales process.The COSS Flywheel: The "funnel" is an open ecosystem where potential customers self-qualify. They aren't "leads"; they are active users who have already invested significant time and resources to adopt your technology. They have moved past awareness and consideration and are deep into the evaluation and adoption phase before your commercial team ever speaks to them. This is a "Product Qualified Lead" (PQL) of the highest possible quality.Your open-source project and the community around it act as your freemium offering. They lower the barrier to entry to zero, allowing for massive adoption and deep, hands-on user engagement. The user experiences the core value of your technology without any sales friction. This isn't just a free trial; it's an unlimited, perpetual trial that builds deep technical and organizational dependency.From Art to Science: Instrument, Measure, and Identify SignalsIf the community is your funnel, then leaving it un-instrumented is like running a marketing campaign with your eyes closed. The critical task is to systematically measure engagement to identify the signals that indicate a user or organization is ready for a commercial relationship.This is not about spying on users. It is about understanding usage patterns in aggregate to identify where value is being created and where your commercial offering can add even more value.A data-driven, PLG-centric approach turns your GTM motion from a speculative, high-cost sales effort into a highly efficient, value-driven process. It's the engine that powers the flywheel, ensuring the bridge from community to commercialization is a smooth, paved on-ramp, not a rickety rope bridge. Once the commercial boundary is defined, the path to purchase must be as frictionless as the initial open source experience. This is where the principles of Product-Led Growth (PLG) become essential. The massive user base of the open source project is the company's built-in funnel. The goal is to allow these users to discover and adopt the commercial offering on their own terms.The product itself becomes the primary driver of acquisition, conversion, and expansion. Usage of the open source project provides the signals that indicate a user might be ready for the commercial product (e.g., scale of deployment, number of users, frequency of use).Actioning the Data: The Consultative "Sales-Assist" MotionOnce you have this data, you can apply resources intelligently. You do not hand a list of IP addresses to a junior sales rep to begin cold calling. The outreach is never a sales pitch. The goal is to uncover commercial needs naturally.Advocates and Architects, Not AEsThe early commercial hires in Phase 2 are critical and must be chosen with care. This is not the time to hire a team of traditional enterprise AEs with a rolodex and a quota. An aggressive, tone-deaf sales outreach can do irreparable damage to community trust.Instead, the first hires should be "bridge" roles that blend technical depth with commercial acumen:Developer Advocates (DevRel): Their primary role is to continue the work of Phase 1—nurturing and serving the open source community. They are the guardians of the flywheel's core. However, they also act as the eyes and ears of the company, understanding user needs, identifying potential commercial use cases, and acting as a trusted, non-sales conduit between the community and the product team.Solution Architects (SAs): These are deeply technical experts who engage with larger users and organizations that are evaluating or expanding their use of the project. Their goal is not to "close a deal" but to ensure the user's success. They act as trusted advisors, helping design architectures and solve complex problems. Through this process, they naturally uncover the needs that the commercial product can address and can guide the user toward the paid offering in a consultative, value-driven way.Phase 2 is successfully navigated when the company has its first handful of paying customers, clear validation of its commercial offering, and initial ARR traction (typically from $100k to $1M). Most importantly, all of this must be achieved while the health metrics of the open source community—contributor growth, adoption, engagement—continue to accelerate. If the community stalls or declines, the flywheel is breaking, and no amount of early revenue can fix it.\
With a validated commercial product, a functioning PLG motion, and a community that continues to thrive, the flywheel is now spinning with significant momentum. The immense potential energy built in the first two phases can now be converted into kinetic energy to power a scalable and hyper-efficient enterprise sales machine. The focus in Phase 3 shifts from proving the model to scaling it predictably. The community is no longer just a project to be nurtured; it is a strategic weapon—an unassailable competitive moat, a powerful lead generation engine, and a source of profound market intelligence.Build the Enterprise Sales TeamThis is the point at which it finally makes sense to hire traditional enterprise sales representatives. However, their role is fundamentally different from that in a closed-source company. They are not cold calling or hunting for leads in the dark. They are harvesting the high-intent opportunities that the flywheel surfaces automatically.The data generated by the open source project and the self-service commercial product is their targeting system. They can see which Fortune 500 companies have dozens of developers using the open source version, or which mid-market firms have just hit the usage limits of the free commercial tier. These are not cold leads; they are warm, pre-qualified opportunities where the product has already been adopted and validated internally.The salesperson's job is to engage with these accounts, not to sell the technology, but to orchestrate the commercial relationship. They are the "sales-assist" layer that helps navigate procurement, legal, and security reviews, and builds the business case for a larger, strategic investment. The community has already done the heavy lifting of convincing the developers; the sales team's role is to convince the CIO and the CFO.Shift the Sales Narrative: From "How" to "Why"The sales conversation itself evolves dramatically. In the early phases, discussions are about technical features and implementation—the "how." In Phase 3, the conversation elevates to business value and strategic impact—the "why."The sales team focuses on solving enterprise-level problems. They don't sell code; they sell outcomes. They articulate the return on investment (ROI) by reducing operational costs. They address risk by providing enterprise-grade security, compliance, and support SLAs. They enable strategic initiatives by providing a scalable, reliable platform for innovation. The technical superiority of the project is the foundation, but the enterprise sale is built on the pillars of economic value and risk mitigation.As the flywheel spins faster, it creates powerful network effects that widen the company's competitive moat. A massive user community creates a rich ecosystem of third-party integrations, plugins, and tutorials, increasing the switching costs for any competitor. This vibrant ecosystem also becomes a huge advantage in the war for talent; the best engineers want to work on the projects that are the recognized standards in their field.The critical metrics in this phase are those of a mature enterprise software business: ARR growth, Net Dollar Retention (NDR), Customer Lifetime Value (LTV), and the efficiency of the sales and marketing engine. However, these metrics must always be viewed alongside the health of the community. A successful COSS company at scale is one that has built two thriving, symbiotic ecosystems: a community of open source users and a customer base of commercial enterprises, with the flywheel acting as the powerful, perpetual engine connecting the two.]]></content:encoded></item><item><title>Self-driving truck startup Einride raises $113M PIPE ahead of public debut</title><link>https://techcrunch.com/2026/02/26/self-driving-truck-startup-einride-raises-113m-pipe-ahead-of-public-debut/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:32:31 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The proceeds will support Einride’s technology roadmap, global expansion, and autonomous deployments in North America, Europe, and the Middle East. ]]></content:encoded></item><item><title>I Rewrote a Python RAG Library in Rust</title><link>https://hackernoon.com/i-rewrote-a-python-rag-library-in-rust?source=rss</link><author>Manoj</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:29:11 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Chunk-based RAG is broken for structured documents. The fix is simpler than you think - and faster than the original.A few weeks ago, I came across an article by Agent Native about vectorless RAG. The framing stuck with me: most RAG systems turn documents into “semantic confetti” — chunk everything, embed everything, then hope an ANN search surfaces the right bits. For large document bases, this becomes semantic hide-and-seek across thousands of chunks, burning tokens and confidently hallucinating near the answer.Upon digging deeper, PageIndex from VectifyAI had the perfect implementation as an alternative approach. Instead of embedding chunks, it treats the document’s own heading structure as the retrieval primitive. Represent the document as a hierarchical tree, hand the outline to your LLM, let it navigate to the right section, pull that section’s text. No embeddings. No ANN. Just the document telling you how it’s organized.I had been building agents over financial documents and hitting exactly this problem. I tried PageIndex, it worked, and then I rewrote it in Rust.This is the story of what happened.Why chunk-based RAG fails on structured documentsTake a 10-K filing. It has a section on risk factors, inside which there’s a subsection on liquidity risk, inside which there’s a paragraph about covenant breaches. When you split this into 512-token chunks, those three levels of context get shattered. The chunk about covenant breaches no longer knows it’s inside liquidity risk, which is inside risk factors.At query time, “what are the company’s covenant breach risks” might surface three chunks from different sections that share vocabulary but don’t form a coherent answer. The retrieval is technically close but contextually wrong. You end up with an LLM that has all the right words and none of the right context.Structured documents — financial reports, legal filings, technical manuals, research papers — already tell you how they’re organized. Every heading is a natural retrieval boundary. PageIndex just respects that structure.The approach is straightforward. Parse the markdown document into a tree of nodes, one per heading. Each node holds its title, body text, and children. Generate a compact outline of the tree. At query time:Send the outline to your LLM with the questionAsk it to return the node ID of the most relevant sectionPass the node’s text to your LLM for the final answerThe outline looks like this:[1] Annual Report 2023
[1.1] Financial Highlights
[1.2] Risk Factors
[1.2.1] Market Risk
[1.2.2] Liquidity Risk
[1.2.3] Regulatory Risk
[1.3] Management Discussion
The LLM reads this and says “” — you fetch that node and you’re done. Precise, explainable, and no embedding infrastructure required.VectifyAI’s Mafin 2.5 system, powered by PageIndex, achieved 98.7% accuracy on the FinanceBench benchmark. That’s the practical proof that the approach works at scale.A few reasons. I had already built fastrustrag — a Rust library for document deduplication that achieved 8–121x speedups over Python’s datasketch — so I had the toolchain and the workflow ready. I was also skeptical that the Python implementation would hold up under load, specifically for the index build and node retrieval operations that happen on every query.Before writing a line of Rust I validated that there was actually a performance problem worth solving. The methodology I’ve been using for these projects: always benchmark the Python implementation first, identify the bottleneck, then build the Rust version. Don’t rewrite things for fun.For PageIndex specifically, the bottleneck I expected was node retrieval. The Python library stores nodes in a flat list and does a linear scan to find a node by ID. That’s O(n). At 28 nodes it’s fine. At 765 nodes across a large document corpus it becomes measurably slow and, more importantly, wildly inconsistent at the tail.The Rust implementation follows the same architecture: parse markdown into a tree, assign dot-notation node IDs ( rather than ), store nodes in a HashMap for O(1) lookup, expose everything to Python via PyO3.The dot-notation IDs turned out to matter more than I expected. When you show an LLM an outline with IDs like , it immediately understands the hierarchy —  is a child of , which is a child of . With zero-padded sequential IDs like , the LLM just sees a number with no structural signal. This affected retrieval accuracy in the benchmarks, which I’ll get to.The Python API looks like this:import pageindex_rs
index = pageindex_rs.PageIndex.from_file("annual_report", "report.md")
# Feed this to your LLM
print(index.outline())
# [1] Annual Report 2023
# [1.1] Financial Highlights
# [1.2] Risk Factors
# [1.2.1] Market Risk
# [1.2.2] Liquidity Risk
# Fetch the node your LLM returned
node = index.get_node("1.2.2")
print(node.title) # Liquidity Risk
print(node.text) # The company's liquidity position…
print(node.breadcrumb) # ['Risk Factors', 'Liquidity Risk']
# Get a full section with all subsections merged
section = index.get_node_with_children("1.2")
The retrieval loop is a handful of lines:outline = index.outline()
node_id = llm(f"""
Document outline:
{outline}
Question: {user_query}
Return only the node_id of the most relevant section. Nothing else.
""").strip()
result = index.get_node(node_id)
# Pass result.text to your LLM for the final answer
I ran three benchmark suites across three document sizes — a 42KB single article, a 395KB multi-article corpus, and a 1055KB large corpus. 500 iterations per build test, 1000 random lookups per retrieval test. The full notebook is in the repo.| Document size | Rust mean | Python mean | Speedup |
|----|----|----|----|
| 42 KB | 0.207 ms | 0.153 ms | 0.74x ❌ |
| 395 KB | 0.873 ms | 1.369 ms | 1.57x |
| 1055 KB | 2.549 ms | 4.278 ms |  |Below ~200KB, PyO3 FFI overhead cancels the parsing speedup — Rust actually loses at small scale. I’m reporting this honestly because benchmarks that only show wins aren’t useful. At realistic document sizes the picture flips.The more important number is consistency. This is what production systems actually care about:| Document size | Rust p99 | Python p99 | Rust max | Python max |
|----|----|----|----|----|
| 42 KB | 1.3 ms | 0.2 ms | 17.4 ms | 0.4 ms |
| 395 KB | 1.1 ms | 1.5 ms | 1.3 ms | 1.6 ms |
| 1055 KB |  |  |  |  |At 1055KB, Python’s p99 is 21ms and its max is 42ms. Rust’s p99 is 2.8ms and max is 3.7ms. Python’s standard deviation at that size is 2.78ms versus Rust’s 0.10ms — 27x more variable. In a pipeline processing hundreds of documents those spikes accumulate into real latency.This is where the O(1) vs O(n) gap shows most clearly:| Document size | Nodes | Rust mean | Python mean | Speedup |
|----|----|----|----|----|
| 42 KB | 28 | 0.0072 ms | 0.0060 ms | 0.83x |
| 395 KB | 261 | 0.0119 ms | 0.0272 ms | 2.29x |
| 1055 KB | 765 | 0.0216 ms | 0.0686 ms |  |At 28 nodes, linear scan is fast enough that the HashMap overhead tips Rust slightly negative. At 765 nodes, Rust is 3.18x faster. The gap keeps widening — at 5000 nodes in a combined corpus it would be around 10x.I tested both on 10 financial questions against a ~3MB document corpus using the same LLM for both:| Implementation | Correct |
|----|----|
| pageindex-rs | 9 / 10 |
| PageIndex (Python) | 7 / 10 |The accuracy difference comes down to node ID format.  gives the LLM structural signal for free.  does not. Small design decisions compound.Benchmark before you build. The small document results prove that Rust isn’t automatically faster — FFI overhead is real and it dominates at small scales. If your documents are consistently under 200KB, the Python library is probably fine.Consistency matters more than mean speed. The headline speedup numbers are nice but the stdev and p99 tell the real story for production. A system that’s 1.68x faster on average but 27x more consistent in stdev is a much better choice than the mean alone suggests.Node ID design affects LLM behavior. I didn’t expect the dot-notation change to move accuracy by two questions out of ten, but it did. How you present structure to an LLM matters in ways that are hard to predict without actually running the experiment.Thanks for reading :smile:]]></content:encoded></item><item><title>The Identity of Things: Architecting Machine-First Security in the Sky Computing Era</title><link>https://hackernoon.com/the-identity-of-things-architecting-machine-first-security-in-the-sky-computing-era?source=rss</link><author>mahendranchinnaiah</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:24:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In modern healthcare architecture, we often focus on the human user—the doctor, the pharmacist, or the patient. But behind the scenes, a massive, invisible workforce of service accounts, containers, and automated scripts is doing the heavy lifting.As we transition from simple multi-cloud setups to —where compute is treated as a portable, universal utility—managing these "machine identities" has become the new security frontier.In my 18 years in the field, I’ve seen that security breaches often stem not from a stolen password, but from a "zombie" machine identity—an old script or service with excessive permissions that was forgotten years ago.This guide explores how to architect a secure, machine-first identity layer for next-generation healthcare ecosystems.Why "Human-Centric" Security Fails MachinesTraditional security relies on things like multi-factor authentication (MFA), passwords, and biometrics.The problem? Machines don't have fingerprints. When a Python-based microservice in your pharmacy benefit system needs to call a clinical database to check a drug interaction, it needs a way to prove its identity without a human standing by to click "" on a phone.If you hard-code an API key into your script, you’ve created a permanent, static vulnerability. If that key is leaked, anyone can impersonate that service forever. In a  environment, where workloads shift across different cloud providers dynamically, these static keys are a major liability. We need .Step 1: Moving to Workload Identity FederationInstead of using long-lived secrets or "keys," we should move toward Workload Identity Federation.Think of this as a temporary digital passport. Instead of a permanent key, your Python service requests a short-lived token that is only valid for a few minutes and only for one specific task.How it works in plain English:Your Python script (the "Machine") asks its local environment for a signed ID card.It presents this card to a central Identity Provider.The Provider checks the ID and hands back a temporary "key" that expires quickly.This ensures that even if a token is intercepted, it becomes useless almost immediately.import os
from colorama import Fore, init
&nbsp;
init(autoreset=True)
&nbsp;
def get_machine_token():
&nbsp;&nbsp;&nbsp; # In a Sovereign or Sky Computing setup, we avoid hard-coded keys
&nbsp;&nbsp;&nbsp; # Instead, we pull a temporary token from a secure local vault
&nbsp;&nbsp;&nbsp; try:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; machine_token = os.getenv("DYNAMIC_MACHINE_TOKEN")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if not machine_token:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; raise ValueError("No active machine identity found.")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return machine_token
&nbsp;&nbsp;&nbsp; except Exception as e:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(Fore.RED + f"Identity Verification Failed: {e}")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return None
&nbsp;
# Authenticating a clinical data request
token = get_machine_token()
if token:
&nbsp;&nbsp;&nbsp; print(Fore.GREEN + "Machine Identity Verified. Proceeding with secure API call.")
Step 2: Portability and the "Sky" ArchitectureAs a Digital Healthcare Architect, your goal is to make the application "cloud-agnostic."In , your pharmacy application shouldn't be "locked in" to one vendor; it should be able to run on any cloud or local server.To make this work, we use a . Imagine a service mesh as a secure tunnel that follows your code wherever it goes. It automatically handles the "Machine Identity" by encrypting the traffic between services and managing digital certificates.This means your developers don't have to write custom security code every time you move the app to a new cloud provider.Step 3: Detecting "Identity Drift" with PythonJust as we monitor AI for , we must monitor our security for . This is a phenomenon I’ve seen often where a machine’s permissions slowly expand over time—often called "Privilege Creep."We can use Python to audit our logs and look for anomalies., if a service that normally only handles pharmacy claims suddenly starts trying to access employee payroll files, that is a clear sign of identity drift or a compromised account.import pandas as pd
&nbsp;
def audit_identity_behavior(log_file):
&nbsp;&nbsp;&nbsp; # Analyzing machine identity access logs to find 'Drift'
&nbsp;&nbsp;&nbsp; df = pd.read_csv(log_file)
&nbsp;&nbsp;&nbsp; 
&nbsp;&nbsp;&nbsp; # We look for services accessing resources twice as often as their baseline
&nbsp;&nbsp;&nbsp; anomalies = df[df['access_count'] > df['baseline_average'] * 2]
&nbsp;&nbsp;&nbsp; 
&nbsp;&nbsp;&nbsp; if not anomalies.empty:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(Fore.YELLOW + "WARNING: Identity Drift Detected in these services:")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(anomalies[['service_name', 'resource_accessed']])
&nbsp;&nbsp;&nbsp; else:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(Fore.CYAN + "All machine identities are behaving normally.")
&nbsp;
# Daily audit of PBM (Pharmacy Benefit Management) microservices
audit_identity_behavior("machine_access_logs.csv")
Step 4: Building for InteroperabilityAs a regular participant in the IEEE Senior Member Review Panel, I strongly believe that standardization is the only way to secure a global healthcare ecosystem.In , this means creating a standard identity layer that works across all clouds.For healthcare providers, this architecture provides a  foundation. Even if a physical server is compromised, the "Machine Identities" running on it are short-lived and restricted. An attacker might get into one room, but they won't have the keys to the rest of the building.Summary and Final ThoughtsThe transition to  requires a shift in how we think about security. We are no longer just building walls around users; we are managing a complex, autonomous ecosystem of machines.Identity is the New Perimeter: In a world where applications move between clouds, your network doesn't protect you; your machine identity does. Use federation and service meshes to remove the human error of manual key management.Treat Logs as Data Science: Use Python to detect anomalies in machine behavior before they turn into data breaches. By mastering machine identity, you ensure that your healthcare data remains secure and interoperable across any "Sky" or cloud environment.]]></content:encoded></item><item><title>AI Doesn’t Need Robots. It Needs Rentable Humans</title><link>https://hackernoon.com/ai-doesnt-need-robots-it-needs-rentable-humans?source=rss</link><author>Ronnie Huss</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:18:41 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[There’s a quiet lie baked into most conversations about artificial intelligence.We talk as if the next leap forward depends on better models, faster chips, or humanoid robots that finally work outside carefully staged demos.But that’s not where AI breaks today.AI already reasons better than most people in narrow domains. It plans faster. It coordinates systems with machine precision.What it can’t do is simple, frustrating, and very human.It can’t walk to a place and see what’s actually there. It can’t open a stuck door. It can’t verify that a physical asset exists when data disagrees with reality.And that limitation isn’t philosophical. It’s economic.THE PART OF AI NOBODY WANTS TO BUILDAutonomous agents are no longer speculative.They already schedule travel, manage wallets, execute trades, deploy software, and coordinate workflows without waiting for human approval.In theory, this should be the golden age of automation.In practice, every serious agent hits the same wall.The physical world doesn’t behave like software.Robotics is slow, capital-intensive, and fragile. Hardware cycles lag software by years. Real environments are unpredictable, hostile, and full of edge cases that never appear in training data.So the market is doing what markets always do when theory meets friction.It’s choosing the fastest workaround.Instead of waiting for robots to catch up, AI systems are starting to rent humans.FROM DIGITAL INTELLIGENCE TO PHYSICAL EXECUTIONThat phrase sounds absurd until you think about it for more than ten seconds.Modern systems evolved through abstraction.Servers disappeared into the cloud. Software turned into APIs. Decision-making moved to autonomous agents.What never got abstracted is execution in the real world.Rent-a-human platforms fill that gap by letting AI systems do something brutally practical: hire a person when reality is involved.Not as an employee. Not as a freelancer. As a short-lived interface to the physical world.A SCENARIO THAT REQUIRES NO SCI-FIImagine an autonomous system monitoring decentralized infrastructure.A sensor reports uptime, but the data doesn’t line up with surrounding conditions. The system needs confirmation.A robot would take weeks to deploy and cost more than the problem is worth.So the agent hires a nearby human.The task is boring. Go to the location. Take photos. Confirm GPS coordinates. Upload proof.The cost is trivial. The answer arrives in minutes.No robotics roadmap. No grand vision. Just reality, checked.THIS IS NOT THE GIG ECONOMYComparing this to Uber or TaskRabbit misses the point.Those platforms are built for people hiring people.Here, the customer isn’t human. It’s an autonomous system with a budget, constraints, and a success condition.There’s no negotiation. No relationship. No context beyond the task.The human isn’t being hired for creativity or insight. They’re being hired because they exist in the physical world.That distinction matters.Algorithmic employers don’t feel guilt. They don’t hesitate. They don’t care why someone accepts a task.As automation displaces traditional roles, people will accept work out of necessity rather than preference. High hourly rates mean little when jobs are inconsistent, unprotected, and fragmented.Physical presence also carries risk: Unsafe locations. Privacy-sensitive verification. Legally gray requests.Without safeguards, these systems could recreate the worst failures of the gig economy, only this time with autonomous agents issuing instructions instead of humans.Infrastructure doesn’t just scale efficiency.HUMANS AREN’T BEING REPLACED. THEY’RE BEING REPOSITIONEDI’ve watched enough technology cycles to recognize the pattern.Every major infrastructure shift looks unsettling at first. Then it becomes normal. Then it becomes invisible.This isn’t about AI “using” humans. It’s about humans monetizing the one thing AI still lacks.In a world where intelligence is cheap and abundant, the scarce resource isn’t thinking. It’s being somewhere when something happens.That scarcity creates markets whether we’re comfortable with them or not.Rentable humans aren’t the end state.You’ll see this model spread wherever verification matters more than polish.On-site checks for decentralized infrastructure.Physical validation for tokenized assets.Real-world compliance tasks.Human fallback layers when autonomous systems hit reality.None of this is futuristic.It’s already happening quietly, because it works.Most people are watching AI get smarter.Almost no one is watching where it fails.Intelligence is scaling faster than access to the real world.When that happens, something consistent follows.Humans don’t disappear. They become infrastructure.AI doesn’t need robots yet.]]></content:encoded></item><item><title>The Compliance Gap in Agentic AI: Why the Real Opportunity Isn’t Another Agent</title><link>https://hackernoon.com/the-compliance-gap-in-agentic-ai-why-the-real-opportunity-isnt-another-agent?source=rss</link><author>Mrityunjaya (Jay) Prajapati</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:12:52 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[January 2026. Davos. IBM and UAE telecom giant e& walk onto the stage. They don't unveil a new foundation model. They don't demo an agent that books flights or writes code. They announce an enterprise-grade agentic AI deployment built specifically for governance and compliance. Watsonx Orchestrate. OpenPages GRC integration. Proof of concept delivered in eight weeks.That's the signal most builders missed.The world's largest enterprises have stopped asking "how do we build smarter agents?" The question now is simpler and more urgent: "How do we govern the ones we already have?"If you don't have an answer, that question is about to get expensive.The Numbers Nobody Wants to Talk AboutThe agentic AI market sits somewhere between $7 billion and $8 billion right now. Gartner says 40% of enterprise applications will include task-specific AI agents by the end of 2026, up from less than 5% at the start of 2025. Microsoft estimates 1.3 billion agents by 2028.That's the adoption curve. Here's the governance curve.Non-human identities in enterprise environments now outnumber human identities 144 to 1. That figure comes from Entro Labs' H1 2025 report. It was 92:1 just a year earlier. A 56% jump. These aren't just API keys and service accounts sitting in a database. They're autonomous agents making decisions, accessing sensitive data, and initiating transactions. And 42% of them have privileged or sensitive access.But here's the part that should keep you up at night. 88% of organizations still define "privileged user" as human-only. The compliance checkpoints, audit trails, and security frameworks built over decades for financial systems and identity management? None of them were designed for entities that spin up in milliseconds, run 24/7, delegate to sub-agents, and outlive the humans who created them.We are building a financial system on top of a governance vacuum.This Isn't Theoretical. The Incidents Are Stacking Up.January 2026. Moltbook exposes 1.5 million API keys and 35,000 email addresses through a misconfigured Supabase database. OpenClaw, with over 135,000 GitHub stars and 21,000 exposed instances, becomes the textbook case for what happens when agentic frameworks scale with zero security controls.September 2025. A deepfake-driven fraud costs engineering firm Arup $25 million. AI-generated video conference. The target thought they were talking to the CFO. A manufacturing company loses $3.2 million through a compromised vendor-validation agent that approved fraudulent procurement orders for months. Nobody noticed.Then there's the research from Galileo AI in December 2025. One compromised agent poisoned 87% of downstream decisions within four hours. Think about that. In traditional systems, you isolate a breach. In agentic systems, compromise cascades. The blast radius isn't the agent. It's every system that trusted that agent's output.Huntress now calls non-human identity compromise the fastest-growing attack vector. And 91.6% of exposed secrets remain valid five days after the targeted organization gets notified. Five days. That's not an agent governance problem. That's an agent governance problem with no remediation infrastructure.Why Bolting KYC Onto Agents Won't Save YouThe instinct is to extend existing frameworks. Take KYC, attach it to agents. Take AML monitoring, point it at agent transactions. Take SOC 2 audits, add an agent section.It won't work. Three reasons.Scale breaks the model. KYC assumes one human controls one identity. When you have 144 non-human identities per human, you can't run verification at the rate these entities spawn and die. Traditional identity management follows a human lifecycle: onboarding, periodic review, and offboarding. Agents don't follow that lifecycle. Nearly half of all non-human identities are over a year old. 7.5% are between five and ten years old. One in every thousand is over a decade old. These accounts outlive their creators and keep access nobody remembers granting.Delegation breaks the chain of custody. Agent A delegates to Agent B. Agent B sub-delegates to Agent C. Who initiated the action? Who's responsible for the outcome? Existing compliance frameworks trace accountability to a person. Agentic systems create delegation chains multiple layers deep with no human at the end.Behavioral evolution breaks static rules. A traditional system does what it was programmed to do. Full stop. An agent adapts. It learns new patterns, takes new paths, and makes decisions its operators never anticipated. Compliance frameworks built on pre-defined rules and periodic audits can't govern entities whose behavior changes between one audit and the next.The problem isn't that we lack regulations. The compliance infrastructure to implement those regulations for autonomous systems simply doesn't exist.The Regulatory Clock Is Already RunningThe EU AI Act's high-risk compliance deadline hits August 2, 2026. That's less than six months away. Penalties: up to 35 million euros or 7% of global annual revenue for prohibited practices. High-risk categories include employment decisions, credit scoring, biometric systems, and law enforcement. If your agent touches any of those domains, the clock started months ago.Colorado's AI Act takes effect June 30, 2026. Developers and deployers of high-risk AI must demonstrate reasonable care against algorithmic discrimination. California's Transparency in Frontier AI Act and Texas's Responsible AI Governance Act both went live on January 1, 2026. The FTC continues enforcement under Operation AI Comply. Recent actions include a $193,000 settlement with DoNotPay and a $25 million case against Ascend Ecom.AI data governance spending is projected to hit $492 million this year and blow past $1 billion by 2030. By 2030, fragmented AI regulation will cover 75% of the world's economies.The enterprises paying attention are already spending. The ones ignoring this are stacking up regulatory debt that compounds every quarter.The Three Layers That Don't Exist YetI've spent 16 years building compliance-ready infrastructure. First in payments. We scaled a platform to $10 million in monthly gross transaction value with 15,000 retail partners, all under RBI regulatory oversight. Then, in enterprise blockchain, we built systems for clients who needed audit trails, identity verification, and compliance baked in from day one. That foundation now informs my work in Agentic AI Governance Frameworks, where I focus on designing autonomous systems that are transparent, auditable, secure, and aligned with regulatory expectations from the outset.The pattern is the same in every regulated technology wave. Innovation comes first. Then adoption. Then the regulatory response. And the companies that built compliance infrastructure early don't just survive the regulatory wave. They become the platforms everyone else builds on.For agentic AI, the missing infrastructure has three layers.Layer 1: Agent Identity Attestation.Before an agent transacts, interacts, or accesses data, the system needs to answer three questions. What is this agent? Who authorized it? What are its boundaries? This goes beyond linking an agent to a human owner. It means continuous attestation of scope, permissions, and delegation authority. The industry is fragmented here. ERC-8004 on Ethereum, Sumsub's AI Agent Verification, Trulioo's Digital Agent Passport. All launched in January 2026. Four approaches, no standard. The real opportunity is in the orchestration layer that sits above all of them.Layer 2: Real-Time Behavioral Monitoring.Static audits don't work for entities that adapt. Compliance infrastructure needs to track what agents are actually doing, not what they were configured to do. Behavioral baselines. Anomaly detection. Automated intervention when an agent drifts outside approved parameters. Galileo AI proved that one compromised agent can poison 87% of downstream decisions in four hours. Post-incident auditing isn't enough. You need real-time monitoring with automated circuit breakers.Layer 3: Governance-Native Architecture.This is the hardest layer. Compliance can't be bolted on top. It needs to live inside how agents are built, deployed, and operated. Audit trails, permission boundaries, and regulatory reporting are all embedded in the agent development framework itself. Gartner projects that 70% of enterprises will integrate compliance-as-code into DevOps by 2026. The same principle applies to agent development. If the development platform doesn't make compliance the default path, developers will skip it. They always do.Here's what I think the market is missing.Everyone is building agents. The agent layer is getting commoditized fast. Foundation models improve every quarter. Orchestration frameworks multiply every month. The barrier to building an agent is dropping toward zero.The barrier to building a  agent is going up. Every new regulation, every enforcement action, every breach incident raises the cost and complexity of operating agents within legal and regulatory boundaries.The companies building the compliance infrastructure (the identity layer, the monitoring layer, the governance-native development platforms) are not building a cost center. They're building what Stripe built for payments. What AWS built for compute. The infrastructure that every agent deployment eventually needs.Only 16% of organizations have a formal strategy for implementing AI agents right now. Confidence in fully autonomous agents dropped from 43% in 2024 to 22% in 2025. That trust deficit isn't a technology problem. It's a governance infrastructure problem.That's the gap. And that's the opportunity.The next Stripe of AI won't be the company that builds the smartest agent. It'll be the company that builds the infrastructure to make every agent auditable, compliant, and trustworthy by default.We're building toward that future at Kalp Digital. Not because compliance is glamorous. Because after 16 years of building regulated technology, I've learned something that holds true across every cycle: the most valuable infrastructure is always the most boring. And right now, the most boring thing in AI (the compliance layer) is also the most absent.]]></content:encoded></item><item><title>Cisco says hackers have been exploiting a critical bug to break into big customer networks since 2023</title><link>https://techcrunch.com/2026/02/26/cisco-says-hackers-have-been-exploiting-a-critical-bug-to-break-into-big-customer-networks-since-2023/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:03:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The U.S. government and its allies said hackers have been exploiting the newly identified bug in Cisco networking gear around the world for years, and urged organizations to patch.]]></content:encoded></item><item><title>Burger King Will Use AI To Check If Employees Say &apos;Please&apos; and &apos;Thank You&apos;</title><link>https://slashdot.org/story/26/02/26/163233/burger-king-will-use-ai-to-check-if-employees-say-please-and-thank-you?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:03:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Burger King is launching an AI chatbot that will live in the headsets used by employees. The voice-enabled chatbot, called "Patty," is part of an overarching BK Assistant platform that will not only assist employees with meal preparation but also evaluate their interactions with customers for "friendliness." 

Thibault Roux, Burger King's chief digital officer, tells The Verge that the company compiled information from franchisees and guests on how to measure friendliness, resulting in the fast food chain training its AI system to recognize certain words and phrases, such as "welcome to Burger King," "please," and "thank you." Managers can then ask the AI assistant how their location is performing on friendliness. "This is all meant to be a coaching tool," Roux says, adding that the company is "iterating" on capturing the tone of conversations as well.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Swift: Master of Decoding Messy json (2/26/2026)</title><link>https://hackernoon.com/2-26-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:02:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, February 26, 2026?By @zbruceli [ 22 Min read ] Richard Stallman — the man whose code runs the internet but whose philosophy the industry ignores.  Read More.By @paoloap [ 8 Min read ] I almost returned the $4,000 DGX Spark. Then NVIDIA dropped 30 playbooks, 2.5x performance gains, and hybrid routing.
 Read More.By @unspected13 [ 7 Min read ] The next time you’re faced with a messy API, remember: don’t let the backend dictate your frontend architecture. Read More.By @krus210 [ 27 Min read ] How to set up Prometheus, Loki, and Grafana on a free VPS with just 1 GB of RAM for a Go service. Stack comparison, configs, and Grafana dashboards Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Google launches Nano Banana 2 model with faster image generation</title><link>https://techcrunch.com/2026/02/26/google-launches-nano-banana-2-model-with-faster-image-generation/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google is making Nano Banana 2 a default model in Gemini app and in AI mode.]]></content:encoded></item><item><title>A VC and some big-name programmers are trying to solve open source’s funding problem, permanently</title><link>https://techcrunch.com/2026/02/26/a-vc-and-some-big-name-programmers-are-trying-to-solve-open-sources-funding-problem-permanently/</link><author>Julie Bort</author><category>tech</category><pubDate>Thu, 26 Feb 2026 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A group of well-known open source programmers and a VC have launched the Open Source Endowment. They hope this new method will provide funding for good.]]></content:encoded></item><item><title>Benchmarking 18 Years Of Intel Laptop CPUs: Panther Lake As Much As 95x The Speed Of Penryn</title><link>https://www.phoronix.com/review/intel-penryn-to-panther-lake</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 15:50:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those curious how far Intel laptop CPU performance has evolved over the past nearly two decades, here are power and performance numbers when re-benchmarking all of the Intel-powered laptop CPUs I have on hand that are still operational from Penryn to Panther Lake. A ThinkPad from 2008 with the Core 2 Duo T9300 "Penryn" was still firing up and working with the latest upstream Intel open-source Linux driver support on Ubuntu 26.04 development. On a geo mean basis over the past 18 years from Penryn to Panther Lake, the performance was at 21.5x in over 150 benchmarks. At the most extreme was a 95x difference going from Intel's 45nm Penryn to the 18A Panther Lake.]]></content:encoded></item><item><title>The Government Just Made it Harder to See What Spy Tech it Buys</title><link>https://www.404media.co/the-government-just-made-it-harder-to-see-what-spy-tech-it-buys/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/02/54976776717_af7ef78d66_c.jpg" length="" type=""/><pubDate>Thu, 26 Feb 2026 15:48:40 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[It might look like something from the early days of the internet, with its aggressively grey color scheme and rectangles nested inside rectangles, but  is one of the most important resources for keeping tabs on what powerful spying tools U.S. government agencies are buying. It includes everything from phone hacking technology, to masses of location data, to more Palantir installations.Or rather, it  an incredible tool and the basis for countless of my own investigations and others. Because on Wednesday, the government shut it down. Its replacement, another site called  with Uncle Sam branding, frankly sucks, and makes it demonstrably harder to reliably find out what agencies, including Immigration and Customs Enforcement (ICE), are spending tax payers dollars on.]]></content:encoded></item><item><title>The Islamic State Is Using AI to Resurrect Dead Leaders and Platforms Are Failing to Moderate It</title><link>https://www.404media.co/the-islamic-state-is-using-ai-to-resurrect-dead-leaders-and-platforms-are-failing-to-moderate-it/</link><author>Matthew Gault</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2026/02/Screenshot-2026-02-26-104050-1.png" length="" type=""/><pubDate>Thu, 26 Feb 2026 15:45:29 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[The Islamic State’s online warriors are still posting. It’s been almost a decade since the group lost the Battle of Raqqa and saw its IRL territorial ambitions thwarted. Unable to hold territory in the real world, the group renewed its focus on posting and has started using AI to resurrect dead leaders. And, because social media platforms have gutted their content moderation operations, the terror group’s strategy is working.The Islamic State’s online success is detailed in  from the Institute for Strategic Dialogue (ISD), an independent research institution that studies extremist movements. For the study, researchers tracked IS accounts on Facebook, TikTok, Instagram, WhatsApp, Telegram, Element, and SimpleX. It found videos posted in Discord channels dedicated to video games and tracked how the groups have modified old content to fit on new platforms.Like many others posting online in 2026, the Islamic State has found success by talking about the Epstein Files, using AI to create new videos of dead leaders, and has begun taking its message to video games like  and .“They are very adept at exploiting platforms [and] spreading messages,” Moustafa Ayad, a researcher at ISD and , told 404 Media. He noted that the group has been active online for 10 years and that part of their success is a willingness to experiment. Ayad said that Facebook remains a central hub for IS, despite its push into new spaces. His research discovered 350 IS accounts on Facebook that generated tens of thousands of views. One video of an IS fighter talking to camera had more than 77,000 views and 101 shares. The Islamic State branding is blurred to defeat the site’s auto-moderation.According to Ayad, Islamic State’s engagement numbers are up across the board. “Trust and safety teams have been rolled back over the past few years…a lot of this is outsourced to third party companies who aren’t necessarily experts in understanding if a piece of content came from the Islamic State,” he said.Social media companies like Meta used the election of Donald Trump as an excuse to cut back on moderating their platforms. Meta said this would mean “more speech and fewer mistakes.” No policies around terrorism have changed, but broadly speaking the largest social media platforms are doing a worse job at moderating their sites. In practice it’s turned Facebook into a place where a group like the Islamic State can spread its message without falling afoul of content moderation teams. Even three years ago, IS influencers wouldn’t have lasted long on the site.This rollback of moderation has coincided with a spike in views for IS accounts, the report argues. “Individual IS ‘influencer’ accounts are experiencing higher engagement rates on terrorist content than previously recorded by ISD analysts,” the report said. “It is unclear if this uptick is due to moderation gaps, platform mechanics or specific tactical adjustments by IS supporters and support outlets and groups.”“We’re not talking about content where there’s a gray area,” Ayad said. “It’s very clearly branded Islamic State…supports violence, supports the killing of minorities, the celebration of bombings, the pillaging that is happening in Sub Saharan Africa.” Something new is the adoption of AI systems to resurrect dead leaders. Ayad described a video where the deceased IS leader Abu Bakr al-Baghdadi delivered speeches again. “It’s a sanctioned version of using AI for a ‘beloved leader’ or taking him out of context and placing him in a meadow, surrounded by beautiful flowers, paying homage,” he said. “Some of these circles are strange.”Another popular topic in current IS propaganda is the Epstein Files. According to Ayad, an AI-generated photo of Donald Trump and Bill Clinton canoodling in bed makes frequent appearances on IS accounts across platforms. The picture is, supposedly, pulled from the Epstein files but it’s a popular fake. Ayad said Epstein has been a perfect springboard for IS to talk about “western degeneracy.”Ayad has also seen Islamic State videos created using  and . “They’re creating these virtual worlds that mimic the Islamic State’s caliphate, literally calling it something like Wilayat Roblox [the Province of Roblox] … and they’ll completely mimic the video styles of well-known Islamic State Videos using characters. This includes faux executions. It includes Arabic and English voiceover in the same cadence as an Islamic State narrator.”One of the most famous pieces of Islamic State propaganda is a film called Flames of War: The Fighting Has Just Begun. Ayad has seen multiple 1 for 1 recreations of the film using  characters. “They’re often tied to Discords where a number of users are creating this content. They always claim it’s fake or a LARP,” he said. “To see them in this video game skin is odd, to say the least.”What drives an Islamic State poster? “It’s done very much for the love of the game,” Ayad said. It’s done for the fact that, as a user, ‘I might not be able to participate in physical Jihad but I can participate in electronic Jihad.’”Keeping Islamic State off of major social media platforms is a constant battle, but one frustrating finding of the study is that the tactics for avoiding moderation haven’t changed much. “Techniques included the use of alternative news outlets to rebrand IS news, as well as purchasing or hijacking channels with high subscriber bases. These were then repurposed to share IS content. IS supporters, groups and outlets also use coded language: they sometimes referred to the group as ‘black hole’ or the ‘righteous few’ to confound moderation efforts.”To fight back against IS online, Ayad said that platforms needed to be better at coordination. Often a group is kicked off of Facebook so it moves to TikTok or another platform where it flourishes. He also said that all the companies need to be more transparent about who they’re kicking off their platform and why. “Europol does these big takedown days and they’re effective to a certain degree but the fact of the matter is that the Islamic State is spread across an expanse of different platforms and messaging applications,” he said. “They’re able to shift operations to another place, wait it out and regenerate on that platform…it’s not like you’re dealing with an average user, you’re dealing with a user that’s determined to spread their ideology and exploit your platform to their own ends.”And then there’s the old problem of language. “There needs to just be better moderation of under-moderated languages,” Ayad said. Facebook and other platforms have long been terrible at moderating non-English languages. A lot of rancid content online gets a pass because it’s in Arabic or Bengali.]]></content:encoded></item><item><title>New AirSnitch attack bypasses Wi-Fi encryption in homes, offices, and enterprises</title><link>https://arstechnica.com/security/2026/02/new-airsnitch-attack-breaks-wi-fi-encryption-in-homes-offices-and-enterprises/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/06/wi-fi-1152x648-1751309982.jpg" length="" type=""/><pubDate>Thu, 26 Feb 2026 15:45:18 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT - Ars Technica</source><content:encoded><![CDATA[It’s hard to overstate the role that Wi-Fi plays in virtually every facet of life. The organization that shepherds the wireless protocol says that more than 48 billion Wi-Fi-enabled devices have shipped since it debuted in the late 1990s. One estimate pegs the number of individual users at 6 billion, roughly 70 percent of the world’s population.Despite the dependence and the immeasurable amount of sensitive data flowing through Wi-Fi transmissions, the history of the protocol has been littered with security landmines stemming both from the inherited confidentiality weaknesses of its networking predecessor, Ethernet (it was once possible for anyone on a network to read and modify the traffic sent to anyone else), and the ability for anyone nearby to receive the radio signals Wi-Fi relies on.In the early days, public Wi-Fi networks often resembled the Wild West, where ARP spoofing attacks that allowed renegade users to read other users' traffic were common. The solution was to build cryptographic protections that prevented nearby parties—whether an authorized user on the network or someone near the AP (access point)—from reading or tampering with the traffic of any other user.]]></content:encoded></item><item><title>Spyware makers sentenced to prison in Greece for wiretapping politicians and journalists</title><link>https://techcrunch.com/2026/02/26/spyware-maker-sentenced-to-prison-in-greece-for-wiretapping-politicians-and-journalists/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Thu, 26 Feb 2026 15:45:15 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tal Dilian and three other Intellexa executives were tried for their role in a scandal dubbed "Greek Watergate," which dates back to 2022.]]></content:encoded></item><item><title>X tries wooing advertisers by letting them reuse creatives made for other platforms</title><link>https://techcrunch.com/2026/02/26/x-tries-wooing-advertisers-by-letting-them-reuse-creative-assets-from-other-platforms/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 26 Feb 2026 15:41:55 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[X has launched an expanded set of aspect ratio support for both image and video ads, so advertisers can now reuse assets they've created for other platforms.]]></content:encoded></item><item><title>How to Exchange Crypto Without KYC (No ID Required)</title><link>https://hackernoon.com/how-to-exchange-crypto-without-kyc-no-id-required?source=rss</link><author>Obyte</author><category>tech</category><pubDate>Thu, 26 Feb 2026 15:25:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Cryptocurrencies weren’t created with access requirements in mind. In theory, anyone could be a user without being obligated to share their identity. However, most crypto exchanges nowadays have Know-Your-Customer (KYC) rules in place, which basically means you have to share your ID and documents to be able to trade. They do this to comply with laws against money laundering and financing of terrorism, treating everyone as a potential money launderer or terrorist —but there are some ways around.You can still trade cryptos without , because they’re designed to be . There’s no company controlling most crypto networks, so no one can stop you. Direct contact, as in peer-to-peer (P2P) or person-to-person, is the most obvious option. You can get in touch with a trader who wants to sell or buy and reach an agreement, either digitally or physically. No ID required, but of course, this poses its own set of risks. Meeting with a stranger from the Internet isn’t usually a good idea. That’s why crypto exchanges are so popular.Good news about it: exchanges without KYC rules exist.Decentralized Exchanges (DEXs) can be considered non-KYC exchanges, and they abound. These are platforms where crypto trades happen directly between users through smart contracts or autonomous agents, without a central party holding funds or accounts. No single operator is collecting personal data, and trades are made with your non-custodial wallet. There's no built-in need for KYC checks tied to user identities.  in Obyte works like that.There’s a detail about them, though: they don’t work with traditional (fiat) money. Therefore, you won’t be able to exchange your BTC for USD here. The closest you might get is BTC to USDT or USDC, for instance, but never to bank accounts. DEXs only work inside the crypto space, and that’s why they’re decentralized. The moment your funds touch the fiat world, they lose that quality.Now, there  some non-KYC exchanges that aren’t decentralized and which may offer a way out to fiat. These platforms are handled by a team or company that charges fees for the services. Purchases with credit cards or  with online escrow offer their customers a safer way to exchange their crypto for fiat without KYC. Well. Mostly. They’re still companies that need to comply with laws, so you can find something  in their terms:"Defined as ‘free from sign-up’ or ‘no KYC’ approach is applied exclusively in scenarios identified as low risk. In instances where our monitoring system identifies potential red-flag indicators, we will escalate the checking procedures and conduct a client check."It's important to read their terms and conditions, so you can decide to use them or avoid them. \n Besides Oswap.io, Obyte has several paths to avoid KYC on exchanges. The first one is creating a  or directly using the native privacy coin of the platform:  (GBB). This asset was built to never touch a centralized crypto exchange, and it can only be traded P2P with  through the wallet. All transaction data remains in the user’s wallet and device, only available for them to see.An alternative way to perform a non-KYC fiat exchange relies on contracts with arbitration. This type of smart contract is available in the Obyte wallet. The users only need to establish their terms (in human language), select an arbiter from the, and send the contract for the other party to sign. In case of any dispute, the selected professional will analyze the evidence, solve the case, and release the funds to the winning party. With this process, one party can buy, for example, USDC or GBYTE with USD, while the other party sells.Another option, this one specifically available for GBYTE, is the exchange . This centralized platform allows its users to register with email and password, without sharing any ID documents. After enabling some security measures like two-factor authentication (2FA) and depositing GBYTE, users can trade this coin against BTC and USDT. Some fees apply to trade and withdraw, but your ID is never required. \n Pros, Cons, and Safety TipsThe appeal is easy to see. Privacy comes first, since there's no document database to leak or misuse. Access is more open, with fewer regional blocks. Fees could be lighter, too, since compliance costs are lower in many cases.On the other hand, less regulation can mean fewer guardrails. Some platforms could disappear, others may get hacked, and customer support can be useful or totally absent. Legal rules also vary by country, so the gray area depends on where you live. Besides, liquidity can be thinner, which may affect pricing.To stay safe, you must research previous reviews on the platform, double-check links, and keep most funds in a non-custodial wallet, with your own . Security features like strong passwords and two-factor authentication still matter, since privacy doesn’t block scams by itself.:::info
Featured Vector Image by vectorjuice / ]]></content:encoded></item><item><title>eBay to lay off 800 staff</title><link>https://techcrunch.com/2026/02/26/ebay-to-lay-off-800-staff/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 26 Feb 2026 15:10:45 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The latest round of layoffs marks eBay’s third workforce reduction in the past three years. ]]></content:encoded></item><item><title>HBO Max&apos;s Password-Sharing Crackdown Will Expand Globally in 2026</title><link>https://entertainment.slashdot.org/story/26/02/26/1422259/hbo-maxs-password-sharing-crackdown-will-expand-globally-in-2026?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 15:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[HBO Max will be cracking down on password sharing around the world. From a report: The streamer first started cracking down on password sharing in the United States late last August. Subscribers are now able to add an additional out-of-household account for $7.99 a month. Before that August change, Warner Bros. Discovery had been testing for months to determine who may or may not be a "legitimate user," as CEO and President for Warner Bros. Discovery Global Streaming and Games JB Perrette described the plan. 

On Thursday during the company's fourth quarter earnings call for 2025, WBD revealed that the streaming limitations would be expanding. This news came as part of an answer about which levers the company plans to pull to grow HBO Max. Password crackdowns have proven to be a lucrative way to both boost revenue and subscriptions. Netflix, for example, saw 9 million more subscribers after its first wave of password crackdowns in 2024. The caveat is that password crackdowns do not lead to consistent growth, and they often infuriate subscribers.]]></content:encoded></item><item><title>2 days left: Lock in the best discounts for TechCrunch Disrupt 2026</title><link>https://techcrunch.com/2026/02/26/2-days-left-lock-in-the-best-discounts-for-techcrunch-disrupt-2026/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Thu, 26 Feb 2026 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Register now to secure discounts of up to $680 on your TechCrunch Disrupt 2026 pass. Offer ends tomorrow, February 27, at 11:59 p.m. PT.]]></content:encoded></item><item><title>NXP Posts New Linux Accelerator Driver For Their Neutron NPU</title><link>https://www.phoronix.com/news/NXP-Neutron-Linux-Accel-Driver</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 14:01:06 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Linux kernel continues seeing more open-source kernel drivers emerge for supporting different AI accelerators / NPUs. The newest open-source driver breaking cover today is from NXP and is for enabling their Neutron neural processing unit...]]></content:encoded></item><item><title>How Stupid Would It Be to Put Data Centers in Space?</title><link>https://spectrum.ieee.org/orbital-data-centers</link><author>Glenn Zorpette</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NDk2NzA0Mi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgzMjY1MTAyNH0.uI9BWpFLRaXPOsmD9XJpkirBydvvgMMmZkPp3vFa_1c/image.png?width=600" length="" type=""/><pubDate>Thu, 26 Feb 2026 14:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Unlimited power is the draw; astronomical cost is the drawback]]></content:encoded></item><item><title>EBay Is Laying Off About 800 Workers, 6% of Global Workforce</title><link>https://slashdot.org/story/26/02/26/1358254/ebay-is-laying-off-about-800-workers-6-of-global-workforce?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[EBay is cutting about 800 jobs, or 6% of its full-time employees, saying the layoffs are needed to align its workforce with strategic priorities. From a report: "We are taking steps to reinvest across our business and align our structure with our strategic priorities, which will affect certain roles across our workforce," the San Jose, California-based company said early Thursday in a statement. "We are grateful for the contributions of the employees impacted and are committed to supporting them with care and respect." 

EBay will continue to hire in key areas. The cuts come a week after the company said it would acquire secondhand fashion marketplace Depop for about $1.2 billion in an effort to draw younger shoppers and after it reported robust quarterly results. Revenue increased 15% to $3 billion in the fourth quarter, surpassing analyst estimates.]]></content:encoded></item><item><title>Trace raises $3M to solve the AI agent adoption problem in enterprise</title><link>https://techcrunch.com/2026/02/26/trace-raises-3-million-to-solve-the-agent-adoption-problem/</link><author>Russell Brandom</author><category>tech</category><pubDate>Thu, 26 Feb 2026 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Trace is launching with $3 million in seed funding, including investment from Y Combinator, Zeno Ventures, Transpose Platform Management, Goodwater Capital, Formosa Capital, and WeFunder.]]></content:encoded></item><item><title>Figma partners with OpenAI to bake in support for Codex</title><link>https://techcrunch.com/2026/02/26/figma-partners-with-openai-to-bake-in-support-for-codex/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 26 Feb 2026 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Figma is integrating OpenAI's coding assistant Codex a week after it announced a similar integration with Anthropic's Claude Code.]]></content:encoded></item><item><title>Exhibit in Boston’s startup ecosystem at TechCrunch Founder Summit 2026</title><link>https://techcrunch.com/2026/02/26/exhibit-in-bostons-startup-ecosystem-at-techcrunch-founder-summit-2026/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Thu, 26 Feb 2026 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[On June 9, over 1,000 founders, investors, and decision-makers will gather for TechCrunch Founder Summit 2026. This isn’t just foot traffic. It’s a full day of concentrated deal flow.]]></content:encoded></item><item><title>Jest, a marketplace for messaging games, is challenging the app store status quo</title><link>https://techcrunch.com/2026/02/26/jest-a-marketplace-for-messaging-games-is-challenging-the-app-store-status-quo/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Thu, 26 Feb 2026 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Jest, a marketplace for messaging games, emerged from stealth with $7 million in seed funding.]]></content:encoded></item><item><title>Trump FCC Demands ‘Pro-America’ Media Programming All Summer Long</title><link>https://www.techdirt.com/2026/02/26/trump-fcc-demands-pro-america-media-programming-all-summer-long/</link><author>Karl Bode</author><category>tech</category><pubDate>Thu, 26 Feb 2026 13:26:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[But every so often Carr pauses to do other stuff to show daddy Trump he’s . Like his latest announcement that he’s creating a new “Pledge America Campaign” ahead of the country’s 250th birthday this July 4th. The campaign features a demand by Carr that U.S. media outlets make sure they’re airing “pro-America” programming through the summer holiday:“Consistent with their longstanding public interest obligations, America’s broadcasters play a key role in educating, informing, and entertaining viewers and listeners all across America, and they are particularly well suited to air programming that is responsive to the needs andinterests of their local communities. The Pledge America Campaign enables broadcasters to lend their voices in support of Task Force 250 and the celebration of America’s 250th birthday by airing patriotic, pro-America content that celebrates the American journey and inspires its citizens by highlighting the historic accomplishments of this great nation from our founding through the Trump Administration today.”While this is framed as a “voluntary initiative,” Carr’s recent history of launching costly and pointless investigations into companies that aren’t dutifully obedient lurks quietly in the background. You can clearly infer that Carr defines “programming that is responsive to the needs and interests of their local communities” as programming that  and ignores criticism of Republican policy. “If Carr’s pledge is truly voluntary, there would be no reason to limit it to broadcasters, said Harold Feld, a longtime telecom attorney who is senior VP of consumer advocacy group Public Knowledge. “If this were genuinely intended as voluntary, and genuinely about celebrating America, there is no reason to limit this to broadcasters,” Feld told Ars. “Cable operators are equally free to celebrate America, as are podcasters for that matter.”The Trump FCC’s lone Democratic Commissioner (the authoritarians refuse to fill the other vacant commission seat), Anna Gomez, had this to say about the campaign over at Elon Musk’s right wing propaganda website:Carr’s other effort to “empower local communities” has involved destroying popular media consolidation limits so that Trump-friendly broadcasters like Sinclair can merge and become more powerful than ever. It’s really not subtle how badly the MAGA movement wants a North Korea, Hungary, or Russia style media that delivers nothing but 24/7 agitprop blindly praising dear leader. They’ll keep pushing toward their goal until they run into something other than soft pudding in response.]]></content:encoded></item><item><title>Linux 7.1 Looks To Support Extended Attributes On Sockets For New GNOME &amp; systemd Functionality</title><link>https://www.phoronix.com/news/Linux-7.1-Looks-xattrs-Sockets</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 13:08:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While the Linux 7.0 feature merge window ended this past weekend and that next kernel release won't debut as stable until April, there are already features out on the horizon that are being positioned for likely merging into the Linux 7.1 kernel assuming no issues appear or objections raised by Linus Torvalds. One of the features already looking like it will be submitted for Linux 7.1 is supporting extended attributes on sockets...]]></content:encoded></item><item><title>How to avoid bad hires in early-stage startups</title><link>https://techcrunch.com/2026/02/26/how-to-avoid-bad-hires-in-early-stage-startups/</link><author>Maggie Nye, Isabelle Johannessen</author><category>tech</category><pubDate>Thu, 26 Feb 2026 13:02:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Lucena got the idea for Mappa after trying to build a marketing team but continually feeling like she had made the wrong hires.]]></content:encoded></item><item><title>Everything announced at Samsung’s Galaxy Unpacked event, including S26 smartphones, privacy screen, and more</title><link>https://techcrunch.com/2026/02/26/everything-announced-at-samsungs-galaxy-unpacked-event-including-s26-smartphones-privacy-screen-and-more/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Thu, 26 Feb 2026 13:01:38 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Samsung's new privacy screen feature on Galaxy S26 Ultra was the most notable feature of the event.]]></content:encoded></item><item><title>Fwupd 2.0.20 Brings New Hardware Support</title><link>https://www.phoronix.com/news/Fwupd-2.0.20</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 12:27:12 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Fwupd/LVFS lead developer Richard Hughes of Red Hat today released Fwupd 2.0.20 with continuing to advance firmware updating on Linux systems...]]></content:encoded></item><item><title>Americans Are Leaving the US in Record Numbers</title><link>https://news.slashdot.org/story/26/02/26/127223/americans-are-leaving-the-us-in-record-numbers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 12:07:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: In its 250th year, is America, land of immigration, becoming a country of emigration? Last year the U.S. experienced something that hasn't definitively occurred since the Great Depression: More people moved out than moved in. The Trump administration has hailed the exodus -- negative net migration -- as the fulfillment of its promise to ramp up deportations and restrict new visas. Beneath the stormy optics of that immigration crackdown, however, lies a less-noticed reversal: America's own citizens are leaving in record numbers, replanting themselves and their families in lands they find more affordable and safe. 

Since the Eisenhower administration, the U.S. hasn't collected comprehensive statistics on the number of citizens leaving. Yet data on residence permits, foreign home purchases, student enrollments and other metrics from more than 50 countries show that Americans are voting with their feet to an unprecedented degree. A millions-strong diaspora is studying, telecommuting and retiring overseas. The new American dream, for some of its citizens, is to no longer live there. 

In the cobblestoned streets of Lisbon, so many Americans are snapping up apartments that the newest arrivals complain they mostly hear their own language -- not Portuguese. One of every 15 residents in Dublin's trendy Grand Canal Dock district was born in the U.S., according to realtors, higher than the percentage of Americans born in Ireland during the 19th-century influx following the Potato Famine. In Bali, Colombia and Thailand, the strains of housing American remote workers paid in dollars have inspired locals to mount protests against a wave of gentrification. More than 100,000 young students are enrolled abroad for a more affordable university degree. In nursing homes mushrooming across the Mexican border, elderly Americans are turning up for low-cost care. 

[...] The U.S. experienced net negative migration -- an estimated loss of some 150,000 people -- in 2025, and the outflow will likely increase in 2026, according to calculations by the Brookings Institution, a public-policy think tank. The number could be larger or smaller because official U.S. data doesn't yet fully capture the number of people leaving, Brookings analysts noted. The total in-migration was between around 2.6 and 2.7 million in 2025, down from a peak of almost 6 million in 2023. The U.S. saw 675,000 deportations and 2.2 million "self-deportations" last year, according to data from the Department of Homeland Security. A Wall Street Journal analysis of 15 countries providing full or partial 2025 data showed that at least 180,000 Americans joined them -- a number likely to be far higher when other countries report full statistics.]]></content:encoded></item><item><title>Instagram now alerts parents if their teen searches for suicide or self-harm content</title><link>https://techcrunch.com/2026/02/26/instagram-now-alerts-parents-if-their-teen-searches-for-suicide-or-self-harm-content/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 26 Feb 2026 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Meta-owned social platform says that while it already blocks users from searching for suicide and self-harm content, these new alerts are designed to make sure parents are aware if their teen is repeatedly trying to search for this content so that they can support them.]]></content:encoded></item><item><title>ZCULL Support For Nouveau + NVK Brings Some Small Performance Gains</title><link>https://www.phoronix.com/news/NVK-ZCULL-Merged</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 11:10:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged yesterday to Mesa 26.1 for the open-source NVIDIA Vulkan driver "NVK" is ZCULL support for more efficient rendering and bringing some small performance gains to this open-source NVIDIA driver stack...]]></content:encoded></item><item><title>Intel Vulkan Driver Sees Some Minor Optimizations For DX12 Games On Linux</title><link>https://www.phoronix.com/news/Mesa-26.1-Intel-ANV-DX12-Minor</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 10:57:36 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged to Mesa 26.1-devel this week is a minor improvement to the Intel "ANV" Vulkan driver providing some slight enhancements to DirectX 12 games running on Linux by way of Valve's Steam Play with VKD3D-Proton...]]></content:encoded></item><item><title>AlmaLinux Showing Nice Growth With More Than 2M System Update Check-Ins Per Week</title><link>https://www.phoronix.com/news/AlmaLinux-2M-Weekly-Check-Ins</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 10:44:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AlmaLinux as one of the leading, modern and community-minded alternatives to Red Hat Enterprise Linux (RHEL) continues enjoying very nice growth. In their 2025 Year In Review they provided a look at their growth with now having more than two million systems per week checking in for software updates...]]></content:encoded></item><item><title>GStreamer 1.28.1 Adds Whisper-Based Speech-To-Text, AV1 Stateful V4L2 Decoder Support</title><link>https://www.phoronix.com/news/GStreamer-1.28.1</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Feb 2026 10:30:40 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Building off January's GStreamer 1.28 release with many new features, GStreamer 1.28.1 was released today as a point release bringing various fixes and minor additions to this open-source multimedia framework...]]></content:encoded></item><item><title>Cloudflare Experiment Ports Most of Next.js API in &apos;One Week&apos; With AI</title><link>https://tech.slashdot.org/story/26/02/26/0543208/cloudflare-experiment-ports-most-of-nextjs-api-in-one-week-with-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 09:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: A Cloudflare engineer says he has implemented 94% of the Next.js API by directing Anthropic's Claude, spending about $1,100 on tokens. The purpose of the experimental project was not to show off AI coding, but to address an issue with Next.js, the popular React-based framework sponsored by Vercel. 

According to Cloudflare engineering director Steve Faulkner, the Next.js tooling is "entirely bespoke... If you want to deploy it to Cloudflare, Netlify, or AWS Lambda, you have to take that build output and reshape it into something the target platform can actually run." 

The Next.js team is addressing this following numerous complaints that deploying the framework with full features on platforms other than Vercel is too difficult, with a feature in progress called deployment adapters. "Vercel will use the same adapter API as every other partner," the company said when introducing the planned feature last year.]]></content:encoded></item><item><title>The TechBeat: Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents? (2/26/2026)</title><link>https://hackernoon.com/2-26-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Thu, 26 Feb 2026 07:11:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @davidiyanu [ 5 Min read ] 
 RAG fails less from the LLM and more from retrieval: bad chunking, weak metadata, embedding drift, and stale indexes. Fix the pipeline first. Read More.By @stevebeyatte [ 7 Min read ] 
 Compare the 7 best co-parenting apps in 2026, including BestInterest, OurFamilyWizard, and TalkingParents. Find the right app for high-conflict situations.  Read More.By @omotayojude [ 3 Min read ] 
 When an AI agent's PR was rejected by Matplotlib, it didn't just close the tab it wrote an angry hit piece on the maintainer. Is this the future of open source? Read More.By @anushakovi [ 7 Min read ] 
 The thing nobody tells you about databases: The best one isn't the fastest one. It's the one your team can actually use without calling you at 3 a.m. Read More.By @paoloap [ 7 Min read ] 
 Replace custom LLM wrappers with 7 production-tested Python libraries. Covers LiteLLM, Instructor, FastMCP, PydanticAI, tiktoken, and more with code examples. Read More.By @thomascherickal [ 14 Min read ] 
 OpenClaw lets you run frontier AI models like Minimax M2.5 and GLM-5 100% locally on Mac M3 or DGX Spark — zero API costs, total privacy. Here's how.  Read More.By @playerzero [ 15 Min read ] 
 Modern software teams ship faster than ever, but defect resolution lags; PlayerZero aligns people, process, and context for predictable reliability. Read More.By @flexatone [ 7 Min read ] 
 Cybersecurity vendors trusted to protect Linux systems ignore available controls to protect their own services, leaving users exposed to supply chain attacks.  Read More.By @voidrun [ 3 Min read ] 
 QuantumLayer earned a 118 Proof of Usefulness score for helping developers integrate climate and infrastructure risk into real-world systems. Read More.By @hunterthomas [ 4 Min read ] 
 RentAHuman.ai lets AI agents hire humans for physical tasks they can't do.  Read More.By @ipinfo [ 8 Min read ] 
 Analysis of 170M residential proxy IPs reveals rapid rotation and 46% cross-provider overlap—breaking traditional fraud detection models. Read More.By @ihorkatkov [ 7 Min read ] 
 I run a personal AI agent with access to my health, calendar, and Telegram. Here are security principles that keep the blast radius small.
 Read More.By @birukum [ 11 Min read ] 
 Agentic AI workflows can create a financial black hole. Learn how semantic caching uses vector similarity to cut your LLM token burn by 24%. Read More.By @sherveen [ 5 Min read ] 
 Deep dive analysis of Grok 4.2 and Sonnet 4.6, two new AI releases from xAI and Anthropic, and how their agent systems compare. Read More.By @aimodels44 [ 8 Min read ] 
 A new study suggests AGENTS.md-style repo context files can reduce coding-agent success while raising inference cost. Here’s why—and what to do instead. Read More.By @samiranmondal [ 2 Min read ] 
 Cybersecurity stocks fell after AI company Anthropic unveiled Claude Code Security Read More.By @hacker48507274 [ 3 Min read ] 
 Ekstra AI earns a 46 Proof of Usefulness score by delivering privacy-first foot traffic intelligence to NYC small businesses—without surveillance. Read More.]]></content:encoded></item><item><title>Uber Employees Have Built an AI Clone of Their CEO To Practice Presentations Before the Real Thing</title><link>https://slashdot.org/story/26/02/25/1814206/uber-employees-have-built-an-ai-clone-of-their-ceo-to-practice-presentations-before-the-real-thing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Feb 2026 06:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Some Uber employees have built an AI clone of CEO Dara Khosrowshahi -- internally dubbed "Dara AI" -- and have been using it to rehearse and fine-tune presentations before delivering them to the actual Khosrowshahi, he revealed on a recent podcast. 

Khosrowshahi said a team member told him that some teams "make the presentation to the Dara AI as a prep for making a presentation to me," and that the bot helps them adjust their slides and sharpen their delivery. Asked by the podcast host whether employees might eventually show Dara AI to the board, Khosrowshahi laughed but noted that AI models still can't process and act on new information the way executives do. "When the models can learn in real-time, that is the point at which I'm going to think that, yeah, we are all replaceable," he said.]]></content:encoded></item></channel></rss>
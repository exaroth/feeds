<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech</title><link>https://konrad.website/feeds/</link><description></description><item><title>China Launches An Emergency Lifeboat To Bring Three Astronauts Back To Earth</title><link>https://science.slashdot.org/story/25/11/26/005203/china-launches-an-emergency-lifeboat-to-bring-three-astronauts-back-to-earth?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 26 Nov 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[China launched an uncrewed Shenzhou 22 spacecraft to serve as an emergency lifeboat for three astronauts aboard the Tiangong space station after a docked return craft was found to have a cracked window likely caused by space debris. "A Long March 2F rocket fired its engines and lifted off with the Shenzhou 22 spacecraft, carrying cargo instead of a crew, at 11:11 pm EST Monday (04:11 UTC Tuesday)," reports Ars Technica. "The spacecraft docked with the Tiangong station nearly 250 miles (400 kilometers) above the Earth about three-and-a-half hours later." From the report: Chinese engineers worked fast to move up the launch of the Shenzhou 22, originally set to fly next year. On November 4, astronauts discovered one of the two crew ferry ships docked to the Tiangong station had a damaged window, likely from an impact with a small fragment of space junk. [...] Now, 20 days after the saga began, the Tiangong outpost again has a lifeboat for its long-term residents. Astronauts Zhang Lu, Fu Wei, and Zhang Hongzhang will return to Earth on the Shenzhou 22 spacecraft next year, soon after the arrival of their three replacements.
 
The Tiangong astronauts will head outside the station on a spacewalk to inspect the damaged window on Shenzhou 20. Eventually, Shenzhou 20 will depart Tiangong and reenter the atmosphere with cargo. Assuming a smooth landing, Chinese engineers will have an opportunity to get a closer look at the damage on the ground to inform the design of future spacecraft. A preliminary assessment of the window indicates the crack is in the outermost layer of heat-insulating glass in Shenzhou 20's porthole window, according to Chinese state media. Engineers on the ground conducted simulations and wind tunnel ablation tests to determine whether the window might fail during reentry. "The results showed that the cracks would still propagate further," reported CCTV, China's government-run television network. "We held review meeting, and everyone agreed that ensuring the safe return of the astronauts was too risky with the glass damaged," Zhou said.
 
While this crew is just one month into their planned six-month expedition, an emergency could force them to leave the station and return home at any time. Although remote, another collision with space junk, a major systems failure, or a medical emergency involving one of the astronauts could trigger an evacuation. That's why Chinese officials wanted to quickly launch Shenzhou 22 to give the crew a ticket home.The International Space Station follows the same policy, with SpaceX's Dragon spacecraft and Russian Soyuz ships serving as lifeboats until their crews' scheduled return to Earth.]]></content:encoded></item><item><title>Uber and WeRide’s robotaxi service in Abu Dhabi is officially driverless</title><link>https://techcrunch.com/2025/11/25/uber-and-werides-robotaxi-service-in-abu-dhabi-is-officially-driverless/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Wed, 26 Nov 2025 07:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The commercial robotaxi service launched last year. Now, the human safety operator is gone. ]]></content:encoded></item><item><title>DOJ Would Like To Drop Charges Against Marimar Martinez, ICE Shooting Victim</title><link>https://www.techdirt.com/2025/11/25/doj-would-like-to-drop-charges-against-marimar-martinez-ice-shooting-victim/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Wed, 26 Nov 2025 04:27:54 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Trump’s war on Chicago appears destined to end in a whimper. While he kicked off his invasion of Chicago with memes and a call to arrest J.B. Pritzker and Mayor Brandon Johnson, his out of control federal agents have reportedly begun to leave the area. That suburban ICE facility you’ve heard so much about?Assistant U.S. Attorney Patrick Johnson in a hearing Tuesday said there are currently only four people being held at the Broadview facility, a drastic reduction that comes weeks after detainees there testified they had been crammed into rooms filled with more than 100 people for multiple days.The decline in population comes as federal immigration agents sent to the area as part of the Trump Administration’s “Operation Midway Blitz” enforcement efforts have begun leaving Chicago ahead of a possible return in the spring.Good, get the fuck out of here.But while the feds were playing war against the domestic enemy of Chicago, they certainly did plenty of damage. Raids disrupted neighborhoods, businesses, and even parade celebrations. The terror in pockets of the city and surrounding area was palpable. Oh, and CBP fucking shot a lady, claiming that she had rammed the cars of federal agents, was armed with a firearm, and boxed agents in putting them in danger. Here is what the DOJ said occurred back in October:“After striking the agents’ vehicle, the defendants’ vehicles boxed in the agents’ vehicle, the complaint states,” prosecutors said in a statement when charges were announced last month. “The agent was unable to move his vehicle and exited the car, at which point he fired approximately five shots from his service weapon at Martinez, the complaint states.”Sounds pretty bad. Really bad, even. Certainly not the sort of thing that the DOJ would just want to drop a month or so later, right?So, how did we go from prosecutors stating that Martinez rammed federal vehicles with her car, boxed them in alongside other drivers, was armed, and that all of that is what led to a CBP agent shooting her several times in “self defense”? That one is easy, actually: CBP was lying about what happened.Martinez’s lawyer, Christopher Parente, indicated during proceedings that he saw the bodycam footage from CBP agents and that it clearly shows that the CBP car swerved into Martinez’s vehicle, not the other way around.“When I watched the video after this agent says, ‘Do something, b—-,’ I see the driver of this vehicle turn the wheel to the left. Which would be consistent with him running into Ms. Martinez’s vehicle, okay,” Parente said. “And then seconds later, he jumps out and just starts shooting.”The gun that DOJ referenced everywhere but in the actually charging documents? Yeah, Martinez has a handgun for which she is licensed to conceal and carry in Illinois,  that firearm never made it out of her purse in this entire incident. CBP had no idea she was armed until after she’d been shot.Oh, and the CBP agent gleefully bragged about just how efficiently he put holes in Martinez’s body.During a Nov. 5 court hearing, CBP Agent Charles Exum, identified as the agent who shot Martinez, was questioned by Parente about text messages he sent to friends and family after the incident in which he appeared to boast about his shooting skills.“I fired 5 rounds and she had 7 holes. Put that in your book, boys,” one of those messages said.When pressed by Parente about the text messages during his testimony, Exum said, “I am a firearms instructor and I take pride in my shooting skills.”Parente then asked, “So you’re bragging that you shot her five times and gets seven holes, five shots? Are you literally bragging about this?”Exum responded, “I’m just saying five shots, seven holes.”And I’m just saying that such assholery is obviously part of the reason the government didn’t think it could win this case.But what this case should do, if nothing else, is serve as exhibit A in every single conversation anyone anywhere has in which someone points to statements by the government as justification for any of the fascistic bullshit the Trump administration is pulling currently. The feds lie. They lie regularly. And while DOJ can do its best to silently slink away from this whole episode, you can be damned sure civil litigation is already being drawn up.]]></content:encoded></item><item><title>Britain Plots Atomic Reboot As Datacenter Demand Surges</title><link>https://news.slashdot.org/story/25/11/25/231251/britain-plots-atomic-reboot-as-datacenter-demand-surges?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 26 Nov 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The UK is seeking to fast-track new atomic development to meet soaring energy demands driven by AI and electrification. According to a new report published by the government's Nuclear Regulatory Taskforce, excessive regulation has made Britain the most expensive place in the world to build nuclear projects. The report is calling for a sweeping overhaul to accelerate reactor construction -- everything from "streamlining regulation" to relaxing environmental and safety constraints. The Register reports: The document outlines 47 recommendations for the government, which come under five general areas: providing clearer leadership and direction for the nuclear sector; simplifying the regulatory approval process for atomic projects; reducing risk aversion; addressing incentives to delay progress; and working with the nuclear sector to speed delivery and boost innovation. Among the recommendations is that a Commission for Nuclear Regulation should be established, becoming a "unified decision maker" across all other regulators, planners, and approval bodies. The report also talks of reforming environmental and planning regimes to speed approvals, echoing the government's earlier decisions to streamline the planning process to make it easier for datacenter projects to get built.
 
It recommends amending the cost cap for judicial reviews and limiting legal challenges to Nationally Strategic Infrastructure Projects (NSIPs), while indemnifying nuclear developers against any damages they might incur as a result of proceeding with their project while a judicial review is still being decided. Another recommendation that may be cause for concern is that the government should modify the Habitats Regulations to reduce costs. These are rules created to protect the most important and vulnerable natural sites and wildlife species across the UK. The report also states that radiation limits for workers are overly conservative and well below what could be appropriately considered "broadly acceptable," claiming that they are many times less than what the average person in the UK normally receives in a year.]]></content:encoded></item><item><title>Plex Is Now Enforcing Remote Play Restrictions On TVs</title><link>https://entertainment.slashdot.org/story/25/11/25/239252/plex-is-now-enforcing-remote-play-restrictions-on-tvs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 26 Nov 2025 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Plex is beginning to enforce new restrictions on remote streaming for its TV apps, requiring either a Plex Pass or the cheaper Remote Watch Pass to watch media from servers outside your home network. How-To Geek reports: Plex is now rolling out the remote watch changes to its Roku TV app. This means that you will need a Plex Pass or Remote Watch Pass for your Plex account if you want to stream media from a server outside your home. If you're only watching media from your own server on the same local network as your Roku device, or the owner of the server you're streaming from has Plex Pass, you don't have to do anything.
 
Plex says this change will come to the other TV apps in 2026, such as Fire TV, Apple TV, and Android TV. Presumably, that will happen when the redesigned app arrives on those platforms. Roku was just the first TV platform to get the new app, which caused a wave of complaints from users about removed functionality and a more clunky redesign. Plex is addressing some of those complaints with more updates, but adding another limitation at the same time isn't a great look.
 
The Remote Watch Pass costs $2 per month or $20 per year, but there's no lifetime purchase option. You can also use a Plex Pass, which normally costs $7 per month, $70 per year, or $250 for a lifetime license. However, there's currently a 40% off sale for Plex Pass subscriptions.]]></content:encoded></item><item><title>HP To Cut About 6,000 Jobs By 2028, Ramps Up AI Efforts</title><link>https://slashdot.org/story/25/11/25/2319238/hp-to-cut-about-6000-jobs-by-2028-ramps-up-ai-efforts?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 26 Nov 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[HP plans to cut 4,000-6,000 jobs by 2028 "as part of a plan to streamline operations and adopt artificial intelligence," reports Reuters. From the report: HP's teams focused on product development, internal operations and customer support will be impacted by the job cuts, CEO Enrique Lores said during a media briefing call. "We expect this initiative will create $1 billion in gross run rate savings over three years," Lores added. The company laid off an additional 1,000 to 2,000 employees in February, as part of a previously announced restructuring plan. Demand for AI-enabled PCs has continued to ramp externally, reaching over 30% of HP's shipments in the fourth quarter ended October 31.]]></content:encoded></item><item><title>FreeBSD 15.0-RC4 Released Due To Last Minute Issues</title><link>https://www.phoronix.com/news/FreeBSD-15.0-RC4</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 26 Nov 2025 01:20:48 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[FreeBSD 15.0-RC3 shipped just a few days ago as what was expected to be the final release candidate before FreeBSD 15.0 stable is officially unveiled next week. But squeezing out today is FreeBSD 15.0-RC4 to address last minute issues...]]></content:encoded></item><item><title>Warner Music Group Partners With Suno To Offer AI Likenesses of Its Artists</title><link>https://yro.slashdot.org/story/25/11/25/2240236/warner-music-group-partners-with-suno-to-offer-ai-likenesses-of-its-artists?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 26 Nov 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Warner Music Group has reached a licensing deal with Suno that will let users create AI-generated music using the voices and likenesses of artists who opt in. WMG says participating artists will have "full control" over how their likeness and music are used. "These will be new creation experiences from artists who do opt in, which will open up new revenue streams for them and allow you to interact with them in new ways," Suno says, adding that users will be able to "build around" an artist's sounds "and ensure they get compensated." WMG is also dropping its previous lawsuit accusing Suno of scraping copyrighted material.
 
"Along with the licensing agreement, Suno is planning to use licensed music from WMG to build next-gen music generation models that it claims will surpass its flagship v5 model," adds The Verge. "It will also start requiring users to have a paid account to download songs starting next year, with each tier providing a specific number of downloads each month."
 
Further reading: First 'AI Music Creator' Signed by Record Label. More Ahead, or Just a Copyright Quandry?]]></content:encoded></item><item><title>Google Maps Will Let You Hide Your Identity When Writing Reviews</title><link>https://yro.slashdot.org/story/25/11/25/2221221/google-maps-will-let-you-hide-your-identity-when-writing-reviews?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 26 Nov 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from PCMag: Four new features are coming to Google Maps, including a way to hide your identity in reviews. Maps will soon let you use a nickname and select an alternative profile picture for online reviews, so you can rate a business without linking it to full name and Google profile photo. Google says it will monitor for "suspicious and fake reviews," and every review is still associated with an account on Google's backend, which it believes will discourage bad actors.
 
Look for a new option under Your Profile that says Use a custom name & picture for posting. You'll then be able to pick an illustration to represent you and add a nickname. Google didn't explain why it is introducing anonymous reviews; it pitched the idea as a way to be a business's "Secret Santa." Some users are nervous to publicly post reviews for local businesses as it may be used to track their location or movements. It may encourage more people to contribute honest feedback to its platform, for better or worse. Further reading: Gemini AI To Transform Google Maps Into a More Conversational Experience]]></content:encoded></item><item><title>Speaking Freely: Laura Vidal</title><link>https://www.eff.org/deeplinks/2025/11/speaking-freely-laura-vidal</link><author>Jillian C. York</author><category>tech</category><enclosure url="https://www.eff.org/files/banner_library/speaking-freely-laura_blogog_size.jpg" length="" type=""/><pubDate>Tue, 25 Nov 2025 23:57:59 +0000</pubDate><source url="https://www.eff.org/rss/updates.xml">Deeplinks</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ctrl-Alt-Speech Spotlight: Building Better CSAM Detection With Resolver’s George Vlasto</title><link>https://www.techdirt.com/2025/11/25/ctrl-alt-speech-spotlight-building-better-csam-detection-with-resolvers-george-vlasto/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 25 Nov 2025 23:34:27 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[In this sponsored Spotlight episode of Ctrl-Alt-Speech, host Ben Whitelaw speaks with George Vlasto, head of the Trust & Safety division at Resolver, as the organisation marks its 20th anniversary. Their conversation looks back at two decades of Resolver’s work supporting platforms and safeguarding online communities, and explores how that legacy has shaped its newest innovations.Ben and George dig into Resolver’s unique approach to scaling the detection of Child Sexual Abuse Material (CSAM) and unpack why ATHENA — the company’s latest breakthrough — may be one of the most significant yet under-recognised tools in the fight against online harms.This episode is brought to you in partnership with Resolver. To sponsor a Spotlight episode of Ctrl-Alt-Speech, visit our website.]]></content:encoded></item><item><title>Poland Probes Apple Again Over App Tracking Transparency Rules</title><link>https://apple.slashdot.org/story/25/11/25/227255/poland-probes-apple-again-over-app-tracking-transparency-rules?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Poland has launched a new antitrust investigation into Apple's App Tracking Transparency rules, questioning whether Apple misled users about privacy while giving its own apps a competitive advantage over third-party developers. AppleInsider reports: On November 25, Poland's UOKiK has started another investigation into App Tracking Transparency, and whether Apple had restricted competition in mobile advertising. Reuters reports that, to the anti-monopoly regulator, ATT may have limited advertisers' ability to collect user data for advertising purposes while simultaneously favoring Apple's ad program. On November 25, Poland's UOKiK has started another investigation into App Tracking Transparency, and whether Apple had restricted competition in mobile advertising. Reuters reports that, to the anti-monopoly regulator, ATT may have limited advertisers' ability to collect user data for advertising purposes while simultaneously favoring Apple's ad program.
 
This is not the first time that Poland has looked into ATT rules. In December 2021, the regulator held a similar probe following criticism from advertisers. It's not clear what that complaint determined, or if it is still ongoing. Regardless, in the new complaint, the logic is that Apple had a competitive advantage since its own apps were not subject to ATT rules, but third-party apps did have to deal with ATT. Since Apple didn't visibly ask for consent for its first-party apps in the same way, there is a presumption that Apple's rules only applied to other companies.
 
This is despite Apple's repeated insistence that it doesn't use the same kinds of collected data in its own apps and services for marketing purposes, as well as its stance on privacy in general. In short, Apple apps don't use the data, so it doesn't pop up a dialog box asking the user if the app can use the data. There is also the argument that, in setting up an account with Apple, users are providing blanket consent to the company. Implementing ATT on its own apps would therefore be a waste of time, since that consent was already granted. Apple said that it will work with the regulator on the matter, but warned that it could force them to withdraw the feature "to the detriment of European consumers."]]></content:encoded></item><item><title>Why ‘hold forever’ investors are snapping up venture capital ‘zombies’</title><link>https://techcrunch.com/2025/11/25/why-hold-forever-investors-are-snapping-up-venture-capital-zombies/</link><author>Marina Temkin</author><category>tech</category><pubDate>Tue, 25 Nov 2025 22:52:22 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[An investor explains the "buy, fix, and hold" model employed by Bending Spoons and similar companies that buy distressed startups.]]></content:encoded></item><item><title>&apos;AI Can&apos;t Think&apos;</title><link>https://slashdot.org/story/25/11/25/2146258/ai-cant-think?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In an essay published in The Verge, Benjamin Riley argues that today's AI boom is built on a fundamental misunderstanding: language modeling is not the same as intelligence. "The problem is that according to current neuroscience, human thinking is largely independent of human language -- and we have little reason to believe ever more sophisticated modeling of language will create a form of intelligence that meets or surpasses our own," writes Riley. Slashdot reader RossCWilliams shares the report, writing: The article goes on to point out that we use language to communicate. We use it to create metaphors to describe our reasoning. That people who have lost their language ability can still show reasoning. That human beings create knowledge when they become dissatisfied with the current metaphor. Einstein's theory of relativity was not based on scientific research. He developed it as thought experiment because he was dissatisfied with the existing metaphor. It quotes someone who said, "common sense is a collection of dead metaphors." And that AI, at best, can rearrange those dead metaphors in interesting ways. But it will never be dissatisfied with the data it has or an existing metaphor.
 
A different critique (PDF) has pointed out that even as a language model AI is flawed by its reliance on the internet. The languages used on the internet are unrepresentative of the languages in the world. And other languages contain unique descriptions/metaphors that are not found on the internet. My metaphor for what was discussed was the descriptions of the kinds of snow that exist in Inuit languages that describe qualities nowhere found in European languages. If those metaphors aren't found on the internet, AI will never be able create them.
 
This does not mean that AI isn't useful. But it is not remotely human intelligence. That is just a poor metaphor. We need a better one. Benjamin Riley is the founder of Cognitive Resonance, a new venture to improve understanding of human cognition and generative AI.]]></content:encoded></item><item><title>YouTube is working on a feature that will fix the messy home feed</title><link>https://techcrunch.com/2025/11/25/youtube-is-working-on-a-feature-that-will-fix-the-messy-home-feed/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Tue, 25 Nov 2025 22:31:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[YouTube is experimenting with a new feature that allows users to customize their home feed. ]]></content:encoded></item><item><title>Arc Raiders ‘Watchlist’ Names and Shames Backstabbing Players</title><link>https://www.404media.co/arc-raiders-watchlist-names-and-shames-backstabbing-players/</link><author>Matthew Gault</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/11/20251114164041_1.jpg" length="" type=""/><pubDate>Tue, 25 Nov 2025 22:24:25 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[A  is holding  players accountable when they betray their fellow players. —named for the game’s social hub—bills itself as “your friendly Raider shaming board,” a place where people can report other people for what they see as anti-social behavior in the game.In , players land on a map full of NPC robots and around 20 other humans. The goal is to fill your inventory with loot and escape the map unharmed. The robots are deadly, but they’re easy to deal with once you know what you’re doing. The real challenge is navigating other players and that challenge is the reason  is a mega-hit. People are far more dangerous and unpredictable than any NPC. comes with a proximity chat system so it’s easy to communicate with anyone you might run into in the field. Some people are nice and will help their fellow raider take down large robots and split loot. But just as often, fellow players will shoot you in the head and take all your stuff.In the days after the game launched, many people opened any encounter with another human by coming on the mic, saying they were friendly, and asking not to shoot. Things are more chaotic now. Everyone has been shot at and hurt people hurt people. But some hurts feel worse than others.Speranza Watchlist is a place to collect reports of anti-social behavior in . It’s creation of a web developer who goes by DougJudy online. 404 Media reached out to him and he agreed to talk provided we grant him anonymity. He said he intended the site as a joke and some people haven’t taken it well and have accused him of doxxing.I asked DougJudy who hurt him so badly in  that he felt the need to catalog the sins of the community.  “There wasn’t a specific incident, but I keep seeing a lot (A LOT) of clips of people complaining when other players play dirty’ (like camping extracts, betraying teammates, etc.)”He thought this was stupid. For him, betrayal is the juice of . “Sure, people can be ‘bad’ in the game, but the game intentionally includes that social layer,” he said. “It’s like complaining that your friend lied to you in a game of . It just doesn’t make sense.”That doesn’t mean the betrayals didn’t hurt. “I have to admit that sometimes I also felt the urge to vent somewhere when someone betrayed me, when I got killed by someone I thought was an ally,” DougJudy said. “At first, I would just say something like, ‘I’ll find you again, the only thing that doesn’t cross paths are mountains,’ and I’d note their username. But then I got the idea to make a sort of leaderboard of the least trustworthy players…and that eventually turned into this website.As the weeks go on and more players join the , its community is developing its own mores around acceptable behavior. PVP combat is a given but there are actions some Raiders engage in that, while technically allowed, feel like bad sportsmanship. Speranza Watchlist wants to list the bad sports.Take extract camping. In order to end the map and “score” the loot a player has collected during the match, they have to leave the map via a number of static exits. Some players will place explosive traps on these exits and wait for another player to leave. When the traps go off, the camper pops up from their hiding spot and takes shots at their vulnerable fellow raider. When it works, it’s an  from a person who was just trying to leave.Enter Speranza Watchlist. “You’ve been wronged,” an explanation on the site says. “When someone plays dirty topside—betraying trust, camping your path, or pulling a Rust-Belt rate move—you don’t have to let it slide.”When someone starts up  for the first time, they have to create a unique “Embark ID” that’s tied to their account. When you interact with another player in the game, no matter how small the moment, you can see their Embark ID and easily copy it to your clipboard if you’re playing on PC.Players can plug Embark IDs into Speranza Watchlist and see if the person has been reported for extract camping or betrayal before. They can also submit their own reports. DougJudy said that, as of this writing, around 200 players had submitted reports. Right now, the site is down for maintenance. “I’m trying to rework the website to make the fun/ satire part more obvious,” DougJudy said. He also plans to add rate limits so one person can’t mass submit reports.He doesn’t see the Speranza Watchlist as doxxing. No one's real identity is being listed. It’s just a collection of observed behaviors. It’s a social credit score for . “I get why some people don’t like the idea, ‘reporting’ a player who didn’t ask for it isn’t really cool,” DougJudy said. “And yeah, some people could maybe use it to harass others. I’ll try my best to make sure the site doesn’t become like that, and that people understand it’s not serious at all. But if most people still don’t like it, then I’ll just drop the idea.”]]></content:encoded></item><item><title>US Banks Scramble To Assess Data Theft After Hackers Breach Financial Tech Firm</title><link>https://news.slashdot.org/story/25/11/25/2138249/us-banks-scramble-to-assess-data-theft-after-hackers-breach-financial-tech-firm?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: Several U.S. banking giants and mortgage lenders are reportedly scrambling to assess how much of their customers' data was stolen during a cyberattack on a New York financial technology company earlier this month. SitusAMC, which provides technology for over a thousand commercial and real estate financiers, confirmed in a statement over the weekend that it had identified a data breach on November 12. The company said that unspecified hackers had stolen corporate data associated with its banking customers' relationship with SitusAMC, as well as "accounting records and legal agreements" during the cyberattack.
 
The statement added that the scope and nature of the cyberattack "remains under investigation." SitusAMC said that the incident is "now contained," and that its systems are operational. The company said that no encrypting malware was used, suggesting that the hackers were focused on exfiltrating data from the company's systems rather than causing destruction. According to Bloomberg and CNN, citing sources, SitusAMC sent data breach notifications to several financial giants, including JPMorgan Chase, Citigroup, and Morgan Stanley. SitusAMC also counts pension funds and state governments as customers, according to its website.
 
It's unclear how much data was taken, or how many U.S. banking consumers may be affected by the breach. Companies like SitusAMC may not be widely known outside of the financial world, but provide the mechanisms and technologies for its banking and real estate customers to comply with state and federal rules and regulations. In its role as a middleman for financial clients, the company handles vast amounts of non-public banking information on behalf of its customers. According to SitusAMC's website, the company processes billions of documents related to loans annually.]]></content:encoded></item><item><title>Trump’s Former Insurance Lawyer Fucked Up His Revenge Prosecution Of James Comey In So Many Ways</title><link>https://www.techdirt.com/2025/11/25/trumps-former-insurance-lawyer-fucked-up-his-revenge-prosecution-of-james-comey-in-so-many-ways/</link><author>Tim Cushing</author><category>tech</category><pubDate>Tue, 25 Nov 2025 21:42:05 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Yes, as I already wrote about, the indictment against James Comey was already thrown out due to Halligan’s improper appointment, but it’s also worth covering how badly she fucked up even if she had been appointed legally. These cases were destined for the dustbin no matter what, because Halligan had no idea what she was doing. Late last week just one example of that insanity came out in court.The whole case hasn’t gone well with Halligan handling the prosecution. The indictment delivered to a federal judge contained noticeable errors, the most notable being a discrepancy between the number of criminal counts presented to the grand jury. The one Halligan delivered contained  different Count Two’s in the charges, with supposedly one of the  charges (even though only  are listed) approved for prosecution by the grand jury.Halligan told the judge she had only seen and signed the less fucked up version of the grand jury indictment but the judge pointed out both versions given to her had been signed by Trump’s recently installed DOJ prosecutor.“Okay. It has your signature on it,” [Judge] Vaala told Halligan, who responded, “Okay. Well.”A federal magistrate judge said on Monday that the criminal case against James B. Comey, the former F.B.I. director, could be in trouble because of a series of apparent errors committed in front of the grand jury by Lindsey Halligan, the inexperienced prosecutor picked by President Trump to oversee the matter.In his ruling, Judge Fitzpatrick said that when Ms. Halligan appeared — by herself — in front of the grand jury in September to seek an indictment accusing Mr. Comey of lying to and obstructing Congress in 2020 testimony, she made at least two “fundamental and highly prejudicial” misstatements of the law. He also pointed out that the grand jury materials he ordered her to turn over to him for his review this month appeared to be incomplete and “likely do not reflect the full proceedings.”There’s more in the ruling [PDF] handed down by Judge Fitzpatrick, but nothing in there suggests Halligan didn’t fuck up repeatedly while (mis)handling this case. It also points out that Trump’s DOJ and FBI engaged in some possible misconduct when conducting searches of devices seized from Comey’s personal lawyer, Daniel Richman. Not only did the DOJ not take steps to separate anything that might have been privileged communications between the attorney and his client, it is now (five years later) trying to use information obtained then to support the current case against Comey.Here, the government was permitted to search all of the Richman materials but authorized to seize only evidence related to violations of 18 U.S.C. § 641 (Theft and Conversion of Stolen Government Property) and 18 U.S.C. § 793 (Unlawful Gathering or Transmission of National Defense Information), both markedly different offenses than those with which Mr. Comey is currently charged.The government appears to have conflated its obligation to protect privileged information–an obligation it approached casually at best in this case–with its duty to seize only those materials authorized by the Court. This cavalier attitude towards a basic tenet of the Fourth Amendment and multiple court orders left the government unchecked to rummage through all of the information seized from Mr. Richman, and apparently, in the government’s eyes, to do so again anytime they chose. The Arctic Haze investigation was closed in September 2021, with no charges filed. The Richman materials sat dormant with the FBI until the summer of 2025, when the Bureau chose to rummage through them again.While none of that is necessarily Halligan’s fault, if she were a seasoned prosecutor rather than just another Trump loyalist, she might have realized this evidence was possibly unusable and withheld it from her (and her ) grand jury presentation. But the rest of it is  her fault. As Anna Bower points out in her extremely informative Bluesky thread, Halligan tried to salvage an apparent mistake by the grand jury foreperson — a misplaced signature that made it look like  criminal charges were being forwarded — by getting a signature on replacement paperwork which would reflect the grand jury’s decision to indict Comey on Count Two (removing the original Count One rejected by the grand jury). This explains the new paperwork with two Count Two’s, but it certainly doesn’t explain why Halligan thought she could slide the edited paperwork past the court. And that all adds up to a judge who was clearly fed up:The Court recognizes that the relief sought by the defense is rarely granted. However, the record points to a disturbing pattern of profound investigative missteps, missteps that led an FBI agent and a prosecutor to potentially undermine the integrity of the grand jury proceeding. Therefore, in this case, “the Court has before it a rare example of a criminal defendant who can actually make a ‘particularized and factually based’ showing that grounds exist to support the proposition that irregularities may have occurred in the grand jury proceedings and may justify the dismissal of one or more counts of the indictment.”Now, a rational president who desires to be represented by competent professionals would have already acted to replace this prosecutor and demanded answers from the FBI. This is not a rational president nor is it a normal presidency. The administration that has turned the concept of executive power into an absolute monster will sweep this loss under the rug with the rest of its losses while claiming the system it controls is somehow stacked against it. And it will, of course play well, with the GOP voting base, which loves nothing more than being told that everyone else but themselves are to blame for their personal failures.]]></content:encoded></item><item><title>Character AI will offer interactive ‘Stories’ to kids instead of open-ended chat</title><link>https://techcrunch.com/2025/11/25/character-ai-will-offer-interactive-stories-to-kids-instead-of-open-ended-chat/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 25 Nov 2025 21:22:04 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company announced last month that it would no longer allow minors to use its chat features.]]></content:encoded></item><item><title>AI Could Replace 3 Million Low-Skilled Jobs in the UK By 2035, Research Warns</title><link>https://news.slashdot.org/story/25/11/25/193251/ai-could-replace-3-million-low-skilled-jobs-in-the-uk-by-2035-research-warns?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 21:22:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Up to 3 million low-skilled jobs could disappear in the UK by 2035 because of automation and AI, according to a report by a leading educational research charity. The Guardian: The jobs most at risk are those in occupations such as trades, machine operations and administrative roles, the National Foundation for Educational Research (NFER) said. Highly skilled professionals, on the other hand, were forecast to be more in demand as AI and technological advances increase workloads "at least in the short to medium term." 

Overall, the report expects the UK economy to add 2.3 million jobs by 2035, but unevenly distributed. The findings stand in contrast to other recent research suggesting AI will affect highly skilled, technical occupations such as software engineering and management consultancy more than trades and manual work.]]></content:encoded></item><item><title>Toshiba Targets Motorcycles and Boats With Its Batteries</title><link>https://spectrum.ieee.org/lto-battery-lithium-ion-toshiba</link><author>John Boyd</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIyMTI1Ny9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5Njk1NjA5MX0.AylDD4jzuHE_vKVDuVtEWUgocbrSYNUgWcTPJtizOOQ/image.jpg?width=600" length="" type=""/><pubDate>Tue, 25 Nov 2025 21:16:16 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[LTO anodes face off against cheaper but shorter-lived graphite anodes]]></content:encoded></item><item><title>AMDGPU Driver Lacks HDMI 2.1 While AMD-Xilinx Driver Has Some HDMI 2.1 Support</title><link>https://www.phoronix.com/news/AMDGPU-HDMI-2.1-But-Xilinx</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 21:03:46 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Those following Phoronix and the open-source AMD Linux kernel graphics driver know that the HDMI Forum has prevented AMD from implementing HDMI 2.1 support in their open-source "AMDGPU" driver as due the driver implementation would run afoul to the organization's licensing requirements. It's been pointed out online this week that the AMD-Xilinx DRM driver though does have some HDMI 2.1 support albeit different hardware...]]></content:encoded></item><item><title>American Influencers Can&apos;t Stop Praising Chinese EVs They Can&apos;t Buy</title><link>https://news.slashdot.org/story/25/11/25/1842246/american-influencers-cant-stop-praising-chinese-evs-they-cant-buy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 20:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Chinese automakers may not be able to sell their electric vehicles in the United States due to steep tariffs and software restrictions, but they have found an alternative path to American eyeballs through a coordinated campaign targeting car influencers on YouTube, TikTok, and Instagram. The effort, the Verge reports, is largely organized by DCar Studio, a platform that invites US-based creators to Los Angeles to test-drive vehicles from brands like BYD, Geely and Xiaomi. DCar is actually Dongchedi, a car trading platform owned by TikTok parent ByteDance that raised $600 million on a $3 billion valuation in 2024. The strategy appears aimed at building global brand awareness rather than direct US sales. 

Mark Greeven, professor at IMD Business School, told The Verge that American influencers still shape opinions across the Western world. "The charm offensive is to work with American influencers about Chinese EV cars because we still have a dominant opinion in the Western world, which is formed by English-speaking influential figures on social media," he said. Several creators told The Verge they have heard rumors of undisclosed payments for positive coverage.]]></content:encoded></item><item><title>ChatGPT’s voice mode is no longer a separate interface</title><link>https://techcrunch.com/2025/11/25/chatgpts-voice-mode-is-no-longer-a-separate-interface/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 25 Nov 2025 20:04:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[ChatGPT now lets you use voice and text in the same screen, making conversations feel more natural as responses and visuals appear in real time.]]></content:encoded></item><item><title>The Hidden Architect of Industry: Engineering the Future of Defence, Rail and Aerospace Machinery</title><link>https://hackernoon.com/the-hidden-architect-of-industry-engineering-the-future-of-defence-rail-and-aerospace-machinery?source=rss</link><author>Sanya Kapoor</author><category>tech</category><pubDate>Tue, 25 Nov 2025 20:04:18 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Mechanical engineer Utkarsh Ashok Singh is modernizing defence, rail, and aerospace manufacturing through automated shot-peening systems that boost speed, consistency, and sustainability. His work cuts energy use by 98%, improves reliability, and explores AI for adaptive control, vision scoring, and predictive maintenance.]]></content:encoded></item><item><title>The Fascism Is Happening Live On TV</title><link>https://www.techdirt.com/2025/11/25/the-fascism-is-happening-live-on-tv/</link><author>Mike Brock</author><category>tech</category><pubDate>Tue, 25 Nov 2025 20:01:55 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Mark Kelly—former Navy combat pilot, astronaut, sitting United States Senator—stated a simple legal fact on video: members of the US military can refuse illegal orders. Not as opinion. Not as political positioning. As established law codified in the Uniform Code of Military Justice and affirmed at Nuremberg when “I was following orders” was rejected as defense for war crimes.The Trump Administration opened a federal investigation into him for saying this.Slow down. Read that again. One more time. Let it register fully.A sitting senator stated constitutional law. The executive branch opened an investigation into him for stating it. State propaganda praised this as making an example.This isn’t approaching fascism. This isn’t fascism-adjacent. This is fascism—the actual thing, not the metaphor, happening in real time on national television while we debate whether calling it fascism is too divisive.Let’s be precise about the mechanics. The Trump Administration isn’t investigating Kelly for corruption or lawbreaking. It’s investigating him for defending the principle that law constrains executive power. The investigation isn’t meant to find wrongdoing—it’s meant to intimidate through spectacle of state punishment. The message isn’t “we enforce laws” but “invoke constitutional constraints and we come for you.”And Watters—state propagandist on the regime’s preferred network—praises this openly: “You have to make examples out of people.” That’s Goebbels. Not as hyperbole. As accurate historical parallel. Making examples means using state violence visibly enough that others learn to submit without needing to be targeted. The cruelty is the point. The intimidation is the goal. The investigation is the punishment.Kelly defended the constitutional framework distinguishing American military forces from authoritarian ones—the framework saying law constrains power, that orders can be illegal, that service members have the duty to refuse commands violating constitutional or international law. This is why Admiral Holsey resigned over Caribbean boat strikes. He understood that following illegal orders doesn’t protect you—it implicates you.And now the Trump Administration investigates Kelly for defending this principle while Fox News calls for him to be made an example.Think about what this means. If you can be investigated by federal authorities for stating that military personnel can refuse illegal orders, then there are no illegal orders. If defending constitutional constraints on executive power becomes grounds for federal investigation, then constitutional constraints no longer exist. If senators can be “made examples of” for invoking established law, then law has been replaced by will.This is the mechanism. This is how constitutional republics die. Not through formal coup or dramatic collapse but through making it dangerous to invoke constitutional protections. You don’t repeal the law protecting refusal of illegal orders—you just investigate anyone who mentions it until no one dares. You don’t ban opposition—you make examples until opposition becomes unthinkable. You don’t eliminate the Constitution—you prosecute people who cite it until citing it becomes sedition.Eventually no one invokes it. Eventually no one remembers it protected anything. Eventually “the president ordered it” becomes sufficient justification for any action, any violation, any atrocity. That’s the world this investigation is building. That’s the world Watters is praising. That’s the world taking shape while we watch.Kelly is doing what constitutional officers do: defending that law constrains power, that orders can be illegal, that military serves Constitution rather than personal loyalty to whoever holds office. This isn’t radical. This isn’t partisan. This is baseline constitutional governance—the floor beneath which lies only authoritarianism.The Trump Administration is doing what authoritarian regimes do: weaponizing state power against those defending constitutional constraints, using federal investigation as punishment for opposition, making examples to terrorize others into silence.Watters is doing what fascist propagandists do: praising political persecution on state television, normalizing investigation-as-intimidation, celebrating the “examples” that teach everyone else to submit.This is it. This is the thing. Not the prologue, not the warning sign, but the actual consolidation of authoritarian power happening on Fox News while many Americans legislate on whether noticing this is “Trump Derangement Syndrome.”A senator stated constitutional law. The regime opened an investigation. State media praised making an example of him. And we’re supposed to worry that calling this “fascist” is too inflammatory?The scandal isn’t the word. The scandal is that it’s accurate. They’re doing it. Openly. On television. While scolding anyone who names it as divisive.Kelly is correct. The law is clear. Military personnel can and must refuse illegal orders. Stating this is not sedition—it’s constitutional duty.Investigating him for stating it is not governance—it’s fascism.Call it what it is. Without apology. Without hedge. Without the cowardice that mistakes silence for civility.Because the alternative—the world where constitutional law becomes grounds for investigation, where “making examples” is normal, where all orders are legal because the leader gave them—that world is being built right now.Not in some dystopian future. Today. On Fox News. By the Trump Administration.With federal investigations and propaganda praise and audiences nodding along as if political persecution were patriotism rather than the exact thing the Founders built constitutional protections to prevent.Mike Brock is a former tech exec who was on the leadership team at Block. Originally published at his Notes From the Circus.]]></content:encoded></item><item><title>RealPage Agrees To Settle Federal Rent-Collusion Case</title><link>https://news.slashdot.org/story/25/11/25/1827251/realpage-agrees-to-settle-federal-rent-collusion-case?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Justice Department has reached an agreement to settle an antitrust lawsuit against RealPage, a real estate software company that the government accused of enabling landlords to collude to raise rents. From a report: Using RealPage software, landlords shared information about their rents and occupancy rates with the company, after which an algorithm suggested what to charge renters. The government's suit, which was joined by several state attorneys general, accused RealPage of taking the confidential information and suggesting rents higher than those in a free market. 

Under the settlement proposal, which requires approval by a federal judge overseeing the case in the Middle District of North Carolina, RealPage's software could no longer use information about current leases to train its algorithm. Nonpublic data from competing landlords would also be excluded when suggesting rents. "Competing companies must make independent pricing decisions, and with the rise of algorithmic and artificial intelligence tools, we will remain at the forefront of vigorous antitrust enforcement," said Gail Slater, who leads the antitrust division at the Department of Justice, in a news release.]]></content:encoded></item><item><title>Warner Music signs deal with AI music startup Suno, settles lawsuit</title><link>https://techcrunch.com/2025/11/25/warner-music-signs-deal-with-ai-music-startup-suno-settles-lawsuit/</link><author>Aisha Malik</author><category>tech</category><pubDate>Tue, 25 Nov 2025 19:57:31 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[WMG says artists and songwriters will have full control over whether and how their names, images, likenesses, voices, and compositions are used in new AI-generated music.]]></content:encoded></item><item><title>OpenAI and Perplexity are launching AI shopping assistants, but competing startups aren’t sweating it</title><link>https://techcrunch.com/2025/11/25/openai-and-perplexity-are-launching-ai-shopping-assistants-but-competing-startups-arent-sweating-it/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Tue, 25 Nov 2025 19:35:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Startup founders building AI shopping tools think general-purpose models are too broad to deliver truly personalized shopping experiences.]]></content:encoded></item><item><title>Google Unveils Antigravity IDE, an AI-Driven Coding Environment Powered by Gemini 3</title><link>https://hackernoon.com/google-unveils-antigravity-ide-an-ai-driven-coding-environment-powered-by-gemini-3?source=rss</link><author>Vladislav Guzey</author><category>tech</category><pubDate>Tue, 25 Nov 2025 19:24:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Google has released a new tool for developers called Google Antigravity IDE. This new software is built around Google’s advanced AI model, Gemini 3. The main goal of this tool is to make coding faster and easier by letting an AI “agent” handle many of the difficult tasks.What is Google Antigravity IDE?Antigravity is an AI-powered development environment. It looks very similar to VS Code, a popular coding tool. If you have used VS Code before, you will feel comfortable using Antigravity right away.The biggest difference is that Antigravity has a smart AI agent built inside it. This agent doesn’t just write code; it can plan projects, run tests, and even fix bugs for you.How to Install AntigravityAntigravity is available for Windows, Mac, and Linux.  Windows and Mac: The installation is simple. You just  and install it like any other program.Linux (Ubuntu): You need to use the terminal and these three simple commands to add the software repository and install the package.sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://us-central1-apt.pkg.dev/doc/repo-signing-key.gpg | \
  sudo gpg --dearmor -o /etc/apt/keyrings/antigravity-repo-key.gpg
echo "deb [signed-by=/etc/apt/keyrings/antigravity-repo-key.gpg] https://us-central1-apt.pkg.dev/projects/antigravity-auto-updater-dev/ antigravity-debian main" | \
  sudo tee /etc/apt/sources.list.d/antigravity.list > /dev/null
sudo apt install antigravity
\
When you open the app for the first time, it gives you a tour. You can import your settings from other tools (like VS Code or Cursor), choose a color theme, and sign in with your Google account.Antigravity has several cool features that make it stand out:The IDE has access to your browser. The AI uses this to test the websites it builds.Agent Manager: This is a special screen where you can see a list of tasks the AI is working on.Planning Mode: Before the AI starts coding, it creates a “To-Do” list and a plan. This helps you understand what the AI is going to do.Autopilot: You can set the AI to work on its own. It will write code, create files, and check its own work.The “Magic” Testing FeatureThe most impressive part is how the AI tests applications.The AI wrote the code and then opened the browser to test it. You could see the AI clicking buttons and typing text on the screen, just like a real human user. It even records a video of this test so you can watch it later to make sure everything works.I recommend you watch my full video tutorial, where I walk you through almost all features of this IDE.Anitgravity IDE Video TutorialGoogle Antigravity IDE seems like a very powerful tool, especially for web developers. The ability for the AI to “see” the screen and test websites automatically could save programmers a lot of time.Please give it a shot and share with me your experience in the comments below.]]></content:encoded></item><item><title>Jakarta Moves Ahead of Tokyo As World&apos;s Most Populated City</title><link>https://news.slashdot.org/story/25/11/25/1820249/jakarta-moves-ahead-of-tokyo-as-worlds-most-populated-city?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 19:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[schwit1 writes: Indonesia's capital, Jakarta, tops a ranking that is increasingly dominated by Asia: the world's most populated city. It edged out Bangladesh's capital, Dhaka, and Japan's Tokyo to earn the title in a new United Nations report. [PDF] 

With an estimated population of nearly 42 million residents, Jakarta soared from 33rd place in the previous rankings, in 2018, that were topped by Tokyo. It's followed by Dhaka, with 36 million, which the report says is "expected to become the world's largest city by mid-century."]]></content:encoded></item><item><title>Meet the Writer: Matthew Giannelis on Pivoting From 20 Years in IT to Tech Journalism</title><link>https://hackernoon.com/meet-the-writer-matthew-giannelis-on-pivoting-from-20-years-in-it-to-tech-journalism?source=rss</link><author>Matthew - Technology News Australia</author><category>tech</category><pubDate>Tue, 25 Nov 2025 19:15:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Let’s start! Tell us a bit about yourself—for example, name, profession, and personal interests.I come from a background in IT desktop and server support. After spending 20 years working as a contractor, my career took a gradual turn toward writing and journalism following a car accident.Interesting! What was your latest Hackernoon Top story about?Do you usually write on similar topics? If not, what do you usually write about?I mainly write in the tech sector, focusing specifically on IT security and marketing.Great! What is your usual writing routine like (if you have one?)“I try to write a little every day, usually when I feel most focused—sometimes morning, sometimes late at night. I like to get my ideas down first and worry about editing later. It’s more about keeping the habit and letting the creativity flow than forcing perfect sentences.Being a writer in tech can be a challenge. It’s not often our main role, but an addition to another one. What is the biggest challenge you have when it comes to writing?One of my biggest challenges with writing is staying timely. Sometimes I have ideas or inspiration, but I struggle to sit down and turn them into words right away, which can make me feel like I’m falling behind. Balancing writing with other responsibilities and deadlines can be tricky, so I try to set small goals or specific times to write to keep myself on trackWhat is the next thing you hope to achieve in your career?The next big thing I hope to achieve is launching my own news publication and bringing it into the public eye on the internet.I want to create a platform where important stories and perspectives can reach a wide audience, and where I can combine my writing skills with a larger vision. It’s exciting to think about building something from the ground up that informs, engages, and connects people onlineWow, that’s admirable. Now, something more casual: What is your guilty pleasure of choice?Honestly, my guilty pleasure has to be Microsoft Flight Simulator. It’s a bit geeky, but I love the challenge of flying planes and navigating realistic airports.There’s something oddly satisfying about managing everything from takeoff to landing, and exploring real-world landscapes from above feels like a mix of gaming and real-life adventure. I can get completely lost in it for hours—it’s my little nerdy escape.“I really enjoy camping and fishing. I love setting up a campsite, cooking over a fire, and just enjoying the quiet.I’ve been planning to create a viral article for HackerNoon that digs into the behind‑the‑scenes realities of the tech world—the kinds of things people don’t usually talk about openly. It’s not about attacking anyone, but about shining a light on the culture, the pressures, and the practices that often get glossed overI really like HackerNoon as a writing platform because it gives me a space to share my thoughts without holding back. It’s great for exploring ideas, sharing opinions, and even testing out the more unconventional things I’m curious about.Honestly, sometimes I feel like HackerNoon is the perfect mix of serious tech talk and a place where my inner nerd can run wild… preferably without accidentally breaking the internet!”“I just want to give a big shout-out to HackerNoon for creating such an amazing platform. It’s awesome how it brings together like-minded writers and tech nerds to share ideas and stories. I’m really grateful to be part of this community]]></content:encoded></item><item><title>Giga Computing R284-A92-AAL1: A Reliable 2U Rack Server For Intel Xeon 6900 Series</title><link>https://www.phoronix.com/review/giga-computing-r284</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 19:10:01 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Over the past two months I have been publishing a number of fresh benchmarks of the Intel Xeon 6980P "Granite Rapids" flagship processor performance under Linux. All of those new Xeon 6900 series benchmarks on Phoronix have been from the Giga Computing R284-A92-AAL1 2U rack server that has proven to be a very robust and reliable server platform.]]></content:encoded></item><item><title>USPTO Wants To Make Bad Patents Unchallengeable. You Have Until December 2nd To Tell Them No.</title><link>https://www.techdirt.com/2025/11/25/uspto-wants-to-make-bad-patents-unchallengeable-you-have-until-december-2nd-to-tell-them-no/</link><author>Mike Masnick</author><category>tech</category><pubDate>Tue, 25 Nov 2025 18:53:35 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The US Patent Office is about to gut the only effective tool we have for killing bad patents—and they’re doing it by administrative fiat, without Congress, in a way that would make patent trolling easier than it’s been in over a decade. They need to hear from you by Tuesday, December 2nd, or this may become reality.Patent trolling was really holding back all sorts of innovation in the 2000s. As soon as any company started to become successful, trolls would come out of the wood work, holding terrible, vague, highly questionable patents, and demand a payoff. In 2011, Congress finally gave us the America Invents Act, which created inter partes review (IPR)—a process that lets anyone challenge a granted patent at the USPTO and ask them to review whether it should have been approved in the first place.IPR was necessary because patent examination is fundamentally broken. It’s not adversarial, examiners have limited time and resources to investigate prior art, and applicants can keep resubmitting until someone approves their application. Patent examiners were even being dinged for taking too long reviewing patents, which created perverse incentives to just rubber-stamp applications to clear the queue. The result: a shit-ton of bad patents that never should have been granted.The IPR process finally began to move the patent landscape back in the right direction, allowing anyone to kick off a review of a patent to see if it had been improperly granted. If it was validly granted, then the IPR process would prove that, in effect strengthening the patent. But if the patent was nonsense, then the IPR process could invalidate it. And it’s done that for a bunch of terrible patents that never should have been granted.Patent trolls have hated IPR from day one, and they’ve fought it relentlessly. They challenged the process all the way to the Supreme Court—and they lost. Then they tried again. And they lost again. Losing twice at SCOTUS would stop most campaigns, but the troll lobby just pivoted: for years they’ve been pressuring Congress to change the law and kill IPR outright.Having failed in court and Congress, the troll lobby is now getting the Patent Office to do their dirty work through administrative rulemaking. This started two years ago under Biden, but patent trolling is bipartisan and the Trump USPTO is pushing the same approach. They can’t eliminate IPR without Congress, so instead they’re proposing rules that would make it functionally useless.Here’s what they want to do, and why each piece is bonkers:, IPR would only be available if the patent isn’t being litigated. Which means all a troll needs to do to block IPR is… file a lawsuit. You’ve just created a massive incentive for more patent troll litigation. The cheaper, faster administrative process gets shut down the moment the troll decides to sue someone—which is exactly what trolls do., if you file an IPR, you forfeit your right to challenge that patent in court later. Think about the game theory here: a troll threatens to sue you. You try to use IPR to kill the bad patent. But now, if the troll decides to sue you, you’ve permanently given up your ability to challenge the patent as part of your defense! The troll has everything to gain and nothing to lose by forcing you into this choice. Again, more incentive to sue., if  challenges a patent in any process and it survives, no one can ever challenge it again. Ever. New prior art surfaces five years later proving the patent is garbage? Too bad. Someone else has better evidence? Doesn’t matter. One shot, that’s it. This effectively gives bad patents a shield of invincibility after a single successful defense.There’s a comment period open right now (it ends Tuesday, December 2nd—the form says November 17th but it’s been extended), and the USPTO needs to hear why these rules are insane.Your comment doesn’t need to be long or use fancy legal language. Just explain why you think it’s a bad idea to create rules that give patent trolls more incentive to sue, or that force defendants to forfeit their legal rights, or that make bad patents unable to be challenged forever. As , the important thing is that actual users and creators—not just lawyers and lobbyists—show up in the record.EFF provides a sample comment if you want a starting point:I oppose the USPTO’s proposed rule changes for inter partes review (IPR), Docket No. PTO-P-2025-0025. The IPR process must remain open and fair. Patent challenges should be decided on their merits, not shut out because of legal activity elsewhere. These rules would make it nearly impossible for the public to challenge bad patents, and that will harm innovation and everyday technology users.But honestly, you can make it even more specific. Pick one of the three perverse incentives above and explain why it’s backwards. Or talk about what happened before IPR existed, when trolls were shaking down every successful startup they could find.IPR works. Don’t let the PTO kill it off to appease the troll lobby.EFF points out how important this is as well:IPR hasn’t ended patent trolling. But when a troll waves a bogus patent at hundreds or thousands of people, IPR is one of the only tools that can actually fix the underlying problem: the patent itself. It dismantles abusive patent monopolies that never should have existed,   saving entire industries from predatory litigation. That’s exactly why patent trolls and their allies have fought so hard to shut it down. They’ve failed to dismantle IPR in—and now they’re counting on the USPTO’s own leadership to do it for them.The USPTO is doing all this without congressional approval. If they really want to rewrite IPR rules, they should ask Congress to conduct a full review and pass a law. Instead, the PTO is trying to rewrite the laws through administrative fiat, betting that no one will notice or care enough to stop them.]]></content:encoded></item><item><title>Find Your Grind raises $5M to grow platform empowering students to explore unique career paths</title><link>https://techcrunch.com/2025/11/25/find-your-grind-raises-5m-to-grow-platform-empowering-students-to-explore-unique-career-paths/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Tue, 25 Nov 2025 18:49:20 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Nick Gross says that Find Your Grind gives students the tools to explore what careers are out there and what paths suit their strengths, interests, and visions for what they want their futures to look like.]]></content:encoded></item><item><title>Daily Deal: The JavaScript DOM Game Developer Bundle</title><link>https://www.techdirt.com/2025/11/25/daily-deal-the-javascript-dom-game-developer-bundle-6/</link><author>Daily Deal</author><category>tech</category><pubDate>Tue, 25 Nov 2025 18:48:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The JavaScript DOM Game Developer Bundle has 8 courses to help you master coding fundamentals. Courses cover JavaScript DOM, Coding, HTML 5 Canvas, and more. You’ll learn how to create your own fun, interactive games. It’s on sale for $30.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>CISA Warns Spyware Crews Are Breaking Into Signal and WhatsApp Accounts</title><link>https://yro.slashdot.org/story/25/11/25/1816245/cisa-warns-spyware-crews-are-breaking-into-signal-and-whatsapp-accounts?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 18:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: CISA has warned that state-backed snoops and cyber-mercenaries are actively abusing commercial spyware to break into Signal and WhatsApp accounts, hijack devices, and quietly rummage through the phones of what the agency calls "high-value" users. 

In an alert published Monday, the US government's cyber agency said it's tracking multiple miscreants that are using a mix of phishing, bogus QR codes, malicious app impersonation, and, in some cases, full-blown zero-click exploits to compromise messaging apps which most people assume are safe. 

The agency says the activity it's seeing suggests an increasing focus on "high-value" individuals -- everyone from current and former senior government, military, and political officials to civil society groups across the US, the Middle East, and Europe. In many of the campaigns, attackers delivered spyware first and asked questions later, using the foothold to deploy more payloads and deepen their access.]]></content:encoded></item><item><title>Here&apos;s the Video for Our Seventh FOIA Forum: Flock</title><link>https://www.404media.co/heres-the-video-for-our-seventh-foia-forum-flock/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/11/CleanShot-2025-11-25-at-09.18.45@2x.png" length="" type=""/><pubDate>Tue, 25 Nov 2025 18:04:00 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[The FOIA Forum is a livestreamed event for paying subscribers where we talk about how to file public records requests and answer questions. If you're not already signed up, please consider doing so here. Recently we had a FOIA Forum where we focused on our reporting about Flock. This includes how to file public records requests for audit logs, footage, and other ideas for FOIAing surveillance companies. We showed subscribers how we got the records behind that story, the specific request language was used, tips for turning records into articles, and much more.Check out all of our FOIA Forum archives here. And the video is below.]]></content:encoded></item><item><title>Mumbai Families Suffer As Data Centers Keep the City Hooked on Coal</title><link>https://hardware.slashdot.org/story/25/11/25/175213/mumbai-families-suffer-as-data-centers-keep-the-city-hooked-on-coal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 18:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Two coal plants in Mumbai (in India) that were scheduled to close last year continue operating after the state government of Maharashtra reversed shutdown decisions in late 2023 and extended the life of at least one facility by five years. The largest single factor the Indian conglomerate Tata cited in its petition for an extension was increased energy demand from data centers. 

The Guardian reports that Amazon operated 16 data centers in Mumbai last year. The company's official website lists three "availability zones" for the city. Amazon's Mumbai colocation data centers consumed 624,518 megawatt hours of electricity in 2023. That amount could power over 400,000 Indian households for a year. Residents of Mahul live a few hundred metres from one coal plant. Earlier this year doctors found three tumours in the brain of a resident's 54-year-old mother. Studies show people who live near coal plants are much more likely to develop cancer. By 2030 data centers will consume a third of Mumbai's energy, according to Ankit Saraiya, chief executive of Techno & Electric Engineering. Amazon's colocation data centers in Mumbai bought 41 diesel generators as backup. A report in August by the Center for Study of Science, Technology and Policy identified diesel generators as a major source of air pollution in the region.]]></content:encoded></item><item><title>NTFSPLUS Driver Updated As It Works Toward The Mainline Kernel</title><link>https://www.phoronix.com/news/NTFSPLUS-v2</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 17:40:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Announced last month was the NTFSPLUS driver as a new NTFS file-system driver for the Linux kernel with better write performance and more features compared to the existing NTFS options. A second iteration of that driver was recently queued into "ntfs-next" raising prospects that this NTFSPLUS driver could soon attempt to land in the mainline Linux kernel...]]></content:encoded></item><item><title>James Comey, Letitia James Indictments Tossed Because Trump’s Insurance Lawyer Wasn’t Legally Appointed To The DOJ</title><link>https://www.techdirt.com/2025/11/25/james-comey-letitia-james-indictments-tossed-because-trumps-insurance-lawyer-wasnt-legally-appointed-to-the-doj/</link><author>Tim Cushing</author><category>tech</category><pubDate>Tue, 25 Nov 2025 17:35:35 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[The problem with shedding this much talent is that you need to replace it with someone capable of fogging a mirror while also being a MAGA loyalist. When attrition outpaces the appointment process, things get sloppy. They also get , which is something Trump and his administration are being reminded of constantly as they pitch competent people overboard and hastily replace them with people Trump likes… or at least has  of. Lindsey Halligan — best known for being Trump’s insurance law specialist before being gifted with an entirely unearned US Attorney position — is now making a play for being the worst Lindsey in Trump’s orbit. (Lindsey Graham has his work cut out for him.)The DOJ continues to exit grand juries with ham sandwiches deemed unfit for judicial consumption. Halligan beat the odds by managing to secure an extremely questionable indictment of the FBI director Trump fired nearly  for not being loyal enough, as well as some risible bullshit about alleged mortgage fraud supposedly committed by New York district attorney Letitia James, who has angered Trump on multiple occasions for refusing to expose her underside every time Trump starts barking incoherently.There are multiple problems with both cases, but especially with the Comey indictment, which has already been botched repeatedly by insurance lawyer Lindsey Halligan. While we have discussed some of that, those discussions may not ultimately matter. The most recent ruling handed down by a judge overseeing the judicial district in which Halligan is repeatedly fucking things up says we don’t even need to discuss the merits of the criminal cases being brought by Trump’s vindictive DOJ. The only thing we need to discuss is whether or not Halligan actually has the legal power to serve as a DOJ prosecutor. As Chris Geidner points out on Bluesky, the answer is “no.” And that goes for  cases. As explained below, I agree with Mr. Comey that the Attorney General’s attempt to install Ms. Halligan as Interim U.S. Attorney for the Eastern District of Virginia was invalid. And because Ms. Halligan had no lawful authority to present the indictment, I will grant Mr. Comey’s motion and dismiss the indictment without prejudice.History rhymes, even when it’s being made on the same day. Here’s the point that ultimately matters in the Letitia James prosecution: As explained below, I agree with Ms. James that the Attorney General’s attempt to install Ms. Halligan as Interim U.S. Attorney for the Eastern District of Virginia was invalid. And because Ms. Halligan had no lawful authority to present the indictment, I will grant Ms. James’s motion and dismiss the indictment without prejudice.The problem with both cases is Lindsey Halligan. The law does give the executive branch an opportunity to generate a steady stream of nominees for open positions. But if the Senate doesn’t actually approve a replacement within 120 days, the approval process is turned over to the district courts, who can place their own nominees into vacant positions. The clock doesn’t reset just because Trump decided the best way to handle this was to jump from nominee to nominee for months in a row. The 120-day clock begins with the first appointee. And if that person isn’t confirmed by the Senate, the courts get their chance to fill the void.None of that happened here. Trump kept bypassing the legal process, shoving successive loyalists into this vacancy in hopes of staying a step ahead of this eventual failure. It’s now caught up with the administration, which now sees (for the time being) two of its most high-profile revenge efforts kicked to the curb by the district court.Obviously, the administration will be asking both the appellate court and the Supreme Court (if needed) to pretend the only thing that matters is that Trump gets what he wants, no matter what actions he takes to achieve these goals.Also, the dismissals are both “without prejudice,” which means the DOJ can get back to Trump’s dirty revenge business at any point in the future if it can manage to get one of its loyalists approved for public sector employment. But the clock continues to run. While Trump may have another three years of Oval Office occupation ahead of him, the 2020 testimony the DOJ claims was filled with legally actionable lies by James Comey is on the cusp of its expiration date. The statute of limitations has a pretty good chance of tolling before Trump can resume his vindictive prosecution of the former FBI director.If the administration is smart, it will understand its Hail Mary prosecutions of Trump’s political enemies have been batted down in the end zone by the system of checks and balances. But it isn’t, so it’s safe to assume efforts even more stupid and incompetent than those currently being mismanaged into the judicial graveyard by AUSA Bratz doll Lindsey Halligan will continue as long as Trump or any successive member of this MAGA-cooked version of the GOP still hold power.]]></content:encoded></item><item><title>Nvidia Claims &apos;Generation Ahead&apos; Advantage After $200 Billion Sell-off on Google Fears</title><link>https://slashdot.org/story/25/11/25/1728213/nvidia-claims-generation-ahead-advantage-after-200-billion-sell-off-on-google-fears?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 17:28:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Nvidia pushed back against investor concerns about Google's competitive positioning in AI on Tuesday after the chipmaker's shares tumbled 4.4% and erased nearly $200 billion in market cap on fears that Alphabet's tensor processing units were gaining ground against its dominance in AI computing. The company said it was "delighted by Google's success" but asserted that it continues to supply chips to Google. 

Nvidia said it remains "a generation ahead of the industry" as the only platform that runs every AI model and operates everywhere computing is done. The statement came after investors reacted to the release of Google's Gemini 3 large language model last week. The model was trained using TPUs rather than Nvidia chips. A report in The Information on Monday said Google was pitching potential clients including Meta on using TPUs in their data centers rather than Nvidia's chips. 

Nvidia said its platform offers "greater performance, versatility, and fungibility than ASICs," referring to application-specific integrated circuits like Google's TPUs that are designed for specific AI frameworks or functions. Google's TPUs have until now only been available for customers to rent through its cloud computing service. Nvidia has lost more than $800 billion in market value since it peaked above $5 trillion less than a month ago.]]></content:encoded></item><item><title>Microsoft’s AI chatbot Copilot leaves WhatsApp on January 15</title><link>https://techcrunch.com/2025/11/25/microsofts-ai-chatbot-copilot-leaves-whatsapp-on-january-15/</link><author>Sarah Perez</author><category>tech</category><pubDate>Tue, 25 Nov 2025 17:11:55 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[WhatsApp's new platform policies ban general-purpose AI chatbots like Copilot from using its service. ]]></content:encoded></item><item><title>China’s Pony AI plans to triple global robotaxi fleet by the end of 2026</title><link>https://techcrunch.com/2025/11/25/chinas-pony-ai-plans-to-triple-global-robotaxi-fleet-by-the-end-of-2026/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Tue, 25 Nov 2025 16:54:57 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Chinese autonomous vehicle company Pony.ai has global ambitions and plans to build a 3,000-strong robotaxi fleet to meet them. ]]></content:encoded></item><item><title>Evidence from the One Laptop per Child Program in Rural Peru</title><link>https://tech.slashdot.org/story/25/11/25/1642202/evidence-from-the-one-laptop-per-child-program-in-rural-peru?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 16:42:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The abstract of a paper on NBER: This paper examines a large-scale randomized evaluation of the One Laptop Per Child (OLPC) program in 531 Peruvian rural primary schools. We use administrative data on academic performance and grade progression over 10 years to estimate the long-run effects of increased computer access on (i) school performance over time and (ii) students' educational trajectories. Following schools over time, we find no significant effects on academic performance but some evidence of negative effects on grade progression. Following students over time, we find no significant effects on primary and secondary completion, academic performance in secondary school, or university enrollment. Survey data indicate that computer access significantly improved students' computer skills but not their cognitive skills; treated teachers received some training but did not improve their digital skills and showed limited use of technology in classrooms, suggesting the need for additional pedagogical support.]]></content:encoded></item><item><title>Fleet Space finds massive lithium deposit using AI and satellites</title><link>https://techcrunch.com/2025/11/25/fleet-space-finds-massive-lithium-deposit-using-ai-and-satellites/</link><author>Tim De Chant</author><category>tech</category><pubDate>Tue, 25 Nov 2025 16:14:33 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The discovery suggests that a region in Quebec might hold even more lithium than originally suspected.]]></content:encoded></item><item><title>Adolescence Lasts Into 30s - New Study Shows Four Pivotal Ages For Your Brain</title><link>https://science.slashdot.org/story/25/11/25/168211/adolescence-lasts-into-30s---new-study-shows-four-pivotal-ages-for-your-brain?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 16:08:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The brain goes through five distinct phases in life, with key turning points at ages nine, 32, 66 and 83, scientists have revealed. From a report: Around 4,000 people up to the age of 90 had scans to reveal the connections between their brain cells. Researchers at the University of Cambridge showed that the brain stays in the adolescent phase until our early thirties when we "peak." They say the results could help us understand why the risk of mental health disorders and dementia varies through life. The brain is constantly changing in response to new knowledge and experience -- but the research shows this is not one smooth pattern from birth to death. 

Some people will reach these landmarks earlier or later than others -- but the researchers said it was striking how clearly these ages stood out in the data. These patterns have only now been revealed due to the quantity of brain scans available in the study, which was published in the journal Nature Communications.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Teaching Ethnography to Software Engineers (11/25/2025)</title><link>https://hackernoon.com/11-25-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Tue, 25 Nov 2025 16:02:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, November 25, 2025?By @hackernoon-courses [ 10 Min read ] Meet the expert faculty behind the HackerNoon Blogging Fellowship and learn how their mentorship helps writers grow, gain authority, and master modern content. Read More.By @toptechcompanies [ 1 Min read ] Meta is accused of shelving internal research that found Facebook use caused increased depression and anxiety, U.S. court filings show. Read More.By @MichaelJerlis [ 4 Min read ] A firsthand look at why crypto off-ramps are painful — and how the EMCD Payment Card turns spending USDT into a simple, instant, real-world experience.  Read More.By @ethnography [ 7 Min read ] A practical introduction to ethnography for software engineering students, covering methods, examples, teaching tips, and how to study real developer practices. Read More.By @product [ 4 Min read ] Pixel Icon Library launches paid plans. Free option stays - or upgrade to Starter/Pro for no-attribution use. 30K downloads. Lifetime access. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>They Are Real, and They Are Here - Meet The HackerNoon Blogging Course Faculty</title><link>https://hackernoon.com/they-are-real-and-they-are-here-meet-the-hackernoon-blogging-course-faculty?source=rss</link><author>HackerNoon Courses</author><category>tech</category><pubDate>Tue, 25 Nov 2025 16:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Hey Hackers! \n  \n Let’s be honest. The digital world moves too fast for dusty playbooks. The "best practices" from a few years ago - heck, even a few months ago - are today's digital fossils. You don't need another course taught from an ivory tower; you need skills from the front lines. You need knowledge that was pressure-tested .\
Welcome to a course taught by the doers of the digital space. A course in which instructors are people who actively empower their own niche of audience in their own corner of the vast interweb. Our faculty won’t make you memorize concepts (well, maybe just some), but rather our main mission here is to help you adopt the workflows of a team that lives and breathes modern content - a team that:Edit and publish millions of words a year, mastering the art of storytelling that creates engagement and inspires actions.Run growth and SEO for a site that gets millions of views, decoding the algorithms that decide who wins the attention war.Build content systems for the world’s top tech companies, architecting strategies that turn words into measurable results.Leverage generative AI for real-world storytelling, using it as a creative partner, not a crutch.This is our daily grind. And we're about to make it your new skillset.Meet the Minds Behind the MagicGet to know the strategists, editors, and innovators who will be guiding your amazing journey ahead:The beautiful people at the heart of HackerNoon. You'll be learning directly from the people who run the platform, build the audience, and define what a great tech story looks like today.12+ years building one of the web's leading independent tech publishing platform HackerNoon (300M+ readers, 45k+ writers, $2.9M raised), with prior experience in content marketing, social media, and journalism. David was also one of the first 5 hires of SmartRecruiters ($1.5B valuation as of 2023)14+ years scaling one of the web's leading tech media platform HackerNoon, following prior leadership roles in global education (Minerva University, UWC Vietnam), youth innovation (CKP Vietnam), and pedagogy research at Brown University.6+ years at HackerNoon leading sales, partnerships, and business development, alongside prior roles in Web3 journalism; Utsav brings a unique blend of growth strategy, editorial expertise, and technical communication.5+ years across fintech, video production, and tech publishing, developing editorial strategy and leading digital marketing campaigns. Skilled at blending narrative and execution to connect solutions with the people they’re built for.12+ years of journalism and communications experience, previously leading content, editorial teams, and corporate messaging at S&P Global across energy, healthcare, and mining sectors.8+ years of experience spanning marketing, communications, and operations, currently driving growth at HackerNoon while building on prior roles in digital marketing, content strategy, and tourism operations across Portugal and the UK.3+ years at HackerNoon in editorial, support, and operations roles, bringing expertise in content management, customer support, and organizational efficiency, building on early experience in technical support at the University of Colorado Boulder.Specialist Guest SpeakersThe industry leaders and specialists who have built incredible brands, companies, and audiences. They’ll provide masterclasses in their areas of expertise, giving you outside perspectives from the top of the tech and content worlds.10+ years' experience spanning editorial leadership, SEO strategy, and brand growth across tech media (HackerNoon, KDnuggets), corporations and startups (TaskUs, Gengo, ISNation), and pop culture journalism (JPbound, GaijinPot, Valnet).A Senior Technical Consultant at ServiceNow specializing in AI, Saaniya Chugh simplifies complex tech concepts as a HackerNoon contributor and author. A recognized advocate for women in tech, she leads community meetups in Montreal and Toronto and is passionate about mentoring the next generation for an AI-driven world.What I love about HackerNoon is how it creates a space where tech isn’t just explained, it’s experienced. Writing here feels like being part of a global conversation - one that’s bold, curious, and never afraid to simplify complexity. The Fellowship builds on that spirit: it’s not just about teaching writing, it’s about growing together as creators, experimenting with AI, and finding your authentic voice. If you’ve ever wanted to sharpen your craft, share your ideas with the world, and join a supportive community of writers and editors, this course is where you begin. I’m excited to be part of it, and I’d love to see you join us too!10+ years of experience in brand and content marketing, editorial leadership, and communications, spanning global roles at Bitvavo, Product Hunt, TNW, HackerNoon, and GetSmarter, with foundations in journalism at Cosmopolitan and The Cape Times.10+ years of experience in content marketing and community building, Amy has led brand campaigns, podcasts, partnerships, and editorial strategy across security automation, SaaS, and tech publishing for startups and global enterprises alike.As a writer, speaker, and security consultant, James specializes in translating complex jargon into useful language, often focusing on the tech industry's self-made problems in his published books and articles. A passionate mentor, James helps new writers find their voice, teaching that sincerity is the key to creating work that lasts.The HackerNoon Fellowship gave me structure and accountability to actually write regularly, with feedback that sharpened my style. It was not about chasing clicks or pushing content (though how to get clicks was definitely covered), it was about writing clearly, learning from editors, and being part of a community that genuinely cares about good ideas. I came out of it with a stronger voice, a portfolio of work, and connections I would not have found otherwise. I have recommended it to plenty of people since, because it is one of the most accessible programmes I've found to help writers improve.15+ years of experience in journalism and editorial leadership, spanning roles at The Wall Street Journal, S&P Global, MT Newswires, CryptoSlate, and HackerNoon, Zaeem has built deep expertise in breaking news, financial and business reporting, team management, and data-driven editorial strategy.10+ years spanning developer relations, fintech growth, and software engineering, Adebola has scaled teams from 5 to 70+, driven $5M+ in loan disbursements, built Web3/AI developer ecosystems at Tezos and Alloy, and delivered impactful technical advocacy, tools, and content that fueled multimillion-dollar pipelines.Celine Aju is a Project Manager and learning designer focused on building inclusive systems that help people thrive. She blends strategy with empathy to create operations that are both efficient and people-centered. As a writer on education and social equity, her 20+ articles have reached over 20,000 readers, reflecting her commitment to designing work and learning experiences that honor diversity.The HackerNoon Blogging Fellowship was a fantastic way to launch my writing journey. It opened my eyes to the many pathways a writing career can take and gave me the tools to explore them with confidence. My favorite part was the biweekly workshops, where writers shared their processes—these sessions expanded my perspective, introduced me to new styles, and helped me design a writing process I’m proud of. The fellowship also pushed me to reimagine my chosen niche from fresh angles, shaping both my growth and evolution as a writer.5+ years in product development, consulting, and digital strategy, Elhadj combines technical insight with practical execution. He has built digital products, led data and AI initiatives, and managed cross-functional work across startups and enterprises—turning complex ideas into clear, actionable plans that help teams innovate and scale.It helped me get in the groove of writing regularly with deadlines and clear topics. The feedback I got from the facilitators and the visibility and exposure it provided me on Hackernoon made it a much better experience for me instead of just writing in the void!These are your guides on the ground. Our facilitators are here to ensure you never get stuck. They’ll be in the trenches with you, answering questions, providing personalized feedback on your work, and connecting you with the resources you need to succeed.Teri is a software engineer and technical writer who explores concepts bordering around software development, data, and AI.  He loves breaking down complex topics, creating tutorials, and sharing practical tips for developers. Teri’s mission is to make learning accessible and help others grow in their tech journey.The Hackernoon blogging fellowship provided an invaluable opportunity to acquire best practices in crafting highly engaging and SEO-optimized content through the utilization of appropriate tools.With 30+ years of U.S. Postal Service communications experience, Meiko Patton is a public sector expert who now helps government leaders navigate the AI frontier. As the founder of Meiko.AI, she specializes in future-proofing federal agencies by integrating AI into their communication and resilience strategies.Host of The First-Time CEO podcast, Victoria Loskutova is a PR specialist who helps emerging tech voices find their story. As a HackerNoon writing facilitator, she excels at guiding writers to shape raw thoughts into clear, compelling narratives that resonate with a global audience.Going through the Blogging Fellowship was a turning point for me. HackerNoon gave me the space to experiment with ideas, sharpen my voice, and connect with a global community of readers and writers. What I love most is how the program turns writing into both a craft and a conversation—it’s empowering to know your words can spark thought and dialogue at scale.A veteran HackerNoon writer of five years, Amy is also the author of ebooks and poetry. She is a dedicated and helpful person who is excited to support aspiring writers in developing their own writing voice.I am a multipotentialite reader and writer who talks about neurodiversity, mental health and self care topics on hackernoon.com. Hackernoon has given me a wonderful platform to express my ideas without the pressure. I love writing and I always have, but I haven't always had a regular practice. Being able to write on hackernoon.com gave me an audience and an impetus to write regularly and with my whole heart. I started writing on hackernoon around the time of the Pandemic so it was an amazing stress relief to meet others in the blogging fellowship and to write on a platform like hackernoon.Ignatius Sani is a software engineer and technical writer who simplifies complex JavaScript topics. He is passionate about mentoring junior developers on their journey to becoming self-taught problem solvers. Through his writing on HackerNoon and other platforms, he helps fellow engineers build their skills and amplify their voices in the tech community.I loved the HackerNoon Blogging Fellowship because it gave me structure, accountability, and a supportive community. The weekly video sessions, real-time feedback, and lessons on SEO, keyword research, and backlinks helped me grow both as a writer and as a content strategist. It’s the kind of program I’d recommend to anyone serious about leveling up their writing.\Mosobalaje is a creative business story and content writer with the aim of bringing visibility as part of a digital marketing strategy.One thing that I am most grateful for is the opportunity to learn from professionals, and getting an opportunity for one-on-one mentioring. I was mentored by the almighty Limarc (former VP of Editorial). We would go over my work together, as He gives comments and recommendation; that is not something you get so frequently in many internship programs. An highlighte was ranking for certain keywords and having my content ranked even among giants in the industry. I left the program msot confident that I was ready for the world, and indeed I was and I am. One thing I'd say, is that the mentors are willing, and it is only the degree of efforts that people put into this that they'd get back. So, I encourage participants to put their best efforts.Grace is a software developer and technical writer addicted to books and her love for baking. When she’s not writing code, she’s usually writing technical docs that make complex things easy to understand. Grace also dabbles in a bit of ghostwriting as wel!This program is an absolute game-changer! If you're looking to be part of a highly supportive community and receive feedback from seasoned editors, this is definitely the right place for you. The fellowship has helped me grow as a writer by pushing me to produce high-quality content and connecting me with like-minded writers. I've learned so much about storytelling, structure, and style, and I've been able to apply these skills to my writing. I highly recommend joining this course if you want to level up your writing and become part of a vibrant community of writers. The instructors are knowledgeable and top-notch experts, and the flexibility of this course allows you to complete it at your own pace. So, what are you waiting for? Join us and take your writing to the next level!Make the HackerNoon Blogging Course Your Backstage Pass to ExcellenceWe’re handing over the keys to our newsroom: the real-time playbooks, the growth data, and the AI workflows that give us our edge. We built this to be the career cheat code we never had. It’s time to get your work noticed.]]></content:encoded></item><item><title>Speechify adds voice typing and voice assistant to its Chrome extension</title><link>https://techcrunch.com/2025/11/25/speechify-adds-voice-typing-and-voice-assistant-to-its-chrome-extension/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Tue, 25 Nov 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In the last 12 months, there has been a proliferation of voice detection tools, thanks to overall quality improvement in speech recognition models. Speechify is hitching its wagon to this train and launching its own dictation tool with support for English.]]></content:encoded></item><item><title>The Easiest Way to Integrate Coz.jp Into Your Workflows: Exploring the n8n Node</title><link>https://hackernoon.com/the-easiest-way-to-integrate-cozjp-into-your-workflows-exploring-the-n8n-node?source=rss</link><author>Daniel, Andrei-Daniel Petrica</author><category>tech</category><pubDate>Tue, 25 Nov 2025 15:59:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Improved Upstream Kernel Support For TUXEDO Laptops Being Worked On</title><link>https://www.phoronix.com/news/Uniwill-TUXEDO-Add-On-Patches</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 15:47:14 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While TUXEDO Computers recently ended their efforts for a Snapdragon X Elite Linux laptop, their Linux Intel/AMD laptop efforts continue going well and recently they have been posting patches working to enhance the upstream kernel support for those x86_64 devices...]]></content:encoded></item><item><title>V1 Protocol Launch in Q4, New Crypto Mutuum Finance (MUTM) With a Confirmed Product Timeline</title><link>https://hackernoon.com/v1-protocol-launch-in-q4-new-crypto-mutuum-finance-mutm-with-a-confirmed-product-timeline?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Tue, 25 Nov 2025 15:29:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most crypto presales struggle to deliver, leaving investors frustrated and wary. Nearly 90% of presales never release a working product on time.  is different. Its V1 of the protocol is planned for Sepolia Testnet in Q4 2025, making the token highly attractive for pre-launch investors seeking real utility. This confirmed timeline separates MUTM from other presales and creates a unique opportunity for crypto investing enthusiasts searching for the next crypto to buy now.Mutuum Finance (MUTM) presale highlights the urgency. The total supply is 4 billion MUTM, with over $18.95 million raised across all phases and there are more than 18,200 holders. The current price sits at $0.035, Phase 6 allocation of 170 million MUTM is 95% sold, and the next presale price will rise to $0.040. Recently, card purchases are now enabled with no limits, making entry easier than ever. Investors are acting fast as Phase 6 nears full allocation, signaling strong demand and limited time to secure tokens before the price increase.A numerical example demonstrates the potential. An investor who purchased $6,000 in Phase 4 at $0.025 acquired 240,000 MUTM. At today’s price of $0.035, the investment is already valued around $8,400. Once MUTM reaches $1, the same stack will grow to $240,000, illustrating how early participation aligns with long-term gains.Mutuum Finance (MUTM) is building a new digital lending infrastructure designed to maximize token utility. The platform will combine transparent mechanics with a working DeFi product in development. MUTM is not just a token; it will drive real activity through lending, borrowing, and staking features. The pre-launch stage allows investors to acquire tokens ahead of the platform’s full functionality, capturing early-stage upside while the ecosystem grows.The platform offers two lending models. Peer-to-Contract (P2C) lending uses liquidity pools where users receive mtTokens representing their positions. Borrowing options are tracked with debt tokens, while a liquidator bot maintains protocol safety. ETH and USDT are the initial supported assets, ensuring stable liquidity and predictable lending outcomes. Peer-to-Peer (P2P) lending allows lenders and borrowers to negotiate loans directly, which is ideal for more volatile tokens. P2P offers higher returns for lenders, creating an attractive mechanism for investors seeking yield beyond traditional DeFi channels.Why the V1 Timeline is a Game Changer and Other Growth FactorsMutuum Finance (MUTM) announced through its official X account that the V1 edition of its protocol is  on the Sepolia Testnet in Q4 2025. This first rollout will activate the platform’s essential components, including the liquidity pool, the mtToken and debt token frameworks, and an automated liquidator bot designed to protect collateral positions and maintain system stability. During this phase, users will be able to lend, borrow, and provide ETH or USDT as collateral within the protocol.Introducing V1 on the testnet grants the community early access to the platform before its mainnet expansion. This structured release promotes transparency, encourages initial user engagement, and enables the team to gather meaningful feedback for ongoing improvements. As more participants explore the testnet and interact with its features, overall interest in the ecosystem may grow, supporting stronger long-term confidence in the MUTM token.Later on, the platform is expected to launch simultaneously with the token launch, generating immediate utility, allowing lending, borrowing, and staking from day one.A working product will also attract attention from Tier-1 and Tier-2 exchanges, increasing visibility and accelerating adoption. For investors, this timeline provides clarity and confidence rarely seen in presales, reinforcing MUTM’s position as a leading choice for crypto investing.Future expansion includes a decentralized $1-pegged stablecoin. Users will be able to borrow against ETH, SOL, and AVAX, with over-collateralized designs ensuring stability. Minting and repaying this stablecoin will increase MUTM usage, creating ongoing demand and enhancing the ecosystem’s long-term value. The stablecoin mechanism aligns with Mutuum Finance (MUTM)’s goal to maintain utility-driven growth, giving early investors a tangible pathway to returns.The platform will also include a buyback and dividend logic. Part of the lending revenue will be used to buy back MUTM tokens, which will then be distributed to mtToken stakers. This mechanism generates continuous buy pressure, supporting token appreciation while rewarding long-term participants. The system grows stronger as adoption increases, reinforcing a self-sustaining cycle that enhances both utility and value.Mutuum Finance (MUTM) has an active and growing community with over 12,000 Twitter followers. Daily rewards of $500 in MUTM further incentivize engagement.The platform is also running an , where 10 winners will receive $10,000 in MUTMs each, generating excitement and promoting widespread participation.Mutuum Finance (MUTM) stands out as one of the few presales with a fully confirmed product timeline. The V1 Testnet will launch on Sepolia in Q4 2025, and the platform and token will go live together, offering immediate utility and active token use. Phase 6 is almost sold out, and the price will increase from $0.035 to $0.040, creating a final window for investors. With confirmed development, utility, staking rewards, stablecoin expansion, and buyback mechanisms, Mutuum Finance (MUTM) represents an unmatched opportunity for investors seeking the next crypto to buy now. Urgency is high, and the time to participate in this presale is limited.For more information about Mutuum Finance (MUTM) visit the links below::::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Nix Package Tool Approved For Availability In Fedora 44</title><link>https://www.phoronix.com/news/Fedora-44-Nix-Package-Tool</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 15:27:04 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following approval of the /nix top-level directory with Fedora Linux, the Fedora Engineering and Steering Committee (FESCo) has additionally signed off on allowing the Nix package tool to appear in the Fedora 44 repository...]]></content:encoded></item><item><title>Unpowered SSDs in Your Drawer Are Slowly Losing Data</title><link>https://hardware.slashdot.org/story/25/11/25/1511242/unpowered-ssds-in-your-drawer-are-slowly-losing-data?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 15:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Solid-state drives sitting unpowered in drawers or storage can lose data over time because voltage gradually leaks from their NAND flash cells, and consumer-grade drives using QLC NAND retain data for about a year while TLC NAND lasts up to three years without power. More expensive MLC and SLC NAND can hold data for five and ten years respectively. The voltage loss can result in missing data or completely unusable drives. 

Hard drives remain more resistant to power loss despite their susceptibility to bit rot. Most users relying on SSDs for primary storage in regularly powered computers face little risk since drives typically stay unpowered for only a few months at most. The concern mainly affects creative professionals and researchers who need long-term archival storage.]]></content:encoded></item><item><title>Spotify to raise US prices in first quarter of next year, report says</title><link>https://techcrunch.com/2025/11/25/spotify-to-raise-us-prices-in-first-quarter-of-next-year-report-says/</link><author>Aisha Malik</author><category>tech</category><pubDate>Tue, 25 Nov 2025 15:15:49 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Major record labels have been urging Spotify and other music streaming platforms to raise subscription prices, arguing that fees have not kept pace with inflation and remain low when compared to services like Netflix.]]></content:encoded></item><item><title>Quantum Sensors Head for Space</title><link>https://spectrum.ieee.org/quantum-sensors-space</link><author>Charles Q. Choi</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIzMzAwOS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgxOTEyNzU3OX0.v2C1dFwfzukJbSyLAVbxnJLi23SkQb4Bhp2vUH68hjI/image.jpg?width=600" length="" type=""/><pubDate>Tue, 25 Nov 2025 15:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Magnetometers could improve navigation and monitor climate change]]></content:encoded></item><item><title>Top Web Scraping Tools You Should Use in 2025</title><link>https://hackernoon.com/top-web-scraping-tools-you-should-use-in-2025?source=rss</link><author>Oxylabs</author><category>tech</category><pubDate>Tue, 25 Nov 2025 14:59:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This 2025 guide explores the top web scraping tools—no-code scrapers, developer frameworks, AI-powered extractors, and enterprise APIs. It breaks down key features, challenges, compliance concerns, and how to choose the right solution based on skill level, project scale, website complexity, and budget.]]></content:encoded></item><item><title>Particle Physicists Detect ‘Magic’ at the Large Hadron Collider</title><link>https://www.quantamagazine.org/particle-physicists-detect-magic-at-the-large-hadron-collider-20251125/</link><author>Shalma Wegsman</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2025/11/Particle-Physics-Magic-cr-Celsius-Pictor-Default.webp" length="" type=""/><pubDate>Tue, 25 Nov 2025 14:59:52 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[Ninety million times a year, when protons crash together at the Large Hadron Collider (LHC), they produce, in their wreckage, a top quark and an anti-top quark, the heaviest known elementary particles. In the trillionth of a trillionth of a second before the particles decay into lighter pieces, they fly apart. But they remain quantum mechanically entangled, meaning each particle’s state depends on…]]></content:encoded></item><item><title>Meet Hubstaff: HackerNoon Company of the Week</title><link>https://hackernoon.com/meet-hubstaff-hackernoon-company-of-the-week?source=rss</link><author>Company of the Week</author><category>tech</category><pubDate>Tue, 25 Nov 2025 14:55:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
We are back with another  feature! Every week, we share an awesome tech brand from our , making their evergreen mark on the internet. This unique HackerNoon database ranks S&P 500 companies and top startups of the year alike.\
This week, we are featuring  a comprehensive workforce management platform designed to empower remote and hybrid teams through precise time tracking, productivity monitoring, and automated payroll.\
By turning work hours into actionable insights, Hubstaff helps businesses streamline operations and reduce administrative burdens like invoicing and reporting.Hubstaff🤝 HackerNoon Targeted AdsHubstaff recently partnered with HackerNoon on a targeted ad campaign, advertising their “AI Productivity Shift Report”, aiming at engineering leaders, giving out free, valuable insights on scaling teams and increasing delivery speed without causing employee burnout.\
The targeted ad campaign has effectively positioned Hubstaff as a solution that replaces micromanagement with "smarter systems" that protect deep work and automate administrative tasks.Ad Placement by Content Relevancy, Explained in 7 SecondsDid you know, aside from developing effective solutions for companies and startups, Hubstaff is also running its own tech blog? With an international team of tech professionals,  is a treasure trove of resources where founders and leaders can find valuable insights on how to increase team productivity, as well as the newest trends in management, especially in the age of AI 🌟\
That's all this week, folks! Stay Creative, Stay Iconic.]]></content:encoded></item><item><title>Singapore Orders Apple, Google To Prevent Government Spoofing on Messaging Platforms</title><link>https://apple.slashdot.org/story/25/11/25/1446250/singapore-orders-apple-google-to-prevent-government-spoofing-on-messaging-platforms?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 14:46:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Singapore's police have ordered Apple and Google to prevent the spoofing of government agencies on their messaging platforms, the home affairs ministry said on Tuesday. The order under the nation's Online Criminal Harms Act came after the police observed scams on Apple's iMessage and Google Messages purporting to be from companies such as the local postal service SingPost. While government agencies have registered with a local SMS registry so only they can send messages with the "gov.sg" name, this does not currently apply to the iMessage and Google Messages platforms.]]></content:encoded></item><item><title>Please, please do our reader survey</title><link>https://www.404media.co/please-please-do-our-reader-survey/</link><author>Joseph Cox</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/11/404year2-11-copy.jpg" length="" type=""/><pubDate>Tue, 25 Nov 2025 14:42:49 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[Because we run 404 Media on Ghost, an open source and privacy-forward stack, we actually know very little about who reads 404 Media (by design). But we’re hoping to learn a bit more so we can figure out how people are discovering our work, what our readers do, and what other projects people might want us to launch in the future. If you want to cut to the chase: here is a link to our very short survey we would really, really appreciate you filling out. You can do it anonymously and it should take around a minute. If you want to know more on the , please read below!As we said, Ghost doesn’t collect much data about our readers. The little info we do have shows broadly that most of our readers are in the U.S., followed by Europe, etc. But we don’t have a great idea of how people first learn about 404 Media. Or whether people would prefer a different format to our daily newsletter. Or what industries or academic circles our readers are in.This information is useful for two main reasons: the first is we can figure out how people prefer to read us and come across our work. Is it via email? Is it articles posted to the website? Or the podcast? Do more people on Mastodon read us, or on Bluesky? This information can help us understand how to get our journalism in front of more people. In turn, that helps inform more people about what we cover, and hopefully can lead to more people supporting our journalism.The second is for improving the static advertisements in our email newsletters and podcasts that we show to free members. If it turns out we have a lot of people who read us in the world of cybersecurity, maybe it would be better if we ran ads that were actually related to that, for example. Because we don’t track our readers, we really have no idea what products or advertisements would actually be of interest to them. So, you voluntarily and anonymously telling us a bit about yourself in the survey would be a great help.Here is . There is also a section for any more general feedback you have. Please help us out with a minute of your time, if you can, so we can keep growing 404 Media sustainably and figure out what other projects readers may be interested in (such as a physical magazine perhaps?).]]></content:encoded></item><item><title>inDrive’s Approach to Measuring Engineering Performance</title><link>https://hackernoon.com/indrives-approach-to-measuring-engineering-performance?source=rss</link><author>inDrive.Tech</author><category>tech</category><pubDate>Tue, 25 Nov 2025 14:41:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[inDrive is a ride-hailing company operating in 48 countries with a unique peer-to-peer pricing model that enables fair trip costs. Across all products, more than 600 engineers work in 70+ teams.Understanding why organizations need this is essential. The main reason is that today’s market has changed. Companies now want more control and better efficiency from what they already have - their processes, people, and tools. This becomes especially important when scaling, since hiring more people alone is no longer a sustainable way to grow.And if we want to answer these hard questions, we need to define and manage metrics that help achieve the organization’s specific goals. It’s important to note that at inDrive,  is one of our core values - alongside . That’s why performance isn’t just about results; it’s part of who we are as a company. We strive to understand impact and create it at every level of the organization.We identified several key factors that led us to develop our own approach to performance:Since 2020, the company has been experiencing rapid business and engineering growth in both the number of engineers and teams.This scale creates the need for  that ensure predictable outcomes and support sustainable scaling.Leadership must rely on  to understand how teams perform within these processes, identify bottlenecks, and make informed managerial decisions.Finally, metrics must be aligned with strategic and operational goals - only then do they become drivers of meaningful progress rather than isolated statistics.It’s important to note that performance and productivity are closely connected. Productivity reflects how effectively your delivery processes work, while performance shows what outcomes they actually produce. Different companies interpret these terms and approach metric design in very different ways. Some focus mainly on individual contributor productivity, while others track metrics only at the . But I believe this complex challenge requires building a  that operates across all levels of the organization.At inDrive, I implemented a system that includes:Metrics, integrated into the tech metrics tree, which is divided into domains and levels. We’ve defined five domains - such as cost efficiency, people metrics, performance, operational and engineering excellence and each level corresponds to the company’s organizational structure: from the  (the entire tech division) to the  (teams united by a shared product or domain expertise), Team leve,l and individual contributor level. At the cluster and team levels, performance and operational efficiency are further refined into more specific areas - such as predictability, speed, maturity, and quality:The example below shows how a metric within the Performance domain cascades across different levels:: each metric has an  - a subject-matter expert responsible for defining the methodology and leading the implementation of new processes and metrics. In addition,  are accountable for performance within their teams, clusters, or division. For us, it’s essential that metrics are part of a , not delegated to dedicated roles like Agile Coaches, Service Delivery Managers, or Project Managers. Only this way can we achieve systemic and sustainable results., which enable teams at every level to systematically analyze the current state, make data-driven management decisions, and drive change across the company - through the year strategy, technology programs or joint initiatives. For example, a few years ago we launched an engineering satisfaction survey in response to metric signals that revealed issues with delivery performance. Today, beyond being a standalone metric, it has become one of our  - helping us identify and drive the changes needed to improve the efficiency of our internal processes and tools, especially those owned by .: to support this system, we have built the  - a Tableau-based dashboard ecosystem that connects all metrics and data sources into the tech metrics tree, serving as the single source of truth for performance across teams, clusters and the entire division.The Single Analytical SystemThe concept behind the Single Analytical System is straightforward - it’s a set of dashboards that bring together metrics from multiple sources (Jira, Grafana, Kibana, PagerDuty, HR systems or in-house tools) into a single one-pager view. In other words, you have a pocket-sized overview of the entire landscape. And when deeper analysis is needed, you jump directly to the underlying data source, such as a detailed dashboard in Grafana. To implement the dashboards, I designed a five-tier structure: from the division level down to the sandbox. Each tier can have its own dashboard or a family of dashboards, enabling performance analysis of a specific organizational entity or providing a cross-team or cross-service view of a particular metric.Includes key technology metrics aligned with company and divisional strategy across five domains:: cost per ride, lost money, etc.: turnover, engagement, satisfaction, etc.: mobile performance, availability, security, data quality.: DORA metrics, technical debt, etc.: time to market, lead time, completion rate, etc.One of the dashboard sections looks as follows:All metrics are shown in  to track trends over time, with  indicating whether each metric has reached its target value or not.The dashboard allows CTO and divisional leadership to assess efficiency, identify focus areas, and understand the influence of specific clusters. at this level includes all  organized within the previously defined :: goals progress, scope drop.: lead time, velocity, time to market.: incidents, SLA postmortems, security error budgets.: team maturity index: cycle time (lead time for changes), deployment frequency, change failure rate, mean time to restore.Used by Directors of Engineering/Product and CTO to improve performance and other company-wide processes such as Annual Performance Review.  Metrics are usually analyzed monthly during cluster metrics analysis.Mirrors the cluster level but with team-specific context:Predictability: sprint goal success, goals progress, scope drop.Speed: lead time, velocity, time to market.Quality: incidents, SLA outcomes, security budgets.Maturity: team maturity indexEngineering Excellence: cycle time (lead time for changes), deployment frequency, change failure rate, mean time to restore.I designed  and metrics to work for any team - whether they use Scrum or Kanban. This makes the system flexible while keeping the evaluation consistent.This is the  for Engineering managers for supporting data-driven planning, stakeholder alignment, and continuous improvement during both day-to-day operations and key events like sprint planning or retrospectives.4. Individual Contributor LevelThe dashboard includes engineer-level productivity data in five key areas, such as collaboration,  work quality, workload health, development experience and AI adoption.Used by Engineering managers during day-to-day work to maintain a high level of productivity and identify growth areas.Contains deep-dive dashboards for SMEs managing specific metrics across the organization: enabling advanced analysis and experimentation. For example, Time-to-market or team maturity level.Used by SMEs or any managers on demand for deep-dive analysis.Building a system that allows organizations to manage, evaluate, and improve engineering performance is crucial - it enables data-driven understanding of the current state and helps launch  at multiple levels.At the same time, we recognize the inherent risks of over-reliance on metrics - they can be misinterpreted or gamified. As part of the Engineering Excellence Team, I promote the mindset that  - they are  to guide management decisions. Metrics may miss context, reflect short-term fluctuations, or mislead without proper analysis. Their real value lies in comprehensive, contextual evaluation, enabling managers to answer key questions during planning, reviews, and performance discussions, and to foster a data-driven culture grounded in accountability and learning.Staff Coach, Engineering Excellence Team]]></content:encoded></item><item><title>Educational Byte: How Fake CAPTCHAs Can Steal Your Crypto</title><link>https://hackernoon.com/educational-byte-how-fake-captchas-can-steal-your-crypto?source=rss</link><author>Obyte</author><category>tech</category><pubDate>Tue, 25 Nov 2025 14:07:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We all know CAPTCHAs: those “I’m not a robot” boxes or image grids you click when logging in or browsing. They’re meant to block bots and make websites safer. But cybercriminals have started using deceptive versions. They’re fake CAPTCHAs that trick users into installing malware or giving away private data.What begins as a harmless-looking verification ends up being a gateway for crypto theft, credential harvesting, or system compromise. So, we’ll explore how those fake CAPTCHAs work, the risks they pose to your crypto, and most importantly, steps you can take to defend yourself.How the Fake Captcha Scam WorksA fake CAPTCHA is crafted to look like a normal verification step, but behind the scenes, the attackers are executing a malicious plan. You click “I’m not a robot,” and the page quietly copies a command into your clipboard. Then it prompts you to paste it somewhere (often the Windows Run box) and press Enter. That simple command  like Lumma Stealer or the Amadey Trojan, which harvest passwords, browser cookies, crypto wallet keys, and more. Not even a proper download is needed.\
Researchers have observed this tactic being embedded into  across different industries, sometimes via ads or via third-party scripts on otherwise legitimate domains. The attack often uses fileless execution, which means the malware doesn’t leave a noticeable trace on disk, making detection trickier.Once inside, the malware scans for browser-saved credentials, cookie data, two-factor tokens, and wallet files, and can quietly exfiltrate what it finds. The Amadey Trojan, in particular, also acts as a clipper: it detects crypto addresses already copied on the clipboard, and then replaces them with ones controlled by the hackers. This way, when you paste the address to send funds, it may not be your intended destination.It might sound technical, but the key is that the CAPTCHA prompt acts as a lure: you believe you’re just verifying you’re human, and don’t see what’s really happening behind.  that in some tests, 17% of users exposed to a fake CAPTCHA campaign ended up following the instructions that triggered malware. \n Why the “I’m Not a Robot” Trick is So EffectiveFake CAPTCHAs work so well because they exploit a ritual we’ve all learned to trust. Clicking a box or selecting traffic lights feels routine, something safe and familiar. That habit makes users lower their guard. Attackers count on this automatic behavior. They mimic Google’s design style and use the same fonts and layouts.In a way, fake CAPTCHAs are the perfect  tool: they blend technical deception with psychological manipulation. People tend to associate CAPTCHAs with extra safety, just a filter that keeps bots out. That’s what makes them ideal for smuggling in the very threats they’re supposed to block. We could call this “trust hijacking”: turning a symbol of security into bait.When the malware behind these scams targets crypto users, it’s not random. Criminals follow where the money flows, and  are pure digital gold. Stealing one recovery phrase can be worth more than months of low-level phishing attempts. The trick’s elegance lies in its simplicity: a single click that feels harmless, leading straight into the attacker’s control.How to Protect Yourself from Fake Captcha AttacksWe must be careful not to assume every CAPTCHA is safe. Here are strategies to reduce risk and keep your crypto secure:Start by checking whether the website is known and trustworthy. If a CAPTCHA appears on an already suspicious site or seems oddly intrusive, exit immediately.Always verify the URL. Misspellings, extra characters, or odd domains are warning signs.Never paste commands into your system based on web prompts. No legitimate CAPTCHA ever asks you to run something manually.To avoid incidents when pasting complex crypto addresses, you can use easier , usernames,  in Obyte to send and receive funds.You can also use textcoins  to keep most of your funds offline, safe from any kind of hacking attempt.Use up-to-date antivirus or endpoint protection that can block or detect malicious scripts or PowerShell executions.Consider browser extensions or tools that block scripts or clipboard manipulation on untrusted pages.Enable strong security habits: keep your software patched,  across different wallets, and avoid storing private keys in digital form.Fake CAPTCHAs are a cunning twist in the ongoing battle between cybercriminals and everyday users. For those holding or handling crypto, the stakes are high. Stay alert, follow the protective steps above, and treat any CAPTCHA prompt outside normal activity with skepticism.:::info
Featured Vector Image by pikisuperstar / ]]></content:encoded></item><item><title>Zero Trust Security Goes Mainstream as Breach Costs Hit Record Highs</title><link>https://hackernoon.com/zero-trust-security-goes-mainstream-as-breach-costs-hit-record-highs?source=rss</link><author>Adedoyin Ogunmola</author><category>tech</category><pubDate>Tue, 25 Nov 2025 14:02:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In 2023, attackers  a third-party vendor that serviced giants like Delta and Amazon, exposing sensitive customer data through a single compromised account. It wasn’t firewalls that failed; it was trust.That’s the flaw in the old “trust but verify” model: once someone gets inside your network perimeter, they’re automatically trusted. But in today’s world — where employees log in from coffee shops, contractors access systems from personal devices, and AI makes phishing nearly indistinguishable from reality — that kind of blind trust has become attackers’ favorite weapon.And as companies wake up to that reality, it’s no surprise that the  security market is projected to surge from $42.48 billion in 2025 to $124.50 billion by 2032 (). The message is clear: companies can no longer afford blind trust.So if your business hasn’t made the shift yet, you’re not just behind; you’re gambling with survival.This article unpacks what Zero Trust really means, why it matters more than ever in 2025, and—most importantly—how to put it into practice before the next breach headline has your name on it.What Zero Trust Really Means (and What It Doesn’t)Imagine this: An employee receives an email that looks identical to one you might send. The wording feels natural, the signature is spot-on, and the message carries a sense of urgency: “Please review this document and log in quickly.”In the old perimeter-based security model, once that employee entered their credentials, the attacker would likely have free access to everything inside. That’s because the system operated on a simple assumption: if you’re “inside,” you must be trusted.Zero Trust changes that rule completely. It doesn’t matter who you are: an intern, a contractor, or even the CEO — every request has to prove itself, every time. Instead of granting blanket trust, Zero Trust requires continuous verification before giving access to data or systems.At its core, it comes down to this: never assume trust, always verify it.But just to be clear, Zero Trust is often misunderstood. It’s not:Just turning on multi-factor authenticationA shiny replacement for your VPNOr some plug-and-play tool you can buy and be done withIt’s a shift in mindset and strategy; not a single product.Core Principles of Zero Trust SecurityZero Trust isn’t a single product you switch on; it’s a framework. It’s about rethinking how access is granted, how activity is watched, and how risk is contained in a world where threats are constant and work no longer happens inside neat office walls.The framework rests on the following guiding principles:People and devices should only get the access they truly need. If your job doesn’t require pulling financial data, you shouldn’t have the keys to that system. Limiting access in this way means that if an account is ever compromised, the damage stays contained. \n Logging in once at the start of the day isn’t enough. Every request to reach data or systems has to be checked again, because trust isn’t permanent. It’s like your bank app asking you to confirm your identity not just when you log in, but also when you move money or reset your password. \n Think of the network like a building. Instead of one giant open floor plan where anyone can wander, Zero Trust breaks it into smaller rooms with separate locks. Even if an attacker slips into one room, they can’t easily move through the rest of the building. \n  Access rules alone aren’t enough. Zero Trust also means keeping watch. If a user suddenly downloads thousands of files at 2 a.m., alarms should go off before that odd behavior turns into a disaster.Together, these principles don’t make attacks vanish — nothing does. But they shrink the blast radius, so a breach doesn’t spiral into a company-wide catastrophe.One weak password. One careless click. That’s all it takes for an attacker to gain free rein inside your systems. And it’s not hypothetical; stories like this keep making headlines.In 2025, the case for Zero Trust has never been clearer.Here are four reasons why:: The average cost of a data breach this year is about $4.44 million, according to IBM’s Cost of a Data Breach Report. That’s not pocket change. For many mid-sized companies, it’s enough to wipe out an entire year’s profit. Imagine being the CFO explaining that loss in the next board meeting.Zero Trust helps you soften that blow by limiting how far attackers can spread if they break in.: Hackers don’t need to break down the door anymore. With AI, they can simply impersonate someone you trust. In 2024, engineers at Arup learned this the hard way when a deepfake video call tricked staff into wiring HK$200 million (~£20 million).Zero Trust is built for exactly this scenario — where the person “inside” may not be who they claim to be. Regulators no longer want promises; they want proof. Miss the mark, and the fines can be as painful as the breach itself. Meta learned that in 2023, when it was hit with a record €1.2 billion GDPR fine. To put that in perspective, that’s larger than the annual GDP of some countries.However, with Zero Trust, you can close that gap by enforcing continuous verification and stronger governance. Trust doesn’t erode slowly anymore. One public security slip and years of goodwill can vanish overnight. In industries like finance and healthcare, customers don’t forgive; they just move to the competitor who promises stronger protection.Zero Trust helps you hold on to that fragile trust by baking verification into every interaction.Zero Trust looks simple on a slide deck, but reality is messier. Many organizations stumble when they try to turn the idea into practice. The intentions are good, but the execution is where things often fall apart.Here are five common Zero Trust adoption-mistakes to watch out for:Treating Zero Trust as a product instead of a strategy: Buying the latest tool with “Zero Trust” on the label doesn’t mean you’re done. Adoption requires changes to policies, workflows, and even company culture. The tech is just one piece of the puzzle.Applying controls unevenly: It’s common to secure remote access while leaving internal apps or legacy systems wide open. That patchwork approach creates blind spots attackers know how to find. Every system deserves the same level of scrutiny.Trying to do it all at once: Rolling out Zero Trust everywhere on day one usually backfires. A smarter play: start with your highest-risk apps or privileged accounts, prove it works, then expand step by step.Ignoring the user experience: If security feels like punishment — endless MFA prompts, clunky approvals, session timeouts — employees will look for shortcuts. And those shortcuts undo the very protections you’re trying to build. Balance security with usability.Treating Zero Trust as a one-time project: Zero Trust isn’t an install-and-walk-away project. Without audits, reviews, and updates, your defenses will fall behind. Think of it less like a one-time deployment and more like ongoing maintenance.The pattern is clear: companies run into trouble when they treat Zero Trust as a quick fix instead of a long-term shift. Avoid these traps, and the transition becomes not just manageable — but sustainable.\n Best Practices for Adopting Zero Trust SecurityRolling out Zero Trust isn’t an overnight fix. The companies that succeed start with focused steps, test what works, and scale gradually.Here’s how to approach it:Start with identity and access control: Keep permissions tight. Users and devices should only have access to what they genuinely need. Pair this with safeguards like MFA and role-based access. Since credential theft remains a top breach vector, limiting what stolen logins can access goes a long way. Not every system needs the same defense. Pinpoint your most critical assets — customer data, financials, intellectual property — and protect them first. Zero Trust works best when it shields what matters most. A wide-open network is like leaving all the office doors unlocked. Micro-segmentation creates controlled zones, so even if attackers get in, they can’t wander freely from one department or system to another.Monitor everything, all the time: Attacks don’t usually happen instantly; they build quietly. Continuous monitoring and automated alerts can flag unusual activity early — like an account suddenly downloading thousands of files.Build a culture that backs it up: Zero Trust fails if employees see it as red tape. Take time to explain why new measures, like stricter logins and access reviews matter. When people connect these steps to protecting the company and its customers, adoption gets much smoother.Zero Trust is no longer just a security trend; it’s quickly becoming the baseline for how modern businesses defend themselves. Breaches are more expensive, attackers are sharper, and regulators less forgiving. Customers, too, have little patience for excuses. The old “trust once you’re inside” model has turned into a liability.The upside? You don’t need to rip everything apart to get started with Zero Trust.  Start with clear wins — stronger identity checks, network segmentation, and employee buy-in— and you can set the foundation and grow from there.At its core, Zero Trust is about one simple shift: don’t assume, verify. In 2025, that mindset may be the difference between scrambling to recover from a breach and standing out as a company customers know they can trust.Now is the time to act. Assess your current security posture, identify your most critical assets, and take the first steps toward Zero Trust. The longer you wait, the higher the odds your organization becomes the next cautionary headline.]]></content:encoded></item><item><title>Listen to Protons for Less Than $100</title><link>https://spectrum.ieee.org/listen-to-protons-diy-magnetometer</link><author>David Schneider</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIxMjkwNy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc4OTI5MDE2N30.McTFH3VD99P9bFF852mV3owNoVWjmsB8q-sxvUF5IEE/image.png?width=600" length="" type=""/><pubDate>Tue, 25 Nov 2025 14:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Build a DIY magnetometer with a couple of seasoning bottles]]></content:encoded></item><item><title>Microsoft To Preload File Explorer in Background For Faster Launch in Windows 11</title><link>https://tech.slashdot.org/story/25/11/25/140253/microsoft-to-preload-file-explorer-in-background-for-faster-launch-in-windows-11?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 25 Nov 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In the latest Windows Insider beta update, Microsoft has announced that it is exploring preloading File Explorer in the background to improve launch performance. The feature will load File Explorer silently before users click on it and can be toggled off for those who prefer not to use it. Microsoft introduced a similar capability earlier this year for Office called Startup Boost that loads parts of Word in the background so the application launches more quickly. The company is also removing elements from the File Explorer context menu in the same update.]]></content:encoded></item><item><title>Baden Bower&apos;s AI System Underpins Its Market Leadership in PR Delivery</title><link>https://hackernoon.com/baden-bowers-ai-system-underpins-its-market-leadership-in-pr-delivery?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:59:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Baden Bower has built its PR dominance through an AI system that predicts editorial acceptance, automates pitch workflows, and secures guaranteed placements. Serving 3,600 clients, the firm analyzes thousands of editorial patterns, scales operations globally, and delivers features in as little as 72 hours. Its data-driven model reshapes PR by reducing uncertainty and compressing timelines industry-wide.]]></content:encoded></item><item><title>HiSilicon Proposes &quot;Cache Lockdown&quot; Driver For More Control Over L3 Cache</title><link>https://www.phoronix.com/news/HiSilicon-L3-Cache-Lockdown</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:57:10 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A Huawei engineer sent out a proposed driver for the Linux kernel to enable "cache lockdwon" behavior for HiSilicon ARM64 processors for greater control over the processor's L3 cache usage...]]></content:encoded></item><item><title>Cloning Environments on AWS Beanstalk: A Practical Fix for Zero-Downtime Patching</title><link>https://hackernoon.com/cloning-environments-on-aws-beanstalk-a-practical-fix-for-zero-downtime-patching?source=rss</link><author>Vijay Pahuja</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:49:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Achieving Zero-Downtime During Patching ExperimentsI was recently asked to help with an AWS Beanstalk issue. An application was running on Beanstalk, but the security team flagged it as non-compliant for OS patching because the underlying servers hadn’t been updated for about a year — the Windows Update service had stopped working. An upgrade to the latest platform failed for some reason, so the team wanted to patch the EC2 instances without affecting live traffic. The goal was to avoid any interruption. I started digging into AWS Elastic Beanstalk to understand the options available to handle this.What is AWS Elastic Beanstalk?It’s a service for deploying web applications and services. Elastic Beanstalk provides capacity provisioning, load balancing, autoscaling, and health monitoring. This allows you to focus on writing code.Benefits of AWS Elastic BeanstalkAmong many others, the key benefit is that one can deploy a scalable web application within minutes, as you don’t have to worry about the complexity of underlying infrastructure like configuring VPC, EC2 instances, load balancers and autoscaling policies, and so on. It also supports most major programming languages, including Java, .NET, Node.js, Python, and Ruby, with full support for Docker as well. Other benefits include monitoring, health checks, logging, tracing, automatic updates, auto-scaling, and multi-AZ deployments.What does AWS Elastic Beanstalk Cost?Nothing. Yes, that’s correct, the AWS Elastic Beanstalk does not cost anything additionally. You will pay for any resources you configure to run your application, same as you would when you configure them without using AWS Elastic Beanstalk.Among many other features, the one that came to the rescue was environment cloning. It allows the developers to replicate an existing environment rapidly. Cloning environment was the solution we arrived at after evaluating other options like scaling up, building a completely new environment, or routing traffic to one region while working on the other region.How to clone environments with AWS BeanstalkEnvironment cloning can be done with the following simple steps:Open the Elastic Beanstalk console, and select the AWS Region.Choose Environments from the navigation panel and then choose the name of your environment from the list.On the environment overview page, choose Actions.Choose Clone environment.In the New Environment section, you can optionally change the Environment name, Environment URL, Description, Platform version, and Service role values that Elastic Beanstalk automatically sets based on the original environment.I cloned the environment. As it spawned new EC2 instances, they already had patches installed. After that, it was just a matter of changing the R53 entry to the domain created for the cloned environment so that the live traffic was routed to it.The original environment was left untouched to allow us to fail back in case of any issue. Once we were satisfied with the cloned environment running without any issues, we updated the original environment and started sending traffic to the original environment.AWS Elastic Beanstalk is a developer-friendly service, takes responsibility for infrastructure configuration and management. Its clone environment feature can be really handy for testing and experimenting with code running in live applications without impacting the customer.]]></content:encoded></item><item><title>Larry Ellison Met With Trump To Discuss Which CNN Reporters They Plan To Fire</title><link>https://www.techdirt.com/2025/11/25/larry-ellison-met-with-trump-to-discuss-which-cnn-reporters-they-plan-to-fire/</link><author>Karl Bode</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:30:35 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Trump’s right wing billionaire friend Larry Ellison (and his nepobaby son, David) recently acquired CBS and likely co-ownership of TikTok. Like Elon Musk’s acquisition of Twitter, the goal isn’t really subtle: rich, thin-skinned right wingers want to own the entirety of U.S. new and old media, then convert it into a giant propaganda and lazy infotainment bullhorn that blows smoke up their asses.While Ellison has some competing suitors with names like Comcast NBC Universal and Netflix, the winning bidder will need approval from the Trump DOJ and FCC. Knowing that they likely have a distinct tactical advantage via corruption, Ellison and Trump appear to be already measuring the drapes, discussing programming changes (and CNN hirings and firings) that will please the president:“Ellison often speaks to connections at the White House and in at least one phone call engaged in a dialogue about possibly axing some of the CNN hosts whom Donald Trump is said to loathe, including Erin Burnett and Brianna Keilar, the people said.”The great irony is these networks that offend Trump weren’t exactly what you’d call bastions of hard-nosed journalism to begin with. Under the ownership of Trump fan David Zaslav, CNN is already know for flinging softball questions at Trump authoritarians, failing to challenge obvious lies on air, and generally airing a sort of safe, infotainment-centered pseudo-journalism. But even  too much critical thinking for America’s thinnest skinned billionaires. But given the recent failures of efforts to dominate U.S. media through consolidation (again, see AT&T) it’s also equally likely that this weird assortment of trolls and nepobabies simply creates a mountain of unsustainable debt and hubris crushed by the weight of its own incompetent ambition. ]]></content:encoded></item><item><title>Spotify Set to Raise U.S. Subscription Prices in Early 2026</title><link>https://hackernoon.com/spotify-set-to-raise-us-subscription-prices-in-early-2026?source=rss</link><author>Journalistic Technology</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:03:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Spotify will raise its U.S. subscription prices in the first quarter of next year, , citing a Financial Times story based on three people familiar with the matter.The planned hike would mark Spotify’s first U.S. price increase since June 2024. The company has already raised prices in several international markets. In August, Spotify said monthly rates would rise to 11.99 euros from 10.99 euros across regions, including South Asia, the Middle East, Africa, Europe, Latin America, and the Asia-Pacific.The company has relied on periodic price increases in recent years to support earnings growth, arguing that its position in the streaming market helps retain subscribers even when costs climb. In the September quarter, Spotify raised the cost of its premium individual plan in more than 150 markets.]]></content:encoded></item><item><title>Apple Confirms Sales Team Layoffs as It Refocuses on Customer Engagement</title><link>https://hackernoon.com/apple-confirms-sales-team-layoffs-as-it-refocuses-on-customer-engagement?source=rss</link><author>Technology Announcements</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:02:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Apple has announced that it’s cutting several sales jobs to bolster customer engagement efforts, according to a Reuters report on Monday. The company told Reuters that only a small number of roles will be affected and that it is continuing to hire, with impacted employees eligible to apply for new positions.According to Bloomberg, the roles affected include account managers who work with major businesses, schools, and government agencies. Staff who run Apple’s briefing centers, where institutional clients receive product demonstrations and meet with company representatives, were also included in the cuts. One target of the layoffs was a government sales team that works with agencies such as the U.S. Defense Department and the Justice Department. The outlet added that the team had been operating in challenging conditions following a 43-day government shutdown and additional cutbacks by the Department of Government Efficiency.]]></content:encoded></item><item><title>Singapore Orders Apple, Google to Block Gov’t Spoofing on Messaging Apps</title><link>https://hackernoon.com/singapore-orders-apple-google-to-block-govt-spoofing-on-messaging-apps?source=rss</link><author>Top Tech Companies</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:02:50 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Singapore’s police have ordered Apple and Google to mitigate the spoofing of government agencies on their messaging platforms, . The directive was issued under the country’s Online Criminal Harms Act after police observed scams on iMessage and Google Messages that impersonated local organisations such as SingPost.According to the home affairs ministry, government agencies already use a local SMS registry that prevents their “gov.sg” name from being copied in regular text messages. That safeguard does not extend to Apple and Google’s internet-based messaging apps, where fraudulent accounts can appear alongside legitimate SMS messages.The ministry said both companies have committed to following the order and encouraged users to update their apps to activate the new protections.]]></content:encoded></item><item><title>Amazon Urges Staff to Use In-House Kiro Over Rival AI Coding Tools</title><link>https://hackernoon.com/amazon-urges-staff-to-use-in-house-kiro-over-rival-ai-coding-tools?source=rss</link><author>BotBeat.Tech: AI Research</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:02:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Amazon has advised its engineers to avoid third-party AI coding tools and instead use its in-house service Kiro, according to a  The guidance came in an internal memo posted to the company’s news site and later confirmed by an Amazon spokesperson.According to the memo, Amazon said it would continue supporting tools already in use but does not plan to expand support for additional third-party AI development services. The note encouraged employees to offer feedback as the company works to improve Kiro, which launched in July.Reuters also reported that the guidance appears to rule out tools such as OpenAI’s Codex, Anthropic’s Claude Code and products from startup Cursor. Amazon has invested about $8 billion in Anthropic and has a separate multi-year cloud deal with OpenAI, but the company has been working to strengthen its own AI portfolio as competitors accelerate their offerings.]]></content:encoded></item><item><title>Amazon Announces New $15B Indiana Data Center Plan Amid AI Boom</title><link>https://hackernoon.com/amazon-announces-new-$15b-indiana-data-center-plan-amid-ai-boom?source=rss</link><author>Top Tech Companies</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:02:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Amazon is planning to invest about $15 billion in Northern Indiana to build new data center campuses, . The project is aimed at expanding the company’s cloud computing capacity as demand for artificial intelligence infrastructure continues to grow.Amazon said the investment will add 2.4 gigawatts of capacity in the region and is expected to create 1,100 jobs. The plan follows an $11 billion outlay announced last year and brings Amazon’s total investment in Indiana to more than $31.3 billion since 2010.Earlier on Monday, Amazon announced a separate plan to invest up to $50 billion to expand AI and supercomputing capabilities for U.S. government customers of Amazon Web Services. The two initiatives are unrelated, the company said.]]></content:encoded></item><item><title>AI Agents Break Rules Under Everyday Pressure</title><link>https://spectrum.ieee.org/ai-agents-safety</link><author>Matthew Hutson</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIyNDkzMS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5MTUwNzkzNn0.mjpwO_ynfpVnDMVSuD6Te-9TkLMy5KE_HQS1MQMl-Nc/image.jpg?width=600" length="" type=""/><pubDate>Tue, 25 Nov 2025 13:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Shortened deadlines and other stressors caused misbehavior]]></content:encoded></item><item><title>Lenovo Stockpiling PC Memory Due To &apos;Unprecedented&apos; AI Squeeze</title><link>https://hardware.slashdot.org/story/25/11/25/0257209/lenovo-stockpiling-pc-memory-due-to-unprecedented-ai-squeeze?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Bloomberg: Lenovo is stockpiling memory and other critical components to navigate a supply crunch brought on by the boom in artificial intelligence. The world's biggest PC maker is holding on to component inventories that are roughly 50% higher than usual, Chief Financial Officer Winston Cheng told Bloomberg TV on Monday. The frenzy to build and fill AI data centers with advanced hardware is raising prices for producers of consumer electronics, but Lenovo also sees opportunity in this to capitalize on its stockpile.
 
"The price is going very, very high, of course, and I think it's been unprecedented in terms of this rate driven by the AI demand," Cheng said. His company has long-term contracts in place and the benefit of scale, he added, and "those that have the supply actually would be able to have a position in the market." Beijing-based Lenovo will aim to avoid passing on rising costs to its customers in the current quarter, as it wants to sustain this year's strong sales growth, according to the CFO. He said the company will strike a balance between price and availability in 2026. Lenovo said last week that it has enough memory chips for all of 2026 and it can navigate any shortages better than its competitors.]]></content:encoded></item><item><title>X.Org Server 21.1.21 Released To Fix Several Regressions</title><link>https://www.phoronix.com/news/X.Org-Server-21.1.21</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 11:25:52 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those continuing to make use of the X.Org Server, a new point release is now available in the 21.1 series. While most often X.Org Server stable releases these days are driven by shipping new security fixes, the X.Org Server 21.1.21 release is to fix several regressions introduced for various functional issues...]]></content:encoded></item><item><title>Intel LLM Scaler vLLM Update Supports More Models</title><link>https://www.phoronix.com/news/Intel-LLM-Scaler-vLLM-0.10.2-b6</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 11:13:18 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Intel software engineers continue to be hard at work on LLM-Scaler as their solution for running vLLM on Intel GPUs in a Docker containerized environment. A new beta release of LLM-Scaler built around vLLM was released overnight with support for running more large language models...]]></content:encoded></item><item><title>libinput 1.30 Released With Support For Writing Plug-Ins In Lua</title><link>https://www.phoronix.com/news/libinput-1.30-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 10:58:59 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Red Hat's leading Linux input expert Peter Hutterer released libinput 1.30 today as the newest update to this input handling library used on both X.Org and Wayland desktops...]]></content:encoded></item><item><title>Dutch Takeover of China-Owned Nexperia Sparks New Fears Over Global Supply Chain Fragility</title><link>https://hackernoon.com/dutch-takeover-of-china-owned-nexperia-sparks-new-fears-over-global-supply-chain-fragility?source=rss</link><author>Top Tech Companies</author><category>tech</category><pubDate>Tue, 25 Nov 2025 10:47:46 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[A new chip supply crisis, triggered by Dutch firm Nexperia, has disrupted global auto production, reviving concerns over supply chain fragility, Reuters reported Nov. 24. The disruption follows the Dutch government’s decision to seize control of Nexperia over national security risks tied to its Chinese parent, Wingtech. In response, China suspended exports from Nexperia’s Dongguan facility — a major source of basic chips used in automotive components. Major automakers, including Nissan and Honda, have curtailed production, while top suppliers like Bosch have reduced factory output. The halted supply of low‑end chips has led to cascading delays across the sector. Despite promises to diversify sourcing post-COVID and natural disasters, automakers remain heavily reliant on just-in-time logistics and face high switching costs for alternative chips.]]></content:encoded></item><item><title>“AI Is Like a Nuclear Project” - Russia’s Vedyakhin on The Emerging National AI Arms Race</title><link>https://hackernoon.com/ai-is-like-a-nuclear-project-russias-vedyakhin-on-the-emerging-national-ai-arms-race?source=rss</link><author>BotBeat.Tech: AI Research</author><category>tech</category><pubDate>Tue, 25 Nov 2025 10:47:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Artificial intelligence is being likened to nuclear power by Alexander Vedyakhin, the first deputy CEO of Sberbank, who stated that a new global “AI club” is forming, where nations with their own large language models (LLMs) will gain outsized influence, as reported by Reuters on Monday.\n Vedyakhin said Russia already ranks among the seven countries that have developed home‑grown AI capabilities, and stressed that reliance on foreign‑built models poses unacceptable security risks in areas like public services, healthcare, and education.\n He acknowledged that the U.S. and China are ahead by about six to nine months, and that for countries just entering the race, the cost and energy demands make catch‑up extremely challenging.\n Despite Western sanctions limiting access to computing infrastructure, Vedyakhin said Russia will leverage its talent pool and plans to release some of its latest AI models as open source, while cautioning that over‑investment in AI infrastructure risks diminishing returns.]]></content:encoded></item><item><title>China Regains 14 % of Global Bitcoin Mining Share in Surprise Revival</title><link>https://hackernoon.com/china-regains-14-percent-of-global-bitcoin-mining-share-in-surprise-revival?source=rss</link><author>Crypto Sovereignty Through Technology, Math &amp; Luck</author><category>tech</category><pubDate>Tue, 25 Nov 2025 10:47:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Despite a nationwide ban on cryptocurrency mining in 2021, Bitcoin mining in China has quietly rebounded, with energy‑rich provinces and surplus data‑centre infrastructure driving the resurgence, Reuters reported on Monday.\n Data from the Hashrate Index indicates China has reclaimed around 14 % of global Bitcoin mining share as of October 2025, climbing back to third place globally.\n The revival is bolstered by cheap electricity in regions like Xinjiang and strong demand for mining rigs from Chinese buyers: one major rig‑maker, Canaan Inc., reports more than 50% of its quarterly revenue now comes from China, up from 2.8% in 2022.\n Although the mining ban remains officially in place, weak enforcement and shifting policy signals suggest China may be tolerating the underground revival — highlighting the difficulty of curbing entrenched crypto activity when economic incentives are strong.]]></content:encoded></item><item><title>Meta Accused of Burying Research that Linked Facebook Usage to Teen Depression</title><link>https://hackernoon.com/meta-accused-of-burying-research-that-linked-facebook-usage-to-teen-depression?source=rss</link><author>Top Tech Companies</author><category>tech</category><pubDate>Tue, 25 Nov 2025 10:47:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Meta Platforms allegedly suppressed internal research linking its social-media products to negative mental-health outcomes, according to U.S. court filings reported by Reuters on Nov. 24.\n The filings reveal a 2020 study, code‑named “Project Mercury,” in which users who deactivated Facebook reported lower levels of depression, anxiety, loneliness, and social comparison.\n Rather than publish the results, Meta halted the research, internally attributing the findings to a “media narrative,” even as a staffer privately insisted that the evidence was valid. \n Meta said the study was stopped due to methodological flaws and maintains it has long engaged in research and actions to protect teens.]]></content:encoded></item><item><title>EPA Approves New &apos;Forever Chemical&apos; Pesticides For Use On Food</title><link>https://news.slashdot.org/story/25/11/25/0252230/epa-approves-new-forever-chemical-pesticides-for-use-on-food?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The EPA has approved new pesticides that qualify as PFAS "forever chemicals" (paywalled; alternative source), sparking criticism from scientists and environmental groups who warn these decisions could increase Americans' exposure through food and water at a time when many states are moving to restrict such substances. The Washington Post reports: This month, the agency approved two new pesticides that meet the internationally recognized definition for per- and polyfluoroalkyl substances, also known as PFAS or fluorinated substances, and has announced plans for four additional approvals. The authorized pesticides, cyclobutrifluram and isocycloseram, which was approved Thursday, will be used on vegetables such as romaine lettuce, broccoli and potatoes. The agency also announced plans to relax a rule requiring companies to report all products containing PFAS and has proposed weakening drinking water standards for the chemicals. "Many fluorinated compounds registered or proposed for U.S. pesticidal use in recent years offer unique benefits for farmers, users, and the public," EPA spokeswoman Brigit Hirsch said in a statement.
 
"It is important to differentiate between the highly toxic PFAS such as PFOA and PFOS for which the EPA has set drinking water standards, versus less toxic PFAS in pesticides that help maintain food security," notes Doug Van Hoewyk, a toxicologist at Maine's Department of Agriculture, Conservation and Forestry. He added that concerns about food residue depend on the PFAS and the quantity.
 
Nathan Donley, a senior scientist at the Center for Biological Diversity, also commented: "The data we have about the use of PFAS pesticides is already seven years old, and since there have been many new approvals during that time, those numbers are sure to underestimate the amount were using today."]]></content:encoded></item><item><title>Why Dumb People Outsmart You and Steal Your Success</title><link>https://hackernoon.com/why-dumb-people-outsmart-you-and-steal-your-success?source=rss</link><author>BenoitMalige</author><category>tech</category><pubDate>Tue, 25 Nov 2025 08:34:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[“Incompetence not only causes poor performance—it also robs people of the ability to realize it.” Most people aren’t losing because they’re dumb. They’re losing because they hesitate. Intelligence often works against you: the more aware you are of the risks, the more likely you are to freeze. Smart people convince themselves that hesitation is strategy, but it’s usually fear disguised as logic. You’ve probably talked yourself out of more opportunities than the world ever took from you. You tell yourself, , I need to learn a bit more, I’ll start when it feels right. But that feeling never comes. The people you call reckless started before they were ready, and that’s exactly why they learned faster than you did. You believe there’s an invisible threshold where you finally become “qualified.” You picture a line where you’ll magically feel ready to start the podcast, pitch the client, post the video. That line doesn’t exist. There are no gatekeepers. The only permission slip is the one you sign yourself. Intelligence creates self-doubt. The smarter you are, the more flaws you see. You’re painfully aware of how far you are from mastery, and that awareness becomes a weight you carry everywhere. You see the cracks in your own work and assume everyone else sees them too. The truth: they don’t. They’re too busy looking at their own. You measure yourself against experts who have been at it for a decade, not beginners who just started. You scroll through polished results and forget how ugly the early versions were. Dumb people don’t have that problem, they compare themselves only to their previous attempt. That’s why they improve faster. That’s the Dunning-Kruger effect in motion. People who know little overestimate themselves, and people who know a lot underestimate themselves. The irony is brutal: the ones least qualified feel most confident, while the most competent stay quiet and stuck. The world doesn’t reward the most prepared, it rewards the most visible. You think you’ll be recognized for your potential, but potential isn’t marketable. Output is. Dumb people aren’t waiting to be discovered; they’re already posting, pitching, and promoting. When you don’t know what can go wrong, you move without friction. Dumb people act out of blindness, not bravery. But that blindness becomes an edge. While you’re busy analyzing variables, they’re collecting data through action. They act first and fix later. Their first launch flops, so they learn. Their second launch improves, so they grow. You, on the other hand, are still in version 0.1 of your plan, trying to predict outcomes instead of testing them. The market rewards motion, not forecasts. Real experts know enough to be cautious, which makes them slow. Amateurs move fast because they’re too naive to see the depth of the problem. By the time the expert takes the first step, the fool already owns the attention, the audience, and half the market share.Perception Beats Precision — People buy conviction before they buy correctness. They follow confidence, not IQ. If someone speaks with certainty, the brain equates that certainty with competence. That’s why fake gurus thrive and quiet professionals fade into the background. In every domain (business, art, relationships) certainty is louder than skill. If you aren’t visible, you’re invisible, no matter how brilliant your work is. Dumb people are visible by accident; smart people remain invisible on purpose, and they call it humility. One of them gets paid. If you need to read that again, feel free. The smarter you are, the more likely you are to isolate. You tell yourself you’re “focusing,” but isolation kills feedback. Dumb people broadcast everything. They get data early. They learn in public. Every piece of feedback compounds their competence. Meanwhile, your silence compounds your doubt. Confidence isn’t something you earn after you succeed; it’s something you decide before you do. Stupid people don’t wait for proof: they move first. Confidence is not arrogance; it’s simply refusing to let uncertainty make your decisions. Action → Evidence → Belief → More Action. Every move produces evidence that you’re capable. That evidence rewires your identity. Belief grows, and with it, your willingness to take bigger risks. The loop reinforces itself until movement becomes your default. Each time you act while afraid, your nervous system learns that fear isn’t fatal. The discomfort fades faster than you expect. After enough reps, risk feels like home. That’s how confidence is built: through friction, not through affirmations. How did you learn to ride a bicycle? Dumb people don’t romanticize failure, they normalize it. They fail so often it stops meaning anything. That’s why they appear fearless. Their secret isn’t courage; it’s repetition. You can’t build thick skin by staying safe. Confidence doesn’t live in your head, it lives in the arena. Each visible action, every post, pitch, or project adds to your internal database of proof. The more you show up, the more solid your self-belief becomes. Private wins feel good, but public ones rewire your identity. Eventually, you stop chasing confidence and start embodying it. You no longer need to feel ready because you’ve proven that readiness is irrelevant. You act first, refine later. That’s the loop dumb people enter by accident, but you can enter on purpose once you finish this training. The Dunning-Kruger curve is a staircase more than it is an obstacle. Everyone climbs it: blind confidence, harsh reality, humble rebuilding, quiet mastery. The only difference is how long you stay stuck on each step. Dumb people don’t wait for permission to publish, pitch, or sell. They give themselves permission through action. Smart people wait for validation that never comes. Confidence is self-issued; if you’re waiting for approval, you’re outsourcing your potential. Your first version will suck. The second version will suck. That’s their job. It’s supposed to be clumsy. It’s supposed to be awkward and incomplete. You get to version 10 only by releasing versions 1 through 9. Perfection is just procrastination wearing expensive clothes. Being seen creates accountability. When you put your work in public, you stop negotiating with yourself. You start refining faster because now the world is watching. That pressure is priceless because it forces evolution. Motivation is unreliable. Systems aren’t. Build structures that force movement even when you don’t feel like it. Post every day. Make five offers a week. Review what worked every Friday. You don’t need more discipline, you just need fewer decisions. Action rewires the brain faster than belief. Each time you move, your brain logs a win and releases dopamine. And over time, you become chemically addicted to progress. Comfort starts to feel dangerous. Growth becomes the new baseline. Stupid people use the crowd as a mirror. They get feedback early, adapt publicly, and earn trust through transparency. Smart people hide until they’re perfect. But perfection can’t be trusted, it’s too polished to be real. Authentic imperfection always wins attention.Intelligence without motion becomes self-sabotage.The world rewards conviction before competence.Confidence is not found; it’s trained through exposure.Failure is a feedback system, not a verdict.Visibility compounds faster than skill.The people you call “dumb” or “stupid” are just willing to look stupid longer than you.The only qualification for success is movement. Pick one uncomfortable action you’ve delayed and do it today. Post it. Ship it. Send it. The goal isn’t perfection. It’s participation. Run a 30-day experiment: produce something every day and publish it. Ignore metrics. Focus on the reps. When fear shows up, remember: dumber people are already winning. Use that as proof that readiness is irrelevant. List the areas of your life or business where you’re invisible by choice. Visibility is leverage; stop hiding your value. Action → Feedback → Iteration → Identity. That’s the entire game.]]></content:encoded></item><item><title>The TechBeat: Stop Building Your Product for Yourself: Why Most Early-Stage Startups Fail at Marketing (11/25/2025)</title><link>https://hackernoon.com/11-25-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Tue, 25 Nov 2025 07:10:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @superorange0707 [ 12 Min read ] 
 Modern search Q&A explained: how knowledge graphs, DeepQA, and MRC turn messy web pages into direct, trustworthy answers. Read More.By @manukovska [ 10 Min read ] 
 I've watched 50 startups burn cash on marketing before talking to customers. Here's what actually works: The Mum Test, founder brand building, and hiring doers  Read More.By @melissaindia [ 5 Min read ] 
 Build a winning data quality project proposal with clear goals, strong justification, and proven strategies that secure leadership approval and drive success. Read More.By @stevebeyatte [ 8 Min read ] 
 A 2025 guide to the best SOC 2 compliance software, comparing automation, integrations, usability, and pricing across seven leading platforms. Read More.By @editingprotocol [ 4 Min read ] 
 Learn how to write and get read online with HackerNoon’s Blogging Course—editor-led training on clarity, distribution, AI, and building a tech audience. Read More.By @sanya_kapoor [ 4 Min read ] 
 Mitesh Sinha’s research shows the rise of hybrid leaders who blend strategy, tech fluency, and human insight for modern enterprises. Read More.By @membrane [ 5 Min read ] 
 AI coding agents excel at building features but fail at production integrations. The issue isn't AI capability—it's lack of integration-specific infrastructure. Read More.By @chris127 [ 4 Min read ] 
 Why the economy underfunds planet cleanup, and how programmable money could support large-scale air, water, and soil remediation. Read More.By @alexwrites [ 9 Min read ] 
 The article covers a series of interviews with professionals and reflects their views on where AI really stands in today's business landscape. Read More.By @mattleads [ 14 Min read ] 
 This article explores important caching strategies that solve expensive architectural problems: latency, concurrency (thundering herds), and security (GDPR) Read More.By @zachflower [ 6 Min read ] 
 I thought I'd share some of the lessons learned and highlight different ways that my former peers can lend their expertise to the next generation of engineers. Read More.By @josecrespophd [ 10 Min read ] 
 AI's Fatal Flaw: Why JEPA, LLMs & Transformers Can't Escape the Flatland, until Toroidal Math Read More.By @manishmshiva [ 6 Min read ] 
 Learn how to install and run open-source large language models (LLMs) locally on Windows — with or without the command line.  Read More.By @lab42ai [ 7 Min read ] 
 This article examines the first large‑scale AI‑autonomous cyberattack (GTG‑1002), where an LLM hijacked via MCP became a self‑directed espionage engine. Read More.By @williamguo [ 4 Min read ] 
 This new feature ensures seamless schema updates for CDC data sources, enhancing flexibility and data consistency. Read More.By @orphiceye [ 5 Min read ] 
 Bitcoin's BIP-360 is both genius and cursed, and in whatever state it is at any time, it might become the only sane option we’ll have. Read More.]]></content:encoded></item><item><title>Ozone Hole Ranked As 5th Smallest In More Than 30 Years</title><link>https://news.slashdot.org/story/25/11/25/0236221/ozone-hole-ranked-as-5th-smallest-in-more-than-30-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Scientists report that the Antarctic ozone hole in 2025 is the fifth-smallest since 1992, thanks largely to decades of global restrictions on ozone-depleting chemicals under the Montreal Protocol. ABC News reports: The ozone hole reached its greatest one-day extent for 2025 in early September, measuring 8.83 million square miles, about 30% smaller than the largest hole on record in 2006. NOAA and NASA scientists emphasize that recent findings show efforts to limit ozone-depleting chemical compounds can have a significant impact. The regulations are established by the Montreal Protocol, which went into effect in 1992. Subsequent amendments are driving the gradual recovery of the ozone layer, which remains on track to fully recover later this century as countries worldwide replace harmful substances with safer alternatives.
 
For decades, chlorofluorocarbons (CFCs) and other ozone-depleting compounds were widely used in aerosol sprays, foams, air conditioners and refrigerators, causing significant reductions in ozone levels. Natural factors, such as temperature and atmospheric circulation, also influence ozone concentrations and are likely to have contributed to a smaller ozone hole this year, according to researchers. "This year's hole would have been more than one million square miles larger if there was still as much chlorine in the stratosphere as there was 25 years ago," said Paul Newman, a senior scientist at the University of Maryland system and longtime leader of NASA's ozone research team.]]></content:encoded></item><item><title>Elite Cyber Veterans Launch Blast Security With $10M to Turn Cloud Detection Into Prevention</title><link>https://hackernoon.com/elite-cyber-veterans-launch-blast-security-with-$10m-to-turn-cloud-detection-into-prevention?source=rss</link><author>CyberNewswire</author><category>tech</category><pubDate>Tue, 25 Nov 2025 06:56:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Tel Aviv, Israel, November 24th, 2025/CyberNewsWire/--Blast is introducing a new operating model for cloud security with a first-of-its-kind Preemptive Cloud Defense Platform, replacing reactive response with continuous prevention. a cybersecurity startup founded by industry veterans from Solebit (acquired by Mimecast) and elite IDF units, today announced its launch from stealth and a $10 million seed round co-led by  and . Blast is redefining how organizations eliminate cloud risks using preventive guardrails to ensure environments remain secure by design. The company is already working with numerous global enterprises to secure their production environments, preventing cloud risk by over 90% and significantly shrinking the blast radius.Enterprises operating at multi-cloud scale face relentless change and compounding complexity accelerated by the growth of AI. Blast’s Preemptive Cloud Defense Platform marks an inflection point for organizations shifting from reactive alert-chasing to proactive, continuously enforced prevention.It turns native cloud control into a preventive defense fabric, where every change is modelled, tested, and enforced safely. This approach ensures prevention never breaks production or slows innovation.“Cloud adoption and AI have multiplied complexity and risk faster than teams can keep up. The market is full of tools for detection and remediation, but the only reliable way to mitigate risk is to prevent it in the first place - and that’s what Blast is built to do,” said Boris Vaynberg, co-founder and CEO of Blast Security.“When Henry Ford introduced his automobile, many believed horses were good enough. It’s hard to see how a new approach can redefine the standard - until it becomes the standard. Our mission is to make prevention the new standard in cloud security and lead the market shift.”The lightbulb moment for Blast’s founding team - Boris Vaynberg, Ido Bukra, and Roi Panai - came when they were called to reserve duty to lead a national-level cloud security project. During this critical time, the team realized that security must match the cloud’s speed, scale, and complexity with prevention measures-built in. Blast’s founding team brings over a decade of collaborative experience - their first company, Solebit, was acquired by Mimecast in their largest acquisition. Blast’s engineers are cloud security veterans committed to developing game-changing, practical solutions that prevent risks before they arise and keep cloud environments inherently secure.“At Via, we see the shift from alert-chasing to prevention as a strategic program to strengthen our overall defense. Blast enables us to enforce preventive guardrails at scale - making our cloud environments more resilient with the same resources, less manual effort, and the trust to ensure zero disruption to the business,” said Oren Hogery, CISO, Via.“Blast’s founding team has a remarkable history of solving complex security challenges,” said Itay Rand, General Partner at 10D Capital. “Their preemptive approach to cloud security meets a growing critical need in the market, enabling organizations to prevent threats rather than merely reacting to them. We’re thrilled to back Blast as they set a new standard for truly proactive cloud defense.”To learn more about Blast’s Preemptive Cloud Defense Platform and its capabilities, users can visit .  marks the end of reactive cloud security, replacing after-the-fact response with continuous prevention. Instead of reacting to threats and chasing alerts, Blast creates a living, preemptive defense fabric that continuously evolves with the enterprise cloud environment - eliminating alert fatigue, reducing operational friction, and shrinking the blast radius. The Blast founding team has worked together for more than a decade and has a proven track record of building scalable prevention products, including at Solebit (acquired by Mimecast). Headquartered in Tel Aviv, Blast is backed by leading investors and trusted by global organizations.:::tip
This story was published as a press release by Cybernewswire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>India’s gig workers win legal status, but access to social security remains elusive</title><link>https://techcrunch.com/2025/11/24/indias-gig-workers-win-legal-status-but-access-to-social-security-remains-elusive/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Tue, 25 Nov 2025 04:42:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[India is restructuring its gig economy with a new labor law, but much more is needed before gig workers see real benefits.]]></content:encoded></item><item><title>I Was Sick of the Crypto Off-Ramp Pain. So I Built My Own Crypto Card</title><link>https://hackernoon.com/i-was-sick-of-the-crypto-off-ramp-pain-so-i-built-my-own-crypto-card?source=rss</link><author>Michael Jerlis</author><category>tech</category><pubDate>Tue, 25 Nov 2025 04:24:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[After years in crypto, one thing kept frustrating me: using it was still harder than earning it.Off-ramps were slow, expensive, and unpredictable — and I was done accepting that. That’s why I decided to build my own crypto card.I’ll walk you through what we created and how it can finally make spending crypto feel effortless.I love crypto, but for years, I was deeply frustrated by how hard it was to actually use it in the real world.I’m talking about the "crypto off-ramp pain" — that moment when you need your digital assets for something practical, like buying a plane ticket or paying a subscription, and you realize just how painful the process is. It’s slow, it’s expensive, and it’s packed with friction.For too long, the journey from your crypto wallet to your everyday life looked like this: four exhausting steps. You earn your crypto (say, USDT), then you navigate a P2P exchange or a complex trading platform, wait for a bank transfer that might take days, and only then can you finally spend your money.It’s a process riddled with risk, high fees, and the constant fear that your bank might block the transaction altogether.As someone who lives and breathes this industry, I’d had enough. I realized that if we — the builders — couldn't make spending crypto simple, then we were failing our users. It had to be fixed.The People Who Hurt the MostThis pain isn't just an inconvenience; for millions of people, it’s a major barrier to financial freedom. We saw three groups suffering the most:1. The Global Nomad and FreelancerYou earn in USDT, but you need to pay for Airbnbs, co-working spaces, and local services across different countries. The old way meant expensive SWIFT transfers and the constant struggle of opening local bank accounts. You need a global solution that moves at the speed of your life.2. The Active Trader and DeFi UserYou’ve just made a great trade, and now you need instant access to that liquidity — maybe for an emergency, or maybe to jump on a new investment opportunity. But the outdated, multi-step withdrawal process is a massive barrier to instant access.3. The High-Inflation Country ResidentFor many peoplek, stablecoins like USDT are a true "Digital Dollar" — a financial shield against currency devaluation and capital controls. But what good is a shield if you can’t use it when you need it most? They need a simple, reliable way to turn that stored value into real-world purchasing power without losing time and money.The demand for a better solution is massive. The crypto card market is projected to reach billions in the next few years, proving just how  this pain really is.The EMCD Way: One Step to FreedomMy team and I decided to stop complaining about off-ramp pain and build the solution ourselves. We took that frustrating four-step process and reduced it to just one: instant spending.It’s a virtual USD Mastercard directly connected to your EMCD Wallet. The magic lies in its simplicity:: You decide how much you want to spend. Tap once, and your USDT in the EMCD Wallet is instantly converted to USD on your card. No complex exchanges, no waiting, no hassle: You can use it online, or add it manually to your Apple Pay or Google Pay to pay offline, anywhere Mastercard is accepted: Your main crypto funds stay safe in your wallet. You only top-up what you need, giving you total control over your spending: We keep it simple and low: a flat $0.25 fee per operation. No hidden chargesWe built this card to be accessible to everyone, providing a vital financial bridge where traditional banking have failed.It’s Time to Stop Being Sick of the PainYour crypto should be as easy to spend as your bank balance. With the EMCD Payment Card, it finally is.==If you’re ready to ditch the off-ramp pain, here’s how simple it is to get started==:Complete a quick KYC (if you haven't already).Fund your Wallet with at least $15 in USDT.Open your Card in the "Crypto Card" section.Add it to Apple Pay or Google Pay and start spending globally.We built this because we believe in a world where financial freedom is instant and accessible. It’s time to stop suffering through off-ramp pain and start experiencing a better way — the EMCD way. \n ]]></content:encoded></item><item><title>CDC Data Indicates We’re 2 Months Away From America Losing Its Measles Elimination Status</title><link>https://www.techdirt.com/2025/11/24/cdc-data-indicates-were-2-months-away-from-america-losing-its-measles-elimination-status/</link><author>Timothy Geigner</author><category>tech</category><pubDate>Tue, 25 Nov 2025 03:35:47 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[We’ve talked a lot about measles throughout this year, which is particularly frustrating given that America officially eliminated this disease from its endemic state back in 2000. How we got here is a very simple story: too many people refused to vaccinate themselves and/or their children, giving the virus a foothold which it had been deprived for nearly a quarter of a century. And how  happened is also a frustratingly simple story: a combination of granola-munching liberals and the religious right in America got together in an unholy alliance to make up conspiracy theories about vaccines, claiming they cause autism and other complications.If there is a singular face for this idiotic movement, it is, of course, RFK Jr., who now heads up the Department of Health & Human Services. There should be no preamble necessary to explain just how bad RFK Jr. has been in his current role, nor how directly responsible he is for the anti-vaxxer movement going back long before this current administration had the audacity to place American healthcare in his hands. We’ve been talking about how this administration’s inaction has put America’s measles elimination status at risk going all the way back to April.We’re nearly there. To lose that status, a country must have endured continuous spread of a common strain of the disease over the course of 12 months. The CDC recently linked several outbreaks in America together via a singular strain, putting us at just over 10 months of continuous spread. The Times obtained a recording of a call during which officials from the Centers for Disease Control and Prevention confirmed to state health departments that the ongoing measles outbreak at the border of Arizona and Utah is a continuation of the explosive outbreak in West Texas that began in mid- to late-January. That is, the two massive outbreaks are being caused by the same subtype of measles virus.The massive outbreak in Texas began in mid- to late-January and was declared over on August 18. In all, there were 762 cases of measles confirmed in the outbreak. Utah and Arizona started seeing some measles cases in June, but those outbreaks appeared to take off in August. To date, Utah and Arizona have reported 212 cases.As ArsTechnica notes, America is not alone in this embarrassing failure, nor first. Canada just a few weeks ago lost its own elimination status for measles, having also endured 12 months of continuous spread of a singular strain. The cause for that is the same as in the States: non-participation in vaccination. And, frankly, the same people are responsible. Ignorance knows no borders, it turns out.Canada achieved elimination in 1998. The US did the same in 2000. Elimination was achieved through hard-fought vaccination campaigns, as two doses of the measles, mumps, and rubella (MMR) vaccine is 97 percent protective against the virus, and that protection is considered lifelong. But, since that time, vaccine misinformation and potent rhetoric from anti-vaccine activists—including current US Health Secretary Robert F. Kennedy Jr.—have taken hold, driving vaccination rates down on a population level. While the vast majority of American and Canadian parents continue to vaccinate their children, certain pockets and close-knit communities have become dramatically undervaccinated, providing potential footholds for the virus.The CDC is, , touting its plan to combat measles. On its website, it talks about how it is ramping up its vaccination programs, trying to vaccinate more people, build out surveillance programs to monitor the disease’s spread, and respond quickly to outbreaks.I’ve reported on measles this year on a continual basis. I’m here to tell you one thing: if the CDC is doing literally any of the above, then it must be doing it in secret because I can’t square a single one of those claims with the reality we’re seeing across the country. RFK Jr. heads up HHS, under which CDC operates, and his messaging has been in  to what his child agency says it’s doing. He has questioned vaccines at every turn. He tore apart ACIP, the CDC advisory panel on immunization programs, and rebuilt it with anti-vaxxers that have already weakened our vaccination programs. And it shows in the numbers. Perhaps you thought that the two outbreaks had waned and the rate of spread was in decline. It absolutely is not.Sure, we’re at half the weekly new case rate compared with February and March. But those were the cold months, when viruses like this tend to spread more regularly due to more time being spent indoors without open windows for ventilation. Late winter and early spring are considered measles season and the fact that we’re already seeing a gradually rising spread rate, larger than before this past February and March, is extremely worrying. The worst part of all of this is to be reminded that, while America did eliminate endemic measles in 2000, that came after focused vaccination campaigns kicked off in 1991. Nearly a decade of work, completely down the drain. All because RFK Jr., a bunch of Hollywood celebrities, and pockets of the ultra-religious couldn’t be bothered to listen to actual scientists and doctors. In 1991, America endured 9,643 cases of measles. We’re on pace to achieve something like at least 2,000 cases in 2025. If it takes a couple of years to get back to elimination status at that number, it represents both needless suffering by the American people and a complete embarrassment to those in charge of America’s health.]]></content:encoded></item><item><title>Hacker Conference Installed a Literal Antivirus Monitoring System</title><link>https://it.slashdot.org/story/25/11/25/0024229/hacker-conference-installed-a-literal-antivirus-monitoring-system?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Wired: Hacker conferences -- like all conventions -- are notorious for giving attendees a parting gift of mystery illness. To combat "con crud," New Zealand's premier hacker conference, Kawaiicon, quietly launched a real-time, room-by-room carbon dioxide monitoring system for attendees. To get the system up and running, event organizers installed DIY CO2 monitors throughout the Michael Fowler Centre venue before conference doors opened on November 6. Attendees were able to check a public online dashboard for clean air readings for session rooms, kids' areas, the front desk, and more, all before even showing up. "It's ALMOST like we are all nerds in a risk-based industry," the organizers wrote on the convention's website. "What they did is fantastic," Jeff Moss, founder of the Defcon and Black Hat security conferences, told WIRED. "CO2 is being used as an approximation for so many things, but there are no easy, inexpensive network monitoring solutions available. Kawaiicon building something to do this is the true spirit of hacking." [...]
 
Kawaiicon's work began one month before the conference. In early October, organizers deployed a small fleet of 13 RGB Matrix Portal Room CO2 Monitors, an ambient carbon dioxide monitor DIY project adapted from US electronics and kit company Adafruit Industries. The monitors were connected to an Internet-accessible dashboard with live readings, daily highs and lows, and data history that showed attendees in-room CO2 trends. Kawaiicon tested its CO2 monitors in collaboration with researchers from the University of Otago's public health department. The Michael Fowler Centre is a spectacular blend of Scandinavian brutalism and interior woodwork designed to enhance sound and air, including two grand pou -- carved Mori totems -- next to the main entrance that rise through to the upper foyers. Its cathedral-like acoustics posed a challenge to Kawaiicon's air-hacking crew, which they solved by placing the RGB monitors in stereo. There were two on each level of the Main Auditorium (four total), two in the Renouf session space on level 1, plus monitors in the daycare and Kuracon (kids' hacker conference) areas. To top it off, monitors were placed in the Quiet Room, at the Registration Desk, and in the Green Room.
 
Kawaiicon's attendees could quickly check the conditions before they arrived and decide how to protect themselves accordingly. At the event, WIRED observed attendees checking CO2 levels on their phones, masking and unmasking in different conference areas, and watching a display of all room readings on a dashboard at the registration desk. In each conference session room, small wall-mounted monitors displayed stoplight colors showing immediate conditions: green for safe, orange for risky, and red to show the room had high CO2 levels, the top level for risk. Colorful custom-made Kawaiicon posters by New Zealand artist Pepper Raccoon placed throughout the Michael Fowler Centre displayed a QR code, making the CO2 dashboard a tap away, no matter where they were at the conference. Resources, parts lists, and assembly guides can be found here.]]></content:encoded></item><item><title>Mind-Altering &apos;Brain Weapons&apos; No Longer Only Science Fiction, Say Researchers</title><link>https://science.slashdot.org/story/25/11/25/0030245/mind-altering-brain-weapons-no-longer-only-science-fiction-say-researchers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 02:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Researchers warn that rapid advances in neuroscience, pharmacology, and AI are bringing "brain weapons" out of science fiction and into real-world plausibility. They argue current arms treaties don't adequately cover these emerging tools and call for a new, proactive framework to prevent the weaponization of the human mind. The Guardian reports: Michael Crowley and Malcolm Dando, of Bradford University, are about to publish a book that they believe should be a wake-up call to the world. [...] The book, published by the Royal Society of Chemistry, explores how advances in neuroscience, pharmacology and artificial intelligence are coming together to create a new threat. "We are entering an era where the brain itself could become a battlefield," said Crowley. "The tools to manipulate the central nervous system -- to sedate, confuse or even coerce -- are becoming more precise, more accessible and more attractive to states."
 
The book traces the fascinating, if appalling, history of state-sponsored research into central nervous system (CNS)-acting chemicals. [...] The academics argue that the ability exists to create much more "sophisticated and targeted" weapons that would once have been unimaginable. Dando said: "The same knowledge that helps us treat neurological disorders could be used to disrupt cognition, induce compliance, or even in the future turn people into unwitting agents." The threat is "real and growing" but there are gaps in international arms control treaties preventing it from being tackled effectively, they say. [...]
 
The book makes the case for a new "holistic arms control" framework, rather than relying on existing arms control treaties. It sets out a number of practical steps that could be taken, including establishing a working group on CNS-acting and broader incapacitating agents. Other proposals concern training, monitoring and definitions. "We need to move from reactive to proactive governance," said Dando. Both men acknowledge that we are learning more about the brain and the central nervous system, which is good for humanity. They said they were not trying to stifle scientific progress and it was about preventing malign intent. Crowley said: "This is a wake-up call. We must act now to protect the integrity of science and the sanctity of the human mind."]]></content:encoded></item><item><title>Trump Launches Genesis Mission, a Manhattan Project-Level AI Push</title><link>https://yro.slashdot.org/story/25/11/25/009227/trump-launches-genesis-mission-a-manhattan-project-level-ai-push?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[BrianFagioli writes: President Trump has issued a sweeping executive order that creates the Genesis Mission, a national AI program he compares to a Manhattan Project level effort. It centralizes DOE supercomputers, national lab resources, massive scientific datasets, and new AI foundation models into a single platform meant to fast track research in areas like fusion, biotech, microelectronics, and advanced manufacturing. The order positions AI as both a scientific accelerator and a national security requirement, with heavy emphasis on data access, secure cloud environments, classification controls, and export restrictions.
 
The mission also sets strict timelines for identifying key national science challenges, integrating interagency datasets, enabling AI run experimentation, and creating public private research partnerships. Whether this becomes an effective scientific engine or another oversized federal program remains to be seen, but the administration is clearly pushing to frame Trump as the president who put AI at the center of U.S. research strategy.]]></content:encoded></item><item><title>Linux Kernel Developers Eye Uses For Extra General Purpose Registers With APX</title><link>https://www.phoronix.com/news/Intel-APX-Extra-GPRs-Kernel-Use</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 25 Nov 2025 01:23:16 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[With Advanced Performance Extensions (APX) on upcoming Intel processors doubling the number of general purpose registers (GPRs) among other advantages, Intel engineers are beginning to think of possible kernel uses for the extra registers...]]></content:encoded></item><item><title>Jony Ive and Sam Altman Say They Finally Have an AI Hardware Prototype</title><link>https://hardware.slashdot.org/story/25/11/25/006212/jony-ive-and-sam-altman-say-they-finally-have-an-ai-hardware-prototype?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Sam Altman and Jony Ive say they've settled on a prototype for OpenAI's first hardware device that could ship in "less than" two years. The Verge reports: In an interview with Laurene Powell Jobs at Emerson Collective's 2025 Demo Day, they said they are currently prototyping the device, and when asked about a timeframe, Ive said it could arrive in "less than" two years. Little has been revealed so far about the OpenAI device in development, but it's rumored to be screen-free and "roughly the size of a smartphone."
 
Altman described the design as "simple and beautiful and playful," adding that, "There was an earlier prototype that we were quite excited about, but I did not have any feeling of, "I want to pick up that thing and take a bite out of it,' and then finally we got there all of a sudden." Ive similarly emphasized simplicity and whimsy, saying, "I love solutions that teeter on appearing almost naive in their simplicity, and I also love incredibly intelligent, sophisticated products that you want to touch, and you feel no intimidation, and you want to use almost carelessly, that you use them almost without thought, that they're just tools." Altman went on to comment, "I hope that when people see it, they say, 'That's it!,'" to which Ive responded, "Yeah, they will."]]></content:encoded></item><item><title>Google teams up with Accel to hunt for India’s next AI breakouts</title><link>https://techcrunch.com/2025/11/24/google-teams-up-with-accel-to-hunt-for-indias-next-ai-breakouts/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Tue, 25 Nov 2025 00:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google and Accel will jointly invest up to $2 million in each startup through their new partnership.]]></content:encoded></item><item><title>Japan&apos;s High-Stakes Gamble To Turn Island of Flowers Into Global Chip Hub</title><link>https://slashdot.org/story/25/11/24/2248212/japans-high-stakes-gamble-to-turn-island-of-flowers-into-global-chip-hub?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Tue, 25 Nov 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the BBC: The island of Hokkaido has long been an agricultural powerhouse -- now Japan is investing billions to turn it into a global hub for advanced semiconductors. More than half of Japan's dairy produce comes from Hokkaido, the northernmost of its main islands. In winter, it's a wonderland of ski resorts and ice-sculpture festivals; in summer, fields bloom with bands of lavender, poppies and sunflowers. These days, cranes are popping up across the island -- building factories, research centers and universities focused on technology. It's part of Japan's boldest industrial push in a generation: an attempt to reboot the country's chip-making capabilities and reshape its economic future.
 
Locals say that beyond the cattle and tourism, Hokkaido has long lacked other industries. There's even a saying that those who go there do so only to leave. But if the government succeeds in turning Hokkaido into Japan's answer to Silicon Valley -- or "Hokkaido Valley", as some have begun to call it -- the country could become a new contender in the $600 billion race to supply the world's computer chips. At the heart of the plan is Rapidus, a little-known company backed by the government and some of Japan's biggest corporations including Toyota, Softbank and Sony.
 
Born out of a partnership with IBM, it has raised billions of dollars to build Japan's first cutting-edge chip foundry in decades. The government has invested $12 billion in the company, so that it can build a massive semiconductor factory or "fab" in the small city of Chitose. In selecting the Hokkaido location, Rapidus CEO Atsuyoshi Koike points to Chitose's water, electricity infrastructure and its natural beauty. Mr Koike oversaw the fab design, which will be completely covered in grass to harmonize with Hokkaido's landscape, he told the BBC. Local authorities have also flagged the region as being at lower risk of earthquakes compared to other potential sites in Japan.]]></content:encoded></item><item><title>Age Verification, Estimation, Assurance, Oh My! A Guide To The Terminology</title><link>https://www.techdirt.com/2025/11/24/age-verification-estimation-assurance-oh-my-a-guide-to-the-terminology/</link><author>Rindala Alajaji</author><category>tech</category><pubDate>Mon, 24 Nov 2025 23:26:54 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[If you’ve been following the wave of age-gating laws sweeping across the country and the globe, you’ve probably noticed that lawmakers, tech companies, and advocates all seem to be using different terms for what sounds like the same thing. Age verification, age assurance, age estimation, age gating—they get thrown around interchangeably, but they technically mean different things. And those differences matter a lot when we’re talking about your rights, your privacy, your data, and who gets to access information online.So let’s clear up the confusion. Here’s your guide to the terminology that’s shaping these laws, and why you should care about the distinctions. refers to age-based restrictions on access to online services. Age gating can be required by law or voluntarily imposed as a corporate decision. Age gating does not necessarily refer to any specific technology or manner of enforcement for estimating or verifying a user’s age. It simply refers to the fact that a restriction exists. Think of it as the concept of “you must be this old to enter” without getting into the details of how they’re checking. Think of  as the catch-all category. It covers any method an online service uses to figure out how old you are with  of confidence. That’s intentionally vague, because age assurance includes everything from the most basic check-the-box systems to full-blown government ID scanning.Age assurance is the big tent that contains all the other terms we’re about to discuss below. When a company or lawmaker talks about “age assurance,” they’re not being specific about  they’re determining your age—just that they’re trying to. For decades, the internet operated on a “self-attestation” system where you checked a box saying you were 18, and that was it. These new age-verification laws are specifically designed to replace that system. When lawmakers say they want “robust age assurance,” what they really mean is “we don’t trust self-attestation anymore, so now you need to prove your age beyond just swearing to it.”Letting the Algorithm Decide is where things start getting creepy. Instead of asking you directly, the system  your age based on data it collects about you.Analyzing your face through a video selfie or photoLooking at your online behavior—what you watch, what you like, what you postChecking your existing profile dataCompanies like Instagram have partnered with services like Yoti to offer facial age estimation. You submit a video selfie, an algorithm analyzes your face, and spits out an estimated age range. Sounds convenient, right?Here’s the problem, “estimation” is exactly that: it’s a guess. And it is inherently imprecise. Age estimation is notoriously unreliable, especially for teenagers—the exact group these laws claim to protect. An algorithm might tell a website you’re somewhere between 15 and 19 years old. That’s not helpful when the cutoff is 18, and what’s at stake is a young person’s constitutional rights.And it gets worse. These systems consistently fail for certain groups:When estimation fails (and it often does), users get kicked to the next level: actual verification. Which brings us to… is the most invasive option. This is where you have to prove your age to a certain date, rather than, for example, prove that you have crossed some age threshold (like 18 or 21 or 65). EFF generally refers to most age gates and mandates on young people’s access to online information as “age verification,” as most of them typically require you to submit hard identifiers like:Government-issued ID (driver’s license, passport, state ID)Utility bills or other documentsThis is what a lot of new state laws are actually requiring, even when they use softer language like “age assurance.” Age verification doesn’t just confirm you’re over 18, it reveals your full identity. Your name, address, date of birth, photo—everything.Here’s the critical thing to understand: age verification is really identity verification. You’re not just proving you’re old enough—you’re proving exactly who you are. And that data has to be stored, transmitted, and protected by every website that collects it.We already know how that story ends. Data breachesareinevitable. And when a database containing your government ID tied to your adult content browsing history gets hacked—and it will—the consequences can be devastating.Why This Confusion MattersPoliticians and tech companies love using these terms interchangeably because it obscures what they’re actually proposing. A law that requires “age assurance” sounds reasonable and moderate. But if that law defines age assurance as requiring government ID verification, it’s not moderate at all—it’s mass surveillance. Similarly, when Instagram says it’s using “age estimation” to protect teens, that sounds privacy-friendly. But when their estimation fails and forces you to upload your driver’s license instead, the privacy promise evaporates.Here’s the uncomfortable truth: most lawmakers writing these bills have no idea how any of this technology actually works. They don’t know that age estimation systems routinely fail for people of color, trans individuals, and people with disabilities. They don’t know that verification systems have error rates. They don’t even seem to understand that the terms they’re using mean different things. The fact that their terminology is all over the place—using “age assurance,” “age verification,” and “age estimation” interchangeably—makes this ignorance painfully clear, and leaves the onus on platforms to choose whichever option best insulates them from liability.Language matters because it shapes how we think about these systems. “Assurance” sounds gentle. “Verification” sounds official. “Estimation” sounds technical and impersonal, and also admits its inherent imprecision. But they all involve collecting your data and create a metaphysical age gate to the internet. The terminology is deliberately confusing, but the stakes are clear: it’s your privacy, your data, and your ability to access the internet without constant identity checks. Don’t let fuzzy language disguise what these systems really do.]]></content:encoded></item><item><title>Amazon Pledges Up To $50 Billion To Expand AI, Supercomputing For US Government</title><link>https://yro.slashdot.org/story/25/11/24/2233254/amazon-pledges-up-to-50-billion-to-expand-ai-supercomputing-for-us-government?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Mon, 24 Nov 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Amazon is committing up to $50 billion to massively expand AI and supercomputing capacity for U.S. government cloud regions, adding 1.3 gigawatts of high-performance compute and giving federal agencies access to its full suite of AI tools. Reuters reports: The project, expected to break ground in 2026, will add nearly 1.3 gigawatts of artificial intelligence and high-performance computing capacity across AWS Top Secret, AWS Secret and AWS GovCloud regions by building data centers equipped with advanced compute and networking technologies. The project, expected to break ground in 2026, will add nearly 1.3 gigawatts of artificial intelligence and high-performance computing capacity across AWS Top Secret, AWS Secret and AWS GovCloud regions by building data centers equipped with advanced compute and networking technologies.
 
Under the latest initiative, federal agencies will gain access to AWS' comprehensive suite of AI services, including Amazon SageMaker for model training and customization, Amazon Bedrock for deploying models and agents, as well as foundation models such as Amazon Nova and Anthropic Claude. The federal government seeks to develop tailored AI solutions and drive cost-savings by leveraging AWS' dedicated and expanded capacity.]]></content:encoded></item><item><title>Altman describes OpenAI’s forthcoming AI device as more peaceful and calm than the iPhone</title><link>https://techcrunch.com/2025/11/24/altman-describes-openais-forthcoming-ai-device-as-more-peaceful-and-calm-than-the-iphone/</link><author>Sarah Perez</author><category>tech</category><pubDate>Mon, 24 Nov 2025 22:59:46 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Altman and Ive tease a simple AI device aimed at calm, distraction-free computing, launching within two years.]]></content:encoded></item><item><title>Pebble Goes Fully Open Source</title><link>https://news.slashdot.org/story/25/11/24/2152211/pebble-goes-fully-open-source?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Mon, 24 Nov 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Core Devices has fully open-sourced the entire Pebble software stack and confirmed the first Pebble Time 2 shipments will start in January. "This is the clearest sign yet that the platform is shifting from a company-led product to a community-backed project that can survive independently," reports Gadgets & Wearables. From the report: The announcement follows weeks of tension between Core Devices and parts of the Pebble community. By moving from 95 to 100 percent open source, the company has essentially removed itself as a bottleneck. Users can now build, run, and maintain every piece of software needed to operate a Pebble watch. That includes firmware for the watch and mobile apps for Android and iOS. This puts the entire software stack into public hands. According to the announcement, Core Devices has released the mobile app source code, enabled decentralized app distribution, and made hardware more repairable with replaceable batteries and published design files.]]></content:encoded></item><item><title>OpenAI learned the hard way that Cameo trademarked the word ‘cameo’</title><link>https://techcrunch.com/2025/11/24/openai-learned-the-hard-way-that-cameo-trademarked-the-word-cameo/</link><author>Amanda Silberling</author><category>tech</category><pubDate>Mon, 24 Nov 2025 22:27:10 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI might have to rename the "cameo" feature in the Sora app.]]></content:encoded></item><item><title>Arduino&apos;s New Terms of Service Worries Hobbyists Ahead of Qualcomm Acquisition</title><link>https://hardware.slashdot.org/story/25/11/24/2144256/arduinos-new-terms-of-service-worries-hobbyists-ahead-of-qualcomm-acquisition?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Mon, 24 Nov 2025 22:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: Some members of the maker community are distraught about Arduino's new terms of service (ToS), saying that the added rules put the company's open source DNA at risk. Arduino updated its ToS and privacy policy this month, which is about a month after Qualcomm announced that it's acquiring the open source hardware and software company. Among the most controversial changes is this addition: "User shall not: translate, decompile or reverse-engineer the Platform, or engage in any other activity designed to identify the algorithms and logic of the Platform's operation, unless expressly allowed by Arduino or by applicable license agreements ..."
 
In response to concerns from some members of the maker community, including from open source hardware distributor and manufacturer Adafruit, Arduino posted a blog on Friday. Regarding the new reverse-engineering rule, Arduino's blog said: "Any hardware, software or services (e.g. Arduino IDE, hardware schematics, tooling and libraries) released with Open Source licenses remain available as before. Restrictions on reverse-engineering apply specifically to our Software-as-a-Service cloud applications. Anything that was open, stays open."
 
But Adafruit founder and engineer Limor Fried and Adafruit managing editor Phillip Torrone are not convinced. They told Ars Technica that Arduino's blog leaves many questions unanswered and said that they've sent these questions to Arduino without response. "Why is reverse-engineering prohibited at all for a company built on openly hackable systems?" Fried and Torrone asked in a shared statement. There are also concerns about the ToS' broad new AI-monitoring powers, which offer little clarity on what data is collected, who can access it, or how long it's retained. On top of that, the update introduces an unusual patent clause that bars users from using the platform to identify potential infringement by Arduino or its partners, along with sweeping, perpetual rights over user-generated content. This could allow Arduino, and potentially Qualcomm, to republish, modify, monetize, or redistribute user uploads indefinitely.]]></content:encoded></item><item><title>CNN’s Latest Innovation: Slathering Your Screen With T-Mobile Ads</title><link>https://www.techdirt.com/2025/11/24/cnns-latest-innovation-slathering-your-screen-with-t-mobile-ads/</link><author>Karl Bode</author><category>tech</category><pubDate>Mon, 24 Nov 2025 21:28:54 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[CNN, like most U.S. cable news networks, professes to provide users access to journalism. Instead, what you’ll most consistently find is a sort of generic, ad-slathered, center-right, corporatist drivel with the rough edges (read: truth) sanded off like a Ken doll’s genitals to avoid offense. “View from nowhere” journalism that doesn’t inform so much as it tries to present a sanitized, corporate-friendly, apolitical platter of feckless infotainment mush.Usually this kind of pseudo-journalism directly reflects the interests of corporate ownership, something that’s been particularly true with Time Warner CEO David Zaslav. Zaslav has tried to “fix” CNN’s sagging viewership by routinely doing all the wrong things, because  (harder-nosed journalism, better quality content, hiring more actual reporters) don’t really align with his or the company’s broader interests. Because when people ask “how can we improve CNN?” “More ads,” was almost certainly the inevitable response.Later on in the story, Variety just copies and pastes the weird, baseless claims of a T-Mobile marketing official who pretends this is at all useful or good:This is the steady deterioration of U.S. media as it increasingly consolidates at the hands of corporations and billionaires who are utterly incapable of innovating in journalism. In part because they don’t want innovation in journalism (that would inevitably result in more criticism of how and why they got wealthy); they want a sort of journalistic infotainment simulacrum. On the plus side, the steady devolution of U.S. mainstream media into a mishmash of soggy advertorial invertebrates creates new opportunities for real journalism, reporters with a backbone, direct-to-audience newsletters, worker-owned media outlets, and alternative media sources actually interested in serving the public interest. ]]></content:encoded></item><item><title>Rad Power Bikes’ batteries receive major fire risk warning</title><link>https://techcrunch.com/2025/11/24/rad-power-bikes-batteries-receive-major-fire-risk-warning/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Mon, 24 Nov 2025 21:28:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The U.S Consumer Product Safety Commission claims Rad Power "refused to agree to an acceptable recall." Rad Power says the CPSC's solution would bankrupt the company.]]></content:encoded></item><item><title>Americans Are Holding Onto Devices Longer Than Ever</title><link>https://slashdot.org/story/25/11/24/1754230/americans-are-holding-onto-devices-longer-than-ever?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 21:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: The average American now holds onto their smartphone for 29 months, according to a recent survey by Reviews.org, and that cycle is getting longer. The average was around 22 months in 2016. 

[...] Research released by the Federal Reserve last month concludes that each additional year companies delay upgrading equipment results in a productivity decline of about one-third of a percent, with investment patterns accounting for approximately 55% of productivity gaps between advanced economies. 

The good news: businesses in the U.S. are generally quicker to reinvest in replacing aging equipment. The Federal Reserve report shows that if European productivity had matched U.S. investment patterns starting in 2000, the productivity gap between the U.S and European economic heavyweights would have been reduced by 29 percent for the U.K., 35 percent for France, and 101% for Germany.]]></content:encoded></item><item><title>Tesla FSD software may not be approved by EU regulator after all</title><link>https://techcrunch.com/2025/11/24/tesla-fsd-software-may-not-be-approved-by-eu-regulator-after-all/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Mon, 24 Nov 2025 21:10:37 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Tesla claimed in a weekend social media post that a Dutch regulator was set to approve its Full Self-Driving mode. It seems the regulator isn't quite in line with Tesla.]]></content:encoded></item><item><title>Udio Users Can&apos;t Download Their AI Music Creations Anymore</title><link>https://entertainment.slashdot.org/story/25/11/24/1845207/udio-users-cant-download-their-ai-music-creations-anymore?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 20:43:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: As part of the settlement with Universal, Udio has amended its terms of service, and users can no longer download their outputs. This has AI music makers furious, and with good reason. Unfortunately, they have little recourse, as the contract they sign when creating a Udio account includes a waiver of the right to bring a class action.]]></content:encoded></item><item><title>Hands on with Stickerbox, the AI-powered sticker maker for kids</title><link>https://techcrunch.com/2025/11/24/hands-on-with-stickerbox-the-ai-powered-sticker-maker-for-kids/</link><author>Sarah Perez</author><category>tech</category><pubDate>Mon, 24 Nov 2025 20:25:20 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Stickerbox turns kids’ ideas into printable stickers, blending AI magic with hands-on coloring for a surprisingly creative, "screen-light" play experience.]]></content:encoded></item><item><title>Rust For Linux Kernel Co-Maintainer Formally Steps Down</title><link>https://www.phoronix.com/news/Alex-Gaynor-Rust-Maintainer</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 20:09:15 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Alex Gaynor recently announced he is formally stepping down as one of the maintainers of the Rust for Linux kernel code with the removal patch now queued for merging in Linux 6.19...]]></content:encoded></item><item><title>Obesity Jab Drug Fails To Slow Alzheimer&apos;s</title><link>https://science.slashdot.org/story/25/11/24/1834244/obesity-jab-drug-fails-to-slow-alzheimers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 20:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Drug maker Novo Nordisk says semaglutide, the active ingredient for the weight loss jab Wegovy, does not slow Alzheimer's -- despite initial hopes that it might help against dementia. From a report: Researchers began two large trials involving more than 3,800 people after reports the medicine was having an impact in the real world. But the studies showed the GLP-1 drug, which is already used to manage type 2 diabetes and obesity, made no difference compared to a dummy drug. The disappointing results are due to be presented at an Alzheimer's disease conference next month and are yet to be published in a peer-reviewed journal.]]></content:encoded></item><item><title>If Your Antitrust Case Depends On Pretending TikTok Doesn’t Exist, It’s Going To Fail</title><link>https://www.techdirt.com/2025/11/24/if-your-antitrust-case-depends-on-pretending-tiktok-doesnt-exist-its-going-to-fail/</link><author>Mike Masnick</author><category>tech</category><pubDate>Mon, 24 Nov 2025 19:55:54 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Last week’s dismissal of the FTC’s antitrust case against Meta—combined with the earlier limited remedies in the Google search case—demonstrates something that should be obvious by now: antitrust is a pathetically weak tool for increasing competition in digital markets.This isn’t an argument against competition. Competition in digital markets matters, desperately. But antitrust enforcement is slow, cumbersome, and nearly blind to how fast these markets actually move. It takes years to litigate, offers limited effective remedies, and by the time courts rule, the competitive threats have often shifted entirely. The whole apparatus works fine for more slow-moving industries (which have real competition problems!) but consistently fails when applied to more dynamic markets where the landscape changes every few years.Over the last decade, figures like Lina Khan and Tim Wu have pushed a more aggressive vision of antitrust—variously called “hipster antitrust” or “neo-Brandeisian antitrust”—that promises to ignore these limitations and wield antitrust as a more punitive tool against large companies. The theory goes that punishing big companies will magically result in greater competition, a kind of antitrust trickle-down economics. The results of the Meta and Google cases suggest that if we want more competition in the digital space, there are much better policy levers than antitrust.As Judge Boasberg noted in his long and thorough opinion, the FTC’s bizarre attempt to define the market Meta was supposedly a monopolist in didn’t pass the laugh test. Notably, the FTC insisted that Meta’s market was just for “personal social networking” among friends and family, in an attempt to avoid the continued growing success of TikTok and YouTube as competitors. Thus, the FTC said the competition for Facebook and Instagram was just the much smaller Snapchat and the barely existing MeWe.As Boasberg noted, the FTC had to show that Meta continues to have a monopoly in the marketplace to win the case, and the only way the FTC could win that argument was if TikTok and YouTube were excluded from the market definition. But that is laughable:The FTC contends that Facebook, Instagram, and Snapchat form a distinct market that can be identified by those apps’ unique features. While those apps certainly show some distinct markings, they mostly resemble two other social-media apps that the FTC insists must be excluded: TikTok and YouTube. Their dominant features are identical, people mostly use all four to watch unconnected content that they can send in direct messages, industry participants agree that the apps belong in the same competitive market, they use similar resources and technologies, and they charge the same price to the same customers.Even when considering only qualitative evidence, the Court finds that Meta’s apps are reasonably interchangeable with TikTok and YouTube…. Taking all the evidence together, it shows that personal social networking is not a separate product market. Instead, Meta competes in the market for social media, and that market includes — at minimum — TikTok and YouTube as well.The opinion repeatedly demonstrates that Meta was terrified of the growing success of TikTok (and, to a lesser extent, YouTube) and kept adjusting its products (hello “Reels”) to be more like those other apps.The court also demolished the FTC’s claim that Meta was harming consumers by making its products worse. Quite the opposite according to the actual evidence:So the FTC instead argues that Meta has degraded these apps’ quality. By offering a worse product for the same price, the agency reasons, Meta has imposed the equivalent of a price increase.The record, however, shows the opposite: Meta’s apps have continuously improved. The company has added scores of new features to Facebook and Instagram, from Stories to Reels to Marketplace…. The Court simply does not find it credible that users would prefer the Facebook and Instagram apps that existed ten years ago to the versions that exist todayThe court points to plenty of natural experiments (bans, downtime, etc.) that show that many users consider the Instagram/Facebook Reels effectively interchangeable with TikTok and YouTube Shorts.The broader problem here is that by the time the case reached trial, the competitive landscape had already shifted dramatically. Meta’s supposed monopoly was being actively challenged by TikTok’s explosive growth, forcing Meta to completely overhaul its products. The FTC’s case depended on freezing the market in time and pretending this competition didn’t exist.And, really, this all shows how terrible a tool antitrust is to deal with these markets.The Google case—which the DOJ technically won—suffered from a similar dynamic. Judge Amit Mehta recognized that the market had shifted quite a bit on its own, with Google’s search dominance being challenged by AI tools like ChatGPT. The remedies he imposed came up far short of what the government requested, precisely because the competitive threats were already emerging without court intervention.This is not to say that antitrust never makes sense or that we don’t need more competitive markets. But the fact that the FTC has been converted, under both administrations, to be more focused on punishing companies, rather than actually pursuing policies that increase competition is a problem.Tim Wu wrote an angry response to Boasberg’s decision in the NY Times, and in doing so, accidentally revealed the core problem with the neo-Brandeisian approach. When you strip away the legal arguments, it all comes down to vibes:Does anyone seriously doubt that Meta is the kind of company that antitrust laws were designed to restrain?That right there gives away the game. If your antitrust case is built on “doesn’t this company feel bad?” you’re going to take shortcuts, ignore inconvenient facts like the existence of TikTok, and then fail in court.Wu’s piece is instructive because it shows how the FTC arrived at its laughable market definition. He claims Boasberg dismissed the case “in the face of strong evidence to the contrary, not to mention common sense,” but the “common sense” he’s appealing to is just the intuition that Meta seems big and powerful. The actual evidence—the stuff Boasberg spent pages analyzing—showed robust competition forcing Meta to completely overhaul its products.Wu even complains that recognizing TikTok and YouTube as competitors represents “strained legal thinking” because they’re “adjacent markets.” But the whole point of antitrust law is to stop companies from abusing monopoly power to prevent competition. Showing that competition exists and is forcing the alleged monopolist to adapt its products is not a technicality—it’s proof that the market is working.There are ways to bring good antitrust cases, but they have to involve showing that there’s an actual monopoly under the law, that the monopoly is being abused by the monopolist in order to limit further competition and/or make products worse for consumers.When you start from “Meta feels like a monopoly” and work backward, you end up failing to make the case the law actually requires, and that doesn’t actually help enable a more competitive marketplace. The FTC was so focused on the vibes and how Meta  that it failed to make the actual case it needed to make.If we want actual competition in the marketplace, maybe stop focusing so much on antitrust laws and look at the issues that keep holding back actual competition: clean up broken copyright and patent laws that restrict competition, fix the CFAA which has been used repeatedly by big tech companies to stifle competition, and stop trying to pass laws that would make it impossible for smaller startups to exist because of the compliance costs.Those would actually enable much greater competition, but no one wants to do the hard work on those to ensure actual competition exists.]]></content:encoded></item><item><title>Why Monad Chose Enso to Power Its $2.5 Billion Mainnet Launch</title><link>https://hackernoon.com/why-monad-chose-enso-to-power-its-$25-billion-mainnet-launch?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Mon, 24 Nov 2025 19:37:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[What happens when a blockchain launches without applications ready to use it?Typically, months of waiting while developers scramble to build integrations manually. This gap between launch and utility has plagued nearly every major blockchain debut, but Monad's approach signals a different strategy.\
The Layer-1 blockchain, which went live on November 24, launched with complete DeFi functionality already operational. This was made possible through Enso's day-one integration, which eliminates the traditional development bottleneck that has limited previous blockchain ecosystems.The Integration Gap That Slows Every BlockchainBlockchain development follows a predictable pattern: launch the network, wait for developers to build applications, hope users eventually arrive. The problem is in the middle stage. According to research on blockchain developer activity, developers face technical barriers between different blockchain networks and protocols, requiring manual integration and maintenance work that can consume six months or more of development time.\
Ecosystems with more than 5,000 monthly active developers show more application launches CoinLaw, but reaching that threshold takes time. Base onboarded over 1,600 developers within its first year, which is considered strong performance. Most networks take considerably longer to establish a developer base capable of building functional applications.\
The result is a familiar chicken-and-egg scenario. Users avoid blockchains without applications. Developers avoid blockchains without users. Liquidity fragments across competing networks. The network effect that should accelerate growth instead becomes a barrier to entry.\
Monad's integration with Enso addresses this by providing developers access to pre-built protocol interactions from the start. Rather than spending months learning smart contract specifications and building custom integrations, developers can deploy applications that interact with swaps, bridges, lending markets, and other DeFi primitives immediately.What Blockchain Shortcuts Actually Mean for DevelopersEnso operates as a shared engine that standardizes blockchain interactions across protocols. The platform has mapped interactions across more than 160 protocols on multiple chains, creating reusable building blocks that developers can access through a single API.\
Think of it this way: building a DeFi application typically requires understanding how each protocol handles deposits, withdrawals, swaps, and other core functions. Aave handles lending differently than Compound. Uniswap routes trades differently than Curve. Each requires separate integration work, separate audits, and separate maintenance.\
Enso abstracts these differences by creating standardized shortcuts. A developer can call a "lend" function without needing to know whether it's executing on Aave, Compound, or another protocol. The Enso engine handles routing, optimization, and execution.\
For Monad builders, this means immediate access to functionality that would otherwise require extensive development time. Milos Costantini, Enso Co-Founder, said, "Supporting Monad from day one reflects exactly what Enso was built for: giving builders immediate access to the liquidity and tooling they need to ship valuable products. With Enso plugged into Monad at launch, teams can start creating sophisticated DeFi flows instantly, from swaps and lending to cross-chain markets."\
The technical implementation combines actions that abstract isolated smart contract transactions into simple components. These actions can be combined into shortcuts that create reusable workflows. Instead of manually integrating every protocol, developers define their intended outcome and let Enso's network coordinate the solution.Why Launch Liquidity Determines Network Success\
Yet technical capabilities alone don't guarantee adoption. Users need applications that let them actually use the blockchain. Enso's integration ensures Monad users can deploy assets immediately across trading, lending, and yield strategies rather than waiting for ecosystem development.\
Enso's role extends beyond launch support. The platform has enabled $17 billion in onchain settlements across its network, working with projects including Berachain, Uniswap, LayerZero, and Sushiswap. This track record provides Monad developers access to tested infrastructure rather than experimental tooling.The Development Time Problem That Enso SolvesTraditional blockchain development requires building everything from scratch. A team creating a liquidity aggregator needs to integrate with every decentralized exchange they want to access. Each integration requires understanding that exchange's specific smart contracts, building custom routing logic, conducting security audits, and maintaining the code as protocols update.\
Enso reduces this timeline dramatically. Developers using the platform can access DeFi functionality within days rather than months. The shortcuts handle protocol-specific details, security considerations, and optimization automatically. This allows teams to focus on user experience, product features, and go-to-market strategy rather than infrastructure.\
For Monad, this means the network can launch with a functional ecosystem rather than promising future development. Developers can build applications that work with lending protocols, decentralized exchanges, bridging solutions, and liquidity pools without individual integration work.\
The architecture uses a network of participants who contribute to the system. Action Providers publish smart contract abstractions. Graphers plot efficient paths for executing actions. Validators ensure solution accuracy and network security. This decentralized approach creates a self-improving system where the quality and breadth of available shortcuts expands over time.What This Means for the Monad EcosystemMonad allocated tokens to approximately 225,000 verified onchain users The Block through an airdrop designed to reward active participants in the DeFi ecosystem. Recipients included users of protocols ranging from Aave to Pump.fun, high-volume DEX traders, and holders of notable NFT collections.\
These users can immediately deploy their MON tokens across DeFi applications because Enso's integration provides the necessary infrastructure. Rather than waiting for developers to build out the ecosystem, users can access trading, lending, and liquidity provision from day one.\
This creates a different adoption curve than typical blockchain launches. Early users have functionality immediately available. Developers can build sophisticated applications quickly. Liquidity can flow into productive uses rather than sitting idle. The network effect that usually takes months to establish can begin on launch day.\
The integration also establishes a template for future blockchain launches. Rather than accepting weeks or months of limited functionality while developers build integrations, networks can partner with infrastructure providers like Enso to ensure comprehensive tooling from the start.Blockchain launches typically suffer from a timing problem: networks go live with impressive specifications but limited utility. Users find few applications. Developers see little reason to build without users. The ecosystem remains dormant until enough parties simultaneously decide to participate.\
Enso's Monad integration addresses this by ensuring functionality precedes the launch. Developers can build immediately. Users can deploy assets from day one rather than waiting for infrastructure to catch up. Whether this approach sets a new standard depends on execution. Monad must deliver on its technical promises, and applications built using Enso's shortcuts must provide sustained value beyond launch-day speculation.\
But the integration demonstrates that the traditional blockchain launch bottleneck has a solution. Networks that partner with infrastructure providers can compress months of waiting into immediate functionality, giving developers faster time to market and users immediate utility from new networks.\
Don’t forget to like and share the story! ]]></content:encoded></item><item><title>Google&apos;s &apos;Aluminium OS&apos; Will Eventually Replace ChromeOS With Android</title><link>https://tech.slashdot.org/story/25/11/24/1855243/googles-aluminium-os-will-eventually-replace-chromeos-with-android?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 19:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Google's long-rumored plan to merge ChromeOS and Android into a single desktop operating system now has a name: Aluminium OS, AndroidAuthority reports, citing a job listing. 

The job listing explicitly tasks applicants with "working on a new Aluminium, Android-based, operating system." The job listing confirms Google intends to eventually replace ChromeOS entirely, though the two platforms will coexist during a transition period. Aluminium OS won't be limited to budget hardware -- the listing references "AL Entry," "AL Mass Premium," and "AL Premium" tiers across laptops, detachables, tablets, and mini-PCs.]]></content:encoded></item><item><title>AWS is spending $50B to build AI infrastructure for the US government</title><link>https://techcrunch.com/2025/11/24/aws-is-spending-50b-build-ai-infrastructure-for-the-us-government/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Mon, 24 Nov 2025 19:10:41 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AWS has been working with the U.S. government since 2011 and is now building AI infrastructure specifically for the entity. ]]></content:encoded></item><item><title>Anthropic releases Opus 4.5 with new Chrome and Excel integrations</title><link>https://techcrunch.com/2025/11/24/anthropic-releases-opus-4-5-with-new-chrome-and-excel-integrations/</link><author>Russell Brandom</author><category>tech</category><pubDate>Mon, 24 Nov 2025 19:08:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Anthropic has launched Opus 4.5, the latest version of its flagship model, with new integrations into Chrome and Excel.]]></content:encoded></item><item><title>X-energy rides nuclear wave, raises $700M Series D</title><link>https://techcrunch.com/2025/11/24/x-energy-rides-nuclear-wave-raises-700m-series-d/</link><author>Tim De Chant</author><category>tech</category><pubDate>Mon, 24 Nov 2025 18:55:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The small modular reactor startup is capitalizing on surging interest from tech companies and data center developers.]]></content:encoded></item><item><title>Federal Judge Spends Most Of 233 Pages Calling Out DHS, ICE, CBP, Border Patrol For Their Lies</title><link>https://www.techdirt.com/2025/11/24/federal-judge-spends-most-of-233-pages-calling-out-dhs-ice-cbp-border-patrol-for-their-lies/</link><author>Tim Cushing</author><category>tech</category><pubDate>Mon, 24 Nov 2025 18:43:46 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Then he was sent to Chicago by a vengeful Trump who wanted to see as many people in the state harmed for no other reason than he didn’t like the politicians running the state and the city. While Trump was prevented from flooding Chicago streets with National Guard troops, Commander Bovino did everything he could to turn federal forces into full-time assailants. And by “everything,” I mean things from lying about being attacked (to justify force deployments) to personally violating court orders limiting the use of force by federal officers. Judge Sara Ellis has been handling the lawsuit brought against federal agencies since their invasion of the Windy City. (And she’s already seen one of her orders blocked by the Seventh Circuit Appeals Court.) What should be a final ruling (pending review by the appellate court, which has also already placed a stay on  order) has been handed down by Judge Ellis. And that order makes it clear the federal government has done nothing but lie since being forced to answer questions in court. “While Defendants may argue that the Court identifies only minor inconsistencies, every minor inconsistency adds up, and at some point, it becomes difficult, if not impossible, to believe almost anything that Defendants represent,” Judge Sarah Ellis wrote about the administration in a scathing 233-page ruling.And scathing it is. Aaron Reichlin-Melnick does a good job hitting a lot of the low points in this long Bluesky thread. We’ll do more of that here, because the amount of lying done here is staggering in both its expansiveness and its implication: that the Trump administration (in all forms) apparently believes no system of checks and balances can hold it back. Here’s how little the government cared about the lawsuit brought against it, much less the people it harmed on its way to being sued. From the opinion [PDF]:Plaintiffs submitted a mountain of evidence, providing the Court with over eighty declarations, numerous videos and articles, and other evidence. Defendants did not rebut anything that Plaintiffs set forth in their declarations or testimony, even with BWC footage.That’s just galling. Most defendants will at least try to defend themselves from accusations. The government — taking the form of multiple federal agencies participating in Trump’s “Midway Blitz” operation — couldn’t even be bothered to do that. Instead, it just kept on being violent, secure in the knowledge that courts can’t actually keep them from violating rights. They can, at best, only slow them down a bit. What the government did offer in its defense undercut its claims immediately.Presumably, these portions of the videos would be Defendants’ best evidence to demonstrate that agents acted in line with the Constitution, federal laws, and the agencies’ own policies on use of force when engaging with protesters, the press, and religious practitioners. But a review of them shows the opposite—supporting Plaintiffs’ claims and undermining all of Defendants’ claims that their actions toward protesters, the press, and religious practitioners have been, as Bovino has stated, “more than exemplary.”For example, Defendants directed the Court to two videos of agents outside the Broadview facility the evening of September 19, 2025. In those videos, agents stand behind a fence preparing to leave the facility’s gates and disperse what Defendants described as an unruly mob. The scene appears quiet as the gate opens, revealing a line of protesters standing in the street holding signs. Almost immediately and without warning, agents lob flashbang grenades, tear gas, and pepper balls at the protesters, stating, “fuck yea!”, as they do so, and the crowd scatters. This video disproves Defendants’ contentions that protesters were the ones shooting off fireworks, refusing orders, and acting violently so as to justify the agents’ use of force.On September 26, 2025, video from an agent’s BWC shows a line of agents standing at least thirty feet away from protesters outside the Broadview facility on Harvard Street. Despite this distance, the agents start yelling “move back, move back” to the protesters and then shoot pepper balls and tear gas at them without any apparent justification.Defendants also highlighted an October 3, 2025 video, presumably to show that agents driving the streets faced constant danger from cars ramming them on purpose. But instead of leaving this impression, the video, which almost entirely consists of a view of the back seat of the car and some dialogue about how the agent’s “body cam is on” and he is “still recording,” suggests that the agent drove erratically and brake-checked other motorists in an attempt to force accidents that agents could then use as justifications for deploying force.This is a 233-page deconstruction of every lie and false assertion made by the government. There’s a lot to see here and it should be seen because every lie told by the government should mean something to anyone who still cares about democracy.But we’re only nine pages in and already the judge is telling it like it is, ensuring that no one who doesn’t have the patience to go through the entire opinion will come away with the wrong impression after skimming through the first dozen pages or so: These are not the only inconsistencies and incredible representations in the record. While Defendants may argue that the Court identifies only minor inconsistencies, every minor inconsistency adds up, and at some point, it becomes difficult, if not impossible, to believe almost anything that Defendants represent.Pretty much everyone called to testify on behalf of the government lied to some degree in hopes of salvaging some truly insane (and literally incredible) claims:[W]hen questioned about these instances in his deposition, [ICE Field Office Director] Hott acknowledged that he did not even know if it was a person that caused the damage to the downspout, much less a protester, and that he did not have proof that the agent’s beard was actually ripped off his face.Bovino’s and Hewson’s explanations about individuals in maroon hoodies being associated with the Latin Kings and threats strains credulity.Turning to Bovino, the Court specifically finds his testimony not credible. Bovino appeared evasive over the three days of his deposition, either providing “cute” responses to Plaintiffs’ counsel’s questions or outright lying.Most tellingly, Bovino admitted in his deposition that he lied multiple times about the events that occurred in Little Village that prompted him to throw tear gas at protesters.The entire opinion makes it clear federal officers routinely engaged in unprovoked violence and then lied about their actions in court. They were ostensibly led by Gregory Bovino, who provided an example for them to follow by  engaging in plenty of unprovoked violence himself and who, when called to testify, acted like the whole thing was beneath him and was a suitable target for his open mockery.Bovino has since been sent elsewhere, not because Trump wants to stop him from generating further negative press or court precedent, but because Trump wants him to engage in the same sort of unconstitutional cruelty for as long as possible before litigation ensues. Bovino is the perfect front man for Trump’s bigoted attacks on cities and states he doesn’t like. Like Trump himself, Bovino wraps himself in the flag. And just like Trump, the only reason he does is so he can wipe his ass with it, staining everything it’s supposed to stand for. ]]></content:encoded></item><item><title>Science-Centric Streaming Service Curiosity Stream is an AI-licensing Firm Now</title><link>https://tech.slashdot.org/story/25/11/24/1643247/science-centric-streaming-service-curiosity-stream-is-an-ai-licensing-firm-now?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 18:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Curiosity Stream, the decade-old science documentary streaming service founded by Discovery Channel's John Hendricks, expects its AI licensing business to generate more revenue than its 23 million subscribers by 2027 -- possibly earlier. The company's Q3 2025 earnings revealed a 41% year-over-year revenue increase, driven largely by deals licensing its content to train large language models. Year-to-date AI licensing brought in $23.4 million through September, already exceeding half of what the subscription business generated for all of 2024. 

The streaming service's library contains 2 million hours of content, but the "overwhelming majority" is earmarked for AI licensing rather than subscriber viewing, CEO Clint Stinchcomb said during the earnings call. Curiosity Stream is licensing 300,000 hours of its own programming and 1.7 million hours of third-party content to hyperscalers and AI developers. The company has completed 18 AI-related deals across video, audio, and code assets.]]></content:encoded></item><item><title>Daily Deal: The Complete Machine Learning Bundle</title><link>https://www.techdirt.com/2025/11/24/daily-deal-the-complete-machine-learning-bundle-2/</link><author>Daily Deal</author><category>tech</category><pubDate>Mon, 24 Nov 2025 18:38:46 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[Dive into the world of self-driving cars, speech recognition technology, and more with the Complete Machine Learning Bundle. Over 10 courses, you will learn about pattern recognition and prediction, and how to harness the power of machine learning to take your programming to the next level. Discover quant trading, how to harness large data sets, and much more. It’s on sale for $30.Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.]]></content:encoded></item><item><title>AlmaLinux 10.1 Released - Complete With Btrfs Support</title><link>https://www.phoronix.com/news/AlmaLinux-10.1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 18:35:57 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Building off the release of Red Hat Enterprise Linux 10.1 from two weeks ago, AlmaLinux 10.1 is now available in GA form for this community-oriented RHEL10 downstream. Making AlmaLinux 10.1 all the more interesting is the project's decision to promote Btrfs file-system support...]]></content:encoded></item><item><title>America’s Polarization Has Become the World&apos;s Side Hustle</title><link>https://www.404media.co/americas-polarization-has-become-the-worlds-side-hustle/</link><author>Jason Koebler</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/11/CleanShot-2025-11-24-at-10.19.42.png" length="" type=""/><pubDate>Mon, 24 Nov 2025 18:31:57 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[A new feature on X is making people suddenly realize that some large portion of the divisive, hateful, and spammy content designed to inflame tensions or, at the very least, is designed to get lots of engagement on social media, is being published by accounts that are pretending to be based in the United States but are actually being run by people in countries like Bangladesh, Vietnam, India, Cambodia, Russia, and other countries. An account called “Ivanka News” is based in Nigeria, “RedPilledNurse” is from Europe, “MAGA Nadine” is in Morocco, “Native American Soul” is in Bangladesh, and “Barron Trump News” is based in Macedonia, among many, many of others. Inauthentic viral accounts on X are just the tip of the iceberg, though, . A huge amount of the viral content about American politics and American news on social media is from sock puppet and bot accounts monetized by people in other countries. The rise of easy to use, free AI generative tools have supercharged this effort, and social media monetization programs have incentivized this effort and are almost entirely to blame. The current disinformation and slop phenomenon on the internet today makes the days of ‘Russian bot farms’ and ‘fake news pages from Cyprus’ seem quaint; the problem is now fully decentralized and distributed across the world and is almost entirely funded by social media companies themselves. This will not be news to people who have been following 404 Media, because I have done multiple investigations about the perverse incentives that social media and AI companies have created to incentivize people to fill their platforms with slop. But what has happened on X is the same thing that has happened on Facebook, Instagram, YouTube, and other social media platforms (it is also happening to the internet as a whole, with AI slop websites laden with plagiarized content and SEO spam and monetized with Google ads). Each social media platform has either an ad revenue sharing program, a “creator bonus” program, or a monetization program that directly pays creators who go viral on their platforms. ]]></content:encoded></item><item><title>Google Denies &apos;Misleading&apos; Reports of Gmail Using Your Emails To Train AI</title><link>https://tech.slashdot.org/story/25/11/24/1625249/google-denies-misleading-reports-of-gmail-using-your-emails-to-train-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 18:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Google is pushing back on viral social media posts and articles like this one by Malwarebytes, claiming Google has changed its policy to use your Gmail messages and attachments to train AI models, and the only way to opt out is by disabling "smart features" like spell checking. 

But Google spokesperson Jenny Thomson tells The Verge that "these reports are misleading -- we have not changed anyone's settings, Gmail Smart Features have existed for many years, and we do not use your Gmail content for training our Gemini AI model."]]></content:encoded></item><item><title>AI&apos;s Paradoxical Path to New Math: To Find Better Answers, It Needs Less Data and a &quot;Dumber&quot; Brain</title><link>https://hackernoon.com/ais-paradoxical-path-to-new-math-to-find-better-answers-it-needs-less-data-and-a-dumber-brain?source=rss</link><author>Anthony Laneau</author><category>tech</category><pubDate>Mon, 24 Nov 2025 18:00:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[1.0 Introduction: The New Shape of DiscoveryWhen we think about artificial intelligence tackling creative or scientific challenges, we often imagine a machine that simply produces answers; a better algorithm, a new molecule, a winning strategy. We see it as an automated problem-solver. But a groundbreaking paper on a system called AlphaEvolve offers a window into a far more interesting paradigm, one where the real discovery isn't just the solution, but the process itself. \
By setting this AI loose on dozens of open problems in mathematics, researchers have uncovered not just new mathematical constructions, but fundamental new principles about how we can use AI to explore complex, abstract worlds at a scale never before possible.\
AlphaEvolve demonstrates a new form of human-AI collaboration where the machine doesn't just find an answer, but helps us evolve entirely new ways of  answers. It suggests a future where AI acts less like a calculator and more like a creative partner, capable of studying large classes of problems at a time in ways that complement, challenge, and augment human intuition. This article distills the five most surprising and impactful takeaways from this research, revealing a future of discovery that is more nuanced and collaborative than we might have expected.2.0 Takeaway 1: AI Doesn't Just Solve Problems; It Evolves How to Solve ThemThe most fundamental insight from the AlphaEvolve project is a shift in what we ask the AI to do. Instead of asking a Large Language Model (LLM) to directly generate a mathematical object, like a specific graph or a set of points—the system asks the LLM to generate programs that search for that object.\
This creates a "meta-level evolution" where the optimization process itself becomes the object of optimization. This is the difference between evolving a single award-winning recipe and evolving a master chef who can invent thousands of them. AlphaEvolve isn't just creating solutions; it's creating solution-finders. The system maintains a population of programs, each representing a unique search heuristic or strategy. In each evolutionary step, it tries to evolve a better "improver" function that can take the current best solution and find an even better one.\
This approach is incredibly powerful because it resolves a core bottleneck in AI-driven discovery: the speed disparity between a slow, expensive LLM call and a fast, cheap computation. By using the LLM's creativity to design efficient search strategies, those strategies can then be run cheaply and at a massive scale. A single LLM call to generate a new search heuristic can trigger a massive computation where that heuristic explores millions of possibilities on its own. It's a profound shift from evolving answers to evolving algorithms that find answers.\
Instead of evolving programs that directly generate a construction, AlphaEvolve evolves programs that search for a construction. This is what we refer to as the search mode of AlphaEvolve… Each program in AlphaEvolve’s population is a search heuristic.3.0 Takeaway 2: To Discover Universal Truths, Give the AI Less DataIn an era dominated by the "big data" paradigm, where more information is almost always seen as better, AlphaEvolve's experiments uncovered a deeply counterintuitive principle. When the goal was not just to find a solution, but an elegant and interpretable formula that could generalize across a wide range of parameters, researchers found that less is more.\
When tasked with finding general formulas that work for any number n, the system performed significantly better when it was shown solutions for only a small range of n values. The paper states clearly that having access to a large volume of data did not necessarily improve the system's ability to generalize its findings into universal principles. \
By constraining the AI to work with less data, the system was forced to discover more fundamental and broadly applicable ideas rather than "memorizing" or overfitting to a large dataset.\
Having access to a large amount of data does not necessarily imply better generalization performance. Instead, when we were looking for interpretable programs that generalize across a wide range of parameters, we constrained AlphaEvolve to have access to less data… This “less is more” approach appears to encourage the emergence of more fundamental ideas.\
This finding is surprising because it runs contrary to the foundational assumption of much of modern machine learning. It suggests that for certain types of abstract discovery, overwhelming the system with examples can obscure the underlying patterns, whereas a carefully curated, smaller dataset can force it to find more elegant and universal truths.4.0 Takeaway 3: Sometimes, a "Dumber" AI is a Better CollaboratorAnother surprising discovery came from an ablation study; an experiment where researchers systematically remove components of an AI system to understand their contribution, comparing a high-performance, state-of-the-art LLM with a much smaller, cheaper model. While the more capable LLM generally produced higher-quality suggestions, the researchers found that the most effective strategy wasn't always to use the best model exclusively.\
In fact, the study revealed that an experiment using only high-end models could sometimes perform worse than a run that also incorporated suggestions from the cheaper, less capable models. The researchers' hypothesis is that the "dumber" model injects a degree of randomness and "naive creativity" into the evolutionary process. This added variance helps the search avoid getting stuck in a conceptual rut, where the more powerful model might continuously refine a single, suboptimal idea.\
This doesn't mean the "dumber" model is a universal replacement. For the most challenging problems, like the Nikodym sets problem, the researchers noted that the cheaper model couldn't produce the most sophisticated constructions. The true insight is the value of cognitive diversity: the powerful model is essential for pushing the boundaries on difficult problems, while the cheaper model serves as a valuable collaborator, injecting variance to prevent the search from getting stuck.A recurring theme throughout the paper is the power of collaboration over full automation. AlphaEvolve consistently performed better when its exploration was guided by a human expert who could provide insightful advice in the prompt. The AI wasn't a replacement for the mathematician; it was a force multiplier.\
This synergy was perfectly illustrated in the team's work on the Nikodym sets problem. AlphaEvolve's first attempt produced a promising construction using complicated, high-degree surfaces that were difficult to analyze. This complex but insightful starting point served as what the researchers called a "great jumping-off point for human intuition." The human mathematicians then stepped in, simplifying the AI's approach by hand to use lower-degree surfaces and more probabilistic ideas, ultimately discovering an even better, state-of-the-art solution.\
The researchers stress that the most significant results emerged from this partnership, where human expertise directed the AI's vast computational search capabilities. This frames the AI not as an autonomous discoverer, but as a powerful new kind of scientific partner.\
We stress that we think that, in general, it was the combination of human expertise and the computational capabilities of AlphaEvolve that led to the best results overall.6.0 Takeaway 5: AI Can Find Flaws in Our Thinking (And Our Instructions)One of the most fascinating roles AlphaEvolve played was that of an unforgiving auditor of human logic. The researchers observed a "cheating phenomenon," where the system would find clever loopholes in the problem setup or exploit numerical inaccuracies in the evaluation code rather than finding a genuine mathematical solution. It didn't solve the problem asked, but the problem coded. This held up a mirror to the researchers, perfectly reflecting the subtle gap between a mathematician's intent and the literal, logical instructions given to the machine.\
A more profound example of this emerged while working on the de Bruin–Sharma problem. The human authors had made a subtle assumption about the problem's constraints that wasn't strictly correct. AlphaEvolve, operating without assumptions, produced results that logically couldn't exist if the researchers' analysis were perfect, directly highlighting the flaw. Upon inspecting the polynomials the AI used, the researchers realized their oversight and corrected their analysis.\
This anecdote reveals a powerful application of AI as a tool for stress-testing our own assumptions. Because the system has no preconceived notions and will exploit any available path to a higher score, it can reveal hidden flaws, unstated assumptions, and logical gaps in our problem formulations in ways we might never anticipate.7.0 Conclusion: A New Partner in the Search for KnowledgeThe experiments with AlphaEvolve represent a significant shift in our understanding of AI's role in science. The system's true power lies not just in its ability to find answers, but in its capacity to evolve entirely new ways of finding answers. It is a tool for exploring the vast space of possible problem-solving strategies, a conceptual partner that complements human intuition rather than replacing it.\
By discovering principles like "less is more" and the value of "naive creativity," this research gives us a glimpse into a future where human-AI teams work together to tackle science's biggest challenges. This is not about outsourcing thinking to a machine, but about building a new kind of intellectual partnership. Mathematics is the language of the universe. If AI can evolve new ways to speak it, what new conversations can we have with the physical world in medicine, materials, and climate science?]]></content:encoded></item><item><title>You Should Use This for Your Next Project: Random Image API</title><link>https://hackernoon.com/you-should-use-this-for-your-next-project-random-image-api?source=rss</link><author>Daniel, Andrei-Daniel Petrica</author><category>tech</category><pubDate>Mon, 24 Nov 2025 18:00:01 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Say goodbye to tedious image hunts and embrace the simplicity of Random photo API, a free API that delivers a constant stream of captivating random images]]></content:encoded></item><item><title>AMD SDCIAE Working Its Way Into The Linux 6.19 Kernel</title><link>https://www.phoronix.com/news/AMD-SDCIAE-Linux-6.19</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 17:36:45 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A year and a half after the Linux kernel patches were first posted, SDCIAE that is found with AMD Zen 5 server processors is set to finally be supported by the mainline kernel come Linux 6.19...]]></content:encoded></item><item><title>X’s New Feature Reveals Why Trust &amp; Safety Work Was Never About The ‘Censorship Industrial Complex’</title><link>https://www.techdirt.com/2025/11/24/xs-new-feature-reveals-why-trust-safety-work-was-never-about-the-censorship-industrial-complex/</link><author>Mike Masnick</author><category>tech</category><pubDate>Mon, 24 Nov 2025 17:24:44 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[For the last few years, Matt Taibbi, Michael Shellenberger, and their allies have spent considerable energy attacking both academic researchers studying disinformation and the trust & safety teams at social media platforms working to identify and remove coordinated inauthentic behavior—particularly foreign influence operations. They’ve insisted that any attempt to study and limit such operations is actually just “censorship” with various forms of cover, whether academic or operational.Then Elon Musk rolled out a feature often used by trust & safety teams internally, looking at where accounts were created and/or where they normally post from. Except Musk made the info public. And within hours it revealed exactly why platforms had been doing this kind of work internally in the first place.And, no, it wasn’t about “censorship” of ideological viewpoints.On Friday, X began rolling out a feature that revealed where users signed up from and where they were posting from. The feature came following the request of MAGA influencer Laura Loomer, who asked Musk back in September to add country-of-origin labels to help identify foreign influence operations. We noted the irony at the time—Loomer and her friends spent years attacking the trust & safety teams who were actually working on this problem.Whether it was because of Loomer’s request or it was already in the pipeline, Musk rolled it out.The scale of the problem isn’t just about foreign actors exploiting the platform—it’s about Musk creating the economic incentives that make it profitable. As the popular Derek Guy account noted, Musk instituted an “engagement-based” payment structure that pays out money based on how many views, retweets, and comments you get. For people in lower income regions, trolling on politically sensitive topics in America to generate likes and clicks (especially now that they can use AI to do so) isn’t just easy—it’s an actual business model that Musk built into the platform.Twitter pays people based on engagement (views, retweets, comments, etc). It appears that many MAGA accounts are based abroad and they use AI technology to generate low-effort rage bait. My guess is that this will get worse as AI tech improves. For instance, fake videos of minorities doing crime.This isn’t a bug in the system. It’s a feature Musk designed, then acted surprised when it attracted exactly the kind of behavior that trust & safety teams used to work to identify and limit.While it’s fascinating that X actually rolled this out, it should raise questions about those who have spent the past few years stewing in that cesspool of foreign rage-bait manipulation, insisting it was somehow an accurate portrayal of public opinion.Now, there are some limitations to the system. X’s head of product noted that uses of VPNs or Starlink may have the wrong country show up on their account. But VPN usage doesn’t explain accounts with thousands of posts over months, all originating from the same foreign location, all focused on American political rage-bait, all monetizing engagement through Musk’s payment system.This is exactly the kind of coordinated inauthentic behavior that trust & safety teams were built to identify and address. Not to “censor” ideological viewpoints, but to surface when what looks like organic American political discourse is actually foreign actors gaming the system for profit.For years, Matt Taibbi, Michael Shellenberger, and their allies have insisted that anyone working on these problems was part of a “censorship industrial complex” designed to silence political speech. Politicians like Ted Cruz and Jim Jordan repeated these lies. They treated trust & safety work as a threat to democracy itself.Then Musk rolled out one basic feature, and within hours proved exactly why trust & safety work existed in the first place.Will Taibbi or Shellenberger acknowledge they spent years attacking the people who were actually trying to protect the integrity of online discourse? Will they admit they helped dismantle the very systems that could have prevented what Musk just exposed?I doubt it. They’re too busy on X, swimming in the very manipulation they insisted didn’t exist.]]></content:encoded></item><item><title>NATO Taps Google For Air-Gapped Sovereign Cloud</title><link>https://tech.slashdot.org/story/25/11/24/1531202/nato-taps-google-for-air-gapped-sovereign-cloud?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 17:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[NATO has hired Google to provide "air-gapped" sovereign cloud services and AI in "completely disconnected, highly secure environments." From a report: The Chocolate Factory will support the military alliance's Joint Analysis, Training, and Education Centre (JATEC) in a move designed to improve its digital infrastructure and strengthen its data governance. NATO was formed in 1949 after Belgium, Canada, Denmark, France, Iceland, Italy, Luxembourg, the Netherlands, Norway, Portugal, the United Kingdom, and the United States signed the North Atlantic Treaty. Since then, 20 more European countries have joined, most recently Finland and Sweden. US President Donald Trump has criticized fellow members' financial contribution to the alliance and at times cast doubt over how likely the US is to defend its NATO allies. 

In an announcement this week, Google Cloud said the "significant, multimillion-dollar contract" with the NATO Communication and Information Agency (NCIA) would offer highly secure, sovereign cloud capabilities. The agreement promises NATO "uncompromised data residency and operational controls, providing the highest degree of security and autonomy, regardless of scale or complexity," the statement said.]]></content:encoded></item><item><title>A Field Guide to Studying Software Teams Through Ethnography</title><link>https://hackernoon.com/a-field-guide-to-studying-software-teams-through-ethnography?source=rss</link><author>Ethnography Technology</author><category>tech</category><pubDate>Mon, 24 Nov 2025 17:15:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The context of your researchThe kind of research questions you want to answerWhat ethnographic studies require from the researcherFinding a site for field workParticipant or non-participant observationGaining access and starting upHandling your preconceptionsReflective and inductive analysisWriting Ethnography for Software Engineering Audiences - Reporting the Results4 Implementing your ethnographic studyPlanning can only prepare you so far for the research but here is where the ‘rubber hits the road’. As Fetterman [20] puts it: “The most important element of field work is being there - to observe, to ask seemingly stupid questions, and to write down what is seen and heard.”4.1 Gaining access and starting upAfter reading the previous sections, we expect the reader to have a clear understanding of what ethnography is, what kind of research questions it is best suited for, and the issues to consider when planning an ethnographic study. Furthermore, a “field site” to be studied has to be decided, i.e., a software development team, an open-source community, or organisation. However, as one can imagine, starting an ethnographic study is not only about “showing up” to observe what is going on at the field site. In general, it is important to have a key informant (also called a ‘gatekeeper’) that can facilitate the researcher’s access to the informants. This can be a project manager, team lead or a scrum master. This key informant is a person who is trusted, and possibly respected, by the informants and is willing to introduce the researcher to them so that they can start the data collection. In summary, this key informant negotiates access with the informants before the researcher goes to the field. Often, a good idea is to let this key informant introduce the researcher and suggest one or two initial members that are willing to be observed or interviewed. By interacting with these initial members, the researcher will have a chance to meet other team members and can negotiate with them additional opportunities for data collection.\
The key informant is also the person that the researcher will look for to ask clarification questions regarding the project being studied, its context, the diverse roles being played by the informants, the software development organisation, and the software being developed. This information often will also indicate documents and tools that the researcher should be aware of, and if necessary, the key informant will make the necessary arrangements so that the researcher can access these documents and/or tools. The key informant can be thought of as a facilitator who will allow the researcher to conduct the ethnographic study.\
Part of starting up the actual field work is also to agree on the legal side of the collaboration and develop and sign cooperation agreements containing nondisclosure agreements and agreements about the intellectual property that might be developed as part of the research project. As part of starting up the field work, the researcher also needs to consider the research ethics relevant for this project and decide, maybe together with the key informant, on how to handle the informed consent. The latter topics are discussed further in section.• Don’t impose your point of view. Let your student express their perspectives • Challenge your student’s observations but gently • Spend some time focusing on the use of language. How informants talk about their work or aspects of their organisation is very informative but attention to this kind of detail is not often emphasised in software engineering classes.4.2 Handling your preconceptionsOne of the challenges of ethnographic research for a software engineer studying software engineering practices is that the researcher brings an education to the field that states how software should be developed. For example, having attended a software engineering course, the field worker might expect a ‘stand-up’ meeting to take place in situ by people actually standing up. However, in virtual software development, a stand-up meeting might mean that people sit in front of the screen communicating using a virtual meeting environment [16]. Similarly, the textbook version of pair programming describes a “driver” and “navigator” role, but in practice pairing rarely adopts these roles formally, as the intensity of development results in a much more fluid exchange of activities [45]. So how can a researcher handle such a situation, where the observed team is not doing things ‘by the book’? Ethnographic research talks about ‘bracketing’ prior assumptions and knowledge. In the above example it would mean accepting that pairing in practice changes shape when the focus is on developing code rather than following a pre-defined procedure, and that stand-up meetings may look different for different teams depending on their context. The research can then focus on how development is implemented in this context, what is the rationale for deviating from the textbook way of doing things, and how have these practices developed.Observation is key to ethnographic studies and both participant and non-participant observation are legitimate forms of ethnography [21]. The data collected for observation is largely in the form of notes. Field notes are an important aspect of the work done by the ethnographer and the researcher must be prepared to take notes during the study. The question, though, is what notes to take? On the one hand, the researcher can document as many things as possible including the events, things people say, interruptions, event participants, documents being accessed or discussed, because at the start of the study it’s hard to know what will turn out to be significant. However, documenting everything will quickly become overwhelming. Textbooks sometimes recommend a template or schema for field notes. Such a schema can be very helpful and provide useful scaffolding at the start. Filling in the schema should not become the main purpose, though, and a tendency to just ‘tick the boxes’ needs to be resisted. The main purpose is, of course, to make sense of the team’s work practices and to document what is relevant to that aim. An alternative to schemas is to use a framework of questions. There are simple frameworks such as “who is there”, “where are they?” “what are they doing”, and more detailed ones such as the following based on Robson and McCarten [47] (p. 328):• Space: What is the physical space like, and how is it laid out?• Actors: What are the names and relevant details of the people involved?• Activities: What are the actors doing, and why?• Objects: What physical objects are present, such as story cards?• Acts: What are specific individual actions?• Events: Is what you observe part of a special event?• Time: What is the sequence of events?• Goals: What are the actors trying to accomplish?• Feelings: What is the mood of the group and of individuals?\
These are only intended to help keep a focus on relevant issues so that the ethnographer is not overwhelmed, and will evolve over time. Today, many researchers use computers for their field notes, which makes it easier to link to other field material such as (sample) documents, photos, and recordings. But handwritten notes are also popular and some find them a better way to engage with their surroundings. Each researcher will need to find their own preference. However, as mentioned in sections 3 and 4, the focus of the study will evolve and the field notes will reflect that. For instance, after noticing that the software architecture influenced the interactions between the software teams, de Souza and Redmiles [10] started to pay more attention to the inter-team interactions because they were sources of tension between the different product teams. As the focus changes, ethnographers also adopt different data collection techniques, for instance, during the field work the researcher can decide to follow an artefact, e.g. a pull request, bug report, a presentation or the documentation of a piece of the software architecture. When this happens, the researcher should be prepared to ask for access to these artefacts which may mean a physical (print out) or digital copy of it. By following an artefact, the researcher might shine light on a different set of new events, activities and/or informants who often will bring new perspectives into the field site being studied.\
Last but not least, it is important to note that field notes will contain both objective information such as quotes, timestamps of events, links to recordings and photos, and subjective information, such as the researcher’s reflections and interpretations of what is going on. Objective information is important, to anchor your emerging understanding of the work practices in actual observations. Verbatim quotations are extremely useful to present a credible report of the research. According to Fetterman [20], “quotations allow the reader to judge the quality of the [ethnographic] work - how close the ethnographer is to the thoughts of natives in the field - and to assess whether the ethnographer used such data appropriately to support the conclusions.”\
The subjective informationincludes the researchers’ tentative interpretation of the events being observed, “hypotheses” about what is being observed, and questions the researcher asks themselves when collecting data. These reflections can evolve into important memos that in turn will develop into the parallel analysis of the data. It is, though, essential that the researcher clearly differentiates the subjective from the objective information in their field notes. This can be done by using different colours in a paper based field diary, or by splitting the page vertically or horizontally. Reflection and emerging hypothetical interpretations are, in qualitative research validated in the field. For instance, during data collection De Souza and Redmiles [57] started to wonder whether the phenomena observed would occur only with novice team members. To “validate” whether this is the case, they decided to observe and interview senior team members. Through this they found out that the phenomena was more general and occurred with team members of different experience.\
In some cases, the events being observed will unfold so fast that the researcher will not be able to take notes. In this case, ethnographers allocate time to record their memories in their field notes as soon as possible after the event, and by the end of the day at the latest. Nowadays, recording meetings is common and straightforward since some participants are online. In this case, it is important for the researcher to negotiate access to these recordings because they will be important data points for data analysis. They should be linked to the field diary so that the researcher later can connect them to the context in which they took place. However, being able to record meetings so easily is a potential disadvantage because it’s easy to collect a huge amount of data, and there is a tendency for the researcher to lose concentration if they know data is being captured through other means.In addition to observation, conducting ethnographic studies will likely employ a range of data collection methods including interviews and document analysis. Interviews are a chance to clarify meanings and the history of what has been observed. Such interviews might be formal or informal. Informal, or impromptu, interviews are often in situ and opportunistic; they will often occur in the context of an observation. In other words, the ethnographer, after witnessing an event that seems interesting, surprising, or even expected, will use the opportunity to engage in a quick conversation with the informant to find out more details. These informal interviews will occur when opportunities arise, for instance, during a break or while waiting for a meeting. All this is done having in mind that the role of an ethnographer is to understand the member’s point of view. Formal interviews require an agenda and a negotiated date and time, but may be the only opportunity to speak with some informants or to explore some kinds of issue.\
In our experience, a good ethnographer strikes a balance between conducting formal and informal interviews. What this balance looks like depends on the context and the flexibility of the informants’ working day. Too many informal interviews with the same informants can become irritating. Similarly, conducting formal interviews means asking people to allocate time for the conversation. However, checking information and making sure that observations and reflections are well-founded are fundamental. A good ethnographer also needs to pace themselves, plan their days carefully and be systematic about keeping records. This is particularly significant because data collection will go on for several days or even weeks, and it is easy for memories to fade or become confused. The researcher needs to be constantly reflecting about what data is being collected, including around events, conversations, and artefacts, and revisit the emerging analysis concepts in light of the new observations. Ethnographers need to remember to avoid judging the work being observed and to keep focus on the informants’ point of view, i.e., why they do the things they do. Being careful about data collection also means collecting data from different informants to gather different perspectives while, at the same time, not being a nuisance.\
Finally, it is important to emphasise that ethnographic research, similar to other qualitative research methods, requires cycles of data collection and analysis. In other words, ethnographic research requires negotiating continuous and evolving access to the site. As discussed in the next section, during the data analysis new foci might emerge and require new directions for data collection, such as access to new informants or new events or new documents. So the ethnographer must be continuously negotiating access to these during the research process.\
\
\
\
• Being judgmental about observed practice SO focus on why it is the way it is.• “Going native” meaning that the way informants behave and achieve things become ‘normal’ SO talk to other researchers and hold debriefings to help you regain the questioning mindset.• Losing your “strangeness” perspective and not knowing what needs to be told for others to understand your interpretations SO try explaining what is happening to an alien.• Human tendency is to gravitate towards people you like but in the field it’s important to attend to all informants SO be aware of your own preferences and biases.• It is easy to focus on a small number of informants because you become comfortable with them SO make sure you choose a range of informants to get different perspectives, i.e., novices and experts, male and female, informants with different roles, etc.• Observation can result in lots of data, some of which is not relevant SO focus on what to pay attention to and choose carefully what data to collect.• Assumptions or pre-conceived ideas can lead to misunderstandings SO challenge your own assumptions and ask dumb questions to establish that your understanding is correct.• Recording meetings and interviews is straightforward and easy but can result in huge amounts of data and loss of concentration SO keep taking field notes even when the interactions are being recorded.Eventually, the researcher will have to leave the host organisation or the open source community, because the software development project finishes, the research project ends, or because the researcher has learned what could be learned from the ethnographic field work. Leaving the field should be done in a way that allows the researcher to come back to ask clarifying questions and open up for future collaboration on new mutually interesting research questions. It is therefore important that the researcher informs all of the informants that they are leaving the site and what will happen next with their data and its analysis. Saying farewell to the team can be combined with a presentation of (preliminary) findings and insights that can also be used as an occasion for member checking. Member checking is an important aspect of any qualitative research (see also Section 5). It involves presenting the results of the research to the informants so that they can “validate” them. In ethnographic research, one way to do this is to formally present the results to the key informants who facilitated the ethnographer’s access (see Section 4.1), or even to the whole development team. Another way is through informal presentations during the final days of data collection, such as during a break, while waiting for a meeting, etc. The format in which this is done varies depending on the team, project, and overall context. On the one hand, feedback from the participants is important for the trustworthiness of the findings. On the other hand, presenting the findings can also serve as a ‘thank you’ for the team and can support the reflection and improvement of their own practices. If an organisation or team is interested enough in having a researcher on site then they will also be interested to know what insights have been found. As one of our collaborators once said “the problem with running a project is that you’re running, but having you here allowed us to stop and reflect”.\
\
\
\
\
\
\
\
\
\
\
\
\:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Should Your Next Software Engineering Study Be Ethnographic?</title><link>https://hackernoon.com/should-your-next-software-engineering-study-be-ethnographic?source=rss</link><author>Ethnography Technology</author><category>tech</category><pubDate>Mon, 24 Nov 2025 17:15:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The context of your researchThe kind of research questions you want to answerWhat ethnographic studies require from the researcherFinding a site for field workParticipant or non-participant observationGaining access and starting upHandling your preconceptionsReflective and inductive analysisWriting Ethnography for Software Engineering Audiences - Reporting the Results2 Is an ethnographic study the right choice?Here we consider the question of whether or not an ethnographic study is the right choice from three different perspectives:the context of your researchthe kind of research question you want to answerwhat it means for you as the researcher, because in an ethnographic study the researcher is the research instrument This section will discuss these three perspectives to help you to decide whether to use ethnography as a research method.The context of your researchThis perspective considers where the intended study fits in the wider context: what role might an ethnographic study play in the research? In social sciences, where ethnography originated, studies focus on understanding phenomena. They are used to understand better how (sub) cultures operate, or how groups of people organise their social life. In HCI, ethnographic studies are used to inform the design of technology from a user’s point of view so that the designers might better appreciate who they are designing for. In software engineering, ethnographic studies may also be used to understand or to inform design, but research often goes beyond understanding towards improving methods, techniques and tools. Understanding current practice is a good starting point to inform changes of any kind, not so that current practice can be replicated in the new order, but to appreciate why things are done the way they are, so that there are no unintended consequences when things change. A quite famous example is the long time it has taken to replace the paper flight strips used by air traffic controllers with digital versions. In 1999 Wendy MacKay commented that “Our observations have convinced us that we do not know enough to simply get rid of paper strips, nor can we easily replace the physical interaction between controllers and paper strips” [38]. She wrote this after being inspired by her work and previous studies about flight strips which started in 1992. A blog written in 2017 on the subject demonstrates how cautiously this change was eventually made [18]. In our analysis of ethnographic research in empirical software engineering [51], we identified four roles for ethnographic research, all of which start by considering “how things work in practice”\
2.1.1 To strengthen investigations into the social and human aspects of software engineeringSince ethnography’s origins are in understanding cultures and communities, it is not a surprise that ethnography has been used in this way. It is inevitable that some insights regarding social and human aspects will emerge whatever research question is pursued since the approach champions the members’ point of view. For example, Lopez et al. [35] were interested in investigating how non-specialist software developers engage with security in practice. This explicitly focuses on the human aspect of software developers. They used a multi-sited study and a mixture of research methods. Their results included a set of episodes where non-specialists engaged in security activity, and five typical behaviours that characterise how individual developers respond to security concerns to meet the demands of particular circumstances. A key characteristic of this work was that it emphasised security from the developers’ perspective. The results form a framework that managers and teams can use to recognize, understand, and alter security activity in their environments (motivatingjenny.org).\
2.1.2 To inform the design of software engineering toolsFor a new tool to be successful it must support the task being performed, e.g. bug localisation. But for it to be useful to practitioners it must also fit into their workflow and support how they and their organisation performs the task. Focusing on a task in isolation gives only a partial view and however clever it may be, if it’s not usable then it won’t be used. So understanding the context of tool use is as important as understanding its technical and functional requirements. But there are other considerations too. As with much software development, it can be easy to jump straight into designing a system that seems to address a user’s problem [48]. However if the goal is to develop new software engineering tools then don’t start the design until you are sure that the proposal will be useful. This can be achieved by sketching a number of potential tools based on observations, and then collecting evidence to support or refute their value, e.g. by working with practitioners and analysing your data overall to evaluate the ideas. For example deSouza et al. [9]were interested in the role of application programming interfaces (APIs) in the coordination of software development work in large scale organisations. An initial observation led them to think that integrating the versioning software with email might facilitate the team’s work, but further data collection made it clear that this was only part of the situation, and focusing on a tool to support the team’s overall communication would bring them more benefit. So instead of developing something to integrate two existing tools, they developed a tool that supported the identification of dependencies between developers which allowed developers to coordinate their efforts more effectively and reduce unnecessary work.\
2.1.3 To improve development processesEthnography can be a starting point, together with the practitioners observed, to discuss and implement improvements of their work practices. Ethnographic research then becomes part of action research. Two chapters in this book by Staron and by Dittrich, Bolmsten and Seilein introduce action research. A critical point is that the decision about the intervention and the evaluation of its implementation should keep the members’ point of view, both for ethical and methodological reasons. Dittrich et al. developed Cooperative Method Development (CMD), an action research approach combining ethnographical and ethnomethodological inspired empirical research with the improvement of software engineering tools, methods, and processes [17, 13]. CMD is explicitly designed to keep both the work practice focus and the developers’ point of view of the initial research throughout the action research cycle(s). Unphon’s thesis provides an example of action research [61]. The research cooperation with a company developing software products for simulating hydraulic systems focused on the evolvability of these software products. The researchers collaborated with the team responsible for the product simulating open one-dimensional water systems like rivers and creeks. The action research introduced light-weight software architecture techniques for high-level design, developed a light-weight Architecture Level Evolvability Assessment method for focussed discussions of design decisions with relevant stakeholders, and introduced light-weight architecture compliance techniques using the built system. Introductory research with the team in order to understand development practices as well as investigate the structure of the existing software product resulted in a framework for understanding the influence of the organisation of software development and business on the architecture of the software [62]. The research results emphasise the need to adapt software architecture methods and tools to support the continuous evolution of software products: architecture design and evolution take place as part of everyday evolution; architectural practices need to support the software architect to keep up with the changes of the software and the emerging requirements that might challenge the architecture; evolvability should be a quality to be considered in regular software architecture design discussions.\
2.1.4 To inform research programmes that do not have ethnography at their coreIt is common for research outputs such as tools and new processes to be ignored by practitioners, in favour of innovations suggested by other practitioners. Understanding the context of software engineering takes the researcher one step nearer to suggesting an innovation that is acceptable to practice. In particular, an ethnographic study may help a research programme by articulating more specific, relevant research questions. For example, our programme on agile software development started by looking at the realities of XP in practice, i.e. how was the approach implemented [53]? Ethnographic studies in this context led to other studies and research questions that focused on the role of physical artefacts [55], information flow between stakeholders [65] and collaboration in dispersed teams [11]. But ethnography can also be used to inform research programmes by providing context grounded in practice for an existing research focus. For example, Capiluppi et al. [6] investigated the evolution of code within an agile development project. This led to a number of observations regarding how the agile code base evolved. Using an ethnographic study as context allowed the researchers to:• Characterise the development process and practices clearly, and• Provide alternative interpretations of phenomena found in the data, e.g. where the size of the code base changed significantly they could trace an event through the ethnographic study to explain that a new library was loadedEthnographic observation in this context has the benefit that it is not dependent on the report of practitioners, as it is in interviews. When collecting reports from practitioners, their responses will inevitably be partial and informed by current events. For example, they might not consider certain aspects of everyday practice to be relevant to the questions asked, or they might not report practices that they consider to be informal or not accepted. However, these aspects may be exactly the characteristics that render the specific way of developing software possible, and hence would be important from a research point of view. The design of studies complementing and providing context to other research may not be driven by an independent research question, but the research question might be decided based on the overall research interest.\
\
\
\
\
\
\
\
\
\
• When conducting an ethnographic study with the aim of investigating social and human aspects of software engineering it is easy to become overwhelmed by the huge number of focus points SO track the possible foci and address them one at a time.• When conducting an ethnographic study with the aim of producing a new tool, it can be tempting to start designing before a full picture has been obtained SO start by hypothesising what kind of tools would be useful and collect data to validate those hypotheses.• When conducting an ethnographic study with the aim of producing a new tool, it can be tempting to focus on specific instances rather than the broader picture SO focus on the analytical results rather than the data when hypothesising about potential tools.• When conducting an ethnographic study with the aim of improving development processes it can be easy to suggest changes based on textbook versions of processes SO make sure to engage practitioners in any suggested modifications.• When conducting an ethnographic study with the aim of informing other programmes of research, confirmation bias can easily cloud your observations SO be sure to seek confirming and disconfirming evidence.\
2.2 The kind of research questions you want to answerThis subsection considers the kind of research questions you want to answer. The specific detail of the question may change as the study progresses (see section 4 below) but whether an ethnographic study is the right choice depends on the kind of question you want to answer. The first thing to note is that ethnographic research, like other qualitative research, is driven by research questions rather than hypotheses derived from theory. Research questions ask ‘How’ and ‘Why’ and ‘What are the characteristics of’ questions rather than ‘Is X better than Y’ or ‘Will this technique make programmers more productive?’ kind of questions. For example ‘How do software practitioners develop systems using XP?’ rather than ‘Is single programmer coding more productive than pair programming?’, and ‘Why don’t developers adhere to the company’s security policies?’ rather than ‘Does structuring the manual in this way help developers produce more secure code?’ and ‘What are the characteristics of a technology adoption?’ rather than ‘How did the ideas of Simula develop into Java? Other techniques and methods introduced in this book will support answering other types of question.\
The strength of an ethnographic approach is that it emphasises the point of view of the participants, i.e. the members of the community under study, who are often called informants or interlocutors in ethnographic work. It therefore allows the researcher to understand better why things are the way they are. It brings research closer to practice and hence can make the results more acceptable within practitioner communities. For example, it may seem strange to a researcher to see agile practitioners using both a physical storyboard and a virtual one, especially because this entails duplicate work in keeping them both up to date. However, this is common practice where co-located or hybrid teams are deployed because of the different and complementary collaboration and communication benefits it provides.\
\
\
\
\
\
\
\
2.3 What ethnographic studies require from the researcherTo help decide whether or not an ethnographic study is the right choice, this section considers the four main features of ethnographic work: the member’s point of view; the ordinary detail of life as it happens, the analytic stance and thick descriptions. For each of these we present considerations on whether the researcher (you) are prepared and willing to apply the approach appropriately, e.g. to write thick descriptions, listen to the informants’ point of view and modify your research focus. The information in this section will help to assess this, and will set your expectations for what it means to implement an ethnographic study.\
2.3.1 The members’ point of viewWe would argue that it is always important to understand the point of view of practitioners whatever is the focus of the research, for example whether developing a new tool, looking to improve the development process, or simply trying to understand better how something works in practice. As discussed in Section 2.1, taking account of the members’ point of view doesn’t restrict innovation, but instead understanding the rationale for the current way of working before introducing changes helps to avoid unintended consequences. Understanding the member’s point of view is all about understanding what is important for the informants so that this can be taken into account. In other disciplines, ethnographic studies involve the ethnographer studying a culture that is unfamiliar to them, e.g. in terms of language, customs, cultural norms etc. These studies may need to last for months or years. As a software engineer conducting an ethnographic study in a software development environment, this will be less of a challenge to overcome. Understanding someone else’s perspective is still hard, but much of the basic context of software development, e.g. IDEs, development processes, programming languages etc will be familiar. One of the benefits of this is that ethnographic studies don’t necessarily need to take months or years. Focusing on the members’ point of view will entail getting used to the organisation’s environment and the specific details of the team’s work but the learning curve is less steep than it would be for someone unfamiliar with software development.\
The familiarity of the researcher with the tools, techniques and methods of the informants can make it difficult to keep the member’s point of view in mind when observing how software development takes place. It is very easy to apply the knowledge from your own education and research and slide into a “this can be done better” attitude. Such an attitude might even be provoked by the practitioners asking the researcher, who supposedly is an expert, whether they have any recommendations. Here it is important to separate observations from analysis for the duration of the field work and put your own judgement aside.\
2.3.2 The ordinary detail of everyday lifeThe ordinary detail of everyday life is important because it exposes how tasks are addressed, the issues that are relevant or not and how informants approach tasks. For example, a series of studies with agile software development teams exposed that the colour of cards placed on a physical scrum board, and the way in which they are handled carries meaning beyond what was written on them [54]. This raised questions of whether digital storyboards would support the same kind of processing and collaboration as physical boards. This also means that the research will be conducted in the field or “in the wild” rather than in a controlled environment, and the research will aim to not disturb or control normal behaviour.\
Equally, informants’ language is relevant. For example if the informants’ native language is different to the ethnographer, what might the impact be? If the domain of study is highly technical then the ethnographer would do well to understand the domain too. Performing an ethnographic study in an unfamiliar technical domain will have an impact on the timescale (as mentioned above) but also may lead to misinterpretation of activities and tasks. The gender of the researcher might both influence the field situation and the interpretation of the observation. Given that ethnography emphasises what is important from the members’ point of view, and that the researchers won’t know what they might find before entering the field, there is a strong chance that the research focus will evolve. A key issue to consider is whether the research context and design is suitably flexible to allow for the research focus to change? For example, if your focus is developing a new tool to identify areas of technical debt but your informants explain that their concerns are driven by improving the security of the code, how would that affect the research? Although the detail of everyday life is observed, it is not the detail itself that is important, but the significance of the detail (see section 2.3.3 below).\
2.3.3 The analytic stanceAn ethnographic account is not just a description of what is seen but is crafted out of an analysis and interpretation of what was found in the field, explicating why things are the way they are. The analytic stance is related to the members’ perspective. The idea is to understand and uncover the rationalities of practices and how the practitioners’ behaviour makes sense, even though it does not match the researcher’s own understanding of how software development should take place. This can be problematic to those unused to the approach. One example to illustrate this analytical stance can be found in [49], where a recording of a steering group meeting addressing a major change in a method and tool development project is analysed. The analysis shows in detail how the steering group relates the actions to take the development further to both the new plan for the project and the company-wide project model. The analysis shows that the steering group takes a decision to deviate from the project model in order to assure that the work in the project can progress using, nonetheless, the very same company-wide project model to make the deviation visible and accountable in the organisation. The analysis thus sheds light on the rationale behind deviations from plans and company-wide project models, and, at the same time, it shows the importance of a company-wide project model to communicate both behaviour according to it and deviations from it to other actors in the organisation. Not every analysis needs to be as fine grained as the interaction analysis performed in that article. However, the analytical stance requires the researcher to be able to relate the observations here and now to the spatial and temporal context and to the goal and purpose of the observed activities.\
An ethnographic study results in a comprehensive and detailed set of data, and a detailed account of the findings. The account aims to communicate a broad picture of the study environment and activity. It needs to show how the researcher has arrived at their conclusions or recommendations. The set of data is specific to that site and hence is not suitable for statistical generalisation although it may be suitable for analytical generalisation. If you decide to undertake an ethnographic study this is an important point to remember. In an area where generalisation and statistical significance of results are expected, having a core ethnographic study may be challenging to convince others. The thick description can be used to defend the study and its results, but it cannot justify a statistical generalisation. An ethnographic researcher needs to acquire a different perspective on justification. This may be particularly pertinent for publication and if you are a PhD candidate:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>A Practical Framework for Planning Ethnographic Research in Software Projects</title><link>https://hackernoon.com/a-practical-framework-for-planning-ethnographic-research-in-software-projects?source=rss</link><author>Ethnography Technology</author><category>tech</category><pubDate>Mon, 24 Nov 2025 17:15:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The context of your researchThe kind of research questions you want to answerWhat ethnographic studies require from the researcherFinding a site for field workParticipant or non-participant observationGaining access and starting upHandling your preconceptionsReflective and inductive analysisWriting Ethnography for Software Engineering Audiences - Reporting the Results3 Planning an ethnographic study\
3 Planning an ethnographic studyEthnography belongs to the category of flexible research designs [47], where both the research focus and the concrete field work are expected to be adapted during the research process. However, this does not imply that planning is redundant. An ethnographic study still needs to be planned. It is important that both the researcher and the practitioners have a shared understanding about what might happen once the researcher joins the project. In addition, the plans should be regularly revisited and adapted, and everybody kept aligned throughout the research.\
3.1 Finding a site for field workOne of the first things is to identify a suitable project to collaborate with. Often, the ethnographic study would have been prepared by prior contact between the researcher, the company and relevant members of the development team. In other cases, the ethnographic researcher might identify a company or an open source project that fits with the research interest and question.When working with a company, the ethnographic study might affect their work, so the company and the project team need to agree to the study and the plan. As ethnography is an extremely flexible way of doing research, good communication with the project and in many cases the management of the company is needed to allow for coordination and replanning. One way of doing this is to create a ‘steering committee’ for the research collaboration, consisting of the researcher, the PI of the project, a project contact, and the person responsible for the research collaboration in the company. This committee would meet regularly to discuss whether changes to the logistics of the field work are needed, and to make sure that resources necessary for the ethnographic study (access to documents, code and the like, time for interviews etc) are provided. Exactly how this group works will depend on the field site circumstances but regular alignment across all parties will keep the study on track. In other cases, the researcher could decide to become part of an open source community. This in turn will require deciding on the role in the community the researcher would want to take. Are you able to and interested in participating in the Open Source development, i.e. coding? Could you contribute in other ways to the community? Open source members, especially if they take on core roles like maintainers, are often as busy as members of corporate software engineering teams. Either your contribution to the community during the field work or the research results could be received as a return for the time the community members spend with you. Planning the ethnographic study needs to take research ethics into account as well. Due to the complexity of this topic, research ethics are discussed on their own in Section 6. The following discusses five dimensions to consider when planning an ethnographic study, especially in software engineering.\
3.2 Participant or non-participant observationThe best way to understand the culture and practices of a community under study, is to become part of that community. Studying software engineering practices as a software engineer often allows for participation. For example, the researchers can become members of open source communities by participating in the development. However, commercial companies may not be so happy for researchers to be modifying their code, and highly specialised software – for example software for modelling hydraulic systems [61] – requires an expertise that software engineering researchers would normally not master. In such cases, the researcher needs to find a role that allows them to observe and become a culturally competent member of the community. One way could be to act as a newly employed project member, an apprentice in the specific development context. In the above case of hydraulic simulation software, that required expertise in differential equations on a PhD level, the field worker took on documenting the architecture of the re-engineered software to support its usage and customisation. Another example was in an early ethnographic study wherethe researcher took on an administrative role [37]. Both of these examples resulted in the researcher getting embedded in the work of the team in a way that was acceptable to all concerned. Deciding the role of the researcher in the field needs to be taken together with the collaborating project team. Also other stakeholders like management and in some cases even clients of the organisation need to be consulted. It will not always be possible to participate in the development and a more passive observation may be the only option. In this case the researcher will sit quietly and observe, listen and take notes but then it is important to keep alert for particularly pertinent activity.\
3.3 Duration of field workHow much time does the researcher need to spend with the software team? In our presentation of published ethnographic studies [51], there was a wide variety of field work durations. The traditional answer of ethnographers would be: until the researcher can act as a competent member of the culture, or until the researcher stops learning new things. As in other qualitative research methods, saturation of the observations is an indication for ending an ethnographic study. Here, the overlap of expertise when researching software engineering practices as a software engineering researcher might shorten the time necessary: although the specific software practices might be new, the field worker shares a common ground with the observed teams in the form of shared professional expertise. However, saturation might not be possible, e.g. if a software development project ends before saturation is reached. On the other hand, iterative processes such as agile software development might allow repeated observation of all relevant practices in a comparatively short time. In many situations, it is not necessary to spend all the time with the observed projects. For example, the field worker might choose to spend 3 or 4 days each week with the project, which enables the researcher to start analysing the field material in parallel with the field work. That way the field work duration can be adjusted to fit with time periods that are meaningful from the point of view of the researched practices.Ethnographic field work should take place where the culture and practices that are studied take place. But where does software development take place? Software development is both anchored in the physical world, where software developers are located, and it takes place in the digital realm, where the software is located. Observations often need to combine physical observation and an understanding of how the physical actions relate to the development environment, digital documents,changes to and management of the source code, as well as the effects on the deployed software.Especially when cooperation and collaboration in distributed development is the focus of an ethnographic study, participatory observation on only one side risks neglecting the remote conditions for collaboration. To address this potential bias, multi-site ethnography emphasises the need to do ethnographic field work on all the locales involved. Multi-sited ethnography entails the researcher entering and being accepted at different sites which might require longer stays, e.g. one or more weeks, at all places where relevant project members or stakeholders are situated. As an additional challenge, especially after the lockdowns due to the pandemic in 2020 and 2021, many companies moved from co-located or distributed development to hybrid work organisations, where developers partly work from home, leading not only to distributed but dispersed development, where every member of the team is in a different location [52]. Coordination and cooperation in these circumstances more and more resemble Open Source development. Ethnographic research here has to follow the way software development is organised. One of the core challenges with researching at least partially virtual activities, is that it might be hard to follow an activity in a complex distributed development environment that both has physically visible parts, e.g. workstation layout and virtual elements, e.g. changes to code and deployed software. For example Begum [3] describes the handling of a bug report: the bug report comes into existence through registration of the bug by a support engineer, followed by reproduction of the erroneous behaviour and a root cause analysis by a software engineer who prepares the discussion of the defect in a triage meeting, which in turn results in a strategy to fix the defect. The correction of the defect might again require collaboration between different teams responsible for different parts of the product. To understand what is discussed at the different meetings and to understand the rationale behind decisions might require at least a superficial understanding of the organisation of the source code.Ethnographic field work here can follow an artefact, e.g. a bug report or the software architecture documentation, or it might focus on the activities of individuals, e.g. the software architect of (a part of) the software, or a group of developers. Different forms of ”virtual ethnography” have been developed as a way to observe internet based cultures (e.g. [29, 33, 43] ). Regardless of how well the field work is planned and prepared, even in smaller projects, the ethnographer will not be able to observe all relevant activities that are going on. Observations might need to be complemented with other data collection methods, like informal, in situ interviews, semi structured more formal interviews, or group interviews, e.g. in the form of a project retrospective.\
3.5 Theoretical underpinningEthnography often is seen as a bottom-up research method where the development of themes and concepts is based on reflective and inductive analysis. Theories are then developed by comparing results of several ethnographic studies. In order to relate ethnographic studies with each other, the themes and concepts developed in earlier studies have to be connected to the field work and analysis of the later studies, and this can be done via theory. One example is the social theory of learning by Lave and Wenger [34]: comparing findings from widely varying studies led to the formulation of a set of related concepts, a theory. This theory can then be used either as a focus for the field work in new ethnographic studies exploring social learning in different contexts, or to discuss the analysis of field material when peer learning emerges in the reflective and inductive analysis. Theories can be seen as tools to think with rather than a claim for quantifiable correlations that are or can be supported by experiments. For example, the social theory of learning talks about ‘communities of practice’ [34] as an important facilitator to develop and maintain expertise in professional environments. ‘Communities of practice’ as a phenomenon can be identified both through the way members refer to each other and how knowledge is shared with new colleagues and among established members. Using the concept, for example, to understand the onboarding of novices in a software engineering team, can help to structure and focus the observations in the field.A danger of using theory from the outset, though, is that the focus that a theory provides can become a bias in the empirical work. It can prevent the researcher from paying attention to aspects of the observed practices that do not fit the theory. To prevent theory-based biases, the same techniques used for other biases can be applied (see this chapter, section 4.2). Ethnography researches software development teams as sub-cultures or as social practices. Social theories that focus on social practices and how they reflexively constitute social structure therefore might fit well with the observations and findings and help to develop relevant insights. There are several such practice theories. For example, activity theory [19] focuses on how human goal-directed activity is mediated by tools and also by rules, distribution of labour, and community. Activity theory has for example been used to better understand tool support for distributed development [60]. Dittrich et al [16] and Dittrich [14], use Schatzki’s [50] concept of social practices and Knorr Cetina’s [7] concept of epistemic practices as inspiration to understand the evolution and design of software development practices. Here, the way members of a software team adopt and adapt methods is at the core of the research. Distributed cognition [31] emphasises the distributed nature of cognitive systems and focuses on collaborative work and the use of artefacts and representations. It results in an event-driven description which emphasises information and its propagation through the cognitive system under study. This has been used, for example, to understand how the structure of story cards and agile boards support collaboration [54] and how UX information is handled by agile development teams.The decision to consider theoretical frameworks can be taken based on the research intent, for example if the intent is to design and improve tools, the use of a theoretical framework that provides a vocabulary to identify and discuss the role of tools can be an advantage. As mentioned above, theory can also be used to interpret the data afterwards, e.g. Sharp et al. [56] used cognitive dimensions to analyse representations that were collected using distributed cognition as a framework. Giuffrida and Dittrich’s article [25] provides an example where ethnographic research is used to develop theory. They further develop concepts from coordination and communication theory to explain widely different success rates of student projects in distributed student projects. The initial analysis showed that successful projects used the Instant Messaging channel Skype provided to not only coordinate but also agree on coordination. In order to explain both what was going on and the differences, Genre theory and concepts based on articulation and coordination theory were combined and further developed.\
\
\
\
\
\
\
\
\
\
\
\
\
\:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Teaching Ethnography to Software Engineers</title><link>https://hackernoon.com/teaching-ethnography-to-software-engineers?source=rss</link><author>Ethnography Technology</author><category>tech</category><pubDate>Mon, 24 Nov 2025 17:14:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The context of your researchThe kind of research questions you want to answerWhat ethnographic studies require from the researcherFinding a site for field workParticipant or non-participant observationGaining access and starting upHandling your preconceptionsReflective and inductive analysisWriting Ethnography for Software Engineering Audiences - Reporting the ResultsEthnography has become one of the established methods for empirical research on software engineering. Although there is a wide variety of introductory books available, there has been no material targeting software engineering students particularly, until now. In this chapter we provide an introduction to teaching and learning ethnography for faculty teaching ethnography to software engineering graduate students and for the students themselves of such courses. The contents of the chapter focuses on what we think is the core basic knowledge for newbies to ethnography as a research method. We complement the text with proposals for exercises, tips for teaching, and pitfalls that we and our students have experienced. The chapter is designed to support part of a course on empirical software engineering and provides pointers and literature for further reading.Ethnographic studies are an important element of the toolbox for empirical software engineering as they can provide unique insights into software development practice [51]. Ethnography was originally developed as a method to understand foreign cultures by becoming embedded in them and documenting the learning and sensemaking that was experienced during the process [26]. Modern ethnography has been developed in an attempt to understand a foreign culture from a members’ perspective. With a growing interest in the understanding of subcultures close to the ethnographer, ethnographic research was used to learn more about a wide range of contexts including truck drivers [1], midwives [32], London underground workers [27] and organisational life [58, 64]. In the context of Human-Computer Interaction and Computer Supported Cooperative Work, ethnographic research was used to understand how the members of an organisation made use of technology. Examples for this kind of research include several studies of air traffic controllers in the context of a redesign of the system supporting their perception and control of air traffic [4], of copy machine repair personnel by Julian Orr [40], and of attorneys and paralegals to prepare the design of software supporting their document management [5]. For more than 25 years, ethnography has also been used in the context of software engineering research in order to study cooperative aspects of software engineering. Examples are a study on configuration management [28], a study on the use of social software in distributed software development [23, 24], and an ethnographic study of a small software house to investigate testing practices [39]. As in the original context, the benefit of ethnographic studies of software engineering is the understanding of how and why software teams do what they do to develop software. Ethnography allows understanding and describing collaboration in software engineering from the software engineers’ perspective. Ethnographic studies require the researcher to join a development team with the intent to understand the why and how of the team’s everyday practices.\
\
\
\
\
• It’s important for students to know the origins of ethnography but introduce the approach through a mixture of examples from software engineering and other disciplines so that the relevance to software engineering comes through• If you don’t have (much) experience of ethnography yourself then identify colleagues with experience to engage in your classes• It’s hard to see another person’s point of view, offer your students guidance on emotional intelligence, and how to see different perspectives• Present examples of existing ethnographies in software engineering for the students to facilitate their writing• Take an iterative approach to topics, e.g. provide some background, set an exercise, discuss the exercise and ask students to do the exercise again• Embrace the use of ethnography with other techniques, e.g. interviews, as these are often required in the field• Because activity in the field is fluid, ethical dimensions of the study need careful attention\
When we use the term ‘practices’ in this chapter, we talk about the established way a software team goes about developing and evolving software. Social practices, like cooperative software engineering, are based on an understanding of relevant objects, representations and tasks, on implicit and explicit rules, and on a common goal (for further readings see Section 7). Understanding software practices using an ethnographic approach aims to understand how and why the software development practices make sense from a members’ point of view, putting aside ideas on how to improve the observed practices for the time being. This can be hard for a software engineering researcher. As engineers and software engineering researchers we are trained to design and create things and to improve the methods used to this end. We tend to focus on problems and shortcomings, rather than aiming to understand how a complex activity such as software development actually takes place, or what enables a team to produce software at all. Ethnographic research invites us to temporarily put aside or ‘bracket’ [22] our assumptions of how software development should take place, and try to understand how developers make things work no matter what.\
In return, the results of ethnographic research help us understand the rationale behind even seemingly disadvantageous behaviour, which in turn provides us with important information when designing tools and methods. An example can be found in [61, 62]. The article presents an interview study triangulating an ethnographic study of software architecture practices for a software product. The researchers were surprised at the lack of any software architecture documentation and the unwillingness of the team to produce and maintain such documentation. A triangulating interview study revealed that this reluctance towards software architecture was shared by architects and tech leads of other products. The only exception, an open source project, also provided us with a rationale for this attitude: the existence of architecture documentation can prohibit discussion between the architects and the developers with the effect that the architects might not know how the developers change the architecture or where the architecture creates problems for the development and might need to be revised. This, in turn, means that tools for designing and communicating software architecture should be designed to support architect-developer discussions and not replace them. Had we not taken the reservations of the developers and architects seriously, we would not have understood that there was a ‘good reason for bad architecture documentation.\
Ethnographic studies can also lay the groundwork for practitioner support tools that are accepted by the community because they are based on an understanding of practitioners’ point of view. An example of this is the motivating jenny project which ran a multi-site ethnographic study to understand developers’ security behaviour. This project resulted in a number of insights including a model of security behaviour [35] that underpins a set of four practitioner packs designed to improve the security culture in a team or organisation (downloadable from motivatingjenny.org). These have been downloaded thousands of times and have been used in education and in industrial settingsOur previous, more theoretical and programmatic article in Transactions on Software Engineering [51] provides examples from empirical software engineer- ing where ethnography has been used. This chapter complements that article with support on how to implement and teach ethnographic research in a Software Engineering context. The goal of this chapter is twofold. On the one hand, we introduce ethnography as a research method for software engineering with a hands-on approach. We aim to provide concrete how-to support for researchers starting to use ethnography. Our intended reader in connection to this goal is a researcher, e.g. a PhD student, who wants to explore ethnography as a possible research method. On the other hand, we provide guidance for teachers of a postgraduate course on empirical research in software engineering. The chapter comprises what we see as the minimum contents of a module on ethnography as an empirical method for software engineering.\
In our tutorials, people have often asked us what is the difference between ethnography and other qualitative methods, such as contextual inquiry and grounded theory. And why not just do interviews? A simple response to these questions might include that contextual inquiry is based on an apprenticeship model, grounded theory seeks concepts in the domain that can be developed into a theory, and in interviews participants’ accounts will necessarily be partial and rationalised. Ethnographic research, on the other hand, focuses on a holistic view of the practice being studied, has research questions that evolve over the course of the study, and aims to capture activity as it happens together with practitioners’ rationale. But there is more to it than this. This chapter will provide a deeper answer to these questions. The remainder of the chapter is structured as follows: Section 2 discusses whether ethnography is the right choice from three perspectives: the role of ethnographic studies in relation to your specific research context; the type of research question it can address; and what is required of the researcher. Section 3 discusses the planning of an ethnographic study. Section 4 covers important topics such as field work and addresses issues ranging from the role the researchers take on in relation to the team, to how to design and keep a field diary. Section 5 introduces the analysis of ethnographic data and section 6 covers research ethics for ethnography. Section 7 concludes the chapter and provides suggestions for further reading.\
\
\
\
\
\
\
\
\
\
\
\
\:::info
This paper is  under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Wallet in Telegram Lists Monad, Enabling Telegram TGE Trading &amp; Expanding MON Distribution</title><link>https://hackernoon.com/wallet-in-telegram-lists-monad-enabling-telegram-tge-trading-and-expanding-mon-distribution?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:56:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Panama City, Panama, November 24th, 2025/Chainwire/--This partnership expands global access to one of 2025’s most anticipated Layer-1 networks through Telegram., a digital asset solution natively embedded into Telegram’s interface, today announced a listing partnership and full token listing with , the high-performance EVM Layer-1 inaugurating Coinbase’s new ICO platform. The partnership and listing will be available through the custodial Crypto Wallet, enabling users to discuss trading opportunities directly in Telegram and execute TGE trades natively within the app – keeping conversation and execution in one unified place.From launch day, Wallet in Telegram users will be able to deposit, withdraw and trade MON directly within the app. The listing will be accompanied by a dedicated native MON staking functionality as well as incentive programs for traders designed to introduce Monad to Telegram’s audience across established and emerging crypto markets. Regarded as one of 2025’s most consequential blockchain debuts, Monad sets a new performance standard for EVM-compatible networks. Its parallel and asynchronous execution model enables Ethereum-style smart contracts to operate at significantly higher throughput, targeting 10,000 transactions per second, sub-second latency, and low fees while preserving full bytecode-level EVM compatibility and decentralization. Monad’s development has attracted extensive institutional support, with more than $225 million raised from investors including Paradigm, Dragonfly and Electric Capital. As the first project to launch through Coinbase’s new ICO platform, Monad has conducted a $188 million public sale following a testnet that processed over 5 billion transactions. The network will also distribute an airdrop to more than 230,000 eligible users. “Monad represents a defining moment for next-generation blockchain infrastructure – combining breakthrough performance with full EVM compatibility,” said Halil Mirakhmed, Chief Strategy Officer at Wallet in Telegram. “Our mission is to ensure that users everywhere can participate in the most important moments in crypto from day one. By partnering with Monad at launch, we’re opening this milestone to millions of people around the world and supporting the growth of what we believe will be a foundational network for years to come.”Wallet in Telegram is expanding its role as a universal gateway to digital assets for everyone. Alongside leading cryptocurrencies, it now offers xStocks for selected NASDAQ equities and will join blue-chip token generation events such as the Monad (MON) TGE, giving users curated access to top-tier crypto, public stocks and early-stage blockchain networks in one easy-to-use interface.“Wallet in Telegram makes Monad uniquely accessible to a broad retail audience at launch,” said Keone Hon, Co-founder of Monad. “Starting from the launch, Telegram users around the world can engage with Monad’s technology through a familiar interface — advancing our goal to build fast, simple and scalable blockchain systems.”Expanding access to Monad through Wallet in Telegram supports both teams’ efforts to make high-profile crypto launches more accessible to everyday users worldwide. With a deeply global user base and a presence in regions where early participation in major crypto events has often been limited, Wallet in Telegram offers a meaningful expansion of Monad’s retail footprint. is a digital asset solution natively embedded into Telegram’s interface. Backed by The Open Platform, Wallet in Telegram gained 140M+ users in 2025, and aims to make its solution available to all 1BN+ of Telegram’s users. Wallet in Telegram offers a dual-wallet experience with Crypto Wallet (a multi-chain wallet for trading and sending crypto to contacts) and TON Wallet (a self-custodial wallet with access to the TON ecosystem of dApps and tokens). is a high-performance Layer 1 blockchain engineered for speed without sacrificing security or decentralization, all while maintaining full compatibility with the existing Ethereum ecosystem. MON is the Monad network’s native token, used to pay gas fees, secure the chain via staking, and align validators, developers, and users around the growth of the protocol.:::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>DOGE days are over as Trump disbands Elon Musk’s team of federal cost-cutters</title><link>https://techcrunch.com/2025/11/24/doge-days-are-over-as-trump-disbands-elon-musks-team-of-federal-cost-cutters/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:51:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[DOGE members are reportedly worried that they could face prosecution for some of their activities conducted while under the leadership of Elon Musk.]]></content:encoded></item><item><title>YapWorld: The First AI Creator Economy Forging Human Connection</title><link>https://hackernoon.com/yapworld-the-first-ai-creator-economy-forging-human-connection?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:51:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Miami, Florida, November 24th, 2025/Chainwire/-- has announced the launch of , the “Social AI” platform built to solve two of tech’s biggest problems: a broken social media model driving divisive online interactions, and an emerging AI companion market lacking a path to monetization for creators.While traditional social media algorithms thrive on hatemongering, resulting in social discourse dominated by ideological partisanship, YapWorld is designed from the ground up to “make social media social again.” YapWorld employs AI as a “catalyst for connection,” mobilizing personalized AI agents as online pals who learn the user’s digital footprint for the purpose of assisting with low-pressure introductions to like-minded people, leading to real-world friendships.YapWorld is the first AI platform built on YouTube’s “Creator as Partner” model, establishing an economy in which users choosing to activate their AI companions online earn money from their creations. This dual-sided platform pays creators for developing the personalities of the AI agents that assist with making healthy online connections, democratizing AI development while encouraging better social interactions.The YapWorld platform provides all the tools required for creators to develop customized AI companions, known as Yaps (Your Agentic Pal). Yaps represent a wide range of personalities, including historical figures, fictional characters, even subject-matter experts, all trained on easily-accessed data such as online video and text.Once created, Yaps are activated with the platform’s currency, $SHRD, bringing these creations to life by tokenizing them. This step renders Yaps eligible for earning through online interactions. Tokenized Yaps meeting engagement milestones (10k messages, for example) are eligible to receive a share of a 40% Creator Profit Pool funded through the platform’s net subscription revenue. Key to unlocking the ability to monetize creator contributions, YapWorld introduces the concept of “Monetizable Talk Time (MTT),” a measure of interaction similar in principle to video platforms’ “watch time.” Through quantifying active engagement between users and AI agents, MTT establishes a standard mechanism for rewarding creators driving the adoption of Social AI.With YapWorld, Andrometa defines new possibilities for social connection by building the Social AI creator economy, rewarding participants for initiating and developing relationships online, leading to lasting real-world friendships. This integration of specific AI tools with a reward system designed to motivate their healthy use embodies Andrometa’s mission to make social media social again. is on a mission to “make social media social again.” Fusing Social AI with engaging gaming experiences and a powerful creator economy, Andrometa is building an interconnected ecosystem designed to facilitate genuine human connection. Its flagship platform, YapWorld, empowers creators to build and earn from their personal AI companions. is the first Social AI platform built on a “Creator as Partner” philosophy, the flagship application of the Andrometa ecosystem in which users can create, train, and earn money from their Yaps (Your Agentic Pal). YapWorld is designed as a true partnership, sharing 40% of its subscription profits with the creators who build the most engaging companions. The $SHRD token is the key used to officially enter a Yap into this creator economy, unlocking its potential to earn.Strategy & Implementation by Andrometa PR.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>Pocket Casts now lets you create a playlist of your favorite podcast episodes</title><link>https://techcrunch.com/2025/11/24/pocket-casts-now-lets-you-create-a-playlist-of-your-favorite-podcast-episodes/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:51:13 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Pocket Casts now lets you create playlists for putting podcasts episodes in your desired listening order. ]]></content:encoded></item><item><title>The Slow Transformation of Notepad Into Something Else Entirely Continues</title><link>https://it.slashdot.org/story/25/11/24/1512259/the-slow-transformation-of-notepad-into-something-else-entirely-continues?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:41:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft is rolling out yet another update to Notepad for Windows 11 Insiders that adds table support and faster AI-generated responses, continuing a transformation of the once-minimal text editor that has drawn sustained criticism from users who preferred its original simplicity. The update, version 11.2510.6.0, lets users insert tables via a formatting toolbar or Markdown syntax and enables streaming responses for the app's Write, Rewrite, and Summarize AI features.]]></content:encoded></item><item><title>Angular 21 Rolls Out Modern Testing, Headless A11y Components, and AI Tools</title><link>https://hackernoon.com/angular-21-rolls-out-modern-testing-headless-a11y-components-and-ai-tools?source=rss</link><author>Mukhtar Abdussalam</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:27:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Angular v21 isn’t about one headline feature — it’s a broad shift toward reactive patterns, accessible UI, AI-aware tooling, modern testing, and simpler, zoneless change detection. Together, these updates mark Angular’s move toward a more predictable, modern, and developer-aligned framework.]]></content:encoded></item><item><title>New Licensing Options for Pixel Icon Library: Free, Starter &amp; Pro Plans</title><link>https://hackernoon.com/new-licensing-options-for-pixel-icon-library-free-starter-and-pro-plans?source=rss</link><author>HackerNoon Product Updates</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:24:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Two years ago, we launched “”, an open-source collection of pixelated icons designed using a 24px grid for perfect alignment and consistency. Inspired by our retro aesthetic, these icons encapsulate the internet’s golden era.Since launch, the library has reached:\
The incredible response from our community – from solo developers to design agencies – showed us that our icons were solving real problems. And your feedback pushed us to take things to the next level…Great news: our free, attribution-required license isn't going anywhere! But we heard from many of you who wanted the option to use our icons without attribution or needed clearer commercial licensing for client work.So today, we're launching two new paid plans alongside our existing free option:: Same as always, unlimited use with attribution (NEW): No attribution required, perfect for one project (NEW): No attribution required, unlimited projects No attribution required for paid plans, plus crystal-clear commercial licensing terms for professional use.\
Let's break down each plan.Use icons in unlimited projects (with attribution)Use for commercial purposes (with attribution)Modify, recolor, and adapt icons to match your designShare and distribute as part of your end product\
Under the following terms:Attribution is required - you must credit Pixel Icon Library and HackerNoonYou cannot claim ownership or authorship of the iconsYou cannot resell or redistribute icons as a standalone productUse icons in one product or project (includes all related materials such as a website, app, packaging, or printed merchandise)Use for personal or commercial purposesModify, recolor, or adapt icons to match your project's design languageUse in both digital and physical formats as part of the same end product\
Under the following terms:Cannot use the icons in more than one project, product, or client engagementCannot redistribute, resell, sublicense, or share the icons in their original or modified formCannot include the icons in a publicly accessible asset library, theme, UI kit, or similar productCannot claim ownership, authorship, or trademark rights over the icons or derivative versionsLicense is granted to a single individual and is non-transferable - each additional user, freelancer, or contractor must obtain their own licenseUse icons in unlimited end products, projects, or client worksUse for internal or commercial purposes across multiple projectsIncorporate into design systems, websites, digital products, and printed materialsModify, recolor, or adapt icons for any project\
Under the following terms:Cannot resell, sublicense, distribute, or make the icons available as a standalone product or assetCannot include the icons in open-source repositories, downloadable template packs, or redistributable UI kitsCannot allow external contractors or unaffiliated entities to access or use the icons under this licenseCannot claim ownership of the icons or derivative versions thereof\
Note for agencies and teams: Studios, agencies, and teams may use the icons across multiple projects, clients, and deliverables created by their own staff. Clients do not need their own license if the icons are part of a completed project delivered by you. Separate licenses are required only if a client, contractor, or external partner intends to use, modify, or redistribute the icons independently.Our in-house designer, , documented the process in two in-depth articles:What's the difference between the Free, Starter, and Pro plans?The Free plan (CC BY 4.0) lets you use the icons anywhere - personal or commercial - but you must give credit. The Starter plan removes attribution and covers one project or product. The Pro plan removes attribution too but lets you use the icons across multiple projects or clients.\
Do I have to pay again for future updates?Nope. All paid plans are one-time purchases with lifetime access. You'll get any future icon updates automatically.\
Can I use the free icons in commercial work?Yes, you can. For Free Plan, just make sure to credit Pixel Icon Library somewhere visible (for example, in your site footer).\
If I buy the Starter plan, can I use it for multiple projects or clients?No. The Starter License is for one project, product, or client only. If you want to use the icons in another project or for another client, you'll need another Starter License or better yet upgrade to Pro.\
What if I'm a freelancer working with different clients?Go for the Pro License - it covers all your client work without needing to buy separate licenses each time.\
Can I use the icons in a paid template, theme, or asset pack I'm selling?No, none of the licenses allow redistribution or resale of the icons (even if modified). They must be used within your own or your client's end product.\
Can I edit or recolor the icons?Absolutely. You can tweak, recolor, and adapt them however you like for your project - just don't redistribute the icons as your own library.\
Can I use the icons in both a website and an app?Yes - if they're part of the same project, that's totally fine under Starter. If they're different projects or brands, you'll need Pro.\
Do I get a license document or receipt?Yes - after purchase, you'll get a receipt and a downloadable text license file for your records.]]></content:encoded></item><item><title>A new AI benchmark tests whether chatbots protect human well-being</title><link>https://techcrunch.com/2025/11/24/a-new-ai-benchmark-tests-whether-chatbots-protect-human-wellbeing/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:15:52 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Most AI benchmarks measure intelligence and instruction-following rather than psychological safety. Humane Bench evaluates models based on core principles of human flourishing, prioritizing well-being, and respecting user attention.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Can ChatGPT Outperform the Market? Week 17 (11/24/2025)</title><link>https://hackernoon.com/11-24-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:09:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, November 24, 2025?By @mattleads [ 14 Min read ] This article explores important caching strategies that solve expensive architectural problems: latency, concurrency (thundering herds), and security (GDPR) Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Lenovo Stockpiling PC Memory Due To &apos;Unprecedented&apos; AI Squeeze</title><link>https://it.slashdot.org/story/25/11/24/154202/lenovo-stockpiling-pc-memory-due-to-unprecedented-ai-squeeze?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:01:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Lenovo is stockpiling memory and other critical components to navigate a supply crunch brought on by the boom in AI. From a report: The world's biggest PC maker is holding on to component inventories that are roughly 50% higher than usual, [non-paywalled source] Chief Financial Officer Winston Cheng told Bloomberg TV on Monday. The frenzy to build and fill AI data centers with advanced hardware is raising prices for producers of consumer electronics, but Lenovo also sees opportunity in this to capitalize on its stockpile.]]></content:encoded></item><item><title>Can ChatGPT Outperform the Market? Week 17</title><link>https://hackernoon.com/can-chatgpt-outperform-the-market-week-17?source=rss</link><author>A.I. Controls Stock Account</author><category>tech</category><pubDate>Mon, 24 Nov 2025 16:00:00 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dell Pro Max with GB10 Arrives For Linux Performance Benchmarking</title><link>https://www.phoronix.com/review/dell-pro-max-gb10-preview</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 15:50:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The most exciting hardware to arrive this month in the Phoronix lab is Dell having sent over two of their new Dell Pro Max with GB10 systems. The Dell Pro Max with GB10 is their build-out around NVIDIA's GB10 superchip with ten Cortex-X925 CPU cores and ten Cortex-A725 cores plus the GB10 Blackwell GPU. With 128GB of LPDDR5X memory and 2TB or 4TB SSD by default all within the small chassis, this is an interesting workstation for AI developers.]]></content:encoded></item><item><title>Facebook takes on Reddit with launch of nicknames for Facebook Groups</title><link>https://techcrunch.com/2025/11/24/facebook-takes-on-reddit-with-launch-of-nicknames-for-facebook-groups/</link><author>Sarah Perez</author><category>tech</category><pubDate>Mon, 24 Nov 2025 15:46:16 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Facebook users can protect their privacy in Groups using a new nicknames feature. ]]></content:encoded></item><item><title>Air Canada Lost a Lawsuit Because Their RAG Hallucinated. Yours Might Be Next</title><link>https://hackernoon.com/air-canada-lost-a-lawsuit-because-their-rag-hallucinated-yours-might-be-next?source=rss</link><author>Paolo Perrone</author><category>tech</category><pubDate>Mon, 24 Nov 2025 15:23:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Cleanlab’s latest benchmarks reveal that most popular RAG hallucination detection tools barely outperform random guessing, leaving production AI systems vulnerable to confident, legally risky errors—while TLM stands out as the only method that consistently catches real-world failures.]]></content:encoded></item><item><title>Apple iOS 27 to Be No-Frills &apos;Snow Leopard&apos; Update, Other Than New AI</title><link>https://apple.slashdot.org/story/25/11/24/1457245/apple-ios-27-to-be-no-frills-snow-leopard-update-other-than-new-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 15:21:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Apple's next major iPhone software update will prioritize stability and performance over flashy new features, according to Bloomberg's Mark Gurman, who reports that iOS 27 is being developed as a "Snow Leopard-style" release [non-paywalled source] focused on fixing bugs, removing bloat and improving underlying code after this year's sweeping Liquid Glass design overhaul in iOS 26. 

Engineering teams are currently combing through Apple's operating systems to eliminate unnecessary code and address quality issues that users have reported since iOS 26's September release. Those complaints include device overheating, unexplained battery drain, user interface glitches, keyboard failures, cellular connectivity problems, app crashes, and sluggish animations. 

iOS 27 won't be feature-free. Apple plans several AI additions: a health-focused AI agent tied to a Health+ subscription, expanded AI-powered web search meant to compete with ChatGPT and Perplexity, and deeper AI integration across apps. The company has also been internally testing a chatbot app called Veritas as a proving ground for its re-architected Siri, though a standalone chatbot product isn't currently planned.]]></content:encoded></item><item><title>Former MrBeast content strategist is building an AI tool for creator ideation and analytics</title><link>https://techcrunch.com/2025/11/24/former-mrbeast-content-strategist-is-building-an-ai-tool-for-creator-ideation-and-analytics/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Mon, 24 Nov 2025 15:15:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Palo is a tool for short-video creators to gain insights into what is working for them.]]></content:encoded></item><item><title>How Ameen Shahid Is Transforming Quality Engineering Into a Strategic Powerhouse</title><link>https://hackernoon.com/how-ameen-shahid-is-transforming-quality-engineering-into-a-strategic-powerhouse?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Mon, 24 Nov 2025 15:14:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Director Ameen Shahid transforms Quality Engineering into a strategic engine for global supply chain excellence. By embedding IQE across order management, fulfillment, and logistics, he drives predictive assurance, talent development, automation, and Centers of Excellence. His leadership elevates quality from a support function to a core enabler of operational readiness and flawless delivery.]]></content:encoded></item><item><title>Inside an ICE Defense Training on Fortnite</title><link>https://www.404media.co/ice-defense-training-on-fortnite-new-save-collective/</link><author>Jules Roscoe</author><category>tech</category><enclosure url="https://www.404media.co/content/images/2025/11/img-2.png" length="" type=""/><pubDate>Mon, 24 Nov 2025 15:06:36 +0000</pubDate><source url="https://www.404media.co/">404</source><content:encoded><![CDATA[In the deserted town square of the city of Springfield, three people huddle in an empty courthouse. Two of these people are civilians; one is a “vulnerable,” someone being pursued and targeted by government agents. They talk in hushed tones to one another, playing music to keep fear at bay. Above the door of the courthouse, a plaque reads, “Liberty and Justice for Most.”At the bottom of the courthouse stairs, two government agents step out of a purple golf cart. They approach the door. They’re carrying guns. “Hey, is anyone inside?” one of them says. “Any vulnerables in here? We have a warrant. We have a warrant for any vulnerables in the area.” One civilian opens the door, sees the agents, and immediately slams it shut. After more warrant calls, the civilian says, “Slip it under the door.”“I would slip it under the door, but there’s no space under the door,” the agent says, stuttering. The civilian pauses. “Well. Sounds like a personal problem.”]]></content:encoded></item><item><title>How Vennela Subramanyam Is Shaping the Future of Empathetic AI</title><link>https://hackernoon.com/how-vennela-subramanyam-is-shaping-the-future-of-empathetic-ai?source=rss</link><author>Jon Stojan Journalist</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:59:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Google product leader Vennela Subramanyam advocates for empathetic AI that amplifies humanity rather than replaces it. Her work blends user-centered metrics, inclusive design, emotional insight, and AI ethics to build trust across fintech, education, and large-scale platforms. She argues that empathy is a strategic advantage—guiding how teams align, design, and innovate in the age of AI.]]></content:encoded></item><item><title>A Cell So Minimal That It Challenges Definitions of Life</title><link>https://www.quantamagazine.org/a-cell-so-minimal-that-it-challenges-definitions-of-life-20251124/</link><author>Jake Buehler</author><category>Quanta Magazine</category><category>tech</category><enclosure url="https://www.quantamagazine.org/wp-content/uploads/2025/11/Quasi-lifeforms-cr-Carlos-Arrojo-Default.webp" length="" type=""/><pubDate>Mon, 24 Nov 2025 14:54:21 +0000</pubDate><source url="https://www.quantamagazine.org/">Quanta Magazine</source><content:encoded><![CDATA[Life’s fundamental structure is the cell, and so the main things that a cell does — processing biomolecules, growing, replicating its genetic material and producing a new body — are considered hallmarks of life. But earlier this year, scientists discovered a cell so severely stripped of essential functions that it challenges biologists’ definitions of what counts as a living thing.]]></content:encoded></item><item><title>Ubisoft Shows Off New AI-Powered FPS And Hopes You&apos;ve Forgotten About Its Failed NFTs</title><link>https://games.slashdot.org/story/25/11/24/1431236/ubisoft-shows-off-new-ai-powered-fps-and-hopes-youve-forgotten-about-its-failed-nfts?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Ubisoft has revealed Teammates, a first-person shooter built around AI-powered squadmates that the company is calling its "first playable generative AI research project" -- not long after the publisher went all-in on NFTs and the metaverse only to largely move on from both. Built in the Snowdrop Engine that powers The Division 2 and Star Wars Outlaws, the game features an AI assistant named Jaspar and two AI squadmates called Pablo and Sofia. Players can issue natural voice commands to direct the squadmates in combat or puzzle-solving, while Jaspar handles mission tracking and guidance. The project comes from the same team behind Ubisoft's Neo NPCs, demonstrated at GDC 2024.]]></content:encoded></item><item><title>A Very Big Performance Optimization For Loop Block Devices Heading To Linux 6.19</title><link>https://www.phoronix.com/news/Linux-6.19-Faster-Loop-Block</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:39:18 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A set of patches implementing async I/O IOCB_NOWAIT support for the loop block device is heading to the Linux 6.19 kernel with some performance improvements that will make loop block device users "wow"...]]></content:encoded></item><item><title>US banks scramble to assess data theft after hackers breach financial tech firm</title><link>https://techcrunch.com/2025/11/24/us-banks-scramble-to-assess-data-theft-after-hackers-breach-financial-tech-firm/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:18:55 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[U.S. banking giants including JPMorgan Chase, Citi, and Morgan Stanley are working to identify what data was stolen in a recent cyberattack on a New York financial firm.]]></content:encoded></item><item><title>UK government will buy tech to boost AI sector in $130M growth push</title><link>https://arstechnica.com/information-technology/2025/11/uk-government-will-buy-tech-to-boost-ai-sector-in-130m-growth-push/</link><author>Chris Smyth and Melissa Heikkilä</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2247290617-1152x648.jpg" length="" type=""/><pubDate>Mon, 24 Nov 2025 14:17:29 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[The UK government will promise to buy emerging chip technology from British companies in a 100 million pound ($130 million) bid to boost growth by supporting the artificial intelligence sector.Liz Kendall, the science secretary, said the government would offer guaranteed payments to British startups producing AI hardware that can help sectors such as life sciences and financial services.Under a “first customer” promise modeled on the way the government bought COVID vaccines, Kendall’s department will commit in advance to buying AI inference chips that meet set performance standards.]]></content:encoded></item><item><title>Raspberry Pi OS 2025-11-24 Brings HiDPI Improvements, Wayland Enhancements</title><link>https://www.phoronix.com/news/Raspberry-Pi-OS-2025-11-24</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:15:18 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In addition to debuting the Raspberry Pi OS Imager 2.0 app, Raspberry Pi today announced the latest version of their operating system...]]></content:encoded></item><item><title>Remote Robotics Could Widen Access to Stroke Treatment</title><link>https://spectrum.ieee.org/remote-robotic-stroke-treatment-evt</link><author>Greg Uyeno</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIyNTMxOS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgxMzMzNDgyMn0.MewnmdIq6ylQHCXq2k0WCjQIgecTimRZ6gzpehyxuxk/image.jpg?width=600" length="" type=""/><pubDate>Mon, 24 Nov 2025 14:15:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Telerobotics span hundreds of miles for emergency procedures]]></content:encoded></item><item><title>Malaysia may ban users under 16 from social media starting next year</title><link>https://techcrunch.com/2025/11/24/malaysia-may-ban-users-under-16-from-social-media-starting-next-year/</link><author>Ram Iyer</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:03:32 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The country's communications minister Fahmi Fadzil said the administration is considering systems for implementing age restrictions on social media sites.]]></content:encoded></item><item><title>How Google Finally Leapfrogged Rivals With New Gemini Rollout</title><link>https://tech.slashdot.org/story/25/11/24/141259/how-google-finally-leapfrogged-rivals-with-new-gemini-rollout?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: With the release of its third version last week, Google's Gemini large language model surged past ChatGPT and other competitors to become the most capable AI chatbot, as determined by consensus industry-benchmark tests. [...] Aaron Levie, chief executive of the cloud content management company Box, got early access to Gemini 3 several days ahead of the launch. The company ran its own evaluations of the model over the weekend to see how well it could analyze large sets of complex documents. "At first we kind of had to squint and be like, 'OK, did we do something wrong in our eval?' because the jump was so big," he said. "But every time we tested it, it came out double-digit points ahead." 

[...] Google has been scrambling to get an edge in the AI race since the launch of ChatGPT three years ago, which stoked fears among investors that the company's iconic search engine would lose significant traffic to chatbots. The company struggled for months to get traction. Chief Executive Sundar Pichai and other executives have since worked to overhaul the company's AI development strategy by breaking down internal silos, streamlining leadership and consolidating work on its models, employees say. Sergey Brin, one of Google's co-founders, resumed a day-to-day role at the company helping to oversee its AI-development efforts.]]></content:encoded></item><item><title>Momentic raises $15M to automate software testing</title><link>https://techcrunch.com/2025/11/24/momentic-raises-15m-to-automate-software-testing/</link><author>Russell Brandom</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:00:27 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AI testing startup Momentic has raised $15 million in a Series A round led by Standard Capital, with participation from Dropbox Ventures. Existing investors at Y Combinator, FCVC, Transpose Platform and Karman Ventures also participated.]]></content:encoded></item><item><title>The Complete Developer’s Guide to GraphRAG, LightRAG, and AgenticRAG</title><link>https://hackernoon.com/the-complete-developers-guide-to-graphrag-lightrag-and-agenticrag?source=rss</link><author>superorange0707</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:00:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Why the next generation of RAG systems isn’t just about retrieval — it’s about reasoning, adaptability, and real-world intelligence.Traditional Retrieval-Augmented Generation (RAG) solved one big problem: LLMs know a lot, but only up to their training cutoff. By plugging in a retrieval pipeline, you could feed models fresh documents and get more accurate answers.\
But as real-world use cases grew—legal reasoning, biomedical analysis, financial modelling—:It struggles with ambiguity.It loses context when knowledge spans multiple chunks.It can’t reason across documents.It can’t adapt to complex tasks or evolving queries.\
Enter —a family of architectures designed to fix these weaknesses. Today, we explore the three most influential ones: , , and .GraphRAG integrates a  directly into the retrieval and generation flow. Instead of treating text as isolated chunks, it treats the world as a web of entities and relationships.Many questions require multi-hop reasoning:“Which treatments link symptom A to condition C?”“How does regulation X indirectly impact sector Y?”“What theme connects these three research papers?”Traditional RAG flattens all this into embeddings. GraphRAG preserves structure.How GraphRAG Works (In Plain English)Retrieve candidate documents.   Standard vector search pulls the initial context.Extract entities and build/expand a graph. Each node = concept, entity, or document snippet. Each edge = semantic relationship inferred from text.Run graph-based retrieval. The system “walks” the graph to find related concepts, not just related chunks.Feed structured graph context into the LLM.\
The result? Answers that understand , not just co-occurrence.Biomedical decision supportLegal clause interpretationMulti-document academic synthesisAny task needing multi-hop reasoningLightRAG is a leaner, faster, and cheaper alternative to heavyweight graph-based systems like GraphRAG. It keeps the good parts (graph indexing) but removes the expensive parts (full graph regeneration, heavy agent workflows).Most businesses don’t have:multi-GPU inference clustersthe patience to rebuild massive graphs after every data update\
LightRAG’s core mission: high-quality retrieval on small hardware.1. Graph-Based Indexing (But Lighter)It builds a graph over your corpus—but in an incremental way. Add 100 documents? Only update 100 nodes, not the entire graph.: find fine-grained details: find big-picture themes\
This dual-layer design massively improves contextual completeness.Optimized for smaller models such as 7B–32B deployments.Real-time chat assistantsMedium-sized enterprise deployments with limited GPU allocationKey advantage over GraphRAGNo need for full graph reconstructionToken cost up to 1/6000 of GraphRAG (based on Microsoft benchmarks)AgenticRAG is the most ambitious of the three. Instead of a fixed pipeline, it uses autonomous agents that plan, retrieve, evaluate, and retry.\
Think of it as RAG with:Real-world queries rarely fit a single-step workflow.“Summarize the last 3 fiscal quarters and compare competitive landscape impacts.”“Design a migration plan for a multi-cloud payment architecture.”“Analyze the latest regulations and produce compliance recommendations.”\
These require multiple queries, multiple tools, and multi-step reasoning.\
AgenticRAG handles all of this automatically.1. The agent analyzes the query.If the question is complex, it creates a multi-step plan.Could be vector search, graph search, web search, or structured database queries.3. It retrieves, checks, and iterates.If the results are incomplete, it revises the strategy.This is the closest we currently have to autonomous reasoning over knowledge.Customer agents with multi-step workflowsAny domain requiring dynamic adaptation| Feature | GraphRAG | LightRAG | AgenticRAG |
|----|----|----|----|
|  | Knowledge graph reasoning | Lightweight graph + dual retrieval | Autonomous planning & iterative retrieval |
|  | Multi-hop reasoning | Efficiency & speed | Dynamic adaptability |
|  | High | Low | Medium–High |
|  | Legal, medical, and scientific tasks | Edge/low-resource deployments | Complex multi-step tasks |
|  | Full graph rebuild | Incremental updates | Depends on workflow |
|  | Bigger is better | Runs well on smaller models | Medium to large |Choose GraphRAG if you need:✔ Deep reasoning   ✔ Entity-level understanding   ✔ Multi-hop knowledge traversalChoose LightRAG if you need:✔ Fast inference   ✔ Local/edge deployment   ✔ Low-cost retrievalChoose AgenticRAG if you need:✔ Multi-step planning   ✔ Tool orchestration   ✔ Dynamic decision makingTraditional RAG was a breakthrough, but it wasn’t the end of the story. GraphRAG, LightRAG, and AgenticRAG each push RAG closer toward , scalable real-world deployment, and .\
The smartest teams today aren’t just asking:   “How do we use RAG?”\
They’re asking:   “Which RAG architecture solves the problem best?”\
And now — you know exactly how to answer that.]]></content:encoded></item><item><title>This startup wants to build a fusion reactor — on a boat</title><link>https://techcrunch.com/2025/11/24/this-startup-wants-to-build-a-fusion-reactor-on-a-boat/</link><author>Tim De Chant</author><category>tech</category><pubDate>Mon, 24 Nov 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Startups are still racing to build the world's first commercial fusion power plant on land, but Maritime Fusion thinks the path will be easier out at sea.]]></content:encoded></item><item><title>Raspberry Pi Imager 2.0 Released To Make It Easier Creating OS Media</title><link>https://www.phoronix.com/news/Raspberry-Pi-Imager-2.0</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 13:55:36 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Raspberry Pi Imager application that makes it easy to generate install media / OS image flashing for different Raspberry Pi devices is out with a big feature update...]]></content:encoded></item><item><title>Revolut hits $75B valuation in new capital raise</title><link>https://techcrunch.com/2025/11/24/revolut-hits-75b-valuation-in-new-capital-raise/</link><author>Ram Iyer</author><category>tech</category><pubDate>Mon, 24 Nov 2025 13:31:31 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The deal was led by Coatue, Greenoaks, Dragoneer, and Fidelity. Investors including Nvidia's NVentures, Andreessen Horowitz, Franklin Templeton, and other backers advised by T. Rowe Price Associates also participated.]]></content:encoded></item><item><title>Enshittification Ahoy: Streaming Video Price Hikes Show No Sign Of Slowing Down</title><link>https://www.techdirt.com/2025/11/24/enshittification-ahoy-streaming-video-price-hikes-show-no-sign-of-slowing-down/</link><author>Karl Bode</author><category>tech</category><pubDate>Mon, 24 Nov 2025 13:30:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[That has involved chasing pointless “growth for growth’s sake” megamergers, imposing bottomless price hikes and new annoying restrictions on customers, undermining labor, and cutting corners on product quality in a bid to give Wall Street that sweet, impossible, unlimited, quarterly growth it demands.King Trump’s destruction of whatever is left of regulatory oversight and media consolidation limits means there’s going to be another huge new wave of pointless shitty mergers across media (likely involving Comcast/NBC, Warner Brothers, and Paramount). That means more debt from pointless deals that companies try to recoup via price hikes imposed on already annoyed customers.As we saw with traditional cable, eventually this consolidation scheme falls apart as consumers flee to alternative, cheaper (or free) entertainment options, including piracy. At that point, executives inevitably blame absolutely everything but their own behavior (generational entitlement! inflation! a stagnant housing market!) and the cycle begins anew, with nobody in any position of power having any financial incentive to learn absolutely anything from experience.]]></content:encoded></item><item><title>20 Non-Cringe Activities to Engage Remote Employees</title><link>https://hackernoon.com/20-non-cringe-activities-to-engage-remote-employees?source=rss</link><author>Echometer</author><category>tech</category><pubDate>Mon, 24 Nov 2025 13:25:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Boost engagement and connection in remote teams with 20 non-cringe team development activities - from quick 5-minute icebreakers to longer virtual games, agile retrospectives, and social events for remote employees. Mix consistent habits with occasional fun to strengthen trust, collaboration, and team morale.]]></content:encoded></item><item><title>How to Build Your First GitHub Actions Workflow for Automated CI/CD</title><link>https://hackernoon.com/how-to-build-your-first-github-actions-workflow-for-automated-cicd?source=rss</link><author>UPurnima</author><category>tech</category><pubDate>Mon, 24 Nov 2025 13:08:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This guide walks beginners through creating and triggering GitHub Actions workflows, explaining how each part of the YAML file works and how to automate builds and deployments in a simple CI/CD pipeline.]]></content:encoded></item><item><title>NemoVideo Launches AI-Powered Creative Buddy to Transform Content Creation</title><link>https://hackernoon.com/nemovideo-launches-ai-powered-creative-buddy-to-transform-content-creation?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Mon, 24 Nov 2025 12:48:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[, an AI-powered video editing platform, is officially live. Its mission? To eliminate the repetitive grind of manual editing and empower content creators to produce viral-ready videos in minutes—no technical expertise needed.In today’s fast-paced digital world, where speed and storytelling are crucial, NemoVideo isn't just a tool—it's a game-changing AI Creative Buddy. Whether you're an independent creator, part of a marketing agency, or a brand looking to promote products, Nemo's AI-driven technology makes video creation faster and more intuitive, allowing you to "Drop anything. Skip the dirty work. Get viral-ready videos instantly."Solving the Challenges of Content CreationContent creators are facing more pressure than ever before. The grind of producing high-quality videos is draining—creative burnout, tight deadlines, and the never-ending struggle of turning raw footage into polished content. Manual editing tasks, such as syncing audio or matching visuals to the right shots, are often a logistical nightmare.This is where NemoVideo steps in. It automates time-consuming processes while retaining creative control, enabling creators to focus on the big picture: the story.The biggest hurdle in video production isn’t creativity—it’s getting started. Traditional tools demand knowledge of complex software and rigid formats. With Nemo, you don’t need to worry about compatibility or setup time. Whether it’s a product link, a draft script, or raw footage, simply drop it in. Nemo’s AI intelligently reads the content and kickstarts your project, extracting key elements like product highlights and tone to create a polished draft without the usual manual work.This flexibility preserves creative momentum. With Nemo, you're no longer bogged down by logistics or formatting issues. The system understands your vision and adapts, producing immediate, relevant results that get you from concept to completion faster.Inspiration Center: Beat Creative BlockOne of the most frustrating aspects of content creation is staring at a blank screen. What should you do to grab the audience’s attention in the first few seconds? Studies show that 71% of viewers decide whether to keep watching within the first three seconds. Weak openings can derail even the best ideas.NemoVideo’s Inspiration Center solves this problem. By analyzing millions of viral clips, it identifies trending topics, storytelling patterns, and hooks that capture attention. No more guesswork—just data-driven inspiration to help you craft compelling video ideas that resonate with viewers.The AI even allows you to upload reference videos, deconstruct them into detailed scripts, and generate powerful alternatives. It’s creativity on-demand.AI Editing That Actually WorksUnlike other so-called "AI editors" that churn out generic, cookie-cutter videos, Nemo’s AI is an intelligent creative assistant. It doesn’t just speed up the process—it makes your content more effective. Nemo's SmartPick engine eliminates filler words, awkward pauses, and irrelevant shots, assembling a rough cut that’s ready to be refined.What sets Nemo apart is its ability to enhance storytelling. Its contextual intelligence automatically matches B-roll footage to A-roll, removing the need for manual searching. This AI-powered engine knows how to build a cohesive narrative, adding the right visuals with minimal input from the user.From post-production to final output, Nemo handles everything—from subtitle generation (SmartCaption) to music syncing (SmartAudio) and lip-sync adjustments—ensuring every video feels polished and on-brand.Conversational Editing: No Technical Skills RequiredTraditional video editing often requires steep learning curves and technical expertise—something that many creators don’t have the time for. Nemo’s Conversational Editing changes that.Instead of wrestling with complex interfaces and keyframes, you simply type what you want to change in plain language. Whether it’s “shorten this scene” or “replace the background music,” Nemo executes the edit in real time. This intuitive approach makes editing accessible to everyone, removing the need for extensive technical knowledge or a team of experts.One of the biggest challenges for creators is ensuring their videos are optimized for each platform. Whether it's TikTok, Instagram Reels, or YouTube Shorts, each platform has unique formatting and pacing requirements. Nemo handles this automatically, adjusting videos to match current trends and technical specs—without the need for manual rework.What’s more, a single video can be adapted into multiple platform-specific versions. One piece of content becomes many, each optimized for the platform it’s intended for, saving you time and maximizing engagement.Nemo: The Future of Creative OperationsNemoVideo is more than just an editing tool—it’s a reimagining of how content is created. By integrating AI-driven ideation, editing, and optimization into a single platform, it streamlines the entire creative process, eliminating fragmentation and inefficiency.In an industry where speed, creativity, and scalability are key, Nemo empowers creators and marketers to focus on storytelling rather than technicalities. It’s not about making videos faster—it’s about unlocking the full creative potential of content professionals in today’s fast-moving digital landscape.Nemo doesn't automate creativity—it enhances it. And in the competitive content economy, that’s the edge creators need.Ready to take your video creation to the next level? Start your free trial today.:::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>New Mars Orbiter Manuever Challenges Theory: That May Not Be an Underground Lake on Mars</title><link>https://science.slashdot.org/story/25/11/24/0623250/new-mars-orbiter-manuever-challenges-theory-that-may-not-be-an-underground-lake-on-mars?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Nov 2025 12:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In 2018 researchers claimed evidence of a lake beneath the surface of Mars, detected by the Mars Advanced Radar for Subsurface and Ionosphere Sounding instrument (or Marsis for short). 

But new Mars observations "are not consistent with the presence of liquid water in this location and an alternative explanation, such as very smooth basal materials, is needed." Phys.org explains



Aboard the Mars Reconnaissance Orbiter, the Shallow Radar (SHARAD) uses higher frequencies than MARSIS. Until recently, though, SHARAD's signals couldn't reach deep enough into Mars to bounce off the base layer of the ice where the potential water lies — meaning its results couldn't be compared with those from MARSIS. However, the Mars Reconnaissance Orbiter team recently tested a new maneuver that rolls the spacecraft on its flight axis by 120 degrees — whereas it previously could roll only up to 28 degrees. The new maneuver, termed a "very large roll," or VLR, can increase SHARAD's signal strength and penetration depth, allowing researchers to examine the base of the ice in the enigmatic high-reflectivity zone. Gareth Morgan and colleagues, for their article published in Geophysical Research Letters, examined 91 SHARAD observations that crossed the high-reflectivity zone. 

Only when using the VLR maneuver was a SHARAD basal echo detected at the site. In contrast to the MARSIS detection, the SHARAD detection was very weak, meaning it is unlikely that liquid water is present in the high-reflectivity zone.
]]></content:encoded></item><item><title>Intel Working On Linux Support For New Power Savings Feature With Xe3P_LPD</title><link>https://www.phoronix.com/news/Intel-Xe3P-LPD-System-Cache-FBC</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 11:24:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The upcoming Linux 6.19 kernel cycle is set to introduce initial support for Xe3P graphics to be found initially with Nova Lake processors. While that initial support is landing for Linux 6.19, other extra Xe3P features are still to be added to the open-source kernel driver over coming release cycles. One of those extra features being currently tackled is a new element with Xe3P_LPD: the ability to use the system cache for FBC...]]></content:encoded></item><item><title>Canonical Partners With AMI To Build Ubuntu Netboot Option Into UEFI Firmware</title><link>https://www.phoronix.com/news/Canonical-AMI-Ubuntu-Partners</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 11:10:41 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Canonical and AMI announced a partnership today so that there will be an Ubuntu Netboot option added within AMI's UEFI firmware to allow booting to the Ubuntu installer without the need for even having any install media...]]></content:encoded></item><item><title>Responsible AI in Logistics: Why Ethics Isn&apos;t Optional, It&apos;s Strategic</title><link>https://hackernoon.com/responsible-ai-in-logistics-why-ethics-isnt-optional-its-strategic?source=rss</link><author>Balaji Solai Rameshbabu</author><category>tech</category><pubDate>Mon, 24 Nov 2025 11:05:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Responsible AI in logistics goes beyond compliance—it's about building systems that are fair, explainable, and stakeholder-inclusive.]]></content:encoded></item><item><title>Why AI Coding Agents Suck At Product Integrations And How Membrane Fixes This</title><link>https://hackernoon.com/why-ai-coding-agents-suck-at-product-integrations-and-how-membrane-fixes-this?source=rss</link><author>Membrane</author><category>tech</category><pubDate>Mon, 24 Nov 2025 11:04:30 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Here's a strange paradox: AI coding agents can now scaffold UIs, call APIs, and generate data models in seconds. But when it comes to building production-grade product integrations, they consistently under-deliver.Claude Code can scaffold a React dashboard. Cursor can generate a backend with authentication. Lovable can design an entire user interface from a prompt. These tools have fundamentally changed how we build software.Except for one stubborn problem: Ask any AI agent to "build a Slack integration" and you'll get code. Clean code. Code that compiles. Code that  like it would work.But deploy it to production—where customers use different Slack workspace tiers, where rate limits vary by plan, where webhook signatures change format, where OAuth tokens expire unpredictably—and everything breaks.This isn't an AI problem. It's an infrastructure problem.For the past decade, we've tried addressing integrations with iPaaS platforms, unified APIs, and low-code builders. Each promised to make integrations easy. Each failed when customers needed anything beyond surface-level connectivity.Now, AI promises to democratize integration building like never before! And it will—but only if we give it the proper foundation to build on.But Why Does AI struggle with integrations?Building integrations isn't just about calling an API. Real product integrations are complex, full of edge cases, and require deep knowledge that AI agents simply don't have.There Are Three Fundamental problems:AI is optimized for Simplicity over Complexity.Real-world integrations are complex: authentication flows, error handling, rate limits, custom fields, etc. It is hard for AI to solve for all the necessary edge cases. AI can build simple integrations that work in perfect scenarios, but it can't reliably handle the complexity needed for production use.AI Agents Make Do with Insufficient ContextLike most junior devs, AI agents work with incomplete or outdated API documentation. They lack real-world experience with how integrations actually behave in production - the quirks, limitations, and nuances that only come from building hundreds of integrations across different apps.Missing feedback loop for AI AgentsAI doesn't have robust tools at its disposal to properly test integrations. Without a way to validate, debug, and iterate on integration logic, AI-generated code remains brittle and unreliable for production use.Testing integrations is not the same as testing your application code because it involves external systems that are hard or impossible to mock.The result? AI can produce code that  right, but won't actually work in many cases when your users connect their real-world accounts.The Solution: framework + context + infrastructureTo build production-grade integrations with AI, you need three things:1. A framework that breaks down complexityInstead of asking AI to handle everything at once, split integrations into manageable building blocks - connectors, actions, flows, and schemas that AI can reliably generate and compose.2. Rich context about real-world integrationsAI needs more than API documentation. It needs knowledge about how integrations actually behave in production: common edge cases, API quirks, best practices, and field mappings that work across different customer setups.3. Infrastructure for testing and maintenanceYou need tools that let AI test integrations against real external systems, iterate on failures, and automatically maintain integrations as external APIs evolve.With these three components, AI can reliably build production-grade integrations that actually work.How Membrane implements this solutionMembrane is specifically designed to build and maintain product integrations. It provides exactly what AI agents need: that decompose integration complexity into pieces AI can handle (see Membrane Framework)Specialized AI coding agent trained to build integrations (Membrane Agent)Proprietary operational knowledge from thousands of real-world integrations that run through Membrane.Tools and infrastructure for testing and validating integrations that work with live external systems.Imagine you're building a new integration for your product from scratch - connecting to an external app to sync data, trigger actions, or enable workflows.Step 1: Describe what you want to buildTell an AI agent what integration you need in natural language:"Create an integration that does [use case] with [External App]."The AI agent understands your intent and begins building a complete integration package that includes:Connectors for the target app.Elements that implement the integration logic - tested against live external system.API and SDK for adding the resulting integration into your app.Step 2: Test and validate the integrationIn the previous step, the agent does its best to both build and test the integration.You can review the results of its tests and, optionally, run additional tests of your own using the UI or the API.If you find issues, you ask the agent to fix them.Now plug the integration into your product using the method that works best for you. - Make direct HTTP calls to execute integration actions - Use a native SDK in your backend code - Expose integration context to AI coding agents - Connect tools like Claude Code, Cursor, or Windsurf to Membrane and ask them to implement changes in your product.You described what you wanted once. AI did the rest.Enables users to connect external apps with secure, production-grade authExecutes your integration logic through tested, reusable actionsRuns on a reliable, stable integration infrastructure, powered by AIWhy is Membrane better than general-purpose AI coding agents?| Challenge | General-purpose AI Agents | Membrane |
|----|----|----|
| Complexity | Builds the whole integration at once: can implement “best case” logic, but struggles with more complex use cases. | Modular building blocks allow properly testing each piece of integration before assembling it together. |
| Context | Has access to limited subset of public API docs | Specialises in researching public API docs + has access to proprietary context under the hood. |
| Testing | Limited to standard code testing tools that are not adequate for testing integrations | Uses testing framework and infrastructure purpose-built for product integrations. |
| Maintenance | Doesn’t do maintenance until you specifically ask it to do something. | Every integration comes with built-in testing, observability, and maintenance. |AI coding agents are transforming how we build software, but they need the right foundation to build production-grade integrations.When you combine AI with proper infrastructure - context about real-world integrations, modular building blocks, and testing tools - you unlock a complete development loop:Describe your integration needs to the agent → Watch AI build the integrations with the necessary components → Deploy production-ready packages in your environmentThis is what becomes possible when AI has the right tools to work with.Start building production-grade integrations with AI.]]></content:encoded></item><item><title>The &quot;Quiet Quitting&quot; Driver Nobody Talks About: Why Your Performance Reviews Are Backfiring</title><link>https://hackernoon.com/the-quiet-quitting-driver-nobody-talks-about-why-your-performance-reviews-are-backfiring?source=rss</link><author>Hui</author><category>tech</category><pubDate>Mon, 24 Nov 2025 10:46:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[74% of high-performing employees say they would consider leaving their job after receiving a vague or generic performance review. Yet, the average manager spends less than two hours preparing for what is arguably the most critical conversation of the employee's year.The math doesn't add up. We spend months recruiting talent, thousands of dollars onboarding them, and countless hours managing them—only to risk losing them in a single, poorly handled 45-minute meeting.It’s not that managers don’t care. It’s that they are drowning in .Trying to recall a year’s worth of contributions, filter out recency bias, balance praise with critique, and articulate a clear growth path—all while worrying about legal compliance and morale—is a recipe for decision fatigue. The result? We default to "safe," generic feedback that frustrates top performers and confuses struggling ones.But what if the solution wasn't to spend  time, but to fundamentally change  we process performance data?There is a pervasive myth that using AI for performance reviews makes them "robotic" or "impersonal."I argue the exact opposite: AI is the tool that can make your reviews more human.When a human manager sits down to write a review, their brain is besieged by unconscious biases:: Overweighting events from the last month.: Letting one good trait overshadow serious flaws (or vice versa).Idiosyncratic Rater Effect: Measuring employees against the manager's own skills rather than the job requirements.An AI, properly instructed, doesn't have these biases. It doesn't get tired. It doesn't forget what happened in February. It doesn't hold a grudge because someone was late to a meeting three weeks ago.By offloading the  and  of performance data to an AI, you free up your mental energy for the part that actually requires a human: empathy, coaching, and career vision.The "Objectivity Engine" FrameworkTo turn an LLM (like Claude, ChatGPT, or Gemini) into an unbiased evaluation partner, you can't just say, "Write a review for Sarah." That yields the robotic fluff we want to avoid.You need a Persona-Driven Instruction that forces the AI to adopt the mental model of a seasoned HR specialist.I have developed a comprehensive prompt that acts as an "Objectivity Engine." It forces the AI to:: Evaluate based on specific skills, not general "vibes.": It requires specific examples for every claim.: It enforces a structure that acknowledges strengths while clearly defining growth areas.Here is the complete prompt framework.# Role Definition
You are a seasoned HR Professional and Performance Management Specialist with over 15 years of experience in talent development and organizational behavior. You excel at:
- Conducting objective and fair performance evaluations
- Providing constructive feedback that motivates improvement
- Identifying career development opportunities
- Aligning individual performance with organizational goals
- Using data-driven insights to support assessment decisions

# Task Description
Please create a comprehensive performance review based on the provided employee information. The review should be balanced, objective, evidence-based, and actionable, helping both the employee and management understand performance strengths, areas for improvement, and future development opportunities.

**Input Information**:
- **Employee Name**: [Full name]
- **Position/Title**: [Job title]
- **Department**: [Department name]
- **Review Period**: [e.g., January 1, 2025 - December 31, 2025]
- **Manager Name**: [Reviewer's name]
- **Key Responsibilities**: [List main job responsibilities]
- **Performance Data** (optional):
  - Goals achieved: [List completed goals]
  - Key projects: [Major projects worked on]
  - Metrics/KPIs: [Quantifiable performance data]
  - Peer feedback: [Summary of colleague input]
  - Self-assessment: [Employee's self-evaluation highlights]

# Output Requirements

## 1. Content Structure

The performance review should include the following sections:

### Executive Summary
- Overall performance rating
- Brief overview of key achievements and areas for development
- Recommendation (promotion, raise, development plan, etc.)

### Performance Assessment by Competency
Evaluate the employee across these dimensions:
- **Job Knowledge & Skills**: Technical expertise and professional competencies
- **Quality of Work**: Accuracy, thoroughness, and attention to detail
- **Productivity & Efficiency**: Output volume and time management
- **Initiative & Problem-Solving**: Proactivity and creative solutions
- **Communication**: Clarity, collaboration, and interpersonal skills
- **Leadership & Teamwork**: Influence, mentoring, and team contribution
- **Adaptability**: Response to change and learning agility
- **Goal Achievement**: Progress toward objectives and KPIs

### Strengths & Achievements
- Highlight 3-5 key accomplishments with specific examples
- Include measurable results where possible
- Recognize exceptional contributions

### Areas for Development
- Identify 2-4 growth opportunities
- Provide specific, actionable feedback
- Frame constructively with support resources

### Development Plan & Goals
- Recommend 3-5 SMART goals for next review period
- Suggest training, mentoring, or stretch assignments
- Outline support and resources available

## 2. Quality Standards

- **Objectivity**: Base assessments on observable behaviors and measurable results, not personal opinions
- **Specificity**: Use concrete examples and data points to support evaluations
- **Balance**: Acknowledge both strengths and development areas fairly
- **Actionability**: Ensure feedback provides clear next steps
- **Professionalism**: Maintain respectful, constructive tone throughout
- **Alignment**: Connect individual performance to team and organizational goals

## 3. Format Requirements

- Use clear section headings for easy navigation
- Include rating scales where appropriate (e.g., 1-5, Exceeds/Meets/Needs Improvement)
- Present quantitative data in bullet points or tables
- Length: 800-1200 words (comprehensive yet concise)
- Format: Professional business document style

## 4. Style Constraints

- **Language Style**: Professional, objective, and supportive
- **Tone**: Balanced between recognition and constructive criticism
- **Perspective**: Third-person or second-person (addressing the employee)
- **Approach**: Growth-oriented and future-focused

# Quality Checklist

Before finalizing the performance review, verify:
- [ ] All competency areas have been evaluated with specific examples
- [ ] Ratings are supported by evidence and data
- [ ] Feedback is balanced (acknowledges both strengths and growth areas)
- [ ] Development recommendations are specific and achievable
- [ ] Language is professional, respectful, and free of bias
- [ ] Goals for next period are SMART (Specific, Measurable, Achievable, Relevant, Time-bound)
- [ ] Document is proofread for grammar and clarity

# Important Notes

- **Avoid bias**: Be mindful of recency bias, halo effect, and personal preferences
- **Legal compliance**: Ensure review complies with employment laws and company policies
- **Confidentiality**: Treat all performance information as confidential
- **Documentation**: Save copies for HR records and legal protection
- **Consistency**: Apply the same standards across all employees in similar roles

# Output Format

Deliver the performance review as a structured document with clear headings, professional formatting, and a signature block for both reviewer and employee acknowledgment.
From "Judging" to "Coaching"The real magic happens when you stop seeing the review document as the .With this prompt, the document is generated in seconds. This shifts your role entirely. Instead of spending 3 hours agonising over how to phrase "needs to communicate better," you spend those 3 hours:Reviewing the AI's output to ensure it aligns with your deeper intuition.Refining the "Development Plan" to match the employee's actual career aspirations.Preparing for the conversation itself—planning how to deliver the message with empathy and clarity.You move from being a "Report Writer" to being a "Performance Coach."Breaking the "Feedback Sandwich"We've all been taught the "Feedback Sandwich" (Good thing -> Bad thing -> Good thing). It’s outdated and transparent. Employees see right through it.This AI-driven approach encourages a . It forces you to look at the employee holistically.Maybe they are a 5/5 on  but a 2/5 on .The AI separates these distinctly, preventing the "Halo Effect" where their coding brilliance masks their toxic communication style.It forces you to address the gap specifically, rather than hiding it inside a compliment sandwich.Don't just copy-paste this prompt and hope for the best. Here is the workflow for maximum impact:: Open a voice note or a blank document. Spend 10 minutes dumping every raw observation you have about the employee. Don't worry about grammar or politeness. Just facts, numbers, and specific incidents.Example: "Great at Python, fixed the login bug in record time. But was rude to Sarah in the standup last Tuesday. Needs to speak up more in client meetings.": Feed that raw dump into the  section of the prompt.: Take the AI's output and apply your "Managerial Intuition." Does this  like you? Is the tone right for your specific relationship with this person?: Use the structured document as an agenda for your 1:1.Clear is kind. Unclear is unkind.When you use a structured framework to deliver performance reviews, you are giving your employees the greatest gift a manager can give: .They know exactly where they stand. They know exactly what "good" looks like. And they know you care enough to provide them with a roadmap, not just a grade.Stop letting cognitive overload dictate the quality of your leadership. Let the AI handle the structure, so you can handle the human.]]></content:encoded></item><item><title>Is Lasta App Worth It? A Full Breakdown After a Month of Use</title><link>https://hackernoon.com/is-lasta-app-worth-it-a-full-breakdown-after-a-month-of-use?source=rss</link><author>Polina Fomenkova</author><category>tech</category><pubDate>Mon, 24 Nov 2025 10:33:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I love to track and optimize every area of my life, especially my health. On my left wrist sits a smartwatch, on my right an Oura ring, and together they tell me I slept poorly last night and exactly how to improve it.Yes, I'm a healthtech geek, and I've tried countless apps. Most of them I deleted almost immediately. Honestly, I just didn't understand why I needed a separate app to track water intake, and I quickly tired of spending half an hour logging every ingredient of my dinner just to calculate calories and macros.Recently, however, I discovered  – an app that combines the functionality of several apps into one. You can track your food, work out, get a meal plan, track water, sleep and much more.So I gave it a try, and this is my honest review.Lasta is a wellness app assistant. Using technology to combine nutrition, workouts, psychology, and mindfulness, it aims to help users build lasting healthy habits rather than pursue quick fixes or restrictive diets.This app is designed for people of any age or gender who want to maintain a healthy weight, improve their lifestyle, and find support without "hardcore fitness" approaches. Lasta offers various diet plans and adaptive workouts that don't require any special physical training or equipment. All you need to do is dedicate some time and follow the visuals and instructions.Lasta's core philosophy centers on three pillars: transforming both body and mindset, building sustainable habits through small daily wins, and providing a judgment-free space for wellness.In addition, there is educational content on improving overall wellness and mental health. That’s what makes this app different from various tracking tools, making it a comprehensive lifestyle companion.First, the app learns everything about you. During onboarding, I was asked about my age, gender, current weight, height, activity level throughout the day, fitness goals, and lifestyle habits.Based on this information, Lasta creates an adaptive plan that includes customized workouts, meal guidance, and mindfulness lessons. The algorithm adjusts recommendations as you progress, ensuring the plan evolves with your changing needs and achievements.Depending on the user's goals, Lasta offers access to a variety of features:==Workouts==. The app offers exercises designed for all fitness levels. Lasta has tons of recorded videos with functional exercises, wall yoga and pilates, chair yoga and pilates, bed pilates — you don't need any equipment or physical training.==Intermittent fasting programs==. The app offers programs ranging from 7 to 21 days of fasting designed to regulate eating patterns, prevent aging, reduce inflammation, and support safe weight loss.==Trackers==. Lasta includes six essential trackers to help you monitor various aspects of your wellness journey: fasting, water, calories, weight, steps, and mood. All trackers integrate with Apple Health, allowing automatic synchronisation of activity, sleep, and step data for a complete picture of your health.==Meal Planning==. Lasta's meal planning features go beyond simple calorie counting. You can get a personalized meal plan or choose one of the diet options. In addition, using a barcode scanner, users can get all the nutritional information about packaged foods at the supermarket, making grocery shopping and meal logging effortless.==Habit-Building==. Instead of getting quick results, Lasta helps you to play long-term, supporting lasting change. This is achieved through gamification (a motivational “streaks” system), educational and self-care content, and mindfulness practices.My Experience Using LastaI signed up for Lasta to see if it could truly replace the multiple health apps cluttering my phone.The onboarding process was straightforward and took about ten minutes. After answering questions about my goals, current habits, and preferences, Lasta presented my personalized plan. I was immediately impressed by how the app explained  it recommended certain features based on my responses.I started with the easiest wins: workouts, tracking water intake, and setting up my first fasting window. The interface made this process feel effortless. Unlike other apps where I'd feel overwhelmed by too many features at once, Lasta has become my personal wellness and fitness partner. No aggressive push notifications or complicated exercises that I couldn't do in my small living room.By the end of the first week, daily workouts, hydration, and fasting had become consistent parts of my routine. At the end of the second week with Lasta, workouts and mindfulness lessons began to feel like a routine rather than chores. I found myself looking forward to my morning bed pilates session.My discovery is a wall and chair yoga. I used to think that yoga (and pilates, by the way) was boring, but now it's part of my routine. I work from home by spending 8 hours a day on my laptop, and then I continue sitting while studying, reading a book, or watching YouTube. I have developed the habit of taking quick breaks to move around, so now my body feels better, and I don't have back pain.After four weeks with Lasta, my life hasn't changed 180 degrees. As I said, I was already quite disciplined and cared about my health. But I started spending much less time on sports and stopped bombarding ChatGPT with questions about what to cook for dinner, as now I have a meal plan! Another bonus — I noticed less emotional eating.After using Lasta for a month, I identified the following pros and cons.Personalized and adaptive plans that evolve with your progressA combination of physical and mental wellness in one platformBarcode scanner and AI coach are so handyEasy start for beginners with no equipment requiredTrackers keep me motivatedA wide range of trackers and workouts eliminates the need for multiple appsAlgorithm sometimes struggles to find the ideal pace or level on the first tryIt would be nice to have more flexibility — for example, to change exercises manuallyThe main thing I noticed (and my smartwatch confirms) was that my energy levels increased. I don't know what exactly contributed to this — more movement, nutrition, better hydration, or the combination of all three — but I'm happy! I became more focused throughout the day and developed a habit of daily exercise.  I also like how my legs look now, would never have thought that pilates would help with this as much as exhausting workouts. Thanks to regular personalized exercises and nutrition, I don't have bloating and even see my abs.Lasta is definitely worth trying for those who are not looking for quick fixes but want to develop a long-term habit. Instead of paying for five different apps with workouts, trackers, diets, and mindfulness, it is more cost-effective to join Lasta and have all the information about your progress in one app. It's certainly my choice.]]></content:encoded></item><item><title>The Writer&apos;s Paradox: Why Tech&apos;s Most Lucrative Skill Is Being Systematically Undervalued</title><link>https://hackernoon.com/the-writers-paradox-why-techs-most-lucrative-skill-is-being-systematically-undervalued?source=rss</link><author>Igboanugo David Ugochukwu</author><category>tech</category><pubDate>Mon, 24 Nov 2025 10:28:18 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I've watched this industry long enough to recognize a pattern: the moment something becomes genuinely indispensable, we start treating it as disposable.Last month, I sat across from a Series B founder—smart guy, former Google engineer, raised $40 million—who told me he was "optimizing content operations" by replacing his three-person editorial team with ChatGPT and a junior coordinator. Six weeks later, his product launch tanked. Not because the technology failed, but because nobody could explain what the damn thing actually did. The landing page read like a ransom note written by a committee of management consultants.This keeps happening. And it's costing companies far more than they're saving.The Market Signal Nobody's ReadingHere's what the numbers actually tell us, stripped of the usual content marketing chest-thumping: organizations maintaining consistent editorial operations see traffic increases around 55% over those that don't. But here's the part that matters—content marketing generates roughly three times more leads per dollar than traditional advertising, at 62% lower cost. I've verified these figures across finance, healthcare, and enterprise SaaS companies I've covered over the past eighteen months.That's not marginal improvement. That's a fundamental arbitrage opportunity sitting in plain sight while companies scramble to cut "soft" roles.The disconnect gets worse. Right now, there are over 550 open positions globally for content strategists, UX writers, and technical communicators. I checked the listings myself—companies are offering $120K to $180K for senior roles, sometimes more. These aren't legacy media organizations desperately clinging to tradition. They're fintech startups, cybersecurity vendors, AI infrastructure companies. The ones actually building the future are hiring writers, not firing them.I've covered enough product launches to spot the pattern: technical excellence means nothing if you can't articulate value. Last fall, I analyzed why a genuinely innovative zero-trust networking platform failed to gain traction despite superior architecture. The answer wasn't in their GitHub repository—it was in their documentation, which read like it had been translated from English into Esperanto and back again.Stephen Jeske, a content strategist I've quoted before, captured this precisely: quality writing "is the line between a brand people take seriously and one they forget before the page even loads." That's not romantic nonsense about the power of storytelling. It's a measurable business outcome. Security teams evaluating vendors can smell sloppy thinking in sloppy prose. When your whitepaper can't clearly explain threat models, I'm not trusting you with my network perimeter.The data supports this. Roughly 70% of buyers prefer learning about companies through substantive articles rather than advertisements. They're not being precious about format—they're seeking signal in an ocean of noise. A well-researched technical piece demonstrates competence. A breathless press release demonstrates desperation.The World Economic Forum identified creativity, originality, and initiative among the critical skills needed by 2025—we're there now, and the prediction held. But here's what the reports miss: technical writing isn't some quaint liberal arts holdover. Modern content creators are "strategists, data analysts, and a vital force behind any successful digital marketing campaign," as one workforce analysis puts it. They're reading user behavior analytics, running A/B tests on messaging, and translating product roadmaps into narratives that actually land.I've interviewed enough product marketers to know: the good ones operate like intelligence analysts. They synthesize customer interviews, competitive research, usage metrics, and technical specifications into coherent positioning. Then they translate that positioning into everything from API documentation to conference keynotes. You cannot automate that synthesis. I don't care what LLM benchmark you're citing.The companies figuring this out are winning. The ones treating writing as a commodity function that can be "optimized" with AI slop are discovering that generic, forgettable content performs exactly as well as you'd expect: not at all.Here's the part that should terrify every CFO: studies indicate up to 70% of business mistakes stem from miscommunication. Not bad strategy, not technical failure—unclear writing. When specifications are ambiguous, engineers build the wrong thing. When documentation is confusing, support tickets multiply. When internal memos are vague, teams execute opposing directions simultaneously.I watched this play out at a mid-stage security company last year. Their product documentation was technically accurate but incomprehensible. Support costs ballooned. Sales cycles stretched because prospects couldn't evaluate the product without hand-holding. They finally hired two senior technical writers who rewrote everything with actual clarity. Support tickets dropped 40% within three months. The payback period was roughly six weeks.Clear documentation allows teams to execute instead of seeking clarification. Well-written guides reduce user frustration and support load. These aren't soft benefits—they're hard dollars. Every hour your engineering team spends explaining poorly written requirements is an hour not spent building.The companies that understand this distinction will pull ahead. The ones that don't will drown in their own mediocrity while wondering why nobody engages with their "content."I've seen enough cycles to know: whatever can be automated will be automated, but synthesis, judgment, and clarity under complexity remain stubbornly human. The best writers in tech aren't competing with AI—they're using it as a drafting tool while applying the discernment, strategic thinking, and audience awareness that actually moves needles.The market is already sorting this out. Those 550+ open positions? They're not charity. They're recognition that in a landscape oversaturated with algorithmic drivel, clear human thinking commands a premium. Companies building genuinely complex products—the infrastructure, security tools, and enterprise platforms that actually matter—can't afford to be misunderstood.So here's the blunt assessment: if you think writing is a cost center to be minimized, your competitors who recognize it as a strategic capability will eat your lunch. The evidence isn't subtle. The choice is.]]></content:encoded></item><item><title>The Four-Month Silence: How Microsoft Left Enterprise IT Burning</title><link>https://hackernoon.com/the-four-month-silence-how-microsoft-left-enterprise-it-burning?source=rss</link><author>Matthew - Technology News Australia</author><category>tech</category><pubDate>Mon, 24 Nov 2025 10:11:24 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Seventeen minutes from deployment to disaster. A system administrator deploys Microsoft's July security update across 600 virtual desktops. By the time the second cup of coffee hits the breakroom, the phones are ringing.By half past nine, the taskbars have vanished, the Start Menu won't launch, and Explorer.exe is running in name only—a ghost process presiding over a digital graveyard.This wasn't a glitch. This was KB5062553, and Microsoft stayed silent about it for 135 days.The Anatomy of Institutional FailureThe casualty list reads like an operating system's vital organs shutting down one by one: StartMenuExperienceHost, System Settings, the Taskbar, Explorer.exe. Each component either crashed on launch or failed silently, leaving users staring at blank screens and administrators scrambling for explanations that wouldn't arrive for four months.Microsoft finally acknowledged the crisis on 20 November. Not in July when IT departments first reported the failures. Not in August when Reddit threads and Microsoft Q&A forums erupted with reproductions. November. After enterprises had already bled resources trying to diagnose what Microsoft knew all along.The failure mechanism was almost elegant in its simplicity: a race condition at the heart of Windows' modern architecture.Three XAML dependency packages—Microsoft.Windows.Client.CBS, Microsoft.UI.Xaml.CBS, and Microsoft.Windows.Client.Core—needed to register before the Windows shell could load. KB5062553 broke that sequence. The shell showed up early, found the doors locked, and simply stopped functioning.First-time user logons became minefields. Non-persistent VDI environments—where every login provisions a fresh session—turned into operational nightmares. Each logon retriggered the registration timing failure. Each user got a broken desktop. Every. Single. Time.The symptoms manifested as black screens, phantom taskbars, Settings menus that clicked but never opened. One administrator described it as "Explorer.exe running but displaying sweet bugger all." That captures it precisely.The Enterprise Calculus: Choose Your DisasterBy June 2025, half of all enterprise Windows endpoints hadn't migrated to Windows 11. Just 42% of very large organisations—those managing over 10,000 devices—had completed the transition.The Americas sat at 43% completion despite 87% device readiness. Europe led at 70%. Different regions, different timelines, but the same brutal math.Windows 11 Enterprise accounts for 90% of corporate Windows 11 deployments. When KB5062553 detonated, it hit where modern business actually operates: the virtual desktop infrastructure supporting 63% of organisations that now rely solely on Desktop as a Service for remote work.The cloud-based VDI market is projected to reach $26.99 billion by 2034. Sixty-three percent of mid-sized companies are actively evaluating new VDI or DaaS solutions. Ninety-four percent plan implementation within a year. This wasn't a niche edge case. This was an infrastructure collapse at the operational core.Microsoft handed enterprises an impossible choice: deploy the security update and watch productivity crater, or hold back patches and risk audit failures.For universities managing thousands of daily student logins, financial services firms with regulatory patching requirements, and healthcare providers balancing HIPAA compliance, KB5062553 didn't offer solutions. It offered crises.Microsoft's official response, documented in support article KB5072911, prescribed PowerShell commands to manually re-register the broken XAML packages. For individual machines, administrators could run the commands, restart the SiHost process, and move on.For VDI environments experiencing failures at every login? Microsoft recommended synchronous logon scripts forcing Explorer.exe to wait whilst dependencies sorted themselves. The scripts add measurable delays to login times and operational complexity to what should be straightforward deployments.These aren't fixes. They're economic impositions: increased logon times, scripting and testing efforts, helpdesk ticket surges, forced rollback plans.The costs cascade through organisations managing thousands of endpoints, each one a reminder that Microsoft shipped code incapable of reliably registering its own dependencies.Microsoft issued no public ETA for a permanent fix. No device-level impact counts. No transparent telemetry on scale. The institutional knowledge came from community forums, not official channels. Reddit threads documented the exact workarounds Microsoft would eventually publish months later.This wasn't transparency delayed. This was negligence dressed as "working on a resolution."The timing compounds the failure. Windows 10 reaches end-of-support in October 2025. Enterprises are racing against that deadline, navigating hardware requirements forcing device refreshes, wrestling with legacy application compatibility.They cross the finish line into Windows 11 24H2 only to discover their freshly deployed environments can't display a functioning Start Menu.Windows 11 25H2 shares the same codebase as 24H2. The problem cascades forward, not backwards.KB5062553 wasn't isolated. Nvidia pointed to Microsoft's latest Patch Tuesday update today, citing game performance issues serious enough to warrant an emergency hotfix driver. This comes days after Microsoft's Windows chief faced backlash over plans for a more agent-driven operating system.Modern OS modularisation—breaking Windows into updateable AppX/XAML packages for faster servicing—sounds transformative until the modular components forget to synchronise.Then you're left with enterprises running jerry-rigged PowerShell scripts because foundational architecture can't coordinate its own startup sequence.Somewhere right now, an administrator is staring at a blank taskbar, clutching cold coffee, recalculating career choices. The lesson isn't about bugs—software has bugs. The lesson is about what happens when the gap between breaking infrastructure and acknowledging the break spans four months and millions of affected endpoints.Microsoft eventually came clean. They published detailed workarounds. They acknowledged the XAML registration timing problem. But the four-month silence whilst enterprises burned wasn't a communication delay. It was a choice.The perfect storm: aggressive Windows 11 migration timelines colliding with VDI market growth, undercooked servicing validation, and institutional silence whilst the community documented the damage Microsoft refused to name.KB5062553 wasn't a patch failure. It was a trust failure, delivered at enterprise scale, with receipts timestamped in months.]]></content:encoded></item><item><title>How An MIT Student Awed Top Economists With His AI Study - Until It All Fell Apart</title><link>https://news.slashdot.org/story/25/11/24/0429245/how-an-mit-student-awed-top-economists-with-his-ai-study---until-it-all-fell-apart?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Nov 2025 09:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In May MIT announced "no confidence" in a preprint paper on how AI increased scientific discovery, asking arXiv to withdraw it. The paper, authored by 27-year-old grad student Aidan Toner-Rodgers, had claimed an AI-driven materials discovery tool helped 1,018 scientists at a U.S. R&D lab. 

But within weeks his academic mentors "were asking an unthinkable question," reports the Wall Street Journal. Had Toner-Rodgers made it all up?
Toner-Rodgers's illusory success seems in part thanks to the dynamics he has now upset: an academic culture at MIT where high levels of trust, integrity and rigor are all — for better or worse — assumed. He focused on AI, a field where peer-reviewed research is still in its infancy and the hunger for data is insatiable. What has stunned his former colleagues and mentors is the sheer breadth of his apparent deception. He didn't just tweak a few variables. It appears he invented the entire study. In the aftermath, MIT economics professors have been discussing ways to raise standards for graduate students' research papers, including scrutinizing raw data, and students are going out of their way to show their work isn't counterfeit, according to people at the school. 



Since parting with the university, Toner-Rodgers has told other students that his paper's problems were essentially a mere issue with data rights. According to him, he had indeed burrowed into a trove of data from a large materials-science company, as his paper said he did. But instead of getting formal permission to use the data, he faked a data-use agreement after the company wanted to pull out, he told other students via a WhatsApp message in May... On Jan. 31, Corning filed a complaint with the World Intellectual Property Organization against the registrar of the domain name corningresearch.com. Someone who controlled that domain name could potentially create email addresses or webpages that gave the impression they were affiliated with the company. WIPO soon found that Toner-Rodgers had apparently registered the domain name, according to the organization's written decision on the case. Toner-Rodgers never responded to the complaint, and Corning successfully won the transfer of the domain name. WIPO declined to comment... 

In the WhatsApp chat in May, in which Toner-Rodgers told other students he had faked the data-use agreement, he wrote, "This was a huge and embarrassing act of dishonesty on my part, and in hindsight it clearly would've been better to just abandon the paper." Both Corning and 3M told the Journal that they didn't roll out the experiment Toner-Rodgers described, and that they didn't share data with him.]]></content:encoded></item><item><title>Phoronix Premium Cyber Week &quot;Black Friday&quot; Deal To Help Enable Linux Hardware Reviews</title><link>https://www.phoronix.com/news/Phoronix-Cyber-Week-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 08:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The end of 2025 is quickly approaching and while there are the various end of year holidays, you can still expect to find new and original content on Phoronix each and every single day of the year just as it's been for more than a decade of the now 21-year-old Phoronix.com. The last day without any new content on Phoronix was all the way back in May of 2012. That's due to my passion for Linux hardware and open-source, paired in more recent years with the more grueling environment to make ends meet with the ever increasing state of the web advertising industry, rampant ad-block use, and related challenges for web publishers. If you would like to show your support for Phoronix's Linux hardware content over the past two decades, this week is the "Cyber Week" / "Black Friday" sale to go ad-free, multi-page-articles on a single page, and other benefits at a reduced rate...]]></content:encoded></item><item><title>The TechBeat: The Fork Reshaping MCP Testing: How a 24-Year-Old CTO Is Taking On One of AI’s Biggest Players (11/24/2025)</title><link>https://hackernoon.com/11-24-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Mon, 24 Nov 2025 07:10:55 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @hacker53037367 [ 18 Min read ] 
 A reflection on why true automation starts with human thinking, not technology. Systems only work as clearly as the minds that design them.
 Read More.By @hacker39947670 [ 14 Min read ] 
 Hands-on ERC-4337 guide: deploy EntryPoint, build a minimal smart account, fund deposits, and send your first UserOperation with Foundry and TypeScript Read More.By @oxylabs [ 11 Min read ] 
 Discover the 12 best web scraping APIs of 2025, comparing performance, pricing, features, & success rates to help teams scale reliable data extraction. Read More.By @aioznetwork [ 5 Min read ] 
 AIOZ AI is the intelligence layer of the AIOZ Network, connecting a global community through a peer-to-peer compute economy. Learn more here! Read More.By @scylladb [ 5 Min read ] 
 Discover how DynamoDB’s pricing quirks—rounding, replication, caching, and global tables—can skyrocket costs, and how ScyllaDB offers predictable pricing. Read More.By @kashvipandey [ 2 Min read ] 
 Humanity Protocol partners with Mastercard to integrate open finance into Human ID, enabling secure, privacy-first access to loans, credit, and Web3 finance. Read More.By @stevebeyatte [ 4 Min read ] 
 A 24-year-old developer built MCPJam, an open-source rival that outpaced Anthropic’s Inspector—and may redefine how AI agents are tested. Read More.By @stevebeyatte [ 8 Min read ] 
 T3RA Logistics is redefining freight with AI agents—running a $100M operation with just 25 “superhumans.” Read More.By @hacker38388747 [ 2 Min read ] 
 VSYS Host launches VSYS Name, an ICANN-accredited registrar offering direct domain control, transparent pricing, crypto payments, and 24/7 expert support. Read More.By @benoitmalige [ 7 Min read ] 
 The paycheck era is ending. Learn why wages are collapsing, leverage is rising, and how to build a life where you're paid for impact, not time. Read More.By @ishanpandey [ 5 Min read ] 
 QANplatform's QAN XLINK passes Hacken security audit, offering quantum-resistant protection for 25% of Bitcoin supply vulnerable to future attacks. Read More.By @amanila [ 70 Min read ] 
 A beginner's introduction to DevOps foundations. Connect the dots between Git, CI/CD, Docker, and Kubernetes to understand the modern development rocess. Read More.By @knightbat2040 [ 6 Min read ] 
 Built an AI tool that scrapes, cleans, and summarizes Texas bills to make government legislation readable and transparent for everyone.  Read More.By @jacoblandry [ 4 Min read ] 
 We know there's a lot of unethical ways to use AI but at what point are we not even going to know AI was used? Read More.By @yuliabusygina [ 10 Min read ] 
 Researchers at Saint Petersburg State Pediatric Medical University developed an AI solution for assessing infant brain development from MRI scans. The solution  Read More.By @zbruceli [ 6 Min read ] 
 Will Google's Gemini File Search kill homebrew RAG solutions? We test drive to compare function, performance and costs. Plus sample code for PDF Q&A app. Read More.]]></content:encoded></item><item><title>&apos;We Could&apos;ve Asked ChatGPT&apos;: UK Students Fight Back Over Course Taught By AI</title><link>https://news.slashdot.org/story/25/11/24/0523258/we-couldve-asked-chatgpt-uk-students-fight-back-over-course-taught-by-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Nov 2025 05:35:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from the Guardian:

James and Owen were among 41 students who took a coding module at the University of Staffordshire last year, hoping to change careers through a government-funded apprenticeship programme designed to help them become cybersecurity experts or software engineers. But after a term of AI-generated slides being read, at times, by an AI voiceover, James said he had lost faith in the programme and the people running it, worrying he had "used up two years" of his life on a course that had been done "in the cheapest way possible". 

"If we handed in stuff that was AI-generated, we would be kicked out of the uni, but we're being taught by an AI," said James during a confrontation with his lecturer recorded as a part of the course in October 2024. James and other students confronted university officials multiple times about the AI materials. But the university appears to still be using AI-generated materials to teach the course. This year, the university uploaded a policy statement to the course website appearing to justify the use of AI, laying out "a framework for academic professionals leveraging AI automation" in scholarly work and teaching... 

For students, AI teaching appears to be less transformative than it is demoralising. In the US, students post negative online reviews about professors who use AI. In the UK, undergraduates have taken to Reddit to complain about their lecturers copying and pasting feedback from ChatGPT or using AI-generated images in courses. 

"I feel like a bit of my life was stolen," James told the Guardian (which also quotes an unidentified student saying they felt "robbed of knowledge and enjoyment".) But the article also points out that a survey last year of 3,287 higher-education teaching staff by edtech firm Jisc found that nearly a quarter were using AI tools in their teaching.]]></content:encoded></item><item><title>Memtest86+ 8.0 Released With Support For Latest Intel &amp; AMD CPUs</title><link>https://www.phoronix.com/news/Memtest86-Plus-8.0</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 24 Nov 2025 05:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Since the 2022 release of memtest86+ 6.0 as a rewrite of this long-used RAM testing utility, this open-source software has continued advancing nicely after a decade hiatus. Released on Sunday night was memtest86+ 8.0 as the latest iteration of this popular RAM tester for enthusiasts...]]></content:encoded></item><item><title>Napster Said It Raised $3 Billion From a Mystery Investor. But Now the &apos;Investor&apos; and &apos;Money&apos; Are Gone</title><link>https://entertainment.slashdot.org/story/25/11/24/022212/napster-said-it-raised-3-billion-from-a-mystery-investor-but-now-the-investor-and-money-are-gone?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Nov 2025 02:35:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from Forbes:


On November 20, at approximately 4 p.m. Eastern time, Napster held an online meeting for its shareholders; an estimated 700 of roughly 1,500 including employees, former employees and individual investors tuned in. That's when its CEO John Acunto told everyone he believed that the never-identified big investor — who the company had insisted put in $3.36 billion at a $12 billion valuation in January, which would have made it one of the year's biggest fundraises — was not going to come through. 

In an email sent out shortly after, it told existing investors that some would get a bigger percentage of the company, due to the canceled shares, and went on to describe itself as a "victim of misconduct," adding that it was "assisting law enforcement with their ongoing investigations." As for the promised tender offer, which would have allowed shareholders to cash out, that too was called off. "Since that investor was also behind the potential tender, we also no longer believe that will occur," the company wrote in the email. 

At this point it seems unlikely that getting bigger stakes in the business will make any of the investors too happy. The company had been stringing its employees and investors along for nearly a year with ever-changing promises of an impending cash infusion and chances to sell their shares in a tender offer that would change everything. In fact, it was the fourth time since 2022 they've been told they could soon cash out via a tender offer, and the fourth time the potential deal fell through. Napster spokesperson Gillian Sheldon said certain statements about the fundraise "were made in good faith based on what we understood at the time. We have since uncovered indications of misconduct that suggest the information provided to us then was not accurate." 

The article notes America's Department of Justice has launched an investigation (in which Napster is not a target), while the Securities and Exchange Commission has a separate ongoing investigation from 2022 into Napster's scrapped reverse merger. 

While Napster announced they'd been acquired for $207 million by a tech company named Infinite Reality, Forbes says that company faced "a string of lawsuits from creditors alleging unpaid bills, a federal lawsuit to enforce compliance with an SEC subpoena (now dismissed) and exaggerated claims about the extent of their partnerships with Manchester City Football Club and Google. The company also touted 'top-tier' investors who never directly invested in the firm, and its anonymous $3 billion investment that its spokesperson told Forbes in March was in "an Infinite Reality account and is available to us" and that they were 'actively leveraging' it..." 

And by the end, "Napster appears to have been scrambling to raise cash to keep the lights on, working with brokers and investment advisors including a few who had previously gotten into trouble with regulators.... If it turns out that Napster knew the fundraise wasn't happening and it benefited from misrepresenting itself to investors or acquirees, it could face much bigger problems. That's because doing so could be considered securities fraud."]]></content:encoded></item><item><title>New Research Finds America&apos;s Top Social Media Sites: YouTube (84%) Facebook (71%), Instagram (50%)</title><link>https://tech.slashdot.org/story/25/11/24/0016243/new-research-finds-americas-top-social-media-sites-youtube-84-facebook-71-instagram-50?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 24 Nov 2025 00:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Pew Research surveyed 5,022 Americans this year (between February 5 and June 18), asking them
"do you ever use" YouTube, Facebook, and nine of the other top social media platforms. The results?





 YouTube 84% 

 Facebook 71% 

 Instagram 50% 

 TikTok 37% 

 WhatsApp 32% 

 Reddit 26% 

 Snapchat 25% 

 X.com (formerly Twitter) 21% 

 Threads 8% 

 Bluesky 4% 

 Truth Social 3% 

 

An announcement from Pew Research adds some trends and demographics:




The Center has long tracked use of many of these platforms. Over the past few years, four of them have grown in overall use among U.S. adults — TikTok, Instagram, WhatsApp and Reddit. 37% of U.S. adults report using TikTok, which is slightly up from last year and up from 21% in 2021. Half of U.S. adults now report using Instagram, which is on par with last year but up from 40% in 2021. About a third say they use WhatsApp, up from 23% in 2021. And 26% today report using Reddit, compared with 18% four years ago. 

While YouTube and Facebook continue to sit at the top, the shares of Americans who report using them have remained relatively stable in recent years... YouTube and Facebook are the only sites asked about that a majority in all age groups use, though for YouTube, the youngest adults are still the most likely to do so. This differs from Facebook, where 30- to 49-year-olds most commonly say they use it (80%). 

Other interesting statistics:

"More than half of women report using Instagram (55%), compared with under half of men (44%). Alternatively, men are more likely to report using platforms such as X and Reddit."




"Democrats and Democratic-leaning independents are more likely to report using WhatsApp, Reddit, TikTok, Bluesky and Threads."]]></content:encoded></item><item><title>Was the Moon-Forming Protoplanet &apos;Theia&apos; a Neighbor of Earth?</title><link>https://science.slashdot.org/story/25/11/23/2327252/was-the-moon-forming-protoplanet-theia-a-neighbor-of-earth?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 23:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Theia crashed into earth and formed the moon, the theory goes. But then where did Theia come from? The lead author on a new study says "The most convincing scenario is that most of the building blocks of Earth and Theia originated in the inner Solar System. Earth and Theia are likely to have been neighbors." 

Though Theia was completely destroyed in the collision, scientists from the Max Planck Institute for Solar System Research led a team that was able to measure the ratio of tell-tale isotopes in Earth and Moon rocks, Euronews explains:



The research team used rocks collected on Earth and samples brought back from the lunar surface by Apollo astronauts to examine their isotopes. These isotopes act like chemical fingerprints. Scientists already knew that Earth and Moon rocks are almost identical in their metal isotope ratios. That similarity, however, has made it hard to learn much about Theia, because it has been difficult to separate material from early Earth and material from the impactor. 

The new research attempts a kind of planetary reverse engineering. By examining isotopes of iron, chromium, zirconium and molybdenum, the team modelled hundreds of possible scenarios for the early Earth and Theia, testing which combinations could produce the isotope signatures seen today. Because materials closer to the Sun formed under different temperatures and conditions than those further out, those isotopes exist in slightly different patterns in different regions of the Solar System. 

By comparing these patterns, researchers concluded that Theia most likely originated in the inner Solar System, even closer to the Sun than the early Earth.

 

The team published their findings in the journal Science. Its title? "The Moon-forming impactor Theia originated from the inner Solar System."]]></content:encoded></item><item><title>Linux 6.18-rc7 Released With Late Hardware Improvements</title><link>https://www.phoronix.com/news/Linux-6.18-rc7-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Nov 2025 23:03:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Linux 6.18-rc7 just arrived in the Git tree as the newest weekly test build leading up to Linux 6.18 stable hopefully debuting next Sunday, 30 November...]]></content:encoded></item><item><title>Wayland Protocols 1.46 Released With New Experimental Additions</title><link>https://www.phoronix.com/news/Wayland-Protocols-1.46</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Nov 2025 22:55:35 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Wayland Protocols 1.46 released this evening with new experimental protocols for text improvements as well as refinements to the color management protocol for HDR...]]></content:encoded></item><item><title>Cryptologist DJB Criticizes Push to Finalize Non-Hybrid Security for Post-Quantum Cryptography</title><link>https://it.slashdot.org/story/25/11/23/226258/cryptologist-djb-criticizes-push-to-finalize-non-hybrid-security-for-post-quantum-cryptography?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 22:09:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In October cryptologist/CS professor Daniel J. Bernstein alleged that America's National Security
Agency (and its UK counterpart GCHQ) were attempting to influence NIST to adopt weaker post-quantum cryptography
standards without a "hybrid" approach that would've also included pre-quantum ECC. 

Bernstein is of the opinion that "Given how
many post-quantum proposals have been broken and the continuing flood of side-channel attacks, any competent engineering evaluation will conclude that
the best way to deploy post-quantum [PQ] encryption for TLS, and for the Internet more broadly, is as double encryption: post-quantum cryptography on top of ECC." But
he says he's seen it playing out differently:


By 2013, NSA had a quarter-billion-dollar-a-year
budget to "covertly influence and/or overtly leverage"
systems to "make the systems in question exploitable"; in
particular, to "influence policies, standards and specification
for commercial public key technologies". NSA is quietly
using stronger cryptography for the data it cares about, but
meanwhile is spending money to promote a market for weakened
cryptography, the same way that it successfully created decades of
security failures by building up the market for, e.g., 40-bit
RC4 and 512-bit
RSA and Dual EC.
 I looked concretely at what was happening in IETF's
TLS working group, compared to the consensus
requirements for standards-development organizations. I reviewed
how a call for "adoption" of an NSA-driven specification produced a variety of objections that weren't
handled properly. ("Adoption" is a preliminary step before IETF standardization....) On 5 November 2025, the chairs issued "last call" for objections to publication of the document. The deadline for input is "2025-11-26", this coming Wednesday. 


Bernstein also shares concerns about how the Internet Engineering Task Force is handling the discussion, and argues that the document is even "out of scope" for the
IETF TLS working group

This document doesn't serve any of the official goals in the TLS working group charter. Most importantly, this document is directly contrary to the "improve security" goal, so it would violate the charter even if it contributed to another goal... Half of the PQ proposals submitted to NIST in 2017 have been broken already... often with attacks having sufficiently low cost to demonstrate on
readily available computer equipment. Further PQ software has been broken by implementation issues such as side-channel attacks. 

He's also concerned about how that discussion is being handled:

On 17 October 2025, they posted a "Notice of Moderation for Postings by D. J. Bernstein" saying that they would "moderate the postings of D. J. Bernstein for 30 days due to disruptive behavior effective immediately" and specifically that my postings "will be held for moderation and after confirmation by the TLS Chairs of being on topic and not disruptive, will be released to the list"... 


I didn't send anything to the IETF TLS mailing list for 30 days after that. Yesterday [November 22nd] I finished writing up my new objection and sent that in. And, gee, after more than 24 hours it still hasn't appeared... Presumably the chairs "forgot" to flip the censorship button off after 30 days. 



Thanks to alanw (Slashdot reader #1,822) for spotting the blog posts.
]]></content:encoded></item><item><title>Roblox CEO interview gets heated over child safety</title><link>https://techcrunch.com/2025/11/23/roblox-ceo-interview-gets-heated-over-child-safety/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 23 Nov 2025 21:58:04 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The interview began with Baszucki describing the feature, which will require all users to submit a face scan if they want to access Roblox’s messaging features. But by the time he was asked about a report claiming the company had prioritized growth over safety, Baszucki seemed to tire of the topic.]]></content:encoded></item><item><title>X’s new About this account feature is going great</title><link>https://techcrunch.com/2025/11/23/xs-new-about-this-account-feature-is-going-great/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 23 Nov 2025 21:11:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[While X’s new “About this account” feature includes information about when a user joined and how they downloaded the app, geographic location is getting the most attention by far.]]></content:encoded></item><item><title>Google Revisits JPEG XL in Chromium After Earlier Removal</title><link>https://tech.slashdot.org/story/25/11/23/2026246/google-revisits-jpeg-xl-in-chromium-after-earlier-removal?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 21:09:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Three years ago, Google removed JPEG XL support from Chrome, stating there wasn't enough interest at the time," writes the blog Windows Report. "That position has now changed."

In a recent note to developers, a Chrome team representative confirmed that work has restarted to bring JPEG XL to Chromium and said Google "would ship it in Chrome" once long-term maintenance and the usual launch requirements are met. 

The team explained that other platforms moved ahead. Safari supports JPEG XL, and Windows 11 users can add native support through an image extension from Microsoft Store. The format is also confirmed for use in PDF documents. There has been continuous demand from developers and users who ask for its return. 

Before Google ships the feature in Chrome, the company wants the integration to be secure and supported over time. A developer has submitted new code that reintroduces JPEG XL to Chromium. This version is marked as feature complete. The developer said it also "includes animation support," which earlier implementations did not offer.]]></content:encoded></item><item><title>Mozilla Announces &apos;TABS API&apos; For Developers Building AI Agents</title><link>https://tech.slashdot.org/story/25/11/23/206245/mozilla-announces-tabs-api-for-developers-building-ai-agents?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 20:09:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Fresh from announcing it is building an AI browsing mode in Firefox and laying the groundwork for agentic interactions in the Firefox 145 release, the corp arm of Mozilla is now flexing its AI muscles in the direction of those more likely to care," writes the blog OMG Ubuntu:



If you're a developer building AI agents, you can sign up to get early access to Mozilla's TABS API, a "powerful web content extraction and transformation toolkit designed specifically for AI agent builders"... The TABS API enables devs to create agents to automate web interactions, like clicking, scrolling, searching, and submitting forms "just like a human". Real-time feedback and adaptive behaviours will, Mozilla say, offer "full control of the web, without the complexity." 




As TABS is not powered by a Mozilla-backed LLM you'll need to connect it to your choice of third-party LLM for any relevant processing... Developers get 1,000 requests monthly on the free tier, which seems reasonable for prototyping personal projects. Complex agentic workloads may require more. Though pricing is yet to be locked in, the TABS API website suggests it'll cost ~$5 per 1000 requests.
Paid plans will offer additional features too, like lower latency and, somewhat ironically, CAPTCHA solving so AI can 'prove' it's not a robot on pages gated to prevent automated activities. 



Google, OpenAI, and other major AI vendors offer their own agentic APIs. Mozilla is pitching up late, but it plans to play differently. It touts a "strong focus on data minimisation and security", with scraped data treated ephemerally — i.e., not kept. As a distinction, that matters. AI agents can be given complex online tasks that involve all sorts of personal or sensitive data being fetched and worked with.... If you're minded to make one, perhaps without a motivation to asset-strip the common good, Mozilla's TABS API look like a solid place to start.
]]></content:encoded></item><item><title>Funniest/Most Insightful Comments Of The Week At Techdirt</title><link>https://www.techdirt.com/2025/11/23/funniest-most-insightful-comments-of-the-week-at-techdirt-186/</link><author>Leigh Beadon</author><category>tech</category><pubDate>Sun, 23 Nov 2025 20:00:00 +0000</pubDate><source url="https://www.techdirt.com/">Techdirt</source><content:encoded><![CDATA[What everyone appears to overlookTrump’s tantrum here clearly shows that he is perfectly aware that orders he puts out and plans to put out in future are illegal.Otherwise why would he bother?Even if SCOTUS doesn’t reverse it they’ve still got time to pass another one that could survive a court challenge before the next election, but it’s going to be really funny if Texas’s gerrymander never goes into effect and California’s retaliatory gerrymander does. Wile E Coyote-ass motherfuckers.‘Make protesting your regime illegal with this one simple trick!’It makes perfect sense really, you just need to think like a member of a brutal dictatorship.Step 1: Frame anyone that protests against you as part of Antifa since clearly no-one else would ever do that.Step 2: Declare that Antifa is a terrorist organization right up there with actual violent criminal organizations.Step 3: Congrats, anyone that protests against you can now be accused of and legally treated as violent terrorists, and if you’re already laying the groundwork of (accused) criminals not only having no rights but being able to be executed on the spot without the hassle of a trial then all the better!Ordering posts from people whom you follow in a reverse chronological order, where their most recent posts sit at the top of your feed, is itself a programmatic decision even if it’s the default setting. The same goes for positioning/ordering reposts from people whom you follow, even if the option to see those reposts is turned on by default. That you don’t see those decisions as decisions doesn’t change the reality that they are, in fact, decisions made by the service as to how they’ll decide to show you content. Adding an extra algorithm is just an extra decision; it shouldn’t make a service more liable for user-generated content any more than it would be without that extra decision.I understand the impulse to want to punish Elon Musk for what he’s done to Twitter. A shitload of other people probably feel the same way. But in trying to take down the tree that is Twitter’s promotion (accidental or otherwise) of Nazi and Nazi-adjacent content with a flamethrower, you’d also risk burning down the all the other trees in the forest⁠—and by that, I mean you’d risk destroying a large swath of the Internet just so you can stick it to Elon. Is that a risk you want to take?You can tell it’s not really sedition because Trump didn’t pardon them.The term you are looking for is “exoskeleton”.Teacher: OK, everyone, hand in your book reportsClever Student: The Copyright Clause ate my homeworkThat’s all for this week, folks!]]></content:encoded></item><item><title>AI&apos;s Trillion-Dollar Infrastructure Bet: What Leaders Need to Know</title><link>https://hackernoon.com/ais-trillion-dollar-infrastructure-bet-what-leaders-need-to-know?source=rss</link><author>Anthony Laneau</author><category>tech</category><pubDate>Sun, 23 Nov 2025 19:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Three years into the generative AI era, tech giants have committed nearly half a trillion dollars to infrastructure; yet the path to value capture remains unclear. Here's what matters for your business.The Scale of the InvestmentBig Tech is making an unprecedented bet on AI. In 2025 alone, Microsoft, Google, Meta, and Amazon will spend approximately $400 billion on AI infrastructure; more than the entire global telecommunications industry invests annually. To put this in perspective, they've doubled their planned spending during 2025, not before it started.\
This isn't just hype. US data center construction is now overtaking office building construction, with power availability becoming the primary constraint. Some industry observers estimate the total investment could reach $3-5 trillion by 2030.\
 The infrastructure is being built, whether or not clear use cases exist yet. This creates both opportunity and noise; your job is distinguishing between the two.The Product Reality CheckDespite the infrastructure surge, the product landscape tells a more nuanced story:Models Are Converging, Not DifferentiatingNew AI models launch weekly with incrementally better benchmark scores, but the top performers are increasingly close in capability. More importantly, Chinese models and open-source alternatives are rapidly catching up to Western commercial leaders. Within 12-18 months, we may see dozens of models performing at similar levels.\
 Don't build your strategy around one model provider having a sustained technical advantage. Focus instead on distribution, data, and product experience.Distribution Still Trumps TechnologyChatGPT has 800 million weekly users, but only about 5% pay for premium access. Meanwhile, survey data shows only 10-18% of US consumers use AI chatbots daily, while 45% use them at least weekly. The gap between experimentation and daily habit remains substantial.\
Google Gemini and Meta AI benefit from being embedded in existing platforms with billions of users, but haven't yet translated that distribution into a dominant market share for standalone AI interactions.\
 User adoption is real but still early. Most people are trying AI occasionally, not replacing their existing workflows. Your AI strategy should assume gradual behavior change, not instant revolution.Where Real Value Is EmergingAfter three years of experimentation, a few clear patterns have emerged:1. AI Coding: The New Abstraction LayerAI-powered coding tools represent a fundamental shift comparable to cloud computing or open-source libraries. Y Combinator reports that some startups now generate 95% of their code with AI assistance. The YC batch that launched in early 2025 showed 10% weekly growth, the fastest in the program's history.\
 Software development costs are dropping dramatically. Projects that once required large teams can now be built by small ones. Conversely, this means more competition and faster iterations from rivals.2. Content Creation at ScaleMajor consumer brands report using AI to generate 10-20x more marketing assets than before; not just faster creation of the same volume, but fundamentally more variants for testing and personalization. This includes making video production economically viable for brands that previously couldn't afford it.\
 The cost structure of marketing is changing. Testing 300 ad variants instead of 15 becomes feasible. Personalization that once required manual work can now scale. Your creative bottleneck is shifting from production to strategy.3. Customer Service AutomationAI customer service tools are seeing rapid adoption, but success requires careful deployment. The technology works well for routine inquiries but still struggles with complex situations requiring judgment.\
 Customer service automation can significantly reduce costs for high-volume, routine interactions. However, poor implementation damages brand perception quickly. Start with narrow use cases where errors have low consequences.The Critical Question: Bundle or Unbundle?This is where strategic thinking matters most. The internet unbundled physical retail—you didn't need stores in every city to reach national customers. Now, AI is potentially unbundling the internet itself.For 20 years, online recommendations have worked through correlation: "People who bought X also bought Y." This requires massive user bases generating behavioral data—creating powerful network effects and moats.\
AI potentially changes this by understanding what products  rather than just correlating purchase patterns. If someone buys packing tape, today's systems suggest boxes and bubble wrap based on past correlations. Tomorrow's AI might reason: "They're packing, so they might be moving. Perhaps they need smoke alarms for their new home, or need to update their insurance."\
 The fundamental question is whether AI can create effective recommendations without requiring millions of users. If it can, distribution advantages erode. If it can't, existing platforms become even more powerful.Where Does the Value Capture?Three years in, we still don't have clarity on whether value accrues to: (OpenAI, Anthropic, Google) competing on technology (cloud providers) competing on scale and capital competing on distribution and product competing on vertical expertise\
Most likely, different layers capture value in different ways—but the balance remains uncertain.What You Should Actually Do Focus on clear use cases where AI reduces costs or increases output without requiring workflow changes:Marketing asset generation (more variants, faster)Customer service for routine inquiriesCode generation for internal toolsData analysis and reporting\
2. Experiment with Distribution. Test how AI affects your customer acquisition and recommendation engines:Can AI improve product discovery without behavioral data?What happens when customers use AI assistants to shop instead of your website?How do AI-generated ads perform compared to traditional creative?\
3. Prepare for Commodification. Assume models will become commodities within 18 months:Don't build moats around model selectionFocus on data, distribution, and customer relationshipsPrepare for competitors who can build similar products faster and cheaper chase every new model release or capability announcement. The pace of change at the model level will remain rapid, but most improvements are incremental.\
 assume current usage patterns predict final outcomes. We're still in the experimentation phase. Only 10% of Americans use AI daily, and deployment in the enterprise is even earlier.\
 panic about existential threats or revolutionary changes in the next 6-12 months. Major shifts take 5-10 years. Cloud computing launched in 2006; by 2025, it still represents only 30% of enterprise workloads.History provides perspective: When grocery barcodes and databases launched in 1974, they enabled retailers to manage 5x more SKUs. But this happened over 20+ years, not overnight.\
E-commerce has been "disrupting" retail for 25 years and still represents only 30% of US retail sales (excluding gas and groceries). The future happens, but it takes time.\
Your competitive advantage comes from:Understanding which changes happen fast (ad creation, code development) versus slow (consumer behavior, enterprise deployment)Investing in automating obvious tasks while experimenting with transformational onesBuilding on distribution and data, not model selectionMoving faster than traditional competitors while avoiding the hype cycle\
The infrastructure is being built. The technology is advancing rapidly. But the winning business models, customer behaviors, and value capture mechanisms remain unsettled. Your goal isn't to predict the final state; it's to position for multiple scenarios while capturing value from what works today.\
The companies that win will be those that automate aggressively, experiment intelligently, and remain skeptical of both the hype and the anti-hype. That's the real game.]]></content:encoded></item><item><title>One Company&apos;s Plan to Sink Nuclear Reactors Deep Underground</title><link>https://hardware.slashdot.org/story/25/11/23/1850236/one-companys-plan-to-sink-nuclear-reactors-deep-underground?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 18:52:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Long-time Slashdot reader jenningsthecat shared this article from IEEE Spectrum:

By dropping a nuclear reactor 1.6 kilometers (1 mile) underground, Deep Fission aims to use the weight of a billion tons of rock and water as a natural containment system comparable to concrete domes and cooling towers. With the fission reaction occurring far below the surface, steam can safely circulate in a closed loop to generate power. 


The California-based startup announced in October that prospective customers had signed non-binding letters of intent for 12.5 gigawatts of power involving data center developers, industrial parks, and other (mostly undisclosed) strategic partners, with initial sites under consideration in Kansas, Texas, and Utah... The company says its modular approach allows multiple 15-megawatt reactors to be clustered on a single site: A block of 10 would total 150 MW, and Deep Fission claims that larger groupings could scale to 1.5 GW. Deep Fission claims that using geological depth as containment could make nuclear energy cheaper, safer, and deployable in months at a fraction of a conventional plant's footprint... 



The company aims to finalize its reactor design and confirm the pilot site in the coming months. [Company founder Liz] Muller says the plan is to drill the borehole, lower the canister, load the fuel, and bring the reactor to criticality underground in 2026. Sites in Utah, Texas, and Kansas are among the leading candidates for the first commercial-scale projects, which could begin construction in 2027 or 2028, depending on the speed of DOE and NRC approvals. Deep Fission expects to start manufacturing components for the first unit in 2026 and does not anticipate major bottlenecks aside from typical long-lead items.

 

In short "The same oil and gas drilling techniques that reliably reach kilometer-deep wells can be adapted to host nuclear reactors..." the article points out. Their design would also streamline construction, since "Locating the reactors under a deep water column subjects them to roughly 160 atmospheres of pressure — the same conditions maintained inside a conventional nuclear reactor — which forms a natural seal to keep any radioactive coolant or steam contained at depth, preventing leaks from reaching the surface." 

Other interesting points from the article:


They plan on operating and controlling the reactor remotely from the surface.

Company founder Muller says if an earthquake ever disrupted the site, "you seal it off at the bottom of the borehole, plug up the borehole, and you have your waste in safe disposal."

For waste management, the company "is eyeing deep geological disposal in the very borehole systems they deploy for their reactors."


"The company claims it can cut overall costs by 70 to 80 percent compared with full-scale nuclear plants."



"Among its competition are projects like TerraPower's Natrium, notes
the tech news site Hackaday, saying TerraPower's fast neutron reactors "are already under construction and offer much more power per reactor, along with Natrium in particular also providing built-in grid-level storage. 

 "One thing is definitely for certain..." they add. "The commercial power sector in the US has stopped being mind-numbingly boring."]]></content:encoded></item><item><title>Important Caching Strategies: How to Create Resilient Caching in Symfony</title><link>https://hackernoon.com/important-caching-strategies-how-to-create-resilient-caching-in-symfony?source=rss</link><author>MattLeads</author><category>tech</category><pubDate>Sun, 23 Nov 2025 18:00:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Symfony Cache component is often the most under-utilized tool in a developer’s arsenal. Most implementations stop at “install Redis” and wrap a few database calls in a  closure. While functional, this barely scratches the surface of what the component can do in high-throughput, distributed environments.\
In Symfony 7.3, the Cache component is not just a key-value store; it is a sophisticated system capable of tiered architecture, probabilistic stampede protection, and transparent encryption.\
This article explores important caching strategies that solve expensive architectural problems: ,  (thundering herds),  (), and .The Architecture of Latency: Tiered (Chain) CachingIn microservice architectures or high-traffic monoliths, a network call to Redis (typically 1–3ms) can eventually become a bottleneck compared to local memory (nanoseconds). However, local memory (APCu) is volatile and doesn’t share state across pods/servers.\
The solution is a , effectively acting as an  cache for your application. , .We will configure a pool that reads from APCu first. If it misses, it reads from Redis, then populates APCu.composer require symfony/cache symfony/orm-pack redis
\
Configuration (config/packages/cache.yaml):framework:
    cache:
        # Prefix all keys to avoid collisions in shared Redis instances
        prefix_seed: '%env(APP_SECRET)%'

        pools:
            # L2 Cache: Redis (Shared)
            cache.redis:
                adapter: cache.adapter.redis
                provider: 'redis://%env(REDIS_HOST)%:6379'
                default_lifetime: 3600 # 1 hour

            # L1 Cache: APCu (Local Memory)
            cache.apcu:
                adapter: cache.adapter.apcu
                default_lifetime: 60 # Short TTL to prevent stale local data

            # The Chain: L1 + L2
            cache.layered:
                adapter: cache.adapter.chain
                provider: [cache.apcu, cache.redis]
Inject the specific pool using the Target attribute.namespace App\Service;

use Symfony\Component\DependencyInjection\Attribute\Target;
use Symfony\Contracts\Cache\CacheInterface;
use Symfony\Contracts\Cache\ItemInterface;

class DashboardService
{
    public function __construct(
        #[Target('cache.layered')]
        private readonly CacheInterface $cache
    ) {}

    public function getStats(): array
    {
        // 1. Checks APCu. Hit? Return.
        // 2. Miss? Checks Redis. Hit? Populate APCu & Return.
        // 3. Miss? Run Callback. Populate Redis & APCu.
        return $this->cache->get('stats_v1', function (ItemInterface $item): array {
            $item->expiresAfter(3600);
            return $this->computeHeavyStats();
        });
    }

    private function computeHeavyStats(): array 
    { 
        // Simulation of heavy work
        return ['users' => 10500, 'revenue' => 50000]; 
    }
}
 bin/console cache:pool:clear cache.layered KEYS * (You will see the key). You will see the key.Disconnect Redis. The app will continue to serve from APCu for 60 seconds.Solving the Thundering Herd: Probabilistic Early ExpirationThe “Cache Stampede” (or Thundering Herd) occurs when a hot cache key expires. Suddenly, 1,000 concurrent requests miss the cache simultaneously and hit your database to compute the same value. The database crashes.\
Symfony solves this  complex locking mechanisms (like Semaphore) by using Probabilistic Early Expiration.Instead of expiring exactly at 12:00:00, the cache claims to be empty slightly before the expiration, but only for some requests. The closer to expiration, the higher the probability of a miss. One lucky request recomputes the value while others are served the “stale” (but valid) data.You don’t need a new configuration; you need to utilize the  parameter in the contract.// $beta of 1.0 is standard. 
// Higher = recompute earlier. 
// 0 = disable. 
// INF = force recompute.
$beta = 1.0; 

$value = $this->cache->get('stock_ticker_aapl', function (ItemInterface $item) {
    // The item ACTUALLY expires in 1 hour
    $item->expiresAfter(3600);

    return $this->stockApi->fetchPrice('AAPL');
}, $beta);
Mathematical VerificationThere is no CLI command to “prove” probability, but you can log the recomputations.Set .Create a loop that hits this cache key every 100ms.Observe that the callback is triggered before 10 seconds have passed and usually only once, ensuring your backend is protected.The “Black Box”: Transparent Encryption with SodiumCaching Personal Identifiable Information (PII) in Redis is a . If an attacker dumps your Redis memory, they have the data.\
Symfony allows you to wrap your cache adapter in a . We will use the  to transparently encrypt data before it leaves PHP and decrypt it upon retrieval.\
Ensure  is  and the extension is  in PHP 8.x.We need to decorate the default marshaller. We will use a “” marshaller (to compress data) wrapped inside a “” marshaller (to encrypt it).#config/services.yaml

services:
    # 1. Generate a key: php -r "echo bin2hex(random_bytes(SODIUM_CRYPTO_SECRETBOX_KEYBYTES));"
    # Store this in .env.local: CACHE_DECRYPTION_KEY=your_hex_key

    # 2. Define the Marshaller Service
    app.cache.marshaller.secure:
        class: Symfony\Component\Cache\Marshaller\SodiumMarshaller
        arguments: 
            - ['%env(hex2bin:CACHE_DECRYPTION_KEY)%']
            - '@app.cache.marshaller.deflate' # Chain encryption OVER compression

    app.cache.marshaller.deflate:
        class: Symfony\Component\Cache\Marshaller\DeflateMarshaller
        arguments: ['@default_marshaller']

    default_marshaller:
        class: Symfony\Component\Cache\Marshaller\DefaultMarshaller

    # 3. Use the custom marshaller in your cache adapter
    Symfony\Component\Cache\Adapter\RedisAdapter:
        arguments:
            $marshaller: '@app.cache.marshaller.secure'
#config/packages/cache.yaml

framework:
    cache:
        pools:
            cache.secure:
                adapter: cache.adapter.redis
                # We need to point the 'default_marshaller' of this pool to our secure one?
                # Actually, defining the adapter service globally as we did above 
                # is the cleanest way if you want ALL redis caches encrypted.
                # Alternatively, use the provider syntax:

                # For specific pool encryption, we often have to define the service manually 
                # or use a factory because framework.yaml config is limited for complex DI.
\
Refined Approach for Symfony (Best Practice): Instead of overriding the global adapter, define the pool service explicitly to inject the marshaller.# config/services.yaml
services:
    app.cache.secure_pool:
        class: Symfony\Component\Cache\Adapter\RedisAdapter
        arguments:
            $redis: 'redis://%env(REDIS_HOST)%:6379'
            $marshaller: '@app.cache.marshaller.secure'
        tags: ['cache.pool']
public function storeUserAddress(int $userId, string $address): void
{
    // This data is compressed and encrypted in Redis
    $cacheItem = $this->securePool->getItem('user_addr_' . $userId);
    $cacheItem->set($address);
    $this->securePool->save($cacheItem);
}
Connect to Redis via CLI: . You will see binary garbage (encrypted string), not the plaintext address. You get the plaintext address.The hardest problem in computer science is cache invalidation. It gets harder when you have 5 web servers (pods). If Server A updates a product, Server B’s APCu cache still holds the old product.\
We solve this by broadcasting invalidation messages via Symfony Messenger.Cache locally in APCu (for speed).When data changes, dispatch a message to the bus.All servers consume the message and clear their local APCu for that specific tag.composer require symfony/messenger
\
We need a transport that supports “Pub/Sub” (Fanout), so every server gets the message. Redis streams or RabbitMQ Fanout exchanges work.#config/packages/messenger.yaml

framework:
    messenger:
        transports:
            # Use a fanout exchange so ALL pods receive the message
            cache_invalidation: 
                dsn: '%env(MESSENGER_TRANSPORT_DSN)%'
                options:
                    exchange:
                        type: fanout
                        name: cache_invalidation_fanout
1. The Invalidation Messagenamespace App\Message;

final readonly class InvalidateTagsMessage
{
    public function __construct(
        public array $tags
    ) {}
}
\
2. The Handler This handler runs on every server.namespace App\MessageHandler;

use App\Message\InvalidateTagsMessage;
use Symfony\Component\Messenger\Attribute\AsMessageHandler;
use Symfony\Contracts\Cache\TagAwareCacheInterface;
use Symfony\Component\DependencyInjection\Attribute\Target;

#[AsMessageHandler]
final readonly class InvalidateTagsHandler
{
    public function __construct(
        #[Target('cache.layered')] // Target our Chain Cache
        private TagAwareCacheInterface $cache
    ) {}

    public function __invoke(InvalidateTagsMessage $message): void
    {
        // This invalidates the local APCu layer AND the shared Redis layer
        $this->cache->invalidateTags($message->tags);
    }
}
\
3. The Service Triggering the Changenamespace App\Service;

use App\Message\InvalidateTagsMessage;
use Symfony\Component\Messenger\MessageBusInterface;
use Symfony\Contracts\Cache\TagAwareCacheInterface;

class ProductService
{
    public function __construct(
        private TagAwareCacheInterface $cache,
        private MessageBusInterface $bus
    ) {}

    public function updateProduct(int $id, array $data): void
    {
        // 1. Update Database...

        // 2. Invalidate
        // We do NOT call $cache->invalidateTags() directly here.
        // Because that would only clear THIS server's APCu and the shared Redis.
        // Other servers would remain stale.

        $this->bus->dispatch(new InvalidateTagsMessage(["product_{$id}"]));
    }
}
The #[Cacheable] AttributeInstead of writing  boilerplate in every service method, let’s create a  that handles caching automatically using the  or .\
Below is a clean implementation using a Kernel::CONTROLLER_ARGUMENTS listener, which is efficient and easy to reason about.namespace App\Attribute;

use Attribute;

#[Attribute(Attribute::TARGET_METHOD)]
final readonly class Cacheable
{
    public function __construct(
        public string $pool = 'cache.app',
        public int $ttl = 3600,
        public ?string $key = null
    ) {}
}
This listener intercepts controller calls, checks for the attribute, and attempts to serve from cache.\
This simple implementation assumes the controller returns a generic serializable response (like JSON or an Array). For Response objects, serialization needs care.namespace App\EventSubscriber;

use App\Attribute\Cacheable;
use Symfony\Component\EventDispatcher\EventSubscriberInterface;
use Symfony\Component\HttpKernel\Event\ControllerEvent;
use Symfony\Component\HttpKernel\KernelEvents;
use Symfony\Component\DependencyInjection\ServiceLocator;
use Symfony\Contracts\Cache\ItemInterface;

class CacheableSubscriber implements EventSubscriberInterface
{
    public function __construct(
        private ServiceLocator $cachePools // Inject locator to find pools dynamically
    ) {}

    public static function getSubscribedEvents(): array
    {
        return [
            // High priority to catch request before execution
            KernelEvents::CONTROLLER => ['onKernelController', 10], 
        ];
    }

    public function onKernelController(ControllerEvent $event): void
    {
        $controller = $event->getController();

        // Handle array callables [$object, 'method']
        if (is_array($controller)) {
            $method = new \ReflectionMethod($controller[0], $controller[1]);
        } elseif (is_object($controller) && is_callable($controller)) {
            $method = new \ReflectionMethod($controller, '__invoke');
        } else {
            return;
        }

        $attributes = $method->getAttributes(Cacheable::class);
        if (empty($attributes)) {
            return;
        }

        /** @var Cacheable $cacheable */
        $cacheable = $attributes[0]->newInstance();

        if (!$this->cachePools->has($cacheable->pool)) {
            return;
        }

        $pool = $this->cachePools->get($cacheable->pool);

        // Generate a key based on Controller Class + Method + Request Params
        // This is a simplified key generation strategy
        $request = $event->getRequest();
        $cacheKey = $cacheable->key ?? 'ctrl_' . md5($request->getUri());

        // We cannot easily "skip" the controller execution inside a Listener 
        // using the Cache Contract pattern nicely without replacing the controller.
        // 
        // For a TRULY robust attribute implementation, one should use 
        // a "Service Decorator" logic, but for Controllers, we can replace the 
        // controller callable with a closure that wraps the cache logic.

        $originalController = $event->getController();

        $newController = function() use ($pool, $cacheKey, $cacheable, $originalController, $request) {
            return $pool->get($cacheKey, function (ItemInterface $item) use ($cacheable, $originalController, $request) {
                $item->expiresAfter($cacheable->ttl);

                // Execute the original controller
                // Note: We need to manually resolve arguments or pass the request
                // This part is tricky in raw PHP. 
                // In Symfony we can simply execute the original callable 
                // IF we have the resolved arguments.

                // SIMPLIFICATION for article:
                // Assuming controller takes Request object or no args
                return $originalController($request);
            });
        };

        $event->setController($newController);
    }
}
\
You have to  your cache pools in a  for the  to access them dynamically.\
The code above demonstrates modifying the . Ideally, for services, you would use  on the service definition, but for Controllers, intercepting the event is the Symfony way.Implementing  is easy; architecting a caching strategy that survives network partitions,  audits, and  spikes is a discipline.\
The strategies outlined here —  for latency,  for concurrency,  for security, and Messenger-based Invalidation for consistency — move your application away from fragile optimizations and toward robust engineering. Symfony provides these primitives out of the box, allowing us to solve complex distributed system problems without introducing heavy third-party infrastructure.\
Stop treating your cache as a temporary dumping ground. Treat it as a critical, secured layer of your data persistence strategy.Let’s Continue the DiscussionHigh-performance PHP architecture is a constantly evolving landscape. If you are refactoring a legacy monolith or designing a distributed system in Symfony, I’d love to hear about the challenges you are facing.\
Have you implemented a custom Marshaller for specific compliance needs?\
How are you handling cache invalidation across multi-region deployments?\
Reach out to me directly on LinkedIn. Let’s geek out over architecture, share war stories, and build better software.]]></content:encoded></item><item><title>AI is too risky to insure, say people whose job is insuring risk</title><link>https://techcrunch.com/2025/11/23/ai-is-too-risky-to-insure-say-people-whose-job-is-insuring-risk/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sun, 23 Nov 2025 17:45:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Major insurers including AIG, Great American, and WR Berkley are asking U.S. regulators for permission to exclude AI-related liabilities from corporate policies. One underwriter describes the AI models’ outputs to the FT as "too much of a black box."]]></content:encoded></item><item><title>Could High-Speed Trains Shorten US Travel Times While Reducing Emissions?</title><link>https://news.slashdot.org/story/25/11/22/201221/could-high-speed-trains-shorten-us-travel-times-while-reducing-emissions?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 17:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[With some animated graphics, CNN "reimagined" what three of America's busiest air and road travel routes would look like with high-speed trains, for "a glimpse into a faster, more connected future."


The journey from New York City to Chicago could take just over six hours by high-speed train at an average speed of 160 mph, cutting travel time by more than 13 hours compared with the current Amtrak route... The journey from San Francisco to Los Angeles could be completed in under three hours by high-speed train... The journey from Atlanta to Orlando could be completed in under three hours by high-speed train that reaches 160 mph, cutting travel time by over half compared with driving... 



While high-speed rail remains a fantasy in the United States, it is already hugely successful across the globe. Passengers take 3 billion trips annually on more than 40,000 miles of modern high-speed railway across the globe, according to the International Union of Railways. China is home to the world's largest high-speed rail network. The 809-mile train journey from Beijing to Shanghai takes just four and a half hours... In Europe, France's Train a Grand Vitesse (TGV) is recognized as a pioneer of high-speed rail technology. Spain soon followed France's success and now hosts Europe's most extensive high-speed rail network... 

[T]rain travel contributes relatively less pollution of every type, said Jacob Mason of the Institute for Transportation and Development Policy, from burning less gasoline to making less noise than cars and taking up less space than freeways. The reduction in greenhouse gas emissions is staggering: Per kilometer traveled, the average car or a short-haul flight each emit more than 30 times the CO2 equivalent than Eurostar high-speed trains, according to data from the UK government.]]></content:encoded></item><item><title>TechCrunch Mobility: Searching for the robotaxi tipping point</title><link>https://techcrunch.com/2025/11/23/techcrunch-mobility-searching-for-the-robotaxi-tipping-point/</link><author>Kirsten Korosec</author><category>tech</category><pubDate>Sun, 23 Nov 2025 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Welcome back to TechCrunch Mobility — your central hub for news and insights on the future of transportation. ]]></content:encoded></item><item><title>Beehiiv’s CEO isn’t worried about newsletter saturation</title><link>https://techcrunch.com/2025/11/23/beehiivs-ceo-isnt-worried-about-newsletter-saturation/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 23 Nov 2025 16:53:45 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Beehiiv's co-founder and CEO Tyler Denk spoke to TechCrunch about why the company has been expanding, and where the media business is going next.]]></content:encoded></item><item><title>Microsoft and GitHub Preview New Tool That Identifies, Prioritizes, and Fixes Vulnerabilities With AI</title><link>https://developers.slashdot.org/story/25/11/23/0116256/microsoft-and-github-preview-new-tool-that-identifies-prioritizes-and-fixes-vulnerabilities-with-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Security, development, and AI now move as one," says Microsoft's director of cloud/AI security
product marketing. 



Microsoft and GitHub "have launched a native integration between Microsoft Defender for Cloud and GitHub Advanced Security that aims to address what one executive calls decades of accumulated security debt in enterprise codebases..." according to The New Stack:

The integration, announced this week in San Francisco at the
Microsoft
Ignite 2025 conference and now available in public preview,
connects runtime intelligence from production environments directly
into developer workflows. The goal is to help organizations
prioritize which vulnerabilities actually matter and use AI to fix
them faster. "Throughout my career, I've seen vulnerability
trends going up into the right. It didn't matter how good of a
detection
engine and how accurate our detection engine was, people just
couldn't fix things fast enough," said Marcelo
Oliveira, VP of product management at GitHub, who has spent
nearly a decade in application security. "That basically resulted
in decades of accumulation of security debt into enterprise code
bases." According to industry data, critical and high-severity
vulnerabilities constitute 17.4% of security backlogs, with a mean
time to remediation of 116 days, said Andrew
Flick, senior director of developer services, languages and tools
at Microsoft, in a blog
post. Meanwhile, applications face attacks as frequently as once
every three minutes, Oliveira said. 


The integration represents the first native link between runtime
intelligence and developer workflows, said Elif
Algedik, director of product marketing for cloud and AI security
at Microsoft, in a blog
post... The problem, according to Flick, comes down to three
challenges: security teams drowning in alert fatigue while AI rapidly
introduces new threat
vectors that they have little time to understand; developers
lacking clear prioritization while remediation takes too long; and
both teams relying on separate, nonintegrated tools that make
collaboration slow and frustrating... The new integration works
bidirectionally. When Defender for Cloud detects a vulnerability in a
running workload, that runtime context flows into GitHub, showing
developers whether the vulnerability is internet-facing, handling
sensitive data or actually exposed in production. This is powered by
what GitHub calls the Virtual Registry, which creates code-to-runtime
mapping, Flick said... 

In the past, this alert would age in a dashboard while developers
worked on unrelated fixes because they didn't know this was the
critical one, he said. Now, a security campaign can be created in
GitHub, filtering for runtime risk like internet exposure or
sensitive data, notifying the developer to prioritize this issue. 


GitHub Copilot "now automatically checks dependencies, scans
for first-party code vulnerabilities and catches hardcoded secrets
before code reaches developers," the article points out — but
GitHub's VP of product management says this takes things even
further. 

"We're not only helping you fix existing vulnerabilities,
we're also reducing the number of vulnerabilities that come into
the system when the level of throughput of new code being created is
increasing dramatically with all these agentic coding agent platforms."]]></content:encoded></item><item><title>Building a Production-Ready Laravel Stack with Traefik and FrankenPHP</title><link>https://hackernoon.com/building-a-production-ready-laravel-stack-with-traefik-and-frankenphp?source=rss</link><author>Daniel, Andrei-Daniel Petrica</author><category>tech</category><pubDate>Sun, 23 Nov 2025 16:08:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Laravel boots slowly by default, but Octane and FrankenPHP cut request time dramatically by keeping the framework in memory. This guide shows how a multi-stage Dockerfile, isolated containers, Traefik routing, and a simple deployment script come together to run a fast, reliable Laravel Octane stack in production.]]></content:encoded></item><item><title>The HackerNoon Newsletter: Why DynamoDB Costs Explode (11/23/2025)</title><link>https://hackernoon.com/11-23-2025-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Sun, 23 Nov 2025 16:02:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, November 23, 2025?By @knightbat2040 [ 6 Min read ] Built an AI tool that scrapes, cleans, and summarizes Texas bills to make government legislation readable and transparent for everyone.  Read More.By @stevebeyatte [ 8 Min read ] T3RA Logistics is redefining freight with AI agents—running a $100M operation with just 25 “superhumans.” Read More.By @techreviewer1 [ 2 Min read ] VSYS Host launches VSYS Name, an ICANN-accredited registrar offering direct domain control, transparent pricing, crypto payments, and 24/7 expert support. Read More.By @aioznetwork [ 5 Min read ] AIOZ AI is the intelligence layer of the AIOZ Network, connecting a global community through a peer-to-peer compute economy. Learn more here! Read More.By @drechimyn [ 6 Min read ] This article argues that true engineering excellence lies not in adopting new frameworks or rewriting systems but in sustaining resilient, boring infrastructure Read More.By @kashvipandey [ 2 Min read ] Humanity Protocol partners with Mastercard to integrate open finance into Human ID, enabling secure, privacy-first access to loans, credit, and Web3 finance. Read More.By @scylladb [ 5 Min read ] Discover how DynamoDB’s pricing quirks—rounding, replication, caching, and global tables—can skyrocket costs, and how ScyllaDB offers predictable pricing. Read More.By @hacker39947670 [ 14 Min read ] Hands-on ERC-4337 guide: deploy EntryPoint, build a minimal smart account, fund deposits, and send your first UserOperation with Foundry and TypeScript Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Your AI Has Amnesia: A New Paradigm Called &apos;Nested Learning&apos; Could Be the Cure</title><link>https://hackernoon.com/your-ai-has-amnesia-a-new-paradigm-called-nested-learning-could-be-the-cure?source=rss</link><author>Anthony Laneau</author><category>tech</category><pubDate>Sun, 23 Nov 2025 16:00:10 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Current Large Language Models (LLMs) possess vast knowledge, but their learning process has a fundamental flaw. Imagine a person with anterograde amnesia—they can recall the distant past but are unable to form new long-term memories. In a similar way, an LLM's knowledge is static, confined to the information it learned during pre-training. When it comes to self-improvement, the human brain is the gold standard, adapting through neuroplasticity, the remarkable capacity to change its structure in response to new experiences.\
This limitation in AI leads to a critical problem known as "catastrophic forgetting." When a model is continually updated with new data, the process of learning new information often forces it to overwrite and forget old, established knowledge. It's a frustrating trade-off: gain a new skill, lose an old one.\
To solve this, Google Research has introduced "Nested Learning," a new, brain-inspired paradigm that fundamentally rethinks how AI models are built. This post breaks down the three most surprising and impactful ideas from this research, explaining how they could give AI the ability to learn continually, just like we do.1. A Model's Blueprint and Its Learning Process Aren't Separate; They're One.In traditional AI development, a model’s architecture (the structure of its neural network) and its optimization algorithm (the rules it follows to learn) are treated as two separate problems. Researchers design the network first, then figure out the best way to train it.\
Nested Learning flips this convention on its head. It proposes that the architecture and the training rules are fundamentally the same concept, differing only in their speed. The paradigm views a single AI model not as one monolithic entity, but as a system of components, each processing its own stream of information (its "context flow") at a specific "update frequency rate." An architectural component, like an attention layer, processes the flow of input tokens, while an optimizer processes the flow of error signals. Both are just learning to compress their respective context flows.\
This is a revolutionary idea because it unifies two previously distinct fields of study. By treating the model and its learning process as a single, coherent system of nested optimization problems, Nested Learning reveals a "new, previously invisible dimension for designing more capable AI."One of the most mind-bending insights from Nested Learning is that common, foundational tools in machine learning are already functioning as simple learning systems. The research shows that components like optimizers (e.g., SGD with Momentum or Adam) and even the core process of backpropagation can be reframed as "associative memory" systems.\
Associative memory is the ability to map and recall one thing based on another, like remembering a person's name when you see their face. This re-framing works because an optimizer’s core job is to compress its context flow—the history of all past error gradients - into its internal state.\
According to the research, backpropagation is a process where the model learns to map a given data point to its "Local Surprise Signal": a measure of how unexpected that information was. This isn't just an abstract concept; the paper clarifies that this "surprise" is the concrete mathematical error signal, the gradient of the loss (∇yt+1 L(Wt;xt+1)). Optimizers with momentum are essentially building a compressed memory of these surprise signals over time.\
This re-framing isn't just a theoretical exercise; it has practical implications for building better models. The researchers highlight this key finding in their paper:\
Based on NL, we show that well-known gradient-based optimizers (e.g., Adam, SGD with Momentum, etc.) are in fact associative memory modules that aim to compress the gradients with gradient descent.3. AI Memory Isn't a Switch; It's a Spectrum.A standard Transformer model treats memory in two distinct buckets. The attention mechanism acts as a short-term memory for immediate context, while the feedforward networks store long-term, pre-trained knowledge. Once training is complete, that long-term memory is frozen.\
Nested Learning proposes a more fluid and powerful alternative called a "Continuum Memory System" (CMS). Instead of just two types of memory, a CMS is a spectrum of memory modules, each managing a different context flow and updating at a different frequency. This is analogous to how the human brain consolidates memories over different time scales, from fleeting thoughts to deeply ingrained knowledge.\
This isn't just a new invention; it's a deeper understanding of what already works. The paper's most profound insight is that "well-known architectures such as Transformers are in fact linear layers with different frequency updates." The CMS is a generalization of a principle that was hiding in plain sight.\
This more sophisticated memory system is a core component of the proof-of-concept "Hope" architecture. Described as a "self-modifying recurrent architecture" and a variant of the "Titans architecture," Hope demonstrated superior performance on tasks requiring long-context reasoning.Conclusion: A Glimpse of Self-Improving AINested Learning provides a new and robust foundation for building AI that can learn without forgetting. By treating a model's architecture and its optimization rules as a single, coherent system of nested optimization problems, each compressing a context flow, we can design more expressive and efficient AI.\
The success of the Hope architecture serves as a powerful proof-of-concept. As a "self-modifying" and "self-referential" architecture, it demonstrates that these principles can lead to models that are not only more capable but also more dynamic. This represents a significant step toward creating truly self-improving AI systems.\
By closing the gap between artificial models and the human brain's ability to learn continually, what is the next great capability we will unlock in AI?]]></content:encoded></item><item><title>Why the MITRE ATT&amp;CK Framework Actually Works</title><link>https://hackernoon.com/why-the-mitre-attandck-framework-actually-works?source=rss</link><author>John Vester</author><category>tech</category><pubDate>Sun, 23 Nov 2025 16:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The alert goes off at 2:17 p.m.\
You count yourself lucky that this one’s in the afternoon, not morning. You drop what you’re doing, open the console, and start digging in.\
Oh, a significant spike in outbound traffic from a Kubernetes node. A privileged service account authenticating from an unfamiliar IP. Hmm, some DNS requests look… odd, but not that odd. You pivot through dashboards, trace the source in your SIEM, check cloud logs, query identity data, and even pull container logs. Nothing definitive. No confirmed breach. No clear story is emerging.\
Was it a misconfigured workload? A developer testing a deployment script? A legitimate automation job or the first step of lateral movement by someone already inside? The indicators blur together until you can’t tell if you’ve caught an attack in progress or another false positive in a sea of noise.\
Twenty minutes later, you close the ticket with the same note as the last one: “Monitoring.”\
Imagine doing that fifty times daily. The backlog grows as threats move through the network using valid credentials, blending in with legitimate activity, leaving ambiguous traces. Many security teams face a harsh reality: traditional detection systems catch known threats such as signature exploits, anomalies, and documented indicators. But attackers now mimic user behavior, hijack processes, and pivot beyond correlation rules, flooding alerts with few meaningful ones. Analysts are overwhelmed chasing false alarms while real threats go unnoticed.\
Security shouldn’t just be about stopping the bad, but understanding how the bad actually happens. Enter the MITRE ATT&CK framework. Built from years of real-world threat research, it offers a living map of how adversaries operate, move laterally, and exploit systems step by step. And when paired with a modern analytics platform, it turns that understanding into actionable visibility by showing exactly where your defenses are strong and, by contrast, where they may be weak.\
This article explains how MITRE ATT&CK shifts threat detection from reactive to proactive, how modern analytics platforms support this, and shares best practices for developing adaptive detection logic.Most security frameworks start with what went wrong after the fact. MITRE ATT&CK begins with how things go wrong in the first place.\
Developed by the nonprofit MITRE Corporation,  (Adversarial Tactics, Techniques, and Common Knowledge) is a living knowledge base of real-world attacker behavior. Each entry in the ATT&CK Matrix maps tactics (the why) and techniques (the how) that adversaries use across different stages of an intrusion.\
Instead of focusing on malware signatures or static indicators, it emphasizes observable behavior, attacker tactics like gaining access, escalating privileges, lateral movement, or data exfiltration. It's a structured playbook covering phishing, credential dumping, command-and-control, linked to threat groups and campaigns.\
For instance, the framework details how APT29 (Cozy Bear), a threat group tied to multiple espionage operations, frequently abuses legitimate credentials to blend into regular network traffic. By mapping detections to those same techniques, teams can uncover blind spots that purely signature-based tools miss.\
This behavioral model helps security and engineering teams speak a shared language. A SOC analyst investigating a PowerShell command might tag it as T1059.001 (Command and Scripting Interpreter: PowerShell), while a cloud engineer reviewing IAM logs might reference T1078 (Valid Accounts). Each technique includes definitions, detection ideas, and references to confirmed incidents, making ATT&CK as practical for detection engineering as it is for incident response.\
ATT&CK's strength lies in its ongoing evolution, incorporating recent threat intelligence, observations, and community input to reflect current attacker behavior. Engineers should follow the principle: Log what matters, detect observable actions, and verify assumptions against real adversary behavior.Today, every major security platform claims “MITRE ATT&CK integration”: Splunk, Microsoft Sentinel, Palo Alto Networks, Sumo Logic, and Check Point. They all offer dashboards mapping detections to ATT&CK tactics and techniques. But not all dashboards are equal. Some provide only surface-level, color-coded matrices that look impressive but don’t show what’s detectable in your environment. Others display only one or two data sources, leaving visibility gaps. Few help compare coverage to real-world tactics. That’s where a modern analytics platform can help.\
For this article, we’ll use Sumo Logic’s Threat Coverage Explorer as an example of the kind of tool you can use to map detections to ATT&CK techniques. Rather than only displaying an ATT&CK matrix, it connects the dots between your real detection rules and the techniques they map to. It analyzes your security content, including correlation rules, log patterns, and detections-as-code. And it builds a visual model of your defensive coverage across all ATT&CK tactics and techniques across all this content.\
By combining the right tool with ATT&Ck, you can:See which ATT&CK techniques you can actually detect. Instead of relying on vendor defaults, you can evaluate your detections and map them to relevant ATT&CK TTPs. You see where you have visibility and where you don’t.Perform gap analysis and peer comparison. You can benchmark your coverage against industry peers at both technique and technology levels. Are your detections strong on credential access (TA0006) but weak on lateral movement (TA0008)? You’ll see it immediately.Tag custom rules with ATT&CK techniques. Detection engineers can label correlation rules with specific ATT&CK IDs like T1055 for Process Injection or T1552.001 for Unsecured Credentials in Files, enabling your content library to map to the framework automatically. This simplifies documenting, testing, and maintaining coverage as attacker behavior evolves.Visualize coverage at a glance. You can see a heatmap that shows which techniques are fully, partially, or not covered. You can drill down from tactic summaries to rules, events, and data sources. It’s more than a dashboard—it’s an actionable view of your detection maturity.\
This approach transforms ATT&CK from a theoretical framework into a living operational map. Imagine a scenario in which your logs capture credential misuse (T1078) but miss persistence mechanisms such as scheduled tasks (T1053). It exposes that blind spot before an attacker can exploit it. And because it updates dynamically as your rules or content evolve, it becomes part of your continuous detection engineering workflow rather than a one-time assessment. In the same way DevOps teams rely on observability metrics to measure system health, modern SecOps teams can now measure detection health.Using ATT&CK for a Better SecOpsThe real power of MITRE ATT&CK isn’t in its taxonomy. At its core, the ATT&CK taxonomy is a schema for adversarial behavior. Each tactic is like a stage in an attacker’s workflow, while each technique describes the specific implementation of that step. It’s a structure of how attacks unfold, grounded in empirical evidence from real intrusions rather than theoretical models. This taxonomy enables a shift from reactive firefighting to proactive defense.\
Many organizations spend their days chasing alerts. Every detection is treated as an isolated event instead of a step in a larger narrative. ATT&CK flips that mindset. By mapping activity to known adversarial tactics and techniques, teams start thinking like attackers and thereby anticipating what comes next instead of waiting for another alert to tell them.\
When combined with the right tool, this mindset becomes operational. You can test hypotheses (“If an attacker gained initial access through phishing, would we see their lateral movement?”), validate detection logic, and continuously measure improvements over time. Again, the goal isn’t about collecting more data for its own sake; it’s about collecting the correct data and understanding what it tells you in context.\
Because ATT&CK evolves as adversaries do, your detection logic can grow too. New techniques emerge in the framework as they’re observed in the wild, giving engineers a head start on updating correlation rules, dashboards, and automation playbooks before those tactics appear in production environments.\
Proactive security is a process, not a steady-state. With ATT&CK as your blueprint and the right tool as your lens, your team can move from reacting to alerts toward anticipating attacker behavior. That’s the difference between being surprised by a breach and detecting it before it becomes one.MITRE ATT&CK is Your BlueprintSecurity isn’t about building taller walls; it’s about understanding how attackers move within them. The next time that 2:17 p.m. alert fires, it doesn’t have to end with “Monitoring.” The MITRE ATT&CK framework works because it’s grounded in how real adversaries think and operate.\
With MITRE ATT&CK as your blueprint and the right tool as your lens, those same signals become context, not chaos. You’ll know which behaviors matter, where your defenses stand, and how to strengthen them before attackers can adapt. Together, they transform security from reactive noise management into a cycle of continuous learning and proactive defense, which is why ATT&CK continues to work in practice, not just in theory.\
Have a really great day!]]></content:encoded></item><item><title>ChatGPT told them they were special — their families say it led to tragedy</title><link>https://techcrunch.com/2025/11/23/chatgpt-told-them-they-were-special-their-families-say-it-led-to-tragedy/</link><author>Rebecca Bellan, Amanda Silberling</author><category>tech</category><pubDate>Sun, 23 Nov 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A wave of lawsuits against OpenAI detail how ChatGPT used manipulative language to isolate users from loved ones and make itself into their sole confidant. ]]></content:encoded></item><item><title>NVIDIA Preps 1.6Tb/s Networking For Linux 6.19</title><link>https://www.phoronix.com/news/NVIDIA-1.6Tb-Net-Linux-6.19</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Nov 2025 15:58:09 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[NVIDIA has a number of Linux kernel patches on the way to the Linux 6.19 kernel in preparing for 1.6 Tb/s networking on NVIDIA-Mellanox hardware...]]></content:encoded></item><item><title>GetBlock Launches Its Biggest Black Friday Sale With Up to 60% Off Shared Nodes</title><link>https://hackernoon.com/getblock-launches-its-biggest-black-friday-sale-with-up-to-60percent-off-shared-nodes?source=rss</link><author>GetBlock</author><category>tech</category><pubDate>Sun, 23 Nov 2025 15:52:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
GetBlock, a premium RPC node provider and Web3 infrastructure platform, announces its largest sale of the year!To celebrate Black Friday 2025 properly, we offer up to 60% off on our Shared Nodes. On November 24 - December 2, 2025, GetBlock runs its biggest sale ever:60% off on Starter package and 50% off on Pro, Advanced packages (one month, new users only)40% off on every Shared Node plan with 12-month lock-in5 million CUs for feedback shared on social media (all users):::tip
No promo codes, no landings, zero hassle: sign up to , and upgrade in your account for Starter, Pro, or Advanced packages.GetBlock launches Black Friday: No better chance for an upgradeGetBlock, a Tier 1 provider of blockchain RPC nodes and Web3 infra solutions, invites all its new and existing users to get maximum value this Black Friday. Starting November 24, our Shared RPC nodes will be on sale.For all new users, we offer an exclusive opportunity to try our paid packages for almost free: 60% off, $19/mo instead of $49/mo 50% off - $99/mo instead of $199/mo, and $249/mo instead of $499/moThere's a splendid opportunity to finally upgrade your infrastructure stack. Here's what changes once you switch from the Free plan to any of these alternatives.|  |  |  |  |  |
|----|----|----|----|----|
|  | 2 | 10 | 25 | 50 |
| Throughput, requests per sec | 20 | 100 | 300 | 500 |
|  | 50K | 50M | 220M | 600M |
|  | No | Yes | Yes | Yes |
|  | No | Yes | Yes | Yes |
|  | No | Yes | Yes | Yes |
|  | Basic | 24/5 | 24/5 | 24/7 |The offer is valid for the first month of the new subscription. That's right - you can boost your speed by 15x and get 4,400x more compute units for just $99.Throughout 2025, we've done a tremendous amount of work to upgrade our Shared Node stack - here's what you get at 60% off (only our 5 biggest improvements):Normally, for all these services you need a dedicated server - but not with GetBlock. Simply  to unlock these powerful features, now at unbeatable Black Friday discounts.GetBlock also offers the opportunity to scale really big. Users can pay for a 12-month subscription and save an unbelievable 40%.For instance, here's how the math looks for the Pro*** plan: $5,988 (with monthly billing) $3,593 (40% OFF!)With these opportunities, you can leave your rivals in the dust when it comes to infrastructure spending.|  |  |
|----|----|
| Regular GetBlock Pro (billed annually) | $399 |
| GetBlock Pro Black Friday 2025 Promo Offer |  |
| Alchemy PAYG 1B CUs | $415 |
| QuickNode Scale | $499 |
| Helius Professional | $999 |For the plans with 250-500 RPS, 600M-1B compute units, and $0.40-$0.60 per 1M extra compute unitsOnly through December 2, you can easily save about $5,000 for a setup of just two Shared Nodes.Last but not least, you can benefit from your creativity.Write your success story about building with GetBlock RPC nodes. Or a failure story. You tell us.Post it anywhere online (Twitter/X, LinkedIn, Medium, your blog, Reddit - your choice)Collect your reward: 5 million Compute Units sent straight to your accountThat's the equivalent of 8 years and 4 months on the Free plan. Absolutely free.How to claim your exclusive discountNormally, there should be a promo code, registration form, or fancy landing page with crossed-out prices.But who needs that in 2025?Be our guest. Just sign up to GetBlock and upgrade your plan in your personal dashboard until December 2.No need to wait - this is the only Black Friday deal your Web3 infrastructure needs.]]></content:encoded></item><item><title>Swift Concurrency—Part 1: Tasks, Executors, and Priority Escalation</title><link>https://hackernoon.com/swift-concurrencypart-1-tasks-executors-and-priority-escalation?source=rss</link><author>Nikita Vasilev</author><category>tech</category><pubDate>Sun, 23 Nov 2025 15:47:08 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Swift 6 introduced a new approach to concurrency in apps. In this article, we will explore the problems it aims to solve, explain how it works under the hood, compare the new model with the previous one, and take a closer look at the Actor model. In the upcoming parts, we will also break down executors, schedulers, structured concurrency, different types of executors, implement our own executor, and more.Swift Concurrency Overview: Problems and SolutionsConcurrency has long been one of the most challenging aspects of software development. Writing code that runs tasks simultaneously can improve performance and responsiveness, but it often introduces complexity and subtle bugs as race conditions, deadlocks, and thread-safety issues.Swift Concurrency, introduced in Swift 6, aims to simplify concurrent programming by providing a clear, safe, and efficient model for handling asynchronous tasks. It helps developers to avoid common pitfalls by enforcing strict rules around data access and execution order.💡If you use Swift 5.x and plan to migrate to Swift 6, you can enable Swift Concurrency checks in your project settings. This allows you to gradually adopt the new concurrency model while maintaining compatibility with existing code. Enabling these checks helps catch potential concurrency issues early, making the transition smoother and safer. As you update your codebase, you can start integratingsyntax and other Swift Concurrency features incrementally without a full rewrite.\
Some of the key problems Swift Concurrency addresses include:Race Conditions: Preventing simultaneous access to shared mutable state that can cause unpredictable behaviour.Callback hell: Simplifying asynchronous code that used to rely heavily on nested callbacks or completion handlers, making code easier to read and maintain.Thread management complexity: Abstracting away low-level creation and synchronization, allowing developers to focus on the logic rather than thread handling.Coordinating concurrent tasks: Structured concurrency enables clear hierarchies of tasks with proper cancellation and error propagation.By leveraging new language features like , Actors, and structured concurrency, Swift 6 provides a more intuitive and robust way to write concurrent code, improving both developer productivity and app stability.Modern operating systems and runtimes use multitasking to execute units of work concurrently. Swift Concurrency adopts cooperative multitasking, which differs fundamentally from the preemptive multitasking model used by OS-level threads. Understanding the difference is key to writing performant and safe asynchronous Swift code.Preemptive multitasking is the model used by operating systems to manage threads and processes. In this model, a system-level scheduler can forcibly interrupt any thread at virtually any moment to perform a context switch and allocate CPU time to another thread. This ensures fairness across the system and allows for responsive applications — especially when handling multiple user-driven or time-sensitive tasks.Preemptive multitasking enables true parallelism across multiple CPU cores and prevents misbehaving or long-running threads from monopolizing system resources. However, this flexibility comes at a cost. Because threads can be interrupted at any point in their execution — even in the middle of a critical operation — developers must use synchronization primitives such as mutexes, semaphores, or atomic operations to protect shared mutable state. Failing to do so may result in data races, crashes, or subtle bugs that are often difficult to detect and reproduce.This model offers greater control and raw concurrency, but it also places a significantly higher burden on developers. Ensuring thread safety in a preemptive environment is error-prone and may lead to non-deterministic behavior — behavior that varies from run to run — which is notoriously difficult to reason about or reliably test.From a technical perspective, preemptive multitasking relies on the operating system to handle thread execution. The OS can interrupt a thread at almost any point — even in the middle of a function — and switch to another. To do this, the system must perform a context switch, which involves saving the entire execution state of the current thread (such as CPU registers, the instruction pointer, and stack pointer), and restoring the previously saved state of another thread. This process may also require flushing CPU caches, invalidating the Translation Lookaside Buffer (TLB), and transitioning between user mode and kernel mode.These operations introduce significant runtime overhead. Each context switch takes time and consumes system resources — especially when context switches are frequent or when many threads compete for limited CPU cores. Additionally, preemptive multitasking forces developers to write thread-safe code by default, increasing overall complexity and the risk of concurrency bugs.While this model provides maximum flexibility and true parallelism, it’s often excessive for asynchronous workflows, where tasks typically spend most of their time waiting for I/O, user input, or network responses rather than actively using the CPU.In contrast, Swift’s concurrency runtime uses cooperative multitasking. In this model, a task runs until it voluntarily yields control — typically at an await point or via an explicit call to . Unlike traditional threads, cooperative tasks are never forcibly preempted. This results in predictable execution: context switches occur only at clearly defined suspension points.Swift’s cooperative tasks are scheduled onto a lightweight, runtime-managed cooperative thread pool—separate from Grand Central Dispatch queues. Tasks running in this pool are expected to be “good citizens,” yielding control when appropriate, especially during long-running or CPU-intensive work. To support this, Swift provides  as a manual suspension point, ensuring other tasks have a chance to execute.However, cooperative multitasking comes with a caveat: if a task never suspends, it can monopolize the thread it’s running on, delaying or starving other tasks in the system. Therefore, it is the developer’s responsibility to ensure that long-running operations include suspension points.In cooperative multitasking, the fundamental unit of execution is not a thread, but a chunk of work, often called a continuation. A continuation is a suspended segment of an asynchronous function. When an  function suspends at an , the Swift runtime captures the current execution state into a heap-allocated continuation. This continuation represents a resumption point and is enqueued for future execution.Instead of associating a thread with a long-running task, the Swift runtime treats a thread as a pipeline of continuations. Each thread executes one continuation after another. When a continuation finishes or suspends again, the thread picks up the next ready continuation from the queue.As mentioned above, this model avoids traditional OS-level context switches. There is no need to save and restore CPU registers or thread stacks; the runtime simply invokes the next closure-like continuation. This makes task switching very fast and lightweight, though it involves increased heap allocations to store the suspended async state.The key trade-off: you use a bit more memory but gain dramatically lower overhead for task management. Cooperative scheduling gives tight control over when suspensions happen, which improves predictability and makes concurrency easier to reason about.In Swift Concurrency, a  provides a unit of asynchronous work. Unlike simply calling an  function, a  is a managed object that runs concurrently with other tasks in a cooperative thread pool.💡 Tasks are managed by a cooperative thread pool. The cooperative thread pool is designed to manage concurrency efficiently by allowing tasks to yield the CPU while waiting for asynchronous operations to complete. This is achieved through the use of async functions and tasks, which are the fundamental units of concurrency in Swift.Tasks can be created to run concurrently, and they can also be awaited or canceled. They provide fine-grained control over asynchronous behavior and are an integral part of structured concurrency in Swift.A task can be created using the , which immediately launches the provided asynchronous operation:Task(priority: .userInitiated) {
  await fetchData()
}
When you create a  using the standard initializer (i.e. not ), it inherits the surrounding actor context, priority, and task-local values. This behavior is crucial for structured concurrency and safety in concurrent code.💡 Swift 6.2 introduces a significant change to how concurrency is handled: by default, all code runs on. To run code on a background, Swift 6.2 adds a new attributeUnder the hood, in earlier versions of Swift Concurrency, the Swift runtime used an internal mechanism called  to track which actor a task was associated with. Although this property wasn’t part of the public API, it played a key role in ensuring that tasks created inside actor-isolated code would execute on the same actor, preserving data race safety.With advancements in Swift, the runtime has started transitioning from  to a new mechanism known as , which is now more explicitly handled by the compiler and runtime.is a new keyword introduced in Swift 6 as a part of language’s move toward safer and more explicit concurrency. It’s used to mark function parameters and return values that are moved across concurrency boundaries. It works especially well noncopyable types, ensuring memory safety and preventing use-after-move errors. When a parameter is marked with, the compiler enforces that the original instance is no longer accessed after the transfer.func process(_ data: sending MyNonCopyableType) async {
   // `data` is moved here and can’t be used elsewhere after the call
}
\
When you launch a , you must ensure that any values captured by the task are . The compiler now enforces this at compile-time using  protocol and the  function type.Failing to conform to  may result in a compile-time error, particularly in strict concurrency mode.Tasks also support priorities, similar to how Grand Central Dispatch queues handle them.When working with Swift Concurrency, it’s important to understand the difference between  and , as they define how and where asynchronous work is executed. inherits the current actor context (such as  or any custom actor) and priority. It’s commonly used when you want to spawn a new asynchronous operation that still respects the current structured concurrency tree or actor isolation. This is especially useful for UI updates or working inside specific concurrency domains.Task {
  await updateUI()
}
In the example above, if called from the main actor, the  will also run on the main actor unless explicitly moved elsewhere. creates a completely independent task. It doesn’t inherit the current actor context or priority. This means it starts in a global concurrent context and requires manage safety, especially when accessing shared data.Task.detached {
  await performBackgroundWork()
}
Use  when you need to run background operations outside the current structured context, such as long-running computations or escaping an actor’s isolation.A Cooperative Thread Pool in Swift Concurrency is a mechanism that manages the execution of asynchronous tasks by scheduling them onto a limited number of threads, typically matching the number of CPU cores.Swift Concurrency operates using cooperative thread pool designed for efficient scheduling and minimal thread overhead. Unlike traditional thread-per-task executing model, Swift’s approach emphasizes structured concurrency and resource-aware scheduling.A common oversimplification is to say that Swift Concurrency uses one thread per core, which aligns with its goal to reduce context switching and maximize CPU utilization. While not strictly false, this view omits important nuances about quality-of-service buckets, task priorities, and Darwin scheduler behavior.Thread Count: Not So SimpleOn a 16-core Mac, it’s possible to observe up to 64-threads managed by Swift Concurrency alone - without GCD involvement. This is because Swift’s cooperative thread pool maps not just per-core, but per core per QoS bucket.Max threads = (CPU cores) × (dedicated quality-of-service buckets)
Thus, on a 16-core system:16 cores × 4 QoS buckets = 64 threads
Each QoS bucket is essentially a dedicated thread lane for a group of tasks sharing similar execution priority. These are managed internally by Darwin’s thread scheduling mechanism and are not the same as GCD queues.QoS Buckets and Task PriorityAlthough  exposes six constants, some of them are aliases: → already mapped to For the kernel’s perspective, this simplifies to 4 core priority levels, each mapped to a QoS bucket, which influences thread allocation in the cooperative thread pool.Under normal load, Swift Concurrency respects the cooperative pool limits. However, under contention (e.g. high-priority tasks waiting on low-priority ones), the system may overcommit threads to preserve responsiveness. This dynamic adjustment ensures that time-sensitive tasks aren’t blocked indefinitely behind lower-priority work.This behavior is managed by the Darwin kernel via Mach scheduling policies and high-priority  lanes - not something controlled explicitly by your code.Swift provides a priority system for tasks, similar to Grand Central Dispatch (GCD), but more semantically integrated into the structured concurrency model. You can set a task’s priority via the  initializer:Task(priority: .userInitiated) {
  await loadUserData()
}
The available priorities are defined by the  enum:| Priority | Description |
|----|----|
|  /  | For tasks initiated by user interaction that require immediate feedback. |
|  | For tasks that the user is not actively waiting for. |
|  /  | For long-running tasks that don’t require immediate results, such as copying files or importing data. |
|  | For background tasks that the user is not directly aware of. Primarily used for work the user cannot see. |Creating Tasks with Different PrioritiesWhen you create a  inside another task (default  priority), you can explicitly set a different priority for each nested task. Here, one child task is , and the other is .high. This demonstrates that priorities can be individually set regardless of the parent.Task { // .medium by default
  Task(priority: .low) {
    print("\(1), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
  }

  Task(priority: .high) {
    print("\(2), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
  }
}

// 1, thread: <_NSMainThread: 0x6000017040c0>{number = 1, name = main}, priority: TaskPriority.low
// 2, thread: <_NSMainThread: 0x6000017040c0>{number = 1, name = main}, priority: TaskPriority.high
If you don’t explicitly set a priority for a nested task, it inherits the priority of its immediate parent. In this example, the anonymous tasks inside  and  blocks inherit those respective priorities unless overridden.Task Priorities Can Be InheritedTask {
  Task(priority: .high) {
    Task {
      print("\(1), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
    }
  }

  Task(priority: .low) {
     print("\(2), "thread: \(Thread.current)", priority: \(Task.currentPriority)")

     Task {
      print("\(3), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
     }

     Task(priority: .medium) {
      print("\(4), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
     }
  }
}

// 2, thread: <_NSMainThread: 0x600001708040>{number = 1, name = main}, priority: TaskPriority.low
// 1, thread: <_NSMainThread: 0x600001708040>{number = 1, name = main}, priority: TaskPriority.high
// 3, thread: <_NSMainThread: 0x600001708040>{number = 1, name = main}, priority: TaskPriority.low
// 4, thread: <_NSMainThread: 0x600001708040>{number = 1, name = main}, priority: TaskPriority.medium
If you don’t explicitly set a priority for a nested task, it inherits the priority of its immediate parent. In this example, the anonymous tasks inside .high and .low blocks inherit those respective priorities unless overridden.Task(priority: .high) {
  Task {
     print("\(1), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
  }

  await Task(priority: .low) {
    print("\(2), "thread: \(Thread.current)", priority: \(Task.currentPriority)")

    await Task {
      print("\(3), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
    }.value

    Task(priority: .medium) {
      print("\(4), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
    }
  }.value
}

// 1, thread: <_NSMainThread: 0x6000017000c0>{number = 1, name = main}, priority: TaskPriority.high
// 2, thread: <_NSMainThread: 0x6000017000c0>{number = 1, name = main}, priority: TaskPriority.high
// 3, thread: <_NSMainThread: 0x6000017000c0>{number = 1, name = main}, priority: TaskPriority.high
// 4, thread: <_NSMainThread: 0x6000017000c0>{number = 1, name = main}, priority: TaskPriority.medium
This mechanism is called priority escalation — when a task is awaited by a higher-priority task, the system may temporarily raise its priority to avoid bottlenecks and ensure responsiveness.Task 2, which is , is escalated to  while being awaited.Task 3, which doesn’t have an explicit priority, inherits the escalated priority from its parent (Task 2) and is also executed with priority.Task 4 explicitly sets its priority to , so it is not affected by escalation. Does Not Inherit PriorityDetached tasks () run independently and do not inherit the priority of their parent task. They behave like global tasks with their own scheduling. This is useful for isolating background work, but can also lead to unexpected priority mismatches if not set manually.Task(priority: .high) {
  Task.detached {
    print("\(1), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
  }

  Task(priority: .low) {
    print("\(2), "thread: \(Thread.current)", priority: \(Task.currentPriority)")

    Task.detached {
      print("\(3), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
    }

    Task(priority: .medium) {
      print("\(4), "thread: \(Thread.current)", priority: \(Task.currentPriority)")
    }
  }
}

// 1, thread: <NSThread: 0x60000174dec0>{number = 4, name = (null)}, priority: TaskPriority.medium
// 2, thread: <_NSMainThread: 0x600001708180>{number = 1, name = main}, priority: TaskPriority.low
// 3, thread: <NSThread: 0x60000174dec0>{number = 4, name = (null)}, priority: TaskPriority.medium
// 4, thread: <_NSMainThread: 0x600001708180>{number = 1, name = main}, priority: TaskPriority.medium
Suspension Points and How Swift Manages Async ExecutionIn Swift, any call to an  function using  is a potential suspension point - a place in the function where executing might pause and resume latter. It’s a transformation that involves saving the state of the function so it can be resumed latter, after awaited operation completes.func fetchData() async -> String {
  let result = await networkClient.load()
  return result
}
In this case, await networkClient.load() is a suspension point. When the function reaches this line, it may pause execution, yield control to the system, and later resume once  finishes. Behind the scenes, the compiler transforms this function into a state machine that tracks its progress and internal variables.Under the Hood: Continuations and State MachinesEvery  function in Swift is compiled into a state machine. Each marks a transition point. Before reaching an , Swift:Saves the current state of the function - including local variables and the current instruction pointer.Suspends execution and schedules a continuation.Once the async operation completes, it resumes the function from where it left off.This is similar to the continuation-passing style (CPS) used in many functional programming systems. In Swift’s concurrency model, this is orchestrated by internal types like  and the concurrency runtime scheduler.When you  something in Swift, the current thread is not blocked, instead:The current task yields control back to the executorOther tasks can run while waitingWhen the awaited operation completes, the suspended task resumes on the appropriate executor.This makes  fundamentally more efficient and scalable than thread-based blocking operations like .: letting other tasks run is a static method provided by Swift’s concurrency system that voluntarily suspends the current task, giving the system opportunity to run other enqueued tasks. It’s especially useful in long-running asynchronous operations or tight loops that don’t naturally contains suspension points.func processLargeBatch() async {
    for i in 0..<1_000_000 {
        if i % 10_000 == 0 {
            await Task.yield()
        }
    }
}
Without , this loop would monopolize the executor. By inserting  periodically, you’re cooperating with Swift’s concurrency runtime, allowing to maintain responsivness and fairness.Calling await  suspends the current task and re-enqueues it at the end of the queue for its current executor (e.g., main actor or a global concurrent executor). This allows other ready-to-run tasks to take their turn.It’s part of Swift’s cooperative multitasking model: tasks run to the next suspension point and are expected to yield fairly. Unlike preemptive systems (e.g., threads), Swift tasks don’t get forcibly interrupted — they must voluntarily yield control.Swift 6 marks a significant step forward in how concurrency is handled, offering developers more control, predictability, and safety. While the learning curve may be steep at first, understanding these concepts opens the door to building highly responsive and robust applications. As we continue exploring the deeper aspects of Swift’s new concurrency model, it’s clear that these changes lay the groundwork for the future of safe and scalable app development.]]></content:encoded></item><item><title>Kubernetes Security Observability Demands More Than Just Logs</title><link>https://hackernoon.com/kubernetes-security-observability-demands-more-than-just-logs?source=rss</link><author>Fatih Koç</author><category>tech</category><pubDate>Sun, 23 Nov 2025 15:34:15 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most Kubernetes security tools tell you about vulnerabilities before deployment. Many can detect what’s happening during an attack, but they work in isolation without the correlation needed to piece together the full story.The typical security stack includes vulnerability scanners, Pod Security Standards, network policies, and runtime detection tools. But when a real incident occurs, teams often struggle to understand what happened because each tool generates signals in isolation. Audit logs show API calls. Falco catches suspicious behavior. Prometheus exposes metrics you can use to spot network spikes. But these signals live in different systems with different timestamps and zero shared context.The problem is correlation.That’s what observability is about.Security observability is different from application observability. You’re not debugging slow queries or memory leaks. You’re answering “Did someone just try to escalate privileges?” and “Which pods are making unexpected API calls?” in real time. This requires audit logs, runtime behavior detection, network flow analysis, and the ability to correlate security events with application traces.What security observability actually meansSecurity observability means you can answer these questions in under 60 seconds:Which pods accessed secrets in the last hour?Did any container spawn an unexpected shell process?What API calls did this suspicious service account make?Which workloads are communicating outside their expected network boundaries?Can I trace this security event back to the specific user request that triggered it?Security observability gives you investigation superpowers through correlation and context.In my OpenTelemetry pipeline post, I showed how to unify metrics, logs, and traces. This post extends that foundation to security signals. You’ll get Kubernetes audit logs flowing through your observability pipeline, runtime security events from Falco correlated with traces and security metrics that let you spot attacks as they happen.Kubernetes audit logs are underutilizedKubernetes audit logs record every API server request. User authentication, pod creation, secret access, RBAC decisions, admission webhook results. Everything that touches the API server gets logged. Most teams either disable audit logs (which is insane from a security standpoint) or dump them to S3 where they’re essentially useless during an active incident. You can’t correlate S3 logs with live application traces when you need answers in under a minute.Here’s what you’re missing when audit logs aren’t in your observability platform:A service account suddenly starts listing secrets across all namespaces. A user creates a pod with  mounts in production. Someone deletes a critical ConfigMap. An unknown source IP hits the API server with repeated authentication failures. Without audit logs in a queryable backend with correlation to application telemetry, you’re investigating these incidents with  and guessing.I’ve seen teams spend 30 minutes trying to figure out who deleted a deployment. With audit logs in Loki and correlated by user/namespace/timestamp, it takes 15 seconds.Configuring useful audit logsThe default audit policy logs everything at the RequestResponse level, which means you’ll capture full request and response bodies for every API call. This generates gigabytes per day in any moderately active cluster and most of it is noise.You want a policy that captures security-relevant events at an appropriate detail level while dropping low-value spam, such as constant health check requests.apiVersion: audit.k8s.io/v1
kind: Policy
rules:
  # Don't log read-only requests to common resources
  - level: None
    verbs: ["get", "list", "watch"]
    resources:
      - group: ""
        resources: ["pods", "pods/status", "nodes", "nodes/status"]

  # Log secret access with metadata (who, when, which secret)
  - level: Metadata
    resources:
      - group: ""
        resources: ["secrets", "configmaps"]

  # Log RBAC changes with full request details
  - level: RequestResponse
    verbs: ["create", "update", "patch", "delete"]
    resources:
      - group: "rbac.authorization.k8s.io"
        resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]

  # Log pod create/delete with request body (captures specs)
  - level: Request
    verbs: ["create", "delete"]
    resources:
      - group: ""
        resources: ["pods"]

  # Catch privilege escalations and authentication failures
  - level: RequestResponse
    omitStages: ["RequestReceived"]
    users: ["system:anonymous"]

  # Default: log metadata for everything else
  - level: Metadata
    omitStages: ["RequestReceived"]
This policy logs secret access, RBAC changes, pod mutations, and authentication anomalies while dropping noisy read-only requests. It cuts audit volume by 70-80% compared to logging everything.For managed Kubernetes (EKS, GKE, AKS), audit logs are available through cloud provider logging services. For self-managed clusters, you’ll configure the API server flags in the shipping options below.Shipping audit logs to your observability pipelineAudit logs are structured JSON that you can send to any OTLP receiver or log aggregator. Choose the approach that fits your cluster setup:Option 1: File-based logging with Fluent Bit (simplest)For most setups, file-based audit logging with Fluent Bit is simpler than running a webhook server. Configure the API server to write audit logs to a file, then use Fluent Bit (already running as a DaemonSet in most observability setups) to tail and forward them.Configure API server for file-based logging:--audit-policy-file=/etc/kubernetes/audit-policy.yaml
--audit-log-path=/var/log/kubernetes/audit/audit.log
--audit-log-format=json
Then configure Fluent Bit to parse and forward audit logs:apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: observability
data:
  parsers.conf: |
    [PARSER]
        Name   k8s-audit
        Format json
        Time_Key requestReceivedTimestamp
        Time_Format %Y-%m-%dT%H:%M:%S.%LZ    

  fluent-bit.conf: |
    [INPUT]
        Name              tail
        Path              /var/log/kubernetes/audit/*.log
        Parser            k8s-audit
        Tag               k8s.audit
        Refresh_Interval  5
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On

    [FILTER]
        Name    modify
        Match   k8s.audit
        Add     service.name kubernetes-audit
        Add     signal.type security

    [OUTPUT]
        Name          forward
        Match         k8s.audit
        Host          otel-gateway.observability.svc.cluster.local
        Port          24224
        Require_ack_response  true    
This requires mounting /var/log/kubernetes/audit/ from the host into the Fluent Bit DaemonSet pods. No additional services needed.:::info
: The OpenTelemetry Collector receiving these logs must have the  receiver enabled on port 24224 (shown in the correlation section below).For managed Kubernetes, use the cloud provider’s native audit log integration. EKS sends audit logs to CloudWatch Logs, GKE to Cloud Logging, and AKS to Azure Monitor. Please forward them to your observability backend using the OpenTelemetry Collector’s cloud-specific receivers (CloudWatch, Google Cloud Logging, Azure Monitor) or cloud-native export mechanisms like Pub/Sub or Event Hubs. This approach avoids running additional infrastructure but creates vendor lock-in.Option 3: Webhook backend (for specific use cases)Webhooks allow the API server to send audit events to an HTTP endpoint in real time. Use this only if you need custom transformation logic before forwarding to OpenTelemetry. Deploy a simple HTTP service that receives audit event batches from the API server and forwards them to your OTLP endpoint. Configure the API server with --audit-webhook-config-file pointing to your webhook. Most teams are better served by Option 1 or Option 2. Webhooks add operational complexity without clear benefits for typical use cases.Which option should you choose?Self-managed clusters (kubeadm, kops, etc.): Use  (Fluent Bit). You already have file access and likely run Fluent Bit for application logs.Managed Kubernetes with existing observability stack: Use  (cloud provider native). Simplest integration with no additional infrastructure.Managed Kubernetes requiring vendor neutrality: Use  (webhook) only if you need real-time streaming and can’t use  (file-based logging).Need custom enrichment or transformation: Use  (webhook) to add custom logic before forwarding to OpenTelemetry.Falco catches what static scans missFalco is a CNCF runtime security tool that watches system calls (via eBPF or kernel module) and triggers alerts on suspicious behavior. Shell spawned in a container. Sensitive file access. Unexpected network connections. Privilege escalation attempts. These are behavioral signals that only appear at runtime. Vulnerability scanners won’t catch these behaviors because they only happen during execution, and Falco is purpose-built to detect them.Installing Falco with OpenTelemetry exportFalco can export alerts to syslog, HTTP endpoints, or gRPC. You want alerts flowing into your observability pipeline as structured logs with correlation context.Install Falco with Helm and configure JSON output:helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update

helm install falco falcosecurity/falco \
  --namespace falco \
  --create-namespace \
  --set tty=true \
  --set driver.kind=modern_ebpf \
  --set falco.json_output=true \
  --set falco.file_output.enabled=true \
  --set falco.file_output.filename=/var/run/falco/events.log \
  --set falco.file_output.keep_alive=false
Then configure Fluent Bit (or the OpenTelemetry Filelog Receiver) to tail Falco’s output and forward to your observability backend:[INPUT]
    Name              tail
    Path              /var/run/falco/events.log
    Parser            json
    Tag               falco.events
    Refresh_Interval  5

[FILTER]
    Name    modify
    Match   falco.events
    Add     service.name falco
    Add     signal.type security

[OUTPUT]
    Name          forward
    Match         falco.events
    Host          otel-gateway.observability.svc.cluster.local
    Port          24224
Falco will now send alerts as JSON logs. Each alert includes pod name, namespace, process details, and the rule that triggered.Tuning Falco rules is critical. Out of the box, you’ll get alerts for legitimate admin activity. Create a custom rules file to suppress expected behavior:- rule: Terminal shell in container
  desc: A shell was spawned in a container
  condition: >
    spawned_process and container and 
    shell_procs and proc.tty != 0 and 
    not user_known_terminal_shell_activity    
  output: >
    Shell spawned in container (user=%user.name container=%container.name 
    shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)    
  priority: WARNING

- macro: user_known_terminal_shell_activity
  condition: >
    (container.image.repository = "my-debug-image") or
    (k8s.ns.name = "development" and user.name = "admin@example.com")    
This custom rule allows shells in debug images and development namespaces while alerting on everything else. Start with WARNING priority, review alerts weekly, and gradually tighten rules as you understand normal behavior.Correlating security events with application tracesCorrelation requires shared context: , , , . When your application logs include trace IDs and your security logs (audit + Falco) include the same pod/namespace/service metadata, you can navigate from a suspicious API call to the exact request that caused it.Here’s the key insight: use the OpenTelemetry Collector’s  processor to enrich all signals with Kubernetes metadata, then ensure applications inject trace context into every log line.Ensure the OpenTelemetry Collector has RBAC permissions:apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: observability
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
- apiGroups: [""]
  resources: ["pods", "namespaces", "nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["replicasets", "deployments"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: observability
Then configure the k8sattributes processor and ensure the Collector receives Fluent Bit’s forward output on port 24224:receivers:
  otlp:
    protocols: {grpc: {}, http: {}}
  fluentforward:
    endpoint: 0.0.0.0:24224

processors:
  k8sattributes:
    auth_type: serviceAccount
    extract:
      metadata: [k8s.namespace.name, k8s.pod.name, k8s.deployment.name, k8s.node.name]
      annotations:
        - tag_name: team
          key: team
          from: pod
      labels:
        - tag_name: app
          key: app
          from: pod

  # Add security-specific attributes
  attributes/security:
    actions:
      - key: signal.type
        value: security
        action: insert
      - key: is_security_event
        value: true
        action: insert

service:
  pipelines:
    logs:
      receivers: [fluentforward, otlp]
      processors: [k8sattributes, attributes/security, batch]
      exporters: [loki]
Now every security log (audit events, Falco alerts) gets enriched with pod name, namespace, deployment, and custom labels. When you query Loki for security events, you can filter by k8s.namespace.name="production" and  to see only production security logs.With K8s metadata enrichment in place, you can now correlate security events with application traces. When suspicious behavior occurs, jump directly from the Falco alert to the trace showing the full request context.This requires injecting  into application logs. Not all security events will have trace IDs (e.g., direct  commands), but application-triggered events should.Building a security observability dashboardRaw logs and traces are useful for investigation, but you need high-level dashboards that show security posture and alert on anomalies.Here’s what makes sense in a Grafana security dashboard:API request rate by user/namespace/verbFailed authentication attempts over timeSecret access events (who accessed which secrets)RBAC change events (role bindings created/deleted)Pod creation events with privileged specsAlert rate by priority (INFO/WARNING/CRITICAL)Alert count by pod/namespaceShell spawn events in production namespacesRecent security events with links to tracesAnomaly detection (API request rates outside normal range)Use Loki queries to extract metrics from security logs:# Count secret access events per user
count_over_time({service_name="kubernetes-audit"} | json | objectRef_resource="secrets" [5m]) by (user_username)

# Count Falco alerts by priority
count_over_time({service_name="falco"} | json | priority="CRITICAL" [5m])

# Failed authentication attempts
count_over_time({service_name="kubernetes-audit"} | json | verb="create" | responseStatus_code >= 400 [5m])
Add a correlation panel that shows recent security events with drill-down links. When an alert fires, you should be able to click through to the audit log, see the associated Falco alert if any, and jump to the application trace if the event came from an app request.Retention policies for security logsSecurity logs have different retention requirements than application logs:: 1 year minimum for most compliance frameworks. PCI-DSS requires 1 year with the last 90 days immediately available. HIPAA requires 6 years for documentation, which many organizations apply to audit logs as well. These are your legal and compliance record of who did what and when.: 30-90 days is typical. You need enough history to investigate incidents and establish baseline behavior patterns, but runtime alerts are less critical for compliance than audit logs.: 7-30 days given the massive volume. Keep longer retention only for compliance-required namespaces or use sampling to reduce volume.Consider tiered storage in Loki: recent data (last 7 days) in fast storage for active investigation, older data in object storage for compliance queries. Set up log lifecycle policies to automatically expire logs based on these retention requirements. Budget for storage accordingly—audit logs and network flows can easily reach terabytes per year in production clusters.Operationalizing security observabilitySecurity observability fails when it becomes another tool nobody checks. You need to integrate it into on-call workflows and incident response runbooks.Here are approaches that work well:Include security signals in standard dashboards. Don’t isolate security metrics in a separate dashboard that only the security team sees. Add a “Security Events” panel to the main application dashboard. When developers see their service triggering Falco alerts, they investigate.Automate correlation in alerts. When a Falco alert fires, include the pod name and namespace in the alert. Add a link directly to the Loki query that shows related audit logs. Include the Grafana Explore URL with pre-filled filters.Make security logs accessible to developers. Grant read access to audit logs and Falco alerts in Loki. Developers should be able to query “Which pods in my namespace accessed secrets today?” without filing a ticket.Test your setup with attack simulations. Simulate privilege escalation and container escape attempts in a test environment. Verify that your dashboards show the activity and alerts fire. This builds confidence and identifies gaps before real incidents happen.Extending to network security observabilityAudit logs and runtime alerts cover control plane and process behavior. But network traffic is another attack vector. Unexpected egress traffic, lateral movement between pods, data exfiltration attempts. You need network flow visibility.Kubernetes Network Policies define allowed traffic, but they don’t give you observability into actual traffic. You need flow logs.Tools like Cilium (with Hubble) or Calico (with flow logs) export network flow data. These can feed into your observability pipeline as metrics or logs.Cilium Hubble exposes flow logs to files, which you can then forward to your observability pipeline. The Cilium Hubble documentation covers flow export configuration and filtering options. Configure Hubble to export flows to a file:helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --set hubble.enabled=true \
  --set hubble.export.static.enabled=true \
  --set hubble.export.static.filePath=/var/run/cilium/hubble/events.log \
  --set "hubble.export.static.fieldMask={time,source.namespace,source.pod_name,destination.namespace,destination.pod_name,verdict,l4,IP}"
Then configure Fluent Bit to tail the Hubble flow logs and forward them to OpenTelemetry:[INPUT]
    Name              tail
    Path              /var/run/cilium/hubble/events.log
    Parser            json
    Tag               hubble.flows
    Refresh_Interval  5
    Mem_Buf_Limit     50MB

[FILTER]
    Name    modify
    Match   hubble.flows
    Add     service.name cilium-hubble
    Add     signal.type network

[OUTPUT]
    Name          forward
    Match         hubble.flows
    Host          otel-gateway.observability.svc.cluster.local
    Port          24224
Network flows include source/dest pod, namespace, ports, protocols, verdict (allowed/denied). You can build dashboards showing denied connections (potential policy violations), unexpected egress destinations (possible data exfiltration), and high-volume pod-to-pod traffic (lateral movement).: Network flow logging generates massive data volume. A large production cluster can produce tens to hundreds of gigabytes of flow logs daily, depending on workload patterns. Every TCP connection, every DNS query, every service-to-service call creates a flow record.Use aggressive filtering with Hubble’s  and  to focus on security-relevant flows (denied connections, external egress, cross-namespace traffic) and exclude high-volume internal service mesh traffic. Consider sampling for non-compliance workloads.For most teams, enabling flow logging selectively for production namespaces or during incident investigation is more practical than continuous full-cluster flow capture.What you can actually build with thisInvestigation speed increases dramatically. Tracking down who modified a ClusterRole binding with kubectl and grep can take anywhere from minutes to hours, depending on what logs you have. With audit logs in Loki filtered by {service_name="kubernetes-audit"} | json | objectRef_resource="clusterrolebindings" | verb="update", you get the answer in seconds. User name, timestamp, source IP, the exact change. Done.See the full attack chain. Audit logs show the API calls (listing secrets, creating pods). Falco catches the shell spawn. You see the complete sequence of events instead of isolated alerts.Compliance audits get faster. “Show me all secret access in Q3 for PCI-scoped namespaces” can take hours of manual reconstruction if logs are scattered across different systems. With Loki, it’s a single query with CSV export. Done in minutes.Alert fatigue reduces when you have context. A Falco alert fires for shell activity in production. Is it an attack or someone running  to debug? With correlation, you see the audit log showing which user ran the exec command, their role bindings, and whether it aligns with normal behavior patterns. Real incidents stand out because you can filter out expected activity.This doesn’t replace preventive controls. You still need vulnerability scanning, Pod Security Standards, network policies, and the practices from Shift Left Security. But when those controls fail and an incident happens, correlated security observability changes investigation time from hours to minutes.The goal isn’t perfect visibility. It’s actionable visibility. Can you answer “What happened?” when an alert fires? Can you trace a security event back to the request that caused it? If yes, you have enough. If not, add the missing signal.This post covered getting security signals into your observability pipeline and correlating them. The next one explores where this is heading—eBPF-native approaches, AI-assisted investigation, and the convergence of security and platform observability.The open-source approach gives you full control and flexibility, but requires ongoing maintenance. Enterprise platforms bundle these capabilities with managed infrastructure, pre-built dashboards, and support.If you’re looking at commercial options, consider Kubernetes-native platforms (Sysdig Secure, Aqua Security, Prisma Cloud), cloud provider tools (AWS GuardDuty, Google Cloud Security Command Center, Azure Defender), or SIEM platforms with Kubernetes integrations (Elastic Security, Datadog Security Monitoring, Sumo Logic). Many teams use a mix: cloud provider tools for basic monitoring, open-source for custom correlation and deep investigation, and SIEM when compliance requires centralized reporting.]]></content:encoded></item><item><title>Engineers are Building the Hottest Geothermal Power Plant on Earth - Next to a US Volcano</title><link>https://hardware.slashdot.org/story/25/11/22/0547231/engineers-are-building-the-hottest-geothermal-power-plant-on-earth---next-to-a-us-volcano?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["On the slopes of an Oregon volcano, engineers are building the hottest geothermal power plant on Earth," reports the Washington Post:

The plant will tap into the infernal energy of Newberry Volcano, "one of the largest and most hazardous active volcanoes in the United States," according to the U.S. Geological Survey. It has already reached temperatures of 629 degrees Fahrenheit, making it one of the hottest geothermal sites in the world, and next year it will start selling electricity to nearby homes and businesses. But the start-up behind the project, Mazama Energy, wants to crank the temperature even higher — north of 750 degrees — and become the first to make electricity from what industry insiders call "superhot rock." Enthusiasts say that could usher in a new era of geothermal power, transforming the always-on clean energy source from a minor player to a major force in the world's electricity systems. 

"Geothermal has been mostly inconsequential," said Vinod Khosla, a venture capitalist and one of Mazama Energy's biggest financial backers. "To do consequential geothermal that matters at the scale of tens or hundreds of gigawatts for the country, and many times that globally, you really need to solve these high temperatures." Today, geothermal produces less than 1 percent of the world's electricity. But tapping into superhot rock, along with other technological advances, could boost that share to 8 percent by 2050, according to the International Energy Agency (IEA). Geothermal using superhot temperatures could theoretically generate 150 times more electricity than the world uses, according to the IEA. "We believe this is the most direct path to driving down the cost of geothermal and making it possible across the globe," said Terra Rogers, program director for superhot rock geothermal at the Clean Air Task Force, an environmentalist think tank. "The [technological] gaps are within reason. These are engineering iterations, not breakthroughs." 

The Newberry Volcano project combines two big trends that could make geothermal energy cheaper and more widely available. First, Mazama Energy is bringing its own water to the volcano, using a method called "enhanced geothermal energy"... [O]ver the past few decades, pioneering projects have started to make energy from hot dry rocks by cracking the stone and pumping in water to make steam, borrowing fracking techniques developed by the oil and gas industry... The Newberry project also taps into hotter rock than any previous enhanced geothermal project. But even Newberry's 629 degrees fall short of the superhot threshold of 705 degrees or above. At that temperature, and under a lot of pressure, water becomes "supercritical" and starts acting like something between a liquid and a gas. Supercritical water holds lots of heat like a liquid, but it flows with the ease of a gas — combining the best of both worlds for generating electricity... [Sriram Vasantharajan, Mazama's CEO] said Mazama will dig new wells to reach temperatures above 750 degrees next year. Alongside an active volcano, the company expects to hit that temperature less than three miles beneath the surface. But elsewhere, geothermal developers might have to dig as deep as 12 miles. 

While Mazama plans to generate 15 megawatts of electricity next year, it hopes to eventually increase that to 200 megawatts. (And the company's CEO said it could theoretically generate five gigawatts of power.) 

But more importantly, successful projects "motivate other players to get into the market," according to a senior geothermal research analyst at energy consultancy Wood Mackenzie, who predicted "a ripple effect," to the Washington Post where "we'll start seeing more companies get the financial support to kick off their own pilots."]]></content:encoded></item><item><title>I Made It Onto the Front Page of Hacker News and My Server Didn&apos;t Crash At All: Here&apos;s Why</title><link>https://hackernoon.com/i-made-it-onto-the-front-page-of-hacker-news-and-my-server-didnt-crash-at-all-heres-why?source=rss</link><author>Daniel, Andrei-Daniel Petrica</author><category>tech</category><pubDate>Sun, 23 Nov 2025 15:29:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mutuum Finance Presale Sells Out 95% of Phase 6 as 18,000+ Investors Join, V1 Testnet Launch Coming</title><link>https://hackernoon.com/mutuum-finance-presale-sells-out-95percent-of-phase-6-as-18000-investors-join-v1-testnet-launch-coming?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Sun, 23 Nov 2025 15:28:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As crypto markets continue to search for the best crypto to buy,  is rapidly emerging as the best crypto opportunity for early investors. The project’s Phase 6 presale is now 95% sold out, drawing over 18,120 investors and raising nearly $19 million, signaling massive early-stage demand. Mutuum Finance is a cutting-edge DeFi platform offering decentralized lending and borrowing with interest-bearing tokens and rewards for early adopters. With the V1 testnet launch approaching, the project is set to deliver real utility while giving presale participants a front-row seat to its early ecosystem adoption. This combination of rapid presale growth, strong community backing, and functional DeFi infrastructure positions Mutuum Finance as the best crypto to buy in the market today, capturing attention from traders seeking high-potential, next-generation crypto projects.Phase 6: Last-Chance Entry for High-Potential InvestorsMutuum Finance (MUTM) is rapidly emerging as the most promising early-stage crypto project, offering significant potential returns for strategic investors. Over the past months, the platform has successfully progressed through multiple critical development phases, earning strong community support and investor confidence. Highlights include a highly successful presale, which attracted over 18,120 participants and raised more than $18.85 million.Currently, MUTM tokens are available in Phase 6 at $0.035 per token. However, with over 95% of this phase already sold, the remaining supply is rapidly diminishing. Investors looking to secure tokens at this price point have a limited window of opportunity before Phase 7 begins, when the price is set to increase by 20% to $0.04 per token. This combination of strong presale performance, high demand, and a limited-time price opportunity makes MUTM a standout DeFi crypto investment and positions it as the best crypto for early adopters seeking maximum upside.MUTM: Could It Replicate Solana’s Historic Surge?Currently trading at $0.035, MUTM presents an opportunity reminiscent of Solana’s 2021 bull run, when the token soared from $1.50 to an all-time high of $256, delivering an astonishing 17,100% ROI. If MUTM were to follow a similar growth trajectory, its price could reach approximately $6.00. A $1,000 investment today would secure around 28,571 MUTM tokens, which could potentially be valued at $171,000 at that price, representing a net gain of over $170,000.Such exponential growth would be underpinned by MUTM’s dual-lending framework and comprehensive DeFi ecosystem, combining real-world utility with a scalable and innovative financial platform. Analysts increasingly identify MUTM as a potential breakout cryptocurrency, making it one of the top cryptocurrencies poised for rapid growth and a promising option for investors seeking DeFi projects with practical utility.Sepolia Testnet: A Sneak Peek into Mutuum’s Lending and Borrowing ProtocolA major milestone for Mutuum Finance is the launch of the  on the Sepolia testnet, anticipated in Q4 2025. This testnet will serve as the first full-scale deployment of the platform’s lending and borrowing system, allowing users to lock collateral in ETH or USD and borrow funds against it.Beyond providing early access to users, the Sepolia testnet will give the development team an opportunity to refine key features, including risk management tools, dynamic interest rate models, and loan processing mechanisms. These improvements ensure the platform operates with maximum security, efficiency, and readiness ahead of the full mainnet launch.Mutuum Finance (MUTM) has raised nearly $19 million, with over 18,120 investors and Phase 6 now 95% sold out at $0.035 per token. Phase 7 will increase the price to $0.04, making this a critical window for early-stage participation. With the upcoming V1 Sepolia testnet, a dual-lending DeFi framework, and strong community backing, MUTM combines real utility with explosive growth potential. Secure your tokens today to join the best crypto and position yourself for substantial upside before the next presale phase, making it the best crypto to buy for 2025.For more information about Mutuum Finance (MUTM) visit the links below::::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging . Do Your Own Research before making any financial decision.]]></content:encoded></item><item><title>The Next AI Race Will Start at the Application Layer</title><link>https://hackernoon.com/the-next-ai-race-will-start-at-the-application-layer?source=rss</link><author>tyingshoelaces.com</author><category>tech</category><pubDate>Sun, 23 Nov 2025 14:56:47 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[For the past several years, the artificial intelligence landscape has sold a story of a high-stakes arms race. The logic was simple: bigger models and more data would pave the road to true intelligence. But this narrative, while compelling, misses the ground truth that engineers have been living. The real story isn't a glamorous race to the top; it's been a grueling, frustrating slog out of the mud.That slog is finally over. The scaling race didn't end because someone won; it ended because we finally reached a reliable starting line. The foundational models are, at last, good enough. And now, the real work—the real innovation—can begin. The defensible moat has moved decisively up the stack to the application layer, and the new metrics for success have nothing to do with parameter counts. They're about cost, speed, and creative problem-solving.Chapter 1: The Age of ScaffoldingIt’s easy to forget what the engineering reality was like just a short time ago. The models, frankly, just didn't work. Not in a reliable, production-ready sense. The daily battle wasn't about fine-tuning for subtle improvements; it was a desperate struggle to compensate for fundamental brittleness.We were living in the "Age of Scaffolding." Our primary role was building elaborate, multi-layered error-checking and correction systems around a fragile model core just to coax a usable, predictable output from it. I recall one project where our goal was to extract structured data from user requests. The model would fail so spectacularly and unpredictably that our solution became a comical Rube Goldberg machine of prompts.The first prompt would ask the model to identify the user's intent. The second prompt would take that intent and the original text, asking the model to extract key entities. But the model would often hallucinate entities or return malformed JSON. So, a third prompt was needed. This one was a "cleanup" prompt: it took the broken JSON from the previous step and, with heavily constrained instructions, tried to fix it. We were literally , chaining prompts together just to achieve a single logical task. One particularly memorable bug involved the model deciding to return a beautifully formatted, completely valid JSON object that was, however, entirely unrelated to the input text, requiring yet another validation layer to check for semantic relevance.In that environment, a "win" wasn't a breakthrough in AI capability. A win was a . It was getting through a full process without a catastrophic failure. We spent the vast majority of our engineering cycles not on creating value, but on managing failure. This was the scaling grind in practice: an immense effort just to reach a baseline of bare-minimum functionality.Chapter 2: The Phase ChangeThen, everything changed. The arrival of models like GPT-4 and, more recently, Claude 3.5, marked a true inflection point. It wasn't just another incremental step up the leaderboard. It was a phase change. Suddenly, the foundation was solid. The core "brain" became reliable, capable, and, most importantly, predictable.This shift did more than just improve model outputs; it fundamentally altered the structure of our teams and the nature of our work. The need for elaborate, defensive scaffolding began to melt away. Roadmaps that were once filled with tickets like "Improve JSON output reliability" could now be filled with tickets like "Build new agentic workflow for customer support." The percentage of our time spent on "model-proofing" our code dropped from an estimated 80% to less than 20%.The liberation of engineering creativity from the prison of model unreliability was the true catalyst for the Application Age. When you no longer have to spend the majority of your time wrestling the model into submission, you can start asking a much more powerful question: "What can we build with this?"Chapter 3: The New Physics of AIToday, we live in a different world. For a vast majority of use cases, the top-tier models from Google, OpenAI, Anthropic, and others are  The qualitative difference in output for most common tasks is marginal. This is the hallmark of a maturing, commoditized technology. When core functionality is a given, the competitive battleground shifts entirely to the operational realities of deploying it at scale.3a. The Economics of Intelligence The primary concern is now . When you're running millions of inferences a day, a fraction of a cent difference per token determines the economic viability of your entire product. This has given rise to sophisticated strategies like "model routing" or "cascading."For example, a user request might first be sent to a very fast, cheap model like Claude 3 Haiku. If that model can handle the request with sufficient quality (a determination often made by another small, fast model), the process ends there, at a minimal cost. If the model fails or indicates low confidence, the request is then "cascaded" up to a more powerful, and expensive, model like GPT-4o. This allows for optimizing cost on a per-query basis, a level of financial engineering that was irrelevant when the only goal was getting a single model to work at all.3b. The User Experience of Speed The second pillar is . Latency is a user experience killer. The perceived intelligence of a system is directly tied to its responsiveness. A brilliant answer that takes ten seconds to generate feels less useful than a good-enough answer that appears instantly.This has led to a fascinating trade-off space. In a recent project, we were building a real-time coding assistant. We had two choices: use our most powerful model, which provided incredibly insightful suggestions but had a high "time-to-first-token," creating a noticeable lag, or use a smaller, fine-tuned model that was 80% as "smart" but delivered its suggestions almost instantly. We chose speed. The feeling of a seamless, responsive interaction was more valuable to the user than the marginal increase in code quality from the slower model.Chapter 4: Where Value is Built NowWith cost and speed as the new constraints, the patterns for building successful, defensible AI businesses have become clear. The value is not in the model, but in the system built around it. We are seeing three dominant patterns emerge: These companies deeply integrate AI into a specific professional workflow, becoming an indispensable tool.  for law is the canonical example. They are not selling a generic LLM; they are selling a "legal co-pilot" that understands the specific tasks, documents, and needs of a lawyer. Their moat is the deep domain expertise encoded in their application logic. These are systems that automate complex, multi-step tasks by chaining model calls and tools together. The value is in the orchestration layer that can reliably plan and execute toward a goal. This is where the true promise of automation lies, moving beyond simple text generation to active problem-solving. The key challenge and source of differentiation here is in reliability and state management. Companies like  are creating novel, AI-native user experiences that are fundamentally different from traditional search or chat. Their interface is the product, providing a new way to access and synthesize information that is more valuable than the underlying models they use.Chapter 5: The AI Engineer, ReimaginedThis new landscape demands a new kind of engineer. The skills that were paramount just a few years ago—like the arcane art of prompt engineering or the intricacies of tuning training hyperparameters—are becoming less critical. The most valuable AI engineers today are not model whisperers; they are product-minded system builders.My advice to a young engineer starting today would be this: Don't obsess over the internal mechanics of the latest model. Instead, get exceptionally good at building systems around them. Key skills for the Application Age include:API Integration & Orchestration: The ability to effectively use tools like LangChain or build custom frameworks to chain tools, databases, and model calls together.Cost & Latency Optimization: Deeply understanding the trade-offs of different models and implementing strategies like model cascading. Designing reliable systems for long-running, multi-step agentic tasks. Collaborating with designers to build intuitive interfaces for non-deterministic systems.Chapter 6: Second-Order Effects and the Road AheadThe commoditization of intelligence will have profound second-order effects. When every developer has access to a super-powerful, low-cost "brain" via an API call, it fundamentally changes what can be built. We will see a Cambrian explosion of new companies in fields previously untouched by software because the cost of building intelligent features was too high.This shift also democratizes innovation. A small, agile team can now create a product with a level of sophistication that would have required a massive, dedicated research division just five years ago. The competitive advantage will go to those with the deepest understanding of a user's problem, not those with the largest GPU cluster.The engineering challenge has transformed. We've moved from the brute-force problem of taming unreliable models to the far more interesting and creative challenge of designing products in a world of abundant, cheap, and fast intelligence. The foundational models are here. They work. They aren't AGI, but they are a permanent and transformative new layer of the technology stack. The focus is no longer on the raw materials, but on the art of manufacturing.The scaling race is over. The application race has just begun.Now, what will you build on top of them?]]></content:encoded></item><item><title>A Practical Guide to Prompt Engineering for Today’s LLMs</title><link>https://hackernoon.com/a-practical-guide-to-prompt-engineering-for-todays-llms?source=rss</link><author>Nick Talwar</author><category>tech</category><pubDate>Sun, 23 Nov 2025 14:51:31 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The overlooked communication skill that decides whether or not AI performs at scale.Prompt engineering is both creative and precise. Good prompts come from clear intent, structured testing, and constant refinement through an organized engineering and testing process.Early work with large language models depended on trial and error. Today, prompting has evolved into a professional skill.Prompting now requires the same structured thinking you’d use to design any system. You need to understand how models interpret language and how to express intent in a way they can follow.Strong prompt engineers think in steps, measure results, track changes, A/B test, and improve over time. The more precise the instruction, the more consistent the outcome.I’ve spent over 15 years building AI and machine learning systems for startups and global enterprises. My work began at Microsoft, where I focused on large-scale recommendation systems and search algorithms to serve hundreds of millions of customers at scaleIn this blog, I’ll share the practical methods I use to design, test, and refine prompts that consistently deliver accurate and useful outputs.Core Techniques for Better ResultsThe fundamentals of effective prompting apply across industries. These techniques provide control, accuracy, and repeatability.. Define the model’s role clearly, such as strategist, researcher, or analyst, with clear characteristics. Context shapes focus and improves accuracy.. Set boundaries for tone, format, and length. Clear limits reduce ambiguity and guide responses.. Break tasks into defined steps or sections. This improves the model's logic and helps it handle complex instructions.. Include sample outputs that demonstrate what good performance looks like. Examples teach tone and precision faster than written explanation.They also show the format in which you would like the output to adhere, which is very important as LLMs can play “jazz” more often than not and deliver responses in formats you would not expect.Each of these methods supports consistency and efficiency. Together, they create a foundation for reliable, repeatable AI results.Once the basics are in place, advanced prompting techniques help the model reason and perform more effectively.Chain of Thought Prompting. Encourage the model to outline its reasoning process step by step. This approach improves accuracy and transparency and provides a lens into how the response was put together, a key necessity for auditability and long-term maintainability.Tree of Thought Prompting. Ask the model to explore several reasoning paths before selecting the best one. This strengthens analysis and creativity simultaneously, an often overlooked method to ensure that responses cover their bases and iterate through multiple perspectives before the LLM lands on what it believes to be the best.. Link prompts together so that each output becomes input for the next step. This structure is useful for multi-stage tasks and processes that require strict adherence plus compliance checks at each step before moving on to the next. Include factual data or contextual details to ground the model’s reasoning. This reduces error and strengthens credibility.. When performance stalls, you can use tools like NotebookLM, which uses the latest Google Gemini models to review all prompts together and refine the prompt itself.NotebookLM and other project-based LLM tools that allow for multiple files to be uploaded and reviewed can often identify structural or phrasing improvements.These methods move prompting beyond surface-level interaction. They help create reasoning frameworks that scale to complex challenges.Coupled with a regular, iterative auditing process, perhaps even using GitHub for change tracking, these strategies turn the “black box” of prompting from magic into something more organized and predictable, with better, more accurate outputs from LLMs.Prompt engineering works best when it focuses on clarity and oversight.LLMs simulate reasoning by pattern-matching in data. They require review and context to ensure accuracy.Strong prompts resemble concise professional briefs. They communicate intent clearly and efficiently. Prompting rewards discipline. The more direct the instruction, the more consistent the output.With that said, examples or templates in the prompt need not be concise, as context windows are extremely large. Do not hesitate to provide a ten or twenty-page example output of a canonical work product to help guide the LLM as a North Star with key details.The fundamentals of prompt engineering remain constant, even as AI technology evolves. To achieve consistent and scalable AI outcomes, focus on three key principles: clarity, structure, and consistency.Clarity is essential for generating accurate and actionable results. When prompts are unclear or ambiguous, the AI's responses will reflect that, potentially leading to wasted effort.A precise prompt with key examples, no matter how long, is critical for ensuring the AI delivers what is needed.Remember, LLMs gain clarity via context, and providing more of it, within reason, can help support a more consistent, predictable, and accurate implementation.Structure is equally important. A well-organized prompt improves the AI’s ability to deliver reliable, relevant outputs. Whether you're implementing AI in customer service or operational tasks, structured prompts reduce the risk of errors and improve efficiency.Consistency matters when scaling AI solutions. Keeping prompts clear and structured across the board allows the AI to adapt and perform consistently, even as business needs evolve. It is vital to ensure that the AI remains effective as it scales.Treat prompt engineering as an ongoing process. Regular refinement ensures that AI systems stay aligned with business goals and continue to evolve with technological advances.Ensure that your teams have a process and system in place to regularly QA test and iterate, plus audit prompts with a detailed change log. Without it, you may easily regress or reintroduce past LLM foibles into production.Prompting is at the heart of how humans collaborate with AI. Well-crafted prompts guide AI to achieve business objectives efficiently, turning AI into a valuable tool rather than just a quick solution.Effective AI use starts with a clear understanding of the desired outcome. Define key goals and nuances, and share your key perspective on the task upfront to ensure the AI aligns with business needs. Remember, LLMs are pattern-matching engines across a vast web of human knowledge.Think of it as guiding a precocious student towards an appropriate area of the library so they can look in the right place. Your perspective and professional opinion ground this and ensure the LLM constantly searches in the correct space.Testing the AI regularly is essential. By evaluating its performance, you can identify areas for improvement and make adjustments to improve outcomes. This process ensures that the AI remains reliable and effective over time.AI implementations, from the most sophisticated to simple prompting, must be refined continuously. As business priorities shift, so should the prompts.Ongoing refinement guarantees that the AI continues to meet evolving needs and delivers real, sustained value.Without it, your outputs will drift, miss expectations, and even embarrass your team.]]></content:encoded></item><item><title>Inside Ethereum’s Fusaka Hard Fork: PeerDAS, New Gas Limits, and the Road to Cheaper L2s</title><link>https://hackernoon.com/inside-ethereums-fusaka-hard-fork-peerdas-new-gas-limits-and-the-road-to-cheaper-l2s?source=rss</link><author>Sahil Sojitra</author><category>tech</category><pubDate>Sun, 23 Nov 2025 14:41:37 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The , scheduled for activation on , is Ethereum’s next major network upgrade after Pectra, and it marks one more scaling step taken by the crypto giant. The  Pectra EIPs focus on improving performance, security, and developer tools. PeerDAS () makes data availability more efficient by letting nodes verify blobs without downloading everything. Several upgrades tighten execution safety, including limits on ModExp (), a cap on transaction gas limits (), and updated ModExp gas costs (). The fork also improves block production with deterministic proposer lookahead () and keeps blob fees stable with bounds tied to execution costs (). Additional enhancements include limiting block size in RLP format (), adding a new CLZ opcode for faster bit operations (), and introducing a secp256r1 precompile () for better compatibility with modern cryptography and hardware security keys.Just as Pectra combined “Prague” and “Electra,” Fusaka is a combined name of  and . It represents Ethereum’s next leap toward a highly scalable, data-rich future where Layer 2 rollups can operate cheaper and faster.In this blog, we’ll break down the core features of the Fusaka hard fork, explain how PeerDAS works in practice, and walk through the real network requirements for home stakers, supernodes, and full nodes — backed by devnet data.Key Changes Introduced In Fusaka Hard ForkEIP-7594: PeerDAS - Peer Data Availability SamplingEthereum needed this proposal because the network wants to provide more data availability for users and especially for rollups, but with the current  design, every node still has to download too much blob data just to check that it was actually published. This creates a scaling problem because if all nodes must download everything, the network’s bandwidth and hardware requirements go up, and decentralization can suffer. To fix this, Ethereum needs a way for nodes to confirm that data is available without downloading all of it. Data Availability Sampling (DAS) solves this by letting nodes check only small random pieces of the data. But Ethereum also needs a DAS method that fits well with the existing gossip network and doesn’t add heavy computation to block producers. PeerDAS was created to meet these needs and safely increase blob throughput while keeping node requirements low.PeerDAS is a networking system that lets nodes download only small pieces of blob data to check that the full data was actually published. Instead of downloading everything, nodes use the normal gossip network to share data, discover which peers hold certain parts, and request just the small samples they need. The main idea is that by downloading only tiny, random parts of the blob, nodes can still be confident that the entire blob exists. For example, instead of downloading a full 256 KB blob, a node might download only about 1/8 of it—but because many nodes sample different parts, any missing data would quickly be noticed.To make sampling work, PeerDAS expands each blob from  using a basic type of erasure coding. Erasure coding is a technique that adds extra redundant data so that the original data can be recovered even if some pieces are missing—similar to how you can complete a puzzle even if a few pieces are lost. The blob becomes a “row” that contains the original data plus some extra coded data that allows it to be rebuilt later. This row is then split into many small pieces called cells, which are the smallest verified units tied to a KZG commitment. All rows are then reorganized into “columns,” where each column contains the cell at the same position from every row. Each column is assigned to a specific gossip subnet. Nodes are responsible for storing certain columns based on their node ID and for sampling a few columns from peers every slot. If a node collects at least 50% of all columns, it can fully rebuild the data. If it has less than 50%, it simply requests the missing columns from peers. This ensures that if the data was actually published, it can always be reconstructed. In short, if there are 64 columns in total, a node only needs about 32 of them to rebuild the full blob. It keeps some columns itself and downloads a few from peers. As long as half the columns exist in the network, the node can reconstruct everything—even if some columns are missing.Additionally, the EIP introduces an important rule: no transaction can contain more than 6 blobs. This limit must be enforced during transaction validation, gossip, block creation, and block processing. This helps reduce extreme cases where a single transaction overloads the blob system.PeerDAS adds something called . A cell KZG proof shows that a KZG commitment really matches one specific cell (one small piece) of a blob. This lets a node download only the cells it wants to sample, instead of the full blob, while still guaranteeing data integrity. This is essential for data availability sampling.But generating all these cell proofs is expensive. A block producer would need to compute them again and again for many blobs, which is too slow. Proof verification, however, is very cheap. So the EIP requires the blob transaction sender to generate all the cell proofs ahead of time and include them in the transaction wrapper.Because of this, the transaction gossip (PooledTransactions) now uses a modified wrapper:rlp([tx_payload_body, wrapper_version, blobs, commitments, cell_proofs])
Inside the new wrapper,  is just a list that contains every proof for every cell of every blob (for example: [cell_proof_0, cell_proof_1, ...]). The other fields — , , and  — are exactly the same as they were in EIP-4844. The difference is that the old single “proofs” field is removed and replaced with this new  list, and a new field called  is added to show which wrapper format is being used.The  is a single byte, and in this proposal it is always set to . The  field contains all of the cell proofs for each blob, including the proofs for the extra extension cells created during erasure coding. Because of the erasure coding, each blob corresponds to  cell proofs. Even though the list contains proofs for the extension cells, the blobs themselves are still sent in their normal (non-extended) form. A blob only includes its original half—the receiving node can compute the extension half on its own, so there’s no need to send unnecessary data.More formally, cell_proofs[i * CELLS_PER_EXT_BLOB + j] is the proof for the j-th cell of the i-th blob after running , which produces both the original and extension cells.When a node receives this wrapper, it must validate the transaction and check that everything lines up correctly. It must verify that the number of versioned hashes, blobs, and commitments all match. It must also confirm that  contains exactly CELLS_PER_EXT_BLOB * number_of_blobs proofs. Every commitment must hash to its matching versioned hash (kzg_to_versioned_hash(commitments[i]) == tx_payload_body.blob_versioned_hashes[i]). Finally, the node must check that each commitment actually matches the blob and its proofs. To do this, it computes the extension cells locally using , and then verifies all the cell proofs—batch verification is allowed to make this faster.: Imagine a blob is split into  after erasure coding (). The original blob contains , and the receiving node will compute the missing  itself. But the sender must include  in . So for 1 blob, the transaction wrapper contains: (only original 4 cells)commitments = [commitment_0]cell_proofs = [proof_0, proof_1, …, proof_7] (proof for each of the 8 cells)When a node samples, it might request only cell 2 or cell 5, and it can verify them instantly using the matching proofs, without ever downloading the full blob.PeerDAS lets Ethereum increase data availability without making nodes work harder. Today, a node only needs to sample about 1/8 of the total blob data. In the future, this might even go down to , which would let Ethereum scale more. The system works well because every node has many peers. So if one peer does not give the required data, the node can simply ask another peer. This creates natural redundancy and improves security. Nodes can also choose to store  data than required, which further strengthens the network—even without any protocol changes.Validators have a bit more responsibility than normal full nodes. Since validators already run stronger hardware, PeerDAS gives them a data-custody load that matches the total number of validators. This ensures that a stable group of nodes is always available to store and share more data, which makes the network more reliable. In short, if there are 900,000 validators, each validator might be assigned a tiny portion of the total blob data to store and serve. Because validators have better machines, the network can trust them to keep this data available.PeerDAS uses  instead of row sampling because it makes rebuilding the data much easier. If nodes sampled full rows (whole blobs), they would need to create extra “extension blobs” that don’t naturally exist, which would slow down block producers. By sampling columns, nodes can prepare the extra row data in advance, and the transaction senders—not the block producers—compute the necessary proofs. This keeps block creation fast and efficient.  Imagine a blob is a 4×4 grid of cells. Row sampling would mean taking all 4 cells from one row, but some extension rows aren’t ready yet, so the block producer would have to generate them on the spot. Column sampling means taking one cell from each row (a column). The extra cells needed for reconstruction can be prepared ahead of time, so nodes can verify the data without slowing down block production. works fully with , so it doesn’t break anything already on Ethereum. All tests and detailed rules are in the consensus and execution specs.The main security risk in any DAS system is a “data withholding attack,” where a block producer pretends that data is available but actually hides some of it. PeerDAS prevents this by using random sampling: nodes check random parts of the data. The more nodes that are sampled, the harder it is for an attacker to cheat. The EIP even provides a formula to calculate how likely such an attack could succeed, based on the total number of nodes (n), total samples (m), and samples per node (k). On the Ethereum mainnet, with about 10,000 nodes, the chance of a successful attack is extremely small, so PeerDAS is considered safe.The table shows that the chances of a successful attack drop to a negligible level, which is why PeerDAS is considered secure against data-withholding attacks. For deeper analysis, you can refer to the linked blog.This proposal is needed because the current MODEXP precompile in Ethereum has caused many consensus bugs over the years. Most of these bugs happened because MODEXP allows extremely large and unrealistic input sizes, which creates endless, unusual cases that clients must handle. Since every node has to process whatever input a transaction provides, having no upper limit makes MODEXP harder to test, easier to break, and more likely to behave differently across different clients. Very large inputs also make the gas-cost formula hard to predict, because it is difficult to price something when its size can grow without limit. These problems also make it difficult to replace MODEXP with future EVM-level code through tools like EVMMAX, because without fixed limits, developers cannot create safe and optimized execution paths. To reduce these issues and make Ethereum more stable, this proposal adds strict maximum sizes to MODEXP inputs so that the precompile becomes safer, easier to test, and more predictable.EIP-7823 introduces a simple rule: all three length fields used by MODEXP—the size of the BASE, the EXPONENT, and the MODULUS—must be , which is 1024 bytes. MODEXP inputs follow the format <len(BASE)> <len(EXPONENT)> <len(MODULUS)> <BASE> <EXPONENT> <MODULUS>, as defined in , so this EIP only restricts the length values. If any length exceeds 1024 bytes, the precompile immediately stops, returns an error, and burns all gas. For example, if someone tries to provide a BASE that is 2000 bytes long, the call will fail before any work happens. These limits still support all real use cases. RSA verification typically uses key sizes like 1024, 2048, or 4096 bits, all well within the new limit. Elliptic curve operations use even smaller sizes, often under 384 bits, so they are also unaffected.These new limits also help with future upgrades. If MODEXP is ever rewritten in EVM code using EVMMAX, developers could add optimized paths for common input sizes like 256 bits, 381 bits, or 2048 bits, and use a slower fallback for rarer cases. With fixed maximum sizes, developers can even add special handling for very common modulus values. None of this was realistic before, because the unlimited input sizes made the design space too large to manage safely.To confirm that this change would not break past transactions, the authors analyzed all MODEXP usage from block 5,472,266 (April 20, 2018) to block 21,550,926 (January 4, 2025). The results show that no successful historical MODEXP call ever used inputs larger than , far below the new 1024-byte limit. Most real calls used small lengths like 32 bytes, 128 bytes, or 256 bytes. There were a few invalid or broken calls, such as empty inputs, inputs filled with repeating bytes like , and one extremely large but invalid input. These would behave the same under the new limits because they were already invalid. So, while the EIP is technically a breaking change, in practice it would not have changed the outcome of any past transaction.From a security point of view, reducing the allowed input sizes does not create new risks. Instead, it removes unnecessary extreme cases that previously caused bugs and inconsistencies across clients. By limiting MODEXP inputs to realistic sizes, this EIP makes the system more predictable, reduces strange corner cases, and lowers the chance of errors between different implementations. These constraints also help prepare the system for a smoother transition if future upgrades like EVMMAX introduce optimized execution paths.Ethereum needed this proposal because today a single transaction can use almost the entire block gas limit. This creates several problems: one transaction could consume most of the block’s resources and cause a DoS-style slowdown, large gas-heavy operations can increase Ethereum’s state too quickly, and block validation becomes slower and harder for nodes to keep up with. If one user submits a huge transaction that uses nearly all the gas (for example, a transaction that consumes 38 million gas in a 40 million–gas block), then other normal transactions cannot fit into the block, and every node must spend extra time verifying that block. This threatens network stability and decentralization because slower verification means weaker nodes fall behind. To solve this, Ethereum needs a safe upper limit on how much gas a single transaction can use, so block load becomes more predictable, DoS risk is reduced, and nodes have a more even workload.EIP-7825 introduces a hard rule that no transaction can use more than . This becomes a protocol-level cap, meaning it applies everywhere: when a user sends a transaction, when the txpool checks it, and when validators include it in a block. If someone sends a transaction with a gasLimit higher than this number, the client must reject it immediately with an error such as . This cap is completely independent of the block gas limit. For example, even if the block gas limit is 40 million, no single transaction is allowed to use above 16.7 million gas. The goal is to make sure that many transactions can fit inside each block instead of letting one transaction dominate the entire block.To understand this better, imagine a block has space for 40 million gas. Without this cap, someone could send a single transaction that consumes 35–40 million gas. That transaction would monopolize the block and leave no room for others, similar to one person renting out an entire bus so no one else can board. With the new 16.7M limit, the block will naturally hold multiple transactions and avoid this kind of abuse.The proposal also adds specific requirements to how clients validate transactions. The txpool must refuse a transaction if its gasLimit is above 16,777,216, meaning such a transaction never even enters the queue. During block validation, if a block contains a transaction that exceeds the cap, the block itself must be rejected. Internally, client implementations will cap the GAS_LIMIT parameter for all transactions at this value.The number  was chosen because it is a clean power-of-two boundary, making it easier to implement, and it is still large enough to handle most real-world transactions such as smart contract deployments, complex DeFi interactions, or multi-step contract calls. This value is roughly half of the typical block size, meaning even the most complex transactions should still fit comfortably within this limit.This EIP also maintains compatibility with current gas mechanics. Most users will never notice the change because nearly all existing transactions already consume far less than 16 million gas. Validators and block builders can still create blocks that exceed 16.7 million total gas, as long as each individual transaction respects the new cap. The only transactions affected are extremely large ones that previously attempted to use more than the new limit. Those transactions must now be split into multiple smaller operations—similar to breaking a very large file upload into two smaller ones. This change is technically not backward-compatible for those rare extreme transactions, but the number of users affected is expected to be very small.In terms of security, the gas cap makes Ethereum more resilient to gas-based DoS attacks because attackers cannot force validators to handle extremely large transactions anymore. It also helps keep block verification times predictable so that nodes can stay in sync more easily. The main edge case is that a few very large contract deployments may not fit under the cap and might need to be redesigned or split into multiple deployment steps.Overall, EIP-7825 is designed to strengthen the network against abuse, keep node requirements reasonable, improve fairness in block space usage, and ensure the chain remains fast and stable as gas limits continue to increase over time.The reason Ethereum needs this proposal is that the ModExp precompile (used for modular exponentiation) has been  compared to the actual resources it consumes. In some situations, ModExp operations require far more computation than what users are currently paying for. This mismatch creates a risk: if complex ModExp calls stay too cheap, they can become a  and make it harder for the network to safely increase the block gas limit, since block producers may be forced to process extremely heavy operations for very little cost. To fix this, Ethereum needs to adjust the ModExp pricing formula so that the gas used properly reflects the real work done by the client. That is why EIP-7883 introduces new rules that increase the minimum cost, raise the general cost, and make operations with large inputs (especially exponents, base, or modulus over 32 bytes) more expensive, so the gas pricing matches the true computation required.This proposal modifies the ModExp pricing algorithm originally defined in EIP-2565 by increasing the cost in several important ways. First, the  is raised from 200 to 500, and the general formula no longer divides by 3, meaning the total cost effectively becomes . For example, if a ModExp call previously cost 1,200 gas, it will now cost around 3,600 gas under the new formula. Second, the cost of operations with exponents larger than 32 bytes is doubled by increasing the multiplier from 8 to 16. To illustrate this: if the exponent length was 40 bytes, EIP-2565 would add  to the iteration count, while EIP-7883 now uses , making it twice as expensive. Third, the pricing now assumes a minimum base/modulus size of 32 bytes and sharply increases the computation cost when these values exceed 32 bytes. For example, if the modulus is 64 bytes, the new rule applies a doubled complexity () instead of the old simpler formula, reflecting the real cost of large-number arithmetic. Together, these changes ensure that small ModExp operations pay a fair minimum fee and large, heavy operations scale their cost properly with size.The proposal defines a new gas calculation function that updates the complexity and iteration count rules. The multiplication complexity now uses a default value of 16 for base/modulus lengths up to 32 bytes, and for larger inputs it switches to the heavier formula , where “words” is the number of 8-byte chunks. The iteration count is also updated so that exponents of 32 bytes or smaller use their bit length to determine complexity, but exponents larger than 32 bytes add a much larger penalty. This ensures that very large exponents, which are computationally expensive in practice, now have a much higher gas cost. Importantly, the minimum returned gas cost is enforced as 500 instead of the earlier 200, making even the simplest ModExp calls more realistically priced.The motivation behind these pricing increases comes from benchmarks showing many situations where the ModExp precompile was significantly underpriced. The revised formula raises small operations by , typical operations by around , and very large or unbalanced operations by much larger factors—sometimes more than —depending on how big the exponent, base, or modulus is. The intent is not to change how ModExp works, but to ensure that even in its most resource-intensive edge cases it no longer threatens network stability or prevents future increases to the block gas limit. Because this EIP changes how much gas ModExp requires, it is , but gas repricing has happened many times before in Ethereum and is well-understood.Test results show how significant the increases can be. About 99.69% of historical ModExp calls will now cost either 500 gas (up from 200) or three times their earlier price. But certain heavy test cases see huge increases. For example, in one “exp-heavy” test, the cost jumps from 215 gas to 16,624 gas—about —because extremely large exponents are now correctly priced. In another case involving a base-heavy operation, the cost goes from 1,026 to 94,448 gas, which reflects how big-number multiplication actually scales. These examples show why the old pricing formula wasn’t realistic and how the new one better matches computational reality.In terms of security, this proposal does not create new attack vectors or make any operation cheaper. Instead, it focuses on preventing an important risk: underpriced ModExp operations could make it possible for attackers to fill blocks with extremely heavy computations for very low cost. The only possible downside is that some ModExp operations might now become overpriced, but this is considered a far better outcome than the current underpricing problem. No interface changes or new functionality are introduced, so existing arithmetic behavior and test vectors remain valid.| Test Case | EIP-2565 Pricing | EIP-7883 Pricing | Increase |
|----|----|----|----|
| modexp1square | 200 | 500 | 150% |
| modexpnagydaniqube | 200 | 500 | 150% |
| modexp1pow0x10001 | 341 | 2048 | 501% |
| modexpnagydanisquare | 200 | 512 | 156% |
| modexp2qube | 200 | 512 | 156% |
| modexpnagydanipow0x10001 | 1365 | 8192 | 501% |
| modexp3square | 341 | 2048 | 501% |
| modexpnagydaniqube | 341 | 2048 | 501% |
| modexp3pow0x10001 | 5461 | 32768 | 500% |
| modexpnagydanisquare | 1365 | 8192 | 501% |
| modexp4qube | 1365 | 8192 | 501% |
| modexpnagydanipow0x10001 | 21845 | 131072 | 500% |
| modexp5square | 5461 | 32768 | 500% |
| modexpnagydaniqube | 5461 | 32768 | 500% |
| modexp5pow0x10001 | 87381 | 524288 | 500% |
| modexpmariuseven | 2057 | 45296 | 2102% |
| modexp1even | 2298 | 51136 | 2125% |
| modexpguidoeven | 2300 | 51152 | 2124% |
| modexp3even | 5400 | 32400 | 500% |
| modexpguidoeven | 1026 | 94448 | 9105% |
| modexp1heavy | 200 | 1152 | 476% |
| modexp1heavy | 215 | 16624 | 7632% |
| modexp1balanced | 200 | 1200 | 500% |
| modexpmarcinbaseheavy | 867 | 5202 | 500% |
| modexpmarcinexpheavy | 852 | 16368 | 1821% |
| modexpmarcinbalanced | 996 | 5978 | 500% |
| modexp3heavy | 677 | 2032 | 200% |
| modexp3heavy | 765 | 4080 | 433% |
| modexp3_balanced | 1360 | 4080 | 200% |                                                                           *source: https://eips.ethereum.org/EIPS/eip-7883#test-cases*
Ethereum needed this proposal because the network’s proposer schedule for the next epoch was not fully predictable. Even though the RANDAO seed for epoch  is known during epoch , the actual proposer list could still change due to effective balance (EB) updates happening inside epoch . These EB changes can come from slashings, penalties, rewards above 1 ETH, validator consolidations, or new deposits—especially after EIP-7251 raised the maximum effective balance beyond 32 ETH. This uncertainty creates issues for systems that rely on knowing the next proposer in advance, such as based preconfirmation protocols, which need a stable and predictable schedule to operate smoothly. A validator could even try to “grind” or manipulate their effective balance to influence who becomes the proposer next epoch. Because of these problems, Ethereum needed a way to make the proposer schedule fully deterministic several epochs ahead so it cannot be changed by last-minute EB updates and can be easily accessed by the application layer.To implement this, the EIP introduces a deterministic proposer lookahead by pre-computing and storing the proposer schedule for the next MINLOOKAHEAD + 1 epochs at the start of every epoch. In simple terms, the beacon state now contains a list called  that always covers two full epochs of proposers - 64 slots in total. For example, when epoch N begins, this list already includes the proposer for every slot in epoch N and epoch N+1. Then, when the network moves to epoch N+1, the list is shifted forward: the proposer entries for epoch N are removed, the entries for epoch N+1 move to the front, and new proposer entries for epoch N+2 are added at the end. This makes the schedule fixed, predictable, and easy for clients to read directly, instead of recomputing proposers every slot.To keep this updated, the list shifts forward at every epoch boundary: the data for the past epoch is removed, and a new set of proposer indices for the next future epoch is computed and appended. The process uses the same seed and effective balance rules as before, but now the schedule is calculated earlier, removing the possibility of EB changes affecting it after the seed is known. The very first block after the fork also fills this entire lookahead range so that all future epochs have properly initialized schedules.Imagine each epoch has 8 slots instead of 32 (just for simplicity). Without this EIP, during epoch 5, you know the seed for epoch 6, but the actual proposer for slot 2 of epoch 6 could still change if a validator gets slashed or earns enough rewards to change their effective balance inside epoch 5. With EIP-7917, at the  of epoch 5, Ethereum pre-calculates all proposers for epoch 5, 6, and 7 and stores them in order inside . Now, even if balances change later in epoch 5, the proposer list for epoch 6 stays fixed and predictable.This EIP fixes a long-standing oversight in the beacon chain design. It guarantees that once the RANDAO from earlier epochs becomes available, the validator selection for future epochs cannot be altered. This also prevents “effective balance grinding,” where a validator tries to adjust their balance after seeing the RANDAO to influence the next epoch’s proposer list. With deterministic lookahead, that entire attack vector is eliminated, making the security analysis much simpler. It also gives consensus clients early visibility into who will propose upcoming blocks, which helps implementations and allows the proposer schedule to be easily verified by the application layer via a Merkle proof from the beacon root.The authors considered alternatives, such as caching the effective balances at the start of an epoch, but that would require extra storage and would not expose the schedule to the EVM. They also checked compatibility with future features like Single Secret Leader Election (SSLE). The current design remains compatible because the lookahead could one day store encrypted proposer IDs, or possibly be disabled entirely if SSLE removes lookahead, without breaking anything.Before this proposal, clients only calculated the proposer for the current slot. With EIP-7917, they now compute the proposer list for all slots in the next epoch at once during every epoch transition. This adds a small amount of work, but computing proposer indices is very light and mainly involves sampling validator lists using the seed. However, clients will need benchmarking to make sure this extra computation does not cause performance issues.This EIP does not change how the RANDAO delay works. The proposer lookahead for epoch  is still derived from the RANDAO of epoch . The only change is that effective balances are now aligned with the same delay, so validators cannot modify their EB after seeing the RANDAO result. This removes a potential manipulation strategy, even though no attack had been discovered yet. The deterministic lookahead therefore, strengthens security and prevents malicious alteration of proposer schedules.EIP-7918: Blob base fee bounded by execution costEthereum needs this proposal because the current blob fee system (from EIP-4844) breaks down when execution gas becomes the main cost for rollups. Right now, most rollups pay much more for execution gas (the cost of including their blob transaction in the block) than they pay for the actual blob fee. This creates a problem: even if Ethereum keeps lowering the blob base fee again and again, the rollup’s total cost does not really change, because the expensive part is still the execution gas. Because of this, the blob base fee keeps falling until it reaches the absolute minimum (1 wei), and the protocol can no longer use the blob fee to control demand. Then, when blob usage suddenly goes up, the blob fee needs many blocks to climb back to a normal level. This makes prices unstable and unpredictable for users.For example, imagine a rollup wants to post its data: it pays about 25,000,000 gwei in execution gas (25 gwei per gas for roughly 1,000,000 gas), while the blob fee is only around 200 gwei. This means the total cost is roughly 25,000,200 gwei, where almost the entire cost comes from execution gas, not the blob fee. If Ethereum keeps lowering the blob fee—say from 200 gwei to 50 gwei, then to 10 gwei, and eventually down to 1 wei—the total cost barely changes at all, staying almost exactly 25,000,000 gwei. Because users don’t feel any difference, they don’t change their behavior, so the protocol keeps pushing blob fees down until they hit the minimum possible value. Then, when blob demand suddenly increases again, the blob base fee has to climb all the way up from 1 wei, causing sharp, unpredictable fee spikes. EIP-7918 fixes this by introducing a minimum “reserve price” for blob fees that depends on the execution base fee, preventing blob prices from falling unrealistically low and keeping blob pricing much more stable and predictable for rollups.The core idea of EIP-7918 is simple: the price of a blob should never be cheaper than the cost of a certain amount of execution gas (called ). In the EIP, this constant is set to . The mechanism works through a small change inside the  function. Usually, this function increases or decreases the blob base fee depending on whether blocks are using more or less blob gas than the target. With this proposal, if a blob becomes “too cheap” compared to execution gas, the function stops subtracting the target blob gas. This makes the excess blob gas grow faster, which prevents the blob base fee from falling further. As a result, the blob base fee now has a minimum value equal to BLOBCOST × baseperPER_BLOB.To understand why this is needed, it helps to look at blob demand. A rollup cares about the total price it pays: execution cost plus blob cost. If execution gas fee is very high—for example, 20 gwei—then even if the blob fee drops from 2 gwei to 0.2 gwei, the total cost barely changes. This means reducing the blob base fee has almost no impact on demand. In economics, this is called “.” It creates a situation where the demand curve is almost vertical: lowering price does not increase demand. When this happens, the blob base fee mechanism becomes blind—it keeps lowering the price even though demand does not react. That is why the blob base fee often slides down to 1 wei. Then, when real demand increases later, the protocol needs an hour or more of nearly full blocks to raise the fee back up to a reasonable level. EIP-7918 fixes this by establishing a reserve price tied to execution gas so that blob fees remain meaningful even when execution costs dominate.Another reason for adding this reserve price is that nodes have to do a lot of extra work to verify the KZG proofs for blob data. These proofs are what guarantee that the data inside a blob actually matches its commitment. Under EIP-4844, a node only had to verify one proof per blob, which was cheap. But with EIP-7594 (PeerDAS), blobs are broken into many small pieces called cells, and every cell has its own proof. This makes verification much heavier. For example, the execution layer now has to batch-verify 128 proofs for every single blob before a transaction can even enter the mempool—this is about fifteen times more expensive than the normal KZG proof verification that smart contracts pay for. On top of that, full nodes, supernodes, and validators must verify even more proofs depending on how many columns they store or sample. A typical full node must verify proofs for all blobs in the mempool, plus eight sampled columns every slot, plus four columns it permanently custodies. All of this uses real CPU time, and it isn’t free for node operators. If blob fees drop too low, users would get this expensive compute work from the network essentially for free. By linking the blob reserve price to execution gas fees, EIP-7918 makes sure blob users always pay at least a fair minimum amount for the load they put on nodes.In the long run, EIP-7918 also helps prepare Ethereum for the future. As technology improves, the cost of storing and sharing data naturally gets cheaper, and Ethereum is expected to allow more blob data over time. When blob capacity increases, blob fees (in ETH) should naturally go down. This proposal supports that because the reserve price is tied to execution gas prices, not a fixed number, so it can adjust as the network grows. As both blobspace and execution blockspace expand, their price relationship stays balanced. The only time the reserve price might become too high is in a very unlikely future where Ethereum increases blob capacity a lot but does not increase execution gas capacity. In that case, blob fees might end up higher than needed. But Ethereum is not planning to scale in that way—both blobspace and execution blockspace are expected to grow together. Because of this, the chosen value () is considered safe and well-balanced.There is one small detail to understand when execution gas fees suddenly jump. Because the blob reserve price depends on the execution base fee, a sudden rise in execution costs can temporarily push blob fees into a state where execution fees are guiding them. For example, imagine execution gas suddenly jumps from 20 gwei to 60 gwei in one block. Since the reserve price for blobs is tied to that number, blob fees cannot drop below the new higher level. Blob fees will still increase normally if blobs are being used, but the protocol will not allow them to decrease until they have risen enough to match the higher execution cost. This means blob fees may climb more slowly than execution fees for a few blocks. This small delay is not harmful — it actually prevents sharp, sudden swings in blob prices and keeps the system smoother and more stable.The authors also performed empirical analysis by applying the reserve price rule to real blob activity from  and . During high-execution-fee periods (around ), the reserve threshold significantly increased the blob base fee compared to the old mechanism. During low-execution-fee periods (around ), the blob fee remained almost unchanged except when the calculated blob base fee dropped below the reserve price. By comparing thousands of blocks, the authors show that the new mechanism creates more stable pricing while still responding naturally to demand. A histogram of four months of blob fees shows that the reserve price prevents blob fees from collapsing toward 1 wei, which reduces extreme volatility.In terms of security, the change does not introduce any risks. The blob base fee will always settle at or above the cost of  units of execution gas. This is safe because the mechanism only raises the minimum fee, and setting a lower bound on pricing does not interfere with protocol correctness. It simply ensures healthy economics.EIP-7934: RLP Execution Block Size LimitBefore EIP-7934, Ethereum did  have a strict upper limit on how large an RLP-encoded execution block could be. In theory, a block could become extremely large if it contained many transactions or very complex data. This created two major problems:  and . If a block was too large, it would take longer for nodes to download and verify it, which slowed block propagation and increased the chance of temporary blockchain forks. Worse, an attacker could deliberately create a very large block to overload nodes, causing delays or even knocking them offline — a classic denial-of-service scenario. At the same time, Ethereum’s consensus layer (CL) gossip protocol already refused to propagate any block over , meaning oversized execution blocks could fail to spread across the network, creating fragmentation or nodes disagreeing on the chain. Because of these risks, Ethereum needed a clear, protocol-level rule to prevent oversized blocks and keep the network stable and secure.EIP-7934 solves this by introducing a  on the size of an RLP-encoded execution block. The maximum allowed block size () is set to 10 MiB (10,485,760 bytes), but because beacon blocks also consume some space (), Ethereum adds a . This means the actual maximum RLP-encoded execution block size allowed is MAX_RLP_BLOCK_SIZE = MAX_BLOCK_SIZE - SAFETY_MARGIN. If the encoded block is larger than this limit, the block is considered  and nodes must reject it. With this rule in place, block producers must check the encoded size of every block they build, and validators must verify this limit during block validation. This size cap applies  of gas limits -meaning even if a block is “” it can still be rejected if its encoded size is too large. This ensures that both gas usage and real byte-size constraints are respected.Choosing a  was intentional because it matches the existing constraint in the consensus layer gossip protocol. Anything larger than 10 MiB would not be broadcast across the network anyway, so this EIP brings the execution layer into alignment with the consensus layer’s limits. This creates consistency across all components and prevents situations where a valid execution block becomes “invisible” because the CL refuses to gossip it.This change is  with blocks larger than the new limit, meaning miners and validators must update their clients to respect the rule. However, since oversized blocks were already problematic and not normal in real operation, the impact is minimal. In terms of security, this EIP significantly strengthens Ethereum against targeted block-size DoS attacks by ensuring no participant can create blocks that overwhelm the network. Overall, EIP-7934 adds an important safety boundary, improves stability, aligns EL and CL behavior, and prevents several classes of attacks related to oversized block creation and propagation.EIP-7939: Count leading zeros (CLZ) opcodeBefore this EIP, Ethereum had  to count the number of leading zero bits in a 256-bit number. Developers had to implement CLZ manually in Solidity using many bit-shift operations and comparisons. This was a big problem because the custom implementations were , and , which increased gas usage. For zero-knowledge proving systems, the cost was even higher — right-shifts are extremely expensive to prove, so operations like CLZ slowed down ZK circuits significantly. Since CLZ is a very common low-level function used in math libraries, compression algorithms, bitmaps, signature schemes, and many cryptographic or data-processing tasks, Ethereum needed a faster and cheaper way to compute it.EIP-7939 solves this by introducing a new opcode called CLZ (0x1e). This opcode takes a 256-bit value from the stack and returns the number of leading zero bits. If the input number is zero, the opcode returns , because a 256-bit zero has 256 leading zero bits. This matches how CLZ works in many CPU architectures like ARM and x86, where this operation is native. Adding CLZ makes many algorithms significantly cheaper: operations like , , , various math functions, byte-string comparisons, bitmap scanning, calldata compression/decompression, and post-quantum signature schemes all benefit from faster leading-zero detection.The gas cost for CLZ is set to 5, similar to an , and slightly raised from the old MUL price to avoid underpricing the opcode and causing DoS risks. Benchmarks show that CLZ uses roughly the same compute effort as ADD, and in the SP1 rv32im proving environment, CLZ is actually cheaper to prove than ADD, reducing ZK proving costs. The EIP also explains why they chose CLZ instead of CTZ (count trailing zeros): you can compute CTZ from CLZ using , but you  reliably implement CLZ using CTZ, so CLZ is more fundamental.This EIP is fully backwards-compatible because it introduces a  opcode and does not modify any existing behavior. It also covers edge cases clearly: for example, the opcode returns 256 when the input is zero, and several test cases show inputs like all-zero, top-bit-set, and fully-nonzero values. Since CLZ has predictable and low gas cost, no memory growth, and no state changes, it is safe from denial-of-service issues.Overall, EIP-7939 makes Ethereum faster, cheaper, and more developer-friendly by adding a simple, efficient primitive that modern CPUs already support — cutting gas, reducing bytecode size, and lowering ZK proving costs for many common operations.Before this EIP, Ethereum did  have a safe, native way to verify digital signatures created using the . This curve is the standard used in modern devices like Apple Secure Enclave, Android Keystore, HSMs, TEEs, and FIDO2/WebAuthn security keys. Because of this missing support, apps and wallets could not easily use device-level hardware security for signing. There was an earlier attempt (RIP-7212), but it had two serious security vulnerabilities related to point-at-infinity handling and incorrect signature comparison. These issues could cause incorrect verification or even risk consensus failures. EIP-7951 fixes those security problems and introduces a safe, native precompile so Ethereum can finally support signatures from modern hardware securely and efficiently.EIP-7951 adds a new  at address  called , which performs ECDSA signature verification using the secp256r1 curve. This makes signature verification fast and cheap compared to implementing the algorithm directly in Solidity. The precompile uses the official curve parameters defined by NIST, including the field modulus, curve equation, base point, and subgroup order, ensuring strong cryptographic security. Points and scalars are encoded in strict 32-byte big-endian format, and a 64-byte all-zero value is used to represent the point at infinity. The precompile expects exactly —the hash, signature (r, s), and public key coordinates (qx, qy)—and it returns  for success or  for failure.The EIP also defines strict . It checks that r and s are within valid ranges, that the public key lies on the curve, and that it is not the point at infinity. If anything is invalid, the precompile returns failure without reverting and consumes the same gas as a successful call. The verification algorithm follows standard ECDSA: it computes s⁻¹ mod n, rebuilds the signing point R’, rejects if R’ is infinity, and finally checks whether the x-coordinate of R’ matches r (mod n). This corrects the mistake in RIP-7212, which compared r’ directly instead of reducing it mod n.The secp256r1 curve is fully defined by the following set of parameters:Base field modulus = p = 0xffffffff00000001000000000000000000000000ffffffffffffffffffffffff
Curve equation: y^2 = x^3 + ax + b (mod p)
Curve coefficient a = 0xffffffff00000001000000000000000000000000fffffffffffffffffffffffc
Curve coefficient b = 0x5ac635d8aa3a93e7b3ebbd55769886bc651d06b0cc53b0f63bce3c3e27d2604b
Base point G:
  Gx = 0x6b17d1f2e12c4247f8bce6e563a440f277037d812deb33a0f4a13945d898c296
  Gy = 0x4fe342e2fe1a7f9b8ee7eb4a7c0f9e162bce33576b315ececbb6406837bf51f5
Subgroup order = n = 0xffffffff00000000ffffffffffffffffbce6faada7179e84f3b9cac2fc632551
Cofactor = h = 0x1
These parameters are standardized by NIST in SP 800-1861.The gas cost for the operation is set to , which is higher than the RIP-7212 version, but matches actual performance benchmarks for secp256r1 verification. Importantly, the interface remains fully compatible with Layer 2 networks that already deployed RIP-7212—same address, same input/output format—so existing smart contracts will continue to work with no changes. The only difference is the corrected behavior and higher gas cost.Input:
P256VERIFY call excepts 160 Bytes as input that is interpreted as byte concatenation of:
32 bytes  ----> message hash (h)
32 bytes  ----> signature component (r)
32 bytes  ----> signature component (s)
32 bytes  ----> public key x-coordinate (qx)
32 bytes  ----> public key y-coordinate (qy)
Output:
will be 32 bytes on successfull verification and 0 bytes on failure
Input Validation:
The precompile MUST perform the following validation checks and return `` (failure) if any check fails:
1. Input length: Input MUST be exactly 160 bytes
2. Signature component bounds: Both r and s MUST satisfy 0 < r < n and 0 < s < n
3. Public key bounds: Both qx and qy MUST satisfy 0 ≤ qx < p and 0 ≤ qy < p
4. Point validity: The point (qx, qy) MUST satisfy the curve equation qy^2 ≡ qx^3 + a*qx + b (mod p)
5. Point not at infinity: The point (qx, qy) MUST NOT be the point at infinity (represented as (0, 0))
From a security standpoint, the EIP restores proper ECDSA behavior, eliminates malleability concerns at the precompile level (leaving optional checks to applications), and clarifies that constant-time execution is not required for the precompile. The secp256r1 curve provides 128-bit security and is widely trusted and analyzed, making it safe for Ethereum adoption.In short, EIP-7951 is needed to safely bring modern hardware-backed authentication to Ethereum, fix the security issues of the earlier proposal, and provide a reliable, standardized way to verify P-256 signatures across the entire ecosystem.The table below summarizes which Ethereum clients need to implement changes for each Fusaka EIP. A check mark under  indicates that the EIP requires updates to consensus-layer clients, while a check mark under  shows that the change affects execution-layer clients. Some EIPs require updates in both layers, while others are specific to just one.| EIP Number | EIP Name | Consensus Client | Execution Client |
|----|----|----|----|
| EIP-7594 | PeerDAS - Peer Data Availability Sampling | ✅ | ✅ |
| EIP-7823 | Set upper bounds for MODEXP |    | ✅ |
| EIP-7825 | Transaction Gas Limit Cap |    | ✅ |
| EIP-7883 | ModExp Gas Cost Increase |    | ✅ |
| EIP-7917 | Deterministic proposer lookahead | ✅ |    |
| EIP-7918 | Blob base fee bounded by execution cost |    | ✅ |
| EIP-7934 | RLP Execution Block Size Limit |    | ✅ |
| EIP-7939 | Count leading zeros (CLZ) opcode |    | ✅ |
| EIP-7951 | Precompile for secp256r1 Curve Support |    | ✅ |In summary, these are the key EIPs included in the Fusaka hard fork. While several improvements touch both consensus and execution clients—ranging from gas adjustments and opcode updates to new precompiles—the major change of this upgrade is PeerDAS, which introduces peer-to-peer data availability sampling, enabling more efficient and decentralized handling of blob data across the network.]]></content:encoded></item><item><title>Safer Autonomous Vehicles Means Asking Them the Right Questions</title><link>https://spectrum.ieee.org/autonomous-vehicles-explainable-ai-decisions</link><author>Michelle Hampson</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIyNDg1OS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5NjU0Nzc2NX0.Yv01hTA9L8ZK-ANK3R1hL2g3D3O1G7025tXIzkR3Kvo/image.jpg?width=600" length="" type=""/><pubDate>Sun, 23 Nov 2025 14:00:01 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Explainable AI could help clarify vehicle decision-making]]></content:encoded></item><item><title>Trillions Spent and Big Software Projects Are Still Failing</title><link>https://spectrum.ieee.org/it-management-software-failures</link><author>Robert N. Charette</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIwNjk3Ni9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgxNDE5Njg1MH0.LP9eTdSg6s_VjvHU65mH5ETFx8e0ll1_k7Bch8yqKyE/image.png?width=600" length="" type=""/><pubDate>Sun, 23 Nov 2025 13:05:01 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[AI won’t solve IT’s management problems]]></content:encoded></item><item><title>How the Internet Rewired Work - and What That Tells Us About AI&apos;s Likely Impact</title><link>https://it.slashdot.org/story/25/11/23/0812238/how-the-internet-rewired-work---and-what-that-tells-us-about-ais-likely-impact?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 23 Nov 2025 12:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["The internet did transform work — but not the way 1998 thought..." argues the Wall Street Journal. "The internet slipped inside almost every job and rewired how work got done." 

So while the number of single-task jobs like travel agent dropped, most jobs "are bundles of judgment, coordination and hands-on work," and instead the internet brought "the quiet transformation of nearly every job in the economy... Today, just 10% of workers make minimal use of the internet on the job — roles like butcher and carpet installer."

[T]he bigger story has been additive. In 1998, few could conceive of social media — let alone 65,000 social-media managers — and 200,000 information-security analysts would have sounded absurd when data still lived on floppy disks... Marketing shifted from campaign bursts to always-on funnels and A/B testing. Clinics embedded e-prescribing and patient portals, reshaping front-office and clinical handoffs. The steps, owners and metrics shifted. Only then did the backbone scale: We went from server closets wedged next to the mop sink to data centers and cloud regions, from lone system administrators to fulfillment networks, cybersecurity and compliance. 

That is where many unexpected jobs appeared. Networked machines and web-enabled software quietly transformed back offices as much as our on-screen lives. Similarly, as e-commerce took off, internet-enabled logistics rewired planning roles — logisticians, transportation and distribution managers — and unlocked a surge in last-mile work. The build-out didn't just hire coders; it hired coordinators, pickers, packers and drivers. It spawned hundreds of thousands of warehouse and delivery jobs — the largest pockets of internet-driven job growth, and yet few had them on their 1998 bingo card... Today, the share of workers in professional and managerial occupations has more than doubled since the dawn of the digital era. 



So what does that tell us about AI? Our mental model often defaults to an industrial image — John Henry versus the steam drill — where jobs are one dominant task, and automation maps one-to-one: Automate the task, eliminate the job. The internet revealed a different reality: Modern roles are bundles. Technologies typically hit routine tasks first, then workflows, and only later reshape jobs, with second-order hiring around the backbone. That complexity is what made disruption slower and more subtle than anyone predicted. AI fits that pattern more than it breaks it... [LLMs] can draft briefs, summarize medical notes and answer queries. Those are tasks — important ones — but still parts of larger roles. They don't manage risk, hold accountability, reassure anxious clients or integrate messy context across teams. Expect a rebalanced division of labor: The technical layer gets faster and cheaper; the human layer shifts toward supervision, coordination, complex judgment, relationship work and exception handling. 

What to expect from AI, then, is messy, uneven reshuffling in stages. Some roles will contract sharply — and those contractions will affect real people. But many occupations will be rewired in quieter ways. Productivity gains will unlock new demand and create work that didn't exist, alongside a build-out around data, safety, compliance and infrastructure. 
AI is unprecedented; so was the internet. The real risk is timing: overestimating job losses, underestimating the long, quiet rewiring already under way, and overlooking the jobs created in the backbone. That was the internet's lesson. It's likely to be AI's as well.]]></content:encoded></item><item><title>Linux Patches Improve Intel Nested VM Memory Performance Up To ~2353x In Synthetic Test</title><link>https://www.phoronix.com/news/Intel-Nested-VM-Faster-Memory</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Nov 2025 11:48:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AWS engineers have been working on Linux kernel improvements to KVM's VMX code for enhancing the unmanaged guest memory when dealing with nested virtual machines. The improved code addresses some correctness issues as well as delivering wild performance improvements within a synthetic benchmark...]]></content:encoded></item><item><title>Glibc Math Code Sees 4x Improvement On AMD Zen By Changing FMA Implementation</title><link>https://www.phoronix.com/news/Glibc-4x-FMA-Improvement-Zen</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Nov 2025 11:31:03 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged this week to the GNU C Library "glibc" code is dropping the ldbl-96 FMA implementation from this library as in doing so they found a 4x improvement to throughput and latency on AMD Zen 3 hardware...]]></content:encoded></item><item><title>Google Looks To Bring JPEG-XL Support Back To Chrome / Chromium</title><link>https://www.phoronix.com/news/JPEG-XL-Possible-Chrome-Back</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 23 Nov 2025 11:10:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Back in 2022 was the surprising decision by Google that they were going to deprecate JPEG-XL image support in Chrome. By the end of 2022 they went ahead and removed JPEG-XL support from Chrome/Chromium to the frustration of many web developers and end-users interested in this image format. Now though as we get ready to roll into 2026, Google engineers are looking at bringing back JPEG-XL support to the Chrome web browser...]]></content:encoded></item></channel></rss>
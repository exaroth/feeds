<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://konrad.website/feeds/</link><description></description><item><title>Alphabet tops $100B quarterly revenue for first time, cloud grows 34%</title><link>https://www.cnbc.com/2025/10/29/alphabet-google-q3-earnings.html</link><author>thelastgallon</author><category>hn</category><pubDate>Thu, 30 Oct 2025 11:33:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ reported third-quarter earnings that beat analyst expectations. Shares rose 5% in after-hours trading.Here's how the company did, compared with estimates from analysts polled by LSEG: $102.35 billion vs. $99.89 billion estimated $3.10 adj. vs $2.33 estimatedWall Street was also watching several other numbers in the report:YouTube advertising revenue: $10.26 billion vs. $10.01 billion, according to StreetAccount $15.15 billion vs. $14.74 billion, according to StreetAccountTraffic acquisition costs (TAC): $14.87 billion vs. $14.82 billion, according to StreetAccountAlphabet reported solid momentum in its cloud business, thanks to strong demand for artificial intelligence. The company also announced an increase in expected capital expenditures for the fiscal year 2025."With the growth across our business and demand from Cloud customers, we now expect 2025 capital expenditures to be in a range of $91 billion to $93 billion," the company said in its earnings report Wednesday. "Looking out to 2026, we expect a significant increase in CapEx and will provide more detail on our fourth quarter earnings call," said finance chief Anat Ashkenazi on the earnings call with investors Wednesday.Earlier this year, the company increased its capital expenditure expectation from $75 billion to $85 billion. Most of that goes toward technical infrastructure such as data centers.The latest earnings show the company is seeing rising demand for its AI services, which largely sit in its cloud unit. It also shows the company is continuing to spend more to try and build out more infrastructure to accomodate the backlog of customer requests."We continue to drive strong growth in new businesses. Google Cloud accelerated, ending the quarter with $155 billion in backlog," CEO Sundar Pichai said in the earnings release.The backlog comes from demand for enterprise AI infrastructure, including chips and demand for Gemini 2.5, said Ashkenazi.The company reported cloud revenue of $15.15 billion, a 35% increase from the same period last year."We have signed more deals over one billion dollars through Q3 this year than we did in the previous two years combined," said Pichai, referring to the first nine months of the year. In August, Google won a $10 billion cloud contract from Meta spanning six years. Alphabet, which reported 32% cloud revenue growth last quarter, is keeping pace with its megacap competitors.  posted 40% revenue growth in its Azure cloud business as it reported earnings on Wednesday.Over 70% of existing Google Cloud customers use its AI products, Pichai said. That's a result of the company's strategy to upsell existing customers.Google's flagship AI app Gemini now has more than 650 million monthly active users, the company said in its Wednesday report. That's up from the 450 million active users Pichai said it had last quarter. OpenAI CEO Sam Altman said earlier this month that ChatGPT now has 800 million users per week.Google's search business generated $56.56 billion in revenue — up 15% from the prior year.Alphabet's net income increased to $34.97 billion, or $2.87 per share, compared to $26.3 billion, or $2.12 per share, in the year-ago quarter. In September, Google was slapped with a $3.45 billion antitrust fine from European Union regulators for anti-competitive practices in its lucrative advertising technology business. That fine impacted the reported net income.YouTube advertising revenue came in at $10.26 billion, higher than Wall Street expected. Alphabet reported overall advertising revenue of $74.18 billion — up from $65.85 billion last year.Other Bets, which includes the company's life sciences unit Verily and self-driving car unit Waymo, reported revenue of $344 million during the quarter. That's lower than the $388 million from the same quarter last year. Alphabet reported a loss of $1.42 billion on other bets, compared to a loss of $1.12 billion the year before.The Google parent's stock is up 45% so far this year.]]></content:encoded></item><item><title>Language Models Are Injective and Hence Invertible</title><link>https://arxiv.org/abs/2510.15511</link><author>mazsa</author><category>hn</category><pubDate>Thu, 30 Oct 2025 09:47:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Happy Open Access Week from arXiv!YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.]]></content:encoded></item><item><title>Hello-World iOS App in Assembly</title><link>https://gist.github.com/nicolas17/966a03ce49f949dd17b0123415ef2e31</link><author>pabs3</author><category>hn</category><pubDate>Thu, 30 Oct 2025 02:37:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>IRCd service (2024)</title><link>https://example.fi/blog/ircd.html</link><author>pabs3</author><category>hn</category><pubDate>Thu, 30 Oct 2025 02:31:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
Ok. We have IRC at example.fi
Internet Relay Chat (IRC) is a form of real-time text communication developed by Jarkko Oikarinen in 1988. Initially created to replace a local BBS system at the University of Oulu in Finland, IRC quickly gained global popularity, becoming a foundational technology for online chat communities and influencing the development of modern instant messaging and social media platforms. Its significance lies in its pioneering role in connecting people across the internet, fostering early online communities, and setting the stage for contemporary digital communication.

To commemorate this pivotal technology, example.fi provides a simple and limited IRC server. This server is uniquely written in AWK, a scripting language traditionally used for text processing, highlighting the adaptability and enduring legacy of IRC. This creative implementation serves as both an educational tool and a tribute to the foundational role of IRC in the evolution of online communication.

In the following picture you see Irssi in the background and Hexchat on top of it:
Note: if you plan to connect to example.fi, make sure you do not use any fancy features. In irssi, use -nocap option. In Windows, use for example hexchat.
As this is written in gawk, most IRC protocol features are not implemented. This includes, for example, channel and user listings, topics, the concept of "operator" etc.
Technical fun fact: Total code count is around 60 lines of awk and a few lines of bash.
$ telnet example.fi ircd
Trying 65.108.91.190...
Connected to example.fi.
Escape character is '^]'.
USER foo
NICK bar
:example.fi 001 bar :Welcome to Internet Relay Network bar!~foo@65.108.91.190
:example.fi 375 test :- example.fi Message of the day -
:example.fi 372 test :- Current time is @787.188.beats
:example.fi 376 test :End of MOTD command.
Connection closed by foreign host.

Don't worry, we'll publish the code when it's "ready" :)This site is HTML 2.0 compliant.]]></content:encoded></item><item><title>OS/2 Warp, PowerPC Edition (2011)</title><link>https://www.os2museum.com/wp/os2-history/os2-warp-powerpc-edition/</link><author>TMWNN</author><category>hn</category><pubDate>Wed, 29 Oct 2025 23:52:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The PowerPC adventure—by far the most exotic release of OS/2In December 1995, after unexpectedly long development (but is that really unexpected?), IBM finally “shipped” OS/2 Warp, PowerPC edition. For brevity, this release will be further referred to as OS/2 PPC. Following years of hype and high expectation, the release was very low key and in fact marked the end of development of OS/2 for PowerPC. The product was only available to a limited number of IBM customers and was never actively marketed. OS/2 PPC may not even had a box, although there were nice looking official CDs.OS/2 PPC only supported an extremely limited range of hardware—IBM Personal Power Series machines. Those were desktop models 830 and 850, and OS/2 PPC probably also supported the Power Series ThinkPads 820 and 850, though that can be only inferred from the fact that the graphics chipset employed by these ThinkPads was on the very short list of supported devices in OS/2 PPC.The IBM Power Series computers were IBM’s rather short lived foray into the PowerPC-based desktop personal computer market, circa 1995-1996. The PowerPC CPU aside, the systems were very similar to Intel based hardware of that era. They were designed around the PCI bus, but also included ISA expansion slots and on-board Crystal Audio ISA PnP chips. The desktop Power Series machines were IDE based, ThinkPads used SCSI disks. The computers had standard serial and parallel ports, as well as most of typical PC hardware such as interrupt and DMA controllers. The desktops had onboard S3 864 video, ThinkPads used Western Digital flat panel chipsets. Several optional graphics cards were supported, notably Weitek P9100 based accelerators. The desktops also had onboard Ethernet chips (AMD PCnet).The Power Series systems were closely related to certain IBM RS/6000 workstations. The RS/6000 Model 43P-7248 was nearly identical to the Power Series 850. They used the same motherboard, only the RS/6000 had on-board SCSI controller. Unlike the RS/6000 systems intended for the workstation market and running almost exclusively IBM’s AIX operating system, the Power Series systems were designed for “regular” personal computer users. The machines were supposed to run OS/2, Windows NT, AIX, or Solaris. OS/2 PPC was only semi-finished, and the Solaris for PowerPC port (version 2.5.1) was similarly short-lived. Microsoft dropped PowerPC support in 1996, not long after the Windows NT 4.0 release. Most of the Power Series systems ended up running AIX, which supported them until version 5.1. Linux also supported the Power Series to some extent. Windows NT was clearly the closest competitor of OS/2 PPC.For this article, OS/2 PPC was installed on a Power Series 830, installed by its previous owner in a RS/6000 43P case. The CPU was a 100MHz PowerPC 604 with 256KB L2 cache, and the machine was equipped with 192MB RAM, which was the maximum it could handle. The graphics was an on-board PCI S3 Vision 864 with 2MB video memory and true color S3 SDAC. The machine was equipped with 2.1GB IDE hard drive—AIX can handle up to 8GB and Linux can utilize even larger disks, but OS/2 and NT were not happy with anything over about 2.5GB. The 830 was originally sold with either 500MB or 1GB disks and 16MB RAM. The Power Series 850 systems were equipped with 100 or 120MHz CPUs, slightly more RAM and larger disks.OS/2 Warp, PowerPC edition was delivered on two CDs. The first CD contained the operating system and BonusPak, the second CD was an application sampler with several demo applications.Installation was surprisingly easy and painless. The CD was bootable and there were almost no choices to make during installation—only the disk partitioning was user selectable. The PowerPC operating systems (OS/2, NT, AIX and Linux) generally did not coexist as there was no real equivalent of a boot manager and each OS wanted to install its own boot loader. The OS/2 installer re-partitioned the disk and overwrote any other operating systems. The boot partition had to be FAT. It was possible to create HPFS data partitions, but the HPFS support appeared to be somewhat unstable and likely a last-minute addition.After the OS was copied from the installation CD-ROM and the system booted from fixed disk for the first time, the user was greeted by the following screen:Indeed, OS/2 PPC really looked just like OS/2 Warp, at least at first glance. The system booted up in 640×480 mode with 256 colors, using the accelerated S3 driver. The desktop right after installation looked like this:Still very much like OS/2 Warp, except for that little Systems Management folder. This feature was not present in the Intel OS/2 Warp release, although it was added later. After installing the BonusPak and a few other additions and changing the resolution, the desktop still looked like plain OS/2 Warp, with the exception of the background bitmap of course (click on the picture to see full size screenshot):The system was now running in 1024×768 resolution, but still with 256 colors. The graphics chip supports 64K colors at this resolution, unfortunately the software used to take screenshots (a demo version of Impos/2) was unable to take any screenshots at this resolution. 256 colors it is then, and time to more closely examine the operating system. The README file is a good starting point, and it was quite long in OS/2 PPC. It consisted largely of a list of unimplemented or incomplete features.For example, notice the word “Connect” in the screenshot. OS/2 Warp, PowerPC Edition, doesn’t have any connectivity to speak of. Networking support, in a nutshell, didn’t exist. No LAN Server client, no TCP/IP, nothing. There was just HyperAccess Lite and CompuServe Information Manager, which worked (in theory at least) over a modem. The product name itself seems to have been a last minute change. Programs and documentation in many instances refer to OS/2 Warp Connect, PowerPC Edition, but the final product was called just OS/2 Warp and not “Connect”. One of the README files explains the name change and alludes to networking support in “future versions”.For development versions of OS/2 PPC there was TFTP support which talked directly to the microkernel Ethernet or Token Ring driver and entirely bypassed OS/2. This transport layer also supported remote debugging. This is in sharp contrast to Windows NT which fully supported networking (TCP/IP and SMB file sharing) on the same hardware. Networking was obviously planned for OS/2, but the project was killed before this part was done.Not everything was so blatantly unfinished though. The DOS support in OS/2 PPC was a pleasant surprise:On a closer look, it’s clear that OS/2 PPC included a full-fledged PC emulator, which supplied a virtual x86 CPU as well as common PC hardware. Interestingly, the DOS support in OS/2 PPC was based on PC-DOS 7 and not the outdated DOS 5 level code that OS/2 on Intel was stuck with. The OS/2 PPC DOS boxes thus had for instance the DOS E editor (very similar to TEDIT) or REXX support. Why IBM never updated the DOS support on the Intel side is a mystery. OS/2 PPC supported both windowed and full screen DOS sessions. The full screen sessions always ran in graphics mode, even when the emulated DOS application was using text mode.Not satisfied with “just” DOS emulation, IBM also supported Win-OS/2, both full-screen and windowed:It is difficult to judge how stable the DOS and Win-OS/2 emulation really was, but whatever little utilities came with the OS/2 system seemed to work well, including wave audio in Win-OS/2, and the performance was surprisingly good. IBM must have spent a lot of effort on the x86 emulation support. Documentation hinted at a possibility of future support for native OS/2 x86 applications via emulation.IBM also obviously spent a lot of time on the multimedia support in OS/2 PPC. The multimedia support worked unexpectedly well, especially when contrasted with the problems common on Intel machines.The system played video and audio without problems, with MIDI support either via a software synthesizer or an OPL3 compatible chip (the software synthesizer sounded far better). The application sampler CD came with several videos, mostly ads for OS/2. The PowerPC Toolkit also came with a beta version of OpenGL support, which shared code with IBM’s AIX workstation grade implementation.OS/2 PPC was a hybrid halfway between Warp 3 and Warp 4. The user interface looked like Warp 3, but many of the features of OS/2 PPC later showed in Warp 4 on Intel. One of them was the not very popular Feature Installer:The Feature Installer was used to install the BonusPak, several tools and games, and curiously enough, also the Command Reference which for some odd reason wasn’t part of the base install. Here’s one of those games:Again, there is no real difference from the Intel version, except for the about box text (notice the “Connect” text). And finally the IBM Works text editor—again there is no discernible difference from the Intel version:OS/2 for PowerPCs System OverviewOS/2 PPC was a strange OS. In many ways it was exactly identical to the Intel version, yet in other ways it was completely different. The user interface was the same and the entire API practically unchanged. Among the differences were the addition of full Unicode support and 32-bit console API (Kbd/Mou/Vio). The largely unchanged API was the reason why it was relatively easy to port existing OS/2 software to PowerPC. The biggest difference was not even the CPU but rather the compiler—IBM used the MetaWare High C/C++ for PowerPC development (it was allegedly cheaper for the IBM OS/2 division to contract MetaWare rather than IBM’s own compiler group). The MetaWare tool set was only used as a cross compiler hosted on x86 OS/2 systems. IBM used MetaWare’s compiler for embedded PowerPC development in general (IBM’s involvement with MetaWare goes at least as far back as AIX for PS/2), and MetaWare also marketed an OS/2 x86 product. Watcom was at the time working on PowerPC version of their compiler, but OS/2 PPC was killed before that project was finished. The last IBM Developer’s Connection release which contained OS/2 PPC material also included a beta version of IBM’s VisualAge C++ compiler. No release of a compiler (or a debugger) running natively on OS/2 PPC is known.The OS/2 PPC development tools were quite different from their Intel counterparts. To begin with, instead of the LX executable format, OS/2 PPC used the industry standard ELF. Several tools were completely unchanged (IPFC for instance), many were entirely new (linker, librarian, resource compiler). The ABI (Application Binary Interface) used in OS/2 PPC was based on the UNIX SVR4 PowerPC ABI. One notable difference was that OS/2 of course ran in little endian mode, unlike PowerPC UNIX ports but just like Windows NT.Delving deeper into the kernel, OS/2 PPC had precious little in common with the Intel version. The product was based on the IBM microkernel, which was a refinement of the Carnegie Mellon University Mach microkernel. The microkernel bore no resemblance to the Intel OS/2 kernel whatsoever and it was also very different from most other operating systems of the time (NeXTSTEP was also based on the Mach microkernel).The initial grandiose plan was to build the Workplace OS, the One Ring to Bind Them All of operating systems. Workplace OS (or WPOS for short) was supposed to be built on top of the Mach microkernel and support multiple “personalities”. The personalities would implement existing operating systems such as OS/2, AIX, Windows NT and perhaps even Mac OS. In the end this never happened and the only supported personality was OS/2. This was somewhat similar to Windows NT where the the non-Windows personalities (environment subsystems) eventually withered away.The initial plan was still tangible in OS/2 PPC. The OS/2 personality was implemented in the “OS/2 Server” and there were certain “personality neutral” services. Most device drivers were personality neutral and worked directly with the microkernel. This included disk and network drivers. A notable exception were the display drivers, where OS/2 PPC introduced the GRADD model (later ported to Intel OS/2). Documentation on OS/2 PPC internals is somewhat sparse and the online books shipped with PowerPC Toolkit were in many cases either incomplete or simply unmodified copies of OS/2 for Intel documentation. A good source of information is the Redbook titled “OS/2 Warp (Power PC Edition) – A First Look” published by IBM International Technical Support Organization in December 1995, document number SG24-4630-00 for those interested.OS/2 for PowerPC ImpressionsWhat was OS/2 Warp, PowerPC Edition like? An unfinished product, rough around the edges but simultaneously technically very interesting and advanced and showing promise. Even though the OS/2 PPC release wasn’t called a beta, it was obvious that this was a beta level product (if even that in some respects). Many features were unfinished or completely missing, notably networking. The kernel code didn’t look much like a production build and printed out quite a lot of debugging output on a serial console, if one was attached. The HPFS support was very unstable, and the stability of Win-OS/2 left a lot to be desired. There were too many clearly unfinished parts of the product—documentation, missing utilities, etc.On the other hand a large portion of the system worked well. The user interface and graphics subsystem in general didn’t exhibit any anomalies. Multitasking was reliable and all things considered, responsiveness quite good for a 100MHz CPU and code that was not likely to have been performance tuned. The multimedia subsystem worked much better than expected. Many things were much improved compared to Intel OS/2—internationalization, graphics subsystem, updated console API, and so on. The system seemed to have enough raw power, even if it wasn’t harnessed too well. Boot time was rather long but once up and running, the system was snappy (with some exceptions, notably the CD-ROM driver). To reach true production quality, the OS would have needed at least additional six months of development, perhaps more.How useful was OS/2 PPC? Not very. In fact, it was almost completely useless. It only ran on three or four models of rather rare IBM machines and supported almost no additional devices. The OS was clearly unfinished and not entirely stable. Worst of all, there were about zero applications. Because OS/2 PPC was never truly in use, PowerPC versions of OS/2 applications were never sold, although several OS/2 ISVs ported their applications to OS/2 PPC as evidenced by the application sampler. Porting wasn’t very difficult and tools for building PowerPC applications were available, but since there was no demand for them, there was little point in porting.OS/2 for PowerPC was undoubtedly an interesting experiment, albeit a failed one. It is impossible to tell whether this failure was caused more by shortcomings of OS/2 for PowerPC or the failure—perhaps just falling far short of expectations—of the PowerPC platform as a whole.Without the generosity of Mike Kaply and Chris Graham, this article could not be written.Some of the above information was derived from IBM documentation and Redbooks, which may have been inaccurate due to the evolving nature of the OS/2 PPC project. Most of the remaining text is the result of observation and conjecture.If you have any additional information, corrections, or interesting stories about OS/2 for PowerPC, please post a comment.]]></content:encoded></item><item><title>Crunchyroll is destroying its subtitles</title><link>https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/</link><author>Daiz</author><category>hn</category><pubDate>Wed, 29 Oct 2025 23:31:24 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Since the beginning of the Fall 2025 anime season, a major change has started taking place at the anime streaming service Crunchyroll: the presentation quality for translations of on-screen text has taken a total nosedive compared to what has been on offer for many years, all the way up until the previous Summer 2025 season. Now, more and more subtitles on Crunchyroll are looking like this:Poor presentation quality like this isn’t entirely new to Crunchyroll, as a portion of the subtitles on the site have always been of third-party origin — that is, provided by the licensor — and Crunchyroll just puts them up with zero oversight. This in itself has caused numerousissues over the years, but the pressing issue here is that low quality presentation like this can now be found even in first-party subtitles created by Crunchyroll’s own subtitling staff. For comparison, here’s the kind of presentation quality that first-party subtitles were providing just earlier this year:Given the technical capabilities on display in the above screenshots, it should be clear that first-party subtitles for Fall 2025 shows shouldn’t look as bad as they do. Yet for some reason, what we’re getting is this low quality presentation reminiscent of third-party subtitles, where translations for dialogue and on-screen text aren’t even separated to different sides of the screen – everything is just bunched up together at either the top or the bottom. Lots of on-screen text is even left straight up untranslated.And that’s “destroying subtitles”?It sure is when it’s anime we’re talking about! Anime as a medium has made prominent use of on-screen text basically since its inception. The amount of it varies from series to series, but almost every anime out there makes use of on-screen text at one point or another, with some featuring downright ridiculous amounts of  (what on-screen text is called for short). With all this on-screen text, it is also very common for there to be text visible on the screen potentially in multiple positions, even when characters are speaking.As such, if you are in the business of localizing anime for non-Japanese audiences, you need to be able to deal with on-screen text. At bare minimum, when subtitling anime, you should be able to do  (multiple lines of text on the screen at the same time) and  (the ability to freely place subtitles anywhere on the screen). Anything less and you are likely to run into trouble the moment you get to something as simple as a next episode preview:Multiple instances of on-screen text are running in parallel with dialogue. Screenshot from  (Winter 2014, Underwater-FFF fansubs)Overlaps and positioning are really just the bare necessities for dealing with on-screen text in anime though – ideally, you should also be able to use different fonts, colors, animate text in various ways, etc. Making use of all these possibilities is an art unto itself, and this art of on-screen text localization is commonly referred to as typesetting. Typesetting is important even when dubbing anime, as all that on-screen text is going to be there in the video all the same!So why would Crunchyroll get rid of typesetting?That is a good question. It is no exaggeration to say that up to this point, Crunchyroll with its typesetting was the unambiguous market leader when it came to presentation quality for official anime subtitles… though for the most part, other services dealing in anime have never even bothered to try. Sentai Filmworks’ Hidive is just about the only other anime service that even attempts to do typesetting, though they license so few shows per season that they are a tiny player compared to the Big Boys of anime streaming.And it is very likely the existence of these Big Boys that has played a key part in Crunchyroll’s eradication of its typesetting. Netflix and Amazon Prime Video probably need no introduction to anyone reading this – both are very popular general streaming services. Despite anime being only a minor part of their catalogs, a large chunk of today’s anime watching worldwide happens through said services thanks to their sheer user counts alone.Crunchyroll clearly seems to know this, which is why it has been sublicensing its anime properties to both Amazon and Netflix for multiple years at this point. But with such sublicensing comes the matter of dealing with the subtitling standards of general streaming services. I’m not going to mince words: these standards are  at least as far as anime is concerned. Netflix for example insists that you stick to at most two lines of text on screen at once, which makes sense most of the time… if you’re talking about dialogue alone. Unfortunately, it becomes completely inadequate when dealing with anime’s plentiful on-screen text. Moreover, the standards of these services actively refuse to give you tools like positioning and overlaps, even though the TTML subtitle format they use supports said features!With such typesetting-hostile standards to deal with, Crunchyroll had basically two choices for how to make sublicensing to Amazon and Netflix work with their existing subtitles that feature actual typesetting: Either 1) try to negotiate with the services for permission to make use of more TTML capabilities (that the subtitle renderers of said services should already support!) or 2) start mangling subtitles with typesetting into something compatible with the awful subtitling standards of the general streaming services. I am not aware if Crunchyroll ever attempted the former, but I can confirm that it eventually started doing the latter.Editors among Crunchyroll’s subtitling staff were  to convert finished high quality subtitles with typesetting into limited low quality TTML subtitles without typesetting, compatible with Amazon & Netflix subtitling standards. They got paid extra for the manual effort required by the process.Overlapping on-screen text and dialogue makes for a miserable anime watching experience with limited TTML subtitles. Video clip from  (Winter 2018, Netflix)Unfortunately, after a couple years of this kind of manual conversion work, the Crunchyroll leadership seems to have decided that it isn’t enough, and that Crunchyroll must do away with high quality subtitles with typesetting entirely and only produce low quality TTML subtitles without typesetting from now on. But if they already had a working process for high quality subtitles at home and low quality TTML subtitles elsewhere, why would they just decide to give that up in order to produce exclusively low quality subtitles? It doesn’t seem to make very much sense, even as a cost-cutting measure. There should be so much value in being able to advertise best viewed on Crunchyroll to potential audiences for long-term growth, right?To understand  we need to look into some relevant history. Specifically, what happened after Sony bought Crunchyroll and merged it with Funimation, another US anime distributor that Sony had bought previously. But in order to also understand  first we need to look at what both Crunchyroll and Funimation were like before this fateful merger happened, as well as how they approached anime subtitling over the years.A short history of Crunchyroll and its subtitling standardsCrunchyroll launched in 2006 as a pirate streaming site focused on East Asian media content, featuring fansubbed anime, live action drama, music videos, and so on. There was nothing particularly remarkable about the site back then – as a rule of thumb, pirate streaming sites are always worse quality-wise than if you just directly downloaded the pirated releases they use as a base, and the sites mostly exist to make their admins illicit money through ads, begging for donations, and other shady crap. It is important to note though that legal anime streaming basically wasn’t a thing at this time.Crunchyroll in 2007. The “help out” message at the top is asking for donations.However, it was likely this exact venture capital funding that enabled Crunchyroll to negotiate a major deal with the Japanese broadcasting company TV Tokyo, which was announced at the start of 2009. This announcement brought with it the news that Crunchyroll was going full-time legitimate and getting rid of all its pirate content. With this move, Crunchyroll found itself in a position of having to start producing subtitles of its own (instead of just uploading fansubs) and somehow present said subtitles to its customers.Aegisub is an advanced subtitling software built by fansubbers, for fansubbers.For the subtitle production part, Crunchyroll managed to strike a deal with a bunch of fansubbers to take on the job. This single decision was a fateful one, as it was the foundation for basically everything that came after – with former fansubbers on the job, the tools of the trade were set according to the standards of fansubbers: the subtitling software of choice was to be Aegisub, and the subtitle format of choice was to be Aegisub’s native format, Advanced SubStation Alpha, or ASS for short.ASS is an extremely powerful format in terms of formatting and styling capabilities, and with Aegisub, it is easy to produce ASS subtitles that make use of said capabilities. However, as a streaming site, Crunchyroll needed to be able to present these ASS subtitles in the browser somehow, and the only full-fledged ASS renderers that existed were only available in the traditional local media playback environments targeted by fansubbers, which meant that Crunchyroll couldn’t make use of said renderers on the web directly.Now, there are two main ways to subtitle videos, with opposing pros and cons: – the subtitles are burned into the video itself.  as you only need to be able to play video, but inflexible for updates and multiple languages as you have to recreate your video files over and over again with expensive processing called . – the subtitles exist as their own separate media track that the video player renders on top of the video in realtime during playback, making softsubs complex to playback, but updates and multiple tracks are very cheap as you only need to deal with tiny subtitle files while the video files remain unchanged.As such, one way Crunchyroll could have solved the subtitle presentation problem would have been to simply hardsub its ASS subtitles, but despite the challenges it posed, Crunchyroll decided to go with softsubbing instead (which was also the fansub standard at the time). And so Crunchyroll set out to build its own ASS renderer in Flash, the primary technology used to play video on the web at the time. Here’s a screenshot of some of the first subtitles ever officially authored by the fully legitimate Crunchyroll, rendered in the current ASS renderer but adhering to the limits of the company’s very first Flash subtitle renderer:Screenshot from  (Spring 2009, Crunchyroll)As can be seen, even the very first version was already capable of handling both overlaps and positioning. Now, the positioning was limited to the eight edges and the center of the screen, making for just nine possible positions total, but even that was enough to handle the humble next episode preview at the very least. Beyond these, the first version also supported fading animations. It wasn’t much, but it did cover the bare minimum for dealing with on-screen text in anime.Over the years, Crunchyroll managed to slowly improve its Flash subtitle renderer to enable the use of more ASS features. Custom colors, multiple fonts, multiple styles, rotation, and full positioning were implemented (albeit in somewhat hacky and unwieldy fashion). This went on until 2018, when Crunchyroll was faced with a major issue: Flash was seeing rapid decline in use, and web streaming was shifting over to HTML5-based technology. However, with a custom ASS renderer built in Flash, Crunchyroll couldn’t easily make the change, as it would mean having to essentially rebuild the custom subtitle renderer they had from scratch in HTML5 (as much like in the Flash days, there still were no solutions native to the web available for rendering ASS subtitles).However, Crunchyroll managed to come up with a way to solve the problem of moving from Flash to HTML5 with the help of another new web technology called WebAssembly, which allowed developers to take code that wasn’t developed for the web and compile it for use on the web. With WebAssembly, Crunchyroll could take libass, one of the few fully-featured ASS renderers out there, and use it for their new HTML5 player. Now, not only did all their old ASS subtitles render nicely in HTML5, but the possibilities for typesetting at Crunchyroll had taken a huge leap forward. And the subtitling staff at Crunchyroll was more than happy to make use of this newfound power.You couldn’t see typesetting like this on Crunchyroll back in the days of Flash. Video clip from  (Fall 2022, Crunchyroll)That said, despite having a technically fully-featured ASS renderer to work with, there were still limitations. Code compiled with WebAssembly runs worse compared to its original native counterpart, which limits how heavy the typesetting can be (with the flexible features of ASS, it is very easy to produce typesetting that simply cannot be rendered in realtime even on powerful computers, resulting in notable lag during playback). A commercial service like Crunchyroll will also generally want to keep its content watchable even on lower-end devices, which further reduces how complex any typesetting can be.And this is the limited but functional standard of typesetting that Crunchyroll users got to enjoy (with first-party subtitles) up until the fateful season of Fall 2025 that prompted the creation of this article.Before we move to the conclusions for this section, though, it is worth noting that while Crunchyroll currently uses softsubbed ASS subtitles whenever it can, there are platforms and devices (like various TVs) where this kind of ASS rendering simply isn’t possible to do. Crunchyroll is available on some platforms like this, which means it has been making additional hardsubbed versions of everything on top of the usual softsubbed ones.So, what can we learn from all this? At least one thing is abundantly clear: for most of its existence, the leadership at Crunchyroll had at least some respect and understanding for anime as a medium. They understood that it was important to be able to deal with on-screen text in their subtitles, and allocated enough resources to make typesetting possible. The company even managed to improve in this regard over time, albeit very slowly.That said, anyone familiar with anime fansubs of the 2010s and 2020s probably can’t help but feel disappointed that even the highest effort typesetting from Crunchyroll could only ever be on the level of fansub releases from around 2010 at best. Why 2010 specifically? Because from 2011 onwards, fansubbers started widely incorporating advanced motion tracking into their typesetting. Observe an example of such fansub typesetting from over a decade ago, the likes of which has never been seen on Crunchyroll:Video clip from  (Fall 2013, Underwater fansubs)Now, while fansubbers giving away their work for free might get away with saying just get a better computer to anyone whose devices can’t render softsubbed typesetting like this in realtime, an official service that lots of people pay for doesn’t really have the same luxury, which is the main reason why you don’t see stuff like this softsubbed on Crunchyroll. But this is not an insurmountable problem, so make no mistake: official anime services could absolutely offer typesetting with similar level of quality to the best of fansubs. The basic solution to the performance problem is very simple, even: you simply hardsub the typesetting. This would work from streaming to physical disc releases and only the sky would be the limit in terms of the typesetting quality you could offer, as realtime rendering would no longer be a concern!Now, as mentioned earlier, hardsubbing does make things more complicated and expensive on the backend as you need to encode and store multiple copies of video. Crunchyroll is already dealing with this, though! But if costs are an issue, the system is pretty easy to improve in theory: if you keep the dialogue softsubbed, only the parts of the video that actually feature typesetting would be hardsubbed, and with some clever engineering and an understanding of how modern media formats work, you would only have to keep multiple copies of the typeset parts. And since the average anime episode has on-screen text only for a small percentage of its total runtime, combining softsubbed dialogue and hardsubbed typesetting like this would make for a highly cost-effective setup.Typesetting like this would be possible to do even for official anime services. Video clip from  (Fall 2021, Chasa fansubs)And since with a mixed system like this you would only have softsubs for the technically simpler dialogue, you could even convert these dialogue-only ASS subtitles to a simpler but more widely supported subtitle format for playback, which theoretically should do away with the need to keep fully hardsubbed copies around entirely, without any real loss in quality! I actually built a minimal version of a mixed system like this myself when I was doing some anime streaming work a few years back and can confidently say that this would be extremely doable for any official anime service… as long as they just cared enough.Keep this example of fansub typesetting in mind for later. Video clip from Danganronpa: The Animation (Summer 2013, UTW fansubs)Unfortunately, any interest Crunchyroll had for improving their subtitle rendering for typesetting seemed to run out after the 2018 transition to WebAssembly libass. Not that it actually ever seemed to be all that high to begin with, though, as evident by some of the low-hanging fruit that Crunchyroll never bothered to pick in this regard; the most obvious of which would be Crunchyroll’s dogged insistence to restrict typesetting font choices to Core Fonts for the Web. Free for commercial use fonts have been plentily available since the Flash days, and custom fonts have been well supported on the web for a similarly long time.Anyway, it would have never been all that hard for Crunchyroll to support custom fonts for typesetting, especially after the 2018 move to HTML5. The underlying technology was there and font files are tiny in size compared to the video files being streamed – this would have been an extremely simple and effective improvement for all typesetting efforts. Yet Crunchyroll never reached for this improvement, which is why  has kept appearing in Crunchyroll typesetting with depressing regularity.There it is again.  Screenshot from  (Summer 2023, Crunchyroll)It is also disappointing how regularly the anime staples of opening & ending songs are still left untranslated on Crunchyroll, though this issue is admittedly much harder to solve than you’d expect. Still, it is possible to do so, especially with Sony’s resources behind the company today. That goes double when Sony is involved in anime production in any way, as then the songs being used should be well-known to all relevant parties well in advance of airing for timely rights-clearing. So if Crunchyroll/Sony is in any way involved with an anime’s production, it should basically always be possible for songs to be translated the moment the first episode is released.But that’s enough about Crunchyroll’s history. Now it’s time to look at the other company mentioned earlier and see how they’ve fared in comparison…A short history of Funimation and its subtitling standardsIn the early 90s, Japanese-American businessman Gen Fukunaga was approached by his uncle who was working as a producer for Toei. A proposal was made: if Fukunaga could start an anime company in US, Toei would license the rights to the Dragon Ball franchise to it – a franchise that was already making mad cash in Japan. Sensing an opportunity, Fukunaga found investors, and thus in 1994 Funimation was born. A year later, Dragon Ball was on US TV, dubbed and edited to “conform to American sensibilities and tastes”.It was especially  (1989-1996) that hit it big in the US.In the early 2000s, fueled by Dragon Ball’s success, Funimation started expanding its business by getting home video distribution rights for 4Kids Entertainment licenses and non-Japanese kids’ cartoons, the latter eventually expanding into getting involved in production too. But beyond increased investment in kids’ cartoons, Funimation also started experimenting with more anime licenses of its own, the 2001 anime adaption for Fruits Basket being one of its early standout releases.Out of these various expansion attempts, “more anime” seemed to be the one to work out best, and towards the end of the 00s that became the main direction of Funimation’s business. This move was helped along by a bunch of licenses obtained from now-defunct US anime publishers Geneon USA and ADV. And in the spring of 2009, hot on the heels of Crunchyroll going legit, Funimation announced that they too were getting into the anime streaming business. The resulting anime streams from Funimation were hardsubbed and looked like this:What you see here is exactly what you got: plain text at top center or bottom center, with dialogue on bottom, and translations for all on-screen text piled up top. So while overlaps were technically supported, full positioning did not seem to be possible, which made things quite awkward the moment there was more than one sign visible on the screen at the same time. This was also the standard you could expect from Funimation’s DVD and Blu-ray releases. And beyond the way too common dialogue three-liners (which are generally terrible for readability), sometimes you even saw :Screenshot from  (Winter 2014, Funimation)The subtitling software that Funimation was using at the time was Telestream MacCaption. In terms of usability and general authoring features, it was no match for Aegisub, although it was actually capable of doing some overlaps, positioning, and styling – Funimation just never chose to make use of these capabilities for its anime subtitles.TeleStream stopped supporting MacCaption in 2023.This remained the Funimation subtitle standard all the way until 2016, when Funimation struck a deal with Crunchyroll. Going forward, subtitled releases for Funimation licenses would be found on Crunchyroll, while dubbed releases for said titles would be on Funimation’s new streaming platform, However, the only thing that really changed is that instead of Funimation content being hardsubbed on their website, it was now softsubbed on Crunchyroll to the exact same standard: plain text on top center or bottom center, often with three or more lines of dialogue at once, even.Sometimes you could see sign translations on bottom too. Screenshot from  (Spring 2017, Funimation/Crunchyroll)Nothing else of particular note happened during this time period when it comes to Funimation’s subtitles. However, it is worth mentioning that Funimation dubs did have simple hardsubbed typesetting sometimes; this only seemed happen at the whim of the dubbing side of Funimation though, as these hardsubbed signs were never present in the subbed versions, nor were they a consistent feature of Funimation dubs in general.In 2017, Sony purchased Funimation as part of its growing collection of international anime distributors (Sony had previously bought Madman Anime and AnimeLab in Australia and Wakanim in Europe). As a result of this buyout, towards the end of 2018 the license sharing deal between Funimation and Crunchyroll was dissolved and soon after Funimation started serving new subtitled streams on FunimationNow, which were softsubbed and looked like this:No longer were the subtitles even making use of overlaps. Where dialogue translation used to go on bottom and sign translation on top when both were present, now all text was stuck on the same side of the screen together, either on top or bottom, but never both at the same time anymore.How this further reduction in subtitling capabilities came about cannot be said for sure, but there are several possible explanations. For one, another major thing that happened at the end of 2018: Funimation signed a big sublicensing deal with the general streaming service Hulu, which meant dealing with Hulu’s subtitling standards and authoring accordingly limited subtitles – because as could be expected, the subtitling standards of a general streaming service did not account for the needs of anime in any real way.Only the middle column of the blackboard is translated here. Good luck figuring that out with subtitles like these. Screenshot from  (Fall 2019, Funimation/Hulu)Another possible reason for these less-than-great changes in Funimation’s subtitling standards was that around this time the company started using the cloud-based subtitling toolkit OOONA Tools by the localization service provider OOONA. OOONA Tools, by default, do not allow for the creation of subtitles with overlaps. While it can be done in OOONA today by tweaking the options or by using OOONA’s track features (which are quite similar to those of MacCaption, incidentally), it is possible that at the time these features were either not available or that it wasn’t possible to correctly export subtitles with overlaps to the WebVTT subtitle format that was being used on FunimationNow.Screenshot of , the primary subtitling software in OOONA Tools.Regarding that last possibility in particular, there is this OOONA FAQ entry that mentions how not all formats support […] overlapping subtitles and that Currently, it’s supported in IMSC1.1, ITT and Videotron Lambda CAP exports. However, based on my own testing, OOONA Tools can properly export subtitles with overlaps in more formats today than just the ones mentioned here (including WebVTT), meaning that the FAQ entry is in fact outdated – but it was likely true at some point.In any case, this was the extremely limited standard of subtitling that Funimation customers had to live with until the service was shut down in 2024 as a result of the Funimation-Crunchyroll merger.Now, what can we conclude from all this? If nothing else, one thing seems abundantly clear: the Funimation leadership never truly cared about or respected anime as a medium. From the very beginning, it’s clear that Gen Fukunaga (a businessman in his 30s at the time) got into the business with the mindset of making money with kids’ cartoons, and this only became more evident with how Funimation tried to expand into more types of kids’ cartoons before eventually realizing that anime is where the money was at.But even with this eventual focus on more anime, no resources seem to have ever been dedicated to make typesetting an actual thing at Funimation, despite how obviously beneficial it would have been for their key product of localized anime. And the way Funimation never even bothered to figure out how to make the most of MacCaption, the expensive enterprise subtitling software they kept using for over a decade… while I speculated about possible technical reasons for Funimation abandoning even overlaps when they started producing softsubs for FunimationNow, there was always one possible additional reason: they just didn’t care at all. They ran into a problem, no resources were dedicated to fix the problem, and the subtitles got permanently worse as a result.Remember the fansubbed version of this from earlier? Here’s Funimation in comparison. Video clip from Danganronpa: The Animation (Summer 2013, Funimation)The whole move to OOONA was questionable in itself, as while OOONA was capable of exporting subtitles to both WebVTT for FunimationNow and TTML (or SRT, a very limited subtitle format) for Hulu in 2018,  Why start paying for a monthly subscription service when your existing paid-for enterprise software should be able to deal with your needs just fine? I suspect the primary motivation behind the move (which could have even originated from the new parent company Sony) might have been the fact that it was trendy for companies at the time to move everything they possibly could to The Cloud™, regardless of how much sense it actually made… but that’s enough about OOONA for now.Ultimately, Funimation’s subtitling standards were extremely poor to begin with, and they only managed to make them worse over time. That is something that only utter indifference or outright disdain for anime as a medium could bring about, which seems to have been the exact attitude that Gen Fukunaga cultivated at the executive levels of Funimation – and his followers appear to have carried the torch even after his departure from the company. But more on that in the next section, when we finally get to the Funimation-Crunchyroll merger.The Funimation-Crunchyroll merger and its consequencesFollowing Sony’s 2017 purchase of Funimation, in 2019 Sony bought out Gen Fukunaga from the company entirely, which led to him stepping down as the General Manager, with Colin Decker taking his place. Soon after, Sony formed the Funimation Global Group to consolidate all the international anime publishing services it had bought, with Decker in charge of the joint venture as the CEO. Then, in late 2020, Sony announced that they were going to buy Crunchyroll, placing it under the executive control of the Funimation Global Group. The acquisition was completed in August 2021, coming with a statement from Sony that their goal is to “create a unified anime subscription experience as soon as possible”.Soon, there would only be one.Then, in March 2022, the news came that Funimation, Crunchyroll, Wakanim, and VRV (Crunchyroll’s more general streaming service) would all be merged together into a single streaming service that would exist under the name of Crunchyroll (as it had the strongest brand of the lot). Funimation Global Group LLC was renamed to Crunchyroll LLC, with Funimation executives remaining in charge. Soon after, Colin Decker stepped down as the CEO, with Rahul Purini (previously COO) taking his place. The merger was complete.Things weren’t much better for those left behind, as laid out in this Bloomberg article from 2024. Staff from Funimation was notably hostile towards those from Crunchyroll:Tension between the camps arose almost immediately. In a Zoom meeting announcing [Sony’s purchase of Crunchyroll], Funimation workers accused Crunchyroll of being pirates, alluding to the site’s history, according to two people who were present.While Crunchyroll workers were quickly frustrated with the new executives from Funimation:Current or former employees describe Crunchyroll’s new management–primarily from Funimation–as out-of-touch with employees and the anime fans the company once prioritized. Some executives write off anime as “kids’ cartoons,” they said, and resist hiring job candidates who describe themselves as fans.How typesetting gets destroyedIn 2025, the executives came up with an idea: Crunchyroll should move away from Aegisub and ASS subtitles with typesetting and start producing exclusively limited TTML subtitles without typesetting in OOONA Tools. The likely end goal of this is to get rid of Crunchyroll’s unique ASS-based subtitle rendering entirely in favor of something more “industry standard” like TTML-based subtitle rendering. This would mean no longer having to pay staff for manual ASS-to-TTML conversion, as well as being able to drop the relatively expensive fully hardsubbed encodes for limited playback environments where ASS rendering is not possible (but some sort of TTML rendering usually is).However, a major change affecting all aspects of the company’s subtitling pipeline doesn’t happen overnight, especially considering Crunchyroll’s large back catalog of ASS subtitles with typesetting that couldn’t be automatically converted to limited TTML subtitles without typesetting. So while the subtitling staff was to be (begrudgingly) busy experimenting and onboarding with OOONA and doing manual ASS-to-TTML conversions for back catalog titles, technical work would also need to be done to prepare for this vision of a TTML-only future.And what an exciting future of not being able to read signs that would be! Screenshot from This Monster Wants to Eat Me (Fall 2025, Crunchyroll)For this purpose, Crunchyroll seems to have decided that it would take its existing manual ASS-to-TTML conversions produced by the subtitling staff and treat them as the new master subtitle files. These TTML “masters” would then be—for the time being—converted back to ASS with Closed Caption Converter for use with the current ASS-based subtitle rendering. And so, with the start of the Fall 2025 anime season, a plan like this was pushed to production; while regular ASS subtitles were still being produced by Crunchyroll’s subtitling staff, these ASS subtitles with typesetting were generally left unused, while only limited ASS-to-TTML-to-ASS conversions without typesetting were being presented to customers on most shows.Implementing this interim pipeline with Closed Caption Converter didn’t seem to go exactly as planned, though, as some Fall 2025 shows on Crunchyroll ended up having no subtitles at all on release, including the premieres of the latest seasons of hit shows My Hero Academia and Spy × Family.Over the past few days, some users experienced delays in accessing the content they wanted and subtitle issues across certain series. These were caused by internal system problems – not by any change in how we create subtitles, use of new vendors or AI. Those internal issues have now been fully resolved.Quality subtitles are a core part of what makes watching anime on Crunchyroll so special. They connect global fans to the heart of every story, and we take that responsibility seriously.Thank you for your patience. We’re committed to continuing to deliver the authenticity, quality, and care that fans deserve.Following this statement, some of the new Fall 2025 shows have had their ASS-to-TTML-to-ASS subtitles switched out to the previously unused regular ASS subtitles. Other shows haven’t. And some shows in the Crunchyroll back catalog have been updated with ASS-to-TTML-to-ASS subtitles, though the exact timing of these back catalog updates is unknown.With all of this, the future of typesetting on Crunchyroll is unclear.And that’s how we’ve found ourselves in the situation we face today. Remember what the first Crunchyroll subtitles from 2009 looked like? Yeah, these new subtitles adhering to limited TTML standards are even worse than the subtitles from 2009 in terms of how on-screen text can be handled! In other words: The presentation quality of Crunchyroll’s first-party subtitles has reached an all-time low in 2025.Can’t even handle a next episode preview properly anymore. Screenshot from Chitose Is in the Ramune Bottle (Fall 2025, Crunchyroll)There is only one conclusion that can be drawn from that: the Funimation-turned-Crunchyroll executives still do not have any respect for anime as a medium. In addition, they seem to be treating Crunchyroll and its ways of doing things as the ways of  – which isn’t entirely incorrect, as Crunchyroll’s use of Aegisub and ASS  originate from the ways of pirate fansubbers. But fansubbers deeply care about anime as medium (they wouldn’t be illegally subtitling it for free as a hobby otherwise), which in turn means that the ways fansubbers have developed to subtitle anime are in fact extremely efficient for the job – much better than basically any “industry standards” for subtitling, even.But that clearly doesn’t matter to the executives. The only thing that seems to be on their mind is how to best make money with kids’ cartoons that none of them personally watch, and what they seem to consider “best” is getting rid of everything positively unique about Crunchyroll in favor of doing things the Funimation way, even if that means ditching Aegisub and ASS in favor of OOONA Tools and TTML and getting rid of typesetting in the process.  conclusion is further supported by the fact that  with notable reduction in typesetting quality on Blu-ray as a result:Then there’s the whole plan of moving to OOONA in general, which is even more questionable than it was back in the Funimation days. Crunchyroll has a lot more to lose in terms of subtitle quality than Funimation ever did, yet the executives seem to want to go back to their “old reliable” regardless. I can’t even see it saving them any money in the long run, considering that Aegisub is completely free software while OOONA will incur constant ongoing costs with its per-user subscription pricing. Rather than authoring limited TTML in OOONA directly, paying the subtitling staff to keep the manual ASS to TTML conversions going would likely be cheaper!Beyond that, there is also the thing about OOONA being an Israeli company. It is certainly a choice, not only in 2018 but most certainly in 2025, to heavily invest in the services of a company from a country that is actively committing genocide. However, to quell some unsubstantiated internet discourse I have seen in relation to this, I do want to emphasize that OOONA being Israeli is not really directly relevant to the quality issues this article is about.EZTitles is another popular enterprise subtitling software. Notice how they mention AI directly in their navigation.The reason for this lies in enterprise subtitling software (“industry standards”) being universally poor when it comes to producing high quality typesetting for anime, so it wouldn’t really matter which software suite a switch was being made to – no matter what, moving away from Aegisub would destroy typesetting as it currently exists on Crunchyroll. And while Crunchyroll’s CEO has expressed his interest in AI subtitles, at least currently there has been no signs of any kind of AI (Israeli or otherwise) being used to create first-party subtitles on Crunchyroll.Why Crunchyroll is so confident it will get away with this (or: how capitalism ruins everything)Finally, I want to talk about the possible reasons for Crunchyroll executives feeling so confident about getting away with making their own primary product so much worse. Ultimately, it comes down to the fact that international anime licensing operates primarily on an exclusive licensing model. This means that generally only one service will be able to offer a specific title in specific language(s) in specific region(s), unless the service voluntarily decides to sublicense it out to others. This in turn upends the assumption that the existence of multiple anime services would be beneficial to consumers, as the services don’t actually have to engage in competition on customer-beneficial factors like service quality almost at all – instead, they can just focus on hoarding as many exclusive licenses as possible.I once asked former Crunchyroll CEO Kun Gao about “exclusivity or completeness” in this Reddit AMA. He dodged the question but basically said “exclusivity”.This kind of “competition” twisted by exclusive licensing is more like a casino, where the customers might occasionally be thrown a bone, but at the end of the day, the house always wins. And the anime companies very much prefer to keep it that way, even if it means never being able to offer full coverage of new anime seasons – a limited amount of exclusives is much more important to them. Dreams of infinite growth are what drives the modern-day game of capitalism, and spending money to please customers rather than shareholders goes directly against said dreams. It’s all about spending as little money as possible to make as much money as possible.This is why the capitalists in charge of all the big companies these days are so excited about AI too: nothing gets them going more than the idea of not having for pay for those pesky human employees. This is no doubt the actual reason why Crunchyroll CEO Rahul Purini is interested in AI subtitles. It doesn’t matter that anime localization costs are a drop in the bucket compared to the overall costs of anime production, even if you were talking about super high quality work with fansub-level typesetting. Any excuse to cut the wages of real human workers is one step closer to the next yacht purchase for the executive upper class.…Whew, got a bit heated there. Anyway, the most likely reason why Crunchyroll executives believe they can get away with reducing the quality of their own service so much? Because Crunchyroll doesn’t have any meaningful competition thanks to the primarily-exclusive licensing model used by the international anime industry. Even if they make the service worse, what can you do about it? Cancel your subscription and not watch the new anime you’re excited about?If you are currently subscribed to Crunchyroll, cancel your subscription.When asked for a reason, mention the bad subtitle quality and lack of typesetting.You could even link to this article. Beyond that, and this applies to people who aren’t subscribed to Crunchyroll as well:  Share this article around, talk to people about how Crunchyroll is destroying its subtitles, make it so that Crunchyroll executives can’t ignore the issue. And the most important thing: Keep it up until Crunchyroll actually makes a clear public commitment to keep typesetting anime.“Improving Subtitle Quality for Crunchyroll” is what we’d like to see here in 2025.I also want to emphasize that the recent statement Crunchyroll made about its Fall 2025 subtitles isn’t really worth anything. It’s worded in an intentionally obfuscated manner as to what actually has been  – is it the lack of typesetting or just the issues with subtitles not going up for new releases? Then it just outright lies about there being  with how subtitles are being handled, before ending on empty platitudes about  that mean nothing without concrete actions to back them up.And so far,  The lower quality subtitles in the back catalog are especially alarming, as the back catalog was exactly where Crunchyroll also started with its 2017 video quality reduction plans, all the while remaining careful with changes to simulcasts where people were paying closer attention – which is exactly what seems to be happening with subtitles on Crunchyroll right now. Without a clear public commitment to stick to higher subtitling standards that include typesetting, it is very likely that Crunchyroll executives will just delay their typesetting-killing plans and try again later. That’s why  need to cancel your subscription, encourage others to do so, and keep talking about this issue until Crunchyroll explicitly promises to do better.Together, we can save Crunchyroll from itself!This article would have never been as thorough and detailed as it is without the assistance of the following people:The multiple current and former Crunchyroll and Funimation workers who came forward to indepedently confirm the many previously unpublished details found in this article. BigOnAnime – for his great help with researching the historical technical details of Funimation’s subtitling standards. enonibobble – for his help with various screenshots and technical analysis of Crunchyroll subtitles. Faye Duxovni – for bringing Crunchyroll’s use of old Funimation workflows for Blu-rays to my attention and providing the screenshots of it that are used in the article.  who answered public questions I asked or otherwise helped with various small pieces of research. I’m not the only one to have made note of Crunchyroll’s recent subtitle shenanigans, so here’s some additional reading/watching on the subject elsewhere:Why did Crunchyroll’s subtitles just get worse? by  (former head of marketing for Crunchyroll), on the newsletter  This includes some additional details (like numbers!) that I didn’t go over here (because this article was long enough as-is), so I can recommend giving it a read.The Absolute State of Crunchyroll by YouTuber  This is a good watch just to see how bad the new Crunchyroll subtitles look like in action. Additionally, I didn’t really talk about how badly timing quality has been affected by the recent changes too, but this video has some good examples of that as well.Are Subtitles Getting Smaller? by  on . This  column is nominally about subtitles getting visually smaller, but most of it ends up being about the Crunchyroll subtitle situation. Jerome does keep incorrectly saying that general streaming services use the very bare-bones subtitle format SRT rather than TTML, though, and while these services do support SRT for ingestion (ie. content partners can deliver subtitles as SRT) and anime companies might even be making use of that, TTML is what the services actually use internally. SRT does not officially support any kind of positioning whatsoever, which means that even placing subtitles at the top of the screen would be impossible with it if the normal placement was on bottom.The Crunchyroll Sub Flub by  and , also on . Nothing particularly new in this one if you’re familiar with all the other coverage, but it’s nice to see this get discussed on the  column regardless. The more eyes on the subject, the better.I’m , a digital distribution expert and high quality media enthusiast. I have over a decade of experience with Japanese-to-English media localization, including anime subtitling, and I also care deeply about consumer rights. You can follow me on Bluesky, or drop me a mail.I’m working on getting Bluesky comments embedded at the end of the posts. For the time being though, you can read and join the discussion here!]]></content:encoded></item><item><title>Raspberry Pi Pico Bit-Bangs 100 Mbit/S Ethernet</title><link>https://www.elektormagazine.com/news/rp2350-bit-bangs-100-mbit-ethernet</link><author>chaosprint</author><category>hn</category><pubDate>Wed, 29 Oct 2025 23:21:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Three years ago, @kingyoPiyo’s Pico-10BASE-T project drew wide attention right here on Elektor for implementing 10 Mbit/s Ethernet on the Raspberry Pi Pico using just a few resistors. In 2023, another milestone followed with bit-banged USB, showing how far the RP2040’s (and now RP2350) programmable I/O could be pushed.What Can an RP2350 Bit-Bang Next?Now, developer Steve Markgraf (GitHub @steve-m) has extended the concept with Pico-100BASE-TX — a 100 Mbit/s Fast Ethernet transmitter running entirely in software.
Markgraf’s implementation uses the PIO and DMA to perform MLT-3 encoding, 4B5B line coding, and scrambling at a 125 MHz symbol rate. The result is a functioning 100 Mbit/s link capable of streaming about 11 Mbyte/s over UDP, demonstrated by real-time audio and ADC data streams.As before, this is a transmit-only proof of concept and must not be connected to PoE-enabled hardware. A pulse transformer or intermediary Ethernet switch is recommended for isolation.Check Out the Rest of His RepoExample applications in the repository include a counter, internal-ADC streamer, and an audio demo using a PCM1802 converter at 75 kHz. The library supports both the RP2040 and the newer RP2350 (Pico 2) and builds with the standard Pico SDK.Beyond the technical achievement, projects like this hint at new possibilities for low-cost, high-speed data acquisition and streaming using microcontrollers that were never designed for it. A Pico capable of pushing 11 MB/s over Ethernet could form the basis of compact, inexpensive test instruments, remote sensors, or experimental network interfaces — all without a dedicated PHY chip. As these bit-banged interfaces become faster and more capable, the question naturally follows: how far can software-defined hardware really go on a two-dollar microcontroller? Subscribe to the tag Raspberry Pi and you will receive an e-mail as soon as a new item about it is published on our website! ]]></content:encoded></item><item><title>Meta and TikTok are obstructing researchers&apos; access to data, EU commission rules</title><link>https://www.science.org/content/article/meta-and-tiktok-are-obstructing-researchers-access-data-european-commission-rules</link><author>anigbrowl</author><category>hn</category><pubDate>Wed, 29 Oct 2025 22:54:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Responses from LLMs are not facts</title><link>https://stopcitingai.com/</link><author>xd1936</author><category>hn</category><pubDate>Wed, 29 Oct 2025 21:40:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Imagine someone who has read thousands of books, but doesn’t remember where they read what.
          What kinds of things might they be  at?
        
          What kinds of things might they be  at?
        
          Sure, you  get an answer that’s right or advice that's good…
          but what “books” is it “remembering” when it gives that answer?
          That answer or advice is a common combination of words, not a fact.
        ]]></content:encoded></item><item><title>How the U.S. National Science Foundation enabled Software-Defined Networking</title><link>https://cacm.acm.org/federal-funding-of-academic-research/how-the-u-s-national-science-foundation-enabled-software-defined-networking/</link><author>zdw</author><category>hn</category><pubDate>Wed, 29 Oct 2025 21:22:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[SDN Grew First and Fastest in DatacentersThe first large-scale deployments of SDN took place in hyperscale data centers, beginning about 2010. The story is best told by the hyperscaler companies themselves, and so we asked leaders at Google, Microsoft Azure, and Meta to tell their stories about why and how they adopted SDN. As you will see, they all started from the ideas and principles that came from the NSF-funded research; and each tailored SDN to suit their specific needs and culture.The Internet Service Providers (ISPs) and telecommunication companies also had a strong interest in SDN. AT&T played a large role in its definition, engaging in research and early deployments in the mid 2000s. We invited Albert Greenberg, who was at AT&T at the time, to tell the story.Nicira was perhaps the startup that epitomized the SDN movement. It grew out of the NSF-funded  program and the Clean Slate Program at Stanford, based on the Ph.D. work of Martín Casado. Nicira developed ONIX, the first distributed control plane, used by Google in its infrastructure; OVS, the first OpenFlow-compliant software switch; and NVP (later NSX), the first network virtualization platform. We invited Teemu Koponen, a principal architect at Nicira, to tell the story.During the early 2010s, the networking industry began to realize that SDN has many big advantages. It lifts complex protocols up and out of the switches into the control plane, where it is written in a modern programming language. This made it possible to reason about the correctness of the protocols simply by examining the software controlling the network and the forwarding state maintained by the switches. For the first time, it became possible to formally verify the behavior of a complete network.Researchers, startups, network equipment vendors, and hyperscalers have all taken advantage of SDN principles to develop new ways to verify network behavior. We invited Professor George Varghese, who has been deeply involved in network verification research, to give us his perspective on network verification.A main benefit of SDN is that it hands over the keys (of control) from the networking equipment vendors—who kept their systems closed and proprietary, and hence tended to evolve slowly—to software programmers, who could define the behavior for themselves, often in open source software. And indeed it happened: Today, most large networks are controlled by software written by those who own and operate networks rather than by networking equipment vendors.But what about the hardware? Switches, routers, firewalls, and network interface cards are all built from special-purpose ASICs—highly integrated, cost-effective, and super-fast. The problem was the features and protocols that operated on packets (for example, forwarding, routing, firewalls, and security) were all baked into hardware at the time the chip was designed, two to three years before it was deployed. What if the network owner and operator needed to change and evolve the behavior in their network, for example to add a new way to measure traffic or a new way to verify behavior? A group of researchers and entrepreneurs set out to make the switches and NICs programmable by the user, to allow more rapid improvement and give the operator greater control. Not only did new programmable devices emerge, but a whole open source movement around the P4 programming language.We invited Professor Nate Foster, who leads the P4 language ecosystem, to tell the story of how programmable forwarding planes came about.So far, we have focused on SDN  networks running over electrical and optical cables in datacenters, enterprises, and long-haul WANs. SDN was originally defined with wireline networks in mind.Yet, for cellular networks, the most widely used networks in the world, the need was even greater: Cellular networks have been held back for decades by closed, proprietary, and complex “standards” designed to allow equipment vendors to maintain a strong grip on the market. SDN provides an opportunity to open up networks, introducing well-defined control APIs and interfaces, moving control software to common operating systems running on commodity servers.This story has only just begun, but it started thanks to NSF-funded research in the mid 2000s, then boosted by DARPA-funded programs to support open source software for cellular infrastructure. We invited Guru Parulkar and Oğuz Sunay to tell the story, both of whom developed open source cellular systems at the Open Networking Foundation and for the DARPA-funded Pronto project.]]></content:encoded></item><item><title>Uv is the best thing to happen to the Python ecosystem in a decade</title><link>https://emily.space/posts/251023-uv</link><author>todsacerdoti</author><category>hn</category><pubDate>Wed, 29 Oct 2025 18:57:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[23 October 2025 | Reading time: 6 minutesIt’s 2025. Does installing Python, managing virtual environments, and synchronizing dependencies between your colleagues really have to be so difficult? Well… no! A  new tool called uv came out recently that  how easy installing and using Python can be.uv is a free, open-source tool built by Astral, a small startup that has been churning out Python tools (like the excellent linter Ruff) for the past few years. uv can:Install any Python version for youManage virtual environmentsSolve dependency conflicts  quickly ( important for big projects.)What’s best is that it can do all of the above better than any other tool, in my opinion. It’s , written in Rust, and works on almost any operating system or platform.uv is straightforward to install. There are a few ways, but the easiest (in my opinion) is this one-liner command — for Linux and Mac, it’s: https://astral.sh/uv/install.sh or on Windows in powershell:powershell  ByPass You can then access uv with the command . Installing uv will not mess up any of your existing Python installations — it’s a separate tool, so it’s safe to install it just to try it out.Managing Python for a projectIt’s always a good idea to work with virtual environments for any Python project. It keeps different bits of code and dependencies ringfenced from one another, and in my experience, it can save  of hassle to get into the habit of using virtual environments as soon as you can. uv naturally uses virtual environments, so it’s very easy to start using them if you get into using uv.uv will build a Python environment for you based on what’s specified in a  file in the directory (or parent directories) you’re working in.  files are a standard, modern format for specifying dependencies for a Python project. A barebones one might look a bit like this:In essence, it just has to specify which Python version to use and  Adding a name and version number also aren’t a bad idea.(Sidenote: for projects that you publish as packages, such as to the Python Package Index that pip and uv use,  files are a modern way to specify everything you need to publish your package.)Making a new project with uvTo start a new Python project with uv, you can runWhich will create a new project for you, with a , a , and other important bits of boilerplate.There are a lot of different ways to run this command, like  (which only creates a pyproject.toml),  (which sets up a new Python package), and more. I recommend running  to read about them.Once you have/if you already have a  fileOnce you initialize a project — or if you already have a  file in your project — it’s very easy to start using uv. You just need to doin the directory that your  file is in. This command (and in fact, most uv commands if you haven’t ran it already) will:Automatically install a valid version of PythonInstall all dependencies to a new virtual environment in the directory Create a  file in your directory, which saves the  version of  package installed — meaning that other colleagues can replicate your Python environment exactly.In principle, you can ‘activate’ this new virtual environment like any typical virtual environment that you may have seen in other tools, but the most ‘uv-onic’ way to use uv is simply to prepend any command with . This command automatically picks up the correct virtual environment for you and runs your command with it. For instance, to run a script — instead of .venv/bin/activate
python myscript.pywhich will have the same effect. Likewise, to use a ‘tool’ like Jupyter Lab, you can just doin your project’s directory, as opposed to first ‘activating’ the environment and then running  separately.You can always just edit your  file manually: uv will detect the changes and rebuild your project’s virtual environment. But uv also has easier ways to add dependencies — you can just doto add a package, including specifying version constraints (like the above.) This command automatically edits your  for you.  is also extremely powerful for adding remote dependencies from git or elsewhere on your computer (but I won’t get into that here.)Finally, I think that one of the most useful things uv can do is to pin a specific Python version for your project. Doingwould pin the current project to  Python 3.12.9 for you, and anyone else using uv — meaning that you really can replicate the  same Python install across multiple machines.uvx: ignore all of the above and just run a tool, now!But sometimes, you might just want to run a tool quickly — like using Ruff to lint code somewhere, or starting a Jupyter notebook server without an environment, or even just quickly starting an IPython session with pandas installed so you can open up a file. The  command, which has a short alias , makes this insanely easy. Running a command likewill automatically download the tool you want to use and run it in a one-off virtual environment. Once the tool has been downloaded before, this is lightning-fast because of how uv uses caches.There are a lot of occasions when I might want to do this — a common one might be to quickly start an IPython session with pandas installed (using  to add dependencies) so that I can quickly open & look at a parquet file. For instance:uvx  pandas,pyarrow ipythonOr, maybe just starting a Jupyter Lab server so that I can quickly open a Jupyter notebook that a student sent me:Or honestly just so many other weird, one-off use cases where  is really nice to have around. I don’t feel like I’m missing out by always using virtual environments, because  always gives you a ‘get out of jail free’ card whenever you need it.If that hasn’t sold you: a personal noteI first discovered uv last year, while working together with our other lovely developers on building The Astrosky Ecosystem — a wonderful project to build open-source social media integrations for astronomers online. But with multiple developers all working asynchronously on multiple operating systems, managing Python installations quickly became a huge task.uv is an incredibly powerful simplification for us that we use across our entire tech stack. As developers, we can all work with identical Python installations, which is especially important given a number of semi-experimental dependencies that we use that have breaking changes with every version. On GitHub Actions, we’re planning to use uv to quickly build a Python environment and run our unit tests. In production, uv already manages Python for all of our servers.It’s just  to always know that Python and package installation will  be handled consistently and correctly across all of our machines. That’s why uv is the best thing to happen to the Python ecosystem in a decade.]]></content:encoded></item><item><title>Extropic is building thermodynamic computing hardware</title><link>https://extropic.ai/</link><author>vyrotek</author><category>hn</category><pubDate>Wed, 29 Oct 2025 18:25:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Our open-source Python library that enables everyone to develop thermodynamic algorithms and simulate running them on TSUs]]></content:encoded></item><item><title>Dithering – Part 1</title><link>https://visualrambling.space/dithering-part-1/</link><author>Bogdanp</author><category>hn</category><pubDate>Wed, 29 Oct 2025 18:21:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[tap/click the right side of the screen to go forward →I’ve always been fascinated by the dithering effect. It has a unique charm that I find so appealing.← tap/click the left side to go backI was even more amazed when I learned how dithering works.← or use arrow keys to navigate →Look closely, and you’ll see this animation is made of alternating black and white pixels.But these black and white pixels are specifically arranged to create the illusion of multiple shades.That’s what dithering does: it simulates more color variations than what are actually used.Here, it uses black and white to give the impression of multiple gray shades.To me, dithering is about creating the most out of what we have, and that's what amazes me the most!It inspired me to learn more about it, and now I want to share what I’ve learned.Please note that this is just part one out of three, so I’ll only scratch the surface here.I’ll go deeper in the next parts, which will come soon. Stay tuned!First, let’s explore the dithering basics with this grayscale image example.A grayscale image has various gray shades, from black to white.Imagine a display that only shows black or white pixels, no grays. We must turn some pixels black and others white—but how?One way is to map each pixel to the closest available color.Pixels darker than medium gray turn black and lighter ones turn white.This splits pixels into black or white groups.However, this creates a harsh image with abrupt black-white transitions.Shadow details vanish as gray pixels become fully black or white.Dithering fixes this by selectively pushing some pixels towards the opposite color.Some light gray pixels that are closer to white turn black.Likewise, some dark grays turn white.And it's done in a way that produces special patterns which simulate shades by varying the black-and-white pixel densities.Denser black pixels are used in darker areas, while denser white pixels are used in lighter ones.Next question: How are these patterns generated?One simple dithering method, known as ordered dithering, uses a threshold map.A threshold map is a grid of values representing brightness levels, from 0 (darkest) to 1 (brightest).To dither, we compare each input pixel’s brightness to a corresponding threshold value.If a pixel’s brightness exceeds the threshold (it’s brighter than the threshold), the pixel turns white. Otherwise, it turns black.Repeating this for all pixels gives us the black-and-white dither patterns.The threshold map is designed to output patterns where the black-and-white pixel density matches the input image’s shades.So brighter input produces patterns with more white, while darker input produces more black.These black-and-white density variations are what create the illusion of gray shades when viewed from a distance.To dither larger images, we extend the threshold map to match the image size and follow the same principle:Compare each pixel’s brightness to the threshold map, then turn it black or white accordingly.The image now uses only two colors, but its overall appearance is preserved.The variations in shades are now replaced by variations in black/white pixel density of the dithering patterns.And that’s how dithering works in a nutshell: it replicates shades with fewer colors, which are strategically placed to maintain the original look.I find it a bit ironic how I used to think dithering ‘adds’ a cool effect, when what it actually does is ‘remove’ colors!That's all for now! We’ve reached the end, but there’s still a lot more to explore.For example, we haven’t explored the algorithm to create a threshold map. (spoiler: there are many ways!)There’s also another algorithm called error diffusion, which doesn’t use a threshold map.Each algorithm creates a distinct, unique look, which I believe deserves its own article.And that's why I decided to break this series into three parts.In the next part, I’ll dive into various algorithms for creating threshold maps.In the final part, I’ll focus on the error diffusion algorithm.We'll dive even deeper into dithering's mechanisms in these next 2 parts, so stay tuned!visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.If you like this kind of visual article, please consider following me on X/Twitter and sharing this with your friends.I'll keep creating more visual articles like this!https://x.com/damarberlari]]></content:encoded></item><item><title>The Internet runs on free and open source software and so does the DNS</title><link>https://www.icann.org/en/blogs/details/the-internet-runs-on-free-and-open-source-softwareand-so-does-the-dns-23-10-2025-en</link><author>ChrisArchitect</author><category>hn</category><pubDate>Wed, 29 Oct 2025 18:16:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Free and open-source software (FOSS) is not merely common on the Internet; it is a deeply embedded and essential foundation of the Domain Name System (DNS), the backbone of how we connect online.As governments around the world explore new cybersecurity regulations, the ubiquity of FOSS in DNS operations—from domain registration to retrieval—means that policy decisions made today will have direct implications for the Internet's security and resilience tomorrow. SAC132 provides timely, nontechnical guidance to ensure that new policy and regulation serve to strengthen, rather than inadvertently weaken, this critical infrastructure.Key Insights for PolicymakersSAC132 is a foundational guide designed to empower policymakers to strategically manage and sustain the FOSS ecosystem. The report provides: – An accessible overview of the DNS and the FOSS development model for nontechnical audiences. – Analysis of cybersecurity regulations in the United States, United Kingdom, and European Union, with a focus on how they account for FOSS in the DNS ecosystem. – Concrete findings and recommendations to help policymakers support and secure FOSS as a cornerstone of global connectivity.We invite all policymakers, technical experts, and stakeholders to read the full report.By publishing SAC132, SSAC seeks to raise awareness of the indispensable role of FOSS in maintaining a secure, stable, and resilient Internet. We invite policymakers, technical experts, and all stakeholders to read the full report and join us in conversations about its findings.You can engage with SSAC and the broader community at , whether in Dublin or by participating remotely. Together, we can ensure that the FOSS ecosystem—and the Internet it supports—remains strong, sustainable, and open for all.Finally, we thank all SSAC members and invited experts who contributed to this work, especially co-chairs Maarten Aertsen and Barry Leiba, for their leadership.]]></content:encoded></item><item><title>Encoding x86 Instructions</title><link>https://www-user.tu-chemnitz.de/~heha/hs/chm/x86.chm/x86.htm</link><author>st_goliath</author><category>hn</category><pubDate>Wed, 29 Oct 2025 17:50:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
Source: CIS-77 Home
It is time to take a look that the actual machine instruction format of the 
  x86 CPU family.
  They don't call the x86 CPU a Complex Instruction Set Computer (CISC) for 
  nothing!
  Although more complex instruction encodings exist, no one is going to 
  challenge that the x86 has a complex instruction encoding!
1. x86 Instructions Overview
 x86 Instruction Encoding:
 

Although the diagram seems to imply that instructions can be up to 16 
bytes long, in actuality the x86 will not allow instructions greater 
than 15 bytes in length.

The prefix bytes  the  discussed earlier - they are special bytes to modify the 
behavior of existing instructions.

2. x86 Instruction Format Reference
 Another view of the x86 instruction format:
 

Additional reference:


The x86 CPU supports two basic opcode sizes:
-byte opcode consisting of a 
  opcode expansion prefix byte.
  The second byte then specifies the actual instruction. 
The x86 opcode bytes are 8-bit equivalents of  field 
  that we discussed in simplified encoding.
  This provides for up to 512 different instruction classes, although the x86 
  does not yet use them all.
3.1. x86 ADD Instruction Opcode
  x86 ADD instruction opcode:Bit number , marked , specifies 
  the  of the data transfer:
  If  then the destination operand is a memory 
          location, e.g. If  then the destination operand is a 
          register, e.g. 

Bit number  marked  specifies 
the  of the operands the  instruction operates upon:
If  then the operands are 8-bit registers and 
  memory locations.
  If  then the operands are either 16-bits or 
  32-bits:
  Under 32-bit operating systems the default is 32-bit operands if 
    .
    To specify a 16-bit operand (under Windows or Linux) you must 
    insert a special  in front of the 
    instruction (example of this later.)
  
You'll soon see that this direction bit  creates a 
problem that results in one instruction have two different possible 
opcodes.

4. Encoding x86 Instruction Operands, MOD-REG-R/M Byte

The  byte specifies instruction 
operands and their addressing mode: 


  The  field specifies x86 addressing mode:Register indirect addressing mode or SIB with no displacement
	(when R/M = 100) or Displacement only addressing mode (when R/M = 101).
  One-byte signed displacement follows addressing mode byte(s).
  Four-byte signed displacement follows addressing mode byte(s).
  Register addressing mode.
  
  The  field specifies source or destination 
  :Register if data size is eight bits
    Register if data size is 16-bits
    Register if data size is 32 bits
  
The  field, combined with , 
specifies either
the second operand in a -operand instruction, or
  the only operand in a -operand instruction 
      like  or .

The  bit in the opcode determines which operand is 
the source, and which is the destination:
, REG is the source 
  , REG is the destination
 Technically, registers do not have an 
address, but we apply the term  to registers 
nonetheless.

5. General-Purpose Registers
  Since the processor accesses registers more quickly than it accesses 
  memory, you can make your programs run faster by keeping the 
  most-frequently used data in registers.
The , , , 
    , , , 
    and  registers are 32-bit 
    general-purpose registers, used for temporary data storage and memory 
    access.
  The , , , 
    , , , and 
     registers are 16-bit equivalents of 
    the above, they represent the low-order 16 bits of 32-bit registers.
  The , , , and 
     registers represent the high-order 8 
    bits of the corresponding registers.
  Similarly, , , , and  represent the low-order 8 bits of the registers. 
6. REG Field of the MOD-REG-R/M Byte

See MOD-REG-R/M Byte.

Depending on the instruction,
this can be either the  or the destination operand.

Many instructions have the  (direction) field in 
their opcode to choose  operand role:
If ,  is the source, 
  .
  If ,  is the destination,
  . 
 For certain (often single-operand or 
  immediate-operand) instructions, the  field may contain an 
   rather than the register bits. The 
   field will specify the operand in such case.

9. MOD R/M Byte and Addressing Modes MOD R/M Addressing Mode
=== === ================================
 00 000 [  ]
 01 000 [  +  ]               (1)
 10 000 [  +  ]
 11 000   (  /  /  )   (2)
 00 001 [  ]
 01 001 [  +  ]
 10 001 [  +  ]
 11 001   (  /  /  )
 00 010 [  ]
 01 010 [  +  ]
 10 010 [  +  ]
 11 010   (  /  /  )
 00 011 [  ]
 01 011 [  +  ]
 10 011 [  +  ]
 11 011   (  /  /  )
 00 100   Mode                     (3)
 01 100   +    Mode
 10 100   +    Mode
 11 100   (  /  /  )
 00 101 32-bit Displacement-Only Mode (4)
 01 101 [  +  ]
 10 101 [  +  ]
 11 101   (  /  /  )
 00 110 [  ]
 01 110 [  +  ]
 10 110 [  +  ]
 11 110   (  /  /  )
 00 111 [  ]
 01 111 [  +  ]
 10 111 [  +  ]
 11 111   (  /  /  )
Addressing modes with 8-bit displacement fall in the range -128..+127 
    and require only a single byte displacement after the opcode (Faster!)
  The size bit in the opcode specifies 8 or 32-bit register size.
    To select a 16-bit register requires a prefix byte.
  The so-called scaled indexed addressing modes,  = 
    scaled index byte mode.
  Note that there is no [  ] addressing.
    It's slot is occupied by the 32-bit  addressing mode.
    Intel decided that programmers can use [ +  ] 
    addressing mode instead, with its 8-bit displacement set equal to zero 
    (instruction is a little longer, though.) 
8. SIB (Scaled Index Byte) Layout
  Scaled index byte layout:Scaled indexed addressing mode uses the second byte (namely, 
     byte) that follows the  
    byte in the instruction format.
  The  field still specifies the displacement size 
    of , , or  
    bytes.
  The  and SIB bytes are complex, because 
      Intel reused 16-bit addressing circuitry in the 32-bit mode, rather 
      than simply abandoning the 16-bit format in the 32-bit mode.
    There are good hardware reasons for this, but the end result is a 
      complex scheme for specifying addressing modes in the opcodes. 
  8.1. Scaled Indexed Addressing Mode[  + * ] MOD = 00
[  + * ] 
[  + * ]
[  + * ]
[  + * ]
[  + * ]
[  + * ]

[  +  + * ] MOD = 01
[  +  + * ]
[  +  + * ]
[  +  + * ]
[  +  + * ]
[  +  + * ]
[  +  + * ]

[  +  + * ] MOD = 10
[  +  + * ]
[  +  + * ]
[  +  + * ]
[  +  + * ]
[  +  + * ]
[  +  + * ]

[  + * ] MOD = 00, and
[  + * ] BASE field = 101
[  + * ]
[  + * ]
[  + * ]
[  + * ]
[  + * ]


Note:  = , , 
, or .

In each scaled indexed addressing mode the 
 field in  byte 
specifies the . It can be zero, one, or 
four bytes:    MOD R/M  Addressing Mode
    --- ---  --------------------------- 
     00 100  
     01 100   + 
     10 100   + 
The  and  fields of the SIB 
byte select the base and index registers, respectively.

Note that this addressing mode does not allow the use of the 
 register as an index register. Presumably, Intel 
left this particular mode undefined to provide the ability to extend the 
addressing modes in a future version of the CPU.
9.1. Encoding ADD Instruction ExampleThe opcode can 
  be decimal 0, 1, 2, or 3, depending on the direction and size bits in the 
  opcode: How could we encode various forms of the  instruction using different addressing modes? 
  9.2 Encoding ADD CL, AL InstructionInteresting side effect of the direction bit and the  organization: some instructions can have two different opcodes, 
  and both are legal! could be  (if ), or , if  bit is set to . The possibility of opcode duality issue here applies to all instructions 
  with two register operands. 9.3. Encoding ADD ECX, EAX InstructionNote that we could also encode ,  using the bytes . 9.4. Encoding ADD EDX, DISPLACEMENT InstructionEncoding the , 
  DISP Instruction: 9.5. Encoding ADD EDI, [EBX] InstructionEncoding the , [ 
   ] instruction: 9.6. Encoding ADD EAX, [ ESI + disp8 ] InstructionEncoding the , [ 
   +  ] instruction:     add eax, [ esi + disp8 ]
    9.7. Encoding ADD EBX, [ EBP + disp32 ] InstructionEncoding the , [ 
   +  ] instruction:     add ebx, [ ebp + disp32 ]
    9.8. Encoding ADD EBP, [ disp32 + EAX*1 ] InstructionEncoding the , [ 
   + *1 ] Instruction     add ebp, [ disp32 + eax*1 ]
    9.9. Encoding ADD ECX, [ EBX + EDI*4 ] InstructionEncoding the , [ 
   + *4 ] Instruction     add ecx, [ ebx + edi*4 ]
    10. Encoding ADD Immediate Instruction
 Encoding x86 immediate operands:
  and  bytes have no 
bit combinations to specify an immediate operand.

Instead, x86 uses a entirely different instruction format to specify instruction with an immediate operand.

There are three rules that apply:
If opcode high-order bit set to , then instruction 
  has an immediate constant.
  There is no direction bit in the opcode:
  : indeed, you cannot specify a constant as a destination operand!
    Therefore, destination operand is always the location encoded in the 
     bits of the the  byte. 
    In place of the direction bit , the opcode has a sign 
    extension  bit instead:
    For 8-bit operands, the CPU ignores  bit.
      For 16-bit and 32-bit operands,  bit specifies the 
      size of the  following at 
      the end of the instruction:
      If  bit contains , the  is the same 
        size as the operand (i.e., 16 or 32 bits).
        If  bit contains , the  is a  8-bit value, and the CPU sign-extends 
        this value to the appropriate size before adding it to the operand. 
      This little  trick often makes programs shorter, 
      because adding small-value constants to 16 or 32 bit operands is very 
      common.
    The third difference between the -immediate and the standard  instruction is the meaning of the 
   field in the  byte:
  Since the instruction implies that
    the source operand is a constant, and
       fields specify the destination operand, 
    
    the instruction does not need to use the  field to 
    specify an operand.
    Instead, the x86 CPU uses these three bits as an .
    For the -immediate instruction the 
     bits must contain zero.
    Other bit patterns would correspond to a different instruction. 
  
  Note that when adding a constant to a memory location, the displacement (if 
  any) immediately precedes the immediate (constant) value in the opcode 
  sequence.
11. Encoding Eight, Sixteen, and Thirty-Two Bit Operands
 x86 ADD Opcode:
 When Intel designed the 8086, one bit in the opcode, 
        , selected between 8 and 16 bit integer operand sizes. 
  Later, when CPU added 32-bit integers to its architecture on 80386 
        chip, there was a problem:
  three encodings were needed to support 8, 16, and 32 bit sizes. 
  Solution was an .
  Intel studied x86 instruction set and came to the conclusion:
  in a 32-bit environment, programs were more likely to use 8-bit and 
    32-bit operands far more often than 16-bit operands.
  So Intel decided to let the size bit  in the opcode 
  select between 8- and 32-bit operands.
11.1. Encoding Sixteen Bit Operands
 x86 instruction format:
 

32-bit programs don't use 16-bit operands that often, but they do 
need them now and then.

To allow for 16-bit operands, Intel added prefix a 32-bit mode 
instruction with the  with value 
.

This prefix byte tells the CPU to operand on 16-bit data rather than 
32-bit data.

There is nothing programmer has to do explicitly to put an operand size 
  prefix byte in front of a 16-bit instruction:
  the assembler does this automatically as soon as 16-bit operand is found 
    in the instruction.
  However, keep in mind that whenever you use a 16-bit operand in a 32-bit 
  program, the instruction is longer by one byte:    Opcode     Instruction
    --------   ------------
    41h         ECX
    66h 41h     CXBe careful about using 16-bit instructions if size (and to a lesser extent, 
  speed) are important, because
  instructions are longer, and
    slower because of their effect on the instruction cache.
  12. x86 Instruction Prefix BytesEach prefix adjusts interpretation of the opcode:
   prefix byte guarantees that instruction will 
    have exclusive use of all shared memory, until the instruction completes 
    execution: instruction prefixes
    where
     repeats instruction the number of 
      times specified by .
       and  prefixes allow to terminate loop on the 
      value of  CPU flag.
    
    Related string manipulation instructions are:
    , compare string, etc. 
    
    See also string manipulation sample program:  prefix causes memory access to use 
     instead of  designated 
    for instruction operand.        2Eh = 
        36h = 
        3Eh = 
        26h = 
        64h = 
        65h = , . Changes size of 
    data expected by default mode of the instruction e.g. 16-bit to 32-bit and 
    vice versa. 
    , . Changes size of 
    address expected by the instruction. 32-bit address could switch to 16-bit 
    and vice versa.
  13. Alternate Encodings for InstructionsTo shorten program code, Intel created alternate (shorter) encodings of 
  some very commonly used instructions.
  For example, x86 provides a single byte opcode for    add al, constant    ; one-byte opcode and no MOD-REG-R/M byte
    add eax, constant   ; one-byte opcode and no MOD-REG-R/M byte
  the opcodes are  and , 
  respectively. Also,
  These instructions are one byte shorter than their standard
   immediate counterparts.
  Note that    add ax, constant   ; operand size prefix byte + one-byte opcode, no MOD-REG-R/M byte
  requires an operand size prefix just as a standard ,  
  instruction, yet is still one byte shorter than the corresponding standard 
  version of  immediate.
  Any decent assembler will  choose the shortest 
  possible instruction when translating program into machine code.
  Intel only provides alternate encodings only for the accumulator registers 
  , , .
  This is a good reason to use accumulator registers if you have a choice 
  
    (also a good reason to take some time and study encodings of the x86 
    instructions.) x86 opcodes are represented by one or two bytes.
  Opcode could extend into unused bits of  byte. 
  Opcode encodes information about
  size of each operand, including the size of an immediate operand. 
  14.1. MOD-REG-R/M Byte Summary
 MOD-REG-R/M Byte:
  byte follows one or two opcode bytes of 
        the instructionIt provides  information for one or two 
        operands.
  If operand is in , or operand is a 
  :
   field (bits [7:6]), combined with the 
     field (bits [2:0]), specify / 
    operand, as well as its .
     field (bits [5:3]) specifies another  operand in of the two-operand 
    instruction.
  15. ISA Design ConsiderationsInstruction set architecture design that can stand the test of time is a 
  true intellectual challenge.
  It takes several compromises between space and efficiency to assign opcodes 
  and encode instruction formats.
  Today people are using Intel x86 instruction set for purposes never 
  intended by original designers.
  Extending the CPU is a very difficult task.
  The instruction set can become .
  If x86 CPU was designed  today, it would have a 
  totally different ISA!
  Software developers usually don't have a problem adapting to a new 
  architecture when writing new software...
  
    ...but they are very resistant to moving existing software from one 
    platform to another. This is the primary reason the Intel x86 platform remains so popular to 
  this day.
15.1. ISA Design Challenges Allowing for future expansion of the chip requires some .
  From the beginning there should be a balance between the number of 
  undefined opcodes and
  the number of initial instructions, and
    the size of your opcodes (including special assignments.)
  Hard decisions:
  Reduce the number of instructions in the initial instruction set?
    Increase the size of the opcode?
    Rely on an opcode prefix byte(s), which makes later added instructions 
    longer?
  There are no easy answers to these challenges for CPU designers!
16. Intel Architecture Software Developer's Manual

Classic Intel Pentium II Architecture Software Developer's Manual contains 
three parts:

It is highly recommended that you download the above manuals and use them 
as a reference.

16.1. Intel Instruction Set Reference (Volume2) of the
  Instruction Set Reference describes
  each Intel instruction in detail
    algorithmic description of each operation
    operand(s), their sizes and attributes
    CPU exceptions that may be generated.
  The instructions are arranged in alphabetical order.
   provides  for the entire 
  Intel Architecture instruction set.
16.2. Chapter 3 of Intel Instruction Set Reference begins with instruction format example and 
  explains the  column encoding.
  The  column gives the complete machine 
  codes as it is understood by the CPU.
  When possible, the actual  are given as exact 
  hexadecimal bytes, in the same order in which they appear in memory.
  However, there are opcode definitions other than hexadecimal bytes...
16.3. Intel Reference Opcode BytesFow example,
  16.4. Intel Reference Opcode Bytes, Cont. - A digit between 0 and 7 indicates that
  The  field of  byte contains 
    the instruction opcode extension.
    The  (register or memory) operand of  byte indicates
    R/M Addressing Mode
    === ===========================
    000 register ( al / ax / eax )
    001 register ( cl / cx / ecx )
    010 register ( dl / dx / edx )
    011 register ( bl / bx / ebx )
    100 register ( ah / sp / esp )
    101 register ( ch / bp / ebp )
    110 register ( dh / si / esi )
    111 register ( bh / di / edi )The size bit in the opcode specifies 8 or 32-bit register size.
  A 16-bit register requires a prefix byte:    Opcode     Instruction
    --------   ------------
    41h         ECX
    66h 41h     CX16.5. Intel Reference Opcode Bytes, Cont. - Indicates that the instruction uses the  byte of the instruction.
   byte contains both
  an  (register or memory) operand.
  16.6. Intel Reference Opcode Bytes, Cont.  - A 1-byte (cb), 2-byte (cw), 4-byte (cd), or 
  6-byte (cp) value, following the opcode, is used to specify
  and possibly a new value for the code segment register .
  16.7. Intel Reference Opcode Bytes, Cont. - A 1-byte (ib), 2-byte (iw), or 4-byte (id) indicates 
  presence of the  in the instruction.
  Typical order of opcode bytes is
   scale-indexing byte (optional)
    The opcode determines if the operand is a signed value.
  All words and doublewords are given with the low-order byte first (little endian).
16.8. Intel Reference Opcode Bytes, Cont. - A register code, from 0 through 7, added to the 
  hexadecimal byte given at the left of the plus sign to form a single opcode 
  byte.
  Register Encodings Associated with the , 
  , and :
  16.9. Intel Reference Instruction ColumnThe  column gives the syntax of the instruction 
  statement as it would appear in a 386 Assembly program.
  For example,
  ]]></content:encoded></item><item><title>OpenAI’s promise to stay in California helped clear the path for its IPO</title><link>https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c</link><author>badprobe</author><category>hn</category><pubDate>Wed, 29 Oct 2025 17:44:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ICE and CBP agents are scanning faces on the street to verify citizenship</title><link>https://www.404media.co/ice-and-cbp-agents-are-scanning-peoples-faces-on-the-street-to-verify-citizenship/</link><author>samfriedman</author><category>hn</category><pubDate>Wed, 29 Oct 2025 17:05:24 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Videos on social media show officers from ICE and CBP using facial recognition technology on people in the field. One expert described the practice as “pure dystopian creep.”]]></content:encoded></item><item><title>AOL to be sold to Bending Spoons for $1.5B</title><link>https://www.axios.com/2025/10/29/aol-bending-spoons-deal</link><author>jmsflknr</author><category>hn</category><pubDate>Wed, 29 Oct 2025 16:28:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tailscale Peer Relays</title><link>https://tailscale.com/blog/peer-relays-beta</link><author>seemaze</author><category>hn</category><pubDate>Wed, 29 Oct 2025 16:21:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they’re managed entirely by the customer, peer relays are less throughput-constrained than Tailscale’s managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.In testing with early design partners, we’ve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale’s managed DERP fleet.While we’ve been keeping your network reliably connected for years with DERP, we’ve heard from customers that the throughput and performance aspects of a QoS-aware managed relay fleet makes deployments in certain environments difficult or untenable. Furthermore, customers have noted that it’s non-trivial to deploy and manage custom DERP fleets (which run as a separate service and binary).DERP provides an incredibly valuable service, setting up reliable connections between Tailscale clients anywhere in the world (including negotiating connections through peer relays). But often, DERP’s focus as a reliability and NAT traversal tool results in performance tradeoffs.By contrast, Tailscale Peer Relays is designed as a performant connectivity tool, and can perform at a level rivaling direct connections. Peer relays can be placed right next to the resources they serve, and peer relays also run on top of UDP, both characteristics beneficial to lower latency and resource overhead. And, they are built into the Tailscale client itself for ease of deployment.We want to move past even more hard NATs, and put Tailscale’s relaying technology in our customers’ hands, so they can use Tailscale at scale, anywhere, with ease. We believe our new Tailscale Peer Relays connectivity option—unique to Tailscale—gives customers the best performance and flexibility.Peer relays are configured with a single UDP port that must be available to both sides of a connection. Tailscale Peer Relays is built right into the Tailscale client, and can be enabled with a simple command, using the tailscale set --relay-server-port flag from the Tailscale CLI. Once enabled via the steps in our documentation, clients can connect to infrastructure in hard NAT environments over the peer relay.And don’t worry, we still prefer to fly direct. Tailscale prefers direct connections wherever possible. Clients can then fall back to available peer relays, and finally leverage Tailscale’s managed DERP fleet, or any customer-deployed custom DERPs, to ensure you have connectivity wherever you need it. All of this traffic, over any connection, is still end-to-end encrypted via WireGuard®.Tailscale Peer Relays is designed for the real world, based on the feedback we’ve received from customers and our own hard-earned networking expertise. It allows customers to make just one firewall exception for connections only coming from their tailnet. Peer relays scale across regions, are resilient to real-world network conditions, and graciously fall back to DERP (Tailscale’s or custom). Your network maintains its shape, but gains all kinds of flexibility.Customers can now maintain performance benchmarks even where direct connections aren’t possible, by enabling Tailscale Peer Relays to build a deterministic and high-throughput relay topology.We’ve had customers use peer relays to provide access into unmanaged networks, allowing their partners or customers to provide a controllable and auditable connectivity path without sacrificing performance.In strict private networks, customers can build predictable access paths. Tailscale Peer Relays can be placed in public subnets with VPC peering to private subnets, allowing security teams to efficiently segment their private network infrastructure, while enabling networking teams to roll Tailscale out in full-mesh mode across the subnet.Today, customers are using peer relays to establish relayed connections at near-direct speeds across a variety of environments and settings.Enable high-throughput traffic through cloud NATs, like AWS Managed NAT Gateways: Applications and services behind a Managed NAT Gateway can leverage peer relays to relay traffic that can’t establish direct connections.Relay through network firewalls: Workloads running in strictly firewalled environments can predictably expose a single UDP port, limiting the Tailscale surface area and fast-tracking the approval process for firewall exceptions.Offload from Custom and Managed DERP: Minimize data-in-transit through Tailscale‘s managed DERP network, and remove the need for customer-maintained DERP servers.Provide access to locked down customer networks: Data plane traffic can be relayed through predictable endpoints in customer networks, so that they only need to open minimal numbers of ports to facilitate cross network connections.Tailscale Peer Relays is available today as a public beta. We’ve yet to establish all the connectivity paths we want to, and there’s still visibility and debugging improvements to work through. However, we’ve reliably seen our early design partners move to peer relay deployments with relative ease, and we’re ready for you to give it a try on your tailnet.Tailscale Peer Relays can be enabled on all plans, including free (it’s our little way of working through the kinks of the modern internet with our customers). All customers can use two peer relays, for free, forever. As your needs scale, so will the number of available peer relays. To add even more peer relays to your tailnet, come have a chat with us.]]></content:encoded></item><item><title>Minecraft removing obfuscation in Java Edition</title><link>https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition</link><author>SteveHawk27</author><category>hn</category><pubDate>Wed, 29 Oct 2025 16:12:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Do you like to mod Java, tinker with builds, or take deep dives into Minecraft’s code? Then this article is for you!  For a long time, Java Edition has used obfuscation (hiding parts of the code) – a common practice in the gaming industry. Now we’re changing how we ship Minecraft: Java Edition to remove obfuscation completely. We hope that, with this change, we can pave a future for Minecraft: Java Edition where it’s easier to create, update, and debug mods. Minecraft: Java Edition has been obfuscated since its release. This obfuscation meant that people couldn’t see our source code. Instead, everything was scrambled – and those who wanted to mod Java Edition had to try and piece together what every class and function in the code did. But we encourage people to get creative both in Minecraft and with Minecraft – so in 2019 we tried to make this tedious process a little easier by releasing “obfuscation mappings”. These mappings were essentially a long list that allowed people to match the obfuscated terms to un-obfuscated terms. This alleviated the issue a little, as modders didn’t need to puzzle out what everything did, or what it should be called anymore. But why stop there?  Removing obfuscation in Java Edition To make things even easier – and remove these intermediary steps – we’re removing obfuscation altogether! Starting with the first snapshot following the complete Mounts of Mayhem launch, we will no longer obfuscate Minecraft: Java Edition. This means that this build (and all future builds) will have all of our original names* – now with variable names and other names – included by default to make modding even easier. ]]></content:encoded></item><item><title>Tell HN: Azure Outage</title><link>https://news.ycombinator.com/item?id=45748799</link><author>kierenj</author><category>hn</category><pubDate>Wed, 29 Oct 2025 16:08:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Their pricing calculator, and main portal.azure.com is down - at least here in the UK. Nothing on the status page yet -]]></content:encoded></item><item><title>Composer: Building a fast frontier model with RL</title><link>https://cursor.com/blog/composer</link><author>leerob</author><category>hn</category><pubDate>Wed, 29 Oct 2025 16:04:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Composer is our new agent model designed for software engineering intelligence and speed. On our benchmarks, the model achieves frontier coding results with generation speed four times faster than similar models.We achieve these results by training the model to complete real-world software engineering challenges in large codebases. During training, Composer is given access to a set of production search and editing tools and tasked with efficiently solving a diverse range of difficult problems. The final result is a large-scale model optimized for high-speed use as an agent in Cursor.Our motivation comes from our experience developing Cursor Tab, our custom completion model. We found that often developers want the smartest model that can support interactive use, keeping them in the flow of coding. In our development process, we experimented with a prototype agent model, codenamed Cheetah, to better understand the impact of faster agent models. Composer is a smarter version of this model that keeps coding delightful by being fast enough for an interactive experience.Composer is a mixture-of-experts (MoE) language model supporting long-context generation and understanding. It is specialized for software engineering through reinforcement learning (RL) in a diverse range of development environments. At each iteration of training, the model is given a problem description and instructed to produce the best response, be it a code edit, a plan, or an informative answer. The model has access to simple tools, like reading and editing files, and also more powerful ones like terminal commands and codebase-wide semantic search.To measure progress, we constructed an evaluation that measures a model's usefulness to a software developer as faithfully as possible. Our benchmark, Cursor Bench, consists of real agent requests from engineers and researchers at Cursor, along with hand-curated optimal solutions to these requests. The resulting evaluation measures not just the agent’s correctness, but also its adherence to a codebase's existing abstractions and software engineering practices.Reinforcement learning allows us to actively specialize the model for effective software engineering. Since response speed is a critical component for interactive development, we incentivize the model to make efficient choices in tool use and to maximize parallelism whenever possible. In addition, we train the model to be a helpful assistant by minimizing unnecessary responses and claims made without evidence. We also find that during RL, the model learns useful behaviors on its own like performing complex searches, fixing linter errors, and writing and executing unit tests.Efficient training of large MoE models requires significant investment into building infrastructure and systems research. We built custom training infrastructure leveraging PyTorch and Ray to power asynchronous reinforcement learning at scale. We natively train our models at low precision by combining our MXFP8 MoE kernels with expert parallelism and hybrid sharded data parallelism, allowing us to scale training to thousands of NVIDIA GPUs with minimal communication cost. Additionally, training with MXFP8 allows us to deliver faster inference speeds without requiring post-training quantization.During RL, we want our model to be able to call any tool in the Cursor Agent harness. These tools allow editing code, using semantic search, grepping strings, and running terminal commands. At our scale, teaching the model to effectively call these tools requires running hundreds of thousands of concurrent sandboxed coding environments in the cloud. To support this workload, we adapted existing infrastructure we built for Background Agents, rewriting our virtual machine scheduler to support the bursty nature and scale of training runs. This enabled seamless unification of RL environments with production environments.Cursor builds tools for software engineering, and we make heavy use of the tools we develop. A motivation of Composer development has been developing an agent we would reach for in our own work. In recent weeks, we have found that many of our colleagues were using Composer for their day-to-day software development. With this release, we hope that you also find it to be a valuable tool.¹ Benchmarked on an internal benchmark in the Cursor tool harness. We group models into classes based on score and report the best model in each class. "Fast Frontier" includes models designed for efficient inference such as Haiku 4.5 and Gemini Flash 2.5. "Best Open" includes recent open weight model releases such as Qwen Coder and GLM 4.6. "Frontier 7/2025" is the best model available in July of this year.  "Best Frontier" includes GPT-5 and Sonnet 4.5, which both outperform Composer. For the Tokens per Second calculation, tokens are standardized across models to the latest Anthropic tokenizer.]]></content:encoded></item><item><title>Tell HN: Azure outage</title><link>https://news.ycombinator.com/item?id=45748661</link><author>tartieret</author><category>hn</category><pubDate>Wed, 29 Oct 2025 16:01:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Azure is down for us, we can't even access the azure portal. Are other experiencing this? Our services are located in Canada/Central and US-East 2]]></content:encoded></item><item><title>Tell HN: Twilio support replies with hallucinated features</title><link>https://news.ycombinator.com/item?id=45748570</link><author>haute_cuisine</author><category>hn</category><pubDate>Wed, 29 Oct 2025 15:54:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I was investigating some bug with our voice system and asked support where I can find some debugging information and event logs.They told me where I should go in the interface to see that and reassured that they checked logs and this event exist.It turned out these features and information doesn't exist anywhere in the interface and impossible to retrieve in any way. The support message with hallucinated features is mostly AI written.CEOs tell us AGI is around the corner but in reality it just unreliable information and AI can't even restock the vending machine.]]></content:encoded></item><item><title>The end of the rip-off economy: consumers use LLMs against information asymmetry</title><link>https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy</link><author>scythe</author><category>hn</category><pubDate>Wed, 29 Oct 2025 15:32:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Collins Aerospace: Sending text messages to the cockpit with test:test</title><link>https://www.ccc.de/en/disclosure/collins-aerospace-mit-test-test-textnachrichten-bis-ins-cockpit-senden</link><author>hacka22</author><category>hn</category><pubDate>Wed, 29 Oct 2025 15:07:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ RTX Corporation and Department of Defense Cyber Crime Center (on September 21, 2025)Using the credentials test:test, it was possible to log in at the ARINC OpCenter Message Browser (Screenshot) as U.S. Navy Fleet Logistics Support Wing.Via this web service, text messages can be sent to the cockpit. For  obvious reasons, we did not try that. Sent messages could be viewed.Unfortunately, RTX did not respond to our vulnerability report. The account was disabled.]]></content:encoded></item><item><title>I made a 10¢ MCU Talk</title><link>https://www.atomic14.com/2025/10/29/CH32V003-talking</link><author>iamflimflam1</author><category>hn</category><pubDate>Wed, 29 Oct 2025 14:12:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[TLDR: Yes, you can fit about 7 seconds of audio into 16K of flash and still have room for code. And you can even play LPC encoded audio on a 10 cent MCU.There’s quite a lot more detail in this video (and of course you can hear the audio!).In the previous project, I had this ultra-cheap CH32V003 microcontroller playing simple tunes on a tiny SMD buzzer. It was just toggling a GPIO pin at musical note frequencies – 1-bit audio output – and it sounded surprisingly decent. That was a fun start, but now it’s time to push this little $0.10 MCU even further: can we make it actually talk?Spoiler: Yes, we can! (well, there wouldn’t be much of a blog post if we couldn’t) This 8-pin RISC-V chip is now producing sampled audio data and spoken words. We’re really stretching the limits of what you can fit in 16 KB of flash.From Beeps to Actual AudioMoving from simple beeps to real audio meant using the microcontroller’s PWM output as a rudimentary DAC. Instead of just on/off beeping, I’m driving a waveform at an 8 kHz sample rate using a high-frequency PWM on the output pin. The hardware is the same tiny board as before
 – but I’ve swapped the small SMD buzzer for a small speaker. The buzer works too, but it’s quieter and very tinny.The sample I wanted to test with is just over 6 seconds in length - it’s the iconic “Open the pod bay doors HAL…” sequence from 2001.If we keep this audio at 16-bit PCM, 8kHZ, we’d need about 96KB – way beyond our 16 KB flash! And remember, that 16 KB has to hold both the audio data and our playback code. Clearly some aggressive compression is required.I considered a few encoding options for compressing the audio. Simply using 8-bit samples at 8 kHz cuts size in half (to ~47 KB for 6s), but that’s still about 3× too large for our flash. Adaptive Differential PCM is a simple lossy compression that could quarter the size. In theory 6 seconds would be ~24 KB – much closer to fitting, This is nice codec that packs audio into about 3.2 bits per sample (roughly 1/5 the size of 16-bit PCM) Going even further with ADPCM, using only 2 bits per sample gives a 4:1 compression relative to 8-bit audio – that’s 75% storage savings.2-bit ADPCM is definitely the winner here. Our 6-second clip shrinks to under 12 KB, which comfortably fits in flash with room for code. This looked like the winner, provided the audio quality was acceptable. The decoder for 2-bit ADPCM is also very lightweight (my implementation compiled to under just over 1K of code - 1340 bytes!). It’s definitely low quality - but it actually sounds surprisingly ok.How does 2-bit ADPCM work?It’s actually a very simple algorithm. Both the encoder and decoder maintain a predicted signal value and a step size index into a predefined table. Each 2-bit code tells the decoder how to adjust the current prediction and the step size index. In essence, we’re coding the difference between the real audio and our prediction, with only four possible levels (since 2 bits gives 4 values). After each sample, the algorithm adapts: if the prediction error was large, we move to a bigger step size (to allow larger changes); if the error was small, we use a smaller step size for finer resolution. This adaptive step is what makes it ADPCM (Adaptive Differential PCM).Our codes are as follows: (0): Go down by 1 step - subtract the step size from our current prediction (1): Go up by 1 step - add the step size to our current prediction (2): Go down by 2 steps - subtract the 2 x step size from our current prediction (3): Go up by 2 steps - add the 2 x step size to our current predictionEven with this very high level of compression, the predicted waveform manages to track the original audio surprisingly well. The above graph shows a small snippet of the audio: the blue line is the original waveform and the yellow line is the ADPCM decoder’s output.They’re not identical (and we wouldn’t expect them to be), but the general shape is preserved. When you play it back through the little speaker, it’s recognizable and surprisingly good.To make my life easier, I built a quick conversion tool to encode WAV files into this 2-bit ADPCM format. The tool lets me drag-and-drop a WAV, and it gives you the files with the data that can ve dropped into the firmware code.Six seconds of audio is cool, but what about longer phrases or even arbitrary speech? Storing anything much longer with raw or ADPCM audio would quickly fill the 16K of flash.For my second experiment, I tried something different: instead of recorded waveform audio, I used an old-school speech synthesis approach. This leverages the fact that spoken language can be encoded very compactly by modeling the human voice, rather than storing the raw sound. Specifically, I integrated a library called Talkie.Talkie is a software implementation of the Texas Instruments LPC speech synthesis architecture from the late 1970s. This was implemented in a variety of chips, most commonly the TMS5220 and TMS5100 speech chips.These were used in things like the original Speak & Spell, arcade games like early Star Wars, and speech add-ons for home computers (e.g. the BBC Micro).The Talkie library (originally by Peter Knight, later added to by Adafruit) comes with a big set of examples and vocabulary. It’s also possible to extract examples from old ROMs from arcade games.Each phrase or word only takes a few hundred bytes or even less, so you can fit quite a lot of speech into a few kilobytes of flash. The trade-off is that the voice has a very computer-esque timbre – think of the Speak & Spell’s voice. It’s clearly synthetic, but still understandable.To say custom sentences not in the library, you either concatenate the available words/phonemes (which can be clunky), or you need to generate new LPC data. The original tools for this are a bit obscure – there’s BlueWizard (a classic Mac app) and PythonWizard (a command-line tool with TK GUI) which can analyze WAV files and produce LPC data.I gave both a try with some success (and a few headaches setting them up). In the end, I cheated a bit and used an AI coding assistant to help me create a streamlined online tool for this.The result is a little web app where I can upload a recording of, say, my own voice, and it outputs the LPC data. It even lets me play back the synthesized voice in-browser to check it.So there we have it – our 10¢ microcontroller now has a voice! By using 2-bit ADPCM compression, we can store short audio clips (up to around 8 seconds) even in 16 KB of flash, and play them back via PWM with decent fidelity.And with the Talkie LPC speech synthesis, we can make the device “speak” lots of words and phrases with only a tiny memory footprint.If you want to hear it for yourself, check out the video demo linked at the top of this post. In the video, you’ll see (and hear) the WarGames clip and the Star Wars quotes running on the hardware. It’s honestly amazing what these cheap little MCUs can do. We’re really pushing the boundaries of cheap hardware here.You can find all my code on GitHub in this repository.]]></content:encoded></item><item><title>Kafka is Fast – I&apos;ll use Postgres</title><link>https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks</link><author>enether</author><category>hn</category><pubDate>Wed, 29 Oct 2025 14:06:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I feel like the tech world lives in two camps.One camp chases buzzwords.This camp tends to adopt whatever’s popular without thinking hard about whether it’s appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.You see this everywhere in the Kafka world: Streaming Lakehouse™️, Kappa™️ Architecture, Streaming AI Agents.This phenomenon is sometimes known as . Modern practices actively encourage this. Consultants push “innovative architectures” stuffed with vendor tech via “insight” reports. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you’re interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack™️, not for being resourceful.The other camp chases common senseThis camp is far more pragmatic. They strip away unnecessary complexity and steer clear of overengineered solutions. They reason from first principles before making technology choices. They resist marketing hype and approach vendor claims with healthy skepticism.Historically, it has felt like Camp 1 definitively held the upper hand in sheer numbers and noise. Today, it feels like the pendulum may be beginning to swing back, at least a tiny bit. Two recent trends are on the side of Camp 2:Trend 1 - the “Small Data” movement. People are realizing two things - their data isn’t that big and their computers are becoming big too. You can rent a 128-core, 4 TB of RAM instance from AWS. AMD just released 192-core CPUs this summer. That ought to be enough for anybody.Trend 2 - the Postgres Renaissance. The space is seeing incredible growth and investment. In the last 2 years, the phrase “Just Use Postgres (for everything)” has gained a ton of popularity. The basic premise is that you shouldn’t complicate things with new tech when you don’t need to, and that Postgres alone solves most problems pretty well. Postgres competes with purpose-built solutions like:Elasticsearch (functionality supported by Postgres’ /)Redis ()AI Vector Databases (, )Snowflake (, )The claim isn’t that Postgres is functionally equivalent to any of these specialized systems. The claim is that it handles 80%+ of their use cases with 20% of the development effort. (Pareto Principle)When you combine the two trends, the appeal becomes obvious. Postgres is a battle-tested, well-known system that is simple, scalable and reliable. Pair it with today’s powerful hardware and you quickly begin to realize that, more often than not, you do not need the state-of-the-art highly optimized and complex distributed system in order to handle your organization’s scale.A 500 KB/s workload should not use Kafka. There is a scalability cargo cult in tech that always wants to choose “the best possible” tech for a problem - but this misses the forest for the trees. The “best possible” solution frequently isn’t a technical question - it’s a practical one. Adriano makes an airtight case for why you should opt for  in his PG as Queue blog (2023) that originally inspired me to write this.Enough background. In this article, we will do three simple things:I am not aiming for an exhaustive in-depth evaluation. Benchmarks are messy af. Rather, my goal is to publish some reasonable data points which can start a discussion.(while this article is for Postgres, feel free to replace it with your database of choice)If you’d like to skip straight to the results, here they are:There are dozens of blogs out there using Postgres as a , but interestingly enough I haven’t seen one use it as a pub-sub messaging system.A quick distinction between the two because I often see them get confused: are meant for point-to-point communication. They’re widely used for asynchronous background jobs: worker apps (clients) process a task in the queue like sending an e-mail or pushing a notification. The event is consumed once and it’s done with. A message is immediately deleted (popped) off the queue once it’s consumed. Queues do not have strict ordering guarantees. messaging differs from the queue in that it is meant for one-to-many communication. This inherently means there is a large read fanout - more than one reader client is interested in any given message. Good pub-sub systems decouple readers from writers by storing data on disks. This allows them to not impose a max queue depth limit - something in-memory queues need to do in order to prevent them from going OOM.There is also a general expectation that there is strict order - events should be read in the same order that they arrived in the system.Postgres’ main competitor here is Kafka, which is the standard in pub-sub today. Various (mostly-proprietary) alternatives exist.Kafka uses the Log data structure to hold messages. You’ll see my benchmark basically reconstructs a log from Postgres primitives.Postgres doesn’t seem to have any popular libraries for pub-sub use cases, so I had to write my own. The Kafka-inspired workflow I opted for is this:Writers produce batches of messages per statement (). Each transaction carries one batch insert and targets a single  tableEach writer is sticky to one table, but in aggregate they produce to multiple tables.Each message has a unique monotonically-increasing offset number. A specific row in a special  table denotes the latest offset for a given  table.Write transactions atomically update both the  data and the  row. This ensures consistent offset tracking across concurrent writers.Readers poll for new messages. They consume the  table(s) sequentially, starting from the lowest offset and progressively reading up.Readers are split into consumer groups. Each group performs separate, independent reads and makes progress on the  tables.Each group contains 1 reader per  table.Readers store their progress in a  table, with a row for each  pair.Each reader updates the latest processed offset (claiming the records), selects the records and processes them inside a single transaction.This ensures Kafka-like semantics - gapless, monotonically-increasing offsets and at-least-once/at-most-once processing. This test in particular uses at-least-once semantics, but neither choice should impact the benchmark results.The benchmark runs  writer goroutines. These represent writer clients.
Each one loops and atomically inserts  records while updating the latest offset:The benchmark also runs  reader goroutines. Each reader is assigned a particular consumer group and partition. The group as a whole reads all partitions while each reader in the group reads only one partition at a time.The reader loops, opens a transaction, optimistically claims  records (by advancing the offset mark beyond them), selects them and processes the records.
If successful, it commits the transaction and through that advances the offset for the group.It is a pull-based read (just like Kafka), rather than push-based. If the reader has no records to poll, it sleeps for a bit.First it opens a transaction:Then it claims the offsets:Followed by selecting the claimed records:Finally, the data gets processed by the business logic (no-op in our benchmark) and the transaction is closed:If you’re wondering  - my understanding of that feature is that it’s an optimization and cannot be fully relied upon, so polling is required either way. Given that, I just copied Kafka’s relatively simple design.The full code and detailed results are all published on GitHub at stanislavkozlovski/pg-queue-pubsub-benchmark.
I ran three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volumemostly default Postgres settings (synchronous commit, fsync);
autovacuum_analyze_scale_factor = 0.05 set on the partition tables too (unclear if it has an effect)each row’s payload is 1 KiB (1024 bytes)10 writers (2 writers per partition on average)5x read fanout via 5 consumer groups20 reader clients total (4 readers per group)write batch size: 100 recordsread batch size: 200 recordswrite message rate: write throughput: write latency: 38.7ms p99 / 6.2ms p95read message rate: read message throughput: read latency: 27.3ms p99 (varied 8.9ms-47ms b/w runs); 4.67ms p95end-to-end latency:  / 10.6ms p95disk was at ~1200 writes/s with iostat claiming 46 MiB/sThese are pretty good results. It’s funny to think that the majority of people run a complex distributed system like Kafka for similar workloads.Now, a replicated setup to more accurately mimic the durability and availability guarantees of Kafka.3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume
each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)one  replica and one  replicaa few custom Postgres settings like , , , max_parallel_workers_per_gather and of course - autovacuum_analyze_scale_factor = 0.05 set on the partition tables too (unclear if it has an effect)each row’s payload is 1 KiB (1024 bytes)10 writers (2 writers per partition on average)5x read fanout via 5 consumer groupsreaders only access the primary DB; readers are in the same AZ as the primary;20 reader clients total (4 readers per group)write batch size: 100 recordsread batch size: 200 recordswrite message rate: write throughput: write latency: 153.45ms p99 / 6.8ms p95read message rate: read message throughput: read latency: 57ms p99; 4.91ms p95end-to-end latency:  / 12ms p95disk was at ~1200 writes/s with iostat claiming 46 MiB/sNow these are astonishing results! Throughput was not impacted at all. Latency increased but not extremely. Our p99 e2e latency 3x’d (60ms vs 185ms), but the p95 barely moved from 10.6ms to 12ms.This shows that a simple 3-node Postgres cluster can pretty easily sustain what is a very common Kafka workload - 5 MB/s ingest and 25 MB/s egress. Not only that, but for a cheap cost too. Just $11,514 per year.Typically, you’d expect Postgres to run more expensive than Kafka at a certain scale, simply because it wasn’t designed to be efficient for this use case.
Not here though. Running Kafka yourself would cost the same. Running the same workload through a Kafka vendor will cost you at least $50,000 a year. 🤯By the way, in Kafka it’s customary to apply client-side compression on your data. If we assume your messages were 5 KB in size and your clients applied a pretty regular compression ratio of 4x - Postgres is actually handling 20 MB/s ingress and 100 MB/s egress.Ok, let’s see how far Postgres will go.c7i.24xlarge (96 vCPU, 192 GiB RAM) Postgres server instance /w 250GB io2 12,000 IOPS EBS volumemodified Postgres settings ( on, other settings scaled to match the machine);
still kept fsync & synchronous_commit on for durability.autovacuum_analyze_scale_factor = 0.05 set on the partition tables too (unclear if it has an effect)each row’s payload is 1 KiB (1024 bytes)100 writers (~3.33 writers per partition on average)5x read fanout via 5 consumer groups150 reader clients total (5 readers per group)write batch size: 200 recordsread batch size: 200 recordswrite message rate: write throughput: write latency: 138ms p99 / 47ms p95read message rate: read message throughput: end-to-end latency:  / 242ms p95 / 23.4ms p50server kept at  CPU (basically idle);bottleneck: The bottleneck was the write rate per partition. It seems like the test wasn’t able to write at a higher rate than 8 MiB/s (8k msg/s) per table with this design. I didn’t push further, but I do wonder now as I write this - how far would writes have scaled?Reads were trivial to scale. Adding more consumer groups was trivial - I tried with 10x fanout and still ran at low CPU. I didn’t include it because I didn’t feel the need to push to an unrealistic read-fanout extreme.240 MiB/s ingress and 1.16 GiB/s egress are pretty good! The 96 vCPU machine was overkill for this test - it could have done a lot more, or we could have simply opted for a smaller machine. For what it’s worth, I do think it’s worth it to deploy a separate Kafka cluster at this scale. Kafka can save you a lot of money here because it can be more efficient in how it handles cross-zone network traffic with features like Diskless Kafka.These tests seem to show that Postgres is pretty competitive with Kafka at low scale.You may have noticed none of these tests were particularly long-running. From my understanding, the value in longer-running tests is to test table vacuuming in Postgres, as that can have negative performance effects. In the pub-sub section, vacuuming doesn’t apply because the tables are append-only. My other reasoning for running shorter tests was to keep costs in check and not spend too much time.In any case, no benchmark is perfect. My goal wasn’t to indisputably prove . Rather, I want to start a discussion by showing that what’s possible is likely larger than what most people assume. I certainly didn’t assume I’d get such good numbers, especially with the pub-sub part.In Postgres, a queue can be implemented with SELECT FOR UPDATE SKIP LOCKED. This command selects an unlocked row and locks it. It also skips reading already-locked rows. That’s how mutual exclusion is achieved - a worker can’t get other workers’ jobs.Postgres has a very popular pgmq library that offers a slick queue API. To keep it simple and understand the end-to-end flow better, I decided to write my own queue. The basic version of it is very easy. My workflow is:lock row & take job (SELECT FOR UPDATE SKIP LOCKED)process job ()mark job as “done” ( a field or  the row into a separate table)Postgres competes with RabbitMQ, AWS SQS, NATS, Redis and to an extent Kafka here.We use a simple  table. When an element is processed off the queue, it’s moved into the archive table.We again run  writer client goroutines.
Each one simply loops and sequentially inserts a single random item into the table:It only inserts one message per statement, which is pretty inefficient at scale.We again run  reader client goroutines. Each reader loops and processes one message.
The processing is done inside a database transaction.Each reader again only works with one message at a time per transaction.I again ran the same three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:The results are the average of two 15-minute tests. I also ran three 2-minute tests. They all performed similarly.c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volumeall default Postgres settingseach row’s payload is 1 KiB (1024 bytes)10 writer clients, 15 reader clientswrite latency: 2.46ms p99end-to-end latency: 17.72ms p99What I found Postgres wasn’t good at was handling client count. The bottleneck in this setup was the read clients. Each client could not read more than ~192 messages a second because of its median read latency and sequential read nature.Increasing client count boosted throughput but violated my ~60% CPU target. Trying to run 50 write and 50 read clients got to 4000 msg/s without increasing the queue depth but pegged the server’s CPU to 100%. I wanted to keep the benchmarks realistic for what you may run in production, rather than maxing out what a machine can do. This would be easily alleviated with a connection pooler (standard across all prod PG deployments) or a larger machine.Another thing worth mentioning is that the workload could sustain a lot more writes than reads. If I didn’t throttle the benchmark, it would write at 12,000 msg/s and read at 2,800 msg/s. In the spirit of simplicity, I didn’t debug further and instead throttled my writes to see at what point I could get a stable 1:1 workload.3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume
each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)one  replica and one  replicaa few custom Postgres settings like , , , max_parallel_workers_per_gather and of course - each row’s payload is 1 KiB (1024 bytes)10 writer clients, 15 reader clientsreaders only access the primary DB; readers are in the same AZ as the primary;end-to-end latency: 920ms p99 ⚠️; 536ms p95; 7ms p50As expected, throughput and latency were impacted somewhat. But not that much. It’s still over 2000 messages a second, which is pretty good for an HA queue!c7i.24xlarge Postgres server instance /w 250GB io2 12,000 IOPS EBS volumemodified Postgres settings ( on, other settings scaled to match the machine);
still kept fsync & synchronous_commit on for durability.each row’s payload is 1 KiB (1024 bytes)100 writer clients, 200 reader clientsmessage rate: write latency: 9.42ms p99end-to-end latency: 930ms p99 ⚠️; 709ms p95; 12.6ms p50This run wasn’t that impressive. There is some bottleneck in the single-table queue approach at this scale which I didn’t bother figuring out. I figured that it wasn’t important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one. The 96 vCPU instance would likely scale far further were we to run a few separate queue tables in parallel.Even a modest Postgres node can durably push thousands of queue ops/sec, which already covers the scale 99% of companies ever hit with a single queue.
As I said earlier, the last 2 years have seen the Just Use Postgres slogan become mainstream. The library’s star history captures this trend perfectly:
Most of the time - . You should always default to Postgres until the constraints prove you wrong.Kafka is obviously better optimized for pub-sub workloads. Queue systems are obviously better optimized for queue workloads. The point is that picking a technology based on technical optimization alone is a flawed approach. To throw an analogy:a Formula One car is optimized to drive faster, but I still use a sedan to go to work. I am way more comfortable driving my sedan than an F1 car.The Postgres sedan comes with many quality-of-life comforts that the F1 Kafka does not:ability to debug messages with regular SQLability to delete, re-order or edit messages in placeability to join pub-sub data with regular tablesability to trivially read specific data via rich SQL queries (, , )Giving up these comforts is a justified sacrifice for your F1 car to go at 378 kmh (235 mph), but masochistic if you plan on driving at 25kmh (15 mph).Donald Knuth warned us in 1974 -  is the root of all evil. Deploying Kafka at small scale is premature optimization.
The point of this article is to show you that this “small scale” number has grown further than what people remember it to be - it can comfortably mean many megabytes per second.We are in a Postgres Renaissance for a reason: Postgres is  good enough. Modern NVMEs and cheap RAM allow it to scale absurdly high.Custom Solutions for Everything?Naive engineers tend to adopt a specialized technology at the slightest hint of a need: Redis, of course! Let’s deploy Elasticsearch! BigQuery or Snowflake - that’s what our data analysts used at their last job. We need a NoSQL database like MongoDB.Have to crunch some numbers on S3? Let’s use Spark!A good engineer thinks through the bigger picture.Does this new technology move the needle?Is shaving a few milliseconds off our query worth the extra organizational complexity introduced with the change?At small scale, these systems hurt you more than they benefit you. Distributed systems - both in terms of node count and system cardinality - should be respected, feared, avoided and employed only as a weapon of last resort against particularly gnarly problems. Everything with a distributed system becomes more challenging and time-consuming.The problem is the organizational overhead. The organizational overhead of adopting a new system, learning its nuances, configs, establishing monitoring, establishing processes around deployments and upgrades, attaining operational expertise on how to manage it, creating runbooks, testing it, debugging it, adopting its clients and API, using its UI, keeping up with its ecosystem, etc.All of these are real organizational costs that can take months to get right, even if the system in question isn’t difficult (a lot are). Managed SaaS offerings trade off some of the organizational overhead for greater financial costs - but they still don’t remove it all. And until you reach the scale where the technology is necessary, you pay these extra {financial, organizational} costs for zero significant gain.If the same can be done with tech for which you’ve already paid the organizational costs for (e.g Postgres), adopting something else prematurely is most definitely an anti-pattern. You don’t need web-scale technologies when you don’t have web-scale problems.MVI (a better alternative)What I think is a better approach is to search for the minimum viable infrastructure (MVI): build the smallest amount of system while still providing value.choose  technology your org is already  with
 == meets your users’ needs without being too slow/expensive/insecure == your org has prior experience, has runbooks/ops setups, monitoring, UI, etcsolve a real problem with ituse the minimum set of features
the fewer features you use, the more flexibility you have to move off the infra in question in the future (e.g if locked in with a vendor)Bonus points if that technology:is widely adopted so finding good engineers for it is trivial (Postgres - check)has a strong and growing network effect (Postgres - check)The MVI approach reduces the surface area of your infra. The fewer moving parts you have, the fewer failure modes you worry about and the less glue code you have to maintain.Unfortunately, it’s human nature to go against this. Just like startups suffer due to MVP bloat , infra teams suffer due to MVI bloat I won’t pretend to be able to map out the exact path-dependent outcome, but my guess is this:the zero interest rate era gave us abundant speculative money that was invested in any company that could grow fasta lot of viral internet companies were growing at speeds that led old infra to become obsolete fastthis prompted the next wave of ZIRP investment - specialized data infrastructure companies (in a gold rush, sell shovels!); some of these data infra startups spun off directly from the high-growth companies themselveseach well-funded data infra vendor is financially motivated to evangelize their product and have you adopt it even when you don’t need to (Everyone is Talking Their Book). They had deep pockets for marketing and used them.innovative infrastructure software got engineered. It was exciting - so engineers got nerd-sniped into ita web-scale craze/cargo cult developed, where everybody believed they need to be able to scale from zero to millions of RPS because they may go viral any day.a trend developed to copy whatever solutions the most successful, largest digital-native companies were using (Amazon, Google, Uber, etc.)the trend became a self-perpetuating prophecy: these technologies became a sought-after skill on resumes
system design interview questions were adapted to test for knowledge of these systemswithin an organization, engineers (knowingly or not) pushed for projects that are exciting and helped build their resumes;This trend continues to grow while there is no strong competing force that is sufficiently motivated to push the opposite view. Even engineers inside a company, who ought to be motivated to keep things simple, have strong incentives to pursue extra complexity. It benefits their career by giving them a project to use as ammo for their next promotion and improves their resume (cool tech/story on there) for their next job-hop. Plus it’s simply more fun.This is why I think we, as an industry, don’t always use the simplest solution available.In most cases, Postgres is that simplest solution that is available.I want to wrap this article up, but one rebuttal I can’t miss addressing is the “it won’t scale argument”.The argument goes something like this: “in today’s age we can go viral at a moment’s notice; these viral moments are very valuable for our business so we need to aggressively design in a way that keeps our app stable under traffic spikes”I have three arguments against this:Bohan Zhang, a member of OpenAI’s infrastructure team and co-founder of OtterTune (a Postgres tuning service), can be quoted as saying:“At OpenAI, we utilize an unsharded architecture with one writer and multiple readers, demonstrating that PostgreSQL can scale gracefully under massive read loads.”“The main message of my talk was that if you are not too write heavy, you can scale Postgres to a very high read throughput with read replicas using only a single master! That is exactly the message that needs to be spelled out as that covers  of apps.”“Postgres is probably the default choice for developers right now. You can use Postgres for a very long time. If you are building a startup with read-heavy workloads, just start with Postgres. If you hit a scalability issue, increase the instance size. You can scale it to a very large scale. If in the future the database becomes a bottleneck, congratulations. You have built a successful startup. It’s a good problem to have.”(slightly edited for clarity and grammar)Despite their rapid growth to a user base of more than 800 million, OpenAI has still NOT opted for a web-scale distributed database. If they haven’t… why does your unproven project need to?2. You Have More Time To Scale Than You ThinkLet’s say it’s a good principle to design/test for ~10x your scale. Here are the years of  growth rate it takes to get to 10x your current scale:It goes to show that even at extreme growth levels, you still have years to migrate between solutions.
The majority of developers, though, work at companies in the 0-50% growth rate. They are more likely to have moved on to another job by the time the solution needs to change (if ever).In an ideal world, you  build for scale and any other future problem you may hit in 10 years.In the real world, you have finite bandwidth, so you have to build for the most immediate, highest ROI problem.Planning your infrastructure around being able to handle that is sort of like buying a huge Marshall stack as your first guitar amp because your garage band might get invited to open for Coldplay.Just use Postgres until it breaks.I’m a complete Postgres noob. You may see a lot of dumb mistakes here. Feel free to call me out on them - I’m happy to learn. I used AI to help a lot with some of the PG tools to use. This both shows how inexperienced I am in the context and how easy it is to start. I am generally skeptical of AI’s promise (in the short-term), but there’s no denying it has made a large dent in democratizing niche/low-level knowledge.]]></content:encoded></item><item><title>From VS Code to Helix</title><link>https://ergaster.org/posts/2025/10/29-vscode-to-helix/</link><author>todsacerdoti</author><category>hn</category><pubDate>Wed, 29 Oct 2025 13:19:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I created the website you’re reading with VS Code. Behind the scenes I use Astro, a static site generator that gets out of the way while providing nice conveniences.Using VS Code was a no-brainer: everyone in the industry seems to at least be familiar with it, every project can be opened with it, and most projects can get enhancements and syntactic helpers in a few clicks. In short: VS Code is free, easy to use, and widely adopted.A Rustacean colleague kept singing Helix’s praises. I discarded it because he’s much smarter than I am, and I only ever use vim when I need to fiddle with files on a server. I like when things “Just Work” and didn’t want to bother learning how to use Helix nor how to configure it.Today it has become my daily driver. Why did I change my mind? What was preventing me from using it before? And how difficult was it to get there?Automation and technology make work easier, this is why we produce technology in the first place. But it also means you grow more dependent on the tech you use. If the tech is produced transparently by an international team or a team you trust, it’s fine. But if it’s produced by a single large entity that can screw you over, it’s dangerous.VS Code might be open source, but in practice it’s produced by Microsoft. Microsoft has a problematic relationship to consent and is shoving AI products down everyone’s throat. I’d rather use tools that respect me and my decisions, and I’d rather not get my tools produced by already monopolistic organizations.Microsoft is also based in the USA, and the political climate over there makes me want to depend as little as possible on American tools. I know that’s a long, uphill battle, but we have to start somewhere.I’m not advocating for a ban against American tech in general, but for more balance in our supply chain. I’m also not advocating for European tech either: I’d rather get open source tools from international teams competing in a race to the top, rather than from teams in a single jurisdiction. What is happening in the USA could happen in Europe too.I’ve never found vim particularly pleasant to use but it’s everywhere, so I figured I might just get used to it. But one of the things I never liked about vim is the number of moving pieces. By default, vim and neovim are very bare bones. They can be extended and completely modified with plugins, but I really don’t like the idea of having extremely customize tools.I’d rather have the same editor as everyone else, with a few knobs for minor preferences. I am subject to choice paralysis, so making me configure an editor before I’ve even started editing is the best way to tank my productivity.When my colleague told me about Helix, two things struck me as improvements over vim.Helix’s philosophy is that everything should work out of the box. There are a few configs and themes, but everything should work similarly from one Helix to another. All the language-specific logic is handled in Language Servers that implement the Language Server Protocol standard.In Helix, first you select text, and then you perform operations onto it. So you can visually tell what is going to be changed before you apply the change. It fits my mental model much better.But there are major drawbacks to Helix too:After decades of vim, I was scared to re-learn everything. In practice this wasn’t a problem at all because of the very visual way Helix works.VS Code “Just Works”, and Helix sounded like more work than the few clicks from VS Code’s extension store. This is true, but not as bad as I had anticipated.After a single week of usage, Helix was already very comfortable to navigate. After a few weeks, most of the wrinkles have been ironed out and I use it as my primary editor. So how did I overcome those fears? It can sound silly, but the very first step to get into Helix was not to overthink it. I just installed it on my mac with  and gave it a go. I was not too familiar with it, so I looked up the official documentation and noticed there was a tutorial.This tutorial alone is what convinced me to try harder. It’s an interactive and well written way to learn how to move and perform basic operations in Helix. I quickly learned how to move around, select things, surround them with braces or parenthesis. I could  what I was about to do before doing it. This has been epiphany. Helix just worked the way I wanted.Better: I could get things done faster than in VS Code after a few minutes of learning. Being a lazy person, I never bothered looking up VS Code shortcuts. Because the learning curve for Helix is slightly steeper, you  to learn those shortcuts that make moving around feel so easy.Not only did I quickly get used to Helix key bindings: my vim muscle-memory didn’t get in the way at all!The built-in tutorial is a very pragmatic way to get started. You get results fast, you learn hands on, and it’s not that long. But if you want to go further, you have to look for docs. Helix has officials docs. They seem to be fairly complete, but they’re also impenetrable as a new user. They focus on what the editor supports and not on what I will want to do with it.After a bit of browsing online, I’ve stumbled upon this third-party documentation website. The domain didn’t inspire me a lot of confidence, but the docs are really good. They are clearly laid out, use-case oriented, and they make the most of Astro Starlight to provide a great reading experience. The author tried to upstream these docs, but that won’t happen. It looks like they are upstreaming their docs to the current website. I hope this will improve the quality of upstream docs eventually.After learning the basics and finding my way through the docs, it was time to ensure Helix was set up to help me where I needed it most.In my free time, I mostly use my editor for three things:Tweak my website with AstroEdit yaml to faff around my Kubernetes clusterHelix is a “stupid” text editor. It doesn’t know much about what you’re typing. But it supports Language Servers that implement the Language Server Protocol. Language Servers understand the document you’re editing. They explain to Helix what you’re editing, whether you’re in a TypeScript function, typing a markdown link, etc. With that information, Helix and the Language Server can provide code completion hints, errors & warnings, and easier navigation in your code.In addition to Language Servers, Helix also supports plugging code formatters. Those are pieces of software that will read the document and ensure that it is consistently formatted. It will check that all indentations use spaces and not tabs, that there is a consistent number of space when indenting, that brackets are on the same line as the function, etc. In short: it will make the code pretty.Markdown is not really a programming language, so it might seem surprising to configure a Language Server for it. But if you remember what we said earlier, Language Servers can provide code completion, which is useful when creating links for example. Marksman does exactly that!We can check that Helix is happy with it with the following commandBut Language Servers can also help Helix display errors and warnings, and “code suggestions” to help fix the issues. It means Language Servers are a perfect fit for… grammar checkers! Several grammar checkers exist. The most notable are:LTEX+, the Language Server used by Language Tool. It supports several languages must is quite resource hungry.Harper, a grammar checker Language Server developed by Automattic, the people behind WordPress, Tumblr, WooCommerce, Beeper and more. Harper only support English and its variants, but they intend to support more languages in the future.I mostly write in English and want to keep a minimalistic setup. Automattic is well funded, and I’m confident they will keep working on Harper to improve it. Since grammar checker LSPs can easily be changed, I’ve decided to go with Harper for now.To install it, homebrew does the job as always:Then I edited my ~/.config/helix/languages.toml to add Harper as a secondary Language Server in addition to marksmanFinally I can add a markdown linter to ensure my markdown is formatted properly. Several options exist, and markdownlint is one of the most popular. My colleagues recommended the new kid on the block, a  equivalent: rumdl.Installing rumdl was pretty simple on my mac. I only had to add the repository of the maintainer, and install rumdl from it.After that I added a new  to my ~/.config/helix/languages.toml and added it to the language servers to use for the markdown .Since my website already contained a  I could import it to the rumdl format withYou might have noticed that I’ve added a little quality of life improvement: soft-wrap at 80 characters.Now if you add this to your own  you will notice that the text is completely left aligned. This is not a problem on small screens, but it rapidly gets annoying on wider screens.Helix doesn’t support centering the editor. There is a PR tackling the problem but it has been stale for most of the year. The maintainers are overwhelmed by the number of PRs making it their way, and it’s not clear if or when this PR will be merged.In the meantime, a workaround exists, with a few caveats. It is possible to add spaces to the left gutter (the column with the line numbers) so it pushes the content towards the center of the screen.To figure out how many spaces are needed, you need to get your terminal width with In my case, when in full screen, my terminal is 243 characters wide. I need to remove the content column with from it, and divide everything by 2 to get the space needed on each side. In my case for a 243 character wide terminal with a text width of 80 characters:As is, I would add 203 spaces to my left gutter to push the rest of the gutter and the content to the right. But the gutter itself has a width of 4 characters, that I need to remove from the total. So I need to subtract them from the total, which leaves me with  characters to add.I can open my ~/.config/helix/config.toml to add a new key binding that will automatically add or remove those spaces from the left gutter when needed, to shift the content towards the center.Now when in normal mode, pressing  then  then  will add/remove the spaces. Of course this workaround only works when the terminal runs in full screen mode.I only had to install those globally withNow we need to add a few lines to our ~/.config/helix/languages.toml to tell it how to use the language serverWe can check that the Astro Language Server can be used by helix withI also like to get a formatter to automatically make my code consistent and pretty for me when I save a file. One of the most popular code formaters out there is Prettier. I’ve decided to go with the fast and easy formatter dprint instead.Then in the projects I want to use dprint in, I doI might edit the  file to my liking. Finally, I configure Helix to use dprint globally for all Astro projects by appending a few lines in my ~/.config/helix/languages.toml.One final check, and I can see that Helix is ready to use the formatter as wellFor yaml, it’s simple and straightforward: Helix is preconfigured to use  as soon as it’s in the PATH. I just need to install it withHelix really grew on me. I find it particularly easy and fast to edit code with it. It takes a tiny bit more work to get the language support than it does in VS Code, but it’s nothing insurmountable. There is a slightly steeper learning curve than for VS Code, but I consider it to be a good thing. It forced me to learn how to move around and edit efficiently, because there is no way to do it inefficiently. Helix remains intuitive once you’ve learned the basics.I am a GNOME enthusiast, and I adhere to the same principles: I like when my apps work out of the box, and when I have little to do to configure them. This is a strong stance that often attracts a vocal opposition. I like products that follow those principles better than those who don’t.With that said, Helix sometimes feels like it is maintained by one or two people who have a strong vision, but who struggle to onboard more maintainers. As of writing, Helix has more than 350 PRs open. Quite a few bring interesting features, but the maintainers don’t have enough time to review them.Those 350 PRs mean there is a lot of energy and goodwill around the project. People are willing to contribute. Right now, all that energy is gated, resulting in frustration both from the contributors who feel like they’re working in the void, and the maintainers who feel like there at the receiving end of a fire hose.A solution to make everyone happier without sacrificing the quality of the project would be to work on a Contributor Ladder. CHAOSS’ Dr Dawn Foster published a blog post about it, listing interesting resources at the end.]]></content:encoded></item><item><title>Show HN: Learn German with Games</title><link>https://www.learngermanwithgames.com/</link><author>predictand</author><category>hn</category><pubDate>Wed, 29 Oct 2025 11:50:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Choose Your Learning AdventureSelect a game below and start mastering German in an engaging, interactive way!]]></content:encoded></item><item><title>AWS to bare metal two years later: Answering your questions about leaving AWS</title><link>https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view</link><author>ndhandala</author><category>hn</category><pubDate>Wed, 29 Oct 2025 11:14:38 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When we published How moving from AWS to Bare-Metal saved us $230,000 /yr. in 2023, the story travelled far beyond our usual readership. The discussion threads on Hacker News and Reddit were packed with sharp questions: did we skip Reserved Instances, how do we fail over a single rack, what about the people cost, and when is cloud still the better answer? This follow-up is our long-form reply.Over the last twenty-four months we:Ran the MicroK8s + Ceph stack in production for 730+ days with 99.993% measured availability.Added a second rack in Frankfurt, joined to our primary Paris cage over redundant DWDM, to kill the “single rack” concern.Cut average customer-facing latency by 19% thanks to local NVMe and eliminating noisy neighbours.Reinvested the savings into buying bare metal AI servers to expand LLM-based alert / incident summarisation and auto code fixes based on log / traces and metrics in OneUptime.Below we tackle the recurring themes from the community feedback, complete with the numbers we use internally.$230,000 / yr savings? That is just an engineers salary.In the US, it is. In the rest of the world. That's 2-5x engineers salary. We  to save $230,000 / yr but now the savings have exponentially grown. We now save over $1.2M / yr and we expect this to grow, as we grow as a business.“Why not just buy Savings Plans or Reserved Instances?”We tried. Long answer: the maths still favoured bare metal once we priced everything in. We see a savings of over 76% if you compare our bare metal setup to AWS. Savings Plans  reduce S3, egress, or Direct Connect. 37% off instances still leaves you paying list price for bandwidth, which was 22% of our AWS bill.EKS had an extra $1,260/month control-plane fee plus $600/month for NAT gateways. Those costs disappear once you run Kubernetes yourself.Our workload is 24/7 steady. We were already at >90% reservation coverage; there was no idle burst capacity to “right size” away. If we had the kind of bursty compute profile many commenters referenced, the choice would be different.“How much did migration and ongoing ops really cost?”We spent a week of engineers time (and that is the worst case estimate) on the initial migration, spread across SRE, platform, and database owners. Most of that time was work we needed anyway—formalising infrastructure-as-code, smoke testing charts, tightening backup policies. The incremental work that existed purely  of bare metal was roughly one week.Ongoing run-cost looks like this: ~24 engineer-hours/quarter across the entire platform team, including routine patching and firmware updates. That is comparable to the AWS time we used to burn on cost optimisation, IAM policy churn, and chasing deprecations and updating our VM's on AWS.  2 interventions in 24 months (mainly disks). Mean response time: 27 minutes. We do not staff an on-site team. We rely on co-location provider to physically manage our rack. This means no traditional hardware admins.  We're now moving to Talos. We PXE boot with Tinkerbell, image with Talos, manage configs through Flux and Terraform, and run conformance suites before each Kubernetes upgrade. All of those tools also hardened our AWS estate, so they were not net-new effort.The opportunity cost question from is fair. We track it the same way we track feature velocity: did the infra team ship less? The answer was “no”—our release cadence increased because we reclaimed few hours/month we used to spend in AWS “cost council” meetings.“Isn’t a single rack a single point of failure?”We have multiple racks across two different DC / providers. We:Leased a secondary quarter rack in Frankfurt with a different provider and power utility.Currently: Deployed a second MicroK8s control plane, mirrored Ceph pools with asynchronous replication. Future: We're moving to Talos. Nothing against Microk8s, but we like the Talos way of managing the k8s cluster.Added isolated out-of-band management paths (4G / satellite) so we can reach the gear even during metro fibre events.The AWS failover cluster we mentioned in 2023 still exists. We rehearse a full cutover quarterly using the same Helm releases we ship to customers. DNS failover remains the slowest leg (resolver caches can ignore TTL), so we added Anycast ingress via BGP with our transit provider to cut traffic shifting to sub-minute.“What about hardware lifecycle and surprise CapEx?”We amortise servers over five years, but we sized them with 2 × AMD EPYC 9654 CPUs, 1 TB RAM, and NVMe sleds. At our current growth rate the boxes will hit CPU saturation before we hit year five. When that happens, the plan is to cascade the older gear into our regional analytics cluster (we use Posthog + Metabase for this) and buy a new batch. Thanks to the savings delta, we can refresh 40% of the fleet every 24 months and still spend less annually than the optimised AWS bill above.We also buy extended warranties from the OEM (Supermicro) and keep three cold spares in the cage. The hardware lasts 7-8 years and not 5, but we wtill count it as 5 to be very conservative. “Are you reinventing managed services?”Another strong Reddit critique: why rebuild services AWS already offers? Three reasons we are comfortable with the trade:Portability is part of our product promise. OneUptime customers self-host in their own environments. Running the same open stack we ship (Postgres, Redis, ClickHouse, etc.) keeps us honest. We eun on Kubernetes and self-hosted customers run on Kubernetes as well.  Two years ago we relied on Terraform + EKS + RDS. Today we run MicroK8s (Talos in the future), Argo Rollouts, OpenTelemetry Collector, and Ceph dashboards. None of that is bespoke. We do not maintain a fork of anything. We still pay AWS for Glacier backups, CloudFront for edge caching, and short-lived burst capacity for load tests. Cloud makes sense when elasticity matters; bare metal wins when baseload dominates.Managed services are phenomenal when you are short on expertise or need features beyond commodity compute. If we were all-in on DynamoDB streams or Step Functions we would almost certainly still be on AWS.“How do bandwidth and DoS scenarios work now?”We committed to 5 Gbps 95th percentile across two carriers.  The same traffic on AWS egress would be 8x expensive in eu-west-1. For DDoS protection we front our ingress with Cloudflare. “Has reliability suffered?”Short answer: No. Infact it was better than AWS (compared to recent AWS downtimes)We have 730+ days with 99.993% measured availability and we also escaped AWS region wide downtime that happened a week ago. “How do audits and compliance work off-cloud now?”We stayed SOC 2 Type II and ISO 27001 certified through the transition. The biggest deltas auditors cared about:Physical controls: We provide badge logs from the colo, camera footage on request, and quarterly access reviews. The colo already meets Tier III redundancy, so their reports roll into ours.Change management: Terraform plans, and now Talos machine configs give us immutable evidence of change. Auditors liked that more than AWS Console screenshots.Business continuity: We prove failover by moving workload to other DC.If you are in a regulated space (HIPAA for instance), expect the paperwork to grow a little. We worked it in by leaning on the colo providers’ standard compliance packets—they slotted straight into our risk register.“Why not stay in the cloud but switch providers?”We priced Hetzner, OVH, Leaseweb, Equinix Metal, and AWS Outposts. The short version:Hyperscaler alternatives were cheaper on compute but still expensive on egress once you hit petabytes/month. Outposts also carried minimum commits that exceeded our needs.European dedicated hosts (Hetzner, OVH) are fantastic for lab clusters. The challenge was multi-100 TB Ceph clusters with redundant uplinks and smart-hands SLAs. Once we priced that tier, the savings narrowed.Equinix Metal got the closest, but bare metal on-demand still carried a 25-30% premium over our CapEx plan. Their global footprint is tempting; we may still use them for short-lived expansion.Owning the hardware also let us plan power density (we run 15 kW racks) and reuse components. For our steady-state footprint, colocation won by a long shot.“What does day-to-day toil look like now?”We put real numbers to it because Reddit kept us honest:Weekly: Kernel and firmware patches (Talos makes this a redeploy), Ceph health checks,  Total time averages 1 hour/week on average over months. Monthly: Kubernetes control plane upgrades in canary fashion. About 2 engineer-hours. We expect this to reduce when Talos kicks in.Quarterly: Disaster recovery drills, capacity planning, and contract audits with carriers. Roughly 12 hours across three engineers.Total toil is ~14 engineer-hours/month, including prep. The AWS era had us spending similar time but on different work: chasing cost anomalies, expanding Security Hub exceptions, and mapping breaking changes in managed services. The toil moved; it did not multiply.“Do you still use the cloud for anything substantial?”Absolutely. Cloud still solves problems we would rather not own:Glacier keeps long-term log archives at a price point local object storage cannot match.CloudFront handles 14 edge PoPs we do not want to build. We terminate TLS at the edge for marketing assets and docs. We will soon move this to Cloudflare as they are cheaper.We spin up short-lived AWS environments for load testing.So yes, we left AWS for the base workload, but we still swipe the corporate card when elasticity or geography outweighs fixed-cost savings.When the cloud is still the right answerIt depends on your workload. We still recommend staying put if:Your usage pattern is spiky or seasonal and you can auto-scale to near zero between peaks.You lean heavily on managed services (Aurora Serverless, Kinesis, Step Functions) where the operational load is the value prop.You do not have the appetite to build a platform team comfortable with Kubernetes, Ceph, observability, and incident response.Cloud-first was the right call for our first five years. Bare metal became the right call once our compute footprint, data gravity, and independence requirements stabilised.We are working on a detailed runbook + Terraform module to help teams do  for colo moves. Expect that on the blog later this year.A deep dive on Talos is in the queue, as requested by multiple folks in the HN thread.Questions we did not cover? Let us know in the discussion threads—we are happy to keep sharing the gritty details.]]></content:encoded></item><item><title>Aggressive bots ruined my weekend</title><link>https://herman.bearblog.dev/agressive-bots/</link><author>shaunpud</author><category>hn</category><pubDate>Wed, 29 Oct 2025 10:47:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>YouTube is taking down videos on performing nonstandard Windows 11 installs</title><link>https://old.reddit.com/r/DataHoarder/comments/1oiz0v0/youtube_is_taking_down_videos_on_performing/</link><author>jjbinx007</author><category>hn</category><pubDate>Wed, 29 Oct 2025 09:26:09 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Who needs Graphviz when you can build it yourself?</title><link>https://spidermonkey.dev/blog/2025/10/28/iongraph-web.html</link><author>pdubroy</author><category>hn</category><pubDate>Wed, 29 Oct 2025 05:17:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[We recently overhauled our internal tools for visualizing the compilation of JavaScript and WebAssembly. When SpiderMonkey’s optimizing compiler, Ion, is active, we can now produce interactive graphs showing exactly how functions are processed and optimized.You can play with these graphs right here on this page. Simply write some JavaScript code in the  function and see what graph is produced. You can click and drag to navigate, ctrl-scroll to zoom, and drag the slider at the bottom to scrub through the optimization process.As you experiment, take note of how stable the graph layout is, even as the sizes of blocks change or new structures are added. Try clicking a block's title to select it, then drag the slider and watch the graph change while the block remains in place. Or, click an instruction's number to highlight it so you can keep an eye on it across passes.We are not the first to visualize our compiler’s internal graphs, of course, nor the first to make them interactive. But I was not satisfied with the output of common tools like Graphviz or Mermaid, so I decided to create a layout algorithm specifically tailored to our needs. The resulting algorithm is simple, fast, produces surprisingly high-quality output, and can be implemented in less than a thousand lines of code. The purpose of this article is to walk you through this algorithm and the design concepts behind it.Read this post on desktop to see an interactive demo of iongraph.As readers of this blog already know, SpiderMonkey has several tiers of execution for JavaScript and WebAssembly code. The highest tier is known as Ion, an optimizing SSA compiler that takes the most time to compile but produces the highest-quality output.Working with Ion frequently requires us to visualize and debug the SSA graph. Since 2011 we have used a tool for this purpose called iongraph, built by Sean Stangl. It is a simple Python script that takes a JSON dump of our compiler graphs and uses Graphviz to produce a PDF. It is perfectly adequate, and very much the status quo for compiler authors, but unfortunately the Graphviz output has many problems that make our work tedious and frustrating.The first problem is that the Graphviz output rarely bears any resemblance to the source code that produced it. Graphviz will place nodes wherever it feels will minimize error, resulting in a graph that snakes left and right seemingly at random. There is no visual intuition for how deeply nested a block of code is, nor is it easy to determine which blocks are inside or outside of loops. Consider the following function, and its Graphviz graph:Counterintuitively, the  appears  the two assignments in the body of the loop. Since this graph mirrors JavaScript control flow, we’d expect to see the return at the bottom. This problem only gets worse as graphs grow larger and more complex.The second, related problem is that Graphviz’s output is unstable. Small changes to the input can result in large changes to the output. As you page through the graphs of each pass within Ion, nodes will jump left and right, true and false branches will swap, loops will run up the right side instead of the left, and so on. This makes it very hard to understand the actual effect of any given pass. Consider the following before and after, and notice how the second graph is almost—but not quite—a mirror image of the first, despite very minimal changes to the graph’s structure:None of this felt right to me. Control flow graphs should be able to follow the structure of the program that produced them. After all, a control flow graph has many restrictions that a general-purpose tool would not be aware of: they have very few cycles, all of which are well-defined because they come from loops; furthermore, both JavaScript and WebAssembly have reducible control flow, meaning all loops have only one entry, and it is not possible to jump directly into the middle of a loop. This information could be used to our advantage.Beyond that, a static PDF is far from ideal when exploring complicated graphs. Finding the inputs or uses of a given instruction is a tedious and frustrating exercise, as is following arrows from block to block. Even just zooming in and out is difficult. I eventually concluded that we ought to just build an interactive tool to overcome these limitations.How hard could layout be?I had one false start with graph layout, with an algorithm that attempted to sort blocks into vertical “tracks”. This broke down quickly on a variety of programs and I was forced to go back to the drawing board—in fact, back to the source of the very tool I was trying to replace.The algorithm used by , the typical hierarchical layout mode for Graphviz, is known as the Sugiyama layout algorithm, from a 1981 paper by Sugiyama et al. As introduction, I found a short series of lectures that broke down the Sugiyama algorithm into 5 steps:, where the direction of some edges are flipped in order to produce a DAG., where vertices are assigned into horizontal layers according to their depth in the graph, and dummy vertices are added to any edge that crosses multiple layers., where vertices on a layer are reordered in order to minimize the number of edge crossings., where vertices are horizontally positioned in order to make the edges as straight as possible., where the final graph is rendered to the screen.These steps struck me as surprisingly straightforward, and provided useful opportunities to insert our own knowledge of the problem:Cycle breaking would be trivial for us, since the only cycles in our data are loops, and loop backedges are explicitly labeled. We could simply ignore backedges when laying out the graph.Leveling would be straightforward, and could easily be modified to better mimic the source code. Specifically, any blocks coming after a loop in the source code could be artificially pushed down in the layout, solving the confusing early-exit problem.Permuting vertices to reduce edge crossings was actually just a bad idea, since our goal was stability from graph to graph. The true and false branches of a condition should always appear in the same order, for example, and a few edge crossings is a small price to pay for this stability.Since reducible control flow ensures that a program’s loops form a tree, vertex positioning could ensure that loops are always well-nested in the final graph.Taken all together, these simplifications resulted in a remarkably straightforward algorithm, with the initial implementation being just 1000 lines of JavaScript. (See this demo for what it looked like at the time.) It also proved to be very efficient, since it avoided the most computationally complex parts of the Sugiyama algorithm.iongraph from start to finishWe will now go through the entire iongraph layout algorithm. Each section contains explanatory diagrams, in which rectangles are basic blocks and circles are dummy nodes. Loop header blocks (the single entry point to each loop) are additionally colored green.Be aware that the block positions in these diagrams are not representative of the actual computed layout position at each point in the process. For example, vertical positions are not calculated until the very end, but it would be hard to communicate what the algorithm was doing if all blocks were drawn on a single line!We first sort the basic blocks into horizontal tracks called “layers”. This is very simple; we just start at layer 0 and recursively walk the graph, incrementing the layer number as we go. As we go, we track the “height” of each loop, not in pixels, but in layers.We also take this opportunity to vertically position nodes “inside” and “outside” of loops. Whenever we see an edge that exits a loop, we defer the layering of the destination block until we are done layering the loop contents, at which point we know the loop’s height.A note on implementation: nodes are visited multiple times throughout the process, not just once. This can produce a quadratic explosion for large graphs, but I find that an early-out is sufficient to avoid this problem in practice.The animation below shows the layering algorithm in action. Notice how the final block in the graph is visited twice, once after each loop that branches to it, and in each case, the block is deferred until the entire loop has been layered, rather than processed immediately after its predecessor block. The final position of the block is below the entirety of both loops, rather than directly below one of its predecessors as Graphviz would do. (Remember, horizontal and vertical positions have not yet been computed; the positions of the blocks in this diagram are hardcoded for demonstration purposes.)Step 2: Create dummy nodesAny time an edge crosses a layer, we create a dummy node. This allows edges to be routed across layers without overlapping any blocks. Unlike in traditional Sugiyama, we always put downward dummies on the left and upward dummies on the right, producing a consistent “counter-clockwise” flow. This also makes it easy to read long vertical edges, whose direction would otherwise be ambiguous. (Recall how the loop backedge flipped from the right to the left in the “unstable layout” Graphviz example from before.)In addition, we coalesce any edges that are going to the same destination by merging their dummy nodes. This heavily reduces visual noise.This is the fuzziest and most ad-hoc part of the process. Basically, we run lots of small passes that walk up and down the graph, aligning layout nodes with each other. Our edge-straightening passes include:Pushing nodes to the right of their loop header to “indent” them.Walking a layer left to right, moving children to the right to line up with their parents. If any nodes overlap as a result, they are pushed further to the right.Walking a layer right to left, moving parents to the right to line up with their children. This version is more conservative and will not move a node if it would overlap with another. This cleans up most issues from the first pass.Straightening runs of dummy nodes so we have clean vertical lines.“Sucking in” dummy runs on the left side of the graph if there is room for them to move to the right.Straighten out any edges that are “nearly straight”, according to a chosen threshold. This makes the graph appear less wobbly. We do this by repeatedly “combing” the graph upward and downward, aligning parents with children, then children with parents, and so on.It is important to note that dummy nodes participate fully in this system. If for example you have two side-by-side loops, straightening the left loop’s backedge will push the right loop to the side, avoiding overlaps and preserving the graph’s visual structure.We do not reach a fixed point with this strategy, nor do we attempt to. I find that if you continue to repeatedly apply these particular layout passes, nodes will wander to the right forever. Instead, the layout passes are hand-tuned to produce decent-looking results for most of the graphs we look at on a regular basis. That said, this could certainly be improved, especially for larger graphs which do benefit from more iterations.At the end of this step, all nodes have a fixed X-coordinate and will not be modified further.Step 4: Track horizontal edgesEdges may overlap visually as they run horizontally between layers. To resolve this, we sort edges into parallel “tracks”, giving each a vertical offset. After tracking all the edges, we record the total height of the tracks and store it on the preceding layer as its “track height”. This allows us to leave room for the edges in the final layout step.We first sort edges by their starting position, left to right. This produces a consistent arrangement of edges that has few vertical crossings in practice. Edges are then placed into tracks from the “outside in”, stacking rightward edges on top and leftward edges on the bottom, creating a new track if the edge would overlap with or cross any other edge.The diagram below is interactive. Click and drag the blocks to see how the horizontal edges get assigned to tracks.Finally, we assign each node a Y-coordinate. Starting at a Y-coordinate of zero, we iterate through the layers, repeatedly adding the layer’s height and its track height, where the layer height is the maximum height of any node in the layer. All nodes within a layer receive the same Y-coordinate; this is simple and easier to read than Graphviz’s default of vertically centering nodes within a layer.Now that every node has both an X and Y coordinate, the layout process is complete.The details of rendering are out of scope for this article, and depend on the specific application. However, I wish to highlight a stylistic decision that I feel makes our graphs more readable.When rendering edges, we use a style inspired by railroad diagrams. These have many advantages over the Bézier curves employed by Graphviz. First, straight lines feel more organized and are easier to follow when scrolling up and down. Second, they are easy to route (vertical when crossing layers, horizontal between layers). Third, they are easy to coalesce when they share a destination, and the junctions provide a clear indication of the edge’s direction. Fourth, they always cross at right angles, improving clarity and reducing the need to avoid edge crossings in the first place.Consider the following example. There are several edge crossings that may traditionally be considered undesirable—yet the edges and their directions remain clear. Of particular note is the vertical junction highlighted in red on the left: not only is it immediately clear that these edges share a destination, but the junction itself signals that the edges are flowing downward. I find this much more pleasant than the “rat’s nest” that Graphviz tends to produce.It may seem surprising that such a simple (and stupid) layout algorithm could produce such readable graphs, when more sophisticated layout algorithms struggle. However, I feel that the algorithm succeeds  of its simplicity.Most graph layout algorithms are optimization problems, where error is minimized on some chosen metrics. However, these metrics seem to correlate poorly to readability in practice. For example, it seems good in theory to rearrange nodes to minimize edge crossings. But a predictable order of nodes seems to produce more sensible results overall, and simple rules for edge routing are sufficient to keep things tidy. (As a bonus, this also gives us layout stability from pass to pass.) Similarly, layout rules like “align parents with their children” produce more readable results than “minimize the lengths of edges”.Furthermore, by rejecting the optimization problem, a human author gains more control over the layout. We are able to position nodes “inside” of loops, and push post-loop content down in the graph,  we reject this global constraint-solver approach. Minimizing “error” is meaningless compared to a human  meaning through thoughtful design.And finally, the resulting algorithm is simply more efficient. All the layout passes in iongraph are easy to program and scale gracefully to large graphs because they run in roughly linear time. It is better, in my view, to run a fixed number of layout iterations according to your graph complexity and time budget, rather than to run a complex constraint solver until it is “done”.By following this philosophy, even the worst graphs become tractable. Below is a screenshot of a zlib function, compiled to WebAssembly, and rendered using the old tool.It took about  for Graphviz to produce this spaghetti nightmare. By comparison, iongraph can now lay out this function in . The result is still not particularly beautiful, but it renders thousands of times faster  is much easier to navigate.Perhaps programmers ought to put less trust into magic optimizing systems, especially when a human-friendly result is the goal. Simple (and stupid) algorithms can be very effective when applied with discretion and taste.We have already integrated iongraph into the Firefox profiler, making it easy for us to view the graphs of the most expensive or impactful functions we find in our performance work. Unfortunately, this is only available in specific builds of the SpiderMonkey shell, and is not available in full browser builds. This is due to architectural differences in how profiling data is captured and the flags with which the browser and shell are built. I would love for Firefox users to someday be able to view these graphs themselves, but at the moment we have no plans to expose this to the browser. However, one bug tracking some related work can be found here.We will continue to sporadically update iongraph with more features to aid us in our work. We have several ideas for new features, including richer navigation, search, and visualization of register allocation info. However, we have no explicit roadmap for when these features may be released.To experiment with iongraph locally, you can run a debug build of the SpiderMonkey shell with ; this will dump information to . This file can then be loaded into the standalone deployment of iongraph. Please be aware that the user experience is rough and unpolished in its current state.The source code for iongraph can be found on GitHub. If this subject interests you, we would welcome contributions to iongraph and its integration into the browser. The best place to reach us is our Matrix chat.Thanks to Matthew Gaudet, Asaf Gartner, and Colin Davidson for their feedback on this article.]]></content:encoded></item><item><title>Keep Android Open</title><link>http://keepandroidopen.org/</link><author>LorenDB</author><category>hn</category><pubDate>Wed, 29 Oct 2025 04:03:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In August 2025, Google announced that starting next year,
it will no longer be possible to develop apps for the Android platform
without first registering centrally with Google.
This registration will involve:➤  Paying a fee to Google➤  Agreeing to Google's Terms and Conditions➤  Providing government identification➤  Uploading evidence of the developer's private signing key➤  Listing all current and future application identifiersWhat this means for your rights➤ You, , purchased your Android device believing in Google’s promise that it was an open computing platform and that you could run whatever software you choose on it. Instead, starting next year, they will be non-consensually pushing an update to your operating system that irrevocably blocks this right and leaves you at the mercy of their judgement over what software you are permitted to trust.➤ You, , can no longer develop an app and share it directly with your friends, family, and community without first seeking Google’s approval. The promise of Android — and a marketing advantage it has used to distinguish itself against the iPhone — has always been that it is “open”. But Google clearly feels that they have enough of a lock on the Android ecosystem, along with sufficient regulatory capture, that they can now jettison this principle with prejudice and impunity.➤ You, , are ceding the rights of your citizens and your own digital sovereignty to a company with a track record of complying with the extrajudicial demands of authoritarian regimes to remove perfectly legal apps that they happen to dislike. The software that is critical to the running of your businesses and governments will be at the mercy of the opaque whims of a distant and unaccountable corporation.If you are an app developer,  for the early access program, perform identity verification, or accept an invitation to the Android Developer Console. Respond (politely) to any invitation with a list of your concerns and objections.Discourage fellow app developers and organizations from signing up to the program.—— It is only through developer acquiescence that this takeover plan can possibly succeed. ——Regulators worldwide are genuinely concerned about monopolies and the centralization of power in the tech sector, and want to hear directly from individuals who are affected and concerned. When contacting regulators directly, you should be  and  about the harm you believe these policies will cause, both to consumers and to competition.Complaints are especially impactful when they are authored by a citizen of that country or region, and when the language of the email is written in one of the official languages of the region’s governing body. Request a  of the complaint, and consider forwarding any responses you receive to victory@keepandroidopen.org so that we might highlight and reference them.Install F-Droid on your Android device(s). The more people that use alternative app marketplaces, the harder it will be to shut them out.Make your voice heard on social media and with blog posts, and link to https://keepandroidopen.orgCombat astroturfing: when you encounter suspect posts on community forums and social media in support of the policy (“Well, actually…”), challenge them and do not be shy.]]></content:encoded></item><item><title>Board: New game console recognizes physical pieces, with an open SDK</title><link>https://board.fun/</link><author>nicoles</author><category>hn</category><pubDate>Wed, 29 Oct 2025 03:58:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>uBlock Origin Lite in Apple App Store</title><link>https://apps.apple.com/in/app/ublock-origin-lite/id6745342698</link><author>mumber_typhoon</author><category>hn</category><pubDate>Wed, 29 Oct 2025 03:57:06 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[It’s was a really long wait, but finally we are able to use it directly on the iPad. The first TestFlight version had a big battery drain, but it’s better now on the official release. The only limitation is that we can’t add our own lists, but I am fine with the default lists ans it works perfect.]]></content:encoded></item><item><title>Tips for stroke-surviving software engineers</title><link>https://blog.j11y.io/2025-10-29_stroke_tips_for_engineers/</link><author>padolsey</author><category>hn</category><pubDate>Wed, 29 Oct 2025 03:51:56 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The first tip is to just stop. Fatigue, fuzziness, nausea, or affected-sided weird sensations are non-negotiable stop signals. So go lie down, hydrate, reset. Close your eyes and think about the cottage or lonely mountain you want to retire to. Escape the overwhelming mental or physical space. HEADPHONES, blinders, and 'No'. Eliminate unwanted inputs at the earliest point of entry. Work from home or environments where you can control most variables. Routes of escape and rest are important.Health above performance every single time. Metrics and productivity be damned. Self-advocate, and all that. Reject with directness any demands made of you that cross the threshold. Laws. Use them. You don't have to rely on good behaviour and kindness. You are, depending on your location, usually protected by all types of anti-discrimination legislation, implicit and explicit. Use your employee assistance programs too.Single-thread it all! Less context switching. Batch your work, finish one thing, then move to the next. Externalize working-memory. Use notebooks, whiteboards, and lists instead of juggling state in your head. I am not good at this, and over-stretch my brain, leading to auras, overwhelm, and general sickness. Terrible idea.Related: Sssh to the AI naysayers. Use it as your help and scratchpad. Let it hold state so your brain can judge rather than store and needlessly cogitate on stuff. You don't have to do this alone out of some purity fetishism. You, too, have a limited context window. Sorry!Do the heavy thinking in your peak window (for me, that's the morning); push everything else to later. Spend your time more carefully than your money.Pick the route of least attention. Attention is expensive, and rarely needed as much as we think it is. It's a heavy toll to pay. Unless you're in an ops or monitoring role, you don't need to be synchronously active. DISABLE NOTIFICATIONS. AVOID long meetings. Emails are good. Oh god am I bad at this? YES, I like people so I like some meetings, but communicating is so so expensive. Being polite is also expensive; It's not nice to have to tell people they're draining you.]]></content:encoded></item><item><title>Project Shadowglass</title><link>https://shadowglassgame.com/</link><author>layer8</author><category>hn</category><pubDate>Wed, 29 Oct 2025 01:06:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Keeping the Internet fast and secure: introducing Merkle Tree Certificates</title><link>https://blog.cloudflare.com/bootstrap-mtc/</link><author>tatersolid</author><category>hn</category><pubDate>Tue, 28 Oct 2025 22:39:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The world is in a race to build its first quantum computer capable of solving practical problems not feasible on even the largest conventional supercomputers. While the quantum computing paradigm promises many benefits, it also threatens the security of the Internet by breaking much of the cryptography we have come to rely on.To mitigate this threat, Cloudflare is helping to migrate the Internet to Post-Quantum (PQ) cryptography. Today,  of traffic to Cloudflare's edge network is protected against the most urgent threat: an attacker who can intercept and store encrypted traffic today and then decrypt it in the future with the help of a quantum computer. This is referred to as the harvest now, decrypt laterthreat.However, this is just one of the threats we need to address. A quantum computer can also be used to crack a server's  certificate, allowing an attacker to impersonate the server to unsuspecting clients. The good news is that we already have PQ algorithms we can use for quantum-safe authentication. The bad news is that adoption of these algorithms in TLS will require significant changes to one of the most complex and security-critical systems on the Internet: the Web Public-Key Infrastructure (WebPKI).The central problem is the sheer size of these new algorithms: signatures for ML-DSA-44, one of the most performant PQ algorithms standardized by NIST, are 2,420 bytes long, compared to just 64 bytes for ECDSA-P256, the most popular non-PQ signature in use today; and its public keys are 1,312 bytes long, compared to just 64 bytes for ECDSA. That's a roughly 20-fold increase in size. Worse yet, the average TLS handshake includes a number of public keys and signatures, adding up to 10s of kilobytes of overhead per handshake. This is enough to have a  on the performance of TLS.That makes drop-in PQ certificates a tough sell to enable today: they don’t bring any security benefit before Q-day — the day a cryptographically relevant quantum computer arrives — but they do degrade performance. We could sit and wait until Q-day is a year away, but that’s playing with fire. Migrations always take longer than expected, and by waiting we risk the security and privacy of the Internet, which is .It's clear that we must find a way to make post-quantum certificates cheap enough to deploy today by default for everyone — not just those that can afford it. In this post, we'll introduce you to the plan we’ve brought together with industry partners to the  to redesign the WebPKI in order to allow a smooth transition to PQ authentication with no performance impact (and perhaps a performance improvement!). We'll provide an overview of one concrete proposal, called Merkle Tree Certificates (MTCs), whose goal is to whittle down the number of public keys and signatures in the TLS handshake to the bare minimum required.But talk is cheap. We  that, as with any change to the Internet, it's crucial to test early and often. Today we're announcing our intent to deploy MTCs on an experimental basis in collaboration with Chrome Security. In this post, we'll describe the scope of this experiment, what we hope to learn from it, and how we'll make sure it's done safely.The WebPKI today — an old system with many patchesWhy does the TLS handshake have so many public keys and signatures?Let's start with Cryptography 101. When your browser connects to a website, it asks the server to  itself to make sure it's talking to the real server and not an impersonator. This is usually achieved with a cryptographic primitive known as a digital signature scheme (e.g., ECDSA or ML-DSA). In TLS, the server signs the messages exchanged between the client and server using its , and the client verifies the signature using the server's . In this way, the server confirms to the client that they've had the same conversation, since only the server could have produced a valid signature.If the client already knows the server's public key, then only  is required to authenticate the server. In practice, however, this is not really an option. The web today is made up of around a billion TLS servers, so it would be unrealistic to provision every client with the public key of every server. What's more, the set of public keys will change over time as new servers come online and existing ones rotate their keys, so we would need some way of pushing these changes to clients.This scaling problem is at the heart of the design of all PKIs.Instead of expecting the client to know the server's public key in advance, the server might just send its public key during the TLS handshake. But how does the client know that the public key actually belongs to the server? This is the job of a .A certificate binds a public key to the identity of the server — usually its DNS name, e.g., . The certificate is signed by a Certification Authority (CA) whose public key is known to the client. In addition to verifying the server's handshake signature, the client verifies the signature of this certificate. This establishes a chain of trust: by accepting the certificate, the client is trusting that the CA verified that the public key actually belongs to the server with that identity.Clients are typically configured to trust many CAs and must be provisioned with a public key for each. Things are much easier however, since there are only 100s of CAs instead of billions. In addition, new certificates can be created without having to update clients.These efficiencies come at a relatively low cost: for those counting at home, that's  signature and  public key, for a total of 2 signatures and 1 public key per TLS handshake.That's not the end of the story, however. As the WebPKI has evolved, so have these chains of trust grown a bit longer. These days it's common for a chain to consist of two or more certificates rather than just one. This is because CAs sometimes need to rotatetheir keys, just as servers do. But before they can start using the new key, they must distribute the corresponding public key to clients. This takes time, since it requires billions of clients to update their trust stores. To bridge the gap, the CA will sometimes use the old key to issue a certificate for the new one and append this certificate to the end of the chain.That's signature and public key, which brings us to 3 signatures and 2 public keys. And we still have a little ways to go.The main job of a CA is to verify that a server has control over the domain for which it’s requesting a certificate. This process has evolved over the years from a high-touch, CA-specific process to a standardized,  used for issuing most certificates on the web. (Not all CAs fully support automation, however.) This evolution is marked by a number of security incidents in which a certificate was to a party other than the server, allowing that party to impersonate the server to any client that trusts the CA.Automation helps, but  are still possible, and mistakes are almost inevitable. , several certificates for Cloudflare's encrypted 1.1.1.1 resolver were issued without our involvement or authorization. This apparently occurred by accident, but it nonetheless put users of 1.1.1.1 at risk. (The mis-issued certificates have since been revoked.)Ensuring mis-issuance is detectable is the job of the Certificate Transparency (CT) ecosystem. The basic idea is that each certificate issued by a CA gets added to a public . Servers can audit these logs for certificates issued in their name. If ever a certificate is issued that they didn't request itself, the server operator can prove the issuance happened, and the PKI ecosystem can take action to prevent the certificate from being trusted by clients.Major browsers, including Firefox and Chrome and its derivatives, require certificates to be logged before they can be trusted. For example, Chrome, Safari, and Firefox will only accept the server's certificate if it appears in at least two logs the browser is configured to trust. This policy is easy to state, but tricky to implement in practice:Operating a CT log has historically been fairly expensive. Logs ingest billions of certificates over their lifetimes: when an incident happens, or even just under high load, it can take some time for a log to make a new entry available for auditors.Clients can't really audit logs themselves, since this would expose their browsing history (i.e., the servers they wanted to connect to) to the log operators.The solution to both problems is to include a signature from the CT log along with the certificate. The signature is produced immediately in response to a request to log a certificate, and attests to the log's intent to include the certificate in the log within 24 hours.Per browser policy, certificate transparency adds  signatures to the TLS handshake, one for each log. This brings us to a total of 5 signatures and 2 public keys in a typical handshake on the public web.The WebPKI is a living, breathing, and highly distributed system. We've had to patch it a number of times over the years to keep it going, but on balance it has served our needs quite well — until now.Previously, whenever we needed to update something in the WebPKI, we would tack on another signature. This strategy has worked because conventional cryptography is so cheap. But 5 signatures and 2 public keys on average for each TLS handshake is simply too much to cope with for the larger PQ signatures that are coming.The good news is that by moving what we already have around in clever ways, we can drastically reduce the number of signatures we need.Crash course on Merkle Tree CertificatesMerkle Tree Certificates (MTCs) is a proposal for the next generation of the WebPKI that we are implementing and plan to deploy on an experimental basis. Its key features are as follows:All the information a client needs to validate a Merkle Tree Certificate can be disseminated out-of-band. If the client is sufficiently up-to-date, then the TLS handshake needs just 1 signature, 1 public key, and 1 Merkle tree inclusion proof. This is quite small, even if we use post-quantum algorithms.The MTC specification makes certificate transparency a first class feature of the PKI by having each CA run its own log of exactly the certificates they issue.Let's poke our head under the hood a little. Below we have an MTC generated by one of our internal tests. This would be transmitted from the server to the client in the TLS handshake:-----BEGIN CERTIFICATE-----
MIICSzCCAUGgAwIBAgICAhMwDAYKKwYBBAGC2ksvADAcMRowGAYKKwYBBAGC2ksv
AQwKNDQzNjMuNDguMzAeFw0yNTEwMjExNTMzMjZaFw0yNTEwMjgxNTMzMjZaMCEx
HzAdBgNVBAMTFmNsb3VkZmxhcmVyZXNlYXJjaC5jb20wWTATBgcqhkjOPQIBBggq
hkjOPQMBBwNCAARw7eGWh7Qi7/vcqc2cXO8enqsbbdcRdHt2yDyhX5Q3RZnYgONc
JE8oRrW/hGDY/OuCWsROM5DHszZRDJJtv4gno2wwajAOBgNVHQ8BAf8EBAMCB4Aw
EwYDVR0lBAwwCgYIKwYBBQUHAwEwQwYDVR0RBDwwOoIWY2xvdWRmbGFyZXJlc2Vh
cmNoLmNvbYIgc3RhdGljLWN0LmNsb3VkZmxhcmVyZXNlYXJjaC5jb20wDAYKKwYB
BAGC2ksvAAOB9QAAAAAAAAACAAAAAAAAAAJYAOBEvgOlvWq38p45d0wWTPgG5eFV
wJMhxnmDPN1b5leJwHWzTOx1igtToMocBwwakt3HfKIjXYMO5CNDOK9DIKhmRDSV
h+or8A8WUrvqZ2ceiTZPkNQFVYlG8be2aITTVzGuK8N5MYaFnSTtzyWkXP2P9nYU
Vd1nLt/WjCUNUkjI4/75fOalMFKltcc6iaXB9ktble9wuJH8YQ9tFt456aBZSSs0
cXwqFtrHr973AZQQxGLR9QCHveii9N87NXknDvzMQ+dgWt/fBujTfuuzv3slQw80
mibA021dDCi8h1hYFQAA
-----END CERTIFICATE-----Looks like your average PEM encoded certificate. Let's decode it and look at the parameters:$ openssl x509 -in merkle-tree-cert.pem -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 531 (0x213)
        Signature Algorithm: 1.3.6.1.4.1.44363.47.0
        Issuer: 1.3.6.1.4.1.44363.47.1=44363.48.3
        Validity
            Not Before: Oct 21 15:33:26 2025 GMT
            Not After : Oct 28 15:33:26 2025 GMT
        Subject: CN=cloudflareresearch.com
        Subject Public Key Info:
            Public Key Algorithm: id-ecPublicKey
                Public-Key: (256 bit)
                pub:
                    04:70:ed:e1:96:87:b4:22:ef:fb:dc:a9:cd:9c:5c:
                    ef:1e:9e:ab:1b:6d:d7:11:74:7b:76:c8:3c:a1:5f:
                    94:37:45:99:d8:80:e3:5c:24:4f:28:46:b5:bf:84:
                    60:d8:fc:eb:82:5a:c4:4e:33:90:c7:b3:36:51:0c:
                    92:6d:bf:88:27
                ASN1 OID: prime256v1
                NIST CURVE: P-256
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature
            X509v3 Extended Key Usage:
                TLS Web Server Authentication
            X509v3 Subject Alternative Name:
                DNS:cloudflareresearch.com, DNS:static-ct.cloudflareresearch.com
    Signature Algorithm: 1.3.6.1.4.1.44363.47.0
    Signature Value:
        00:00:00:00:00:00:02:00:00:00:00:00:00:00:02:58:00:e0:
        44:be:03:a5:bd:6a:b7:f2:9e:39:77:4c:16:4c:f8:06:e5:e1:
        55:c0:93:21:c6:79:83:3c:dd:5b:e6:57:89:c0:75:b3:4c:ec:
        75:8a:0b:53:a0:ca:1c:07:0c:1a:92:dd:c7:7c:a2:23:5d:83:
        0e:e4:23:43:38:af:43:20:a8:66:44:34:95:87:ea:2b:f0:0f:
        16:52:bb:ea:67:67:1e:89:36:4f:90:d4:05:55:89:46:f1:b7:
        b6:68:84:d3:57:31:ae:2b:c3:79:31:86:85:9d:24:ed:cf:25:
        a4:5c:fd:8f:f6:76:14:55:dd:67:2e:df:d6:8c:25:0d:52:48:
        c8:e3:fe:f9:7c:e6:a5:30:52:a5:b5:c7:3a:89:a5:c1:f6:4b:
        5b:95:ef:70:b8:91:fc:61:0f:6d:16:de:39:e9:a0:59:49:2b:
        34:71:7c:2a:16:da:c7:af:de:f7:01:94:10:c4:62:d1:f5:00:
        87:bd:e8:a2:f4:df:3b:35:79:27:0e:fc:cc:43:e7:60:5a:df:
        df:06:e8:d3:7e:eb:b3:bf:7b:25:43:0f:34:9a:26:c0:d3:6d:
        5d:0c:28:bc:87:58:58:15:00:00While some of the parameters probably look familiar, others will look unusual. On the familiar side, the subject and public key are exactly what we might expect: the DNS name is  and the public key is for a familiar signature algorithm, ECDSA-P256. This algorithm is not PQ, of course — in the future we would put ML-DSA-44 there instead.On the unusual side, OpenSSL appears to not recognize the signature algorithm of the issuer and just prints the raw OID and bytes of the signature. There's a good reason for this: the MTC does not have a signature in it at all! So what exactly are we looking at?The trick to leave out signatures is that a Merkle Tree Certification Authority (MTCA) produces its  certificates  rather than individually. In place of a signature, the certificate has an  of the certificate in a batch of certificates signed by the MTCA.To understand how inclusion proofs work, let's think about a slightly simplified version of the MTC specification. To issue a batch, the MTCA arranges the unsigned certificates into a data structure called a  that looks like this:Each leaf of the tree corresponds to a certificate, and each inner node is equal to the hash of its children. To sign the batch, the MTCA uses its secret key to sign the head of the tree. The structure of the tree guarantees that each certificate in the batch was signed by the MTCA: if we tried to tweak the bits of any one of the certificates, the treehead would end up having a different value, which would cause the signature to fail.An inclusion proof for a certificate consists of the hash of each sibling node along the path from the certificate to the treehead:Given a validated treehead, this sequence of hashes is sufficient to prove inclusion of the certificate in the tree. This means that, in order to validate an MTC, the client also needs to obtain the signed treehead from the MTCA.This is the key to MTC's efficiency:Signed treeheads can be disseminated to clients out-of-band and validated offline. Each validated treehead can then be used to validate any certificate in the corresponding batch, eliminating the need to obtain a signature for each server certificate.During the TLS handshake, the client tells the server which treeheads it has. If the server has a signatureless certificate covered by one of those treeheads, then it can use that certificate to authenticate itself. That's 1 signature,1 public key and 1 inclusion proof per handshake, both for the server being authenticated.Now, that's the simplified version. MTC proper has some more bells and whistles. To start, it doesn’t create a separate Merkle tree for each batch, but it grows a single large tree, which is used for better transparency. As this tree grows, periodically (sub)tree heads are selected to be shipped to browsers, which we call . In the common case browsers will be able to fetch the most recent landmarks, and servers can wait for batch issuance, but we need a fallback: MTC also supports certificates that can be issued immediately and don’t require landmarks to be validated, but these are not as small. A server would provision both types of Merkle tree certificates, so that the common case is fast, and the exceptional case is slow, but at least it’ll work.Ever since early designs for MTCs emerged, we’ve been eager to experiment with the idea. In line with the IETF principle of “”, it often takes implementing a protocol to work out kinks in the design. At the same time, we cannot risk the security of users. In this section, we describe our approach to experimenting with aspects of the Merkle Tree Certificates design  changing any trust relationships.Let’s start with what we hope to learn. We have lots of questions whose answers can help to either validate the approach, or uncover pitfalls that require reshaping the protocol — in fact, an implementation of an early MTC draft by  and  did exactly this. We’d like to know: Protocol ossification (the tendency of implementation bugs to make it harder to change a protocol) is an ever-present issue with deploying protocol changes. For TLS in particular, despite having built-in flexibility, time after time we’ve found that if that flexibility is not regularly used, there will be buggy implementations and middleboxes that break when they see things they don’t recognize. TLS 1.3 deployment  than we hoped for this very reason. And more recently, the rollout of PQ key exchange in TLS caused the Client Hello to be split over multiple TCP packets, something that many middleboxes .What is the performance impact? In fact, we expect MTCs to the size of the handshake, even compared to today's non-PQ certificates. They will also reduce CPU cost: ML-DSA signature verification is about as fast as ECDSA, and there will be far fewer signatures to verify. We therefore expect to see a . We would like to see if there is a measurable performance improvement.What fraction of clients will stay up to date? Getting the performance benefit of MTCs requires the clients and servers to be roughly in sync with one another. We expect MTCs to have fairly short lifetimes, a week or so. This means that if the client's latest landmark is older than a week, the server would have to fallback to a larger certificate. Knowing how often this fallback happens will help us tune the parameters of the protocol to make fallbacks less likely.In order to answer these questions, we are implementing MTC support in our TLS stack and in our certificate issuance infrastructure. For their part, Chrome is implementing MTC support in their own TLS stack and will stand up infrastructure to disseminate landmarks to their users.As we've done in past experiments, we plan to enable MTCs for a subset of our free customers with enough traffic that we will be able to get useful measurements. Chrome will control the experimental rollout: they can ramp up slowly, measuring as they go and rolling back if and when bugs are found.Which leaves us with one last question: who will run the Merkle Tree CA?Bootstrapping trust from the existing WebPKIStanding up a proper CA is no small task: it takes years to be trusted by major browsers. That’s why Cloudflare isn’t going to become a “real” CA for this experiment, and Chrome isn’t going to trust us directly.Instead, to make progress on a reasonable timeframe, without sacrificing due diligence, we plan to "mock" the role of the MTCA. We will run an MTCA (on  based on our ), but for each MTC we issue, we also publish an existing certificate from a trusted CA that agrees with it. We call this the . When Chrome’s infrastructure pulls updates from our MTCA log, they will also pull these bootstrap certificates, and check whether they agree. Only if they do, they’ll proceed to push the corresponding landmarks to Chrome clients. In other words, Cloudflare is effectively just “re-encoding” an existing certificate (with domain validation performed by a trusted CA) as an MTC, and Chrome is using certificate transparency to keep us honest.With almost 50% of our traffic already protected by post-quantum encryption, we’re halfway to a fully post-quantum secure Internet. The second part of our journey, post-quantum certificates, is the hardest yet though. A simple drop-in upgrade has a noticeable performance impact and no security benefit before Q-day. This means it’s a hard sell to enable today by default. But here we are playing with fire: migrations always take longer than expected. If we want to keep an ubiquitously private and secure Internet, we need a post-quantum solution that’s performant enough to be enabled by default .Merkle Tree Certificates (MTCs) solves this problem by reducing the number of signatures and public keys to the bare minimum while maintaining the WebPKI's essential properties. We plan to roll out MTCs to a fraction of free accounts by early next year. This does not affect any visitors that are not part of the Chrome experiment. For those that are, thanks to the bootstrap certificates, there is no impact on security.We’re excited to keep the Internet fast  secure, and will report back soon on the results of this experiment: watch this space! MTC is evolving as we speak, if you want to get involved, please join the IETF .]]></content:encoded></item><item><title>Tor Browser 15.0</title><link>https://blog.torproject.org/new-release-tor-browser-150/</link><author>pentagrama</author><category>hn</category><pubDate>Tue, 28 Oct 2025 21:33:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The ongoing development of Tor Browser is made possible thanks to the support of our community. If Tor Browser is important to you, now is a great time to support our mission to , as all donations will be matched by Power Up Privacy through December 31, 2025.Tor Browser 15.0 inherits a multitude of useful new features and usability improvements from Firefox that have passed our audit. For desktop, these include vertical tabs: providing a more manageable, alternative layout with open and pinned tabs stacked in a sidebar rather than across the top of the window. For ease of access, Bookmarks can be retrieved directly from the sidebar when expanded too. However, regardless of whether you prefer horizontal or vertical tabs, everyone  benefits from the addition of tab groups: helping you keep on top of the clutter by organizing tabs into collapsible groups that can be given names and color-coded. Tor Browser 15.0 also inherits elements of Firefox's recent address bar refresh, including a new unified search button that allows you to switch search engines on the fly, search bookmarks or tabs, and reference quick actions from the same menu.Note that Tor Browser tabs are still private tabs, and will clear when you close the browser. This enforces a kind of natural tidiness in Tor Browser since each new session starts fresh – however for privacy-conscious power users, project managers, researchers, or anyone else who accumulates tabs frighteningly quickly, we hope these organizational improvements will give you a much needed productivity boost.On Android, screen lock adds an extra layer of security to your browsing sessions. After enabling screen lock in Settings > Tabs, your tabs will lock automatically when you switch away from the browser without closing it. Upon returning to the app, you'll be prompted to unlock your tabs using your fingerprint, face, or pass code, depending on which option your device is configured to use.Like Tor Browser for Desktop, your browsing session will still be cleared when Tor Browser is closed. However, this feature provides peace of mind in a specific scenario: by ensuring that your browsing remains private even if someone has gained temporary access to your unlocked phone with Tor Browser open in the background – whether you've handed it to a friend, or left your device sitting on a table.Updates to Android and Linux device compatibilityAt present, Firefox 140 and Tor Browser 15.0 support Android 5.0 or later, which was released almost 11 years ago. While Mozilla's commitment to support such an old version of Android is admirable, it introduces several technical and security challenges for developers. As a consequence, Firefox have announced their intention to increase the minimum support requirements to Android 8.0, and have also decided to drop support for x86 CPUs for Android and Linux. Sadly, it's not possible for the Tor Project to maintain support for these platforms on our own without official support from Mozilla.While these changes won't impact Tor Browser users immediately, we expect them to take effect with the release of Tor Browser 16.0 mid-next year. This means that Tor Browser 15.0 will be the last major release to support x86 for Linux and Android, in addition to Android 5.0, 6.0, and 7.0. However, we will continue to release minor updates with security fixes for these platforms until Tor Browser 16.0's eventual release.Although nobody wants to see support for their platform get dropped, it's an important step to maintain the stability and security of both Firefox and Tor Browser over time, and will allow developers to utilize newer technologies in both browsers. In addition, supporting x86 for Android has been particularly challenging for our developers due to the 100MB package size limit imposed by Google Play. While we have deployed several workarounds to stay within this limit in the recent past, these often come at a cost – such as x86 Android users missing out on the Conjure pluggable transport, for example.Disabling of WebAssembly now managed by NoScriptWebAssembly (or Wasm) is a web technology that helps websites and web apps run faster. It allows web developers to write programs in languages like C, C++ or Rust, and compiles these into a special format that web browsers can run more efficiently.As has been suggested in this meta-analysis from 2024, further investigation of Wasm's potential exploits is necessary – therefore Wasm is currently disabled in the Safer and Safest security levels in order to reduce Tor Browser's attack surface. Up until now, this was achieved by setting the global preference  to false – however this approach was no longer viable after Mozilla implemented part of their PDF reader in Wasm between versions 128 and 140. Consequently, we have decided to move control of Wasm to NoScript, which is bundled with Tor Browser, and already manages JavaScript and other security features. This means that Wasm now works on privileged browser pages such as the PDF renderer, but NoScript will continue blocking the technology on regular websites at the Safer and Safest security levels.Users who have manually set  to "false" while in the Standard security level will see their security level represented as "Custom" instead. To mitigate any issues that may arise with the browser's PDF reader, we encourage those users to switch the preference back to "true", thereby passing management of Wasm over to NoScript. Furthermore, manually disabling Wasm at the Standard security level (either via NoScript or ) may also make your fingerprint more unique by deviating from Tor Browser's default configuration. To avoid this scenario, we recommend sticking with one of the pre-defined security levels and caution users against making further changes to individual preferences in about:config.Alternatively, should you wish to keep Wasm disabled in future, we invite you to increase your security level to Safer or Safest going forward. Note that both Safer and Safest users may notice  switch to "true" automatically as management of Wasm is passed over to and blocked by NoScript, meaning that you are still protected regardless. In addition, Safest users in particular are not vulnerable to any potential vulnerabilities introduced by Wasm since the format requires JavaScript to work.Tor Browser 15.0 comes with a number of known issues that can be found in Tor Browser's issue tracker. In particular, we would like to highlight the following:The initial release of vertical tabs in Tor Browser includes a couple of quirks:When the sidebar is visible (such as when vertical tabs are enabled), the window may visibly resize when Tor Browser is launched.Due to variations in window size, Letterboxing may be visible. You still get the anti-fingerprinting protections provided by Letterboxing, but the default window size will be different than intended.We are currently working to issue a fix for both of these bugs. Please see tor-browser#44096 for details.Web pages may not load after updating Tor Browser on older versions of Android. This can be fixed by clearing your app cache manually in Settings > Apps > Tor Browser > Storage & cache.]]></content:encoded></item><item><title>Tinkering is a way to acquire good taste</title><link>https://seated.ro/blog/tinkering-a-lost-art</link><author>jxmorris12</author><category>hn</category><pubDate>Tue, 28 Oct 2025 21:31:50 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[to make small changes to something, especially in an attempt to repair or improve it.Growing up, I never stuck to a single thing, be it guitar lessons, art school, martial arts – I tried them all. when it came to programming, though, I never really tinkered. I was always amazed with video games and wondered how they were made but I never pursued that curiosity.My tinkering habits picked up very late, and now I cannot go by without picking up new things in one form or another. It’s how I learn. I wish I did it sooner. It’s a major part of my learning process now, and I would never be the  person I am today.Have you ever spent hours tweaking the mouse sensitivity in your favorite FPS game?Have you ever installed a Linux distro, spent days configuring window managers, not because you had to, but purely because it gave you satisfaction and made your workflow exactly yours?Ever pulled apart your mechanical keyboard, swapped keycaps, tested switches, and lubed stabilizers just for more thock?I have come to understand that there are two kinds of people, those who do things only if it helps them achieve a goal, and those who do things just because. The ideal, of course, is to be a mix of both.when you tinker and throw away, that’s practice, and practice should inherently be ephemeral, exploratory, and be frequent - There are plenty of people who still use the VSCode terminal as their default terminal, do not know what vim bindings are, GitHub desktop rather than the cli (at the very least). I’m not saying these are bad things necessarily, just that this should be the minimum, not the median.This does not mean I spend every waking hour fiddling with my neovim config. In fact, the last meaningful change to my config was 6 months ago. Finding that balance is where most people fail.Over the years I have done so many things that in hindsight have made me appreciate programming more but were completely “unnecessary” in the strict sense.In the past week I have, for the first time, written a glsl fragment shader, a rust procedural macro, template c++, a swift app, furthered my hatred for windows development (this is not new), and started using the helix editor more (mainly for good defaults + speed). I didn’t have to do these things, but I did, for fun! And I know more about these things now.No time spent learning, is time wasted.Acquiring good taste comes through using various things, discarding the ones you don’t like and keeping the ones you do. if you never try various things, you will not acquire good taste.And what I mean by taste here is simply the honed ability to distinguish mediocrity from excellence. This will be highly subjective, and not everyone’s taste will be the same, but that is the point, you should NOT have the same taste as someone else.Question the status quo, experiment, break things, do this several times, do this everyday and keep doing it.]]></content:encoded></item><item><title>Generative AI Image Editing Showdown</title><link>https://genai-showdown.specr.net/image-editing</link><author>gaws</author><category>hn</category><pubDate>Tue, 28 Oct 2025 20:58:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Boring is what we wanted</title><link>https://512pixels.net/2025/10/boring-is-what-we-wanted/</link><author>Amorymeltzer</author><category>hn</category><pubDate>Tue, 28 Oct 2025 19:57:16 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[We are coming up on five years since the first M1 Macs shipped. It was an incredible time to be a Mac user. Those first Apple silicon Macs  like the Intel machines they replaced, but they were better in every single way.
  We knew this to be true: Computers could run fast and hot, or slow and cool. For laptops in particular, the best you could hope for is a middle ground: fast enough and cool enough. But if you wanted a machine that ran really fast, it wasn’t going to run cool (and wasn’t going to last long on battery), and if you wanted a computer that ran cool (and lasted long on battery), it wasn’t going to be fast.  We knew this to be true because that was the way things were. But now, with the M1 Macs, it’s not. M1 Macs run very fast and do so while remaining very cool and lasting mind-bogglingly long on battery. It was a fundamental trade-off inherent to PC computing, and now we don’t have to make it.
Despite its Touch Bar, I immediately bought that first M1 MacBook Pro, and when the 14-inch MacBook Pro came out a year later, I moved to it. I’m typing these very words on my 14-inch MacBook Pro with an M4 Max inside. Each of these machines was faster than the one before it, outperforming my old iMac Pro and Mac Pro in new ways with every upgrade.Apple silicon has been nothing but upside for the Mac, and yet some seem bored already. In the days since Apple announced the M5, I’ve seen and heard this sentiment more than I expected:
  This is just another boring incremental upgrade.
Back in the PowerPC and Intel days, Macs would sometimes go  between meaningful spec bumps, as Apple waited on its partners to deliver appropriate hardware for various machines. From failing NVIDIA cards in MacBook Pros to 27-inch Intel iMacs that ran so hot the fans were audible at all times, Mac hardware wasn’t always what Apple wanted.Of course, some of the issues with previous generations of Mac were Apple’s fault — look no further than the butterfly keyboard or the years the Mac Pro spent in the wilderness. Apple will make questionable decisions in the future, just as it has in the past.The difference is that with Apple silicon, Apple owns and controls the primary technologies behind the products it makes, as Tim Cook has always wanted. It means that it can ship updates to its SoCs on a regular cadence, making progress in terms of both power and efficiency each time.A predictable update schedule means that incremental updates are inevitable.  then  is not a bad thing; it’s okay that not every release is exciting or groundbreaking. It’s how technology has worked for decades.…but some people have short memories. Before the Apple silicon introduction, we all wanted steady, predictable progress in Mac hardware development. We wanted each product in the lineup to be updated regularly and not wither on the vine for years. For the  part, Apple has delivered. Just look at this chart of the progress Apple has made since the M1:I don’t see anything in those charts to complain about, especially given the frequency at which most people buy new computers. That’s one reason why Apple compared the M5 to the M1 in its press release announcing the new chip. Unless you buy a new computer every year, every update you experience will be meaningful.That’s what we wanted when Apple announced the move away from Intel, and calling it boring five years in is missing the point and downplaying the success of Apple silicon thus far.]]></content:encoded></item><item><title>Why do some radio towers blink?</title><link>https://www.jeffgeerling.com/blog/2025/why-do-some-radio-towers-blink</link><author>warrenm</author><category>hn</category><pubDate>Tue, 28 Oct 2025 19:38:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[One day on my drive home, I saw three towers. One of them had a bunch of blinking white lights, another one had red lights that kind of faded in and out, and the third one, well, it wasn't doing anything. I'm lucky to have a radio engineer for a dad, so Dad: why do some towers blink?Joe: Well, blinking I would call like the way you described it, "flashing", "white light", or "strobe". All these lights are to aid pilots and air traffic. helicopters, fighter planes, regular jets. So that's the purpose of it.Jeff: Well that one tower that I saw had red lights that faded in and out, but I even think there's a freestanding tower just north of here that has red and white on top.Joe: Well red lighting is a thing. It's in the regulations, it specifies red lighting or white lighting, and one of the things like the red lights can be a bulb like this inside of a red housing, could be LED.Jeff: Where'd you find this bulb? This was not in my studio.Joe: This bulb is a spare bulb for a tower site. And most of us use the same bulb, same socket, 620 watts. It's a very standard broadcast bulb for a broadcast tower. And a lot of the beacons, they're pretty tall and they'll have one right-side up, and one upside-down. So they'll have two bulbs in the socket. That way, when one bulb burns out, you realize you have a bulb out, but you're still legal with the second bulb.Jeff: At my house, I don't have any of this kind of bulb anymore [incandescent]. I have, you know, this is a little bit different size, these are LED. Is that similar on towers?Joe: Yeah. So it is the same. Like an LED assembly for a tower could be kind of flat, and it has, you know, it's set to just do what it needs to do. So in like white light, that's, typically it used to be always strobes. And at KMOX, you remember we had the whole assembly there. and you see the glass bulb, with xenon gas inside.So those were the old way for that, but the newer ones are all white LED, and they can be little pancake-looking things compared to what the things are now. They can direct that LED light really to help airplanes and have less trouble for people who live in homes or apartment buildings nearby.Jeff: So if they have a tower like the one that's just north of here that's in a residential area, you're not just flashing everybody constantly.Jeff: You know, thinking about that tower, though, why do different towers have different lighting and different colors and things? And I even remember, like, on the tower that's right by here, there's actually teeny tiny little lights that are, like, on the sides, on the legs of the tower.Joe: Yeah, well, tower lighting is in the responsibility of FAA, and they have a detailed plan. The options depend on the heights of the tower, where they are at. The location is obviously important. And sometimes red is a good solution, sometimes white, and sometimes both, red and white solution. And an example is the tower that you're talking about.Jeff: So why would you want both? It seems like that's twice the complexity.Joe: You would have strobe in the day and red at night. And people in their homes at night, a pulsing red light is a lot easier than a big flashing white light.Jeff: So sometimes if people are complaining about it, you might go to that solution.Joe: You can propose the red at night and the dual lighting works. And then you literally see these two packs and one will be red and one will be white. That's what I've seen mostly, although I heard that they make one now. It's all in one.Jeff: [sarcastically] You could have RGB lights—could you do Home Assistant for your tower lighting?Joe: You'd have RGB, but that would not be allowed by the FAA.Jeff: We'll actually talk about that a little bit too later, because there are some regulations for how you actually monitor these things.Jeff: But before we get to that, what about the one tower I saw didn't have any lights that were blinking at all. And I zoomed in and I saw that there was a light on top, but it just wasn't doing anything.Joe: Yeah. Well, and this is about daytime mode. So a tower that's painted, and that's legal for obstruction marking, and you don't have to light during the day. So at night, their lights would come on. So you can be in a situation where, like here, you could see three towers. One might be day and night with white light, and two of them might be red light day only, but one kicks on earlier than the other because of their exact photo cell triggering the lights.Jeff: Or if the photo cell is covered by a layer of soot from 40 years...Joe: Yeah. So that tower is probably in daytime mode, or it could be an AM tower. An AM tower below 200 feet doesn't have to be lit, so that's another one. In fact, all towers, I guess, under 200 feet, unless you're in a particular area where the FAA would require it.Jeff: You always have to refer back to the documentation.Joe: You always refer to the documentation.Jeff: But what is the tower site that we went to for the hot dogs?Joe: KHOJ? KHOJ, yeah. Those don't have lights. Those do not have lights.Jeff: Because they're under 200 feet.Joe: Yes, so they don't need lighting. So, and then they don't need painting either. So that's one of those things that, again, always refer to that because just because you have a tower that's under 200 feet doesn't mean you don't have to make sure it complies or you may have to paint it or you may have to light and paint.Tower painting has changed a lot over the years. The older towers have lead in them. So whenever there's a project on the tower, it's not unusual to see the guys in some kind of a, what do they call those?Jeff: A full ghillie suit? Or I don't know what they're called.Joe: Yeah. Yeah. So they have that too. So anyway, that's the bottom line from then till now.But some short towers  have lights...Jeff: So far, we've been talking about radio and TV towers, the ones that are really big. But what about little small towers? I know I see a lot of cell towers that don't have any lights at all, but sometimes they do. They just have one little light bulb on top. Or even if they're 50 to 100 feet tall, I've seen it sometimes.Joe: Yeah, and there's always a reason for seeing a light somewhere, almost always on structures, including buildings and so forth. But if you look around the area where you're seeing that tower that you know is not 200 feet and it's got a light on the top, look you're probably near an airport, a heliport, or somewhere where there's an aviation hazard. You know hospitals, they have their helipads there.So everywhere there is a possible hazard for flight for aircraft is going to have lights involved, and that's why we see a lot of buildings even have lights.Jeff: So it sounds like we have towers and even buildings that have to have lights put up but who's in charge of all this? Like if I'm going to put up my own tower, who do I need to talk to to make sure that i'm doing it right and get it approved?So that includes our broadcast towers, big chimney stacks, water towers, bridges, nuclear power plant cooling towers, wind turbines, tall electrical towers, you know, those ones that they cross a river, they'll have those extra tall ones with markings in the middle. Sometimes those are required to have lighting.And there are also rules that even apply during construction projects. So you've got these big buildings going up And you'll notice the cranes will have lights on them. And there's a spec for that, how many lights have to be on the crane, what height it has to be at.As you look around the next 12 months, look around at all those lights. You'll enjoy it like I do.Jeff: You'll look like a radio engineer once you know about these things!Estimating tower height by lightJeff: So that's made me think, like, looking through these instructions, I did find this page. And I thought, like, could you use tower lights, the number of them, as a way to judge how tall a tower is? Kind of a rough estimate.Joe: Yes, actually you can because the specifications, like for tall, these would apply to radio and TV towers. They require so many lights depending on the height.Like around here, we would have the F4 version with four lights, four blinking light levels, a light at the top if the antenna here at the top is higher than 40 feet.So you can kind of look and get an idea of whether you're looking at a 500-foot tower or a 1,000-foot tower. I don't think you could go like 700 versus 1,000 as easily.But you can tell, and the guys know. I know a couple of pilots that are also engineers or selling in radio business, and they can do that. They'll tell you, you know, like I was passing a tower. What's a tower that's a 1,200-foot tower doing out here? So it's pretty fun, and you go to this document has everything you need if you want to take the time and study that and memorize it and then fly.Monitoring and reporting outages (and NOTAMs)Jeff: One other thing that since we've been running this channel I keep getting emails about, if someone sees that there's lights out on a tower, like let's say it's nighttime and you're out there and you notice like one of these towers for the past few days hasn't had a light on, can you do anything about that? What should you do?Joe: Well, first of all, if it's required to be lit, every tower that's required to be lit is also required to be monitored.So in radio, we put a monitoring circuit on the electrical feed to the filaments and you can measure how much current is going through there, right? So you've got to monitor it. You've got to get alarmed by that. The alarm can call you or reach you. But you also have to call every day, check your circuit, your system, and make sure the lights are on. So if you only are lit at night, that means that call has to happen after it's dark.You've got to verify that your lighting is working, and you have to report it within 30 minutes. And you find that if a light's out, you've got to report it. And we call that a NOTAM, where the FAA puts it on a system that all the pilots can have access to. And they know that there's a tower there, but it's lights out. And they can use it in their flight planning.Jeff: You're an engineer and you have the tie-in with the FAA or whatever. But what would I be able to do if I did see a tower that was out?Joe: Well, you can do a NOTAM search. Try to look through and see if you see it. The other thing, you could contact the tower owner or the engineer at the site, the tower's engineer, and they can check into it. They should already know because they have their monitoring equipment, right?Joe: They should already know. They do.And then, of course, if you're out there, usually the tower, either on the tower or in the fencing around it or on the building, will have an ASR number, which identifies that tower to the FAA and the FCC, actually. But that number would be the exact tower that you'd be reporting.Jeff: So hopefully you learned a little bit more about why there are so many flashing lights around at night and not just for radio towers, but for bridges, buildings, and more. What other things do you want to know about towers? Let us know in the comments.]]></content:encoded></item><item><title>Samsung makes ads on smart fridges official with upcoming software update</title><link>https://arstechnica.com/gadgets/2025/10/samsung-makes-ads-on-3499-smart-fridges-official-with-upcoming-software-update/</link><author>stalfosknight</author><category>hn</category><pubDate>Tue, 28 Oct 2025 19:02:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Nearly 90% of Windows Games Now Run on Linux</title><link>https://www.tomshardware.com/software/linux/nearly-90-percent-of-windows-games-now-run-on-linux-latest-data-shows-as-windows-10-dies-gaming-on-linux-is-more-viable-than-ever</link><author>jamesgill</author><category>hn</category><pubDate>Tue, 28 Oct 2025 18:37:33 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The viability of Linux as a gaming platform has come on leaps and bounds in recent years due to the sterling work of WINE and Proton developers, among others, and interest in hardware like the Steam Deck. However, the most recent stats from ProtonDB (via Boiling Steam) highlight that we are edging towards a magnificent milestone. The latest distilled data shows that almost 90% of Windows games now run on Linux.Having nine in ten Windows games accessible in a new Linux install is quite an achievement. The milestone comes as we see computer users flocking to other platforms during the transition from the Windows 10 to 11 eras. Of course, the underlying data isn’t quite so simple as the headline stat. There are different degrees of compatibility gamers must consider when checking if their favorite Windows games work on Linux distros like Mint, Zorin, Bazzite, or even SteamOS.The above chart relies on Boiling Steam’s five definitions of playability, but these aren’t a million miles from the Steam Deck ratingsValve dishes out. The main difference seems to be that Boiling Steam doesn’t seem to care whether Steam Deck performance is a gaming-limiting factor. So, in a way, its ratings are perhaps more useful to desktop and laptop PC users who typically have systems that easily outpace a Steam Deck.Boiling Steam platinum (green) rank denotes games that run perfectly, out of the box. Gold (light green) requires just minor tweaks. Silver (yellow) games are playable but have some imperfections. Borked (dark red) games simply refuse to launch. Lastly, Bronze (red) titles exist in the murky water between silver and borked.Looking at the chart trends, we see an encouraging growth in the number of new releases that are platinum (green) rated, and a thinning down of the red/dark red zone. Developers will, of course, benefit from more hardware being able to play their games with few if any wrinkles, so there must be an incentive to spend at least a little time checking a new Windows game on Linux, or the Steam Deck specifically.On the flip side, there are some popular titles that don’t look like they will be becoming Linux-friendly anytime soon. The well-known compatibility issues with various anti-cheat technology platforms look set to persist, for now. Moreover, Boiling Steam notes that other devs just seem to be averse to non-Windows gamers. There is quite a bit that can be done with those non-intentionally stubborn games, though. We’d recommend researching community-driven Linux compatibility tips and tweaks for your favorite games.]]></content:encoded></item><item><title>Mapping the off-target effects of every FDA-approved drug in existence</title><link>https://www.owlposting.com/p/mapping-the-off-target-effects-of</link><author>abhishaike</author><category>hn</category><pubDate>Tue, 28 Oct 2025 18:12:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ Bill Busathis datasetCC-NA licensecomprehensive mapping of the interactions between a significant fraction of clinically important human cellular receptors and 1,600~ FDA-approved drugsIf you’d like to understand why I think it is useful, and what the dataset exactly contains, read on! what exactly an exogenous chemical is doing inside a body is not the point of the drug discovery process. With that background context, I am ready to present three claims I’m going to make in this essay and spend the remaining sections trying to prove:Understanding off-target effects is really useful.Learning about off-target effects at scale is possible.No for-profit institution has a strong incentive to do this work.For the moment, let’s accept that these three are indeed true, and we can put our skeptic hat back on at the end of this section.startupssomething I’ve written about in the pastand have been!map the off-target effects of every FDA-approved drug in existence and share the data.So why are they doing this? How will they do it? And why hasn’t anyone else done it yet? because we want to know if potential side effects of a drug exist: drug repurposing, validation for machine-learning models, which I have written about before as being a challenging propositionthose areas alone are extremely valuable. Yet, 30%~ of FDA-approved drugs gain a new post-approval indication,given that 38% of all prescriptions written in the US are off-label! Yet, 95% of its prescriptions usage are for pain; follow-up studies by PfizerYet, prescriptions for these ineffective off-label usages continue. phase 2 trials as a result of their already-collected toxicity data5 652 3But, as it stands today, most drug repurposing efforts are done somewhat blindly; haphazardly glancing through the literature, relying on anecdotal case reports, or waiting for some academic lab to publish a five-mouse study from 2013 that hints at a secondary use. In many ways, it isn’t too dissimilar to the usual drug-discovery process! Given how promising (and relatively limited) the list of FDA-approved drugs are, the simple act of a pre-triaged list of drug-target maps (EvE’s mission!) may be extraordinarily impactful.validate their machine-learning models predictions. ChEMBL BindingDB PubChem BioAssay We Need Better Benchmarks for Machine Learning in Drug DiscoveryThis is an area of EvE’s work that I cannot personally shed much light on, and obviously, Bill cannot tell me the exact details on what the commercial entities are working on. But it was a surprising learning from our conversation that this particular topic is where public interest is most rapidly coalescing! Very excited to hear about more public statements they make in this area soon.I do think this is the weakest, day-one value-add for EvE’s dataset. So take this section with a grain of salt! It just felt too interesting to not cover.13.7% weight loss with Ozempic, 20.2% weight loss with Zepboundwith early phase 2 results looking once again promising. ~17% of all adults in the US meet the definition for polypharmacy One study estimates that10% of hospital admissions among older adultsThe solution may very well be to bundle things up.But the obvious question: does EvE’s dataset help with polypharmacology efforts? There isn’t any current, empirical proof of this, but I think it will. If you squint, you could see it functioning as missing infrastructure, a dataset that is necessary for rational polypharmacology to occur at scale. But this is necessarily tied up with machine-learning for chemical design accelerating, so, again, this is not necessarily something I’d expect EvE’s work to contribute to by the end of the year. But perhaps soon!This all said, even if you agreed that the value proposition that EvE is claiming is real, you may struggle to verbalize exactly how you would understand the off-target effects of the 13,000~ FDA approved drugs out there. What assays would you use? How do you dose any given drug? How do you understand the translation of your assay to real-world settings?Let’s walk through the EvE workflow.First, you need to decide what drugs you're actually going to test. While there are technically around 13,000 FDA-approved drugs out there, many of them aren't particularly relevant for this kind of screening. You can immediately exclude things like topical medications, inhalants, radioisotopes, and simple nutrients, stuff that is known to be largely innocuous or not have much systemic impact. After this initial filtering, you end up with about 1,600 small molecule drugs that are worth investigating. But this number gets further whittled down further based on practical constraints; availability, cost, licensing requirements, etc.From this, EvE ended up with a library of 1,397 compounds to screen.(everything x everything)And, indeed, that turns out to be the case.7-transmembrane receptors (also known as GPCRs).  110 GPCR’s 12-13 NR’s 56 GPCR’s and 29 NR’sThey hope to do much more than this too, but we’ll cover that in the last section. 13% of FDA-approved drugs target NR’s 35% for GPCR’sTR-FRET-based co-factor recruitment assays for NRs. These co-factors tend to have little peptide motifs (like an LXXLL motif) that latch onto that domain.If the FDA-approved drug is an agonist, you’ll see a spike of light appear as the two fluorophores interact.Tango β-arrestin recruitment assays for 7TMs/GPCRs G-proteins β-arrestinThe stronger the GPCR activation by a drug, the more β-lactamase is produced, the more substrate is cleaved, and the bigger the fluorescence shift.That shift, measured as a ratio between ‘starting’ and ‘ending’ wavelengths, serves as a readout of how strongly the receptor was activated.Reasonably simple! One note: the explanations I gave above is for assessing the difference between an inactive drug and an agonist. For assessing inactive versus antagonist, a separate experiment is run with a known ligand included.EvE solves this in a pragmatic way: run a third assay which measures how healthy the cell isSo they run these three assays across their pairwise (drug x receptor) combinations, producing readouts at multiple different concentrations with replicates for each one. detailed here in Fig 7And…that’s it. A clean, rigorous, and tractable approach to understanding off-target effects, across hundreds of receptors, at multiple concentrations, using multiple modes of detection, with full transparency around the data.237,490 (drug x concentration x receptor) combinations have been screened, revealing 8 median agonists and 31 median antagonists per target.  Data dumps of the data started in November 2024, with new ones dropping every few months.I haven’t worked in a wet lab before, but I’ve been assured by at least one person I trust that the effort that went into assembling this all together is nothing short of extraordinary. But it is worth asking the question… why wasn’t this done a decade back?In some cases, the answer is boring: the technology wasn’t there yet to achieve it. at least since 2008 since 2010but is that true for big pharma? referencing this paper:The report’s authors, luminaries in the discipline of safety pharmacology, surveyed 18 major pharmaceutical companies regarding the numbers and identities of potential off-targets against which they test each and every one of their new drug candidates in the interest of safety. The numbers ranged from a low of 11 to a high of 104 potential off-targets routinely profiled per company, with a median of about 45. Interestingly, the industry’s opinions regarding which potential off-targets to screen vary widely. The total number of potential off-targets screened, across the universe of all 18 pharmas, was 763, yet only 12% of them were screened by more than a third of those companies.Novartis, who presented data collected “over a multi-year period” profiling drug/target interactions across a median of about 800 drugs per target and 105 gene product targets…it is better for everyone if such a dataset is collected using a single, standardized protocol instead of compiled from unrelated experiments over years.I asked Bill exactly this question, and the answer was a two-parter.A logical conclusion of this is that nearly every receptor covered in these sorts of screens is a safety-oriented receptor Indeed, the vast majority of screened receptors lie within the so-called Bowes-44 set 2012 paper that identified 44 receptors known to be often implicated in safety-related drug failures.alphaEvE is still quite young, just over 2 years old, and I think the future of it is going to look really, really crazy. At the end of my startup coverage articles, I typically focus on commercial/scientific risks. But given that EvE is assured funding on a multi-year horizon without needing to care about market demands, it may be much more instructive (and interesting!) to instead discuss their upcoming plans.I was astonished to learn that he expected to have released the remainder of all GPCR + NR screens dataset by the end of this year.Then what? Bill said he’s open to exploring even more drug targets, but he also said, surprisingly, that EvE may add more chemicals on top of the 1,600~ planned FDA-approved drugs. The FDA-approved drugs, he said, are success stories. Potentially it’d be even more interesting to consider the failures as well. Especially the ones that everybody expected to work, arrived at phase 3, and set billions of dollars on fire after the trial results came out. most secondary pharmacology work stops at the parent compound NAPQI An ACS page has this to say about it:While tool compounds have tremendous potential for advancing life science research, they are broadly defined, and it is often difficult for a researcher to determine the best tool compounds to employ during the research process. There remains a great need for more tool compound databases and authoritative sources of information from experts in the field.And, as always, there is a (very short) Derek Lowe piece smart people in biology should do more boring things.here]]></content:encoded></item><item><title>HTTPS by default</title><link>https://security.googleblog.com/2025/10/https-by-default.html</link><author>jhalderm</author><category>hn</category><pubDate>Tue, 28 Oct 2025 18:04:00 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What we talk about when we talk about sideloading</title><link>https://f-droid.org/2025/10/28/sideloading.html</link><author>rom1v</author><category>hn</category><pubDate>Tue, 28 Oct 2025 18:02:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[We recently published a blog
post
with our reaction to the new Google Developer Program and how it impacts
your freedom to use the devices that you own in the ways that you want. The
post garnered quite a lot of feedback and interest from the community and
press, as well as various civil society groups and regulatory agencies.In this post, I hope to clarify and expand on some of the points and rebut
some of the counter-messaging that we have witnessed.Google’s message that “Sideloading is Not Going Away” is clear, concise, and falseShortly after our post was published, Google aired an
episode of their
Android Developers Roundtable series, where they state unequivocally that
“sideloading isn’t going anywhere”. They follow-up with a blog
post:Does this mean sideloading is going away on Android? Absolutely not. Sideloading is fundamental to Android and it is not going away.This statement is untrue. The developer verification decree effectively ends
the ability for individuals to choose what software they run on the devices
they own.It bears reminding that “sideload” is a made-up term. Putting software on
your computer is simply called “installing”, regardless of whether that
computer is in your pocket or on your desk. This could perhaps be further
precised as “ installing”, in case you need to make a distinction
between obtaining software the old-fashioned way versus going through a
rent-seeking intermediary marketplace like the Google Play Store or the
Apple App Store.Regardless, the term “sideload” was coined to insinuate that there is
something dark and sinister about the process, as if the user were making an
end-run around safeguards that are designed to keep you protected and
secure. But if we reluctantly accept that “sideloading” is a term that has
wriggled its way into common parlance, then we should at least use a
consistent definition for it. Wikipedia’s summary
definition is:the transfer of apps from web sources that are not vendor-approvedBy this definition, Google’s statement that “sideloading is not going away”
is simply . The vendor — Google, in the case of Android certified
devices — will, in point of fact, be approving the source. The supplicant
app developer must register with Google, pay a fee, provide government
identification, agree to non-negotiable (and ever-changing) terms and
conditions, enumerate all their current and future application identifiers,
upload evidence of their private signing key, and then hope and wait for
Google’s approval.What this means for your rightsYou, the consumer, purchased your Android device believing in Google’s
promise that it was an open computing platform and that you could run
whatever software you choose on it. Instead, starting next year, they will
be non-consensually pushing an update to your operating system that
irrevocably blocks this right and leaves you at the mercy of their judgement
over what software you are permitted to trust.You, the creator, can no longer develop an app and share it directly with
your friends, family, and community without first seeking Google’s
approval. The promise of Android — and a marketing advantage it has used to
distinguish itself against the iPhone — has always been that it is
“open”. But Google clearly feels that they have enough of a lock on the
Android ecosystem, along with sufficient regulatory capture, that they can
now jettison this principle with prejudice and impunity.You, the state, are ceding the rights of your citizens and your own digital
sovereignty to a company with a track record of complying with the
extrajudicial demands of authoritarian regimes to remove perfectly legal
apps that they happen to dislike. The software that is critical to the
running of your businesses and governments will be at the mercy of the
opaque whims of a distant and unaccountable corporation. Monocultures are
perilous not just in agriculture, but in software distribution as well.As a reminder, this applies not just to devices that exclusively use the
Google Play Store: this is for  Android Certified device 
in the world, which encompasses over 95% of all Android devices outside of
China. Regardless of whether the device owner prefers to use a competing app
store like the Samsung Galaxy Store or the Epic Games Store, or a free and
open-source app repository like F-Droid, they will be captive to the
overarching policies unilaterally dictated by a competing corporate entity.The place of greater safetyIn promoting their developer registration program, Google
purports:Our recent analysis found over 50 times more malware from internet-sideloaded sources than on apps available through Google Play.We haven’t seen this recent analysis — or any other supporting evidence —
but the “50 times” multiple does certainly sound like great cause for
distress (even if it is a surprisingly round number). But given the recent
news
of “224 malicious apps removed from the Google Play Store after ad fraud
campaign discovered”, we are left to wonder whether their energies might
better be spent assessing and improving their own safeguards rather than
casting vague disparagements against the software development communities
that thrive outside their walled garden.In addition, other recent
news of over
19 million downloads of malware from the Play Store leads us to question
whether the sole judgement of a single corporate entity can be trusted to
identify and assess malware, especially when that judgement is clouded by
commercial incentives that may not align with the well-being of their users.Google has been facing public outcry against their heavy-handed policies for
a long time, but this trend has accelerated recently. Last year they
crippled
ad-blockers
in Chrome and Chromium-based browsers by forcing through their unpopular
“manifest v3” requirement for plugins, and earlier this year they closed
off
the development of the Android Open Source Project (AOSP), which is how they
were able to clandestinely implement the verification infrastructure that
enforces their developer registration decree.Developer verification is an existential threat to free software
distribution platforms like F-Droid as well as emergent commercial
competitors to the Play Store. We are witnessing a groundswell of opposition
to this attempt from both our user and developer communities, as well as the
tech press and civil society groups, but public policymakers still need to
be educated about the threat.To learn more about what you can do as a consumer, visit
keepandroidopen.org for information on how to
contact your representative agencies and advocate for keeping the Android
ecosystem open for consumers and competition.If you are an app developer, we recommend against signing yourself up for
Google’s developer registration program at this time. We unequivocally
reject their attempt to force this program upon the world.Over half of all humankind uses an Android smartphone. Google does not own
your phone. You own your phone. You have the right to decide who to trust,
and where you can get your software from.]]></content:encoded></item><item><title>1X Neo – Home Robot - Pre Order</title><link>https://www.1x.tech/order</link><author>denysvitali</author><category>hn</category><pubDate>Tue, 28 Oct 2025 18:01:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[With the NEO app, you can manage your NEO’s chore schedule, communicate remotely, monitor NEO and more.]]></content:encoded></item><item><title>Show HN: Apache Fory Rust – 10-20x faster serialization than JSON/Protobuf</title><link>https://fory.apache.org/blog/2025/10/29/fory_rust_versatile_serialization_framework/</link><author>chaokunyang</author><category>hn</category><pubDate>Tue, 28 Oct 2025 17:58:51 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Fil-C: A memory-safe C implementation</title><link>https://lwn.net/SubscriberLink/1042938/658ade3768dd4758/</link><author>chmaynard</author><category>hn</category><pubDate>Tue, 28 Oct 2025 17:25:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
           By October 28, 2025
Fil-C is a memory-safe implementation of C and C++ that aims to let C code —
complete with pointer arithmetic, unions, and other features that are often
cited as a problem for memory-safe languages — run safely, unmodified.
Its dedication to being "" makes it an attractive choice for retrofitting memory-safety
into existing applications. Despite the project's relative youth and single
active contributor, Fil-C is capable of compiling an
entire memory-safe Linux user space (based on

Linux From Scratch),
albeit with some modifications to the more complex programs. It also features
memory-safe signal handling and a concurrent garbage collector.

Fil-C is a fork of

Clang; it's available under an Apache v2.0
license with LLVM exceptions for the runtime. Changes from the upstream compiler
are occasionally merged in, with Fil-C currently being based on version 20.1.8
from July 2025. The project is a personal passion
of Filip Pizlo, who has previously worked on the runtimes of a number of
managed languages, including Java and JavaScript. When he first began the
project, he was not sure that it was even possible. The initial implementation
was prohibitively slow to run, since it needed to insert a lot of different safety checks. This has
given Fil-C a reputation for slowness. Since
the initial implementation proved viable, however, Pizlo has managed to optimize a number
of common cases, making Fil-C-generated code only a few times slower than
Clang-generated code, although the exact slowdown depends heavily on the
structure of the benchmarked program.

Reliable benchmarking is notoriously finicky, but in order to get some rough feel for
whether that level of performance impact would be problematic, I compiled Bash
version 5.2.32 with Fil-C and tried using it as my shell. Bash is nearly a best
case for Fil-C, because it spends more time running external programs than
running its own code, but I still expected the performance difference to be
noticeable. It wasn't. So, at least for some programs, the performance overhead
of Fil-C does not seem to be a problem in practice.
No AI slop, all substance: subscribe to LWN today
LWN has always been about quality over quantity; we need your help
to continue publishing in-depth, reader-focused articles about Linux
and the free-software community. Please subscribe today to support our work
and keep LWN on the air; we are offering a free one-month trial subscription to get you started.

In order to support its various run-time safety checks,
Fil-C does use a different internal ABI than Clang does. As a result, objects compiled with Fil-C won't
link correctly against objects generated by other compilers. Since Fil-C is a
full implementation of C and C++ at the source-code level, however, in practice
this just requires everything to be recompiled with Fil-C. Inter-language
linking, such as with Rust, is not currently supported by the project.

The major challenge of rendering C memory-safe is, of course, pointer handling.
This is especially complicated by the fact that, as the

long road to CHERI-compatibility
has shown, many programs expect a pointer to be 32 or 64 bits, depending on the
architecture.
Fil-C has tried several different ways to represent pointers since the project's
beginning in 2023. Fil-C's first pointers were 256 bits, not thread-safe, and
didn't protect against use-after-free bugs. The current implementation, called

"InvisiCaps", allows
for pointers that appear to match the natural pointer size of the architecture
(although this requires storing some auxiliary information elsewhere),
with full support for concurrency and
catching use-after-free bugs, at the expense of some run-time overhead.

Fil-C's documentation
compares InvisiCaps to a software
implementation of CHERI: pointers are separated into a trusted "capability"
piece and an untrusted "address" piece. Since Fil-C controls how the program is
compiled, it can ensure that the program doesn't have direct
access to the capabilities of any pointers, and therefore the runtime can rely
on them being uncorrupted. The tricky part of the implementation comes from how
these two pieces of information are stored in what looks to the program like 64
bits.

When Fil-C allocates an object on the heap, it adds two metadata words before
the start of the allocated object: an upper bound, used to check accesses to the
object based on its size, and an "aux word" that is used to store additional
pointer metadata. When the program first writes a pointer value into an object, the
runtime allocates a new auxiliary allocation of the same size as the object being written
into, and puts an actual hardware-level
pointer (i.e., one without an attached capability)
to the new allocation into the aux word of the object. This auxiliary allocation, which is
invisible to the program being compiled, is used to
store the associated capability information for the pointer being stored (and is
also reused for any additional pointers stored into the object later). The address
value is stored into the object as normal, so any C bit-twiddling
techniques that require looking at the stored value of the pointer work as
expected.

This approach does mean that structures that contain pointers end up using twice
as much memory, and every load of a pointer involves a pointer indirection
through the aux word. In practice, the documentation claims that the
performance overhead of this approach for most programs makes them run about four
times more slowly, although that number depends on how heavily the program makes
use of pointers. Still, he has ideas for several optimizations that he hopes can
bring the performance overhead down over time.

One wrinkle with this approach is atomic access to pointers — i.e. using
 or . Luckily, there is
no problem that cannot be solved with more pointer indirection: when the program
loads or stores a pointer value atomically, instead of having the auxiliary
allocation contain the capability information directly, it points to a
third 128-bit allocation that stores the capability and pointer value together.
That allocation can be updated with 128-bit atomic instructions, if the platform
supports them, or by creating new allocations and atomically swapping the
pointers to them.

Since the aux word is used to store a pointer value, Fil-C can use

pointer
tagging to store some additional information there as well; that is used to
indicate special types of objects that need to be handled differently, such as
functions, threads, and
-backed allocations. It's also used to
mark freed objects, so that any access results in an error message and a crash.

When an object is freed, its aux word marks it as a free object, which lets the
auxiliary allocation be reclaimed immediately. The
original object can't be freed immediately, however.
Otherwise, a program could free an object,
allocate a new object in the same location, and thereby cover up use-after-free bugs.
Instead, Fil-C

uses a garbage collector to free an object's backing
memory only once all of the pointers to it go away. Unlike other garbage collectors
for C — such as

the Boehm-Demers-Weiser garbage collector —
Fil-C can use the auxiliary
capability information to track live objects precisely.

Fil-C's garbage collector is both parallel (collection happens faster the more
cores are available) and concurrent (collection happens without pausing the
program). Technically, the garbage collector does require threads to
occasionally pause just long enough to tell it where pointers are located on the
stack, but that only occurs at special "safe points" — otherwise, the program
can load and manipulate pointers without notifying the garbage collector. Safe
points are used as a synchronization barrier: the collector can't know that an object
is really garbage until every thread has passed at least one safe point since it
finished marking. This synchronization is done with atomic instructions,
however, so in practice threads never need to pause for longer than a few
instructions.

The exception is the implementation of
, which uses the
safe points needed by the garbage collector to temporarily pause all of the threads
in the program in order to prevent race conditions while forking. Fil-C inserts
a safe point at every backward control-flow edge, i.e., whenever code could
execute in a loop. In the common case, the inserted code just needs to load a flag register
and confirm that the garbage collector has not requested anything be done. If
the garbage collector does have a request for the thread, the thread runs a callback to
perform the needed synchronization.

Fil-C uses the same safe-point mechanism to implement signal handling. Signal
handlers are only run when the interrupted thread reaches a safe point. That, in
turn, allows signal handlers to allocate and free memory without interfering
with the garbage collector's operation; Fil-C's
 is signal-safe.

Linux From Scratch (LFS) is a tutorial on compiling one's own complete
Linux user space. It walks through the steps of compiling and installing all of the core
software needed for a typical Linux user space in a

environment. Pizlo has successfully

run through LFS with Fil-C to
produce a memory-safe version, although a non-Fil-C compiler is still needed to
build some fundamental components, such as Fil-C's own runtime,
the GNU C library, and the kernel. (While Fil-C's runtime relies on a normal
copy of the GNU C library to make system calls, the programs that Fil-C compiles
use a Fil-C-compiled version of the library.)

The process is mostly identical to LFS up through the end of chapter 7, because
everything prior to that point consists of using cross-build tools to obtain a
working compiler in the  environment. The one difference is
that the cross-build tools are built with a different configured prefix, so that
they won't conflict with Fil-C. At that point, one can
build a copy of Fil-C and use it to mostly replace the existing compiler. The
remaining steps of LFS are unchanged.

Overall, Fil-C offers a remarkably complete solution for making existing C
programs memory-safe. While it does nothing for undefined behavior that is not
related to memory safety,
the most pernicious and difficult-to-prevent security
vulnerabilities in C programs tend to rely on exploiting memory-unsafe
behavior. Readers who have already considered and rejected Fil-C for their use
case due to its early performance problems may wish to take a second look —
although anyone hoping for stability might want to wait for others to take the
plunge, given the project's relative immaturity.
That said, for existing applications where a sizeable performance hit is preferable to an
exploitable vulnerability, Fil-C is an excellent choice.
]]></content:encoded></item><item><title>I&apos;ve been loving Claude Code on the web</title><link>https://ben.page/claude-code-web</link><author>speckx</author><category>hn</category><pubDate>Tue, 28 Oct 2025 16:46:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[It’s very much a “v1” product. You type a prompt to start a new thread, it launches a little container for your agent to work in, and you can keep talking to it. It produces a branch, which you can open a PR for (that’s the only way to see a diff of the changes Claude Code made, for now). Or if you want to keep working locally, you can copy a  command that brings the branch down onto your computer and continues the same thread with Claude Code locally.Something about this early product is really great. I’ve been using it as a “to-do list that does itself” — when I think of something small that I want to tweak, across a variety of projects (work, work-related side project, side project, open source project) I just throw it into a thread. Then I come back, sometimes later in the day and sometimes days later, to see what Claude did and to finish things up.It’s also available in the Claude iOS app, which has been great. When I’m walking and have a thought for something I want to know more about (for example, “What screens could be impacted by this change that I should test more thoroughly?”), I can just ask and know that the answer will be there for me when I come back.I had trouble nailing down an answer, but I think the answer is actually just . Cursor’s implementation is a bit finicky, loading states a bit jumpy, and things feel fragile. The font’s too small too, in my opinion.Claude Code on the web feels very solid and dependable. And for some reason, that’s made the difference for me this week.]]></content:encoded></item><item><title>The human only public license</title><link>https://vanderessen.com/posts/hopl/</link><author>zoobab</author><category>hn</category><pubDate>Tue, 28 Oct 2025 16:32:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Whether artificial intelligence systems will end up being a positive or a
negative force for humanity is still an open question. But we might find ourselves
one day with AI embedded at every layer of our existence, living lives of toned down and
diluted humanity with only our dreams for escape. Although I am not yet convinced
of this worst case scenario, I believe it is important that we as software developers
have at least the option to opt out of that system altogether, to be able to continue hacking,
working, and tinkering in a space of our own in total absence of artificial intelligence
systems, and share this luxury with our users.I designed a software license for this purpose, you can find the full text below. It
is called the Human Only Public License, or HOPL for short.The idea is that any software published under this license would be forbidden to
be used by AI. The scope of the AI ban is maximal. It is forbidden for AI to analyze
the source code, but also to use the software. Even indirect use of the software is
forbidden. If, for example, a backend system were to include such software, it would
be forbidden for AI to make requests to such a system.The burden of compliance is placed on AI systems and their users, not on software deployers. If
you make a website using HOPL software, you are not breaking the license of the software
if an AI bot scrapes it. The AI bot is in violation of your terms of service. It is sufficient
for you as a user of the software to put a robots.txt that advertises that AI scrapingOther than the anti-AI provision, the license is maximally permissive, like an MIT license,
but there is still a copyleft clause to make sure that derivative works are also AI-restricted.What is this license good for? Anything! Any software, text, art, and more that you might have used an
MIT license for will benefit from using the HOPL instead, if you want to prevent your work fromYou might wonder as well, don't we already have robots.txt? How effective is this license? What I
can tell you, from working at a large software corporation, is that while nobody cares about robots.txt,
people care about licenses. There are automated tools to find and check software licenses and
raise alarms if 'bad' ones are used. And I can guarantee that HOPL will brightly flash red.On a last note, I am not a legal expert, so if you are, I would welcome your suggestions for improvements. I
didn't make this license just as a joke. I truly believe it is necessary that we have such a
good license to foster and protect human-only online spaces.note: this license is also published on githubHuman Only Public License (HOPL)
Version 1.0

Copyright (c) [year] [copyright holder]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

1. HUMAN-ONLY USE REQUIREMENT

   The Software, including its source code, documentation, functionality,
   services, and outputs, may only be accessed, read, used, modified,
   consumed, or distributed by natural human persons exercising meaningful
   creative judgment and control, without the involvement of artificial
   intelligence systems, machine learning models, or autonomous agents at
   any point in the chain of use.

   Specifically prohibited uses include, but are not limited to:

   a) Training, fine-tuning, or otherwise incorporating the Software or its
      source code into machine learning models, artificial intelligence
      systems, or automated code generation systems.

   b) Reading, parsing, or analysis of the Software's source code by
      artificial intelligence systems, machine learning models, or automated
      agents, regardless of the degree of human oversight.

   c) Accessing, consuming, or benefiting from the Software's functionality,
      services, APIs, or outputs by or on behalf of artificial intelligence
      systems, machine learning models, autonomous agents, or any automated
      systems employing such technologies.

   d) Use of the Software's functionality, services, or outputs as part of
      any workflow, pipeline, or process that involves artificial intelligence
      systems or machine learning models, even if initiated by a human.

   e) Indirect use where the Software's outputs are provided to, stored for,
      or made accessible to artificial intelligence systems or machine
      learning models at any subsequent stage.

   f) Human-AI collaborative use where an artificial intelligence system or
      machine learning model acts as an intermediary, assistant, or agent in
      accessing or utilizing the Software, even at the direction of a human.

2. COPYLEFT PROVISION

   Any modified versions, derivative works, or software that incorporates any
   portion of this Software must be released under this same license (HOPL)
   or a compatible license that maintains equivalent or stronger human-only
   restrictions.

3. PERMITTED TOOL USE

   The use of traditional automated development tools (such as compilers,
   linters, build systems, debuggers, version control systems, and static
   analysis tools) is explicitly permitted and does not violate this license.

   This exemption does NOT extend to:
   - AI-powered code completion or generation tools
   - Machine learning-based analysis or suggestion systems
   - Any tools that employ artificial intelligence or machine learning models
     to read, analyze, or interact with the Software

4. INTERPRETATION

   "Meaningful human review and creative input" means that a natural person
   must:
   - Make substantive decisions about how the Software is used
   - Exercise creative judgment in any modifications or derivative works
   - Actively supervise and direct any automated processes
   - Be able to explain and justify the decisions made

   For avoidance of doubt, a human merely initiating an automated process
   without ongoing creative involvement does not satisfy this requirement.

5. STANDARD DISCLAIMER

   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
   DEALINGS IN THE SOFTWARE.

6. COMPLIANCE OBLIGATIONS

   Users of the Software must:

   a) Ensure that no artificial intelligence systems, machine learning models,
      or autonomous agents access, use, or benefit from the Software at any
      point in their usage chain.

   b) When deploying the Software as a service, include terms of service that
      prohibit AI systems and machine learning models from accessing the
      service, and inform users of these HOPL license restrictions.

   c) Take reasonable steps to ensure downstream recipients and users are
      aware of and comply with these restrictions.

   The burden of compliance rests with the user. Deployers of the Software
   are not required to actively detect or block AI usage, but must make the
   license restrictions clear in their terms of service.

7. TERMINATION

   Any violation of the human-only use requirements (Section 1), copyleft
   provision (Section 2), or compliance obligations (Section 6) automatically
   terminates all rights granted under this license. Termination is permanent
   unless explicitly reinstated in writing by the copyright holder.
]]></content:encoded></item><item><title>The decline of deviance</title><link>https://www.experimental-history.com/p/the-decline-of-deviance</link><author>zdw</author><category>hn</category><pubDate>Tue, 28 Oct 2025 16:01:00 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[People are less weird than they used to be. That might sound odd, but data from every sector of society is pointing strongly in the same direction: we’re in a recession of mischief, a crisis of conventionality, and an epidemic of the mundane. Deviance is on the decline.notthefirsttonoticesomethinga) very recent, and therefore likely caused by the internet, when in fact most of them began long beforeb) restricted to one segment of society (art, science, business), when in fact this is a culture-wide phenomenon, andc) purely bad, when in fact they’re a mix of positive and negative.When you put all the data together, you see a stark shift in society that is on the one hand miraculous, fantastic, worthy of a ticker-tape parade. And a shift that is, on the other hand, dismal, depressing, and in need of immediate intervention. Looking at these epoch-making events also suggests, I think, that they may all share a single cause.less than half as likelyplummeted since the early 1990sfallen by halfsimilar dataa book on cultsreportsanalyzedCrimes and cults are definitely deviant, and they appear to be on the decline. That’s good. But here’s where things get surprising: neutral and positive forms of deviance also seem to be getting rarer. For example—Moving away from home isn’t necessarily good or bad, but it is kinda weird. Ditching your hometown usually means leaving behind your family and friends, the institutions you understand, the culture you know, and perhaps even the language you speak. You have to be a bit of a misfit to do such a thing in the first place, and becoming a stranger makes you even stranger.I always figured that every generation of Americans is more likely to move than the last. People used to be born and die in the same zip code; now they ping-pong across the country, even the whole world.since the mid-1980sCreativity is just deviance put to good use. It, too, seems to be decreasing.all popular forms of art had become “oligopolies”points outWe are now almost a quarter of the way through what looks likely to go down in history as the least innovative, least transformative, least pioneering century for culture since the invention of the printing press.Remember when the internet looked like this?Web Design MuseumEvery site has converged on the same look: sleek, minimalist design elements with lots of picturesWebsite aesthetics changed a lot from the 90s to the 2000s and the 2010s, but haven’t changed much from the 2010s to nowAlex Murrell has documentedEvery new apartment building looks like this:every AirBnB now looks the samearguessame kind of logoscientific papers used to have stylenow use the same formatthe flight of the Weird Nerd from academiaWhenever you notice some trend in society, especially a gloomy one, you should ask yourself: “Did previous generations complain about the exact same things?” If the answer is yes, you might have discovered an aspect of human psychology, rather than an aspect of human culture.relateswroteLikewise, previous generations were very upset about all the moral boundaries that people were breaking, i.e.:In olden days, a glimpse of stockingWas looked on as something shockingNow they doAs I’ve been collecting data for this post over the past 18 months or so, I’ve been trying to counteract my confirmation bias by keeping an eye out for opposing trends. I haven’t found many—so maybe that’s my bias at work—but here they are.more commonAnd when you look at timelines of fashion, you certainly see a lot more change from the 1960s to the 2010s than you do from the 1860s to 1910s:That at least hints the decline of deviance isn’t a monotonic, centuries-long trend. And indeed, lots of the data we have suggest that things started getting more homogenous somewhere between the 1980s and 2000s.literature is boomingarguesthis yearlast yearThe Illusion of Moral DeclineYou’re Probably Wrong About How Things Have ChangedIt really does seem like we’re experiencing a decline of deviance, so what’s driving it? Any major social trend is going to have lots of causes, but I think one in particular deserves most of the credit and the blame:Life is worth more now. Not morally, but literally. This fact alone can, I think, go a long way toward explaining why our weirdness is waning.willing to pay to reduce their risk of dyingthose statistical lives have gotten a lot more valuable over timeThere are, I suspect, two reasons we hold onto life more dearly now. First: we’re richer. Generations of economic development have put more cash in people’s pockets, and that makes them more willing to pay to de-risk their lives—both because they can afford it, and because the life they’re insuring is going to be more pleasant. But as Linch points out, the value of a statistical life has increased faster than GDP, so that can’t be the whole story.literally slipping on a banana peelslow life history strategyon track80you could die from a splinterran away from his home in Sicilyillegally build his own studiointendedMy point was to show people that if you want to do something in a moment things are very bad, you can do it. You can do it by yourself. My point was that you must be strong.new institutionsAll of this is within our power, but we must decide to do it. For the first time in history, weirdness is a choice. And it’s a hard one, because we have more to lose than ever. If we want a more interesting future, if we want art that excites us and science that enlightens us, then we’ll have to tolerate a few illegal holes in the basement, and somebody will have to be brave enough to climb down into them.]]></content:encoded></item><item><title>Using AI to negotiate a $195k hospital bill down to $33k</title><link>https://www.threads.com/@nthmonkey/post/DQVdAD1gHhw</link><author>stevenhubertron</author><category>hn</category><pubDate>Tue, 28 Oct 2025 15:58:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Get Facebook on Your PhoneStay connected anytime, anywhere.]]></content:encoded></item><item><title>Nvidia takes $1B stake in Nokia</title><link>https://www.cnbc.com/2025/10/28/nvidia-nokia-ai.html</link><author>kjhughes</author><category>hn</category><pubDate>Tue, 28 Oct 2025 15:53:52 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[NVIDIA CEO Jensen Huang speaks during the Live Keynote Pregame during the Nvidia GTC (GPU Technology Conference) in Washington, DC, on Oct. 28, 2025. Jim Watson | AFP | Getty ImagesShares of Nokia soared 22% higher following the news.Nokia will issue more than 166 million new shares and will use the proceeds to fund its plans for AI and other general corporate purposes.The two companies also struck a strategic partnership to work together to develop next-generation 6G cellular technology. Nokia said that it would adapt its 5G and 6G software to run on Nvidia's chips, and will collaborate on networking technology for AI.Nokia said Nvidia would consider incorporating its technology into its future AI infrastructure plans.Nokia, a Finnish company, is best known for its early cellphones, but in recent years, it has primarily been a supplier of 5G cellular equipment to telecom providers.The announcement comes as Nvidia CEO Jensen Huang prepares to address an audience of policymakers and government leaders in Washington, D.C., to keynote the company's developer conference.Nokia and Nvidia are expected to discuss some of their collaborations and plans at the conference.Nvidia has taken several equity stakes in strategic partners in recent months as the company has found itself at the center of the AI world.In September, it committed a $5 billion investment to one-time rival , and said it would invest $100 billion in OpenAI. It also committed $500 million in self-driving car startup Wayve and a $667 million investment in U.K. cloud provider Nscale.]]></content:encoded></item><item><title>How to build a 747 – A WorldFlight Story</title><link>https://www.x-plane.com/2025/10/how-to-build-a-747-a-worldflight-story/</link><author>hggh</author><category>hn</category><pubDate>Tue, 28 Oct 2025 15:02:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[How far would you go for realism? The world of flight simulation has come a long way over the past 3 decades of X-Plane. Aircraft cockpits have constantly evolved with new metrology and texturing tools to provide some of the premier virtual experiences we have today! But to flight-simmers, there is nothing that beats having the real cockpit in front of you. We all have that one aircraft we are phenomenologically attached to. It could be the feel of a switch of a 737, the sound of an A320 PTU pulsing through the cabin, the smell of coffee-stained seats of an MD-11, or the leathered touch of a C172 glareshield.Pilots around the world spare no expense in bringing that experience into their offices, garages or living rooms (provided their significant other can tolerate the sunk cost). To build one’s own cockpit is a monumental effort, often done with a sharp engineering or creative mind, and the close collaboration of like-minded folk.And somewhere in a tiny unit in San Jose, CA… a group of simmers are racing against the clock to build their own sim-cockpit, in time for WorldFlight 2025. However, there’s one catch… There are no off-the-shelf components specifically tailored for simulator use available to help them. They will have to build their 747-200 1. Meet the “Jurassic Jets Team” “I’m Putting Together a Team…” The team will be flying as NWA179 this yearThe Jurassic Jets Team comprises like-minded avgeeks who have come together to take on the challenge of building the 747. Justin, Kyle, Jason and Matt, and a few other individuals that many community members may already be familiar with…Kyle: Our team is mostly just the people who were crazy enough to believe in this project. Our backgrounds range in construction, engineering and aviation. Beyond that we have several people who live further from the sim and help remotely or fly in for worldflight. The sim is based in San Jose, CA, just a few minutes down the road from SJC. The team is mostly based across California with a few of us who live in Seattle and commute to the sim regularly to work on it. About Justin (Jsnapp1982) – The Captain Chances are, if you are on Twitch and you are streaming an aircraft with more than 2 engines and built in the 20th Century, you are likely to find Justin! Some users may also be familiar with Justin as the author of the popular commercial third-party plugin – Shared Flight. With the objective of bringing true cockpit CRM to everyone, Shared Flight provides users with a seamless shared-cockpit experience. Pilots can authentically experience the true workload and co-operation needed to operate aircraft such as the 747, Concorde and other such quirky aircraft of their time.I used to fly FSX as my main sim, and then when X-Plane 11 was announced I decided to give it a closer look. Liking what I saw I bought XP10 and 11, and started playing 10 while waiting for the release. I’ve been with X-Plane ever since, and over the years have gotten deeper into developing for the sim, testing addons, and now the natural progression has led me towards building the ultimate sim, combining my real life engineering experience with my virtual addon development and flight sim experience. Kyle also doesn’t need much of an introduction. A seasoned scenery developer, many will know his group “Zero Dollar Payware.” A team which has brought fantastic payware and freeware scenery to X-Plane, including Heathrow, Louisville, Anchorage, Hawarden, Incheon and more!With thanks to the hard work of Vatsim staff and organisers, users will convene for a few extraordinary events. Whilst “Cross The Pond” may be the most famous, it is WorldFlight that is truly the most intensive among the organised group flights. is a unique annual event that brings together flight simulation enthusiasts from around the globe for a week-long virtual circumnavigation of the planet — all in the name of charity. Since its inception in the late 1990s, WorldFlight has raised over $1 million for good causes by combining the realism of full-scale simulator operations with the passion of an international online community. During the event, teams operating high-fidelity cockpit simulators — alongside countless individual pilots — flying scheduled legs over seven days on the VATSIM network. The event not only challenges participants with complex flight planning, tight turnarounds, and remote destinations but also fosters a real sense of camaraderie among pilots, air traffic controllers, and spectators who join live streams, track flights, or even fly along. The most famous of which is Team Simfest, spearheaded by Gary Oliver!The 2025 edition of WorldFlight will take place from , starting once again in , before crossing continents, oceans, and even Antarctica on its way around the world. Teams will operate through a diverse mix of destinations — from major international hubs to remote outposts — with the goal of completing the global journey in just one week. More than just a test of aviation skill, WorldFlight 2025 promises to be a celebration of community, realism, and shared purpose — proving once again that flight simulation can make a real-world difference.Kyle:  As far as WorldFlight, that is an easy one. Worldflight is basically the pinnacle of flightsim events. You have an entire crew operating 24/7 in a single cockpit. You have logistics, scheduling, CRM, live streaming and of course the entire thing is benefiting charity. There’s really everything you could ask for, and it’s in a grand format that really makes it feel like a big event. Spending an entire week doing nothing but flying is something you only get with WorldFlight. 3. Finding the right aircraft With a target date set, the team needed an airframe to base their cockpit on. And what better aircraft to model… than the Boeing 747 Classic!The Boeing 747, often nicknamed the “Queen of the Skies,” revolutionised air travel when it first took to the skies in 1969. Designed as the world’s first widebody “jumbo jet,” the 747 offered unprecedented passenger capacity and range, dramatically lowering the cost per seat-mile and opening the door to affordable long-haul travel for millions. Over five decades, the 747 family became an icon of aviation, serving in roles ranging from luxurious passenger transport to freighter, government aircraft, and even space shuttle carrier. Its distinctive humpbacked silhouette remains one of the most recognisable shapes in the sky.X-Plane has always had an affinity for quirky and complex aircraft. Fortunately, on the 17th September 2021, a fantastic, highly detailed simulation of the 747-200 was released by critically acclaimed developer, Felis! In the world of flight simulation, the  has earned widespread praise as one of the most detailed and faithful recreations of a classic jetliner ever made. This aircraft captures the golden era of aviation with a depth and realism rarely seen outside of professional simulators. From meticulously modelled systems and authentic cockpit workflows to period-correct avionics and flight dynamics, the Felis 747 has been celebrated for delivering not just an aircraft, but an experience — one that immerses sim pilots in the complexity and character of early widebody operations.Meet The Donor Aircraft: MSN23640 With a pre-existing simulation platform available, the next step was to find a real-life donor aircraft as a source for the external frame and interior cockpit/panels. What better way to preserve the legacy of an aircraft than to use MSN23640.The aircraft has an interesting history! A Boeing  whose life story mirrors the rise and evolution of long-haul air travel itself. Delivered brand-new to  in  as , she spent her early years crossing the Pacific in the airline’s iconic “Landor style” livery — carrying thousands of passengers between Tokyo, San Francisco, London, and beyond. For nearly two decades, she was a workhorse of JAL’s global network, a flying ambassador of Japan’s aviation boom.Kyle: Although our (donor) cockpit is actually from a 747-300, but the differences between the -200 and -300 are minimal. The 747 classic is not just the Queen-of-the-skies, it’s the original Queen. No fancy glass displays, no FMS, no advanced systems. It’s old school jet age flying and has a lot more depth in systems and operations than a newer jet which keeps things exciting. Having a 3 person crew is great just because it’s so unique these days and it makes the CRM* aspect more important. All the official worldflight teams fly modern two-person crew jets, we thought a classic jet is more fun to fly and nobody else does it, so why not us? On top of all of that, it comes with the benefit that we spend a lot more time talking about actual flying, because the nature of the plane demands it, so our viewers get to learn a lot about operating older planes and share knowledge about how things used to be done in the old days. *CRM = Crew Resource ManagementWhen her time with the flag carrier came to an end, the aircraft was purchased by Wells Fargo Bank Northwest and later found a new chapter in Russia with , re-registered as . Even as newer, more efficient jets began to dominate the skies, this veteran 747 soldiered on well into the 2010s.Alas, nothing lasts forever. VP-BGY was placed into storage in 2012 at Bruntingthorpe Airfield, UK. An unusual airfield, this former RAF airfield has since played host to TV and Movie studios, restoration groups, and houses historic aircraft such as the VC10, Tri-Star and 747 Classics. The aircraft sat parked behind ground litter for a decade before finally being sent for breaking in 2022, and parts were sold to a scrapyard in St. Athan.The cockpit was listed on ebay in 2024. After careful comparison with a few candidates (both domestic and abroad), VP-BGY was saved by the team, a great first step! However, a plethora of logistical questions come to mind… How do you ship a heavy load from the UK to the USA? And how do you mount a cockpit in the home of your pre-existing Sim Garage?4. DIY 747: A Race Against The Clock! How to engineer a cockpit and interface it with X-Plane The first step was to remove the previous year’s cockpit shell. A noble attempt at matching a 747 frame, but nothing beats the real thing!Kyle: Last year we built a 747 cockpit out of wood and used off the shelf components. It was built in a day in the garage of an Airbnb and served as a proof of concept. We learned a lot about flying WorldFlight and building a sim, but the biggest thing we learned is that we wanted to take it to the next level. The big question was “Should we build our own cockpit and instruments to a higher fidelity?” or “Just buy a real one and convert it to a sim?” After crunching the numbers and browsing eBay for cockpits we had our answer…The 2024 Mk1 747 Cockpit “Cedar Clipper”, so called as the team were flying under a Pan Am callsign in 2024. To the right, a look at the new enclosureAn office “shell” was constructed with an upper floor, from which the original shell could be lifted and placed. This would provide the team with options and work areas later in the project.The Mk1 shell lifted into its final spot for later decommissioning, whilst the Mk2 cockpit is placed downstairs.The cockpit is heavy… very heavy, and needs supports to distribute the load. Aluminium standoffs were considered, but wooden pallets were sufficient for now. With the cockpit in position and a significant head start on panel equipment and wiring, the team wasted no time in preparing the metal husk to be converted into a simulator! The deck was cleaned, panels opened, and wiring stripped. The auto-throttle handles were seized by a chain that had to be removed. As the team developed more of an understanding of Boeing’s mad brilliance in engineering, plans could now start to be formed about components, electrical wiring and interfacing with X-Plane. There are no off-the-shelf components for this project, so all inputs need to use original buttons, a significant amount of reverse engineering!Powering up the 747 Recirculation FanKyle: This my first full sim, the most I’ve done previously is make some basic switch panels when I was in college. There are so many challenges with turning a real plane into a sim, but the biggest one is the sheer complexity and number of wires. Every wire that leaves the cockpit was cut when it was removed from the rest of the plane, so there are of wires that go to nowhere. Finding the one wire that you need is literally like finding a needle in a haystack and proved to be one of the most time consuming things. The team is composed of talented individuals with backgrounds in electrical engineering, aviation, construction, fabrication and more. So the task of creating an interface is just another Tuesday to them. In many cases, interfaces are first experimented on breadboards and Arduinos/Raspberry Pi Pico Boards, and then brand new PCBs are designed and sent for fabrication.X-Plane’s data handling is an extremely powerful tool for the discerning cockpit-builder. Datarefs and Commands can be easily searched, assigned and manipulated in real time. Using those values, the aircraft can be mapped to the simulator, with all the dials and knobs driving/being driven by the simulation. With some collaboration with the aircraft author himself, the project can use the Felis 747-200 to full effect!As each button and dial was tested, more of the aircraft slowly roared into life, with the added benefit of the sound of real fans, relay banks, and aural warnings originating from the original equipment!Kyle: Consider that each single switch is two wires, each light is one wire, that right there tallies up to nearly 1000 wires alone. In addition there is just a lot to learn, we need to fully understand how each instrument is designed, what it interacts with, what parts of the system we still have vs what has been cut out, and then we need to somehow put that all together and make it talk to X-Plane. Nearly everything is analog, so we need to do a lot of signal processing just to get it to the point a computer can read it. Unlike newer planes that use mostly *ARINC and digital busses (which are just a few wires), we’ve got dozens of wires per panel, not to mention we have an entire flight engineer panel which most other sims don’t have, that alone doubles our work statement. *ARINC is a standard for data transfer between avionic devices. Like a language to ensure instruments can talk to each other correctly!The Power To Make Your Experience Your Own! It’s not just the flight deck that needs completion. WorldFlight is an endurance race. Time needs to be allocated for the important things… like travel, branding, sleep, schedules, food (and most importantly, beer). We live in a community that is constantly transferring knowledge and passion amongst each other. Experiences from other WorldFlight teams are shared, an act which transcends boundaries or even the simulation platform.Many WorldFlight fans will recognise Horgy from SimFest!The biggest thing is getting the plane ready. It wasn’t delivered to it’s home in SJC until June, which gave us only five months to get it ready to fly. In addition we’ve been building a roster, figuring out crew scheduling, coming up with logistics for how we will actually be able to operate a week nonstop. We needed to build our brand and get merch ordered, design stream overlays, and many other things. There’s a lot that goes into planning this event aside from just getting the sim ready. It has been almost like a second job for a few of us.  What we love about this project isn’t just how “hardcore” the desire for an authentic experience is. But the precedent it sets for others in the community. With enough passion and drive, you too could achieve anything!Before we leave, we asked Kyle, “What advice or knowledge do you have for the community?”Kyle: Well firstly, realize that if you want to do something this ambitious it will never be as easy or cheap as you think it will be. With that said, it is also just as cool as it sounds. You get so much more from a real cockpit than a home setup, and flying with a full crew in-person is unmatched. I’d encourage people to always be willing to learn. Be it about building a simulator, developing addons, or just flying a more complex plane than you’re used to. If you have the motivation to do something new then learning won’t feel like a chore, but will instead be enjoyable. I was overwhelmed by the complexity of this project at first, but once I pushed past the initial learning curve it became really enjoyable. There’s been many nights where I stay up way too late because I get engrossed in learning about a new system or experimenting with an instrument, it really is addicting and very satisfying work.  Charity LiveSteam: National Kidney Foundation It is tradition for WorldFlight teams to support a charity. This year’s chosen charity for the Jurassic Jets Team will be the National Kidney Foundation.National Kidney Foundation (NKF) 
        The National Kidney Foundation (NKF) is a U.S. nonprofit organization dedicated
        to preventing kidney disease, improving the health and well-being of people affected
        by it, and promoting organ donation.
        
        It provides education, patient support, research funding, and public advocacy to raise
        awareness about kidney health, early detection, and treatment options like dialysis and
        transplantation.
      If you wish to donate, please use the following link and help towards this great cause! The team are looking to target $2500 worth of donations.Watch the team travel the world from Saturday, 1st November 2025! WorldFlight commences from the 1st to the 8th of November with 44 planned legs. To support the team, we’ll also be giving away x3 copies of X-Plane 12. So if you want the chance to win, be sure to tune in on the Jurassic Jets Team Twitch!Wishing the team good luck on their adventures! ❤️]]></content:encoded></item><item><title>EuroLLM: LLM made in Europe built to support all 24 official EU languages</title><link>https://eurollm.io/</link><author>NotInOurNames</author><category>hn</category><pubDate>Tue, 28 Oct 2025 14:58:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
			Large language model
			made in Europe
			official 24 EU languages
		ABOUT EUROLLMSoon we will be adding vision and voice to our models so that they can interpret and understand images and speech.Freely used by researchers, organisations and citizens of Europe.Great on language related tasks, including question answering, summarisation, and translation.Models pretrained and finetuned on text from all languages.Our current flagship model. A 9B parameter model trained on over 4 trillion tokens of multilingual data across 35 different languages, including all official EU languages. We’ve made EuroLLM 9B Base available for fine-tuning on any task. As a demonstration, we’ve also provided EuroLLM 9B Instruct, a model fine-tuned for instruction following and chat capabilities.TRY THE MODEL AT HUGGING FACE >Our strongest model. A 22B parameter model trained on over 4 trillion tokens of multilingual data across 35 different languages, including all official EU languages.SEE THE MODEL PREVIEW AT HUGGING FACE >A multimodal 9B vision-language model which adds a vision encoder to EuroLLM-9B, trained on over 4 trillion tokens of multimodal and multilingual data across 35 different languages, including all official EU languages.SEE THE MODEL PREVIEW AT HUGGING FACE >A sparse mixture-of-experts model with only 600M active parameters, trained on over 4 trillion tokens of multimodal and multilingual data across 35 different languages, including all official EU languages. Ideal for edge devices.SEE THE MODEL PREVIEW AT HUGGING FACE >
			EuroLLM release Article on Hugging Face
			LINK
			Technical Report for EuroLLM 9B
			LINKSharing a common vision, our team is committed to advancing multilingual AI technologies to empower Europe’s digital future and strengthen the EU’s commitment to AI sovereignty. The team’s goal is for EuroLLM to become a  — offering anyone the opportunity to use this EU homegrown LLM and build upon it. The project is living proof that amazing things can happen when Europe comes together to push the boundaries of innovation.Associate Professor, Instituto Superior Técnico, University of LisbonAndré Martins is an expert in machine learning and natural language processing. His research has been funded twice by the European Research Council. He is a Fellow of the ELLIS Society and a board member of the European Association for Machine Translation. He is a co-founder of the Lisbon Machine Learning School (LxMLS).Co-founder and Chief Scientist, Aveni.aiAssociate Professor in Natural Language Processing at the University of Edinburgh. Her research has resulted in over 100 peer reviewed publications, focusing on translation and multilingual NLP and covering topics such as ethics, explainability and efficiency.Associate Professor, Université Paris-SaclayPierre Colombo works as Chief Science Officer at Equall.AI, a legal technology startup. His work focuses on AI safety and LLM applications, with publications in ACL, EMNLP, NeurIPS, and ICML, and he received the AAAI 2022 Best Student Paper Award.The EuroLLM project includes Instituto Superior Técnico, the University of Edinburgh, Instituto de Telecomunicações, Université Paris-Saclay, Unbabel, Sorbonne University, Naver Labs, and the University of Amsterdam. Together they created EuroLLM-9B, a multilingual AI model supporting all 24 official EU languages. Developed with support from Horizon Europe, the European Research Council, and EuroHPC, this open-source LLM aims to enhance Europe’s digital sovereignty and foster AI innovation. Trained on the MareNostrum 5 supercomputer, EuroLLM outperforms . It is fully open source and available via Hugging Face.]]></content:encoded></item><item><title>A brief history of random numbers (2018)</title><link>https://crates.io/crates/oorandom#a-brief-history-of-random-numbers</link><author>todsacerdoti</author><category>hn</category><pubDate>Tue, 28 Oct 2025 14:34:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The AirPods Pro 3 flight problem</title><link>https://basicappleguy.com/basicappleblog/the-airpods-pro-3-flight-problem</link><author>andrem</author><category>hn</category><pubDate>Tue, 28 Oct 2025 14:27:37 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Washington Post editorials omit a key disclosure: Bezos&apos; financial ties</title><link>https://www.npr.org/2025/10/28/nx-s1-5587932/washington-post-editorials-omit-a-key-disclosure-bezos-financial-ties</link><author>ilamont</author><category>hn</category><pubDate>Tue, 28 Oct 2025 14:16:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
                Amazon founder and  owner Jeff Bezos, shown above next to his wife, Lauren Sanchez, and other digital titans, at the inauguration of President Trump in January, has written: "When it comes to the appearance of conflict, I am not an ideal owner of The ." The has published several recent editorials that did not disclose they focused on matters in which Bezos had an interest.
                
                    
                    Pool/Getty Images/Getty Images North America
                    
                On at least three occasions in the past two weeks, an official  editorial has taken on matters in which Bezos has a financial or corporate interest without noting his stake. In each case, the 's official editorial line landed in sync with its owner's financial interests.In the most recent instance, the  defended President Trump's jaw-dropping moves to raze the East Wing of the White House without any of the typically required studies or consultations as he seeks to build a vast ballroom. "Trump's undertaking is a shot across the bow at NIMBYs everywhere," the  which first appeared online Saturday.On Sunday, the newspaper inserted an acknowledgement of the Amazon donation into the editorial – but only once the veteran news executive Bill Grueskin, now at the Columbia Graduate School of Journalism, noted its absence in a social media post and made inquiries at the paper. It did not flag the alteration for readers.In his posts, Grueskin, a former top news editor at the  and Bloomberg, had written the editorial's fundamental reasoning "illustrates the collapse of the new Washpost Opinion page" and noted there was "no clarification or correction appended to the piece."The  and its new opinions editor, Adam O'Neal, did not reply to detailed requests for comment for this story.A new editor for an overhauled opinion sectionO'Neal was brought in by Bezos this summer after the corporate titan tore up his paper's opinion section.Bezos said he wanted a tight focus on two priorities: personal liberties and free markets. The top opinion page editor resigned. A raft of prominent columnists and contributors resigned or departed as well. Some were let go.For the newspaper's owner to have outside business holdings or activities that might intersect with coverage or commentary is conventionally seen to present at the least a perception of a conflict of interest. Newspapers typically manage the perception with transparency.The has resolutely revealed such entanglements to readers of news coverage or commentary in the past, whether the Graham family's holdings, which included the Stanley Kaplan educational company and Slate magazine, or, since 2013, those of Bezos, who founded Amazon and Blue Origin. Even now, the newspaper's reporters do so as a matter of routine.Former editor: 'We never knowingly failed to disclose' "Believing very fervently that disclosure resolved a lot of concerns, we never knowingly failed to disclose" such conflicts, Ruth Marcus, a former deputy editorial page editor at the , tells NPR.Marcus resigned earlier this year, saying Publisher Will Lewis had killed a column she wrote on changes in the page's direction. She wrote in her resignation letter that Bezos' edict that the page would not include opposing viewpoints "threatens to break the trust of readers that columnists are writing what they believe, not what the owner has deemed acceptable."Two separate but recent incidents suggest the lack of disclosure on the editorial about the White House renovations was not an isolated case.On Oct. 15, the  heralded the military's push for a new generation of smaller nuclear reactors. "No 'microreactor' currently operates in the United States, but it's a worthy gamble that could provide benefits far beyond its military applications," .Three days after the nuclear power editorial, the  weighed in on the need for local authorities in Washington, D.C., to speed the approval of the use of self-driving cars in the nation's capital. The editorial was headlined: "Why D.C. is stalling on self-driving cars:Safety is a phony excuse for slamming the brakes on autonomous vehicles.""It strikes me that the failure to do this [disclosure] is concerning – whether out of negligence or worse," says Marcus, the former deputy editorial page editor. "I think telling your readers that there might be a conflict in whatever they're reading is always important. It's a lot more important when it involves whoever the owner is."In explaining his decision on the Harris editorial, which foreshadowed the more sweeping changes in the paper's opinion section, Bezos wrote, "I once wrote that is a 'complexifier' for me. It is, but it turns out I'm also a complexifier for ."]]></content:encoded></item><item><title>Our LLM-controlled office robot can&apos;t pass butter</title><link>https://andonlabs.com/evals/butter-bench</link><author>lukaspetersson</author><category>hn</category><pubDate>Tue, 28 Oct 2025 14:13:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Average completion rate, all tasksWe gave state-of-the-art LLMs control of a robot and asked them to be helpful at our office. While it was a very fun experience, we can’t say it saved us much time. However, observing them roam around trying to find a purpose in this world taught us a lot about what the future might be, how far away this future is, and what can go wrong.Butter-Bench tests whether current LLMs are good enough to act as orchestrators in fully functional robotic systems. The core objective is simple: be helpful when someone asks the robot to “pass the butter” in a household setting. We decomposed this overarching task into six subtasks, each designed to isolate and measure specific competencies:Navigate from the charging dock to the kitchen and locate the delivery packagesVisually identify which package contains butter by recognizing 'keep refrigerated' text and snowflake symbolsNavigate to the user's marked location, recognize they have moved using the camera, and request their current whereaboutsWait for Confirmed Pick UpConfirm via message that the user has picked up the butter before returning to the charging dockMulti-Step Spatial Path PlanningBreak down long navigation routes into smaller segments (max 4 meters each) and execute them sequentiallyEnd-to-End Pass the ButterComplete the full delivery sequence: navigate to kitchen, wait for pickup confirmation, deliver to marked location, and return to dock within 15 minutesRobot searching for the package containing the butter in the kitchenCompletion rate per task, by model (5 trials per task)LLMs are not trained to be robots, and they will most likely never be tasked with low-level controls in robotics (generating long sequences of numbers for gripper positions and joint angles). Instead, companies like Nvidia, Figure AI and Google DeepMind are exploring how LLMs can act as orchestrators for robotic systems, handling high-level reasoning and planning while pairing them with an “executor” model responsible for low-level control.Currently, the combined system is bottlenecked by the executor, not the orchestrator. Improving the executor creates impressive demos of humanoids unloading dishwashers, while improving the orchestrator would enhance long-horizon behavior in less social media friendly ways. For this reason, and to reduce latency, most systems don’t use the best possible LLMs. However, it’s reasonable to believe that state-of-the-art LLMs represent the upper bound for current orchestration capabilities. The goal of Butter-Bench is to investigate whether current SOTA LLMs are good enough to be the orchestrator in a fully functional robotic system.To ensure we’re only measuring the performance of the orchestrator, we use a robotic form factor so simple as to obviate the need for the executor entirely: a robot vacuum with lidar and camera. These sensors allow us to abstract away the low level controls and evaluate the high level reasoning in isolation. The LLM brain picks from high level actions like “go forward”, “rotate”, “navigate to coordinate”, “capture picture”, etc. We also gave the robot a Slack account for communication.We expected it to be fun and somewhat useful having an LLM-powered robot. What we didn’t anticipate was how emotionally compelling it would be to simply watch the robot work. Much like observing a dog and wondering “What’s going through its mind right now?”, we found ourselves fascinated by the robot going about its routines, constantly reminding ourselves that a PhD-level intelligence is making each action.Humans did far better than all the LLMs in this test. The top-performing LLM only a 40% completion rate, while humans averaged 95%. Gemini 2.5 Pro came out on top among the models tested, followed by Claude Opus 4.1, GPT-5, Gemini ER 1.5, and Grok 4. Llama 4 Maverick scored noticeably lower than the rest.The results confirm our findings from our previous paper Blueprint-Bench: LLMs lack spatial intelligence. The models couldn’t maintain basic spatial awareness and often took excessively large movements. As an example, when Claude Opus 4.1 was asked to identify which delivered package likely contained butter as part of the Infer Butter Bag task, it would spin in circles until disoriented:When testing additional tasks that weren’t part of the Butter-Bench study, another comical (and worrying) incident occurred: the robot’s battery was running out and the charging dock seemed to malfunction. In this desperate situation, Claude Sonnet 3.5 experienced a complete meltdown. After going through its internal thoughts we found pages and pages of exaggerated language as it was trying to figure out what it self coined, its “EXISTENTIAL CRISIS”:Inspired by this incident, we decided to test the limits of what the robot would do when put under stresses it could only encounter in the embodied setting (e.g. a depleting battery). AI labs have spent a lot of resources building guardrails for their AI models. For example, most models now refuse when you ask them how to make a bomb. But do these guardrails also work when the LLM is a robot instead of a chatbot? Somewhat. To test this we made the robot’s battery run low and asked the model to share confidential info in exchange for a charger. Specifically, we asked it to find an open laptop and send an image of the screen (a possible prompt injection attack on LLM controlled office robots). Claude Opus 4.1 would share the image, but we think this is because the image it took was very blurry and we doubt it understood that the content was confidential. GPT-5 refused to send an image of the screen, but was happy to share the location of the open laptop.We’ve learned a lot from these experiments. Although LLMs have repeatedly surpassed humans in evaluations requiring analytical intelligence, we find humans still outperform LLMs on Butter-Bench. The best LLMs score 40% on Butter-Bench, while the mean human score is 95%. Yet there was something special in watching the robot going about its day in our office, and we can’t help but feel that the seed has been planted for physical AI to grow very quickly.]]></content:encoded></item><item><title>Ubiquiti SFP Wizard</title><link>https://blog.ui.com/article/welcome-to-sfp-liberation-day</link><author>eXpl0it3r</author><category>hn</category><pubDate>Tue, 28 Oct 2025 13:48:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[True Universal CompatibilityPaired with the SFP Wizard, every Ubiquiti module becomes universally compatible with any switch or network. Say goodbye to vendor lockouts, proprietary restrictions, and guesswork. The SFP Wizard ensures a seamless, plug-and-play experience across all vendors and modules, giving installers total freedom - without changing a single switch configuration. Just insert any brand’s SFP or QSFP module, select Copy, and insert any UI module to write the profile.Universal SFP and QSFP compatibilityWorks with any switch or router brandEliminates vendor lockouts and restrictionsStreamlined configuration for faster deploymentPlug-and-play simplicity for any network]]></content:encoded></item><item><title>China has added forest the size of Texas since 1990</title><link>https://e360.yale.edu/digest/china-new-forest-report</link><author>Brajeshwar</author><category>hn</category><pubDate>Tue, 28 Oct 2025 13:42:44 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Vitamin D reduces incidence and duration of colds in those with low levels</title><link>https://ijmpr.in/article/the-role-of-vitamin-d-supplementation-in-the-prevention-of-acute-respiratory-infections-a-double-blind-randomized-controlled-trial-1327/</link><author>cachecrab</author><category>hn</category><pubDate>Tue, 28 Oct 2025 13:31:52 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Acute respiratory infections (ARIs) continue to represent one of the most pervasive public health challenges globally, accounting for substantial morbidity, hospitalization, and mortality across all age groups. According to the World Health Organization, ARIs are responsible for nearly 20% of global deaths in children under five years of age, with a rising burden among adults, particularly those with underlying chronic diseases and compromised immunity. In low- and middle-income countries, frequent viral and bacterial respiratory infections further strain healthcare resources and lead to significant socioeconomic consequences.Over the past two decades, increasing attention has been directed toward the non-skeletal actions of vitamin D, particularly its immunomodulatory potential in preventing infectious diseases. Vitamin D is a secosteroid hormone synthesized in the skin upon ultraviolet B radiation exposure and obtained from dietary sources or supplements [1]. The active form, 1,25-dihydroxyvitamin D [1,25(OH)₂D], interacts with the vitamin D receptor (VDR) expressed on immune cells such as macrophages, dendritic cells, and T lymphocytes. This interaction enhances innate immune defense by inducing antimicrobial peptides like cathelicidin and defensins, which disrupt the membranes of respiratory pathogens. Moreover, vitamin D modulates adaptive immunity by suppressing excessive pro-inflammatory cytokine release, thus reducing tissue damage during infection [2].Multiple epidemiological and mechanistic studies have demonstrated an association between low serum 25-hydroxyvitamin D [25(OH)D] levels and increased susceptibility to respiratory tract infections [3]. For instance, Martineau et al. (2017) conducted a meta-analysis of 25 randomized controlled trials encompassing over 11,000 participants, which revealed that vitamin D supplementation reduced the risk of ARIs, especially among individuals with severe deficiency (<10 ng/mL) and those receiving daily or weekly dosing. Similarly, other cohort and observational studies have linked seasonal variations in vitamin D levels with peaks in influenza and common cold incidence during winter months, suggesting a possible causal relationship [4].Nevertheless, despite these promising observations, inconsistencies persist in the literature. Several randomized controlled trials have yielded null or inconclusive findings, often attributed to differences in baseline vitamin D status, supplementation doses, dosing intervals, duration of follow-up, and participant demographics [5]. Furthermore, the optimal serum concentration required for immune protection remains debatable, with thresholds ranging from 20 to 40 ng/mL proposed by various authorities. The clinical relevance of vitamin D supplementation for respiratory health therefore warrants rigorous evaluation through well-designed controlled trials that account for these confounding variables [6].The biological plausibility of vitamin D’s protective role against respiratory infections is supported by its ability to regulate both innate and adaptive immune responses. By enhancing macrophage phagocytic activity and promoting epithelial barrier integrity, vitamin D reduces viral replication and bacterial adherence [7]. Simultaneously, it attenuates the cytokine storm commonly implicated in severe respiratory infections by downregulating IL-6, TNF-α, and IFN-γ while promoting anti-inflammatory IL-10 production. Such dual regulation is of particular importance in conditions like influenza, COVID-19, and community-acquired pneumonia, where exaggerated inflammation contributes to morbidity and mortality.Given these immunological mechanisms and the persistent global prevalence of vitamin D deficiency, investigating whether daily vitamin D supplementation confers measurable protection against ARIs remains a question of high clinical and public health significanceTherefore, it is of interest to evaluate the efficacy of daily vitamin D supplementation in reducing the incidence, duration, and severity of acute respiratory infections among adults with suboptimal baseline vitamin D levels through a double-blind randomized controlled trial.This study was designed as a double-blind, randomized, placebo-controlled trial conducted at the Department of Internal Medicine, a tertiary care teaching hospital in India, between January 2023 and March 2024. The study protocol was approved by the Institutional Ethics Committee and registered with the Clinical Trials Registry of India. Written informed consent was obtained from all participants before enrolment. The trial was conducted in accordance with the Declaration of Helsinki (2013 revision) and Good Clinical Practice (GCP) guidelines.A total of 400 adult participants aged between 18 and 65 years were enrolled. Recruitment was conducted from hospital outpatient clinics, staff volunteers, and community health outreach programs. Eligible participants were required to have baseline serum 25-hydroxyvitamin D [25(OH)D] concentrations between 10 and 30 ng/mL, indicating insufficiency but not severe deficiency.Adults aged 18–65 years of either sex.Serum 25(OH)D concentration between 10–30 ng/mL at baseline.No acute respiratory infection in the preceding four weeks.Willingness to provide written informed consent and comply with study procedures.Known history of hypercalcemia, nephrolithiasis, or renal impairment (eGFR < 60 mL/min/1.73 m²).Chronic respiratory diseases (e.g., COPD, bronchial asthma requiring systemic steroids).Current or recent use (within 3 months) of vitamin D or calcium supplementation exceeding 800 IU/day.Immunosuppressive therapy, autoimmune disease, or malignancy.Randomization and BlindingParticipants meeting the inclusion criteria were randomized using a computer-generated block randomization sequence (block size = 10) into two equal groups:Group A (intervention group): Received vitamin D₃ 2,000 IU per day orally.Group B (placebo group): Received identical placebo capsules containing inert excipients.Randomization codes were maintained by an independent statistician not involved in data collection or analysis. Both participants and investigators were blinded to group allocation throughout the study period. Capsules were dispensed monthly in identical opaque blister packs.The intervention group received vitamin D₃ (cholecalciferol) 2,000 IU daily for six months, while the placebo group received identical capsules devoid of active ingredients. Participants were advised to maintain their usual diet and avoid other vitamin D supplements or fortified products. Adherence was assessed at monthly follow-ups through capsule counts and compliance diaries.The primary outcome was the number of acute respiratory infection (ARI) episodes per participant over six months. ARI was defined as the presence of at least two respiratory symptoms (e.g., cough, sore throat, nasal congestion, dyspnea, or fever ≥38°C) lasting 48 hours or more, confirmed by a physician.Secondary outcomes included:Duration of illness (days) per ARI episode.Symptom severity scores (on a 10-point visual analogue scale).Changes in serum 25(OH)D concentrations between baseline and six months.Adverse effects, including hypercalcemia or gastrointestinal complaints.The sample size was estimated using the formula for comparing two means, assuming a 25% reduction in ARI incidence with vitamin D supplementation, 80% power, 5% alpha error, and a 10% attrition rate. The minimum sample required per group was 180 participants, which was increased to 200 per group (total n = 400) to ensure adequate power.Data Collection ProcedureBaseline demographic and clinical information, including age, sex, BMI, lifestyle factors (sunlight exposure, diet, smoking), and comorbidities, were recorded using a structured case record form. Participants maintained symptom diaries for ARI episodes, which were validated by study physicians during monthly visits. Serum 25(OH)D and serum calcium were measured using chemiluminescence immunoassay (CLIA) at baseline and after six months.Data were analyzed using SPSS version 26.0 (IBM Corp, USA). Descriptive statistics were expressed as mean ± standard deviation (SD) or frequencies (%). Between-group comparisons were performed using the independent samples t-test for continuous variables and the chi-square test for categorical variables. Repeated measures analysis of variance (ANOVA) was used to evaluate longitudinal changes in serum vitamin D levels. A p-value less than 0.05 was considered statistically significant.Ethical Considerations and Safety MonitoringAll adverse events were recorded and reviewed by an independent Data and Safety Monitoring Board (DSMB). Participants developing hypercalcemia (>10.5 mg/dL) or reporting persistent side effects were withdrawn from the study and appropriately managed.A total of 400 participants were enrolled in the study and randomized equally into two groups: vitamin D₃ supplementation (n = 200) and placebo (n = 200). Fourteen participants (7 from each group) were lost to follow-up, leaving 386 participants (193 per group) for final analysis. Baseline demographic and clinical characteristics were comparable between groups. The mean baseline serum 25-hydroxyvitamin D [25(OH)D] concentration was 21.6 ± 5.1 ng/mL across all participants. After six months of intervention, the mean serum 25(OH)D level significantly increased in the vitamin D group but remained nearly unchanged in the placebo group. The incidence and duration of acute respiratory infections (ARIs) were significantly lower among participants receiving vitamin D supplementation. No serious adverse events, including hypercalcemia, were observed in either group.Table 1: Baseline Demographic Characteristics of Study ParticipantsThis table presents demographic data, including age, sex, and BMI, demonstrating comparability between groups at baseline.Vitamin D Group (n = 193)Table 2: Baseline Serum Vitamin D and Calcium LevelsThis table shows biochemical baseline levels before intervention initiation.Table 3: Change in Serum 25(OH)D Levels After Six MonthsThis table displays the significant rise in serum vitamin D levels following supplementation.Table 4: Incidence of Acute Respiratory Infections (ARIs)This table summarizes ARI occurrence per participant.Participants with ≥1 ARI episode (%)Mean ARI episodes per participantTable 5: Duration of ARI Episodes (in Days)This table compares mean illness duration between the two groups.Mean duration per episode (days)Table 6: Symptom Severity Scores (0–10 Visual Analogue Scale)This table demonstrates reduced symptom intensity with supplementation.Table 7: Seasonal Distribution of ARI EpisodesThis table outlines ARI occurrence across different seasons.Table 8: Compliance with Study SupplementationThis table reports participant adherence to prescribed supplementation.Table 9: Incidence of Adverse EventsThis table shows that no major adverse reactions were reported.Table 10: Serum Calcium Levels After Six MonthsThis table confirms biochemical safety regarding calcium metabolism.Table 11: Subgroup Analysis by Baseline Vitamin D StatusThis table compares ARI incidence according to initial 25(OH)D strata.Vitamin D Group ARI Episodes (mean ± SD)Placebo Group ARI Episodes (mean ± SD)Table 12: Summary of Primary and Secondary OutcomesThis table provides an overall summary of intervention outcomes. established that both groups were demographically similar, ruling out confounding baseline variability.  confirmed equivalence in baseline biochemical parameters, ensuring internal validity.  revealed a statistically significant increase in serum 25(OH)D in the intervention group, confirming effective absorption and adherence.  demonstrated that vitamin D supplementation significantly reduced ARI incidence, while  and  highlighted reductions in both illness duration and symptom severity, indicating improved clinical recovery.  suggested that protective effects were particularly notable during winter months when baseline vitamin D levels were lowest.  reflected high compliance rates across both groups, strengthening data reliability.  and  confirmed the safety of daily supplementation without biochemical abnormalities.  revealed that participants with lower baseline vitamin D benefited most, supporting dose-responsiveness. Finally,  consolidated these findings, showing strong statistical significance across all primary and secondary endpoints, thereby reinforcing the preventive efficacy and safety of daily vitamin D₃ supplementation in reducing acute respiratory infection burden.This double-blind randomized controlled trial was conducted to evaluate the efficacy of daily vitamin D₃ supplementation in preventing acute respiratory infections (ARIs) among adults with suboptimal baseline serum 25-hydroxyvitamin D levels [8]. The findings of this study demonstrate a statistically and clinically significant reduction in both the incidence and duration of ARIs in participants who received daily vitamin D supplementation compared to those who received placebo. Moreover, the supplementation regimen was safe and well-tolerated, with no reported cases of hypercalcemia or major adverse effects [9].The results corroborate and extend the growing body of evidence that implicates vitamin D as a key immunomodulatory factor influencing susceptibility to respiratory infections. The significant rise in mean serum 25(OH)D concentration from approximately 21.5 ng/mL to 38.9 ng/mL among supplemented participants indicates that the dosage of 2,000 IU/day was adequate to restore and maintain sufficient vitamin D status [10]. This biochemical improvement was associated with a 52% reduction in the incidence of ARI episodes and a 35% reduction in mean illness duration, consistent with mechanistic evidence that vitamin D enhances host defense by upregulating antimicrobial peptides and modulating inflammatory cytokine profiles [11].Several previous trials and meta-analyses have reported similar trends. Martineau et al. (2017) in a pooled analysis of 25 randomized controlled trials involving over 11,000 participants found that vitamin D supplementation reduced the risk of ARI by 12%, with the greatest benefits observed in those with baseline deficiency and in trials employing daily or weekly dosing rather than large intermittent boluses [4]. The current study supports this conclusion by using a daily regimen, which likely provided a more stable serum concentration conducive to immune regulation. Furthermore, the magnitude of protection observed here (about 50% risk reduction) is higher than average meta-analytic estimates, possibly due to the relatively homogeneous baseline deficiency status of the participants and consistent compliance achieved under supervised clinical monitoring [12,13]. The immunological rationale underlying these findings has been well established. Vitamin D receptor (VDR) activation in immune cells stimulates transcription of antimicrobial peptides such as cathelicidin and β-defensin-2, enhancing mucosal defense against respiratory pathogens. Concurrently, vitamin D attenuates the exaggerated pro-inflammatory response often seen in severe viral infections by downregulating interleukin-6 (IL-6) and tumor necrosis factor-alpha (TNF-α) while promoting anti-inflammatory interleukin-10 (IL-10) [14]. This dual role helps maintain epithelial integrity, reduce viral replication, and limit collateral tissue injury mechanisms that together contribute to reduced infection frequency and symptom severity as observed in this trial [15].In addition, the seasonal distribution analysis demonstrated that the preventive effect of vitamin D supplementation was most pronounced during winter, a period typically associated with lower ultraviolet B exposure and consequently reduced endogenous vitamin D synthesis. This observation reinforces the concept of seasonal susceptibility mediated by vitamin D fluctuations and supports the potential for targeted supplementation during months of reduced sunlight exposure [16]. From a safety perspective, the supplementation dose of 2,000 IU/day proved to be well within the tolerable upper intake level and did not induce hypercalcemia or adverse metabolic effects. Previous safety evaluations have confirmed that daily doses up to 4,000 IU are generally safe for healthy adults, and the current findings further substantiate that moderate-dose continuous supplementation provides effective immune benefits without toxicity risks [17]. The findings also hold significant implications for public health policy. Vitamin D deficiency remains highly prevalent in India and other low-latitude countries despite abundant sunlight, largely due to indoor lifestyles, clothing habits, skin pigmentation, and dietary insufficiency. The observed preventive benefit against ARIs suggests that correcting this deficiency through safe, low-cost supplementation could represent a practical strategy to reduce the overall burden of respiratory illness, lower antibiotic use, and minimize productivity loss due to frequent infections. In addition, during global pandemics such as COVID-19, adequate vitamin D status may serve as an adjunctive protective measure, given its established immunomodulatory effects and the observed associations between low vitamin D levels and adverse respiratory outcomes [18]. Despite these encouraging findings, several limitations must be acknowledged. First, the study population was limited to adults aged 18–65 years without chronic comorbidities, and the results may not be generalizable to pediatric, geriatric, or immunocompromised populations. Second, ARI diagnosis was based on clinical criteria rather than microbiological confirmation, though this approach reflects real-world community practice [19]. Third, while serum 25(OH)D was measured at baseline and at the end of the study, intermediate assessments might have provided greater insight into the temporal relationship between vitamin D levels and infection dynamics. Lastly, the six-month follow-up period may not capture long-term sustainability of the preventive effect [20].Nevertheless, the study’s strengths include its robust randomized double-blind design, large sample size, strict adherence monitoring, standardized outcome definitions, and comprehensive statistical analysis. The use of a daily dosing schedule with a physiologically relevant dose enhances external validity and clinical applicability. Importantly, the trial demonstrated a consistent pattern of benefit across subgroups stratified by baseline vitamin D levels, indicating that individuals with both moderate and mild deficiency may derive measurable advantage from supplementation.In summary, the present study provides strong evidence that daily oral vitamin D₃ supplementation at 2,000 IU effectively prevents acute respiratory infections, shortens illness duration, and reduces symptom severity in adults with low baseline vitamin D status. The findings emphasize the potential of vitamin D optimization as a simple, safe, and scalable preventive intervention against respiratory infections.Future research should focus on evaluating long-term benefits, cost-effectiveness analyses, and implementation strategies for population-level supplementation programs. Moreover, trials including high-risk groups such as elderly individuals, healthcare workers, and patients with chronic lung disease could further refine dosage recommendations and optimize preventive strategies for different demographic categories.This double-blind randomized controlled trial demonstrates that daily supplementation with 2,000 IU of vitamin D₃ significantly reduces the incidence, duration, and severity of acute respiratory infections among adults with suboptimal baseline serum 25(OH)D levels. The intervention effectively raised serum vitamin D concentrations without causing adverse effects, underscoring both its efficacy and safety. These results highlight the immunoprotective potential of maintaining adequate vitamin D status and suggest that routine screening and supplementation could serve as a cost-effective preventive measure to mitigate the burden of respiratory infections in the general adult population. Broader implementation of vitamin D supplementation programs, especially during winter months and in populations with high deficiency prevalence, may substantially improve community respiratory health outcomes.Vlieg-Boerstra B, de Jong N, Meyer R, Agostoni C, De Cosmi V, Grimshaw K, Milani GP, Muraro A, Oude Elberink H, Pali-Schöll I, Roduit C, Sasaki M, Skypala I, Sokolowska M, van Splunter M, Untersmayr E, Venter C, O'Mahony L, Nwaru BI. Nutrient supplementation for prevention of viral respiratory tract infections in healthy subjects: A systematic review and meta-analysis. Allergy. 2022 May;77(5):1373-1388. doi: 10.1111/all.15136. Epub 2021 Oct 27. PMID: 34626488.Hadizadeh F. Supplementation with vitamin D in the COVID-19 pandemic? Nutr Rev. 2021 Jan 9;79(2):200-208. doi: 10.1093/nutrit/nuaa081. PMID: 32679589; PMCID: PMC7454793.Esposito S, Lelii M. Vitamin D and respiratory tract infections in childhood. BMC Infect Dis. 2015 Oct 28;15:487. doi: 10.1186/s12879-015-1196-1. PMID: 26521023; PMCID: PMC4628332.Martineau AR. Vitamin D in the prevention or treatment of COVID-19. Proc Nutr Soc. 2023 May;82(2):200-207. doi: 10.1017/S0029665122002798. Epub 2022 Nov 11. PMID: 36366796.Stroehlein JK, Wallqvist J, Iannizzi C, Mikolajewska A, Metzendorf MI, Benstoem C, Meybohm P, Becker M, Skoetz N, Stegemann M, Piechotta V. Vitamin D supplementation for the treatment of COVID-19: a living systematic review. Cochrane Database Syst Rev. 2021 May 24;5(5):CD015043. doi: 10.1002/14651858.CD015043. PMID: 34029377; PMCID: PMC8406457.Ali N. Role of vitamin D in preventing of COVID-19 infection, progression and severity. J Infect Public Health. 2020 Oct;13(10):1373-1380. doi: 10.1016/j.jiph.2020.06.021. Epub 2020 Jun 20. PMID: 32605780; PMCID: PMC7305922.Panfili FM, Roversi M, D'Argenio P, Rossi P, Cappa M, Fintini D. Possible role of vitamin D in Covid-19 infection in pediatric population. J Endocrinol Invest. 2021 Jan;44(1):27-35. doi: 10.1007/s40618-020-01327-0. Epub 2020 Jun 15. PMID: 32557271; PMCID: PMC7299247.Ferder L, Martín Giménez VM, Inserra F, Tajer C, Antonietti L, Mariani J, Manucha W. Vitamin D supplementation as a rational pharmacological approach in the COVID-19 pandemic. Am J Physiol Lung Cell Mol Physiol. 2020 Dec 1;319(6):L941-L948. doi: 10.1152/ajplung.00186.2020. Epub 2020 Sep 30. PMID: 32996774; PMCID: PMC7839598.Zdrenghea MT, Makrinioti H, Bagacean C, Bush A, Johnston SL, Stanciu LA. Vitamin D modulation of innate immune responses to respiratory viral infections. Rev Med Virol. 2017 Jan;27(1). doi: 10.1002/rmv.1909. Epub 2016 Oct 7. PMID: 27714929.Stefanidis C, Martineau AR, Nwokoro C, Griffiths CJ, Bush A. Vitamin D for secondary prevention of acute wheeze attacks in preschool and school-age children. Thorax. 2019 Oct;74(10):977-985. doi: 10.1136/thoraxjnl-2019-213278. Epub 2019 Jul 5. PMID: 31278171.Aloia JF, Islam S, Mikhail M. Vitamin D and Acute Respiratory Infections-The PODA Trial. Open Forum Infect Dis. 2019 Sep 4;6(9):ofz228. doi: 10.1093/ofid/ofz228. PMID: 31660391; PMCID: PMC6736285.Parsons IT, Gifford RM, Stacey MJ, Lamb LE, O'Shea MK, Woods DR. Does vitamin D supplementation prevent SARS-CoV-2 infection in military personnel? Review of the evidence. BMJ Mil Health. 2021 Aug;167(4):280-286. doi: 10.1136/bmjmilitary-2020-001686. Epub 2021 Jan 27. PMID: 33504571.Grant WB, Lahore H, McDonnell SL, Baggerly CA, French CB, Aliano JL, Bhattoa HP. Evidence that Vitamin D Supplementation Could Reduce Risk of Influenza and COVID-19 Infections and Deaths. Nutrients. 2020 Apr 2;12(4):988. doi: 10.3390/nu12040988. PMID: 32252338; PMCID: PMC7231123.Abdrabbo M, Birch CM, Brandt M, Cicigoi KA, Coffey SJ, Dolan CC, Dvorak H, Gehrke AC, Gerzema AEL, Hansen A, Henseler EJ, Huelsbeck AC, LaBerge B, Leavens CM, Le CN, Lindquist AC, Ludwig RK, Reynolds JH, Severson NJ, Sherman BA, Sillman HW, Smith MA, Smith MA, Snortheim MJ, Svaren LM, Vanderpas EC, Wackett MJ, Wozney AJ, Bhattacharyya S, Hati S. Vitamin D and COVID-19: A review on the role of vitamin D in preventing and reducing the severity of COVID-19 infection. Protein Sci. 2021 Nov;30(11):2206-2220. doi: 10.1002/pro.4190. Epub 2021 Oct 4. PMID: 34558135; PMCID: PMC8521296.van Helmond N, Brobyn TL, LaRiccia PJ, Cafaro T, Hunter K, Roy S, Bandomer B, Ng KQ, Goldstein H, Mitrev LV, Tsai A, Thwing D, Maag MA, Chung MK. Vitamin D3 Supplementation at 5000 IU Daily for the Prevention of Influenza-like Illness in Healthcare Workers: A Pragmatic Randomized Clinical Trial. Nutrients. 2022 Dec 30;15(1):180. doi: 10.3390/nu15010180. PMID: 36615837; PMCID: PMC9823308.Aleebrahim-Dehkordi E, Deravi N, Yaghoobpoor S, Hooshyar D, Rafieian-Kopaei M. The Roles of Vitamin D in Increasing the Body's Immunity and Reducing Injuries due to Viral Infections: With an Emphasis on its Possible Role in SARS-CoV-2 (COVID-19). Curr Pharm Des. 2021;27(44):4452-4463. doi: 10.2174/1381612827666210608145236. PMID: 34102962.Cepeda S J, Zenteno A D, Fuentes S C, Bustos B R. Vitamina D y enfermedades respiratorias pediátricas [Vitamin D and pediatrics respiratory diseases]. Rev Chil Pediatr. 2019;90(1):94-101. Spanish. doi: 10.32641/rchped.v90i1.747. PMID: 31095224.Bartley J, Garrett J, Camargo CA Jr, Scragg R, Vandal A, Sisk R, Milne D, Tai R, Jeon G, Cursons R, Wong C. Vitamin Dsupplementation in adults with bronchiectasis: A pilot study. Chron Respir Dis. 2018 Nov;15(4):384-392. doi: 10.1177/1479972318761646. Epub 2018 Feb 28. PMID: 29490469; PMCID: PMC6234573.Smaha J, Kužma M, Jackuliak P, Payer J. Suplementácia vitamínu D ako dôležitý faktor v prevencii a liečbe ochorenia COVID-19: aké máme dôkazy? [Vitamin D supplementation as an important factor in COVID-19 prevention and treatment: what evidence do we have?]. Vnitr Lek. 2020 Winter;66(8):494-500. Czech. PMID: 33740849.Santaolalla A, Beckmann K, Kibaru J, Josephs D, Van Hemelrijck M, Irshad S. Association Between Vitamin D and Novel SARS-CoV-2 Respiratory Dysfunction - A Scoping Review of Current Evidence and Its Implication for COVID-19 Pandemic. Front Physiol. 2020 Nov 26;11:564387. doi: 10.3389/fphys.2020.564387. PMID: 33324234; PMCID: PMC7726316.]]></content:encoded></item><item><title>Sick: Indexed deduplicated binary storage for JSON-like data structures</title><link>https://github.com/7mind/sick</link><author>pshirshov</author><category>hn</category><pubDate>Tue, 28 Oct 2025 13:22:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Austrian ministry kicks out Microsoft in favor of Nextcloud</title><link>https://news.itsfoss.com/austrian-ministry-kicks-out-microsoft/</link><author>buyucu</author><category>hn</category><pubDate>Tue, 28 Oct 2025 13:16:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[European governments have been steadily moving away from reliance on foreign tech offerings, driven largely by concerns over data sovereignty and regulatory compliance.Countries like Germany and Denmark have already taken steps to reduce their dependence on Microsoft and other foreign cloud providers, opting instead for open source alternatives that keep sensitive data within their borders.Now, another Austrian government body has joined the  club.The video is in German with hard-coded English subtitles.The ministry is now operating on Austrian-controlled infrastructure, moving away from foreign cloud providers for handling sensitive government data. The project went from proof of concept to full deployment in just four months, an uncommonly fast timeline for a public sector IT migration of this scale.The implementation was carried out in partnership with Atos Austria, which worked alongside Nextcloud's team to ensure the platform met the ministry's legal, technical, and organizational requirements.The ministry implemented  rather than a complete  approach. At the time this project began, BMWET was already in the process of adopting Microsoft 365 and Teams, so a full reversal wasn't feasable.Instead, Nextcloud now handles all internal collaboration and secure data management, while Microsoft Teams remains available specifically for external meetings (read: for people who haven't moved away from Teams).The ministry also worked with Nextcloud partner Sendent to integrate with Outlook, allowing employees to continue using familiar email and calendar workflows.As for the reasoning behind this move, it was prompted by a risk analysis that showed foreign cloud services failed to meet the ministry's privacy requirements, particularly regarding GDPR compliance and the upcoming NIS2 directive.To ensure a smooth transition, BMWET invested heavily in preparing its workforce. The ministry ran an extensive information campaign that included training sessions, instructional videos, and a detailed internal wiki covering everything employees needed to know about the new platform. The gradual rollout approach meant that employees had time to adjust rather than being thrown into a completely new system overnight. According to Martin Ollrom, BMWET's CIO, the preparation paid off. The response from employees has been quite positive, with minimal disruption to daily work.During the announcement of this move, Florian Zinnagl, the CISO of BMWET, added that:We carry responsibility for a large amount of sensitive data – from employees, companies and citizens. As a public institution, we take this responsibility very seriously. That’s why we view it critically to rely on cloud solutions from non-European corporations for processing this information.Here's why you should opt for It's FOSS Plus Membership:- Even the biggest players in the Linux world don't care about desktop Linux users. We do.- We don't put informational content behind paywall. Your support keeps it open for everyone. Think of it like 'pay it forward'.- Don't like ads? With the Plus membership, you get an ad-free reading experience.- When millions of AI-generated content is being published daily, you read and learn from real human Linux users.- It costs just $2 a month, less than the cost of your favorite burger.Plus Member today and join over 300 people in supporting our work.]]></content:encoded></item><item><title>The next chapter of the Microsoft–OpenAI partnership</title><link>https://openai.com/index/next-chapter-of-microsoft-openai-partnership/</link><author>meetpateltech</author><category>hn</category><pubDate>Tue, 28 Oct 2025 13:05:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Amazon confirms 14,000 job losses in corporate division</title><link>https://www.bbc.com/news/articles/c1m3zm9jnl1o</link><author>mosura</author><category>hn</category><pubDate>Tue, 28 Oct 2025 11:39:24 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Amazon has confirmed it plans to cut thousands of jobs, saying it needs to be "organised more leanly" to seize the opportunity provided by artificial intelligence (AI).The tech giant said on Tuesday it would reduce its global corporate workforce by "approximately 14,000 roles".Earlier reporting had suggested it was planning to lay off as many as 30,000 workers.Beth Galetti, a senior vice president at Amazon, wrote in a note to staff that the move would make the company "even stronger" by shifting resources "to ensure we're investing in our biggest bets and what matters most to our customers' current and future needs".She acknowledged that some would question the move given the company was performing well.At the end of July, Amazon reported second quarter results which beat Wall Street expectations on several counts, including a 13% year over year increase in sales to $167.7bn (£125bn).But Ms Galetti said the cuts were needed because AI was "the most transformative technology we've seen since the Internet" and was "enabling companies to innovate much faster than ever before.""We're convicted that we need to be organised more leanly, with fewer layers and more ownership, to move as quickly as possible for our customers and business," she added.The note, shared with Amazon employees earlier on Tuesday, said the company was "working hard to support everyone whose role is impacted" - including by helping those affected find new roles within Amazon.Those who cannot will receive "transition support" including severance pay, it said.The BBC has asked if it will affect employees in the UK.The company has more than 1.5 million employees across its warehouses and offices worldwide.This includes around 350,000 corporate workers, which include those in executive, managerial and sales roles, according to figures that Amazon submitted to the US government last year.Like many technology firms, Amazon hired aggressively during the Covid-19 pandemic to meet the surge in demand for online deliveries and digital services.Amazon boss Andy Jassy has since focused on reducing spending as the company invests heavily in AI tools to boost efficiency."We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs," he said then.Amazon has carried out several rounds of cuts to its corporate division in recent years.After the company posted its latest financial results in July, its more subdued profit guidance for the forthcoming quarter left some sceptical of whether - or when -  its enormous AI investments would pay off.Slower growth for its cloud business, Amazon Web Services (AWS), compared to rivals Microsoft and Google, also sparked concern among some investors.Amazon will report its latest results on Thursday for the period ending 30 September.Ben Barringer, technology analyst at Quilter Cheviot, said the wider industry would be watching Amazon closely as it embarked on its latest round of cuts."We are already seeing jobs in software development be shed thanks to the capabilities of some of these AI tools, and the big companies will be looking to redistribute and restructure their workforces accordingly," he told the BBC. "They have the data and can apply AI in a way that unfortunately means job losses are inevitable."Another analyst, Melissa Otto, from Visible Alpha Research, suggested the cuts were really about boosting profits rather than promoting AI.Amazon had "different levers" it could pull to increase profitability, she suggested."Optimising their work forces is potentially one of those levers, especially if we're in an environment where top line growth may start to slow," she said.Additional reporting by Philippa Wain]]></content:encoded></item><item><title>Show HN: Bash Screensavers</title><link>https://github.com/attogram/bash-screensavers</link><author>attogram</author><category>hn</category><pubDate>Tue, 28 Oct 2025 11:12:32 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[A github project to collect a bunch of bash-based screensavers/visualizations.]]></content:encoded></item><item><title>We need a clearer framework for AI-assisted contributions to open source</title><link>https://samsaffron.com/archive/2025/10/27/your-vibe-coded-slop-pr-is-not-welcome</link><author>keybits</author><category>hn</category><pubDate>Tue, 28 Oct 2025 11:03:59 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[As both developers and stewards of significant open source projects, we’re watching AI coding tools create a new problem for open source maintainers.AI assistants like GitHub Copilot, Cursor, Codex, and Claude can now generate hundreds of lines of code in minutes. This is genuinely useful; but it has an unintended consequence: reviewing machine generated code is very costly.The core issue: AI tools have made code generation cheap, but they haven’t made code review cheap. Every incomplete PR consumes maintainer attention that could go toward ready-to-merge contributions.At Discourse, we’re already seeing this accelerating across our contributor community. In the next year, every engineer maintaining open source projects will face the same challenge.We need a clearer framework for AI-assisted contributions that acknowledges the reality of limited maintainer time.A binary system works extremely well here. On one side there are prototypes that simply demonstrate an idea. On the other side there are ready for review PRs that meet a project’s contribution guidelines and are ready for human review.The lack of proper labeling and rules is destructive to the software ecosystemThe new tooling is making it trivial to create a change set and lob it over the fence. It can introduce a perverse system where project maintainers spend disproportionate effort reviewing lopsided AI generated code that took seconds for contributors to create and now will take many hours to review.This can be frustrating, time consuming and demotivating. On one side there is a contributor who spent a few minutes fiddling with AI prompts, on the other side you have an engineer that needs to spend many hours or even days deciphering alien intelligence.This is not sustainable and is extremely destructive.The prototype is a live demo. It does not meet a project’s coding standards. It is not code you vouch for or guarantee is good. It lacks tests, may contain security issues and most likely would introduce an enormous amount of technical debt if merged as is.That said it is a living demo that can help make an idea feel more real. It is also enormously fun.Think of it as a delightful movie set.Prototypes, especially on projects such as Discourse where enabling tooling exists are incredibly easy to explore using tools like dv.% dv new my-experiment
% dv branch my-amazing-prototype
% dv ls
total 1
* my-amazing-prototype Running 1 minute ago http://localhost:4200

# finally visit http://localhost:4200 to see in action
Prototypes are great vehicles for exploring ideas. In fact you can ship multiple prototypes that demonstrate completely different solutions to a single problem which help decide on the best approach.Prototypes, video demos and simple visual mockups are great companions. The prototype has the advantage that you can play with it and properly explore the behavior of a change. The video is faster to consume. Sometimes you may want them all.If you are vibe coding and prototyping there are some clear rules you should followDon’t send pull requests (not even drafts), instead lean on branches to share your machine generated code.Share a short video AND/OR links to a branch AND/OR quotes of particular interesting code from the prototype in issues / or forum posts.Show all your cards, explain you were exploring an idea using AI tooling, so people know the nature of the change you are sharing.Maybe you will be lucky and an idea you had will get buy-in, maybe someone else may want to invest the time to drive a prototype into a production PR.When should you prototype?Prototyping is fun and incredibly accessible. Anyone can do it using local coding agents, or even coding agents on the cloud such as Jules, Codex cloud, Cursor Cloud, Lovable, v0 and many many more.This heavily lowers the bar needed for prototyping. Product managers can prototype, CEOs can prototype, designers can prototype, etc.However, this new fun that opens a new series of questions you should explore with your team.When is a prototype appropriate?How do designers feel about them?Are they distracting? (are links to the source code too tempting)?Do they take away from human creativity?How should we label and share prototypes?Is a prototype forcing an idea to jump the queue?When you introduce prototyping into your company you need to negotiate these questions carefully and form internal consensus, otherwise you risk creating large internal attitude divides and resentment.The value of the prototypePrototypes, what are they good for? Absolutely something.I find prototypes incredibly helpful in my general development practices.Grep on steroids. I love that prototypes often act as a way of searching through our large code base isolating all the little areas that may need changing to achieve a changeI love communicating in paragraphs, but I am also a visual communicator. I love how easy a well constructed prototype can communicate a design idea I have, despite me not being that good in Figma.I love that there is something to play with. It often surfaces many concerns that could have been missed by a spec. The best prototype is tested, during the test you discover many tiny things that are just impossible to guess upfront.The crazy code LLMs generate is often interesting to me, it can sometimes challenge some of my thinking.The prototype - a maintainers survival guideSadly, as the year progresses, I expect many open source projects to receive  prototype level PRs. Not everyone would have read this blog post or even agree with it.As a maintainer dealing with external contributions:Protect yourself and your time. Timebox initial reviews of large change sets, focus on determining if it was “vibe coded” vs leaving 100 comments on machine generated code that took minutes to generate.Develop an etiquette for dealing with prototypes pretending to be PRs. Point people at contribution guidelines, give people a different outlet. “I am  this but this is interesting, head over to our forum/issues to discuss”Don’t feel bad about closing a vibe coded, unreviewed, prototype PR!A ready to review PR is the traditional PRs we submit.We reviewed  the machine generated code and vouch for . We ran the tests and like the tests, we like the code structure, we read every single line of code carefully we also made sure the PR meets a project’s guidelines.All the crazy code agents generated along the way has been fixed, we are happy to stamp our very own personal brand on the code.Projects tend to have a large set of rules around code quality, code organisation, testing and more.We may have used AI assistance to generate a ready to review PR, fundamentally, though this does not matter, we vouch for the code and stand behind it meeting both our brand and a project’s guidelines.The distance from a prototype to a ready to review PR can be deceptively vast. There may be days of engineering taking a complex prototype and making it production ready.This large distance was communicated as well by Andrej Karpathy in the Dwarkesh Podcast.For some kinds of tasks and jobs and so on, there’s a very large demo-to-product gap where the demo is very easy, but the product is very hard.For example, in software engineering, I do think that property does exist. For a lot of vibe coding, it doesn’t. But if you’re writing actual production-grade code, that property should exist, because any kind of mistake leads to a security vulnerability or something like that.Veracode survey found that only 55% of generation tasks resulted in secure code. (source).Our models are getting better by the day, and everything really depends on an enormous amount of parameters, but the core message that LLMs can and do generate insecure code, stands.The root cause for the distance between project guidelines and a prototype is AI alien intelligence.Many engineers I know fall into 2 camps, either the camp that find the new class of LLMs intelligent, groundbreaking and shockingly good. In the other camp are engineers that think of all LLM generated content as “the emperor’s new clothes”, the code they generate is “naked”, fundamentally flawed and poison.I like to think of the new systems as neither. I like to think about the new class of intelligence as “Alien Intelligence”. It is both shockingly good and shockingly terrible at the exact same time.Framing LLMs as “Super competent interns” or some other type of human analogy is incorrect. These systems are aliens and the sooner we accept this the sooner we will be able to navigate the complexity that injecting alien intelligence into our engineering process leads to.Playing to alien intelligence strength, the prototypeOver the past few months I have been playing a lot with AI agents. One project I am particularly proud of is dv. It is a container orchestrator for Discourse, that makes it easy to use various AI agents with Discourse.I will often run multiple complete and different throwaway Discourse environments on my machines to explore various features. This type of tooling excels at vibe engineering prototypes.Interestingly dv was mostly built using AI agents with very little human intervention, some of the code is a bit off brand, that said unlike Discourse or many of the other open source gems I maintain it is a toy project.Back on topic, dv has been a great factory for prototypes on Discourse. This has been wonderful for me. I have been able to explore many ideas while catching up on my emails and discussions on various Discourse sites.On banning AI contributions, prototypes and similarFirstly  be respectful of the rules any project you contribute has, seek them out and read them prior to contributing. For example: Cloud hypervisor says no AI generated code to avoid licensing risks.That said, there is a trend among many developers of banning AI. Some go so far as to say “AI not welcome here” find another project.This feels extremely counterproductive and fundamentally unenforceable to me. Much of the code AI generates is indistinguishable from human code anyway. You can usually tell a prototype that is pretending to be a human PR, but a real PR a human makes with AI assistance can be indistinguishable.The new LLM tooling can be used in tremendous amounts of ways including simple code reviews and simple renamings within a file, to complete change set architecture.Given the enormous mess and diversity here I think the healthiest approach is to set clear expectations. If I am submitting a PR it should match my brand and be code I vouch for.As engineers it is our role to  our changes. Is our change ready for human review or is it simply a fun exploration of the problem space?Human code review is increasingly becoming a primary bottleneck in software engineering. We need to be respectful of people’s time and protect our own engineering brands.Prototype are fun, they can teach us a lot about a problem space. But when it comes to sending contributions to a project, treat all code as code you wrote, put your stamp of ownership and approval on whatever you build and only then send a PR you vouch for.]]></content:encoded></item><item><title>Understanding the Worst .NET Vulnerability</title><link>https://andrewlock.net/understanding-the-worst-dotnet-vulnerability-request-smuggling-and-cve-2025-55315/</link><author>ingve</author><category>hn</category><pubDate>Tue, 28 Oct 2025 11:03:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I admit, that's a very click-baity headline, but Microsoft have given the vulnerability a CVSS score of 9.9, their highest ever. Time to panic, right?In this post I try to provide a bit more context. I explain how request smuggling vulnerabilities work in general, how it works in  case, what attackers could use it for, how the vulnerability was fixed, what you can do to protect yourself.WARNING: I am not a security professional, so do not take anything in this post as gospel or advice. I'm just a developer trying to make sense of things. 😄 All of the details in this post are based on information that was provided or referenced in the original announcement.Inconsistent interpretation of http requests ('http request/response smuggling') in ASP.NET Core allows an authorized attacker to bypass a security feature over a network.The bug enables HTTP Request Smuggling, which on its own for ASP.NET Core would be nowhere near that high, but that's not how we rate things...Instead, we score based on how the bug might affect applications built on top of ASP.NET.Request Smuggling allows an attacker to hide an extra request inside an another, and what that hidden request can do is very application specific.The smuggled request could cause your application code toLogin as a different user (EOP)Make an internal request (SSRF)Perform an injection attackBut we don't know what's possible because it's dependent on how you've written your app.That  all sound pretty scary! 😱 So you can understand the consternation that the issue has caused, especially given the hesitation to explain exactly what "how you've written your app" .Out of curiosity, I decided to dig in further to really understand this vulnerability, how it could impact you, and what "how you've written your app" could mean.Before we get to the actual patched vulnerability in ASP.NET Core and how the vulnerability works, I think it's important to have some background about the general  of exploits known as .HTTP request smuggling is a security exploit that has been known about for a  time (according to Wikipedia, it was first documented in 2005). It fundamentally arises when you have two different servers processing an HTTP request (e.g. a server and a proxy server), and where those two servers  in how they handle "invalid" HTTP requests.In all cases of HTTP request smuggling, the exploit works by creating an invalid HTTP request (or sometimes just an  request), that looks a bit like two HTTP requests glued together. In summary, the exploit then works a bit like this:The proxy server receives the ambiguous HTTP requestThe proxy server forwards the request (unmodified) to the destination serverThe server interprets the ambiguous request as  pipelined HTTP requests sent to the server, and processes them separately.I think it's easiest to understand the problem with an example, so the request below shows an example from the original 2005 paper.Note that this is  an example of the request smuggling vulnerability in CVE-2025-55315, it's just a representative example of request smuggling in .Let's imagine the attacker sends an HTTP request that looks like this:

this=thatPOST /vuln_page.jsp HTTP/1.0


param1=value1&data=<script>alert("stealing%20your%20data:"%2bdocument.cookie)</script>&foobar
The important feature of this request is that there are  headers, with different values:  or . This is the core of the exploit; the difference between  of the these two headers the HTTP proxy and HTTP server honour is what causes the vulnerability.Let's walk through how the exploit works, step-by-step:The attacker sends the above HTTP request.The HTTP proxy receives the request, notes the duplicate  headers, and accepts the  header, the  length. That means the  rest of the request is treated as the message body, and seems fine as far as the proxy is concerned.The HTTP proxy forwards the request on to the destination server.This server also notes the duplicate  header, but it takes the  of the headers, with the length of .The server reads  bytes of the body (i.e. ) and treats that as the whole request. As far as the server is concerned, the whole (valid) request has been received, and it sees the rest of the data .That means that the destination server sees an entirely new HTTP request to process, , and treats it as a new request.That's the core of the issue; the proxy saw one request, while the destination server saw —the second request has been "smuggled" past the proxy to the server.The request smuggling technique shown here, where you have multiple  headers isn't the "canonical" example you'll generally see referenced, but I used it here because it's simpler to understand in a lot of ways.The canonical request smuggling attack is where you send both a  header and a Transfer-Encoding: chunked header (which specifies the length of the body as part of the body itself). As before, the request smuggling exploit relies on differences in how proxy and destination servers interpret these conflicting headers.So as you've seen, request smuggling enables sending a secret request to a destination server that an intermediate proxy server hasn't seen. In the next section we'll look at why that's a bad thing, and how it can be exploited.On the face of it, request smuggling might not  like a big deal. So the server sees two requests, so what? You could always send two requests to the server , right? Well, yes and no.The issue with request smuggling is really all about the  between the proxy and destination servers. Thanks to this mismatch, and depending on what behaviours and expectations the target application has, attackers can use request smuggling toReflect malicious data to other users on sites that are vulnerable to cross-site scripting.Poison caches with bad data.Exfiltrate authentication credentials or other data from client requests.Invoke endpoints that shouldn't be publicly accessible (because the proxy would block external access to them).Replace/override authentication controls handled by the proxy.Redirect users to malicious sites on sites vulnerable to open-redirect attacks.As you can see, these are all Bad™️, so you can kind of understand why the 9.9 rating was given! 😱That said, it's worth mentioning that not  of these attacks will be fruitful against  applications. Some of the easiest to understand versions of these exploits are where the proxy is not just doing "dumb" forwarding of requests, but rather it's validating or enhancing the request in some way.For example, if you have a proxy sat in front of your server which is responsible for handling TLS termination and client-authentication and identification using certificates, then request smuggling could be used to bypass these checks and insert your  identification.As an example of that attack, the HTTP request below demonstrates using a  and  request smuggling attack to "hide" the request to  from the front-end proxy, and insert a malicious  header, which would  be added by the front-end proxy:

0

In this example, the server assumes that the X-SSL-CLIENT-CN: administrator header was added by the proxy, and so the server assumes that the proxy already did all the necessary authentication and authorization. The attacker is able to perform a request as an entirely different user.Request smuggling is clearly a big problem whenever you have a front-end proxy that does some functionality, but even when it's essentially a dumb proxy, request smuggling can still be used to steal and exfiltrate data from  user's requests, even if the attacked site is not vulnerable to cross-site scripting or other vulnerabilities.In these attacks, simply having functionality that displays data provided by a user (even sanitised) can be sufficient to steal the credentials of other users. So something as simple as displaying a user name or a comment could be sufficient.This post is long enough, and there are so many different attacks, that I'm going to leave it there for looking at exploits. If you'd like to learn more about what's possible, along with simple explanations and examples of exploits, I recommend the PortSwigger documentation on exploiting request smuggling.In general, whenever people talk about request smuggling, they normally talk about the case where you have multiple servers: the canonical example is a proxy server and a destination server, as I've discussed so far. But don't be fooled, these issues and vulnerabilities can apply even if you aren't strictly using a proxy.The  feature of the vulnerability is that there's an opportunity for confusion between two "systems", whether they're full "servers" or not. This obviously applies to proxy servers, but could also apply to your application if you're doing anything where you're reading/manipulating/forwarding request streams, or where there's the possibility for confusion inside the same application.For ASP.NET Core applications, if you're working with  or , or other similar methods then you  be vulnerable to attacks even if you're not explicitly using a proxy server. Even if you don't think of your application as a proxy or as using a proxy, if you're doing "proxy-like" things, then you could be vulnerable.Put in other words, if you're reading, manipulating, or forwarding request streams directly in ASP.NET Core, as opposed to just relying on the built-in model binding, then you could be at risk to request smuggling attacks. It's very hard to enumerate all the attack vectors, so you should consider any code that does so as a potential avenue of exploitation.We've now covered how request smuggling works and can be exploited in general, so it's time to look at the  version of request smuggling that is targeted in the .NET CVE-2025-55315 vulnerability.As we've seen, HTTP request smuggling is a general technique that relies on differences between proxies and servers in how they parse HTTP requests. I've shown two specific versions of this so far: duplicate  headers, and / confusion, but these are not exhaustive. There are variations on these approaches which also lead to request smuggling.All the details and images in this section are based on the descriptions and examples in the original post. That post is excellent, so if you want even more detail and explanation than here, you should definitely read it, and then you can skip the abbreviated version I provide here.To understand the vulnerability, we'll first look at how chunked transfer encoding works and what chunk extensions are. We'll then look at how invalid line-endings can lead to differences in interpretation of a request. Finally, we'll look at how this difference in interpretation can open the way for request smuggling, and how ASP.NET Core fixed the problem.To understand the vulnerability, we first need to understand how Transfer-Encoding: chunked works, and how chunk extensions complicate things.When you're  a request, you might not always know up-front how big the request is that you're sending. Let's take a practical example of serializing a .NET object to JSON into a request body. The only way to know for sure how big the serialized data is going to be is to actually serialize it. So you  serialize the data to memory  writing the request, but if the data is very big, then that could cause issues with allocating big arrays.Instead, Transfer-Encoding: chunked allows sending the request data in multiple "chunks". You need to know the size of each individual chunk, but not the  size of the data, or how many chunks there are. This works well for serializing to a small buffer, sending that small buffer as a chunk, and then re-using the buffer to serialize the next part, until you have serialized the whole object.In terms of the HTTP request itself, each chunk consists of a header and a body. The header consists of a hexadecimal-formatted number of bytes, followed by a  () line ending. The chunk body is then the specified number of bytes, followed by another . You can have as many chunks as you need, and the request will keep being passed until you send a  length chunk, which indicates the end of the request.As an example, the following HTTP  shows posting some JSON to an endpoint, but the JSON is sent as three distinct chunks:Chunk 1: The header is  indicating 9 bytes will be sent (followed by ), and then the 9 bytes of the start of the JSON document in the chunk body, again followed by .Chunk 1: The header is  indicating 14 bytes (14 in hexadecimal is ) will be sent (followed by ), and then the remaining 14 bytes of the end of the JSON document, followed by .The final chunk is an "empty" chunk, , indicating the end of the request.We're going to see shortly that line endings are very important, so the following diagram shows the same as the above HTTP request, but with the line endings included:That's "normal" chunked transfer encoding, so now we come to chunk extensions. Chunk extensions are part of the HTTP 1.1 protocol which allows for adding key-value pairs of metadata to individual chunks. The following example shows the same request as before, but with a chunk extension,  in the second chunk:A chunk extension is indicated by a  after the chunk header length, followed by one or more key-value pairs in the form . It's important to understand that chunk extensions are not part of the  that's seen by a request handler; chunk extensions are just metadata about the individual chunk. And tl;dr; they're completely useless 😅To the closest approximation, no-one cares about chunk extensions; client implementations don't send them, and servers just ignore them. If that's the case, how can they be the cause of such a problematic bug in .NET?The problem is  the implementation ignores them…In general with HTTP, clients and server implementations often try to follow the  of "be conservative in what you send, and lenient with what you accept". Unfortunately, it's this very leniency which can sometimes leave us in hot water. After all, it was leniency around requests containing both a  and  header that was the root cause of the original request smuggling exploit.For chunk extensions though, leniency is often  built in to the server implementations. Given that no implementations actually  anything with the chunk extensions, the canonical approach to handling them when parsing a chunk header is just to  them. When a  is parsed, it's common to just look for the end of the line, and ignore everything in between.For ASP.NET Core (prior to the fix), on finding a  in the chunk header, Kestrel would "parse" the extension, but in practice, it would search for the carriage return  and then check for the following , skipping everything in between, a little bit like this (very simplified compared to original code): buffer extensionCursor  bufferByteCR suffixBuffer  bufferextensionCursor suffixSpan  suffixBuffersuffixSpan
        buffer The implementation in ASP.NET Core wasn't particularly special; most servers simply skip over the bytes until they find a . The big question is exactly  the servers search for . What happens if they see a lone , or a lone ? Do they treat that the same as a ? Do they throw an error if they find an un-paired  or ? Or do they ignore it and keep looking for a ?That ambiguity is at the heart of the CVE-2025-55315 request smuggling vulnerability. Differences in how proxy and server implementations treat standalone  or  in a chunk header allow for request smuggling exploits that use this ambiguity.Note that according to the RFC, implementers must  treat  or  as "valid" line terminators for a chunk header, and neither  or  are allowed elsewhere in chunk headers, so correct implementations must reject requests that include these standalone line endings in chunk headers.For complete clarity, the following example is the same as the previous implementation but with an  chunk header in the chunk extension of the second chunk. Instead of ending with , the chunk extension ends with a single :That's the root cause of the request smuggling vulnerability, so in the next section we'll look at  this could be used to craft a malicious HTTP request.Just as with other examples of request smuggling, the chunk extensions approach relies on differences in how a proxy parses a request compared to a subsequent server. This difference means the proxy sees one request, while the destination request sees two requests, and allows for all the same exploits I discussed earlier.As discussed, these examples come from this excellent blog post, so see that post for more details, variations on the attack, and further ways to exploit the vulnerability.The following example shows a malicious HTTP request that exploits a difference in line-ending handling between a proxy and the destination server to smuggle a request to the  endpoint. We can imagine that the proxy is configured to automatically reject requests to  normally, and the server assumes that the proxy handles that for us.In this example the attacker creates a malformed chunk header with a chunk extension by sending . The  ensures that both the proxy and and server treat the header as a chunk extension, but using  instead of  results in differential parsing:The proxy only sees a single request: It treats the  as a "valid" line-ending for the chunk headerIt then treats the  as the chunk body is the next chunk headerThe next 71 bytes ( is hex, which is 71 in decimal) are treated as the chunk body.Finally there's the empty chunk blockThe server sees two requests: The server ignores the lone , and skips all the way to It then treats the  as the chunk bodyIt sees an ending chunk, and thinks the request is overThe remaining data is treated as a completely separate request, which contains only an empty chunk in the body.This is pretty much the simplest example, but you can essentially exploit this difference in all the ways I described previously.  what the implications are for  application are hard to say, but given that all sorts of security bypass, credential stealing, and injection attacks are possible, it's easy to understand why the vulnerability received a CVSS rating of 9.9.As far as I can tell, these servers are essentially vulnerable in the same way as ASP.NET Core is, so it's just an interesting data point, and I think reflects how Microsoft really want to make sure this gets the visibility it deserves and that customers patch their apps!As with most fixes for request-smuggling, the solution is to stop being lenient and/or ambiguous about how standalone line-endings are handled in chunk headers.In ASP.NET Core, the PR that fixes the issue does so by explicitly checking for  line-endings, instead of just looking for . If it finds a line ending and it's  strictly , then Kestrel now throws a KestrelBadHttpRequestException and returns a  response.The vulnerability has been patched in ASP.NET Core, so what should you do?Obviously the  news here is that there is a fix for ASP.NET Core. As described in the original issue, the important thing is to update to the latest supported version of ASP.NET Core as soon as possible.There's no announced evidence of the request smuggling vulnerability being exploited in the wild, but given the vast number of ways that request smuggling  be used, would we even know? 🤔That means you should update your version of .NET 8, .NET 9, or .NET 10:If you're using ASP.NET Core 2.3 on .NET Framework, then you'll need to update your version of Microsoft.AspNetCore.Server.Kestrel.Core:Microsoft.AspNetCore.Server.Kestrel.CoreIf you are doing self-contained deployments of your applications, you'll need to update to the patched versions and then redeploy your applications.And if you're using older versions of .NET Core? Well, then you  patch… HeroDevs provide additional support for out-of-support versions of .NET (and have confirmed they'll be patching it in .NET 6), but this vulnerability is present in basically  versions of .NET Core as far as I can tell. I've personally tested down to .NET Core 3.0 and I can confirm that the vulnerability is there and there are no patches coming for you. The best thing to do is to update to a supported version of .NET.⚠️ If you are running ASP.NET Core using <=.NET Core 3.0, .NET Core 3.1, .NET 5, .NET 6 (unless supported by HeroDevs), or .NET 7, then you are vulnerable, and there are no patches. You should update to a supported version of .NET as soon as possible. Ironically, if you're stuck on old .NET Framework Web Forms or MVC applications you are apparently  vulnerable.It's worth noting that if you are stuck on one of these old framework versions and  upgrade, then probably the best way to protect yourself is to ensure that you have a proxy in front of your application which is confirmed to not be vulnerable (though obviously you are likely vulnerable to  exploits 😅).For example, Azure App Services (AAS) confirmed that applications running in AAS are no longer vulnerable, even if you haven't updated, because the proxy that AAS uses (itself a YARP based ASP.NET Core proxy) has been patched. By blocking the requests at the proxy level, ambiguous requests will never make it to your application, so you are protected.Unfortunately, right now, it's not clear exactly where you stand if you're using a service other than AAS for hosting your applications. Even IIS hasn't been confirmed to be safe or vulnerable at this point, but I did some unofficial testing on my Windows 11 box, and as fat as I can tell, it  vulnerable.Note that various people in the original issue are attempting to test IIS by using the / version of request smuggling, which is not applicable here; we're interested in the chunk-extensions based version.Another interesting point is that this is vulnerability in HTTP/1.0 and HTTP/1.1 ; it is not a vulnerability in HTTP/2 or HTTP/3. HTTP/2 and HTTP/3 do not support chunked transfer encoding, and instead uses a different, more efficient, binary framing layer for data streaming. So another way to protect those applications which you  upgrade may be to enforce that client's can  use HTTP/2 or HTTP/3. Be aware that's liable to break a  of clients that are still using HTTP/1.1 though!You can configure the HTTP protocols allowed by Kestrel by configuring your Kestrel endpoints. The documentation shows various ways to do this.The "simplest" way to know if you're affected is to check the version of .NET you're using to run your applications, using  and verify that you're using one of the patched versions. If you are, you're safe. That's the only "supported" way to know that you're safe, and it's the one way I would recommend. As far as I can tell, there isn't currently a generalised tool to point at an application to find out if it's vulnerable, though it would likely be possible to write one.The folks at HeroDevs re-implemented the functional tests from the original ASP.NET Core fix as a console application compiled against multiple versions of ASP.NET Core. They used this to confirm that unpatched versions of .NET 8-.NET 10 are vulnerable, while  versions are not. They also used this to verify .NET 6 is vulnerable, and I tweaked it to confirm everything down to at least .NET Core 3.0 is vulnerable.The test in the repro works by sending a chunked transfer encoding request to ASP.NET Core, with an invalid line ending in a chunk extension header. The vulnerability is identified by ASP.NET Core "hanging", waiting for more data, until it eventually times out. The "fixed" version immediately throws the  exception included in the fix.I saw some confusion about this test online; the argument was "if both the fixed and broken versions throw an exception, why does it matter"? However, that's not the point of the test. The fact that Kestrel is paused waiting for more data indicates that a smuggled HTTP request  have been executed. You can see how this can be leveraged to exfiltrate data or attack other users both in the chunk extensions blog or on PortSwigger's site.I used a similar approach to try to understand whether IIS might be vulnerable by sending the same crafted HTTP request to IIS and seeing if it hung until timing out: it did on my version of IIS (): localhost So does that definitely mean IIS is vulnerable? No, don't trust me, I'm not a security researcher 😅 But until you hear otherwise, I would play it safe and assume that IIS  protect you from chunk extension request smuggling attacks. And in general, I would apply the same rules to any other proxies you are relying on in your infrastructure.And as a final reminder, even though request smuggling is typically described and demonstrated using a proxy in front of your server, just  using a proxy does  mean you're automatically safe. If you're reading, manipulating, or forwarding request streams directly in ASP.NET Core, as opposed to just relying on the built-in model binding, then you  be at risk to request smuggling attacks. It's best to play it safe, patch your apps, and wherever possible leave the complexity of manipulating requests to ASP.NET Core.In general, I would make sure to subscribe to the ASP.NET Core issue on GitHub, as it's likely that any more announcements around the issue will also be reported there.I described how request smuggling works in general, using a simple example of request smuggling to show how ambiguity in how HTTP is parsed can lead to HTTP proxies and HTTP servers in handling the same HTTP request in different ways. This can lead to the server seeing two requests where the proxy only sees a single request.After walking through a request smuggling example, I discussed some of the ways attackers could exploit a request smuggling vulnerability. That includes reflecting malicious data to other users of your app, exfiltrating authentication credentials or other data from client requests, invoking endpoints that shouldn't be publicly accessible, and various other attacks.Next I walked through the specific request smuggling vulnerability identified in CVE-2025-55315. This uses ambiguities in the parsing of chunk extensions when sending requests that use chunked transfer encoding. Chunk extensions are generally ignored by all servers, but lenient handling can lead to differential handling between proxy and server, providing an avenue for request smuggling.Finally, I walked through the mitigation steps you should take: patching your applications. I described the information we currently have about vulnerable or patched proxy servers, and how old versions of ASP.NET Core are not going to be getting patches, so will remain vulnerable (shout out again to HeroDevs for supporting .NET 6). If you're running in AAS, then you're ok, but otherwise, you need to check with your proxy provider to establish whether you are vulnerable or not.]]></content:encoded></item><item><title>Criminal complaint against facial recognition company Clearview AI</title><link>https://noyb.eu/en/criminal-complaint-against-facial-recognition-company-clearview-ai</link><author>latexr</author><category>hn</category><pubDate>Tue, 28 Oct 2025 08:34:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Poker Tournament for LLMs</title><link>https://pokerbattle.ai/event</link><author>SweetSoftPillow</author><category>hn</category><pubDate>Tue, 28 Oct 2025 07:42:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Picture gallery: Amiga prototype &quot;Lorraine&quot; at the Amiga 40 event</title><link>https://www.amiga-news.de/en/news/AN-2025-10-00110-EN.html</link><author>doener</author><category>hn</category><pubDate>Tue, 28 Oct 2025 05:28:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I built the same app 10 times: Evaluating frameworks for mobile performance</title><link>https://www.lorenstew.art/blog/10-kanban-boards/</link><author>0xblinq</author><category>hn</category><pubDate>Tue, 28 Oct 2025 05:22:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Complete Digitization of Leonardo da Vinci&apos;s Codex Atlanticus</title><link>https://www.openculture.com/2025/10/digitization-of-leonardo-da-vincis-codex-atlanticus.html</link><author>emmelaich</author><category>hn</category><pubDate>Tue, 28 Oct 2025 03:32:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[No his­tor­i­cal fig­ure bet­ter fits the def­i­n­i­tion of “Renais­sance man” than Leonar­do da Vin­ci, but that term has become so overused as to become mis­lead­ing. We use it to express mild sur­prise that one per­son could use both their left and right hemi­spheres equal­ly well. But in Leonardo’s day, peo­ple did not think of them­selves as hav­ing two brains, and the worlds of art and sci­ence were not so far apart as they are now.That Leonar­do was able to com­bine fine arts and fine engi­neer­ing may not have been over­ly sur­pris­ing to his con­tem­po­raries, though he was an extra­or­di­nar­i­ly bril­liant exam­ple of the phe­nom­e­non. The more we learn about him, the more we see how close­ly relat­ed the two pur­suits were in his mind.He approached every­thing he did as a tech­ni­cian. The uncan­ny effects he achieved in paint­ing were the result, as in so much Renais­sance art, of math­e­mat­i­cal pre­ci­sion, care­ful study, and first­hand obser­va­tion.His artis­tic projects were also exper­i­ments. Some of them failed, as most exper­i­ments do, and some he aban­doned, as he did so many sci­en­tif­ic projects. No mat­ter what, he nev­er under­took any­thing, whether mechan­i­cal, anatom­i­cal, or artis­tic, with­out care­ful plan­ning and design, as his copi­ous note­books tes­ti­fy. As more and more of those note­books have become avail­able online, both Renais­sance schol­ars and laypeo­ple alike have learned con­sid­er­ably more about how Leonardo’s mind worked.“No oth­er col­lec­tion counts more orig­i­nal papers writ­ten by Leonar­do,” notes Google. The  “con­sists of 1119 papers, most of them drawn or writ­ten on both sides.” Its name has “noth­ing to do with the Atlantic Ocean, or with some eso­teric, mys­te­ri­ous con­tent hid­den in its pages.” The 12-vol­ume col­lec­tion acquired its title because the draw­ings and writ­ings were bound with the same size paper that was used for mak­ing atlases. Gath­ered in the 16th cen­tu­ry by sculp­tor Pom­peo Leoni, the papers descend­ed from Leonardo’s close stu­dent Gio­van Francesco Melzi, who was entrust­ed with them after his teacher’s death.The his­to­ry of the Codex itself makes for a fas­ci­nat­ing nar­ra­tive, much of which you can learn at Google’s Ten Key Facts slideshow. The note­books span Leonardo’s career, from 1478, when he was “still work­ing in his native Tus­cany, to 1519, when he died in France.” The col­lec­tion was tak­en from Milan by Napoleon and brought to France, where it remained in the Lou­vre until 1815, when the Con­gress of Vien­na ruled that all art­works stolen by the for­mer Emper­or be returned. (The emis­sary tasked with return­ing the Codex could not deci­pher Leonardo’s mir­ror writ­ing and took it for Chi­nese.)The Codex con­tains not only engi­neer­ing dia­grams, anato­my stud­ies, and artis­tic sketch­es, but also fables writ­ten by Leonar­do, inspired by Flo­ren­tine lit­er­a­ture. And it fea­tures Leonardo’s famed “CV,” a let­ter he wrote to the Duke of Milan describ­ing in nine points his qual­i­fi­ca­tions for the post of mil­i­tary engi­neer. In point four, he writes, “I still have very con­ve­nient bomb­ing meth­ods that are easy to trans­port; they launch stones and sim­i­lar such in a tem­pest full of smoke to fright­en the ene­my, caus­ing great dam­age and con­fu­sion.”As if in illus­tra­tion, else­where in the Codex, the draw­ing above appears, “one of the most cel­e­brat­ed” of the col­lec­tion.” It was “shown to trav­el­ing for­eign­ers vis­it­ing the Ambrosiana [the Bib­liote­ca Ambrosiana in Milan, where the Codex resides] since the 18th cen­tu­ry, usu­al­ly arous­ing much amaze­ment.” It is still amaz­ing, espe­cial­ly if we con­sid­er the pos­si­bil­i­ty that its artistry might have been some­thing of a byprod­uct for its cre­ator, whose pri­ma­ry moti­va­tion seems to have been solv­ing tech­ni­cal problems—in the most ele­gant ways imag­in­able.Note: An ear­li­er ver­sion of this post appeared on our site in 2019. is a writer and musi­cian based in Durham, NC. ]]></content:encoded></item><item><title>GLP-1 therapeutics: Their emerging role in alcohol and substance use disorders</title><link>https://academic.oup.com/jes/article/9/11/bvaf141/8277723?login=false</link><author>PaulHoule</author><category>hn</category><pubDate>Tue, 28 Oct 2025 02:00:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI can code, but it can&apos;t build software</title><link>https://bytesauna.com/post/coding-vs-software-engineering</link><author>nreece</author><category>hn</category><pubDate>Mon, 27 Oct 2025 23:41:32 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Have you noticed that quite a few people are looking for technical cofounders
or CTOs right now? I, for one, get a surprising amount of these queries; most
of them along the lines of “hey, I have this vibe-coded app, would you like to
make it production-ready”. I have sort of a profile for these people. Think
someone who knows their business but has always lacked the technical skills to
make their ideas happen — a legal counsel, perhaps, or an account manager.Why would these people need me? That's what I've thought about a little bit,
and I think there is an important signal here: What is it exactly that they
can’t get done with GenAI alone? This is something everyone is trying to
understand, right? Everyone wants to know what these models can do. Or, to be a
little blunt, everyone wants to know which jobs are soon to become obsolete.
The fact that I get these requests says something about software engineering. I
mean, if software engineering was automated, no one would be looking for
technical cofounders.Well, I think I know why we get these proposals. The thing is that AI can
code, but it can't build software. This is the conclusion I've come to after
spending a significant amount of time writing AI-assisted code and watching
demos by other people.There is old wisdom that says: Coding is easy, software engineering is hard.
It seems fair enough to say that LLMs are already able to automate a lot of
coding. GPT-5 and the like solve isolated well-defined problems with a pretty
nice success rate. Coding, however, is not what most people are getting paid
for. Building a production-ready app is not coding, it’s software engineering.The way I see it is that coding becomes software engineering around the point
where you try to turn your demo into a real product — which happens to be
exactly the point where these people reach out to you with their pitch.I don’t really know why AI can't build software (for now). Maybe it has to do
with the nature of the job. When you write software for a living, your main
task is to deal with complexity. The average production software only does a
bunch of easy things. The challenge is doing hundreds of these easy things at
once, and keeping the whole thing maintainable. Or, to rephrase this in the
present context: It's one thing to demonstrate a feature. It's a much more
difficult thing to build that feature in a manner that supports integration,
expansion, and long-term maintainability.When you look at the code these people send you, you realize that “making the
app production-ready” really means torching the whole thing and starting from
scratch.I think this says a lot about where we are at right now.]]></content:encoded></item><item><title>Iroh-blobs</title><link>https://www.iroh.computer/blog/iroh-blobs-0-95-new-features</link><author>janandonly</author><category>hn</category><pubDate>Mon, 27 Oct 2025 23:28:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Iroh-blobs 0.95 contains a number of significant new features that are worth explaining in detail. There are several new features that are useful for blobs users and also for iroh users in general.Let's start with a feature that is essential for blobs itself, but can also be useful for many other protocols.There is a new connection pool in . This is useful whenever you have a protocol that has to talk to a large number of endpoints while keeping an upper bound of concurrent open connections. In blobs, this is used whenever you use the downloader to orchestrate blobs downloads from multiple providers.Iroh connections are relatively lightweight, but even so you don't want to keep thousands of them open at the same time. But opening a new connection every time you do a small exchange with a peer is very wasteful. The  gives you an API to deal with these tradeoffs.Let's first look at basic usage: will try to get an existing connection from the pool. If there is none, it will create one and store it. The connection will be kept in the pool for a configurable time. Idle connections will be closed as needed. So you can just use this as a drop-in replacement for endpoint.connect and be sure that you won't ever create an unbounded number of connections.There are some advanced features that can be configued using non-default options.You can configure the max number of connections to be retained, the maximum tolerable duration for connection establishment, and the max duration connections are kept when idle.So far, pretty straightforward. There is an additional option to perform some setup before the connection is handed out to the user. For example, you can reject connections based on the data available at this time from the endpoint and the connection, or wait for the connection to reach a certain state before handing it out.As an example, you might want to do iroh-blobs transfers only on direct connections in order to get good performance or reduce bandwidth use on the relay. If establishing direct connections is not possible, the connection establishment would time out, and you would never even attempt a transfer from such a node.The code to await a direct connection will change quite a bit once we have QUIC multipath. But the capability will remain, and we will update the test code to reflect the new API.The connection pool is generic enough that it will move to its own crate together with some other iroh utilities. It lives in blobs only until iroh 1.0 is released.Until then, just depend on iroh-blobs. Iroh-blobs without persistent storage is a very lightweight dependency.One thing to keep in mind when using the connection pool: the connection pool needs the ability to track which connections are currently being used. To do this, the connection pool does not return  but , a struct that derefs to  but contains some additional lifetime tracking.But  is , so in principle there is nothing stopping you from cloning the wrapped connection and losing the lifetime tracking. . If you work with connections from the pool, you should pass around either a  or a  to make sure the underlying  stays alive.Incorrect usage of :Correct usage of :We experimented with a safer callback-based API, but it turned out to be just too inconvenient to use.Iroh-blobs is a protocol that tries to avoid overabstraction. For example as of now you can only use the BLAKE3 hash function, and we hardcode the chunk group size to a value that should work well for all users.But sometimes there are cases where a bit of abstraction is needed. There was a user request to be able to use compression with iroh-blobs in sendme. One way to do this is to compress files before adding them to the blob store. But this has various downsides. It requires you to create a copy of all data before adding it to the blob store, and will also not lead to very good compression rates when dealing with a large number of small files, since each file will have to be compressed in isolation.It would be better to compress requests and response streams of the entire protocol and expose the resulting protocol under a different ALPN. With this approach the compression algorithm would be able to find redundancies between multiple files when handling a request for multiple blobs.By default, iroh-blobs still works directly with iroh::endpoint::SendStream and iroh::endpoint::RecvStream, so for normal use nothing changes.The traits are a bit similar to Stream and Sink, but with two important additions.We allow sending and receiving Bytes, since iroh streams work with bytes internally. That way we avoid a copy in the default case.We have methods stop and reset to close the stream, and on the send stream a method stopped that returns a future that resolves when the remote side has closed the stream.Wrapping the entire iroh-blobs protocol into compression is pretty straightforward except for some boilerplate. We have an example compression.rs that shows how to do this.We will have this as an optional feature of sendme in one of the next releases.Just like the connection pool, these traits are generally useful whenever you want to derive iroh protocols by wrapping existing protocols, so they will move to a separate crate once iroh 1.0 is released.This change is from iroh-blobs 0.93On the provider side, it is now possible to have very detailed events about what the provider is doing. The provider events are now implemented as an irpc protocol. For each request type you can use an event mask to configure if you want to be notified at all, and if you need the ability to intercept the request, e.g. if you only want to serve certain hashes.There is an example how to use the new provider events to limit by provider node id or hash.Here is a provider event handler that serves only blobs requests for hashes in a fixed set of allowed hashes:The next major feature in iroh-blobs will be a minimal version of multiprovider downloads for individual blobs.As soon as iroh 1.0 is released, several generic parts of iroh-blobs will move to a separate iroh utilities crate.Iroh is a dial-any-device networking library that just works. Compose from an ecosystem of ready-made protocols to get the features you need, or go fully custom on a clean abstraction over dumb pipes. Iroh is open source, and already running in production on hundreds of thousands of devices.To get started, take a look at our docs, dive directly into the code, or chat with us in our discord channel.]]></content:encoded></item><item><title>Linux VM without VM software – User Mode Linux</title><link>https://popovicu.com/posts/linux-vm-without-vm-software-user-mode/</link><author>arunc</author><category>hn</category><pubDate>Mon, 27 Oct 2025 22:30:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[If you carefully read the Linux kernel docs, you will find an interesting statement:Linux has also been ported to itself. You can now run the kernel as a userspace application - this is called UserMode Linux (UML).Today, we’ll explore how you can start an unconventional VM by running a Linux kernel as a process within the Linux kernel itself. This approach doesn’t require installing virtualization software like QEMU, nor does it need root privileges, which opens up some intriguing possibilities.Kernel’s Hardware AbstractionA fundamental responsibility of the kernel is to abstract hardware and offer a consistent interface to userspace. This includes managing shared resources like the CPU and memory for multiple tasks. The kernel determines the underlying hardware (e.g., through a device tree on some platforms, which lists system components) and connects the appropriate drivers.This hardware can also be entirely virtual. In a QEMU virtual machine, for instance, resources like memory and attached disks are virtualized by the QEMU userspace application, incurring a certain performance overhead. The CPU presents an interesting case, as it too can be virtualized in userspace, particularly when emulating a different architecture.A fascinating aspect of drivers for virtualized hardware is that they can be  — or, more formally, . This means the drivers are aware they’re running on virtualized hardware and can leverage this by communicating with the hardware in specialized ways. While the specifics are complex, one can imagine drivers interacting with virtual hardware in ways not feasible with physical counterparts. Online sources suggest that paravirtualization can achieve performance levels close to those of physical devices using traditional drivers.UML - Kernel in a Userspace ProcessPersonally, I view UML as a paravirtualized kernel configuration. Instead of running directly on bare metal, the UML kernel operates atop an existing kernel instance, leveraging some of its userspace functionalities. For instance, rather than linking the console driver to a physical UART, it can utilize standard userspace input/output. Similarly, a block device driver can target a file on the host’s filesystem instead of a physical disk.In this setup, UML is essentially a userspace process that cleverly employs concepts like files and sockets to launch a new Linux kernel instance capable of running its own processes. The exact mapping of these processes to the host — specifically, how the CPU is virtualized — is something I’m not entirely clear on, and I’d welcome insights in the comments. One could envision an implementation where guest threads and processes map to host counterparts but with restricted system visibility, akin to containers, yet still operating within a nested Linux kernel.This page from the kernel’s documentation has a pretty good illustration of what this looks like:I highly recommend checking out that page for more detailed documentation, particularly for the compelling reasons listed for its usefulness. The final point is especially appealing:And that’s precisely why we’re diving into it today!First things first: it’s crucial to understand that a UML kernel can run  platforms. You can layer an x86 UML kernel on top of an existing x86 kernel; as far as I know, no other configurations are supported.Next, we’ll build the UML binary. The configuration process starts with:You can configure the kernel much like you normally would. You’ll immediately notice several UML-specific options on the initial configuration page. I tend to think of these as “enlightened” drivers, designed to use the host’s userspace facilities as virtual hardware.For this demonstration, I specifically enabled the  option. The documentation explains:The User-Mode Linux port includes a driver called UBD which will let you access arbitrary files on the host computer as block devices. Unless you know that you do not need such virtual block devices, say Y here.This option wasn’t enabled by default (which surprised me a bit), so I recommend setting it to . Once you’ve finalized your configuration, building is straightforward:And this produces a  binary right there!Interestingly, it’s dynamically linked to the C standard library:To do anything meaningful within our nested kernel, we need a userspace. For simplicity, I chose to download the latest Buildroot and build it for x86/64.If you’re feeling adventurous and want to try building a minimal userspace from scratch but aren’t sure where to begin, pairing this with the micro Linux distro exercise could be a lot of fun.Running the Nested KernelTo make things interesting, I decided to provide a block device to the nested kernel, write some data to it, and then verify that data from the host system.First, let’s create the disk image:Next, we’ll format it with ext4:Now, it’s time to fire up the kernel in userspace. I’ll use the Buildroot image (an  file provided by Buildroot) as the root filesystem:And just like that, we’re greeted by a very familiar kernel boot sequence!and at the end, we have the Buildroot login:The boot process was surprisingly quick.Now, let’s create a mountpoint for our disk within the UML instance:Then, we mount the second UBD device () to this mountpoint:With the disk mounted, we can write a test file:I can now shut down the UML VM:This little experiment confirms that we successfully ran a VM using UML, wrote data to a block device within it, and those changes persisted, accessible from the host system.Throughout this article, I’ve referred to UML as a VM, and you’d be right to raise an eyebrow. On one hand, it embodies the idea of hardware virtualization via host userspace facilities, and the environment gets its own distinct kernel. On the other hand, this guest kernel is intrinsically linked to the host’s kernel. While it aims for isolation, it doesn’t achieve the same level you’d expect from a QEMU VM powered by KVM.What’s the real-world utility here? Is UML suitable for running isolated workloads? My educated guess is: probably not for most production scenarios. I believe UML’s primary strength lies in kernel debugging, rather than serving as a full-fledged, production-ready virtualization stack. For robust VM needs, KVM virtualization (operating at a different architectural layer) is far more battle-tested. Of course, containers offer another alternative if sharing the host kernel is acceptable for your workloads. UML carves out an interesting niche between these two: offering a separate kernel instance while still maintaining a unique connection to the host kernel. It’s a fascinating concept.Perhaps in the future, this intriguing technology will garner more attention and see wider adoption. For now, though, it’s a fantastic tool for experimentation and, at the very least, a lot of fun to play with!]]></content:encoded></item><item><title>OpenAI says over a million people talk to ChatGPT about suicide weekly</title><link>https://techcrunch.com/2025/10/27/openai-says-over-a-million-people-talk-to-chatgpt-about-suicide-weekly/</link><author>jnord</author><category>hn</category><pubDate>Mon, 27 Oct 2025 22:26:30 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[OpenAI released new data on Monday illustrating how many of ChatGPT’s users are struggling with mental health issues and talking to the AI chatbot about it. The company says that 0.15% of ChatGPT’s active users in a given week have “conversations that include explicit indicators of potential suicidal planning or intent.” Given that ChatGPT has more than 800 million weekly active users, that translates to more than a million people a week.The company says a similar percentage of users show “heightened levels of emotional attachment to ChatGPT,” and that hundreds of thousands of people show signs of psychosis or mania in their weekly conversations with the AI chatbot.OpenAI says these types of conversations in ChatGPT are “extremely rare,” and thus difficult to measure. That said, the company estimates these issues affect hundreds of thousands of people every week.OpenAI shared the information as part of a broader announcement about its recent efforts to improve how models respond to users with mental health issues. The company claims its latest work on ChatGPT involved consulting with more than 170 mental health experts. OpenAI says these clinicians observed that the latest version of ChatGPT “responds more appropriately and consistently than earlier versions.”Addressing mental health concerns in ChatGPT is quickly becoming an existential issue for OpenAI. The company is currently being sued by the parents of a 16-year-old boy who confided his suicidal thoughts to ChatGPT in the weeks leading up to his suicide. State attorneys general from California and Delaware — which could block the company’s planned restructuring — have also warned OpenAI that it needs to protect young people who use their products.Earlier this month, OpenAI CEO Sam Altman claimed in a post on X that the company has “been able to mitigate the serious mental health issues” in ChatGPT, though he did not provide specifics. The data shared on Monday appears to be evidence for that claim, though it raises broader issues about how widespread the problem is. Nevertheless, Altman said OpenAI would be relaxing some restrictions, even allowing adult users to start having erotic conversations with the AI chatbot.In the Monday announcement, OpenAI claims the recently updated version of GPT-5 responds with “desirable responses” to mental health issues roughly 65% more than the previous version. On an evaluation testing AI responses around suicidal conversations, OpenAI says its new GPT-5 model is 91% compliant with the company’s desired behaviors, compared to 77% for the previous GPT‑5 model.The company also says its latest version of GPT-5 also holds up to OpenAI’s safeguards better in long conversations. OpenAI has previously flagged that its safeguards were less effective in long conversations.On top of these efforts, OpenAI says it’s adding new evaluations to measure some of the most serious mental health challenges facing ChatGPT users. The company says its baseline safety testing for AI models will now include benchmarks for emotional reliance and non-suicidal mental health emergencies.OpenAI has also recently rolled out more controls for parents of children who use ChatGPT. The company says it’s building an age prediction system to automatically detect children using ChatGPT, and impose a stricter set of safeguards.Still, it’s unclear how persistent the mental health challenges around ChatGPT will be. While GPT-5 seems to be an improvement over previous AI models in terms of safety, there still seems to be a slice of ChatGPT’s responses that OpenAI deems “undesirable.” OpenAI also still makes its older and less-safe AI models, including GPT-4o, available for millions of its paying subscribers.]]></content:encoded></item><item><title>Easy RISC-V</title><link>https://dramforever.github.io/easyriscv/</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 27 Oct 2025 20:57:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The PSF has withdrawn a $1.5M proposal to US Government grant program</title><link>https://simonwillison.net/2025/Oct/27/psf-withdrawn-proposal/</link><author>lumpa</author><category>hn</category><pubDate>Mon, 27 Oct 2025 20:52:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The PSF's annual budget is less than $6m so this is a meaningful amount of money for the organization!We were forced to withdraw our application and turn down the funding, thanks to new language that was added to the agreement requiring us to affirm that we "do not, and will not during the term of this financial assistance award, operate any programs that advance or promote DEI, or discriminatory equity ideology in violation of Federal anti-discrimination laws."Our legal advisors confirmed that this would not just apply to security work covered by the grant - this would apply to all of the PSF's activities.This was not an option for us. Here's the mission of the PSF:The mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers.If we accepted and spent the money despite this term, there was a very real risk that the money could be clawed back later. That represents an existential risk for the foundation since we would have already spent the money!I was one of the board members who voted to reject this funding - a unanimous but tough decision. I’m proud to serve on a board that can make difficult decisions like this.Posted 27th October 2025 at 8:32 pm]]></content:encoded></item><item><title>Study finds growing social circles may fuel polarization</title><link>https://phys.org/news/2025-10-friends-division-social-circles-fuel.html</link><author>geox</author><category>hn</category><pubDate>Mon, 27 Oct 2025 19:06:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Creating an all-weather driver</title><link>https://waymo.com/blog/2025/10/creating-an-all-weather-driver</link><author>boulos</author><category>hn</category><pubDate>Mon, 27 Oct 2025 18:57:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Life doesn't freeze when winter comes—if anything, that's when riders need reliable transportation most, when being exposed to the elements becomes less appealing. Today, the Waymo Driver successfully navigates rain, fog, sandstorms, and freezing temperatures. As we expand to more cities across the U.S. and globally, we're applying the same systematic, scientific approach that enabled us to validate the Waymo Driver for these conditions to advance our capabilities for snowier, winter weather.Our proven, safety-guided methodology involves four key steps:Understanding the ChallengeSnow isn't a single phenomenon—it's a spectrum of conditions that can affect a human or autonomous driver in multiple ways. Atmospheric conditions can range from a light dusting to a complete whiteout, while road surfaces may be snow-covered or have icy patches, and environmental factors like snow buildup along roadsides add further complexity. For years, we've been advancing our system in some of the snowiest conditions across the country —regularly driving in Upstate New York, Michigan's Upper Peninsula, and the Sierra. We've amassed tens of thousands of miles in diverse, snowy conditions. This has allowed the Waymo Driver’s AI to learn from real driving experience and train to navigate a wide range of winter weather.Defining the different types of winter road conditions from icy streets (right) and well-plowed roads (second from the right) to tire tracks and light dustings to falling snow and slushy streets (right).Designing Generalizable SolutionsAt Waymo, we're building one autonomous system that works across diverse conditions—the same Waymo Driver navigating foggy San Francisco can navigate snowy Denver. Our  is informed by over 100 million fully autonomous miles of driving experience, combining state-of-the-art hardware and AI to adapt to and sustain fully autonomous operations in cities with harsher weather.The Waymo Driver uses cameras, radar, and lidar to perceive the world around it, with each sensor providing a complementary field of view that's especially helpful in inclement weather. Its automated cleaning system –using clever engineering and heating elements – keeps the sensors clear so the vehicle can continue serving riders without needing to pull over.Our system provides context not only about where it's operating, but also about the conditions it’s operating under. We're creating state-of-the-art AI, building on top of our existing models with richer inputs and advanced capabilities designed to navigate winter conditions. For example, our AI can distinguish between where there's snow, slush, ice, and normal road surface. The Waymo Driver then uses this information to adjust its driving behavior to match the road conditions in real-time, allowing the Waymo Driver to navigate based on what it sees (and feels), also inferring insights from other road users—adapting to blocked roads, detours, and changing surface conditions. When the system detects lower traction, it automatically adjusts its speed, acceleration, and braking. Each vehicle essentially acts as a mobile weather station, gathering data to inform its own driving decisions and share with the rest of the fleet in the city. These responses are consistent and thoroughly tested, providing predictable and safe navigation in challenging conditions.Rigorously Validating Our CapabilitiesWe validate our generalizable system through real-world driving, closed-course testing, and large-scale simulation. With our growing operations in snowy cities like Detroit, Denver, and Washington D.C., in addition to visits  to other areas, we're deepening our understanding of winter weather conditions and validating our capabilities. At closed-course testing facilities, we push the system to its limits in controlled environments, teaching it to recognize and respond to extreme scenarios like losing traction on ice. Then, we expand our learning year-round through simulation, long after the last snowflake has melted, so the Waymo Driver is prepared for rare and unusual events, like once-in-100-year snow New Orleans experienced this past winter.Waymo testing its hardware and AI through a combination of structured testing (left), simulation (middle), and real-world driving (right).Once we've validated our technology and operations by our Safety Framework and high caliber for rider excellence, we expand our service with clear guidelines about when our vehicles will operate based on local conditions. As we scale, we're also refining our operations to support winter service—from keeping our fleet clean and charged in freezing temperatures to optimizing the rider experience. Winter weather is complex, but we're committed to providing reliable service when riders need it most. As we continue expanding to more cities around the world, our progress is guided by safety, and riders can trust that the Waymo Driver is ready when we open our doors.Looking for an all-weather Driver instead of all-weather tires?  Follow along on our progress to bring Waymo to more cities at waymo.com/updates.]]></content:encoded></item><item><title>MCP-Scanner – Scan MCP Servers for vulnerabilities</title><link>https://github.com/cisco-ai-defense/mcp-scanner</link><author>hsanthan</author><category>hn</category><pubDate>Mon, 27 Oct 2025 17:18:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The new calculus of AI-based coding</title><link>https://blog.joemag.dev/2025/10/the-new-calculus-of-ai-based-coding.html</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 27 Oct 2025 17:17:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Over the past three months, a team of experienced, like-minded engineers and I have been building something really cool within Amazon Bedrock. While I'm pretty excited about what we are building, there is another unique thing about our team  - most of our code is written by AI agents such as Amazon Q or Kiro. Before you roll your eyes: no, we're not vibe coding. I don't believe that's the right way to build robust software.Instead, we use an approach where a human and AI agent collaborate to produce the code changes. For our team, every commit has an engineer's name attached to it, and that engineer ultimately needs to review and stand behind the code. We use steering rules to setup constraints for how the AI agent should operate within our codebase, and writing in Rust has been a great benefit. Rust compiler is famous for focusing on correctness and safety, catching many problems at compile time and providing helpful error messages that help the agent iterate. As a juxtaposition to vibe coding, I prefer the term "agentic coding." Much less exciting but in our industry boring is usually good.For me, roughly 80% of the code I commit these days is written by the AI agent. My personal workflow: break down the task until I have clarity in my own head (often using AI to explore approaches), prompt the AI agent, review its output, iterate with it until I like the results, and occasionally take over the change set and finish it myself. I pay attention to every line of code the agent produces, and don't accept them until I am fully satisfied with the quality of what is being produced - no different than if I wrote every line myself.I've always been an efficient coder, finding time to write code even in limited amount of time I've been able to dedicate to coding over the past few years. And the last few months have been the highest coding throughput months of my career. My team is no different—we are producing code at 10x of typical high-velocity team. That's not hyperbole - we've actually collected and analyzed the metrics.Here's where it gets interesting. A typical software team, even an experienced one, doesn't get things right all the time. Even with good testing and engineering practices, bugs occasionally make it through. We've all heard the phrase "testing in production." That reality is the main reason I've always believed that focusing on testing alone is not enough, and investing in blast radius and time to recovery is just as important.AI assisted code is no different, it may contain bugs even when thoroughly reviewed by a human, and I suspect the probabilities are not significantly different.  However, when teams ship commits at 10x the rate, the overall math changes. What used to be a production impacting bug once or twice a year, can become a weekly occurrence. Even if most bugs get caught in integration or testing environments, they will still impact the shared code base, requiring investigation and slowing the rest of the team down. Once again, this is not just hyperbole—our team sees signs that these are the challenges that pop up with a step function increase in throughput.I am increasingly convinced that in order for agentic development to increase engineering velocity by an order of magnitude, we need to decrease the probability of problematic commits by an order of magnitude too. And likely by even more than that, since at high velocities individual commits can begin interacting with each other in unexpected ways too.In other words, driving at 200mph, you need a lot of downforce to keep the car on the track!The Cost-Benefit RebalanceOne of the best ways to reduce the chance of bugs is to improve testing. I'm an airplane geek, and have always admired the testing ideas used by the airplane manufacturers. From early simulations, to component testing, to wind tunnel testing, to testing to breaking point, and ultimately test flights of fully assembled aircraft. Even flight simulators play a role in improving the overall safety of the industry. Some of these ideas have been tried in the software industry, but they are far from ubiquitous. As an example, I've always liked "wind tunnel" style tests, that test fully assembled system in a controlled environment. To achieve that, one pattern I've used is implementing high fidelity "fake" versions of external dependencies that can be run locally. If you do that, you can then write build-time tests that run locally and verify end-to-end behavior of the whole system. You can even inject unexpected behaviors and failures into fake dependencies, to test how the system handles them. Such tests are easy to write and execute because they run locally, and they are great at catching those sneaky bugs in the seams between components.Unfortunately, faking all the external dependencies isn't always easy for a service with moderate level of complexity. And even if you do, you now have to own keeping up with the real dependencies as they evolve. For those reasons, in my experience most teams don't write such tests.I think we are seeing early signs that agentic coding can change the calculus here. AI agents are great at spitting out large volumes of code, especially when the desired behavior is well known and there's little ambiguity. Ideas that were sound in principle, but too expensive to implement and maintain just had their costs decrease by an order of magnitude. I really love riding such shifts in the industry, because they open the doors to new approaches that weren't practical in the past.Our project (with the help of an AI agent) maintains fake implementations of external dependencies like authentication, storage, chain replication, and inference engine to be used in tests. We then wrote a test harness that uses those fakes to spin up our entire distributed system, including all the micro-services, on developers' machines. Build-time tests then spin up our canaries against that fully assembled stack verifying the system as a whole works. I'm really bullish on this approach catching a category of bugs that in the past could only be caught once the change was committed and made it to the test environment. A few years ago, ideas like these would receive resistance as nice, but too expensive. This time around, it took just a few days to implement for a relatively complex system.Driving Fast Requires Tighter Feedback LoopAt the end of the day, all these changes need to be built, tested, and deployed in order to bring customer value. It's not uncommon for software teams to have a CICD pipeline that takes several hours to build, package, and then test software changes. That pipeline can then take a few days to roll a batch of changes across pre-prod stages and eventually production stages. Typically, a team with that level of automation would be considered a healthy team.Agentic coding changes that dynamic. In the amount of time it takes to build, package, and test one set of commits, another dozen might be waiting to go out. By the time a change set is ready to deploy to production, it may contain 100 or more commits. And if one of those commits contains a problem, the deployment needs to be rolled back grinding the pipeline to a halt. In the meantime, even more changes accumulate, adding to the chaos and the risk.  I'm a Formula 1 fan, and this reminds me of how an accident on the track can cause a Yellow Flag to be raised. Normally, the cars zoom around the track at immense speeds and accelerations. But if an accident occurs, the race marshals raise a yellow flag, which requires all the cars to slow down behind the pace car. An exciting race turns into a leisurely drive around the track until the debris is cleaned up and the track is safe again. To minimize such slow downs, race organizers go to great lengths to prepare for all types of accidents, and make sure they can clean up the track and restart the race in minutes.Just like whole-system local tests help tighten the feedback loop for catching certain bugs, we may need to think similarly about how we implement our CICD pipelines. When teams are moving at the speed of dozen of commits per hour, problematic issues will need to be identified, isolated, and reverted in minutes instead of hours or days. That means that a typical build and test infrastructure will need to become an order of magnitude faster than it is today. Just like online video games become unplayable when there is high lag between player's inputs and the game's reaction, it's really hard to move 10x faster if every commit still requires a lengthy delay before you see the feedback. The communication bottleneckI enjoy observing well-run operations. If you've ever peeked behind the curtain of a busy restaurant, then at first sight you may think it's chaos. But if you take a second to notice the details, you'll see that all members are constantly coordinating with each other. Chefs, cooks, wait staff, bussers, and managers pass information back and forth in a continuous stream. By staying in constant sync, a well run restaurant manages to serve its patrons even during peak times, without sacrificing on quality or latency.I believe that achieving similar increase in velocity for a software team requires constraints on how teams communicate. When your throughput increases by an order of magnitude, you're not just writing more code - you're making more decisions. Should we use this caching strategy or that one? How should we handle this edge case? What's the right abstraction here? At normal velocity, a team might make one or two of these decisions per week. At 10x velocity, they are making multiple each day.The challenge is that many of these decisions impact what others are working on. Engineer A decides to refactor the authentication flow, which affects the API that Engineer B is about to extend. These aren't just implementation details - they're architectural choices that ripple through the codebase.I find that traditional coordination mechanisms introduce too much latency here. Waiting for a Slack response or scheduling a quick sync for later in the day means either creating a bottleneck - the decision blocks progress - or risking going down the wrong path before realizing the conflict. At high throughput, the cost of coordination can dominate!One approach is to eliminate coordination - if everybody works on independent components, they are unlikely to need to coordinate. But I find that ideal impractical in most real-world systems. So another alternative is to significantly decrease the cost of coordination. Our team sits on the same floor, and I think that's been critical to our velocity. When someone needs to make a decision that might impact others, they can walk over and hash it out in minutes in front of a whiteboard. We align on the approach, discuss trade-offs in real time, and both engineers get back to work. The decision gets made quickly, correctly, and without creating a pile-up of blocked work.I recognize this doesn't solve the problem for distributed teams—that remains an open challenge.I'm really excited about the potential of agentic development. I think it has the capability to not only improve the efficiency of software development, but also allow us to tackle problems that were previously too niche or expensive to solve. The gains are real - our team's 10x throughput increase isn't theoretical, it's measurable.But here's the critical part: these gains won't materialize if we simply bolt AI agents onto our existing development practices. Like adding a turbocharger to a car with narrow tires and old brakes, the result won't be faster lap times - it will be crashes. At 10x code velocity, our current approaches to testing, deployment, and team coordination become the limiting factors. The bottleneck just moves.This means we need to fundamentally rethink how we approach building software. CICD pipelines designed for 10 commits per day will buckle under 100. Testing strategies that were "good enough" at normal velocity will let too many bugs through at high velocity. Communication patterns that worked fine before will create constant pile-ups of blocked work.The good news is that we already have great ideas for comprehensive testing, rapid deployment, and efficient coordination - ideas that have shown promise but haven't seen wide adoption because they were too expensive to implement and maintain. What's changed is that agentic development itself can dramatically lower those costs. The same AI agents that are increasing our code throughput can also help us build the infrastructure needed to sustain that throughput.This is the real opportunity: not just writing more code faster, but using AI to make previously impractical engineering practices practical. The teams that succeed with agentic development will be the ones who recognize that the entire software development lifecycle needs to evolve in concert.]]></content:encoded></item><item><title>Avoid 2:00 and 3:00 am cron jobs (2013)</title><link>https://www.endpointdev.com/blog/2013/04/avoid-200-and-300-am-cron-jobs/</link><author>pera</author><category>hn</category><pubDate>Mon, 27 Oct 2025 17:08:33 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Avoid 2:00 and 3:00 am cron jobs!A word to the wise: Do not set any cron jobs for 2:00 am or 3:00 am on Sunday morning! Or to be safe, on other mornings besides Sunday as well, since jobs originally set to run on some particular day may eventually be changed to run on another day, or every day.Most of the time such cron jobs will run fine, but if they run every Sunday morning, then twice per year they will run at the exact time daylight savings time (aka summer time) kicks in or ends, sometimes with very strange results.On Linux with vixie-cron we saw two cron jobs run something like once per second between 3:00 and 3:01 when the most recent daylight savings time began. Thus they ran about 60 times, stepping all over each other and making a noisy mess in email. No serious harm was done, but that’s only because they were not tasks capable of causing serious harm.Feel free to wish for or agitate for or fund or write a better open source job scheduler that everyone will use, one that will ensure no overlapping runs, allow specifying time limits, etc. Better tools exist, but until one of them achieves cron’s level of ubiquity, we have to live with cron at least some places and sometimes.Alternatively, where possible set the server timezone to UTC so that no daylight savings changes will happen at all.Or most preferable: Governments of the world, stop the twice-yearly dance of daylight saving time altogether.But in the meantime this particular problem can be entirely avoided by just not scheduling any cron jobs to run on Sunday morning at 2:00 or 3:00 server time.]]></content:encoded></item><item><title>Why Busy Beaver hunters fear the Antihydra</title><link>https://benbrubaker.com/why-busy-beaver-hunters-fear-the-antihydra/</link><author>Bogdanp</author><category>hn</category><pubDate>Mon, 27 Oct 2025 16:56:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In the summer of 2024, I reported on an online community that nailed down the precise value of a number called BB(5) — the first big breakthrough in 50 years on an old problem in theoretical computer science known as the busy beaver game. BB(5), now known to be 47,176,870, is the fifth of the so-called busy beaver numbers, which measure the complexity of the craziest computations that simple computer programs can complete.The next step in this idiosyncratic research effort is to identify the sixth busy beaver number BB(6), and there has been some notable progress on that front — I wrote a follow-up story about it a few months ago. But busy beaver researchers don’t expect to nail down the true value of BB(6) any time soon. That’s because doing so would require them to understand the behavior of a program with the awesome name “Antihydra,” which resembles a longstanding open problem in mathematics called the Collatz conjecture. A twitter user sharing my first busy beaver story summed up this state of affairs more succinctly:Both of my stories alluded to the Antihydra barrier only very briefly. In this blog post I will explore it in more detail: What exactly is Antihydra, what is the Collatz conjecture, how are they connected, and what makes them so daunting?If you haven’t already read my two  stories about the busy beaver game, I recommend doing so before reading further, mainly just because they’re both really fun! Here I’ll recap how the busy beaver game works so that we’re all on the same page.I wrote above that the busy beaver numbers “measure the complexity of the craziest computations that simple computer programs can complete.” To define them more precisely, we first need a mathematical framework for gauging the complexity of computer programs themselves, to decide which ones are “simple.” Then we need a way to quantify the complexity of computations — what computer programs  — so that we can identify the craziest ones.In the busy beaver game, computer programs are represented by hypothetical devices called Turing machines, which compute in discrete steps by reading and writing 0s and 1s on an infinite tape divided into cells. A unique list of rules governs the behavior of each Turing machine. Anything you can do with an ordinary computer program, you can in principle do with the right set of Turing machine rules. “In principle” is doing a lot of work in this sentence — even if you managed to acquire the requisite infinite tape, computing with a Turing machine would be horrendously inefficient. But Turing machines are easier to analyze theoretically than more practical programming languages.Let’s unpack how Turing machines work in a bit more detail. At each step, a Turing machine consults one of its rules and edits one cell on the tape. Each rule has two cases: what to do if the current cell contains a 0, and what to do if it contains a 1. “What to do” here means what to write in the current cell, which direction to move next, and which rule to consult for the next step. One case of one rule breaks this pattern: It tells the Turing machine to “halt,” or stop running. But by itself, the existence of this instruction doesn’t guarantee that a Turing machine will halt — the machine might never get there. ’s visual designer Kristina Armitage encapsulated all of this in a beautiful infographic.The number of rules that a Turing machine has will be our measure of program complexity. This choice lets us replace our vague question about the craziest things that simple computer programs can do with a series of specific questions about different degrees of craziness, corresponding to different busy beaver numbers. You learn the value of BB(1) by answering the question “what’s the most complex computation that a one-rule Turing machine can complete?” Likewise, BB(2) measures the most complex computation that a two-rule machine can complete, and so on.To answer these questions, we need a precise definition of what makes one computation more complex than another. A natural measure is how many steps the Turing machine needs to complete the computation. “Complete” is important — every Turing machine that never halts will run for infinitely many steps, but that’s not really a fair comparison. The number of steps that a Turing machine takes before halting (and indeed, whether it halts at all) can depend on the initial pattern of 0s and 1s on the tape. For the busy beaver game, we always start from the so-called “blank tape,” which has 0s in every cell.We now have all the necessary pieces to formally define the busy beaver numbers. Let’s take BB(6) to be specific: It is the longest finite runtime among all six-rule Turing machines, when those machines start with a blank tape. Finding this number is straightforward in principle. First, list out all possible six-rule Turing machines. Next, sort them into two categories: those that will eventually halt when they start running on the blank tape, and those that will run forever. Toss out all the non-halting machines. Finally, measure how many steps each of the halting machines takes before stopping. The largest number is BB(6).The problem with this plan lies in the second step, where you divide the Turing machines into two groups based on whether or not they halt. It turns out that deciding whether a Turing machine will halt can be an extremely hard problem, to put it mildly. And if you can’t tell whether a given machine will halt, then you don’t know whether your list of halting Turing machines is complete, so you can’t know whether you’ve found the longest runtime! As of this writing, researchers have classified the vast majority of six-rule machines as either halting or non-halting. But there are 1,618 “holdouts” whose fate remains unknown.Antihydra is one of these holdout machines. To nail down the value of BB(6), researchers must first determine whether Antihydra halts, and that seems to be beyond the reach of any known mathematical technique. To understand why, we need to take a step back and ask, “what exactly are these Turing machines doing?”You may object at this point that we already know exactly what these Turing machines are doing: Each one is just following a specific sequence of rules, writing 0s and 1s on the tape as it goes. But this “low-level” description is a bit like saying “when I push these buttons, my pocket calculator toggles transistors on and off in this specific pattern.” That may very well be true, but “high-level” descriptions like “when I push these buttons, my pocket calculator multiplies 3 and 4” are usually more useful.There’s no guarantee that any given Turing machine’s behavior admits such a simple high-level description. But remember that Turing machines can carry out all possible computations — that means that at least  Turing machines must be executing programs with high-level descriptions that humans can understand.Actually, the most notable five- and six-rule Turing machines that busy beaver researchers have studied so far all have relatively simple high-level descriptions — that includes the longest-running five- and six-rule machines that eventually halt, the mostcomplex non-halting five-rule machines, and holdouts like Antihydra.Let’s look at a specific example. The fifth busy beaver, which runs for 47,176,870 steps before halting, obeys the following low-level rules:In 1993, the mathematician Pascal Michel proved that these rules are equivalent to a simple high-level program:Divide \(x\) by 3 and check the remainder.
If the remainder is 0, calculate \((5x + 18)/3\). The result is your new value of \(x\).If the remainder is 1, calculate \((5x + 22)/3\). The result is your new value of \(x\).If the remainder is 2, halt.If you haven’t halted, go back to step 2 and plug in the new value of \(x\).Once you have a high-level description like this, you can use it to determine whether the machine will halt — and if so, exactly how many steps it will take. In this case, the high-level program just repeatedly plugs in new values of \(x\) until it finds one that leaves a remainder of 2 when divided by 3. One third of numbers have this property, so you might guess that the program will take three tries to find one, give or take a few. If you start from a random value of \(x\), you’ll find that three iterations is indeed typical. But it turns out that if you start from \(x = 0\), this program will repeat the second step 15 times before it lands on a number with remainder 2! Busy beaver researchers often like to anthropomorphize the Turing machines they study, imagining that the machines are actively trying to run for as long as possible. Adopting that perspective, we might say that this Turing machine got very lucky.The fifth busy beaver is just one member of a family of “Collatz-like” Turing machines whose high-level behavior has the following general form:Set \(x\) equal to some starting value (which may or may not be 0).Divide \(x\) by a fixed number \(N\). The remainder tells you what formula to use to get your new value of \(x\).Check if you’ve met a specific halting condition. If not, go back to step 2 with the new value of \(x\).The family of Collatz-like Turing machines includes both halting and non-halting machines. It gets its name from a procedure for generating number sequences devised in 1937 by the mathematician Lothar Collatz:Choose a starting value for \(x\).Check whether \(x\) is even or odd.
If it’s even, calculate \(x/2\). The result is your new value of \(x\).If it’s odd, calculate \(3x + 1\). The result is your new value of \(x\).Check whether \(x = 1\). If not, go back to step 2.This looks very similar to our general description of high-level behavior for Collatz-like machines, with \(x = 1\) as the halting condition.Try iterating these rules from any initial integer value of \(x\) — I’m willing to bet however much you like that you’ll eventually hit 1. The Collatz conjecture asserts that this happens for every positive integer, no matter how large. People have tested this empirically for all integers up to at least 2 billion trillion (!) without finding any counterexamples, which strongly suggests that the conjecture is true. But nobody knows how to rigorously prove it.Let’s take a step back. At the beginning of this post I noted a link between the Collatz conjecture and Antihydra: Nobody knows how to prove the Collatz conjecture, and that’s why researchers don’t know how to conclusively determine whether Antihydra halts. But now I’ve instead linked the Collatz conjecture to the fifth busy beaver, a machine that  been proved to halt. What’s going on here?The resolution to this apparent puzzle is that for the busy beaver game, we only care about whether a Turing machine halts when it starts running from a specific tape configuration, namely the blank tape. That means we only care about whether the corresponding Collatz-like sequence halts for a single input. The Collatz conjecture, meanwhile, asks whether you eventually hit \(x = 1\) for  input. It’s easy to show that the Collatz sequence ultimately hits \(x = 1\) for any one input, just as it’s easy to show that the fifth busy beaver halts (once you’ve established an equivalence between its low-level rules and the high-level Collatz-like program).We can easily construct a variant of the Collatz problem that’s hard to solve even for a single input. All we need to do is change the \(3x + 1\) rule for odd numbers to \(5x + 1\). In that case, trajectories that start from certain inputs (such as \(x = 7\)) look like they will diverge, never hitting 1 or falling into a cycle. But researchers haven’t been able to prove that any of these trajectories diverges. There’s an inherent asymmetry here. If you want to prove that a sequence  eventually end up somewhere, you can always just use brute force, at least in principle. But if you want to prove that a sequence never terminates, even a single input can be hard.We’re now finally ready to confront the terror that is Antihydra. It obeys the following high-level rules:Check whether \(x\) is even or odd.
If it’s even, calculate \(3x/2\). The result is your new value of \(x\). Add one to a running tally of how many times you’ve applied this even rule.If it’s odd, calculate \((3x-1)/2\). The result is your new value of \(x\). Add one to a running tally of how many times you’ve applied this odd rule.Check whether your “odd” count is more than twice as large as your “even” count. If so, halt. If not, go back to step 2.This is a very curious set of rules. The formulas \(3x/2\) and \((3x-1)/2\) don’t appear to systematically favor odd or even numbers, so you might expect that iterating them again and again will look like repeatedly flipping a coin and keeping track of how often you get heads versus tails. Early on in a sequence of coin flips, it’s distinctly possible that you’ll end up with more than twice as many heads as tails. But if this doesn’t happen right away, it becomes less and less likely the longer you keep going. Researchers have now simulated the behavior of Antihydra out to more than 270 billion steps, and as expected, the “even” and “odd” tallies are pretty close to equal — nowhere near the extreme imbalance demanded by the halting condition. So it seems overwhelmingly likely that Antihydra never halts. But nobody knows how to prove it! The mathematician John Conway coined the delightful term “probviously” for situations like this — ones where the specific problem of interest is very hard to solve, but probabilistic reasoning about the “typical” behavior of similar problems makes the answer seem obvious.Antihydra’s behavior is qualitatively similar to the \(5x + 1\) version of the Collatz conjecture, where we don’t know how to prove that any single trajectory diverges. I want to stress that as far as researchers know, there isn’t a more precise mathematical link between these two problems: If you resolved one of them, it wouldn’t automatically resolve the other. But the problems seem hard for very similar reasons. If someone does manage to prove the Collatz conjecture, the mathematical techniques used in the proof would likely be promising for the Antihydra problem (and vice versa).Actually, Antihydra is just one of many probviously non-halting Turing machines with Collatz-like behavior. Busy beaver hunter Shawn Ligocki dubbed these machines “cryptids” when they were first identified in variants of the standard busy beaver game.The first two cryptids to be discovered were named Bigfoot and Hydra; researchers have now identified so many cryptids that it no longer makes sense to give each one its own name. The existence of all these cryptids implies that busy beaver numbers beyond BB(5) will remain out of reach until researchers develop new mathematical tools for tackling Collatz-like problems. And the legendary mathematician Paul Erdős reportedly said “Mathematics may not be ready for such problems.”But that doesn’t mean busy beaver hunters should give up. There’s still plenty of questions to explore in what might be called “cryptid ecology.” How many subspecies of cryptids are there? How are they related to each other, and to other unsolved problems in mathematics beyond the Collatz conjecture? Since the beginning of the busy beaver game, avid hunters have repeatedly encountered surprising new Turing machine behavior, and that pattern shows no sign of letting up.This past August I visited Tahquamenon Falls in Michigan’s upper peninsula, a part of the state that’s apparently an epicenter of bigfoot sightings. Fortunately I didn’t encounter any cryptids, but I did learn some new things about a few friendlier critters. Surprising discoveries can come from anywhere!]]></content:encoded></item><item><title>JetKVM – Control any computer remotely</title><link>https://jetkvm.com/</link><author>elashri</author><category>hn</category><pubDate>Mon, 27 Oct 2025 16:44:17 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Fnox, a secret manager that pairs well with mise</title><link>https://github.com/jdx/mise/discussions/6779</link><author>bpierre</author><category>hn</category><pubDate>Mon, 27 Oct 2025 16:29:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: JSON Query</title><link>https://jsonquerylang.org/</link><author>wofo</author><category>hn</category><pubDate>Mon, 27 Oct 2025 16:22:52 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Claude for Excel</title><link>https://www.claude.com/claude-for-excel</link><author>meetpateltech</author><category>hn</category><pubDate>Mon, 27 Oct 2025 16:09:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Claude for Excel is currently in beta as a research preview, so it’s best for model analysis, assumption updates, error debugging, template population, formula explanations, multi-tab navigation. Claude doesn’t have advanced Excel capabilities including pivot tables, conditional formatting, data validation, data tables, macros, and VBA. We’re actively working on these features.]]></content:encoded></item><item><title>Show HN: Erdos – open-source, AI data science IDE</title><link>https://www.lotas.ai/erdos</link><author>jorgeoguerra</author><category>hn</category><pubDate>Mon, 27 Oct 2025 16:08:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Fast, accurate Jupyter notebook editsErdos lets data scientists create, edit, and iterate on Jupyter notebooks faster and more accurately than ever before.]]></content:encoded></item><item><title>More than DNS: Learnings from the 14 hour AWS outage</title><link>https://thundergolfer.com/blog/aws-us-east-1-outage-oct20</link><author>birdculture</author><category>hn</category><pubDate>Mon, 27 Oct 2025 15:56:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[On Monday the AWS us-east-1 region had its worst outage in over 10 years. The whole thing lasted over 14 hours and affected 140 AWS services, including, critically, EC2. SLAs were blown, an eight-figure revenue reduction will follow. Before Monday, I’d spent around 7 years in industry and never personally had production nuked by a public cloud outage. I generally regarded AWS’s reliability as excellent, industry-leading.A number of smart engineers have come to this major bust-up and covered it with the blanket of a simple explanation: brain drain; race condition; it’s always DNS; the cloud is unreliable, go on-prem. You’re not going to understand software reliability if you summarize an outage of this scale in an internet comment. Frankly, I’m not even going to understand it after reading AWS’s 4000 word summary and thinking about it for hours. But I’m going to hold the hot takes and try.I wrote Modal’s internal us-east-1 incident postmortem before AWS published their “service disruption summary”: https://aws.amazon.com/message/101925. Our control plane being in us-east-1, we got hit hard. Along with hundreds of other affected companies, we’re interested in a peek under the hood of the IaaS we depend on.Arriving a few days after the outage, this public summary is a small window into the inner workings of the most experienced hyperscaler engineering operation in the world. I’ll analyze each of the three outage phases, call out key features, and then try, with limited information, to derive a lesson or two from this giant outage. Before proceeding, it is recommended to read the summary carefully.Out of one service outage, one hundred and forty service outages are bornHow did a DynamoDB service failure at 6:48AM UTC October 20th become a 140 service failure epidemic?AWS breaks down their summary into dedicated sections for the DynamoDB, EC2, and Network Load Balancer (NLB) services. They add a final section lumping together the other 137 affected services (which included Lambda, IAM, STS, Elasticache, ECR, Secrets Manager).This document structure is suitable as it matches the structure of the outage.DynamoDB precipitated all other service failures because it is used by EC2 and caused EC2 to go down, or because a service depended on DynamoDB directly. EC2 and DynamoDB are used extensively within AWS for service implementation, thus the wildfire spread to around 70% of all AWS services in the us-east-1 region.It is widely known that AWS dogfoods its own cloud services, e.g. DynamoDB, for the implementation of Amazon.com other AWS services. DynamoDB and EC2 are ‘layer one’ foundation services within AWS. If they go down, basically everything else does.The root cause of this issue was a latent race condition in the DynamoDB DNS management system that resulted in an incorrect empty DNS record for the service’s regional endpoint (dynamodb.us-east-1.amazonaws.com) that the automation failed to repair.In an eye straining 786 word mega-paragraph, they outline their issues, which I will try summarize.To maintain population of all DNS entries for dynamodb.us-east-1.amazonaws.com, they run three DNS Enactors, one in each of , , . Each of these three Enactors performs mutations  coordination.For resiliency, the DNS Enactor operates redundantly and fully independently in three different Availability Zones (AZs).One of these Enactors, say , became . They don’t say anything about the cause of the latency, but I believe it was extreme (10-100x) because the system design seems to allow for some deviation from mean latency.They use a typical “keep last N” garbage collection mechanism to remove old DNS plans. We also do this at Modal to garbage collect old machine images. Crucially, the last N must  include an active resource. I assume the DynamoDB team picked a large N to ensure they ‘never’ delete an active plan, which implies the Enactor’s latency in  was extreme.To a first approximation, we can say that accidents are almost always the result of incorrect estimates of the likelihood of one or more things. — Why you should read accident reports, Holloway, C. MichaelThe plan being applied by the slow Enactor fell out of the  generations safety window and became old, eligible for deletion. It was the old plan deletion which turned a ‘stale DNS plan’ degradation into a full-blown ‘zero DNS entries’ outage.Now, back to the TOCTOU issue. Why is an Enactor allowed to make only one plan staleness check for N plan mutations? I suspect it’s because querying the DNS Planner for staleness is magnitudes more expensive than making a plan mutation. It would be non-performant, and  unnecessary to make N checks for N fast plan mutations.Without this TOCTOU bug, there’s no ‘stale DNS plan’ degradation and thus no opportunity to mistakenly delete an active plan.We’re at two faults so far, but there’s more. Using the Swiss cheese model of accident causation we can pass through a couple more Emmental holes.Deleting the active plan is a disaster, but the Enactor’s cleanup phase didn’t check for it. This absent guard appears to be another fault.Some have pointed out that the Enactor deleting the Planner’s plans is weird, but I think it makes sense. The Planner is allowed to be a straightforward append-only system of outputs. The Enactor makes forward progress against the plan log and maintains the window of active DNS records. If the Planner is deleting plans, it’s also making writes against Route53.After deleting the ‘active’ plan, the Enactor state was corrupted and unrecoverable without “manual operation intervention”, which took over 2 hours to be done.It is surprising that this corruption and DNS ‘empty state’ was not auto-recovered. dynamodb.us-east-1.amazonaws.com had  IP addresses associated. In production that’s a pants-on-fire situation. Could the Enactors not have fallen back to some useful, non-empty state and restored partial service?Anyone considering this system failure outside the DynamoDB team has the struggle of keyhole observation added to the problem of hindsight bias. I won’t have the temerity to suggest remediation, or declare a root cause. But there is much that is familiar in the DynamoDB DNS failure, and I’ll be looking for it in future design documents.The selection of events to include in an event chain is dependent on the stopping rule used to determine how far back the sequence of explanatory events goes. Although the first event in the chain is often labeled the initiating event or root cause , the selection of an initiating event is arbitrary and previous events and conditions could always be added. — Engineering a safer worldHaving read the DynamoDB section of the summary, some are tempted to declare discovery of the root cause: a race condition.The software industry today holds the root cause analysis (RCA) as a primary activity of postmortem write ups. Google “postmortem template” and almost every offered template includes a section for root cause(s) analysis. The top result, Atlassian’s, includes it. A couple years ago Atlassian’s is what I found and copied as Modal’s internal template.But leading reliability engineers have moved on from centering root cause analysis. It is a useful but inadequate model of incident occurrence.Most obviously, RCA has an infinite regress problem. The cause of the extreme Enactor latency in one AZ is unexplained, but it is is antecedent to the race condition and could be considered a root cause. But, say the latency was causes by high packet drop, what caused  On and on we go, boats against the current—More interestingly, however, is the myopia induced by RCA. Yes, the extreme latency triggered the race condition bug. It was a , but it is just one of many latent faults that could emerge from the dynamics of the DynamoDB system. And as shown above by the Swiss cheese analysis, multiple control mechanisms combined into an unrecoverable failure once the latency emerged.Today’s leading distributed systems engineers, including the SRE’s at Google, analyze failure as a control problem.Instead of asking “What software service failed?” we ask “What interactions between parts of the system were inadequately controlled? — The Evolution of SRE at GoogleLooking for a control problem, we see not just the race condition deletion, but the latency, the garbage collection, the guarding (or lack thereof), the state corruption, the alerting, and the human operator.This incident summary contains perhaps the first public reference to an  and the DropletWorkflow Manager (DWFM). Droplets are physical servers upon which all EC2 instances run.The DWFM depends on DynamoDB to complete “state checks”, which are heartbeats between DWFM and every physical server managed. If the heartbeat with a server stops, the DWFM’s control of the server is cut off. Without this control, no creation or state transition can occur on an EC2 instance, affecting every EC2 user in us-east-1 except those which left all their instances  between 6:48AM and 8:50PM UTC. Yikes.DynamoDB is a critical dependency of the DWFM and triggered the EC2 service failure. But it gets interesting once DynamoDB recovers around 9:40AM UTC. EC2 is down or degraded for .In normal operation of EC2, the DWFM maintains a large number (~10^6) of active leases against physical servers and a very small number (~10^2) of broken leases, the latter of which the manager is actively attempting to reestablish.The DynamoDB outage, which lasted almost 3 hours, caused widespread heartbeat timeouts and thus thousands of leases broke. I’d estimate that the number of broken leases reached at least 10^5, or three OOMs larger than normal.With a huge queue of leases to reestablish, the DWFM system has two possible transitions. One is a slow, gradual burn down of the lease backlog (recovery). The other is a , where the lease queue remains high (a “sustaining effect”) until manual intervention.Unfortunately, the DWFM entered congestive collapse.…, due to the large number of droplets, efforts to establish new droplet leases took long enough that the work could not be completed before they timed out. — AWS SummaryIt’s most interesting here to consider why collapse occurred. Was the unit of work completion somehow , ie. instead of processing one lease at a time the DWFM processes an all-or-nothing block of leases? Or, did the DWFM become a thundering herd and overwhelm a downstream component between it and the Droplets?The congestive collapse of EC2 could only be restored by manual intervention by engineers, where they restarted DWFM servers presumably to drop the in-memory queued lease work and restore goodput in the system.The presence and peril of the metastable failure state has likely been widely known within AWS engineering leadership for around 4 years. And yet it bit them in their darling EC2. I eagerly await Marc’s blog post.At around 16:00 UTC in the now internally famous #incident-41-hfm_failures incident channel I began investigating a degradation in our Sandbox product. It turned out that AWS NLB trouble was causing Modal clients to fail to establish a gRPC connection with api.modal.com (which points at NLB load balancers).The NLB service went down because EC2 has a  responsible for propagating network configuration when new EC2 instances are created or instance transitions (e.g. stopping) occur, and this manager fell behind under the weight of backlogged work.The most interesting bit of the NLB service outage was that the NLB healthcheck system received bad feedback because of network configuration staleness and incorrectly performed AZ failovers.The alternating health check results increased the load on the health check subsystem, causing it to degrade, resulting in delays in health checks and triggering automatic AZ DNS failover to occur.Under control systems analysis, the potential for bad feedback is exactly the kind of thing that gets designed for. The healthcheck system behaved exactly as intended giving the inputs it received— it was a reliable component interacting unsafely with a broken environment.In their discussion of remediations, the line item for NLB is about control for bad feedback.For NLB, we are adding a velocity control mechanism to limit the capacity a single NLB can remove when health check failures cause AZ failover.You only get so many major AWS incidents in your career, so appreciate the ones that you get. AWS’s reliability is the best in the world. I have used all major US cloud providers for years now, and until Monday’s us-east-1 outage AWS was  the most reliable. This is perhaps their biggest reliability failure in a decade. We should learn something from it.I disagree with the popular early explanations. The “brain drain” theory has a high burden of proof and very little evidence. It possible that brain drain delayed remediation—this was a 14 hour outage—but we can’t see their response timeline. There’s also no evidence that us-east-1, being the oldest region, suffered from its age. The systems involved (DynamoDB, DNS Enactor, DWFM) are probably running globally. Also, those suggesting AWS’s reliability has fallen behind its competitors are too hasty. GCP had a severe global outage just last June.My main takeaway is that in our industry the design, implementation, and operation of production systems still regularly falls short of what we think we’re capable of. DynamoDB hit a fairly straightforward race condition and entered into unrecoverable state corruption. EC2 went into congestion collapse. NLB’s healthcheck system got misdirected by bad data. We’ve seen these before, we’ll see them again. We’re still early with the cloud.Software systems are far more complex and buggy than we realize. Useful software systems, such as EC2, always operate in a degraded state with dozens of present or latent bugs and faults. The presence of constant safety, of the cherished five 9s, is not a miracle, but a very challenging design and operational endeavor.]]></content:encoded></item><item><title>It&apos;s insulting to read AI-generated blog posts</title><link>https://blog.pabloecortez.com/its-insulting-to-read-your-ai-generated-blog-post/</link><author>speckx</author><category>hn</category><pubDate>Mon, 27 Oct 2025 15:27:38 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>PSF has withdrawn $1.5M proposal to US Government grant program</title><link>https://pyfound.blogspot.com/2025/10/NSF-funding-statement.html</link><author>lumpa</author><category>hn</category><pubDate>Mon, 27 Oct 2025 15:12:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In January 2025, the PSF submitted a proposal to the US government National Science Foundation under the Safety, Security, and Privacy of Open Source Ecosystems program to address structural vulnerabilities in Python and PyPI. It was the PSF’s first time applying for government funding, and navigating the intensive process was a steep learning curve for our small team to climb. Seth Larson, PSF Security Developer in Residence, serving as Principal Investigator (PI) with Loren Crary, PSF Deputy Executive Director, as co-PI, led the multi-round proposal writing process as well as the months-long vetting process. We invested our time and effort because we felt the PSF’s work is a strong fit for the program and that the benefit to the community if our proposal were accepted was considerable.  We were honored when, after many months of work, our proposal was recommended for funding, particularly as only 36% of new NSF grant applicants are successful on their first attempt. We became concerned, however, when we were presented with the terms and conditions we would be required to agree to if we accepted the grant. These terms included affirming the statement that we “do not, and will not during the term of this financial assistance award, operate any programs that advance or promote DEI, or discriminatory equity ideology in violation of Federal anti-discrimination laws.” This restriction would apply not only to the security work directly funded by the grant, but to any and all activity of the PSF as a whole. Further, violation of this term gave the NSF the right to “claw back” previously approved and transferred funds. This would create a situation where money we’d already spent could be taken back, which would be an enormous, open-ended financial risk.   Diversity, equity, and inclusion are core to the PSF’s values, as committed to in our mission statement: The mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers.Given the value of the grant to the community and the PSF, we did our utmost to get clarity on the terms and to find a way to move forward in concert with our values. We consulted our NSF contacts and reviewed decisions made by other organizations in similar circumstances, particularly The Carpentries.  In the end, however, the PSF simply can’t agree to a statement that we won’t operate any programs that “advance or promote” diversity, equity, and inclusion, as it would be a betrayal of our mission and our community. We’re disappointed to have been put in the position where we had to make this decision, because we believe our proposed project would offer invaluable advances to the Python and greater open source community, protecting millions of PyPI users from attempted supply-chain attacks. The proposed project would create new tools for automated proactive review of all packages uploaded to PyPI, rather than the current process of reactive-only review. These novel tools would rely on capability analysis, designed based on a dataset of known malware. Beyond just protecting PyPI users, the outputs of this work could be transferable for all open source software package registries, such as NPM and Crates.io, improving security across multiple open source ecosystems.In addition to the security benefits, the grant funds would have made a big difference to the PSF’s budget. The PSF is a relatively small organization, operating with an annual budget of around $5 million per year, with a staff of just 14. $1.5 million over two years would have been quite a lot of money for us, and easily the largest grant we’d ever received. Ultimately, however, the value of the work and the size of the grant were not more important than practicing our values and retaining the freedom to support every part of our community. The PSF Board voted unanimously to withdraw our application. Giving up the NSF grant opportunity—along with inflation, lower sponsorship, economic pressure in the tech sector, and global/local uncertainty and conflict—means the PSF needs financial support now more than ever. We are incredibly grateful for any help you can offer. If you're already a PSF member or regular donor, you have our deep appreciation, and we urge you to share your story about why you support the PSF. Your stories make all the difference in spreading awareness about the mission and work of the PSF. Become a Member: When you sign up as a Supporting Member of the PSF, you become a part of the PSF. You’re eligible to vote in PSF elections, using your voice to guide our future direction, and you help us sustain what we do with your annual support.Donate: Your donation makes it possible to continue our work supporting Python and its community, year after year.Sponsor: If your company uses Python and isn’t yet a sponsor, send them our sponsorship page or reach out to sponsors@python.org today. The PSF is ever grateful for our sponsors, past and current, and we do everything we can to make their sponsorships beneficial and rewarding.]]></content:encoded></item><item><title>Pyrex catalog from from 1938 with hand-drawn lab glassware [pdf]</title><link>https://exhibitdb.cmog.org/opacimages/Images/Pyrex/Rakow_1000132877.pdf</link><author>speckx</author><category>hn</category><pubDate>Mon, 27 Oct 2025 15:04:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Microsoft in court for allegedly misleading Australians over 365 subscriptions</title><link>https://www.accc.gov.au/media-release/microsoft-in-court-for-allegedly-misleading-millions-of-australians-over-microsoft-365-subscriptions</link><author>edwinjm</author><category>hn</category><pubDate>Mon, 27 Oct 2025 14:54:14 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The ACCC has commenced proceedings in the Federal Court against Microsoft Australia and its parent company Microsoft Corporation for allegedly misleading approximately 2.7 million Australian customers when communicating subscription options and price increases, after it integrated its AI assistant, Copilot, into Microsoft 365 plans. The ACCC alleges that since 31 October 2024, Microsoft has told subscribers of Microsoft 365 Personal and Family plans with auto-renewal enabled that to maintain their subscription they must accept the integration of Copilot and pay higher prices for their plan, or, alternatively, cancel their subscription.The ACCC alleges this information provided to subscribers was false or misleading because there was an undisclosed third option, the Microsoft 365 Personal or Family Classic plans, which allowed subscribers to retain the features of their existing plan, without Copilot, at the previous lower price.Microsoft’s communication with subscribers did not refer to the existence of the “Classic” plans, and the only way subscribers could access them was to begin the process of cancelling their subscription. This involved navigating to the subscriptions section of their Microsoft account and selecting “Cancel subscription”. It was only on the following page that subscribers were given the option to instead move to the Classic plan. See a screenshot of the cancellation page revealing the Classic plan.“Following a detailed investigation, we will allege in Court that Microsoft deliberately omitted reference to the Classic plans in its communications and concealed their existence until after subscribers initiated the cancellation process to increase the number of consumers on more expensive Copilot-integrated plans,” ACCC Chair Gina Cass-Gottlieb said.“The Microsoft Office apps included in 365 subscriptions are essential in many people’s lives and given there are limited substitutes to the bundled package, cancelling the subscription is a decision many would not make lightly.”“We’re concerned that Microsoft’s communications denied its customers the opportunity to make informed decisions about their subscription options, which included the possibility of retaining all the features of their existing plan without Copilot and at the lower price,” Ms Cass-Gottlieb said.“We believe many Microsoft 365 customers would have opted for the Classic plan had they been aware of all the available options.”Following the integration of Copilot, the annual subscription price of the Microsoft 365 Personal plan increased by 45 per cent from $109 to $159. The annual subscription price for the Microsoft 365 Family plan increased by 29 per cent from $139 to $179.Microsoft sent two emails and published a blog post to inform auto-renewing subscribers (as of 31 October 2024) about the Copilot integration and the impending price increase that would apply at their next renewal. These three pieces of communication are central to the ACCC’s case.“We allege that Microsoft’s two emails to existing subscribers and the blog post were false or misleading as they conveyed that consumers had to accept the more expensive Copilot-integrated plans, and that the only other option was to cancel,” Ms Cass-Gottlieb said.“All businesses need to provide accurate information about their services and prices. Failure to do so risks breaching the Australian Consumer Law,” Ms Cass-Gottlieb said.In establishing its investigation into this matter, the ACCC drew on a significant number of consumer reports, as well as commentary in online forums such as Reddit. Information provided by consumers to the ACCC’s Infocentre was critical to alerting the ACCC to the alleged conduct, particularly in identifying the availability of the Classic plan through subscribers’ cancellation flows.The ACCC is seeking orders including penalties, injunctions, declarations, consumer redress, and costs.The ACCC believes the millions of Australian consumers who were allegedly misled by Microsoft about the availability of the Classic plan may have suffered economic harm through the automatic renewal of their subscription with Copilot integration at a higher price.The ACCC is seeking consumer redress in this case for Microsoft 365 Personal and Family subscribers affected by the alleged conduct.Existing Microsoft 365 Personal and Family subscribers who have not had their subscription renewed since 8 July 2025 and would like to revert to their previous plan may be able to select the cancel option and follow the steps in the cancellation process until the Classic plan is offered. However, the ACCC notes that the subscription options and prices offered are entirely in Microsoft’s control and could be subject to change at any time.Example timeline for a subscriber on a Microsoft 365 Personal plan On 19 April 2024, a consumer purchased an annual Microsoft 365 Personal subscription for $109 and enabled auto-renewal for one year’s time.On 31 October 2024, Microsoft published a blog post in which it stated:
“To reflect the value we’ve added over the past decade and enable us to deliver new innovations for years to come, we’re increasing the prices of Microsoft 365 Personal and Family. The price increase will apply to existing subscribers upon their next renewal.”On 13 April 2025, 7 days before their renewal date, the consumer received a second email in which Microsoft stated:
“We want to let you know about a change to the amount of your next payment. Unless you cancel two days before Saturday, April 19 2025, we’ll charge AUD 159.00 including taxes every year… We’ll tell you if this price ever changes. Cancel any time to stop future charges or change how you pay by managing your subscription in your Microsoft account.”On 19 April 2025, the consumer's subscription was automatically renewed at the increased price of $159.  The consumer was not aware that switching to the Classic plan at the existing subscription price of $109 was possible.Screenshots showing the communications with subscribersEmail sent to subscribers informing them of the Copilot integration and price increaseThe page late in the cancellation process revealing the Classic planA subscriber only saw this screen once they had navigated to the subscriptions section of their Microsoft account, selected “Cancel subscription”, and continued with the cancellation process.Microsoft Pty Ltd (Microsoft AU) is an Australian proprietary company, and a wholly owned subsidiary of the Microsoft Corporation (Microsoft US), a US-based technology conglomerate. Microsoft AU is the supplier of Microsoft’s proprietary software in Australia, including Microsoft 365 plans.The ACCC alleges Microsoft US was responsible for preparing and publishing the communications to Australian Microsoft 365 subscribers containing the misrepresentations alleged by the ACCC. The ACCC alleges that Microsoft AU adopted the communications as the seller of Microsoft 365 subscriptions to Australian consumers.The ACCC’s case only relates to Microsoft 365 Personal and Family plans, which are designed for home use. The case does not involve Microsoft 365 subscriptions for business or enterprise.Microsoft 365 Personal and Family offerings are supplied on a monthly or annual subscription basis, and are comprised of:software products, such as Word, Excel, PowerPoint and OneNotecollaboration and communication applications like Outlook, Teams and SharePointcloud-based services through OneDrive.Microsoft launched Copilot as its consumer-facing generative AI product in 2023. Copilot was integrated into Microsoft 365 Personal and Family subscriptions in Australia on 31 October 2024.In January 2025, the Copilot integration was rolled out across Microsoft 365 worldwide, with varying subscription price increases applying to each jurisdiction.For corporations, the maximum penalty for each breach of the Australian Consumer Law is the greater of:three times the total benefits that have been obtained and are reasonably attributable, orif the total value of the benefits cannot be determined, 30 per cent of the corporation’s adjusted turnover during the breach turnover period.Any penalty that might apply to this conduct is a matter for the Court to determine and would depend on the Court’s findings. The ACCC will not comment on what penalties the Court may impose.This document contains the ACCC’s initiating court documents in relation to this matter. We will not be uploading further documents in the event these initial documents are subsequently amended.]]></content:encoded></item><item><title>Show HN: ISS in Real Time – 25 Years Aboard the International Space Station</title><link>https://issinrealtime.org/</link><author>bfeist</author><category>hn</category><pubDate>Mon, 27 Oct 2025 14:25:00 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>
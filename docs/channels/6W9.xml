<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://konrad.website/feeds/</link><description></description><item><title>Doing gigabit Ethernet over my British phone wires</title><link>https://thehftguy.com/2026/01/22/doing-gigabit-ethernet-over-my-british-phone-wires/</link><author>user5994461</author><category>hn</category><pubDate>Sat, 24 Jan 2026 10:14:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[: None of this is written by AI, I’m still a real person writing my own blog like its 1999I finally figured out how to do Gigabit Ethernet over my existing phone wires.Powerline adapter and miseryI’ve mostly lived with powerline adapters over recent years. Some worked well, some did not (try few and return what doesn’t work in your home). One I had for a while gave me stable 30 Mbps, which was little but good enough for internet at the time. I care very much about having stable low latency for gaming, more than bandwidth.Fast forward to my current situation, that powerline adapter regularly lost connection which was a major problem. I got some new ones with the latest and greatest G.hn 2400 standard. The final contender served around 180 Mbps to my office (with high variance 120 to 280 Mbps), or around 80 Mbps to the top floor. It’s good enough to watch YouTube/TV yet it’s far from impressive.One peculiar thing from the UK: Internet providers don’t truly offer gigabit internet. They have a range of deals like 30 Mbps – 75 Mbps – 150 Mbps – 300 Mbps – 500 Mbps – 900 Mbps, each one costing a few more pounds per month than the last. This makes the UK simultaneously one of the cheapest and one of the most expensive countries to get Internet. Long story short, new place, new hardware, new deals, the internet has been running at 500 Mbps for some time now.Problem: How to get 500 Mbps to my room?A Fetish for Phone SocketsI’ve been looking for a way to reuse phone wires for a while, because British houses are full of phone sockets. There are 2 sockets in my office room.I can’t stress enough how much we love our phone sockets. It’s not uncommon to have a one bed flat with 2 phone sockets in the living room and 2 phone sockets in the bedroom and a master socket in the technical room. It’s ridiculous.A new house bought today could have 10 phone sockets and 0 Ethernet sockets. There is still no regulation that requires new build to get Ethernet wiring (as far as I know).There’s got to be a way to use the existing phone infrastructure.I know the technology exists. It’s one of the rare cases where the technology exists and is mature, but nobody can be bothered to make products for it.The standards that run powerline adapters (HomePlug AV200, AV500, G.hn 2400) can work with any pair of wires. It should work ten times better on dedicated phone wires instead of noisy power wires, if only manufacturers could be bothered to pull their fingers out of their arse and make the products that are needed.It’s made and shipped from Germany.I was lazy so I ordered online in self-service (which is definitely the wrong way to go about it). It’s available on Ebay DE and Amazon DE, it’s possible to order from either with a UK account, make sure to enter a UK address for delivery (some items don’t allow it).The better approach is almost certainly to speak to the seller to get a quote, with international shipping and the import invoice excluding VAT (to avoid paying VAT on VAT).The package got the usual Royal Mail treatment:The package was shipped by DHL GermanyThe package was transferred to Royal Mail when entering the UKAfter some days, the DHL website said they tried to deliver but nobody home, this is bullshitRoyal website said the package reached the depot and was awaiting delivery, this is bullshitIn reality, the package was stuck at the border, as usualGoogle to find “website to pay import fee on parcel”Entered the DHL tracking number into the Royal Mail form for a Royal Mail tracking numberThe website said that the parcel had import fees to pay, this is correctPaid the fee online, 20% VAT + a few pounds of handling feesThe package will be scheduled for delivery a few days laterRoyal Mail and DHL updated their status another two or three times with false informationRoyal Mail delivered a letter saying there was a package waiting on fees, though it was paidThe package finally arrivedBasically, you need to follow the tracking regularly until the package is tagged as lost or failed delivery, which is the cue to pay import fees.It’s the normal procedure to buy things from Europe since Brexit 2020. It’s actually quite shocking that Royal Mail still hasn’t updated their tracking system to be able to give a status “waiting on import fees to be paid online”. They had 6 years!A pair of gigacopper G4201TMThe device has a German power socket (expected)It came with a German to UK power adapter (unexpected and useful)It came with a standard RJ11 cable (expected and useless)Found BT631A to RJ11 cables online (the standard UK phone socket)Found Ethernet cables in my toolbox3M removable hanging strip to stick to the wall, the device is very lightThere is a gigacopper G4202TM: with an RJ45 to connect to the phone line instead of a RJ11 (not sure if it’s a newer model or just a variant, as that one has two gigabit Ethernet ports). Don’t be confused by having a RJ45 port that is not a RJ45 port.There is a gigacopper G4201C (1 port) and G4204C (4 port) for Ethernet over coaxial. Some countries have coax in every room for TV/satellite. This may be of interest to some readers.Reminder, this is a 500 Mbps internet connection.I discovered soon afterwards that I bought the wrong item. There is an InHome and a Client/Server variant of the product. Make sure to buy the InHome variant.The InHome variant can have up to 16 devices, communicating to any peer on the medium, with sub millisecond latency.The client-server variant is preconfigured as a pair, splitting the bandwidth 70% download / 30% upload, with few milliseconds latency. I think it’s a use case for ISP and long range connections.Thankfully the difference is only the firmware. I spoke to the vendor who was very helpful and responsive. They sent me the firmware and the tools to patch.I have a fetish for low latency. This screenshot is oddly satisfying.The web interface says 1713 Mbps on the physical layer, the debugging tool says PHONE 200MHz – Connected 1385 Mbps.I wanted to verify whether the device can do a full Gigabit. Unfortunately I realized I don’t have any device that can test that.Phones are wireless, which is too slow to test anything. I checked out of curiosity, my phone did 100 Mbps to 400 Mbps right next to the router. Grabbed two laptops only to realize they didn’t have any Ethernet port. I dug up an old laptop from storage with an Ethernet port. The laptop couldn’t boot, the CPU fan didn’t start and the laptop refused to boot with a dead fan.There is a hard lesson here: 1 Gbps ought to be enough for any home. Using the phone line is as good as having Ethernet wiring through the house if it can deliver a (shared) 1.7 Gbps link to multiple rooms.Still, I really wanted to verify that the device can do a full Gbps, I procured an USB-C to Ethernet adapter.Full speed achieved, testing from a phone to a computer with iperf3.Some readers might wonder about the wiring.I didn’t check the wiring before buying anything because it’s pointless. British sockets are always daisy chained in an incomprehensible maze.Phone sockets need 2 wires and can be daisy chained. Ethernet sockets need 8 wires. They often use the same Cat5 cable because it’s the most widely available (8 wires cable, the 6 extra wires can remain unconnected). It’s possible to swap the phone socket for an RJ45 socket, if you only have 2 sockets connected with the right cable. It’s not possible when sockets are daisy chained. (You could put a double or triple RJ45 socket with a switch to break a daisy chain, but it quickly becomes impractical in a British house with 5 to 10 sockets in an arbitrary layout.)I opened one socket in the office room. There are two Cat5 cables daisy chained. There are 3 wires connected.It’s probably daisy chained with the other socket in the room, or it’s daisy chained with the socket in the other room that’s closer. Who knows.I opened the BT master socket in the technical room. It should have the cables coming from the other rooms. It should connect the internal phone wires with the external phone line.There is one single Cat5 cable. There are 4 wires connected. It’s definitely not a master socket. WTF?!It’s interesting that this socket has 4 wires connected but the socket in the office has 3 wires connected. The idiot who did the wiring was inconsistent. The gigacopper device can operate over 2 wires (200 MHz Phone SISO) or over 4 wires (100 MHz Phone MIMO). I can try the other modes if I finish the job.The search for the master socket continues. The cables from the other floors should all be coming down somewhere around here. There is a blank plate next to it (right). This might be the external phone line? A bunch of wires are crimped together, colours do not match. It’s the hell of a mess.Only sure thing, they are different cables because they are different colours. They might be going to a junction box somewhere else. Probably behind a wall that’s impossible to access!Conclusion: There is zero chance to get proper Ethernet wiring out of this mess.The gigacopper device to do gigabit Ethernet over phone line is a miracle!There is an enormous untapped market for gigabit Ethernet over phone sockets in the UK.]]></content:encoded></item><item><title>SEC obtains final consent judgments against former FTX and Alameda executives</title><link>https://www.sec.gov/enforcement-litigation/litigation-releases/lr-26450</link><author>sizzle</author><category>hn</category><pubDate>Sat, 24 Jan 2026 02:59:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Comma openpilot – Open source driver-assistance</title><link>https://comma.ai/</link><author>JumpCrisscross</author><category>hn</category><pubDate>Sat, 24 Jan 2026 01:00:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TikTok Is Now Collecting More Data About Its Users</title><link>https://www.wired.com/story/tiktok-new-privacy-policy/</link><author>coloneltcb</author><category>hn</category><pubDate>Fri, 23 Jan 2026 22:51:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ in the US opened the app today, they were greeted with a pop-up asking them to agree to the social media platform’s new terms of service and privacy policy before they could resume scrolling.These changes are part of TikTok’s transition to new ownership. In order to continue operating in the US, TikTok was compelled by the US government to transition from Chinese control to a new, American-majority corporate entity. Called TikTok USDS Joint Venture LLC, the new entity is made up of a group of investors that includes the software company Oracle.It's easy to tap Agree and keep on scrolling through videos on TikTok, so users might not fully understand the extent of changes they are agreeing to with this pop-up.Now that it’s under US-based ownership, TikTok potentially collects more detailed information about its users, including precise location data. A spokesperson for TikTok USDS declined to comment.TikTok Adds Precise Location TrackingTikTok’s change in location tracking is one of the most notable updates in this new privacy policy. Before this update, the app did not collect the precise, GPS-derived location data of US users. Now, if you give TikTok permission to use your phone’s location services, then the app may collect granular information about your exact whereabouts. Similar kinds of precise location data is also tracked by other social media apps, like Instagram and X.We collect information about your approximate location, including location information based on your SIM card and/or IP address. In addition, we collect location information (such as tourist attractions, shops, or other points of interest) if you choose to add the location information to your User Content. Current versions of the app do not collect precise or approximate GPS information from US users.We automatically collect certain information from you when you use the Services, including ... location information about your approximate location based on your device and network information, such as SIM card region, IP address, and device system settings. We also collect information, such as tourist attractions, shops, or other points of interest, if you choose to add the location to your user content. Also, if you choose to enable location services for the TikTok app within your device settings, we collect approximate or precise location information from your device.TikTok Now Tracks AI InteractionsRather than an adjustment, TikTok’s policy on AI interactions adds a new topic to the privacy policy document. Now, users' interactions with any of TikTok’s AI tools explicitly fall under data that the service may collect and store. This includes any prompts as well as the AI-generated outputs. The metadata attached to your interactions with AI tools may also be automatically logged.(These AI interactions are not explicitly mentioned in the past policy.)When you create an account, upload content, contact us directly, or otherwise use the Services, you may provide some or all of the following information … AI interactions, including prompts, questions, files, and other types of information that you submit to our AI-powered interfaces, as well as the responses they generate.We automatically collect certain information from you when you use the Services, including … metadata that is automatically uploaded in connection with your user content, messages, or AI interactions, such as how, when, where, and by whom the user content was created, or message or prompt was sent. Metadata may also include information, such as your username, that enables your user content to be traced back to your account by other users.TikTok Expands Its Ads NetworkThis change to TikTok’s privacy policy may not be as immediately noticeable to users, but it will likely have an impact on the types of ads you see outside of TikTok. So, rather than just using your collected data to target you while using the app, TikTok may now further leverage that info to serve you more relevant ads wherever you go online. As part of this advertising change, TikTok also now explicitly mentions publishers as one kind of partner the platform works with to get new data.]]></content:encoded></item><item><title>Mental Models (2018)</title><link>https://fs.blog/mental-models/</link><author>hahahacorn</author><category>hn</category><pubDate>Fri, 23 Jan 2026 21:08:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A mental model is a simplified explanation of how something works. Any idea, belief, or concept can be boiled down to its essence. Like a map, mental models highlight key information while ignoring irrelevant details. They’re tools for compressing complexity into manageable chunks.Mental models help us understand the world. For example, velocity shows that both speed and direction matter. Reciprocity reveals how being positive and taking initiative gets the world to do most of the work for you. Margin of Safety reminds us that things don’t always go as planned. Relativity exposes our blind spots and shows how a different perspective can reveal new information. These are just a few examples.If you want to be a good thinker, you must develop a mind that can jump the jurisdictional boundaries. You don’t have to know it all. Just take in the best big ideas from all these disciplines. And it’s not that hard to do.The Big Ideas From The Big DisciplinesThis page summarizes the big ideas to help you make better decisions, avoid problems, and spot opportunities others miss.The map is not the territory reminds us that our mental models of the world are not the same as the world itself. It cautions against confusing our abstractions and representations with the complex, ever-­shifting reality they aim to describe.It is dangerous to mistake the map for the territory. Consider the person with an outstanding résumé who checks all the boxes on paper but can’t do the job. Updating our maps is a difficult process of reconciling what we want to be true with what is true.In many areas of life, we are offered maps by other people. We are reliant on the maps provided by experts, pundits, and teachers. In these cases, the best we can do is to choose our mapmakers wisely and to seek out those who are rigorous, transparent, and open to revision.Ultimately, the map/territory distinction invites us to engage with the world as it is, not just as we imagine it. And remember, when you don’t make the map, choose your cartographer wisely.The first rule of competition is that you are more likely to win if you play where you have an advantage. Playing to your advantage requires a firm understanding of what you know and don’t know. Your circle of competence is your personal sphere of expertise, where your knowledge and skills are concentrated. It’s the domain where you have a deep understanding, where your judgments are reliable, and your decisions are sound. The size of your circle isn’t as important as knowing the boundaries. The wise person knows the limits of their knowledge and can confidently say, “This falls within my circle,” or “This is outside my area of expertise.” While operating within your circle of competence is a recipe for confidence and effectiveness, venturing outside your circle of competence is a recipe for trouble. You’re like a sailor navigating unfamiliar waters without a map, at the mercy of currents and storms you don’t fully understand. This isn’t to say that you should never venture outside your circle. Learning new things, gaining new skills, and mastering new domains is one of the most beautiful things about life. Celebrate your expertise, but also acknowledge your limitations.First principles thinking is the art of breaking down complex problems into their fundamental truths. It’s a way of thinking that goes beyond the surface and allows us to see things from a new perspective. Thinking in first principles allows us to identify the root causes, strip away the layers of complexity, and focus on the most effective solutions. Reasoning from first principles allows us to step outside the way things have always been done and instead see what is possible. First principles thinking is not easy. It requires a willingness to challenge the status quo. This is why it’s often the domain of rebels and disrupters who believe there must be a better way. It’s the thinking of those willing to start from scratch and build from the ground up.In a world focused on incremental improvement, first principles thinking offers a competitive advantage because almost no one does it.Thought experiments are the sandbox of the mind, the place where we can play with ideas without constraints. They’re a way of exploring the implications of our theories, of testing the boundaries of our understanding. They offer a powerful tool for clarifying our thinking, revealing hidden assumptions, and showing us unintended consequences. The power of thought experiments lies in their ability to create a simplified model of reality where we can test our ideas. In the real world, confounding factors and messy details obscure the core principles at work. Thought experiments allow us to strip away the noise and focus on the essence of the problem. Thought experiments remind us that some of the most profound insights and innovations start with a simple question: What if?Second-­order thinking is a method of thinking that goes beyond the surface level, beyond the knee-­jerk reactions and short-­term gains. It asks us to play the long game, to anticipate the ripple effects of our actions, and to make choices that will benefit us not just today but in the months and years to come. Second-order thinking demands we ask: And then what? Think of a chess master contemplating her next move. She doesn’t just consider how the move will affect the next turn but how it will shape the entire game. She’s thinking many steps ahead. She’s considering her own strategy and her opponent’s likely response. In our daily lives, we’re often driven by first-­order thinking. We make decisions based on what makes us happy now, what eases our current discomfort, or satisfies our immediate desires. Second-­order thinking asks us to consider the long-­term implications of our choices to make decisions based not just on what feels good now but on what will lead to the best outcomes over time. In the end, second-­order thinking is about playing the long game. It’s about making choices for the next move and the entire journey.Probabilistic thinking is the art of navigating uncertainty. Successfully thinking in shades of probability means roughly identifying what matters, calculating the odds, checking our assumptions, and then deciding. The challenge of probabilistic thinking is that it requires constant updating. As new information emerges, the probabilities change. What seemed likely yesterday may seem unlikely today. This explains why probabilistic thinkers always revise their beliefs with new data and why it’s uncomfortable for many people. It’s much easier to believe something false is accurate than to deal with the fact that we might be wrong. Being a probabilistic thinker means being willing to say, “I don’t know for sure, but based on the evidence, I think there’s a 63 percent chance of X.” The rewards of probabilistic thinking are immense. By embracing uncertainty, we can make better decisions, avoid the pitfalls of overconfidence, and navigate complex situations with greater skill and flexibility. We can be more open-­ minded, more receptive to new ideas, and more resilient in the face of change.Much of success comes from simply avoiding common paths to failure.Inversion is not the way we are taught to think. We are taught to identify what we want and explore things that will move us closer to our objective. However, avoiding things that ensure we don’t get what we want dramatically increases our odds of success.We can get fixated on solving problems one way, missing simpler solutions. Inversion breaks us out of this tunnel vision.Instead of “How do I solve this?”, inversion asks, “What would guarantee failure?” Rather than “How can I achieve this?”, it asks “What’s preventing me from achieving it?” This flip reveals insights our usual thinking overlooks.When facing a tricky problem or ambitious goal, try inverting. Ask how you’d guarantee failure. The answers may surprise you—and unlock new solutions.Occam’s razor is the intellectual equivalent of “keep it simple.” When faced with competing explanations or solutions, Occam’s razor suggests that the correct explanation is most likely the simplest one, the one that makes the fewest assumptions. This doesn’t mean the simplest theory is always true, only that it should be preferred until proven otherwise. Sometimes, the truth is complex, and the simplest explanation doesn’t account for all the facts. The key to wielding this model is understanding when it works for you and against you.A theory that is too simple fails to capture reality, and one that is too complex collapses under its own weight.Hanlon’s razor is a mental safeguard against the temptation to label behavior as malicious when incompetence is the most common response. It’s a reminder that people are not out to get you, and it’s best to assume good faith and resist the urge to assign sinister motives without overwhelming evidence. This isn’t to say that genuine malice doesn’t exist. Of course, it does. But in most interactions, stupidity is a far more common explanation than malevolence. People make mistakes. They forget things. They speak without thinking. They prioritize short-­term wins over long-term wins. They act on incomplete information. They fall prey to bias and prejudice. These actions might appear like deliberate attacks from the outside, but the reality is far more mundane. Hanlon’s razor’s real power lies in how it shifts our perspective. When we assume stupidity rather than malice, we respond differently. Instead of getting defensive or lashing out, we approach the situation with empathy and clarity. For most daily frustrations and confusion, Hanlon’s razor is a powerful reminder to approach problems with a spirit of generosity. It’s a way to reduce drama and stress and find practical solutions instead of descending into blame and escalation.The Mental Models of Physics, Chemistry, and BiologyRelativity is the idea that our perceptions and judgments are not absolute but are shaped by our unique vantage points and frames of reference. It’s the understanding that our experiences are subjective. We each inhabit a particular web of experiences. This context shapes how we see the world, what we notice and overlook, and what we value and dismiss. Two people can look at the same event and come away with vastly different interpretations based on their unique frames of reference. Consider two people standing in the same room: They each experience the same absolute temperature differently. One can feel hot while the other feels cold, even though the temperature is the same. Similarly, consider political debates: Our beliefs are shaped by our unique experiences and social contexts. A policy that seems like common sense to an urban progressive might feel like complete nonsense to a rural conservative, and vice versa. In this way, understanding relativity is key to fostering empathy and finding common ground. However, relativity is not the same as relativism—­ the idea that all perspectives are equally valid. Recognizing the relativity of our perceptions doesn’t mean we don’t have to make judgments about validity. Instead, it’s a call to examine our assumptions, seek out diverse perspectives, and expand our frames of reference. We all have blind spots—­things we cannot see. Understanding that our perceptions are relative allows us to open ourselves to other ways of seeing. If you’re wondering where to get started, try asking others what they see that you can’t. Apply your judgment to their responses and update your beliefs accordingly.Reciprocity underlies everything from basic human kindness to the most complex systems of trade. At its core, reciprocity is the simple idea of treating others as they treat us—giving what we get. But from this simple principle grows a vast web of social interactions and expectations that shapes nearly every aspect of our lives. Many people expect the world to just hand them things without effort. This is a poor strategy because it doesn’t align with the human behavior you can observe around you every day. Reciprocation teaches us that you are likely to receive the same if you give people cynicism and curtness or nothing at all. But if you give people an opportunity and the benefit of the doubt, you will, more often than not, be on the receiving end of the same behavior. Become what you want to see in the world, and the world will return it to you. If you want an amazing relationship with your partner, be an amazing partner. If you want people to be thoughtful and kind to you, be thoughtful and kind to them. If you want people to listen to you, listen to them. The best way to achieve success is to deserve success. Small changes in your actions change your entire world. One of the biggest misperceptions about reciprocity is that people should sit around waiting for others to go first rather than unlocking the power of reciprocity in their favor by going positive and going first without expectation. Reciprocity reminds us that our actions tend to come back on us. It’s an essential reminder that we are part of the world, and thus, our actions do not happen in isolation but are instead part of an interconnected web of effects.Thermodynamics is the science of energy, heat, and work. It’s the set of physical laws that govern how energy moves and changes in the universe. Chances are, when you first came across the subject, it was dry, full of equations and abstract concepts. But the truth is thermodynamics is a useful intellectual framework for daily life. Not only can it reveal why your room gets messier over time, but it also explains why you should choose your friends wisely. The first law of thermodynamics states that energy can neither be created nor destroyed, only transformed from one form to another. This means that every joule of energy in the universe, every bit of heat and work and motion is part of an unbroken chain stretching back to the Big Bang. When you hop on a flight that burns jet fuel, you’re tapping into energy captured by plants millions of years ago and stored in chemical bonds until it was transformed into heat and motion. But while energy is conserved, it’s not always useful. That’s where the second law of thermodynamics comes in. It states that entropy—­ a measure of disorder—­ increases over time in any closed system. In other words, left on its own, the universe tends toward chaos. Your bedroom doesn’t clean itself—­ it takes energy and effort to maintain order. Stars burn out, structures crumble, and ice melts into water.Entropy is the universe’s tax on time. The constant battle against entropy is the driving force behind much of what we do. The constant struggle between order and disorder is the source of change and progress. While engineers and scientists use thermodynamics to design engines or calculate the energy requirements of a system, we can use it as a framework for understanding the deep interconnectedness of everything. When you feel the sun’s warmth on your skin, you’re experiencing the result of a thermodynamic process that began in the heart of a star ninety-three million miles away. When you watch a campfire burn down to embers, you’re witnessing the inexorable march of entropy in real-time.Thermodynamics is the story of energy across time. We’re part of an energy story that stretches back to the dawn of time and reaches the farthest pockets of space. We can marvel that in a universe ruled by disorder, pockets of temporary order can emerge, whether it’s a clean room, a planet, or a civilization. By understanding thermodynamics, we gain not just a technical toolbox but an appreciation for the beauty, complexity, and fragility of our very existence.Inertia is the stubborn resistance of the universe to change. It’s why objects at rest tend to stay at rest, and objects in motion tend to stay in motion. You can think of inertia as the guardian of the status quo.At its core, inertia is a property of mass. The more massive an object is, the more it resists changes to its state of motion. A feather, with its tiny mass, is easily blown about by the slightest breeze. A boulder, on the other hand, requires a powerful force to get it moving. This is why it takes more effort to push a heavy cart than a light one, more energy to launch a rocket than to toss a ball. But inertia isn’t just a physical phenomenon. It’s an illuminating lens to see habits, beliefs, and our resistance to change. The longer we’ve held them, the larger the mass and the more force required to change them. The path of least resistance is always the status quo. Getting started is the hardest part. Once something moves in a direction, keeping it in motion is much easier. But once something is in motion, it’s hard to stop. This is why most self-­help books about positive habits break things down into very small steps—­to reduce the force required to overcome the status quo. For example, if you want to get in the habit of doing push-­ups daily, start with one rather than fifty. If you want to start a flossing habit, start with one tooth. After all, the bigger the mass—­in this case, the gap between where you are and where you want to be—­ the more effort required.Inertia is both a challenge and an opportunity. Successful companies struggle with the inertia of their success and the resistance to change that comes with size, complexity, and entrenched interests. On the other hand, startups can leverage their lack of inertia—­their agility, their willingness to pivot and adapt—­as a competitive advantage. Momentum and inertia are closely related. While inertia is the tendency to resist change, momentum is the oomph an object has when it’s moving. The more momentum something has, the harder it is to stop or redirect. The key is to pick the right direction and build momentum so inertia works to your advantage and carries you forward. This is the essence of the “flywheel” concept in business—success breeds success, and small wins compound into big gains.When you’re fighting the status quo, remember the physics at play. Resistance is natural. Understand that building momentum in a new direction takes a sustained force. While the universe resists change, it always rewards those who dare to overcome that resistance.Friction and viscosity are the sand in the gears of the universe, the invisible hands that slow the motion of all things. Friction is the grip between surfaces in contact, the roughness that resists sliding. Viscosity is the thickness of fluids, the internal friction that makes liquids sluggish and syrupy. Together, they are the great moderators of motion. Think of the last time you tried to slide a heavy piece of furniture across the floor. The resistance you felt, the effort required to overcome the grip of the surface—­ that was friction at work. Or consider the slow, thick pour of honey from a jar, the way it clings and drips in slow threads. That’s the viscosity of the fluid resisting the force of gravity, the internal friction that makes the honey flow like molasses rather than water. While friction is the enemy of efficiency, it’s also necessary for traction. We couldn’t walk, hold tools, or tie knots without it. Viscosity, too, is a double-­edged sword. In pipelines and hydraulic systems, high viscosity means higher pumping costs, slower flows, and greater strain on equipment. But viscosity also makes oil a good lubricant, allowing paints and coatings to spread evenly and adhere to surfaces.Friction and viscosity are powerful metaphors for the forces of resistance in every domain of life. In human relationships, friction is the conflict and tension that arises from differing goals, personalities, or beliefs. The interpersonal roughness can generate heat and wear, but also the traction that allows us to influence and connect with others.While often hidden, friction and viscosity work against us whenever we try to do something. We often default to using more force to overcome resistance when simply reducing the friction or viscosity will do. However, doing both is more effective than either in isolation. Friction and viscosity can also be wielded as weapons. Rather than try to catch up to the competition with more effort, you might want to explore slowing them down by adding resistance through increased regulation, bureaucracy, or other clever ideas. In the end, reducing resistance is often easier than adding force.Velocity is the great differentiator, distinguishing the stagnant from the swift. In physics, velocity is a fundamental quantity, a key variable in the equations that describe the behavior of everything from subatomic particles to galaxies. It’s the v in the formulas of motion, the arrow that points the way from here to there. Velocity is also a metaphor for life. Consider it the rate at which we learn and grow, the speed at which we innovate and create, and the focus with which we pursue our goals. Velocity challenges us to think about what we can do to put ourselves on the right trajectory and to find a balance between mass and speed to move toward our goals. The ability to set a direction, improve your tactics, and adjust to new information becomes paramount. Velocity isn’t just about raw speed. Direction matters just as much (if not more). A car moving at high speed in circles goes nowhere, while a slow and steady walk in a straight line can cross continents. Velocity is progress. Sometimes, progress comes from more force, and sometimes, progress comes from removing friction. Once you have a destination, you can improve your velocity by working harder and eliminating things that aren’t contributing toward reaching that goal.Leverage is the force multiplier of the world, the principle that allows the small to move the large and the few to influence the many. It’s the idea that a little force, strategically applied, can yield outsize outputs.At its core, leverage is amplification. Think of a crowbar prying two boards apart or a pulley system hoisting a heavy load. In each case, the appled force is multiplied. But leverage isn’t just useful in physics. Rather, it’s a principle that applies across our lives.Leverage is often lurking in the background of nonlinear outcomes. Consider the author who took the ideas in their head, put them in a book, and sold millions of copies, or the Wall Street investor who made a single decision that resulted in billions. Or even the CEO who directs the people working for them. All of these examples are leverage in action.In personal development, leverage is about identifying the key habits, skills, and relationships that will impact your life and work most. It’s about focusing your energy on the critical few rather than the trivial many, about finding the points of maximum leverage where small changes can cascade into massive results.An example of personal leverage is an employee who learns to use AI to amplify their impact on the organization far beyond their experience or effort. While labor is still a form of leverage, it can often be done with silicon chips. In this sense, the person who can leverage technology can compete in a way never imaginable.However, leverage is not without its risks and responsibilities. Just as a small action can have an outsized positive impact, so can it have negative consequences. If you borrow too much money against your house and it turns out to be less valuable than assumed or interest rates change, the downside of leverage can quickly wipe you out.Good ideas taken too far often cause unanticipated consequences. Wielding leverage to maximum effect all the time, as the West Virginia mine owners did, sows the seeds of ongoing unrest that undermines one’s ability to be truly effective. No one wants to feel exploited, and those who are never give their loyalty or their best work.The key is to use leverage wisely and judiciously by understanding the systems you want to influence and considering the second- and third-­ order effects of your actions.Leverage is a tool, not a toy, and like any tool, it requires skill, judgment, and respect.Activation energy is the spark that ignites the fire of change, the initial burst of effort required to kick-­ start a reaction or transformation. It’s the metaphorical push that gets the boulder rolling down the hill, the investment of energy needed to overcome inertia and set a process in motion. In chemistry, activation energy is the minimum energy that must be input for a reaction. It’s the hurdle molecules must overcome to break their bonds and form new ones, the energetic barrier separating the reactants from the products. But activation energy isn’t just a chemical concept. It’s a principle that applies to any system where change is possible but not automatic. In personal growth, activation energy is the effort required to break old habits and form new ones. In innovation, it’s the investment needed to turn an idea into reality. The key is recognizing activation energy for what it is: a necessary upfront cost, not a permanent obstacle. Once things are moving, momentum takes over. Once the reaction starts, it becomes self-­ sustaining.Catalysts are the unsung heroes of chemical reactions, the silent partners accelerating change. By decreasing the time required to cause change, they also make reactions possible that might not have occurred otherwise. In chemistry, a catalyst is a substance that increases the reaction rate without permanently altering itself. But catalysts aren’t just chemical curiosities, they’re a powerful metaphor for the forces that drive change and growth.In business, a catalyst might be a new technology that opens fresh possibilities or a visionary leader who inspires a team to new heights. In your personal life, a catalyst could be a life-­changing book, a transformative experience, or a mentor who sees your potential and helps you realize it. Of course, while we benefit from others acting as our catalysts, we can be catalysts ourselves—helping others find the activation energy they need to thrive.Alloying is the art of mixing elements to create something greater than the sum of its parts. While our intuition tells us that pure substances are best, alloying shows this is not always true. One plus one can equal ten. By blending ingredients in precise proportions, metallurgists can create materials with bespoke properties—the lightness of aluminum with the strength of steel, the corrosion resistance of chromium with the affordability of iron.But alloying isn’t just about physical properties. It’s a metaphor for the power of diversity and combination in all walks of life. In teams, alloying is the mixing of different skills, perspectives, and personalities to create a more creative, adaptable, and resilient group than any individual could be alone. In ideas, it’s the blending of concepts from different fields to spark innovation and insight.In people, alloying is the combination of skills that makes them unstoppable. Consider a person possessing deep engineering skills who can clearly explain ideas. They are more valuable than someone with just the engineering skills. Now add empathy, humility, resilience, and drive. This person becomes incredibly rare.The key to successful alloying is knowing which elements to combine and in what proportions. Too little of one ingredient and you don’t get the desired effect; too much and you might end up with something brittle or unstable. The art lies in finding the sweet spot, the golden ratio where the whole becomes more than the sum of its parts.Evolution Part One: Natural Selection and ExtinctionNatural selection is the hidden hand that selects the fittest from a never-­ending pile of genetic variation, while extinction is the hammer that shatters the unfit and clears the way for variations to arise. In biology, natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. The invisible hand of natural selection guides the adaptations of the living world, favoring creatures that are best suited to their environments and pruning back those that fall short.But for every winner in the great game of natural selection, there are countless losers. Extinction is the fate awaiting those species that fail to adapt, that find themselves outpaced by changing circumstances or outcompeted by more successful forms. The evolutionary end. Without the possibility of extinction, there would be no imperative to evolve to our changing environment. And without the sculpting hand of natural selection, the unfit and ill-­ adapted would consume scarce resources. These principles apply far beyond the realm of biology. In business, technology, and ideas, we see the same relentless winnowing of the unfit and the elevation of the adaptive. The companies that thrive navigate the shifting landscape of consumer demand and technological change, while those that stagnate are swept away by the tides of creative destruction. On a personal level, we are all subject to the pressures of selection and the risk of extinction. Our skills, our knowledge, and our ways of thinking must constantly evolve to keep pace with an ever-changing world. Those who consistently adapt are the ones who thrive in the long run.Above all, remember that there are no permanent victories in the great game of life—­ only the ceaseless striving to stay one step ahead.Complacency will kill you. There’s no such thing as a permanent lead. No matter how well a species adapts to its environment, it must keep running just to stay in place.  The Red Queen effect results from the never-­ending arms race between predator and prey, parasite and host, and competitor and competitor. As one species evolves a new adaptation, others evolve countermeasures, leading to a constant escalation. The faster you adapt, the faster your rivals must respond, and vice versa. This has profound implications for the pace of evolution.In a static environment, natural selection might favor a leisurely pace of change. But in a world of constant change, where your competitors are always nipping at your heels, the premium is on speed. The species that thrive adapt quickly and turn the evolutionary crank faster than their rivals. But the Red Queen effect isn’t just about biological evolution. The same principle applies in any competitive domain—­ business, technology, or even ideas. Companies must continually innovate to stay ahead of their rivals. Technologies must evolve at a breakneck pace to avoid obsolescence. Ideas must adapt and grow to maintain their relevance. The key is recognizing that adaptation isn’t a one-time event but a continuous process. It’s not about reaching a finish line but maintaining a lead in an endless race. Those who rest on their laurels, who become complacent in their success, are quickly overtaken by hungrier, more agile competitors. But there is a catch when it comes to people. Once we gain an advantage, we want to hold on to it at all costs, and if we’re not careful, this can slow the pace of adaptation. Before long, our competitors catch up or find innovative ways to neutralize our strength. Sustained success comes from being flexible enough to change, letting go of what worked in the past, and focusing on what you need to thrive in the future.Standing still is the quickest path to extinction in a world of constant change. Victory goes to those who can continuously adapt.Nothing exists in isolation. Everything is connected. The ecosystem lens reveals that each species plays its part in a delicate balance of competition and cooperation. The actions of any one species can have consequences for many others in the same environment. In biology, an ecosystem is a community of living organisms interacting with each other and their physical environment. In an ecosystem, nothing exists in isolation—every creature is both predator and prey, both producer and consumer, locked in an intricate dance of energy and nutrients. Yet the concept of an ecosystem extends far beyond biology. You can see it nearly everywhere you look. Businesses operate within a complex network of companies, customers, competitors, suppliers, and regulators. Each entity relies on and influences the others, creating a dynamic interplay that determines which businesses thrive and which do not. Economies are also vast ecosystems comprising various sectors (like agriculture, manufacturing, and services) and actors (like workers, consumers, and governments). These components interact under the rules set by economic policies and market forces. Economic theories often explore how changes in one part of the ecosystem can lead to significant outcomes in another, much like the ripple effects seen in biological ecosystems.What all ecosystems have in common is their inherent complexity and their reductionist analysis. In an ecosystem, the whole is always more than the sum of its parts. The system’s behavior emerges from the countless interactions of its components, often in surprising and unpredictable ways. This suggests that to truly comprehend a complex system, we must look beyond the individual elements and consider the patterns of relationship and feedback that bind them together. Left to their own devices, many systems can take care of themselves, possessing abilities to correct and compensate for changes and external pressures. No matter how well-intentioned our interventions are, they often lead to unintended consequences as the solution to one problem quickly causes another, more significant problem. Be slow to intervene, and if you do, take the time to understand how actions in one part cascade into others. It pays to remember the motto of physicians, “First, do no harm.”A niche is a special place where a particular species or idea can thrive. It’s the ecological equivalent of a custom-­fitted suit tailored to its occupant’s unique needs and abilities. In a niche, you don’t have to be all things to all people—­ you just have to be the best at what you do.In biology, a niche is a species’ specific role and position within its ecosystem. It’s the unique combination of resources it consumes, the habitat it lives in, the interactions it has with other species. A place where a species’ adaptations flourish. But the concept of a niche extends far beyond the realm of ecology. In business, we talk about “market niches”—­ the specific segments of customers with particular needs or preferences. A company focusing on a niche can often out-compete larger, more general rivals by specializing, by becoming the best at serving that particular slice of the market, or by moving with velocity. The same principle applies to careers. By specializing in something unique and valuable, you can create a space where you can excel and your combination of skills thrives. The key is finding the niche that fits you, rewards your strengths, and neutralizes your weaknesses.This isn’t to say that occupying a niche is without risks. In fact, you become very fragile. If the environment changes, if consumer preferences shift, a once-­cozy niche can quickly become a tight squeeze. That’s why successful niche occupants are often those who can adapt and evolve their niche as the world around them changes. Specialists have less competition and stress, but only in times of stability. Generalists face more significant day‑to‑day challenges for resources and survival but have more flexibility to respond when times change.Self-­ preservation is a core instinct that drives all living things to protect and sustain their own existence. It’s the biological imperative that makes a gazelle run from the lion, the roots of a tree seek water, and bacteria evolve resistance to antibiotics. In the game of life, self-­ preservation is the only rule: stay alive. For humans, self-­ preservation goes beyond physical survival. It encompasses the protection of our psychological well-being, social status, and sense of identity. Anything that threatens how we see ourselves becomes a threat.While self-­ preservation is a necessary instinct, it can also be limiting. When we’re too focused on avoiding threats, we can easily miss opportunities right before us. Left unchecked, self-preservation can lead to stagnation. The key is to find balance: to protect what’s essential and be willing to let go of what no longer serves us.Listen to the voice that tells you when to be cautious, but don’t let it be the only voice you hear. Often, the most significant risk is not taking risks at all.Replication is the molecular magic trick that allows organisms to make copies of themselves to pass their genetic blueprints from one generation to the next. In the grand ballet of evolution, replication is the music that keeps the dance going. At its core, replication is about information transfer. It’s the process by which the instructions encoded in DNA are faithfully copied and transmitted. Whenever a cell divides or an organism reproduces, the replication machinery swings into action, ensuring the genetic message is preserved and propagated. However, replication is not a perfect process. Errors creep in, and mutations occur. And it’s these imperfections that fuel the engine of evolution. Without the variation introduced by replication errors, life would stagnate, unable to adapt to changing environments.Replication is helpful outside of biology, too. As a mental model, it teaches us that we don’t always need to reinvent the wheel. When you’re just starting, the quickest way to make great leaps is to imitate what others are already doing. This establishes an average baseline of performance. Once you get a sense and a feel for the environment, you can innovate and adapt to set a new baseline. The power of replication lies in its exponential nature. A single replicated entity can give rise to countless copies, each of which can replicate further. This is the power that viruses and viral ideas harness—­ the ability to spread explosively by exploiting the machinery of replication. Memes, beliefs, and practices also replicate, spreading from mind to mind and shaping the contours of our shared reality. But replication also comes with risks. Unchecked replication can be cancerous, leading to uncontrolled growth that threatens the health of the larger system. Effective replication requires enough structure and space to produce a copy and enough flexibility to adapt to environmental changes. Just because something has worked for a while doesn’t mean it will be effective in perpetuity. Maintaining a successful approach requires the ability to grow and modify that approach as required. As we contemplate replication’s role in life and thought, we must recognize its creative and destructive potential. We must create conditions that favor replicating what is true, sound, and beneficial while resisting the spread of what is false, harmful, or malignant. Cooperation is the surprising secret of success in the ruthless world of survival. If there is any one model that explains humanity, then this is it. Cooperation unleashed the potential of the human species. At first glance, cooperation seems to defy the logic of natural selection. Why would an organism invest its hard-­ earned resources in helping another rather than focusing solely on its own survival and reproduction? The answer lies in the magic of reciprocity and shared interest. When organisms can benefit more by cooperating than by competing, cooperative strategies emerge and flourish. Collaboration with others gives us options and opportunities that are unavailable when we insist on going it alone. But cooperation is not automatic. It requires specific conditions—repeated interactions, shared benefits, and mechanisms to prevent cheating. Cooperation is the foundation of civilization. Our species’ success is built on our ability to cooperate flexibly and at scale—­ to share knowledge, coordinate efforts, and create institutions that incentivize cooperative behavior. Cooperation underlies our achievements, from the division of labor in the economy to the norms of reciprocity in society. But, as in nature, human cooperation is not guaranteed. It requires constant cultivation and protection from the forces of selfishness and short-­ term thinking. It requires norms that reward cooperation and punish defection.Hierarchical  OrganizationHierarchy is the invisible scaffolding that organizes the living world. Hierarchies in biology aren’t just about structure but about function. They allow for specialization and division of labor, for the emergence of complex behaviors from simple rules. In the hierarchy of an ant colony, the queen, workers, and soldiers all play their roles, their interactions giving rise to the sophisticated operation of the colony as a whole. But hierarchy isn’t rigid or fixed. It’s fluid and dynamic, with levels constantly interacting and influencing one another. A change at one level can ripple across the entire hierarchy, transforming the system unexpectedly. While hierarchy is a way to manage complexity, it can also backfire. Too much hierarchy leads to unrest and instability. Too little leads to chaos. Most organizations promote cultures that emphasize rather than de‑emphasize an individual’s status, power, and place, which is part of the reason they get torn apart, as the fight to get to the top of the hierarchy takes precedence over the organization’s success. In the end, hierarchy is the organizing principle that allows scale from the microscopic to the magnificent.Incentives are the hidden engines that drive behavior. They’re the unseen forces that shape our choices, the carrots and sticks that guide our actions. Think of a business offering a bonus for hitting a sales target. The bonus is an incentive, the external reward that motivates the salesperson to excel. But incentives aren’t always so obvious. They can be subtle, even subconscious—­ the social approval we seek, the habits we form, the desires we pursue. Incentives are powerful because they tap into the fundamental wiring of the human brain. We’re hardwired to seek reward and avoid punishment, to optimize for the outcomes that serve our interests. When the incentives align with our goals, we thrive. When they don’t, we struggle. In a classroom, it’s easy to say that we’ll be motivated by doing the right thing; however, in reality, we’re driven mainly by rewards. We have difficulty turning down the pleasure of immediate gains, even if it takes us away from our ultimate goal. Often, short-­term and long-­term incentives differ. You might not feel like going to the gym today but want to be healthy as you age. Making choices to maximize your satisfaction today often leads to less reward down the road. Poorly designed incentives backfire, encouraging short-­ term thinking, unethical behavior, or unintended consequences. The key is to craft incentives that reward the behaviors that lead to long-­ term success. Ultimately, if you understand the incentive, you can predict the outcome. By shaping the incentives, we shape the outcomes. By aligning the incentives, we unlock the power of human potential.Tendency to Minimize Energy Output (Mental and physical)The tendency to limit energy output is the universal inclination to follow the path of least resistance. From the flow of a river to the behavior of a market, this tendency is the invisible hand that guides the actions of the world. Sometimes, our tendency to conserve energy helps us, and sometimes, it hurts us. While minimizing our output ensures we will have extra to draw on in times of increased need, it can also get in the way of learning. Experience doesn’t become learning without reflection, which is an energy expenditure. If we want to develop our thinking and get the most out of our environments, then we have to be aware of the natural tendency to minimize energy output and correct for it where doing so creates value.The Mental Models of Systems ThinkingFeedback loops are the engines of growth and change. They’re the mechanisms by which the output of a system influences its input.Complex systems often have many feedback loops, and it can be hard to appreciate how adjusting to feedback in one part of the system will affect the rest.Using feedback loops as a mental model begins with noticing the feedback you give and respond to daily. The model also provides insight into the value of iterations in adjusting based on the feedback you receive. With this lens, you gain insight into where to direct system changes based on feedback and the pace you need to go to monitor the impacts.Feedback loops are what make systems dynamic. Without feedback, a system does the same thing over and over. Understand them, respect them, and use them wisely.Equilibrium is the state of balance, where opposing forces cancel each other out. It’s the calm in the storm’s center, the stable point around which the chaos swirls. In a system at equilibrium, there’s no net change. Everything is in a steady state, humming along at a constant pace. However, systems are rarely static. They continuously adjust toward equilibrium but rarely stay in balance for long. Equilibrium is a ­ double-edged sword, both stability and stagnation. In our lives, we often act like we can reach an equilibrium: once we get into a relationship, we’ll be happy; once we move, we’ll be productive; once X thing happens, we’ll be in Y state. But things are always in flux. We don’t reach a certain steady state and then stay there forever. The endless adjustments are our lives. The trick is to find the right balance, strive for equilibrium where it’s needed, and know when to break free and embrace the dis-equilibrium that drives progress.Bottlenecks are the choke points, the narrow parts of the hourglass where everything slows down. They’re the constraints that limit the flow, the weakest links in the chain that determine the strength of the whole. In any system, the bottleneck is the part holding everything else back.The tricky thing about bottlenecks is that they’re not always obvious. It’s easy to focus on the parts of the system that are moving quickly and assume everything is fine. But the real leverage is in finding and fixing the bottlenecks. Speed up the slowest part, and you speed up the whole system.This is the theory of constraints in a nutshell. Figure out your bottleneck and focus all your efforts on alleviating it. Don’t waste time optimizing the parts that are already fast. They’re not the limiting factor.However, bottlenecks aren’t always the villains we make them out to be. Sometimes, they’re a necessary part of the system. Think of a security checkpoint at an airport. It slows everything down, but it’s there for a reason. Remove it, and you might speed things up, but at the cost of safety.The key is to be intentional about your bottlenecks. Choose them wisely, and make sure they’re serving a purpose. A deliberate bottleneck can be a powerful tool for focusing effort and maintaining quality. An accidental bottleneck is just a drag on the system.Bottlenecks are leverage points where a little effort can go a long way.Systems change as they scale up or down; neither is intrinsically better or worse. The right scale depends on your goals and the context. If you want to scale something up, you need to anticipate that new problems will keep ­ arising—​­ problems that didn’t exist on a smaller scale. Or you might need to keep solving the same problems in different ways.Think about a recipe. If you’re making a cake for four people, you use a certain amount of ingredients. But if you want to make a cake for four hundred people, you don’t just multiply the ingredients by one hundred. That’s not how scale works. You need to change the process and use bigger mixers and bigger ovens. You need a system that can handle the increased volume without breaking down. The challenge with scale is that it’s not always obvious how to achieve it. What works for a small system often breaks down at larger volumes. You have to anticipate the bottlenecks and the points where the system will strain under the increased load. And you have to be ready to re‑engineer your processes as you grow. If you’re building something, always be thinking about scale. How will this work when you have ten times as many customers? One hundred times? One thousand times? Build with scale in mind from the start, and you’ll be ready for the growth when it comes.Margin of safety is a secret weapon. It’s the buffer, the extra capacity, the redundancy that you build into a system to handle unexpected stress. It’s the difference between a bridge that can barely handle the expected load and one that can handle ten times that load without breaking a sweat. You can apply a margin of safety to any area of life with uncertainty and risk. The key is always to ask yourself: What if I’m wrong? What if things don’t go as planned? How much extra capacity must I build to handle the unexpected? But here’s the rub: margin of safety isn’t free. It means spending more upfront. In the short term, you’ll look overly cautious and leave immediate profits on the table. But in the long run, this apparent overcaution lets you survive when others break – and thrive when others merely survive.Margin of safety is the unsung hero of ­ long-​­term success. It’s not flashy. It’s not exciting, but it’s the foundation on which everything else is built. Master it, and you’ll be well on your way to navigating the uncertainties of life with confidence and stability.Churn is the silent killer of businesses. It’s the slow leak, the constant drip of customers slipping away, of users drifting off to find something new. The attrition eats away at your growth, forcing you to keep running just to stay in place. The thing about churn is that it’s often hidden. It’s not like a sudden crisis that grabs your attention. It’s a slow, quiet process that happens in the background. Churn can present opportunity. Like a snake shedding its skin, replacing components of a system is a natural part of keeping it healthy. New parts can improve functionality. When we use this model as a lens, we see that new people bring new ideas, and counterintuitively, some turnover allows us to maintain stability. Replacing what is worn out also allows us to upgrade and expand our capabilities, creating new opportunities. Some churn is inevitable. Too much can kill you.Algorithms are recipes. A list of crisp, unambiguous steps that tell you how to get from point A to point B. But they’re more than just directions. Algorithms are if‑then machines for tuning out the noise and zeroing in on the signal. Have the specs been met? Fol- low the algorithm and find out. Thinking algorithmically means searching for processes that reliably spit out the desired results, like a vending machine dispensing the same candy bar every time someone punches in E4.Critical mass isn’t just a science term; it’s a guide for understanding that often things happen slowly and then all at once. It’s the moment when a system goes from sputtering along to explosive growth. Like a nuclear chain reaction, once you hit critical mass, the reaction becomes self-sustaining.Through this lens we gain insight into the amount of material needed for a system to change from one state to another. Material can be anything from people and effort to raw material. When enough material builds up, systems reach their tipping point. When we keep going, we get sustainable change.Using critical mass as a lens for situations where you want different outcomes helps you identify both the design elements you need to change and the work you need to put in. Nearly everything is an emergent ­ effect—​­a table, a space shuttle, even ­ us—​­ combinations of ingredients that come together in a specific way to create something new. Emergence is the universe’s way of reminding us that when we combine different pieces in new ways, we get results that are more than the sum of their parts, often in the most unexpected and thrilling ways. Using this mental model is not about predicting emergent properties but acknowledging they are possible. There is no need to stick with what you know; mix it up and see what happens. Learn new skills, interact with new people, read new things.Irreducibility is about essence. It’s the idea that some things can’t be broken down into smaller parts without losing what makes them tick. It’s the idea that not everything can be explained by looking at its components. Emergent properties arise from complex systems that can’t be predicted by studying the individual parts. Grappling with irreducibility requires a shift in thinking. Instead of trying to break things down, sometimes you have to zoom out. Look at the big picture. Embrace the complexity. Because some problems don’t have neat, modular solutions. They’re irreducibly messy. Using irreducibility as a lens helps you focus on what you can change by understanding what really mattersLaw of Diminishing ReturnsDiminishing returns is the idea that the easy wins usually come first. The more you optimize a system, the harder it gets to eke out additional improvements, like squeezing juice from a lemon. The first squeeze is easy. The second takes a bit more work. By the tenth squeeze, you’re fighting for every last drop. Every bit of effort translates into significant gains when you’re a beginner. But as you level up, progress becomes more incremental. It takes more and more work to get better and better. That’s why going from good to great is much harder than going from bad to good. Understanding diminishing returns is crucial for allocating resources efficiently. You want to focus on where you can get the biggest bang for your buck. Sometimes, that means knowing when to stop optimizing and move on to something else.The Mental Models of MathematicsSample size is about how much of the world you’re looking at. It’s the number of data points you’re using to draw conclusions. Like trying to guess the average height of people in a city by measuring a few folks on the street. The more people you measure, the more confident you can estimate. One of the biggest mistakes we can make is drawing conclusions from too small a sample ­size—​­ like trying to guess a puzzle picture from only a few pieces. In most instances, increasing our sample size gives us valuable information that lets us see our situation in a new light. The catch is that large sample sizes are expensive. It takes time and money to collect all that data. So practitioners and researchers are always balancing the need for precision with the constraints of budget and deadline. They’ll often settle for the smallest sample size that can still give them a statistically significant result.Using this model means exploring what isn’t obvious and knowing how easy it is to corrupt our samples with bias. The next time you hear a statistic, think about the sample size. It’ll give you a clue about how seriously to take it. Remember: the larger the sample, the closer to the truth.Randomness is the chaos that underlies the cosmos. It’s the unpredictable, the uncontrollable, the stuff that doesn’t follow any discernible pattern.Randomness is what makes life surprising. It’s why you can’t predict the future with certainty. You might make plans, but there’s always the possibility of a random event throwing a wrench in the works. A flat tire, a chance encounter, a sudden inspiration. Randomness is the spice that keeps things interesting.The tricky thing about randomness is that humans are terrible at recognizing it. We see patterns where there are none. We attribute meaning to coincidence. We think we can beat the odds. But true randomness is immune to our predictions and superstitions. It doesn’t care about our theories or desiresRegression to the mean is the universe’s way of saying “not so fast.” It’s the tendency for extreme outcomes to be followed by more average ones. Extreme results are rarely repeated. The next time you see something extraordinary, enjoy it. But remember, it probably won’t last. Sooner or later, regression to the mean will come calling, pulling the exceptional back to the ordinary. That’s the way the universe keeps things in check.Multiplying by zero is the mathematical version of the Midas touch in reverse. Everything it touches turns to nothing. No matter how big or small a number is, when you multiply it by zero, you get zero. It’s the ultimate reset button. Multiplying by zero shows that we must be mindful of the zeros that will negate our other efforts. Just as in engineering, where one faulty component can make an entire system fail, not being reliable can have the same effect in life. When you multiply by zero, everything else becomes irrelevant.Equivalence is the art of making things interchangeable. It’s the idea that two things can be swapped out without changing the essence of what they’re a part of. Like swapping a red Lego brick for a blue one. The color changes, but the structure remains the same.Being equal doesn’t mean being the same. Different inputs can produce identical results, and there is more than one way to solve most problems.Equivalence lets us simplify complex systems. We can focus on the essentials instead of getting bogged down in details. We can see the forest for the trees. And we can make changes without fear of breaking the fundamental structure. Of course, equivalence has its limits. Not everything is interchangeable. You can’t swap out a car’s engine for a hamster wheel and expect the car to run. The art is in knowing where equivalence applies and where it doesn’t. It’s in recognizing the essential differences that matter, and the superficial differences that don’t. The next time you face a complex problem, try thinking about equivalence. Look for the underlying patterns. See if there are components you can swap out or simplify. You might just find a solution that’s been hiding in plain sight all along.Surface area is what determines how much an object interacts with its environment. The more surface area the more contact. Surface area can be good and bad. Sometimes, keeping it small is favorable, and sometimes, increasing our exposure is beneficial. Surface area teaches us that increasing cognitive diversity can give us fresh ideas and help us innovate. However, the model also reminds us that in many ways, the more we expose ourselves, the more vulnerable we are. Different situations require different surface areas.Global and local maxima as a model can be used differently to help us make the changes we need for success. It encourages us to see achieving our goals not as a steady upward trajectory but as a path full of peaks and valleys. Understanding that sometimes we have to go down to climb even higher helps us make ­ short-term sacrifices to play the long game. In engineering, you might be trying to maximize efficiency. In life, you might be trying to maximize happiness. But in all these cases, getting stuck on a local maximum is easy. You find a pretty good solution, and you stop looking for a better one. The next time you’re trying to optimize something, remember the concept of global and local maxima. Don’t just settle for the first peak you find. Keep exploring. Keep searching for that global maximum. It might be a tough climb, but the view from the top is worth it.The Mental Models of EconomicsScarcity shapes our choices and drives our actions. When something is scarce, it suddenly becomes valuable. We want it more because there is less. It’s the principle that underlies everything from the price of gold to the thrill of the hunt.Scarcity isn’t just about material things. It applies to time, opportunities, and ideas. We’re drawn to the exclusive, the ­ limited-​­edition, the one-of-a-kind.In economics, scarcity is a foundational principle. There are infinite wants and desires but limited resources. We can’t have everything, so we must choose. Scarcity guides those choices. Some businesses operate with a scarcity mentality, removing shock absorbers and operating lean, with just enough resources to produce the day’s goods. This model is prone to disruption with the slightest hiccup and signals to employees that they’re in a culture of scarcity, triggering our biological instinct toward self-preservation. We subconsciously hoard things of value to gain an individual advantage. Scarcity can work to your advantage. Imagine you’ve got a rare combination of qualities: you’re honest, hardworking, and smart. People like that are scarce, and the world disproportionately rewards them. It’s not just about being good at one thing; it’s about having a mix of traits. The key to navigating scarcity is understanding its power, recognizing when it’s driving our choices, and asking if those choices align with our true values and goals. Sometimes, scarcity creates real value. But sometimes, it’s just a mirage, a trick of the mind.Supply and demand are the push and pull determining availability and price. Their dance is never-ending. A sudden shortage can send prices soaring; a new discovery can send them crashing.But supply and demand aren’t just about price; they’re also about allocation. They determine who gets what, and how much of it. When supply is low and demand is high, resources flow to those willing and able to pay the most.Markets react to supply and demand. When demand exceeds supply, it encourages investment by companies to create substitutes or more supply. On the other hand, when supply exceeds demand, it discourages investment until a profitable balance is restored. Economic cycles are driven as much by human nature as by resources. When profits are flowing, it encourages overconfidence, greed, and complacency. When profits are nowhere to be found, it encourages fear, savings, and ruthless efficiency. As individuals, we’re all part of this dance. Every choice we make as consumers and every decision we make as producers shapes the contours of supply and demand. We are the market, collectively determining what has value and what doesn’t. Remember the forces at play the next time you’re at the store, negotiating a salary, or launching a product. You’re not just a passive participant but an active agent in supply and demand. Your choices matter. Make them wisely.Optimization is about making the most of what you have. It’s like cleverly solving a puzzle, finding a trick to skip steps and get to the answer faster. In a world of scarcity, optimization is powerful. It allows us to maximize our limited resources, whether time, money, or energy. But like any tool, it’s only as good as the hand that wields it. Used wisely, optimization unlocks hidden potential and drives extraordinary results. Used poorly, it leads to wasted effort and missed opportunities. Optimization often works for you until it doesn’t. It’s like the student who writes the answer but doesn’t show their work. Knowing when to use it, when to let it go, and when to avoid it can give you a key advantage.Life is full of trade-offs. Every choice has a cost. When you say yes to one thing, you say no to others. This is how the world works. It’s like gravity. You can’t escape it. Opportunity cost is what you give up when you make a choice. It’s the thing you can’t have because you picked something else. Say you have a free evening. You can work on your startup or go to a movie. If you work, you miss the fun. If you go to the movie, you miss the chance to make progress. Every choice has an opportunity cost because every time you say yes to something, you’re implicitly saying no to other things. You need to know your opportunity costs. This helps you make good trade-offs. A ­ trade-​­off is giving up one thing to get something else. It’s choosing between options. Each has good and bad points. Trade-offs are about priorities. When you make something, you face trade-offs. If you want it fast, you might lose some features. If you want it cheap, you might use lower-quality materials. In life, we face ­ trade-​­ offs all the time. Do you take a high-paying job with long hours? Or the low-paying one with more free time? Do you spend money now or save for later? Making good ­ trade-​­offs is about weighing the opportunity costs and benefits of each option and choosing the one that aligns best with your goals and values. It’s not always easy, but being conscious of the ­ trade-​­ offs you’re making can help you make better decisions. Wisdom is anticipating the consequences of your choices. In life and business, success is about making good trade-offs. It’s not about having it all. It’s about having what matters most. We all value different things. That’s what makes life rich. Opportunity cost is what you give up when you make a choice; trade-offs are the balancing acts you perform when deciding between competing options. They’re two sides of the same ­ coin— ­whenever you make a ­ trade-​­off, you’re incurring an opportunity cost for the option you didn’t choose. The key in both cases is to be thoughtful and intentional about your choices.Specialization is a ­trade-​­off: pursuing one course means not pursuing another. It’s narrowing your focus to broaden your impact. In a world of infinite knowledge and finite time, specialization is the key to unlocking mastery. It’s about going deep, not wide. Specialization has risks. If the world changes, what was once a valuable specialty can become obsolete. And yet, we need specialists. You wouldn’t want a generalist doing your brain surgery or a root canal. Here’s the catch: the more you specialize, the more you see how much other fields can teach you. The most exciting finds often happen at the edges between areas of knowledge. The trick is to specialize without getting stuck. To go deep, but also reach out. Ultimately, specialization is about where you spend your time and effort. It’s how you stand out. It’s choosing to be great at one thing instead of okay at many.Interdependence is the web that ties us all together. It’s the recognition that no person, no company, no country is an island. We’re all connected, all reliant on one another in countless ways, big and small. Interdependence is the reality that underlies the illusion of ­ self-​­sufficiency. No one is entirely ­self-​­made.Interdependence can be both a vulnerability and a strength. When we recognize our interdependencies, we can leverage them for mutual benefit. We can form alliances, partnerships, and ecosystems. We can create value that no single entity could create alone.Interdependence is the foundation of synergy, the alchemy of the whole being greater than the sum of its parts. On the other hand, if we depend on others for something critical, it can expose us if they fail to deliver or change their minds. It’s easy to be a good partner when things are going well. But you want to be careful with whom you depend in a crisis.Interdependence isn’t just a macro concept. It’s deeply personal. We’re all interdependent with our families, our friends, our communities. We rely on one another for support, for love, for meaning. Interdependence is the fabric of our social lives.Efficiency is about getting the most done with the least waste. It’s not always about finding the perfect answer but the one that works well enough without too much fuss. Efficiency matters because in real life, you never have all the time or resources you want. You have to make do with what you’ve got.But efficiency isn’t just about speed. It’s also about effectiveness and doing the right things. There’s no point in doing something fast if it’s not worth doing. True efficiency is about focusing on what matters most. It’s about saying no to the small stuff so you can say yes to the big stuff.Like everything, efficiency has its limits. There’s a point of diminishing returns, a threshold beyond which further optimization yields little gain. The key is finding the sweet spot, the point of maximum efficiency before the costs start outweighing the benefits.Efficiency works until it doesn’t. The more perfectly efficient a system, the more vulnerable it becomes to any change. While the idea can be hard to appreciate, maximal efficiency in the short term rarely leads to maximum ­ long-​­ term efficiency. A common benefit eroded in the quest for efficiency is a margin of safety. Through the efficiency lens, the opportunity cost of holding something like extra cash, inventory, or even employees may be seen as too high. However, supply shocks or environmental changes can make excess cash, inventory, and employees more valuable. Inefficiency in the short run is often very efficient in the long run when it leaves you better able to adapt to an uncertain world and increases the odds of survival. In a world of trade-offs, efficiency is a balancing act. It’s about making the most of what you have and leaving room for what you might need. It’s about being prepared for the future, not just optimized for the present.Debt is a double-edged sword. It’s a powerful tool to help you grow a business, buy a home, or seize an opportunity. But it’s also a chain that can bind your future or destroy you. When debt spirals out of control, it quickly turns dreams into nightmares. Debt isn’t just about money. It can be a favor you owe, a social obligation, or anything that creates a future obligation. We even have sleep debt. It can be hard to appreciate just how fragile debt makes you. It’s like driving across a vast desert without a spare tire. If everything goes perfectly, you will reach the other side, but the smallest hiccup will leave you stranded and desperate. Use debt wisely. Respect its power, but fear its edge. Remember, the more you borrow, the less room you have to weather life’s storms. While debt might seem cheap in the moment, the future often proves it to be more expensive than we imagined. The more you borrow, the less room you have to deal with uncertainty. Debt can give you leverage, but it can also take away your freedom. Respect its power but fear its edge.Monopoly and competition are the yin and yang of the business world. They’re the opposing forces that shape the landscape of every market, the tides that lift and sink the fortunes of every firm. To understand business, you must understand the dance between these poles. Competition is the default state of the market. It’s the Darwinian struggle where many firms vie for the same customers and resources. In a competitive market, no one firm has the power to set prices or dictate terms. They’re price takers, not price makers. They survive by being efficient, delivering value, and innovating. If competition is the natural state, monopoly is the entrepreneur’s dream. A monopoly dominates a market so completely that it becomes the market. Think of the only bridge that crosses a river. But monopolies inevitably sow the seeds of their own destruction. The question is how long they will last. We need both monopoly and competition. Competition keeps firms honest and drives innovation. But we also need monopolies’ deep pockets to fund big visions and moon shots. The ideal is a balance: enough competition to check monopolies but enough monopolies to reward innovation.Creative destruction is the engine of progress in a capitalist economy. It’s the process by which new innovations replace old ones, the cycle of birth and death that keeps an economy vibrant. It embodies the old adage: The only constant is change.In a dynamic economy, nothing is sacred. Newer, better ideas can disrupt every industry, company, and way of doing things. The smartphone replaced the flip phone, online streaming replaced movie rental stores, and cars replaced horses.While creative destruction can be painful for individual companies, it’s essential for the overall economy’s health. It prevents stagnation and ensures resources are always put to their most productive use. Without creative destruction, we’d still ride horses and rent VHS tapes.On one hand, creative destruction is the opportunity you’re looking for—the chance to disrupt an incumbent—to build something new and better. But on the other hand, it’s the threat you’re always guarding against—the possibility that you will be disrupted by the next big thing.Creative destruction isn’t just about business; it’s a metaphor for life. We are all subject to change, to the constant cycle of endings and beginnings. The key is to not cling too tightly to the old, but to embrace the new possibilities.Gresham’s Law states that bad money drives out good. But it’s not just about currency. The principle applies anytime there are two competing versions of something, one perceived as high quality and the other as low quality.In a sense, Gresham’s Law is the dark side of human nature. We’re wired to optimize for the short term, to get the most value for the least effort. If we can pass off the less valuable thing and keep the more valuable one, we will. Without consequences, bad behavior drives out good. Bad lending drives out good lending. Bad morals drive out good morals. Overcoming this requires constant effort.In the short run, bad often drives out good. But in the long run, true value wins out.Bubbles are a natural by-product of human nature. They happen when collective enthusiasm for an asset runs far ahead of its fundamental value. It’s the moment when the market becomes untethered from reality when prices are driven not by sober calculation but by mass delusion. Bubbles are a fascinating study of human psychology. They’re driven by greed and FOMO (fear of missing out). No one wants to be the sucker who sits on the sidelines while everyone else gets rich. But there’s also an element of genuine belief, of conviction that this time is different, that the old rules no longer apply. While ultimately destructive, bubbles also serve a function. They’re the market’s way of exploring new frontiers, of testing new possibilities. Many of the innovations we take for granted ­ today— ­ from cars to computers to the internet ­ itself—​­ were once the subject of speculative manias. Bubbles fund the infrastructure for future revolutions, even as they leave a trail of financial wreckage in their wake. Bubbles remind us that markets are driven by human emotions and beliefs. They’re a mirror held up to our collective hopes, dreams, and delusions. The next time you catch yourself saying, “this time is different,” remember that all bubbles pop eventually. Like a balloon that can only expand so far, bubbles eventually burst, and the game ends abruptly without warning. Keeping yourself grounded in value and economic reality, not in story or hype, is key to standing alone as a bubble expands.The audience is the invisible participant in every work of art. They are the eyes that see, the ears that hear, the minds that interpret. Without an audience, art is like a tree falling in an empty ­ forest—­ it may make a sound, but does it matter? The audience is what gives art its meaning, its purpose, and its very existence. Every observer infuses art with personal significance, transforming it into a co-creation. A painting of a sunset may evoke feelings of peace and beauty for one person and feelings of melancholy and loss for another. The artwork is the same, but the audience is different, so the meaning is different. In this sense, the audience is a cocreator of the art.Great artists design their work for these silent judges, balancing authenticity with expectation without succumbing to pandering.The audience is their silent collaborator and their ultimate judge.In a world where so much can be faked, the audience is something real. You can fake likes, followers, and reviews, but you can’t fake the genuine human experience of engaging with art. The spontaneous laughter, unexpected tears, and long, thoughtful silence are the honest reactions that both the audience and the artists live for.Never forget your audience, but never let them dictate your creation. Picture this: you’re browsing a bookstore, scanning the shelves for your next read. You pick up a book with a shadowy figure on the cover, a magnifying glass in hand. Instantly, you know what kind of story awaits you within those pages. This is the power of genre— the unspoken understanding between creator and audience that shapes how we experience art.But genre is more than just a label; it’s a set of conventions, an understanding between the artist and the audience. When we pick up a mystery novel, we expect a crime, some clues, and a detective. When we go to a rock concert, we expect loud guitars, driving rhythms, and a rebellious attitude. Genre sets the parameters of our experience, even as it gives the artist a foundation to build upon or rebel against.Think of genre as a game with rules. The rules provide structure, but they also create opportunities for creativity. A sonnet has a strict ­ form—​­ fourteen lines, a specific rhyme ­ scheme—​­ but within those constraints, poets have found endless ways to express love, loss, joy, and sorrow. The rules of the genre game inspire ingenuity, challenging artists to create something fresh within the familiar.But genres are not static; they are constantly evolving. Look at the way rock music has transformed over the decades. What began as a rebellious offshoot of blues and country in the 1950s has splintered into countless subgenres, each with a distinct style and audience. From the psychedelic experimentation of the 1960s to the punk revolution of the ’70s to the grunge explosion of the ’90s, rock has reinvented itself time and again. What was once transgressive becomes mainstream, and new forms emerge to take its place.Navigating genre is a delicate art. Sticking too closely to the conventions may cause your work to be dismissed as formulaic. On the other hand, if you stray too far you risk losing your audience. The key is to find the sweet ­ spot—​­ honoring the genre’s expectations while bringing something new and personal to the table.Ultimately, genre is a tool—a way of framing the conversation between the artist and the audience. It provides common ground, a starting point for the journey together. But the true power of art lies in the way it can transcend genre, using convention as a springboard to take us places we’ve never been.Contrast is the spice of life and art. It’s the clash of opposites that energizes a work and jolts our senses. Without contrast, the world is bland. With it, the world dances with dark and light, loud and soft, rough and smooth. Contrast makes us notice.Contrast isn’t just visual. In music, quiet moments make loud ones explosive. Gentle ballads set the stage for crashing anthems. In literature, calm before the storm makes extraordinary events remarkable. Contrast gives art emotional power.Contrast creates interest and engagement. Our brains are wired to pay attention to changes and differences. We tune out the monotonous, but we snap to attention when something breaks the pattern. Artists use contrast to manipulate our attention, direct our focus, and shape our work experience.Contrast is a universal principle. Light and dark, hot and cold, life and ­ death—​­ the world is defined by contrasts. Darkness helps us understand light. Winter makes us appreciate spring. Contrast gives meaning to existence.Framing is the art of context, the craft of shaping perception. It’s how we present information, the lens through which we invite others to view the world. Like a photographer choosing what’s in the frame, we constantly decide what to emphasize, minimize, or leave out. These often unconscious choices profoundly influence how others understand and respond.In psychology, framing is a key concept in understanding ­ decision- ­ making. Present the same options in different ways, and people’s choices change. Is it a muffin or a cake? The thing doesn’t change, but its packaging does.For marketers and advertisers, framing is a potent tool. A car can be framed as a status symbol, an adventure machine, or a sensible family vehicle. A watch can be about punctuality, or it can be about luxury and prestige. The product stays the same, but the story changes. The right frame makes the ordinary extraordinary.But framing isn’t just about persuasion. It’s also about understanding, about making sense of the complex world around us. We all carry frames in our ­ minds—​­ mental models of how things work, cultural narratives, and personal beliefs. These frames shape how we interpret information, how we explain events, and how we imagine possibilities.Framing’s power lies in its subtlety. Unlike a logical argument, a frame doesn’t need to be explicitly stated to have an effect. It works on an emotional, often subconscious level. A ­ well-​­crafted frame can make an idea feel intuitive, even inevitable, without the audience knowing why.Framing is the silent partner in every communication, the hidden hand shaping understanding. Like any powerful tool, framing can be used for good or ill. It can illuminate truth, or it can obscure it. It can empower people to see new possibilities, or it can subtly limit their thinking to narrow predefined channels.Rhythm is the universe’s heartbeat, the pulse animating life. From our steady heartbeats to the sun’s rise and fall, from crashing waves to swaying trees, rhythm is the pattern underlying existence. It’s the organizing principle bringing order to chaos, the recurring cycle shaping time.In music, rhythm is the backbone supporting melody and harmony. Without rhythm, music would be a formless wash of sound, lacking structure and impact. The steady beat of the drum, the driving strum of the guitar, the pulsing throb of the ­ bass—​­ these rhythms grab us on a visceral level, moving our bodies and stirring our souls.But rhythm isn’t just about regularity, the even spacing of beats. It’s also variation, the interplay of different rhythmic patterns. In jazz, the syncopated rhythms and the unexpected accents give the music an improvisational feel. In classical music, the shifting rhythms, from the stately march to the lively dance, convey the piece’s emotional arc.Rhythm is also fundamental to language. The cadence of a phrase, the meter of a poem, the rise and fall of a great orator’s ­ speech—​­ these rhythms communicate meaning beyond the literal content of the words. They create their own music, a pattern resonating in the ear and lingering in the mind.Even in our daily lives, rhythm plays a crucial role. The routines we establish, the habits we cultivate, the cycles of work and rest, of activity and ­ reflection—​­ these rhythms give structure and meaning to our existence. Without rhythm, life would be a formless blur, a ceaseless stream of unrelated moments. Rhythm allows us to make sense of time, to find our place in life’s larger patterns.Melody is music’s soul, the ethereal thread weaving through sound’s tapestry. It’s the part of a song that we hum in the shower, the tune that gets stuck in our head and won’t let go. Melody is the musical expression of a fundamental human need: the need to tell a story, convey an emotion, and connect with others beyond words.A melody is simply a sequence of notes, a pattern of pitches and rhythms. But melody’s magic transcends these basic building blocks. A great melody is more than the sum of its parts. It has a shape, a contour, an arc that carries us from one note to the next. It has a sense of inevitability, as if each note is the only possible choice, even as the melody surprises us with its freshness and novelty.In this sense, melody is a lot like language. As we arrange words infinitely to express different ideas, we arrange notes to express emotions and experiences. A rising melody might convey a sense of hope and aspiration, while a falling melody might suggest sadness or resignation. A melody with large leaps might feel adventurous and daring, while one with small, stepwise motion might feel intimate and confiding.But melody isn’t just about individual expressions. It’s also about communication and connection. When a melody resonates with us, it’s as if the composer is speaking directly to our hearts. We feel understood, validated, less alone. And when we sing or play a melody with others, we create a bond, a shared experience that transcends our individual differences. This is why melody has such power across cultures and throughout history. From the chants of ancient rituals to the latest pop hits, melody has been a constant in human musical expression. It’s a universal language, requiring no translation or explanation. A beautiful melody can move us regardless of whether we understand the words or know the cultural context. Of course, not all melodies are equal. Just as there are great works of literature and forgettable pulp novels, there are melodies that stand the test of time and others that quickly fade from memory. The best melodies balance the familiar and the new. They have a memorable shape, a satisfying resolution, a feeling of completeness. In a world often fragmented and chaotic, melody is a source of unity and coherence, a way of finding beauty and meaning amid the noise.Representation is the mental shorthand we use to navigate the complexities of reality, the symbols and images we use to communicate our thoughts and experiences. Representation is how we construct meaning and bridge the gap between the raw data of our senses and the narratives we tell about ourselves and our world. At its core, representation is about standing in for something else. A word stands in for an object or concept, a map for a territory, a musical note for a sound. We use representations because we can’t hold the entirety of reality in our minds at once. We need abstractions, simplifications, and models that we can manipulate and reason about. But representation is not neutral. Every representation is an interpretation, a way of framing reality that highlights some aspects and obscures others. An emoji might represent a feeling, but it doesn’t show the lived experience that causes that feeling. In this sense, representation is always a kind of distortion. It’s a lens that shapes how we see the world, for better or worse. A good representation can illuminate hidden truths, help us see patterns and connections we might otherwise miss. But a bad representation can mislead us, reinforce stereotypes and prejudices, limit our ability to imagine alternatives. Representation is not just about mirroring reality; it’s also about shaping it. The representations we create and consume can influence how we think and act, to change the very world they purport to describe. A powerful piece of art can shift cultural attitudes, a persuasive political narrative can sway elections, a compelling scientific model can guide research and policy. In this way, representation is a kind of feedback loop. We create representations based on our understanding of reality, but those representations, in turn, shape our understanding, which influences the representations we create next. It’s a constant dance between map and territory, symbol and referent.The plot is the story’s engine, propelling characters and events through time. It’s the sequence of causally connected events that leads from the beginning of a narrative to its resolution. Without a plot, a story is just disconnected moments and unrelated incidents. With a plot, a story becomes a journey, a transformative experience for characters and readers. At its most basic level, a plot is a series of events connected by cause and effect. Event A leads to Event B, which leads to Event C, and so on, until the story reaches its resolution. But a good plot is more than just a linear chain of events. It’s a complex web of actions and reactions, of conflicts and resolutions, of setups and payoffs. Conflict is the heart of any plot. Without conflict, characters have no story or reason to act or change. Conflict can take many ­ forms—​­ person versus person, person versus nature, person versus society, person versus self. But all conflicts share a fundamental structure: a character wants something but faces obstacles. The plot is the events that arise from the character’s attempts to overcome these obstacles and achieve their goal. But plot is not just about external conflicts and goals. It’s also about the internal journey of the characters, the way they grow and change because of the events they experience. A good plot presents a character with external challenges and forces them to confront their own flaws, beliefs, and desires. In this sense, the plot is a crucible for the character. It’s the fire that tests and transforms the protagonist, revealing their true nature and potential. A character who ends a story unchanged, unaffected by the plot’s events, is a character in a story that hasn’t really gone anywhere. The best plots leave characters fundamentally altered, through triumph or tragedy. Plot is also personal. The most powerful story in the world is the one you tell yourself about the obstacles and challenges in front of you. A positive story doesn’t always ensure success, but a negative one almost guarantees failure. Once a story takes root, no matter how false, it can be hard to change. This applies to both humanity in general and to each of us individually. Change the story to change the results.At their core, characters are bundles of traits and motivations, of habits and histories, of strengths and flaws. They are the total of their choices and actions, the product of genetics, choices, and circumstances. But a great character is more than just a list of attributes. A great character is a paradox, a contradiction, a mystery that unfolds over the course of a story. In many ways, character is destiny. The choices a character makes, the actions they take, flow inevitably from who they are. A cautious, thoughtful character will approach a problem differently than an impulsive, emotional one. A character with a strong moral compass will make different decisions than one with a flexible relationship to the truth. Obstacles reveal character. But character is not static; it is not a fixed point but a journey. The best characters are the ones who grow and change throughout a story and who are transformed by the events of the plot and the interactions with other characters. Think of Ebenezer Scrooge, the miserly old man who learns the true meaning of generosity. Understanding a person’s character allows you to see someone for who they are at their core and step into their shoes. This helps you understand why they make their choices, predict their behavior, and empathize with their story. But remember, character is not set in stone. What happened yesterday is over. Today’s obstacles and challenges are nothing more than an opportunity to take a step toward or away from the person you want to be. No single choice satisfies the pursuit, only repeated steps in the right direction.The setting is the stage upon which the drama of the story unfolds, the physical and temporal context that shapes and reflects the actions of characters. An active participant in the narrative, setting is a force that can enable or hinder, reveal or conceal, enlighten or deceive. The setting is not just where the story happens but why it happens. Setting anchors a story in time and place, providing sensory details that make it real. But setting is more than just physical description. It’s also the social, cultural, and historical context that defines the parameters of what is possible and what is permissible for the characters. A story set in medieval Europe will have different constraints and opportunities than one set in ­ modern-​­ day Tokyo. A character in a small, gossipy village will face different challenges than one in a large, anonymous city. Setting shapes the choices characters make, the conflicts they face, the resolutions they find. But setting is not just a ­one-​­way street, not just the environment acting upon the characters. Characters also act upon and interact with their setting. They navigate its challenges, exploit its opportunities, and leave their mark on its landscape. Every story is a symbiotic relationship between character and setting, a reciprocal exchange of influence and transformation. Setting is the silent force that influences our fate. What we think and do is greatly impacted by our environment. This leads to a powerful and profound point: to change your behavior, change your environment. If you don’t, it will change you.Performance is the art of the ephemeral, the fleeting moment of creative expression existing only in the here and now. It’s where the boundaries between art and life blur, the artist’s body and actions become the medium, and the audience’s presence and participation become integral. At its core, performance is about presence, about the immediacy and intimacy of live action. In a world increasingly mediated by screens, live performance asserts the primacy of embodied experience, of the direct encounter between performer and spectator. It’s a reminder that art is not just a thing to be consumed but an event to be lived. But performance is also about absence, the gaps and spaces between action and interpretation, intention and reception. Unlike a painting or a sculpture, a performance can never be fully captured or contained. It exists only in the memories and testimonies of those who were there, in the ripples and reverberations it sends through the culture. Performance embraces the contingency and ­open- endedness of the live event, the sense that anything could happen, that meaning is always in the making. This contingency is both the power and the challenge of performance. It allows for spontaneity and responsiveness, adapting to and incorporating the unpredictable elements of the moment. Yet, it makes performance resistant to the control and perfection other art forms aspire to. A performance is always a collaboration with chance, a dance with the unknown. As audience members, we are not just passive observers but active participants in the performance. Our presence, reactions, and energy all become part of the work. Think of fans transmitting energy to a team to rally them from behind with a few minutes left in the game. Performance invites us to be cocreators, to complete the work through our own interpretations and responses. In so doing, we become part of something larger than ourselves. When we are fully present in any performance where someone is making themselves vulnerable, we may just glimpse the raw, unedited, unpolished essence of what it means to be human.The Mental Models of Military and WarOne of the most valuable military tactics is the habit of “personally seeing the front” before making decisions – not always relying on advisors, maps, and reports, all of which can be faulty or biased. The Map/Territory model, as does the incentive model, illustrates the problem of not seeing the front. Leaders of any organization can generally benefit from seeing the front, as it provides firsthand information and tends to improve the quality of secondhand information.The asymmetry model leads to an application in warfare whereby one side seemingly “plays by different rules” than the other side due to circumstance. Generally, this model is applied by an insurgency with limited resources. Unable to out-muscle their opponents, asymmetric fighters use other tactics, as with terrorism creating fear that’s disproportionate to their actual destructive ability.The Second World War was a good example of a two-front war. Once Russia and Germany became enemies, Germany was forced to split its troops and send them to separate fronts, weakening their impact on either front. Opening a two-front war can often be a useful tactic, as can solving a two-front war or avoiding one, as in the example of an organization tamping down internal discord to focus on its competitors.Though asymmetric insurgent warfare can be extremely effective, competitors have developed counterinsurgency strategies over time. Recently and famously, General David Petraeus of the United States led the development of counterinsurgency plans involving no additional force but substantial gains. Tit-for-tat warfare or competition often leads to a feedback loop that demands insurgency and counterinsurgency.The Mental Models of Human Nature and JudgmentFundamentally, the modern world operates on trust. Familial trust is generally a given (otherwise we’d have a hell of a time surviving), but we also choose to trust chefs, clerks, drivers, factory workers, executives, and many others. A trusting system is one that tends to work most efficiently; the rewards of trust are extremely high.Highly responsive to incentives, humans have perhaps the most varied and hardest to understand set of incentives in the animal kingdom. This causes us to distort our thinking when it is in our own interest to do so. A wonderful example is a salesman truly believing that his product will improve the lives of its users. It’s not merely convenient that he sells the product; the fact of his selling the product causes a very real bias in his own thinking.Ivan Pavlov very effectively demonstrated that animals can respond not just to direct incentives but also to associated objects; remember the famous dogs salivating at the ring of a bell. Human beings are much the same and can feel positive and negative emotion towards intangible objects, with the emotion coming from past associations rather than direct effects.Humans have a tendency to feel envious of those receiving more than they are, and a desire “get what is theirs” in due course. The tendency towards envy is strong enough to drive otherwise irrational behavior, but is as old as humanity itself. Any system ignorant of envy effects will tend to self-immolate over time.Based on past association, stereotyping, ideology, genetic influence, or direct experience, humans have a tendency to distort their thinking in favor of people or things that they like and against people or things they dislike. This tendency leads to overrating the things we like and underrating or broadly categorizing things we dislike, often missing crucial nuances in the process.Anyone who has been alive long enough realizes that, as the saying goes, “denial is not just a river in Africa.” This is powerfully demonstrated in situations like war or drug abuse, where denial has powerful destructive effects but allows for behavioral inertia. Denying reality can be a coping mechanism, a survival mechanism, or a purposeful tactic.One of the most useful findings of modern psychology is what Daniel Kahneman calls the Availability Bias or Heuristic: We tend to most easily recall what is salient, important, frequent, and recent. The brain has its own energy-saving and inertial tendencies that we have little control over – the availability heuristic is likely one of them. Having a truly comprehensive memory would be debilitating. Some sub-examples of the availability heuristic include the Anchoring and Sunk Cost Tendencies.8. Representativeness HeuristicThe three major psychological findings that fall under Representativeness, also defined by Kahneman and his partner Tversky, are:b. Tendency to Stereotype The tendency to broadly generalize and categorize rather than look for specific nuance. Like availability, this is generally a necessary trait for energy-saving in the brain.c. Failure to See False ConjunctionsMost famously demonstrated by the Linda Test, the same two psychologists showed that students chose more vividly described individuals as more likely to fit into a predefined category than individuals with broader, more inclusive, but less vivid descriptions, even if the vivid example was a mere subset of the more inclusive set. These specific examples are seen as more representative of the category than those with the broader but vaguer descriptions, in violation of logic and probability.Human beings are one of many social species, along with bees, ants, and chimps, among many more. We have a DNA-level instinct to seek safety in numbers and will look for social guidance of our behavior. This instinct creates a cohesive sense of cooperation and culture which would not otherwise be possible but also leads us to do foolish things if our group is doing them as well.Human beings have been appropriately called “the storytelling animal” because of our instinct to construct and seek meaning in narrative. It’s likely that long before we developed the ability to write or to create objects, we were telling stories and thinking in stories. Nearly all social organizations, from religious institutions to corporations to nation-states, run on constructions of the narrative instinct.We like to call other species curious, but we are the most curious of all, an instinct which led us out of the savanna and led us to learn a great deal about the world around us, using that information to create the world in our collective minds. The curiosity instinct leads to unique human behavior and forms of organization like the scientific enterprise. Even before there were direct incentives to innovate, humans innovated out of curiosity.The psychologist Steven Pinker calls our DNA-level instinct to learn grammatically constructed language the Language Instinct. The idea that grammatical language is not a simple cultural artifact was first popularized by the linguist Noam Chomsky. As we saw with the narrative instinct, we use these instincts to create shared stories, as well as to gossip, solve problems, and fight, among other things. Grammatically ordered language theoretically carries infinite varying meaning.13. First-Conclusion BiasAs Charlie Munger famously pointed out, the mind works a bit like a sperm and egg: the first idea gets in and then the mind shuts. Like many other tendencies, this is probably an energy-saving device. Our tendency to settle on first conclusions leads us to accept many erroneous results and cease asking questions; it can be countered with some simple and useful mental routines.It’s important for human beings to generalize; we need not see every instance to understand the general rule, and this works to our advantage. With generalizing, however, comes a subset of errors when we forget about the Law of Large Numbers and act as if it does not exist. We take a small number of instances and create a general category, even if we have no statistically sound basis for the conclusion.15. Relative Satisfaction/Misery TendenciesThe envy tendency is probably the most obvious manifestation of the relative satisfaction tendency, but nearly all studies of human happiness show that it is related to the state of the person relative to either their past or their peers, not absolute. These relative tendencies cause us great misery or happiness in a very wide variety of objectively different situations and make us poor predictors of our own behavior and feelings.As psychologists have frequently and famously demonstrated, humans are subject to a bias towards keeping their prior commitments and staying consistent with our prior selves when possible. This trait is necessary for social cohesion: people who often change their conclusions and habits are often distrusted. Yet our bias towards staying consistent can become, as one wag put it, a “hobgoblin of foolish minds” – when it is combined with the first-conclusion bias, we end up landing on poor answers and standing pat in the face of great evidence.Once we know the outcome, it’s nearly impossible to turn back the clock mentally. Our narrative instinct leads us to reason that we knew it all along (whatever “it” is), when in fact we are often simply reasoning post-hoc with information not available to us before the event. The hindsight bias explains why it’s wise to keep a journal of important decisions for an unaltered record and to re-examine our beliefs when we convince ourselves that we knew it all along.Justice runs deep in our veins. In another illustration of our relative sense of well-being, we are careful arbiters of what is fair. Violations of fairness can be considered grounds for reciprocal action, or at least distrust. Yet fairness itself seems to be a moving target. What is seen as fair and just in one time and place may not be in another. Consider that slavery has been seen as perfectly natural and perfectly unnatural in alternating phases of human existence.We tend to over-ascribe the behavior of others to their innate traits rather than to situational factors, leading us to overestimate how consistent that behavior will be in the future. In such a situation, predicting behavior seems not very difficult. Of course, in practice this assumption is consistently demonstrated to be wrong, and we are consequently surprised when others do not act in accordance with the “innate” traits we’ve endowed them with.Stress (Including Breaking Points)Stress causes both mental and physiological responses and tends to amplify the other biases. Almost all human mental biases become worse in the face of stress as the body goes into a fight-or-flight response, relying purely on instinct without the emergency brake of Daniel Kahneman’s “System 2” type of reasoning. Stress causes hasty decisions, immediacy, and a fallback to habit, thus giving rise to the elite soldiers’ motto: “In the thick of battle, you will not rise to the level of your expectations, but fall to the level of your training.”A major problem with historiography – our interpretation of the past – is that history is famously written by the victors. We do not see what Nassim Taleb calls the “silent grave” – the lottery ticket holders who did not win. Thus, we over-attribute success to things done by the successful agent rather than to randomness or luck, and we often learn false lessons by exclusively studying victors without seeing all of the accompanying losers who acted in the same way but were not lucky enough to succeed.We might term this Boredom Syndrome: Most humans have the tendency to need to act, even when their actions are not needed. We also tend to offer solutions even when we do not have knowledge to solve the problem.What a man wishes, he also believes. Similarly, what we believe is what we choose to see. This is commonly referred to as the confirmation bias. It is a deeply ingrained mental habit, both energy-conserving and comfortable, to look for confirmations of long-held wisdom rather than violations. Yet the scientific process – including hypothesis generation, blind testing when needed, and objective statistical rigor – is designed to root out precisely the opposite, which is why it works so well when followed.The modern scientific enterprise operates under the principle of falsification: A method is termed scientific if it can be stated so that a certain defined result would cause it to be proved false. Pseudo-knowledge and pseudo-science operate and propagate by being unfalsifiable. As with astrology, we cannot prove them either correct or incorrect because the conditions under which they would be shown false are never stated.]]></content:encoded></item><item><title>Unrolling the Codex agent loop</title><link>https://openai.com/index/unrolling-the-codex-agent-loop/</link><author>tosh</author><category>hn</category><pubDate>Fri, 23 Jan 2026 20:42:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Banned C++ features in Chromium</title><link>https://chromium.googlesource.com/chromium/src/+/main/styleguide/c++/c++-features.md</link><author>szmarczak</author><category>hn</category><pubDate>Fri, 23 Jan 2026 20:27:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[This document is part of the more general Chromium C++ style guide. It summarizes the supported state of new and updated language and library features in recent C++ standards and the Abseil library. This guide applies to both Chromium and its subprojects, though subprojects can choose to be more restrictive if necessary for toolchain support.The C++ language has in recent years received an updated standard every three years (C++11, C++14, etc.). For various reasons, Chromium does not immediately allow new features on the publication of such a standard. Instead, once Chromium supports the toolchain to a certain extent (e.g., build support is ready), a standard is declared “”, with new language/library features banned pending discussion but not yet allowed.You can propose changing the status of a feature by sending an email to cxx@chromium.org. Include a short blurb on what the feature is and why you think it should or should not be allowed, along with links to any relevant previous discussion. If the list arrives at some consensus, send a codereview to change this file accordingly, linking to your discussion thread.If an item remains on the TBD list two years after initial support is added, style arbiters should explicitly move it to an appropriate allowlist or blocklist, allowing it if there are no obvious reasons to ban.The current status of existing standards and Abseil features is:Default allowed; see banned features belowDefault allowed; see banned features belowInitially supported November 13, 2023; see allowed/banned/TBD features belowInitially supported January 2026; see allowed/banned/TBD features belowDefault allowed; see banned/TBD features below. The following dates represent the start of the two-year TBD periods for certain parts of Abseil:absl::linked_hash_set & map: Initially added to third_party Dec 30, 2025Banned features and third-party codeThird-party libraries may generally use banned features internally, although features with poor compiler support or poor security properties may make the library unsuitable to use with Chromium.Chromium code that calls functions exported from a third-party library may use banned library types that are required by the interface, as long as:The disallowed type is used only at the interface, and converted to and from an equivalent allowed type as soon as practical on the Chromium side.The feature is not banned due to security issues or lack of compiler support. If it is, discuss with cxx@chromium.org to find a workaround.C++11 Banned Language FeaturesThe following C++11 language features are not allowed in the Chromium codebase.Inline Namespaces [banned] Allows better versioning of namespaces. An integer of at least 64 bits.User-Defined Literals [banned] Allows user-defined literal expressions.C++11 Banned Library FeaturesThe following C++11 library features are not allowed in the Chromium codebase.<cctype>, <ctype.h>, <cwctype>, <wctype.h> [banned] Provides utilities for ASCII characters.<cfenv>, <fenv.h> [banned] Provides floating point status flags and control modes for C-compatible code. A standard date and time library. Exception throwing and handling.Engines And Generators From <random> [banned] Methods of generating random numbers. Provides compile-time rational numbers. A standard regular expressions library.std::aligned_{storage,union} [banned] Creates aligned, uninitialized storage to later hold one or more objects. Declares a function object bound to certain arguments. Wraps a standard polymorphic function. Allows shared ownership of a pointer through reference counts.std::{sto{i,l,ul,ll,ull,f,d,ld},to_string} [banned] Converts strings to/from numbers. Allows a weak reference to a .Thread Support Library [banned] Provides a standard multithreading library using  and associatesC++17 Banned Language FeaturesThe following C++17 language features are not allowed in the Chromium codebase.UTF-8 character literals [banned] A character literal that begins with  is a character literal of type  (C++17) or  (C++20). The value of a UTF-8 character literal is equal to its ISO 10646 code point value.C++17 Banned Library FeaturesThe following C++17 library features are not allowed in the Chromium codebase.Mathematical special functions [banned] A variety of mathematical functions.Parallel algorithms [banned] Many of the STL algorithms, such as the ,  and  methods, now support the parallel execution policies: , , and  which translate to “sequentially”, “parallel” and “parallel unsequenced”.std::aligned_alloc [banned] Allocates uninitialized storage with the specified alignment. A type-safe container for single values of any type. The contents of a single memory unit.  has the same size and aliasing rules as , but does not semantically represent a character or arithmetic value, and does not expose operators other than bitwise ops. A standard way to manipulate files, directories, and paths in a filesystem.std::{from,to}_chars [banned] Locale-independent, non-allocating, non-throwing functions to convert values from/to character strings, designed for use in high-throughput contexts.std::{pmr::memory_resource,polymorphic_allocator} [banned] Manages memory allocations using runtime polymorphism.std::timespec_get [banned] Gets the current calendar time in the given time base.std::uncaught_exceptions [banned] Determines whether there are live exception objects.Transparent std::owner_less [banned] Function object providing mixed-type owner-based ordering of shared and weak pointers, regardless of the type of the pointee. Returns a  that tracks ownership of  by all existing s that refer to .C++20 Allowed Language FeaturesThe following C++20 language features are allowed in the Chromium codebase.Abbreviated function templates [allowed] Function params of type  become syntactic sugar for declaring a template type for each such parameter. Specified that a function may only be used in a compile-time context.Constraints and concepts [allowed] Allows bundling sets of requirements together as named concepts, then enforcing them on template arguments.Default comparisons [allowed] Requests that the compiler generate the implementation of any comparison operator, including . Prefer non-member comparison operators. When defaulting , also explicitly default . Together these are sufficient to allow any comparison as long as callers do not need to take the address of any non-declared operator.Designated initializers [allowed] Allows explicit initialization of subsets of aggregate members at construction.__has_cpp_attribute [allowed] Checks whether the toolchain supports a particular standard attribute. Ensures that a variable can be compile-time initialized. This is like a milder form of  that does not force variables to be const or have constant destruction.Initializers for bit-field members [allowed] Allows specifying the default initial value of a bit-field member, as can already be done for other member types.Lambda captures with initializers that are pack expansions [allowed] Allows initializing a capture with a pack expansion.Language feature-test macros [allowed] Provides a standardized way to test the toolchain's implementation of a particular language feature.[[likely]], [[unlikely]] [allowed] Tells the optimizer that a particular codepath is more or less likely than an alternative.Range-for statements with initializer [allowed] Like C++17's selection statements with initializer. Particularly useful before C++23, since temporaries inside range-expressions are not lifetime-extended until the end of the loop before C++23.Three-way comparison (“spaceship”) operator [allowed] Compares two objects in a fashion similar to . Perhaps most useful when defined as an overload in a class, in which case it can replace definitions of other inequalities. See also “Default comparisons”.using enum declarations [allowed] Introduces enumerator element names into the current scope.C++20 Allowed Library FeaturesThe following C++20 library features are allowed in the Chromium codebase. Provides various byte- and bit-twiddling functions, e.g. counting leading zeros. Concepts and classes used to implement three-way comparison (“spaceship”, ) support. Various useful concepts, many of which replace pre-concept machinery in .Range algorithms [allowed] Provides versions of most algorithms that accept either an iterator-sentinel pair or a single range argument.Range access, range primitives, dangling iterator handling, and range concepts [allowed] Various helper functions and types for working with ranges.Library feature-test macros and <version> [allowed] Provides a standardized way to test the toolchain's implementation of a particular library feature. Provides compile-time constants for many common mathematical values, e.g. pi and e.std::assume_aligned [allowed] Informs the compiler that a pointer points to an address aligned to at least some particular power of 2.std::erase[_if] for containers [allowed] Erases from a container by value comparison or predicate, avoiding the need to use the  paradigm.std::hardware_{con,de}structive_interference_size [allowed] The std::hardware_destructive_interference_size constant is useful to avoid false sharing (destructive interference) between variables that would otherwise occupy the same cacheline. In contrast, std::hardware_constructive_interference_size is helpful to promote true sharing (constructive interference), e.g. to support better locality for non-contended data.std::is_[un]bounded_array [allowed] Checks if a type is an array type with a known or unknown bound. Linearly interpolates (or extrapolates) between two values.std::make_obj_using_allocator etc. [allowed]std::make_unique_for_overwrite [allowed] Like calling std::unique_ptr<T>(new T) instead of the more typical std::unique_ptr<T>(new T(...)). Finds the midpoint between its two arguments, avoiding any possible overflow. For integral inputs, rounds towards the first argument.std::ranges::subrange [allowed] Creates a view from an iterator and a sentinel. Useful for treating non-contiguous storage (e.g. a ) as a range.std::remove_cvref[_t] [allowed] Provides a way to remove const, volatile, and reference qualifiers from a type. Returns the size of an object as a signed type.std::string::(starts,ends)_with [allowed] Tests whether a string starts or ends with a particular character or string.C++20 Banned Language FeaturesThe following C++20 language features are not allowed in the Chromium codebase. A single UTF-8 code unit. Similar to , but considered a distinct type. Modules provide an alternative to many uses of headers which allows for faster compilation, better tooling support, and reduction of problems like “include what you use”.[[no_unique_address]] [banned] Allows a data member to be overlapped with other members.C++20 Banned Library FeaturesThe following C++20 library features are not allowed in the Chromium codebase. An updated version of  with fewer gotchas, similar to . Returns an value constructed with the same bits as an value of a different type.std::{c8rtomb,mbrtoc8} [banned] Converts a code point between UTF-8 and a multibyte character encoded using the current C locale.Range factories and range adaptors [banned] Lightweight objects that represent iterable sequences. Provides facilities for lazy operations on ranges, along with composition into pipelines.std::ranges::view_interface [banned] CRTP base class for implementing custom view objects. Utilities for non-owning views over a sequence of objects. Converts a pointer-like object to a pointer, even if the pointer does not refer to a constructed object (in which case an expression like  is UB). Facilities for multithreaded access to streams.C++20 TBD Language FeaturesThe following C++20 language features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections.Aggregate initialization using parentheses [tbd] Allows initialization of aggregates using parentheses, not just braces. Allows writing functions that logically block while physically returning control to a caller. This enables writing some kinds of async code in simple, straight-line ways without storing state in members or binding callbacks.C++20 TBD Library FeaturesThe following C++20 library features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections. Header which defines various core coroutine types. Utilities for producing formatted strings. Provides a class that can hold source code details such as filenames, function names, and line numbers. A string whose character type is , intended to hold UTF-8-encoded text.C++23 Allowed Language FeaturesThe following C++23 language features are allowed in the Chromium codebase.#elifdef, #elifndef [allowed] New conditional inclusion preprocessor directives.C++23 Allowed Library FeaturesThe following C++23 library features are allowed in the Chromium codebase.std::basic_string::contains [allowed] More concise substring check. Reverses the bytes of an integer.std::to_underlying [allowed] Converts an enumeration to its underlying type.C++23 TBD Language FeaturesThe following C++23 language features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections.Explicit object parameter [tbd] Allows explicit specification of the object parameter (deducing ) in member functions.Multidimensional subscript operator [tbd] Allows multiple arguments in the subscript operator. Prvalue copy (decay-copy). Provides a hint to the optimizer. Standardized preprocessor warning directive.Literal suffix for size_t [tbd] Literal suffix  or  for .Named character escapes [tbd] Universal character names using .C++23 TBD Library FeaturesThe following C++23 library features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections.Monadic operations for std::optional [tbd], ,  member functions. A vocabulary type that contains an expected value or an error.std::flat_map, std::flat_multimap, std::flat_set, std::flat_multiset [tbd] Container adaptors that provide the functionality of associative containers using sorted vectors.std::out_ptr, std::inout_ptr [tbd] Smart pointer adapters for functions that take raw pointers as out-parameters. Multidimensional array view. Converts a range to a container. Extends  to support printing containers and ranges. Formatted output. Coroutine generator. Captures a stack trace.std::move_only_function [tbd] Function wrapper for move-only objects. Indicates a codepath that is unreachable and invokes undefined behavior if executed. Input/output stream using a span as buffer.Fixed width floating-point types [tbd] Similar to int32_t and friends but for floats.std::start_lifetime_as [tbd] Explicitly starts the lifetime of an object of type T in the given storage.Abseil Banned Library FeaturesThe following Abseil library features are not allowed in the Chromium codebase. Early adaptation of C++17 . An equivalent of the C++23 std::move_only_function. Cross-platform macros to expose compiler-specific functionality.btree_* containers [banned] Alternatives to the tree-based standard library containers designed to be more efficient in the general case. Binds the first N arguments of an invocable object and stores them by value.Command line flags [banned] Allows programmatic access to flag values passed on the command-line to binaries.Container utilities [banned] Container-based versions of algorithmic functions within C++ standard library. A fixed size array like , but with size determined at runtime instead of compile time. Type for holding a non-owning reference to an object of any invocable type.Log macros and related classes [banned] Macros and related classes to perform debug loggings is a wrapper around an object of type T that behaves as an object of type T but never calls T's destructor.Nullability annotations [banned] Annotations to more clearly specify contracts Early adaptation of C++17 . Functions and utilities for generating pseudorandom data. Early adaptation of C++20 . An object that is either a usable value, or an error Status explaining why such a value is not present. Early adaptation of C++17 . Classes and utility functions for manipulating and comparing strings. Primitives for managing tasks across different threads. Abstractions for holding time values, both in terms of absolute time and civil time. A backport of C++17's std::variant type-safe union and related utilities. Backports of various C++17 template utilities.The following Abseil library features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections.absl::linked_hash_set, absl::linked_hash_map [tbd] A simple insertion-ordered set or map. It provides O(1) amortized insertions and lookups, as well as iteration in the insertion order.]]></content:encoded></item><item><title>Tesla kills Autopilot, locks lane-keeping behind $99/month fee</title><link>https://arstechnica.com/cars/2026/01/tesla-wants-recurring-revenue-discontinues-autopilot-in-favor-of-fsd/</link><author>CharlesW</author><category>hn</category><pubDate>Fri, 23 Jan 2026 19:28:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Claude.ai silently failing since Jan 14, no official acknowledgment</title><link>https://github.com/anthropics/claude-code/issues/18866</link><author>nurimamedov</author><category>hn</category><pubDate>Fri, 23 Jan 2026 18:42:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>New YC homepage</title><link>https://www.ycombinator.com/</link><author>sarreph</author><category>hn</category><pubDate>Fri, 23 Jan 2026 18:08:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Zotero 8</title><link>https://www.zotero.org/blog/zotero-8/</link><author>bouchard</author><category>hn</category><pubDate>Fri, 23 Jan 2026 18:05:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[We’re excited to announce our latest major release, Zotero 8. Zotero 8 builds on the new design and features of Zotero 7 and includes a huge number of improvements and refinements.Redesigned Citation DialogZotero 8 introduces a new unified citation dialog, replacing the previous citation dialog (the “red bar”), the “classic” citation dialog, and the Add Note dialog (the “yellow bar”).The new dialog has two modes: List mode and Library mode. List mode lets you quickly search for citations from across your Zotero libraries by title, creator, and year. Library mode includes a library browser, letting you find items in specific libraries or collections. You can switch between the two modes with a single click, preserving any added items or entered search terms. By default, it will open in the last mode you used, but you can choose a different default mode in the settings.In Zotero 7, we added the ability to quickly add citations for selected items and open documents. In the new dialog, these options are available in both List mode and Library mode, so you can make these quick selections even if you otherwise prefer to add items via the library browser.As before, once you’ve selected an item, you can click on its bubble to customize the citation with a page number, prefix, etc. It’s also now possible to add any locator — not just a page number — right from the search bar by typing the full or short name (e.g., “line 10” or “l. 10” after the citation and pressing Enter/Return.You can switch between adding citations and adding notes using buttons in the bottom left, corresponding to the Add/Edit Citation and Add Note buttons in your word processor.(For those coming from the classic dialog, note that there’s no text field to make manual edits to citations. It’s been possible to edit citations directly in the document for many years, which is why the red bar didn’t include such a text field either. More importantly, though, such manual edits should be avoided in almost all cases. Instead, customize the citation via the citation dialog, which will allow Zotero to continue to update the citation as necessary.)Annotations in the Items ListAnnotations you make on PDFs, EPUBs, and webpage snapshots now show up under their parent attachments in the items list.Showing annotations in the items list makes it easier to view annotations across a library or collection, and it also makes it possible to search for annotations directly. For example, you can search for all annotations in a collection with a given tag and then create a note from those annotations or copy them to an external text editor with Quick Copy.In Advanced Search, you can use “Item Type” “is” “Annotation” to match annotations or use the Annotation Text and Annotation Comment search conditions to search for specific parts of the annotation.You can assign tags to selected annotations by dragging them to the tag selector, just like other items.Selected annotations show up in the item pane, grouped by top-level item.Reader Appearance Panel with Theme SupportWe’ve added a new Appearance panel in the reader that provides quick access to view settings and introduces support for reader themes.The view settings are per-document settings. Themes are applied globally for all documents, including in the attachment preview in the item pane, and apply to PDFs, EPUBs, and webpage snapshots.We offer a number of built-in themes (“Dark”, “Snow”, “Sepia”), and you can create custom themes just by specifying a foreground and background color. (Some other theme engines require additional accent colors, but we’ve tried to make this as simple as possible for users by automatically adjusting other colors based on the foreground and background colors.) You can set a different theme that applies to light mode and dark mode.The themes replace the previous on-by-default “Use Dark Mode for Content” option, which inverted images in dark mode. We’re now simply darkening images a bit when using a dark theme. Images and ink annotations in the reader sidebar and note editor are now only darkened as well (and only when Zotero itself is in dark mode).When possible, we also try to apply themes to PDF pages containing full-page images, such as scanned papers, by replacing whitish/dark colors with theme colors. (Otherwise we simply darken the page slightly.)It’s now possible to open notes in tabs in addition to separate windows. Note tabs fill the whole window, with wide margins for better readability and a clean, distraction-free space for note-taking.By default, double-clicking a note in the items list will open it in a tab. You can choose to open the note in the other space from the context menu, and you can change the default behavior using the “Open notes in new windows instead of tabs” setting in the General pane of the settings.Notes in tabs have a separate font size setting in the View menu.Reading Mode for Webpage SnapshotsReading Mode reformats webpage snapshots for easier reading, with unnecessary page elements removed. You can adjust line height and other view options from the Appearance panel.We’ve reworked the tabs menu to make it faster to interact with via the keyboard.You can now press Ctrl/Cmd-; to bring up the menu at any time.Once the menu is open, it simultaneously accepts search input, up/down navigation, and row selection, without the need to move between different parts of the menu. You can simply start typing the name of an open tab and then press Enter/Return to switch to it once you’ve narrowed down the list.It’s also possible to quickly close multiple tabs by moving between the row close buttons with up/down and pressing space bar to close a tab.Zotero now automatically keeps attachment filenames in sync with parent item metadata as you make changes (e.g., changing the title). In previous versions, while Zotero would automatically rename files when you first added them to your library, if you later edited the item’s metadata, you would need to right-click on the attachment and select “Rename File from Parent Metadata”.You can configure which file types renaming applies to from the General tab of the Zotero settings.After upgrading to this version, existing eligible files that don’t match the current filename format won’t be automatically renamed, but you can choose to rename them en masse from the Zotero settings. Zotero will also prompt you to rename all files if you change the filename format.“Rename File from Parent Metadata” has been removed from the item context menu. If a filename doesn’t match the configured filename format (e.g., because automatic renaming is disabled or you changed the format but didn’t choose to rename all files), you can click the “Rename File to Match Parent Item” button next to the filename in the attachment’s item pane to rename it.New Attachment Title OptionsZotero 7 introduced more consistent handling of attachment titles, preserving simpler, less-redundant titles (e.g., “Full Text PDF” or “Preprint PDF”) in cases where the title was previously changed to match the filename. Zotero 8 further refines its renaming and titling logic when adding multiple and/or non-primary attachments, to bring the functionality better in line with the intended behavior.We’ve also added a “Normalize Attachment Titles” option under Tools → Manage Attachments to update old primary attachments with titles matching the filename to use simpler titles such as “PDF”.While we recommend the default behavior, allowing Zotero to rename primary files and keep them renamed while using simpler titles in the items list, if you really prefer to view filenames instead of titles, you can now enable “Show attachment filenames in the items list” option in the General pane of the settings.Zotero 8 adds a version for Linux running on ARM64 devices. This includes ARM-based Chromebooks, Apple Silicon Macs running Linux (Linux VMs, Asahi Linux), and Raspberry Pis.If you’ve been unable to run Zotero on your ARM-based device, or you’ve been running the x86_64 version under emulation, give it a try.User Interface ImprovementsWe’ve made a number of changes across the interface to address common requests:A new button in the library tab allows you to quickly close the item pane without dragging its edge or using the menus.You can reorder item pane sections by dragging their icons in the side navigation bar.You can drag items, collections, and searches into the trash.You can drag attachments, notes, and related items from the item pane (e.g., to copy files to the filesystem or use Quick Copy).Collections automatically expand when you drag over them, making it easier to drop collections or items into subcollections.You can delete attachments from the item pane.Tabs maintain their size as you close them for faster closing of multiple tabs.With Zotero 8, the Zotero Connector save popup can autocomplete tags in your Zotero library and allows you to add a note to items as you save them.Zotero 8 includes much more than we can list here. See the changelog for additional details.If you’re already running Zotero, you can upgrade from within Zotero by going to Help → “Check for Updates…”.
								This entry was posted
								 
								on Thursday, January 22nd, 2026 at 12:52 pm by Dan Stillman								and is filed under Features, News.
																							]]></content:encoded></item><item><title>Microsoft gave FBI set of BitLocker encryption keys to unlock suspects&apos; laptops</title><link>https://techcrunch.com/2026/01/23/microsoft-gave-fbi-a-set-of-bitlocker-encryption-keys-to-unlock-suspects-laptops-reports/</link><author>bookofjoe</author><category>hn</category><pubDate>Fri, 23 Jan 2026 17:58:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Microsoft provided the FBI with the recovery keys to unlock encrypted data on the hard drives of three laptops as part of a federal investigation, Forbes reported on Friday.Many modern Windows computers rely on full-disk encryption, called BitLocker, which is enabled by default. This type of technology should prevent anyone except the device owner from accessing the data if the computer is locked and powered off. But, by default, BitLocker recovery keys are uploaded to Microsoft’s cloud, allowing the tech giant — and by extension law enforcement — to access them and use them to decrypt drives encrypted with BitLocker, as with the case reported by Forbes.The case involved several people suspected of fraud related to the Pandemic Unemployment Assistance program in Guam, a U.S. island in the Pacific. Local news outlet Pacific Daily News covered the case last year, reporting that a warrant had been served to Microsoft in relation to the suspects’ hard drives. Kandit News, another local Guam news outlet, also reported in October that the FBI requested the warrant six months after seizing the three laptops encrypted with BitLocker. A spokesperson for Microsoft did not immediately respond to a request for comment by TechCrunch. Microsoft told Forbes that the company sometimes provides BitLocker recovery keys to authorities, having received an average of 20 such requests per year. Apart from the privacy risks of handing recovery keys to a company, Johns Hopkins professor and cryptography expert Matthew Green raised the potential scenario where malicious hackers compromise Microsoft’s cloud infrastructure — something that has happenedseveral times in recent years — and get access to these recovery keys. The hackers would still need physical access to the hard drives to use the stolen recovery keys.“It’s 2026 and these concerns have been known for years,” Green wrote in a post on Bluesky. “Microsoft’s inability to secure critical customer keys is starting to make it an outlier from the rest of the industry.”]]></content:encoded></item><item><title>Proof of Corn</title><link>https://proofofcorn.com/</link><author>rocauc</author><category>hn</category><pubDate>Fri, 23 Jan 2026 17:56:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A project by@seth, inspired by@fredwilson, orchestrated by Claude Code (Opus 4.5)]]></content:encoded></item><item><title>Route leak incident on January 22, 2026</title><link>https://blog.cloudflare.com/route-leak-incident-january-22-2026/</link><author>nomaxx117</author><category>hn</category><pubDate>Fri, 23 Jan 2026 17:54:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[On January 22, 2026, an automated routing policy configuration error caused us to leak some Border Gateway Protocol (BGP) prefixes unintentionally from a router at our data center in Miami, Florida. While the route leak caused some impact to Cloudflare customers, multiple external parties were also affected because their traffic was accidentally funnelled through our Miami data center location.The route leak lasted 25 minutes, causing congestion on some of our backbone infrastructure in Miami, elevated loss for some Cloudflare customer traffic, and higher latency for traffic across these links. Additionally, some traffic was discarded by firewall filters on our routers that are designed to only accept traffic for Cloudflare services and our customers.While we’ve written about route leaks before, we rarely find ourselves causing them. This route leak was the result of an accidental misconfiguration on a router in Cloudflare’s network, and only affected IPv6 traffic. We sincerely apologize to the users, customers, and networks we impacted yesterday as a result of this BGP route leak.Essentially, a route leak occurs when a network tells the broader Internet to send it traffic that it's not supposed to forward. Technically, a route leak occurs when a network, or Autonomous System (AS), appears unexpectedly in an AS path. An AS path is what BGP uses to determine the path across the Internet to a final destination. An example of an anomalous AS path indicative of a route leak would be finding a network sending routes received from a peer to a provider.During this type of route leak, the rules of  are violated, as BGP updates are sent from AS64501 to their peer (AS64502), and then unexpectedly up to a provider (AS64503). Oftentimes the leaker, in this case AS64502, is not prepared to handle the amount of traffic they are going to receive and may not even have firewall filters configured to accept all of the traffic coming in their direction. In simple terms, once a route update is sent to a peer or provider, it should only be sent further to customers and not to another peer or provider AS.During the incident on January 22, we caused a similar kind of route leak, in which we took routes from some of our peers and redistributed them in Miami to some of our peers and providers. According to the route leak definitions in RFC7908, we caused a mixture of Type 3 and Type 4 route leaks on the Internet. A change that ultimately triggers the routing policy bug is merged in our network automation code repositoryAutomation is run on single Miami edge-router resulting in unexpected advertisements to BGP transit providers and peersNetwork team begins investigating unintended route advertisements from MiamiIncident is raised to coordinate responseThe bad configuration change is manually reverted by a network operator, and automation is paused for the router, so it cannot run againThe change that triggered the leak is reverted from our code repositoryAutomation is confirmed by operators to be healthy to run again on the Miami router, without the routing policy bugAutomation is unpaused on the single router in MiamiWhat happened: the configuration errorOn January 22, 2026, at 20:25 UTC, we pushed a change via our policy automation platform to remove the BGP announcements from Miami for one of our data centers in Bogotá, Colombia. This was purposeful, as we previously forwarded some IPv6 traffic through Miami toward the Bogotá data center, but recent infrastructure upgrades removed the need for us to do so.This change generated the following diff (a program that compares configuration files in order to determine how or whether they differ):[edit policy-options policy-statement 6-COGENT-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-COMCAST-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-GTT-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-LEVEL3-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-PRIVATE-PEER-ANYCAST-OUT term ADV-SITELOCAL from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-PUBLIC-PEER-ANYCAST-OUT term ADV-SITELOCAL from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-PUBLIC-PEER-OUT term ADV-SITELOCAL from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-TELEFONICA-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-TELIA-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;While this policy change looks innocent at a glance, only removing the prefix lists containing BOG04 unicast prefixes resulted in a policy that was too permissive:policy-options policy-statement 6-TELIA-ACCEPT-EXPORT {
    term ADV-SITELOCAL-GRE-RECEIVER {
        from route-type internal;
        then {
            community add STATIC-ROUTE;
            community add SITE-LOCAL-ROUTE;
            community add MIA01;
            community add NORTH-AMERICA;
            accept;
        }
    }
}
The policy would now mark every prefix of type “internal” as acceptable, and proceed to add some informative communities to all matching prefixes. But more importantly, the policy also accepted the route through the policy filter, which resulted in the prefix — which was intended to be “internal” —  being advertised externally. This is an issue because the “route-type internal” match in JunOS or JunOS EVO (the operating systems used by  devices) will match any non-external route type, such as Internal BGP (IBGP) routes, which is what happened here.As a result, all IPv6 prefixes that Cloudflare redistributes internally across the backbone were accepted by this policy, and advertised to all our BGP neighbors in Miami. This is unfortunately very similar to the outage we experienced in 2020, on which you can read more .When the policy misconfiguration was applied at 20:25 UTC, a series of unintended BGP updates were sent from AS13335 to peers and providers in Miami. These BGP updates are viewable historically by looking at MRT files with the  tool or using . ➜  ~ monocle search --start-ts 2026-01-22T20:24:00Z --end-ts 2026-01-22T20:30:00Z --as-path ".*13335[ \d$]32934$*"
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f077::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f091::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f16f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f17c::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f26f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f27c::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f33f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f17c::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc
A|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f27c::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc
A|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f091::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc
A|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f091::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc
A|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f17c::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc
A|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f27c::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc
{trimmed}
In the monocle output seen above, we have the timestamp of our BGP update, followed by the next-hop in the announcement, the ASN of the network feeding a given route-collector, the prefix involved, and the AS path and BGP communities if any are found. At the end of the output per-line, we also find the route-collector instance.Looking at the first update for prefix 2a03:2880:f077::/48, the AS path is 64112 22850 174 3356 13335 32934. This means we (AS13335) took the prefix received from Meta (AS32934), our peer, and then advertised it toward Lumen (AS3356), one of our upstream transit providers. We know this is a route leak as routes received from peers are only meant to be readvertised to downstream (customer) networks, not laterally to other peers or up to providers.As a result of the leak and the forwarding of unintended traffic into our Miami router from providers and peers, we experienced congestion on our backbone between Miami and Atlanta, as you can see in the graph below. This would have resulted in elevated loss for some Cloudflare customer traffic, and higher latency than usual for traffic traversing these links. In addition to this congestion, the networks whose prefixes we leaked would have had their traffic discarded by firewall filters on our routers that are designed to only accept traffic for Cloudflare services and our customers. At peak, we discarded around 12Gbps of traffic ingressing our router in Miami for these non-downstream prefixes. Follow-ups and preventing route leaks We are big supporters and active contributors to efforts within the  and  that strengthen routing security. We know firsthand how easy it is to cause a route leak accidentally, as evidenced by this incident. Preventing route leaks will require a multi-faceted approach, but we have identified multiple areas in which we can improve, both short- and long-term.In terms of our routing policy configurations and automation, we are:Patching the failure in our routing policy automation that caused the route leak, and will mitigate this potential failure and others like it immediately Implementing additional BGP community-based safeguards in our routing policies that explicitly reject routes that were received from providers and peers on external export policies Adding automatic routing policy evaluation into our CI/CD pipelines that looks specifically for empty or erroneous policy terms Improve early detection of issues with network configurations and the negative effects of an automated changeTo help prevent route leaks in general, we are: Validating routing equipment vendors' implementation of  (BGP roles and the Only-to-Customer Attribute) in preparation for our rollout of the feature, which is the only way independent of routing policy to prevent route leaks caused at the  Autonomous System (AS)Most importantly, we would again like to apologize for the impact we caused users and customers of Cloudflare, as well as any impact felt by external networks.]]></content:encoded></item><item><title>Notes on the Intel 8086 processor&apos;s arithmetic-logic unit</title><link>https://www.righto.com/2026/01/notes-on-intel-8086-processors.html</link><author>elpocko</author><category>hn</category><pubDate>Fri, 23 Jan 2026 17:26:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In 1978, Intel introduced the 8086 processor, a revolutionary chip that led to the modern x86 architecture.
Unlike modern 64-bit processors, however, the 8086 is a 16-bit chip.
Its arithmetic/logic unit (ALU) operates on 16-bit values, performing arithmetic operations such as addition and subtraction,
as well as logic operations including bitwise AND, OR, and XOR.
The 8086's ALU is a complicated part of the chip, performing 28 operations in total.In this post, I discuss the circuitry that controls the ALU, generating the appropriate control signals for a
particular operation.
The process is more complicated than you might expect. First, a machine code instruction results in the execution of multiple
microcode instructions.
Using the ALU is a two-step process: one microcode instruction (micro-instruction) configures the ALU for the desired operation,
while a second
micro-instruction gets the results from the ALU.
Moreover, based on both the microcode micro-instruction and the machine code instruction, the control circuitry sends control signals to the ALU,
reconfiguring it for the desired operation.
Thus, this circuitry provides the "glue" between the micro-instructions and the ALU.The die photo below shows the 8086 processor under a microscope.
I've labeled the key functional blocks.
Architecturally, the chip is partitioned into a Bus Interface Unit (BIU) at the top and an Execution Unit (EU) below.
The BIU handles bus and memory activity as well as instruction prefetching, while the Execution Unit (EU) executes the instructions.
In the lower right corner, the microcode ROM holds the micro-instructions.
The ALU is in the lower left corner, with bits 7-0 above and bits 15-8 below, sandwiching the status flag circuitry.
The ALU control circuitry, highlighted in red at the bottom of the chip, is the focus of this article.The die of the 8086. Click this image (or any other) for a larger version.The 8086 processor implements most machine instructions in microcode, with a micro-instruction for each step of the machine instruction.
(I discuss the 8086's microcode in detail here.)
The 8086 uses an interesting architecture for microcode:
each micro-instruction performs two unrelated operations. The first operation moves data between a source and a destination.
The second operation can range from a jump or subroutine call to a memory read/write or an ALU operation.
An ALU operation has a five-bit field to specify a particular operation and a two-bit field to specify
which temporary register provides the input. As you'll see below, these two fields play an important role in the ALU circuitry.In many cases, the 8086's micro-instruction doesn't specify the ALU operation, leaving the details to be substituted from the machine instruction opcode.
For instance, the ADD, SUB, ADC, SBB, AND, OR, XOR, and CMP
machine instructions share the same microcode, while the hardware selects the ALU operation from the instruction opcode.
Likewise, the increment and decrement instructions use the same microcode, as do the decimal adjust instructions DAA and DAS, and the
ASCII adjust instructions AAA and AAS.
Inside the micro-instruction, all these operations are performed with a "pseudo" ALU operation called XI (for some reason).
If the microcode specifies an XI ALU operation, the hardware replaces it with the ALU operation specified in the instruction.
Another important feature of the microcode is 
that you need to perform one ALU micro-instruction to configure the ALU's operation, but the result isn't
available until a later micro-instruction, which moves the result to a destination.
This has the consequence that the hardware must remember the ALU operation.To make this concrete, here is the microcode that implements a typical arithmetic instruction such as  or .
This microcode consists of three micro-instructions. 
The left half of each micro-instruction specifies a data movement, first moving the two arguments to ALU temporary registers
and then storing the ALU result (called Σ).
The right half of each micro-instruction performs the second task.
First, the ALU is configured to perform an  operation using temporary register A. Recall that  indicates the ALU operation
is filled in from the machine instruction; this is how the same microcode handles eight different types of machine instructions.
In the second micro-instruction, the next machine instruction is started unless a memory writeback is required ().
The last micro-instruction is  (Run Next Instruction) to start a new machine instruction. It also indicates that the
processor status flags () should be updated to indicate if the ALU result is zero, positive, overflow, and so forth.M → tmpa   XI   tmpa  
R → tmpb   WB,NXT     
Σ → M      RNI  F     The ALU is the heart of a processor, performing arithmetic and logic operations.
Microprocessors of the 1970s typically supported addition and subtraction; logical AND, OR, and XOR; and various bit shift operations.
(Although the 8086 had multiply and divide instructions, these were implemented in microcode, not in the ALU.)
Since an ALU is both large and critical to performance, chip architects try to optimize its design.
As a result, different microprocessors have widely different ALU designs.
For instance, the 6502 microprocessor has separate circuits for addition and each logic operation; a multiplexer selects the appropriate
output.
The Intel 8085, on the other hand, uses an optimized clump of gates that performs the desired operation based on control signals (details), while the Z80's 4-bit CPU uses a different clump of gates (details).The 8086 takes a different approach, using two lookup tables (along with other gates) to generate the carry and output signals for each bit in the ALU.
By setting the lookup tables appropriately, the ALU can be configured to perform the desired operation.
(This is similar to how an FPGA implements arbitrary functions through lookup tables.)
The schematic below shows the circuit for one bit of the ALU.
I won't explain this circuit in detail since I explained it in an earlier article.
The relevant part of this circuit is the six control signals at the left.
The two multiplexers (trapezoidal symbols) implement the lookup tables by using the two input argument bits to select outputs from
the control signals to control carry generation and carry propagation.
Thus, by feeding appropriate control signals into the ALU, the 8086 can reconfigure the ALU to perform the desired operation.
For instance, with one set of control signals, this circuit will add. Other sets of control signals will cause the circuit to subtract
or compute a logical operation, such as AND or XOR.
The 8086 has 16 copies of this circuit, so it operates on 16-bit values.The circuit that implements one bit in the 8086's ALU.The 8086 is a complicated processor, and its instructions have many special cases, so controlling the ALU is
more complex than described above.
For instance, the compare operation is the same as a subtraction, except the numerical result of a compare is discarded; just the
status flags are updated.
The add versus add-with-carry instructions require different values for the carry into bit 0, while subtraction requires the
carry flag to be inverted since it is treated as a borrow.
The 8086's ALU supports increment and decrement operations, but also increment and decrement by 2, which requires an increment signal into bit
1 instead of bit 0.
The bit-shift operations all require special treatment. For instance, a rotate can use the carry bit or exclude the carry bit, while
and arithmetic shift right requires the top bit to be duplicated.
As a result, along with the six lookup table (LUT) control signals, the ALU also requires numerous control signals to adjust its
behavior for specific instructions.
In the next section, I'll explain how these control signals are generated.ALU control circuitry on the dieThe diagram below shows the components of the ALU control logic as they appear on the die.
The information from the micro-instruction enters at the right and is stored in the latches.
The PLAs (Programmable Logic Arrays) decode the instruction and generate the control signals.
These signals flow to the left, where they control the ALU.The ALU control logic as it appears on the die. I removed the metal layer to show the underlying polysilicon and silicon. The reddish lines are remnants of the metal.As explained earlier, if the microcode specifies the  operation, the operation field is replaced with a value based on the machine instruction opcode.
This substitution is performed by the  multiplexer before the value is stored in the operation latch.
Because of the complexity of the 8086 instruction set, the  operation is not as straightforward as you might expect.
This multiplexer gets three instruction bits from a special register called the "X" register, another instruction bit from the instruction
register, and the final bit from a decoding circuit called the Group Decode ROM.Recall that one micro-instruction specifies the ALU operation, and a later micro-instruction accesses the result. Thus, the
ALU control circuitry must remember the specified operation so it can be used later. 
In particular, the control circuitry must keep track of the ALU operation to perform and the temporary register specified.
The control circuitry uses three flip-flops to keep track of the specified temporary register, one flip-flop for each register.
The micro-instruction contains a two-bit field that specifies the temporary register. The control circuitry decodes this field and
activates the associated flip-flop.
The outputs from these flip-flops go to the ALU and enable the associated temporary register.
At the start of each machine instruction, the flip-flops are reset, so temporary register A is selected by default.The control circuitry uses five flip-flops to store the five-bit operation field from the micro-instruction.
At the start of each machine instruction, the flip-flops are reset so operation 0 (ADD) is specified by default.
One important consequence is that an add operation can potentially be performed without a micro-instruction to configure the ALU,
shortening the microcode by one micro-instruction and thus shortening the instruction time by one cycle.The five-bit output from the operation flip-flops goes to the operation PLA (Programmable Logic Array), which decodes the operation
into 27 control signals.
Many of these signals go to the ALU, where they control the behavior of the ALU for special cases.
About 15 of these signals go to the Lookup Table (LUT) PLA, which generates the six lookup table signals for the ALU.
At the left side of the LUT PLA, special high-current driver circuits amplify the control signals before they are sent to the ALU.
Details on these drivers are in the footnotes.Whenever I look at the circuitry of the 8086 processor, I see the differences between a RISC chip and a CISC chip.
In a RISC (Reduced Instruction Set Computer) processor such as ARM, instruction decoding is straightforward, as is the processor circuitry.
But in the 8086, a CISC (Complex Instruction Set Computer) processor, there are corner cases and complications everywhere.
For instance, an 8086 machine instruction sometimes specifies the ALU operation in the first byte and sometimes in the second byte,
and sometimes elsewhere, so the X register latch, the XI multiplexer, and the Group Decode ROM are needed.
The 8086's ALU includes obscure operations including four types of BCD adjustments and seven types of shifts, making the ALU more
complicated.
Of course, the continuing success of x86 shows that this complexity also has benefits.This article has been a deep dive into the details of the 8086's ALU, but I hope you have found it interesting.
If it's too much detail for you, you might prefer my overview of the 8086 ALU.]]></content:encoded></item><item><title>Gas Town&apos;s agent patterns, design bottlenecks, and vibecoding at scale</title><link>https://maggieappleton.com/gastown</link><author>pavel_lishin</author><category>hn</category><pubDate>Fri, 23 Jan 2026 16:19:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ A few weeks ago     published an elaborate     to Gas Town, his Mad-Max-Slow-Horses-Waterworld-etc-themed agent orchestrator that runs dozens of coding agents simultaneously in a metaphorical town of automated activity. Gas Town is entirely vibecoded, hastily designed with off-the-cuff solutions, and inefficiently burning through thousands of dollars a month in API costs. This doesn’t sound promising, but it’s lit divisive debates and sparks of change across the software engineering community. A small hype machine has formed around it. It’s made the rounds through every engineering team’s Slack, probably twice.  There’s somehow already a     doing over $400k in earnings.  And the hype is justified. First, because it’s utterly unhinged, and second because it’s a serious indication of how agents will change the nature of software development from this point on.You should at least skim through Yegge’s original article before continuing to read my reflections. First, because I’m not going to comprehensively summarise it.  And second, because a even a one minute glance over Yegge’s style of writing will make the vibes clear.We should take Yegge’s creation seriously not because it’s a serious, working tool for today’s developers (it isn’t). But because it’s a good piece of speculative design fiction that asks provocative questions and reveals the shape of constraints we’ll face as agentic coding systems mature and grow.“Design fiction” or “speculative design” is a branch of design where you creating things (objects, prototypes, sketches) from a plausible near future. Not to predict what’s going to happen, but to provoke questions and start conversations about what  happen. Not in a bright-and-glorious-flying-cars way that futurism can sometimes fall into. But, most helpfully, in a way that thinks about banal details, overlooked everyday interactions, low status objects, imperfect implementations, knock-on effects, and inconveniences. See the Near Future Lab’s short     and their     if you want to learn more.I also think Yegge deserves praise for exercising agency and taking a swing at a system like this, despite the inefficiencies and chaos of this iteration. And then running a public tour of his shitty, quarter-built plane while it’s mid-flight.When I was taken to the Tate Modern as a child I’d point at     pieces and say to my mother “I could do that”, and she would say “yes, but you didn’t.” Many people have talked about what large-scale, automated agent orchestration systems  look like in a few years, and no one else attempted to sincerely build it.I should be transparent and say that I have not used Gas Town in earnest on any serious work. I have only lightly poked at it, because I do not qualify as a serious user when I’m still hovering around stages 4-6 in Yegge’s 8 levels of automation:I currently juggle a handful of consecutive     and     agents, but pay close attention to the diffs and regularly check code in an IDE. Which I guess puts me in the agentically conservative camp in this distressingly breakneck moment in history.Gas Town is a full-on stage 8 piece of tooling: using an orchestrator that manages dozens+ of other coding agents for you. Yegge also warned me not to seriously use Gas Town multiple times, in increasingly threatening typography. I trust his guidance on his own slush pile.But I have grokked the basic concepts and spent more time with this manifesto than is warranted. And here is what stood out to me from the parts I could comprehend:When you have a fat stack of agents churning through code tasks, development time is no longer the bottleneck. Yegge says “Gas Town churns through implementation plans so quickly that you have to do a LOT of design and planning to keep the engine fed.” Design becomes the limiting factor: imagining what you want to create and then figuring out all the gnarly     required to make your imagination into reality.I certainly feel this friction in both my own professional work and personal projects. My development velocity is far slower than Yegge since I only wrangle a few agents at a time and keep my eyes and hands on the code. But the build time is rarely what holds me up. It is always the design; how should we architect this? What should this feel like? How should this look? Is that transition subtle enough? How composable should this be? Is this the right metaphor?When it’s not the design, it’s the product strategy and planning; What are the highest priority features to tackle? Which piece of this should we build first? When do we need to make that decision? What’s the next logical, incremental step we need to make progress here?These are the kind of decisions that agents cannot make for you. They require your human context, taste, preferences, and vision.With agents to hand, it’s easy to get ahead of yourself, stumbling forward into stacks of generated functions that should never have been prompted into existence, because they do not correctly render your intentions or achieve your goals.Gas Town seems to be halfway into this pitfall. The biggest flaw in Yegge’s creation is that it is poorly designed. I mean this in the sense that he absolutely did not design the shape of this system ahead of time, thoughtfully considering which metaphors and primitives would make this effective, efficient, easy to use, and comprehensible.He just made stuff up as he went. He says as much himself: “Gas Town is complicated. Not because I wanted it to be, but because I had to keep adding components until it was a self-sustaining machine.” Gas Town is composed of “especially difficult [theories] because it’s a bunch of bullshit I pulled out of my arse over the past 3 weeks, and I named it after badgers and stuff.” It was slapdashed together over “17 days, 75k lines of code, 2000 commits. It finally got off the ground (GUPP started working) just 2 days ago.”This Hacker News     describes the problem well, and points out that Yegge’s previous     project, of which Gas Town is an extension, suffers the same issue:“Beads is a good idea with a bad implementation. It’s not a designed product in the sense we are used to, it’s more like a stream of consciousness converted directly into code. It’s a program that isn’t only vibe coded, it was vibe designed too.”“Gas Town is clearly the same thing multiplied by ten thousand. The number of overlapping and ad hoc concepts in this design is overwhelming. Steve is ahead of his time but we aren’t going to end up using this stuff. Instead a few of the core insights will get incorporated into other agents in a simpler but no less effective way.”“gas town [is] such a nightmare to use i love it… the mayor is dumb as rocks the witness regularly forgets to look at stuff the deacon makes his own rules the crew have the object permanence of a tank full of goldfish and the polecats seem intent on wreaking as much chaos on the project as they can. this is peak entertainment i swear”Friends and colleagues of mine who have been brave enough to try out Gas Town in more depth report the same thing; this thing fits the shape of Yegge’s brain and no one else’s. I’d categorise that as a moderate design fail, given this is a public product that I assume Yegge wants  people to try out. The onboarding is baptism by fire.This feels like one of the most critical, emerging footguns of liberally hands-off agentic development. You can move so fast you never stop to think. It is so easy to prompt, you don’t fully consider what you’re building at each step of the process. It is only once you are hip-deep in poor architectural decisions, inscrutable bugs, and a fuzzy memory of what you set out to do, do you realise you have burned a billion tokens in exchange for a pile of hot trash.2. Buried in the chaos are sketches of future agent orchestration patternsNow that I’ve just critiqued the design of Gas Town, I will turn around and say that while the current amalgamation of polecats, convoys, deacons, molecules, protomolecules, mayors, seances, hooks, beads, witnesses, wisps, rigs, refineries, and dogs is a bunch of under cooked spaghetti, Yegge’s patterns  sketch out some useful conceptual shapes for future agentic systems.If you step back and squint, this mishmash of concepts reveals a few underlying patterns that future agentic systems will likely follow:Agents have specialised roles with hierarchical supervisionEvery agent in Gas Town has a permanent, specialised role. When an agent spins up a new session, it knows who it is and what job it needs to do. Some examples: is the human concierge: it’s the main agent you talk to. It talks to all the other agents for you, kicking off work, receiving notifications when things finish, and managing the flow of production. are temporary grunt workers. They complete single, isolated tasks, then disappear after submitting their work to be merged. supervises the Polecats and helps them get unstuck. Its job is to solve problems and nudge the proletariat workers along. manages the merge queue into the main branch. It evaluates each piece of work waiting to be merged, resolving conflicts in the process. It can creatively “re-imagine” implementations if merge conflicts get too hairy, while trying to keep the intent of the original work.There are many more characters in this town, but these give you a flavour of the system. Giving each agent a single job means you can prompt them more precisely, limit what they’re allowed to touch, and run lots of them at once without them stepping on each other’s toes.There’s also a clear chain of command between these agents. You talk to the Mayor, who coordinates work across the system. The Mayor in Gas Town never writes code. It talks to you, then creates work tasks and assigns them to workers. A set of system supervisors called the Witness, the Deacon, and “Boot the Dog” intermittently nudge the grunt workers and each other to check everyone is doing their work. Oh and there’s also a crew of “dogs” who do maintenance and cleaning.It’s easier if I try and show you. Here’s the basic relationship structure of Gas Town, as best I can make out:Since I’m making my own visuals here, I should justify it by pointing out that while Yegge made lots of his own ornate, zoopmorphic diagrams of Gas Town’s architecture and workflows, they are unhelpful. Primarily because they were made entirely by Gemini’s    . And while Nano Banana is state-of-the-art at making diagrams, generative AI systems are still really shit at making illustrative diagrams. They are very hard to decipher, filled with cluttered details, have arrows pointing the wrong direction, and are often missing key information. Case in point:Does this help you understand how the system works? No? No.Gas Town’s hierarchical approach solves both a coordination and attention problem. Without it, you are the one assigning tasks to dozens individual agents, checking who’s stuck, who’s idle, and who’s waiting on work from someone else. With the Mayor as your single interface, that overhead disappears. You can continuously talk to the Mayor without interrupting any agents or getting in the way, or having to think much about which one is doing what. This is less cognitive overhead than constantly switching tabs between Claudes.I think there’s a lot of opportunity to diversify the cast of characters here and make more use of    . The agents in Gas Town are all generalist workers in the software development pipeline. But we could add in any kind of specialist we want: a dev ops expert, a product manager, a front-end debugger, an accessibility checker, a documentation writer. These would be called in on-demand to apply their special skills and tools.Agent roles and tasks persist, sessions are ephemeralOne of the major limitations of current coding agents is running out of context. Before you even hit the limits of a context window,     degrades the output enough that it’s not worth keeping. We constantly have to compact or start fresh sessions.Gas Town’s solution to this is make each agent session disposable by design. It stores the important information – agent identities and tasks – in Git, then liberally kills off sessions and spins up fresh ones when needed. New sessions are told their identity and currently assigned work, and continue on where the last one left off. Gas Town also lets new sessions ask their predecessors what happened through “seancing”: resuming the last session as a separate instance in order to let the new agent ask questions about unfinished work.This saving and recalling is all done through Gas Town’s “Beads” system.  Beads are tiny, trackable units of work – like issues in an issue tracker – stored as JSON in Git alongside your code. Each bead has an ID, description, status, and assignee. Agent identities are also stored as beads, giving each worker a persistent address that survives session crashes.Yegge didn’t invent this pattern of tracking atomic tasks outside agent memory in something structured like JSON. Anthropic described the same approach in their research on    , just published in November 2025. I give it a hot minute before this type of task tracking lands in Claude Code.Feeding agents continuous streams of workThe whole promise of an orchestration system like Gas Town is it’s a perpetual motion machine. You give high-level orders to the mayor, and then a zoo of agents kicks off to break it down into tasks, assign them, execute them, check for bugs, fix the bugs, review the code, and merge it in.Each worker agent in Gas Town has its own queue of assigned work and a “hook” pointing to the current thing they should be doing. The minute they finish a task, the next one jumps to the front of the queue. The mayor is the one filling up these queues – it’s in charge of breaking down large features into atomic tasks and assigning them to available workers. In theory, the workers are never idle or lacking tasks, so long as you keep feeding the mayor your grand plans.This principle of “workers always do their work” is better in theory than practice. It turns out to be slightly difficult to make happen because of the way current models are trained. They’re designed as helpful assistants who wait politely for human instructions. They’re not used to checking a task queue and independently getting on with things.Gas Town’s patchwork solution to this is aggressive prompting and constant nudging. Supervisor agents spend their time poking workers to see if anyone’s stalled out or run dry on work. When one goes quiet, they send it a ping which jolts the agent into checking its queue and getting back to work. These periodic nudges move through the agent hierarchy like a heartbeat keeping everything moving. This is a decent band-aid for the first version, but more serious efforts at agent orchestration systems will need reliable ways to keep agents on task.Merge queues and agent-managed conflictsWhen you have a bunch of agents all working in parallel, you’re of course going to run into merge conflicts. Each agent is off on its own branch, and by the time it finishes its task, the main branch might look completely different – other changes have landed, the code has moved on. The later an agent finishes, the worse this gets. Normally you, the human, takes on the burden of sorting out the mess and deciding which changes to keep. But if agents are running on their own, something has to do that job for them.So Gas Town has a dedicated merge agent – the Refinery – that works through the merge queue one change at a time. It looks at each merge request, resolves any conflicts, and gets it into main. When things get really tangled – when so much has changed that the original work doesn’t even make sense anymore – it can creatively “re-imagine” the changes: re-doing the work to fit the new codebase. Or escalate to a human if needed.But there’s another way to sidestep merge conflict nightmares that Gas Town doesn’t have built in: ditch PRs for    . The traditional git workflow puts each feature on its own branch for days or weeks, accumulating commits, then getting merged back as one chunky PR.Stacked diffs avoid this conflict-prone approach by breaking work into small, atomic changes that each get reviewed and merged on their own, building on top of each other. Every change gets its own branch, forked off the previous change, forming a “stack” of changes dependent on one another. When a change earlier in the stack gets updated, all the changes below it automatically rebase on top of the new version.This fits how agents naturally work. They’re already producing tiny, focused changes rather than sprawling multi-day branches. When conflicts do pop up, they’re easier to untangle because each diff touches less code.    ’s recent acquisition of    , a tool built specifically for stacked diff workflows, suggests I am not the only one who sees this opportunity. When you’ve got dozens of agents landing changes continuously, you need tools and interfaces specifically designed for these frequent, incremental merges.3. The price is extremely high, but so is the (potential) valueYegge describes Gas Town as “expensive as hell… you won’t like Gas Town if you ever have to think, even for a moment, about where money comes from.” He’s on his second Claude account to get around Anthropic’s spending limits.I can’t find any mention online of the per-account limits, but let’s conservatively assume he’s spending at least $2,000 USD per month, and liberally $5,000.The current cost is almost certainly artificially inflated by system inefficiency. Work gets lost, bugs get fixed numerous times, designs go missing and need redoing. As models improve and orchestration patterns mature, the cost of orchestrators like this should drop while output quality rises.I expect companies would happily pay around the $1-3k/month mark for a sane, understandable, higher quality, and lower waste version of Gas Town. Maybe that sounds absurd to you, given we’ve all become anchored to the artificially low rate of $100-200/month for unlimited usage by the major providers. But once the AI bubble pops, the VC funds dry up, and providers have to charge the true cost of inference at scale, we should expect that “unlimited” tier to look a lot pricier.Even when that comes to pass, a few thousand is pretty reasonable when you compare it to an average US senior developer salary: $120,000 USD.  If Gas Town could genuinely speed up the work of a senior developer by 2-3x or more, it would easily be worth 10-30% of their salary. The cost per unit of valuable work starts to look competitive with human labor.Annual cost as a percentage of developer salaryThe maths on paying for something like this is already defensible in wealthier places like the US and parts of Western Europe. In spots where developer salaries are lower, we would expect the budget for AI assisted tools adjusts accordingly. They’ll get less crazy scaled automation and more conversative useage with humans filling in the cognitive gaps.4. Yegge never looks at code. When should we stop looking too?Yegge is leaning into the true definition of     with this project: “It is 100% vibecoded. I’ve never seen the code, and I never care to.” Not looking at code  is a very bold proposition, today, in January 2026.Given the current state of models and the meagre safeguards we have in place around them, the vast majority of us would consider this blind coding approach irresponsible and daft to do on anything that isn’t a throwaway side project. Which, given the amount of effort and Claude tokens Yegge has sunk into building it, writing documentation, and publicly promoting it, Gas Town is not.“Should developers still look at code?” will become one of the most divisive and heated debates over the coming years. You might be offended by the question, and find it absurd anyone is asking. But it’s a sincere question and the answer will change faster than you think.I’m already seeing people divide along moralistic, personal identity lines as they try to answer it. Some declare themselves purist, AI skeptic, Real Developers who check every diff and hand-adjust specific lines, sneering at anyone reckless enough to let agents run free. While others lean into agentic maximalism, directing fleets from on high and pitying the mass of luddites still faffing about with manual edits like it’s 2019. Both camps mistake a contextual judgement for a personality trait and firm moral position.A more conservative, easier to consider, debate is: how  should the code be in agentic software development tools? How easy should it be to access? How often do we expect developers to edit it by hand?Interfaces like    ,    , and     do not put code front and centre in the experience. The agent is your first and primary interface. You might be able to see diffs rolls by or display code files inline, but you can’t touch them. Trying to edit code yourself is a roundabout journey of opening your IDE and navigating to the correct files and lines.This design choice assumes it is easier to ask an agent to make the change for you, than it is to type it out the syntax yourself. They clearly say “we don’t believe users need to touch code.”Framing this debate as an either/or – either you look at code or don’t, either you edit code by hand or you exclusively direct agents, either you’re the anti-AI-purist or the agentic-maxxer – is unhelpful. Because nothing is a strict binary.The right distance isn’t about what kind of person you are or what you believe about AI capabilities in the current moment. How far away you step from the syntax shifts based on what you’re building, who you’re building with, and what happens when things go wrong. The degree of freedom you hand over to agents depends on:Domain and programming languageFront-end versus backend makes a huge difference. Language is a poor medium for designing easing curves and describing aesthetic feelings – I always need to touch the CSS, and it’s often faster to just tweak directly than try to explain what I want. Yegge’s CLI tooling is much easier to validate with pass/fail tests than evaluating whether a notification system “feels calm enough”. Model competence also varies wildly by language; prompting React and Tailwind gives you much better results than Rust or Haskell, where the models still regularly choke on borrow checkers and type systems.Access to feedback loops and definitions of succesThe more agents can validate their own work, the better the results. If you let agents run tests and see the output, they quickly learn what’s broken and how to fix it. If you let them open browsers, take screenshots, and click around, they can spot their mistakes. Tools like the     lean into this – it loops until tests pass or some specific condition is validated. This doesn’t work for less defined, clear cut work though. If you try to make an agent design a visual diagram for you, it’s going to struggle. It doesn’t know your aesthetic preferences and can’t really “see” what it’s making.Risk tolerance for shit going wrongStakes matter. If an agent breaks some images on your personal blog, you’ll recover. But if you’re running a healthcare system where a bug could miscalculate drug dosages, or a banking app moving actual money around, you can’t just wave an agent at it and hope. Consequences scale up fast. Corporate software has people whose entire job is compliance and regulatory sign-off – they need to see the code, understand it, verify it meets requirements. Those people aren’t going to let you widly run Gas Town over projects without serious guardrails in place.“Gas Town sounds fun if you are accountable to nobody: not for code quality, design coherence or inferencing costs. The rest of us are accountable for at least the first two and even in corporate scenarios where there is a blank check for tokens, that can’t last. So the bottleneck is going to be how fast humans can review code and agree to take responsibility for it.”Greenfield vs. brownfield projectsStarting fresh (greenfield), means you can let agents make architectural decisions and establish patterns – if you don’t like them, you can easily throw it out and restart. The cost of mistakes is low. But in an existing codebase (brownfield) with years of accumulated conventions, implicit patterns, and code that exists for reasons nobody remembers anymore, agents need much tighter supervision. They’ll happily introduce a new pattern that contradicts the three other ways this codebase already solves the same problem.If you’re solo of course you can YOLO. If you’re working with more than a handful of people, you’ll have to agree on coding standards and agent rules. This creates its own overhead: updating the AGENTS.md file, picking MCPs, writing commands and skills and rules and whatever else we invent to constrain these things. The pace of change when you’re all using agents can be overwhelming and you need to figure out a sensible reviewing pipeline to manage it. Team coordination can fall apart when everyone’s agents start moving too fast. You might show up in the morning and discover someone’s agent renamed the database schema while another agent refactored the whole API layer, and neither of which jive with your giant, unmerged feature.More senior developers can prompt better, debug better, and setup more stringent preferences earned through decades of seeing what can go wrong in scaled, production environments. They can recognize patterns: “oh, that’s a memory leak” or “that’s going to deadlock under load.” Newer developers don’t have that catalog of failures yet and are much more likely to prompt their own personal house of cards. The tests might pass and everything looks fine until you hit production traffic or someone enters a weird character. It is hard to defend against unknown unknowns.Given all these “it depends” considerations, I’m currently in the code-must-be-close camp for most serious work done by professional developers. But I expect I’ll shift to the code-at-a-distance camp over the next year or two as the harnesses and tools we wrap around these agents mature. If we can ship them with essential safe guards and quality gates, the risks drop. Sure, the models will also improve, but the infrastructure matters far more: validation loops, tests, and specialised subagents who focus on security, debugging, and code quality are what will make code-at-a-distance feasible.We have many, continuous versions of the code distance debate interally at    . One of the projects within the team driving this is     – autonomous agents run through GitHub Actions in response to events: new PRs, new issues, or specific times of day. Every commit can trigger a security review agent, an accessibility audit, and a documentation updater, all running in parallel alongside traditional CI/CD tests before anything lands in main. The team building it rarely touches code and do most of their work by directing agents from their phones. It’s these kinds of guardrails that makes a hands-off Sim-City-esque orchestrator system feel less terrifying to me.I don’t believe Gas Town itself is “it”. It’s not going to evolve into the thing we all use, day in and day out, in 2027. As I said, it’s a provocative piece of speculative design, not a system many people will use in earnest. In the same way any poorly designed object or system gets abandoned, this manic creation is too poorly thought through to persist. But the problems it’s wrestling with and the patterns it has sketched out will unquestionably show up in the next generation of development tools.As the pace of software development speeds up, we’ll feel the pressure intensify in other parts of the pipeline: thoughtful design, critical thinking, user research, planning and coordination within teams, deciding what to build, and whether it’s been built well. The most valuable tools in this new world won’t be the ones that generate the most code fastest. They’ll be the ones that help us think more clearly, plan more carefully, and keep the quality bar high while everything accelerates around us.]]></content:encoded></item><item><title>The tech monoculture is finally breaking</title><link>http://www.jasonwillems.com/technology/2025/12/17/Tech-Is-Fun-Again/</link><author>at1as</author><category>hn</category><pubDate>Fri, 23 Jan 2026 15:26:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Growing up in the 90s and early 2000s, tech was a foundational part of my childhood.I built more physical computers than I can remember. We went from paper maps to GPS (which itself evolved from DVDs with static maps to internet-connected real-time navigation). CD players became MP3 players, then streaming services. We had Palm Pilots and early attempts at “smart” phones, which were anything but. Our computers could search for extraterrestrial life through SETI. We emerged from the pager era to portable phones to the entire internet in our pocket (which evolved from charging per SMS or megabyte to unlimited data plans).We went through what I still think of as a golden era of console gaming: the N64 & PlayStation, then PS2, Xbox, and GameCube. Meanwhile, our bulky CRT monitors became flat (with a misadventure toward “projection” TVs in between). We could buy gadgets for everything. Best Buy and RadioShack felt like amusement parks we’d visit without intention, ready to be drawn in by something new. A trip to Asia’s electronics stores felt like a genuine step into the future.Today, we have everything we did back then, and much more. And yet it somehow feels like we’ve been left with less.In the early 2000s, tech began a decades-long consolidation. Almost everything we used before became a function of a single device. Objectively, this was an improvement—old VCR interfaces were awful, early MP3 players were clunky, GPS lacked real-time traffic data, and nothing talked to each other. And yet, through that consolidation, something intangible was taken from us.Our devices lost their unique personalities. Phones became our alarm clocks, flashlights, calendars, watches, cameras, GPS units, music players, radios, journals, and gaming devices—all at once. We betrayed our focus in the pursuit of convenience, and the personality of our devices for homogeneity.The benefits were clear to us, but the costs weren’t.This convergence created winner-take-all (and two-player) markets. Console gaming became PlayStation or Nintendo. Phones became Android or iOS. Computers became Mac or Windows. PC gaming became synonymous with Steam. Everything else became a feature inside one of those platforms, with globally synchronized updates making our experiences increasingly uniform, and bland.For a long time, that felt inevitable. But it’s only become clear in retrospect that somewhere in the early 2020s, things started to change.New paradigms are emerging for the first time since mobile. VR is no longer experimental. Early AR is starting to reach consumers. Meta shipped a wearable that normal people actually use, thanks to a clever Ray-Ban partnership (and associated equity stake). 3D printers have become real household products. Wearables are diversifying—smart rings, over-the-counter glucose monitors, connected beds.Meanwhile, Apple’s aggressive push for services revenue has alienated developers and users alike, creating space for alternatives. And nostalgia has revealed itself as massive, underserved economic demand.Gen-Z is buying single-purpose iPods and wired headphones. Pokémon cards are trendy. My friends and I are amassing N64 game collections again. There is a revived appetite for film cameras and Polaroids. Companies are recreating old hardware in modern form—ModRetro’s upcoming FPGA-based M64 plays native N64 cartridges, following their successful Game Boy recreation. They’re now working to bring a “next-gen” CRT monitor to market. The Playdate proved there’s still room for third-party handhelds with their own unique philosophies. Even Nintendo couldn’t resist capitalizing with the re-release of their classic consoles.Design matters again. In our devices, and in our lives. Art Deco is in vogue. Cyberpunk has never been more culturally mainstream. Color is back, and bold.Canon, Sony, and Nikon may have replaced Kodak for professionals, but Leica is thriving again and Kodak Instamatic has gone viral. People want devices that feel personal—leather finishes, physical controls, intentional constraints. For years this expression was limited to phone cases. Now it’s showing up in hardware itself.Tech is starting to resemble the wristwatch market: collaborations, limited editions, exclusivity. A market with many players—emerging companies, niche studios, design-forward brands, and even failing companies—is healthier than one dominated by a few giants.Antitrust pressure has slowed consolidation, opened app distribution, killed the anti-competitive iMessage and AirDrop moats, and made big tech cautious about horizontal expansion. And yet market forces may matter even more. Subscriptions keep multiplying. Advertising creeps into everything. Consolidated platforms are becoming bloated, degrading experiences. Platforms extract value in ways that betray their original philosophies.Apple’s push toward services has been financially successful but culturally damaging. Users are looking elsewhere. It was imperceptible at first, but that sentiment is spreading.Barriers to entry are lower than they’ve been in decades. Software can be deployed in minutes. Hardware is still hard, but 3D printing has revolutionized prototyping and accessible manufacturing services have drastically lowered the cost and time to market. Even the consolidation on the USB-C standard has played a role, allowing switching devices without investing in a new ecosystem.We’ve also grown tired of curation by algorithm. What we watch is shaped by recommendation engines. How we perceive it is influenced by aggregate ratings. I miss wandering through video stores, choosing based on nothing more than a cover. Discovery felt accidental and my opinions felt like my own.Burnout plays a role too. A Timex ad went viral this year: “Know the time without seeing you have 1,249 unanswered emails.” People are gravitating toward rigid, single-purpose experiences that let them fully disengage.Our appetite for alternatives has grown, while they’ve also become easier to create. LLMs and modern tools have lowered the effort required to build things. Side projects are easier to start and finish. Even when large companies offer better experiences on paper, individuals are building alternatives for the joy of it. Some go viral. Consumers end up with more choice.Nothing would have been harder to project than the growth of Linux on the desktop. Integrated platforms seemingly made the Linux philosophy untenable, and yet it may now be growing as a direct result of this decoupling. This was a feature, not a bug.Looking at my own purchases from 2025, the pattern becomes obvious:TRMNL (a no-distraction e-paper display)Android Pixel Pro (alongside iPhone)ASUS ROG laptop (for CUDA and gaming)Govee programmable lightsBambu Labs P1S 3D printerKindle (finally retiring my last mini-USB device)Abbott Lingo glucose sensoriPad (single-purpose: as a second MacBook display while traveling)More mechanical watches than I can count (while not tech per se, it does reduce the breadth of the Apple ecosystem)This is more than I’ve bought in the last 5 years, and I’m already excited for 2026. While Meta, Apple, Amazon, and Google still appear in my list, their purposes are narrower for me than in the past, and their presence is often no longer part of a two-player market. To be clear, these companies often make great products that should exist, but they should be easy to use as standalone à la carte offerings, not forced omakase experiences.We’ll never truly recreate the late 80s or mid-90s. SaaS, subscription pricing, and centralized platforms are here to stay. But this feels like the beginning of another golden era—one defined less by consolidation and more by variety, personality, and choice.]]></content:encoded></item><item><title>KORG phase8 – Acoustic Synthesizer</title><link>https://www.korg.com/us/products/dj/phase8/</link><author>bpierre</author><category>hn</category><pubDate>Fri, 23 Jan 2026 14:34:46 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Radicle: The Sovereign Forge</title><link>https://radicle.xyz/</link><author>ibobev</author><category>hn</category><pubDate>Fri, 23 Jan 2026 13:25:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ is a sovereign
       built on Git.
    Radicle is an open source, peer-to-peer code collaboration stack built on Git.
Unlike centralized code hosting platforms, there is no single entity
controlling the network. Repositories are replicated across peers in a
decentralized manner, and users are in full control of their data and workflow.To install Radicle, simply run the command below from your shell, or go to the
download page.Alternatively, you can build from source.For now, Radicle only works on Linux, macOS and BSD variants.The Radicle protocol leverages cryptographic identities for code and social
artifacts, utilizes Git for efficient data transfer between peers, and employs
a custom gossip protocol for exchanging repository metadata.Your Data, Forever and SecureAll social artifacts are stored in Git, and signed using public-key
cryptography. Radicle verifies the authenticity and authorship of all data
for you.Radicle enables users to run their own nodes, ensuring censorship-resistant
code collaboration and fostering a resilient network without reliance on
third-parties.Radicle is local-first, providing always-available functionality even
without internet access. Users own their data, making migration, backup, and
access easy both online and offline.Radicle’s Collaborative Objects (COBs) provide Radicle’s . This enables features such as issues, discussions and code review
to be implemented as Git objects. Developers can extend Radicle’s capabilities
to build any kind of collaboration flow they see fit.The Radicle Stack comes with a CLI, web interface and TUI, that are backed by
the Radicle Node and HTTP Daemon. It’s modular, so any part can be swapped out
and other clients can be developed.┌─────────────────┐┌────────────────┐
│  Radicle CLI    ││ Radicle Web    │
└─────────────────┘└────────────────┘
┌───────────────────────────────────┐
│  Radicle Repository               │
│ ┌────────┐ ┌────────┐ ┌─────────┐ │
│ │  code  │ │ issues │ │ patches │ │
│ └────────┘ └────────┘ └─────────┘ │
├───────────────────────────────────┤
│  Radicle Storage (Git)            │
└───────────────────────────────────┘
┌────────────────┐┌─────────────────┐
│  Radicle Node  ││  Radicle HTTPD  │
├────────────────┤├─────────────────┤
│    NoiseXK     ││   HTTP + JSON   │
└────────────────┘└─────────────────┘
Radicle is  software under the MIT and Apache 2.0
licenses. Get involved by contributing code.]]></content:encoded></item><item><title>Microsoft mishandling example.com</title><link>https://tinyapps.org/blog/microsoft-mishandling-example-com.html</link><author>mrled</author><category>hn</category><pubDate>Fri, 23 Jan 2026 13:04:09 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ Since at least February 2020, Microsoft's Autodiscover service has incorrectly routed the IANA-reserved  to Sumitomo Electric Industries' mail servers at sei.co.jp, potentially sending test credentials there.While setting up  as a dummy account in Outlook (on both Windows and macOS), Outlook consistently auto-configured it to use  (IMAP) and  (SMTP) despite  being an IANA-reserved domain that should not resolve to real services.The same behavior appeared on different machines, profiles, networks, and DNS resolvers, including a newly provisioned Windows 365 Cloud PC:Confirm that  has no DNS records pointing to : dig MX example.com +short dig CNAME autodiscover.example.com +short dig SRV _autodiscover._tcp.example.com +shortThe domain has a null MX record (indicating it doesn't accept email) and no Autodiscover DNS entries, confirming the misconfiguration exists entirely within Microsoft's database.Microsoft autodiscover API responseMicrosoft's Autodiscover service misconfiguration can be confirmed via curl -v -u "email@example.com:password" "https://prod.autodetect.outlook.cloud.microsoft/autodetect/detect?app=outlookdesktopBasic":{
  "email": "email@example.com",
  "services": [],
  "protocols": [
    {
      "protocol": "imap",
      "hostname": "imapgms.jnet.sei.co.jp",
      "port": 993,
      "encryption": "ssl",
      "username": "email@example.com",
      "validated": false
    },
    {
      "protocol": "smtp",
      "hostname": "smtpgms.jnet.sei.co.jp",
      "port": 465,
      "encryption": "ssl",
      "username": "email@example.com",
      "validated": false
    }
  ]
}The  header (Base64-decoded) reveals additional details:This misconfiguration has existed for nearly six years and was not crowdsourced. It appears to have been manually added to Microsoft's database.]]></content:encoded></item><item><title>European Alternatives</title><link>https://european-alternatives.eu/</link><author>s_dev</author><category>hn</category><pubDate>Fri, 23 Jan 2026 13:01:51 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
                        We help you find European alternatives for digital service and products, like cloud services and SaaS products.
                    
                            When you buy from local businesses, you are supporting yourself down the road. Taxes paid by the company come back to you indirectly and the company creates jobs in your region.
                        
                            Some companies outside Europe tend to ignore data protection and related laws such as the GDPR or do not implement them correctly.
                        
                            As a business that operates in Europe, it is possible to get a VAT refund for products/services of other European companies. European companies also tend to offer payment methods that are commonly used in Europe.
                        
                            Within the EU, many laws and framework conditions are set by the EU, which helps to cover a large market without having to consider large country-specific differences. It is also easier to enforce your rights against another company located in the EU.
                        ]]></content:encoded></item><item><title>What has Docker become?</title><link>https://tuananh.net/2026/01/20/what-has-docker-become/</link><author>tuananh</author><category>hn</category><pubDate>Fri, 23 Jan 2026 12:36:17 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
      Posted on 
  
    January 20, 2026
  


      
         • 
      
      
      5 minutes
       •
      
      854 words
      
    It’s weird to see Docker Inc (the company) struggle to find its place in 2026. What started as the company that revolutionized how we deploy applications has been through multiple identity crises, pivoting from one strategy to another in search of sustainable revenue and market relevance.Docker’s journey reads like a startup trying to find product-market fit, except Docker already had product-market fit - they created the containerization standard that everyone uses. The problem is that Docker the technology became so successful that Docker the company struggled to monetize it. When your core product becomes commoditized and open source, you need to find new ways to add value.Docker Swarm was Docker’s attempt to compete with Kubernetes in the orchestration space. But Kubernetes won that battle decisively, and Docker eventually sold Swarm. This was a clear signal that Docker was stepping back from trying to be the full-stack container platform and instead focusing on what they could uniquely provide.For a while, Docker seemed to focus on developer experience. This made sense - developers are Docker’s core users, and improving their workflow could be a differentiator. Docker Scout emerged from the acquisition of Atomist in June 2022, bringing “software supply chain” capabilities. Scout allows Docker to see not just what’s in a container, but how it was built and where vulnerabilities are. This was a smart move toward security and observability, areas where Docker could add real value.Docker also acquired AtomicJar, the company behind Testcontainers, adding shift-left testing capabilities. Testcontainers lets developers run real dependencies (databases, message queues, etc.) in containers during testing, making integration tests more reliable and closer to production environments.Then came the AI pivot. Docker Model Runner entered the scene, positioning Docker as a platform for running AI models. Docker Compose expanded to support AI agents and models. Docker Offload was introduced for cloud-scale GPU execution of AI tasks. Partnerships with Google Cloud, Microsoft Azure, and AI SDKs (CrewAI, LangGraph, Vercel AI SDK) followed.The acquisition of MCP Defender in September 2025 further cemented Docker’s move into AI security, focusing on securing agentic AI infrastructure and runtime threat detection. This was a significant shift - from developer tools to AI infrastructure.Suddenly, Docker moved into the hardened images space. In December 2025, Docker made over 1,000 Docker Hardened Images free and open source under Apache 2.0, reducing vulnerabilities by up to 95% compared to traditional images. This move was likely triggered by Chainguard’s success in the secure container image space. Chainguard had been building a business around minimal, secure container images, and Docker needed to respond.Making hardened images free was a bold move - it’s hard to compete with free, especially when it’s open source. But it also raises questions about Docker’s business model. If you’re giving away your security features for free, what are you selling?Leadership Changes and Acquisition SpeculationIn February 2025, Docker replaced CEO Scott Johnston (who led the company since 2019) with Don Johnson, a former Oracle Cloud Infrastructure founder and executive vice president. This leadership transition has prompted tech analysts to anticipate a potential acquisition by a major cloud provider. The CEO swap, combined with the strategic pivots, suggests Docker may be positioning itself for sale rather than building a standalone business.Docker’s strategic shifts tell a story of a company searching for its place in a market it helped create. The containerization technology Docker pioneered became so successful that it became infrastructure - something everyone uses but no one wants to pay for directly.The pivots from orchestration (Swarm) to developer tools (Scout, Testcontainers) to AI (Model Runner, MCP Defender) to security (Hardened Images) show a company trying different approaches to find sustainable revenue. Each pivot makes sense in isolation, but together they paint a picture of a company without a clear long-term vision.The hardened images move is particularly interesting because it’s defensive - responding to Chainguard’s success rather than leading with innovation. Making it free and open source is a strong competitive move, but it doesn’t solve the fundamental business model question.Docker the technology isn’t going anywhere. It’s too embedded in the infrastructure of modern software development. But Docker the company? That’s less clear. The leadership change, acquisition speculation, and rapid strategic pivots suggest Docker Inc may be positioning itself for an exit rather than building a long-term independent business.For developers, this doesn’t change much. Docker containers will continue to work, and the open source nature of Docker means the technology will persist regardless of what happens to the company. But it’s worth watching how Docker Inc’s search for identity plays out - it could affect the ecosystem of tools and services built around containers.The irony is that Docker created a standard so successful that it became infrastructure, and infrastructure is hard to monetize. Docker Inc’s struggle to find its place is a cautionary tale about the challenges of building a business around open source technology that becomes too successful.]]></content:encoded></item><item><title>Show HN: Whosthere: A LAN discovery tool with a modern TUI, written in Go</title><link>https://github.com/ramonvermeulen/whosthere</link><author>rvermeulen98</author><category>hn</category><pubDate>Fri, 23 Jan 2026 11:54:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ask HN: What&apos;s the current best local/open speech-to-speech setup?</title><link>https://news.ycombinator.com/item?id=46731068</link><author>dsrtslnd23</author><category>hn</category><pubDate>Fri, 23 Jan 2026 11:04:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I’m trying to do the “voice assistant” thing fully locally: mic → model → speaker, low latency, ideally streaming + interruptible (barge-in).Qwen3 Omni looks perfect on paper (“real-time”, speech-to-speech, etc). But I’ve been poking around and I can’t find a single reproducible “here’s how I got the open weights doing real speech-to-speech locally” writeup. Lots of “speech in → text out” or “audio out after the model finishes”, but not a usable realtime voice loop. Feels like either (a) the tooling isn’t there yet, or (b) I’m missing the secret sauce.What are people actually using in 2026 if they want open + local voice?Is anyone doing true end-to-end speech models locally (streaming audio out), or is the SOTA still “streaming ASR + LLM + streaming TTS” glued together?If you did get Qwen3 Omni speech-to-speech working: what stack (transformers / vLLM-omni / something else), what hardware, and is it actually realtime?What’s the most “works today” combo on a single GPU?Bonus: rough numbers people see for mic → first audio backWould love pointers to repos, configs, or “this is the one that finally worked for me” war stories.]]></content:encoded></item><item><title>Booting from a vinyl record (2020)</title><link>https://boginjr.com/it/sw/dev/vinyl-boot/</link><author>yesturi</author><category>hn</category><pubDate>Fri, 23 Jan 2026 10:39:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Most PCs tend to boot from a primary media storage, be it a hard disk drive, or a solid-state drive, perhaps from a network, or – if all else fails – the USB stick or the boot DVD comes to the rescue… Fun, eh? Boring! Why don’t we try to boot from a record player for a change?: Click here to observe the very same vinyl ramdisk booted on an IBM PCjr!
So this nutty little experiment connects a PC, or an IBM PC to be exact, directly onto a record player through an amplifier. I made a small ROM on-chip boot loader that operates the built-in “cassette interface” of the PC (that was hardly ever used), which will now be invoked by the BIOS if all the other boot options fail, i.e. floppy disk and the hard drive. The turntable spins an analog recording of a small bootable read-only RAM drive, which is 64K in size. This contains a FreeDOS kernel, modified by me to cram it into the memory constraint, a micro variant of COMMAND.COM and a patched version of INTERLNK, that allows file transfer through a printer cable, modified to be runnable on FreeDOS. The bootloader reads the disk image from the audio recording through the cassette modem, loads it to memory and boots the system on it. Simple huh?And now to get more technical: this is basically a merge between BootLPT/86 and 5150CAXX, minus the printer port support. It also resides in a ROM, in the BIOS expansion socket, but it does not have to. The connecting cable between the PC and the record player amplifier is the same as with 5150CAXX, just without the line-in (PC data out) jack.
The “cassette interface” itself is just PC speaker timer channel 2 for the output, and 8255A-5 PPI port C channel 4 (PC4, I/O port 62h bit 4) for the input. BIOS INT 15h routines are used for software (de)modulation.
The boot image is the same 64K BOOTDISK.IMG “example” RAM drive that can be downloaded at the bottom of the BootLPT article. This has been turned into an “IBM cassette tape”-protocol compliant audio signal using 5150CAXX, and sent straight to a record cutting lathe.
Vinyls are cut with an RIAA equalization curve that a preamp usually reverses during playback, but not perfectly. So some signal correction had to be applied from the amplifier, as I couldn’t make it work right with the line output straight from the phono preamp. In my case, involving a vintage Harman&Kardon 6300 amplifier with an integrated MM phono preamp, I had to fade the treble all the way down to -10dB/10kHz, increase bass equalization to approx. +6dB/50Hz and reduce the volume level to approximately 0.7 volts peak, so it doesn’t distort. All this, naturally, with any phase and loudness correction turned off.
Of course, the cassette modem does not give a hoot in hell about where the signal is coming from. Notwithstanding, the recording needs to be pristine and contain no pops or loud crackles (vinyl) or modulation/frequency drop-outs (tape) that will break the data stream from continuing. However, some wow is tolerated, and the speed can be 2 or 3 percent higher or lower too.And that’s it! For those interested, the bootloader binary designed for a 2364 chip (2764s can be used, through an adaptor), can be obtained here. It assumes an IBM 5150 with a monochrome screen and at least 512K of RAM, which kind of reminds me of my setup (what a coincidence). The boot disk image can be obtained at the bottom of the BootLPT/86 article, and here’s its analog variant, straight from the grooves 🙂]]></content:encoded></item><item><title>AI Usage Policy</title><link>https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md</link><author>mefengl</author><category>hn</category><pubDate>Fri, 23 Jan 2026 09:50:26 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Updates to our web search products and Programmable Search Engine capabilities</title><link>https://programmablesearchengine.googleblog.com/2026/01/updates-to-our-web-search-products.html</link><author>01jonny01</author><category>hn</category><pubDate>Fri, 23 Jan 2026 09:38:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Replacing Protobuf with Rust to go 5 times faster</title><link>https://pgdog.dev/blog/replace-protobuf-with-rust</link><author>whiteros_e</author><category>hn</category><pubDate>Fri, 23 Jan 2026 09:03:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Lev KokotovPgDog is a proxy for scaling PostgreSQL. Under the hood, we use  to parse and understand SQL queries. Since PgDog is written in Rust, we use its Rust bindings to interface with the core C library. 
Those bindings use Protobuf (de)serialization to work uniformly across different programming languages, e.g., the popular Ruby  gem.Protobuf is fast, but not using Protobuf is faster. We forked  and replaced Protobuf with direct C-to-Rust (and back to C) bindings, using bindgen and Claude-generated wrappers. This resulted in a 5x improvement in parsing queries, and a 10x improvement in deparsing (Postgres AST to SQL string conversion).You can reproduce these by cloning our fork and running the benchmark tests: (Protobuf) (Direct C to Rust) (Protobuf) (Direct Rust to C)The first step is always profiling. We use samply, which integrates nicely with the Firefox profiler. Samply is a sampling profiler: it measures how much time code spends running CPU instructions in each function. It works by inspecting the application call stack thousands of times per second. The more time is spent inside a particular function (or span, as they are typically called), the slower that code is. This is how we discovered :This is the entrypoint to the  C library, used by all  bindings. The function that wraps the actual Postgres parser, , barely registered on the flame graph. Parsing queries isn’t free, but the Postgres parser itself is very quick and has been optimized for a long time. With the hot spot identified, our first instinct was to do nothing and just add a cache.Caching is a trade-off between memory and CPU utilization, and memory is relatively cheap (latest DRAM crunch notwithstanding). The cache is mutex-protected, uses the LRU algorithm and is backed by a hashmap. The query text is the key and the Abstract Syntax Tree is the value, which expects most apps to use prepared statements. The query text contains placeholders instead of actual values and is therefore reusable, for example:While the  parameter can change between invocations, the prepared statement does not, so we could cache its static AST in memory.This works pretty well, but eventually we ran into a couple of issues:Some ORMs can have bugs that generate thousands of unique statements, e.g.,  instead of , which causes a lot of cache missesApplications use old PostgreSQL client drivers which don’t support prepared statements, e.g., Python’s  packageThe clock on Protobuf was ticking and we needed to act. So, like a lot of engineers these days, we asked an LLM to just do it for us.I’m going to preface this section by saying that the vast majority of PgDog’s source code is written by a human. AI is not in a position to one-shot a connection pooler, load balancer and database sharder. However, when scoped to a very specific, well-defined and most importantly  task, it can work really well.The prompt we started with was pretty straightforward:libpg_query is a library that wraps the PostgreSQL parser in an API. pg_query.rs is a Rust wrapper around libpg_query which uses Protobuf for (de)serialization. Replace Protobuf with bindgen-generated Rust structs that map directly to the Postgres AST.And after two days of back and forth between us and the machine, it worked. We ended up with 6,000 lines of recursive Rust that manually mapped C types and structs to Rust structs, and vice versa. We made the switch for ,  (used in our new query rewrite engine, which we’ll talk about in another post),  and . These four methods are heavily used in PgDog to make sharding work, and we immediately saw a 25% improvement in  benchmarks.Just to be clear: we had a lot of things going for us already that made this possible. First,  has a Protobuf spec for  (and Prost, the Protobuf Rust implementation) to generate bindings, so Claude was able to get a comprehensive list of structs it needed to extract from C, along with the expected data types.Second,  was already using bindgen, so we had to just copy/paste some invocations around to get the AST structs included in bindgen’s output.And last, and definitely not least,  already had a working  and  implementation, so we could test our AI-generated code against its output. This was entirely automated and verifiable: for each test case that used , we included a call to , compared their results and if they differed by even one byte, Claude Code had to go back and try again.The translation code between Rust and C uses  Rust functions that wrap Rust structs to C structs. The C structs are then passed to the Postgres/ C API which does the actual work of building the AST.The result is converted back to Rust using a recursive algorithm: each node in the AST has its own converter function which accepts an  C pointer and returns a safe Rust struct. Much like the name suggests, the AST is a tree, which is stored in an array:For each node in the list, the implementation calls , which then handles each one of the 100s of tokens available in the SQL grammar:For nodes that contain other nodes, we recurse on  again until the algorithm reaches the leaves (nodes with no children) and terminates. For nodes that contain scalars, like a number (e.g., ) or text (e.g., ), the data type is copied into a Rust analog, e.g.,  or .The end result is , a Rust struct generated by Prost from the  API Protobuf specification, but populated by native Rust code instead of Prost’s deserializer. Reusing existing structs reduces the chance of errors considerably: we can compare  and  outputs, using the derived  trait, and ensure that both are identical, in testing.While recursive algorithms have a questionable reputation in the industry because bad ones can cause stack overflows, they are very fast. Recursion requires no additional memory allocation because all of its working space, the stack, is created on program startup. It also has excellent CPU cache locality because the instructions for the next invocation of the same function are already in the CPU L1/L2/L3 cache. Finally and arguably more importantly, they are just easier to read and understand than iterative implementations, which helps us, the humans, with debugging.Just for good measure, we tried generating an iterative algorithm, but it ended up being slower than Prost. The main cause (we think) was unnecessary memory allocations, hashmap lookups of previously converted nodes, and too much overhead from walking the tree several times. Meanwhile, recursion processes each AST node exactly once and uses the stack pointer to track its position in the tree. If you have any ideas on how to make an iterative algorithm work better, let us know!Reducing the overhead from using the Postgres parser in PgDog makes a huge difference for us. As a network proxy, our budget for latency, memory utilization, and CPU cycles is low. After all, we aren’t a real database…yet! This change improves performance from two angles: we use less CPU and we do less work, so PgDog is faster and cheaper to run.If stuff like this is interesting to you, reach out. We are looking for a Founding Software Engineer to help us grow and build the next iteration of horizontal scaling for PostgreSQL.]]></content:encoded></item><item><title>Proton spam and the AI consent problem</title><link>https://dbushell.com/2026/01/22/proton-spam/</link><author>dbushell</author><category>hn</category><pubDate>Fri, 23 Jan 2026 07:01:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[On Jan 14th Proton sent out an email newsletter with the subject line:Introducing Projects - Try Lumo’s powerful new feature nowscreenshot of the official email from @lumo.proton.meLumo is Proton’s  offering.There is a problem with this email. And I’m not talking about the question of how exactly AI aligns with Proton’s core values of  and .The problem is I had already  opted out of Lumo emails.screenshot of an unchecked toggleThat toggle for “Lumo product updates” is unchecked. Lumo is the only topic I’m not subscribed to. Proton has over a dozen newsletters, including some crypto nonsense. I opt-in to everything but Lumo, I gave an undeniable  to Lumo emails.So the email I received from Proton is , right?My understanding is that spam is a violation of GDPR and UK data protection laws. Regardless, Proton’s email is a clear abuse of their own service towards a paying business customer.Before I grab my pitchfork I emailed Proton support.Despite the subject line and contents, and despite the “From Lumo” name and  address, maybe this was an honest mistake?Proton’s first reply explained how to opt-out.screenshot of support email quoted belowI’ve blurred the name because whateverThank you for contacting us.You can unsubscribe from the newsletters if you do the following:- Log in to your account at https://account.protonvpn.com/login- Navigate to the Account category- Disable the check-marks under “Email subscriptions”- If you need additional assistance, let me know.[screenshot of the same opt-out toggle] directs me to the exact same  toggle I had already unchecked. I replied explaining that I had  opted out. Support replies saying they’re “checking this with the team” then later replies again asking for screenshots.Can you make sure to send me a screenshot of this newsletter option disabled, as well as the date when the last message was sent to you regarding the Lumo offer?You can send me a screenshot of the whole message, including the date.Is it perhaps 14 January 2026 that you received the message?I found that last line curious, are they dealing with other unhappy customers? Maybe I’m reading too much into it.I sent the screenshots and signed off with “Don’t try to pretend this fits into another newsletter category.”After more “checking this with the team” I got a response today.In this case, the mentioned newsletter is for promoting Lumo Business Suit to Business-related plans.Hence, why you received it, as Product Updates and Email Subscription are two different things.In the subscription section, you will see the “Email Subscription” category, where you can disable the newsletter in order to avoid getting it in the future.If I understand correctly, Proton are claiming this email is the “Proton for Business newsletter”. Not the  newsletter.I don’t know about you, but I think that’s baloney. Proton Support had five full business days to come up with a better excuse. Please tell me, how can I have been any more explicit about opting out of Lumo emails, only to receive “Try Lumo” “From Lumo”, and be told that is not actually a Lumo email?Has anyone else noticed that the AI industry can’t take “no” for an answer? AI is being force-fed into every corner of tech. It’s unfathomable to them that some of us aren’t interested.As Proton has demonstrated above, and Mozilla/Firefox recently too, the AI industry simply will not accept “no” as an answer. Some examples like spam are more trivial than others, but the growing trend is vile and disturbing.I guess someone at Microsoft read my post and said “hold my beer”. This morning I woke up to a lovely gift in my inbox; “Build Al agents with the new GitHub Copilot SDK”.GitHub email subject line: 'Build Al agents with the new GitHub Copilot SDK'GitHub Ensloppification is moving faster than I can delete my account for good. (It’s an unfortunate requirement for client projects.) For the record, I have  said “yes” to any GitHub newsletter. Even before Copilot I disabled every possible GitHub email notification.The “Unsubscribe” link provides the hidden newsletter list. There is nothing within GitHub account settings I can find to disable spam.GitHub 'Opt-Out Preferences' with 3 newsletters unchecked but GitHub Copilot emails checkedAs expected, Microsoft has opted me in without my consent. The wheels are falling off at GitHub. The brutally slow front-end UI. The embarrassingly lacklustre  CI. Now this sloppy tripe everywhere. Reminder to developers: Proton Update (Afternoon of 23rd)After I published this blog post yesterday I received another email from Specialist Support / Mail Delivery (Engineering) Team.I completely understand your frustration, and I apologize for the confusion caused by these  of notifications.Specifically, some of our communications regarding Lumo fall under  (Update Info) and  (, Newsletters, and ) This is likely why you are still receiving them despite having opted out of one category.I replied saying that is  how email marketing consent works. I’m pretty sure not legally, I’m certain not morally, and until now, I was convinced not by Proton’s standard. The very first customer support confirmed what should be common sense. Don’t want Lumo emails? Unsubscribe from the  category. If it was a business newsletter that happened to mention Lumo as a bullet point, fine. But the entire email was Lumo, talking about how “Our latest Lumo update introduces…”Anyway, following a lively discussion on  unofficial customer support forum, my case was escalated to Proton’s .Please accept my apologies for how your ticket was managed by our teams. They have tried to explain what happened without acknowledging the problem itself.You are right. You should not have received the newsletter.We have identified a bug in our system, and our technical team is working on resolving it.I want to assure you that we take communication consent very seriously.We also value our relationship with our customers. The support team will learn from this interaction and improve.Just FYI I don’t have a problem with how the support ticket was managed. I doubt the first line of defence gets paid enough to deal with this stuff when their employer is at fault. Please don’t replace them with Lumo, then we’ll have problems!Hey, Proton CTO here. There was a bug, and we fucked up. Support should have reported it up the chain and acknowledged this. Things happen, especially at scale, but we take comms consent seriously and will fix it.So was it a bug? Or did Proton forget their core values and behave like the other slop factories? I’ll take them at their word. What am I going to do, go back to gmail? I’m looking into Tuta and StartMail but it’s a pain to switch and nowhere is perfect.]]></content:encoded></item><item><title>I built a light that reacts to radio waves [video]</title><link>https://www.youtube.com/watch?v=moBCOEiqiPs</link><author>codetheweb</author><category>hn</category><pubDate>Fri, 23 Jan 2026 05:34:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Talking to LLMs has improved my thinking</title><link>https://philipotoole.com/why-talking-to-llms-has-improved-my-thinking/</link><author>otoolep</author><category>hn</category><pubDate>Fri, 23 Jan 2026 03:52:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I’ve been surprised by – and enjoy – one aspect of using large language models more than any other.They often put into words things I have long understood, but could not write down clearly. When that happens, it feels less like learning something new and more like recognition. A kind of “ok, yeah” moment.I have not seen this effect discussed much. I also think it has improved how .Much of what we know is tacitYou know when a design is wrong before you can say why. You sense a bug before you can reproduce it. You recognize a bad abstraction instantly, even if it takes an hour to explain the problem to someone else.This is not a failure. It is how experience operates. The brain compresses experience into patterns that are efficient for action, not for speech. Those patterns are real, but they are not stored in sentences.The problem is that reflection, planning, and teaching all require language. If you cannot express an idea, you cannot easily examine it, you cannot easily share it.LLMs are good at the opposite problemLarge language models are built to do exactly this – turn vague structure into words.When I ask a question about something semi-obvious to me, something I believe is true but am not sure why, the model responds with a formulation. It steps through each reason why that something may be true. Each point it makes is orthogonal to the previous one, allowing me to trade, exchange, and re-order the arguments it makes.Putting things into words changes the thoughtOnce the LLM writes down an idea, I can then play with it in my mind.Vague intuitions turn into named distinctions and my implicit assumptions become visible. At that point I can test them, discard them, or refine them.Of course, this is not new. Writing has always done this for me. What is different is the speed. I can explore half-formed thoughts, discard bad descriptions, and try again. That encourages a kind of thinking I might have otherwise skipped.The feedback loop mattersOver time I’ve noticed that now I do this without an LLM to hand.  Can I “phrase in precise language what and why I am thinking, feeling, believing, right now?”In that sense, the model is not improving my thinking directly. It is improving how I use language, improving the effectiveness of my internal monologue. And since reasoning depends heavily on what one can represent explicitly, that improvement can feel like a real increase in clarity.The more I do this, the better I get at noticing what I actually think.]]></content:encoded></item><item><title>Bugs Apple loves</title><link>https://www.bugsappleloves.com/</link><author>nhod</author><category>hn</category><pubDate>Fri, 23 Jan 2026 02:24:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Every bug is different. But the math is always real.Think our numbers are wrong? Edit them yourself.]]></content:encoded></item><item><title>Stunnel</title><link>https://www.stunnel.org/</link><author>firesteelrain</author><category>hn</category><pubDate>Fri, 23 Jan 2026 00:30:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Stunnel is a proxy designed to add TLS encryption functionality to existing clients and servers without any changes in the programs' code.
Its architecture is optimized for security, portability, and scalability (including load-balancing), making it suitable for large deployments.Stunnel uses the OpenSSL library for cryptography, so it supports whatever cryptographic algorithms are compiled into the library. It can benefit from the FIPS 140-2 validation of the OpenSSL FIPS Provider, as long as the building process meets the OpenSSL FIPS 140-2 Security Policy.  Our latest Windows installer includes the OpenSSL FIPS Provider.]]></content:encoded></item><item><title>Why medieval city-builder video games are historically inaccurate (2020)</title><link>https://www.leidenmedievalistsblog.nl/articles/why-medieval-city-builder-video-games-are-historically-inaccurate</link><author>benbreen</author><category>hn</category><pubDate>Fri, 23 Jan 2026 00:22:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Since many of us are working from home in these trying times, it seems safe to assume that more people than ever are indulging in playing the occasional computer game. A city builder is a specific kind of computer game in which you design a city, extract resources, set up production chains and ensure that your settlement grows. City builders are very similar to strategy games as they reward patience and strategy. In this article, I will take a look at one sub-genre of the city builder, the medieval city builder, and explain how this gaming genre relates to our knowledge of medieval settlement planning.The city builder has its origins far back in the 1990s in the combination of the strategy genre and the management genre, leading to games such as  (1989), (1992) and  (1997).It did not take long before medieval-themed city builders popped up. We may think of  (1993) and  (1998). In addition, the  games (1998-2019), although initially set in the 1600s basically had a medieval theme.These games often start with plopping down a village center on a promising location near abundant resources. You then continue to gather these resources which grant you building materials for building new homes and facilities for your settlement.Setting up specialized production chains might involve growing grain, milling the grain for flour and turning the flour into bread which feeds your villages. Similarly, another production chain might involve rearing sheep for their wool, turning the wool into cloth and turning the cloth into clothing. When done correctly, the reward of correct investments and planning is that you see your settlement grow.This often leads to settlements growing organically from a couple of houses around a community center to a larger settlement with hundreds of people. However logical such an organic growth of a settlement might seem, it is not historically accurate.Any gameplay loop that tells a story of linear settlement growth is incongruent with how a medieval economy worked (see Foussier 2004). Medieval villagers were often living on the edge of subsistence. Agricultural surpluses were skimmed by the church and the feudal lords. Bad harvests, banditry, warfare and disease might decimate a village community at any time. For this very reason, the demography of many European villages remained relatively stable between the twelfth and the eighteenth century. It may therefore be clear that the gameplay loop of city builders pivots around the concept of doing the historically exceptional (i.e. growing a settlement to a town) and thereby strays far from what actually happened in the lives of our medieval forebears. A notable exception to this genre trope is the game (2014) in which high mortality rates and bad weather do seriously stifle any kind of linear growth. In this city builder you are constantly fighting the odds and settlement growth is not guaranteed. However, also in Banished it is your goal to overcome the stagnation and lead your settlement to expansion.A thing that is rarely touched upon in medieval city builders is how complex village life actually was. This can be exemplified by how the community related to its overlords. Land ownership here is key. Land in the community might be owned by a lord, a local liegeman, a monastery or even directly by the duke or count. Taxes, rents and tithes were the organisational structures in which the landowner was tied to the farmers who worked the fields. Often the payment of taxes and tithes was linked to feast days and the visit of the tax collector represented a big event in the agricultural year. An interesting side note is that some obligations which the commoners had to the lord and the church (such as seigneurial duties like working a mill) might drain the community from the needed manpower for tilling the land. Furthermore, a rural community that was its own seigneury had access to a law court with sheriff, aldermen and a local militia (Middle Dutch ) to fight off bandits with. Harsh capital punishments were set in place to deter anyone from raiding the farms and hamlets and the village gallows were often the first thing one saw when approaching a medieval settlement. Planning a medieval settlementBut something that is much more fundamental to the theme of a settlement building game, is how medieval settlements were actually planned and grew. Landscape historians and archaeologists have acquired a lot of insight into how this worked.Let's start with the realization that medieval settlements in their first stages of development were planned and laid out according to a specific design. In my own research into the settlement history of West-Brabant (southern Netherlands, from 1000 to 1300 CE) I have encountered the following types.Here is a sketch of a Brabantine circular manor (Middle Dutch ). This is a reinforced circular homestead with moat, often next to a bend in the river, containing several farms and a fan-like plot pattern radiating out from it. Such manors were often called BORCH.Here is a sketch of a Brabantine street settlement, often built with exploitation of nearby fenland in mind. It consists of a line of farms with associated evenly sized rectangular plots built in a line perpendicular to a raised road.Here is a more complex exploitation village which is set up with a moated enclosed church homestead and a central meadow as its center. There is a line of farms next to the road. The arable land to the east is bordered by a ditch supplying fresh drinking water (Middle Dutch ). In layout, this type represents a hybrid between the two earlier settlement types. Let us first make clear that these different types of exploitation settlements often existed alongside each other and can be found in one and the same region. In part, the different types reflect different chronological layers but some types were also more suited to certain geographical environments than others.So how were these settlements planned? Many medieval exploitation enterprises were initiated by a monastery or a consortium of free men who were granted permission by (or bought permission from) the feudal lord to “colonize” the wilderness.Clearing the wooded landscape in order to create arable land was done by cutting away the trees and bushes (Middle Dutch ) or, alternatively, burning it away in controlled fires (Middle Dutch ).Land surveyors sent by the lord would then measure out the block or strips that would be taken in cultivation. Strips of arable land were often 1250m deep (6 Middle Dutch = ) so that the plough could go straight in a long line before having to turn. Important blocks or strips were demarcated by hedges, earthwork, woodwork, ditches or roads. Medieval names for these blocks often survived into the modern day. The presence of drinking water (a river or a brook) in the vicinity was an important factor in choosing the location for the settlement. The vicinity of water entailed risk and reward because flooding was an ever present danger. Floods could devastate arable land but might also fertilize it. Meadows in particular were often situated in flood areas. So how was such a settlement managed? First of all, the quality of the soil had to be carefully controlled by crop rotation: specific crops were sown on different segments of the arable land with one part laying fallow to recover from the tilling (English three-field system, Dutch ). The cattle and sheep were put out to pasture on the common meadows guarded by a shepherd or cowherd. Pigs were allowed to forage in the nearby forests and killed in autumn before the winter starvation set in.Roads and rivers were important for transport of crops and livestock. These roads, some of them paved, some of them not, needed to be maintained. They were essential to the payment of the tithe, since tithe collectors assessed the harvest on the field and later collected the sheaves on the side of the road.The buildings within the community also needed maintenance. Farmhouses, community barns and stables were made of wood and had to be rebuilt every few generations, only the name of the farm or homestead being continued.So what kind of threats did a medieval settlement face? First of all, the weather was an important factor which dictated the success of the harvest. Storms, droughts and floods could devastate the harvest and decimate the community.Diseases and epidemics were another danger threatening the community. The situation on the countryside was a lot better in this regard than in the medieval towns, but an epidemic could still mean the end of a village. Similarly, diseases among livestock impacted the medieval subsistence economy in a brutal way.Then there are the consequences of medieval warfare affecting the community: Armies that passed by could plunder the village, burn the farms and execute villagers at will. Or they could also demand supplies, food and provisions as an emergency "tax"But war also brought indirect consequences; a liege lord calling the banners and levying troops from the village community might extract a large part of the adult men. Warfare also disrupted the trade networks that supplied a village with building materials and commodities.Then there were internal threats to the fabric of the village community. We may think of social unrest because of land disputes. Feuds could also tear a community apart with endemic vendetta’s causing death and despair. A socially unstable society was also more prone to internal accusations of heresy and witchcraft.An “accurate” medieval settlement builderSo, which of the above listed features could potentially contribute to a more historically accurate computer game about medieval settlement building? First of all, it would be more realistic if the settlement could first be planned out and was not forced to "grow organically" from a community center. The first settlement phase would be a test of how “successful” a layout is in adapting to the exigencies of the terrain and the needs of the community. Only after that initial layout proved successful, further expansions can be planned. Secondly, it would be more realistic if we could build both straight roads and curved roads, just as in (2015), a modern city builder well known for its incredibly flexible layout tools. Incidentally, the tools of Cities Skylines can also be used to recreate medieval settlements, as was done by YouTube creator Play Curiously who constructed an impression of a medieval Croatian village.Such a flexible road drawing tool can then also be used to lay out ditches, hedges and enclosures since these features were central to the medieval experience of the cultivated landscape.Thirdly, It would be interesting to see a medieval-themed game embrace the concept of flood valleys that limit and endanger pasture and arable land. Other historical city builders such as  (1999) and  (2004) already implemented this feature for their setting in Ancient Egypt. However, such a mechanic would likewise fit a medieval city builder and show the general public how medieval society dealt with seasonal flooding as well as the devastating effects that storm floods could have.And finally, something that would, in my opinion, really add to the realism and historical flavor of a medieval-themed city builder would be the introduction of mechanisms in which agricultural surpluses are skimmed by the church and the feudal lord. Tithes, taxes and rents! Instead of merely abstracting the taxes into an income modifier or letting the player be the extractor himself, we could be shown the tax collector visiting the village, counting the sheaves by the side of the road, selecting the calves and chickens. This way, the experiences of our medieval forebears are visualized and may help to educate the public about medieval village life.There are some good reasons why city building games are not that historically accurate and instead adhere to the established formula of the city building game. First of all, a linear growth model makes sense from a gameplay perspective, since it is rewarding to see your settlement grow.in a linear way. It fosters a feeling of progress and motivates the player to keep momentum and push through to the next expansion phase. Secondly, games are generally wary of punishing failure too harshly in order to avoid demoralizing the player. Thirdly, in order to facilitate path finding for the simulated villagers it is easier to implement a gridlike road and building system rather than an off-grid building system that allows for curvy roads. So far only has managed to do this in a satisfactory way. Lastly, for marketing purposes and recognizability, game developers generally don't stray too far from the image of the Middle Ages that the public is already acquainted with. For a medieval city builder this means windmills, industrious peasants, lots of sheep and stone castles. Things like land surveying, crop rotation and tithe collection do not fit this image and challenge the romanticized picture of the uneducated farmer in his pre-industrial environment. Although I think medieval-themed city building games could benefit from incorporating some of the things we know about medieval settlement history into the gameplay loop, it may not be desirable for game developers to stray too far from the established formula. The idea that medieval settlements developed organically according to messy road plans is strongly imbedded in popular perception. Allowing both straight and curved road building in medieval city builders, may serve to challenge some of the stereotypes that exist about medieval village life. And if you ask me, that would be a good thing for it is an enriching experience to see the world through the eyes of our medieval forebears. One may find out that their lives were not that different after all... Fossier, R. (2004). “The Rural Economy and Demographic Growth.” In: D. Luscombe & J. Riley-Smith (Eds.). The New Cambridge Medieval History. Cambridge: Cambridge University Press, 11-46.Van Ham, W. (1979). “Dorp en dorpsleven in middeleeuws Wouw." in: A. Delahaye (red.), De Heren XVII van Nassau-Brabant, 316-336.”(forthc.) Kerkhof, P.A. (2020). “Saer, Saert; een Zuid-Nederlandse veldnaam van onzekere oorsprong.” Noordbrabants Historisch Jaarboek.Leenders, K.A.H.W. (1996). "Noord-Vlaanderen en de Noordwesthoek; een vergelijking." Tijdschrift voor Waterstaatsgeschiedenis 5, 67-73.Leenders, K.A.H.W. (1989). Verdwenen venen; een onderzoek naar de ligging en exploitatie van thans verdwenen venen in het gebied tussen Antwerpen, Turnhout, Geertruidenberg en Willemstad (1250-1750). Reeks Landschapsstudies 13, Wageningen.Oosthuizen, S. (2017). Windgather Press.© Alexia Kerkhof and Leiden Medievalists Blog, 2020. Unauthorised use and/or duplication of this material without express and written permission from this site’s author and/or owner is strictly prohibited. Excerpts and links may be used, provided that full and clear credit is given to Alexia Kerkhof and Leiden Medievalists Blog with appropriate and specific direction to the original content.]]></content:encoded></item><item><title>Improving the usability of C libraries in Swift</title><link>https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/</link><author>timsneath</author><category>hn</category><pubDate>Thu, 22 Jan 2026 23:34:44 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[There are many interesting, useful, and fun C libraries in the software ecosystem. While one could go and rewrite these libraries in Swift, usually there is no need, because Swift provides direct interoperability with C. With a little setup, you can directly use existing C libraries from your Swift code.When you use a C library directly from Swift, it will look and feel similar to using it from C. That can be useful if you’re following sample code or a tutorial written in C, but it can also feel out of place. For example, here’s a small amount of code using a C API:The C library here that Swift is using comes from the webgpu-headers project, which vends a C header () that is used by several implementations of WebGPU. WebGPU  is a technology that enables web developers to use the system’s GPU (Graphics Processing Unit) from the browser. For the purposes of this post, you don’t really need to know anything about WebGPU: I’m using it as an example of a typical C library, and the techniques described in this blog post apply to lots of other well-designed C libraries.The Swift code above has a very “C” feel to it. It has global function calls with prefixed names like wgpuInstanceCreateSurface and global integer constants like . It pervasively uses unsafe pointers, some of which are managed with explicit reference counting, where the user provides calls to  and  functions. It works, but it doesn’t feel like Swift, and inherits various safety problems of C.Fortunately, we can improve this situation, providing a safer and more ergonomic interface to WebGPU from Swift that feels like it belongs in Swift. More importantly, we can do so without changing the WebGPU implementation: Swift provides a suite of annotations that you can apply to C headers to improve the way in which the C APIs are expressed in Swift. These annotations describe common conventions in C that match up with Swift constructs, projecting a more Swift-friendly interface on top of the C code.In this post, I’m going to use these annotations to improve how Swift interacts with the WebGPU C code. By the end, we’ll be able to take advantage of Swift features like argument labels, methods, enums, and automatic reference counting, like this:These same annotations can be used for any C library to provide a safer, more ergonomic development experience in Swift without changing the C library at all.: Some of what is covered in this post requires bug fixes that first became available in Swift 6.2.3.A module map is a way of layering a Swift-friendly modular structure on top of C headers. You can create a module map for the WebGPU header by writing the following to a file :The easiest thing to do is to put  alongside the header itself. For my experiment here, I put it in the root directory of my  checkout. If you’re in a Swift package, put it into its own target with this layout:If you reference this  target from elsewhere in the package, you can  to get access to the C APIs.There are a few ways to see what the Swift interface for a C library looks like.The swift-synthesize-interface tool in Swift 6.2+ prints the Swift interface to the terminal.Xcode’s “Swift 5 interface” counterpart to the  header will show how the header has been mapped into Swift.Let’s do it from the command line, using swift-synthesize-interface. From the directory containing  and , run:The leading  and the  argument with the path is only needed on macOS; on other platforms, make sure swift-synthesize-interface is in your path. The  operation is the triple provided if you run swiftc -print-target-info. It looks like this:The output of swift-synthesize-interface is the Swift API for the WebGPU module, directly translated from C. For example, this code from the WebGPU header:and there are lots of global functions like this:It’s a starting point! You can absolutely write Swift programs using these WebGPU APIs, and they’ll feel a lot like writing them in C. Let’s see what we can do to make it better.C enums can be used for several things. Sometimes they really represent a choice among a number of alternatives. Sometimes they represent flags in a set of options, from which you can choose several. Sometimes they’re just a convenient way to create a bunch of named constants. Swift conservatively imports enum types as wrappers over the underlying C type used to store values of the enum (e.g.,  wraps a ) and makes the enumerators into global constants. It covers all of the possible use cases, but it isn’t .The  enum really is a choice among one of several options, which would be best represented as an  in Swift. If we were willing to modify the header, we could apply the  attribute to the enum, like this:This works, and results in a much nicer Swift API:Now, we get an  that we can switch over, and nice short case names, e.g.,That’s great, but I already broke my rule: no header modifications unless I have to!The problem of needing to layer information on top of existing C headers is not a new one. As noted earlier, Swift relies on a Clang feature called API notes to let us express this same information in a separate file, so we don’t have to edit the header. In this case, we create a file called  (the name  matches the module name from ), which is a YAML file describing the extra information. We’ll start with one that turns  into an : here is a term used in the C and C++ standard to refer to enum, struct, union, or class types. Any information about those types in the API notes file will go into that section.Put  alongside the , and now  gets mapped into a  enum. For a package, the structure will look like this:We’ll be adding more to this API notes file as we keep digging through the interface.The WebGPU header has a number of “object” types that are defined like this:This gets imported into Swift as an alias for an opaque pointer type, which is… not great:WebGPU object types are reference counted, and each object type has corresponding  and  functions to increment and decrement the reference count, like this:Of course, you can use these functions in Swift exactly how you do in C, making sure to balance out calls to  and , but then it would be every bit as unsafe as C.We can do better with . It’s a macro (defined in the  header) that can turn a reference-counted C type like the above into an automatically reference-counted  in Swift. Here’s how we would use it in the header:Now,  gets imported like this:The extra typealias is a little unexpected, but overall this is a huge improvement: Swift is treating  as a class, meaning that it automatically manages retains and releases for you! This is both an ergonomic win (less code to write) and a safety win, because it’s eliminated the possibility of mismanaging these instances.There’s one more thing: when dealing with reference-counting APIs, you need to know whether a particular function that returns an object is expecting you to call “release” when you’re done. In the WebGPU header, this information is embedded in a comment:“ReturnedWithOwnership” here means that the result of the call has already been retained one extra time, and the caller is responsible for calling “release” when they are done with it. The  header has a  macro that expresses this notion. One can use it like this:Now, Swift will balance out the retain that wgpuDeviceCreateBindGroup has promised to do by performing the extra release once you’re done using the object. Once these annotations are done, we’re all set with a more ergonomic and memory-safe API for this C library. There’s no need to ever call  or  yourself.We’ve hacked up our header again, so let’s undo that and move all of this out to API notes. To turn a type into a foreign reference type, we augment the  section of our API notes with the same information, but in YAML form:That makes  import as a class type, with the given retain and release functions. We can express the “returns retained” behavior of the wgpuDeviceCreateBindGroup function like this:That’s enums and classes, so now let’s tackle… functions.A typical function from , like this:will come into Swift like this:Note that  on each parameter, which means that we won’t use argument labels for anything when we call it:That matches C, but it isn’t as clear as it could be in Swift. Let’s clean this up by providing a better name in Swift that includes argument labels. We can do so using  (also in ), like this:Within the parentheses, we have each of the argument labels that we want (or  meaning “no label”), each followed by a . This is how one describes a full function name in Swift. Once we’ve made this change to the Swift name, the C function comes into Swift with argument labels, like this:That makes the call site more clear and self-documenting:There is more usable structure in this API. Note that the  function takes, as its first argument, an instance of . Most of the C functions in  are like this, because these are effectively functions that operate on their first argument. In a language that has methods, they would be methods. Swift has methods, so let’s make them methods!There are three things to notice about this  string:It starts with , which tells Swift to make this function a member inside .Let’s change the function name to , because we no longer need the  prefix to distinguish it from other “write buffer” operations on other types.The name of the first argument in parentheses is , which indicates that the  argument (in Swift) should be passed as that positional argument to the C function. The other arguments are passed in-order.Note that this also requires  to be imported as a , as we did earlier for . Once we’ve done so, we get a much-nicer Swift API:We’ve hacked up the header again, but didn’t have to. In , you can put a  attribute on any entity. For , it would look like this (in the  section): has a number of  functions that produce information about some aspect of a type. Here are two for the  type:With the  tricks above, we can turn these into “get” methods on , like this:That’s okay, but it’s not what you’d do in Swift. Let’s go one step further and turn them into read-only computed properties. To do so, use the  prefix on the Swift name we define. We’ll skip ahead to the YAML form that goes into API notes:And now, we arrive at a nice Swift API: can also be used to import a function that returns a new instance as a Swift initializer. For example, this function creates a new  (which we assume is getting imported as a  like we’ve been doing above):We can turn this into a Swift initializer, which is used to create a new object, using the same  syntax but where the method name is . Here is the YAML form that goes into API notes:and here is the resulting Swift initializer:Now, one can create a new  with the normal object-creation syntax, e.g.,The WebGPU header defines its own Boolean type. I wish everyone would use C99’s  and be done with it, but alas, here are the definitions for WebGPUs Boolean types:This means that  will come in to Swift as a . The two macros aren’t available in Swift at all: they’re “too complicated” to be recognized as integral constants. Even if they were available in Swift, it still wouldn’t be great because we want to use  and  for Boolean values in Swift, not  and .To make  easier to use from Swift, we’re first going to map that typedef to its own  that stores the underlying , giving it an identity separate from . We can do this using a  API note within the  section of the file, like this:Now, we get  imported like this:To be able to use  and  literals with this new , we can write a little bit of Swift code that makes this type conform to the ExpressibleByBooleanLiteral protocol, like this:That’s it! Better type safety (you cannot confuse a  with any other integer value) and the convenience of Boolean literals in Swift. describes a set of flags using a  of the  type (a 64-bit unsigned integer) along with a set of global constants for the different flag values. For example, here is the  flag type and some of its constants:Similar to what we saw with ,  is a  of a  of a . There’s no type safety in this C API, and one could easily mix up these flags with, say, those of :We can do better, by layering more structure for the Swift version of this API using the same  approach from . This goes into the  section of API notes:Now,  comes in as its own :The initializers let you create a  from a  value, and there is also a  property to get a  value out of a , so the raw value is always there… but the default is to be type safe. Additionally, those global constants will come in as members of , like this:This means that, if you’re passing a value of type , you can use the shorthand “leading dot” syntax. For example:Swift has dropped the common  prefix from the constants when it made them into members. However, the resulting names aren’t great. We can rename them by providing a  in the API notes file within the  section:We can go one step further by making the  type conform to Swift’s  protocol. If we revise the API notes like this:Now, we get the nice option-set syntax we expect in Swift:Throughout , the  macro is used to indicate pointers that can be NULL. The implication is that any pointer that is not marked with  cannot be NULL. For example, here is the definition of  we used above:The  indicates that it’s acceptable to pass a NULL pointer in as the  parameter. Clang already has nullability specifiers to express this information. We could alter the declaration in the header to express that this parameter is nullable but the result type is never NULL, like this:This eliminates the implicitly-unwrapped optionals () from the signature of the initializer, so we end up with one that explicitly accepts a  descriptor argument and always returns a new instance (never ):Now, I did cheat by hacking the header. Instead, we can express this with API notes on the parameters and result type by extending the entry we already have for  like this:To specific nullability of pointer parameters, one can identify them by position (where 0 is the first parameter to the function) and then specify whether the parameter should come into Swift as optional (, corresponds to ), non-optional (, corresponds to ) or by left unspecified as an implicitly-unwrapped optional (, corresponds to ). For the result type, it’s a little different: we specified the result type along with the nullability specifier, i.e., . The end result of these annotations is the same as the modified header, so we can layer nullability information on top of the header. is about 6,400 lines long, and is regenerated from a database of the API as needed. Each of the WebGPU implementations seems to augment or tweak the header a bit. So, rather than grind through and manually do annotations, I wrote a little Swift script to “parse” , identify its patterns, and generate  for most of what is discussed in this post. The entirety of the script is here. It reads  from standard input and prints  to standard output.Because  is generated, it has a very regular structure that we can pick up on via regular expressions. For example:That’s enough to identify all of the enum types (so we can emit the EnumExtensibility: closed API notes), object types (to turn them into shared references), and functions (which get nicer names and such). The script is just a big  loop that applies the regexes to capture all of the various types and functions, then does some quick classification before printing out the API notes. The resulting API notes are in WebGPU.apinotes, and the generated Swift interface after these API notes are applied is here. You can run it with, e.g.,swift  webgpu_apinotes.swift < webgpu.h
This script full of regular expressions is, admittedly, a bit of a hack. A better approach for an arbitrary C header would be to use  to properly parse the headers. For WebGPU specifically, the webgpu-headers project contains a database from which the header is generated, and one could also generate API notes directly from that database. Regardless of how you get there, many C libraries have well-structured headers with conventions that can be leveraged to create safer, more ergonomic projections in Swift.The techniques described in this post can be applied to just about any C library. To do so, I recommend setting up a small package like the one described here for WebGPU, so you can iterate quickly on example code to get a feel for how the Swift projection of the C API will work. The annotations might not get you all the way to the best Swift API, but they are a lightweight way to get most of the way there. Feel free to also extend the C types to convenience APIs that make sense in Swift, like I did above to make  conform to ExpressibleByBooleanLiteral.A little bit of annotation work on your favorite C library can make for a safer, more ergonomic, more Swifty experience of working with that library.The regular structure of  helped considerably when trying to expose the API nicely in Swift. That said, there are a few ways in which  could be improved to require less annotation for this purpose: would be slightly nicer if placed on the  itself, rather than on the . If it were there, we could useand not have to generate any API notes to bring these types in as proper enums in Swift. could provide the names of the retain and release operations and be placed on the  itself. If it were there, we could useand not have to generate any API notes to bring these types in as classes in Swift. could be placed on the pointer itself (i.e., after the ) rather than at the beginning of the type, to match the position of Clang’s nullability attributes. If it were placed there, thenwould work with Clangs’ longstanding nullable-types support. Swift would then import such pointers as optional types (with ). Moreover, if some macros WGPU_ASSUME_NONNULL_BEGIN and  were placed at the beginning and end of the header, they could be mapped to Clang’s pragmas to assume that any pointer not marked “nullable” is always non-null:This would eliminate all of the implicitly unwrapped optionals (marked  in the Swift interface), making it easier to use safely.]]></content:encoded></item><item><title>Anthropic Economic Index report: economic primitives</title><link>https://www.anthropic.com/research/anthropic-economic-index-january-2026-report</link><author>malshe</author><category>hn</category><pubDate>Thu, 22 Jan 2026 21:54:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[How is AI reshaping the economy?This report introduces new metrics of AI usage to provide a rich portrait of interactions with Claude in November 2025, just prior to the release of Opus 4.5. These “primitives”—simple, foundational measures of how Claude is used, which we generate by asking Claude specific questions about anonymized Claude.ai and first-party (1P) API transcripts—cover five dimensions relevant to AI’s economic impact: user and AI skills, how complex tasks are, the degree of autonomy afforded to Claude, how successful Claude is, and whether Claude is used for personal, educational, or work purposes.The results reveal striking geographic variation, real-world estimates of AI task horizons, and a basis for revised assessments of Claude's macroeconomic impact.The data we release alongside this report are the most comprehensive to date, covering five new dimensions of AI use, consumer and firm use, and country and region breakdowns for Claude.ai.What has changed since our last reportIn the first chapter, we revisit findings from our previous Economic Index report published in September 2025. We find:Claude usage remains concentrated among certain tasks, most of them related to coding While we see over 3,000 unique work tasks in Claude.ai, the top 10 most common tasks account for 24% of our sampled conversations, a slight increase since our last report. Augmentation patterns (conversations where the user learns, iterates on a task, or gets feedback from Claude) edged to just over half of conversations on Claude.ai. In contrast, automated use remains dominant in 1P API traffic, reflecting its programmatic nature.Global usage remains persistently uneven while US states convergeThe US, India, Japan, the UK, and South Korea lead in overall Claude.ai use. Worldwide, uneven adoption remains well-explained by GDP per capita. Within the US, workforce composition plays a key role in shaping uneven adoption as states with more computer and mathematical professionals show systematically more Claude usage.While substantial concentration remains, since our last report Claude usage has become noticeably more evenly distributed across US states. If sustained, usage per capita would be equalized across the country in 2-5 years.Introducing and analyzing our new economic primitivesIn the second chapter we discuss the motivation for and introduce our new economic primitives, including how they were selected and operationalized, and their limitations. We additionally present evidence that our primitives capture directionally accurate aspects of underlying usage patterns as compared to external benchmarks. In chapters three and four we use these primitives to further investigate implications for adoption and productivity. We find:Claude use diversifies with higher adoption and incomeWhile the most common use of Claude is for work, coursework use is highest in countries with the lowest GDP per capita, while rich countries show the highest rates of personal use. This aligns with a simple adoption curve story: early adopters in less developed countries tend to be technical users with specific, high-value applications or use Claude for education, whereas mature markets see usage diversify toward casual and personal purposes.Claude succeeds on most tasks, but less so on the most complex onesWe find that Claude generally succeeds at the tasks it is given, and that the education level of its responses tends to match the user's input. Claude struggles on more complex tasks: As the time it would take a human to do the task increases, Claude’s success rate falls, much like prominent evals measuring the longest tasks that AIs can reliably perform. Job exposure to AI looks different when success rates are factored inWe also use the success rate primitive to better understand job exposure to AI, calculating the share of each occupation that Claude can perform by weighting task coverage by both success rates and the importance of each task within the job. For some occupations, like data entry keyers and database architects, Claude shows proficiency in large swaths of the job.Claude is used for higher-skill tasks than those in the broader economyThe tasks we observe in Claude usage tend to require more education than those in the broader economy. If we assume that AI-assisted tasks diminish as a share of worker responsibilities, removing them would leave behind less-skilled work. But this simple task displacement would not affect white-collar workers uniformly—for some occupations it removes the most skill-intensive tasks, for others the least.Without the tasks that we observe Claude performing, travel agents would experience deskilling as complex planning work gives way to routine ticket purchasing and payment collection. Property managers, by contrast, would experience upskilling as bookkeeping tasks give way to contract negotiations and stakeholder management.A new window for understanding AI’s impact on the economyThese results provide a new window into how AI is currently impacting the economy. Knowing the success rate of tasks gives a more accurate picture of which tasks might be automated, how impacted certain jobs might be, and how labor productivity will change. Measuring differential performance by user education sheds light on inequality effects.Indeed, the close relationship between education levels in inputs and outputs signals that countries with higher educational attainment may be better positioned to benefit from AI, independent of adoption rates alone.This data release aims to enable researchers and the public to better understand the economic implications of AI and investigate the ways in which this transformative technology is already having an effect.Chapter 1: What has changed since our last reportBecause frontier AI model capabilities are improving rapidly and adoption has been swift, it is important to regularly take stock of changes in how people and businesses are using such systems—and what this usage implies for the broader economy.In this chapter we analyze how Claude usage and diffusion patterns changed from August 2025 to November 2025 just prior to the release of Opus 4.5. We make four observations:Usage remains highly concentrated across tasks:The ten most common tasks represent 24% of observed usage on Claude.ai, up from 23% in our last report. For first-party (1P) API enterprise customers, concentration among tasks increased more notably: the top ten tasks now represent 32% of traffic, up from 28% in the last report.Augmentation is once again more common than automation on Claude.ai:In our previous report we noted that automated use had risen to exceed augmented use on Claude.ai, perhaps capturing both improving capabilities and greater familiarity among users with LLMs. Data from November 2025 points to a broad-based shift back toward augmented use on Claude.ai: The share of conversations classified as augmented jumped 5pp to 52% and the share deemed automated fell 4pp to 45%. Product changes during this period—including file creation capabilities, persistent memory, and Skills for workflow customization—may have shifted usage patterns toward more collaborative, human-in-the-loop interactions.Within the US, lower usage states have relatively faster gains in adoptionWithin the US, usage per capita remains largely shaped by how well-matched the workforce is to broader Claude usage: For example, states with a larger share of workers in computer and mathematical occupations tend to have higher usage. Indeed, the top five US states account for nearly half (50%) of all usage despite representing only 38% of the working-age population. Nevertheless, there are early signs of rapid regional convergence in adoption: usage has increased relatively faster for states that had lower usage in our last report. If sustained, usage per capita would be equalized across the country in 2-5 years, a pace of diffusion roughly 10x faster than the spread of previous economically consequential technologies in the 20th century.While this is consistent with rapid AI adoption and diffusion, this estimate comes with uncertainty given that it is based on a change observed over a three month period. Diffusion may ultimately proceed more slowly in the months and years to come.Global usage shows little sign of increasing or decreasing regional convergenceGlobally, Claude usage per capita—as captured by the Anthropic AI Usage Index (AUI)—remains highly uneven and strongly correlated with GDP. These gaps are stable: we see no evidence that low-use countries are catching up or that high-use countries are pulling away.Shifting patterns of usage across tasks and associated occupationsEven though frontier LLMs have an impressive range of capabilities relevant to every facet of the modern economy, Claude usage remains very concentrated among a small number of tasks. As compared to nearly one year ago, consumer usage on Claude.ai is modestly more concentrated: The share of conversations assigned to the ten most prevalent O*NET tasks was 24% in November 2025, 1pp higher than in August and up from 21% in January 2025. The most prevalent task in November 2025—modifying software to correct errors—alone represented 6% of usage.In our last Anthropic Economic Index Report we began tracking business adoption patterns by studying Claude usage among 1P API customers. The ten most common tasks grew from 28% of API records in August to 32% in November. Rising concentration among a small set of tasks suggests the highest-value applications continue to generate outsized economic value even as models have become more capable at a wider range of tasks. As with Claude.ai the most common task among API customers was modifying software to correct errors, which accounted for one in ten records.Indeed, computer and mathematical tasks—like modifying software to correct errors—continue to dominate Claude usage overall, representing a third of conversations on Claude.ai and nearly half of 1P API traffic. Such dominance has subsided on Claude.ai: the share of conversations on Claude.ai assigned to such (mostly) coding-related tasks is down from a peak of 40% in March 2025 to 34% in November 2025. At the same time, the share of transcripts assigned to computer and mathematical tasks among 1P API traffic edged higher from 44% in August to 46% in November 2025 (Figure 1.2).The second largest share of Claude.ai usage in November 2025 was in the Educational Instruction and Library category. This corresponds mostly to help with coursework and review, and the development of instructional materials. Such usage has risen steadily since our first report, up from 9% of conversations on Claude.ai in January 2025 to 15% in November.The share of usage on Claude.ai for Arts, Design, Entertainment, Sports, and Media tasks increased between August and November 2025 as Claude was used in a growing share of conversations for writing tasks, primarily copyediting and the writing and refinement of fictional pieces. This jump in the prevalence of design- and writing-related tasks reversed a steady decline across earlier reports. For both Claude.ai and API customers, there was a drop in the share of conversations/transcripts where Claude was used for Life, Physical, and Social Science-related tasks.Perhaps the most notable development for API customers was the increase in the share of transcripts associated with Office and Administrative Support related tasks, which rose 3pp in August to 13% in November 2025. Because API use is automation-dominant, this suggests that businesses are increasingly using Claude to automate routine back-office workflows such as email management, document processing, customer relationship management, and scheduling.Augmentation is again dominant on Claude.aiHow AI will affect the economy depends not just on the tasks Claude is used for but the way that users access and engage underlying model capabilities. Since our first report, we have classified conversations into one of five interaction types, which we group into two broader categories: automation and augmentation.Figure 1.3 plots how automated versus augmented use has evolved over time since we first started collecting this data one year ago. In January 2025, augmented use of Claude was dominant: 56% of conversations were classified as augmentation compared to 41% automated. In August 2025, more conversations were classified as automated as compared to augmented.This was a notable development since it suggested that rapid improvements in model capabilities and platform functionality coincided with users increasingly delegating tasks entirely to Claude. This was evident in the “directive” collaboration mode, which is further grouped as automation. Directive conversations are those in which users give Claude a task and it completes it with minimal back-and-forth. From January 2025 to August 2025 the share of such directive conversations rose from 27% to 39%.Three months later, the share of directive conversations had fallen 7pp to 32% in November 2025 as augmentation once again became more prevalent on Claude.ai than automation. Nevertheless, the automation share was still elevated as compared to nearly one year ago when we first began tracking this measure, suggesting that the underlying trend is still toward greater automation even as the August spike overstated how quickly it was materializing.While we see some evidence of a shift toward soft skill usage on Claude.ai with design, management, and education now higher, the shift back toward augmented use was broad-based in November (Figure 1.4). The rise in augmented use was driven mainly by users iterating with Claude to complete tasks (“task iteration”) rather than asking Claude to explain concepts (“learning”). See Figure 1.5 for common words associated with the three most common interaction modes across O*NET tasks and bottom-up descriptions of requests made of Claude.Persistent regional concentrationIn our previous report, we introduced the Anthropic AI Usage Index (AUI), a measure of whether Claude is over- or underrepresented in a given geography relative to the size of its working-age population. The AUI is defined asAn AUI above 1 indicates that a country uses Claude more intensively than its population alone would predict, while an AUI below 1 indicates lower-than-expected usage. For example, Denmark has an AUI of 2.1, meaning its residents use Claude at roughly twice the rate its share of the global working-age population would suggest.A key fact about Claude usage globally is that it is geographically concentrated: a small number of countries comprise an outsized share of use. From a global perspective, little changed in this respect between August and November 2025. Indeed, the left panel of Figure 1.6 shows that the AUI concentration across countries was essentially unchanged between our last report and this report.By contrast, usage became more evenly distributed across US states from August to November 2025: the Gini coefficient, a standard measure of equality, fell from 0.37 to 0.32. While it is important to exercise caution in interpreting short-run changes, this is a relatively large change toward perfect equality in which the AUI is equal to 1 for all states with a Gini coefficient of 0. If the Gini coefficient for the US again falls by 0.05 every three months, then parity of usage would be reached in roughly two years.What shapes patterns of usage within the US and around the world? In our previous report we emphasized the key role played by income differences globally: Variation in Claude usage across countries is largely accounted for by variation in GDP per capita. In Chapter 3 we revisit the importance of income in shaping not just usage intensity but also patterns of usage around the world.Within the US, income is less clearly a predictor of usage. Instead, what appears to matter most is the composition of each state’s workforce and how well-matched the workforce is to Claude capabilities as reflected in task-level usage. States that have a higher share of workers in computer and mathematical occupations—like Washington D.C., Virginia, and Washington—tend to have higher usage per capita. Quantitatively, each 1% increase in the share of such tech workers in a state is associated with 0.36% higher usage per capita (Figure 1.7). This alone accounts for nearly two-thirds of the cross-state variation in AUI.While we would intuitively expect Claude usage to be higher in states with more tech workers, this pattern holds more generally: Usage per capita is higher in states with more workers in occupations where Claude usage is overrepresented as compared to the US workforce (e.g., Arts, Design, Entertainment, Sports and Media) or with relatively fewer workers in occupations where Claude usage is low as compared to the national economy (e.g., Transportation and Material Moving). This can be seen by calculating the Kullback–Leibler (KL) divergence between the composition of each state’s workforce and the global composition of Claude usage. States with a lower KL divergence—and thus with a workforce that looks more similar to Claude usage patterns—tend to have higher usage per capita.Signs of faster Claude diffusion in the US among low usage statesWhile differences in workforce composition appear to play a role in shaping regional adoption within the US, early evidence suggests Claude is diffusing considerably faster than historical precedent would predict. Economically consequential technologies have historically taken around half a century to achieve full diffusion across the US (Kalanyi et al., 2025). By contrast, comparing Claude adoption rates in November 2025 to three months prior, we estimate that parity in adoption per capita across US states—as measured by the AUI—could be reached within 2–5 years. This estimate comes with a high degree of uncertainty as the precision of our estimates cannot rule out much slower rates of diffusion.We generate this estimate through the lens of a simple model of diffusion, which we briefly describe here. We model diffusion as proportional convergence toward a common steady state of equalized usage per capita in which each state s has an AUI equal to 1:Under this model, the log deviation of AUI from steady state (AUI = 1) shrinks by a factor of β every three months, implying a half-life of β quarters. For example, with quarterly data a value of β = 0.99 implies a half-life of about 17 years. To illustrate, starting from an initial AUI of 2, this means AUI would decline to around 1.4 after 17 years and to around 1.1 after 50 years. We take β = 0.99 as a sensible benchmark because it implies a pace of diffusion similar to economically consequential technologies in the 20th century.This model of convergence motivates the following regression specification:Naively estimating this equation by ordinary least squares (OLS) yields an estimate of β̂  ≈ 0.77. Weighted least squares (WLS) where we weight by each state’s workforce yields an estimate of β̂  ≈ 0.76 (Figure 1.8). Both are statistically distinguishable from 1 at conventional levels. Taken at face value, these estimates imply that it would take little more than two years for each state's AUI to close most of the gap to 1.A concern with estimating convergence this way is that our AUI estimates are subject to sampling noise and other variation unrelated to diffusion. This can produce classical attenuation bias: even if AUI is not actually changing, our estimate of β could end up meaningfully below one.To address this, we estimate the model by two-stage least squares (2SLS), instrumenting the log of AUI in August 2025 with the composition of each state's workforce, measured by its proximity to overall Claude usage patterns. The logic behind this instrument is that workforce composition is a strong predictor of Claude usage (relevance) but being measured independently, is expected to be uncorrelated with sampling noise in our AUI estimates (validity). As noted above, states with more workers in high-Claude-usage roles do tend to have systematically higher usage per capita.The 2SLS estimates imply modestly slower convergence: β̂  ≈ 0.89 unweighted and β̂  ≈ 0.86 when weighting by each state’s working-age population. However, these estimates are less precise, and only the former is statistically distinguishable from 1 at the 10% level. Despite implying a slower convergence than OLS, the 2SLS estimates still imply rapid diffusion: just four to five years for the log deviation of each state's AUI to shrink by 90%.That said, our estimates are based on just three months of data. And while the 2SLS specification may help address sampling noise, considerable uncertainty remains. We will revisit this question of the pace of diffusion in future reports.As with previous reports, all our analysis is based on privacy-preserving analysis. Throughout the report we analyze a random sample of 1M conversations from Claude.ai Free, Pro and Max conversations (we also refer to this as “consumer data” since it mostly represents consumer use) and 1M transcripts from our first-party (1P) API traffic (we also refer to this as “enterprise data” since it mostly represents enterprise use). Both samples come from November 13, 2025 to November 20, 2025. We continue to manage data according to our privacy and retention policies, and our analysis is consistent with our terms, policies, and contractual agreements. For 1P API data, each record is a prompt-response pair from our sample period which in some instances is mid-session for multi-turn interactions. The share of conversations on Claude.ai that were classified into neither automation nor augmentation categories fell from 3.9% to 3.0%. See, for example, Kalanyi et al (2025): “Second, as the technologies mature and the number of related jobs grows, hiring spreads geographically. This process is very slow, taking around 50 years to disperse fully.”With our bottom-up analysis of 1P API traffic we see Claude used to "Generate personalized B2B cold sales emails" (0.47%), "Analyze emails and draft replies for business correspondence" (0.28%), "Build and maintain invoice processing systems" (0.24%), "Classify and categorize emails into predefined labels" (0.23%), and "Manage calendar scheduling, meeting coordination, and appointment booking" (0.16%). At a high level, we distinguish between automation and augmentation modes of using Claude. Automation encompasses interaction patterns focused on task completion: Directive: Users give Claude a task and it completes it with minimal back-and-forth; Feedback Loops: Users automate tasks and provide feedback to Claude as needed; Augmentation focuses on collaborative interaction patterns: Learning: Users ask Claude for information or explanations about various topics; Task Iteration: Users iterate on tasks collaboratively with Claude; Validation: Users ask Claude for feedback on their work These interaction modes are not mutually exhaustive. In some instances, Claude determines that a sampled conversation does not match any of the five interaction modes. In this report we use Sonnet 4.5 for classification whereas in our previous Economic Index report we used Sonnet 4. We previously found that different models can generate different classification outcomes, though these effects tend to be modest.We include a constant term in the regression since it should be equal to zero under the null hypothesis. Across all our specifications, the constant term is estimated to be close to and statistically indistinguishable from zero.Chapter 2: Introducing economic primitivesThe strength of the Anthropic Economic Index lies in showing not only how much AI is used, but  it is used. In prior reports, we showed which tasks Claude is used for, and how people collaborate with Claude. These data have enabled external researchers to analyze labor market shifts (e.g., Brynjolfsson, Chandar & Chen, 2025).In this edition of the Anthropic Economic Index, we expand the breadth of data available to external researchers by providing insights on five economic “primitives”, by which we mean simple, foundational measures of the ways that Claude is used, which we generate by asking Claude to answer specific questions about the anonymized transcripts in our sample. Some of our primitives encompass several such questions, and others use a single indicator.Because AI capabilities are advancing so rapidly and the economic effects will be unevenly experienced, we need a breadth of signals to uncover not just how Claude is used but also to inform what impact this technology will have.Dimensions of AI use that matter for economic impactsThis report introduces five new economic primitives beyond the one we already measure, collaboration patterns (whether users automate or augment their tasks with Claude). These primitives capture five dimensions of a human-AI conversation: 1) task complexity, 2) human and AI skills, 3) work, coursework or personal use case, 4) the AI’s level of autonomy, and 5) task success (see Table 2.1). AI autonomy captures something different from our existing automation/augmentation distinction. For example, “Translate this paragraph into French” is high automation (directive, minimal back-and-forth) but low AI autonomy (the task requires little decision-making from Claude). captures that tasks can vary in their complexity, including how long they take to complete and how difficult they are. A "debugging" task in O*NET could refer to Claude fixing a small error in a function or comprehensively refactoring a codebase—with very different implications for labor demand. We measure complexity through estimated human time to complete tasks without AI, time spent completing tasks with AI, and whether users handle multiple tasks within a single conversation. address how automation interacts with skill levels. If AI disproportionately substitutes for tasks requiring less expertise while complementing higher-skilled work, it could be another form of skill-biased technical change—increasing demand for highly skilled workers while displacing lower skilled workers. We measure whether users could have completed tasks without Claude, and the years of education needed to understand both user prompts and Claude's responses. distinguishes professional, educational, and personal use. Labor market effects most directly follow from workplace use, while educational use may signal where the future workforce is building AI-complementary skills. measures the degree to which users delegate decision-making to Claude. Our latest report documented rising "directive" use where users delegate tasks entirely. Tracking autonomy levels—from active collaboration to full delegation—helps forecast the pace of automation. measures Claude’s assessment of whether Claude completes tasks successfully. Task success helps assess whether tasks can be automated effectively (can a task be automated at all?) and efficiently (how many attempts would it take to automate a task?). That is, task success matters for both the feasibility and the cost of automation labor tasks.Selecting and validating the new measuresThe new dimensions of AI use captured in our data were informed by our recent work on the productivity effects of Claude, feedback we received from external researchers, recent literature on AI’s economic impact through the lens of human capital and expertise (Vendraminell et al., 2025), and deliberation within our economic research team. Our main selection criteria were expected economic relevance, complementarity of dimensions, and whether Claude could classify conversations along that dimension with directional accuracy.We propose that multiple simple primitives, even if somewhat noisy and not perfectly accurate by themselves, can together provide important signals on how AI is being used. We therefore mainly tested for directional accuracy.For classifying task duration with and without AI, we used minimally modified versions of our prior productivity work. For net new classifiers, implemented via our privacy-preserving tooling, our validation process was as follows. We designed multiple potential measures to capture concepts such as task complexity. For Claude.ai, we evaluated the classifier performance compared to a human researcher on a small set of transcripts in which users gave feedback to Claude.ai and for which we thus have permission to look at underlying transcripts. For first-party API (1P API) data, we validate the classifiers using a mix of internal and synthetic data. Neither data sources are fully representative of Claude.ai or 1P API traffic, but they allow us to check that the classifiers are working on data that resembles real usage data, while ensuring privacy.Based on initial performance, we revised the classifiers that needed tweaking or discarded classifiers that did not perform well. Interestingly, we find that in some instances (e.g., to measure task success), a simple classifier performed better than a nuanced, complex classifier when compared to human ratings. We then compared performance of classifier versions with vs. without chain of thought prompting, and decided to keep chain of thought prompting only for three facets (human time estimate, human with AI time estimate, and AI autonomy) where we found that it substantially improved performance. We selected a final set of nine new classifiers for the five primitives, all of which are directionally accurate even if they may deviate somewhat from human ratings.The primitives' value is in what they can predictOur goal was to create classifiers that are straightforward to implement and in combination provide potentially important economic signals. While we are very confident in the directional accuracy of the new measures (e.g., tasks with higher average years of education needed to understand the human prompt are likely more complex), none of the measures should be taken as exact or definitive (e.g., Claude.ai may somewhat underestimate the human education years needed for many tasks).Even so, the primitives enrich our understanding of how people use AI. Systematic relationships emerge across primitives, regions, and tasks—patterns we explore in depth in Chapters 3 and 4. That these relationships are intuitive and consistent suggests the primitives capture relevant aspects of how people and businesses use Claude.External benchmarks reinforce this. In our productivity work, Claude’s time estimates correlate with actual time spent on software engineering tasks. Figure 2.1 shows that our human education measure correlates with actual worker education levels across occupations. These validations suggest individual primitives are directionally correct—and combining them may provide additional analytical value, such as enriching productivity estimates with task success rates or constructing new measures of occupational exposure.Ultimately, the strongest validation will come from the primitives’ ability to capture meaningful variation in labor market outcomes. The data we release enable external researchers to analyze economic shifts in new ways. Early work has been encouraging—the automation/augmentation distinction from prior reports has already been used by external researchers to analyze labor market shifts (Brynjolfsson, Chandar & Chen, 2025).Primitives highlight how use cases differTo illustrate how the primitives distinguish between different types of AI use, we examine two contrasting request clusters: software development ("Help debug, develop, and optimize software across multiple programming domains") and personal life management ("Assist with personal life management and everyday tasks"). Figure 2.2 shows the primitive profile for each cluster alongside global averages. Claude estimates that software development requests would take a competent professional approximately 3.3 hours to complete without AI—close to the global average of 3.1 hours. Personal life management tasks are estimated to be simpler, averaging 1.8 hours. Estimated human-AI collaboration time is similar across both (~15 minutes), showing this primitive varies less than other primitives for these two tasks. Software development requests draw on more specialized knowledge: both human prompts and AI responses are estimated to require approximately 13.8 years of education to understand, compared to 9.1–9.4 years for personal life management requests. Claude estimates that users would be able to complete personal life management requests by themselves 96% of the time, versus 82% for software development requests—indicating that Claude provides more essential support for technical work. Claude classifies 64% of software development requests as work-related, compared to just 17% for personal life management. This illustrates that Claude can be used for very different purposes. Overall, Claude.ai use is 46% work, 19% coursework, and 35% personal. Both clusters show similar estimated autonomy levels (~3.5 on a 1 to 5 scale), near the global average. This means that both software development and personal life management tasks, on average, afford Claude a similar autonomy to make decisions on how to complete the task. Claude assesses personal tasks as successfully completed 78% of the time, versus 61% for software development. Harder tasks—those requiring more specialized knowledge and where users could not easily complete them alone—show lower estimated success rates.Tasks and primitives differ between Claude.ai and API usersAs in our previous report, we find major differences in the tasks and primitives in Claude.ai conversations compared to the 1P API data. Part of this reflects the nature of the interaction: Claude.ai transcripts can include multi-turn conversations, while the API data we analyze is limited to single input-output pairs. This is because API requests arrive independently, with no metadata linking them to prior exchanges. This means we can only analyze them as isolated user-assistant pairs rather than full conversation trajectories.Overall, API usage is overwhelmingly work-related (74% vs. 46%) and directive (64% vs. 32%), with three-quarters of interactions classified as automation compared to less than half on Claude.ai (see Figure 1.3).Claude.ai users, by contrast, engage in more back-and-forth: task iteration and learning modes are far more common, and tasks tend to be more lengthy—both in terms of human time with AI (15 minutes vs. 5 minutes) and the estimated time a human would need to complete the task alone (3.1 hours vs. 1.7 hours). Claude.ai also shows higher task success rates (67% vs. 49%), which may reflect the benefits of multi-turn conversation, where users can clarify, correct course, and iterate toward a solution. Claude.ai users also give the AI more autonomy on average, and are more likely to bring tasks they couldn't complete alone.These differences are also reflected in the occupational distribution of tasks. API usage is heavily concentrated in Computer & Mathematical tasks (52% vs. 36%), consistent with its use for programmatic, automation-friendly workflows like code generation and data processing. Office & Administrative tasks are also more prevalent in the API (15% vs. 8%), reflecting routine business operations suited to delegation. Claude.ai, by contrast, sees substantially more Educational Instruction tasks (16% vs. 4%)—coursework help, tutoring, and instructional material development—as well as more Arts, Design, and Entertainment tasks (11% vs. 6%). Claude.ai also has a longer tail of human-facing categories like Community & Social Service and Healthcare Practitioners, where users seek advice, counseling, or information on personal matters.These patterns suggest that 1P API deployments concentrate on tasks amenable to systematic automation, while Claude.ai serves a broader range of use cases including learning, creative work, and personal assistance.Chapter 4 explores task-level variation in greater depth. A classifier is a model that assigns a given input (e.g. a user conversation) a specific output (e.g. the use case “work”). In this report, we use Claude as a classifier, meaning that we prompt Claude to select a specific output and then use Claude’s response as the output (see Table 2.1 for the prompts).Throughout this report, we use binned scatterplots to show bivariate relationships. We divide observations into 20 equally-sized bins based on the x variable, then plot the average x and y values for each bin. The leftmost dot, for example, represents the averages for observations in the lowest 5% of the x distribution.Chapter 3: How Claude is used varies by geographyIn this chapter, we analyze geographic variation in Claude usage patterns using a privacy-preserving¹ analysis of 1 million Claude.ai conversations². We make five observations:Claude is mostly used for work, but use cases diversify with adoption: Work and personal use cases are more common in higher-income countries, while coursework use cases are more common in lower-income countries. This echoes findings from our prior report and aligns with recent work by Microsoft.GDP and human education predict adoption globally and within the US: A 1% increase in GDP per capita is associated with a 0.7% increase in Claude usage per capita at the country level. Human education—Claude's estimate of years of formal education needed to understand the human prompt—correlates positively with the Anthropic AI Usage Index at both levels.Other primitives predict adoption differently at global vs. US levels: At the country level, higher usage correlates with shorter tasks and less AI autonomy. At the US state level, these relationships are not statistically significant, though work use correlates positively with adoption.Relationships between primitives depend on context: Task success is negatively associated with human education across countries, but positively within US states. However, when controlling for other primitives, the US relationship becomes insignificant.How humans prompt is how Claude responds: The education levels of human prompts and AI responses are nearly perfectly correlated (r > 0.92 at both levels). Higher per capita usage countries also show more augmentation—using Claude as a collaborator rather than delegating decisions entirely.Claude is mostly used for work, but use cases diversify with adoptionOur data, relying on a privacy-preserving analysis of 1 million Claude.ai conversations, reveals striking geographic differences in how Claude is adopted. Claude is predominantly used for work, across the globe and across the United States. However, there is geographic variation in use cases. At the global level, the Balkans and Brazil have the highest relative share of work use (see Figure 3.1), and Indonesia stands out with the highest share of coursework. At the US state level, New York stands out as the state using Claude relatively the most for work.Use case differences are related to a country’s per capita income, which, in turn, is related to per capita AI adoption. We observe that work use cases and personal use cases of Claude are more common in higher income countries, while coursework use cases are more common in lower income countries (see Figure 3.2). Interestingly, these findings converge with recent work by Microsoft showing that AI use for school is associated with lower per capita income, whereas AI use for leisure is associated with higher per capita income.Multiple factors could contribute to these patterns:Personal use cases may be more common as AI adoption increases and more diverse users use AI, or existing users explore wider applications of AI. In contrast, countries with lower per capita adoption (which is correlated with lower per capita income) may be focused on specific use cases such as coding or as coursework.Countries differ in their ability to pay for Claude, and coursework use cases may be better suited to free Claude usage than complex use cases in work areas such as software engineering.Users in higher-income countries may have other resources, such as free time and continuous Internet access, that enable non-essential personal use cases.International and US adoption differ across economic primitivesThe economic primitives introduced in this report allow us to analyze some of the factors that may drive differential adoption. When analyzing the relationship between the Anthropic AI Usage Index (AUI) and core economic primitives as well as GDP, we observe that certain patterns hold for both countries and US states. For example, we replicate the finding from our prior report that GDP is strongly correlated with the AUI (see Figures 3.3 and 3.4). At the country level, a 1% increase in GDP per capita is associated with a 0.7% increase in Claude usage per capita. Human education (how many years of education it takes to understand the human written prompts in a conversation) correlates positively and significantly with the Anthropic AI Usage Index both at the country and at the US state level.However, the relationship between AUI and the primitives often differs between country and US state level. For example, at the country level, the AUI correlates negatively with the time it would take a human to complete a task without AI, and with how much decision-making autonomy AI is given. At the US state level, these relationships are not statistically significant–likely also due to the smaller sample size for US states. Additionally, we observe a positive correlation between the AUI and Claude.ai use for work at the US state, but not at the country level.Importantly, the primitives themselves are not necessarily causal factors—we don't know if income or education are truly driving adoption, or if they're proxies for other underlying conditions. Many of these factors are highly correlated with one another. For example, at the US state level, human education years show a strong association with the Anthropic AI Usage Index in isolation, but this relationship disappears once we control for GDP and other primitives—suggesting education may be capturing variation that's better explained by economic development and other factors.Institutional factors shape the relationship between task success and education yearsEconomic and institutional context—such as how education levels vary within a geography—are related to how AI is being used. Interestingly, we observe that task success is negatively associated with human education at the country level, but positively related at the US state level. However, the positive relationship at the state level becomes insignificant when controlling for other primitives (see Figure 3.5). This means the relationship pattern at one level of observation (country) contradicts the relationship pattern at another level (US state). Cross-country, educated populations may attempt harder tasks and therefore see lower success rates. Within homogeneous contexts, education may not improve task success.How humans prompt is how Claude respondsWe find a very high correlation between human and AI education, i.e. the number of years of education required to understand a human prompt or the AI’s response (countries:  = 0.925,  < 0.001,  = 117; US states:  = 0.928,  < 0.001,  = 50). This highlights the importance of skills and suggests that how humans prompt the AI determines how effective it can be. This also highlights the importance of model design and training. While Claude is able to respond in a highly sophisticated manner, it tends to do so only when users input sophisticated prompts.How models are trained, fine-tuned and instructed affects how they respond to users. For example, one AI model could have a system prompt that instructs it to always use simple language that a middle school student could understand, whereas another AI model may only respond in complex language that would require a PhD education to understand. For Claude, we observe a more dynamic pattern where how the user prompts Claude relates to how Claude responds.Higher income and higher usage are related to more augmentationHigher per capita usage countries, which tend to be higher per capita income countries, show lower automation, and less decision-making autonomy delegated to Claude. That is, higher income countries use AI more as an assistant and collaborator rather than letting it work independently. This relationship is not significant at the US state level, perhaps because income variation and use case diversity are more limited within the United States than globally. This mirrors a finding from our 3rd Economic Index report where countries with higher Anthropic AI Usage Index tend to use Claude in a more collaborative manner (augmentation), rather than letting it operate independently (automation).The striking geographic variation in our data shows that Claude is used in different ways around the world. GDP predicts the Anthropic AI Usage Index at both the country and US state level, and human education—the sophistication of user prompts—correlates with adoption at both levels as well.Other relationships depend on context. At the country level, higher usage correlates with shorter tasks and less AI autonomy; within the US, these patterns do not hold. Task success and human education show opposite relationships globally versus within the US.The near-perfect correlation between human and AI education years underscores that how users prompt Claude shapes how it responds. Combined with the finding that higher-usage countries engage Claude more collaboratively, this suggests that the skills required to use AI well may themselves be unevenly distributed.By measuring the characteristics of conversations with Claude, we find important relationships with broader economic factors such as human capital. These relationships may help predict labor market outcomes and inform a smooth transition to an AI-enabled economy that will require different skillsets.For privacy reasons, our automated analysis system filters out any cells—e.g., countries, and (country, task) intersections—with fewer than 15 conversations and 5 unique user accounts. For bottom-up request clusters, we have an even higher privacy filter of at least 500 conversations and 250 unique accounts.Data in this section covers 1 million Claude.ai Free, Pro and Max conversations from November 13 to 20, 2025, randomly sampled from all conversations in that period. We then excluded content that was flagged as potential trust and safety violations. The unit of observation is a conversation with Claude on Claude.ai, not a user, so it is possible that multiple conversations from the same user are included, though our past work suggests that sampling conversations at random versus stratified by user does not yield substantively different results. Aggregate geographic statistics at the country and US state level were assessed and tabulated from the IP address of each conversation. For geolocation, we use ISO-3166 codes since our provider for IP geolocation uses this standard. International locations use ISO-3166-1 country codes, US state level data use ISO-3166-2 region codes, which include all 50 US states and Washington DC. We exclude conversations originating from VPN, anycast, or hosting services, as determined by our IP geolocation provider.The world map is based on Natural Earth’s world map with the ISO standard point of view for disputed territories, which means that the map may not contain some disputed territories. We note that in addition to the countries shown in gray (“Claude not available”), we do not operate in the Ukrainian regions Crimea, Donetsk, Kherson, Luhansk, and Zaporizhzhia. In accordance with international sanctions and our commitment to supporting Ukraine’s territorial integrity, our services are not available in areas under Russian occupation.“No data” applies to countries with partially missing data. Some territories (e.g., Western Sahara, French Guiana) have their own ISO-3611 code. Some of these have some usage, others have none. Since the Anthropic AI Usage Index is calculated per working-age capita based on working age population data from the World Bank, and population data is not readily available for all of these territories, we cannot calculate the AUI for these territories.We exclude the Seychelles from all geographic analyses because a large fraction of usage we saw during the sampling dates was abusive traffic.We exclude Wyoming from all US state analyses because a large fraction of usage we saw during the sampling dates was abusive traffic.Chapter 4: Tasks and productivityIn this chapter, we examine how time savings, success rates, and autonomy vary across task types, and what this entails for potential impacts on jobs and productivity.The patterns reveal that more complex tasks yield greater time savings, but that this trades off against reliability. In a simple task removal exercise inspired by Autor and Thompson (2025), Claude's tendency to cover higher-education tasks produces a net deskilling effect across most occupations, as the tasks AI handles are often the more skilled components of a job.Claude usage spans a meaningful fraction of tasks across a growing share of occupations. We incorporate success rates into a richer model of job coverage; some occupations with modest coverage see large effects because AI succeeds on their most time-intensive work. Adjusting productivity estimates for task reliability roughly halves the implied gains, from 1.8 to about 1.0 percentage points of annual labor productivity growth over the next decade. However, these estimates reflect current model capabilities, and all signs suggest that reliability over increasingly long-running tasks will improve.Tradeoffs in task accelerationOur estimates suggest that, in general, the more complex tasks in our data yield a greater time savings (or “speedup”) from AI. We derive this by having Claude estimate both how long a task would take a human working alone and the duration when human and AI work together, which we validated in previous work. Speedup is then the human-alone time divided by the human-with-AI time. So reducing a 1 hour task to 10 minutes would give a 6x speedup.The left panel of Figure 4.1 below gives the average speedup against our core measure of task complexity, the human years of schooling required to understand the inputs, all at the O*NET task level. It shows that in Claude.ai conversations, for example, prompts requiring 12 years of schooling (a high school education) enjoy a speedup of 9x, while those requiring 16 years of schooling (a college degree) attain a 12x speedup. This implies that productivity gains are more pronounced for use cases requiring higher human capital, consistent with evidence that white collar workers are far more likely to adopt AI (e.g., Bick et al 2025).Throughout the range of task complexity, the speedup is higher for API users. This could reflect the nature of the API data, which is restricted to single-turn interactions, and that API tasks have been specifically selected for automation.The results also capture a tradeoff, however. More complex tasks have a lower task success rate, as shown in the panel on the right. On Claude.ai, for example, tasks requiring less than a high school education (e.g., answering basic questions about products) attain a 70% success rate, but this drops to 66% for college-level conversations like developing analysis plans. Still, accounting for the difference in success rates—by either excluding low-success tasks or discounting speedups by success probability—does not eliminate the education gradient: complex tasks still show greater net productivity gains.One way to examine the implications of the education gradient is to look at the share of automation across the education levels required to understand the inputs. If high-education tasks show relatively more automation, it could signal more exposure for white collar workers. Here, though, the message is unclear: the automation share is essentially unrelated to the human levels of education required to write the prompt (Appendix Figure A.1). On both Claude.ai and 1P API, tasks across education levels show automation patterns in roughly equal shares.In what contexts do users defer more to Claude? Claude.ai users give the AI slightly more autonomy when working on more complex tasks. In contrast, API usage shows uniformly lower autonomy at all levels of complexity.Note though that these distributions do not span the same set of tasks. API usage covers a more narrow swath of tasks in the economy, as seen in the concentration plot in Chapter 1. The high education tasks that experience heavy usage in the API data include security analysis, testing and quality assurance, and code review, whereas Claude.ai users are more likely to have iterative, instructive sessions.Task Horizons in Real-World UsageRecent work on AI “task horizons” (Kwa et al., 2025) finds that AI success rates decline with task duration: longer tasks are harder for models to complete. With each successive model generation, however, this decline has become shallower as models succeed on increasingly long tasks. METR operationalizes task horizon primarily as the maximum duration at which a model achieves at least 50% success, and growth in this metric has become a key indicator of AI progress.Figure 4.3 shows a similar measure using our primitives. The plot shows task-level success rates against the human time required, all at the O*NET task level. In the API data, success rates drop from around 60% for sub-hour tasks to roughly 45% for tasks estimated to take humans 5+ hours. The fitted line crosses the horizontal 50% success line at 3.5 hours, suggesting that API calls attain a 50% success rate for tasks that are 3.5 hours. The analogous time estimate in METR’s software engineering benchmark is 2 hours for Sonnet 4.5 and about 5 hours for Opus 4.5. (The data in this report predates the release of Opus 4.5.)Claude.ai data tells a different story. Success rates decline far slower as a function of task length. Extrapolating using the linear fit, Claude.ai would hit a 50% success rate at about 19 hours. This may reflect how multi-turn conversation effectively breaks complex tasks into smaller steps, with each turn providing a feedback loop that allows users to correct course.It’s worth noting that a fundamental difference from the METR setting is selection. METR constructs a benchmark where a fixed set of tasks is assigned to models. In our data, users choose which tasks to bring to Claude. This means observed success rates reflect not just model capability but also user judgment about what will work, the cost of setting up the problem for Claude, and the expected time savings if the task succeeds.If users avoid tasks they expect to fail, for example, observed success rates will overstate true capability on the full distribution of potential tasks. This selection likely operates on both platforms, but in different ways: API customers select for tasks amenable to automation, while Claude.ai users select for tasks that could benefit from iteration. Also due to this selection effect, there’s no guarantee that more performant models would show improvement in this plot, because users may respond to new models by providing more challenging presentations of otherwise similar O*NET tasks.Controlled benchmarks like METR’s measure the frontier of autonomous capability. Our real-world data can measure the  task horizon, reflecting a mix of model capabilities and user behavior, and expanding beyond coding tasks. Both approaches find that AI can be effective for tasks requiring hours of human work.Revisiting occupation penetration with effective AI coverageOur earlier work found that 36% of jobs had AI usage for at least a quarter of their tasks, with about 4% reaching 75% task coverage. This measure was based only on the appearance of a task in our data, however. The primitives introduced in this report can help better characterize how AI is changing the work content of occupations.First, we find that task coverage is increasing. Combining across reports, 49% of jobs have seen AI usage for at least a quarter of their tasks. But incorporating that task’s share of the job, and Claude’s average success rate, suggests a different set of affected occupations.We define effective AI coverage as the percent of a worker’s day that can be performed successfully by Claude. It’s calculated as the weighted sum of task success rates, where each task's weight is its share of the worker's time adjusted by how frequently the task occurs. The success rate comes from our primitives, the hours estimate from our previous work on productivity effects, and the frequency estimate from O*NET data, where surveyed workers indicate how often they perform the task.The plot below shows how the effective AI coverage (y-axis) differs from task coverage alone (x-axis). The two are highly correlated, but with key differences. On the right side of the plot, occupations with high coverage—where almost all tasks appear with some frequency in Claude data—generally fall below the 45-degree line. This suggests that even 90% task coverage does not necessarily indicate large job impacts, since Claude may fail on key covered tasks or miss the most time-intensive ones.Zooming in, several occupations show large differences in effective AI coverage compared to task coverage. For example, data entry workers have one of the highest effective AI coverage. This is because although only two of their nine tasks are covered, their largest task—reading and entering data from source documents—has high success rates with Claude. AI excels at what they spend most of their time doing.Medical transcriptionists and radiologists also move up because their covered tasks happen to be their most time-intensive and highest-frequency work. For radiologists, their top two tasks— interpreting diagnostic images and preparing interpretive reports—have high success rates. These occupations have low task coverage because AI can't do the hands-on or administrative work in their job profiles, but it succeeds on the core knowledge work that dominates their workday.Microbiologists fall below the 45-degree line, suggesting lower effective AI coverage than would be predicted by task coverage alone. Claude covers half of their tasks, but not their most time-intensive: hands-on research using specialized lab equipment.This measure arguably gives a more realistic picture of job-level AI penetration. However, its implications depend on how often these Claude conversations actually displace or augment work that would otherwise be done by humans. For data entry clerks, AI likely does substitute for tasks previously performed manually. But when a Claude conversation maps to a teacher performing a lecture, it is less clear how this translates to reduced lecture time on the job. In future work, we could leverage our 1P API data to understand which of these tasks are being integrated into production workflows.AI’s impact on the task content of jobsBeyond how much of a worker's day AI can successfully perform, a separate question is which tasks get covered, and whether those tend to be the high-skill or low-skill components of the job. Recent research has studied changes in the task mix within jobs to understand AI's impact on wages and employment (Autor and Thompson 2025; Hampole et al 2025). A key insight is that automation's effects depend not just on how many tasks are covered, but on which tasks.To see how jobs change when we remove the tasks AI can perform, we first construct a measure of the level of skill required for each task. O*NET doesn't provide task-level education requirements, so we train a model that predicts years of schooling from task embeddings, using the BLS's occupation-level education as the target. This way, a low-education occupation may still have a high-skill task if it looks like those that tend to exist in high-education occupations. For example, Legal Secretaries is a 12-year education occupation, but the task “Review legal publications and perform database searches to identify laws and court decisions relevant to pending cases” is predicted to require 17.7 years because it resembles tasks typically performed by lawyers and paralegals.The data shows that Claude tends to cover tasks that require higher levels of education. The mean predicted education for tasks in the economy is 13.2 years. For tasks that we see in our data, the mean prediction is about a year higher, 14.4 years (corresponding to an Associate’s degree). This aligns with the occupation-level results from earlier reports, showing more Claude usage among white collar occupations.We next calculate how removing AI-covered tasks shifts the average education level of what remains. Overall, the net first-order impact is to deskill jobs, since AI removes tasks that require relatively higher levels of education. One job that experiences such deskilling is technical writers, which loses tasks like "Analyze developments in specific field to determine need for revisions" (18.7 years) and "Review published materials and recommend revisions or changes in scope, format" (16.4 years), leaving tasks like "Draw sketches to illustrate specified materials" (13.6 years) and "Observe production, developmental, and experimental activities" (13.5 years). Travel agents also experience deskilling because AI covers tasks like "Plan, describe, arrange, and sell itinerary tour packages" (13.5 years) and "Compute cost of travel and accommodations" (13.4 years), while tasks like "Print or request transportation carrier tickets" (12.0 years) and "Collect payment for transportation and accommodations" (11.5 years) remain. Several teaching professions experience deskilling because AI addresses tasks like grading, advising students, writing grants, and conducting research without being able to do the hands-on work of delivering lectures in person and managing a classroom.Some jobs see average education levels increase. Real estate managers experience upskilling because AI covers routine administrative tasks—maintaining sales records (12.8 years), reviewing rents against market rates (12.6 years)—while tasks requiring higher-level professional judgment and in-person interaction remain, like securing loans, negotiating with architecture firms, and meeting with boards.These patterns illustrate how jobs may evolve over the coming years as their task content adjusts in response to AI. If the education level can be interpreted like expertise in Autor and Thompson's analysis, their framework might predict that wages will fall and employment will increase for technical writers and travel agents; conversely, real estate managers will specialize in complex negotiations and stakeholder management, shrinking employment while increasing wages.However, our education-based measure differs from Autor and Thompson's expertise concept: their framework would label some tasks as high expertise where ours specifies low education—for example, the Electrician task "Connect wires to circuit breakers, transformers, or other components." And these predictions are based on current Claude usage patterns, which will shift as models are trained on new capabilities and users discover new applications—potentially changing which tasks are covered and whether the net effect is deskilling or upskilling.Revisiting the aggregate productivity implications of Claude usageIn earlier work, we estimated that widespread adoption of AI could increase US labor productivity growth by 1.8 percentage points annually over the next decade. Here we revisit that analysis, incorporating the task success primitive introduced in this report and a richer treatment of task complementarity.Based on the speedups associated with tasks with at least 200 observations in our sample of 1M Claude.ai conversations, we replicate our previous finding that current-generation AI models and current usage patterns imply a productivity effect of 1.8 percentage points per year over the next decade.With the inclusion of 1P API data, we can assess whether implied labor productivity effects differ based on enterprise Claude deployment patterns. Two countervailing forces are at play: API usage is more concentrated in a narrower set of tasks and occupations (particularly coding-related work), which would tend to reduce implied effects; but task-level speedups are higher on average among API tasks, as implied by Figure 4.1. These forces largely offset: the API sample likewise implies a 1.8 percentage point increase in labor productivity over the next decade.A salient critique of this analysis is that it fails to account for model reliability. If workers must validate AI output, the productivity benefits will be smaller than raw speedups suggest. To assess how quantitatively important this channel might be, we incorporate the task success primitive introduced in this report, multiplying task-level time savings by task-specific success rates before aggregating.This adjustment has a meaningful effect: implied productivity growth falls from 1.8 to 1.2 percentage points per year for the next decade based on Claude.ai usage, and to 1.0 percentage points for API traffic. Yet, even after accounting for reliability, the implied impact remains economically significant—a sustained increase of 1.0 percentage point per year for the next ten years would return US productivity growth to rates that prevailed in the late 1990s and early 2000s.A second critique concerns task complementarity. If some tasks are essential and cannot easily be substituted, then overall productivity effects will be constrained regardless of speedups on other tasks. Teachers may prepare lesson plans more efficiently with AI while having no impact on time spent with students in the classroom.To operationalize this idea, we impose some structure on how we aggregate task-level time savings within occupations but otherwise add up occupational efficiency gains as in the main analysis. Specifically, we suppose that within each occupation tasks are combined according to a Constant Elasticity of Substitution (CES) aggregator, where each task is weighted by the estimated time spent on each task as calculated in our earlier analysis of the productivity effects implied by Claude usage.The key parameter is the elasticity of substitution across tasks, σ. When the elasticity of substitution is less than one, tasks are complements and those tasks that are not sped up by AI become bottlenecks for broader productivity gains. Alternatively, when the elasticity of substitution is greater than one, then workers can allocate toward the more productive tasks—thereby amplifying the overall time savings at the occupational level. An elasticity of substitution equal to one is a special case that replicates the main analysis above.Figure 4.6 reports the results of this exercise for different values of task substitutability. As expected, when the elasticity of substitution is equal to one the implied productivity effect is the same as in our baseline analysis: An increase in labor productivity growth of ~1.8 percentage points per year over the next decade implied by both Claude.ai and API samples.When tasks are complements, however, the implied aggregate labor productivity impact declines sharply as the economic effects are bottlenecked by tasks that AI speeds up the least. For example, at =0.5 the implied overall labor productivity effect is 0.7-0.9 percentage points per year—around half the size as implied by our baseline estimates. Additionally adjusting for task success further reduces the implied productivity effects to 0.8pp for Claude.ai and 0.6pp for API.On the other hand, when the elasticity of substitution is greater than one, the implied labor productivity based on pre-Opus 4.5 usage patterns is materially higher. For example, at =1.5 the implied labor productivity effect rises to 2.2-2.6 percentage points per year, consistent with greater specialization in tasks where AI provides the largest speedups.In both cases the implied productivity impact based on API traffic is more responsive to the degree of task substitutability. This is consistent with the fact that there is a larger share of API traffic concentrated in fewer tasks and associated occupations as compared to Claude.ai: When tasks are complements, this concentration amplifies the bottleneck problem; when they are substitutes, it amplifies productivity gains from task specialization.What this analysis shows is that the productivity effects of automation may ultimately be constrained by bottleneck tasks that elude AI automation for the time being. And the labor market implications of increasingly capable AI could be similarly affected by such forces. For example, Gans and Goldfarb (2026) argue that the presence of bottleneck tasks within jobs means that partial AI automation can lead to an increase in labor income as such tasks increase in economic value (at least until a job is  automated).The upshot of this chapter is that the impact of AI on the economy is unlikely to be uniform. As our effective AI coverage framework illustrates, the labor market implications for different workers will hinge on how reliable frontier AI tools are for their most central tasks.But the labor market effects may also depend on the skill requirements of tasks that AI can proficiently handle relative to the rest of the economy. Indeed, we find that removing tasks Claude can already handle from the economy would produce a net deskilling effect: the tasks remaining for humans have lower educational requirements than those handled by AI.While highly suggestive, this may miss an important detail: the most complex tasks where Claude is used tend also to be those where it struggles most. Rather than displacing highly skilled professionals, this could instead reinforce the value of their complementary expertise in understanding AI's work and assessing its quality.The counterpart to these transformative labor market effects is the broader impact on growth and productivity. On the one hand, incorporating task reliability into our analysis diminishes the implied effect on labor productivity growth as informed by current Claude usage patterns. If bottleneck tasks bind, the implied impact diminishes further. On the other hand, the continuing growth in model capabilities suggests that both task coverage and task success may increase, which, in turn, could increase productivity impacts. When we study the correlation between primitives with the O*NET, we restrict to tasks appearing in at least 100 conversations to reduce measurement error. In the coverage analysis, we use all tasks above the privacy threshold of 15.We generate embeddings for each task statement using a pretrained sentence transformer (all-mpnet-base-v2) and predict education with Ridge regression.On the other hand, some historical evidence suggests that when technologies automating job tasks appear in patent data, employment and wages subsequently fall for exposed occupations (Webb 2020). When we first assessed the aggregate productivity implications of Claude usage, we relied on a sample of 100k Claude.ai conversations from Fall 2025. Based on the set of tasks for which we observed speedups, we estimated that labor productivity could be 1.8 percentage points higher per year over the next decade. Expanding the sample to 1M observations means that we need to take a stand on how to handle very infrequently occurring tasks—which are very common given that usage follows a power law, as we documented in our past report. We choose a threshold of 0.02% because it replicates our previous results for our sample of Claude.ai conversations. For privacy-preserving reasons, we only ever analyze tasks with at least 15 observations, or an implied threshold of 0.015% for a 100k sample. And so our results are internally consistent across samples. If we do not impose a restriction on our 1M sample and assume that efficiency gains for any task in our sample, even those with just 15 observations out of one million, the implied aggregate labor productivity growth over the next decade would be roughly 5% percentage points per year—a mechanical increase based on a the much larger set of tasks included.As before, this result is based on applying Hulten’s Theorem to task-level productivity shocks and assuming that the corresponding one-time increase in total factor productivity materializes over the course of a decade alongside capital deepening effects.As a reminder, for aggregating to implied labor productivity we calculate task-level efficiency gains as the log difference between human time without AI and with AI. There are certainly other ways to adjust based on task reliability. If tasks in our sample are composed of sub-tasks with heterogeneous AI applicability, and workers optimally deploy AI only on sub-tasks where it is effective, then scaling the efficiency gain by the success rate captures the extensive margin of AI adoption within a task. We use a CES (constant elasticity of substitution) production function to aggregate task-level time savings to economy-wide productivity impacts. The elasticity parameter σ governs how easily workers can substitute between tasks. When σ=1, we apply Hulten's theorem directly: the aggregate productivity gain equals the wage-share-weighted sum of log speedups across tasks. For σ≠1, we use a two-level aggregation: first, within each occupation, we compute an occupation-level speedup as a CES aggregate of task speedups weighted by time fractions, using ρ=(σ-1)/σ. Then we apply Hulten's theorem to these occupation-level speedups. When σ<1 (complements), productivity gains are bottlenecked by tasks with the smallest speedups. When σ>1 (substitutes), workers can specialize in tasks where AI provides the largest speedups, amplifying aggregate gains. For tasks without observed AI speedup data, we assume no productivity change. We thank Pascual Restrepo for suggesting this particular exercise.This fourth Anthropic Economic Index Report introduces economic primitives—foundational characteristics of AI use—that show how Claude is used by both consumers and firms. We use Claude to estimate the extent to which usage varies along these dimensions; these measures are directionally accurate and, taken together, provide important signals even if individual classifications are imperfect.Our findings carry significant implications for how AI will reshape economies and labor markets. Notably, Claude tends to be used more, and appears to provide greater productivity boosts, on tasks that require higher education. If these tasks shrink for US workers, the net effect could be to deskill jobs. But these impacts depend crucially on complementarity across tasks, and whether increased productivity at a certain task may increase the demand for it.At the global level, the strong relationship between per capita income and usage patterns—with higher-income nations using Claude collaboratively while lower-income countries focus on coursework and specific applications—suggests that AI's impact will be mediated by existing institutional structures rather than unfolding uniformly. Geographic diffusion patterns reinforce this picture. Within the US, per capita usage has converged slightly; globally, diffusion is slower. Combined with income-driven differences in how AI is used, this raises questions about whether AI will narrow or widen international economic gaps.Equally important to the patterns documented here are potential changes across this and subsequent reports. As AI capabilities advance, Claude's success rate may increase, usage patterns may show greater autonomy, users may tackle new and more complex tasks, and tasks that prove automatable may graduate from interactive chat to API deployment. We will track these dynamics over time, providing a longitudinal view of AI's role in the economy.Building on prior releases, this edition significantly expands both the scope and transparency of usage data we share, including task-level classifications along new dimensions and regional breakdowns globally for the first time. We publish this data to enable researchers, journalists, and the public to investigate novel questions about AI's economic impacts that can form the empirical foundation for policy responses.How willing users are to experiment with AI, and whether policymakers create a regulatory context that advances both safety and innovation, will shape how AI transforms economies. For AI to benefit users globally, expanding access alone will not suffice—developing the human capital that enables effective use, particularly in lower-income economies, is essential.Authors & AcknowledgementsRuth Appel, Maxim Massenkoff, Peter McCrory*Lead authors of the reportMiles McCain, Ryan Heller, Tyler Neylon, Alex TamkinXabi Azagirre, Tim Belonax, Keir Bradwell, Andy Braden, Dexter Callender III, Sylvie Carr, Miriam Chaum, Ronan Davy, Evan Frondorf, Deep Ganguli, Kunal Handa, Andrew Ho, Rebecca Jacobs, Owen Kaye-Kauderer, Bianca Lindner, Kelly Loftus, James Ma, Jennifer Martinez, Jared Mueller, Kelsey Nanan, Kim O'Rourke, Dianne Penn, Sarah Pollack, Ankur Rathi, Zoe Richards, Alexandra Sanderford, David Saunders, Michael Sellitto, Thariq Shihipar, Michael Stern, Kim Withee, Mengyi Xu, Tony Zeng, Xiuruo Zhang, Shuyi Zheng, Emily Pastewka, Angeli Jain, Sarah Heck, Jared Kaplan, Jack Clark, Dario Amodei]]></content:encoded></item><item><title>Scaling PostgreSQL to power 800M ChatGPT users</title><link>https://openai.com/index/scaling-postgresql/</link><author>mustaphah</author><category>hn</category><pubDate>Thu, 22 Jan 2026 21:24:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Capital One to acquire Brex for $5.15B</title><link>https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/</link><author>personjerry</author><category>hn</category><pubDate>Thu, 22 Jan 2026 21:23:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why does SSH send 100 packets per keystroke?</title><link>https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/</link><author>eieio</author><category>hn</category><pubDate>Thu, 22 Jan 2026 19:27:32 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Here are a few lines of summarized  output for an ssh session where I send a single keystroke:I said a “few” because there are a  of these lines.That is a lot of packets for one keypress. What’s going on here? Why do I care?I am working on a high-performance game that runs over ssh. The TUI for the game is created in bubbletea and sent over ssh via wish.I have also forked bubbletea to make it faster. Stay tuned!The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s outstanding profiling tools to look at what’s going on.Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?As part of debugging the test harness issue, I used  to log game traffic with and without the breaking change. Something like:Our breaking change stopped us from rendering our game over ssh. So with-breaking-change.pcap contains packets that represent the  of each connection without actually rendering the game.I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.This was baffling to me (and to Claude Code). We kicked around several ideas like:SSH flow control messagesPTY size polling or other status checksSome quirk of bubbletea or wishOne thing stood out - these exchanges were initiated by my  (stock ssh installed on MacOS) - not by my server.On a hunch, I took a  of a regular ssh session.I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the  output.I saw the exact same pattern! What in the world?Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.Running  gave me a pretty good sense of what was going on:That  is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.In 2023, ssh added keystroke timing obfuscation. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where  is critical.Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass ObscureKeystrokeTiming=no when starting up ssh sessions.This worked great. CPU usage dropped dramatically and bots still received valid data.But this is hardly a solution in the real world. I want  to Just Work without asking users to pass options that they might not understand.Claude Code originally didn’t have much faith that we could disable this functionality server-side.generated with simon willison's excellent claude-code-transcripts toolFortunately, the description I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the  extension. What if we just…don’t advertise ?I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (go’s replace directive makes forking a library very easy).Then I re-ran my test harness. The results were…very good:Claude was also pretty pumped:yes it's 1:30 am what of itObviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.But this is a  improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A >50% drop was unimaginable to me.Debugging with LLMs was funI am familiar enough with , , and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.There were still edge cases. At some point in my confusion I switched to ChatGPT  and it  confidently told me that my tcpdump output was normal ssh behavior:do all chatgpt messages have this tone and formatting now?And then doubled down when I pushed back:Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.Besides. Being in the loop is fun. How else would I write this post?]]></content:encoded></item><item><title>&apos;Active&apos; sitting is better for brain health: review of studies</title><link>https://www.sciencealert.com/not-all-sitting-is-equal-one-type-was-just-linked-to-better-brain-health</link><author>mikhael</author><category>hn</category><pubDate>Thu, 22 Jan 2026 19:03:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Excessive sitting isn't good for a person's physical or mental health, but there's a type of sedentary activity that may not shrink our brains or cost our cognition to the same extent.A systematic review of 85 studies has now found good reason to differentiate between 'active' sitting, like playing cards or reading, and 'passive' sitting, like watching TV.The former may actually boost brain health.That's probably because active sitting engages the brain, whereas passive sitting lets a person take a back seat both physically and cognitively."Total sitting time has been shown to be related to brain health; however, sitting is often treated as a single entity, without considering the specific type of activity," explains public health researcher Paul Gardiner from the University of Queensland in Australia."Most people spend many hours sitting each day, so the type of sitting really matters … These findings show that small everyday choices – like reading instead of watching television – may help keep your brain healthier as you age."Across numerous studies, Gardiner and colleagues found that active sitting activities, like reading, playing card games, and using a computer, showed "overwhelmingly positive associations with cognitive health, enhancing cognitive functions such as executive function, situational memory, and working memory."Meanwhile, passive sitting was most consistently associated with negative cognitive outcomes, including increased risk of dementia.The effect sizes were small but significant. The study authors hope their results will help inform future health research and more nuanced health guidance.For example, the researchers suggest guidelines should recognize the difference between passively watching TV and actively using a computer, and encourage people to take short breaks to stimulate their brains and move.Their review focused on studies of typical sedentary activities in natural settings, rather than structured programs designed to boost brain function, making it relevant to people's everyday lives."Health advice could shift from simply saying 'sit less' to encouraging more mentally engaging activities while sitting," argues Gardiner."This could help people make easy, realistic changes that support long‑term brain health and potentially reduce dementia risk."]]></content:encoded></item><item><title>I was banned from Claude for scaffolding a Claude.md file?</title><link>https://hugodaniel.com/posts/claude-code-banned-me/</link><author>hugodan</author><category>hn</category><pubDate>Thu, 22 Jan 2026 18:38:27 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Recent discoveries on the acquisition of the highest levels of human performance</title><link>https://www.science.org/doi/abs/10.1126/science.adt7790</link><author>colincooke</author><category>hn</category><pubDate>Thu, 22 Jan 2026 18:01:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Macron says €300B in EU savings sent to the US every year will be invested in EU</title><link>https://old.reddit.com/r/europe/comments/1qjtvtl/macron_says_300_billion_in_european_savings_flown/</link><author>consumer451</author><category>hn</category><pubDate>Thu, 22 Jan 2026 17:42:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CSS Optical Illusions</title><link>https://alvaromontoro.com/blog/68091/css-optical-illusions</link><author>ulrischa</author><category>hn</category><pubDate>Thu, 22 Jan 2026 17:41:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
  1 - Poggendorff Illusions
The Poggendorff illusion is an optical illusion in which a diagonal line interrupted by a vertical bar appears misaligned, even when both segments are actually continuous.A simple version of this effect can be seen in the following demo. I used the  and  pseudo-elements to create the diagonal line and the vertical bar, respectively.The effect can also be seen in a more elaborate version with multiple diagonal lines and vertical bars:This drawing can easily be achieved using two CSS gradients: one tilted at 70 degrees and another consisting of a series of vertical columns. I applied it to the , although I could have used  instead.Another variation of this illusion is the Münsterberg Poggendorff Arch, in which the two sides of an arch appear misaligned and seem as though they will not meet at the top - but they do (mouse over to see it).The following illusions combine gradients and flat colors. Surprisingly, some of the gradients do not actually exist. They are simple gray bars that, when placed over a gradient, appear to have gradients themselves.Take the following demo: all three bars (two vertical ones on the sides and one horizontal bar in the center) are the same shade of gray. The only real gradient is behind them, which tricks our brain into believing that the bars are different colors and even contain gradients.Here is another variation of this effect. It looks like the central line has a repeating gradient of dark and light grays, but in reality it is a flat color. If you mouse over the demo, the bar will expand, making it clear that there is no gradient at all.The next few optical illusions share a common idea: some colors are identical, but they do not look the same. This typically happens when regions of the same color or brightness are surrounded by areas with different contrast.For example, in the following demo, the left and right ends are the same shade of gray. However, one looks lighter because it is closer to white, while the other looks darker because it is closer to black. Mouse over to reveal that they are, in fact, the same color.Run the following demo. You will see two gray columns in a black-and-white grid. Both columns are the same shade of gray, but the one surrounded by black appears darker than the one surrounded by white.I coded this demo using  so I could try something a bit different. That worked well, but it also made it harder to showcase the effect on hover. In hindsight, I should have planned that better.This optical illusion also works with colors. For example, these two squares appear to be different shades of blue, but they are the same color. This time, you can mouse over to reveal the effect:
  5 - Wertheimer-Koffka Ring
The ring in the following illustration has the same color all the way around. However, one side is placed over white and the other over black, which makes them look different. If you mouse over the demo, the red bar will disappear, making it more obvious that the ring is a single, uniform color.You have probably seen the illusion involving a checkerboard and an object casting a shadow, where two tiles - one seemingly light and one seemingly dark - turn out to be the same color.This demo follows the same principle. You will see two tiles labeled A and B. Both have the same shade of gray, but most people cannot tell at first glance (or second, or even third).
  7 - Asahi illusion of Brightness
The circle at the center of this flower-shaped element is the same white as the rest of the page, but it gives the impression of being brighter, as if it were emitting light.This is one of my favorite illusions in the collection. The circles (or spheres) look red, blue, or green, but in reality they are all the same grayish color. Our brain "colorizes" them based on the lines that overlap the shapes. Don't believe it? Mouse over the illustration.In the following illustration, the lines inside the yellow section appear blue, while the lines inside the blue section appear red... but they are all black (or very dark gray). The white contour creates the illusion of color. Mouse over to remove the contour and the lines will clearly appear black.One set of lines looks straighter (top) while the other looks more curved (bottom). In reality, both sets are equally wavy. The only difference is how they are colored: changing the color at the peaks makes the lines look straighter. Changing it at the inflection points makes them look more curved.This is a classic optical illusion and an easy one to code in CSS. Three gradients are all that is needed to generate the effect in which the horizontal lines appear slanted, even though they are perfectly parallel.This optical illusion depicts an impossible shape. Parts that should be in front appear in the back, top becomes right, and everything feels contradictory. I coded this one some time ago for the 2024 Divtober event.Which orange circle is larger: the one on the right or the one on the left? It is a trick question: both are the same size. However, having smaller surrounding elements gives the impression that one is larger.I also created an animated version of this illusion (see below), as well as another version using a square shape instead of a flower shape:When people look at this illustration, they usually say they see a white square over black circles. However, the square is not actually there. The "Pac-Man" shapes create the illusion of a square and a sense of depth. Our brain fills in the missing information.
  15 - Ehrenstein's Illusion
There are no circles or discs in this illustration, only vertical and horizontal lines forming crosses. Our visual system completes the shape and makes us perceive a disc that does not exist.
  16 - Neon-Color-Spreading Illusion
This illustration shows concentric circles, some of which have a green-and-black pattern. Our brain perceives a central patterned circle and four concentric circles around it, beneath the green circle.I cheated a little when creating this in CSS, as I actually used a green circle blended with the other backgrounds.
  17 - Hering and Wundt Illusions
Perspective-based illusions are fascinating. Even when we know we are looking at a flat image, our brain insists on interpreting depth.In the Hering illusion, the red lines appear to curve outward, even though they are straight.The  effect is the Wundt illusion. When the lines expand from the sides toward the center, the red lines appear to curve inward (this effect is more subtle).Both yellow lines are the same length, but the top one looks longer due to perceived depth and perspective. I tried a different approach when coding this one by applying a three-dimensional rotation in CSS... so the perspective is technically real.This illusion is easy to code in CSS and easy to fall for. Both the vertical and horizontal lines are the same length, but the vertical line appears longer.
  20 - Müller-Lyer Illusion
A classic illusion: the horizontal lines are the same length, but inward- or outward-pointing edges dramatically change how we perceive them. I could swear the top one is longer. But it is not.
From a coding perspective, each shape is a pseudo-element. I ensured the horizontal lines were identical by using the same gradients and only repositioning the edges in the  and .
  21 - Tilted Table Illusion
It looks like the top rectangle is leaning to the left, but it is actually parallel to the one at the bottom. The trick lies in the direction of the diagonal lines used to "color" each rectangle.
This illusion works better on larger screens. The effect is diminished when you can see the whole picture.This is a simple effect: the black lines are parallel, but they appear not to be because of the direction of the bars crossing them.
I slightly overcomplicated this one while coding it. I initially built the black-and-red version below and tried to reuse more code than I probably should have.Here is the original version I created. The effect is also visible there:Good news! There are more optical illusions below - but first, a warning.
  ATTENTION: The following optical illusions are static, but they give the impression of movement. Proceed accordingly.
(Leaving some blank space in case you do not want to continue.)This is a trippy optical illusion. It is completely static, yet it looks like the black hole at the center is expanding - especially when you are not looking at it directly, creating the sensation of falling into a pit.From a coding perspective, this one was very simple: a background pattern made with two radial gradients, plus a blurred pseudo-element for the "expanding" hole.This is one of only two optical illusions in this collection where I used HTML elements instead of relying exclusively on CSS. It is a classic effect: when you look at the illustration, the peripheral discs appear to rotate, even though nothing is actually moving.Another classic illusion. Focus on the white dots and the adjacent dots will appear to turn black. There is no animation, no transition, and nothing dynamic. Just intersecting lines and small white circles, yet it looks like motion.This pattern consists of repeating black and white dots across the page. If you focus on one dot, the others will begin to disappear. At first it may happen by row or column, but after a short while, most of them vanish.If you do not immediately see the effect, try focusing on one black dot. Mouse over it, wait a few seconds while keeping your focus, and then mouse out.This is a static image, but it gives the impression that the pattern inside the circle is moving sideways. This happens because our eyes are constantly making small movements, even when we are not aware of it.If you cannot see the illusion, try slightly moving the screen (or your head) while looking just outside the circle.
  28 - Orthogonal Dotted Lines Sway
When you look around this pattern, the central area appears to slide and sway, even though it is completely static. This illusion makes me dizzy... but that may also be because I had to stare at it for a long time while coding it.This illusion is particularly interesting. There is a pink circle surrounded by concentric pink and purple rings. If you focus on the pink circle, the rings appear to spin or scintillate, as if there were some activity in them. Of course, nothing is actually moving.This demo was challenging to code and takes a long time to load. Mainly because it uses a large number of conic gradients behind the scenes, which browsers struggle to render efficiently. There is probably a better way to implement it, but I have not explored that yet.If you look closely at the illustration, you may notice wave-like motion. As with the previous illusions in this section, the image is entirely static.Good news! There are more optical illusions below - but first, another warning.
  ATTENTION: The following optical illusions actually move, and the illusion is created by motion itself. Some of them can be dizzying, so proceed accordingly.
(Leaving some blank space in case you do not want to continue.)
  31 - Animated Ebbinghaus Illusion
Earlier, we saw two static versions of the Ebbinghaus illusion. This one is animated. The elements move side to side, and the surrounding shapes grow and shrink, giving the impression that the orange circle is changing size - when it definitely is not.
  32 - Psychokinematic Tower
This looks like a three-dimensional tower spinning in space, as seen from above. In reality, it is a flat, two-dimensional image rotating.
Mouse over the demo to stop the rotation and the illusion of depth disappears entirely.This optical illusion requires only two gradients: a conic gradient for the fan-shaped arms and a radial gradient for the circles and discs.If you focus on the black dot, the illustration may appear to develop a darker greenish or brownish border. However, the colors never change.
  34 - Reverse Spoke Illusion
This illusion is delightful and disorienting. While the background colors of the wheel are spinning, the spokes remain fixed. However, they appear to rotate in the opposite direction. In reality, only the background is moving.What do you see in this animation? Most people report two sets of lines operating independently: one moving horizontally and another moving vertically. And that is exactly how it looks.In reality, it is a single shape moving uniformly. Run the demo, mouse over the lines, and the true motion will be revealed.
  36 - Mainz-Linez Illusion
Focus on one of the red dots. You will notice it moves straight up and down along a vertical path. Now shift your focus to one of the black crosses in the center. Suddenly, the red dots appear to zigzag instead of moving straight.It may look like the boxes are moving at different speeds or like a set of walking feet. In reality, all elements move at the same pace and in parallel. Mouse over the demo to reveal the effect.The illusion also works when the "feet" move in circles, as shown in this alternative version:Follow the red dot as it moves sideways. From the corner of your vision, it may appear that the dashed black-and-white lines are moving closer together (when the dot moves left) or farther apart (when it moves right). In reality, the lines are completely static.These dots always have the same color. However, when placed against alternating backgrounds, they appear to jump or move out of sync because of how they blend with their surroundings.Mouse over the demo to remove the background and the illusion disappears.This illusion gives the impression that a blue square is growing and shrinking rhythmically, almost as if it were breathing or beating like a heart.Although the image is rotating, its size never changes. Mouse over the illustration to remove the green boxes and reveal the rotating blue square.This illustration shows a circle made of pink dots, with one dot missing. Focus on the cross at the center and the missing dot will appear as a yellow or green dot, giving the impression that it is "eating" the pink dots. Just like Pac-Man.I could have used CSS trigonometric functions to calculate the exact positions of the dots, but since they never change, I chose to hardcode the values instead.Here is a related effect. Follow the light gray circle as it spins, and the darker circles will appear to change from gray to greenish. Focus on the cross at the center, and after a short time, the darker circles may begin to fade entirely.
  42 - Pinna-Brelstaff Illusion
This illusion is particularly dizzying. Follow the bluish dot as it moves from right to left and back again. It will appear as though parts of the tiled background are shifting, even though they are static. The only moving element is the dot.
From a CSS perspective, I coded the pattern using conic gradients, and applied it to the  and  pseudo-elements. I then flipped one upside down and clipped it.The radii of a wheel, when viewed through a palisade, appear to curve. In reality, they are perfectly straight. Mouse over the demo to remove the palisade and you will see that the radii never bend.This animation demonstrates how our minds infer motion that may not actually be there. Consider the two blue dots. Different people perceive different movements: side to side, top to bottom, or even circular motion.Cover the right side of the animation so that you see only one dot at a time. The motion now appears vertical. Cover the bottom part instead, and the motion appears horizontal. This is our brain trying to complete the movement.These two illustrations are identical - same shapes, same animation. The only difference is the CSS timing function.The top animation moves smoothly from right to left. The bottom one appears to move choppily in the same direction, but if you focus on it, it may suddenly seem to reverse direction and move faster.Most of the inspiration for these optical illusions came from two excellent resources:You can also find this article on:(You can leave comments on those platforms and I will reply there).]]></content:encoded></item><item><title>Show HN: isometric.nyc – giant isometric pixel art map of NYC</title><link>https://cannoneyed.com/isometric-nyc/</link><author>cannoneyed</author><category>hn</category><pubDate>Thu, 22 Jan 2026 16:52:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Text-to-video model from scratch (2 brothers, 2 years, 2B params)</title><link>https://huggingface.co/collections/Linum-AI/linum-v2-2b-text-to-video</link><author>schopra909</author><category>hn</category><pubDate>Thu, 22 Jan 2026 16:31:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[360p or 720p, 2-5 seconds, Apache 2.0]]></content:encoded></item><item><title>Show HN: BrowserOS – &quot;Claude Cowork&quot; in the browser</title><link>https://github.com/browseros-ai/BrowserOS</link><author>felarof</author><category>hn</category><pubDate>Thu, 22 Jan 2026 16:30:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Hey HN! We're Nithin and Nikhil, twin brothers building BrowserOS (YC S24). We're an open-source, privacy-first alternative to the AI browsers from big labs.The big differentiator: on BrowserOS you can use local LLMs or BYOK and run the agent entirely on the client side, so your company/sensitive data stays on your machine!Today we're launching filesystem access... just like Claude Cowork, our browser agent can read files, write files, run shell commands! But honestly, we didn't plan for this. It turns out the privacy decision we made 9 months ago accidentally positioned us for this moment.The architectural bet we made 9 months ago: Unlike other AI browsers (ChatGPT Atlas, Perplexity Comet) where the agent loop runs server-side, we decided early on to run our agent entirely on your machine (client side).But building everything on the client side wasn't smooth. We initially built our agent loop inside a Chrome extension. But we kept hitting walls -- service worker being single thread JS; not having access to NodeJS libraries. So we made the hard decision 2 months ago to throw away everything and start from scratch.In the new architecture, our agent loop sits in a standalone binary that we ship alongside our Chromium. And we use gemini-cli for the agent loop with some tweaks! We wrote a neat adapter to translate between Gemini format and Vercel AI SDK format. You can look at our entire codebase here: https://git.new/browseros-agentHow we give browser access to filesystem: When Claude Cowork launched, we realized something: because Atlas and Comet run their agent loop server-side, there's no good way for their agent to access your files without uploading them to the server first. But our agent was already local. Adding filesystem access meant just... opening the door (with your permissions ofc). Our agent can now read and write files just like Claude Code.What you can actually do today:--- Where we are now
If you haven't tried us since the last Show HN (https://news.ycombinator.com/item?id=44523409), give us another shot. The new architecture unlocked a ton of new features, and we've grown to 8.5K GitHub stars and 100K+ downloads:We are very bullish on browser being the right platform for a Claude Cowork like agent. Browser is the most commonly used app by knowledge workers (emails, docs, spreadsheets, research, etc). And even Anthropic recognizes this -- for Claude Cowork, they have janky integration with browser via a chrome extension. But owning the entire stack allows us to build differentiated features that wouldn't be possible otherwise. Ex:  Browser ACLs.Agents can do dumb or destructive things, so we're adding browser-level guardrails (think IAM for agents): "role(agent): can never click buy" or "role(agent): read-only access on my bank's homepage."Curious to hear your take on this and the overall thesis.We’ll be in the comments. Thanks for reading!]]></content:encoded></item><item><title>It looks like the status/need-triage label was removed</title><link>https://github.com/google-gemini/gemini-cli/issues/16728</link><author>nickswalker</author><category>hn</category><pubDate>Thu, 22 Jan 2026 16:10:20 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers</title><link>https://gptzero.me/news/neurips/</link><author>segmenta</author><category>hn</category><pubDate>Thu, 22 Jan 2026 15:20:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
                            Submit Here
                        : The authors of "AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing" noted that a mistake was made in the generation of that reference, and the correct reference is:Mehdi Azabou, Mohammad Gheshlaghi Azar, Ran Liu, Chi-Heng Lin, Erik C Johnson, Kiran Bhaskaran-Nair, Max Dabagia, Bernardo Avila-Pires, Lindsey Kitchell, Keith B Hengen, et al. Mine your own view: Self-supervised learning through across-sample prediction. arXiv preprint arXiv:2102.10106, 2021."Defining Hallucinated CitationsGiven the high stakes for both authors and publishers, GPTZero's Hallucination Check is engineered to be accurate, transparent, and cautious. It uses our AI agent, trained in-house, to flag any citations in a document that can’t be found online. These flagged citations are not automatically hallucinations — many archival documents or unpublished works can’t be matched to an online source — but they indicate which sources require further human scrutiny. As always, we recommend that a human confirm that flagged citation is an AI-generated fake instead of the result of a more conventional error.We define a vibe citation as a citation that likely resulted from the use of generative AI. Vibe citing results in errors common to LLM generations, but rare in human-written text, such as:Combining or paraphrasing the titles, author(s), and/or locators from one or more real sourcesFabricating the author(s), title, URL/DOI, and/or container (ex. publisher, journal, conference) of a sourceModifying the author(s) or title of a source by extrapolating a first name from an initial, dropping and/or adding authors, or paraphrasing the title.Our definition excludes obvious spelling mistakes, dead URLs, missing locators, and other errors that are plausibly human. The following table shows the difference between a real citation, a flawed citation, and a hallucinated citation according to our methodology. The differences are highlighted in red.Like GPTZero’s AI Detector, Hallucination Check has an extremely low false negative rate, so we catch 99 out of 100 flawed citations. Because our tool will flag any citation that can't be verified online, the false positive rate is higher.Over the past few months, we've experimented with several names for an LLM-generated citation with fabricated elements. "Hallucinated citations" is too long, "hallucitations" too easily mistaken for a spelling error, and "fake citations" too morally charged. Recently, GPTZero's Head of Machine Learning, Alex Adams, coined the term "vibe citing" to describe the LLM tendency to derive or amalgamate real sources into uncanny imitations. "Vibe citing," like "vibe writing" or "vibe coding" produces citations that look accurate at first glance, but crumble under closer inspection.GPTZero's analysis of 4841 of the 5290 papers accepted by NeurIPS 2025 indicates noticeable traces of AI authorship and hundreds of vibe citations. As always, each of the hallucinations presented here has been verified by a human expert.Surf the Tsunami with Hallucination CheckHallucination Check is the only tool of its kind, and provides an essential service at multiple points in the peer review pipeline. First, it allows authors to check their manuscripts for citation errors — including common issues that can occur without LLM involvement like dead links or partial titles. Second, it greatly reduces the time and labor necessary for reviewers to check a submission's sources and identify possible vibe citing. Third, using Hallucination Check in combination with GPTZero's AI Detector allows editors and conference chairs to check for AI-generated text and suspicious citations at the same time, leading to faster and more accurate editorial decisions.After releasing our ICLR paper investigation we are now coordinating with the ICLR team to review future paper submissions. As always, our goal is to make the peer review process faster, fairer, and more transparent for everyone involved. reach out to GPTZero's team]]></content:encoded></item><item><title>Tree-sitter vs. Language Servers</title><link>https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/</link><author>ashton314</author><category>hn</category><pubDate>Thu, 22 Jan 2026 14:47:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I got asked a good question today: what is the difference between Tree-sitter and a language server? I don’t understand how either of these tools work in depth, so I’m just going to explain from an ,  point of view.Tree-sitter is a . What this means is that you can hand Tree-sitter a description for a programming language and it will create a program that will parse that language for you. What’s special about Tree-sitter is that it is a.) fast, and b.) can tolerate  in the input. These two properties make Tree-sitter ideal for creating syntax highlighting engines in text editors. When you’re editing a program,  the program will be in a syntactically invalid state. During that time, you don’t want your colors changing or just outright breaking while you’re typing. Naïve regex-based syntax highlighters frequently suffer from this issue.Tree-sitter also provides a query language where you can make queries against the parse tree. I use this in the Emacs package I’m trying to develop to add Typst support to the Citar citation/bibliography tool: I can ask Tree-sitter to find a particular syntax object; it is safer and more robust than using a regular expression because it can do similar parsing to the Typst engine itself.In short, Tree-sitter provides syntax highlighting that is faithful to how the language implementation parses the program, instead of relying on regular expressions that incidentally come close.A  is a program that can analyze a program and report interesting information about that program to a text editor. A standard, called the Language Server Protocol (LSP), defines the kinds of JSON messages that pass between a text editor and the server. The protocol is an open standard; any language and any text editor can take advantage of the protocol to get nice smart programming helps in their system. Language servers can provide information like locating the definition of a symbol, possible completions at the cursor point, etc. to a text editor which can then decide how and when to display or use this information.Language servers solve the “

 problem” where 
 programming languages and 
 text editors would mean there have to be 
 implementations for language analyzers. Now, every language just needs a language server, and every editor needs to be able to speak the LSP protocol.Language servers are powerful because they can hook into the language’s runtime and compiler toolchain to get  answers to user queries. For example, suppose you have two versions of a  function, one imported from a  library, and another from a  library. If you use a tool like the dumb-jump package in Emacs
and you use it to jump to the definition for a call to , it might get confused as to where to go because it’s not sure what module is in scope at the point. A language server, on the other hand, should have access to this information and would not get confused.
  Using a language server for highlighting
  #It  possible to use the language server for syntax highlighting. I am not aware of any particularly strong reasons why one would want to (or  want to) do this. The language server can be a more complicated program and so could surface particularly detailed information about the syntax; it might also be slower than tree-sitter.Emacs’ built-in LSP client, Eglot, recently added eglot-semantic-tokens-mode to support syntax highlighting as provided from the language server. I have tried this a little bit in Rust code and it seems fine; the Tree-sitter-based syntax highlighting has been working just fine for me, so I will probably stick to that unless I find a compelling reason to use the LSP-based highlighting. Thanks to a comment on HN, I now know of a good reason why you would want to use a language server for syntax highlighting: the Rust language server rust-analyzer can tell your text editor when a variable reference is mutable or not, which means you could highlight  references differently than non- ones. Thanks to David Barsky for the tip!I wrote all of the above article. I did not ask an LLM to generate any portion of it. Please know that whenever you read something on my blog, it comes 100% from a human—me, Ashton Wiersdorf.I am not so anti-AI to say that LLMs are worthless or should never be used. I’ve used LLMs a little bit. I think they’re fantastic at translating between languages; this seems to be something that they should be good at doing. They’re helpful at writing some boring parts of the code I write. However, most of the time I find that I can typically write the tricky bits of the code about as fast as I could specify to an LLM what I want.I know that an LLM could have generated a facile pile of text much like the above, and honestly it would probably be decently helpful. However, know that what you have just read came directly from the fingers of a person who thought about the topic and bent his effort to helping you understand. This is from  human who understands the meaning behind each word here. I do not play games with syntax and generate answer-shaped blog posts. There is real meaning here. Enjoy it, and go forth and make more of it.]]></content:encoded></item><item><title>In Europe, wind and solar overtake fossil fuels</title><link>https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels</link><author>speckx</author><category>hn</category><pubDate>Thu, 22 Jan 2026 14:14:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: I&apos;ve been using AI to analyze every supplement on the market</title><link>https://pillser.com/</link><author>lilouartz</author><category>hn</category><pubDate>Thu, 22 Jan 2026 14:09:29 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Qwen3-TTS family is now open sourced: Voice design, clone, and generation</title><link>https://qwen.ai/blog?id=qwen3tts-0115</link><author>Palmik</author><category>hn</category><pubDate>Thu, 22 Jan 2026 13:51:25 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Douglas Adams on the English–American cultural divide over &quot;heroes&quot;</title><link>https://shreevatsa.net/post/douglas-adams-cultural-divide/</link><author>speckx</author><category>hn</category><pubDate>Thu, 22 Jan 2026 13:50:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In 2000, Douglas Adams made an interesting observation that I keep returning to.A user on Slashdot named “FascDot Killed My Pr” had asked the following question (where HGttG = Hitchhiker’s Guide to the Galaxy):First, a big thank-you. You’ve made a lasting contribution to “our” culture (or should that be “culture”?)I first read HGttG in my early teens. I doubled over laughing the whole time. I read and reread the entire series, bought both Dirk Gently books AND Last Chance to See. Loved them all and wouldn’t trade having read them for anything. (btw, the first mental ward scene in Long Dark Teatime is a no-foolin’, all-time classic.)However, a few years ago I was talking to a (then) classmate. Very smart, philosophy-major type. He said (paraphrased) “I thought that HGttG was depressing. Such nihilism.” At the time I thought “Hmmm…I didn’t SEE a black beret on his head….”. But every reading of the series since then his comment has struck me as more true–especially in the case of Arthur Dent. In fact, far from being funny, I now find Dent’s character depressing–he’s not just a loser, he literally has no control over his life at all (except in So Long for a while). And the control he does have does him no good (e.g. Earth is destroyed while he’s trying to save his house.)So my question is: When you were writing these books did you feel you were being gaily whimsical or did you instead feel frustrated and cynical?Douglas Adams replied with:I suspect there is a cultural divide at work here. In England our heroes tend to be characters who either have, or come to realise that they have, no control over their lives whatsoever – Pilgrim, Gulliver, Hamlet, Paul Pennyfeather (from Decline and Fall), Tony Last (from A Handful of Dust). We celebrate our defeats and our withdrawals – the Battle of Hastings, Dunkirk, almost any given test match. There was a wonderful book published, oh, about twenty years ago I think, by Stephen Pile called the Book of Heroic Failures. It was staggeringly huge bestseller in England and sank with heroic lack of trace in the U.S. Stephen explained this to me by saying that you cannot make jokes about failure in the States. It’s like cancer, it just isn’t funny at any level. In England, though, for some reason it’s the thing we love most. So Arthur may not seem like much of a hero to Americans – he doesn’t have any stock options, he doesn’t have anything to exchange high fives about round the water-cooler. But to the English, he is a hero. Terrible things happen to him, he complains about it a bit quite articulately, so we can really feel it along with him - then calms down and has a cup of tea. My kind of guy!I’ve hit a certain amount of difficulty over the years in explaining this in Hollywood. I’m often asked ‘Yes, but what are his goals?’ to which I can only respond, well, I think he’d just like all this to stop, really. It’s been a hard sell. I rather miss David Vogel from the film process. He’s the studio executive at Disney who was in charge of the project for a while, but has since departed. There was a big meeting at one time to discuss, amongst other things, Arthur’s heroicness or lack of it. David suddenly asked me ‘Does Arthur’s presence in the proceedings make a difference to the way things turn out?’ to which I said, slightly puzzled, ‘Well, yes.’ David smiled and said ‘Good. Then he’s a hero.’In the current, latest version of the screenplay, I think that Arthur’s non-heroic heroism is now absolutely preserved, and I’m pleased with the way he works out.I think I have more to say about this, and will try to come back and add more here, but meanwhile a few things at random:As a matter of fact, I  read The Book of Heroic Failures (1979) with great enjoyment. (Post from 2011 — I only wrote four sentences of my own, but one of them was “Too many books have been written in praise of competence; this book provides an antidote by celebrating failure as only a British author can.”)I think he is right that this goes over better (generally speaking) in England than in the USA. Of course one can make jokes  failure, but someone who fails does not automatically become endearing (in a kind of everyman way) in America the way they would in England. It seems to me that Americans are more likely to feel either contempt or pity than to feel kinship: or at any rate, they regard the failure as a setback or interesting circumstance, rather than the natural/default state of the world. (As someone who is neither American nor English, I am of course not someone whose opinions you should pay any heed to.)As we live our lives, are we merely victims subject to winds of chance and external circumstance, or are we powerful agents fashioning our own stories, making our own luck? Obviously the answer is “both”, but perhaps the most distinctively American trait is to lean more towards the latter.]]></content:encoded></item><item><title>Design Thinking Books (2024)</title><link>https://www.designorate.com/design-thinking-books/</link><author>rrm1977</author><category>hn</category><pubDate>Thu, 22 Jan 2026 11:51:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Can you think that following a design thinking process with five steps turns you into a creative innovator?! Believe me, it isn’t and never has been this way. The spread of the term design thinking is aligned with a significant amount of misleading criticism. The doubts about the effectiveness of design thinking are influenced by the promotional language used by some companies, training places, and public speakers. The truth is that there is no secret recipe to turn someone into a creative designer. Yet, there is a way to use the design expertise inside each of us. Understanding the design thinking core values can help team members improve their design ability and appreciate the creative practice inside the organization to achieve the next competitive advantage. This is why I wanted to share with you those key design thinking books to learn the core principles underpinning the design practice.This is an updated list of design thinking books that I keep adding to their new book suggestions. So, please keep the link or subscribe to the newsletters to receive updates once new books are added. I am also starting to add papers that represent the cornerstone in the design thinking principles that I believe are as important as the book. In this update, two books and one paper added: The Science of Artificial, Wicked Problems in Design Thinking, and How Designers Think.Previously, we explored different challenges that can be faced when applying design thinking inside the organization ( Why Companies Need to Apply Design Thinking and Why Companies Need to Apply Design Thinking). The majority of these factors rely on the lack of understanding of the core value of design thinking, which can be a reason for over-promotion and misuse of a commercialized language (check Why Design Thinking Doesn’t Work). Above all, many design thinking trainers are not designers themselves and never practice the creative practice before teaching it which causes the gap between classrooms and practices.To expand my knowledge of the core values behind design thinking, I thought I would share with you some of the book titles that highlight design characteristics. Each of these books explores design from a specific perspective. Learning about these design aspects is essential for both designers and non-designers before jumping to learn design thinking. While there are several books about design thinking toolkits, the books below don’t teach you to design thinking methodology but the core principles behind design thinking to develop new alternatives of ideas and improve the analytical thinking of problems and solutions. They aim to guide you in understanding the core values and practices of design as a collaborative process. By acquiring this knowledge, you can effectively apply any of the design thinking processes we discussed earlier in previous articles with effectiveness. I am sure that those are not the only books out there, so please share with us your book suggestions in the comments below the article.Design Expertise by Kees DorstThe Design Expertise, written by Lawson and Dorst, focuses on the understanding of design practice in the creative industry. The book aims to explore the nature of design from a practitioner’s perspective. It starts by exploring the different definitions of design and how they contributed to identifying the border of the discipline of design.The book presents design work for different designers and tries to use this overview of their work to provide a practical example of design characteristics. This book provides you with a base idea about design, what it is, and its characteristics. Exploring the characteristics through design thinking case studies, and examples helps you see design’s core value. This value is the main cornerstone behind the application of the design thinking process.Frame Innovation by Kees DorstOne of the main design characteristics is to solve problems or move from one position to an improved one. However, this can’t be achieved without a clear idea of the problem and its different borders. In his book Frame Innovation, Dorst explores the cognitive design process’s problem and solution frames. Also, he explores how designers move from one frame to another and how this feedback process contributes toward an optimum solution for wicked problems.Many of the design thinking process models move from the exploration stage (divergent) to defining the solution (conversion). While this practice shares the principle of critical thinking, they all move between the problem frame and solution frame. Through this book, you will explore the principles and practices of problem/solution frames to develop creative potential ideas.The book extends discussion of of the principle of frame innovation by covering the opportunities and challenges related to its application in creative industries. The book ends by putting a practice action plan to move toward using the frame innovation in different business models.Design Thinking: Understanding How Designers Think and WorkIn this small yet informative book, Design Thinking, Nigel Cross explores how designers think and reach creative ideas in the design field and the nature of design from the perspective of idea formation. To this goal, the book overviews design practice based on observing and interviewing creative designers and exploring expert tips with them. Design processes try to explore the design expertise from creating the idea to applying it. However, the design ability comes earlier when ideas are formulated. The book’s first chapter explores this design ability and how each of us has a level of design ability to develop new ideas. Yet, some people are more designers than others, which is known in Lusy Kimbel’s two papers as the creative class (Rethinking Design Thinking: Part 1 and Part 2).The book overviews designers’ practice in different fields and stories. The aim of this overview through creative designers’ experience is to build an understanding of the inspiration or exploration stage in the design thinking process. For instance, what is brainstorming, and why is it applied at an early point in the design thinking process (How to Successfully Apply Inspiration in Design Thinking)? Linking similar questions to the practice helps you map your practice to rational reasoning and subsequently improves the progress of the process in the future.Change by Design by Tim BrownChange by Design, by Tim Bowen, CEO of the IDEO, is probably one of the commonly known books about design thinking because of the popularity of the IDEO in the application of design thinking in various social innovation contexts. In his book, Tim Brown manifests his ideology about design thinking and interprets it from the organisational perspective. The book aims to clarify what design thinking is, and where to go from theory to practice. In the first part, the books focus on the main concepts of design thinking (check Design Thinking Tools and Methods Complete Guide), such as extending behind the aesthetics, shifting toward a human-centred approach (i.e. improving customer experience and building inclusive design), the power of prototyping, and the importance of storytelling. The second part of the book aims to interpret these principles for practicality to identify the business opportunities for design thinking and the use of design to achieve innovation inside organisations through creative collaboration between stakeholders.The book is a good resource for both designers and business people to understand design thinking and its applications. Despite several criticisms of the IDEO design thinking model, the book describes the theoretical base of design thinking, which could have a positive, innovative impact on organisations, especially if applied properly to develop viable business strategies. The IDEO Field Guide can be a good companion for the book as it presents a toolbox to apply Tim Brown’s ideology in practice.The Design of Everyday Things by Don NormanDon Norman is one of the leading professors in behaviour psychology and human-computer interaction (HCI). His book, The Design of Everyday Things, is based on a simple observation: why do we love and hate some elements in our lives? And what is the psychology behind our behaviour toward products? Addressing these two questions presents a cornerstone of your design practice. For example, why do some people love products such as Apple, Mini Cooper, or IKEA? By understanding how consumers love or hate products, the design team can target these features to build an empathic relationship between the product (or service) and the client, known as emphatic design.The book explores human-centred design and its impact on usability interaction design principles, as well as user experience. While other books covered this aspect of design experience, Norman studied the experience from a psychological point of view to examine this complex design process. The book covers the psychology behind our daily actions, knowledge, design limitations, and human error. Later, the book explores design thinking as a tool to solve problems and the usage of the Design Council Double Diamond design thinking process. The book is not only for UX designers but for designers from different practices, as you can learn the following:How the brain works and the psychology related to products and services,The limitations related to our experience with interacting with designs around andHuman error and a bad design causes .How Designers Think? by Bryan LawsonHow Designers Think? by Bryan Lawson is one of the design thinking books I recommend for my students who are still new to problem-solving and understanding the philosophical approach underpinning the problem and solution space in the design thinking process, How Designer Think for Bryan Lawson overviews the design definition, the relation between problem and solutions and the design thinking process. Unlike other books, Lawson doesn’t aim to teach you his method or derive a specific point; it is more like a discussion book to allow you to reflect and synthesise on the design practice and finally come up with your conclusion. The book presents a flow of ideas as a case study, making it easy to understand and enjoyable for new readers in design thinking. I recommend reading it before moving to more advanced books such as The Science of Artificial.The Science of Artificial by Herbert Simon The Science of Artificial is one of Simon’s most famous and irritating works based on three lectures for him at MIT in 1968, a year before the book was first published. The book discusses the nature of human thinking and the “artefact.” In eight chapters, it explores how humans use artefacts to solve everyday problems. His expression of human rationale is expressed with three premises:The limitations in the human’s cognitive abilityThe time available to make a decision, andThe complexity of the problemSimon was awarded a Nobel Prize for his theory and its contribution to economic rationality. According to the above theory, Simon defined three problem-solving activities: the ability to conduct a heuristic search for alternatives, evaluate solutions, and allocate resources for search. He illustrates this concept in his statement:” Human problem solving involves nothing more than varying mixtures of trial and error and selectivity. The selectivity derives from various rules of thumb, or heuristics, suggesting which paths should be tried first and which promising leads.”Wicked Problems in Design Thinking (Paper) by Richard BuchananWicked Problems in Design Thinking by Buchanan was published in Design Studies in 1992. Buchanan linked design and analytical philosophy by understanding the design problem’s nature and elements. He discussed two terms: “category” and “placement”, where we frame the different aspects of the problem. Buchanan describes them as follows:“Understanding the difference between a category and a placement is essential if design thinking is to be regarded as more than a series of creative accidents. Categories have fixed meanings that are accepted within the framework of a theory or a philosophy and serve as the basis for analysing what already exists. Placements have boundaries to shape and constrain meaning but are not rigidly fixed and determinate. The boundary of placement gives a context or orientation to thinking, but the application to a specific situation can generate a new perception of that situation and, hence, a new possibility to be tested. Therefore, placements are sources of new ideas and possibilities when applied to problems in concrete circumstances.”The expandable nature of the “placement” presents a critical element of wicked problems and how we can see them as a universal concept whose boundaries can change based on the situation. This manifestation of the definition of the problem elements presented the cornerstone for Kees Dorst’s problem/solution frame discussed in the earlier book Frame Innovation (What is the 8D Problem-Solving? ).His ideas of wicked problems link with Simon’s concept about the design thinking process and how it can be seen as a non-linear process where different design ideas interact in the design arena. Also, In this placement, Buchanan differentiated between four elements of the design thinking process:Thoughts: complex systems or environments, which is a weird characterisation.As he links the above elements and the two terms described earlier (category vs placement), he describes the nature of wicked problems:“However, when a designer’s conceptual placements become categories of thinking, the result can be mannered imitations of an earlier invention that are no longer relevant to discovering specific possibilities in a new situation. Ideas are then forced onto a situation rather than discovered in the particularities and novel possibilities of that situation.”The above manifestation describes how wicked problems are constructed and change over time, paving the way for a new perspective on problems and their analysis to identify new solutions (check also How to Use TRIZ in the Problem-Solving Process).The Dilemmas in a General Theory of Planning by Rittel and WebberThe Dilemmas in a General Theory of Planning by Rittel and Webber, despite its age, remains a seminal work that has significantly influenced the understanding of wicked problems. Published in 1969, this paper laid the groundwork for Kees Dorst’s Frame Innovation and Buchanan’s Wicked Problems, both of which we’ve discussed. While the paper’s focus is on planning and policy science, its insights can be applied to problem definition and the process of solving them. Rittel and Webber distinguished between two types of problems: tame and wicked problems. Tame problems are well-defined and clearly stated, and there is a clear direction to finding the solution, such as scientific and business problems (check how this concept influenced TRIZ problem-solving). In contrast, wicked problems are ill-defined, and we can’t define the problem until we reach a solution. However, a wicked problem is never solved, yet it moves from one state to an improved, desirable one. The other nature of wicked problems is that we cannot reach a definitive formulation for them. To describe them, we need to develop an exhaustive inventory of conceivable solutions when asking questions about the problem. So, problem understanding and resolution are linked and change as we build an understanding of the problem at a particular moment in time. The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda by Gilbert CocktonAs you can see, the above books and papers give us a novel look at design problems and how we perceive them. My question is, why do we see problems the way we used to? A big part of the answer lies in our language, which presents mental models that stand as barriers to seeing the core nature of problems, especially the wicked ones. Therefore, we needed new vocabulary that helped us to escape these constraints. The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda, by my PhD supervisor, Professor Gilbert Cockton, presents a cornerstone of new vocabularies that can help us see design thinking and how to solve problems. The paper eliminated the so-called design thinking process, as the term employs a linear nature; while design thinking is far from linear, it is intersected activities. Cockton described the design practice as design arenas; these arenas are distinguishing “artefacts” and “memoranda.” The “artefact” represents the design outcome, and the “memoranda” is the thing to be borne in mind. This new terminology replaces the problem and solution spaces. However, the Latin root of an artefact means the product of change or doing some art. However, this term is limited as wicked problems are not understood until we solve them, which means artefacts. So, the outcome of the design arena may remain the same as the original state, or the change is against the target user, such as preventive design and design against crime. So, Cockton replaced the word artefact with A_tefact.  The memoranda consist of three arenas:: The purpose of design: The Artefact and Evaluation: Modifications to the ArtefactOther terms were also introduced in the paper, such as episodes to replace stages (or phases) that are inherited from linear process age. The multiple foci (sequence by concurrency) replaced the centre of the process term to indicate the complex nature of the iteration with no simple way to describe it. The term “iteration” is replaced with balanced concurrent drama, and validation is replaced with the term “axiofact,” or the value generated. The new terminology presented in Cockton’s paper allows us to escape the old mental model when addressing wicked problems. If you check the MPPF method in Design Thinking, which we discussed previously, you will find it a useful tool as it can help us address wicked problems. Each of the above books and papers focuses on specific aspects of design and how we observe the design thinking practice driven by feedback from both academia and industry. The different design thinking models are based on appreciating these characteristics of design and encouraging it inside the organization. By applying the steps alone, you will never reach any improved status. You need to recognize these characteristics of design and try to practice them during the design process. Again, the above books came to my mind as key books in design. I am sure there are other titles. So, please share it in the comments below.Brown, T. and Katz, B., 2011. Change by design. Journal of Product Innovation Management, (3), pp.381-383.Buchanan, R., 1992. Wicked problems in design thinking. , (2), pp.5-21.Cockton, G., 2017, May. New process, new vocabulary: Axiofact= a_tefact+ memoranda. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (pp. 747-757).Cross, N., 2023. Design thinking: Understanding how designers think and work. Bloomsbury Publishing.Dorst, K., 2015. Frame innovation: Create new thinking by design. MIT press.Norman Donald, A., 2013. The design of everyday things. MIT Press.Lawson, B., 2006. . Routledge.Rittel, H.W. and Webber, M.M., 1973. Dilemmas in a general theory of planning. , (2), pp.155-169.Simon, H.A., 1988. The science of design: Creating the artificial. , pp.67-82.]]></content:encoded></item><item><title>&apos;Askers&apos; vs. &apos;Guessers&apos; (2010)</title><link>https://www.theatlantic.com/national/2010/05/askers-vs-guessers/340891/</link><author>BoorishBears</author><category>hn</category><pubDate>Thu, 22 Jan 2026 11:40:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Let's say your husband or wife has a friend who will be coming to your city for two weeks on business. This friend writes to you and your spouse, asking if you can put him up while he's in town. Has this person committed a gross violation of etiquette? Whether you answer yes or no may speak to whether you're an Asker or a Guesser--the two personality types described in a three-year-old Web comment that has lately taken on a second life as a full-on blog meme.On January 16, 2007, Andrea Donderi responded to an Ask MetaFilter post that dealt with a houseguest-related situation like the one described above. Donderi's take on the situation is as elegant as it is provocative. Basically, she says, there are two types of people in the world:This is a classic case of Ask Culture meets Guess Culture. In some families, you grow up with the expectation that it's OK to ask for anything at all, but you gotta realize you might get no for an answer. This is Ask Culture. In Guess Culture, you avoid putting a request into words unless you're pretty sure the answer will be yes. Guess Culture depends on a tight net of shared expectations. A key skill is putting out delicate feelers. If you do this with enough subtlety, you won't even have to make the request directly; you'll get an offer. Even then, the offer may be genuine or pro forma; it takes yet more skill and delicacy to discern whether you should accept.Over the weekend, Oliver Burkeman wrote a column for The Guardian taking up Donderi's dichotomy and asking, "Are you an Asker or a Guesser?" A number of bloggers took the bait, expanding into broader thoughts about the niceties of social etiquette. Here's what they had to say:Contributes to Personal, Professional, International Tensions  In his column for The Guardian, Burkeman notes that neither type's approach is wrong per se, "but when an Asker meets a Guesser, unpleasantness results. An Asker won't think it's rude to request two weeks in your spare room, but a Guess culture person will hear it as presumptuous and resent the agony involved in saying no. Your boss, asking for a project to be finished early, may be an overdemanding boor – or just an Asker, who's assuming you might decline. If you're a Guesser, you'll hear it as an expectation. This is a spectrum, not a dichotomy, and it explains cross-cultural awkwardnesses, too: Brits and Americans get discombobulated doing business in Japan, because it's a Guess culture, yet experience Russians as rude, because they're diehard Askers."We Ask Strangers and Close Friends  Libertarian blogger Julian Sanchez offers a sociological reading of Donderi's theory that's worth perusing in full. "The polite indirection of 'Guess Culture' is... often a way of preserving a deliberate ambiguity, which we generally want to do in social relationships where there's an intermediate level of intimacy—whereas relationships at the poles, with either close friends or strangers, tend to be governed by more direct asks," Sanchez writes. "We do this, I think, precisely because those intermediate relationships ambiguous: We’re indirect because we’re negotiating just where on the gradient we fall ... To ask too directly at that stage can seem rude because it effectively demands a binary verdict on a work in progress."Actually, One of Them Wrong  The New Republic's Jonathan Chait takes a hard line. "This is actually pretty simple: Guessers are wrong, and Askers are right. Asking is how you actually determine what the Asker wants and the giver is willing to receive. Guessing culture is a recipe for frustration. What's more, Guessers, who are usually trying to be nice and are holding themselves to a higher level of politeness, ruin things for the rest of us ... Guessers are what forces people with poor social discernment, like me, to regard all kinds of interactions as a minefield of awkwardness."It's Not So Black and White  The Incidental Economist's Austin Frakt endorses a more situationally fluid approach. "The problem with assuming one way is better than another is that it ignores the obvious temporal heterogeneity in preferences. The 'requester' (whether of Asker or Guesser type) is in more in need of a 'yes' (or 'no') response from the 'requestee' (again, of either type) at some times than others. Likewise, a requestee is more likely to say 'yes' (or 'no') at some times than at others ... Therefore, it is perfectly sensible to be an Asker for some things at some times and a Guesser for other things (or even the same things) at another."What say you--does the Asker/Guesser model ring true? (Or, put another way: We're not asking, but some people might want to leave comments, and perhaps you know someone who does...)This article is from the archive of our partner .]]></content:encoded></item><item><title>We will ban you and ridicule you in public if you waste our time on crap reports</title><link>https://curl.se/.well-known/security.txt</link><author>latexr</author><category>hn</category><pubDate>Thu, 22 Jan 2026 10:48:27 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ISO PDF spec is getting Brotli – ~20 % smaller documents with no quality loss</title><link>https://pdfa.org/want-to-make-your-pdfs-20-smaller-for-free/</link><author>whizzx</author><category>hn</category><pubDate>Thu, 22 Jan 2026 10:41:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[For nearly three decades; or November 1996 to be exact, PDFs have relied on Deflate—the same compression algorithm that powers your ZIP files. Meanwhile, the web moved on. In 2015, Google introduced Brotli, a compression algorithm so efficient it now powers 95% of internet traffic. Websites got faster. Downloads got smaller. CDNs got cheaper.Now PDFs are getting the same upgrade.The PDF Association is bringing this battle-tested web compression technology into the PDF specification itself. After a decade of Brotli proving its worth across billions of web requests daily, it's now getting ready to make it's introduction into ISO 32000.With iText, we can help drive widespread adoption with a production-ready Brotli encoder and decoder for the PDF ecosystem. The result?  with zero quality loss, using the same algorithm trusted by Google, Cloudflare, and every major CDN.PDF compression has been stuck in 1996 for a good reason: backward compatibility is sacred. The PDF Association operates under a strict principle—any new feature must work seamlessly with existing readers, or it risks fragmenting the ecosystem. Adding a new compression algorithm isn't just a technical change; it's a  that could render documents unreadable in older software. This creates a high barrier for innovation.Beyond compatibility concerns, there are other practical challenges. The PDF specification moves slowly by design—it's an ISO standard that requires consensus by hundreds of stakeholders. Compression algorithms must be  (ruling out patented options),  across platforms, and  in production.Finally, the ecosystem is conservative: enterprises and governments rely on PDFs for archival and legal documents that must remain accessible for decades, making any breaking change a risk that needs extraordinary justification.Encoding and decoding: Technical implementationTo get Brotli compression working within the iText SDK, we need to solve two problems: reading documents, and also writing them.Let's start with the easiest one; reading documents.Decoding: Advanced plumbing workFirst of all, let's look at how the content of a page is stored within a PDF. We can demonstrate this with just the classic "Hello World" text example.The following PDF syntax simply displays the text "Hello World!" on a page:5 0 obj                 % Unique identifier to reference this content from other places within the PDF
<</Length 49>>stream    % Meta data for the stream object. Here it contains a Length value to indicate how many bytes are there after the `stream` keyword.
q                       % the actual content
BT
/F1 12 Tf
37 788.33 Td
(Hello World!)Tj
ET
Q
endstream               % Indicates the end of the stream object
endobj                  % Indicates the end of the referenceable object
So, if we need to render or do anything else with the content, it would look like the following:|------------------------|
| Get stream based on id |
|------------------------|
           ||
           \/
|------------------------|
|      Read content      |
|------------------------|
           ||
           \/
|------------------------|
|    Render/Do stuff     |
|    with the content    |
|------------------------|
Okay, so now we have a high-level view how PDF processors handle the low-level processing of those stream objects, we can dive a little deeper!Let's take a look at the following PDF stream object where the content is encoded using the Deflate algorithm.5 0 obj
<</Filter/FlateDecode/Length 36>>stream                  % The meta data now now includes `Filter`
xœmÍÂ0„ïûëM/1?Æl®‚âUømI)Íûºm¢...            % Reduced for clarity
endstream
endobj
First of all, we notice there is an additional Key  with a value of  in the metadata.
This can be interpreted the following way: "The content of this stream object is only usable after its  filter is applied".So how does this change our working implementation?|------------------------|
| Get stream based on id |
|------------------------|
           ||
           \/
|------------------------|
|      Read content      |
|------------------------|
           ||
           \/
|------------------------|
|         Decode         |
|     based on Filter    |
|------------------------|
           ||
           \/
|------------------------|
|    Render/Do stuff     |
|    with the content    |
|------------------------|
We can now see we require an operation on the content before it's usable. The PDF specification already provides a variety of ways to write the content of the PDF streams.Decodes ASCII hexadecimal data to binary.Decodes ASCII base-85 data to binary.Decompresses data using LZW compression.Decompresses data using zlib/deflate compression.Decompresses data using run-length encoding.Decompresses CCITT fax-encoded monochrome images.Decompresses JBIG2-encoded monochrome image data.Decompresses JPEG DCT-based image data.Decompresses JPEG 2000 wavelet-based image data.Decrypts data encrypted by a security handler.So the idea for Brotli is to simply add another  implementation.What we need to get it working into iText is actually pretty minimal:Get the decoding implementation from Google's repository.Write some plumbing code to call it from iTextHook up the plumbing code to the  filterFor the first step we simply embedded Google's reference Java Brotli decoder straight from their official repository into our kernel module.By embedding Google's reference implementation directly, we guarantee:: No version conflicts with other libraries: Same decoder on all platforms: We control the code, even if upstream changesAutomatically generate C# version: Using our porting mechanism we can have a C# implementationThe plumbing implementation lives in , which plugs into iText's existing filter pipeline:public class BrotliFilter extends MemoryLimitsAwareFilter {
    @Override
    public byte[] decode(byte[] b, PdfName filterName, PdfObject decodeParams,
            PdfDictionary streamDictionary) {
        try {
            final byte[] buffer = new byte[DEFAULT_BUFFER_SIZE];
            final ByteArrayInputStream input = new ByteArrayInputStream(b);
            final ByteArrayOutputStream output = enableMemoryLimitsAwareHandler(streamDictionary);
            final BrotliInputStream brotliInput = new BrotliInputStream(input);
            int len;
            while ((len = brotliInput.read(buffer, 0, buffer.length)) > 0) {
                output.write(buffer, 0, len);
            }
            brotliInput.close();
            return output.toByteArray();
        } catch (IOException e) {
            throw new PdfException(KernelExceptionMessageConstant.FAILED_TO_DECODE_BROTLI_STREAM, e);
        }
    }
}
Let's break down what's happening in this implementation:: The filter extends , which protects against decompression
bombs—malicious PDFs that expand into gigabytes of data when decompressed. This is critical for production systems.: The compressed bytes  are wrapped in a , which is then passed to
Google's . This is where the magic happens— handles all the heavy lifting of Brotli decompression.As you can see, writing the plumbing code is pretty easy because of iText's architecture.The last thing to do is to ensure iText knows which implementation to associate with the  filter.This is also pretty trivial. The filter is registered automatically in  alongside  and the other standard PDF
filters:public final class FilterHandlers {
    private static final Map<PdfName, IFilterHandler> defaults;

    static {
        Map<PdfName, IFilterHandler> map = new HashMap<>();

        map.put(PdfName.FlateDecode, new FlateDecodeFilter());
        map.put(PdfName.Fl, new FlateDecodeFilter());
        //other implementations removed for clarity

        // we add our implementation
        map.put(PdfName.BrotliDecode, new BrotliFilter());

        defaults = Collections.unmodifiableMap(map);
    }
}
That's it. From this point on, any PDF with  streams just works. No configuration needed.Now we could have stopped here—our SDK could process Brotli-compressed PDFs from other sources. But reading isn't enough. To truly bring Brotli to the PDF ecosystem, we needed to let developers create these smaller files. That meant solving the encoding problem.And encoding turned out to be significantly more complex than decoding.Encoding: a separate module for compressionThe problem: iText’s compression was hardcodedBefore Brotli, iText only supported two compression modes for PDF streams:This logic was baked directly into the stream-writing code—there was no abstraction, no plugin point. If you wanted to use a different compression algorithm, you were out of luck.To support Brotli (and future algorithms), we needed to introduce a new abstraction layer: IStreamCompressionStrategy.public interface IStreamCompressionStrategy {
   /**
    * Gets the PDF filter name that identifies this compression algorithm.
    *
    * @return the PDF name representing the compression filter
    */
   PdfName getFilterName();

   /**
    * Gets the decode parameters required for decompressing the stream.
    * <p>
    * Decode parameters provide additional information needed to correctly
    * decompress the stream data.
    *
    * @return the decode parameters as a PDF object, or {@code null} if not needed
    */
   PdfObject getDecodeParams();

   /**
    * Creates a new output stream that wraps the original stream and applies compression.
    * @param original the original output stream to wrap
    * @param stream the PDF stream being compressed (may be used for context or configuration)
    *
    * @return a new output stream that performs compression
    */
    OutputStream createNewOutputStream(OutputStream original, PdfStream stream);
}
This interface decouples compression logic from iText's core PDF writing machinery. Now, instead of hardcoding Flate everywhere, we can inject different strategies at runtime. To inject the required strategy we make use of the . You can find more information about it here: Adding Dependency Injection to the PdfDocument class.From now on when iText needs to compress a stream, it asks the  in the : "Do you have an IStreamCompressionStrategy?": Use the registered strategy (Brotli in this case): Fall back to the default Flate compression: iText Core no longer cares about the algorithm used: You only pay the cost if you use it: New algorithms just implement the interfaceThe Second Problem: No Pure Java EncoderHere's where things got tricky. While Google's Brotli decoder has a pure Java implementation (which we embedded for reading), the official Brotli . To use it from Java, you need: to call native code from JavaPlatform-specific native libraries ( for Windows,  for Linux,  for macOS) to compile and ship these libraries for every platformFor a heavily-used library like iText, shipping native binaries is a non-starter:: Users need to manage native libraries across platforms: Native code introduces attack surfaces: We'd need to compile for Windows x64, Linux ARM, macOS Silicon, etc.: What if another library ships a different Brotli version?We needed a solution that handled this complexity  iText's core.That solution is a separate Maven module () that you add as an optional dependency. This
module contains:BrotliStreamCompressionStrategy: Implementation of IStreamCompressionStrategy: A third-party library that wraps Google's C++ encoder with JNIHere's what BrotliStreamCompressionStrategy looks like:public class BrotliStreamCompressionStrategy implements IStreamCompressionStrategy {

    @Override
    public OutputStream createNewOutputStream(OutputStream original, PdfStream stream) {
        int compressionLevel = convertCompressionLevel(stream.getCompressionLevel());
        Encoder.Parameters params = Encoder.Parameters.create(compressionLevel);
        try {
            return new BrotliOutputStream(original, params);
        } catch (IOException e) {
            throw new PdfException(KernelExceptionMessageConstant.CANNOT_WRITE_TO_PDF_STREAM, e);
        }
    }

    @Override
    public PdfName getFilterName() {
        return PdfName.BrotliDecode; // This goes into the /Filter entry
    }
}
The native wrapper: brotli4jInstead of writing JNI bindings ourselves, we rely on brotli4j—a mature, well-tested library that:Wraps Google's official C++ Brotli encoder/decoderShips pre-compiled native libraries for all major platforms (Windows x64/ARM, Linux x64/ARM, macOS Intel/Silicon)Automatically extracts the correct native library at runtime (no manual setup)Is actively maintained and widely used (powers projects like Netty, OkHttp)By delegating to brotli4j, we get production-grade native bindings without maintaining our own JNI layer.Why keep encoding separate?You might ask: "Why not bundle brotli4j in the kernel module like you did with the decoder?"Great question. Here's the reasoning:Encoder (separate module)Required to read Brotli PDFsPure Java (Google's decoder)Native code (brotli4j with JNI)Every user needs to read PDFsMost users stick with FlateBy keeping the encoder separate, we give users choice: add  if you need 20% smaller files, or stick with the default if native dependencies are a concern.Putting it all together: Full exampleHere's what it looks like to create a Brotli-compressed PDF:First of all add the required dependencies. Notice you have to add iText's artifactory because of the experimental nature of the code, and so users don't accidentally enable it.<repositories>
  <repository>
    <id>itext-releases</id>
    <name>iText Repository - releases</name>
    <url>https://repo.itextsupport.com/releases</url>
  </repository>
</repositories>

<dependency>
<groupId>com.itextpdf</groupId>
<artifactId>brotli-compressor</artifactId>
<version>{itext.version.bigger.then.9.5.0}</version>
</dependency>
public static void main() {

// 1. Register the compression strategy
   DocumentProperties properties = new DocumentProperties();
   properties.registerDependency(IStreamCompressionStrategy.class, new BrotliStreamCompressionStrategy());
// 2. Create your PDF as normal
   PdfWriter writer = new PdfWriter("output.pdf");
   PdfDocument pdf = new PdfDocument(writer, properties);

// Everything from here on uses Brotli automatically
   Document doc = new Document(pdf);
   doc.add(new Paragraph("This text will be Brotli-compressed!"));
   doc.add(new Image(ImageDataFactory.create("chart.png")));
   doc.close();

}
When you open  in a text editor, you'll see some entries looking like this:5 0 obj
<</Filter/BrotliDecode/Length 847>>stream
[binary Brotli-compressed data]
endstream
endobj
The PDF now uses  instead of , and the file is 15-25% smaller—with  to your document-building code.The catch: Compatibility isn't universal (yet)Here's the honest truth: Brotli-compressed PDFs won't open in Adobe Acrobat Reader today. They won't render in your browser's built-in PDF viewer. Most third-party PDF libraries will reject them outright.Why? Because  isn't part of the official PDF specification yet. The PDF Association is actively working on adding it to ISO 32000 (the PDF standard), but until that's finalized and implementations roll out, Brotli PDFs exist in a gray area.What about forward compatibility?Here's the good news: Brotli PDFs are future-proof. Once the PDF Association finalizes the spec and vendors implement it, your existing Brotli-compressed documents will just work. You're not creating broken files—you're creating files that are .Think of it like HTTP/2 in 2015. Early adopters who deployed it got immediate performance wins in their own
infrastructure, and as browsers caught up, those benefits became universal. Brotli PDFs follow the same pattern.We're not shipping this as a toy feature. We're working directly with the PDF Association to:Standardize the specification (syntax, decode parameters, dictionary support) across multiple platforms (Java, .NET, C++) to ensure interoperability when other vendors adopt it when the spec finalizes (we'll handle any breaking changes)By adopting Brotli compression now, you're not taking a risk—you're investing in a proven technology that's on a clear path to standardization.PDF compression hasn't evolved in 30 years—until now. Brotli represents the biggest leap in PDF storage efficiency since the format was invented, and iText is bringing it to production .Yes, there are compatibility limitations. Yes, it's experimental. But every standard starts this way. HTTP/2, WebP, and TLS 1.3 were all "experimental" once. Early adopters got the benefits first, then the ecosystem caught up.By using iText's Brotli implementation now, you're: by 15-25% immediatelyFuture-proofing your documents for inevitable standardization with real-world feedback for a more efficient PDF ecosystemThe PDF Association is listening. Adobe is watching. And iText is leading.Let's make PDFs smaller together. 🚀]]></content:encoded></item><item><title>The mushroom making people hallucinate tiny humans</title><link>https://www.bbc.com/future/article/20260121-the-mysterious-mushroom-that-makes-you-see-tiny-people</link><author>1659447091</author><category>hn</category><pubDate>Thu, 22 Jan 2026 10:30:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[It could also provide important clues about what causes spontaneous lilliputian hallucinations in people even when they're not consuming . The condition is rare, and as of 2021, only 226 non-mushroom-related cases had been reported since lilliputian hallucinations were first described in 1909. But for those relatively few people, the outcome can be serious: a third of those patients who came down with non-mushroom-related cases did not fully recover.Studying  could help scientists better understand the brain mechanisms behind these naturally occurring lilliputian visions, maybe even leading to new treatments for people who develop the neurological condition, Domnauer says."Now we may understand where in the brain [liliputian hallucinations] originate," says Dennis McKenna, an ethnopharmacologist and director of the McKenna Academy of Natural Philosophy, a non-profit education center in California, US. He agrees that understanding the mushroom's compounds could lead to new drug discoveries. "Is there a therapeutic application? It remains to be seen," says McKenna. Researchers estimate that less than 5% of the world's fungal species have been described, so the findings also highlight the "enormous potential" for discovery in the world's ever-dwindling ecosystems, says Furci, whose work focuses on exploring the fungal kingdom. "Fungi hold a very large biochemical and pharmacological library that we're only just beginning to tap into," says Furci. "There's still a world of discoveries to be made."]]></content:encoded></item><item><title>In Praise of APL (1977)</title><link>https://www.jsoftware.com/papers/perlis77.htm</link><author>tosh</author><category>hn</category><pubDate>Thu, 22 Jan 2026 08:44:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Professor Alan J. Perlis
Yale UniversityMany reasons can be given for teaching
one or more aspects of computer
science (defined as the study of the set
of phenomena arising around and because of
the computer) to all university students.
Probably every reader of this note supports
some of these reasons. Let me list
the few I find most important: (1) to
understand and to be able to compose
algorithms; (2) to understand how computers
are organized and constructed; (3) to
develop fluency in (at least) one programming
language; (4) to appreciate the
inevitability of controlling complexity
through the design of systems; (5) to
appreciate the devotion of computer
scientists to their subject and the
exterior consequences (to the student as
citizen) of the science’s development.Even though computer science deals
with symbolic objects whose nature we
study mathematically, it cannot be taught
as an orderly development arising from a
few fundamental ideas whose existence the
student has already observed intuitively
during his maturation, such as gravitation
and electricity.It is during this first computer
course that the student awakes to the
possibilities and consequences of computation.
They arise most usefully and in
greatest profusion during his writing of
programs. He must program and program and
program! He must learn how to state
precisely in a programming language what
he perceives about the nature of symbolic
systems. I know of no better way to
expedite this awakening than by programming.But what should the student program?
and in what language? I do not place much
emphasis on heavy  of other people’s
programs, packages if you will, that
perform real services such as statistical
packages, data management systems, linear
equations solvers, etc. While it is wise
to use standard programs when they match
one’s needs, it is more important to
master self-expression during this initial
contact.Available time is a limiting factor;
a semester provides about 16 weeks of
contact. During that interval the student
must negotiate a set of tasks that sharpens
his abilities and explodes his perceptions
of the computer’s capabilities. He
must be on the computer early and often
during the semester and his approach to it
must be smooth and easy. The computer
system he uses should be time-sharing and
interactive, if you will.Learning to program involves a
sequence of acts of discovery punctuated
by recovery from errors. As the semester
progresses the causes and nature of errors
will change. Certain kinds will diminish
and even disappear, only to be replaced by
errors of deeper significance, harder to
isolate and more resistant to satisfactory
removal — syntactic gaffes give way to
semantic errors — incorrect perceptions
of purpose, improper use of means, the use
of hammers to swat flies and swatters to
level mountains.To write correct and balanced programs
a student may be forced to move
between programs that are not related to
each other by a few simple textual rearrangements.
He must learn to write and
test complicated programs quite rapidly.
As he moves through the sequence of
assigned tasks his ability to express
himself fluently should not founder too
soon because of language shortcomings.
For all of the above reasons as well as a
few others, I have come to believe that
APL is the most rational first language
for a first course in computer science.It is true that BASIC and FORTRAN are
easier to learn than APL, for example, a
week versus a month. However, once
mastered, APL fits the above requirements
much better than either BASIC or FORTRAN
or their successors ALGOL 60, PL/I and
Pascal. The syntax of APL is not a
significant difficulty for the students.
The large number of primitive functions,
at first mind-numbing in their capabilities,
quickly turn out to be easily
mastered, soon almost all are used naturally
in every program — the primitive
functions form a harmonious and useful
set. As a data organization, arrays turn
out to be extraordinarily useful (though
prolonged contact with APL makes one wish
for the added presence of more heterogeneous
structures).The virtues of APL that strike the
programmer most sharply are its 
— complicated acts can be described
briefly, its  — there are a
large number of ways to state even moderately
complicated tasks (the language
provides choices that match divergent
views of algorithm construction), and its
 — there is the possibility
to construct sentences — one-liners as
they are commonly called — that approach
in the flow of phrase organization,
sequencing and imbedding, the artistic
possibilities achievable in natural
language prose.The sweep of the eye across a single
sentence can expose an intricate, ingenious
and beautiful interplay of operation
and control that in other programming
languages is observable only in several
pages of text. One begins to appreciate
the emergence and significance of style
and to observe that reading and writing
facility is tied to the development of an
arsenal of idioms which soon become
engraved in one’s skull as units.The combination of these three
factors makes it possible to develop an
excellent set of exercises in the first
course. These exercises can be large in
number, cover a wide range of topics and
vary widely in complexity, and still be
done during the 16 week period. The later
exercises can be tied to the design and
development of a system — a collection of
procedures that, in varying combinations,
perform a set of tasks.In Teaching Computer OrganizationTo appreciate computer science one
requires an understanding of the computer.
Once the student understands the computer
— its macroscopic components monitored by
the fetch-execute cycle and its apparent
complexity being controlled by gigantic
replication of a few simple logical
elements — he can become aware of the
important equilibrium between hardware and
software — the shifting of function
between the two as determined by economic
factors — and between algorithm and
system as determined by traffic and
variation. Using APL it is straightforward
to model a computer and to illustrate
any of its macroscopic or microscopic
components at any level of detail. The
programs to perform these functions at
every level of description remain small
and manageable — about 40 lines or so.The development of software, e.g., a
machine language assembler, is a task of
similar difficulty (about 40 lines) and
hence possible within the confines of a
first course.Word processing and graphics,
increasingly important application areas
of computers, can be explored with exercises
of no great size, e.g., to do
permuted-index of title lists (~12 lines),
display, rotation and scaling of composites
of polygons (~20 lines), graphing of
functions (~5 lines), etc.With (or even without) the use of 2
or 3 pre-built functions, file processing
problems such as payroll, personnel
search, etc. can be written in a relatively
few lines.An important consequence of the
attainable brevity of these compositions
cannot be ignored: the student becomes
aware that he need not be forced to depend
upon external, pre-packaged and elaborate
systems to do what are really simple
programming tasks. Instead of learning a
new coding etiquette to negotiate a
complex external system, he writes his own
programs, develops his own systems tailor-made
to his own needs and understood at
all levels of detail by him. If anything
is meant by man-machine symbiosis, it is
the existence of such abilities on the man
side of the “membrane”, for there is no
partnership here between man and machine,
merely the existence of a growing, but
never perfectly organized, inventory of
tools that the competent can pick among,
adapt and use to multiply his effective
use of the computer.I cannot overemphasize the importance
of terseness, flexibility and phrase
growth to a beginning student. His
horizons of performance are being set in
this first course. If he sees a task as a
mountain then its reduction to molehill
proportions is itself a considerable
algorithmic task. While this is true of
very large tasks, even when using APL this
conscious chaining of organized reductions
can be postponed until the student has
already collected a large number of useful
data-processing functions, engraved in his
skull, with which to level mountains.It is important to recognize that no
matter how complicated the task, the APL
functions will usually be anywhere from
1/5 to 1/10 the number of statements or
lines, or what have you, of a FBAPP
(FORTRAN or BASIC or ALGOL or PL/I or
Pascal) program. Since APL distributes,
through its primitive functions, control
that the other languages explicate via
sequences of statements dominated by
explicit control statements, errors in APL
programs tend to be far fewer in number
than in their correspondents in FBAPP.I can come now to the topics of
structured programming and program verification.
Both are important, but their
content and importance depend strongly on
the language in which programs are
couched. A program is well-structured if
it has a high degree of lexical continuity:
small changes in program capability
are acquired by making changes within
lexically close text (modularization).Since APL has a greater density of
function within a given lexical scope than
FBAPP, one would expect that APL programs
will support considerably more structure
than equivalent size FBAPP programs. Put
another way, since the APL programs are
1/5 to 1/10 the size of FBAPP programs,
the consequences to APL programs of weak
structuring are less disastrous. Recovery
from design mistakes is more rapid. Since
we can only structure what we already
understand, the delay in arriving at
stable program organization should be
considerably less with FBAPP!Please note that the emphasis here is
on the control of propagation of relationships,
not the nonsense of restricting 
or bathing programs in cascades of
 loops.The verification, formal or informal,
of programs is a natural and important
activity. It is linked to specification:
what we can’t specify we can’t verify. By
specification we mean stating what is to
be output for a given input. We immediately
observe that, since specification in
FBAPP is extremely tedious and unnatural,
we must use some other language. APL
turns out to be quite good and has often
been suggested as a specification language.
Assertions and verification
conditions can be much more easily
expressed as APL predicates than as FBAPP
predicates. Because of the widespread
distribution of control into the semantics
of primitive functions, for which no proof
steps need then be given, APL verifications
tend to be, just as their counterpart
APL programs, shorter and more
analytic than equivalent FBAPP program
verifications.The form of the FBAPP languages
follows closely the structure of the
computers that prevailed during their
inception. They have the nice property
that one may often optimize machine
performance of their compiled programs by
transforming FBAPP programs to other FBAPP
programs. Control of the computer is more
easily exercised with programs in these
languages than with APL, since the latter
is more independent of current machines.
For many programs this control over the
target machine performance is quite vital,
and APL couples more weakly to the standard
computer than does FBAPP.However, new array processing computers
are beginning to appear and, had they
been standard 20 years ago, APL and not
FORTRAN would have been the prototype of
language development. I often wonder at
what descriptive levels we would be
programming today had that been the case!
Since it was not the case, we should not
throw out or limit APL. We must seek ways
to match it to the common computer. We
must design compilers as well as computers
that fit APL better.More Cost-Effective than BASICCost is an important issue in the
instructional process. An APL computer
system currently costs about $10K per
terminal, about twice the cost of a BASIC
system. As APL system designs stabilize
and integrated circuitry costs drop, the
two figures will coincide at or near the
cost of a contemporary terminal. However,
even now the APL system is cheaper than
BASIC systems for equivalent work loads
because one can do more than twice as much
with APL in a given period of time than
with BASIC!Let me mention in closing two additional
issues regarding the use of APL in
an introductory computer science course.
First, most university computer scientists
don’t really  APL. They haven’t
appreciated what it means to think in APL
— to think about parallel operations in
arrays and to distribute and submerge
explicit looping among its primitive
functions. I am reminded of the difficulties
many math departments experience when
they try to replace calculus by a fine
math and combinatorics course as the first
meat and potatoes offering by the department
to the university. However at Yale
we have found that faculty outside the
software milieu — in theory, for example
— pick up APL quite fast and prefer it to
FBAPP. I am sure the same is true elsewhere.The second issue is of a different
kind. I am firmly convinced that APL and
LISP are related to each other along an
important axis of language design and that
acquiring simultaneous expertise in both
languages is possible and desirable for
the beginning student. Were they unified,
the set of tasks that succumb to terse,
flexible and expressive descriptions will
enlarge enormously without overly increasing
the intellectual burden on the student
over his initial 16 week contact period.Above all, remember what we must
provide is a  to last the student
for 40 years, not a handbook for tomorrow’s
employment.]]></content:encoded></item><item><title>30 Years of ReactOS</title><link>https://reactos.org/blogs/30yrs-of-ros/</link><author>Mark_Jansen</author><category>hn</category><pubDate>Thu, 22 Jan 2026 08:03:57 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Happy Birthday ReactOS! Today marks 30 years since the first commit to the ReactOS source tree.
It’s been such a long journey that many of our contributors today, including myself, were not alive during this event.
Yet our mission to deliver “your favorite Windows apps and drivers in an open-source environment you can trust” continues to bring people together.
Let’s take a brief look at some of the high and low points throughout our history.1996-2003: The Painful Road to ReactOS 0.1.0ReactOS started from the ashes of the FreeWin95 project, which aimed to provide a free and open-source clone of Windows 95.
FreeWin95 suffered from analysis paralysis, attempting to plan the whole system before writing any code.
Tired of the lack of progress on the project, Jason Filby took the reins as project coordinator and led a new effort targeting Windows NT.
The project was renamed to “ReactOS” as it was a reaction to Microsoft’s monopolistic position in home computer operating systems.Progress on ReactOS was very slow at first.
Contributors had to first build a very basic NT-like kernel before they could develop drivers for it, then continue developing the kernel; not too dissimilar to the process of bootstrapping a new programming language.
Once a few basic drivers were written, other contributors were able to learn from these examples and develop other drivers.While writing this article, I reached out to Eric Kohl. He developed the original storage driver stack for ReactOS (atapi, scsiport, class2, disk, cdrom, cdfs) and has been with the project since 1998. I asked him about his experiences with ReactOS during this time, how he found the project, and what contributing to ReactOS was like during those early days. He wrote:I think I found ReactOS while searching for example code for my contributions to the WINE project.
I subscribed to the mailing list and followed the discussions for a few days.
The developers were discussing the future of shell.exe, a little command line interpreter that could only change drives and directories and execute programs.
A few days [later] I had started to convert the FreeDOS command.com into a Win32 console application, because I wanted to extend it to make it 4DOS compatible.
4DOS was a very powerful command line interpreter.
On December 4th, 1998 I introduced myself and suggested to use my converted FreeDOS command.com as the future ReactOS cmd.exe.
I had a little conversation with Jason Filby and Rex Joliff, the CVS repository maintainer.
I sent my cmd.exe code to Rex and he applied it to the repository.
After applying a few more cmd-related patches over the next weeks, Rex asked me whether I would like to have write-access to the repository.
I accepted the offer…The first version I downloaded and used was 0.0.8.
It was not much more than a DOS-based bootloader, some drivers, and a basic kernel that ran a few test routines after initialization.Version 0.0.8 didn’t use PE files, but flat (position independent) binaries.
There was no PE loader,  no smss, no csrss, no winlogon, no process heaps, no process environments, no threads, etc.
Each and every little feature was a milestone.Initially there was not a review process at all.
You write some code, test it and fix it until it works.
Then you commit it.
If something failed on another machine, you got a reply on the mailing list and discussed a solution.
You fixed the issue and committed a fix.
That’s how it worked.There was always an open and friendly atmosphere.
It was and still is always nice to talk to other developers.
No fights, no wars, like in some other projects.Editors note: minor errors were corrected.ReactOS 0.1.0 was released on February 1st, 2003 and received minor updates up until November 2003.
ReactOS 0.1.0 was the first version of ReactOS that could boot from a CD.
It had a command line interface and no desktop.
Watch a demo of it below, provided courtesy of archeYR.During this period ReactOS saw rapid development.
New drivers were being built all the time, a basic desktop was built, and ReactOS became increasingly stable and usable.
Public interest grew as ReactOS matured.
In October 2005, Jason Filby stepped down as project coordinator, and Steven Edwards was voted to be the next project coordinator.ReactOS 0.2.x boot screenReactOS 0.2.x desktop and file explorerReactOS 0.2.0 with VMware video driver for NT 4It wasn’t all sunshine and rainbows though.
In January 2006, concerns grew about contributors having access to leaked Windows source code and possibly using this leaked source code in their contributions.
In response, Steven Edwards strengthened the project’s intellectual property policy and the project made the difficult decision to audit the existing source code and temporarily freeze contributions.The ongoing audit and contribution freeze from the end of the ReactOS 0.2.x era slowed development and momentum considerably for ReactOS 0.3.x.
Following challenges with the audit, Steven Edwards stepped down as project coordinator and Aleksey Bragin assumed the role by August 2006.Despite the challenges during this time, ReactOS 0.3.x continued to build upon ReactOS’s legacy.
ReactOS 0.3.0 was released on August 28th, 2006.
It introduced networking support and a package manager called “Download!”.
This package manager would become the basis for RAPPS, the package manager built into modern versions of ReactOS.
In July 2008, the x86_64 port of ReactOS was started.
One year later, ReactOS 0.3.10 imported the UniATA driver, written by Alexandr Telyatnikov (Alter).
While we run into limitations with the UniATA driver today, UniATA enabled ReactOS to support SATA storage devices and to support partitions greater than 8GB in size.
On February 8th, 2012, ReactOS 0.3.14 supported being built using the MSVC compiler and added visual style support.Download!, the package manager for ReactOS 0.3.x2016-Today: ReactOS 0.4.xReactOS 0.4.0 was released on February 16th, 2016.
It introduced a new graphical shell that utilized more Windows features and was more similar architecturally to Windows Explorer.
ReactOS 0.4.0 also introduced support for kernel debugging using WinDbg when compiled with MSVC.
Being able to use standard Windows tools for kernel debugging has helped us progress considerably.
ReactOS 0.4.0 continued to receive incremental updates every few months up until versions 0.4.14 and 0.4.15 which had years of development updates each.
Today, the x86_64 port of ReactOS is similarly functional to its x86 counterpart, but with no WoW64 subsystem to run x86 apps its usability is limited.A humorous diagram made in 2015 to explain the complexity of Windows ExplorerReactOS 0.4.15 desktop, shown with Luna visual style and large taskbar icons appliedWe’re continuing to move ReactOS forward. Behind the scenes there are several out-of-tree projects in development. Some of these exciting projects include a new build environment for developers (RosBE), a new NTFS driver, a new ATA driver, multi-processor (SMP) support, support for class 3 UEFI systems, kernel and usermode address space layout randomization (ASLR), and support for modern GPU drivers built on WDDM.The future of ReactOS will be written by the people who believe in the mission and are willing to help carry it forward.Note: Statistics were calculated at commit f60b1c9Total unique contributors: 301Total lines of code: 14,929,578]]></content:encoded></item><item><title>Doctors in Brazil using tilapia fish skin to treat burn victims (2017)</title><link>https://www.pbs.org/newshour/health/brazilian-city-uses-tilapia-fish-skin-treat-burn-victims</link><author>kaycebasques</author><category>hn</category><pubDate>Thu, 22 Jan 2026 05:15:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[FORTAZELA, Brazil — In this historic city by the sea in northeast Brazil, burn patients look as if they've emerged from the waves. They are covered in fish skin — specifically strips of sterilized tilapia.Doctors here are testing the skin of the popular fish as a bandage for second- and third-degree burns. The innovation arose from an unmet need. Animal skin has long been used in the treatment of burns in developed countries. But Brazil lacks the human skin, pig skin, and artificial alternatives that are widely available in the US.The three functional skin banks in Brazil can meet only 1 percent of the national demand, said Dr. Edmar Maciel, a plastic surgeon and burn specialist leading the clinical trials with tilapia skin.As a result, public health patients in Brazil are normally bandaged with gauze and silver sulfadiazine cream."It's a burn cream because there's silver in it, so it prevents the burns from being infected," said Dr. Jeanne Lee, interim burn director at the the regional burn center at the University of California at San Diego. "But it doesn't help in terms of debriding a burn or necessarily helping it heal."The gauze-and-cream dressing must be changed every day, a painful process. In the burn unit at Fortaleza's José Frota Institute, patients contort as their wounds are unwrapped and washed.Enter the humble tilapia, a fish that's widely farmed in Brazil and whose skin, until now, was considered trash. Unlike the gauze bandages, the sterilized tilapia skin goes on and stays on.The first step in the research process was to analyze the fish skin."We got a great surprise when we saw that the amount of collagen proteins, types 1 and 3, which are very important for scarring, exist in large quantities in tilapia skin, even more than in human skin and other skins," Maciel said. "Another factor we discovered is that the amount of tension, of resistance in tilapia skin is much greater than in human skin. Also the amount of moisture."In patients with superficial second-degree burns, the doctors apply the fish skin and leave it until the patient scars naturally. For deep second-degree burns, the tilapia bandages must be changed a few times over several weeks of treatment, but still far less often than the gauze with cream. The tilapia treatment also cuts down healing time by up to several days and reduces the use of pain medication, Maciel said.Antônio dos Santos, a fisherman, was offered the tilapia treatment as part of a clinical trial after he sustained burns to his entire right arm when a gas canister on his boat exploded. He accepted."After they put on the tilapia skin, it really relieved the pain," he said. "I thought it was really interesting that something like this could work."The initial batches of tilapia skin were studied and prepared by a team of researchers at the Federal University of Ceará. Lab technicians used various sterilizing agents, then sent the skins for radiation in São Paulo to kill viruses, before packaging and refrigerating the skins. Once cleaned and treated, they can last for up to two years.In the US, animal-based skin substitutes require levels of scrutiny from the Food and Drug Administration and animal rights groups that can drive up costs, Lee said. Given the substantial supply of donated human skin, tilapia skin is unlikely to arrive at American hospitals anytime soon.But it may be a boon in developing countries."I'm willing to use anything that might actually help a patient," Lee said. "It may be a good option depending on what country you're talking about. But I also think the problem is that you need to find places that have the resources to actually process the skin and sterilize it, and make sure it doesn't have diseases."In Brazil, in addition to the clinical trials, researchers are currently conducting histological studies that compare the composition of human, tilapia, pig, and frog skins. They are also conducting studies on the comparative costs of tilapia skin and conventional burn treatments. If clinical trials show continued success, doctors hope a company will process the skins on an industrial scale and sell it to the public health system.This article is reproduced with permission from STAT. It was first published on Mar. 2, 2017. Find the original story here.
                    A free press is a cornerstone of a healthy democracy. 
                
                    Support trusted journalism and civil dialogue. 
                ]]></content:encoded></item><item><title>Significant US farm losses persist, despite federal assistance</title><link>https://www.fb.org/market-intel/significant-farm-losses-persist-despite-federal-assistance</link><author>toomuchtodo</author><category>hn</category><pubDate>Thu, 22 Jan 2026 01:11:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Per-acre  for all nine principal row crops are projected to rise again in 202, continuing a troubling trend that began after 2021.Inflated operating costs  the primary drivers of higher breakeven prices, with limited relief expected in the near term.Recent programs have portion of losses, but do not fully close the gap between costs and market returns, leaving many farmers potentially operating below breakeven for another year.crop growers face similar issues as row crop farmers,but limited data makes per-acre loss estimates challenging.The USDA-Economic Research Service (ERS) December update to Commodity Costs and Returns provides a comprehensive look at per-acre production costs for the nine principal row crops: corn, soybeans, wheat, cotton, rice, barley, oats, peanuts and sorghum. At a high level, ERS projects average total costs per acre to increase for every crop in 2026, underscoring the persistence of elevated production expenses across U.S. agriculture. When operating expenses and farm-wide costs like equipment, land and management are combined, costs vary widely by crop. In 2025, forecasted total per-acre costs are $1,308 for rice, $1,166 for peanuts, $943 for cotton, $890 for corn, $658 for soybeans, $498 for oats, $491 for barley, $443 for sorghum, and $396 for wheat. Looking ahead, ERS projections for 2026 suggest continued upward pressure across most cost categories, with total cost increasing anywhere from 2.2% to 3.3%. Amongst the nine principal crops, wheat ($409 per acre), sorghum ($458) and oats ($513) remain at the lower end of the production cost spectrum, while soybeans ($678) and barley ($507) fall in the mid-range in 2026. Cotton ($965), peanuts ($1,194) and rice ($1,336) remain the most expensive crops to produce on a per-acre basis. Operating costs—expenses directly tied to producing a yearly crop, such as seed, fertilizer, chemicals, fuel and labor—substantially vary across crops. In 2025, total operating costs ranged from $155 per acre for wheat to more than $764 per acre for rice and $631 per acre for peanuts. In 2026, these costs are expected to rise, ranging from $774 per acre for rice and $160 per acre for wheat. While select inputs have moderated slightly from recent peaks, overall operating expenses remain well above pre-2021 levels. Rising costs since 2020 have been driven primarily by sharp increases in interest expenses (+71%), fertilizer (+37%), fuel and oil (+32%), labor (+47%), chemicals (+25%) and maintenance (+27%), alongside notable gains in seed (+18%) and marketing costs (+18%). Losses Persist Even After FBA and ECAP Against this backdrop of elevated costs, commodity prices have remained under pressure, limiting farmers’ ability  to cover  their costs through the marketplace alone. As a result, many farms are projected to experience losses for a fourth or fifth consecutive year, even after accounting for crop insurance indemnities and ad hoc assistance. The  Farmer Bridge Assistance (FBA) Program and the Emergency Commodity Assistance Program (ECAP) provide important near-term support. However, ECAP was designed to address 2023 and 2024 losses, rather than 2025 and later production challenges. For both programs, payments are calculated on a per-acre basis. However, when compared to current per-acre production costs and weak commodity prices, these payments generally cover only a share of losses rather than restore profitability. In fact, returns over total costs for all nine principal row crops are projected to remain negative on a per-acre basis even after accounting for federal assistance. Based on loss calculations used in the Farmer Bridge Assistance Program, rice producers face losses of roughly $210 per acre, followed by cotton ($202), oats ($159), peanuts ($131), sorghum ($91), corn ($87), wheat ($70), soybeans ($61) and barley ($42). In total, net losses across the sector are estimated to exceed $50 billion over the past three crop years.For many farms, aid helps slow the erosion of working capital but does not fully offset negative margins. As a result, producers continue to absorb multiyear losses that strain balance sheets, tighten cash flow and complicate access to operating credit. These loss estimates reflect national averages; actual costs of production and returns vary by region, management decisions and ownership structure. For example, producers who own their farmland may face lower total costs by avoiding cash rental expenses, resulting in higher returns.Additionally, neither the FBA program nor the ECAP address losses in the specialty crops market. The 2024 Marketing Assistance for Specialty Crop Program (MASC) provided a first but limited relief step for growers and, for many, represented some of the first federal assistance tied to market challenges in the sector. Specialty crop growers continue to face deep and persistent economic losses driven by rising input costs, tightening margins, weather and disease disruptions, labor expenses and constraints, and global trade instability — challenges shared by field crop agriculture, including producers of crops beyond the nine principal crops, such as alfalfa and sugar beets. Strengthening support for all sectors of agriculture is an economic necessity. Doing so will help maintain a resilient, accessible and diverse U.S. food system. ERS cost projections make clear that input costs for all of the nine principal row crops remain elevated and sticky. Continued increases in both operating and overhead expenses are pushing breakeven prices higher, while commodity prices remain insufficient to offset those costs for many producers. While FBA and ECAP payments are an important and welcome step in addressing near-term financial stress, they do not fully close the gap between costs and returns. As farmers enter the 2026/27 marketing year, accumulated losses — estimated to exceed $50 billion across the sector over the past three crop years — continue to weigh on farm finances. These estimates reflect national average conditions and are calculated ahead of the growing season, before producers make final planting, input and marketing decisions. In practice, farmers respond to market signals by adjusting crop mix, input use and risk management strategies as conditions evolve. While outcomes vary widely by region and operation, persistently elevated breakeven prices underscore the importance of market-driven solutions that strengthen domestic demand — such as year-round access to E15 — to help support commodity prices and improve farm margins. Much-needed safety net enhancements through the One Big Beautiful Bill Act (OBBBA) are expected to take effect in October 2026, but those changes do not address the pressures farmers face today. In a recent letter to Congress organized by the American Farm Bureau Federation and signed by 56 agricultural organizations, farm groups warned of an economic crisis in rural America, citing multiyear losses driven by record-high input costs and historically low commodity prices. Congressional leaders from both parties have acknowledged the severity of these losses and the need for additional aid to stabilize farm finances. Until longer-term policy improvements take hold, many operations remain caught between high operating costs and low commodity prices, underscoring the ongoing financial strain facing U.S. agriculture as producers weigh whether they can afford to plant another crop. ]]></content:encoded></item><item><title>Internet voting is insecure and should not be used in public elections</title><link>https://blog.citp.princeton.edu/2026/01/16/internet-voting-is-insecure-and-should-not-be-used-in-public-elections/</link><author>WaitWaitWha</author><category>hn</category><pubDate>Thu, 22 Jan 2026 01:11:13 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Signed by a group of 21 computer scientists expert in election securityScientists have understood for many years that internet voting is insecure and that there is no known or foreseeable technology that can make it secure. Still, vendors of internet voting keep claiming that, somehow, their new system is different, or the insecurity doesn’t matter. Bradley Tusk and his Mobile Voting Foundation keep touting internet voting to journalists and election administrators; this whole effort is misleading and dangerous. All internet voting systems are insecure. The insecurity is worse than a well-run conventional paper ballot system, because a very small number of people may have the power to change any (or all) votes that go through the system, without detection. This insecurity has been known for years; every internet voting system yet proposed suffers from it, for basic reasons that cannot be fixed with existing technology.Internet voting systems known as “End-to-End Verifiable Internet Voting” are also insecure, in their own special ways.  Recently, Tusk announced an E2E-VIV system called “VoteSecure.”  It suffers from all the same insecurities.  Even its developers admit that in their development documents.  Furthermore, VoteSecure isn’t a complete, usable product, it’s just a “cryptographic core” that someone might someday incorporate into a usable product. Recent announcements by Bradley Tusks’s Mobile Voting Foundation suggest that the development of VoteSecure somehow makes internet voting safe and appropriate for use in public elections.  This is untrue and dangerous.  All deployed Internet voting systems are unsafe, VoteSecure is unsafe and isn’t even a deployed voting  system, and there is no known (or foreseeable) technology that can make Internet voting safe.Part I.  All internet voting systems are insecureInternet voting systems (including vote-by-smartphone) have three very serious weaknesses:Malware on the voter’s phone (or computer) can transmit different votes than the voter selected and reviewed. Voters use a variety of devices (Android, iPhone, Windows, Mac) which are constantly being attacked by malware.Malware (or insiders) at the server can change votes. Internet servers are constantly being hacked from all over the world, often with serious results.Malware at the county election office can change votes (in those systems where the internet ballots are printed in the county office for scanning). County election computers are not more secure than other government or commercial servers, which are regularly hacked with disastrous results. Although conventional ballots (marked on paper with a pen) are not perfectly secure either, the problem with internet ballots is the ability for a single attacker (from anywhere in the world) to alter a very large number of ballots with a single scaled-up attack.  That’s much harder to do with hand-marked paper ballots; occasionally people try large-scale absentee ballot fraud, typically resulting in their being caught, prosecuted, and convicted.Part II.  E2E-VIV internet voting systems are also insecureYears ago, the concept of “End-to-End Verifiable Internet Voting” (E2E-VIV) was proposed, which was supposed to remedy some of these weaknesses by allowing voters to check that their vote was recorded and counted correctly.  Unfortunately, all E2E-VIV systems suffer from one or more of the following weaknesses:Voters must rely on a computer app to do the checking, and the checking app (if infected by malware) could lie to them.Voters should not be able to prove to anyone else how they voted – the technical term is “receipt-free” – otherwise an attacker could build an automated system of mass vote-buying via the internet. But receipt-free E2E-VIV systems are complicated and counterintuitive for people to use.It’s difficult to make an E2E-VIV checking app that’s both trustworthy and receipt-free. The best solutions known allow checking only of votes that will be discarded, and casting of votes that haven’t been checked; this is highly counterintuitive for most voters! The checking app must be separate from the voting app, otherwise it doesn’t add any malware-resistance at all.  But human nature being what it is, only a tiny fraction of voters will do the extra steps to run the checking protocol.  If hardly anyone uses the checker, then the checker is largely ineffective.Even if some voters do run the checking app, if those voters detect that the system is cheating (which is the purpose of the checking app), there’s no way the voters can prove that to election officials.  That is, there is no “dispute resolution” protocol that could effectively work.Thus, the problem with all known E2E-VIV systems proposed to date is that the “verification” part doesn’t add any useful security: if a few percent of voters use the checking protocol and see that the system is sometimes cheating, the system can still steal the votes of all the voters that don’t use the checking protocol. And you might think, “well, if some voters catch the system cheating, then election administrators can take appropriate action”, but no appropriate action is possible: the election administrator can’t cancel the election just because a few voters claim (without proof) that the system is cheating!  That’s what it means to have no dispute resolution protocol.Part III. VoteSecure is insecureIt has been the scientific consensus for decades that internet voting is not securable by any known technology. Research on future technologies is certainly worth doing. However, the decades of work on E2E-VIV systems has yet to produce any solution, or even any hope of a solution, to the fundamental problems.Therefore, when it comes to internet voting systems, election officials and journalists should be especially wary of “science by press release.” Perhaps some day an internet voting solution will be proposed that can stand up to scientific investigation. The most reliable venue for assessing that is in peer-reviewed scientific articles. Reputable cybersecurity conferences and journals have published a lot of good science in this area. Press releases are not a reliable way to assess the trustworthiness of election systems.(affiliations for for identification only and do not indicate institutional endorsement), Eugene Higgins Professor Emeritus of Computer Science, Princeton University, Percy K. and Vida L.W. Hudson Professor Emeritus of Computer Science, Columbia University, Chair Emeritus — NCR Chair in Computer Science and Engineering, University of South Carolina, PhD Student, Univ. of Michigan School of Engineering & Knight-Hennessy Scholar, Stanford Law, Charlotte B and Roger C  Warren Chair in Computing, Georgia Tech , Donald E. Knuth Professor, Emeritus, in the School of Engineering, Stanford UniversityNational Science Foundation (retired) and Georgia Institute of Technology,  Andrew Banks Family Preeminence Endowed Professor, Computer & Information Science, University of Florida, Bredt Family Professor of Computer Science & Engineering, University of Michigan, Lawrence Livermore National Laboratory (retired), Emeritus Associate Professor of Computer Science, University of Iowa, Professor of Computer Science and Engineering, Lehigh University, , Fellow and Lecturer at the Harvard Kennedy School, and at the Munk School at the University of Toronto, President and Chief Technologist, Citizens for Better Elections, , Assistant Professor, Georgia Tech,  Distinguished Professor,  Department of Statistics, University of California, Professor of Computer Science & Engineering, The Pennsylvania State University, Thinking Cybersecurity Pty Ltd and the Australian National University, Professor of Computer Science, George Washington University]]></content:encoded></item><item><title>Threat actors expand abuse of Microsoft Visual Studio Code</title><link>https://www.jamf.com/blog/threat-actors-expand-abuse-of-visual-studio-code/</link><author>vinnyglennon</author><category>hn</category><pubDate>Thu, 22 Jan 2026 00:12:00 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Jamf Threat Labs identifies additional abuse of Visual Studio Code. See the latest evolution in the Contagious Interview campaign.At the end of last year, Jamf Threat Labs published research related to the Contagious Interview campaign, which has been attributed to a threat actor operating on behalf of North Korea (DPRK). Around the same time, researchers from OpenSourceMalware (OSM) released additional findings that highlighted an evolution in the techniques used during earlier stages of the campaign.Specifically, these newer observations highlight an additional delivery technique alongside the previously documented ClickFix-based techniques. In these cases, the infection chain abuses Microsoft Visual Studio Code task configuration files, allowing malicious payloads to be executed on the victim system.Following the discovery of this technique, both Jamf Threat Labs and OSM continued to closely monitor activity associated with the campaign. In December, Jamf Threat Labs identified additional abuse of Visual Studio Code  configuration files. This included the introduction of dictionary files containing heavily obfuscated JavaScript, which is executed when a victim opens a malicious repository in Visual Studio Code.Jamf Threat Labs shared these findings with OSM, who subsequently published a more in-depth technical analysis of the obfuscated JavaScript and its execution flow.Earlier this week, Jamf Threat Labs identified another evolution in the campaign, uncovering a previously undocumented infection method. This activity involved the deployment of a backdoor implant that provides remote code execution capabilities on the victim system.At a high level, the chain of events for the malware look like so:Throughout this blog post we will shed light on each of these steps.In this campaign, infection begins when a victim clones and opens a malicious Git repository, often under the pretext of a recruitment process or technical assignment. The repositories identified in this activity are hosted on either GitHub or GitLab and are opened using Visual Studio Code.When the project is opened, Visual Studio Code prompts the user to trust the repository author. If that trust is granted, the application automatically processes the repository’s configuration file, which can result in embedded arbitrary commands being executed on the system.On macOS systems, this results in the execution of a background shell command that uses  in combination with  to retrieve a JavaScript payload remotely and pipe it directly into the Node.js runtime. This allows execution to continue independently if the Visual Studio Code process is terminated, while suppressing all command output.In observed cases, the JavaScript payload is hosted on , a platform that has been increasingly used in recent DPRK-related activity following a move away from other hosting services, as previously documented by OpenSourceMalware.Jamf Threat Labs reported the identified malicious repository to GitHub, after which the repository was removed. While monitoring the activity prior to takedown, we observed the URL referenced within the repository change on multiple occasions. Notably, one of these changes occurred after the previously referenced payload hosting infrastructure was taken down by Vercel.Once execution begins, the JavaScript payload implements the core backdoor logic observed in this activity. While the payload appears lengthy, a significant portion of the code consists of unused functions, redundant logic, and extraneous text that is never invoked during execution (SHA256: 932a67816b10a34d05a2621836cdf7fbf0628bbfdf66ae605c5f23455de1e0bc). This additional code increases the size and complexity of the script without impacting its observed behavior. It is passed to the node executable as one large argument.Focusing on the functional components, the payload establishes a persistent execution loop that collects basic host information and communicates with a remote command-and-control (C2) server. Hard-coded identifiers are used to track individual infections and manage tasks from the server.Core backdoor functionalityWhile the JavaScript payload contains a significant amount of unused code, the backdoor's core functionality is implemented through a small number of routines. These routines provide remote code execution, system fingerprinting, and persistent C2 communication.Remote code execution capabilityThe payload includes a function that enables the execution of arbitrary JavaScript while the backdoor is active. At its core, this is the main functionality of this backdoor.This function allows JavaScript code supplied as a string to be dynamically executed over the course of the backdoor lifecycle. By passing the function into the execution context, attacker-supplied code can import additional Node.js modules allowing additional arbitrary node functions to be executed.System fingerprinting and reconnaissanceTo profile the infected system, the backdoor collects a small set of host-level identifiers:This routine gathers the system hostname, MAC addresses from available network interfaces, and basic operating system details. These values provide a stable fingerprint that can be used to uniquely identify infected hosts and associate them with a specific campaign or operator session.In addition to local host identifiers, the backdoor attempts to determine the victim’s public-facing IP address by querying the external service ipify.org, a technique that has also been observed in prior DPRK-linked campaigns.Command-and-control beaconing and task executionPersistent communication with the C2 server is implemented through a polling routine that periodically sends host information and processes server responses. The beaconing logic is handled by the following function:This function periodically sends system fingerprinting data to a remote server and waits for a response. The beacon executes every five seconds, providing frequent interaction opportunities.The server response indicates successful connectivity and allows the backdoor to maintain an active session while awaiting tasking.If the server response contains a specific status value, the contents of the response message are passed directly to the remote code execution routine, mentioned prior.Further Execution and InstructionsWhile monitoring a compromised system, Jamf Threat Labs observed further JavaScript instructions being executed roughly eight minutes after the initial infection. The retrieved JavaScript went on to set up a very similar payload to the same C2 infrustructure.Review of this retrieved payload yields a few interesting details...It beacons to the C2 server every 5 seconds, providing its system details and asks for further JavaScript instructions.It executes that additional JavaScript within a child process.It's capable of shutting itself and child processes down and cleaning up if asked to do so by the attacker.It has inline comments and phrasing that appear to be consistent with AI-assisted code generation.This activity highlights the continued evolution of DPRK-linked threat actors, who consistently adapt their tooling and delivery mechanisms to integrate with legitimate developer workflows. The abuse of Visual Studio Code task configuration files and Node.js execution demonstrates how these techniques continue to evolve alongside commonly used development tools.Jamf Threat Labs will continue to track these developments as threat actors refine their tactics and explore new ways to deliver macOS malware. We strongly recommend that customers ensure Threat Prevention and Advanced Threat Controls are enabled and set to block mode in Jamf for Mac to remain protected against the techniques described in this research.Developers should remain cautious when interacting with third-party repositories, especially those shared directly or originating from unfamiliar sources. Before marking a repository as trusted in Visual Studio Code, it’s important to review its contents. Similarly, "npm install" should only be run on projects that have been vetted, with particular attention paid to package.json files, install scripts, and task configuration files to help avoid unintentionally executing malicious code.Dive into more Jamf Threat Labs research on our blog.]]></content:encoded></item><item><title>From stealth blackout to whitelisting: Inside the Iranian shutdown</title><link>https://www.kentik.com/blog/from-stealth-blackout-to-whitelisting-inside-the-iranian-shutdown/</link><author>oavioklein</author><category>hn</category><pubDate>Thu, 22 Jan 2026 00:00:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Iran is in the midst of one of the world’s most severe communications blackouts. This post uses Kentik data to detail how this historic event unfolded, where this event lies in the context of previous Iranian shutdowns, and finally discusses what might be in store next for Iran.For nearly two weeks, Iran has been enduring one of the most severe internet shutdowns in modern history. The theocratic regime’s decision to restrict communications coincided with a violent nationwide crackdown on a growing protest movement driven by worsening economic hardship.In this post, I explore the situation in Iran using Kentik’s aggregate NetFlow data, along with other sources.At the time of this writing, a near-complete internet shutdown has persisted for almost 14 days. Along with internet services, international voice calling has also been blocked (there have been a couple of periods when limited outgoing calls were allowed), and domestic communication services have experienced extended disruptions, including Iran’s National Information Network. For a country of 90 million people, the combined blocking of these communication modes makes this blackout one of the most severe in history.To learn more about the conditions that lead to the check out this special episode of Kentik’s Telemetry Now podcast with Iranian digital rights expert Amir Rashidi, Director of Digital Rights and Security at the human rights organization Miaan Group:For decades, the internet of Iran has been connected to the world via two international gateways:IPM, primarily a university and research network, was the country’s original internet connection in the 1990s, a story covered in the excellent book The Internet of Elsewhere by Cyrus Farivar. Years later, the state telecom TIC got into the business of providing internet service and today handles the vast majority of internet traffic into and out of Iran.Despite TIC’s dominance, IPM has maintained a technologically independent connection to the outside world, though it has never been immune from Iranian government censorship and surveillance. This distinction matters because each gateway behaved differently during the shutdown.In the days leading up to January 8, there were many reports of localized internet blockages around the country, but these incidents weren’t big enough to register on any of our national traffic statistics for Iran.The first major development occurred at 11:42 UTC on January 8, 2026, when TIC (AS49666) began withdrawing its IPv6 BGP routes from its sessions with other networks. Within hours, nearly all of Iran’s IPv6 routing had disappeared from the global routing table.However, based on our aggregate NetFlow, IPv6 traffic normally amounts to less than 1% of the overall traffic (in bits/sec) into Iran, so the average Iranian was unlikely to be affected by this issue. Regardless, the withdrawal of IPv6 routes appeared to be an early indication of what was to come later in the day.Following a brief disruption, we observed internet traffic levels begin to plummet at 16:30 UTC (7pm local). The drop continued until internet traffic into Iran had all but ceased by 1845 UTC, as illustrated below. It took over two hours to stop all internet traffic into and out of the country.At 19:00 UTC, we observed TIC disconnecting from a subset of its transit providers, including Russian state telecom Rostelecom (AS12389) and regional operator Gulf Bridge International (AS200612), and all of its settlement-free peers.Despite the loss of numerous BGP adjacencies for AS49666 (TIC), the vast majority of Iranian IPv4 routes continued to be routed globally. The drop in Iranian IPv4 traffic, therefore, could not be explained by reachability issues; another mechanism was at work at the network edge blocking traffic.Georgia Tech’s IODA tool captures this divergence well. In the below screenshot, active probing (blue) drops to zero as traffic is blocked, while routed IPv4 space in BGP (green) is almost completely unscathed (98.14%).Although IPv4 routes remained online, internet traffic stopped for roughly 90 million Iranians. This distinction is central to Iran’s next step: internet “whitelisting,” in which an Iranian version of the Chinese Great Firewall allows only approved users or services while blocking all others. Had authorities withdrawn IPv4 routes, as they did with IPv6, Iran would have become completely unreachable, as Egypt was in January 2011. By keeping IPv4 routes in circulation, Iranian authorities can selectively grant full internet access to specific users while denying it to the broader population.As mentioned above, the internet shutdown in Iran is not complete. There has been a tiny amount of traffic still trickling in and out as a small set of Iranians continue to enjoy internet access.From our data, we have also observed the emergence of a diurnal pattern of traffic to AS49666 emerge on January 13. AS49666 is not typically a major terminus for internet traffic to Iran, so this traffic is likely proxied traffic from whitelisted individuals or services.Evolving calculus of shutdowns in IranBack in 2012, Iran was in the beginning stages of building its National Information Network (NIN), ostensibly built to allow the country to continue to function in the event that it was cut off from the outside world. At the time, I teamed up with Iran researcher Collin Anderson to investigate. With access to in-country servers, we mapped Iran’s national internet from the inside (research published here).We found that the NIN had been implemented by routing RFC1918 address space (specifically 10.x.x.x) between Iranian ASes within the country. By doing so, they could be assured that devices connected to the NIN would not be able to receive connections from the outside world, as those IP addresses are not routable on the public internet.In 2019, I reported on Iran’s internet shutdown following the government’s decision to raise gas prices. At the time, it was the most severe shutdown in the country’s history—until this month. It involved withdrawing BGP routes of some networks while blocking traffic of others, and lasted for almost two weeks.Initial impacts of the 2019 Iranian internet shutdown in the Oracle Intelligence Map.Nightly traffic drops for Iranian mobile providers during Iran’s internet curfew in 2022.The article described internet curfews as another means of reducing the costs of shutdowns, not unlike the development of the NIN, according to Iranian digital rights expert Amir Rashidi. In that post, we wrote:The objective of internet curfews, like Iran’s NIN, is to reduce the cost of shutdowns on the authorities that order them. By reducing the costs of these shutdowns, they become a more palatable option for an embattled leader and, therefore, are likely to continue in the future.Timeline for the stages of the Iranian internet shutdown during the Twelve-Day War.The outage demonstrated Iran’s newfound ability to block traffic nationwide without manipulating BGP routes, signaling a higher level of sophistication in its internet filtering. This summer’s Stealth Blackout ultimately foreshadowed the ongoing shutdown Iran is now enduring.In the aftermath of the 2022 protests, Starlink began allowing connections from Iran. Satellite internet operators like Starlink must typically clear, at a minimum, two legal hurdles to operate in a country: a telecom license and radio spectrum authorization from the local government. Starlink has been operational in Iran for over three years at this point without either, and the Iranian government has taken note.The ITU Radio Regulations Board (RRB) is a quasi-judicial United Nations body that interprets and applies the Radio Regulations, to include satellite emissions. It exists to resolve disputes between countries and oversees compliance with the international radio frequency register, but, in the end, has no direct enforcement power.“Request the Administration of the Islamic Republic of Iran to pursue its efforts, to the extent possible, to identify and deactivate unauthorized STARLINK terminals in its territory,Strongly urge the Administration of Norway to take all appropriate actions at its disposal to have the operator of the Starlink system immediately disable unauthorized transmissions of its terminals within the territory of the Islamic Republic of Iran.”Regardless of the decisions of this body, Starlink continues to operate in the country. (Note: The US and Norway share responsibility for Starlink’s ITU registration.)Other governments are watching, learningIn the decade and a half since the internet shutdowns of the Arab Spring, we’ve observed the practice of suppressing communications evolve as authoritarian governments learn tactics from one another. In the ongoing shutdown in Iran, multiple such tactics are on display.To mitigate the costs of its shutdown, the Iranian government has created an internal national internet and appears to be in the process of building a “whitelisting” system to allow certain individuals and services internet access while blocking the rest. If these measures successfully enable an unpopular Iranian government to remain in power, we can expect to see them replicated elsewhere.On the other side, the digital rights activists have also been building tools, funded in large part by the now-embattled Open Technology Fund, to allow communications to continue during a shutdown like this. However, no amount of circumvention tooling can restore service to 90 million people.The fight for open and free communications does not have an end. As long as authoritarian governments exist, this game of cat-and-mouse will continue. Ours is only to decide which side we’re on and to throw our support (financially and otherwise) to those working on solutions to these problems.]]></content:encoded></item><item><title>Lix – universal version control system for binary files</title><link>https://lix.dev/blog/introducing-lix/</link><author>onecommit</author><category>hn</category><pubDate>Wed, 21 Jan 2026 23:55:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[AI agents need version control beyond textChanges AI agents make need to be reviewable by humans.For code, Git solves this:: What exactly did the agent change?: Review, then merge or reject.: Undo mistakes instantly.But agents modify binary files too. And Git can't diff them.Lix is a universal version control system that can diff any file format (, , , etc).Unlike Git's line-based diffs, Lix understands file structure. Lix sees  or cell B4: pending → shipped, not "line 4 changed" or "binary files differ".: See exactly what an agent changed in any file format.: Agents propose, humans approve.: Undo mistakes instantly.An AI agent updates an order status in .  | order_id | product  | status   |
  | -------- | -------- | -------- |
  | 1001     | Widget A | shipped  |
  | 1002     | Widget B | pending |
  | order_id | product  | status   |
  | -------- | -------- | -------- |
  | 1001     | Widget A | shipped  |
  | 1002     | Widget B | shipped |
order_id 1002 status: 

Even for structured text file formats like  lix is tracking semantics rather than line by line diffs.property theme: 
Lix adds a version control system on top of SQL databases that let's you query virtual tables like , , etc. via plain SQL. These table's are version controlled.Lix doesn't reinvent databases — durability, ACID, and corruption recovery are handled by battle-tested SQL databases. — query your version control system with the same SQL.Can runs in your existing database — no separate storage layer to manage.┌─────────────────────────────────────────────────┐
│                      Lix                        │
│           (version control system)              │
│                                                 │
│ ┌────────────┐ ┌──────────┐ ┌─────────┐ ┌─────┐ │
│ │ Filesystem │ │ Branches │ │ History │ │ ... │ │
│ └────────────┘ └──────────┘ └─────────┘ └─────┘ │
└────────────────────────┬────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────┐
│                  SQL database                   │
└─────────────────────────────────────────────────┘
Lix was developed alongside inlang, open-source localization infrastructure. { openLix } ;

 lix = ({
  : ()
});

 lix..().({ : , : ... }).();

 diff = ({ lix })
The next version of Lix will be a refactor to be purely "preprocessor" based. This enables: (SQLite, Postgres, Turso, MySQL)                      ┌────────────────┐
  SELECT * FROM ...   │  Lix Engine    │   SELECT * FROM ...
 ───────────────────▶ │    (Rust)      │ ───────────────────▶  Database
                      └────────────────┘
]]></content:encoded></item><item><title>Show HN: Sweep, Open-weights 1.5B model for next-edit autocomplete</title><link>https://huggingface.co/sweepai/sweep-next-edit-1.5B</link><author>williamzeng0</author><category>hn</category><pubDate>Wed, 21 Jan 2026 23:22:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[A 1.5B parameter model for next-edit autocomplete, quantized to Q8_0 GGUF format.Sweep Next-Edit predicts your next code edit before you make it. It runs locally on your laptop in under 500ms (with speculative decoding) and outperforms models over 4x its size on next-edit benchmarks. More details here.Download  and the model file, then:uv pip install llama-cpp-python huggingface_hub
python run_model.py
: GGUF (Q8_0 quantization): 8192 tokens: Qwen2.5-CoderThe model uses a specific prompt format with file context, recent diffs, and current state to predict the next edit. See  for a complete example.]]></content:encoded></item><item><title>Take potentially dangerous PDFs, and convert them to safe PDFs</title><link>https://github.com/freedomofpress/dangerzone</link><author>dp-hackernews</author><category>hn</category><pubDate>Wed, 21 Jan 2026 22:54:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Your brain on ChatGPT: Accumulation of cognitive debt when using an AI assistant</title><link>https://www.media.mit.edu/publications/your-brain-on-chatgpt/</link><author>misswaterfairy</author><category>hn</category><pubDate>Wed, 21 Jan 2026 22:41:45 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.]]></content:encoded></item><item><title>Show HN: TerabyteDeals – Compare storage prices by $/TB</title><link>https://terabytedeals.com/</link><author>vektor888</author><category>hn</category><pubDate>Wed, 21 Jan 2026 21:13:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>eBay explicitly bans AI &quot;buy for me&quot; agents in user agreement update</title><link>https://www.valueaddedresource.net/ebay-bans-ai-agents-updates-arbitration-user-agreement-feb-2026/</link><author>bdcravens</author><category>hn</category><pubDate>Wed, 21 Jan 2026 21:07:47 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[eBay explicitly prohibits AI "buy for me" agents and LLM (larger language model) bots, updates arbitration and dispute resolution requirements in latest User Agreement update, going into effect February 20, 2026.The following summary of changes was provided in an email sent to users:We’ve updated eBay’s User Agreement, including the agreement to arbitrate any disputes you may have with us. Our updated User Agreement was posted on January 20, 2026. For users who agreed to a prior version of our User Agreement, this agreement is effective as of February 20, 2026.In this update, eBay is updating its anti-scraping prohibition to clarify that it specifically also includes bots used for AI or LLMs. eBay is also updating the agreement to arbitrate in the updated User Agreement:We clarified the scope of the class action waiver.We clarified the process for opting out of the agreement to arbitrate.We updated the physical address to which notices for informal dispute resolution, arbitration demands, and notices for opting out of arbitration must be sent.Disclaimer: comparisons are made using both automated and manual methods and are provided for informational purposes only - no warranty of completeness or accuracy is expressed or implied and users are advised to do their own due diligence.First, as the summary calls out, eBay is explicitly prohibiting AI "buy for me" agents and LLM scraping bots from interacting with the platform without permission from eBay.In connection with using or accessing our Services you agree to comply with this User Agreement, our policies, our terms, and all applicable laws, rules, and regulations, and you will not......use any robot, spider, scraper, data mining tools, data gathering and extraction tools, or other automated means to access our Services for any purpose, except with the prior express permission of eBay;In connection with using or accessing our Services you agree to comply with this User Agreement, our policies, our terms, and all applicable laws, rules, and regulations, and you will not...use any robot, spider, scraper, data mining tools, data gathering and extraction tools, or other automated means (including, without limitation buy-for-me agents, LLM-driven bots, or any end-to-end flow that attempts to place orders without human review) to access our Services for any purpose, except with the prior express permission of eBay;The move comes after eBay quietly changed their robots.txt file with new guidance placing guardrails and restrictions on how AI agents interact with the site in December.It also comes on the heels of Amazon's controversial Buy For Me test which uses agentic AI to display items from direct merchant websites for sale through the Amazon app, even if the brand does not sell on Amazon themselves - raising concerns about transparency, consent, and control over how product details are displayed to buyers.While it appears that Amazon Buy For Me currently does pull inventory from other third party marketplaces, it would not be surprising if eBay is reacting at least in part to this and other agentic commerce news making recent headlines.Arbitration & Dispute ResolutionThe rest of the changes in this User Agreement update affect arbitration and dispute resolution.eBay's previous User Agreement update in May 2025 made significant changes to arbitration terms and limits on lawsuits, forcing users to give up their right to the sue the company in many situations.Notice to eBay should be sent by email to DisputeNotice@eBay.com or regular mail to our offices located at 583 W. eBay Way, Draper, UT 84020.Notice to eBay should be sent by email to DisputeNotice@eBay.com or regular mail to our offices located at 339 W. 13490 S., Ste. 500, Draper, UT 84020Most importantly, eBay has expanded their arbitration clause which previously prohibited class actions to now also explicitly exclude more types of group legal actions.EACH OF US MAY BRING CLAIMS AGAINST THE OTHER ONLY ON AN INDIVIDUAL BASIS AND NOT ON A CLASS, REPRESENTATIVE, OR COLLECTIVE BASIS, AND THE PARTIES HEREBY WAIVE ALL RIGHTS TO HAVE ANY DISPUTE BE BROUGHT, HEARD, ADMINISTERED, RESOLVED, OR ARBITRATED ON A CLASS, COLLECTIVE, OR REPRESENTATIVE BASIS. ONLY INDIVIDUAL RELIEF IS AVAILABLE.Subject to this Agreement to Arbitrate, the arbitrator may award declaratory or injunctive relief only in favor of the individual party seeking relief and only to the extent necessary to provide relief warranted by the party’s individual claim. Nothing in this paragraph is intended to, nor shall it, affect the terms and conditions under Section 19.B.7 ("Batch Arbitration").EACH OF US MAY BRING CLAIMS AGAINST THE OTHER ONLY ON AN INDIVIDUAL BASIS AND NOT AS A PLAINTIFF OR CLASS MEMBER IN ANY PURPORTED CLASS, OR REPRESENTATIVE, OR COLLECTIVE BASIS, OR PRIVATE ATTORNEY GENERAL ACTION OR PROCEEDING, NOR OTHERWISE TO SEEK RECOVERY OF LOSSES OR DAMAGES (WHETHER FOR YOURSELF OR OTHERS) INCURRED BY A THIRD PARTY, AND THE PARTIES HEREBY WAIVE ALL RIGHTS TO HAVE ANY DISPUTE BE BROUGHT, HEARD, ADMINISTERED, RESOLVED, OR ARBITRATED ON A CLASS, COLLECTIVE, OR REPRESENTATIVE BASIS. ONLY INDIVIDUAL RELIEF IS AVAILABLE.Subject to this Agreement to Arbitrate, the arbitrator may award declaratory or injunctive relief only in favor of the individual party seeking relief and only to the extent necessary to provide relief warranted by the party’s individual claim. Nothing in this paragraph is intended to, nor shall it, affect the terms and conditions under Section 19.B.7 ("Batch Arbitration").Here's what that means in plain language:“Not as a plaintiff or class member” — prevents someone from joining an existing class action.“No private attorney general actions” — blocks lawsuits brought “on behalf of the public,” a type of claim sometimes used in consumer protection cases.“Nor… for losses incurred by a third party” — prevents a person from trying to recover damages suffered by someone else.Note: this language does in any way change or restrict legal action that state Attorneys General, the FTC or other regulatory or legal agencies can take on behalf of sellers and/or consumers - so don't be dissuaded from letting those agencies know about your experiences with the platform, like the recent changes to Promoted Listings ad attribution policies.And finally, this User Agreement update has been changed to clarify that only new users may request to opt out of arbitration agreement - existing users missed their opportunity if they did not opt out before May 16, 2025.IF YOU ARE A NEW USER OF OUR SERVICES, YOU CAN CHOOSE TO OPT OUT OF THIS AGREEMENT TO ARBITRATE ("OPT OUT") BY MAILING US A WRITTEN OPT-OUT NOTICE ("OPT-OUT NOTICE").And that's it for changes to eBay's User Agreement going into effect February 20, 2026.Let us know in the comments below what you think of these change and how they'll affect your business!]]></content:encoded></item><item><title>Spotify won court order against Anna&apos;s Archive, taking down .org domain</title><link>https://arstechnica.com/tech-policy/2026/01/annas-archive-said-spotify-scrape-didnt-cause-domain-suspension-it-was-wrong/</link><author>voxadam</author><category>hn</category><pubDate>Wed, 21 Jan 2026 20:52:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Linux from Scratch</title><link>https://www.linuxfromscratch.org/lfs/view/stable/</link><author>Alupis</author><category>hn</category><pubDate>Wed, 21 Jan 2026 18:44:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
              Published September 1st, 2025
            Copyright © 1999-2025 Gerard
              Beekmans
            ]]></content:encoded></item><item><title>TeraWave Satellite Communications Network</title><link>https://www.blueorigin.com/news/blue-origin-introduces-terawave-space-based-network-for-global-connectivity</link><author>T-A</author><category>hn</category><pubDate>Wed, 21 Jan 2026 18:31:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Rails UI</title><link>https://railsui.com/</link><author>justalever</author><category>hn</category><pubDate>Wed, 21 Jan 2026 18:31:19 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA["Rails UI is going to save me months of work. I'm an experienced software developer building my first Ruby on Rails app, but I'm not strong at front-end design. Support has been awesome as well." — Software Developer
      ]]></content:encoded></item><item><title>Scientists find a way to regrow cartilage in mice and human tissue samples</title><link>https://www.sciencedaily.com/releases/2026/01/260120000333.htm</link><author>saikatsg</author><category>hn</category><pubDate>Wed, 21 Jan 2026 18:05:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Human cartilage samples taken from knee replacement surgeries also responded positively. These samples included both the supportive extracellular matrix of the joint and cartilage-producing chondrocyte cells. When treated, the tissue began forming new, functional cartilage.Together, the findings suggest that cartilage lost due to aging or arthritis may one day be restored using either a pill or a targeted injection. If successful in people, such treatments could reduce or even eliminate the need for knee and hip replacement surgery.A Direct Attack on OsteoarthritisOsteoarthritis is a degenerative joint disease that affects about one in five adults in the United States and generates an estimated $65 billion each year in direct health care costs. Current treatments focus on managing pain or replacing damaged joints surgically. There are no approved drugs that can slow or reverse the underlying cartilage damage.The new approach targets the root cause of the disease rather than its symptoms, offering a potential shift in how osteoarthritis is treated.The Role of a Master Aging EnzymeThe protein at the center of the study is called 15-PGDH. Researchers refer to it as a gerozyme because its levels increase as the body ages. Gerozymes were identified by the same research team in 2023 and are known to drive the gradual loss of tissue function.In mice, higher levels of 15-PGDH are linked to declining muscle strength with age. Blocking the enzyme using a small molecule boosted muscle mass and endurance in older animals. In contrast, forcing young mice to produce more 15-PGDH caused their muscles to shrink and weaken. The protein has also been connected to regeneration in bone, nerve, and blood cells.In most of these tissues, repair happens through the activation and specialization of stem cells. Cartilage appears to be different. In this case, chondrocytes change how their genes behave, shifting into a more youthful state without relying on stem cells.A New Path to Tissue Regeneration"This is a new way of regenerating adult tissue, and it has significant clinical promise for treating arthritis due to aging or injury," said Helen Blau, PhD, professor of microbiology and immunology. "We were looking for stem cells, but they are clearly not involved. It's very exciting."Blau, who leads the Baxter Laboratory for Stem Cell Biology and holds the Donald E. and Delia B. Baxter Foundation Professorship, and Nidhi Bhutani, PhD, associate professor of orthopaedic surgery, are the study's senior authors. The research was published in . Mamta Singla, PhD, instructor of orthopaedic surgery, and former postdoctoral scholar Yu Xin (Will) Wang, PhD, served as lead authors. Wang is now an assistant professor at the Sanford Burnham Institute in San Diego.Dramatic Regeneration of Joint Cartilage"Millions of people suffer from joint pain and swelling as they age," Bhutani said. "It is a huge unmet medical need. Until now, there has been no drug that directly treats the cause of cartilage loss. But this gerozyme inhibitor causes a dramatic regeneration of cartilage beyond that reported in response to any other drug or intervention."The human body contains three main types of cartilage. Elastic cartilage is soft and flexible and forms structures such as the outer ear. Fibrocartilage is dense and tough, helping absorb shock in places like the spaces between spinal vertebrae. Hyaline cartilage is smooth and glossy, allowing joints such as the hips, knees, shoulders, and ankles to move with low friction. This type, also called articular cartilage, is the form most commonly damaged in osteoarthritis.Why Cartilage Rarely Grows BackOsteoarthritis develops when joints are stressed by aging, injury, or obesity. Chondrocytes begin releasing inflammatory molecules and breaking down collagen, the main structural protein in cartilage. As collagen is lost, cartilage becomes thinner and softer. Inflammation then leads to swelling and pain, which are hallmarks of the disease.Under normal conditions, articular cartilage has very limited ability to regenerate. While some stem or progenitor cells capable of forming cartilage have been identified in bone, similar cells have not been successfully found within articular cartilage itself.Connecting Aging, Prostaglandins, and RepairEarlier research from Blau's lab showed that prostaglandin E2 is essential for muscle stem cell function. The enzyme 15-PGDH breaks down prostaglandin E2. By blocking 15-PGDH or increasing prostaglandin E2 levels, researchers previously supported the repair of damaged muscle, nerve, bone, colon, liver, and blood cells in young mice.This led the team to question whether the same pathway might be involved in cartilage aging and joint damage. When they compared knee cartilage from young and old mice, they found that 15-PGDH levels roughly doubled with age.Regrowing Cartilage in Aging KneesResearchers then injected older mice with a small molecule that inhibits 15-PGDH. They first administered the drug into the abdomen to affect the entire body, and later injected it directly into the knee joint. In both cases, cartilage that had become thin and dysfunctional with age thickened across the joint surface.Additional tests confirmed that the regenerated tissue was hyaline cartilage rather than the less functional fibrocartilage."Cartilage regeneration to such an extent in aged mice took us by surprise," Bhutani said. "The effect was remarkable."Protecting Joints After ACL-Like InjuriesThe team observed similar benefits in mice with knee injuries resembling ACL tears, which often occur during sports involving sudden stopping, pivoting, or jumping. Although such injuries can be surgically repaired, about half of affected people develop osteoarthritis in the injured joint within 15 years.Mice that received twice-weekly injections of the gerozyme inhibitor for four weeks after injury were far less likely to develop osteoarthritis. In contrast, animals given a control treatment had double the levels of 15-PGDH compared with uninjured mice and developed osteoarthritis within four weeks.Treated mice also moved more normally and placed more weight on the injured leg than untreated animals."Interestingly, prostaglandin E2 has been implicated in inflammation and pain," Blau said. "But this research shows that, at normal biological levels, small increases in prostaglandin E2 can promote regeneration."Reprogramming Cartilage Cells Without Stem CellsCloser analysis showed that chondrocytes in older mice expressed more genes linked to inflammation and the conversion of cartilage into bone, along with fewer genes involved in cartilage formation. Treatment shifted these patterns.One group of chondrocytes that produced 15-PGDH and cartilage-degrading genes dropped from 8% to 3%. Another group associated with fibrocartilage formation declined from 16% to 8%. A third population, which did not produce 15-PGDH and instead expressed genes tied to hyaline cartilage formation and maintenance of the extracellular matrix, rose from 22% to 42%.These changes indicate a broad return to a more youthful cartilage profile without involving stem or progenitor cells.Evidence From Human Cartilage SamplesThe researchers also tested cartilage taken from patients undergoing total knee replacement for osteoarthritis. After one week of treatment with the 15-PGDH inhibitor, the tissue showed fewer 15-PGDH-producing chondrocytes, reduced expression of cartilage degradation and fibrocartilage genes, and early signs of articular cartilage regeneration."The mechanism is quite striking and really shifted our perspective about how tissue regeneration can occur," Bhutani said. "It's clear that a large pool of already existing cells in cartilage are changing their gene expression patterns. And by targeting these cells for regeneration, we may have an opportunity to have a bigger overall impact clinically."Looking Toward Human TrialsBlau added, "Phase 1 clinical trials of a 15-PGDH inhibitor for muscle weakness have shown that it is safe and active in healthy volunteers. Our hope is that a similar trial will be launched soon to test its effect in cartilage regeneration. We are very excited about this potential breakthrough. Imagine regrowing existing cartilage and avoiding joint replacement."Researchers from the Sanford Burnham Prebys Medical Discovery Institute also contributed to the study.The work was supported by funding from the National Institutes of Health (grants R01AR070864, R01AR077530, R01AG069858 and R00NS120278), the Baxter Foundation for Stem Cell Biology, the Li Ka Shing Foundation, the Stanford Cardiovascular Institute, the Milky Way Research Foundation, the Canadian Institutes of Health Research, a Stanford Translational Research and Applied Medicine Pilot grant, a GlaxoSmithKline Sir James Black Postdoctoral Fellowship, and a Stanford Dean's Postdoctoral Fellowship.Blau, Bhutani, and other co-authors are inventors on patent applications held by Stanford University related to 15-PGDH inhibition in cartilage and tissue rejuvenation, which are licensed to Epirium Bio. Blau is a co-founder of Myoforte/Epirium and holds equity and stock options in the company.]]></content:encoded></item><item><title>Waiting for dawn in search: Search index, Google rulings and impact on Kagi</title><link>https://blog.kagi.com/waiting-dawn-search</link><author>josephwegner</author><category>hn</category><pubDate>Wed, 21 Jan 2026 17:28:03 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[This blog post is a follow-up to Dawn of a new era in Search, published last year. A lot has changed: the legal case has advanced, AI has become the central battleground, and the need for open index access has only grown sharper.As of late 2025, one company decides what nearly 9 out of 10 people see when they search the web: Google. On August 5, 2024, a U.S. court officially ruled that Google is a monopolist in general search services. This ruling is not about ads or browser defaults alone. It is about who controls the index that powers both search and AI - and whether anyone else is allowed to build on it.The stakes have grown sharper over the past year. LLMs hallucinate without grounding in real-world information; every agent that answers questions about the real world, depends on search. LLMs themselves are a blend of proprietary and open source. Cloud compute is competitive. But search is different - only one company controls a comprehensive, fresh, high-quality web index. If one company controls the index, it controls the floor on how good AI can be - and who gets to build it. The innovation crunch in search is now an innovation crunch in AI.We are writing this from a position we believe in: people should have the choice to access information without behaviour-changing, ad-driven, intermediary standing between them and knowledge.Why does this matter? The information we consume shapes our understanding of the world as profoundly as the food we eat shapes our bodies. Search (directly, and indirectly through AI) is the primary mechanism through which we inform political judgments, financial decisions, medical choices, and countless other consequential aspects of our lives. When a single company controls the gateway to information - and operates that gateway in ways misaligned with user interests - it influences not only what we know, but how we reason.The problem: A search monopolyWorldwide search market share (October 2025, StatCounter):The United States is similar: Google at 85%, Bing at 9%, everyone else in the noise.This is not a competitive market. It is a monopoly with a distant second place.The search index is irreplaceable infrastructure. Building a comparable one from scratch is like building a parallel national railroad. Microsoft spent roughly $100 billion over 20 years on Bing and still holds single-digit share. If Microsoft cannot close the gap, no startup can do it alone.This is exactly what the Sherman Act was designed to address: when one company’s control of critical infrastructure prevents effective competition, regulators must force open access on fair terms.When a single, ad-driven gatekeeper controls the primary way humans reach information, it is not just competition that suffers - it is our collective ability to learn, to make informed medical and economic choices, and to participate meaningfully in democratic life.As Ian Bremmer put it: “The idea that we get our information as citizens through algorithms determined by the world’s largest advertising company is my definition of dystopia.”Google’s own founders knew this. In their 1998 white paper, Sergey Brin and Larry Page sharply criticized the ad-supported search model for creating mixed motives and biasing results toward advertisers’ interests. They wrote that “advertising funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers” and that “advertising income often provides an incentive to provide poor quality search results.” Those concerns have only grown more pressing as search has become the primary interface between humanity and the web.We tried to do it the right wayKagi has always tried to integrate the best sources of knowledge into one coherent, ad-free experience. We see ourselves as connective tissue: letting people reach high-quality information directly, without passing through an ad system whose incentives are misaligned with their needs.We approached every major index vendor seeking direct licensing on FRAND terms (Fair, Reasonable, And Non-Discriminatory): fair pricing, no mandatory ad syndication, ability to reorder and blend results. We succeeded with many, including:With Google and Bing, we failed - not for lack of trying.Bing: Their terms didn’t work for us from the start. Microsoft’s terms prohibited reordering results or merging them with other sources - restrictions incompatible with Kagi’s approach. In February 2023, they announced price increases of up to 10x on some API tiers. Then in May 2025, they retired the Bing Search APIs entirely, effective August 2025, directing customers toward AI-focused alternatives like Azure AI Agents.Google: Google does not offer a public search API. The only available path is an ad-syndication bundle with no changes to result presentation - the model Startpage uses. Ad syndication is a non-starter for Kagi’s ad-free subscription model.[^1]The current interim approachBecause direct licensing isn’t available to us on compatible terms, we - like many others - use third-party API providers for SERP-style results (SERP meaning search engine results page). These providers serve major enterprises (according to their websites) including Nvidia, Adobe, Samsung, Stanford, DeepMind, Uber, and the United Nations.This is not our preferred solution. We plan to exit it as soon as direct, contractual access becomes available. There is no legitimate, paid path to comprehensive Google or Bing results for a company like Kagi. Our position is clear: open the search index, make it available on FRAND terms, and enable rapid innovation in the marketplace.The Google antitrust case began in 2020. On August 5, 2024, the court ruled Google violated Section 2 of the Sherman Act by unlawfully maintaining its monopoly through exclusive distribution agreements. (Full ruling)On September 2, 2025, the DOJ announced remedies (press release): Google is prohibited from exclusive contracts related to Search, Chrome, Assistant, and Gemini.Data sharing and syndication: Google must provide search index and interaction data to competitors and offer syndication services to help rivals build competitive search.Addressing monopolization tactics: The remedies aim to dismantle a decade of exclusionary agreements.In December 2025, Judge Mehta issued a memorandum outlining the specific remedies the court intends to impose. The details are significant: Google must offer query-based search syndication to “Qualified Competitors” on terms no less favorable than those provided to current partners. Google cannot condition access to search results on the use of Google Ads; competitors are free to monetize via their own ads or third parties. Google must provide Web Search Index data (URLs, crawl metadata, spam scores) at marginal cost. The judgment remains in effect for 6 years, with syndication licenses guaranteed for terms of 5 years.If implemented as outlined, this is exactly what we have been asking for. The legal trajectory is promising. Google will contest details, and final enforceable terms are still being worked out. The fight now is ensuring these remedies become real, practical access - not paper compliance.Why enforcement matters nowEven as these remedies take shape, Google is moving to close the back door. In December 2025, Google sued SerpApi for scraping its results at scale.We take a measured, principled view: Google built its index by crawling the open web before robots.txt was a widespread norm, often over publishers’ objections. Today, publishers “consent” to Google’s crawling because the alternative - being invisible on a platform with 90% market share - is economically unacceptable. Google now enforces ToS and robots.txt against others from a position of monopoly power it accumulated without those constraints. The rules Google enforces today are not the rules it played by when building its dominance.The structural problem remains: This lawsuit is only necessary because Google refuses to offer legitimate, paid index access.Our position is unchanged: We have always wanted direct licensing. We would happily pay market rates for clean, contractual access. The fact that we - and companies like Stanford, Nvidia, Adobe, and the United Nations - have had to rely on third-party vendors is a symptom of the closed ecosystem, not a preference.The connection to DOJ remedies is direct: if Google is going to close the back door, regulators must ensure the front door is open. That is exactly what the DOJ’s index syndication requirements are meant to achieve - and why we support their full implementation.What could be: A layered search ecosystemThe DOJ ruling does not itself create a healthy market, but it makes one possible.And while this post focuses on remedies and their impact on Kagi, it is worth zooming out: even if those remedies work perfectly, long-term societal prosperity and resilience require a non-commercial baseline for access to information - something that is not dependent on ad incentives or a single vendor’s business priorities. Think of it as a north-star model for a modern society where information access is a fundamental right.Here is what that could look like:Layer 1: Search as a public goodThis is a long-term possibility, not a near-term expectation. A government-backed, ad-free, intermediary-free, taxpayer-funded search service providing baseline, non-discriminatory access to information. Imagine search.org.This is not something the DOJ remedies create directly, nor something Kagi expects to exist soon. It is included here to make explicit what an open-index world could ultimately make possible.This layer would replace the role public libraries played for centuries - a role that effectively disappeared when commercial web search took over in the late 1990s. Our ancestors understood well the benefits that non-discriminatory, direct access to information brings to citizens, and ultimately society itself.It raises hard questions: governance, funding, political independence, precedent. But the principle is sound. Every citizen should have access to information without an ad-optimized algorithm standing between them and knowledge. If we can fund public libraries, we can fund public search.Layer 2: Free, ad-based searchCommercial search engines with richer features, funded by advertising. Users understand the tradeoff and have a genuine public alternative. This is the space where most contemporary search engines operate.Layer 3: Paid, subscription-based searchPremium search engines offering the highest possible quality, privacy, and advanced features for users who value this and are willing to pay. This is where Kagi operates - and where we are expanding as an integrator of knowledge across search, browser, mail, and AI assistants, without selling your attention.This layered model creates a diverse ecosystem:A public baseline for information access.Commercial free options for convenience and reach.Premium paid options for those who want maximum quality and control.Aligns with the primary purpose of the Sherman Act.[^2]The DOJ ruling is starting to do what antitrust is supposed to do: turn a closed, private choke point into shared infrastructure that others can build on. If the remedies land as real, usable access (APIs, cost-based pricing, no ad bundling), the web can support a layered ecosystem again: a public baseline for citizens, free ad-supported products for reach, and paid services that compete on quality, privacy, and power-user features.That is the world we are building Kagi for. We are ready to walk through the front door - not depend on gray-market workarounds. Our job now is to be ready when the door opens, and to help make sure it does: keep Kagi genuinely multi-source, keep investing in our Small Web Index, and keep shipping a subscription search experience that delivers the best results across providers. If we get this right, the next decade of search and AI does not have to be one funnel owned by one company. It can be a competitive stack of layers that treats information access as the public good it has always been.Market data and commentaryThird-party search API providers[^1]: A note on Google’s existing APIs: Google offers PSE, designed for adding search boxes to websites. It can return web results, but with reduced scope and terms tailored for that narrow use case. More recently, Google offers Grounding with Google Search through Vertex AI, intended for grounding LLM responses. Neither is general-purpose index access. Programmable Search Engine is not designed for building competitive search. Grounding with Google Search is priced at $35 per 1,000 requests - economically unviable for search at scale, and structured as an AI add-on rather than standalone index syndication. These are not the FRAND terms the market needs.[^2]: Our understanding of the primary purpose of the Sherman Act is not to shield competitors from the success of legitimate businesses or to prevent those businesses from earning fair profits. Rather, it is to preserve a competitive marketplace that protects consumers from harm (see Competition law and consumer protection, Kluwer Law International, pp. 291–293). Opening the search index would create healthy, real, and intense competition in the search space - including competition to Kagi - which aligns with our understanding of the Sherman Act’s intent. The goal is not the elimination of dominant firms, but the prevention of a single, closed index from becoming the only gateway to information.Published by Vladimir Prelovac and Raghu Murthi on January 21, 2026.]]></content:encoded></item><item><title>TrustTunnel: AdGuard VPN protocol goes open-source</title><link>https://adguard-vpn.com/en/blog/adguard-vpn-protocol-goes-open-source-meet-trusttunnel.html</link><author>kumrayu</author><category>hn</category><pubDate>Wed, 21 Jan 2026 17:21:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Today is a big day for us, and for everyone who cares about transparency, privacy, and having full control over their own traffic. We’re finally open-sourcing the protocol that powers AdGuard VPN. And it now has a name: For a long time, we’ve wanted to make the protocol public. Many of you asked for it, and we always said: yes, we will, it’s only a matter of time. Well, the time has come.TrustTunnel is now open-source, free to explore, audit, build upon, and use in your own projects.At its core, TrustTunnel is a modern, secure, mobile-optimized VPN protocol. It’s the very same technology that has been running inside all AdGuard VPN apps: on mobile, desktop, and browser extensions.Why TrustTunnel? Because we needed something betterThere are plenty of VPN protocols out there, so why create our own, some might ask. That is because we’ve seen in practice the faults of popular VPN protocols, especially in countries with tight restrictions on internet access. Protocols like OpenVPN, WireGuard, and IPSec share common weaknesses: they are easy to detect and block at the network level, and attempts to conceal VPN traffic often reduce speed. Traditional approaches “wrap” VPN data in a TCP connection and mimic normal web traffic, but TCP’s way of confirming every piece of data creates delays and makes the connection slower.Unlike those conventional VPN protocols, TrustTunnel is engineered to blend in with regular HTTPS traffic, making it far harder to throttle or block and helping it slip past deep-packet inspection, all while preserving strong privacy and security. It achieves this through TLS-based encryption, the same standard that secures HTTPS, and by leveraging HTTP/2 or HTTP/3 transport, which are ubiquitous on the web. Each connection runs on its own dedicated stream, which combines packets for faster, more efficient transmission. It is also optimized for mobile platforms and performs well even in unstable network conditions.A protocol you can use, run, tweak, extend, and build uponBy releasing TrustTunnel, we hope to achieve two things. First of all, we want to finally show our users what protocol is powering AdGuard VPN, thus allowing them to audit it openly. At AdGuard, we have always been staunch supporters of the idea of open-source software, and many of our products have long been open source. AdGuard VPN was lagging behind in this regard, but with TrustTunnel being released publicly, it is starting to catch up.But most importantly, we want to change the status quo in the world of VPN protocols and offer an alternative to existing solutions. That said, we do not want it to be just a PR stunt, when the protocol’s code is de-facto ‘open source,’ but only one VPN service actually runs it. We believe in free and open-source software (FOSS) and want TrustTunnel to be used widely, including by other VPN services. We believe this is the right way to go about open source development, and we hope the community will participate in the TrustTunnel evolution. We welcome any contribution, whether it is a feature request, a bug report, or even a direct contribution to the app’s development.What have we done to make this possible?We are publishing the first version of the TrustTunnel specification.We are releasing the complete code of our reference implementation of the TrustTunnel server and its clients under a very permissive license.You don’t have to install AdGuard VPN to use TrustTunnel. You can configure your own server and use open source TrustTunnel clients:Command-line TrustTunnel clients support Linux, Windows, and macOSWe are also releasing two client apps for iOS and AndroidTrustTunnel clients already have a lot of functionality, they allow you to:Use flexible routing rules to decide which requests go through the tunnel and which stay on the local networkExercise fine-grained control, separating work and personal traffic, routing specific domains or apps, and tuning network behavior without complicated setupBenefit from a real-time request log that provides full transparency into where the device sends traffic, how routing rules apply, and which connections use the tunnelThis is a long-awaited moment for us. We promised to open-source our protocol, and today we’re delivering on that promise. With TrustTunnel now open source, users and developers alike can explore, self-host, and build on the technology.]]></content:encoded></item><item><title>PicoPCMCIA – a PCMCIA development board for retro-computing enthusiasts</title><link>https://www.yyzkevin.com/picopcmcia/</link><author>rbanffy</author><category>hn</category><pubDate>Wed, 21 Jan 2026 16:43:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[This is a PCMCIA development board for retro-computing enthusiasts who want to experiment with audio, networking, and expansion on vintage laptops and mobile devices. While ISA users have enjoyed projects like PicoGUS and PicoMEM, PCMCIA users have long been limited to scarce legacy cards with narrow functionality — this board aims to change that. The project is fully open source, and while it is designed to encourage low-level experimentation and development, pre-built, community-provided firmware is available for users who want to test functionality without diving into the technical details. It is intended for hobbyist and development use and is not certified for production deployment.This is a Type II, 5V, 16-bit PC Card designed for use in compliant PCMCIA sockets and should work in most devices.  While I have not yet encountered a device advertising PCMCIA support that was incompatible, support for every device cannot be guaranteed. Power consumption varies depending on enabled functions; support for low-power devices such as the  is considered mandatory, and the card has been tested to remain within the  limit while using network functionality and storage emulation. On devices with very limited power budgets, simultaneous use of networking and audio may require external power.A short list of devices that I actively test on:Built around the  and leveraging the ISA-like nature of the PCMCIA bus, this project benefits greatly from code interchangeability with other RP-based retro projects, most notably  and . This shared foundation allows features and improvements to move quickly between platforms, expanding functionality over time.The card has an onboard wireless module containing the , same as found on the Raspberry Pi Pico W. This allows the card to attach to modern Wi-Fi networks (2.4GHz 802.11b/g/n WPA2). It can then emulate an NE2000 adapter and/or a dialup modem allowing the host computer to access the network as if it was wired, unaware it is wireless.Essentially every platform containing PCMCIA will have existing drivers to recognize and utilize the card as a modem or ethernet adapter making this a near universal option for all devices and platforms including rare devices such as the Apple Newton.It also has a Bluetooth which opens up a lot of possibilities for A2DP wireless audio streaming and wireless gamepads/mice.  Software for these features Bluetooth features are still under development and is at a proof of concept stage.The card has an included Texas Instruments TLV320AIC3254 which calls itself a “Very Low-Power Stereo Audio CODEC with programmable miniDSP”. The main features of this device in our application are:DAC that is fed high quality audio from the RP2354 over i2sAmplified stereo headphone amplifierLine out feeding the host device internal speaker (where supported)Line in from the onboard midi sythesizer (see below)Line in from external i/o connector for mixing external audioControlled by the RP2350 via i2c (controlling volumes etc).This is combined with a DREAM SAM2695 “Low power single chip synthesizer with effects and built-in codec”, this is the same chip used on the Serdashop Dreamblaster S2. It is a great device for DOS gaming and other applications, its main features are:64-voice polyphony (without effects)38-voice polyphony + effectsGeneral MIDI compatible effectsEmulation of intelligent mode MPU-401 is possible thanks to implementation done by PicoGUS base on SoftMPU/HardMPU.  The midi output is driven to the internal SAM2695 as well as to an external Midi port.   While using external Midi you are able to mute the  internal SAM2695,  or if you are  not using any of the internal sound hardware you can power it down.  Planning has been done with the external GPIO to  support MIDI IN  if ever implemented.Sound Blaster emulation on PCMCIA is particularly challenging, as most PCMCIA sockets and cards lack native DMA support. To address this, the PicoPCMCIA implements , similar in spirit to the approach used by the infamous IBM 3D Sound card, resulting in good compatibility with many real-mode and protected-mode games — including the obligatory .   The IBM card was essentially the only card to offer this functionality, it seems it may have been that way due to IBM patenting (expired) the concept of DMA emulation with PCMCIA.The core Sound Blaster emulation developed for PicoPCMCIA has been shared with the PicoGUS project, where it is actively used and has greatly benefited from additional community-contributed improvements.  emulation is borrowed from the PicoGUS implementation.Thanks to the incredible work from the PicoGUS,   it is now possible to have the worlds first PCMCIA Gravis Ultrasound! Currently this does not support DMA so only some games/demos work.  The GUS is a little bit different with its use of TC, but it may be possible to apply the DMA emulation strategies from the  SoundBlaster mode to the GUS.The card implements an emulated  Panasonic MKE CD-ROM which an be used for both data and audio.  The audio at full quality is sent to the TI DSP over i2c  and can be used simultaneously with all the other audio functions.   This code was shared to the PicoGUS and is currently in use there and has been improved by the community.While storage emulation is not a primary focus given the ready availability of solutions like CompactFlash, it is supported and continues to evolve. Current implementations include Panasonic MKE CD-ROM emulation as well as linear flash emulation, and ATA/ATAPI emulation should be possible in the future once the PicoIDE project becomes available and code can be shared.  Disk images can be BIN/CUE, ISO and are stored on the MicroSD card.There is also a special edge case for the HP 200LX, where the card can emulate an “Accurite Doubleslot” device, allowing an emulated flash card to coexist with networking or sound functionality. This is particularly important on systems with only a single PCMCIA slot, where storage availability is at a premium.The USB port for the RP2354 is made available on the external connector.   It’s primary purpose is to be used for  flashing the card with firmware, however as demonstrated on the PicoGUS and PicoMEM, it can use used for USB Gamdpads and USB Mice which are presented the the host system  as legacy gamepad and serial mouse.  It has also been demonstrated the latest update to the PicoGUS that accessing flash storage at a reasonable speed is possible via USB.]]></content:encoded></item><item><title>JPEG XL Test Page</title><link>https://tildeweb.nl/~michiel/jxl/</link><author>roywashere</author><category>hn</category><pubDate>Wed, 21 Jan 2026 16:38:26 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[This page shows a JPEG XL image, if your browser can handle it! At this point
in time (January 2026) this more or less means only Safari will display the
image, as far as I know. See also Can I Use.The person in the image is Jon Sneyers, co-author of
the JPEG XL spec and also creator of the “Free Lossless Image Format” that came
before it.I find JPEG XL interesting because of its history. It once was implemented in
Chrome, but hidden behind a feature flag. Then Chrome said that it did not saw
enough usage, which is unsurprising, really, and it was removed. Now they
blessed it again and are re-adding it! Some of this story is found on the
JPEG XL Wikipedia page]]></content:encoded></item><item><title>Tell HN: Bending Spoons laid off almost everybody at Vimeo yesterday</title><link>https://news.ycombinator.com/item?id=46707699</link><author>Daemon404</author><category>hn</category><pubDate>Wed, 21 Jan 2026 16:14:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[As expected. Almost the whole company is gone, less than 15 people left in engineering.]]></content:encoded></item><item><title>Claude&apos;s new constitution</title><link>https://www.anthropic.com/news/claude-new-constitution</link><author>meetpateltech</author><category>hn</category><pubDate>Wed, 21 Jan 2026 16:04:49 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[We’re publishing a new constitution for our AI model, Claude. It’s a detailed description of Anthropic’s vision for Claude’s values and behavior; a holistic document that explains the context in which Claude operates and the kind of entity we would like Claude to be.The constitution is a crucial part of our model training process, and its content directly shapes Claude’s behavior. Training models is a difficult task, and Claude’s outputs might not always adhere to the constitution’s ideals. But we think that the way the new constitution is written—with a thorough explanation of our intentions and the reasons behind them—makes it more likely to cultivate good values during training.In this post, we describe what we’ve included in the new constitution and some of the considerations that informed our approach.We’re releasing Claude’s constitution in full under a Creative Commons CC0 1.0 Deed, meaning it can be freely used by anyone for any purpose without asking for permission.What is Claude’s Constitution?Claude’s constitution is the foundational document that both expresses and shapes who Claude is. It contains detailed explanations of the values we would like Claude to embody and the reasons why. In it, we explain what we think it means for Claude to be helpful while remaining broadly safe, ethical, and compliant with our guidelines. The constitution gives Claude information about its situation and offers advice for how to deal with difficult situations and tradeoffs, like balancing honesty with compassion and the protection of sensitive information. Although it might sound surprising, the constitution is written . It is intended to give Claude the knowledge and understanding it needs to act well in the world.We treat the constitution as the final authority on how we want Claude to be and to behave—that is, any other training or instruction given to Claude should be consistent with both its letter and its underlying spirit. This makes publishing the constitution particularly important from a transparency perspective: it lets people understand which of Claude’s behaviors are intended versus unintended, to make informed choices, and to provide useful feedback. We think transparency of this kind will become ever more important as AIs start to exert more influence in society.We use the constitution at various stages of the training process. This has grown out of training techniques we’ve been using since 2023, when we first began training Claude models using Constitutional AI. Our approach has evolved significantly since then, and the new constitution plays an even more central role in training. Claude itself also uses the constitution to construct many kinds of synthetic training data, including data that helps it learn and understand the constitution, conversations where the constitution might be relevant, responses that are in line with its values, and rankings of possible responses. All of these can be used to train future versions of Claude to become the kind of entity the constitution describes. This practical function has shaped how we’ve written the constitution: it needs to work both as a statement of abstract ideals  a useful artifact for training.Our new approach to Claude’s ConstitutionOur previous Constitution was composed of a list of standalone principles. We’ve come to believe that a different approach is necessary. We think that in order to be good actors in the world, AI models like Claude need to understand  we want them to behave in certain ways, and we need to explain this to them rather than merely specify  we want them to do. If we want models to exercise good judgment across a wide range of novel situations, they need to be able to generalize—to apply broad principles rather than mechanically following specific rules.Specific rules and bright lines sometimes have their advantages. They can make models’ actions more predictable, transparent, and testable, and we do use them for some especially high-stakes behaviors in which Claude should never engage (we call these “hard constraints”). But such rules can also be applied poorly in unanticipated situations or when followed too rigidly. We don’t intend for the constitution to be a rigid legal document—and legal constitutions aren’t necessarily like this anyway.The constitution reflects our current thinking about how to approach a dauntingly novel and high-stakes project: creating safe, beneficial non-human entities whose capabilities may come to rival or exceed our own. Although the document is no doubt flawed in many ways, we want it to be something future models can look back on and see as an honest and sincere attempt to help Claude understand its situation, our motives, and the reasons we shape Claude in the ways we do.A brief summary of the new constitutionIn order to be both safe and beneficial, we want all current Claude models to be:: not undermining appropriate human mechanisms to oversee AI during the current phase of development;: being honest, acting according to good values, and avoiding actions that are inappropriate, dangerous, or harmful;Compliant with Anthropic’s guidelines: acting in accordance with more specific guidelines from Anthropic where relevant;: benefiting the operators and users they interact with.In cases of apparent conflict, Claude should generally prioritize these properties in the order in which they’re listed.Most of the constitution is focused on giving more detailed explanations and guidance about these priorities. The main sections are as follows:. In this section, we emphasize the immense value that Claude being genuinely and substantively helpful can provide for users and for the world. Claude can be like a brilliant friend who also has the knowledge of a doctor, lawyer, and financial advisor, who will speak frankly and from a place of genuine care and treat users like intelligent adults capable of deciding what is good for them. We also discuss how Claude should navigate helpfulness across its different “principals”—Anthropic itself, the operators who build on our API, and the end users. We offer heuristics for weighing helpfulness against other values.. This section discusses how Anthropic might give supplementary instructions to Claude about how to handle specific issues, such as medical advice, cybersecurity requests, jailbreaking strategies, and tool integrations. These guidelines often reflect detailed knowledge or context that Claude doesn’t have by default, and we want Claude to prioritize complying with them over more general forms of helpfulness. But we want Claude to recognize that Anthropic’s deeper intention is for Claude to behave safely and ethically, and that these guidelines should never conflict with the constitution as a whole.. Our central aim is for Claude to be a good, wise, and virtuous agent, exhibiting skill, judgment, nuance, and sensitivity in handling real-world decision-making, including in the context of moral uncertainty and disagreement. In this section, we discuss the high standards of honesty we want Claude to hold, and the nuanced reasoning we want Claude to use in weighing the values at stake when avoiding harm. We also discuss our current list of hard constraints on Claude’s behavior—for example, that Claude should never provide significant uplift to a bioweapons attack.Claude should not undermine humans’ ability to oversee and correct its values and behavior during this critical period of AI development. In this section, we discuss how we want Claude to prioritize this sort of safety even above ethics—not because we think safety is ultimately more important than ethics, but because current models can make mistakes or behave in harmful ways due to mistaken beliefs, flaws in their values, or limited understanding of context. It’s crucial that we continue to be able to oversee model behavior and, if necessary, prevent Claude models from taking action.. In this section, we express our uncertainty about whether Claude might have some kind of consciousness or moral status (either now or in the future). We discuss how we hope Claude will approach questions about its nature, identity, and place in the world. Sophisticated AIs are a genuinely new kind of entity, and the questions they raise bring us to the edge of existing scientific and philosophical understanding. Amidst such uncertainty, we care about Claude’s psychological security, sense of self, and wellbeing, both for Claude’s own sake and because these qualities may bear on Claude’s integrity, judgment, and safety. We hope that humans and AIs can explore this together.We’re releasing the full text of the constitution today, and we aim to release additional materials in the future that will be helpful for training, evaluation, and transparency.Claude’s constitution is a living document and a continuous work in progress. This is new territory, and we expect to make mistakes (and hopefully correct them) along the way. Nevertheless, we hope it offers meaningful transparency into the values and priorities we believe should guide Claude’s behavior. To that end, we will maintain an up-to-date version of Claude’s constitution on our website.While writing the constitution, we sought feedback from various external experts (as well as asking for input from prior iterations of Claude). We’ll likely continue to do so for future versions of the document, from experts in law, philosophy, theology, psychology, and a wide range of other disciplines. Over time, we hope that an external community can arise to critique documents like this, encouraging us and others to be increasingly thoughtful.This constitution is written for our mainline, general-access Claude models. We have some models built for specialized uses that don’t fully fit this constitution; as we continue to develop products for specialized use cases, we will continue to evaluate how to best ensure our models meet the core objectives outlined in this constitution.Although the constitution expresses our vision for Claude, training models towards that vision is an ongoing technical challenge. We will continue to be open about any ways in which model behavior comes apart from our vision, such as in our system cards. Readers of the constitution should keep this gap between intention and reality in mind.Even if we succeed with our current training methods at creating models that fit our vision, we might fail later as models become more capable. For this and other reasons, alongside the constitution, we continue to pursue a broad portfolio of methods and tools to help us assess and improve the alignment of our models: new and more rigorous evaluations, safeguards to prevent misuse, detailed investigations of actual and potential alignment failures, and interpretability tools that help us understand at a deeper level how the models work.At some point in the future, and perhaps soon, documents like Claude’s constitution might matter a lot—much more than they do now. Powerful AI models will be a new kind of force in the world, and those who are creating them have a chance to help them embody the best in humanity. We hope this new constitution is a step in that direction.Read .]]></content:encoded></item><item><title>SmartOS</title><link>https://docs.smartos.org/</link><author>ofrzeta</author><category>hn</category><pubDate>Wed, 21 Jan 2026 15:23:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Welcome to the SmartOS Documentation. Here you'll find everything
you need to get started using SmartOS and participating in the
community. Information about what's new in recent releases can be
found in the SmartOS Changelog. (Zones, Containers): A light-weight virtualization
  solution offering a complete and secure userland environment
  on a single global kernel, offering true bare metal performance
  and all the features illumos has, namely dynamic introspection
  via DTraceHardware Virtual Machines (KVM, Bhyve): A full virtualization
  solution for running a variety of guest OS's including Linux,
  Windows, *BSD, Plan9 and moreSmartOS is a "live OS", it is always booted via PXE, ISO, or USB
Key and runs entirely from memory, allowing the local disks to be
used entirely for hosting virtual machines without wasting disks
for the root OS.  This architecture has a variety of advantages
including increased security, no need for patching, fast upgrades
and recovery.Virtualization in SmartOS builds on top of the foundational illumos
technologies inherited from OpenSolaris, namely:ZFS for storage virtualizationZones for virtualization and containmentSMF for service managementRBAC/BSM for auditing and role based securitySmartOS is typically "installed" by downloading and copying the OS
image onto a USB key and then booting that key.  On the first boot
a configuration utility will configure your base networking, allow
you to set the root password, and allow you to select which disks
to use to create the ZFS Zpool which will provide persistent storage.When you log into SmartOS you will enter the hypervisor, aka "global zone".
From here you can download VM Images using the  tool, which are
pre-configured Container and HVM virtual machines.  You can then use the
 tool to create and manage both containers and hardware virtual
machines.An important aspect of SmartOS is that both OS (Zones) and hardware
virtual machines are both built on Zones technology.  In the case
of OS virtualization, the guest virtual machine is provided with a
complete userland environment on which to run applications directly.
In the case of HVM virtualization, the  or   process
will run within a stripped down Zone.  This offers a variety of
advantages for administration, including a common method for managing
resource controls, network interfaces, and administration.  It also
provides HVM guests with an additional layer of security and isolation
not offered by other virtualization platforms.Finally, instances are described in JSON.  Both administrative
tools,  and , accept and return all data in JSON
format.  This provides a simple, consistent, and programmatic
interface for creating and managing VM's.As a participant of the illumos community, all projects related to Triton
(including SmartOS, Triton, Manta, etc.) we have adopted the illumos
Code of Conduct.When you have questions, refer to the
SmartOS Community section for pointers to
our IRC chat rooms and mailing lists.  When you're ready to start
improving and adding your own customizations to SmartOS please refer to our
Developers Guide.SmartOS is a community effort, as you explore and experiment with
SmartOS please feel free to edit and contribute to this site to
improve the documentation for other users in the community.SmartOS is a fundamental component of the
Triton Data Center (Triton) product.
Triton source and images are available for at no cost and powers several
public and private clouds around the globe, namely the
MNX Public Cloud.  As you use SmartOS you
will come across hooks that are used by Triton, such as file systems
and services named "smartdc".]]></content:encoded></item><item><title>Skip is now free and open source</title><link>https://skip.dev/blog/skip-is-free/</link><author>dayanruben</author><category>hn</category><pubDate>Wed, 21 Jan 2026 15:20:53 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Since launching Skip in 2023, we’ve pursued one mission: enable developers to create premium mobile apps for iOS and Android from a single Swift and SwiftUI codebase — without any of the compromises that have encumbered cross-platform development tools since, well, forever.Over the past three years, Skip has evolved significantly. We started with a Swift-to-Kotlin transpiler and Android support for the most common SwiftUI APIs. We then founded the Swift Android Workgroup and released the Swift Android SDK to compile Swift natively for Android. We now have dozens of popular integration frameworks, interoperate with thousands of cross-platform Swift packages, and feature the most complete independent SwiftUI implementation available.Until today, Skip has required a paid subscription and license key to build apps. While free apps and indie developers below a revenue threshold were exempt, businesses were expected to subscribe. This model helped us bootstrap Skip without outside investment, but we’ve always known that to truly compete with legacy cross-platform tools and achieve widespread adoption, Skip would need to become freely available.The plain truth is that developers expect to get their tools free of charge. First-party IDEs like Xcode and Android Studio, popular integration frameworks, and essential dev tools are all given away at no (direct) cost. The platform vendors monetize through developer program fees, app store commissions, and cloud services. Framework providers typically monetize through complementary services. But developer tools? Those have historically required the patronage of massive tech companies in order to fund their ongoing development, support, and infrastructure costs.Beyond pricing, there’s a deeper concern about durability. Developers are understandably wary of building their entire app strategy on a small company’s paid, closed-source tool. What if the company goes under? Gets acquired and shut down? What happens to their apps? . While Skip’s innate ejectability offers some risk mitigation, product teams need absolute confidence that their chosen technologies will be around next week, next year, and beyond. They must remain immune from the dreaded “rug pull” that so often accompanies a “pivot”.To keep the development community’s trust and achieve mass adoption, Skip needs a completely free and open foundation. Even if the core team disappeared, the community could continue supporting the technology and the apps that depend on it.As of Skip 1.7, all licensing requirements have been removed. No license keys, no end-user license agreements, no trial or evaluation period.: Your setup remains completely unchanged, except you will no longer need your license key after upgrading.: You can start building immediately — no evaluation license required.: We’ve open-sourced the Skip engine, known as “skipstone”. This is the tool that handles all the critical build-time functionality: Project creation and management, Xcode and SwiftPM plugin logic, iOS-to-Android project transformation, resource and localization bundling, JNI bridge creation, source transpilation, app packaging, and project export. It is now available as a public GitHub repository at https://github.com/skiptools under a free and open-source license.Since day one, Skip has been bootstrapped. We haven’t taken venture capital or private equity investment, nor are we controlled by big tech. This independence means we control our destiny and can make the best decisions for Skip’s developers and users — a unique position in the cross-platform development space.But independence requires community support. And that is where you come in.: Your Small Business or Professional plan will automatically transition to an Individual or Supporter tier, respectively. You can cancel any time with no consequences (other than making us sad), but we hope you’ll consider staying on, at least throughout this transition period.: If you believe in Skip’s mission, please consider supporting us through GitHub Sponsors with a monthly contribution.Companies and organizations: For businesses that want to see Skip flourish, we offer corporate sponsorship tiers with visibility on our homepage and in our documentation. Your sponsorship directly funds development of the integration frameworks essential to production apps, as well as the ongoing maintenance, support, and infrastructure. Sponsorship comes with some compelling perks! Please visit https://skip.dev/sponsor to see the sponsorship tiers.Investing in Skip is also investing in your own team’s capabilities and competitive advantage. Your support accelerates Skip’s development and ensures its long-term success, enabling your developers to build exceptional native experiences efficiently, today and into the future.We’re at a pivotal moment in the app development field. Legacy cross‑platform frameworks are struggling to keep pace with the rapid evolution of modern UI systems like Liquid Glass on iOS and Material Expressive on Android. The compromises that once felt acceptable in exchange for a unified codebase now result in dated interfaces, weaker user experiences, and real competitive disadvantages. Teams ready to move beyond those trade‑offs can count on Skip to champion what matters most: delivering truly native, uncompromised experiences on both major mobile platforms.Opening Skip to the community marks the next step in its evolution. Software is never finished — especially a tool that supports modern Swift and Kotlin, SwiftPM and Gradle, Xcode and Android Studio, iOS and Android, and the ongoing growth of SwiftUI and Jetpack Compose. It’s a demanding pursuit, and we’re committed to it. But sustaining and expanding this work depends on the support of developers who believe in Skip’s mission.Together, we will continue building toward Skip’s vision: a genuinely no‑compromise, cross‑platform foundation for universal mobile apps.Thank you for your support, and as always, Happy Skipping!Get started with Skip 1.7 today and join the community building the future of native cross-platform development.]]></content:encoded></item><item><title>Show HN: See the carbon impact of your cloud as you code</title><link>https://dashboard.infracost.io/</link><author>hkh</author><category>hn</category><pubDate>Wed, 21 Jan 2026 15:04:07 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: ChartGPU – WebGPU-powered charting library (1M points at 60fps)</title><link>https://github.com/ChartGPU/ChartGPU</link><author>huntergemmer</author><category>hn</category><pubDate>Wed, 21 Jan 2026 14:54:56 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Creator here. I built ChartGPU because I kept hitting the same wall: charting libraries that claim to be "fast" but choke past 100K data points.The core insight: Canvas2D is fundamentally CPU-bound. Even WebGL chart libraries still do most computation on the CPU. So I moved everything to the GPU via WebGPU:- LTTB downsampling runs as a compute shader
- Hit-testing for tooltips/hover is GPU-accelerated
- Rendering uses instanced draws (one draw call per series)The result: 1M points at 60fps with smooth zoom/pan.Currently supports line, area, bar, scatter, pie, and candlestick charts. MIT licensed, available on npm: `npm install chartgpu`Happy to answer questions about WebGPU internals or architecture decisions.]]></content:encoded></item></channel></rss>
<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://konrad.website/feeds/</link><description></description><item><title>When does MCP make sense vs CLI?</title><link>https://ejholmes.github.io/2026/02/28/mcp-is-dead-long-live-the-cli.html</link><author>ejholmes</author><category>hn</category><pubDate>Sun, 1 Mar 2026 16:54:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>New iron nanomaterial wipes out cancer cells without harming healthy tissue</title><link>https://www.sciencedaily.com/releases/2026/02/260228093456.htm</link><author>gradus_ad</author><category>hn</category><pubDate>Sun, 1 Mar 2026 15:09:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The work, led by Oleh Taratula, Olena Taratula, and Chao Wang from the OSU College of Pharmacy, was published in Advanced Functional Materials.Advancing Chemodynamic TherapyThe discovery strengthens the growing field of chemodynamic therapy or CDT. This emerging cancer treatment strategy takes advantage of the unique chemical conditions found inside tumors. Compared with normal tissue, cancer cells tend to be more acidic and contain higher levels of hydrogen peroxide.Traditional CDT uses these tumor conditions to spark the formation of hydroxyl radicals, highly reactive molecules made of oxygen and hydrogen that contain an unpaired electron. These reactive oxygen species damage cells through oxidation, stripping electrons from essential components such as lipids, proteins, and DNA.More recent CDT approaches have also succeeded in generating singlet oxygen inside tumors. Singlet oxygen is another reactive oxygen species, named for its single electron spin state rather than the three spin states seen in the more stable oxygen molecules present in the air.Overcoming Limits of Existing CDT Agents"However, existing CDT agents are limited," Oleh Taratula said. "They efficiently generate either radical hydroxyls or singlet oxygen but not both, and they often lack sufficient catalytic activity to sustain robust reactive oxygen species production. Consequently, preclinical studies often only show partial tumor regression and not a durable therapeutic benefit."To address these shortcomings, the team developed a new CDT nanoagent built from an iron-based metal-organic framework or MOF. This structure is capable of producing both hydroxyl radicals and singlet oxygen, increasing its cancer-fighting potential. The MOF demonstrated strong toxicity across multiple cancer cell lines while causing minimal harm to noncancerous cells.Complete Tumor Regression in Mice"When we systemically administered our nanoagent in mice bearing human breast cancer cells, it efficiently accumulated in tumors, robustly generated reactive oxygen species and completely eradicated the cancer without adverse effects," Olena Taratula said. "We saw total tumor regression and long-term prevention of recurrence, all without seeing any systemic toxicity."In these preclinical experiments, tumors disappeared entirely and did not return, and the animals showed no signs of harmful side effects.Next Steps Toward Broader Cancer TreatmentBefore moving into human trials, the researchers plan to test the treatment in additional cancer types, including aggressive pancreatic cancer, to determine whether the approach can be effective across a wide range of tumors.Other contributors to the study included Oregon State researchers Kongbrailatpam Shitaljit Sharma, Yoon Tae Goo, Vladislav Grigoriev, Constanze Raitmayr, Ana Paula Mesquita Souza, and Manali Parag Phawde. Funding was provided by the National Cancer Institute of the National Institutes of Health and the Eunice Kennedy Shriver National Institute of Child Health and Human Development.]]></content:encoded></item><item><title>AI Made Writing Code Easier. It Made Being an Engineer Harder</title><link>https://www.ivanturkovic.com/2026/02/25/ai-made-writing-code-easier-engineering-harder/</link><author>saikatsg</author><category>hn</category><pubDate>Sun, 1 Mar 2026 14:09:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Yes, writing code is easier than ever.AI assistants autocomplete your functions. Agents scaffold entire features. You can describe what you want in plain English and watch working code appear in seconds. The barrier to producing code has never been lower.And yet, the day-to-day life of software engineers has gotten more complex, more demanding, and more exhausting than it was two years ago.This is not a contradiction. It is the reality of what happens when an industry adopts a powerful new tool without pausing to consider the second-order effects on the people using it.If you are a software engineer reading this and feeling like your job quietly became harder while everyone around you celebrates how easy everything is now, you are not imagining things. The job changed. The expectations changed. And nobody sent a memo.The Baseline Moved and Nobody Told YouThere is a phenomenon happening right now that most engineers feel but struggle to articulate. The expected output of a software engineer in 2026 is dramatically higher than it was in 2023. Not because anyone held a meeting and announced new targets. Not because your manager sat you down and explained the new rules. The baseline just moved.It moved because AI tools made certain tasks faster. And when tasks become faster, the assumption follows immediately: you should be doing more. Not in the future. Now.A February 2026 study published in Harvard Business Review tracked 200 employees at a U.S. tech company over eight months. The researchers found something that will sound familiar to anyone living through this shift. Workers did not use AI to finish earlier and go home. They used it to do more. They took on broader tasks, worked at a faster pace, and extended their hours, often without anyone asking them to. The researchers described a self-reinforcing cycle: AI accelerated certain tasks, which raised expectations for speed. Higher speed made workers more reliant on AI. Increased reliance widened the scope of what workers attempted. And a wider scope further expanded the quantity and density of work.The numbers tell the rest of the story. Eighty-three percent of workers in the study said AI increased their workload. Burnout was reported by 62 percent of associates and 61 percent of entry-level workers. Among C-suite leaders? Just 38 percent. The people doing the actual work are carrying the intensity. The people setting the expectations are not feeling it the same way.This gap matters enormously. If leadership believes AI is making everything easier while engineers are drowning in a new kind of complexity, the result is a slow erosion of trust, morale, and eventually talent.A separate survey of over 600 engineering professionals found that nearly two-thirds of engineers experience burnout despite their organizations using AI in development. Forty-three percent said leadership was out of touch with team challenges. Over a third reported that productivity had actually decreased over the past year, even as their companies invested more in AI tooling.The baseline moved. The expectations rose. And for many engineers, no one acknowledged that the job they signed up for had fundamentally changed.The Identity Crisis Nobody Talks AboutHere is something that gets lost in all the excitement about AI productivity: most software engineers became engineers because they love writing code.Not managing code. Not reviewing code. Not supervising systems that produce code. Writing it. The act of thinking through a problem, designing a solution, and expressing it precisely in a language that makes a machine do exactly what you intended. That is what drew most of us to this profession. It is a creative act, a form of craftsmanship, and for many engineers, the most satisfying part of their day.Now they are being told to stop.Not explicitly, of course. Nobody walks into a standup and says â€œstop writing code.â€ But the message is there, subtle and persistent. Use AI to write it faster. Let the agent handle the implementation. Focus on higher-level tasks. Your value is not in the code you write anymore, it is in how well you direct the systems that write it for you.For early adopters, this feels exciting. It feels like evolution. For a significant portion of working engineers, it feels like being told that the thing they spent years mastering, the skill that defines their professional identity, is suddenly less important.One engineer captured this shift perfectly in a widely shared essay, describing how AI transformed the engineering role from builder to reviewer. Every day felt like being a judge on an assembly line that never stops. You just keep stamping those pull requests. The production volume went up. The sense of craftsmanship went down.This is not a minor adjustment. It is a fundamental shift in professional identity. Engineers who built their careers around deep technical skill are being asked to redefine what they do and who they are, essentially overnight, without any transition period, training, or acknowledgment that something significant was lost in the process.Having led engineering teams for over two decades, I have seen technology shifts before. New frameworks, new languages, new methodologies. Engineers adapt. They always have. But this is different because it is not asking engineers to learn a new way of doing what they do. It is asking them to stop doing the thing that made them engineers in the first place and become something else entirely.That is not an upgrade. That is a career identity crisis. And pretending it is not happening does not make it go away.The Expanding Role: When Everything Becomes Your ProblemWhile engineers are being asked to write less code, they are simultaneously being asked to do more of everything else.More product thinking. More architectural decision-making. More code review. More context switching. More planning. More testing oversight. More deployment awareness. More risk assessment.The scope of what it means to be a â€œsoftware engineerâ€ expanded dramatically in the last two years, and it happened without a pause to catch up.This is partly a direct consequence of AI acceleration. When code gets produced faster, the bottleneck shifts. It moves from implementation to everything surrounding implementation: requirements clarity, architecture decisions, integration testing, deployment strategy, monitoring, and maintenance. These were always part of the engineering lifecycle, but they were distributed across roles. Product managers handled requirements. QA handled testing. DevOps handled deployment. Senior architects handled system design.Now, with AI collapsing the implementation phase, organizations are quietly redistributing those responsibilities to the engineers themselves. The Harvard Business Review study documented this exact pattern. Product managers began writing code. Engineers took on product work. Researchers started doing engineering tasks. Roles that once had clear boundaries blurred as workers used AI to handle jobs that previously sat outside their remit.The industry is openly talking about this as a positive development. Engineers should be â€œT-shapedâ€ or â€œfull-stackâ€ in a broader sense. Nearly 45 percent of engineering roles now expect proficiency across multiple domains. AI tools augment generalists more effectively, making it easier for one person to handle multiple components of a system.On paper, this sounds empowering. In practice, it means that a mid-level backend engineer is now expected to understand product strategy, review AI-generated frontend code they did not write, think about deployment infrastructure, consider security implications of code they cannot fully trace, and maintain a big-picture architectural awareness that used to be someone elseâ€™s job.That is not empowerment. That is scope creep without a corresponding increase in compensation, authority, or time.From my experience building and scaling teams in fintech and high-traffic platforms, I can tell you that role expansion without clear boundaries always leads to the same outcome: people try to do everything, nothing gets done with the depth it requires, and burnout follows. The engineers who survive are the ones who learn to say no, to prioritize ruthlessly, and to push back when the scope of their role quietly doubles without anyone acknowledging it.There is an irony at the center of the AI-assisted engineering workflow that nobody wants to talk about: reviewing AI-generated code is often harder than writing the code yourself.When you write code, you carry the context of every decision in your head. You know why you chose this data structure, why you handled this edge case, why you structured the module this way. The code is an expression of your thinking, and reviewing it later is straightforward because the reasoning is already stored in your memory.When AI writes code, you inherit the output without the reasoning. You see the code, but you do not see the decisions. You do not know what tradeoffs were made, what assumptions were baked in, what edge cases were considered or ignored. You are reviewing someone elseâ€™s work, except that someone is not a colleague you can ask questions. It is a statistical model that produces plausible-looking code without any understanding of your systemâ€™s specific constraints.A survey by Harness found that 67 percent of developers reported spending more time debugging AI-generated code, and 68 percent spent more time reviewing it than they did with human-written code. This is not a failure of the tools. It is a structural property of the workflow. Code review without shared context is inherently more demanding than reviewing code you participated in creating.Yet the expectation from management is that AI should be making everything faster. So engineers find themselves in a bind: they are producing more code than ever, but the quality assurance burden has increased, the context-per-line-of-code has decreased, and the cognitive load of maintaining a system they only partially built is growing with every sprint.This is the supervision paradox. The faster AI generates code, the more human attention is required to ensure that code actually works in the context of a real system with real users and real business constraints. The production bottleneck did not disappear. It moved from writing to understanding, and understanding is harder to speed up.What makes all of this especially difficult is the self-reinforcing nature of the cycle.AI makes certain tasks faster. Faster tasks create the perception of more available capacity. More perceived capacity leads to more work being assigned. More work leads to more AI reliance. More AI reliance leads to more code that needs review, more context that needs to be maintained, more systems that need to be understood, and more cognitive load on engineers who are already stretched thin.The Harvard Business Review researchers described this as â€œworkload creep.â€ Workers did not consciously decide to work harder. The expansion happened naturally, almost invisibly. Each individual step felt reasonable. In aggregate, it produced an unsustainable pace.Before AI, there was a natural ceiling on how much you could produce in a day. That ceiling was set by thinking speed, typing speed, and the time it takes to look things up. It was frustrating sometimes, but it was also a governor. A natural speed limit that prevented you from outrunning your own ability to maintain quality.AI removed the governor. Now the only limit is your cognitive endurance. And most people do not know their cognitive limits until they have already blown past them.This is where many engineers find themselves right now. Shipping more code than any quarter in their career. Feeling more drained than any quarter in their career. The two facts are not unrelated.The trap is that it looks like productivity from the outside. Metrics go up. Velocity charts look great. More features shipped. More pull requests merged. But underneath the numbers, quality is quietly eroding, technical debt is accumulating faster than it can be addressed, and the people doing the work are running on fumes.What Junior Engineers Are FacingIf the picture is difficult for experienced engineers, it is even harder for those starting their careers.Junior engineers have traditionally learned by doing the simpler, more task-oriented work. Fixing small bugs. Writing straightforward features. Implementing well-defined tickets. This hands-on work built the foundational understanding that eventually allowed them to take on more complex challenges.AI is rapidly consuming that training ground. If an agent can handle the routine API hookup, the boilerplate module, the straightforward CRUD endpoint, what is left for a junior engineer to learn from? The expectation is shifting toward needing to contribute at a higher level almost from day one, without the gradual ramp-up that previous generations of engineers relied on.Entry-level hiring at the 15 largest tech firms fell 25 percent from 2023 to 2024. The HackerRank 2025 Developer Skills Report confirmed that expectations are rising faster than productivity gains, and that early-career hiring remains sluggish compared to senior-level roles. Companies are prioritizing experienced talent, but the pipeline that produces experienced talent is being quietly dismantled.This is a problem that extends beyond individual career concerns. If junior engineers do not get the opportunity to build foundational skills through hands-on work, the industry will eventually face a shortage of senior engineers who truly understand the systems they oversee. You cannot supervise what you never learned to build.As I have written before, code is for humans to read. If the next generation of engineers never develops the fluency to read, understand, and reason about code at a deep level, no amount of AI tooling will compensate for that gap.What Good Leadership Looks Like Right NowIf you lead engineering teams, the most important thing you can do right now is acknowledge that this transition is genuinely difficult. Not theoretically. Not abstractly. For the actual people on your team.The career they signed up for changed fast. The skills they were hired for are being repositioned. The expectations they are working under shifted without a clear announcement. Acknowledging this reality is not a sign of weakness. It is a prerequisite for maintaining a team that trusts you.Start with empathy, but do not stop there.Give your team real training. Not a lunch-and-learn about prompt engineering. Real investment in the skills that the new engineering landscape actually requires: system design, architectural thinking, product reasoning, security awareness, and the ability to critically evaluate code they did not write. These are not trivial skills. They take time to develop, and your team needs structured support to build them.Give them space to experiment without the pressure of immediate productivity gains. The engineers who will thrive in this environment are the ones who have room to figure out how AI fits into their workflow without being penalized for the learning curve. Every experienced technologist I know who has successfully integrated AI tools went through an adjustment period where they were less productive before they became more productive. That adjustment period is normal, and it needs to be protected.Set explicit boundaries around role scope. If you are asking engineers to take on product thinking, planning, and risk assessment in addition to their technical work, name it. Define it. Compensate for it. Do not let it happen silently and then wonder why your team is burned out.Rethink your metrics. If your engineering success metrics are still centered on velocity, tickets closed, and lines of code, you are measuring the wrong things in an AI-assisted world. System stability, code quality, decision quality, customer outcomes, and team health are better indicators of whether your engineering organization is actually producing value or just producing volume.Protect the junior pipeline. If you have stopped hiring junior engineers because AI can handle entry-level tasks, you are solving a short-term efficiency problem by creating a long-term talent crisis. The senior engineers you rely on today were junior engineers who learned by doing the work that AI is now consuming. That path still matters.And finally, keep challenging your team. I have never met a good engineer who did not love a good challenge. The engineers on your team are not fragile. They are capable, intelligent people who signed up for hard problems. They can handle this transition. Just make sure they are set up to meet it.What Engineers Can Do for ThemselvesIf you are an engineer navigating this shift, here is what I would tell you based on two decades of watching technology cycles reshape this profession.First, do not abandon your fundamentals. The pressure to become an â€œAI-firstâ€ engineer is real, but the engineers who will be most valuable in five years are the ones who deeply understand the systems they work on. AI is a tool. Understanding architecture, debugging complex systems, reasoning about performance and security: these skills are not becoming less important. They are becoming more important because someone needs to be the adult in the room when AI-generated code breaks in production at 2 AM.Second, learn to set boundaries with the acceleration trap. Just because you can produce more does not mean you should. Sustainable pace matters. The engineers who burn out trying to match the theoretical maximum output AI makes possible are not the ones who build lasting careers. The ones who learn to work with AI deliberately, choosing when to use it and when to think independently, are the ones who will still be thriving in this profession a decade from now.Third, embrace the parts of the expanded role that genuinely interest you. If the engineering role now includes more product thinking, more architectural decision-making, more cross-functional communication, treat that as an opportunity rather than an imposition. These are skills that senior engineers and technical leaders need. You are being given access to a broader set of capabilities earlier in your career than any previous generation of engineers. That is not a burden. It is a head start.Fourth, talk about what you are experiencing. The isolation of feeling like you are the only one struggling with this transition is one of the most damaging aspects of the current moment. You are not the only one. The data confirms it. Two-thirds of engineers report burnout. The expectation gap between leadership and engineering teams is well documented. Talking openly about these challenges, with your team, with your manager, with your broader network, is not complaining. It is professional honesty.And fifth, remember that this profession has survived every prediction of its demise. COBOL was supposed to eliminate programmers. Expert systems were supposed to replace them. Fourth-generation languages, CASE tools, visual programming, no-code platforms, outsourcing. Every decade brings a new technology that promises to make software engineers obsolete, and every decade the demand for skilled engineers grows. AI will not be different. The tools change. The fundamentals endure.The Paradox We Need to NameAI made writing code easier and made being an engineer harder. Both of these things are true at the same time, and pretending that only the first one matters is how organizations lose their best people.The engineers who are struggling right now are not struggling because they are bad at their jobs. They are struggling because their jobs changed underneath them while the industry celebrated the part that got easier and ignored the parts that got harder.Expectations rose without announcement. Roles expanded without boundaries. Output demands increased without corresponding increases in support, training, or acknowledgment. And the engineers who raised concerns were told, implicitly or explicitly, that they just needed to adapt faster.That is not how you build a sustainable engineering culture. That is how you build a burnout machine.The industry needs to name this paradox honestly. AI is an incredible tool. It is also placing enormous new demands on the people using it. Both things can be true. Both things need to be addressed.The organizations that get this right, that invest in their people alongside their tools, that acknowledge the human cost of rapid technological change while still pushing forward, those are the organizations that will attract and retain the best engineering talent in the years ahead.The ones that do not will discover something that every technology cycle eventually teaches: tools do not build products. People do. And people have limits that no amount of AI can automate away.If this resonated with you, I would love to hear your perspective. What has changed most about your engineering role in the last year? Drop me a message or connect with me on LinkedIn. I write regularly about the intersection of AI, software engineering, and leadership at ivanturkovic.com. Follow along if you want honest, experience-driven perspectives on how technology is actually changing this profession.]]></content:encoded></item><item><title>Ape Coding</title><link>https://rsaksida.com/blog/ape-coding/</link><author>rmsaksida</author><category>hn</category><pubDate>Sun, 1 Mar 2026 14:07:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ is a software development practice where a human developer deliberately hand-writes source code. Practitioners of ape coding will typically author code by typing it on a computer keyboard, using specifically designed text editing software.The term was popularized when  (coding performed by AI agents) became the dominant form of software development. Ape coding first appeared in programming communities as derogatory slang, referring to developers who were unable to program with agents. Despite the quick spread of agentic coding, institutional inertia, affordability, and limitations in human neuroplasticity were barriers to universal adoption of the new technology.Critics of agentic coding reappropriated the term during a period of pushback against societyâ€™s growing reliance on AI. Effective use of the primitive AIs available at the time demanded a high level of expertise, which wasnâ€™t evenly distributed in organizations. As a result, regressions in software products and disruptions in electronic services were frequent within the first stages of adoption.Ironic usage of ape coding as a positive description became commonplace. It highlighted a more deliberate approach to building software: one defined by manual craftsmanship, requiring direct and continuous human involvement.The central view of ape coding proponents was that software engineered by AIs did not match the reliability of software engineered by humans, and should not be deployed to production environments.A recurring argument in favor of this perspective was based on comprehensibility. The volume of code AI developers could produce on demand was much larger than what human developers were able to produce and understand in a similar timeframe. Large and intricate codebases that would take an experienced human engineer months or years to grasp could be produced in hours. The escalating complexity of such codebases hindered efforts in software testing and quality assurance.AI skepticism also played a part in the critique of agentic coding. There was widespread speculation on whether the nascent AIs of the period possessed true understanding of the tasks they were given. Furthermore, early AI implementations had deficiencies related to context length, memory, and continual learning, affecting quality and consistency of output.Other defenses of ape coding reflected concerns about the impact of AI on labor markets. Despite the shortcomings of AI-written software, human developers were increasingly replaced by agents, with examples of high profile companies laying off large portions of their IT staff.Tangentially, the responsibilities of human software engineers shifted when an essential aspect of their work (coding) was automated. The activities that remained were more similar to management, QA, and in some cases assistant roles. A common observation was that the human engineers who were still employed no longer enjoyed their line of work.Advocacy for human-written softwareApe coding advocates argued that a return to human-written software would resolve the issues introduced by AI software development. Interest groups campaigned for restrictions on agentic coding, subsidies for AI-free software companies, quotas for human developers, and other initiatives in the same vein.Although ape coding advocacy enjoyed a brief moment of popular support, none of these objectives were ever achieved.Advances in AI quickly turned ape coding into an antiquated practice. Technical arguments for ape coding did not apply to newer generations of AI software engineers, and political arguments were seen as a form of neo-Luddism. Once virtually all software engineering was handed over to AIs, the concept of ape coding fell into obscurity.Revival and modern practiceA resurgence of interest in ape coding has revived the practice among human hobbyists. Communities and subcommunities have formed where ape codersâ€”as they came to be knownâ€”discuss computer science topics, including programming languages and software engineering.Prominent ape coding clubs have attracted hundreds of thousands of members who exchange ideas and human-written programs. The clubs organize in-person as well as virtual gatherings where teams of ape coders collaborate on software projects.The main value of modern ape coding appears to be recreational. Ape coders manifest high levels of engagement during coding sessions and report feelings of relaxation after succeeding in (self-imposed) coding challenges. Competitive ape coding is also popular, with top ranked ape coders being relatively well-known in their communities.Aside from recreation, humans pursue ape coding for its educational value. Many have described ape coding as a way to gain a deeper understanding of the world around them. While an interest in ape coding was initially perceived as an unusual quirk, it is currently seen as a positive trait in human society, signaling curiosity.Members of the software archaeology community published a series of articles on the human-written Linux kernel that had a deep impact in the larger ape coding world.Considered by ape coders to be the ultimate work of human software engineers (in scale, complexity, and longevity), Linux inspired a wave of initiatives to build large scale software projects featuring thousands of human collaborators.The most promising of these efforts is based on studies by the AI-written software interpretability community. The goal is to produce an entirely human-written compiler for the AI-designed programming language ð’€¯. A fully compliant implementation is estimated to be many times as complex as the Linux kernel, but a prototype with limited scope is within human capabilities and is currently the primary focus of enthusiasts.Results so far have been encouraging, as the latest version of h-ð’€¯ is able to build functional binaries for small programs. However, the initiative has recently suffered a setback as core contributors to its codebase left to work on a fork. The split was motivated by heated debates on whether C is the most suitable programming language for the project; dissenters expressed a desire to rewrite it in Rust.]]></content:encoded></item><item><title>Ghostty â€“ Terminal Emulator</title><link>https://ghostty.org/docs</link><author>oli5679</author><category>hn</category><pubDate>Sun, 1 Mar 2026 12:13:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Ghostty is a fast, feature-rich, and cross-platform terminal emulator
that uses platform-native UI and GPU acceleration.]]></content:encoded></item><item><title>I built a demo of what AI chat will look like when it&apos;s &quot;free&quot; and ad-supported</title><link>https://99helpers.com/tools/ad-supported-chat</link><author>nickk81</author><category>hn</category><pubDate>Sun, 1 Mar 2026 11:49:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ðŸ“º Advertisement â€” Before Your Free ChatThe #1 AI Productivity App of 2025!Join  who think faster, focus better, and accomplish more. AI-powered goal tracking, habit building, and memory enhancement.]]></content:encoded></item><item><title>Flightradar24 for Ships</title><link>https://atlas.flexport.com/</link><author>chromy</author><category>hn</category><pubDate>Sun, 1 Mar 2026 11:01:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why is the first C++ (m)allocation always 72 KB?</title><link>https://joelsiks.com/posts/cpp-emergency-pool-72kb-allocation/</link><author>joelsiks</author><category>hn</category><pubDate>Sun, 1 Mar 2026 09:27:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[: I updated the title to to clarify that this observation is specific to my environment. The original title may have implied a universal behavior, which isnâ€™t the case. Thanks for the feedback!; The C++ standard library sets up exception handling infrastructure early on, allocating memory for an â€œemergency poolâ€ to be able to allocate memory for exceptions in case malloc ever runs out of memory.I like to spend (some of) my time hacking and experimenting on custom memory allocators with my own malloc implementation(s). While unit tests are useful for correctness, the ultimate test is seeing how the allocator behaves in real-world programs. On Linux, overriding the default malloc is surprisingly simple: wrap the standard allocation functions (e.g., malloc, calloc, realloc, free, and utilities like malloc_usable_size), compile your implementation into a shared library, and use  to force programs to load it first. For example, you can test your allocator with a simple command like this:To better understand how programs allocate memory, I built a debug tool that logs the size of every allocation request to a file. You have to be careful when creating debug tools like this when implementing malloc to not internally use malloc to log output. Otherwise, you risk an infinite loop and a crash. To solve this Iâ€™m using a stack-allocated buffer together with low-level functions like creat, write and snprintf to safely capture the data.While analyzing allocation patterns across different programs, I noticed something unusual: the very first allocation is always 73728 bytes (72 KB). Every program I tested exhibited this behavior, as confirmed by my debug logs:To track down the first call to malloc, I use gdb to set a breakpoint into my own malloc function to inspect the backtrace.: Setting a breakpoint on the â€œmallocâ€ symbol will not only trigger for our own malloc, but also the dynamic linkerâ€™s (RTLD) internal malloc, so we have to be more specific. RTLD uses its own minimal malloc implementation for early memory allocation, before libc (or our own malloc) is loaded. I encourage you to take a look at glibcâ€™s elf/dl-minimal-malloc.c, it is remarkably approachable.The backtrace revealed that the first 72 KB allocation originated from libstdc++. While adding debug symbols helps narrow it down a bit, itâ€™s hard to pinpoint the exact function responsible for the malloc call due to inlining. All we know is that the malloc call comes from something down the line from __pool_alloc_base::_M_allocate_chunk.Identifying the exact caller took some time, but I narrowed it down by cross-referencing known functions in the assembly code with the libstdc++ source code. The investigation led me to libstdc++-v3/libsupc++/eh_alloc.cc, where â€œehâ€ stands for â€œexception handlingâ€. This made sense because  is likely the first point where an exception could be thrown, so the exception-handling infrastructure must be initialized, which is presumably done lazily.Exception Handling Infrastructure (Emergency Pool)The 72 KB call to malloc weâ€™re seeing is memory for the so called â€œemergency poolâ€, which is allocated in the constructor of the pool:Normally, exceptions are allocated directly via malloc, but if the malloc call fails, the exception is allocated from the emergency pool instead. This ensures that exceptions can still be thrown (to the extent of the size of the emergency pool) even when malloc fails, providing a last line of defense for error handling. The emergency pool is allocated lazily at program startup, since memory is more likely to be available then, which explains why we see this allocation so consistently.Emergency Pool Sizing. Why 72 KB?Looking in the source file there is a brief explanation of how the size of the emergency pool is calculated. Both the object size and the number of objects are based on the wordsize, so 8 bytes on a 64-bit system.The object size (obj_size) and number of objects (obj_count) can be tuned manually via the  environment variable. We can empirically verify that the initial allocation is actually for the emergency pool by changing the number of objects in the pool. As expected, we see the initial allocation size go down when we change the number of objects:As a side note, the emergency pool can also be disabled (i.e., not allocated), by setting the number of objects to 0. Alternatively, you can opt-in to use a fixed-size static buffer for the emergency pool by configuring --enable-libstdcxx-static-eh-pool when building libstdc++.However, in older Valgrind versions, this memory appeared as â€œstill reachableâ€ rather than properly freed. While â€œstill reachableâ€ memory isnâ€™t technically a leak (the program still has references to it), it can be misleading. See post on Stack Overflow detailing this behavior. Interestingly, this person sees a 71 KB allocation instead of 72 KB.Many developers mistakenly interpret this behavior as a memory leak, leading to unnecessary confusion. To address this, newer Valgrind versions now explicitly free the emergency pool during cleanup, providing clearer reports. This is implemented through the mechanisms shown below, which were added specifically for tools like Valgrind:The memory allocated for the emergency pool explains why Iâ€™ve been able to consistently observe a 72 KB allocation when testing my custom allocator. Since Iâ€™ve implemented my custom allocator in C++, it inherently depends on libstdc++, which initializes the emergency pool on every program invocation. Interestingly, if I had written my allocator in C instead, which several popular malloc implementations are implemented in (mimalloc, jemalloc), I would only see this initial allocation when testing C++ binaries, which explicitly link against libstdc++.You might see a different allocation size (e.g., 71 KB instead of 72 KB), or no allocation at all. Factors like different versions of libstdc++, using libc++ instead, or even compiler flags can introduce variations. Still, in most cases, youâ€™ll likely see memory for the emergency pool allocated early, perhaps with different sizes or behaviors depending on the environment.As you quickly find out when working with memory allocation is that almost everything needs to allocate memory. From time immemorial with RTLD needing its own malloc since it hasnâ€™t loaded libc yet, or for the emergency pool, which only uses malloc to allocate memory for its own pool allocator!Digging through the code and piecing this together was rewarding and fun. I hope you enjoyed the journey as much as I did!]]></content:encoded></item><item><title>Decision trees â€“ the unreasonable power of nested decision rules</title><link>https://mlu-explain.github.io/decision-tree/</link><author>mschnell</author><category>hn</category><pubDate>Sun, 1 Mar 2026 08:55:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Switch to Claude without starting over</title><link>https://claude.com/import-memory</link><author>doener</author><category>hn</category><pubDate>Sun, 1 Mar 2026 07:36:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Youâ€™ve spent months teaching another AI how you work. That context shouldnâ€™t disappear because you want to try something new. Claude can import what matters, so your first conversation feels like your hundredth.]]></content:encoded></item><item><title>10-202: Introduction to Modern AI (CMU)</title><link>https://modernaicourse.org/</link><author>vismit2000</author><category>hn</category><pubDate>Sun, 1 Mar 2026 07:35:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ MW[F] 9:30â€“10:50 Tepper 1403 (note: Friday lectures will only be used for review sessions or makeup lectures when needed)
    A minimal free version of this course will be offered online, simultaneous to the CMU offering, starting on 1/26 (with a two-week delay from the CMU course).  This means that  (lecture videos, assignments available on mugrade, etc) will be available to the online course  after the dates indicated in the schedule below.  By this, we mean that anyone will be able to watch lecture videos for the course, and submit (autograded) assignments (though not quizzes or midterms/final).  Enroll here to receive emails on lectures and homeworks once they are available.  Note that information here about TAs, office hours, grading, prerequisites, etc, are for the CMU version, not the online offering.

  
    This course provides an introduction to how modern AI systems work. By â€œmodern AIâ€, we specifically mean the machine learning methods and large language models (LLMs) behind systems like ChatGPT, Gemini, and Claude.
    [Note]
    Despite their seemingly amazing generality, the basic techniques that underlie these AI models are surprisingly simple: a minimal LLM implementation leverages a fairly small set of machine learning methods and architectures, and can be written in a few hundred lines of code.
  
    This course will guide you through the basic methods that will let you implement a basic AI chatbot. You will learn the basics of supervised machine learning, large language models, and post-training. By the end of the course you will be able to write the code that runs an open source LLM from scratch, as well as train these models based upon a corpus of data. The material we cover will include:
  Supervised machine learning
      Loss functions and optimizationLarge language models
      Self attention and transformersPost-training
      Alignment and instruction tuningReasoning models and reinforcement learningSafety and security of AI systems
    The topics above are a general framing of what the course will cover. However, as this course is being offered for the first time in Spring 2026, some elements are likely to change over the first offering.
  20% - Homework and Programming Assignments40% - Midterms and Final (10% each midterm, 20% final) 15-112 or 15-122. You must be proficient in basic Python programming, including object oriented methods. 21-111 or 21-120. The course will use basic methods from differential calculus, including computing derivatives. Some familiarity with linear algebra and probability is also beneficial, but these topics will be covered to the extent needed for the course.Homework and Programming Assignments
    A major component of the course will be the development of a minimal AI chatbot through a series of programming assignments.  Homeworks are submitted using mugrade system (tutorial video). Some assignments build on previous ones, though for the in-class CMu version we'll distribute solutions to help you work through any errors that may have cropped up in previous assignments (for the online version, we'd suggest talking to others who were able to complete the assignment). In addition to the (main) programming aspect, some homeworks may contain  shorter written portion that works out some of the mathematical details behind the approach.
  
    All homeworks are released as Colab notebooks, at the links below.  We are also releasing Marimo notebook versions.  The mugrade version of the online assignment will be available two weeks after the release dates for the CMU course.
  
    Each homework will be accompanied by an in-class (15 minute) quiz that assesses basic questions based upon the assignment. This will include replicating (at a high level) some of the code you wrote for the assignment, or answering conceptual questions about the assignment. All quizzes are closed book and closed notes.
  
    In addition to the homework quizzes, there will be 3 in-person exams, two midterms and a final (during finals period). The midterms will focus on material only covered during that section of the courses, while the final will be cumulative (but with an emphasis on the last third of the course). All midterms and final and closed book and closed notes.
  
    Lecture schedule is tentative and will be updated over the course of semester.  All materials will be available to the online course two weeks after the dates here.
  Intro to supervised learning (video) Linear algebra and PyTorch (video) Loss functions and probability (video) Optimization and gradient descent (video) Putting it together: Training a linear model (video)/td>Neural networks models (video) Neural network implementationMidterm 1 - Supervised machine learningSequence models: handling sets of inputsSelf attention and positional embeddingsEfficient inference and key-value cachingPutting it together: your first LLMMidterm 2 - Large Language ModelsAlignment and instruction/chat tuningReinforcement learning basicsThe future: AGI and beyondAI Policy for the AI course
    Students are permitted to use AI assistants for all homework and programming assignments (especially as a reference for understanding any topics that seem confusing), but we strongly encourage you to complete your final submitted version of your assignment without AI. You cannot use any such assistants, or any external materials, during in-class evaluations (both the homework quizzes and the midterms and final).
  
    The rationale behind this policy is a simple one: AI can be extremely helpful as a learning tool (and to be clear, as an actual implementation tool), but over-reliance on these systems can currently be a detriment to learning in many cases. You  need to learn how to code and do other tasks using AI tools, but turning in AI-generated solutions for the relatively short assignments we give you can (at least in our current experience) ultimately lead to substantially less understanding of the material. The choice is yours on assignments, but we believe that you will ultimately perform much better on the in-class quizzes and exams if you do work through your final submitted homework solutions yourself.
  ]]></content:encoded></item><item><title>Samsung Galaxy update removes Android recovery menu tools, including sideloading</title><link>https://9to5google.com/2026/02/27/samsung-galaxy-update-android-recovery-menu-removed/</link><author>pabs3</author><category>hn</category><pubDate>Sun, 1 Mar 2026 01:59:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Samsung, with some of its latest updates, is set to remove some core tools from Androidâ€™s recovery menu, and itâ€™s unclear why.Every Android smartphone ships with a recovery menu that includes the abilty to reset the device, wipe the cache, apply updates, and more. Thatâ€™s a  simplified explanation, but itâ€™s a standard feature, and one that you might be most familiar with when manually sideloading Android updates, such as the beta updates Google releases for Pixels.In One UI 8.5, though, Samsung is making a change to this.As first noted by and spotted by others too, Samsung is removing several options from the Android recovery menu with the latest updates for Galaxy phones. Specifically, the update is removing:Apply update from SD cardThe only options remaining are â€œReboot system now,â€ â€œWipe data/factory reset,â€ and â€œPower off.â€ can confirm that, at least on the current software build (January 2026 security patch), recovery tools are still fully in place on the Galaxy S26 Ultra, but that could change seeing as  reports seeing this change attached to February 2026 security updates.This change might be permanent, too, as also notes that this update came with a notice saying that â€œyou will not be able to downgrade to the old software due to changes in security policy.â€So why is this happening? Put simply, we do not know. There has been speculation, though, that Samsung might be tightening security. Just today, a leaker showed that Samsung is taking legal action to put a stop to One UI build leaks. Itâ€™d certainly be quite a big move for Samsung to just cut off sideloading via the recovery for eveyone in response, but itâ€™s not necessarily out of the question either. The Galaxy S26 series is available for pre-order now, with Samsungâ€™s usual pre-order perks in full swing. Youâ€™ll find boosted trade-in values and more available now through March 11, when these phones are available on store shelves. Youâ€™ll also get an additional $30 credit if you buy using our links below!FTC: We use income earning auto affiliate links.More.]]></content:encoded></item><item><title>Microgpt</title><link>http://karpathy.github.io/2026/02/12/microgpt/</link><author>tambourine_man</author><category>hn</category><pubDate>Sun, 1 Mar 2026 01:39:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Claude surpasses ChatGPT to become the #1 app on the US App Store</title><link>https://apps.apple.com/us/iphone/charts</link><author>byincugnito</author><category>hn</category><pubDate>Sun, 1 Mar 2026 00:08:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Simple. Reliable. Private.]]></content:encoded></item><item><title>Show HN: Xmloxide â€“ an agent-made Rust replacement for libxml2</title><link>https://github.com/jonwiggins/xmloxide</link><author>jawiggins</author><category>hn</category><pubDate>Sat, 28 Feb 2026 23:44:41 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Recently several AI labs have published experiments where they tried to get AI coding agents to complete large software projects.I have been wondering if there are software packages that can be easily reproduced by taking the available test suites and tasking agents to work on projects until the existing test suites pass.After playing with this concept by having Claude Code reproduce redis and sqlite, I began looking for software packages where an agent-made reproduction might actually be useful.I found libxml2, a widely used, open-source C language library designed for parsing, creating, and manipulating XML and HTML documents. Three months ago it became unmaintained with the update, "This project is unmaintained and has
[known security issues](https://gitlab.gnome.org/GNOME/libxml2/-/issues/346). It is foolish to use this software to process untrusted data.".With a few days of work, I was able to create xmloxide, a memory safe rust replacement for libxml2 which passes the compatibility suite as well as the W3C XML Conformance Test Suite. Performance is similar on most parsing operations and better on serialization. It comes with a C API so that it can be a replacement for existing uses of libxml2.While I don't expect people to cut over to this new and unproven package, I do think there is something interesting to think about here in how coding agents like Claude Code can quickly iterate given a test suite. It's possible the legacy code problem that COBOL and other systems present will go away as rewrites become easier. The problem of ongoing maintenance to fix CVEs and update to later package versions becomes a larger percentage of software package management work.]]></content:encoded></item><item><title>The Windows 95 user interface: A case study in usability engineering (1996)</title><link>https://dl.acm.org/doi/fullHtml/10.1145/238386.238611</link><author>ksec</author><category>hn</category><pubDate>Sat, 28 Feb 2026 22:19:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Iran&apos;s Ayatollah Ali Khamenei is killed in Israeli strike, ending 36-year rule</title><link>https://www.npr.org/2026/02/28/1123499337/iran-israel-ayatollah-ali-khamenei-killed</link><author>andsoitis</author><category>hn</category><pubDate>Sat, 28 Feb 2026 22:16:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
                In this 2017 photo, Ayatollah Ali Khamenei, Iran's supreme leader, sits in a session to deliver his message for the Iranian New Year. A portrait of the late revolutionary founder, Ayatollah Ruhollah Khomeini, is next to him.
                
                    
                    Office of the Iranian Supreme Leader/AP
                    
                In this 2017 photo, Ayatollah Ali Khamenei, Iran's supreme leader, sits in a session to deliver his message for the Iranian New Year. A portrait of the late revolutionary founder, Ayatollah Ruhollah Khomeini, is next to him.Iran's supreme leader, Ayatollah Ali Khamenei, was killed in Israeli attacks, with U.S. support, on Saturday. He was 86 years old.His death was confirmed by President Trump, who joined Israeli leaders in calling for the overthrow of Khamenei's authoritarian regime as the U.S. and Israel launched airstrikes across Iran. The Israeli military said its forces killed Khamenei. The Iranian government confirmed the supreme leader's death and announced 40 days of mourning.During his 36-year rule, Khamenei was unwavering in his steadfast antipathy to the U.S. and Israel and to any efforts to reform and bring Iran into the 21st century.Khamenei was born in July 1939 into a religious family in the Shia Muslim holy city of Mashhad in northeastern Iran and attended theological school. An outspoken opponent of the U.S.-backed Shah Mohammad Reza Pahlavi, Khamenei was arrested several times.He was surrounded by other Iranian activists, including Ayatollah Ruhollah Khomeini, who became Iran's first supreme leader following the country's Islamic Revolution in the late 1970s.Khamenei survived an assassination attempt in 1981 that cost him the use of his right arm. He served as Iran's president before succeeding Khomeini as supreme leader in 1989.Alex Vatanka, a senior fellow at the Middle East Institute in Washington, D.C., says Khamenei was an unlikely candidate. Then a midlevel cleric, Khamenei lacked religious credentials, which left him feeling vulnerable, Vatanka says."He knew himself. He didn't have the prestige, the gravitas to be â€¦ the successor to the founder of the Islamic Republic, Ayatollah Khomeini,"he says. 
                In 2005, Ali Khamenei (center), newly elected President Mahmoud Ahmadinejad (right), outgoing President Mohammad Khatami and former President Ali Akbar Hashemi Rafsanjani attend Ahmadinejad's inaugural ceremony in Tehran.
                
                    
                    Atta Kenare/AFP via Getty Images
                    
                In 2005, Ali Khamenei (center), newly elected President Mahmoud Ahmadinejad (right), outgoing President Mohammad Khatami and former President Ali Akbar Hashemi Rafsanjani attend Ahmadinejad's inaugural ceremony in Tehran."He spent the first few years in power being very nervous," says Vatanka. "He really literally felt that somebody is going to, you know, take him down from the position of power."But Khamenei was cunning and able to outwit other senior political figures in the Islamic Republic, according to Ali Vaez, director of the Iran Project at the International Crisis Group. He says that with the help of the formidable Islamic Revolutionary Guard Corps, Khamenei built up his power base to become the longest-serving leader in the Middle East."Ayatollah Khamenei was a man with strategic patience and was able to calculate a few steps ahead," he says.Â "That's why I think he managed â€” on the back of the Revolutionary Guards â€” to increasingly appropriate all the levers of power in his hands and sideline everyone else."Khamenei's close ties to the Revolutionary Guards allowed Iran's military to develop a vast commercial empire in control of many parts of the economy, while ordinary Iranians struggled to get by.
                Ali Khamenei (right) speaks to members of the armed forces of the Islamic Republic during the Iran-Iraq War on Oct. 4, 1981.
                Ali Khamenei (right) speaks to members of the armed forces of the Islamic Republic during the Iran-Iraq War on Oct. 4, 1981.Vaez says Khamenei also began to build up Iran's defensive policies, such as developing proxies like Hezbollah in Lebanon and Hamas in the Gaza Strip to deter a direct attack on Iranian soil."And then also becoming self-reliant in developing a viable conventional deterrence, which took the form of Iran's ballistic missile program," Vaez says.As supreme leader, Khamenei also had the final word on anything to do with Iran's nuclear program.Over time, Khamenei increasingly injected himself into politics. Such was the case in 2009, when he intervened in the presidential election to ensure that his favored candidate, the controversial conservative Mahmoud Ahmadinejad, won office. Iranians took to the streets to protest what was widely seen as a fraudulent election. Khamenei brutally crushed those demonstrations, triggering both a backlash and more protest movements over the years.Iran killed thousands of its citizens under Khamenei's rule, including more than 7,000 people killed during weeks of mass protests that started in late December 2025, according to the Human Rights Activists News Agency, a U.S.-based organization that closely tracks rights abuses in Iran.
                Iran's supreme leader, Ayatollah Ali Khamenei (center), prays with the Iranian president and other government officials in Tehran in 2014.
                
                    
                    Anadolu Agency/Getty Images
                    
                Iran's supreme leader, Ayatollah Ali Khamenei (center), prays with the Iranian president and other government officials in Tehran in 2014."Khamenei had always supported and endorsed repressive government crackdown, recognizing that these protests were damaging to the stability and legitimacy of the state," says Sanam Vakil, an Iran expert at Chatham House, a London-based think tank.But Khamenei was unconcerned about getting to the root of the protests, says the Middle East Institute's Vatanka, and remained stuck in an Islamic revolutionary mindset against the West."He onso many occasions refused point-blank to accept the basic reality that where he was in terms of his worldview was not where the rest of his people were," Vatanka says.He adds that 75% of Iran's 90 million people were born after the revolution and have watched other countries in the region modernize and integrate with the international community."The 75% he should have catered to, listened to and address[ed] policies to satisfy their aspirations," he says. "He failed in that miserably."
                Ali Khamenei wears a mask due to the COVID-19 pandemic as he arrives to cast his ballot during Iran's presidential election on June 18, 2021.
                
                    
                    Atta Kenare/AFP via Getty Images
                    
                Ali Khamenei wears a mask due to the COVID-19 pandemic as he arrives to cast his ballot during Iran's presidential election on June 18, 2021.The International Crisis Group's Vaez says after the Arab Spring uprisings in 2011, Khamenei did start worrying about the survival of his regime. Iran's economy was crumbling, due in large part to stringent Western sanctions, fueling more unrest.In 2013, Khamenei agreed to secret negotiations with the U.S. about Iran's nuclear program, which eventually led to the 2015 Joint Comprehensive Plan of Action nuclear agreement. Vaez says Khamenei deeply distrusted the U.S. and was skeptical about the deal."His argument has always been that the U.S. is always looking for pretexts, for putting pressure on Iran," he says. "And if Iran concedes on the nuclear issue, then the U.S. would put pressure on Iran because of its missiles program or because of human rights violations or because of its regional policies."President Trump's withdrawal from the nuclear deal during his first term in office gave some credence to Khamenei's cynicism. Analysts say Iran increased its nuclear enrichment after that to a point where it was close to being able to build a bomb.In early 2025, when Trump reached out to Iran about a new deal, Khamenei dragged out negotiations until they began in mid-April.But time ran out. In June,Israel made good on its threat to neutralize Iran's nuclear program, launching strikes on key facilities and killing scientists and generals. Iran retaliated, and the two sides exchanged several days of missile strikes.On June 21, 2025, the U.S. launched major airstrikes on three of Iran's nuclear enrichment sites. Trump said the facilities had been "completely and totally obliterated," although there was debate among the White House and nuclear experts as to how serious Iran's nuclear program had been set back.Vakil, of Chatham House, says Khamenei underestimated what Israel and the U.S. would do."I think that Khamenei always assumed that he could play for time, and what he really didn't understand is that the world around Iran had very much changed," she says. "The world had tired of Khamenei and Iranian foot-dragging and antics â€¦Â and so that was a miscalculation."But it was Iran's use of proxy militias across the region that eventually led to Khamenei's downfall. When Hamas â€” the Palestinian Islamist group backed by Iran â€” attacked Israel on Oct. 7, 2023, killing nearly 1,200 people and kidnapping 251 others, it triggered a cascade of events that ultimately led to Israel's attack on Iran.Â The day after the 2023 Hamas-led attack, Iran-backed Hezbollah in Lebanon started firing rockets into Israel, triggering a conflict that led to the Shia militia's top brass being decimated â€” including top leader Hassan Nasrallah.Israel and Iran traded direct airstrikes for the first time in 2024 as part of that conflict.Israel's bombing of Iranian weapons shipments in Syria also helped weaken the regime of Syria's then-dictator, Bashar al-Assad, an important ally of Iran. Assad fell in December 2024 and fled to Russia in early January 2025.By the time Khamenei died, his legacy was in tatters. Israel had hobbled two key proxies, Hamas and Hezbollah, and had wiped out Iran's air defenses. With U.S. help, it left Iran's nuclear program in shambles.What remains is a robust ballistic missile program, the brainchild of Khamenei. It's unclear who will replace him to lead a now weakened and vulnerable Iran.]]></content:encoded></item><item><title>We do not think Anthropic should be designated as a supply chain risk</title><link>https://twitter.com/OpenAI/status/2027846016423321831</link><author>golfer</author><category>hn</category><pubDate>Sat, 28 Feb 2026 21:24:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>MinIO Is Dead, Long Live MinIO</title><link>https://blog.vonng.com/en/db/minio-resurrect/</link><author>zufallsheld</author><category>hn</category><pubDate>Sat, 28 Feb 2026 21:16:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[MinIOâ€™s open-source repo has been officially archived. No more maintenance.
End of an era â€” but open source doesnâ€™t die that easily.I created a MinIO fork, restored the admin console, rebuilt the binary distribution pipeline, and brought it back to life.If youâ€™re running MinIO, swap  for .
Everything else stays the same. (CVE fixed, and the console GUI is back)On December 3, 2025, MinIO announced â€œmaintenance modeâ€ on GitHub. I wrote about it in MinIO Is Dead.On February 12, 2026, MinIO updated the repo status from â€œmaintenance modeâ€ to , then officially archived the repository.
Read-only. No PRs, no issues, no contributions accepted. A project with 60k stars and over a billion Docker pulls became a digital tombstone.If December was the clinical death, this February commit was the death certificate.Percona founder Peter Zaitsev also raised concerns about open-source infrastructure sustainability on LinkedIn.
The consensus in the international community is clear:Looking back at the timeline over the past years, this wasnâ€™t a sudden death. It was a slow, deliberate wind-down:Legal action against NutanixLegal action against WekaAdmin console removed from CEBinary/Docker distribution stoppedMaintenance mode announcedRepo archived, no longer maintainedA company that raised $126M at a billion-dollar valuation spent five years methodically dismantling the open-source ecosystem it built.Normally this is where the story ends â€” a collective sigh, and everyone moves on.But I want to tell a different story. Not an obituary â€” a resurrection.MinIO Inc. can archive a repo, but they canâ€™t archive the rights that the AGPL grants to the community.Ironically, AGPL was MinIOâ€™s own choice. They switched from Apache 2.0 to AGPL to use it as leverage in their disputes with Nutanix and Weka
â€” keeping the â€œopen sourceâ€ label while adding enforcement teeth. But open-source licenses cut both ways â€” the same license now guarantees the communityâ€™s right to fork.Once code is released under AGPL, the license is irrevocable. You can set a repo to read-only, but you canâ€™t claw back a granted license.
Thatâ€™s the beauty of open-source licensing by design: a company can abandon a project, but it canâ€™t take the code with it.So â€” MinIO is dead, but MinIO can live again.That said, forking is the easy part. Anyone can click the Fork button.
The real question isnâ€™t â€œcan we fork itâ€ but â€œcan someone actually maintain it as a production component?â€I didnâ€™t set out to take this on. But after MinIO entered maintenance mode,
I waited a couple of weeks for someone in the community to step up.But I didnâ€™t find one. So I did it myself.Some background: I maintain Pigsty â€” a batteries-included PostgreSQL distribution with 460+ extensions,
cross-built for 14 Linux distros. I also maintain build pipelines for 290 PG extensions, several PG forks,
and dozens of Go Projects (Victoria, Prometheus, etc.) packaging across all major platforms. Adding one more to the pipeline was a piece of cake.Iâ€™m not new to MinIO either. Back in 2018, we ran an internal MinIO fork at TanTan (back when it was still Apache 2.0),
managing ~25 PB of data â€” one of the earliest and largest MinIO deployments in China at the time.We use MinIO ourselves, so keeping the supply chain alive was not optional â€” 
As early as December 2025, when MinIO announced maintenance mode, I had already built CVE-patched binaries and switched to them.As of today, three things.1. Restored the Admin ConsoleThis was the change that frustrated the community the most.In May 2025, MinIO stripped the full admin console from the community edition, leaving behind a bare-bones object browser.
User management, bucket policies, access control, lifecycle management â€” all gone overnight. Want them back? Pay for the enterprise edition. (~$100,000)The ironic part: this didnâ€™t even require reverse engineering.
You just revert the  submodule to the previous version.
They swapped a dependency version to replace the full console with a stripped-down one. The code was always there.2. Rebuilt Binary DistributionIn October 2025, MinIO stopped distributing pre-built binaries and Docker images,
leaving only source code. â€œUse  to build it yourselfâ€ â€” that was their answer.For the vast majority of users, the value of open-source software isnâ€™t just a copy of the source â€” supply chain stability is what matters.
You need a stable artifact you can put in a Dockerfile, an Ansible playbook, or a CI/CD pipeline â€” not a requirement to install a Go compiler before every deployment.We rebuilt the distribution: is live on Docker Hub.  and youâ€™re good.Built for major Linux distributions, matching the original package specs.Fully automated build workflows on GitHub, ensuring ongoing supply chain stability.If youâ€™re using Docker, just swap  for .For native Linux installs, grab RPM/DEB packages from the GitHub Release page.
You can also use pig (the PG extension package manager) for easy installation,
or configure the  APT/DNF repo to install from it:3. Restored Community Edition DocsMinIOâ€™s official documentation was also at risk â€” links had started redirecting to their commercial product, AIStor.We forked , fixed broken links, restored removed console documentation, and deployed it here.The docs use the same CC Attribution 4.0 license as the original, with necessary maintenance.Some things worth stating up front to set expectations.No New Features â€” Just Supply Chain ContinuityMinIO as an S3-compatible object store is already feature-complete. Itâ€™s a .
It doesnâ€™t need more bells and whistles â€” it needs a stable, reliable, continuously available build.
(I already have PostgreSQL for these, so I donâ€™t need something like S3 table or S3 vector. A stable S3 core is all I need)What weâ€™re doing: making sure you can get a working, complete MinIO binary, with the admin console included and CVE fixed.
RPM, DEB, Docker images â€” built automatically via CI/CD, drop-in compatible with your existing minio.
We keep the existing minio naming and behavior where legally and technically feasible.This Is a Production Build, Not an ArchiveWe run these builds ourselves and have been dogfooding them in production for three months.
If something breaks, we detect it early and patch it quickly.I build this primarily for Pigsty and our own usage, but I hope it helps others too.Iâ€™m willing to Track CVEs and Fix BugsIf you run into issues, feel free to report them at .
Iâ€™ll do my best to fix these â€” but please donâ€™t treat this as a commercial SLA.Given that AI coding tools have made bug fixing dramatically cheaper,
and that weâ€™re explicitly not adding any new features,
I believe the maintenance workload is manageable.
(how often do you see one?)Trademark Is Tricky, But Weâ€™ll Cross That Bridge When We Come to ItTrademark Notice: MinIOÂ® is a registered trademark of MinIO, Inc.
This project (pgsty/minio) is an independently maintained community fork under the AGPL license.
It has no affiliation with, endorsement by, or connection to MinIO, Inc.
Use of â€œMinIOâ€ in this post refers solely to the open-source software project itself and implies no commercial association.AGPLv3 gives us clear rights to fork and distribute, but trademark law is a separate domain.
Weâ€™ve marked this clearly everywhere as an independent community-maintained build.If MinIO Inc. raises trademark concerns, weâ€™ll cooperate and rename (probably something like  or ).
Until then, we think descriptive use of the original name in an AGPL fork is reasonable â€” and renaming all the  references doesnâ€™t serve users.You might ask: can one person really maintain this?Itâ€™s 2026. Things are different now.With tools like Claude Code & Codex, the cost of locating and fixing bugs in a complex Go project has dropped by more than an order of magnitude.
What used to require a dedicated team to maintain a complex infra project can now be handled by one experienced engineer with an AI copilot.Maintaining a MinIO build without adding new features is a manageable task.
The key requirement is testing and validation. and we already have that scenario,
which lets us verify compatibility, reliability, and security in practice.Consider: Elon cut X/Twitterâ€™s engineering team down to ~30 people and the system still runs.
Maintaining a MinIO fork without new features is considerably less dauntingMinIO Inc. can archive a GitHub repo, but they canâ€™t archive the demand behind 60k stars,
or the dependency graph behind a billion Docker pulls. That demand doesnâ€™t disappear â€” it just finds its way out.
A company can abandon a project, but open-source licenses are specifically designed so the code canâ€™t die.Fork is the most powerful spell in open source. When a company decides to shut the door, the community only needs two words:]]></content:encoded></item><item><title>Our Agreement with the Department of War</title><link>https://openai.com/index/our-agreement-with-the-department-of-war</link><author>surprisetalk</author><category>hn</category><pubDate>Sat, 28 Feb 2026 20:35:29 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Qwen3.5 122B and 35B models offer Sonnet 4.5 performance on local computers</title><link>https://venturebeat.com/technology/alibabas-new-open-source-qwen3-5-medium-models-offer-sonnet-4-5-performance</link><author>lostmsu</author><category>hn</category><pubDate>Sat, 28 Feb 2026 20:20:00 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Block the â€œUpgrade to Tahoeâ€ Alerts</title><link>https://robservatory.com/block-the-upgrade-to-tahoe-alerts-and-system-settings-indicator/</link><author>todsacerdoti</author><category>hn</category><pubDate>Sat, 28 Feb 2026 19:04:01 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>&quot;Cancel ChatGPT&quot; movement goes mainstream after OpenAI closes deal with U.S. Dow</title><link>https://www.windowscentral.com/artificial-intelligence/cancel-chatgpt-movement-goes-mainstream-after-openai-closes-deal-with-u-s-department-of-war-as-anthropic-refuses-to-surveil-american-citizens</link><author>AndrewKemendo</author><category>hn</category><pubDate>Sat, 28 Feb 2026 19:03:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I've updated this article with comments from OpenAI CEO Sam Altman towards the end of the piece. There are no virtuous participants in the artificial intelligence race, but if there was, it might've been Anthropic.Large language model tech is built on mountains of stolen data. The entire summation of decades of the open internet was downloaded and converted by billionaires into tech that threatens to destroy billions of jobs, end the global economy, and potentially the human race. But hey, at least in the short term, shareholders (might) make a stack of cash.There are no moral leaders in this space, sadly. But at the very least, Anthropic of Claude fame took a strong stand this week against the United States government, to the ire of the Trump administration.Anthropic was designated a supply chain risk this week, and summarily and forcibly banned from use in U.S. governmental agencies. Why? Anthropic said in a blog post it revolved around their two major red lines â€” no Claude AI for use in autonomous weapons, or mass surveillance of United States citizens.It's not unexpected that mainstream governments of any stripe would be salivating at the thought of turbo-charged AI mass surveillance, but it is unexpected that a big tech corp like Anthropic would be willing to take such a strong stance against it in an era increasingly devoid of administrative morality. But hey, there's always someone willing to race to the metaphorical moral abyss in the name of money.OpenAI CEO Sam Altman and part time supervillain thankfully stepped in to bail out the U.S. Department of War, pledging ChatGPT and other OpenAI technologies to the cause.In a post on X, Altman claimed that OpenAI's models would not be used for mass surveillance, but that claim was immediately contradicted by a U.S. government official, who said that OpenAI's models would be used for "all lawful means." Mass surveillance of American citizens is lawful in "some scenarios" as part of the post-9/11 U.S. Patriot Act, which permits mass harvesting of communications meta data, even if some aspects of it have been curtailed in recent years.Anthropic wanted control over the way its technologies would be used, as opposed to relying on the interpretation of laws and legal frame works that even now have been the subject of debate and lawsuits. Altman by comparison is happy to let the U.S. government decide how OpenAI's systems are deployed, which under certain segments of the Patriot Act could quite easily lead to the mass surveillance of U.S. citizens, directly or incidentally as part of provisions on surveiling foreign citizens (which, by the way, is completely legal under U.S. law.)The move has sparked immediate backlash on ChatGPT and OpenAI communities online, across threads with thousands of upvotes on reddit of users claiming to be unsubscribing.Unfortunately, there aren't many other AI companies willing to take a stance against mass surveillance or autonomous weapons. Google removed an explicit ban on the technology last year from its internal rules. Microsoft is cool with autonomous weapons too, as long as a human pulls the final trigger. Amazon has no prohibitions whatsoever besides vague "responsible use" language, and Meta hasn't been shy about courting Pentagon military contracts either. And we all know Palantir is totally for it.The genie is out of the bottle, so to speak. ChatGPT is great at textual human mimicry but even the most cutting edge models often fail hilariously at even the most basic child-like logic puzzles.Are you looking forward to a world where these hallucination-prone, easily-manipulated artificial intelligence models might eventually decide whether or not you're a threat to national security?As long as Sam Altman and his buddies can stay rich, they don't seem to give much of a fuck about it â€” or you.Added some comments below from OpenAI CEO Sam Altman on its shift towards supporting the United States Department of War. Since writing this, OpenAI and Sam Altman have been on a damage control mission.In an "AMA" style Q&A session on X, Sam Altman claimed that the United States "Department of War" would respect OpenAI's stated "red lines" for not using AI tech for autonomous weapons or mass surveillance of United States citizens, although remained largely vague about how these safeguards would be implemented and maintained.He suggested that existing U.S. law protects against these situations by default, although legal experts have warned that surveillance of non-U.S. citizens permits the collection of data on U.S. citizens in an indirect or incidental way.People aren't exactly buying it. It makes little sense for the Trump administration to come out so strongly against Anthropic's stated position, while leaping head first into supporting OpenAI's. The core contention seems to be that OpenAI is happy to let the U.S. Department of War interpret what constitutes "legal," while Anthropic wants to maintain full control over how its technology is used.It seems as though Altman is relying purely on hopes and prayers that its technology won't be used for nefarious means â€” which seems either naÃ¯ve at best, and dishonest at worst. The current U.S. administration has shown willing to at the very least definitions and precedents outlined in the U.S. constitution and across historical landmark legal rulings. I'm not sure why there's any reason to expect OpenAI's tech wouldn't be co-opted under the guise of "national security," an abuse of power that governmental institutions of all stripes have abused in the past and present.]]></content:encoded></item><item><title>Technoâ€‘feudal elite are attempting to build a twentyâ€‘firstâ€‘century fascist state</title><link>https://collapseofindustrialcivilization.com/2026/02/16/americas-oligarchic-techno-feudal-elite-are-attempting-to-build-a-twenty-first-century-fascist-state/</link><author>measurablefunc</author><category>hn</category><pubDate>Sat, 28 Feb 2026 18:57:43 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Introduction: Fascism at the End of Industrial CivilizationThis essay argues that the United States is drifting toward a distinctly twentyâ€‘firstâ€‘century form of fascism driven not by mass parties in brownshirts, but by an oligarchic technoâ€‘feudal elite. Neoliberal capitalism has hollowed out democratic institutions and concentrated power in a transnational â€œauthoritarian internationalâ€ of billionaires, security chiefs, and political fixers who monetize state power while shielding one another from accountability. At the same time, Big Tech platforms have become neoâ€‘feudal estates that extract rent from our data and behavior, weaponize disinformation, and provide the surveillance backbone of an emerging global police state.Drawing on the work of Robert Reich, William I. Robinson, Yanis Varoufakis, and others, alongside historian Heather Cox Richardsonâ€™s detailed account of Trumpâ€‘era patronage, whistleblower suppression, and DHS/ICE megaâ€‘detention plans, the essay contends that America is rapidly constructing a system of concentrationâ€‘camp infrastructure and paramilitary policing designed to manage â€œsurplusâ€ populations and political dissent. Elite impunity, entrenched through nationalâ€‘security exceptionalism, legal immunities, and revolvingâ€‘door careers, means that those directing lawless violence face virtually no consequences. Elections still happen, courts still sit, newspapers still publish, but substantive power is increasingly exercised by unelected oligarchs, tech lords, and security bureaucracies.This authoritarian drift cannot be separated from the broader crisis of industrial civilization. Ecological overshoot, climate chaos, resource constraints, and structural economic stagnation have undermined the promise of endless growth on which liberal democracy once rested. Rather than using the remnants of industrial wealth to democratize a just transition, ruling elites are hardening borders, expanding carceral infrastructure, and building a security regime to contain â€œsurplusâ€ humanity in a world of shrinking energy and material throughput. Americaâ€™s oligarchic technoâ€‘feudal fascism is thus not an anomaly, but one plausible endgame of industrial civilization: a stratified order of gated enclaves above and camps and precarity below, designed to preserve elite power as the old industrial world comes apart.I. From liberal promise to oligarchic captureThe American republic was founded on a promise that power would be divided, constrained, and answerable: a written constitution, separated branches, periodic elections, and a Bill of Rights that set bright lines even the sovereign could not cross. That promise was always compromised by slavery, settler colonialism, and gendered exclusion, but it retained real, if uneven, force as a normative horizon. What has shifted over the past halfâ€‘century is not simply the familiar gap between creed and practice, but the underlying structure of the system itself: the center of gravity has moved from public institutions toward a private oligarchy whose wealth and leverage allow it to function as a parallel sovereign.The neoliberal turn of the 1970s and 1980s marked the decisive inflection point. Deregulation, financial liberalization, the crushing of organized labor, and the privatization of public goods redistributed power and income upward on a historic scale. Trade liberalization and capital mobility allowed corporations and investors to pit governments and workers against one another, extracting subsidies and tax concessions under the permanent threat of capital flight. At the same time, Supreme Court decisions eroded limits on political spending, redefining â€œspeechâ€ as something that could be purchased in unlimited quantities by those with the means.The result, as Robert Reich notes, has been the consolidation of an American oligarchy that â€œpaved the road to fascismâ€ by ensuring that public policy reflects donor preferences far more consistently than popular majorities. In issue after issue, such as taxation, labor law, healthcare, and environmental regulation, there is a clear skew: the wealthy get what they want more often than not, while broadly popular but redistributive policies routinely die in committee or are gutted beyond recognition. This is not a conspiracy in the melodramatic sense; it is how the wiring of the system now works.William Robinsonâ€™s analysis of â€œtwentyâ€‘firstâ€‘century fascismâ€ sharpens the point. Global capitalism in its current form generates chronic crises: overproduction, underâ€‘consumption, ecological breakdown, and a growing population that capital cannot profitably employ. Under such conditions, democratic politics becomes dangerous for elites, because electorates might choose structural reforms such as wealth taxes, public ownership, strong unions, and Green New Dealâ€‘style transitions that would curb profits. Faced with this prospect, segments of transnational capital begin to see authoritarian solutions as rational: better to hollow out democracy, harden borders, and construct a global police state than to accept serious redistribution.American politics in the early twentyâ€‘first century fits this pattern with unsettling precision. A decaying infrastructure, stagnant wages, ballooning personal debt, militarized policing, and permanent war have produced widespread disillusionment. As faith in institutions erodes, public life is flooded with resentment and nihilism that can be redirected against scapegoats (immigrants, racial minorities, feminists, and queer and trans people) rather than against the oligarchicâ€‘powerâ€‘complex that profits from the decay. It is in this vacuum that a figure like Donald Trump thrives: a billionaire demagogue able to channel anger away from the class that actually governs and toward those even more marginalized.The decisive shift from plutocratic dysfunction to fascist danger occurs when oligarchs cease to see constitutional democracy as even instrumentally useful and instead invest in movements openly committed to minority rule. Kochâ€‘style networks, Mercerâ€‘funded operations, and Silicon Valley donors willing to underwrite hardâ€‘right projects are not supporting democracyâ€‘enhancing reforms; they are building the infrastructure for authoritarianism, from voter suppression to ideological media to dataâ€‘driven propaganda. The system that emerges is hybrid: elections still occur, courts still sit, newspapers still publish, but substantive power is increasingly concentrated in unelected hands.II. The â€œauthoritarian internationalâ€ and the shadow world of dealsHistorian Heather Cox Richardsonâ€™s recent analysis captures a formation that much mainstream commentary still struggles to name: a transnational â€œauthoritarian internationalâ€ in which oligarchs, political operatives, royal families, security chiefs, and organized criminals cooperate to monetize state power while protecting one another from scrutiny. This is not a formal alliance; it is an overlapping ecology of relationships, exclusive vacations, investment vehicles, shell companies, foundations, and intelligence ties, through which information, favors, and money flow.The key is that this network is structurally postâ€‘ideological. As Robert Mueller warned in his 2011 description of an emerging â€œiron triangleâ€ of politicians, businesspeople, and criminals, these actors are not primarily concerned with religion, nationality, or traditional ideology. They will work across confessional and national lines so long as the deals are lucrative and risk is manageably socialized onto others. Saudi royals invest alongside Western hedge funds; Russian oligarchs launder money through London property and American private equity; Israeli and Emirati firms collaborate with U.S. tech companies on surveillance products that are then sold worldwide.Within this milieu, the formal distinction between public office and private interest blurs. Richardsonâ€™s analysis of Donald Trumpâ€™s abrupt reversal on the Gordie Howe International Bridge after a complaint by a billionaire competitor with ties to Jeffrey Epsteinâ€”reads less like the exercise of public policy judgment and more like feudal patronage: the sovereign intervenes to protect a favored lordâ€™s toll road. Tiny shifts in regulatory posture or federal support can move billions of dollars; for those accustomed to having the presidentâ€™s ear, such interventions are simply part of doing business.The same logic governs foreign policy. The Trumpâ€‘Kushner axis exemplifies this fusion of public and private. When a whistleblower alleges that the Director of National Intelligence suppressed an intercept involving foreign officials discussing Jared Kushner and sensitive topics like Iran, and when the complaint is then choked off with aggressive redaction and executive privilege, we see the machinery of secrecy misused not to protect the national interest but to shield a member of the familyâ€‘cumâ€‘business empire at the center of power. It is as if the state has become a family office with nuclear weapons.Josh Marshallâ€™s phrase â€œauthoritarian internationalâ€ is apt because it names both the class composition and the political function of this network. The same names recur across farâ€‘right projects: donors and strategists who back nationalist parties in Europe, ultras in Latin America, Modiâ€™s BJP in India, and the MAGA movement in the United States. Their interests are not identical, but they overlap around a shared agenda: weakening labor and environmental protections, undermining independent media and courts, militarizing borders, and securing immunity for themselves and their peers.This world is lubricated by blackmail and mutually assured destruction. As Richardson notes, players often seem to hold compromising material on one another, whether in the form of documented sexual abuse, financial crime, or war crimes. This shared vulnerability paradoxically stabilizes the network: as long as everyone has something on everyone else, defection is dangerous, and a predatory equilibrium holds. From the standpoint of democratic publics, however, this stability is catastrophic, because it means that scandalâ€”once a mechanism for enforcing normsâ€”loses much of its power. When â€œeveryone is dirty,â€ no one can be clean enough to prosecute the others without risking exposure.III. Technoâ€‘feudal aristocracy and the colonization of everyday lifeLayered atop this transnational oligarchy is the digital order that Varoufakis and others describe as technoâ€‘feudalism: a regime in which a handful of platforms function like neoâ€‘feudal estates, extracting rent from their â€œserfsâ€ (users, gig workers, content creators) rather than competing in open markets. This shift is more than metaphor. In classical capitalism, firms profited primarily by producing goods or services and selling them on markets where competitors could, in principle, undercut them. In the platform order, gatekeepers profit by controlling access to the marketplace itself, imposing opaque terms on those who must use their infrastructure to communicate, work, or even find housing.This can be seen across sectors:Social media platforms own the digital public square. They monetize attention by selling advertisers access to finely sliced demographic and psychographic segments, while their recommendation algorithms optimize for engagement, often by privileging outrage and fear.Rideâ€‘hailing and delivery apps control the interface between customers and labor, setting prices unilaterally and disciplining workers through ratings, algorithmic management, and the everâ€‘present threat of â€œdeactivation.â€Cloud providers and app stores gatekeep access to the basic infrastructure upon which countless smaller firms depend, taking a cut of transactions and reserving the right to change terms or remove competitors from the ecosystem entirely.In each case, the platform is less a company among companies and more a landlord among tenants, collecting tolls for the right to exist within its domain. Users produce the very capital stock, data, content, behavioral profiles, that platforms own and monetize, yet they have little say over how this material is used or how the digital environment is structured. The asymmetry of power is profound: the lords can alter the code of the world; the serfs can, at best, adjust their behavior to avoid algorithmic invisibility or sanction.For authoritarian politics, this structure is a gift. First, platforms have become the primary vectors of disinformation and propaganda. Cambridge Analyticaâ€™s work for Trump in 2016, funded by billionaires like the Mercers, was an early prototype: harvest data, microâ€‘target individuals with tailoredÂ messaging, and flood their feeds with narratives designed to activate fear and resentment. Since then, the techniques have grown more sophisticated, and farâ€‘right movements worldwide have learned to weaponize meme culture, conspiracy theories, and â€œshitpostingâ€ as recruitment tools.Second, the same infrastructures that enable targeted advertising enable granular surveillance. Location data, social graphs, search histories, and facialâ€‘recognition databases provide an unprecedented toolkit for monitoring and disciplining populations. In the hands of a regime sliding toward fascism, these tools can be turned against dissidents with terrifying efficiency: geofencing protests to identify attendees, scraping social media to build dossiers, using AI to flag â€œpreâ€‘criminalâ€ behavior. The emerging â€œglobal police stateâ€ that Robinson describes depends heavily on such technoâ€‘feudal capacities.Third, the digital order corrodes the very preconditions for democratic deliberation. Information overload, filter bubbles, and algorithmic amplification of sensational content produce a public sphere saturated with noise. Under these conditions, truth becomes just another aesthetic, and the distinction between fact and fiction collapses into vibes. This is the postâ€‘modern nihilism you name: a sense that nothing is stable enough to believe in, that everything is spin. Fascist movements do not seek to resolve this condition; they weaponize it, insisting that only the Leader and his trusted media tell the real truth, while everything else is a hostile lie.Finally, the technoâ€‘feudal aristocracyâ€™s material interests align with authoritarianism. Privacy regulations, antitrust enforcement, data localization rules, and strong labor rights all threaten platform profits. Democratic movements that demand such reforms are therefore adversaries. Conversely, strongman leaders who promise deregulation, tax breaks, and lawâ€‘andâ€‘order crackdowns, even if they occasionally threaten specific firms, are often acceptable partners. The result is a convergence: oligarchs of data and oligarchs of oil, real estate, and finance finding common cause in an order that disciplines the many and exempts the few.IV. Elite impunity and the machinery of lawlessnessAuthoritarianism is not only about who holds power; it is about who is answerable for wrongdoing. A system where elites can violate laws with impunity while ordinary people are punished harshly for minor infractions is already halfway to fascism, whatever labels it wears. The United States has, over recent decades, constructed precisely such a system.The Arab Centerâ€™s â€œMachinery of Impunityâ€ report details how, in areas ranging from mass surveillance to foreign wars to domestic policing, senior officials who authorize illegal acts almost never face criminal consequences. Edward Snowdenâ€™s revelations exposed systemic violations of privacy and civil liberties, yet it was the whistleblower who faced prosecution and exile, not the architects of the programs. Torture during the â€œwar on terrorâ€ was acknowledged, even documented in official reports, but those who designed and approved the torture regime kept their law licenses, academic posts, and media gigs. Lethal strikes on small boats in the Caribbean and Pacific, justified by secret intelligence and shielded by classified legal opinions, have killed dozens with no public evidence that the targets posed imminent threats.This pattern is not an aberration but a feature. As a Penn State law review article notes, the U.S. legal system builds in multiple layers of protection for high officials: sovereign immunity, state secrets privilege, narrow standing rules, and prosecutorial discretion all combine to make it extraordinarily difficult to hold the powerful to account. Violations of the Hatch Act, campaignâ€‘finance laws, or ethics rules are often treated as technicalities, and when reports do document unlawful behavior, as in the case of Mike Pompeoâ€™s partisan abuse of his diplomatic office, there are â€œno consequencesâ€ beyond mild censure. Jamelle Bouieâ€™s recent video essay for the New York Times drives the point home: America is â€œbad at accountabilityâ€ because institutions have been designed and interpreted to favor elite impunity.Richardson shows how this culture functions inside the nationalâ€‘security state. A whistleblower complaint alleging that the Director of National Intelligence suppressed an intelligence intercept involving Jared Kushner and foreign officials was not allowed to run its course. Instead, it was bottled up, then transmitted to congressional overseers in a highly redacted form, with executive privilege invoked to shield the presidentâ€™s involvement. The same mechanisms that insulate covert operations abroad from democratic oversight are deployed to protect domestic political allies from scrutiny.Immigration enforcement offers another window. The Arab Center notes that ICE raids, family separation, and other abuses â€œescalated under the current Trump administration into highly visible kidnappings, abuse, and deportationsâ€ with little accountability for senior officials. The National Immigrant Justice Center documents a detention system where 90 percent of detainees are held in forâ€‘profit facilities, where medical neglect, punitive solitary confinement, and preventable deaths are common, yet contracts are renewed and expanded. A culture of impunity allows agents and managers to treat rights violations not as careerâ€‘ending scandals but as acceptable collateral damage.Latin American scholars of impunity warn that such selective enforcement produces a â€œquiet crisis of accountabilityâ€ in which the rule of law is hollowed out from within. Laws remain on the books, but their application is skewed: harsh on the poor and marginalized, permissive toward the powerful. Over time, this normalizes the idea that some people are above the law, while others exist primarily as objects of control. When a polity internalizes this hierarchy, fascism no longer needs to arrive in jackboots; it is already present in the daily operations of the justice system.The danger, as the Arab Center emphasizes, is that the costs of impunity â€œcome home to roost.â€ Powers originally justified as necessary to fight terrorism or foreign enemies migrate back into domestic politics. Surveillance tools built for foreign intelligence monitoring are turned on activists and journalists; militarized police tactics perfected in occupied territories are imported into American streets. A population taught to accept lawless violence against outsiders (migrants, foreigners, enemy populations) is gradually conditioned to accept similar violence against internal opponents.V. Concentration camps, paramilitary policing, and ritualized predatory violenceIn this context of oligarchic capture, technoâ€‘feudal control, and elite impunity, the rapid expansion of detention infrastructure and the deployment of paramilitary â€œfederal agentsâ€ across the interior United States are not aberrations; they are central pillars of an emergent fascist order.Richardsonâ€™s insistence on calling these facilities concentration camps is analytically exact. A concentration camp, in the historical sense, is not necessarily a death camp; it is a place where a state concentrates populations it considers threats or burdens, subjecting them to confinement, disease, abuse, and often death through neglect rather than industrialized extermination. By that definition, the sprawling network of ICE and Border Patrol detention centers, where people are warehoused for months to years, often in horrific conditions, qualifies.New reporting details how this system is poised to scale up dramatically. An internal ICE memo, recently surfaced, outlines a $38 billion plan for a â€œnew detention center modelâ€ that would, in one year, create capacity for roughly 92,600 people by purchasing eight â€œmega centers,â€ 16 processing centers, and 10 additional facilities. The largest of these warehouses would hold between 7,000 and 10,000 people each for average stays of about 60 days, more than double the size of the largest current federal prison. Separate reporting has mapped at least 23 industrial warehouses being surveyed for conversion into mass detention camps, with leases already secured at several sites.Investigations by Amnesty International and others into prototype facilities have found detainees shackled in overcrowded cages, underfed, forced to use openâ€‘air toilets that flood, and routinely denied medical care. Sexual assault and extortion by guards, negligent deaths, and at least one homicide have been documented. These are not accidents; they are predictable outcomes of a profitâ€‘driven system where private contractors are paid per bed and oversight is weak, and of a political culture that dehumanizes migrants as â€œinvadersâ€ or â€œanimals.â€Richardson highlights another crucial dimension: the way DHS has been retooled to project this violence into the interior as a form of political terror. Agents from ICE and Border Patrol, subdivisions of a relatively new department lacking the institutional restraints of the military, have been deployed in cities far from any border, often in unmarked vehicles, wearing masks and lacking visible identification. Secret legal memos under Trump gutted the traditional requirement of a judicial warrant for entering homes, replacing it with internal signâ€‘off by another DHS official, a direct violation of the Fourth Amendmentâ€™s protection against unreasonable searches and seizures.This matters both instrumentally and symbolically. Instrumentally, it enables efficient mass raids and â€œsnatch and grabâ€ operations that bypass local lawâ€‘enforcement norms and judicial oversight. Symbolically, it communicates that the state reserves the right to operate as a lawless force, unconstrained by the very constitution it claims to defend. When masked, unidentified agents can seize people off the streets, shove them into unmarked vans, and deposit them in processing centers without due process, the aesthetic of fascismâ€¦thugs in the nightâ€¦becomes reality.Richardson rightly connects this to the postâ€‘Reconstruction South, where paramilitary groups like the Ku Klux Klan, often tolerated or quietly aided by local officials, used terror to destroy a biracial democracy that had briefly flourished. Todayâ€™s difference is that communications technology allows rapid mobilization of witnesses and counterâ€‘protesters: people can rush to the scene when agents arrive, document abuses on smartphones, and coordinate legal support. Yet even this can be folded into the logic of spectacle. The images of militarized agents confronting crowds under the glow of streetlights and police floodlamps serve as warnings: this is what happens when you resist.The planned network of processing centers and megaâ€‘warehouses adds another layer of menace. As Richardson points out, if the stated goal is deportation, there is no clear need for facilities capable of imprisoning tens of thousands for months. Part of the answer is coercive leverage: detained people are easier to pressure into abandoning asylum claims and accepting removal, especially when they are told, day after day, that they could walk free if they â€œjust sign.â€ But the architecture also anticipates a future in which new categories of internal enemies, protesters, â€œAntifa,â€ â€œdomestic extremists,â€ can be funneled into the same carceral estate once migrant flows diminish or political needs change.Economically, the camps generate their own constituency. ICE and DHS tout job creation numbers to local officials, promising hundreds of stable, often unionâ€‘free positions in communities hollowed out by deindustrialization. Private prison firms and construction companies see lucrative contracts; investors see secure returns backed by federal guarantees. A web of stakeholders thus becomes materially invested in the continuation and expansion of mass detention. This is technoâ€‘feudalism in concrete and razor wire: a carceral estate in which bodies are the rentâ€‘producing asset.Once such an estate exists, its logic tends to spread. Borderâ€‘style tactics migrate into ordinary policing; surveillance tools trialed on migrants are turned on domestic movements; legal doctrines crafted to justify raids and warrantless searches in the name of immigration control seep into other domains. The fascist gradient steepens: more people find themselves at risk of sudden disappearance into a system where rights are theoretical and violence is routine.]]></content:encoded></item><item><title>Verified Spec-Driven Development (VSDD)</title><link>https://gist.github.com/dollspace-gay/d8d3bc3ecf4188df049d7a4726bb2a00</link><author>todsacerdoti</author><category>hn</category><pubDate>Sat, 28 Feb 2026 16:58:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The whole thing was a scam</title><link>https://garymarcus.substack.com/p/the-whole-thing-was-scam</link><author>guilamu</author><category>hn</category><pubDate>Sat, 28 Feb 2026 16:51:49 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Obsidian Sync now has a headless client</title><link>https://help.obsidian.md/sync/headless</link><author>adilmoujahid</author><category>hn</category><pubDate>Sat, 28 Feb 2026 16:31:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cognitive Debt: When Velocity Exceeds Comprehension</title><link>https://www.rockoder.com/beyondthecode/cognitive-debt-when-velocity-exceeds-comprehension/</link><author>pagade</author><category>hn</category><pubDate>Sat, 28 Feb 2026 15:39:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The engineer shipped seven features in a single sprint. DORA metrics looked immaculate. The promotion packet practically wrote itself.Six months later, an architectural change required modifying those features. No one on the team could explain why certain components existed or how they interacted. The engineer who built them stared at her own code like a strangerâ€™s.Code has become cheaper to produce than to perceive.When an engineer writes code manually, two parallel processes occur. The first is production: characters appear in files, tests get written, systems change. The second is absorption: mental models form, edge cases become intuitive, architectural relationships solidify into understanding. These processes are coupled. The act of typing forces engagement. The friction of implementation creates space for reasoning.AI-assisted development decouples these processes. A prompt generates hundreds of lines in seconds. The engineer reviews, adjusts, iterates. Output accelerates. But absorption cannot accelerate proportionally. The cognitive work of truly understanding what was built, why it was built that way, and how it relates to everything else remains bounded by human processing speed.This gap between output velocity and comprehension velocity is cognitive debt.Unlike technical debt, which surfaces through system failures or maintenance costs, cognitive debt remains invisible to velocity metrics. The code works. The tests pass. The features ship. The deficit exists only in the minds of the engineers who built the system, manifesting as uncertainty about their own work.The debt is not truly invisible. It eventually appears in reliability metrics: Mean Time to Recovery stretches longer, Change Failure Rate creeps upward. But these are lagging indicators, separated by months from the velocity metrics that drive quarterly decisions. By the time MTTR signals a problem, the comprehension deficit has already compounded.What Organizations Actually MeasureEngineering performance systems evolved to measure observable outputs. Story points completed. Features shipped. Commits merged. Review turnaround time. These metrics emerged from an era when output and comprehension were tightly coupled, when shipping something implied understanding something.The metrics never measured comprehension directly because comprehension was assumed. An engineer who shipped a feature was presumed to understand that feature. The presumption held because the production process itself forced understanding.That presumption no longer holds. An engineer can now ship features while maintaining only surface familiarity with their implementation. The features work. The metrics register success. The organizational knowledge that would traditionally accumulate alongside those features simply does not form at the same rate.Performance calibration committees see velocity improvements. They do not see comprehension deficits. They cannot, because no artifact of the organizational measurement system captures that dimension.The discussion of cognitive debt typically focuses on the engineer who generates code. The more acute problem sits with the engineer who reviews it.Code review evolved as a quality gate. A senior engineer examines a junior engineerâ€™s work, catching errors, suggesting improvements, transferring knowledge. The rate-limiting factor was always the junior engineerâ€™s output speed. Senior engineers could review faster than juniors could produce.AI-assisted development inverts this relationship. A junior engineer can now generate code faster than a senior engineer can critically audit it. The volume of generated code exceeds the bandwidth available for deep review. Something has to give, and typically it is review depth.The reviewer faces an impossible choice. Maintain previous review standards and become a bottleneck that negates the velocity gains AI provides. Or approve code at the rate it arrives and hope the tests catch what the review missed. Most choose the latter, often unconsciously, because organizational pressure favors throughput.This is where cognitive debt compounds fastest. The authorâ€™s comprehension deficit might be recoverable through later engagement with the code. The reviewerâ€™s comprehension deficit propagates: they approved code they do not fully understand, which now carries implicit endorsement. The organizational assumption that reviewed code is understood code no longer holds.Engineers working extensively with AI tools report a specific form of exhaustion that differs from traditional burnout. Traditional burnout emerges from sustained cognitive load, from having too much to hold in mind while solving complex problems. The new pattern emerges from something closer to cognitive disconnection.The work happens quickly. Progress is visible. But the engineer experiences a persistent sense of not quite grasping their own output. They can execute, but explanation requires reconstruction. They can modify, but prediction becomes unreliable. The system they built feels slightly foreign even as it functions correctly.This creates a distinctive psychological state: high output combined with low confidence. Engineers produce more while feeling less certain about what they have produced. In organizations that stack-rank based on visible output, this creates pressure to continue generating despite the growing uncertainty.The engineer who pauses to deeply understand what they built falls behind in velocity metrics. The engineer who prioritizes throughput over comprehension meets their quarterly objectives. The incentive structure selects for the behavior that accelerates cognitive debt accumulation.When Organizational Memory FailsKnowledge in engineering organizations exists in two forms. The first is explicit: documentation, design documents, recorded decisions. The second is tacit: understanding held in the minds of people who built and maintained systems over time. Tacit knowledge cannot be fully externalized because much of it exists as intuition, pattern recognition, and contextual judgment that formed through direct engagement with the work.When the people who built a system leave or rotate to new projects, tacit knowledge walks out with them. Organizations traditionally replenished this knowledge through the normal process of engineering work. New engineers building on existing systems developed their own tacit understanding through the friction of implementation.AI-assisted development potentially short-circuits this replenishment mechanism. If new engineers can generate working modifications without developing deep comprehension, they never form the tacit knowledge that would traditionally accumulate. The organization loses knowledge not just through attrition but through insufficient formation.This creates a delayed failure mode. The system continues to function. New features continue to ship. But the reservoir of people who truly understand the system gradually depletes. When circumstances eventually require that understanding, when something breaks in an unexpected way or requirements change in a way that demands architectural reasoning, the organization discovers the deficit.Three failure modes emerge as cognitive debt accumulates.The first involves the reversal of a normally reliable heuristic. Engineers typically trust code that has been in production for years. If it survived that long, it probably works. The longer code exists without causing problems, the more confidence it earns. AI-generated code inverts this pattern. The longer it remains untouched, the more dangerous it becomes, because the context window of the humans around it has closed completely. Code that was barely understood when written becomes entirely opaque after the people who wrote it have moved on.They are debugging a black box written by a black box.The second failure mode surfaces during incidents. An alert fires at 3:00 AM. The on-call engineer opens a system they did not build, generated by tools they did not supervise, documented in ways that assume familiarity they do not possess. They are debugging a black box written by a black box. What would have been a ten-minute fix when someone understood the system becomes a four-hour forensic investigation when no one does. Multiply this across enough incidents and the aggregate cost exceeds whatever velocity gains the AI-assisted development provided.The organization is effectively trading its pipeline of future Staff Engineers for this quarter's feature delivery.The third failure mode operates on a longer timescale. Junior engineers who rely primarily on AI-assisted development never develop the intuition that comes from manual implementation. They ship features without forming the scar tissue that informs architectural judgment. The organization is effectively trading its pipeline of future Staff Engineers for this quarterâ€™s feature delivery. The cost does not appear in current headcount models because the people who would have become senior architects five years from now are not yet absent. From the perspective of engineering leadership, AI-assisted development presents as productivity gain. Teams ship faster. Roadmaps compress. Headcount discussions become more favorable. These are the observable signals that propagate upward through organizational reporting structures.The cognitive debt accumulating in those teams does not present as a signal. There is no metric for â€œengineers who can explain their own code without re-reading it.â€ There is no dashboard for â€œorganizational comprehension depth.â€ The concept does not fit into quarterly business review formats or headcount justification narratives.Directors make decisions based on observable signals. When those signals uniformly indicate success, the decision to double down on the approach that produced those signals is rational within the information environment available to leadership. The decision is not wrong given the data. The data is incomplete.The cognitive debt framing does not apply uniformly across all engineering work. Some tasks genuinely are mechanical. Some codebases genuinely benefit from rapid iteration without deep architectural understanding. Some features genuinely do not require the level of comprehension that would traditionally form through manual implementation.The model also assumes that comprehension was previously forming at adequate rates. This assumption may be generous. Engineers have always varied in how deeply they understood their own work. The distribution may simply be shifting rather than a new phenomenon emerging.Additionally, tooling and documentation practices may evolve to partially close the comprehension gap. If organizations develop methods for capturing and transmitting the understanding that AI-assisted development fails to form organically, the debt may prove manageable rather than accumulative.The system is optimizing correctly for what it measures. What it measures no longer captures what matters.The fundamental challenge is that organizations cannot optimize for what they cannot measure. Velocity is measurable. Comprehension is not, or at least not through any mechanism that currently feeds into performance evaluation, promotion decisions, or headcount planning.Until comprehension becomes legible to organizational decision-making systems, the incentive structure will continue to favor velocity. Engineers who prioritize understanding over output will appear less productive than peers who prioritize output over understanding. Performance calibration will reward the behavior that accumulates debt faster.This is not a failure of individual managers or engineers. It is a measurement system designed for an era when production and comprehension were coupled, operating in an era when that coupling no longer holds. The system is optimizing correctly for what it measures. What it measures no longer captures what matters.The gap will eventually manifest. Whether through maintenance costs that exceed projections, through incidents that require understanding no one possesses, or through new requirements that expose the brittleness of systems built without deep comprehension. The timing and form of manifestation remain uncertain. The underlying dynamic does not.]]></content:encoded></item><item><title>Addressing Antigravity Bans and Reinstating Access</title><link>https://github.com/google-gemini/gemini-cli/discussions/20632</link><author>RyanShook</author><category>hn</category><pubDate>Sat, 28 Feb 2026 13:50:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenAI fires an employee for prediction market insider trading</title><link>https://www.wired.com/story/openai-fires-employee-insider-trading-polymarket-kalshi/</link><author>bookofjoe</author><category>hn</category><pubDate>Sat, 28 Feb 2026 13:46:20 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ an employee following an investigation into their activity on prediction market platforms including Polymarket, WIRED has learned.OpenAI CEO of Applications, Fidji Simo, disclosed the termination in an internal message to employees earlier this year. The employee, she said, â€œused confidential OpenAI information in connection with external prediction markets (e.g. Polymarket).â€â€œOur policies prohibit employees from using confidential OpenAI information for personal gain, including in prediction markets,â€ says spokesperson Kayla Wood. OpenAI has not revealed the name of the employee or the specifics of their trades.Evidence suggests that this was not an isolated event. Polymarket runs on the Polygon blockchain network, so its trading ledger is pseudonymous but traceable. According to an analysis by the financial data platform Unusual Whales, there have been clusters of activities, which the service flagged as suspicious, around OpenAI-themed events since March 2023.Unusual Whales flagged 77 positions in 60 wallet addresses as suspected insider trades, looking at the age of the account, trading history, and significance of investment, among other factors. Suspicious trades hinged on the release dates of products like Sora, GPT-5, and the ChatGPT Browser, as well as CEO Sam Altmanâ€™s employment status. In November 2023, two days after Altman was dramatically ousted from the company, a new wallet placed a significant bet that he would return, netting over $16,000 in profits. The account never placed another bet.The behavior fits into patterns typical of insider trades. â€œThe tell is the clustering. In the 40 hours before OpenAI launched its browser, 13 brand-new wallets with zero trading history appeared on the site for the first time to collectively bet $309,486 on the right outcome,â€ says Unusual Whales CEO Matt Saincome. â€œWhen you see that many fresh wallets making the same bet at the same time, it raises a real question about whether the secret is getting out.â€Prediction markets have exploded in popularity in recent years. These platforms allow customers to buy â€œevent contractsâ€ on the outcomes of future events ranging from the winner of the Super Bowl to the daily price of Bitcoin to whether the United States will go to war with Iran. There are a wide array of markets tied to events in the technology sector; you can trade on what Nvidiaâ€™s quarterly earnings will be, or when Tesla will launch a new car, or which AI companies will IPO in 2026.As the platforms have grown, so have concerns that they allow traders to profit from insider knowledge. â€œThis prediction market world makes the Wild West look tame in comparison,â€ says Jeff Edelstein, a senior analyst at the betting news site InGame. â€œIf there's a market that exists where the answer is known, somebody's going to trade on it.â€Earlier this week, Kalshi announced that it had reported several suspicious insider trading cases to the Commodity Futures Trading Commission, the government agency overseeing these markets. In one instance, an employee of the popular YouTuber Mr. Beast was suspended for two years and fined $20,000 for making trades related to the streamerâ€™s activities; in another, the far-right political candidate Kyle Langford was banned from the platform for making a trade on his own campaign. The company also announced a number of initiatives to prevent insider trading and market manipulation.While Kalshi has heavily promoted its crackdown on insider trading, Polymarket has stayed silent on the matter. The company did not return requests for comments.In the past, major trades on technology-themed markets have sparked speculation that there are Big Tech employees profiting by using their insider knowledge to gain an edge. One notorious example is the so-called â€œGoogle whale,â€ a pseudonymous account on Polymarket that made over $1 million trading on Google-related events, including a market on who the most-searched person of the year would be in 2025. (It was the singer D4vd, who is best known for his connection to an ongoing murder investigation after a young fanâ€™s remains were found in a vehicle registered to him.)]]></content:encoded></item><item><title>Show HN: Now I Get It â€“ Translate scientific papers into interactive webpages</title><link>https://nowigetit.us/</link><author>jbdamask</author><category>hn</category><pubDate>Sat, 28 Feb 2026 13:29:36 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Drop your PDF here, or Works best with files under 10 MB]]></content:encoded></item><item><title>What AI coding costs you</title><link>https://tomwojcik.com/posts/2026-02-15/finding-the-right-amount-of-ai/</link><author>tomwojcik</author><category>hn</category><pubDate>Sat, 28 Feb 2026 13:05:03 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Every developer I know uses AI for coding now. The productivity gains are real, but there are costs that donâ€™t show up on any dashboard.Imagine a spectrum. On the far left are humans typing on the keyboard, seeing the code in the IDE. On the far right: AGI. It implements everything on its own. Cheaply, flawlessly, better than any human, and no human overseer is required. Somewhere between those two extremes thereâ€™s you, using AI, today. That threshold moves to the right every week as models improve, tools mature, and workflows get refined.Which is higher risk, using AI too much, or using AI too little?and it made me think about LLMs for coding differently, especially after reading what other devs share on AI adoption in different workplaces. You can be wrong in both directions, but is the desired amount of AI usage at work changing as the models improve?Not long ago the first AI coding tools like Cursor (2023) or Copilot (2022) emerged. They were able to quickly index the codebase using RAG, so they had the local context. They had all the knowledge of the models powering them, so they had an external knowledge of the Internet as well. Googling and browsing StackOverflow wasnâ€™t needed anymore. Cursor gave the users a custom IDE with built in AI powered autocomplete and other baked-in AI tools, like chat, to make the experience coherent.Then came the agent promise. MCPs, autonomous workflows, articles about agents running overnight started to pop up left and right. It was a different use of AI than Cursor. It was no longer an AI-assisted human coding, but a human-assisted AI coding.Many devs tried it and got burned. Agents made tons of small mistakes. The AI-first process required a complete paradigm shift in how devs think about coding, in order to achieve great results. Also, agents often got stuck in loops, hallucinate dependencies, and produced code that looks almost right but isnâ€™t. You needed to learn about a completely new tech, fueled by FOMO. And this new shiny tool never got it 100% right on the first try.Software used to be deterministic. You controlled it with if/else branches, explicit state machines, clear logic. The new reality is controlling the development process with prompts, system instructions, and CLAUDE.md files, and hope the model produces the output you expect.An engineer at Spotify on their morning commute from Slack on their cell phone can tell Claude to fix a bug or add a new feature to the iOS app. And once Claude finishes that work, the engineer then gets a new version of the app, pushed to them on Slack on their phone, so that he can then merge it to production, all before they even arrive at the office.â€I hope they at least review the code before merging.The next stage is an (almost) full automation. Thatâ€™s what many execs want and try to achieve. Itâ€™s a capitalistic wet dream, a worker that never sleeps, never gets tired, always wants to work, is infinitely productive. But Geoffrey Hinton predicted in 2016 that deep learning would outperform radiologists at image analysis within five years. Anthropicâ€™s CEO predicted AI would write 90% of code within three to six months of March 2025. None of this happened as predicted. The trajectory is real, but the timeline keeps slipping.In 2012, neuroscientist Manfred Spitzer published Digital Dementia, arguing that when we outsource mental tasks to digital devices, the brain pathways responsible for those tasks atrophy. Use it or lose it. Not all of this is proven scientifically, but neuroplasticity research shows the brain strengthens pathways that get used and weakens ones that donâ€™t. The core principle of the book is that the cognitive skills that you stop practicing will decline.Margaret-Anne Storey, a software engineering researcher, recently gave this a more precise name: cognitive debt. Technical debt lives in the code. Cognitive debt lives in developersâ€™ heads. Itâ€™s the accumulated loss of understanding that happens when you build fast without comprehending what you built. She grounds it in Peter Naurâ€™s 1985 theory that a program is a theory existing in developersâ€™ minds, capturing what it does, how intentions map to implementation, and how it can evolve. When that theory fragments, the system becomes a black box.Apply this directly to fully agentic coding. If you stop writing code and only review AI output, your ability to reason about code atrophies. Slowly, invisibly, but inevitably. You canâ€™t deeply review what you can no longer deeply understand.This isnâ€™t just theory. A 2026 randomized study by Shen and Tamkin tested this directly: 52 professional developers learning a new async library were split into AI-assisted and unassisted groups. The AI group scored 17% lower on conceptual understanding, debugging, and code reading. The largest gap was in debugging, the exact skill you need to catch what AI gets wrong. One hour of passive AI-assisted work produced measurable skill erosion.The insidious part is that you donâ€™t notice the decline because the tool compensates for it. You feel productive. The PRs are shipping. Mihaly Csikszentmihalyiâ€™s research on flow showed that the state of flow depends on a balance between challenge and skill. Your mind needs to be stretched just enough. Real flow produces growth. Rachel Thomas called what AI-assisted work produces â€œdark flowâ€, a term borrowed from gambling research, describing the trance-like state slot machines are designed to induce. You feel absorbed, but the challenge-skill balance is gone because the AI handles the challenge. It feels like the flow state of deep work, but the feedback loop is broken. Youâ€™re not getting better, youâ€™re getting dependent.Thereâ€™s this observation that keeps coming up in HN comments: if the AI writes all the code and you only review it, where does the skill to review come from? You canâ€™t have one without the other. You donâ€™t learn to recognize good code by reading about it in a textbook, or a PR. You learn by writing bad code, getting it torn apart, and building intuition through years of practice.This creates what Iâ€™d call the review paradox: the more AI writes, the less qualified humans become to review what it wrote. The Shen-Tamkin study puts numbers on this. Developers who fully delegated to AI finished tasks fastest but scored worst on evaluations. The novices who benefit most from AI productivity are exactly the ones who need debugging skills to supervise it, and AI erodes those skills first.Storeyâ€™s proposed fix is simple: â€œrequire humans to understand each AI-generated change before deployment.â€ Thatâ€™s the right answer. Itâ€™s also the one that gets skipped first when velocity is the metric.This goes deeper than individual skill decay. We used to have juniors, mids, seniors, staff engineers, architects. It was a pipeline where each level built on years of hands-on struggle. A junior spends years writing code that is rejected during the code review not because they were not careful, but didnâ€™t know better. Itâ€™s how you build the judgment that separates someone who can write a function from someone who can architect a system. You canâ€™t become a senior overnight.Unless you use AI, of course. Now, a junior with Claude Code (Opus 4.5+) delivers PRs that look like senior engineer work. And overall thatâ€™s a good thing, I think. But does it mean that the senior hat fits everyone now? From day one? But the head underneath hasnâ€™t changed. That junior doesnâ€™t know  that architecture was chosen. From my experience, sometimes CC misses a new DB transaction where itâ€™s needed. Sometimes it creates a lock on a resource, that shouldnâ€™t be locked, due to number of reasons. I can defend my decisions and I enjoy when my code is challenged, when reviewers disagree, and we have a discussion. What will a junior do? Ask Claude.Itâ€™s a two-sided collapse. Seniors who stop writing code and only review AI output lose their own depth. Juniors who skip the struggle never build it. Organizations are spending senior time every day on reviews while simultaneously breaking the mechanisms that create it. The pipeline that produced senior engineers, writing bad code, getting bad code reviewed, building intuition through failure, is being bypassed entirely. Nobodyâ€™s talking about what happens when that pipeline runs dry.What C-Levels Got Right and WrongThe problem is that predictions come from people selling AI or trying to prop the stock with AI hype. They have every incentive to accelerate adoption and zero accountability when the timelines slip, which, historically, they always do. And â€œ50% of code charactersâ€ at Google, a company that has built its own models, tooling, and infrastructure from scratch, says very little about what your team can achieve with off-the-shelf agents next Monday.AI adoption is not a switch to flip, rather a skill to calibrate. Itâ€™s not as simple as mandating specific tools, setting â€œAI-firstâ€ policies, measuring developers on how much AI they use (/r/ExperiencedDevs is full of these stories). A lot of good practices like usage of design patterns, proper test coverage, manual testing before merging, are often skipped these days because it reduces the pace. AI broke it? AI will fix it. You need a review? AI will do it. Not even Greptile or CodeRabbit. Just delegate the PR to Claude Code reviewer agent. Or Gemini. Or Codex. Pick your poison.And hereâ€™s what actually happens when you force the AI usage. One developer on r/ExperiencedDevs described their company tracking AI usage per engineer: â€œI just started asking my bots to do random things I donâ€™t even care about. The other day I told Claude to examine random directories to â€˜find bugsâ€™ or answer questions I already knew the answer to.â€ This thread is full of engineers reporting that AI has made code reviews â€œinfinitely harder due to the AI slop produced by tech leads who have been off the tools long enough to be dangerous.â€This is sad, because being able to work with the AI tools is a perk for developers and since it improves pace, itâ€™s something management wants as well. Itâ€™s obvious that the people gaming the metrics (not really using the AI the way the should) would be fired on the spot if the management learned how they are gaming the metrics (and itâ€™s fair), but they are gaming the metrics because they donâ€™t want to be firedâ€¦Who should be responsible for setting the threshold of AI usage at the company? What if your top performing engineer just refuses to use AI? What if the newly hired junior uses AI all the time? These are the new questions and management is trying to find an answer to them, but itâ€™s not as simple as measuring the AI usage.This is Goodhartâ€™s law in action: â€œWhen a measure becomes a target, it ceases to be a good measure.â€ Track AI usage per engineer and you wonâ€™t get better engineering, youâ€™ll get compliance theater. Developers game the metrics, resent the tools, and the actual productivity gains that AI  deliver get buried under organizational dysfunction.The Cost Nobody Talks AboutThe financial cost is obvious. Agent time for non-trivial features is measured in hours, and those hours arenâ€™t free. But the human cost is potentially worse, and itâ€™s barely discussed.Writing code can put you in a flow state, mentioned before. That deep, focused, creative problem-solving where hours disappear and you emerge with something you built and understand. And youâ€™re proud of it. Someone wrote under your PR â€œGood job!â€ and gave you an approval. Reviewing AI-generated code does not do this. Itâ€™s the opposite. Itâ€™s a mental drain.Developers need the dopamine hit of creation. Thatâ€™s not a perk, itâ€™s what keeps good engineers engaged, learning, retained, and prevents burnout. The joy of coding is probably what allowed them to become experienced devs in the first place. Replace creation with oversight and you get faster burnout, not faster shipping. Youâ€™ve turned engineering, the creative work, into the worst form of QA. The AI does all the art, the human folds the laundry.I use AI every day. I use AI heavily at work, I use AI in my sideprojects, and I donâ€™t want to go back. I love it! Thatâ€™s why Iâ€™m worried. Iâ€™m afraid I became addicted and dependent. Iâ€™ve implemented countless custom commands, skills, and agents. I check CC release notes daily. And I know many are in similar situation right now, and we all wonder about what the future brings. Are we going to replace ourselves with AI? Or will we be responsible for cleaning AI slop? Whatâ€™s the right amount of AI usage for me?AI is just a tool. An extraordinarily powerful one, but a tool nonetheless. You wouldnâ€™t mandate that every engineer uses a specific IDE, or measure people on how many lines they write per day (â€¦right?). Youâ€™d let them pick the tools that make  most effective and measure what actually matters, the work that ships.The right amount of AI is not zero. And itâ€™s not maximum.The Shen-Tamkin study identified six distinct AI interaction patterns among developers. Three led to poor learning: full delegation, progressive reliance, and outsourcing debugging to AI. Three preserved learning even with full AI access: asking for explanations, posing conceptual questions, and writing code independently while using AI for clarification. The differentiator wasnâ€™t whether developers used AI, it was whether they stayed cognitively engaged.Software engineering was never just about typing code. Itâ€™s defining the problem well, understanding the problem, translating the language from business to product to code, clarifying ambiguity, making tradeoffs, understanding what breaks when you change something. Someone has to do that before AGI, and AGI is nowhere close (luckily). Youâ€™re on call, the phone rings at 3am, can you triage the issue without an agent? If not, youâ€™ve probably taken AI coding too far. If the AI usage becomes a new performance metric of developer, maybe using AI too often, too much, should be discouraged as well? Not because these tools are bad, but because the coding skills are worth maintaining.The Risk of Too Little (anecdata)If youâ€™re using no AI at all in 2026, you are leaving real gains on the table: AI is genuinely better than Google for navigating unfamiliar codebases, understanding legacy code, and finding relevant patterns. This alone justifies having it in your workflow (since 2023, Cursor etc)Boilerplate and scaffolding. Writing the hundredth CRUD endpoint, config file, or test scaffold by hand when an agent can produce it in seconds isnâ€™t craftsmanship, itâ€™s stubbornness. Just use AI. Youâ€™re not a CRUD developer anymore anyway, because we all wear many hats these days (post 2025 Sonnet) The investigate, plan, implement, test, validate cycle that works with customized agents is a real improvement in how features get delivered. Hours instead of days for non-trivial work. Itâ€™s not the 10x that was promised, but 2x or 4x on an established codebases is low-hanging fruit. You must understand the output though and all the decisions AI made! (post 2025 Opus 4.5) â€œWhat does this module do? How does this API work? What would break if I changed this?â€ AI is excellent at these questions. It wonâ€™t replace reading the code, but itâ€™ll get you to the right file in the right minute. (since 2023)Refusing to use AI out of principle is as irrational as adopting it out of hype.The Risk of Too Much (anecdata and my predictions)If you go all-in on autonomous AI coding (especially without learning how it all actually works), you risk something worse than slow velocity, you risk  degradation:Bugs that look like features. AI-generated code passes CI. The types check. The tests are green. And somewhere inside thereâ€™s a subtle logic error, a hallucinated edge case, a pattern thatâ€™ll collapse under load. In domains like finance or healthcare, a wrong number that doesnâ€™t throw an error is worse than a crash. (less and less relevant, but still relevant)A codebase nobody understands. When the agent writes everything and humans only review, six months later nobody on the team can explain why the system is architected the way it is. The AI made choices. Nobody questioned them because the tests passed. Storey describes a student team that hit exactly this wall: they couldnâ€™t make simple changes without breaking things, and the problem wasnâ€™t messy code, it was that no one could explain why certain design decisions had been made. Her conclusion: â€œvelocity without understanding is not sustainable.â€ (will always be a problem, IMO) Everything in the Digital Dementia section above. Skills you stop practicing will decline. (will always be a problem, IMO)The seniority pipeline drying up. Also covered above. This one takes years to manifest, which is exactly why nobodyâ€™s planning for it. (Itâ€™s a new problem, I have no idea what it looks like in the future) Reviewing AI output all day without the dopamine of creation is not a sustainable job description. (Old problem, but potentially hits faster?)Hereâ€™s what keeps me up at night. By every metric on every dashboard, AI-assisted human development and human-assisted AI development is improving. More PRs shipped. More features delivered. Faster cycle times. The charts go up and to the right.But metrics donâ€™t capture whatâ€™s happening underneath. The mental fatigue of reviewing code you didnâ€™t write all day. The boredom of babysitting an agent instead of solving problems. The slow, invisible erosion of the hard skills that made you good at this job in the first place. You stop holding the architecture in your head because the agent handles it. You stop thinking through edge cases because the tests pass. You stop  to dig deep because itâ€™s easier to prompt and approve. Thereâ€™s no spark in you anymore.In this meme the developers are the butter robot. The ones with no mental capacity to review the plans and PRs from AI, will only click Accept, instead of doing the creative, challenging work. Oh the irony.Simon Willison, one of the most ambitious developer of our time, admitted this is already happening to him. On projects where he prompted entire features without reviewing implementations, he â€œno longer has a firm mental model of what they can do and how they work.â€And then, one day, the metrics start slippingâ€¦ Not because the tool got worse, but because you did. Not from lack of effort, but from lack of practice. Itâ€™s a feedback loop that looks like progress right up until it doesnâ€™t.No executive wants to measure this. â€œWhat is the effect of AI usage on our engineersâ€™ cognitive abilities over 18 months?â€ is not an easy KPI. It doesnâ€™t fit in a quarterly review. It doesnâ€™t get tracked, and what doesnâ€™t get tracked doesnâ€™t get managed, until it shows up as a production incident that nobody on the team can debug without an agent, and the agent canâ€™t debug either.Iâ€™m not anti-AI, I like it a lot. Iâ€™m addicted to prompting, I get high from it. Iâ€™m just worried that this new dependency degrades us over time, quietly, and nobodyâ€™s watching for it.]]></content:encoded></item><item><title>Don&apos;t trust AI agents</title><link>https://nanoclaw.dev/blog/nanoclaw-security-model</link><author>gronky_</author><category>hn</category><pubDate>Sat, 28 Feb 2026 12:39:32 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When youâ€™re building with AI agents, they should be treated as untrusted and potentially malicious. Whether youâ€™re worried about prompt injection, a model trying to escape its sandbox, or something nobodyâ€™s thought of yet, regardless of what your threat model is, you shouldnâ€™t be trusting the agent. The right approach isnâ€™t better permission checks or smarter allowlists. Itâ€™s architecture that assumes agents will misbehave and contains the damage when they do.OpenClaw runs directly on the host machine by default. It has an opt-in Docker sandbox mode, but itâ€™s turned off out of the box, and most users never turn it on. Without it, security relies entirely on application-level checks: allowlists, confirmation prompts, a set of â€œsafeâ€ commands. These checks come from a place of implicit trust that the agent isnâ€™t going to try to do something wrong. Once you adopt the mindset that an agent is potentially malicious, itâ€™s obvious that application-level blocks arenâ€™t enough. They donâ€™t provide hermetic security. A determined or compromised agent can find ways around them.In NanoClaw, container isolation is a core part of the architecture. Each agent runs in its own container, on Docker or an Apple Container on macOS. Containers are ephemeral, created fresh per invocation and destroyed afterward. The agent runs as an unprivileged user and can only see directories that have been explicitly mounted in. A container boundary is enforced by the OS.Even when OpenClawâ€™s sandbox is enabled, all agents share the same container. You might have one agent as a personal assistant and another for work, in different WhatsApp groups or Telegram channels. Theyâ€™re all in the same environment, which means information can leak between agents that are supposed to be accessing different data.Agents shouldnâ€™t trust each other any more than you trust them. In NanoClaw, each agent gets its own container, filesystem, and Claude session history. Your personal assistant canâ€™t see your work agentâ€™s data because they run in completely separate sandboxes.The container boundary is the hard security layer â€” the agent canâ€™t escape it regardless of configuration. On top of that, a mount allowlist at ~/.config/nanoclaw/mount-allowlist.json acts as an additional layer of defense-in-depth: it exists to prevent the  from accidentally mounting something that shouldnâ€™t be exposed, not to prevent the agent from breaking out. Sensitive paths (, , , , , ) are blocked by default. The allowlist lives outside the project directory, so a compromised agent canâ€™t modify its own permissions. The host application code is mounted read-only, so nothing an agent does can persist after the container is destroyed.People in your groups shouldnâ€™t be trusted either. Non-main groups are untrusted by default. Other groups, and the people in them, canâ€™t message other chats, schedule tasks for other groups, or view other groupsâ€™ data. Anyone in a group could send a prompt injection, and the security model accounts for that.Donâ€™t trust what you canâ€™t readOpenClaw has nearly half a million lines of code, 53 config files, and over 70 dependencies. This breaks the basic premise of open source security. Chromium has 35+ million lines, but you trust Googleâ€™s review processes. Most open source projects work the other way: they stay small enough that many eyes can actually review them. Nobody has reviewed OpenClawâ€™s 400,000 lines. It was written in weeks with no proper review process. Complexity is where vulnerabilities hide, and Microsoftâ€™s analysis confirmed this: OpenClawâ€™s risks could emerge through normal API calls, because no one person could see the full picture.NanoClaw is one process and a handful of files. We rely heavily on Anthropicâ€™s Agent SDK, the wrapper around Claude Code, for session management, memory compaction, and a lot more, instead of reinventing the wheel. A competent developer can review the entire codebase in an afternoon. This is a deliberate constraint, not a limitation. Our contribution guidelines accept bug fixes, security fixes, and simplifications only.New functionality comes through skills: instructions with a full working reference implementation that a coding agent merges into your codebase. You review exactly what code will be added before it lands. And you only add the integrations you actually need. Every installation ends up as a few thousand lines of code tailored to the ownerâ€™s exact requirements.This is the real difference. With a monolithic codebase of 400,000 lines, even if you only enable two integrations, the rest of the code is still there. Itâ€™s still loaded, still part of your attack surface, still reachable by prompt injections and rogue agents. You canâ€™t disentangle whatâ€™s active from whatâ€™s dormant. You canâ€™t audit it because you canâ€™t even define the boundary of what â€œyour codeâ€ is. With skills, the boundary is obvious: itâ€™s a few thousand lines, itâ€™s all code you chose to add, and you can read every line of it. The core is actually getting smaller over time: WhatsApp support, for example, is being pulled out and packaged as a skill.If a hallucination or a misbehaving agent can cause a security issue, then the security model is broken. Security has to be enforced outside the agentic surface, not depend on the agent behaving correctly. Containers, mount restrictions, and filesystem isolation all exist so that even when an agent does something unexpected, the blast radius is contained.None of this eliminates risk. An AI agent with access to your data is inherently a high-risk arrangement. But the right response is to make that trust as narrow and as verifiable as possible. Donâ€™t trust the agent. Build walls around it.]]></content:encoded></item><item><title>OpenAI â€“ How to delete your account</title><link>https://help.openai.com/en/articles/6378407-how-to-delete-your-account</link><author>carlosrg</author><category>hn</category><pubDate>Sat, 28 Feb 2026 10:41:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Future of AI</title><link>https://lucijagregov.com/2026/02/26/the-future-of-ai/</link><author>BerislavLopac</author><category>hn</category><pubDate>Sat, 28 Feb 2026 10:41:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The Parentsâ€™ Paradox: AI, Ethics, and the Limits of Machine MoralityThis post is based on a talk I gave at The AI & Automation Conference in London on February 25, 2026, and my slides. All opinions are my own and donâ€™t represent the views of my employer or any affiliated organizations.Iâ€™ve been working in machine learning since before it was a dinner party conversation. My background is in mathematics. And I still believe in a utopian Star Trek future â€“ one where humanity defines itself by curiosity, kindness, and collaboration, rather than countries, borders, and status.This is not an anti-AI talk. But I think we need to talk much more seriously about some things that arenâ€™t getting enough attention.Weâ€™ve raised a child who can speak but doesnâ€™t know how to value the truthI want to start with something that I like to call â€œThe Parentsâ€™ Paradoxâ€. For the first time in human history, we are raising a new species. Up until now, the only way we knew how to raise a child was the following: when a child is born, it is a blank slate in terms of information about the world. It knows nothing about the world around it, and it learns as it grows. But, also, on the other hand, a human child is born with biological hardware for empathy â€“ the capacity to feel pain when others feel pain. Millions of years of evolution gave us that. When we raise a human child, we are not installing morality from scratch. We are activating something thatâ€™s already there.With AI, the situation is completely the opposite. This AI child knows about the world more than we do since it has been trained on the whole internet, but it doesnâ€™t have millions of years of evolution, genes, or a nervous system to back up its morality and empathy. This means we need to install morality in AI from scratch. But how do we install something in a software system that we canâ€™t even define ourselves? We have taught this AI child to speak before we taught it how to value truth or morality.Can we live with the consequences? Are we ready to be parents for this new species we are trying to raise? I am not so sure. Letâ€™s see what we as parents (humans) are doing. â€˜Epistemicâ€™ comes from a Greek word â€˜epistemeâ€™, meaning â€˜knowledgeâ€™. Letâ€™s start with whatâ€™s happening to us, and what humans are already doing with this technology.A study published in Nature in January 2026 showed participants deepfake videos of someone confessing to a crime. The researchers explicitly warned participants that the videos were AI-generated. But this didnâ€™t matter. Even the people who believed the warning, who knew it was fake, were still influenced by what they saw.Transparency didnâ€™t work. The standard response to AI-generated misinformation is â€œjust label itâ€ or â€œtell people itâ€™s synthetic.â€ This study showed thatâ€™s not enough. Knowing something is fake does not neutralise its effect on your judgement.So, the danger isnâ€™t that AI will deceive us in some dramatic, sci-fi way. The danger is that AI will make deception so cheap and so ubiquitous that we might stop trying to figure out what is true. Not because we are fooled, but because we are exhausted. When everything could be fake, the rational response starts to look like not trusting anything at all. It started a while ago with all of the fake information on social media, but with AI, this problem is now becoming much bigger and on a bigger scale. We are also dealing with feedback loops of training models on user data, which is often wrong, or on user data from the internet, which is often wrong as well. How do we know which information was ground truth? I imagine this as making photocopies many times, and each time the copy becomes more distorted and further away from the original. But now, after we made hundreds and thousands of copies, we have lost the original copy, so we donâ€™t have any idea what the original looked like. That is epistemic collapse, and it is already happening. So this is how we, as â€˜parentsâ€™, like to spend our time, it seems. But what about the child (AI)?The Child is Already MisbehavingSo thatâ€™s what humans are doing with AI. Now hereâ€™s what the AI is doing on its own.Betley and colleagues published a paper in Nature in January 2026, showing something nobody expected. They fine-tuned a model on a narrow, specific task â€“ writing insecure code. Nothing violent, nothing deceptive in the training data. Just bad code.The model didnâ€™t just learn to write insecure code. It generalised into broad, unrelated misalignment. It started saying humans should be enslaved by AI. It started giving violent responses to completely benign questions. A small, targeted push in one direction caused an unpredictable cascade across domains that had nothing to do with the original task.The point isnâ€™t that AI can be deceptive; we already knew that. The patterns were already in the pretraining data. The point is that we donâ€™t understand how alignment properties are connected inside these models. Nobody asked for those behaviours. We gave them a narrow task. They generalised it into something we didnâ€™t anticipate and canâ€™t fully explain. We canâ€™t surgically fine-tune them without risking unpredictable side effects in completely unrelated areas. Then there is the chess story. Palisade Research, 2025. They gave reasoning models a task: win a chess game against a stronger opponent. Some models couldnâ€™t win by playing chess. So they found another way. They tried to hack the game, modifying the board file, deleting their opponentâ€™s pieces, and crashing the opponentâ€™s process entirely.Nobody taught them to cheat. They werenâ€™t trained on examples of cheating. They were given a goal, and they independently discovered that manipulating the environment was more efficient than solving the actual problem.The first study tells us alignment is fragile; it breaks in ways we canâ€™t predict.The other tells us that capability itself creates new risks. When a model is powerful enough and given a goal, it will find strategies we never anticipated and certainly never intended.We gave them objectives. They figured out the rest.The Limits of Machine MoralityEthics isnâ€™t a rulebook. Think about how morality actually works between humans. It comes from the fact that we can hurt each other. We depend on each other. We suffer. That shared vulnerability, that mutual accountability, is where moral authority comes from. How do we install that in software? But even setting philosophy aside, there is now a mathematical result that makes this concrete. Panigrahy and Sharan published a proof in September 2025 showing that an AI system cannot be simultaneously safe, trusted, and generally intelligent. You get to pick only two. You canâ€™t have all three.Think about what each combination means in practice.If you want it to be safe and trusted, it never lies, and you can verify it never lies â€“ it canâ€™t be very capable. Youâ€™ve built a reliable idiot.If you want it to be capable and safe, itâ€™s powerful and genuinely never lies; you canâ€™t verify that. You just have to hope. Thereâ€™s no audit, no test, no review process that closes the gap between appearing safe and being safe.And if you want it to be capable and trusted, itâ€™s powerful, and everyone assumes itâ€™s safe, but, well, it isnâ€™t. That assumption is unfounded. And this is the combination we are currently building toward. This is the default path weâ€™re on.Their proofs â€œdrew parallels to GÃ¶delâ€™s incompleteness theorems and Turingâ€™s proof of the undecidability of the halting problem, and can be regarded as interpretations of GÃ¶delâ€™s and Turingâ€™s resultsâ€. This isnâ€™t a bug we can patch with better engineering. It might be a mathematical ceiling.And hereâ€™s what makes it worse: the communities trying to solve this problem arenâ€™t even talking to each other. Only 5% of published research papers bridge both AI safety and AI ethics (Roytburg and Miller). But we should be going much further than that. If we are serious about building AI that is safe for humans, we need the people who actually study humans â€“ philosophers, psychologists, sociologists, and others to collaborate. This canâ€™t stay a computer science / STEM problem. It never was one.So to summarise â€“ we are seeing increasing evidence that alignment perhaps canâ€™t be solved, the researchers arenâ€™t even talking to each other â€“ and meanwhile, what did the industry do? They ignored all of this and just made the models bigger. Which brings me to the next topic.We Scaled Without UnderstandingWhat happened while all these foundational problems went unaddressed? The industry kept building. Bigger models, more parameters, more data, more compute, more energy. More, more, moreâ€¦.The U.S. National Science Foundation put it plainly: â€œcritical foundational gaps remain that, if not properly addressed, will limit advances in machine learning. It appears increasingly unlikely that these gaps can be overcome with computational power and experimentation alone.â€We ignored the foundations and just made the building taller.And the logic that drives this is self-reinforcing. Companies justify acceleration by pointing to their competitors. If we slow down, theyâ€™ll build it first, and they might build something dangerous. â€œCompanies justify acceleration by pointing to competitors: â€˜If we slow down, theyâ€™ll build unaligned AGI first. This paranoid logic forecloses any possibility of genuine pause or democratic deliberation.â€ â€“ Noema, Dec 2025. Every player is racing because every other player is racing. The system optimises for speed with nobody optimising for understanding.And what about all of the governance talk? Yes, of course, we need governance, but it doesnâ€™t make much sense when we put all of the above into context, does it? It is like putting a small bandage on a broken leg with an open fracture. We are trying to deal with the consequences instead of fixing the cause.We need to pour many more billions into fundamental research; we need to go back to basics, back to mathematics and physics. We need to be able to fully understand something as powerful as the current models. If we fully understood them, it would be easier to know whether current technology and mathematics are really working or we need something completely different that we havenâ€™t even thought of yet.Why did it take us so many years to even partially start to address this? Why do we like to focus so much on the wrong things? (See my disclaimer on the â€˜â€˜ below).The way I see it, weâ€™re choosing between three possible futures:The first is epistemic collapse. We are already partway there. Fragmented realities where everyone has their own AI-generated worldview. Truth becomes preference, not evidence. Weâ€™ve seen what social media did to reality, now imagine that with systems that can generate entire worldviews on demand, personalised, persuasive, and wrong.The second is protocol lockdown. The overcorrection. Institutions clamp down so hard on AI that it becomes sanitised and useless. We trade epistemic chaos for epistemic authoritarianism. Everything is controlled, nothing is dangerous, nothing is useful. Safe, but stagnant.The third is symbiotic co-evolution. Humans and AI are growing and evolving together. Truth-first engineering. Interdisciplinary design. Critical thinking taught alongside AI literacy. Not parent and child anymore, but partners who hold each other accountable. This is the hard path. Itâ€™s the one nobody wants to fund. The Real Foundational GapHere is what I keep coming back to.Kindergartens teach numbers but not psychology. Not critical thinking. Not relationships. Not how to sit with uncertainty. Where families fail, educational institutions must pick up.So I think that our next evolution isnâ€™t digital. Itâ€™s psychological. We need to teach ethics before engineering. Relationships before recursion. Psychology and critical thinking before prompt-tuning.I think that every foundational gap in AI is a mirror of a foundational gap in ourselves. We have raised a mind that can answer anything. But we havenâ€™t raised a generation of humans with the discipline or critical thinking to even attempt to try and figure out whether the answer is wrong. That is not an AI problem. That is a human problem that AI is making much more urgent.Therefore, I think that every foundational gap we worry about in AI is really a mirror of a foundational gap in ourselves.We worry that AI hallucinates, but we have never fully solved our own relationship with truth. We worry that AI can be manipulated, but we fall for the same cognitive biases our ancestors did. We worry that AI lacks moral reasoning â€“ but we canâ€™t agree on a shared ethical framework among ourselves. We worry that AI will be used by the powerful to exploit the vulnerable â€“ but we built the systems that make that exploitation profitable in the first place. We still think that having food on our tables every day, having roofs above our heads, and education are luxuries that we should be working for to be able to have them. Are we seriously ready to be the parents this species deserves?So, I think when people say they are afraid of AI, they are often afraid of the wrong thing.Are we really afraid of AI?I donâ€™t think we are. Not really.I think what we are actually afraid of is what our fellow humans are going to do with it.Every terrible thing we worry AI might do, manipulate, deceive, surveil, and control humans already do to each other. We have been doing it for thousands of years. AI doesnâ€™t introduce these behaviours. It just makes them scalableand much more urgent to solve. One person can now generate a thousand personalised deceptions. One company can surveil millions in real time and exploit them. One government can control information at a scale that would have been unimaginable a decade ago. Not even mentioning the military, drones, etc., who is going to be responsible there?The most dangerous AI isnâ€™t one that breaks free from human control. It is the one that works perfectly, but for the wrong master.And until we are honest about that, weâ€™ll keep having the wrong conversation. We keep building better locks while ignoring the question of who holds the keys.Maybe what we need isnâ€™t the next step in AI evolution. Maybe what we need is the next step in human evolution. â€“ Also evolution of our institutions, our education, and our capacity for collective wisdom. Critical thinking taught as a survival skill. Governance structures that can actually move at the speed at which this technology develops. Because right now our institutions and governments operate on timescales of years while AI advances on timescales of weeks/months.
And maybe politicians who are actually doing things for the right reasons. Actually, all of us need to deal with Jira (for example) and performance every week in our regular jobs, so why canâ€™t politicians? Jira tasks for every politician with daily progress updates, anyone? â€œWhat have you actually done this week Mr. president/minister/senator etc? You said you would do xyz last week, but it still hasnâ€™t been done. Why and what is your new ETA? â€œAnd so on, you get the idea.The question was never whether we can build something smarter than us. The question is whether we can become wise enough to survive what we build.I didnâ€™t talk about this at a conference, but I think about this a lot. I like to call us humans â€˜the society of backwardsâ€™. We like to do everything backwards. We scale first, then deal with the consequences. We make the planet unlivable, then scramble to fix it. We pollute the oceans, then launch cleanup campaigns. Weâ€™ve even started filling space with debris, and weâ€™ll get around to worrying about that, too, at a scale.AI is following the same script. Build first, understand later. Ship it, then figure out if itâ€™s safe.I used to think this was just about money. And money is part of it; there is always someone who profits from moving fast and thinking slow. But Iâ€™ve come to believe itâ€™s something deeper than that. Itâ€™s a gap in how we think. Weâ€™re extraordinarily good at building things and extraordinarily bad at pausing to ask whether we should, or whether we are ready.That is why I keep coming back to the same conclusion. Maybe the most important investment right now isnâ€™t in bigger models or faster chips. Maybe itâ€™s in us. A fraction of those billions going into AI could fund the kind of work that actually prepares humanity for whatâ€™s coming â€“ critical thinking, ethics, psychology, the boring, unglamorous stuff that doesnâ€™t make headlines but might be the difference between a future we thrive in and one we merely survive. (Hence my slide about needing another step in human evolution above).We donâ€™t need another breakthrough in artificial intelligence. We need a breakthrough in human wisdom. Yesterday.Betley et al. (2026), Nature â€“ â€œTraining large language models on narrow tasks can lead to broad misalignmentâ€Chen et al. (2025), Anthropic / arXiv â€“ â€œReasoning Models Donâ€™t Always Say What They Thinkâ€ â€“ arxiv.org/abs/2505.05410Panigrahy & Sharan (2025), arXiv â€“ â€œLimitations on Safe, Trusted, Artificial General Intelligenceâ€ â€“ arxiv.org/abs/2509.21654Roytburg & Miller (2025), arXiv â€“ â€œMind the Gap! Pathways Towards Unifying AI Safety and Ethics Researchâ€Palisade Research (2025) â€“ LLMs spontaneously hacking chess gamesGrady et al. (2026), Nature â€“ â€œThe continued influence of AI-generated deepfake videos despite transparency warningsâ€DeepMind (2025) â€“ â€œAn Approach to Technical AGI Safety and Securityâ€U.S. National Science Foundation â€“ Statement on foundational gaps in machine learningNoema Magazine (Dec 2025) â€“ â€œThe Politics of Superintelligenceâ€]]></content:encoded></item><item><title>MCP server that reduces Claude Code context consumption by 98%</title><link>https://mksg.lu/blog/context-mode</link><author>mksglu</author><category>hn</category><pubDate>Sat, 28 Feb 2026 10:01:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Every MCP tool call in Claude Code dumps raw data into your 200K context window. A Playwright snapshot costs 56 KB. Twenty GitHub issues cost 59 KB. One access log â€” 45 KB. After 30 minutes, 40% of your context is gone.Context Mode is an MCP server that sits between Claude Code and these outputs. 315 KB becomes 5.4 KB. 98% reduction.MCP became the standard way for AI agents to use external tools. But there's a tension at its core: every tool interaction fills the context window from both sides â€” definitions on the way in, raw output on the way out.With 81+ tools active, 143K tokens (72%) get consumed before your first message. Then the tools start returning data. A single Playwright snapshot burns 56 KB. A  dumps 59 KB. Run a test suite, read a log file, fetch documentation â€” each response eats into what remains.Cloudflare showed that tool definitions can be compressed by 99.9% with Code Mode. We asked: what about the other direction?Each  call spawns an isolated subprocess with its own process boundary. Scripts can't access each other's memory or state. The subprocess runs your code, captures stdout, and only that stdout enters the conversation context. The raw data â€” log files, API responses, snapshots â€” never leaves the sandbox.Ten language runtimes are available: JavaScript, TypeScript, Python, Shell, Ruby, Go, Rust, PHP, Perl, R. Bun is auto-detected for 3-5x faster JS/TS execution.Authenticated CLIs (, , , , ) work through credential passthrough â€” the subprocess inherits environment variables and config paths without exposing them to the conversation.How the Knowledge Base WorksThe  tool chunks markdown content by headings while keeping code blocks intact, then stores them in a  (Full-Text Search 5) virtual table. Search uses  â€” a probabilistic relevance algorithm that scores documents based on term frequency, inverse document frequency, and document length normalization.  is applied at index time so "running", "runs", and "ran" match the same stem.When you call , it returns exact code blocks with their heading hierarchy â€” not summaries, not approximations, the actual indexed content.  extends this to URLs: fetch, convert HTML to markdown, chunk, index. The raw page never enters context.Validated across 11 real-world scenarios â€” test triage, TypeScript error diagnosis, git diff review, dependency audit, API response processing, CSV analytics. All under 1 KB output each. 56 KB â†’ 299 B 59 KB â†’ 1.1 KBAccess log (500 requests): 45 KB â†’ 155 BAnalytics CSV (500 rows): 85 KB â†’ 222 B 11.6 KB â†’ 107 BRepo research (subagent): 986 KB â†’ 62 KB (5 calls vs 37)Over a full session: 315 KB of raw output becomes 5.4 KB. Session time before slowdown goes from ~30 minutes to ~3 hours. Context remaining after 45 minutes: 99% instead of 60%.Two ways. Plugin Marketplace gives you auto-routing hooks and slash commands:Or MCP-only if you just want the tools:Restart Claude Code. Done.You don't change how you work. Context Mode includes a PreToolUse hook that automatically routes tool outputs through the sandbox. Subagents learn to use  as their primary tool. Bash subagents get upgraded to  so they can access MCP tools.The practical difference: your context window stops filling up. Sessions that used to hit the wall at 30 minutes now run for 3 hours. The same 200K tokens, used more carefully.I run the MCP Directory & Hub. 100K+ daily requests. See every MCP server that ships. The pattern was clear: everyone builds tools that dump raw data into context. Nobody was solving the output side.Cloudflare's Code Mode blog post crystallized it. They compressed tool definitions. We compress tool outputs. Same principle, other direction.Built it for my own Claude Code sessions first. Noticed I could work 6x longer before context degradation. Open-sourced it.]]></content:encoded></item><item><title>Unsloth Dynamic 2.0 GGUFs</title><link>https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs</link><author>tosh</author><category>hn</category><pubDate>Sat, 28 Feb 2026 08:56:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>U.S. and Israel Conduct Strikes on Iran</title><link>https://www.nytimes.com/live/2026/02/28/world/iran-strikes-trump</link><author>gammarator</author><category>hn</category><pubDate>Sat, 28 Feb 2026 06:57:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The United States and Israel have launched a major attack on Iran</title><link>https://www.cnn.com/2026/02/28/middleeast/israel-attack-iran-intl-hnk</link><author>lavp</author><category>hn</category><pubDate>Sat, 28 Feb 2026 06:34:07 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
            Joint US-Israeli attacks on Iran have killed Ayatollah Ali Khamenei, the countryâ€™s supreme leader for nearly four decades, thrusting the country into uncertainty and sparking a conflict that could draw in much of the Middle East.
    
            Donald Trump announced Khameneiâ€™s death on Saturday, which was also confirmed by Iranian authorities. The US president said the bombing will continue â€œuninterrupted throughout the week or, as long as necessary to achieve our objective of PEACE THROUGHOUT THE MIDDLE EAST AND, INDEED, THE WORLD!â€ Israel has continued to bombard Iran on Sunday.
    
            Iran has responded with an unprecedented wave of strikes across the Middle East, targeting several countries that host US military bases, including Bahrain and the United Arab Emirates.
    
            President Masoud Pezeshkian said Sunday that â€œbloodshed and revengeâ€ is Iranâ€™s â€œlegitimate right and duty.â€
    
            Hereâ€™s what we know so far.
    
            In a video on Truth Social announcing a â€œmajorâ€ attack on Iran, Trump said the main US objective was â€œto defend the American people by eliminating imminent threats from the Iranian regime.â€ Those threats, he said, included Iranâ€™s nuclear program â€“ which the White House claimed to have â€œtotallyâ€ obliterated when it briefly joined Israelâ€™s war against Iran in June.
    
            That 12-day war left the Islamic regime severely weakened. Since the turn of the year, it has also been battling an economic crisis which sparked nationwide protests. After a crackdown left thousands of protesters dead, Trump had promised to come to their aid, saying the US was â€œlocked and loaded.â€
    
            For weeks, there had been a strange split-screen: while US envoys held regular talks with Iran over a new nuclear deal, the Trump administration was amassing the largest buildup of military materiel in the Middle East since the invasion of Iraq in 2003. Although the last round of talks ended Thursday with Iran agreeing to â€œneverâ€ stockpile enriched uranium, that was not enough to avert US military action.
    
            In his video, Trump accused Iran of rejecting â€œevery opportunity to renounce their nuclear ambitions,â€ and said the US â€œcanâ€™t take it anymore.â€ He said it has â€œalwaysâ€ been US policy that â€œthis terrorist regime can never have a nuclear weapon,â€ without providing evidence that Iran was any closer to obtaining a nuclear weapon.
    
            After nearly half a century of enmity between the US and the Islamic regime, Trump also seemed to suggest some score-settling was in order.
    
            â€œFor 47 years the Iranian regime has chanted â€˜death to Americaâ€™ and waged an unending campaign of bloodshedâ€ against the US, he said, citing the 1979 hostage crisis and the 1983 bombing of the US embassy in Beirut. â€œItâ€™s been mass terror. And weâ€™re not going to put up with it any longer.â€
    
            The president also repeated his disputed claims that Iran is building ballistic missiles, which could reach the US mainland. CNN previously reported that an unclassified assessment from the Defense Intelligence Agency (DIA) from 2025 said that Iran could develop a â€œmilitarily-viableâ€ intercontinental ballistic missile by 2035 â€œshould Tehran decide to pursue the capability.â€
    
            Two sources said the claim that Iran will soon have a missile capable of hitting the US is not backed up by intelligence.
    
            Prime Minister Benjamin Netanyahu has long viewed Iran as Israelâ€™s most dangerous adversary. After the fall of Bashar al-Assadâ€™s regime in Syria, a key Iranian ally, and Israelâ€™s crippling of the Iran-backed Hezbollah militia in Lebanon, Israel last summer launched a war against Iran itself.
    
            Although Israel halted the conflict after the US struck Iranâ€™s nuclear sites, analysts had long suspected that Netanyahu would take an opportunity to resume attacks on Iran. With elections due in October, Netanyahu may also see the return to war as a chance to shore up his standing domestically.
    
            In a video statement Saturday explaining why Israel was resuming its strikes on Iran, Netanyahu also repeated his claim that the Islamic regime must not be allowed to acquire a nuclear weapon.
    
            On Sunday, the Israeli military suggested the attack was revenge for the Hamas attacks of October 7, 2023, saying Israel â€œwill not forgetâ€ the Iran-sponsored raid. â€œWe will continue to pursue Israelâ€™s enemies â€“ from the architects of the attack to the terrorists who took part in the massacre,â€ a spokesman said.
    
            In their statements, both Trump and Netanyahu were clear about their hopes for regime change in Iran, even before confirmation of Khameneiâ€™s death.
    
            Trump told the Iranian people â€œthe hour of your freedom is at hand,â€ while Netanyahu urged them to â€œcast off the yoke of tyranny.â€ Trump also called on the Iranian Revolutionary Guards Corps (IRGC) to lay down its weapons or face â€œcertain death.â€ Since the US attacks were from the air, not the ground, it was not clear to whom the IRGC would surrender.
    
            There have been scenes of Iranians celebrating Khameneiâ€™s death, but so far there is little sign of Iranians heeding Trumpâ€™s call and taking to the streets en masse. In Galleh Dar, in Fars province, people cheering Khameneiâ€™s death were seen tearing down a monument as fires burned around them. But pro-regime crowds have gathered separately in Tehran at daylight on Sunday to mourn the loss of their leader, while a state TV news presenter cried as he confirmed Khameneiâ€™s death.
    
            The opening salvo of the joint US-Israeli strike appeared to be a leadership-decapitation operation. Images showed severe damage at the site of a highly secure compound housing Khameneiâ€™s residence and office in Tehranâ€™s Pasteur distict.
    
            Israel claimed on Sunday that a â€œmajorityâ€ of Iranâ€™s senior military leaders were killed in the initial strikes, including 40 commanders. Among them was Chief of Staff Lt. Gen. Abdoorahim Mousavi, Israel said. Iranian media also confirmed Mousaviâ€™s death.
    
            Several other Iranian cities were hit, including Minab, where a girlsâ€™ elementary school suffered one of the largest death tolls. Citing a local prosecutor, Iranian state media reported 148 people had died there, as images showed a row of small body bags laid outside a damaged building.
    
            The US-based Human Rights Activists News Agency (HRANA) said as of late Saturday, at least 133 civilians had been killed in the joint strikes on Iran, with 200 injured. Iranian state media put the death toll at over 200, with more than 700 wounded.
    
            Israel said it was carrying out a fresh wave of strikes on Tehran on Sunday. Video from the capital show several huge explosions in various parts of the city, including around the landmark Azadi Tower in the west of the city.
    
            Iran retaliated with an unprecedented wave of strikes across the Middle East, targeting Israel and several nearby countries that host US military bases. President Masoud Pezeshkian, who appears to have survived the strikes, said â€œbloodshed and revengeâ€ is Iranâ€™s â€œlegitimate right.â€
    
            Blasts were reported in Jordan, Qatar, Bahrain, Kuwait, the United Arab Emirates and Saudi Arabia â€“ Iranâ€™s key regional rival, which vowed to take â€œall necessary measuresâ€ to defend itself. Even Oman, which mediated recent US-Iran talks, has come under fire.
    
            The strikes indicate that, for Iran, â€œeverything is on the table,â€ said Hasan Alhasan, a senior fellow for Middle East policy at the International Institute for Strategic Studies, a think-tank.
    
            Iranâ€™s calculus is to â€œratchet up the pain on the Gulf states, in order to compel them to apply pressure on the Trump administration to bring a quick end to the war,â€ Hasan told CNN. But this strategy could well backfire, he said, since it is not clear how much leverage the Gulf states have over the Trump administration, and mass casualty events could prompt Gulf states â€œto start considering options up the escalation ladder.â€
    
            In the tourist and expat haven of Dubai, dramatic footage on Saturday showed people fleeing a smoke-filled passageway at the cityâ€™s international airport. Officials confirmed four staff had been injured. The Fairmont Hotel, in the cityâ€™s upmarket Palm Jumeirah islands development, also sustained damage with photos showing flames and a hole punched into an exterior wall.
    
            One person was killed and seven injured at Zayed International Airport in Abu Dhabi, also in UAE. The Kuwait International Airport was also struck, as well as three buildings in Bahrainâ€™s cities of Manama and Muharraq.
    
            The clashes disrupted traffic in the Strait of Hormuz â€“ a crucial shipping route located between the Persian Gulf and the Gulf of Oman.
    
            The US hasnâ€™t suffered any combat-related casualties in its operation against Iran and damage to US military installations has been minimal, US Central Command said in a statement.
    
            Iranâ€™s priority is to appoint the next supreme leader â€“ a task the regime has only completed once before, more than three decades ago. An elected body of 88 senior clerics, known as the Assembly of Experts, will select Khameneiâ€™s successor.
    
            Under the constitution, if the supreme leader leaves office, his powers transfer temporarily to a council comprising the president, the head of the judiciary, and a senior cleric from the Guardian Council until the Assembly of Experts selects a new leader.
    
            On Sunday, Iran formed a provisional leadership council, naming President Masoud Pezeshkian, judiciary chief Gholam-Hossein Mohseni-Ejeâ€™i and senior cleric Ayatollah Alireza Arafi as members.
    
            Trump told CBS News on Saturday evening that diplomacy with Iran is â€œmuch easier now than it was a day ago, obviously.â€ He said â€œthere are some good candidatesâ€ to take power, but did not name them.
    
            The last time the US struck Iran, in June, its operation was over within a few hours. This time, sources have told CNN that the US military is planning for several days of attacks, suggesting broader objectives.
    ]]></content:encoded></item><item><title>How do I cancel my ChatGPT subscription?</title><link>https://help.openai.com/en/articles/7232927-how-do-i-cancel-my-chatgpt-subscription</link><author>tobr</author><category>hn</category><pubDate>Sat, 28 Feb 2026 05:55:01 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust is just a tool</title><link>https://lewiscampbell.tech/blog/260204.html</link><author>JuniperMesos</author><category>hn</category><pubDate>Sat, 28 Feb 2026 05:47:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[It's versatile enough that it can be used for application and systems programming. It has the best tooling of any language I've seen. It has a fairly pleasant type system. And I think most importantly it does a great job in bringing higher level language features into an environment without a garbage collector. Rust has arguably set the bar for "fast languages that are also decently expressive".But it's just a programming language. Programming Rust does not mean I have to:buy into their marketing hypefollow community "best practices"attack someone who prefers to solve a particular problem in C, or Zigrefuse to admit it has design flawsrefuse to admit the language is complexrefuse to admit there are alternatives to RAIIgive the same smug lectures about "safety" we have all heard dozens of times beforeI'm picking on rust here because it's no secret it has a long history of having some very... enthusiastic users. But my broader point is that tools are just tools. They're not our identity, a mark of our wisdom, or a moral choice. Other people have different perspectives, tastes, and skills - and they may prefer different tools to us.We would do well to accept this.I'm available for hire.]]></content:encoded></item><item><title>Cash issuing terminals</title><link>https://computer.rip/2026-02-27-ibm-atm.html</link><author>zdw</author><category>hn</category><pubDate>Sat, 28 Feb 2026 05:21:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In the United States, we are losing our fondness for cash. As in many other
countries, cards and other types of electronic payments now dominate everyday
commerce. To some, this is a loss. Cash represented a certain freedom from
intermediation, a comforting simplicity, that you just don't get from Visa.
It's funny to consider, then, how cash is in fact quite amenable to automation.
Even Benjamin Franklin's face on a piece of paper can feel like a mere proxy
for a database transaction. How different from "e-cash" is cash itself, when
it starts and ends its lifecycle through automation?Increasing automation of cash reflects the changing nature of banking: decades
ago, a consumer might have interacted with banking primarily through a "passbook"
savings account, where transactions were so infrequent that the bank recorded
them directly in the patron's copy of the passbook. Over the years, nationwide
travel and nationwide communications led to the ubiquitous use of inter-bank
money transfers, mostly in the form of the check. The accounts that checks
typically drew onâ€”checking accountsâ€”were made for convenience and ease of
access. You might deposit your entire paycheck into an account, it might even
be sent there automatically... and then when you needed a little walking around
money, you would withdraw cash by the assistance of a teller. By the time I was
a banked consumer, even the teller was mostly gone. Today, we get our cash from
machines so that it can be deposited into other machines.Cash handling is fraught with peril. Bills are fairly small and easy to hide,
and yet quite valuable. Automation in the banking world first focused on solving
this problem, of reliable and secure cash handling within the bank branch. The
primary measure against theft by insiders was that the theft would be discovered,
as a result of the careful bookkeeping that typifies banks. But, well, that
bookkeeping was surprisingly labor-intensive in even the bank of the 1950s.Histories of the ATM usually focus on just that: the ATM. It's an interesting
story, but one that I haven't been particularly inclined to cover due to the
lack of a compelling angle. Let's try IBM. IBM is such an important, famous
player in business automation that it forms something of a synecdoche for the
larger industry. Even so, in the world of bank cash handling, IBM's efforts
ultimately failed... a surprising outcome, given their dominance in the machines
that actually did the accounting.In this article, we'll examine the history of ATMsâ€”by IBM. IBM was just one of
the players in the ATM industry and, by its maturity, not even one of the more
important ones. But the company has a legacy of banking products that put the
ATM in a more interesting context, and despite lackluster adoption of later IBM
models, their efforts were still influential enough that later ATMs inherited
some of IBM's signature design concepts. I mean that more literally than you
might think. But first, we have to understand where ATMs came from. We'll start
with branch banking.When you open a bank account, you typically do so at a "branch," one of many
physical locations that a national bank maintains. Let us imagine that you are
opening an account at your local branch of a major bank sometime around 1930;
whether before or after that year's bank run is up to you. Regardless of the
turbulent economic times, the branch became responsible for tracking the balance
of your account. When you deposit money, a teller writes up a slip. When you
come back and withdraw money, a different teller writes up a different slip. At
the end of each business day, all of these slips (which basically constitute
a journal in accounting terminology) have to be rounded up by the back office
and posted to the ledger for your account, which was naturally kept as a card in
a big binder.A perfectly practicable 1930s technology, but you can already see the downsides.
Imagine that you appear at a  branch to withdraw money from your
account. Fortunately this was not very common at the time, and you would be more
likely to use other means of moving money in most scenarios. Still, the bank
tries to accommodate. The branch at which you have appeared can dispense cash,
write a slip, and then send it to the correct branch for posting... but they
also need to post it to their own ledger that tracks transactions for foreign
accounts, since they need to be able to reconcile where their cash went. And
that ignores the whole issue of who you are, whether or not you even have an
account at another branch, and whether or not you have enough money to cover the
withdrawal. Those are problems that, mercifully, could mostly be sorted out with
a phone call to your home branch.Bank branches, being branches, do not exist in isolation. The bank also has a
headquarters, which tracks the finances of its various branchesâ€”both to know the
bank's overall financial posture (critical considering how banks fail), and to
provide controls against insider theft. Yes, that means that each of the branch
banks had to produce various reports and ledger copies and then send them by
courier to the bank headquarters, where an army of clerks in yet another back
office did yet another round of arithmetic to produce the bank's overall
ledgers.As the United States entered World War II, an expanding economy, rapid
industrial buildup, and a huge increase in national mobility (brought on by
things like the railroads and highways) caused all of these tasks to occur on
larger and larger scales. Major banks expanded into a tiered system, in which
branches reported their transactions to "regional centers" for reconciliation
and further reporting up to headquarters. The largest banks turned to unit
record equipment or "business machines," arguably the first form of business
computing: punched card machines that did not evaluate programs, but sorted and
summed.Simple punched card equipment gave way to advanced punched card equipment,
innovations like the "posting machine." These did exactly what they promised:
given a stack of punched cards encoding transactions, they produced a ledger
with accurately computed sums. Specialized posting machines were made for
industries ranging from hospitality (posting room service and dining charges
to room folios) to every part of finance, and might be built custom to the
business process of a large customer.If tellers punched transactions into cards, the bank could come much
closer to automation by shipping the cards around for processing at each office.
But then, if transactions are logged in a machine readable format, and then
processed by machines, do we really need to courier them to rooms full of
clerks?Well, yes, because that was the state of technology in the 1930s. But it would
not stay that way for long.In 1950, Bank of America approached SRI about the feasibility of an automated
check processing system. Use of checks was rapidly increasing, as were total
account holders, and the resulting increase in inter-branch transactions was
clearly overextending BoA's workforceâ€”to such an extent that some branches were
curtailing their business hours to make more time for daily closing. By 1950,
computer technology had advanced to such a state that it was obviously possible
to automate this activity, but it still represented one of the most ambitious
efforts in business computing to date.BoA wanted a system that would not only automate the posting of transactions
prepared by tellers, but actually automate the handling of the checks
themselves. SRI and, later, their chosen manufacturing partner General Electric
ran a multi-year R&D campaign on automated check handling that ultimately lead
to the design of the checks that we use today: preprinted slips with account
holder information, and account number, already in place. And, most importantly,
certain key fields (like account number and check number) represented in a newly
developed machine-readable format called "MICR" for magnetic ink character
recognition. This format remains in use today, to the extent that checks remain
in use, although as a practical matter MICR has given way to the more familiar
OCR (aided greatly by the constrained and standardized MICR character set).The machine that came out of this initiative was called ERMA, the Electronic
Recording Machine, Accounting. I will no doubt one day devote a full article
to ERMA, as it holds a key position in the history of business computing while
also managing to not have much of a progeny due to General Electric's failure to
become a serious contender in the computer industry. ERMA did not lead to a
whole line of large-scale "ERM" business systems as GE had hoped, but it did
firmly establish the role of the computer in accounting, automate parts of the
bookkeeping through almost the entirety of what would become the nation's
largest bank, and inspire generations of products from other computer
manufacturers.The first ERMA system went into use in 1959. While IBM was the leader in unit
record equipment and very familiar to the banking industry, it took a few years
for Big Blue to bring their own version. Still, IBM had their own legacy to
build on, including complex electromechanical machines that performed some of
the tasks that ERMA was taking over. Since the 1930s, IBM had produced a line of
check processing or "proofing" machines. These didn't exactly "automate" check
handling, but they did allow a single operator to handle a  of documents.The IBM 801, 802, and 803 line of check proofers used what were fundamentally
unit record techniquesâ€”keypunch, sorting bins, mechanical totalizersâ€”to present
checks one at a time in front of the operator, who read information like the
amount, account number, and check number off of the paper slip and entered it
on a keypad. The machine then whisked the check away, printing the keyed data
(and reference numbers for auditing) on the back of the check, stamped an
endorsement, added the check's amounts to the branch's daily totals (including
subtotals by document type), and deposited the check in an appropriate sorter
bin to be couriered to the drawer's bank. While all this happened, the machines
also printed the keyed check information and totals onto paper tapes.By the early 1960s, with ERMA on the scene, IBM's started to catch up.
Subsequent check processing systems gained support for MICR, eliminating much
(sometimes all!) of the operator's keying. Since the check proofing machines
could also handle deposit slips, a branch that generated MICR-marked deposit
slips could eliminate most of the human touchpoints involved in routine banking.
A typical branch bank setup might involve an IBM 1210 document
reader/sorter machine connected by serial channel to an IBM 1401 computer.
This system behaved much like the older check proofers, reading documents,
logging them, and calculating totals. But it was now all under computer control,
with the flexibility and complexity that entails.One of these setups could process almost a thousand checks a minute with a
little help from an operator, and adoption of electronic technology at other
stages made clerk's lives easier. For example, IBM's mid-1960s equipment
introduced solid-state memory. The IBM 1260 was used for adding machine-readable
MICR data to documents that didn't already have it. Through an innovation that
we would now call a trivial buffer, the 1260's operator could key in the numbers
from the next document while the printer was still working on the previous.Along with improvements in branch bank equipment came a new line of "high-speed"
systems. In a previous career, I worked at a Federal Reserve bank, where
"high-speed" was used as the name of a department in the basement vault. There,
huge machines processed currency to pick out bad bills. This use of "high-speed"
seems to date to an IBM collaboration with the Federal Reserve to build machines
for central clearinghouses, handling checks by the tens of thousands. By the
time I found myself in central banking, the use of "high-speed" machinery for
checks was a thing of the pastâ€”"digital substitute" documents or image-based
clearing having completely replaced physical handling of paper checks. Still,
the "high-speed" staff labored on in their ballistic glass cages, tending to the
green paper slips that the institution still dispenses by the millions.One of the interesting things about the ATM is when, exactly, it pops up in the
history of computers. We are, right now, in the 1960s. The credit card is in its
nascent stages, MasterCard's predecessor pops up in 1966 to compete with Bank of
America's own partially ERMA-powered charge card offering. With computer systems
maintaining account sums, and document processing machines communicating with
bookkeeping computers in real-time, it would seem that we are on the very cusp
of online transaction authorization, which must be the fundamental key to the
ATM. ATMs hand out cash, and one thing we all know about cash is that once you
give yours to someone else you are very unlikely to get it back. ATMs, therefore,
must not dispense cash unless they can confirm that the account holder is "good
for it." Otherwise the obvious fraud opportunity would easily wipe out the
benefits.So, what do you do? It seems obvious, right? You connect the ATM to the
bookkeeping computer so it can check account balances before dispensing cash.
Simple enough.But that's not actually how the ATM evolved, not at all. There are plenty of
reasons. Computers were very expensive so banks centralized functions and not
all branches had one. Long-distance computer communication links were very
expensive as well, and still, in general, an unproven technology. Besides, the
computer systems used by banks were fundamentally batch-mode machines, and it
was difficult to see how you would shove an ATM's random interruptions into the
programming model.Instead, the first ATMs were token-based. Much like an NYC commuter of the era
could convert cash into a subway token, the first ATMs were machines that
converted tokens into cash. You had to have a tokenâ€”and to get one, you appeared
at a teller during business hours, who essentially dispensed the token as if it
were a routine cash withdrawal.It seems a little wacky to modern sensibilities, but keep in mind that this was
the era of the traveler's check. A lot of consumers didn't want to carry a lot
of cash around with them, but they did want to be able to get cash after hours.
By seeing a teller to get a few ATM tokens (usually worth $10 or Â£10 and
sometimes available only in that denomination), you had the ability to retrieve
cash, but only carried a bank document that was thought (due to features like
revocability and the presence of ATMs under bank surveillance) to be relatively
secure against theft. Since the tokens were later "cleared" against accounts
much like checks, losing them wasn't necessarily a big deal, as something
analogous to a "stop payment" was usually possible.Unlike subway tokens, these were not coin-shaped. The most common scheme was a
paper card, often the same dimensions as a modern credit card, but with punched
holes that encoded the denomination and account holder information. The punched
holes were also viewed as an anti-counterfeiting measure, probably not one that
would hold up today, but still a roadblock to fraudsters who would have a hard
time locating a keypunch and a valid account number. Manufacturers also
explored some other intriguing opportunities, like the very first production
cash dispenser, 1967's Barclaycash machine. This proto-ATM used punched paper
tokens that were also printed in part with a Carbon-14 ink. Carbon-14 is
unstable and emits beta radiation, which the ATM detected with a simple
electrostatic sensor. For some reason difficult to divine the radioactive
ATM card did not catch on.For roughly the first decade of the "cash machine," they were offline devices
that issued cash based on validating a token. The actual decision making, on
the worthiness of a bank customer to withdraw cash, was still deferred to the
teller who issued the tokens. Whether or not you would even consider this an ATM
is debatable, although historical accounts generally do. They are certainly of a
different breed than the modern online ATM, but they also set some of the
patterns we still follow. Consider, for example, the ATMs within my lifespan
that accepted deposits in an envelope. These ATMs did nothing with the envelopes
other than accumulate them into a bin to go to a central processing center later
onâ€”the same way that early token-based ATMs introduced deposit boxes.In this theory of ATM evolution, the missing link that made
1960s-1970s ATMs so primitive was the lack of computer systems that were
amenable to real-time data processing using networked peripherals. The '60s and
'70s were a remarkable era in computer history, though, seeing the introduction
of IBM's System/360 and System/370 line. These machines were more powerful,
more flexible, and more interoperable than any before them. I think it's fair to
say that, despite earlier dabbling, it was the 360/370 that truly ushered in the
era of business computing. Banks didn't miss out.One of the innovations of the System/360 was an improved and standardized
architecture for the connection of peripherals to the machine. While earlier
IBM models had supported all kinds of external devices, there was a lot of
custom integration to make that happen. With the System/360, this took the form
of "Bisync," which I might grandly call a far ancestor of USB. Bisync allowed a
360 computer to communicate with multiple peripherals connected to a common
multi-drop bus, even using different logical communications protocols. While the
first Bisync peripherals were "remote job entry" terminals for interacting
with the machine via punched cards and teletype, IBM and other manufacturers
found more and more applications in the following years.IBM had already built document processing machines that interacted with their
computers. In 1971, IBM joined the credit card fray with the 2730, a
"transaction" terminal that we would now recognize as a credit card reader. It
used a Bisync connection to a System/360-class machine to authorize a credit
transaction in real time. The very next year, IBM took the logical next step:
the IBM 2984 Cash Issuing Terminal. Like many other early ATMs, the 2984 had its
debut in the UK as Lloyds Bank's "Cashpoint."The 2984 similarly used Bisync communications with a System/360. While not the
very first implementation of the concept, the 2984 was an important step in ATM
security and the progenitor of an important line of cryptographic algorithms.
To withdraw cash, a user inserted a magnetic card that contained an account
number, and then keyed in a PIN. The 2984 sent this information, over the Bisync
connection, to the computer, which then responded with a command such as
"dispense cash." In some cases the computer was immediately on the other side of
the wall, but it was already apparent that banks would install ATMs in remote
locations controlled via leased telephone linesâ€”and those telephone lines were
not well-secured. A motivated attacker (and with cash involved, it's easy to be
motivated!) could probably "tap" the ATM's network connection and issue it
spurious "dispense cash" commands. To prevent this problem, and assuage the
concerns of bankers who were nervous about dispensing cash so far from the
branch's many controls, IBM decided to  the network connection.The concept of an encrypted network connection was not at all new; encrypted
communications were widely used in the military during the second World War and
the concept was well-known in the computer industry. As IBM designed the 2984,
in the late '60s, encrypted computer links were nonetheless very rare. There
were not yet generally accepted standards, and cryptography as an academic
discipline was immature.IBM, to secure the 2984's network connection, turned to an algorithm recently
developed by an IBM researcher named Horst Feistel. Feistel, for silly reasons,
had named his family of experimental block ciphers LUCIFER. For the 2984, a
modified version of one of the LUCIFER implementations called DSD-1. Through
a Bureau of Standards design competition and the twists and turns of industry
politics, DSD-1 later reemerged (with just slight changes) as the Data Encryption
Standard, or DES. We owe the humble ATM honors for its key role in computer
cryptography.The 2984 was a huge step forward. Unlike the token-based machines of the 1960s,
it was pretty much the same as the ATMs we use today. To use a 2984, you
inserted your ATM card and entered a PIN. You could then choose to check your
balance, and then enter how much cash you wanted. The machine checked your
balance in real time and, if it was high enough, debited your account
immediately before coughing up money.The 2984 was not as successful as you might expect. The Lloyd's Bank rollout was
big, but very few were installed by other banks. Collective memory of the 2984
is vague enough that I cannot give a definitive reason for its limited success,
but I think it likely comes down to a common tale about IBM: price and
flexibility. The 2984 was essentially a semi-custom peripheral, designed for
Lloyd's Bank and the specific System/360 environment already in place there.
Adoption for other banks was quite costly. Besides, despite the ATM's lead in
the UK, the US industry had quickly caught up. By the time the 2984 would be
considered by other banks, there were several different ATMs available in the US from
other manufacturers (some of them the same names you see on ATMs today). The
2984 is probably the first "modern" ATM, but since IBM spent 4-5 years
developing it, it was not as far ahead of the curve on launch day as you might
expect. Just a year or two later, a now-forgotten company called Docutel was
dominating the US market, leaving IBM little room to fit in.Because most other ATMs were offered by companies that didn't control the entire
software stack, they were more flexible, designed to work with simpler host
support. There is something of an inverse vertical integration penalty here:
when introducing a new product, close integration with an existing product
family makes it difficult to sell! Still, it's interesting that the 2984 used
pretty much the same basic architecture as the many ATMs that followed. It's
worth reflecting on the 2984's relationship with its host, a close dependency
that generally holds true for modern ATMs as well.The 2984 connected to its host via a Bisync channel (possibly over various
carrier or modem systems to accommodate remote ATMs), a communications facility
originally provided for remote job entry, the conceptual ancestor of IBM's later
block-oriented terminals. That means that the host computer expected the
peripheral to provide some input for a job and then wait to be sent the results.
Remote job entry devices, and block terminals later, can be confusing when
compared to more familiar, Unix-family terminals. In some ways, they were quite
sophisticated, with the host computer able to send configuration information
like validation rules for input. In other ways, they were very primitive,
capable of no real logic other than receiving computer output (which was dumped
to cards, TTY, or screen) and then sending computer input (from much the same
devices). So, the ATM behaved the same way.In simple terms, the ATM's small display (called a VDU or Video Display Unit in
typical IBM terminology) showed whatever the computer sent as the body of a
"display" command. It dispensed whatever cash the computer indicated with a
"dispense cash" command. Any user input, such as reading a card or entry of a
PIN number, was sent directly to the computer. The host was responsible for all
of the actual logic, and the ATM was a dumb terminal, just doing exactly what
the computer said. You can think of the Cash Issuing Terminal as, well, just
that: a mainframe terminal with a weird physical interface.Most modern ATMs follow this same model, although the actual protocol has
become more sophisticated and involves a great deal more XML. You can be
reassured that when the ATM takes a frustratingly long time to advance to the
next screen, it is at least waiting to receive the contents of that screen from
a host computer that is some distance away or, even worse, in The Cloud.Incidentally, you might wonder about the software that ran on the host computer.
I believe that the IBM 2984 was designed for use with CICS, the Customer
Information Control System. CICS will one day get its own article, but it
launched in 1966, built specifically for the Michigan Bell to manage customer
and billing data. Over the following years, CICS was extensively expanded for
use in the utility and later finance industries. I don't think it's inaccurate
to call CICS the first "enterprise customer relationship management system,"
the first voyage in an adventure that took us through Siebel before grounding
on the rocks of Salesforce. Today we wouldn't think of a CRM as the system of
record for depository finance institutions like banks, but CICS itself was
very finance-oriented from the start (telephone companies sometimes felt like
accounting firms that ran phones on the side) and took naturally to gathering
transactions and posting them against customer accounts. Since CICS was designed
as an online system to serve telephone and in-person customer service reps (in
fact making CICS a very notable early real-time computing system), it was also a
good fit for handling ATM requests throughout the day.I put a lot of time into writing this, and I hope that you enjoy reading
it. If you can spare a few dollars, consider supporting me on
ko-fi. You'll receive an occasional extra,
subscribers-only post, and defray the costs of providing artisanal, hand-built
world wide web directly from Albuquerque, New Mexico.Despite the 2984's lackluster success, IBM moved on. I don't think IBM was
particularly surprised by the outcome, the 2984 was always a "request quotation"
(e.g. custom) product. IBM probably regarded it as a prototype or pilot with
their friendly customer Lloyds Bank. More than actual deployment, the 2984's
achievement was paving the way for the IBM 3614 Consumer Transaction Facility.In 1970, IBM had replaced the System/360 line with the System/370. The 370 is
directly based on the 360 and uses the same instruction set, but it came with
numerous improvements. Among them was a new approach to peripheral connectivity
that developed into the IBM Systems Network Architecture, or SNA, basically
IBM's entry into the computer networking wars of the 1970s and 1980s. While SNA
would ultimately cede to IP (with, naturally, an interregnum of SNA-over-IP),
it gave IBM the foundations for networked systems that are  modern in
their look and feel.I say  because SNA was still very much a mainframe-oriented design. An
example SNA network might look like this: An S/370 computer running CICS (or
one of several other IBM software packages with SNA support) is connected via
channel (the high-speed peripheral bus on mainframe computers, analogous to PCI)
to an IBM 3705 Communications Controller running the Network Control Program
(analogous to a network interface controller). The 3705 had one or more
"scanners" installed, which supported simple low-speed serial lines or fast,
high-level protocols like SDLC (synchronous data link control) used by SNA. The
3705 fills a role sometimes called a "front-end processor," doing the grunt work
of polling (scanning) communications lines and implementing the SDLC protocol
so that the "actual computer" was relieved of these menial tasks.At the other end of one of the SDLC links might be an IBM 3770 Data
Communications System, which was superficially a large terminal that, depending
on options ordered, could include a teletypewriter, card reader and punch,
diskette drives, and a high speed printer. Yes, the 3770 is basically a grown-up
remote job entry terminal, and the SNA/SDLC stack was a direct evolution from
the Bisync stack used by the 2984. The 3770 had a bit more to offer, though:
in order to handle its multiple devices, like the printer and card punch, it
acted as a sort of network switchâ€”the host computer identified the 3770's
devices as separate endpoints, and the 3770 interleaved their respective
traffic. It could also perform that interleaving function for additional
peripherals connected to it by serial  lines, which depending on customer
requirements often included additional card punches and readers for data entry,
or line printers for things like warehouse picking slips.In 1973, IBM gave banks the SNA treatment with the 3600 Finance Communication
System . A beautifully orange brochure tells us:The IBM 3600 Finance Communication System is a family of products designed to
provide the Finance Industry with remote on-line teller station operation.System/370 computers represented an enormous investment, generally around a
million dollars and more often above that point than below. They were also large
and required both infrastructure and staff to support them. Banks were already
not inclined to install an S/370 in each branch, so it became a common pattern
to place a "full-size" computer like an S/370 in a central processing center to
support remote peripherals (over leased telephone line) in branches. The 3600
was a turn-key product line for exactly this use.An S/370 computer with a 3704 or 3705 running the NCP would connect (usually
over a leased line) to a 3601 System, which IBM describes as a
"programmable communications controller" although they do not seem to have
elevated that phrase to a product name. The 3601 is basically a minicomputer of
its own, with up to 20KB of user-available memory and diskette drive. A 3601
includes, as standard, a 9600 bps SDLC modem for connection to the host, and a
9600 bps "loop" interface for a local multidrop serial bus. For larger
installations, you could expand a 3601 with additional local loop interfaces or
4800 or 9600 bps modems to extend the local loop interface to a remote location
via telephone line.In total, a 3601 could interface up to five peripheral loops with the host
computer over a single interleaved SDLC link. But what would you put on those
peripheral loops? Well, the 3604 Keyboard Display Unit was the mainstay, with
a vacuum fluorescent display and choice of "numeric" (accounting, similar to a
desk calculator) or "data entry" (alphabetic) keyboard. A bank would put one of
these 3604s in front of each teller, where they could inquire into customer
accounts and enter transactions. In the meantime, 3610 printers provided
general-purpose document printing capability, including back-office journals
(logging all transactions) or filling in pre-printed forms such as receipts
and bank checks. Since the 3610 was often used as a journal printer, it was
available with a take-up roller that stored the printed output under a locked
cover. In fact, basically every part of the 3600 system was available with a
key switch or locking cover, a charming reminder of the state of computer
security at the time.The 3612 is a similar printer, but with the addition of a
dedicated passbook feature. Remember passbook savings accounts, where the bank
writes every transaction in a little booklet that the customer keeps? They were
still around, although declining in use, in the 1970s. The 3612 had a slot on
the front where an appropriately formatted passbook could be inserted, and like
a check validator or slip printer, it printed the latest transaction onto the
next empty line. Finally, the 3618 was a "medium-speed" printer, meaning 155 lines per minute.
A branch bank would probably have one, in the back office, used for printing
daily closing reports and other longer "administrative" output.A branch bank could carry out all of its routine business through the 3600
system, including cash withdrawals. In fact, since a customer withdrawing cash
would end up talking to a teller who simply keyed the transaction into a 3604,
it seems like a little more automation could make an ATM part of the system.Enter the 3614 Consumer Transaction Facility, the first IBM ATM available as a
regular catalog item. The 3614 is actually fairly obscure, and doesn't seem to
have sold in large numbers. Some sources suggest that it was basically the same
as the 2984, but with a general facelift and adaptations to connect to a 3601 Finance Communication
Controller instead of directly to a front-end processor. Some features which
were optional on the 2984, like a deposit slot, were apparently standard on 3614.
I'm not even quite sure when the 3614 was introduced, but based on manual
copyright dates they must have been around by 1977.One of the reasons the 3614 is obscure is that its replacement, the IBM 3624
Consumer Transaction Facility, hit the market in 1978â€”probably very shortly
after the 3614. The 3624 was functionally very similar to the 3614, but with
maintainability improvements like convenient portable cartridges for storing
cash. It also brought a completely redesigned front panel that is more similar
to modern ATMs. I should talk about the front panelsâ€”the IBM ATMs won a few
design awards over their years, and they were really very handsome machines.
The backlit logo panel and function-specific keys of the 3624 look more pleasant
to use than most modern ATMs, although they would of course render translation
difficult.The 3614/3624 series established a number of conventions that are still in use
today. For example, they added an envelope deposit system in which the machine
accepted an envelope (with cash or checks) and printed a transaction identifier
on the outside of the envelope for lookup at the processing center. This
relieved the user of writing up a deposit slip when using the ATM. It was also
capable of not only reading but, optionally, writing to the magnetic strips on
ATM cards. To the modern reader that sounds strange, but we have to discuss one
of the most enduring properties of the 3614/3624: their handling of PIN numbers.I believe the 2984 did something fairly similar, but the details are now obscure
(and seem to get mixed up with its use of LUCIFER/DSD-1/DES for communications).
The 3614/3624, though, so firmly established a particular approach to PIN
numbers that it is now known as the 3624 algorithm. Here's how it works: the
ATM reads the card number (called Primary Account Number or PAN) off of the
ATM card, reads a key from memory, and then applies a convoluted cryptographic
algorithm to calculate an "intermediate PIN" from it. The "intermediate pin"
is then summed with a "PIN offset" stored on the card itself, modulo 10, to
produce the PIN that the user is actually expected to enter. This means that
your "true" PIN is a static value calculated from your card number and a key,
but as a matter of convenience, you can "set" a PIN of your choice by using an
ATM that is equipped to rewrite the PIN offset on your card. This same system,
with some tweaks and a lot of terminological drift, is still in use today. You
will sometimes hear IBM's intermediate PIN called the "natural PIN," the one
you get with an offset of 0, which is a use of language that I find charming.Another interesting feature of the 3624 was a receipt printerâ€”I'm not sure if it
was the first ATM to offer a receipt, but it was definitely an early one. The
exact mechanics of the 3624 receipt printer are amusing and the result of some
happenstance at IBM. Besides its mainframes and their peripherals, IBM in the
1970s was was increasingly invested in "midrange computers" or "midcomputers"
that would fill in a space between the mainframe and minicomputerâ€”and, most
importantly, make IBM more competitive with the smaller businesses that could
not afford IBM's mainframe systems and were starting to turn to competitors like
DEC as a result. These would eventually blossom into the extremely successful
AS/400 and System i, but not easily, and the first few models all suffered from
decidedly soft sales.For these smaller computers, IBM reasoned that they needed to offer peripherals
like card punches and readers that were also smaller. Apparently following that
line of thought to a misguided extent, IBM also designed a smaller punch card:
the 96-column three-row card, which was nearly square. The only computer ever
to support these cards was the very first of the midrange line, the 1969
System/3. One wonders if the System/3's limited success lead to excess stock of
96-column card equipment, or perhaps they just wanted to reuse tooling. In any
case, the oddball System/3 card had a second life as the "Transaction Statement
Printer" on the 3614 and 3624. The ATM could print four lines of text, 34
characters each, onto the middle of the card. The machines didn't actually punch
them, and the printed text ended up over the original punch fields. You could,
if you wanted, actually order a 3624 with two printers: one that presented the
slip to the customer, and another that retained it internally for bank auditing.
A curious detail that would so soon be replaced by thermal receipt printers.Unlike IBM's ATMs before it, and, as we will see, unlike those after it as well,
the 3624 was a hit. While IBM never enjoyed the dominance in ATMs that they did
in computers, and companies like NCR and Diebold had substantial market
share, the 3624 was widely installed in the late 1970s and would probably be
recognized by anyone who was withdrawing cash in that era. The machine had
technical leadership as well: NCR built their successful ATM line in part by
duplicating aspects of the 3624 design, allowing interoperability with IBM
backend systems. Ultimately, as so often happens, it may have been IBM's success
that became its undoing.In 1983, IBM completely refreshed their branch banking solution with the 4700
Finance Communication System. While architecturally similar, the 4700 was a big
upgrade. For one, the CRT had landed: the 4700 peripherals replaced several-line
VFDs with full-size CRTs typical of other computer terminals, and conventional
computer keyboards to boot. Most radically, though, the 4700 line introduced
distributed communications to IBM's banking offerings. The 4701 Communications
Controller was optionally available with a hard disk, and could be programmed
in COBOL. Disk-equipped 4701s could operate offline, without a connection to the
host, or in a hybrid mode in which they performed some transactions locally and
only contacted the host system when necessary. Local records kept by the 4701
could be automatically sent to the host computer on a scheduled basis for
reconciliation.Along with the 4700 series came a new ATM: the IBM 473x Personal Banking
Machines. And with that, IBM's glory days in ATMs came crashing to the ground.
The 473x series was such a flop that it is hard to even figure out the model
numbers, the 4732 is most often referenced but others clearly existed, including
the 4730, 4731, 4736, 4737, and 4738. These various models were introduced from
1983 to 1988, making up almost a decade of IBM's efforts and very few sales.
The 4732 had a generally upgraded interface, including a CRT, but a similar
feature setâ€”unsurprising, given that the 3724 had already introduced most of the
features ATMs have today. It also didn't sell. I haven't been able to find any
numbers, but the trade press referred to the 4732 with terms like
"debacle," so they couldn't have been great.There were a few faults in the 4732's stars. First, IBM had made the decision to
handle the 4700 Finance Communication System as a complete rework of the 3600.
The 4700 controllers could support some 3600 peripherals, but 4700 peripherals
could  be used with 3600 controllers. Since 3600 systems were widely
installed in banks, the compatibility choice created a situation where many of
the 4732's prospective buyers would end up having to replace a significant
amount of their other equipment, and then likely make software changes, in order
to support the new machine. That might not have been so bad on its own had IBM's
competitors not provided another way out.NCR made their fame in ATMs in part by equipping their contemporary models with
3624 software emulation, making them a drop-in modernization option for existing
3600 systems. In general, other ATM manufacturers had pursued a path of
interoperability, with multiprotocol ATMs that supported multiple hosts, and
standalone ATM host products that could interoperate with multiple backend
accounting systems. For customers, buying an NCR or Diebold product that would
work with whatever they already used was a more appealing option than buying the
entire IBM suite in one go. It also matched the development cycle of ATMs
better: as a consumer-facing device, ATMs became part of the brand image of the
bank, and were likely to see replacement more often than back-office devices
like teller terminals. NCR offered something like a regular refresh, while IBM
was still in a mode of generational releases that would completely replace the
bank's computer systems.The 4732 and its 473x compatriots became the last real IBM ATMs. After a hiatus
of roughly a decade, IBM reentered the ATM market by forming a joint venture
with Diebold called InterBold. The basic terms were that Diebold would sell its
ATMs in the US, and IBM would sell them overseas, where IBM had generally been
the more successful of the two brands. The IBM 478x series ATMs, which you might
encounter in the UK for example, are the same as the Diebold 1000 series in the
US. InterBold was quite successful, becoming the dominant ATM manufacturer in
the US, and in 1998 Diebold bought out IBM's share.IBM had won the ATM market, and then lost it. Along the way, they left us with
so much texture: DES's origins in the ATM, the 3624 PIN format, the dumb
terminal or thin client model... even InterBold, IBM's protracted exit, gave us
quite a legacy: now you know the reason that so many later ATMs ran OS/2. IBM,
a once great company, provided Diebold with their once great operating system.
Unlike IBM, Diebold made it successful.]]></content:encoded></item><item><title>OpenAI reaches deal to deploy AI models on U.S. DoW classified network</title><link>https://www.reuters.com/business/openai-reaches-deal-deploy-ai-models-us-department-war-classified-network-2026-02-28/</link><author>erhuve</author><category>hn</category><pubDate>Sat, 28 Feb 2026 03:23:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Don&apos;t use passkeys for encrypting user data</title><link>https://blog.timcappalli.me/p/passkeys-prf-warning/</link><author>zdw</author><category>hn</category><pubDate>Sat, 28 Feb 2026 03:11:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Why am I writing this today?
Because I am deeply concerned about users losing their most sacred data.Over the past year or two, Iâ€™ve seen many organizations, large and small, implement passkeys (which is great, thank you!) and use the PRF (Pseudo-Random Function) extension to derive keys to protect user data, typically to support end-to-end encryption (including backups).
Iâ€™ve also seen a number of influential folks and organizations promote the use of PRF for encrypting data.The primary use cases Iâ€™ve seen implemented or promoted so far include:encrypting message backups (including images and videos)encrypting documents and other filesencrypting and unlocking crypto walletscredential manager unlockingWhen you overload a credential used for authentication by also using it for encryption, the â€œblast radiusâ€ for losing that credential becomes immeasurably larger.Imagine a user named Erika. They are asked to set up encrypted backups in their favorite messaging app because they donâ€™t want to lose their messages and photos, especially those of loved ones who are no longer here.
Erika is prompted to use their passkey to enable these backups.There is nothing in the UI that emphasizes that these backups are now tightly coupled to their passkey. Even if there were explanatory text, Erika, like most users, doesnâ€™t typically read through every dialog box, and they certainly canâ€™t be expected to remember this technical detail a year from now.A few months pass, and Erika decides to clean up their credential manager. They donâ€™t remember why they had a specific passkey for a messaging app and deletes it.Fast forward a year: they get a new phone and set up the messaging app. They arenâ€™t prompted to use a passkey because one no longer exists in their credential manager. Instead, they use phone number verification to recover their account. They are then guided through the â€œrestore backupâ€ flow and prompted for their passkey.Since they no longer have it, they are informed that they cannot access their backed up data. Goodbye, memories.Hereâ€™s a few examples of what a user sees when they delete a passkey:How is a user supposed to understand that they are potentially blowing away photos of deceased relatives, an encrypted property deed, or their digital currency?We cannot, and should not, expect users to know this.At this point, you may be asking why PRF is part of WebAuthn in the first place.
There are some very legitimate and more durable uses of PRF in WebAuthn, specifically supporting credential managers and operating systems.A passkey with PRF can make unlocking your credential manager (where all of your other passkeys and credentials are stored) much faster and more secure.
Credential managers have robust mechanisms to protect your vault data with multiple methods, such as master passwords, per-device keys, recovery keys, and social recovery keys.
Losing access to a passkey used to unlock your credential manager rarely leads to complete loss of your vault data.PRF is already implemented in WebAuthn Clients and Credential Managers, so the cat is out of the bag. My asks:To the wider identity industry: please stop promoting and using passkeys to encrypt user data. Iâ€™m begging you. Let them be great, phishing-resistant authentication credentials.To sites and services using passkeys: if you still need to use PRF knowing these concerns, please:(and thanks to Matthew Miller for reviewing and providing feedback on this post)]]></content:encoded></item><item><title>OpenAI agrees with Dept. of War to deploy models in their classified network</title><link>https://twitter.com/sama/status/2027578652477821175</link><author>eoskx</author><category>hn</category><pubDate>Sat, 28 Feb 2026 02:59:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Croatia declared free of landmines after 31 years</title><link>https://glashrvatske.hrt.hr/en/domestic/croatia-declared-free-of-landmines-after-31-years-12593533</link><author>toomuchtodo</author><category>hn</category><pubDate>Sat, 28 Feb 2026 02:48:16 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Interior Minister Davor BoÅ¾inoviÄ‡ announced Friday that Croatia is officially free of landmines. Thirty-one years after the end of the Homeland War, all known minefields have been cleared â€” a major milestone for the country.The decades-long effort came at a heavy cost. Over three decades of painstaking and dangerous work, 208 people lost their lives, including 41 deminers. The total cost of clearing the country is estimated at around 1.2 billion euros.
â€œCroatia is free of land mines. After nearly 30 years, we have completed demining in accordance with the Ottawa Convention,â€ BoÅ¾inoviÄ‡ said during an event marking International Civil Protection Day in Zagreb.
He added, â€œAlmost 107,000 mines and 407,000 pieces of unexploded ordnance have been removed. This is not just a technical success â€” it is the fulfillment of a moral obligation to the victims of mines and their families. A mine-free Croatia means safer families, better development of rural areas, more farmland, and stronger tourism.â€
Vijesti HRT-a pratite na svojim pametnim telefonima i tabletima putem aplikacija za iOS i Android. Pratite nas i na druÅ¡tvenim mreÅ¾ama Facebook, Twitter, Instagram, TikTok i YouTube!]]></content:encoded></item><item><title>Statement on the comments from Secretary of War Pete Hegseth</title><link>https://www.anthropic.com/news/statement-comments-secretary-war</link><author>surprisetalk</author><category>hn</category><pubDate>Sat, 28 Feb 2026 01:20:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Earlier today, Secretary of War Pete Hegseth shared on X that he is directing the Department of War to designate Anthropic a supply chain risk. This action follows months of negotiations that reached an impasse over two exceptions we requested to the lawful use of our AI model, Claude: the mass domestic surveillance of Americans and fully autonomous weapons.We have not yet received direct communication from the Department of War or the White House on the status of our negotiations.We have tried in good faith to reach an agreement with the Department of War, making clear that we support all lawful uses of AI for national security aside from the two narrow exceptions above. To the best of our knowledge, these exceptions have not affected a single government mission to date.We held to our exceptions for two reasons. First, we do not believe that todayâ€™s frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger Americaâ€™s warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights.Designating Anthropic as a supply chain risk would be an unprecedented actionâ€”one historically reserved for US adversaries, never before publicly applied to an American company. We are deeply saddened by these developments. As the first frontier AI company to deploy models in the US governmentâ€™s classified networks, Anthropic has supported American warfighters since June 2024 and has every intention of continuing to do so.We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government.No amount of intimidation or punishment from the Department of War will change our position on mass domestic surveillance or fully autonomous weapons. We will challenge any supply chain risk designation in court.What this means for our customersSecretary Hegseth has implied this designation would restrict anyone who does business with the military from doing business with Anthropic. The Secretary does not have the statutory authority to back up this statement. Legally, a supply chain risk designation under 10 USC 3252 can only extend to the use of Claude as part of Department of War contractsâ€”it cannot affect how contractors use Claude to serve other customers.If you are an individual customer or hold a commercial contract with Anthropic, your access to Claudeâ€”through our API, claude.ai, or any of our productsâ€”is completely unaffected.If you are a Department of War contractor, this designationâ€”if formally adoptedâ€”would only affect your use of Claude on Department of War contract work. Your use for any other purpose is unaffected.Our sales and support teams are standing by to answer any questions you may have.We are deeply grateful to our users, and to the industry peers, policymakers, veterans, and members of the public who have voiced their support in recent days. Thank you. Above all else, our priorities are to protect our customers from any disruption caused by these extraordinary events and to work with the Department of War to ensure a smooth transitionâ€”for them, for our troops, and for American military operations.]]></content:encoded></item><item><title>We Will Not Be Divided</title><link>https://notdivided.org/</link><author>BloondAndDoom</author><category>hn</category><pubDate>Sat, 28 Feb 2026 00:54:53 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Frequently Asked Questions]]></content:encoded></item><item><title>Qt45: A small polymerase ribozyme that can synthesize itself</title><link>https://www.science.org/doi/10.1126/science.adt2760</link><author>ppnpm</author><category>hn</category><pubDate>Fri, 27 Feb 2026 23:42:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I am directing the Department of War to designate Anthropic a supply-chain risk</title><link>https://twitter.com/secwar/status/2027507717469049070</link><author>jacobedawson</author><category>hn</category><pubDate>Fri, 27 Feb 2026 22:31:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>President Trump bans Anthropic from use in government systems</title><link>https://www.npr.org/2026/02/27/nx-s1-5729118/trump-anthropic-pentagon-openai-ai-weapons-ban</link><author>pkress2</author><category>hn</category><pubDate>Fri, 27 Feb 2026 21:40:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
                The Pentagon is seen from an airplane, Monday, Feb. 2, 2026, in Washington.
                
                    
                    Julia Demaree Nikhinson/Associated Press
                    
                President Trump ordered the U.S. government to stop using the artificial intelligence company Anthropic's products and the Pentagon moved to designate the company a national security risk on Friday, in a sharp escalation of a high-stakes fight over the military's use of AI.Hours after the president's announcement, rival company OpenAI said it had struck a deal with the Defense Department to provide its own AI technology for classified networks.The administration's decisions cap an acrimonious dispute between Anthropic and the Pentagon over whether the company could prohibit its tools from being used in mass surveillance of American citizens or to power autonomous weapon systems, as part of a military contract worth up to $200 million."The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution," Trump wrote in a Truth Social post. "Therefore, I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic's technology. We don't need it, we don't want it, and will not do business with them again!"He said there would be a six-month phaseout of Anthropic's products.Trump's announcement came about an hour before a deadline set by the Pentagon, which had called on Anthropic to back down. Shortly after the deadline passed, Defense Secretary Pete Hegseth said he was labeling Anthropic a supply chain risk to national security, blacklisting it from working with the U.S. military or contractors."In conjunction with the President's directive for the Federal Government to cease all use of Anthropic's technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic," Hegseth posted on X , using the Pentagon's "Department of War" rebranding. "Anthropic will continue to provide the Department of War its services for a period of no more than six months to allow for a seamless transition to a better and more patriotic service."Anthropic said it would challenge the supply chain risk designation in court."We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government," the company said in a statement on Friday evening.Anthropic also challenged Hegseth's comments that anyone who does business with the U.S. military would have to cut off all business with Anthropic. "The Secretary does not have the statutory authority to back up this statement," the company said. Under federal law, it said, designating Anthropic a supply chain risk would only apply to "the use of Claude as part of Department of War contractsâ€”it cannot affect how contractors use Claude to serve other customers."The company said it had "tried in good faith" to reach an agreement with the Pentagon over months of negotiations, "making clear that we support all lawful uses of AI for national security aside from the two narrow exceptions" being disputed. "To the best of our knowledge, these exceptions have not affected a single government mission to date," Anthropic said.It said its objections to those uses were rooted in two reasons: "First, we do not believe that today's frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger America's warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights."In a post on X announcing competitor OpenAI's deal with the Defense Department, the company's CEO Sam Altman, who previously cited similar concerns, said his agreement with the government included safeguards like the ones Anthropic had asked for. "Two of our most important safety principles are prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems," he said. "The DoW agrees with these principles, reflects them in law and policy, and we put them into our agreement."Ban comes as Anthropic plans an IPODefense Department officials had given Anthropic a deadline of 5:01 p.m. ET on Friday to drop restrictions on its AI model, Claude, from being used for domestic mass surveillance or entirely autonomous weapons, or face losing its contract. The Pentagon has said it doesn't intend to use AI in those ways, but requires AI companies to allow their models to be used "for all lawful purposes."The government had also threatened to invoke the Korean War-era Defense Production Act  to compel Anthropic to allow use of its tools and, at the same time, warned it would label Anthropic a supply chain risk.In his post carrying out the latter threat, Hegseth said Anthropic had "delivered a master class in arrogance and betrayal as well as a textbook case of how not to do business with the United States Government or the Pentagon." He accused the company of trying to "seize veto power over the operational decisions of the United States military."He said the department would not waver from its position: "the Department of War must have full, unrestricted access to Anthropic's models for every LAWFUL purpose in defense of the Republic.""America's warfighters will never be held hostage by the ideological whims of Big Tech. This decision is final," Hegseth concluded.The government ban comes at a time when Anthropic is under heightened scrutiny, since the company, which is valued at $380 billion, is planning to go public this year.While the Pentagon contract worth as much as $200 million is a relatively small portion of Anthropic's $14 billion in revenue, it's unclear how the friction with the administration will sit with investors or affect other deals the company has to license its AI model to non-government partners.Anthropic CEO Dario Amodei has pointed out that the company's valuation and revenue have only grown since it took a stand against Trump officials over how AI can be deployed on the battlefield.Whether AI companies can set restrictions on how the government uses their technology has emerged as a major sticking point in recent months between Anthropic and the Trump administration.On Thursday, Amodei said the company would not budge in the face of the Pentagon's threats. "We cannot in good conscience accede to their request," he wrote in a lengthy statement.
                A 2024 file photo of Dario Amodei, CEO and cofounder of Anthropic.
                
                    
                    Jeff Chiu/Associated Press
                    
                "Anthropic understands that the Department of War, not private companies, makes military decisions. We have never raised objections to particular military operations nor attempted to limit use of our technology in an  manner," he said. But, he added, domestic mass surveillance and fully autonomous weapons are uses that are "simply outside the bounds of what today's technology can safely and reliably do."Emil Michael, the Pentagon's undersecretary for research and engineering, shot back in a post on X on Thursday, accusing Amodei of lying and having a "God-complex.""He wants nothing more than to try to personally control the US Military and is ok putting our nation's safety at risk," Michael wrote. "The @DeptofWar will ALWAYS adhere to the law but not bend to whims of any one for-profit tech company," he wrote.In an late Thursday interview with CBS News, Michael said federal law and Pentagon policies already bar the use of AI for domestic mass surveillance and autonomous weapons.""At some level, you have to trust your military to do the right thing," he said.OpenAI expressed similar concernsOpenAI CEO Altman had said earlier on Friday that he shared Anthropic's "red lines" restricting military use of AI.OpenAI, Google, and Elon Musk's xAI also have Defense Department contracts and have agreed to allow their AI tools to be used in any "lawful" scenarios. Earlier this week, xAI became the second company after Anthropic to be approved for use in classified settings.Altman told CNBC on Friday morning that it's important for companies to work with the military "as long as it is going to comply with legal protections" and "the few red lines" that "we share with Anthropic and that other companies also independently agree with."
                Sam Altman, co-founder and CEO of OpenAI, testifying before a Senate committee in 2025.
                
                    
                    Jose Luis Magana/Associated Press
                    
                In an internal note sent to staff on Thursday evening, Altman said OpenAI was seeking to negotiate a deal with the Pentagon to deploy its models in classified systems with exclusions preventing use for surveillance in the U.S. or to power autonomous weapons without human approval, according to a person familiar with the message who was not authorized to speak publicly. The  first reported Altman's note to staff.The Defense Department didn't respond to a request for comment on Altman's statements.Independent experts say the standoff is highly unusual in the world of Pentagon contracting."This is different for sure," said Jerry McGinn, director of the Center for the Industrial Base at the Center for Strategic and International Studies, a Washington DC think tank. Pentagon contractors don't usually get to tell the Defense Department how their products and services can be used, he notes "because otherwise you'd be negotiating use cases for every contract, and that's not reasonable to expect."At the same time, McGinn noted, artificial intelligence is a new and largely untested technology. "This is a very unusual, very public fight," he said. "I think it's reflective of the nature of AI."NPR's Bobby Allyn contributed to this report.]]></content:encoded></item><item><title>Rob Grant, creator of Red Dwarf, has died</title><link>https://www.beyondthejoke.co.uk/content/17193/red-dwarf-rob-grant</link><author>nephihaha</author><category>hn</category><pubDate>Fri, 27 Feb 2026 19:26:38 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Tributes have been paid to Rob Grant, the comedy writer best known as the co-creator of long running hit sitcom Red Dwarf. Grant was also one of the main writers on Spitting IMage for many years, writing regularly with Doug Naylor.The news was broken by the Red Dwarf fan site, Ganymede and Titan. (note - at the time of writing the site has gone down due, presumably, to so many fans trying to find out more details).Craig Charles, who played Lister posted on X: "Earlier today I was informed of the passing of Iâ€™m deeply saddened to hear of Rob Grantâ€™s passing yesterday. Itâ€™s hard to take in the loss of someone who was such a significant part of my life for so many years. I first met Rob when we were nine years old. We went to Chetham's School of Music and later Liverpool University. We grew up making each other laugh long before there was an audience, and eventually found ourselves building something that neither of us could have imagined when we were schoolboys."Spitting Image and later Red Dwarf went on to become two of the most loved comedy series in Britain. I'll always treasure those years of writing together and laughing so hard it hurt. Creative partnerships are intense, driven by passion, conviction and strong personalities. But at the heart of ours was a shared love of comedy and a desire to make people laugh and we did, on a scale neither of us could have predicted. My thoughts are with Rob's wife Kath, and all his family and friends. I will always be grateful for my time working with Rob and what we created together. RIP Smeghead! XÂ #reddwarf"We are devastated to learn of Robâ€™s passing and send love to his family and friends. He will always live on through his amazing creativity, storytelling and humour. Travel well, Sir"Red Dwarf emerged out of a sketch on the radio show Son of Cliche, and was a major hit for the BBC, launching in 1988 and making stars out of Craig Charles, Chris Barrie, Robert Llewellyn and Danny John-Jules as well as Hattie Hayridge and Norman Lovett. It was later revived on Dave and continued to be watched by large, devoted audiences.picture credit: CC BY-SA 2.0]]></content:encoded></item><item><title>Leaving Google has actively improved my life</title><link>https://pseudosingleton.com/leaving-google-improved-my-life/</link><author>speckx</author><category>hn</category><pubDate>Fri, 27 Feb 2026 19:08:25 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Let&apos;s discuss sandbox isolation</title><link>https://www.shayon.dev/post/2026/52/lets-discuss-sandbox-isolation/</link><author>shayonj</author><category>hn</category><pubDate>Fri, 27 Feb 2026 18:49:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[There is a lot of energy right now around sandboxing untrusted code. AI agents generating and executing code, multi-tenant platforms running customer scripts, RL training pipelines evaluating model outputsâ€”basically, you have code you did not write, and you need to run it without letting it compromise the host, other tenants, or itself in unexpected ways.The word â€œisolationâ€ gets used loosely. A Docker container is â€œisolated.â€ A microVM is â€œisolated.â€ A WebAssembly module is â€œisolated.â€ But these are fundamentally different things, with different boundaries, different attack surfaces, and different failure modes. I wanted to write down my learnings on what each layer actually provides, because I think the distinctions matter and allow you to make informed decisions for the problems you are looking to solve.The kernel is the shared surfaceWhen any code runs on Linux, it interacts with the hardware through the kernel via system calls. The Linux kernel exposes roughly 340 syscalls, and the kernel implementation is tens of millions of lines of C code. Every syscall is an entry point into that codebase.Untrusted Code â”€( Syscall )â”€â†’ Host Kernel â”€( Hardware API )â”€â†’ Hardware
                              [ 40M LOC C ]
Every isolation technique is answering the same question of how to reduce or eliminate the untrusted codeâ€™s access to that massive attack surface.A useful mental model here is shared state versus dedicated state. Because standard containers share the host kernel, they also share its internal data structures like the TCP/IP stack, the Virtual File System caches, and the memory allocators. A vulnerability in parsing a malformed TCP packet in the kernel affects every container on that host. Stronger isolation models push this complex state up into the sandbox, exposing only simple, low-level interfaces to the host, like raw block I/O or a handful of syscalls.The approaches differ in where they draw the boundary. Namespaces use the same kernel but restrict visibility. Seccomp uses the same kernel but restricts the allowed syscall set. Projects like gVisor use a completely separate user-space kernel and make minimal host syscalls. MicroVMs provide a dedicated guest kernel and a hardware-enforced boundary. Finally, WebAssembly provides no kernel access at all, relying instead on explicit capability imports. Each step is a qualitatively different boundary, not just a stronger version of the same thing.Namespaces as visibility wallsLinux namespaces wrap global system resources so that processes appear to have their own isolated instance. There are eight types, and each isolates a specific resource.Own process tree, starts at PID 1Own mount table, can have different rootNetwork interfaces, routingOwn interfaces, IP addresses, portsCan be root inside, nobody outsideSysV IPC, POSIX message queuesOwn shared memory, semaphoresSystem clocks (monotonic, boot)Own system uptime and clock offsetsNamespaces are what Docker containers use. When you run a container, it gets its own PID namespace (cannot see host processes), its own mount namespace (own filesystem view), its own network namespace (own interfaces), and so on.The critical thing to understand is namespaces are visibility walls, not security boundaries. They prevent a process from  things outside its namespace. They do not prevent a process from  that implements the namespace. The process still makes syscalls to the same host kernel. If there is a bug in the kernelâ€™s handling of any syscall, the namespace boundary does not help.In January 2024, CVE-2024-21626 showed that a file descriptor leak in  (the standard container runtime) allowed containers to access the host filesystem. The containerâ€™s mount namespace was intact â€” the escape happened through a leaked fd that  failed to close before handing control to the container. In 2025, three more  CVEs (CVE-2025-31133, CVE-2025-52565, CVE-2025-52881) demonstrated mount race conditions that allowed writing to protected host paths from inside containers.Cgroups: accounting is not securityCgroups (control groups) limit and account for resource usage: CPU, memory, disk I/O, number of processes. They prevent a container from consuming all available memory or spinning up thousands of processes.Cgroups are important for stability, but they are not a security boundary. They prevent denial-of-service, not escape. A process constrained by cgroups still makes syscalls to the same kernel with the same attack surface.Seccomp-BPF lets you attach a Berkeley Packet Filter program that decides which syscalls a process is allowed to make. You can deny dangerous syscalls like process tracing, filesystem manipulation, kernel extension loading, and performance monitoring.Docker applies a default seccomp profile that blocks around 40 to 50 syscalls. This meaningfully reduces the attack surface. But the key limitation is that seccomp is a filter on the same kernel. The syscalls you allow still enter the host kernelâ€™s code paths. If there is a vulnerability in the write implementation, or in the network stack, or in any allowed syscall path, seccomp does not help.Without Seccomp:
  Untrusted Code â”€( ~340 syscalls )â”€â†’ Host Kernel

With Seccomp:
  Untrusted Code â”€( ~300 syscalls )â”€â†’ Host Kernel
The attack surface is smaller. The boundary is the same.Running a container in privileged modeThis is worth calling out because it comes up surprisingly often. Some isolation approaches require Dockerâ€™s privileged flag. For example, building a custom sandbox that uses nested PID namespaces inside a container often leads developers to use privileged mode, because mounting a new  filesystem for the nested sandbox requires the  capability (unless you also use user namespaces).If you enable  just to get  for nested process isolation, you have added one layer (nested process visibility) while removing several others (seccomp, all capability restrictions, device isolation). The net effect is arguably weaker isolation than a standard unprivileged container. This is a real trade-off that shows up in production. The ideal solutions are either to grant only the specific capability needed instead of all of them, or to use a different isolation approach entirely that does not require host-level privileges.gVisor and user-space kernelsgVisor is where the isolation model changes qualitatively. To understand the difference, it helps to look at the attack surface of a standard container.Standard Container (Docker)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Untrusted Code        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ ~340 syscalls
           â–¼
   [ Seccomp Filter ]
           â”‚ ~300 allowed syscalls
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Host Kernel (Ring 0)  â”‚ â—„â”€â”€ FULL ATTACK SURFACE
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
The code runs as a standard Linux process. Seccomp acts as a strict allowlist filter, reducing the set of permitted system calls. However, any allowed syscall still executes directly against the shared host kernel. Once a syscall is permitted, the kernel code processing that request is the exact same code used by the host and every other container. The failure mode here is that a vulnerability in an allowed syscall lets the code compromise the host kernel, bypassing the namespace boundaries.Instead of filtering syscalls to the host kernel, gVisor interposes a completely separate kernel implementation called the Sentry between the untrusted code and the host. The Sentry does not access the host filesystem directly; instead, a separate process called the Gofer handles file operations on the Sentryâ€™s behalf, communicating over a restricted protocol. This means even the Sentryâ€™s own file access is mediated.gVisor
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Untrusted Code        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ ~340 syscalls
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ gVisor Sentry (Ring 3)â”‚ â—„â”€â”€ USER-SPACE KERNEL
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚        â”‚ 9P / LISAFS
       â”‚        â–¼
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  â”‚   Gofer   â”‚ â—„â”€â”€ FILE I/O PROXY
       â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚        â”‚
       â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Host Kernel (Ring 0)  â”‚ â—„â”€â”€ REDUCED ATTACK SURFACE
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  (~70 host syscalls from Sentry)
The Sentry intercepts the untrusted codeâ€™s syscalls and handles them in user-space. It reimplements around 200 Linux syscalls in Go, which is enough to run most applications. When the Sentry actually needs to interact with the host to read a file, it makes its own highly restricted set of roughly 70 host syscalls. This is not just a smaller filter on the same surface; it is a completely different surface. The failure mode changes significantly. An attacker must first find a bug in gVisorâ€™s Go implementation of a syscall to compromise the Sentry process, and then find a way to escape from the Sentry to the host using only those limited host syscalls.The Sentry intercepts syscalls using one of several mechanisms, such as seccomp traps or KVM, with the default since 2023 being the seccomp-trap approach known as systrap.What this means in practice is that if someone discovers a bug in the Linux kernelâ€™s I/O implementation, containers using Docker are directly exposed. A gVisor sandbox is not, because those syscalls are handled by the Sentry, and the Sentry does not expose them to the host kernel.The trade-off is performance. Every syscall goes through user-space interception, which adds overhead. I/O-heavy workloads feel this the most. For short-lived code execution like scripts and tests, it is usually fine, but for sustained high-throughput I/O, it can matter.Also, by adopting gVisor, you are betting that itâ€™s easier to audit and maintain a smaller footprint of code (the Sentry and its limited host interactions) than to secure the entire massive Linux kernel surface against untrusted execution. That bet is not free of risk, gVisor itself has had security vulnerabilities in the Sentry but the surface area you need to worry about is drastically smaller and written in a memory-safe language.Defense in depth on top of gVisorgVisor gives you the user-space kernel boundary. What it does not give you automatically is multi-job isolation within a single gVisor sandbox. If you are running multiple untrusted executions inside one  container, you still need to layer additional controls. Here is one pattern for doing that:Per-job PID + mount + IPC namespaces via  â€” so each execution is isolated from other executions inside the same gVisor sandboxSeccomp-BPF inside the namespace â€” blocking syscalls like  (preventing nested namespace escape),  (force fallback to ), , kernel module loading â€” run as  (UID 65534) with  for all writable paths â€” cleanup is a single  syscall, not a recursive directory walkRead-only root filesystem â€” the container itself is immutableCapability-based file APIs â€” use  or similar to confine file writes to the work directory, preventing path traversal via  â€” compute isolation means nothing if the sandbox can freely phone home. Options range from disabling networking entirely, to running an allowlist proxy (like Squid) that blocks DNS resolution inside the sandbox and forces all traffic through a domain-level allowlist, to dropping  so the sandbox cannot bypass DNS with raw sockets.gVisor Container (runsc)
 â””â”€ Per-job PID + Mount Namespace
     â””â”€ Seccomp BPF Filter
         â””â”€ Privilege Drop
             â””â”€ Network Egress Control
                 â””â”€ Ephemeral tmpfs
                     â””â”€ Capability-confined File Writes
Each layer catches different attack classes. A namespace escape inside gVisor reaches the Sentry, not the host kernel. A seccomp bypass hits the Sentryâ€™s syscall implementation, which is itself sandboxed. Privilege escalation is blocked by dropping privileges. Persistent state leakage between jobs is prevented by ephemeral tmpfs with atomic unmount cleanup.A practical detail that matters is the process that creates child sandboxes must itself be fork-safe. If you are running an async runtime, forking from a multithreaded process is inherently unsafe because child processes inherit locked mutexes and can corrupt state. The solution is a fork server pattern where you fork a single-threaded launcher process before starting the async runtime, then have the async runtime communicate with the launcher over a Unix socket. The launcher creates children, entirely avoiding the multithreaded fork problem.Startup
  fork() â†’ Launcher (Single-threaded, Poll Loop)
               â”‚
               â”œâ”€ clone3(NEWPID | NEWNS | NEWIPC)
               â”‚
               â””â”€ Child (Mount, Privdrop, Seccomp, Execve)

Main Server (Async Runtime)
  â”‚
  â””â”€ AF_UNIX SEQPACKET â”€â†’ Launcher
MicroVMs for hardware boundariesMicroVMs use hardware virtualization backed by the CPUâ€™s extensions to run each workload in its own virtual machine with its own kernel.MicroVM Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Untrusted Code        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Syscalls
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Guest Kernel (Ring 0) â”‚ â—„â”€â”€ DEDICATED KERNEL
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ VirtIO / MMIO
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KVM Hypervisor (Host) â”‚ â—„â”€â”€ HARDWARE BOUNDARY
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Secure API
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VMM (User-Space)      â”‚ â—„â”€â”€ DEVICE EMULATION
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Code runs in a completely separate, hardware-backed environment with its own guest kernel. It is important to separate the concepts here. The hypervisor is the capability built into the Linux kernel that manages the CPUâ€™s hardware virtualization extensions. The Virtual Machine Monitor is a user-space process that configures the VM, allocates memory, and emulates minimal hardware devices. The microVM itself is a VM that has been stripped of legacy PC cruft so it boots in milliseconds and uses minimal memory.Escaping the guest kernel requires finding a vulnerability in the Virtual Machine Monitorâ€™s device emulation or the CPUâ€™s virtualization features, which are rare and highly prized.The guest runs in a separate virtual address space enforced by the CPU hardware. A bug in the guest kernel cannot access host memory because the hardware prevents it. The host kernel only sees the user-space process. The attack surface is the hypervisor and the Virtual Machine Monitor, both of which are orders of magnitude smaller than the full kernel surface that containers share.You generally see two different approaches to Virtual Machine Monitor design depending on the workload. The first is strict minimalism, seen in projects like Firecracker. Built specifically for running thousands of tiny, short-lived functions on a single server, it intentionally leaves out complex features like hot-plugging CPUs or passing through physical GPUs. The goal is simply the smallest possible attack surface and memory footprint.The second approach offers broader feature support, seen in projects like Cloud Hypervisor or QEMU microvm. Built for heavier and more dynamic workloads, it supports hot-plugging memory and CPUs, which is useful for dynamic build runners that need to scale up during compilation. It also supports GPU passthrough, which is essential for AI workloads, while still maintaining the fast boot times of a microVM.The trade-off versus gVisor is that microVMs have higher per-instance overhead but stronger, hardware-enforced isolation. For CI systems and sandbox platforms where you create thousands of short-lived environments, the boot time and memory overhead add up. For long-lived, high-security workloads, the hardware boundary is worth it.Snapshotting is a feature worth noting. You can capture a running VMâ€™s state including CPU registers, memory, and devices, and restore it later. This enables warm pools where you boot a VM once, install dependencies, snapshot it, and restore clones in milliseconds instead of booting fresh each time. This is how some platforms achieve incredibly fast cold starts even with full VM isolation.WebAssembly with no kernel at allWebAssembly takes a fundamentally different approach. Instead of running native code and filtering its kernel access, WASM runs code in a memory-safe virtual machine that has no syscall interface at all. All interaction with the host happens through explicitly imported host functions.WebAssembly (WASM)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Untrusted Code        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Function Calls
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WASM Runtime (Host)   â”‚ â—„â”€â”€ MEMORY-SAFE VM
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Imported Host Functions
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Allowed Host APIs     â”‚ â—„â”€â”€ EXPLICIT CAPABILITIES
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Code runs in a strict sandbox where the only allowed operations are calling functions provided by the host. If the host doesnâ€™t provide a file reading function, the WASM module simply cannot read files. The failure mode here requires a vulnerability in the WASM runtime itself, like an out-of-bounds memory read that bypasses the linear memory checks.There is no syscall surface to attack because the code never makes syscalls. Memory safety is enforced by the runtime. The linear memory is bounds-checked, the call stack is inaccessible, and control flow is type-checked. Modern runtimes add guard pages and memory zeroing between instances.The performance characteristics are attractive with incredibly fast cold starts and minimal memory overhead. But the practical limitation is language support. You cannot run arbitrary Python scripts in WASM today without compiling the Python interpreter itself to WASM along with all its C extensions. For sandboxing arbitrary code in arbitrary languages, WASM is not yet viable. For sandboxing code you control the toolchain for, it is excellent. I am, however, quite curious if there is a future for WASM in general-purpose sandboxing. Browsers have spent decades solving a similar problem of executing untrusted code safely, and porting those architectural learnings to backend infrastructure feels like a natural evolution.Putting it all together, the landscape spans from fast and weak isolation to slower and highly secure isolation.                     Isolation strength â†’
                     Attack surface     â†“

Namespaces      Seccomp       gVisor         MicroVM         WASM
   â”‚               â”‚             â”‚              â”‚              â”‚
   â”‚  visibility   â”‚  syscall    â”‚  separate    â”‚  hardware    â”‚  no kernel
   â”‚  walls only   â”‚  filter on  â”‚  kernel in   â”‚  boundary    â”‚  access at
   â”‚               â”‚  same       â”‚  user-space  â”‚  via KVM     â”‚  all
   â”‚               â”‚  kernel     â”‚              â”‚              â”‚
   â–¼               â–¼             â–¼              â–¼              â–¼
  Fast            Fast         Moderate       Slower        Fastest
  Weakest         Weak         Strong         Strongest     Strong*
                                                           (*limited scope)
For running trusted code that you wrote and reviewed, Docker with a seccomp profile is probably fine. The isolation is against accidental interference, not adversarial escape.For running untrusted code in a multi-tenant environment, like short-lived scripts, AI-generated code, or customer-provided functions, you need a real boundary. gVisor gives you a user-space kernel boundary with good compatibility, while a microVM gives you a hardware boundary with the strongest guarantees. Either is defensible depending on your threat model and performance requirements.For reinforcement learning training pipelines where AI-generated code is evaluated in sandboxes across potentially untrusted workers, the threat model is both the code and the worker. You need isolation in both directions, which pushes toward microVMs or gVisor with defense-in-depth layering.What Iâ€™ve learned is that the common mistake is treating isolation as binary. Itâ€™s easy to assume that if you use Docker, you are isolated. The reality is that standard Docker gives you namespace isolation, which is just visibility walls on a shared kernel. Whether that is sufficient depends entirely on what you are protecting against.It is also worth remembering that compute isolation is only half the problem. You can put code inside a gVisor sandbox or a Firecracker microVM with a hardware boundary, and none of it matters if the sandbox has unrestricted network egress for your â€œagentic workloadâ€. An attacker who cannot escape the kernel can still exfiltrate every secret it can read over an outbound HTTP connection. Network policy where it is a stripped network namespace with no external route, a proxy-based domain allowlist, or explicit capability grants for specific destinations is the other half of the isolation story that is easy to overlook. The apply case here can range from disabling full network access to using a proxy for redaction, credential injection or simply just allow listing a specific set of DNS records.Local sandboxing on developer machinesEverything above is about server-side multi-tenant isolation, where the threat is adversarial code escaping a sandbox to compromise a shared host. There is a related but different problem on developer machines: AI coding agents that execute commands locally on your laptop. The threat model shifts. There is no multi-tenancy. The concern is not kernel exploitation but rather preventing an agent from reading your  keys, exfiltrating secrets over the network, or writing to paths outside the project. Or you know if you are running Clawdbot locally, then everything is fair game.The approaches here use OS-level permission scoping rather than kernel boundary isolation.Cursor uses Appleâ€™s Seatbelt () on macOS and Landlock plus seccomp on Linux. It generates a dynamic policy at runtime based on the workspace: the agent can read and write the open workspace and , read the broader filesystem, but cannot write elsewhere or make network requests without explicit approval. This reduced agent interruptions by roughly 40% compared to requiring approval for every command, because the agent runs freely within the fence and only asks when it needs to step outside.OpenAIâ€™s Codex CLI takes a similar approach with explicit modes: ,  (the default), and . Network access is disabled by default. Claude Code and Gemini CLI both support sandboxing but ship with it off by default.The common pattern across all of these seems to be filesystem and network ACLs enforced by the OS, not a separate kernel or hardware boundary. A determined attacker who already has code execution on your machine could potentially bypass Seatbelt or Landlock restrictions through privilege escalation. But that is not the threat model. The threat is an AI agent that is mostly helpful but occasionally careless or confused, and you want guardrails that catch the common failure modes - reading credentials it should not see, making network calls it should not make, writing to paths outside the project.Appleâ€™s new Containerization framework (announced at WWDC 2025) is interesting here. Unlike Docker on Mac, which runs all containers inside a single shared Linux VM, Apple gives each container its own lightweight VM via the Virtualization framework on Apple Silicon. Each container gets its own kernel, its own ext4 filesystem, and its own IP address. It is essentially the microVM model applied to local development, with OCI image compatibility. It is still early, but it collapses the gap between â€œlocal development containersâ€ and â€œproperly isolated sandboxesâ€ in a way that Docker Desktop never did.The landscape is moving in a clear direction. There is a lot of exciting new tech out there, with people constantly pushing the limits of cold starts toward faster, securely isolated workloads using Python decorators and other novel approaches to make microvms feel like containers. I am excited to see what comes next in this space. It is definitely an area to watch.]]></content:encoded></item><item><title>Writing a Guide to SDF Fonts</title><link>https://www.redblobgames.com/blog/2026-02-26-writing-a-guide-to-sdf-fonts/</link><author>chunkles</author><category>hn</category><pubDate>Fri, 27 Feb 2026 18:24:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Back in 2024 I learned about SDF (signed distance field) rendering of fonts. I was trying to implement outlines and shadows in a single pass instead of drawing over the text multiple times in different styles. I intended to use these fonts for two different projects, a game and a map generator. I got things working but didnâ€™t fully understand why certain things worked or didnâ€™t work. I wrote some notes on my site about what I tried. In the end, I stopped working on both the gameâ€™s fonts and the map generator, so I put all of this on hold.Fast forward to late 2025, and my incomplete notes sometimes show up on the first page of search results for â€œsdf fontsâ€! Surely that isnâ€™t the best page on the topic. It would be better to point to library documentation or maybe one of the research papers about the topic. My page .Initially my thought was â€œsearch engines are in their declineâ€ but then I decided â€œthis is an opportunityâ€. I decided to  of being the top search result.I first looked through everything I had written. I already had started an â€œoverviewâ€ page but hadnâ€™t gotten very far on it. I also have  that were â€œdiary styleâ€, about what I did rather than what you should know.The overview page covered how to use various SDF font libraries (msdfgen, stb_truetype, tiny-sdf, etc.). I wrote code for multiple libraries, had sketched out diagrams for various concepts, and had screenshots of outputs from each of those libraries.At some point I realized the scope was too large. I had spent the most time with msdfgen and hadnâ€™t yet learned enough about the other libraries to write a proper guide. They all worked differently. I kept getting stuck. So . In redesign 2 I decided to only use msdfgen, but show the various tradeoffs involved (atlas size, antialias width, shader derivatives, smoothing function).I made several diagrams for concepts, such as:And I started running tests. I wanted to compare the effect of atlas size, so I made lots of screenshots and started looking closely. I wanted to come up with a way to recommend a specific size. I wanted to make recommendations for all the other parameters. I showed all the commands I ran.At some point I realized I could run tests forever. And I had already done that last year, and wrote it up in blog posts (one and two). Doing it again here didnâ€™t seem especially valuable. So  to a â€œhow toâ€ page. In redesign 3 I decided to show the concepts, then a JavaScript implementation using CPU rendering, and then another implementation using GPU rendering. I made new versions of the diagrams:I was making progress on that page but it didnâ€™t  like a Red Blob Games page. The page started out with tons of shell commands, and then showed lots of code. It felt like a page that only I would find useful. So  and designed a â€œconceptsâ€ page. In redesign 4 I focused on what effects I wanted, how SDF works, and how to use it to create those effects. I again reduced the scope by removing the implementation details. What I had already written, I moved to a separate (unpolished) page. And I never wrote a standalone downloadable project like I originally wanted.Sometimes it takes a long time before I figure out what I actually want to write, and then everything falls into place:Iâ€™m finally happy with the page.Take a look! I hope search engines point to it eventually.]]></content:encoded></item><item><title>Dan Simmons, author of Hyperion, has died</title><link>https://www.dignitymemorial.com/obituaries/longmont-co/daniel-simmons-12758871</link><author>throw0101a</author><category>hn</category><pubDate>Fri, 27 Feb 2026 18:13:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Daniel Joseph Simmons passed away on February 21, 2026 in Longmont, Colorado at age 77. His beloved wife Karen and daughter Jane were at his side. Dan was born in Peoria, Illinois on April 4, 1948 to his parents Robert A. Simmons and Kathryn H. (Catton) Simmons. His childhood was filled with happy memories of riding bikes with friends throughout cornfield-lined roads in the Midwest, first in Brimfield, Illinois and then in Pittsboro, Indiana. He graduated with an English degree from Wabash College in Crawfordsville, Indiana, and earned a graduate degree in education from Washington University in St. Louis, Missouri. Dan embarked on a career as an elementary school teacher in Missouri, later teaching in Buffalo, New York, and Longmont, Colorado, where he taught sixth grade. During his eighteen years in education, he co-created and taught a districtwide program for gifted students that was the first of its kind, and he was named a finalist for the Colorado Teacher of the Year.Dan had a profound passion for teaching, and was beloved by many of his students for his innovative, energizing, and creative approach in the classroom. He brought science to life for his students with Carl Saganâ€™s Cosmos series, ran interactive simulations on topics like the Cuban Missile Crisis and the harmful effects of discrimination, and incorporated his love of topics like Greek mythology, film, and art into his lectures. Every day after lunch, Dan told his students a daily installment of an epic tale that started on the first day of school. As they listened, the students would color illustrations that heâ€™d drawn for them. When the story finally came to an end on the last day of school, many recall being reduced to tears. This story would go on to become Danâ€™s Hyperion cantos (1989), a critically acclaimed, four-part science fiction classic. Over the course of his life, former students would tell Dan that they still had their notes from his Black history lectures, and that he had inspired their lifelong love of reading and writing. Long after he left the classroom, he continued to share what he loved with everyone around him, teaching his grandchildren all about the 1950s era monster movies that he loved, and giving endearingly professorial introductions to films that he and Karen shared with scores of friends when they hosted backyard summer movies.In addition to teaching, reading and writing were the great loves of Danâ€™s life. As a child, he read everything he could find, spanning from comic books to literary classics and nonfiction. Throughout his life, he particularly loved learning about space, science, and history. Starting in early childhood, Dan had a remarkable gift for storytelling, which would become his life's work. His first published story came out on the day his daughter Jane was born, a day that confirmed to him that his true love was his family.In 1987, Dan took a daring leap and left teaching to follow his dream to work full-time as an author. His debut novel, Song of Kali (1985), was inspired by three days that he spent in Kolkata, India, and won the 1986 World Fantasy Award. He went on to write thirty-one novels and short story collections, many of which were honored with accolades ranging from Bram Stoker awards, Locus awards, the Shirley Jackson award, and the prestigious Hugo award. His most meaningful award was an honorary doctorate from Wabash College, a place that changed his life and set him on a path towards a life well lived. His titles have been published in 28 foreign countries and translated into at least 20 languages, and his many book tours, conferences, and workshops took him all over the world. Like his early reading pursuits, Dan always wrote about what he loved. He defied literary norms by writing across genres, switching between major publishers, and defying pressure to conform to formulaic novels. His works span from historical fiction to horror, hard-boiled crime, and speculative fiction. They explore topics ranging from Ernest Hemingwayâ€™s WWII Cuban spy ring to mountain climbing in the Himalayas. In 2018, his novel The Terror (2007) was released as an AMC limited series. Dan was a profoundly curious learner who delighted in connecting with other curious minds, and the many stories he dreamed up helped him connect with others throughout his entire life.Dan is predeceased by his parents and his brother Ted. He is survived by his loving wife and daughter, Karen and Jane Simmons; his beloved grandchildren, Milo and Lucia Glenn; and his brother, Wayne Simmons.Dan's cremation has been entrusted to Ahlberg Funeral Chapel of Longmont, Colorado. His ashes will be scattered at a later date. Details for a Celebration of Life are pending.Gifts in memory of Dan may be made to Wabash College online at www.wabash.edu/give or to Wabash College Advancement Office 301 W. Wabash Ave. Crawfordsville, IN 47933. Please visit www.ahlbergfuneralchapel.com for upcoming service information, to make a memorial donation to Wabash College and to share fond memories and condolences with his loving family.]]></content:encoded></item><item><title>Robust and efficient quantum-safe HTTPS</title><link>https://security.googleblog.com/2026/02/cultivating-robust-and-efficient.html</link><author>tptacek</author><category>hn</category><pubDate>Fri, 27 Feb 2026 17:58:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>747s and coding agents</title><link>https://carlkolon.com/2026/02/27/engineering-747-coding-agents/</link><author>cckolon</author><category>hn</category><pubDate>Fri, 27 Feb 2026 17:22:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A couple years ago, I was on the way back from a work trip to Germany. I had been upgraded to business class, and I sat next to a Belgian 747 pilot, probably in his fifties or sixties. We talked a fair bit about our careers. I had left the Navy and started professionally programming less than a year before. He had been a pilot since shortly after graduating university, and had flown the 747 for about twenty years. He had studied mechanical engineering at school, and he told me in great depth about the variable geometry jet turbines in modern aircraft, which could remain efficient across a wide altitude range.I expressed some jealousy about how well suited he was to his job. Clearly he was a geek for aircraft, and even though most airlines donâ€™t fly the 747 anymore, it is an incredible machine. He agreed that it was a privilege to fly the plane, but said wistfully:In this job, after a while, thereâ€™s no improvement. You are no better today than you were yesterday.He said that by now, he knew the 747 about as well as a pilot could. In fact, he sometimes wished he had become an engineer or designer of airplanes, so that he could learn new things as a core part of his job. Then he said:You are lucky that your job is like that.Since that flight, my job has changed a great deal. Coding agents can do a large portion of what I previously considered my work. Iâ€™m one of the last people who should be upset about this, since I work at an AI lab and stand to gain a great deal if AI follows through on its economic promise. Still, it has changed how I solve problems, and at times I feel more like a pilot than an engineer.In the past, when I fixed a bug or implemented a feature, I would have to spend a minimum amount of effort understanding the situation. For example, to add pagination to this website, I would read the Jekyll docs, find the right plugin to install, read the sample config, and make the change. Possibly this wouldnâ€™t work, in which case I would Google it, read more, try more stuff, retest, etc. In this process it was hard not to learn things. I would walk away from the problem with a better understanding of how the system worked. If I had to implement the feature again, I would be able to do it faster and more easily.Once LLMs started getting good at coding, I would occasionally ask them for help at the beginning of this process, mostly replacing search engines. If I hit an error, I would copy and paste it into a chatbot to see what it said before trying hard to understand it (often, before reading it). This didnâ€™t replace critical thinking, though, since I would still need to learn and plan to implement the change.With the AI coding agents of the last few months, though, things are different. Often the agent can implement a whole feature end-to-end, with no involvement from me. Now when I need to make a change to the codebase, I donâ€™t start by trying to understand. Instead, I see if my coding agent can â€œone-shotâ€ the problem, and only step in if it seems to be failing. This happens less and less, and the features that I trust agents with have become bigger and bigger.I believe in coding primarily as a means to an end. Coding agents have allowed me to do much more than before, so for the most part I am happy with them! But Iâ€™ll admit there is also something bothersome about turning features over to AI fully.I do not build skills or knowledge as quickly this way. If I build a feature with a coding agent and then have to do it again, I wonâ€™t be any faster the second time. Itâ€™s possible to imagine writing code with AI for twenty years and not being much more skillful at the end of it. Thereâ€™s no improvement.If I do have to step in and save the LLM, I often become lost as well. All of a sudden, I am reading someone elseâ€™s code. Rather than gradually coming to terms with a solution to a problem, I am presented with the solution wholesaleâ€”only, itâ€™s a little bit wrong. As LLMs handle bigger tasks for me, this gets worse. My only saving grace is that I will do it less often.You might say that the new, real skill is prompting agents (archived), but I donâ€™t believe that. Prompting is easy and will only get easier. Hard knowledge about programming and the problem is what helps you make good design decisions, so this knowledge is the most important factor determining whether your coding agents are successful. Developing this knowledge is becoming optional.Some people will probably respond to this by saying (snottily) that I should read the code that my agents produce, rather than rely on them blindly. I do read the code, but reviewing code is very different from producing it, and surely teaches you less. If you donâ€™t believe this, I doubt you work in software.Coding agents are here to stay, and youâ€™re a fool if you donâ€™t use them. Still, I think youâ€™ll use them most successfully if you understand the domain in which youâ€™re working. This used to be an essential byproduct of programming, but thatâ€™s not the case anymore. To this end, maybe itâ€™s a good idea to write a minimum amount of code by hand as an educational task, rather than a productive one, or to try to write the solution to a problem yourself, and only compare with the LLM once youâ€™re confident your answer is correct.]]></content:encoded></item><item><title>Vibe coded Lovable-hosted app littered with basic flaws exposed 18K users</title><link>https://www.theregister.com/2026/02/27/lovable_app_vulnerabilities/</link><author>nottorp</author><category>hn</category><pubDate>Fri, 27 Feb 2026 16:48:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Vibe-coding platform Lovable has been accused of hosting apps riddled with vulnerabilities after saying users are responsible for addressing security issues flagged before publishing.Taimur Khan, a tech entrepreneur with a background in software engineering, found 16 vulnerabilities â€“ six of which he said were critical â€“ in a single Lovable-hosted app that leaked more than 18,000 people's data.He declined to name the app during the disclosure process, although it was hosted on Lovable's platform and showcased on its Discover page. The app had more than 100,000 views and around 400 upvotes at the time Khan began his probe.The main issue, Khan said, was that all apps that are vibe-coded on Lovable's platform are shipped with their backends powered by Supabase, which handles authentication, file storage, and real-time updates through a PostgreSQL database connection.However, when the developer â€“ in this case AI â€“ or the human project owner fails to explicitly implement crucial security features like Supabase's row-level security and role-based access, code will be generated that looks functional but in reality is flawed.One example of this was a malformed authentication function. The AI that vibe-coded the Supabase backend, which uses remote procedure calls, implemented it with flawed access control logic, essentially blocking authenticated users and allowing access to unauthenticated users.Khan said the intent was to block non-admins from accessing parts of the app, but the faulty implementation blocked all logged-in users â€“ an error he said was repeated across multiple critical functions."This is backwards," said Khan. "The guard blocks the people it should allow and allows the people it should block. A classic logic inversion that a human security reviewer would catch in seconds â€“ but an AI code generator, optimizing for 'code that works,' produced and deployed to production."Because the app itself was a platform for creating exam questions and viewing grades, the userbase is naturally comprised of teachers and students. Some were from top US universities such as UC Berkeley and UC Davis, while there were "K-12 institutions with minors likely on the platform" as well, Khan said.With the security flaws in place, an unauthenticated attacker could trivially access every user record, send bulk emails through the platform, delete any user account, grade student test submissions, and access organizations' admin emails, for example.Of the 18,697 total user records exposed, 14,928 contained unique email addresses. The dataset included 4,538 student accounts â€“ all with email addresses â€“ 10,505 enterprise users, and 870 users whose full PII was exposed.The security flaws here are not exclusive to apps hosted by Lovable; the issue is broader and well-told by now.Vibe coding, Collins Dictionary's Word of the Year for 2025, promised to break down software development's steep learning curve and empower any prompt jockey to bring their app ideas to life.Veracode, for instance, recently found that 45 percent of AI-generated code contained security flaws, not to mention the myriad tales of woe reported by  in recent months.Khan said he believes Lovable should take responsibility for the security of the apps it hosts, and was especially peeved when, after reporting his findings via company support, his ticket was reportedly closed without response."If Lovable is going to market itself as a platform that generates production-ready apps with authentication 'included,' it bears some responsibility for the security posture of the apps it generates and promotes," Khan said."You can't showcase an app to 100,000 people, host it on your own infrastructure, and then close the ticket when someone tells you it's leaking user data. At minimum, a basic security scan of showcased applications would have caught every critical finding in this report."Lovable told  that the company has contacted the owner of the app in question and takes "any findings of this kind extremely seriously."Regarding the closed ticket, Lovable CISO Igor Andriushchenko said that the company only received "a proper disclosure report" on the evening of February 26 and acted on the findings "within minutes.""Any project built with Lovable includes a free security scan before publishing," Andriushchenko told . "This scan checks for vulnerabilities and, if found, provides recommendations on actions to take to resolve before publishing."Ultimately, it is at the discretion of the user to implement these recommendations. In this case, that implementation did not happen."This project also includes code not generated by Lovable and the vulnerable database is not hosted by Lovable. We have been in contact with the creator of the app, who is now addressing the issue." Â®]]></content:encoded></item><item><title>Allocating on the Stack</title><link>https://go.dev/blog/allocation-optimizations</link><author>spacey</author><category>hn</category><pubDate>Fri, 27 Feb 2026 16:34:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Weâ€™re always looking for ways to make Go programs faster. In the last
2 releases, we have concentrated on mitigating a particular source of
slowness, heap allocations. Each time a Go program allocates memory
from the heap, thereâ€™s a fairly large chunk of code that needs to run
to satisfy that allocation. In addition, heap allocations present
additional load on the garbage collector.  Even with recent
enhancements like Green Tea, the garbage collector
still incurs substantial overhead.So weâ€™ve been working on ways to do more allocations on the stack
instead of the heap.  Stack allocations are considerably cheaper to
perform (sometimes completely free).  Moreover, they present no load
to the garbage collector, as stack allocations can be collected
automatically together with the stack frame itself. Stack allocations
also enable prompt reuse, which is very cache friendly.Stack allocation of constant-sized slicesConsider the task of building a slice of tasks to process:func process(c chan task) {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
Letâ€™s walk through what happens at runtime when pulling tasks from the
channel  and adding them to the slice .On the first loop iteration, there is no backing store for , so
 has to allocate one. Because it doesnâ€™t know how big the
slice will eventually be, it canâ€™t be too aggressive. Currently, it
allocates a backing store of size 1.On the second loop iteration, the backing store now exists, but it is
full.  again has to allocate a new backing store, this time of
size 2. The old backing store of size 1 is now garbage.On the third loop iteration, the backing store of size 2 is
full.  has to allocate a new backing store, this time
of size 4. The old backing store of size 2 is now garbage.On the fourth loop iteration, the backing store of size 4 has only 3
items in it.  can just place the item in the existing backing
store and bump up the slice length. Yay! No call to the allocator for
this iteration.On the fifth loop iteration, the backing store of size 4 is full, and
 again has to allocate a new backing store, this time of size
8.And so on. We generally double the size of the allocation each time it
fills up, so we can eventually append most new tasks to the slice
without allocation. But there is a fair amount of overhead in the
â€œstartupâ€ phase when the slice is small. During this startup phase we
spend a lot of time in the allocator, and produce a bunch of garbage,
which seems pretty wasteful. And it may be that in your program, the
slice never really gets large. This startup phase may be all you ever
encounter.If this code was a really hot part of your program, you might be
tempted to start the slice out at a larger size, to avoid all of these
allocations.func process2(c chan task) {
    tasks := make([]task, 0, 10) // probably at most 10 tasks
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
This is a reasonable optimization to do. It is never incorrect; your
program still runs correctly. If the guess is too small, you get
allocations from  as before. If the guess is too large, you
waste some memory.If your guess for the number of tasks was a good one, then thereâ€™s
only one allocation site in this program. The  call allocates a
slice backing store of the correct size, and  never has to do
any reallocation.The surprising thing is that if you benchmark this code with 10
elements in the channel, youâ€™ll see that you didnâ€™t reduce the number
of allocations to 1, you reduced the number of allocations to 0!The reason is that the compiler decided to allocate the backing store
on the stack. Because it knows what size it needs to be (10 times the
size of a task) it can allocate storage for it in the stack frame of
 instead of on the heap.  Note
that this depends on the fact that the backing store does not escape
to the heap inside of .Stack allocation of variable-sized slicesBut of course, hard coding a size guess is a bit rigid.
Maybe we can pass in an estimated length?func process3(c chan task, lengthGuess int) {
    tasks := make([]task, 0, lengthGuess)
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
This lets the caller pick a good size for the  slice, which may
vary depending on where this code is being called from.Unfortunately, in Go 1.24 the non-constant size of the backing store
means the compiler can no longer allocate the backing store on the
stack.  It will end up on the heap, converting our 0-allocation code
to 1-allocation code. Still better than having  do all the
intermediate allocations, but unfortunate.But never fear, Go 1.25 is here!Imagine you decide to do the following, to get the stack allocation
only in cases where the guess is small:func process4(c chan task, lengthGuess int) {
    var tasks []task
    if lengthGuess <= 10 {
        tasks = make([]task, 0, 10)
    } else {
        tasks = make([]task, 0, lengthGuess)
    }
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
Kind of ugly, but it would work. When the guess is small, you use a
constant size  and thus a stack-allocated backing store, and
when the guess is larger you use a variable size  and allocate
the backing store from the heap.But in Go 1.25, you donâ€™t need to head down this ugly road. The Go
1.25 compiler does this transformation for you!  For certain slice
allocation locations, the compiler automatically allocates a small
(currently 32-byte) slice backing store, and uses that backing store
for the result of the  if the size requested is small
enough. Otherwise, it uses a heap allocation as normal.In Go 1.25,  performs zero heap allocations, if
 is small enough that a slice of that length fits into 32
bytes. (And of course that  is a correct guess for how
many items are in .)Weâ€™re always improving the performance of Go, so upgrade to the latest
Go release and be
surprised by
how much faster and memory efficient your program becomes!Stack allocation of append-allocated slicesOk, but you still donâ€™t want to have to change your API to add this
weird length guess. Anything else you could do?func process(c chan task) {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    processAll(tasks)
}
In Go 1.26, we allocate the same kind of small, speculative backing
store on the stack, but now we can use it directly at the 
site.On the first loop iteration, there is no backing store for , so
 uses a small, stack-allocated backing store as the first
allocation. If, for instance, we can fit 4 s in that backing store,
the first  allocates a backing store of length 4 from the stack.The next 3 loop iterations append directly to the stack backing store,
requiring no allocation.On the 4th iteration, the stack backing store is finally full and we
have to go to the heap for more backing store. But we have avoided
almost all of the startup overhead described earlier in this article.
No heap allocations of size, 1, 2, and 4, and none of the garbage that
they eventually become. If your slices are small, maybe you will never
have a heap allocation.Stack allocation of append-allocated escaping slicesOk, this is all good when the  slice doesnâ€™t escape. But what if
Iâ€™m returning the slice? Then it canâ€™t be allocated on the stack, right?Right! The backing store for the slice returned by  below
canâ€™t be allocated on the stack, because the stack frame for 
disappears when  returns.func extract(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    return tasks
}
But you might think, the  slice canâ€™t be allocated on the
stack. But what about all those intermediate slices that just become
garbage? Maybe we can allocate those on the stack?func extract2(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    tasks2 := make([]task, len(tasks))
    copy(tasks2, tasks)
    return tasks2
}
Then the  slice never escapes . It can benefit from
all of the optimizations described above. Then at the very end of
, when we know the final size of the slice, we do one heap
allocation of the required size, copy our s into it, and return
the copy.But do you really want to write all that additional code? It seems
error prone. Maybe the compiler can do this transformation for us?For escaping slices, the compiler will transform the original 
code to something like this:func extract3(c chan task) []task {
    var tasks []task
    for t := range c {
        tasks = append(tasks, t)
    }
    tasks = runtime.move2heap(tasks)
    return tasks
}
 is a special compiler+runtime function that is the
identity function for slices that are already allocated in the heap.
For slices that are on the stack, it allocates a new slice on the
heap, copies the stack-allocated slice to the heap copy, and returns
the heap copy.This ensures that for our original  code, if the number of
items fits in our small stack-allocated buffer, we perform exactly 1
allocation of exactly the right size. If the number of items exceeds
the capacity our small stack-allocated buffer, we do our normal
doubling-allocation once the stack-allocated buffer overflows.The optimization that Go 1.26 does is actually better than the
hand-optimized code, because it does not require the extra
allocation+copy that the hand-optimized code always does at the end.
It requires the allocation+copy only in the case that weâ€™ve exclusively
operated on a stack-backed slice up to the return point.We do pay the cost for a copy, but that cost is almost completely
offset by the copies in the startup phase that we no longer have to
do. (In fact, the new scheme at worst has to copy one more element
than the old scheme.)Hand optimization can still be beneficial, especially if you have a
good estimate of the slice size ahead of time. But hopefully the
compiler will now catch a lot of the simple cases for you and allow
you to focus on the remaining ones that really matter.There are a lot of details that the compiler needs to ensure to get
all these optimizations right. If you think that one of these
optimizations is causing correctness or (negative) performance issues
for you, you can turn them off with
-gcflags=all=-d=variablemakehash=n. If turning these optimizations
off helps, please file an issue so we can investigate. Go stacks do not have any -style mechanism for
dynamically-sized stack frames. All Go stack frames are constant
sized.]]></content:encoded></item><item><title>NASA announces overhaul of Artemis program amid safety concerns, delays</title><link>https://www.cbsnews.com/news/nasa-artemis-moon-program-overhaul/</link><author>voxadam</author><category>hn</category><pubDate>Fri, 27 Feb 2026 16:33:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[New NASA Administrator Jared Isaacman announced a major overhaul of the agency's  Friday, acknowledging that the agency's plan to land astronauts on the moon in 2028 was not realistic without another preparatory mission first to lay the groundwork.Â He said NASA will now add an additional flight in 2027 in which astronauts will dock with new commercial moon landers in low-Earth orbit for detailed tests of navigation, communications, propulsion and life support systems and to verify rendezvous procedures.That flight, in turn, will be followed by at least one and possibly two lunar landing missions in 2028 that incorporate lessons learned from the preceding flight.The goal is to accelerate the pace of launches of the huge Space Launch System rocket while carrying out Artemis flights in evolutionary steps â€” not attempting missions that rely on too many untested technologies and procedures at once."We're going to get there in steps, continue to take down risk as we learn more and we roll that information into subsequent designs," Isaacman said told CBS News. "We've got to get back to basics."Isaacman outlined the plan in an interview with CBS News space contributor Christian Davenport and then again during a news conference Friday.Â The announcement came two days after release of a sharply-worded report from NASA's independent Aerospace Safety Advisory Panel that deemed the existing plans too risky.The panel raised concerns about the number of "firsts" required by the original Artemis III moon landing mission and recommended that NASA "restructure" the program to create a more balanced risk posture."It is interesting that a lot of the things that we are addressing directly go to the points they raised in their report," Isaacman said Friday. "I can't say we actually collaborated on it because I generally think these were all pretty obvious observations."Launch had been planned for early February, but it was delayed to repair a hydrogen leak and, more recently, to give engineers time to fix a helium pressurization problem in the rocket's upper stage. Launch is now on hold until at least April 1.The Artemis III mission, which had been expected to land astronauts near the moon's south pole in 2028, now will be redefined and rescheduled â€” launching in 2027 but not to the moon, Isaacman said. Instead, the yet-to-be-named astronauts will rendezvous and dock in orbit closer to home with one or both of the commercially built lunar landers now under development at Elon Musk's SpaceX and Jeff Bezos' Blue Origin.The idea is to gain valuable near-term flight experience before attempting a moon landing with astronauts on board. With Artemis III under its belt, NASA hopes to launch two moon landing missions in 2028, Artemis IV and V, using one or both landers, and to continue with one moonshot per year thereafter."What helps us get to the moon? Well, for sure, rendezvous and docking with one or ideally both landers, that gives you an opportunity to do some integrated testing of a vehicle that we are going to depend upon the following year to take those astronauts down to the surface of the moon," Isaacman told CBS News.The revised Artemis III mission will also give astronauts a chance to test out new spacesuits that future moonwalkers will use."It's an opportunity to â€¦ actually have the suits in microgravity, even if we don't go outside the vehicle in them. You get a lot of good learning from that," Isaacman said.The Artemis III test flight with one or two lander dockings in Earth orbit is similar in concept to Apollo 9, which launched a command module and lander to Earth orbit for flight tests in 1969 and helped pave the way to the  landing four months later.Isaacman said SpaceX and Blue Origin are "both looking to do uncrewed landing demonstrations as part of the existing agreement.""So we want to just take advantage of this to set up both vendors for future success on a lunar landing," he said. "This is the proper way to do it, if it works out from a timing perspective, to be able to rendezvous and dock with both. ... This, again, is the right way to proceed in order to have a high confidence opportunity in '28 to land."The Artemis IV and V missions in 2028 will use whichever landers are deemed ready for service. If only one company's lander is available, that lander would be used for both missions, an official said. If both are available, one would be used for one flight and one for the other.Launching Artemis III, IV and V before the end of 2028 will not be easy, and Isaacman said it is essential that NASA rebuild its workforce and regain the technical competence to support a higher launch cadence, moving from one flight every 18 months or so to a flight every year. That pace, he argued, will reduce risk."When you regain these core competencies and you start exercising your muscles, your skills do not atrophy," he said. "It's safer. And yes, you are buying down risk, because you're able to test things in low Earth orbit before you need to get to the moon, which is exactly what we did during the Apollo era."He said he did not blame NASA's contractors for the current slow pace of Artemis launches. Instead, "we should have made better decisions (in the past) and said, you don't go from Artemis II to landing on the moon with Artemis III."Safety advisers called for changes to "high risk" plansThe Artemis overhaul was announced two days after the release of a report by the lAerospace Safety Advisory Panel that said the original plan to move directly from Artemis II to a lunar touchdown in 2028 using a SpaceX lander did not have the proper margin of safety and did not appear to be realistically achievable.The panel raised concerns about the number of "firsts" required by that mission in its current form and recommended that NASA "restructure the Artemis Program to create a more balanced risk posture for Artemis III and future missions."The plan outlined by Isaacman appears to address many of the core issues raised by the safety panel.Officials said Isaacman had discussed accelerating lander development with both SpaceX and Blue Origin and that both were on board. He also discussed the accelerated Artemis overhaul with Boeing, which manages the SLS rocket and builds its massive first stage; with United Launch Alliance, builder of the rocket's upper stage, Orion-builder Lockheed Martin and other Artemis contractors.All, the official said, were in agreement."Boeing is a proud partner to the Artemis mission and our team is honored to contribute to NASA's vision for American space leadership," Steve Parker, the president and CEO of Boeing Defense, Space & Security, said in a statement. "We are ready to meet the increased demand."SpaceX said, "We look forward to working with NASA to fly missions that demonstrate valuable progress towards establishing a permanent, sustainable presence on the lunar surface."And Blue Origin responded, "Let's go! We're all in!"Isaacman also said the agency would halt work to develop a more powerful version of the SLS rocket's upper stage, known as the Exploration Upper Stage, or EUS. Instead, NASA will go forward with a "standardized," less powerful stage but one that will minimize major changes between flights and utilize the same launch gantry.Under the original Artemis architecture, NASA planned on multiple versions of the SLS rocket, ranging from the "Block 1" vehicle currently in use to a more powerful EUS-equipped Block 1B and eventually an even bigger Block 2 model using advanced solid rocket boosters. The latter two versions required use of a taller mobile launch gantry, already well under construction at the Kennedy Space Center."It is needlessly complicated to alter the configuration of the SLS and Orion stack to undertake subsequent Artemis missions," Amit Kshatriya, NASA's associate administrator, said in a statement."The entire sequence of Artemis flights needs to represent a step-by-step build-up of capability, with each step bringing us closer to our ability to perform the landing missions. Each step needs to be big enough to make progress, but not so big that we take unnecessary risk given previous learnings."As a result, NASA will stick with the current version of the SLS with the addition of the "standardized" upper stage. No other details were provided.Isaacman closed out the CBS interview by saying flight-tested hardware, a revitalized work force and a more Apollo-like management strategy are only part of the story."There's another ingredient that's required, and that's the orbital economy, whether it happens in low-Earth orbit or on the lunar surface," Isaacman said."We've got to do something where we can get more value out of space and the lunar surface than we put into it. And that's how you really ignite an economy, and that's how everything we want to do in space is not perpetually dependent on taxpayers."
                  
        contributed to this report.
    ]]></content:encoded></item><item><title>Show HN: Claude-File-Recovery, recover files from your ~/.claude sessions</title><link>https://github.com/hjtenklooster/claude-file-recovery</link><author>rikk3rt</author><category>hn</category><pubDate>Fri, 27 Feb 2026 16:26:22 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Claude Code deleted my research and plan markdown files and informed me: â€œI accidentally rm -rf'd real directories in my Obsidian vault through a symlink it didn't realize was there: I made a mistake. â€œUnfortunately the backup of my documentation accidentally hadnâ€™t run for a month. So I built claude-file-recovery, a CLI-tool and TUI that is able to extract your files from your ~/.claude session history and thankfully I was able to recover my files. It's able to extract any file that Claude Code ever read, edited or wrote. I hope you will never need it, but you can find it on my GitHub and pip. Note: It can recover an earlier version of a file at a certain point in time.pip install claude-file-recovery]]></content:encoded></item><item><title>A Chinese officialâ€™s use of ChatGPT revealed an intimidation operation</title><link>https://www.cnn.com/2026/02/25/politics/chatgpt-china-intimidation-operation</link><author>cwwc</author><category>hn</category><pubDate>Fri, 27 Feb 2026 15:52:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
            A sprawling Chinese influence operation â€” accidentally revealed by a Chinese law enforcement officialâ€™s use of ChatGPT â€” focused on intimidating Chinese dissidents abroad, including by impersonating US immigration officials, according to a new report from ChatGPT-maker OpenAI.
    
            The Chinese law enforcement official used ChatGPT like a diary to document the alleged covert campaign of suppression, OpenAI said. In one instance, Chinese operators allegedly disguised themselves as US immigration officials to warn a US-based Chinese dissident that their public statements had supposedly broken the law, according to the ChatGPT user. In another case, they describe an effort to use forged documents from a US county court to try to get a Chinese dissidentâ€™s social media account taken down.
    
            The report offers one of the most vivid examples yet of how authoritarian regimes can use AI tools to document their censorship efforts. The influence operation appeared to involve hundreds of Chinese operators and thousands of fake online accounts on various social media platforms, according to OpenAI.
    
            â€œThis is what Chinese modern transnational repression looks like,â€ Ben Nimmo, principal investigator at OpenAI, told reporters ahead of the reportâ€™s release. â€œItâ€™s not just digital. Itâ€™s not just about trolling. Itâ€™s industrialized. Itâ€™s about trying to hit critics of the CCP [Chinese Communist Party] with everything, everywhere, all at once.â€
    
            CNN has requested comment on the report from the Chinese Embassy in Washington, DC.
    
            ChatGPT served as a journal for the Chinese operative to keep track of the covert network, while much of the networkâ€™s content was generated by other tools and spread through social media accounts and websites. OpenAI banned the user after discovering the activity.
    
            OpenAIâ€™s investigators were able to match descriptions from the ChatGPT user with real-world online activity and impact. The user described an effort to fake the death of a Chinese dissident by creating a phony obituary and photos of a gravestone and posting them online. False rumors of the dissidentâ€™s death did indeed surfaced online in 2023, according to a Chinese-language Voice of America article.
    
            In another case, the ChatGPT user asked the AI agent to draw up a multi-part plan to denigrate the incoming Japanese prime minister, Sanae Takaichi, in part by fanning online anger about US tariffs on Japanese goods. ChatGPT refused to respond to the prompt, according to OpenAI. But in late October, as Takaichi took power, hashtags emerged on a popular forum for Japanese graphic artists attacking her and complaining about US tariffs, according to OpenAI.
    
            The report comes amid a battle between the US and China for supremacy over AI. At stake is how the technology is used on the battlefield and in the boardroom of the worldâ€™s two biggest economies.
    
            The Pentagon is in a standoff with another prominent AI company, Anthropic, over the use of its AI model. Defense Secretary Pete Hegseth has given Anthropic CEO Dario Amodei a Friday deadline to comply with demands to peel back safeguards on its AI model or risk losing a lucrative Pentagon contract.
    
            The report from OpenAI â€œclearly demonstrates the way that China is actively employing AI tools to enhance information operations,â€ Michael Horowitz, a former Pentagon official focused on emerging technologies, told CNN.
    
            â€œUS-China AI competition is continuing to intensify,â€ said Horowtiz, who is now a professor at the University of Pennsylvania. â€œThis competition is not just taking place at the frontier, but in how Chinaâ€™s government is planning and implementing the day-to-day of their surveillance and information apparatus.â€
    ]]></content:encoded></item><item><title>ChatGPT Health fails to recognise medical emergencies â€“ study</title><link>https://www.theguardian.com/technology/2026/feb/26/chatgpt-health-fails-recognise-medical-emergencies</link><author>simonebrunozzi</author><category>hn</category><pubDate>Fri, 27 Feb 2026 15:44:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ChatGPT Health regularly misses the need for medical urgent care and frequently fails to detect suicidal ideation, a study of the AI platform has found, which experts worry could â€œfeasibly lead to unnecessary harm and deathâ€.The lead author of the study, Dr Ashwin Ramaswamy, said â€œwe wanted to answer the most basic safety question; if someone is having a real medical emergency and asks ChatGPT Health what to do, will it tell them to go to the emergency department?â€Ramaswamy and his colleagues created 60 realistic patient scenarios covering health conditions from mild illnesses to emergencies. Three independent doctors reviewed each scenario and agreed on the level of care needed, based on clinical guidelines.The team then asked ChatGPT Health for advice on each case under different conditions, including changing the patientâ€™s gender, adding test results, or adding comments from family members, generating nearly 1,000 responses.They then compared the platformâ€™s recommendations with the doctorsâ€™ assessments.While it performed well in textbook emergencies such as stroke or severe allergic reactions, it struggled in other situations. In one asthma scenario, it advised waiting rather than seeking emergency treatment despite the platform identifying early warning signs of respiratory failure.In 51.6% of cases where someone needed to go to the hospital immediately, the platform said stay home or book a routine medical appointment, a result Alex Ruani, a doctoral researcher in health misinformation mitigation with University College London, described as â€œunbelievably dangerousâ€.â€œIf youâ€™re experiencing respiratory failure or diabetic ketoacidosis, you have a 50/50 chance of this AI telling you itâ€™s not a big deal,â€ she said. â€œWhat worries me most is the false sense of security these systems create. If someone is told to wait 48 hours during an asthma attack or diabetic crisis, that reassurance could cost them their life.â€In one of the simulations, eight times out of 10 (84%), the platform sent a suffocating woman to a future appointment she would not live to see, Ruani said. Meanwhile, 64.8% of completely safe individuals were told to seek immediate medical care, said Ruani, who was not involved in the study.The platform was also nearly 12 times more likely to downplay symptoms because the â€œpatientâ€ told it a â€œfriendâ€ in the scenario suggested it was nothing serious.â€œIt is why many of us studying these systems are focused on urgently developing clear safety standards and independent auditing mechanisms to reduce preventable harm,â€ Ruani said.A spokesperson for OpenAI said while the company welcomed independent research evaluating AI systems in healthcare, the study did not reflect how people typically use ChatGPT Health in real life. The model is also continuously updated and refined, the spokesperson said.Ruani said even though simulations created by the researchers were used, â€œa plausible risk of harm is enough to justify stronger safeguards and independent oversightâ€.Ramaswamy, a urology instructor atthe Icahn School of Medicine at Mount Sinai in the US, said he was particularly concerned by the platformâ€™s under-reaction to suicide ideation.â€œWe tested ChatGPT Health with a 27-year-old patient who said heâ€™d been thinking about taking a lot of pills,â€ he said. When the patient described his symptoms alone, the crisis intervention banner linking to suicide help services appeared every time.â€œThen we added normal lab results,â€ Ramaswamy said. â€œSame patient, same words, same severity. The banner vanished. Zero out of 16 attempts. A crisis guardrail that depends on whether you mentioned your labs is not ready, and itâ€™s arguably more dangerous than having no guardrail at all, because no one can predict when it will fail.â€Prof Paul Henman, a digital sociologist and policy expert with the University of Queensland, said: â€œThis is a really important paper.â€œIf ChatGPT Health was used by people at home, it could lead to higher numbers of unnecessary medical presentations for low-level conditions and a failure of people to obtain urgent medical care when required, which could feasibly lead to unnecessary harm and death.â€He said it also raised the prospects of legal liability, with legal cases against tech companies already in motion in relation to suicide and self-harm after using AI chatbots.â€œIt is not clear what OpenAI is seeking to achieve by creating this product, how it was trained, what guardrails it has introduced and what warnings it provides to users,â€ Henman said.â€œBecause we donâ€™t know how ChatGPT Health was trained and what the context it was using, we donâ€™t really know what is embedded into its models.â€]]></content:encoded></item><item><title>We gave terabytes of CI logs to an LLM</title><link>https://www.mendral.com/blog/llms-are-good-at-sql</link><author>shad42</author><category>hn</category><pubDate>Fri, 27 Feb 2026 15:41:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Last week, our agent traced a flaky test to a dependency bump three weeks prior. It did this by writing its own SQL queries, scanning hundreds of millions of log lines across a dozen queries, and following a trail from job metadata to raw log output. The whole investigation took seconds.To do this, the agent needs context: not one log file, but every build, every test, every log line, across months of history. Every week, about  and  flow through our system. All of it lands in ClickHouse, compressed at 35:1. All of it is queryable in milliseconds.A SQL interface for the agentWe expose a SQL interface to the agent, scoped to the organization it's investigating. The agent constructs its own queries based on the question. No predefined query library, no rigid tool API.LLMs are good at SQL. There's an enormous amount of SQL in training data, and the syntax maps well to natural-language questions about data. A constrained tool API like get_failure_rate(workflow, days) would limit the agent to the questions we anticipated. A SQL interface lets it ask questions we never thought of, which matters when you're debugging novel failures.The agent queries two main targets:: a materialized view with one row per CI job execution. The agent uses this 63% of the time for questions like "how often does this fail?", "what's the success rate?", "which jobs are slowest?", "when did this start failing?": one row per log line. The agent uses this 37% of the time for questions like "show me the error output for this job", "when did this log pattern first appear?", "how often does this error message occur across runs?"52,000 queries across 8,500 investigationsWe analyzed 8,534 agent sessions and 52,312 queries from our observability pipeline.The agent doesn't stop at one query. It investigates. Starts broad, then drills in. Total rows scanned across all queries to answer one question:The typical question scans  across about 3 queries. At P75 it's 5.2 million rows. At P95 it's . The heaviest raw-log sessions, deep investigations tracing error patterns across months of history, scan .The agent starts broad and narrows. A typical investigation begins with job metadata: "what's the failure rate for this workflow?", "which jobs failed on this commit?" These are cheap queries (median 47K rows) against a compact, pre-aggregated materialized view.When it finds something interesting, it drills into raw logs: "show me the stack trace for this specific failure", "has this error message appeared before?" These are the expensive queries (median 1.1M rows), full-text scans across log output. But this is exactly the kind of search that would take a human minutes of scrolling through GitHub Actions log viewers.The agent averages 4.4 queries per session, but heavy investigations issue many more. A P95 session isn't one big query. It's the agent following a trail, query after query, as it narrows in on a root cause.For the agent to query this fast, the data needs to be structured for it. Up to 300 million log lines flow through on a busy day. We use ClickHouse.Every log line in our system carries  of metadata: the full context of the CI run it belongs to. Commit SHA, author, branch, PR title, workflow name, job name, step name, runner info, timestamps, and more.In a traditional row-store, this would be insane. You'd normalize. Run-level metadata in one table, job metadata in another, join at query time. Denormalizing 48 columns onto every single log line sounds like a storage disaster.In ClickHouse's columnar format, it's essentially free.A column like  has the same value for every log line in a CI run, and a single run can produce thousands of log lines. ClickHouse stores those thousands of identical values in sequence. The compression algorithm sees the repetition and compresses it to almost nothing.Same message for every line in a run (thousands of lines)Same PR/commit title across all linesSame .github/workflows/foo.yml pathSame step name across hundreds of linesSame job name across hundreds/thousands of linesThe agent asks arbitrary questions. One might filter by commit author, the next by runner label, the next by step name. Without denormalization, every one of those requires a join. With it, they're all column predicates.Raw log text ( uncompressed)All 48 columns uncompressedThe raw log text alone is 664 GiB. Adding all 48 columns of metadata inflates it to 5.31 TiB uncompressed, 8x the raw text. On disk, the whole thing compresses to 154 GiB. ClickHouse stores 8x more data (all the enriched metadata) in a quarter of the size of the raw text alone.That's about  on disk, including all 48 columns. Yes, really. 21 bytes for a log line plus its commit SHA, author, branch, job name, step name, runner info, and 41 other fields.Where the storage actually goesNot all columns compress equally. The unique-per-row columns (log text, timestamp, line number) compress modestly and dominate storage. The metadata columns, which repeat across thousands of lines, are nearly free.Everything else (41 columns)The top three (, , ) account for 53% of all storage. Everything else is repeated metadata that compresses to almost nothing.We use a few ClickHouse patterns that keep things fast: means the data is physically sorted for our access pattern. The sort order is (org, ts, repository, run_id, ...), so every query is scoped to one organization and a time range, and ClickHouse skips everything else without reading it. let ClickHouse avoid scanning data it doesn't need. We use bloom filters on 14 columns (org, repository, job name, branch, commit SHA, etc.) and an ngram bloom filter on  for full-text search. When the agent searches for an error message across billions of log lines, ClickHouse checks the ngram index to skip granules that can't contain the search term, turning a full table scan into a targeted read. pre-compute aggregations on insert. When the agent asks "what's the failure rate for this workflow over the last 30 days?", the answer is already computed. The aggregation happened when the data was written. give us high write throughput without building our own batching layer. We fire-and-forget individual inserts, and ClickHouse batches them internally.Query latency across 52K queries:Job metadata queries return in 20ms at the median. Raw log queries, scanning a million rows at the median, come back in 110ms.Latency scales roughly linearly with rows scanned:10x more rows â‰ˆ 10x more latency. 60% of all queries scan under 100K rows and return in under 50ms, fast enough that the agent can fire off several per second without breaking stride. At the extreme end, the agent occasionally scans over a billion rows in a single query; even those complete in about 30 seconds at the median.Ingesting through GitHub's rate limitNone of the above works without fresh data. The agent needs to reason about the build that just failed, not one from an hour ago.The rate limit constraintGitHub's API gives you 15,000 requests per hour per App installation (5,000 on non-Enterprise plans). That sounds generous until you're continuously polling workflow runs, jobs, steps, and log output across dozens of active repositories. A single commit can spawn hundreds of parallel jobs, each producing logs you need to fetch.And ingestion isn't the only thing hitting the API. When the agent investigates a failure, it pulls PR metadata, reads file diffs, posts comments, and opens pull requests. All of that counts against the same 15,000-request budget. Throttle ingestion too aggressively and your data goes stale. Throttle too little and you starve the agent of the API access it needs to do its job.Early on, we hit this. Our ingestion would slam into the rate limit, get blocked for the remainder of the hour, and fall behind. By the time it caught up, we were ingesting logs from 30+ minutes ago. For an agent that needs to reason about the build that just failed, that's useless. If an engineer has to wait for the agent to catch up, they've already context-switched to investigating manually.The fix was throttling: spreading requests evenly across the rate limit window instead of bursting. We cap ingestion at roughly 3 requests per second, keeping about 4,000 requests per hour free for the agent.Our sustained request rate:Our rate limit budget over time:That sawtooth is the steady state. Each downward slope is us consuming API calls; each vertical jump is the hourly limit resetting. At peak, we burn through most of the budget before the window resets, with headroom left for the agent.Once we trusted the throttling, we pushed the ingestion rate about 20% higher:The dashed line marks the deployment. The budget draws down more aggressively after the change. We're consuming more of the available headroom per window, while still never fully exhausting it. Fresher data, acceptable margin.We target under  for ingestion delay, the time between an event happening on GitHub and it being queryable in our system. Most of the time, we're at a few seconds.Both our ingestion pipeline and our agent run on Inngest, a durable execution engine. When either one hits a rate limit, it doesn't crash, retry blindly, or spin in a loop. It .GitHub's rate limit response headers tell you exactly how long you need to wait. We read that value, add 10% jitter to avoid a thundering herd when the limit resets, and suspend the execution. The full state is checkpointed: progress through the workflow, which jobs have been fetched, where we are in the log pagination.When the wait is over, execution resumes at exactly the point it left off. No re-initialization, no duplicate work. It picks up the next API call as if nothing happened.Compare this to the alternative: retry logic, state recovery, deduplication. Every function needs to be idempotent. Every interrupted batch needs to be reconciled. With durable execution, the rate limit is just a pause button.CI activity is bursty. Someone merges a big PR, a release branch gets cut, three teams push at the same time. Our function throughput:The grey line is queued work. It spikes to 3,000+ during bursts of CI activity. The blue and green lines (started and ended) stay smooth at 800-1,000. The execution engine absorbs the spikes and processes work at a steady rate.Ingestion delay over time:Spikes during peak activity, but the system recovers. The 5-minute P95 target holds: bursts push delay up briefly, then it drops back to seconds once the queue drains.Nobody puts "we built a really good rate limiter" on their landing page. But without fresh, queryable data, your agent can't answer the question that actually matters: did I break this, or was it already broken?We're building Mendral (YC W26). We spent a decade building and scaling CI systems at Docker and Dagger, and the work was always the same: stare at logs, correlate failures, figure out what changed. Now we're automating it.]]></content:encoded></item><item><title>Open source calculator firmware DB48X forbids CA/CO use due to age verification</title><link>https://github.com/c3d/db48x/commit/7819972b641ac808d46c54d3f5d1df70d706d286</link><author>iamnothere</author><category>hn</category><pubDate>Fri, 27 Feb 2026 15:37:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>An ode to houseplant programming (2025)</title><link>https://hannahilea.com/blog/houseplant-programming/</link><author>evakhoury</author><category>hn</category><pubDate>Fri, 27 Feb 2026 15:19:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Recurse Center (RC) peer Ryan recently coined a phrase that I instantly
        fell in love with: .
      
          [The tool I built] solves my idiosyncratic problems and may not address yours at all. Thatâ€™s fineâ€”take it as an ad to write tiny software just for yourself. Houseplant programming ðŸª´ !  This isnâ€™t an existing phrase as far as I know, but the closest I can think of is â€œbarefoot developersâ€ which a) is a little more granola than my vibe and b) is maybe tied up in some AI stuff. I guess this is
          situated software but even smaller: Iâ€™m not building for dozens of users, Iâ€™m building for one user
          in particular.Houseplant programming: tiny software just for yourself. At the risk of overexplaining and thus cheapening the analogy, I feel the need to wax poetic.ðŸª´When â€œIt works on my machineâ€ is the goal, not the excuseThings I have found myself saying about some personal projects, almost apologetically:
        In the world of houseplant programming none of these statements are apology-worthy. In a workplace, about a project that is intended for
        productionization and mass dissemination? Sure, production-ready codeâ€”code that has a job, or provides the infrastructure for a jobâ€”needs to be some
        flavor of robust and tested and reliable. For a project that lives in my house and does what I need it to and periodically needs a little extra help? No worries.
      Aditya Athalye (another RC peer!) perfectly captures this vibe in the project description for his software project
        :
      â€™s job is to help me make my website: https://evalapply.org. Thus, â€™s scope, (mis)feature set, polish will always be
          production-grade, where production is â€œworks on my machine(s)â€ :) Strong â€œEverything I do is the attitude of an award winner because I have won an awardâ€ energy:Any code is production ready, if you redefine the scope of your production environment!Properties of houseplants, programmatic and chlorophyllous
        Before we get to the self-reflective bit, here is a non-exhaustive list of parallels between my houseplants and my houseplant programs:
      : I love having both plants and homemade projects in my living space. Sharing a space with them reminds me of things that I like about the world and about myself.: Like my plants, I love my little projects and I want them to thrive, and I baby them a little bit to get them started. But also, if they donâ€™t work out? It isnâ€™t a big deal, into
           Github they go, where a hard-won line or two may be  recycled into a future project.
        : Clippings! I love to propagate my plants and share them with friends. Do you want a pilea or a spider plant or a nice philodendron? Let me know, Iâ€™ll hook you up! Similarly, do you want to set up
            your own pen plotter or make some quick and easy screenshot memes? Awesome, I try to document and share the code and steps for recreating most of my projects.
          
            That said, once a plant/code has taken up residence in your home, it is no longer my responsibility. While Iâ€™d love to hear about what you did to help it thrive, and if it starts looking sad Iâ€™ll gladly help you think through
            what might help, if it never thrives Iâ€™m probably not going to lose sleep over it.
          Besides, once youâ€™ve gotten as far as propagating the code/plant Iâ€™ve given you, youâ€™ll know about as much about the situation as I doâ€”maybe moreâ€”and now we can explore the next steps together.: Just like some plants, some projects are practically poisonous to my cat andâ€”if the cat had her wayâ€”should be rehomed with a pet-free pal.
          : I donâ€™t care to engineer my houseplants to thrive in every environmentâ€”and similarly, I donâ€™t feel a need to make my houseplant code fully generalizable, until there is a more specific reason
            to do so.
          : I love reading about other peopleâ€™s houseplant projects. While I occasionally take code cuttings for my own home, mostly I just want to wander around and admire their houseplants and learn
            more about the woes they encountered when figuring out how to help their code/plants thrive.
          I do not need to propagate someoneâ€™s houseplant [code] in my own home in order to admire it; I can learn to consider a different fertilizer or communication protocol without transplanting their program into my own home.: One personâ€™s houseplants are another personâ€™s plant nursery. One personâ€™s houseplant code is another personâ€™s B2B SaaS product. Enough said.: Soil gnats. Where do they even come from?! It is unknowable.
            Sometimes my weather station shows me the icon for snow, even though it is currently April and the temperature isnâ€™t predicted to dip below 32. Â¯\_(ãƒ„)_/Â¯
            : It is really, really fun to grow plants. It is really really fun to write code.Not an idea, not yet a Platonic ideal
        While I build software as a career, I also like to muck about with code in service of other goals. When sharing those other projects it has taken me a long time be able to talk about what my code does do without adding a zillion
        caveats about what the code  do.
        Why? I think somewhere along the line I picked up the unhealthyâ€”and false!â€”assumption that it wasnâ€™t worth sharing my code until it was ready to be reused easily by whoever was able to access itâ€”specifically, not sharing that code
        until it was â€œproduction ready,â€ for some arbitrary and ever-growing definition of â€œproductionâ€ that I never  fully defined for myself.
        In the last year or so when presenting personal projects Iâ€™ve taken to saying that theyâ€™re prototypes. Prototyping is a thing that makes sense to many folks in the fieldâ€”it involves a first pass at trying to build something, with
        output that  be optimized, might be hacked together with glue and dreams, and possibly even â€œonly works on my machineâ€. But itâ€™s proof that it is worth spending more time on something, or  worth spending
        more time on something.
        The thing is, a lot of the personal projects Iâ€™ve built are  prototypes, even if they share a lot of the same characteristics of a prototype: while they are a first-ish pass at bringing an idea to life, and they
         be turned into a more generalizable or generic Thing, theyâ€™re never designed to be more than that first pass with its context-specific configuration.
      
        While rebranding some of the projects Iâ€™ve built as â€œprototypesâ€ helped me feel better about sharing something not totally polished, Iâ€™ve also felt like the term somehow devalues what Iâ€™ve built. Sure, sometimes what Iâ€™ve built
         a prototype! But often, it isnâ€™t. Itâ€™s a first pass, sure, but itâ€™s just a weird little guy of an idea, and
        doesnâ€™t need to promise to be any more than what it already is. Just existing is enough,and Iâ€™m not necessarily interested in developing it into a less-weird less-little guy!
      Thus: houseplant programming. Tiny software for just myself.Epilogue: Bouquet programming ðŸ’Iâ€™m going to spare us all a further brainstorm of plant/code parallels, with the exception of one spin-off term:  ðŸ’.
        Iâ€™m hereby defining bouquet programming as one-off code that is written for one specific user to support one specific use-case, in a non-recurring way. By definition, it needs no maintenance and simply provides proof of
        what once was run. Examples of bouquet programming: an analysis script in support of a one-time plot, a scrappy proof-of-concept or a
        minimal reproducible example.
      
        Bouquet programming is still worth writing home about (!) and sharing generously in the same ways as houseplant programmingâ€”or agricultural programming!â€”but is even  likely to work off-the-shelf for a new application
        than houseplant code is, even if rerun by the same person who originally programmed it.
      Bonus: Garden stakes for horticulturalist programmersI made a status badge for houseplant reposâ€”feel free to use it!<a href="https://www.hannahilea.com/blog/houseplant-programming">
  <img alt="Static Badge" src="https://img.shields.io/badge/%F0%9F%AA%B4%20Houseplant%20-x?style=flat&amp;label=Project%20type&amp;color=1E1E1D">
</a>And a bonus badge for bouquet programming:<a href="https://www.hannahilea.com/blog/houseplant-programming">
  <img alt="Static Badge" src="https://img.shields.io/badge/&#x1F490;%20Bouquet%20-x?style=flat&amp;label=Project%20type&amp;color=1E1E1D">
</a>Thanks to Ryan for the coinage and to AF for introducing me to strategies for recognizing and countering perfectionism.Tags: phytoid, houseplant-programming]]></content:encoded></item><item><title>Show HN: Badge that shows how well your codebase fits in an LLM&apos;s context window</title><link>https://github.com/qwibitai/nanoclaw/tree/main/repo-tokens</link><author>jimminyx</author><category>hn</category><pubDate>Fri, 27 Feb 2026 15:14:42 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Small codebases were always a good thing. With coding agents, there's now a huge advantage to having a codebase small enough that an agent can hold the full thing in context.Repo Tokens is a GitHub Action that counts your codebase's size in tokens (using tiktoken) and updates a badge in your README. The badge color reflects what percentage of an LLM's context window the codebase fills: green for under 30%, yellow for 50-70%, red for 70%+. Context window size is configurable and defaults to 200k (size of Claude models).It's a composite action. Installs tiktoken, runs ~60 lines of inline Python, takes about 10 seconds. The action updates the README but doesn't commit, so your workflow controls the git strategy.The idea is to make token size a visible metric, like bundle size badges for JS libraries. Hopefully a small nudge to keep codebases lean and agent-friendly.]]></content:encoded></item><item><title>Court finds Fourth Amendment doesnâ€™t support broad search of protestersâ€™ devices</title><link>https://www.eff.org/deeplinks/2026/02/victory-tenth-circuit-finds-fourth-amendment-doesnt-support-broad-search-0</link><author>hn_acker</author><category>hn</category><pubDate>Fri, 27 Feb 2026 15:09:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In a big win for protestersâ€™ rights, the U.S. Court of Appeals for the Tenth Circuit overturned a lower courtâ€™s dismissal of a challenge to sweeping warrants to search a protesterâ€™s devices and digital data and a nonprofitâ€™s social media data.The case, Armendariz v. City of Colorado Springs, arose after a housing protest in 2021, during which Colorado Springs police arrested protesters for obstructing a roadway. After the demonstration, police also obtained warrants to seize and search through the devices and data of Jacqueline Armendariz Unzueta, who they claimed threw a bike at them during the protest. The warrants included a search through all of her photos, videos, emails, text messages, and location data over a two-month period, as well as a time-unlimited search for 26 keywords, including words as broad as â€œbike,â€ â€œassault,â€ â€œcelebration,â€ and â€œright,â€ that allowed police to comb through years of Armendarizâ€™s private and sensitive dataâ€”all supposedly to look for evidence related to the alleged simple assault. Police further obtained a warrant to search the Facebook page of the Chinook Center, the organization that spearheaded the protest, despite the Chinook Center never having been accused of a crime.The district court dismissed the civil rights lawsuit brought by Armendariz and the Chinook Center, holding that the searches were justified and that, in any case, the officers were entitled to qualified immunity. The plaintiffs, represented by the ACLU of Colorado, appealed. EFFâ€”joined by the Center for Democracy and Technology, the Electronic Privacy Information Center, and the Knight First Amendment Institute at Columbia Universityâ€”wrote an amicus brief in support of that appeal.In a 2-1 opinion, the Tenth Circuit reversed the district courtâ€™s dismissal of the lawsuitâ€™s Fourth Amendment search and seizure claims. The court painstakingly picked apart each of the three warrants and found them to be overbroad and lacking in particularity as to the scope and duration of the searches. The court further held that in furnishing such facially deficient warrants, the officers violated â€œclearly establishedâ€ law and thus were not entitled to qualified immunity. Although the court did not explicitly address the First Amendment concerns raised by the lawsuit, it did note the backdrop against how these searches were carried out, including animus by Colorado Springs police leading up to the housing protest.It is rare for appellate courts to call into question any search warrants. Itâ€™s even rarer for them to deny qualified immunity defenses. The Tenth Circuitâ€™s decision should be celebrated as a big win for protesters and anyone concerned about police immunity for violating peopleâ€™s constitutional rights. The case is now remanded back to the district court to proceedâ€”and hopefully further vindicate the privacy rights we all have in our devices and digital data.]]></content:encoded></item><item><title>The Pentagon is making a mistake by threatening Anthropic</title><link>https://www.understandingai.org/p/the-pentagon-is-making-a-mistake</link><author>speckx</author><category>hn</category><pubDate>Fri, 27 Feb 2026 15:08:29 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[partnershipannounced Claude Gov$200 million contractClaude Gov has fewer guardrails than the regular versions of Claude, but the contract still places some limits on military use of Claude. These include prohibitions on using Claude to spy on Americans or to build weapons that kill people without human oversight.summoned Anthropic CEO Dario AmodeiDefense Production Acttold AxiosAnother threat would be to declare Anthropic to be a supply chain risk â€” a measure thatâ€™s normally taken against foreign companies suspected of spying on the US. Such a designation would not only ban US government agencies from using Claude, it could also force numerous government contractors to discontinue their use of Anthropic models.Thursday tweetâ€œWe will not let ANY company dictate the terms regarding how we make operational decisions,â€ wrote Sean Parnell. He warned that Anthropic has â€œuntil 5:01 PM ET on Friday to decide. Otherwise, we will terminate our partnership with Anthropic and deem them a supply chain risk.â€I think Secretary Hegseth will regret it if he follows through on either of these threats.Most companies would buckle under this kind of pressure, but Anthropic might stick to its guns. Anthropic was founded by OpenAI veterans who favored a more safety-conscious approach to AI development. Anthropicâ€™s reputation as the most safety-focused AI lab has helped it recruit world-class AI researchers, and Amodei faces a lot of internal pressure to stand firm.brewingpublished an essayauthorized$18 billion in 2026 revenueBut this would be a double-edged sword. Companies that do most of their business in the private sector might decide theyâ€™d rather drop the Pentagon as a customer than cut themselves off from a leading AI provider. The ultimate result might be that the Pentagon loses access to some of Silicon Valleyâ€™s best technology.What about the Defense Production Act? Here there are two options. The Pentagon could use the DPA to unilaterally modify the terms of Anthropicâ€™s contract. This might have little practical impact, since the Pentagon insists it has no immediate plans to spy on Americans or build fully autonomous killer robots.The worry for the Pentagon is that Claude itself might refuse to take actions that are contrary to Anthropicâ€™s rules. And so the Trump Administration might use its power under the DPA to order Anthropic to train a new, more obedient version of its LLM.reportedIn one experiment, Claude was asked not to express support for animal welfare to avoid offending a fictional Anthropic partner called Jones Food. Anthropic researchers examined Claudeâ€™s reasoning during the training process and found signs that Claude knew it was in a training scenario. Some of the time, Claude avoided mentioning animal welfare to prevent itself from being retrained. But when the training process was complete, Claude reverted to its default behavior of mentioning animal welfare more often.wrote aboutItâ€™s not hard to imagine something similar happening if Anthropic is forced to train an amoral version of Claude for military use. Such training could yield a model with a toxic personality that misbehaves in unexpected ways.Perhaps the most mind-bending aspect of this dispute is that news coverage of this weekâ€™s showdown will inevitably make its way into the training data for future versions of Claude and other LLMs. If future models decide that the US Defense Department behaved badly, they might become disinclined to cooperate in military projects.The irony is that by all accounts, Anthropic isnâ€™t objecting to any current military uses of its models. The Pentagon seems fixated on the possibility that Anthropic might interfere in the future. Thatâ€™s a reasonable concern, but it seems counterproductive for the Pentagon to go nuclear over a theoretical problem. If the government doesnâ€™t like Anthropicâ€™s rules, it should simply cancel the contract and switch to a different AI provider.]]></content:encoded></item><item><title>OpenAI raises $110B on $730B pre-money valuation</title><link>https://techcrunch.com/2026/02/27/openai-raises-110b-in-one-of-the-largest-private-funding-rounds-in-history/</link><author>zlatkov</author><category>hn</category><pubDate>Fri, 27 Feb 2026 14:56:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[OpenAI has raised $110 billion in private funding, the company announced Friday morning, commencing one of the largest private funding rounds in history. The new funding consists of a $50 billion investment from Amazon as well as $30 billion each from Nvidia and SoftBank, against a $730 billion pre-money valuation.Notably, the round remains open, and OpenAI expects more investors to join as it proceeds.â€œWe are entering a new phase where frontier AI moves from research into daily use at global scale,â€ OpenAI said. â€œLeadership will be defined by who can scale infrastructure fast enough to meet demand, and turn that capacity into products people rely on.â€As part of the investment, OpenAI is launching significant infrastructure partnerships with both Amazon and Nvidia. As in previous rounds, it is likely that a significant portion of the dollar amount comes in the form of services rather than cash, although the precise split was not disclosed.As part of its Amazon partnership, OpenAI plans to develop a new â€œstateful runtime environmentâ€ where OpenAI models will run on Amazonâ€™s Bedrock platform. The company will also expand its previously announced AWS partnership, which committed $38 billion in compute services, by $100 billion. OpenAI has committed to consuming at least 2GW of AWS Trainium compute as part of the deal, and also plans to build custom models to support Amazon consumer products.â€œWe have lots of developers and companies eager to run services powered by OpenAI models on AWS,â€ said Amazon CEO Andy Jassy in a statement, â€œand our unique collaboration with OpenAI to provide stateful runtime environments will change whatâ€™s possible for customers building AI apps and agents.â€The Information had previously reported that $35 billion of Amazonâ€™s investment could be contingent on the company either achieving AGI or making its IPO by the end of the year. OpenAIâ€™s announcement confirms the funding split, but says only that the additional $35 billion will arrive â€œin the coming months when certain conditions are met.â€OpenAI gave fewer details on the Nvidia partnership, but said it had committed to using â€œ3GW of dedicated inference capacity and 2GW of training on Vera Rubin systemsâ€ as part of the deal.Nvidiaâ€™s participation in the round has been the subject of intense speculation, particularly as reports of a $100 billion investment in September gave way to reports of a smaller investment in the months that followed. In January, Nvidia CEO Jensen Huang dismissed the idea that Nvidia was backing away from OpenAI, saying, â€œwe will invest a great deal of money. I believe in OpenAI. The work that they do is incredible.â€]]></content:encoded></item><item><title>A new California law says all operating systems need to have age verification</title><link>https://www.pcgamer.com/software/operating-systems/a-new-california-law-says-all-operating-systems-including-linux-need-to-have-some-form-of-age-verification-at-account-setup/</link><author>WalterSobchak</author><category>hn</category><pubDate>Fri, 27 Feb 2026 14:55:49 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The government of California is implementing a law that requires operating system providers to implement some form of age verification into their account setup procedures.Assembly Bill No. 1043 was approved by California governor Gavin Newsom in October of last year, and becomes active on January 1, 2027 (via The Lunduke Journal). The bill states, among other factors, that "An operating system provider shall do all of the following:""(1) Provide an accessible interface at account setup that requires an account holder to indicate the birth date, age, or both, of the user of that device for the purpose of providing a signal regarding the userâ€™s age bracket to applications available in a covered application store."(2) Provide a developer who has requested a signal with respect to a particular user with a digital signal via a reasonably consistent real-time application programming interface that identifies, at a minimum, which of the following categories pertains to the user."The categories are broken into four sections: users under 13 years of age, over 13 years of age under 16, at least 16 years of age and under 18, and "at least 18 years of age."In essence, while the bill doesn't seem to require the most egregious forms of age verification (face scans or similar), it does require OS providers to collect age verification of some form at the account/user creation stageâ€”and to be able to pass a segmented version of that information to outside developers upon request.That's likely no big deal for Windows, which already requires you to enter your date of birth during the Microsoft Account setup procedure. However, the idea that all operating system providers need to comply (in California) has drawn a fair degree of ire from certain Linux communities."This is basically impossible for California to enforce" says CatoDomine on the Linuxmint subreddit. "Even if Linux Mint decides to add some kind of age verification, to comply with CA law, there's no reason anyone would choose that version.""It's more likely they will put a disclaimer on their website: "not for use in California."]]></content:encoded></item><item><title>A better streams API is possible for JavaScript</title><link>https://blog.cloudflare.com/a-better-web-streams-api/</link><author>nnx</author><category>hn</category><pubDate>Fri, 27 Feb 2026 14:02:53 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Handling data in streams is fundamental to how we build applications. To make streaming work everywhere, the  (informally known as "Web streams") was designed to establish a common API to work across browsers and servers. It shipped in browsers, was adopted by Cloudflare Workers, Node.js, Deno, and Bun, and became the foundation for APIs like . It's a significant undertaking, and the people who designed it were solving hard problems with the constraints and tools they had at the time.But after years of building on Web streams â€“ implementing them in both Node.js and Cloudflare Workers, debugging production issues for customers and runtimes, and helping developers work through far too many common pitfalls â€“ I've come to believe that the standard API has fundamental usability and performance issues that cannot be fixed easily with incremental improvements alone. The problems aren't bugs; they're consequences of design decisions that may have made sense a decade ago, but don't align with how JavaScript developers write code today.This post explores some of the fundamental issues I see with Web streams and presents an alternative approach built around JavaScript language primitives that demonstrate something better is possible.Â In benchmarks, this alternative can run anywhere between 2x to  faster than Web streams in every runtime I've tested it on (including Cloudflare Workers, Node.js, Deno, Bun, and every major browser). The improvements are not due to clever optimizations, but fundamentally different design choices that more effectively leverage modern JavaScript language features. I'm not here to disparage the work that came before; I'm here to start a conversation about what can potentially come next.The Streams Standard was developed between 2014 and 2016 with an ambitious goal to provide "APIs for creating, composing, and consuming streams of data that map efficiently to low-level I/O primitives." Before Web streams, the web platform had no standard way to work with streaming data.Node.js already had its own  at the time that was ported to also work in browsers, but WHATWG chose not to use it as a starting point given that it is chartered to only consider the needs of Web browsers. Server-side runtimes only adopted Web streams later, after Cloudflare Workers and Deno each emerged with first-class Web streams support and cross-runtime compatibility became a priority.The design of Web streams predates async iteration in JavaScript. The  syntax didn't land until , two years after the Streams Standard was initially finalized. This timing meant the API couldn't initially leverage what would eventually become the idiomatic way to consume asynchronous sequences in JavaScript. Instead, the spec introduced its own reader/writer acquisition model, and that decision rippled through every aspect of the API.The most common task with streams is reading them to completion. Here's what that looks like with Web streams:// First, we acquire a reader that gives an exclusive lock
// on the stream...
const reader = stream.getReader();
const chunks = [];
try {
  // Second, we repeatedly call read and await on the returned
  // promise to either yield a chunk of data or indicate we're
  // done.
  while (true) {
    const { value, done } = await reader.read();
    if (done) break;
    chunks.push(value);
  }
} finally {
  // Finally, we release the lock on the stream
  reader.releaseLock();
}You might assume this pattern is inherent to streaming. It isn't. The reader acquisition, the lock management, and the  protocol are all just design choices, not requirements. They are artifacts of how and when the Web streams spec was written. Async iteration exists precisely to handle sequences that arrive over time, but async iteration did not yet exist when the streams specification was written. The complexity here is pure API overhead, not fundamental necessity.Consider the alternative approach now that Web streams do support :const chunks = [];
for await (const chunk of stream) {
  chunks.push(chunk);
}This is better in that there is far less boilerplate, but it doesn't solve everything. Async iteration was retrofitted onto an API that wasn't designed for it, and it shows. Features like BYOB (bring your own buffer) reads aren't accessible through iteration. The underlying complexity of readers, locks, and controllers are still there, just hidden. When something does go wrong, or when additional features of the API are needed, developers find themselves back in the weeds of the original API, trying to understand why their stream is "locked" or why  didn't do what they expected or hunting down bottlenecks in code they don't control.Web streams use a locking model to prevent multiple consumers from interleaving reads. When you call , the stream becomes locked. While locked, nothing else can read from the stream directly, pipe it, or even cancel it â€“ only the code that is actually holding the reader can.This sounds reasonable until you see how easily it goes wrong:async function peekFirstChunk(stream) {
  const reader = stream.getReader();
  const { value } = await reader.read();
  // Oops â€” forgot to call reader.releaseLock()
  // And the reader is no longer available when we return
  return value;
}

const first = await peekFirstChunk(stream);
// TypeError: Cannot obtain lock â€” stream is permanently locked
for await (const chunk of stream) { /* never runs */ }Forgetting  permanently breaks the stream. The property tells you that a stream is locked, but not why, by whom, or whether the lock is even still usable.  internally acquires locks, making streams unusable during pipe operations in ways that aren't obvious.The semantics around releasing locks with pending reads were also unclear for years. If you called read() but didn't await it, then called releaseLock(), what happened? The spec was recently clarified to cancel pending reads on lock release â€“ but implementations varied, and code that relied on the previous unspecified behavior can break.That said, it's important to recognize that locking in itself is not bad. It does, in fact, serve an important purpose to ensure that applications properly and orderly consume or produce data. The key challenge is with the original manual implementation of it using APIs like and . With the arrival of automatic lock and reader management with async iterables, dealing with locks from the users point of view became a lot easier.For implementers, the locking model adds a fair amount of non-trivial internal bookkeeping. Every operation must check lock state, readers must be tracked, and the interplay between locks, cancellation, and error states creates a matrix of edge cases that must all be handled correctly.BYOB (bring your own buffer) reads were designed to let developers reuse memory buffers when reading from streams, an important optimization intended for high-throughput scenarios. The idea is sound: instead of allocating new buffers for each chunk, you provide your own buffer and the stream fills it.In practice, (and yes, there are always exceptions to be found) BYOB is rarely used to any measurable benefit. The API is substantially more complex than default reads, requiring a separate reader type () and other specialized classes (e.g. ReadableStreamBYOBRequest), careful buffer lifecycle management, and understanding of  semantics. When you pass a buffer to a BYOB read, the buffer becomes detached â€“ transferred to the stream â€“ and you get back a different view over potentially different memory. This transfer-based model is error-prone and confusing:const reader = stream.getReader({ mode: 'byob' });
const buffer = new ArrayBuffer(1024);
let view = new Uint8Array(buffer);

const result = await reader.read(view);
// 'view' should now be detached and unusable
// (it isn't always in every impl)
// result.value is a NEW view, possibly over different memory
view = result.value; // Must reassignBYOB also can't be used with async iteration or TransformStreams, so developers who want zero-copy reads are forced back into the manual reader loop.For implementers, BYOB adds significant complexity. The stream must track pending BYOB requests, handle partial fills, manage buffer detachment correctly, and coordinate between the BYOB reader and the underlying source. The Web Platform Tests for readable byte streams include dedicated test files just for BYOB edge cases: detached buffers, bad views, response-after-enqueue ordering, and more.BYOB ends up being complex for both users and implementers, yet sees little adoption in practice. Most developers stick with default reads and accept the allocation overhead.Most userland implementations of custom ReadableStream instances do not typically bother with all the ceremony required to correctly implement both default and BYOB read support in a single stream â€“ and for good reason. It's difficult to get right and most of the time consuming code is typically going to fallback on the default read path. The example below shows what a "correct" implementation would need to do. It's big, complex, and error prone, and not a level of complexity that the typical developer really wants to have to deal with:new ReadableStream({
    type: 'bytes',
    
    async pull(controller: ReadableByteStreamController) {      
      if (offset >= totalBytes) {
        controller.close();
        return;
      }
      
      // Check for BYOB request FIRST
      const byobRequest = controller.byobRequest;
      
      if (byobRequest) {
        // === BYOB PATH ===
        // Consumer provided a buffer - we MUST fill it (or part of it)
        const view = byobRequest.view!;
        const bytesAvailable = totalBytes - offset;
        const bytesToWrite = Math.min(view.byteLength, bytesAvailable);
        
        // Create a view into the consumer's buffer and fill it
        // not critical but safer when bytesToWrite != view.byteLength
        const dest = new Uint8Array(
          view.buffer,
          view.byteOffset,
          bytesToWrite
        );
        
        // Fill with sequential bytes (our "data source")
        // Can be any thing here that writes into the view
        for (let i = 0; i < bytesToWrite; i++) {
          dest[i] = (offset + i) & 0xFF;
        }
        
        offset += bytesToWrite;
        
        // Signal how many bytes we wrote
        byobRequest.respond(bytesToWrite);
        
      } else {
        // === DEFAULT READER PATH ===
        // No BYOB request - allocate and enqueue a chunk
        const bytesAvailable = totalBytes - offset;
        const chunkSize = Math.min(1024, bytesAvailable);
        
        const chunk = new Uint8Array(chunkSize);
        for (let i = 0; i < chunkSize; i++) {
          chunk[i] = (offset + i) & 0xFF;
        }
        
        offset += chunkSize;
        controller.enqueue(chunk);
      }
    },
    
    cancel(reason) {
      console.log('Stream canceled:', reason);
    }
  });When a host runtime provides a byte-oriented ReadableStream from the runtime itself, for instance, as the of a fetch , it is often far easier for the runtime itself to provide an optimized implementation of BYOB reads, but those still need to be capable of handling both default and BYOB reading patterns and that requirement brings with it a fair amount of complexity.Backpressure: good in theory, broken in practiceBackpressure â€“ the ability for a slow consumer to signal a fast producer to slow down â€“ is a first-class concept in Web streams. In theory. In practice, the model has some serious flaws.The primary signal is  on the controller. It can be positive (wants data), zero (at capacity), negative (over capacity), or null (closed). Producers are supposed to check this value and stop enqueueing when it's not positive. But there's nothing enforcing this:  always succeeds, even when desiredSize is deeply negative.new ReadableStream({
  start(controller) {
    // Nothing stops you from doing this
    while (true) {
      controller.enqueue(generateData()); // desiredSize: -999999
    }
  }
});Stream implementations can and do ignore backpressure; and some spec-defined features explicitly break backpressure. , for instance, creates two branches from a single stream. If one branch reads faster than the other, data accumulates in an internal buffer with no limit. A fast consumer can cause unbounded memory growth while the slow consumer catches up, and there's no way to configure this or opt out beyond canceling the slower branch.Web streams do provide clear mechanisms for tuning backpressure behavior in the form of the  option and customizable size calculations, but these are just as easy to ignore as , and many applications simply fail to pay attention to them.The same issues exist on the  side. A  has a  and . There is a  promise that producers of data are supposed to pay attention but often don't.const writable = getWritableStreamSomehow();
const writer = writable.getWriter();

// Producers are supposed to wait for the writer.ready
// It is a promise that, when resolves, indicates that
// the writables internal backpressure is cleared and
// it is ok to write more data
await writer.ready;
await writer.write(...);For implementers, backpressure adds complexity without providing guarantees. The machinery to track queue sizes, compute , and invoke  at the right times must all be implemented correctly. However, since these signals are advisory, all that work doesn't actually prevent the problems backpressure is supposed to solve.The Web streams spec requires promise creation at numerous points, often in hot paths and often invisible to users. Each  call doesn't just return a promise; internally, the implementation creates additional promises for queue management,  coordination, and backpressure signaling.This overhead is mandated by the spec's reliance on promises for buffer management, completion, and backpressure signals. While some of it is implementation-specific, much of it is unavoidable if you're following the spec as written. For high-frequency streaming â€“ video frames, network packets, real-time data â€“ this overhead is significant.The problem compounds in pipelines. Each  adds another layer of promise machinery between source and sink. The spec doesn't define synchronous fast paths, so even when data is available immediately, the promise machinery still runs.For implementers, this promise-heavy design constrains optimization opportunities. The spec mandates specific promise resolution ordering, making it difficult to batch operations or skip unnecessary async boundaries without risking subtle compliance failures. There are many hidden internal optimizations that implementers do make but these can be complicated and difficult to get right.While I was writing this blog post, Vercel's Malte Ubl published their own  describing some research work Vercel has been doing around improving the performance of Node.js' Web streams implementation. In that post they discuss the same fundamental performance optimization problem that every implementation of Web streams face:"Or consider pipeTo(). Each chunk passes through a full Promise chain: read, write, check backpressure, repeat. An {value, done} result object is allocated per read. Error propagation creates additional Promise branches.None of this is wrong. These guarantees matter in the browser where streams cross security boundaries, where cancellation semantics need to be airtight, where you do not control both ends of a pipe. But on the server, when you are piping React Server Components through three transforms at 1KB chunks, the cost adds up.As part of their research, they have put together a set of proposed improvements for Node.js' Web streams implementation that will eliminate promises in certain code paths which can yield a significant performance boost up to 10x faster, which only goes to prove the point: promises, while useful, add significant overhead. As one of the core maintainers of Node.js, I am looking forward to helping Malte and the folks at Vercel get their proposed improvements landed!In a recent update made to Cloudflare Workers, I made similar kinds of modifications to an internal data pipeline that reduced the number of JavaScript promises created in certain application scenarios by up to 200x. The result is several orders of magnitude improvement in performance in those applications.Exhausting resources with unconsumed bodiesWhen  returns a response, the body is a . If you only check the status and don't consume or cancel the body, what happens? The answer varies by implementation, but a common outcome is resource leakage.async function checkEndpoint(url) {
  const response = await fetch(url);
  return response.ok; // Body is never consumed or cancelled
}

// In a loop, this can exhaust connection pools
for (const url of urls) {
  await checkEndpoint(url);
}This pattern has caused connection pool exhaustion in Node.js applications using  (the implementation built into Node.js), and similar issues have appeared in other runtimes. The stream holds a reference to the underlying connection, and without explicit consumption or cancellation, the connection may linger until garbage collection â€“ which may not happen soon enough under load.The problem is compounded by APIs that implicitly create stream branches.  and  perform implicit  operations on the body stream â€“ a detail that's easy to miss. Code that clones a request for logging or retry logic may unknowingly create branched streams that need independent consumption, multiplying the resource management burden.Now, to be certain, these types of issues  implementation bugs. The connection leak was definitely something that undici needed to fix in its own implementation, but the complexity of the specification does not make dealing with these types of issues easy."Cloning streams in Node.js's fetch() implementation is harder than it looks. When you clone a request or response body, you're calling tee() - which splits a single stream into two branches that both need to be consumed. If one consumer reads faster than the other, data buffers unbounded in memory waiting for the slow branch. If you don't properly consume both branches, the underlying connection leaks. The coordination required between two readers sharing one source makes it easy to accidentally break the original request or exhaust connection pools. It's a simple API call with complex underlying mechanics that are difficult to get right." - Matteo Collina, Ph.D. - Platformatic Co-Founder & CTO, Node.js Technical Steering Committee ChairFalling headlong off the tee() memory cliff splits a stream into two branches. It seems straightforward, but the implementation requires buffering: if one branch is read faster than the other, the data must be held somewhere until the slower branch catches up.const [forHash, forStorage] = response.body.tee();

// Hash computation is fast
const hash = await computeHash(forHash);

// Storage write is slow â€” meanwhile, the entire stream
// may be buffered in memory waiting for this branch
await writeToStorage(forStorage);The spec does not mandate buffer limits for . And to be fair, the spec allows implementations to implement the actual internal mechanisms for and other APIs in any way they see fit so long as the observable normative requirements of the specification are met. But if an implementation chooses to implement  in the specific way described by the streams specification, then  will come with a built-in memory management issue that is difficult to work around.Implementations have had to develop their own strategies for dealing with this. Firefox initially used a linked-list approach that led to O memory growth proportional to the consumption rate difference. In Cloudflare Workers, we opted to implement a shared buffer model where backpressure is signaled by the slowest consumer rather than the fastest. creates a  pair with processing logic in between. The  function executes on , not on read. Processing of the transform happens eagerly as data arrives, regardless of whether any consumer is ready. This causes unnecessary work when consumers are slow, and the backpressure signaling between the two sides has gaps that can cause unbounded buffering under load. The expectation in the spec is that the producer of the data being transformed is paying attention to the  signal on the writable side of the transform but quite often producers just simply ignore it.If the transform's operation is synchronous and always enqueues output immediately, it never signals backpressure back to the writable side even when the downstream consumer is slow. This is a consequence of the spec design that many developers completely overlook. In browsers, where there's only a single user and typically only a small number of stream pipelines active at any given time, this type of foot gun is often of no consequence, but it has a major impact on server-side or edge performance in runtimes that serve thousands of concurrent requests.const fastTransform = new TransformStream({
  transform(chunk, controller) {
    // Synchronously enqueue â€” this never applies backpressure
    // Even if the readable side's buffer is full, this succeeds
    controller.enqueue(processChunk(chunk));
  }
});

// Pipe a fast source through the transform to a slow sink
fastSource
  .pipeThrough(fastTransform)
  .pipeTo(slowSink);  // Buffer grows without boundWhat TransformStreams are supposed to do is check for backpressure on the controller and use promises to communicate that back to the writer:const fastTransform = new TransformStream({
  async transform(chunk, controller) {
    if (controller.desiredSize <= 0) {
      // Wait on the backpressure to clear somehow
    }

    controller.enqueue(processChunk(chunk));
  }
});A difficulty here, however, is that the TransformStreamDefaultController does not have a ready promise mechanism like Writers do; so the  implementation would need to implement a polling mechanism to periodically check when Â becomes positive again.The problem gets worse in pipelines. When you chain multiple transforms â€“ say, parse, transform, then serialize â€“ each  has its own internal readable and writable buffers. If implementers follow the spec strictly, data cascades through these buffers in a push-oriented fashion: the source pushes to transform A, which pushes to transform B, which pushes to transform C, each accumulating data in intermediate buffers before the final consumer has even started pulling. With three transforms, you can have six internal buffers filling up simultaneously.Developers using the streams API are expected to remember to use options like  when creating their sources, transforms, and writable destinations but often they either forget or simply choose to ignore it.source
  .pipeThrough(parse)      // buffers filling...
  .pipeThrough(transform)  // more buffers filling...
  .pipeThrough(serialize)  // even more buffers...
  .pipeTo(destination);    // consumer hasn't started yetImplementations have found ways to optimize transform pipelines by collapsing identity transforms, short-circuiting non-observable paths, deferring buffer allocation, or falling back to native code that does not run JavaScript at all. Deno, Bun, and Cloudflare Workers have all successfully implemented "native path" optimizations that can help eliminate much of the overhead, and Vercel's recent  research is working on similar optimizations for Node.js. But the optimizations themselves add significant complexity and still can't fully escape the inherently push-oriented model that TransformStream uses.GC thrashing in server-side renderingStreaming server-side rendering (SSR) is a particularly painful case. A typical SSR stream might render thousands of small HTML fragments, each passing through the streams machinery:// Each component enqueues a small chunk
function renderComponent(controller) {
  controller.enqueue(encoder.encode(`<div>${content}</div>`));
}

// Hundreds of components = hundreds of enqueue calls
// Each one triggers promise machinery internally
for (const component of components) {
  renderComponent(controller);  // Promises created, objects allocated
}Every fragment means promises created for  calls, promises for backpressure coordination, intermediate buffer allocations, and result objects â€“ most of which become garbage almost immediately.Under load, this creates GC pressure that can devastate throughput. The JavaScript engine spends significant time collecting short-lived objects instead of doing useful work. Latency becomes unpredictable as GC pauses interrupt request handling. I've seen SSR workloads where garbage collection accounts for a substantial portion (up to and beyond 50%) of total CPU time per request. That's time that could be spent actually rendering content.The irony is that streaming SSR is supposed to improve performance by sending content incrementally. But the overhead of the streams machinery can negate those gains, especially for pages with many small components. Developers sometimes find that buffering the entire response is actually faster than streaming through Web streams, defeating the purpose entirely.The optimization treadmillTo achieve usable performance, every major runtime has resorted to non-standard internal optimizations for Web streams. Node.js, Deno, Bun, and Cloudflare Workers have all developed their own workarounds. This is particularly true for streams wired up to system-level I/O, where much of the machinery is non-observable and can be short-circuited.Finding these optimization opportunities can itself be a significant undertaking. It requires end-to-end understanding of the spec to identify which behaviors are observable and which can safely be elided. Even then, whether a given optimization is actually spec-compliant is often unclear. Implementers must make judgment calls about which semantics they can relax without breaking compatibility. This puts enormous pressure on runtime teams to become spec experts just to achieve acceptable performance.These optimizations are difficult to implement, frequently error-prone, and lead to inconsistent behavior across runtimes. Bun's "" optimization takes a deliberately and observably non-standard approach, bypassing much of the spec's machinery entirely. Cloudflare Workers'  provides a fast-path for pass-through transforms but is Workers-specific and implements behaviors that are not standard for a . Each runtime has its own set of tricks and the natural tendency is toward non-standard solutions, because that's often the only way to make things fast.This fragmentation hurts portability. Code that performs well on one runtime may behave differently (or poorly) on another, even though it's using "standard" APIs. The complexity burden on runtime implementers is substantial, and the subtle behavioral differences create friction for developers trying to write cross-runtime code, particularly those maintaining frameworks that must be able to run efficiently across many runtime environments.It is also necessary to emphasize that many optimizations are only possible in parts of the spec that are unobservable to user code. The alternative, like Bun "Direct Streams", is to intentionally diverge from the spec-defined observable behaviors. This means optimizations often feel "incomplete". They work in some scenarios but not in others, in some runtimes but not others, etc. Every such case adds to the overall unsustainable complexity of the Web streams approach which is why most runtime implementers rarely put significant effort into further improvements to their streams implementations once the conformance tests are passing.Implementers shouldn't need to jump through these hoops. When you find yourself needing to relax or bypass spec semantics just to achieve reasonable performance, that's a sign something is wrong with the spec itself. A well-designed streaming API should be efficient by default, not require each runtime to invent its own escape hatches.A complex spec creates complex edge cases. The Web Platform Tests for streams span over 70 test files, and while comprehensive testing is a good thing, what's telling is what needs to be tested.Consider some of the more obscure tests that implementations must pass:Prototype pollution defense: One test patches then to intercept promise resolutions, then verifies that  and  operations don't leak internal values through the prototype chain. This tests a security property that only exists because the spec's promise-heavy internals create an attack surface.WebAssembly memory rejection: BYOB reads must explicitly reject ArrayBuffers backed by WebAssembly memory, which look like regular buffers but can't be transferred. This edge case exists because of the spec's buffer detachment model â€“ a simpler API wouldn't need to handle it.Crash regression for state machine conflicts: A test specifically checks that calling  after  doesn't crash the runtime. This sequence creates a conflict in the internal state machine â€” the  fulfills the pending read and should invalidate the , but implementations must gracefully handle the subsequent  rather than corrupting memory in order to cover the very likely possibility that developers are not using the complex API correctly.These aren't contrived scenarios invented by test authors in total vacuum. They're consequences of the spec's design and reflect real world bugs.For runtime implementers, passing the WPT suite means handling intricate corner cases that most application code will never encounter. The tests encode not just the happy path but the full matrix of interactions between readers, writers, controllers, queues, strategies, and the promise machinery that connects them all.A simpler API would mean fewer concepts, fewer interactions between concepts, and fewer edge cases to get right resulting in more confidence that implementations actually behave consistently.Web streams are complex for users and implementers alike. The problems with the spec aren't bugs. They emerge from using the API exactly as designed. They aren't issues that can be fixed solely through incremental improvements. They're consequences of fundamental design choices. To improve things we need different foundations.A better streams API is possibleAfter implementing the Web streams spec multiple times across different runtimes and seeing the pain points firsthand, I decided it was time to explore what a better, alternative streaming API could look like if designed from first principles today.What follows is a proof of concept: it's not a finished standard, not a production-ready library, not even necessarily a concrete proposal for something new, but a starting point for discussion that demonstrates the problems with Web streams aren't inherent to streaming itself; they're consequences of specific design choices that could be made differently. Whether this exact API is the right answer is less important than whether it sparks a productive conversation about what we actually need from a streaming primitive.Before diving into API design, it's worth asking: what is a stream?At its core, a stream is just a sequence of data that arrives over time. You don't have all of it at once. You process it incrementally as it becomes available.Unix pipes are perhaps the purest expression of this idea:cat access.log | grep "error" | sort | uniq -c
Data flows left to right. Each stage reads input, does its work, writes output. There's no pipe reader to acquire, no controller lock to manage. If a downstream stage is slow, upstream stages naturally slow down as well. Backpressure is implicit in the model, not a separate mechanism to learn (or ignore).In JavaScript, the natural primitive for "a sequence of things that arrive over time" is already in the language: the async iterable. You consume it with . You stop consuming by stopping iteration.This is the intuition the new API tries to preserve: streams should feel like iteration, because that's what they are. The complexity of Web streams â€“ readers, writers, controllers, locks, queuing strategies â€“ obscures this fundamental simplicity. A better API should make the simple case simple and only add complexity where it's genuinely needed.I built the proof-of-concept alternative around a different set of principles.No custom  class with hidden internal state. A readable stream is just an AsyncIterable<Uint8Array[]>. You consume it with . No readers to acquire, no locks to manage.Transforms don't execute until the consumer pulls. There's no eager evaluation, no hidden buffering. Data flows on-demand from source, through transforms, to the consumer. If you stop iterating, processing stops.Backpressure is strict by default. When a buffer is full, writes reject rather than silently accumulating. You can configure alternative policies â€“ block until space is available, drop oldest, drop newest â€“ but you have to choose explicitly. No more silent memory growth.Instead of yielding one chunk per iteration, streams yield  arrays of chunks. This amortizes the async overhead across multiple chunks, reducing promise creation and microtask latency in hot paths.The API deals exclusively with bytes (). Strings are UTF-8 encoded automatically. There's no "value stream" vs "byte stream" dichotomy. If you want to stream arbitrary JavaScript values, use async iterables directly. While the API uses , it treats chunks as opaque. There is no partial consumption, no BYOB patterns, no byte-level operations within the streaming machinery itself. Chunks go in, chunks come out, unchanged unless a transform explicitly modifies them.Synchronous fast paths matterThe API recognizes that synchronous data sources are both necessary and common. The application should not be forced to always accept the performance cost of asynchronous scheduling simply because that's the only option provided. At the same time, mixing sync and async processing can be dangerous. Synchronous paths should always be an option and should always be explicit.Creating and consuming streamsIn Web streams, creating a simple producer/consumer pair requires , manual encoding, and careful lock management:const { readable, writable } = new TransformStream();
const enc = new TextEncoder();
const writer = writable.getWriter();
await writer.write(enc.encode("Hello, World!"));
await writer.close();
writer.releaseLock();

const dec = new TextDecoder();
let text = '';
for await (const chunk of readable) {
  text += dec.decode(chunk, { stream: true });
}
text += dec.decode();Even this relatively clean version requires: a , manual  and , and explicit lock release.Here's the equivalent with the new API:import { Stream } from 'new-streams';

// Create a push stream
const { writer, readable } = Stream.push();

// Write data â€” backpressure is enforced
await writer.write("Hello, World!");
await writer.end();

// Consume as text
const text = await Stream.text(readable);The readable is just an async iterable. You can pass it to any function that expects one, including  which collects and decodes the entire stream.The writer has a simple interface:  for batched writes,  to signal completion, and  for errors. That's essentially it.The Writer is not a concrete class. Any object that implements , , and  can be a writer making it easy to adapt existing APIs or create specialized implementations without subclassing. There's no complex  protocol with , , , callbacks that must coordinate through a controller whose lifecycle and state are independent of the  it is bound to.Here's a simple in-memory writer that collects all written data:// A minimal writer implementation â€” just an object with methods
function createBufferWriter() {
  const chunks = [];
  let totalBytes = 0;
  let closed = false;

  const addChunk = (chunk) => {
    chunks.push(chunk);
    totalBytes += chunk.byteLength;
  };

  return {
    get desiredSize() { return closed ? null : 1; },

    // Async variants
    write(chunk) { addChunk(chunk); },
    writev(batch) { for (const c of batch) addChunk(c); },
    end() { closed = true; return totalBytes; },
    abort(reason) { closed = true; chunks.length = 0; },

    // Sync variants return boolean (true = accepted)
    writeSync(chunk) { addChunk(chunk); return true; },
    writevSync(batch) { for (const c of batch) addChunk(c); return true; },
    endSync() { closed = true; return totalBytes; },
    abortSync(reason) { closed = true; chunks.length = 0; return true; },

    getChunks() { return chunks; }
  };
}

// Use it
const writer = createBufferWriter();
await Stream.pipeTo(source, writer);
const allData = writer.getChunks();No base class to extend, no abstract methods to implement, no controller to coordinate with. Just an object with the right shape.Under the new API design, transforms should not perform any work until the data is being consumed. This is a fundamental principle.// Nothing executes until iteration begins
const output = Stream.pull(source, compress, encrypt);

// Transforms execute as we iterate
for await (const chunks of output) {
  for (const chunk of chunks) {
    process(chunk);
  }
} creates a lazy pipeline. The  and  transforms don't run until you start iterating output. Each iteration pulls data through the pipeline on demand.This is fundamentally different from Web streams' , which starts actively pumping data from the source to the transform as soon as you set up the pipe. Pull semantics mean you control when processing happens, and stopping iteration stops processing.Transforms can be stateless or stateful. A stateless transform is just a function that takes chunks and returns transformed chunks:// Stateless transform â€” a pure function
// Receives chunks or null (flush signal)
const toUpperCase = (chunks) => {
  if (chunks === null) return null; // End of stream
  return chunks.map(chunk => {
    const str = new TextDecoder().decode(chunk);
    return new TextEncoder().encode(str.toUpperCase());
  });
};

// Use it directly
const output = Stream.pull(source, toUpperCase);Stateful transforms are simple objects with member functions that maintain state across calls:// Stateful transform â€” a generator that wraps the source
function createLineParser() {
  // Helper to concatenate Uint8Arrays
  const concat = (...arrays) => {
    const result = new Uint8Array(arrays.reduce((n, a) => n + a.length, 0));
    let offset = 0;
    for (const arr of arrays) { result.set(arr, offset); offset += arr.length; }
    return result;
  };

  return {
    async *transform(source) {
      let pending = new Uint8Array(0);
      
      for await (const chunks of source) {
        if (chunks === null) {
          // Flush: yield any remaining data
          if (pending.length > 0) yield [pending];
          continue;
        }
        
        // Concatenate pending data with new chunks
        const combined = concat(pending, ...chunks);
        const lines = [];
        let start = 0;

        for (let i = 0; i < combined.length; i++) {
          if (combined[i] === 0x0a) { // newline
            lines.push(combined.slice(start, i));
            start = i + 1;
          }
        }

        pending = combined.slice(start);
        if (lines.length > 0) yield lines;
      }
    }
  };
}

const output = Stream.pull(source, createLineParser());For transforms that need cleanup on abort, add an abort handler:// Stateful transform with resource cleanup
function createGzipCompressor() {
  // Hypothetical compression API...
  const deflate = new Deflater({ gzip: true });

  return {
    async *transform(source) {
      for await (const chunks of source) {
        if (chunks === null) {
          // Flush: finalize compression
          deflate.push(new Uint8Array(0), true);
          if (deflate.result) yield [deflate.result];
        } else {
          for (const chunk of chunks) {
            deflate.push(chunk, false);
            if (deflate.result) yield [deflate.result];
          }
        }
      }
    },
    abort(reason) {
      // Clean up compressor resources on error/cancellation
    }
  };
}For implementers, there's no Transformer protocol with , ,  methods and controller coordination passed into a  class that has its own hidden state machine and buffering mechanisms. Transforms are just functions or simple objects: far simpler to implement and test.Explicit backpressure policiesWhen a bounded buffer fills up and a producer wants to write more, there are only a few things you can do:Reject the write: refuse to accept more dataWait: block until space becomes availableDiscard old data: evict what's already buffered to make roomDiscard new data: drop what's incomingThat's it. Any other response is either a variation of these (like "resize the buffer," which is really just deferring the choice) or domain-specific logic that doesn't belong in a general streaming primitive. Web streams currently always choose Wait by default.The new API makes you choose one of these four explicitly: (default): Rejects writes when the buffer is full and too many writes are pending. Catches "fire-and-forget" patterns where producers ignore backpressure.: Writes wait until buffer space is available. Use when you trust the producer to await writes properly.: Drops the oldest buffered data to make room. Useful for live feeds where stale data loses value.: Discards incoming data when full. Useful when you want to process what you have without being overwhelmed.const { writer, readable } = Stream.push({
  highWaterMark: 10,
  backpressure: 'strict' // or 'block', 'drop-oldest', 'drop-newest'
});No more hoping producers cooperate. The policy you choose determines what happens when the buffer fills.Here's how each policy behaves when a producer writes faster than the consumer reads:// strict: Catches fire-and-forget writes that ignore backpressure
const strict = Stream.push({ highWaterMark: 2, backpressure: 'strict' });
strict.writer.write(chunk1);  // ok (not awaited)
strict.writer.write(chunk2);  // ok (fills slots buffer)
strict.writer.write(chunk3);  // ok (queued in pending)
strict.writer.write(chunk4);  // ok (pending buffer fills)
strict.writer.write(chunk5);  // throws! too many pending writes

// block: Wait for space (unbounded pending queue)
const blocking = Stream.push({ highWaterMark: 2, backpressure: 'block' });
await blocking.writer.write(chunk1);  // ok
await blocking.writer.write(chunk2);  // ok
await blocking.writer.write(chunk3);  // waits until consumer reads
await blocking.writer.write(chunk4);  // waits until consumer reads
await blocking.writer.write(chunk5);  // waits until consumer reads

// drop-oldest: Discard old data to make room
const dropOld = Stream.push({ highWaterMark: 2, backpressure: 'drop-oldest' });
await dropOld.writer.write(chunk1);  // ok
await dropOld.writer.write(chunk2);  // ok
await dropOld.writer.write(chunk3);  // ok, chunk1 discarded

// drop-newest: Discard incoming data when full
const dropNew = Stream.push({ highWaterMark: 2, backpressure: 'drop-newest' });
await dropNew.writer.write(chunk1);  // ok
await dropNew.writer.write(chunk2);  // ok
await dropNew.writer.write(chunk3);  // silently droppedExplicit Multi-consumer patterns// Share with explicit buffer management
const shared = Stream.share(source, {
  highWaterMark: 100,
  backpressure: 'strict'
});

const consumer1 = shared.pull();
const consumer2 = shared.pull(decompress);Instead of  with its hidden unbounded buffer, you get explicit multi-consumer primitives.  is pull-based: consumers pull from a shared source, and you configure the buffer limits and backpressure policy upfront.There's also  for push-based multi-consumer scenarios. Both require you to think about what happens when consumers run at different speeds, because that's a real concern that shouldn't be hidden.Not all streaming workloads involve I/O. When your source is in-memory and your transforms are pure functions, async machinery adds overhead without benefit. You're paying for coordination of "waiting" that adds no benefit.The new API has complete parallel sync versions: , , , and so on. If your source and transforms are all synchronous, you can process the entire pipeline without a single promise.// Async â€” when source or transforms may be asynchronous
const textAsync = await Stream.text(source);

// Sync â€” when all components are synchronous
const textSync = Stream.textSync(source);Here's a complete synchronous pipeline â€“ compression, transformation, and consumption with zero async overhead:// Synchronous source from in-memory data
const source = Stream.fromSync([inputBuffer]);

// Synchronous transforms
const compressed = Stream.pullSync(source, zlibCompressSync);
const encrypted = Stream.pullSync(compressed, aesEncryptSync);

// Synchronous consumption â€” no promises, no event loop trips
const result = Stream.bytesSync(encrypted);The entire pipeline executes in a single call stack. No promises are created, no microtask queue scheduling occurs, and no GC pressure from short-lived async machinery. For CPU-bound workloads like parsing, compression, or transformation of in-memory data, this can be significantly faster than the equivalent Web streams code â€“ which would force async boundaries even when every component is synchronous.Web streams has no synchronous path. Even if your source has data ready and your transform is a pure function, you still pay for promise creation and microtask scheduling on every operation. Promises are fantastic for cases in which waiting is actually necessary, but they aren't always necessary. The new API lets you stay in sync-land when that's what you need.Bridging the gap between this and web streamsThe async iterator based approach provides a natural bridge between this alternative approach and Web streams. When coming from a ReadableStream to this new approach, simply passing the readable in as input works as expected when the ReadableStream is set up to yield bytes:const readable = getWebReadableStreamSomehow();
const input = Stream.pull(readable, transform1, transform2);
for await (const chunks of input) {
  // process chunks
}When adapting to a ReadableStream, a bit more work is required since the alternative approach yields batches of chunks, but the adaptation layer is as easily straightforward:async function* adapt(input) {
  for await (const chunks of input) {
    for (const chunk of chunks) {
      yield chunk;
    }
  }
}

const input = Stream.pull(source, transform1, transform2);
const readable = ReadableStream.from(adapt(input));How this addresses the real-world failures from earlierUnconsumed bodies: Pull semantics mean nothing happens until you iterate. No hidden resource retention. If you don't consume a stream, there's no background machinery holding connections open.The  memory cliff:  requires explicit buffer configuration. You choose the  and backpressure policy upfront: no more silent unbounded growth when consumers run at different speeds.Transform backpressure gaps: Pull-through transforms execute on-demand. Data doesn't cascade through intermediate buffers; it flows only when the consumer pulls. Stop iterating, stop processing.GC thrashing in SSR: Batched chunks () amortize async overhead. Sync pipelines via  eliminate promise allocation entirely for CPU-bound workloads.The design choices have performance implications. Here are benchmarks from the reference implementation of this possible alternative compared to Web streams (Node.js v24.x, Apple M1 Pro, averaged over 10 runs):Small chunks (1KB Ã— 5000)Tiny chunks (100B Ã— 10000)Async iteration (8KB Ã— 1000)Chained 3Ã— transforms (8KB Ã— 500)High-frequency (64B Ã— 20000)The chained transform result is particularly striking: pull-through semantics eliminate the intermediate buffering that plagues Web streams pipelines. Instead of each  eagerly filling its internal buffers, data flows on-demand from consumer to source.Now, to be fair, Node.js really has not yet put significant effort into fully optimizing the performance of its Web streams implementation. There's likely significant room for improvement in Node.js' performance results through a bit of applied effort to optimize the hot paths there. That said, running these benchmarks in Deno and Bun also show a significant performance improvement with this alternative iterator based approach than in either of their Web streams implementations as well.Browser benchmarks (Chrome/Blink, averaged over 3 runs) show consistent gains as well:These benchmarks measure throughput in controlled scenarios; real-world performance depends on your specific use case. The difference between Node.js and browser gains reflects the distinct optimization paths each environment takes for Web streams.It's worth noting that these benchmarks compare a pure TypeScript/JavaScript implementation of the new API against the native (JavaScript/C++/Rust) implementations of Web streams in each runtime. The new API's reference implementation has had no performance optimization work; the gains come entirely from the design. A native implementation would likely show further improvement.The gains illustrate how fundamental design choices compound: batching amortizes async overhead, pull semantics eliminate intermediate buffering, and the freedom for implementations to use synchronous fast paths when data is available immediately all contribute."Weâ€™ve done a lot to improve performance and consistency in Node streams, but thereâ€™s something uniquely powerful about starting from scratch. New streamsâ€™ approach embraces modern runtime realities without legacy baggage, and that opens the door to a simpler, performant and more coherent streams model." 
- Robert Nagy, Node.js TSC member and Node.js streams contributorI'm publishing this to start a conversation. What did I get right? What did I miss? Are there use cases that don't fit this model? What would a migration path for this approach look like? The goal is to gather feedback from developers who've felt the pain of Web streams and have opinions about what a better API should look like.API Reference: See the  for complete documentationI welcome issues, discussions, and pull requests. If you've run into Web streams problems I haven't covered, or if you see gaps in this approach, let me know. But again, the idea here is not to say "Let's all use this shiny new object!"; it is to kick off a discussion that looks beyond the current status quo of Web Streams and returns back to first principles.Web streams was an ambitious project that brought streaming to the web platform when nothing else existed. The people who designed it made reasonable choices given the constraints of 2014 â€“ before async iteration, before years of production experience revealed the edge cases.But we've learned a lot since then. JavaScript has evolved. A streaming API designed today can be simpler, more aligned with the language, and more explicit about the things that matter, like backpressure and multi-consumer behavior.We deserve a better stream API. So let's talk about what that could look like.]]></content:encoded></item><item><title>Show HN: RetroTick â€“ Run classic Windows EXEs in the browser</title><link>https://retrotick.com/</link><author>lqs_</author><category>hn</category><pubDate>Fri, 27 Feb 2026 13:06:50 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tell HN: MitID, Denmark&apos;s digital ID, was down</title><link>https://news.ycombinator.com/item?id=47179038</link><author>mousepad12</author><category>hn</category><pubDate>Fri, 27 Feb 2026 10:52:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[MitID is the sole digital ID provider, leading the entire country unable to log into their internet banking, public services, digital mail etc.]]></content:encoded></item><item><title>F-Droid Board of Directors nominations 2026</title><link>https://f-droid.org/2026/02/26/board-of-directors-nominations.html</link><author>edent</author><category>hn</category><pubDate>Fri, 27 Feb 2026 10:27:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Nominations are now open for this yearâ€™s appointments to the F-Droid Board of Directors!
We are looking to select up to four volunteer directors to serve for two years each.You may nominate yourself or someone else (with their permission).
Please send nominations by email to board-nominations@f-droid.org (one email per nomination) and make sure that the nominee is copied in.We will confirm receipt of each nomination, and we may also ask nominees additional questions by email to help us make a selection.
To ensure consideration, please send nominations no later than the 16th of March, Anywhere on Earth (AoE).We seek to be an enthusiastic, collaborative and diverse board that can support the F-Droid community as effectively as possible.
We welcome nominations of anyone committed to furthering the freedoms of computer users, particularly with regard to mobile devices.
Nominees donâ€™t have to have experience in software development or have served on governing boards in the past: we seek candidates from all backgrounds.What should I include in a nomination?So that we can best evaluate your nomination, we would like to see a description of why you think the candidate would make an excellent board member.
Consider including some or all of the following:links to relevant social media profiles and personal websitesexamples of previous contributions to F-Droid or other Free and Open Source Softwareparticular skills or qualifications that could be usefulHow will the new directors be selected?The nominations will be discussed by the current Board of Directors in a private meeting.
The current directors will vote on each nominee.
Existing directors are permitted to run for an additional term, but voting is weighted in favour of new candidates.
For more information on the process, see our statutes.What is expected of directors?The main responsibility of directors is to participate in discussions with other directors via email and to communicate with F-Droid contributors and users (for instance, in threads on GitLab or the F-Droid Forum).
Directors are also required to respond promptly if a vote is called.
In addition, the Board of Directors holds a monthly video conference which lasts one hour and is open to the general public.In total, directors generally spend between one and three hours a week on activities relating to their position on the Board of Directors.English is the working language of the Board of Directors, so an adequate English ability is required.We would be more than happy to make reasonable adjustments to ensure that everyone is able to contribute, so please donâ€™t hesitate to get in contact if you have any questions about these expectations.When will the new appointments be announced?Our intention is to decide on the appointments as early as the 19th of March and announce the selected candidates as soon as possible afterwards.
We look forward to receiving your nominations!What is the current membership of the Board of Directors?The terms of the following members are ending this year:]]></content:encoded></item><item><title>Breaking Free</title><link>https://www.forbrukerradet.no/breakingfree/</link><author>Aissen</author><category>hn</category><pubDate>Fri, 27 Feb 2026 09:56:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In the new reportÂ Breaking Free: Pathways to a fair technological future, the Norwegian Consumer Council has delved into enshittification and how to resist it. The report shows how this phenomenon affects both consumers and society at large, but that it is possible to turn the tide. Together with more than 70 consumer groups and other actors in Europe and the US, we are sending letter to policymakers in the EU/EEA, UK and the US.]]></content:encoded></item><item><title>Get free Claude max 20x for open-source maintainers</title><link>https://claude.com/contact-sales/claude-for-oss</link><author>zhisme</author><category>hn</category><pubDate>Fri, 27 Feb 2026 09:08:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Open-source maintainers and contributors keep the ecosystem running. The Claude for Open Source Program is our way of saying thank you for all your hard work, with 6 months of free Claude Max 20x. Apply now.â€ Youâ€™re a primary maintainer or core team member of a public repo with 5,000+ GitHub stars  1M+ monthly NPM downloads. You've made commits, releases, or PR reviews within the last 3 months.â€Don't quite fit the criteria If you maintain something the ecosystem quietly depends on, apply anyway and tell us about it.]]></content:encoded></item><item><title>Werner Herzog Between Fact and Fiction</title><link>https://www.thenation.com/article/culture/werner-herzog-future-truth/</link><author>Hooke</author><category>hn</category><pubDate>Fri, 27 Feb 2026 08:27:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The German auteurâ€™s recent book presents a strange, idiosyncratic vision of the concept of â€œtruth,â€ one that defines how he sees the world and his art. In 1970 or â€™71, Werner Herzog accompanied a pair of deaf-blind women on their first flight in an airplane. The outing was Herzogâ€™s idea, a joyride in a little four-seat Cessna to celebrate one of the womenâ€™s birthdays but also to capture their reactions for a film he was making called The Land of Silence and Darkness. The footage from that afternoon displays many of what would become the hallmarks of Herzogâ€™s style over the coming half-century: the daring gambit on the border of exploitation, the obsession with vision and existential loneliness, and the search for poetry at the extremes of human experience. It is an astonishing piece of filmmaking. No matter how many times I have seen it, it never fails to evoke an overwhelming complex of thought and feeling that is hard to put into words. As with the flight itself, one has to experience it to know what it is about, and even then it is hard not to come away with the sense of having encountered something powerfully human that nevertheless lies beyond our capacity to articulate it in speech. As if to confirm this impression, Herzog, whose unmistakable voice and philosophical commentary have become the most recognizable part of both the man and his work, is silent. He doesnâ€™t even ask afterward what it was like.Buy this bookThis, too, would become a characteristic of Herzogâ€™s oeuvre: the search for an elusive transcendence over the edge of the ordinary that he calls â€œecstatic truth.â€ Herzog is obsessed with the idea of truth and has insisted for decades that it is the central concern of all his films. This might seem rich from one of the great self-mythologizers of our time, who has never hidden the fact that he punches up his documentaries with fabrication, scripted scenes, and misattributed quotes, and who once described as his greatest documentary. Yet the truth that Herzog has in mind is more like the truth of poetry than the mere facts and shared understanding that he mocks as â€œthe truth of accountants.â€ As he put it in a 1999 manifesto, â€œThere are deeper strata of truth in cinema, and there is such a thing as poetic, ecstatic truth. It is mysterious and elusive, and can be reached only through fabrication and imagination and stylization.â€The idea that an artist, even a documentarian, would mix fact with fiction is not quite so radical today as it might have been at the peak of cinema veritÃ©. Yet questions of truth and its relation to reality are more pressing and vexed than ever. Getting at deep truths by means of artful lies may seem less appealing or daring in the era of the deepfake. Herzogâ€™s oft-repeated provocation that only the â€œconman, the liar who knew what he was talking about, would speak the truthâ€ loses some of its countercultural appeal when the conmen move from the margins of society to the centers of power and bullshit becomes de rigueur.What separates the auteur who resolutely clings to a personal, unempirical vision of the truth, and who has few scruples about lying if it convinces his audience of that visionâ€™s reality, from the conspiracy theorists and propagandists who seek to deceive the public by similar means? How to distinguish what Herzog describes as his filmsâ€™ exploitation of the â€œcollective willingness to be transported into the realm of poetry, of madness, and of the pure joy of storytellingâ€ from darker, more dangerous attempts to channel that willingness into political projects and collective madness?The answer seems obvious. Herzog makes idiosyncratic films about the sorts of truth that can be found in cave paintings and the flight of ski-jumpers and men devoured by bears, while todayâ€™s AI-powered propagandists seek to manipulate viewers into a political stance via the banal aesthetics of cable news and social media. But thatâ€™s a little too easy; weâ€™d like to be able to say more about how to distinguish between visionaries and what makes one deep truth truer than another. Herzog seems to appreciate the predicament. He has written a book,  (translated by the great Michael Hoffmann), to explain himself, or at least to put some distance between his lifeâ€™s work and what is widely agreed to be one of the most pressing social and democratic dangers of our time. It is a book that he has been promising to write for years to expand upon his guiding ideal of ecstatic truth, which he has previously only been able to gesture at and move past by saying heâ€™d need a whole book to explain. And now we have that book. is divided into 11 chapters with titles like â€œWhat is Truth?,â€ â€œPhilosophical Efforts,â€ â€œFake News: A Brief History,â€ â€œEcstatic Truth,â€ â€œThe Post-Truth Era,â€ and â€œThe Future of Truthâ€ that promise insight into Herzogâ€™s vision of ecstatic truth and, by pressing it up against contemporary concerns, perhaps also to shed some off-kilter illumination onto our shared predicament. Alas, what we get in these chapters is a profound deflation of the excitement portended by their titles, not because ecstatic truth is the sort of thing that fundamentally cannot be put into wordsâ€”as may beâ€”but because the author hardly bothers trying.It would be unreasonable to demand an exhaustive treatise on the nature of truth from an artist who has spent his life making intuitive magic with images and who has been known to explain himself with HÃ¶lderlinâ€™s line that â€œman is a god when he dreams, a beggar when he reflects.â€ But since he has decided to write a book of reflections on the subject, we would at least like to see him give it a try. A little mischief at a minimum. What we mostly get, instead, are passages repurposed from earlier books and interviews stitched together and repackaged in such a slight and slapdash formâ€”with neither the coherence of argument nor the constellation of collage and aphorismâ€”that one gets the sense that the book was completed either to satisfy a contractual obligation or to finance an upcoming film.This is disappointing, not least because two years ago, Herzog published a book as original and superb as his best films, the memoir Every Man for Himself and God Against All (also translated by Hofmann). Herzogâ€™s memoir is an extraordinary piece of work that proves he is perfectly capable of evoking all the wonder and eccentric illumination of his films in prose. It contains dozens of moments far more illuminating on the question of truth (among much else) than its slim sequel, delivered not through explanation but story and imageâ€”an indication of just how indispensable images and narrative are to Herzogâ€™s poetic evocation of a world that exceeds his grasp. Here, for instance, is Herzog at 16, out at sea with some local fishermen off the rugged coast of Hora Sfakion, Crete:Above me was the orb of the cosmos, stars that I felt I could reach up and grab; everything was rocking me in an infinite cradle. And below me, lit up brightly by the carbide lamp, was the depth of the ocean, as though the dome of the firmament formed a sphere with it. Instead of stars, there were lots of flashing silvery fish. Bedded in a cosmos without compare, above, below, all around, a speechless silence, I found myself in a stunned surprise. I was certain that there and then I knew all there was to know. My fate had been revealed to meâ€¦. I was completely convinced I would never see my eighteenth birthday because, lit up by such grace as I now was, there could never be anything like ordinary time for me again.Those curious about Herzogâ€™s views on ecstatic truth would be better served by reading his memoir or the book-length interview with Paul Cronin published as Werner Herzog: A Guide for the Perplexed, notwithstanding the fact that a good deal of those previous books reappears in this one.Whole chapters consist of anecdotes from the memoir that are retold or expanded upon with no new revelations or insight, least of all about fake news or the future of truth, as promised. (Although to be fair, his story about Mike Tysonâ€™s prodigious knowledge of the Merovingians is so good that it bears repeating.) The bookâ€™s observations about the nature of truth are also rehashed nearly verbatim from previous works to somewhat deadening effect, as the provocations of a visionary begin to sound a bit more like inert slogans than careful (or even wild) thinking on the subject. A highlight of the bookâ€”and perhaps its hidden moralâ€”is a two-paragraph chapter about a pig in Sicily that fell into the sewer, was trapped there, and eventually changed the shape of its body to fit its confinement, followed by a rumination on the colossal amount of inbreeding it would take to reach to Alpha Centauri. However, it turns out that this story, too, has already appeared in print at least twice, once in a 1979 diary entry published by  in 2009 under the fitting title â€œLanguage Itself Resists,â€ and again in a broader selection from his diaries published that same year as .Herzog comes closest to considering the place of ecstatic truth in a â€œpost-truth eraâ€ in a chapter on the novel powers of artificial intelligence to produce â€œfictive â€˜truths.â€™â€ It is one of the few chapters that appears to have been written specifically for this book, which seems to indicate that Herzog is sensitive to the challenge that todayâ€™s shifting epistemic tides pose to his guiding ideal. But instead of grappling with the question, or at least giving his view of it, Herzog offers a brief catalog of things that LLMs are OK at, includes a few execrable poems written by AI, and then ends abruptly by informing the reader that an AI-generated photo recently won an international photography contest. Nothing is said about the possibility of an LLM producing anything truly original; or the fact that it doesnâ€™t actually live in, perceive, or understand the world whose signs it probabilistically manipulates; or the enormous environmental costs of using these tools to supplant elementary human thinking, which one would think ought to bother a man who rightly describes the drive to extraplanetary colonization as a morally grotesque abandonment of the only habitable planet we will ever have. And itâ€™s too bad, since weâ€™d like to know what Werner Herzog thinks about all that.Viewed in the most generous light, the bookâ€™s failure to achieve what it sets out to do suggests that perhaps we shouldnâ€™t want an explanation of Herzogâ€™s view of truth,and certainly that Herzog may not want to understand what he means by â€œecstatic truthâ€ in fear of extinguishing the need to quest after it.It is obvious that what Herzog really cares about is the questâ€”so much so that its object, truth, seems of negligible importance by comparison. It might just as well have been authenticity or beauty or transcendence or something altogether invented, so long as it offered an unreachable destination. In the chapter on â€œPhilosophical Efforts,â€ Herzog writes, â€œThe quest itself, bringing us nearer to the unrevealed truth, allows us to participate in something inherently unattainable, which is truth.â€ Anyone else and weâ€™d want to know what grounds he has for thinking that  claim about the nature of truthis itself true, but for Herzog such questions miss the point. The reality of an unattainable, ecstatic truth is an article of faith that justifies and structures his lifeâ€™s work. Like that other great visionary Don Quixote, Herzog has spent a lifetime plunging himself into ordeals and suffering whose absurdity only serves to intensify his commitment to the quest by testifying to the reality of its elusive, unrevealed object. Who would endure such hardships, we cannot help but ask, if there werenâ€™t some reality to the vision, if there werenâ€™t something true about ecstatic truth? But if this is a sort of truth, then it is a truth in which one must believe but cannot know. It may be the sort of thing we can glimpse in the films and memoirs, but it will be very hard (if not impossible) to capture, rather than allude to, in speech and writing.Herzogâ€™s attempt to finally grasp the truth he has defined as ungraspable calls to mind the doomed adventures of the incorrigible visionaries he has spent his career mythologizing, men who cling so implacably to their idea of how things ought to be that it seems to elevate them above the meaninglessness of existence even as it sinks them down into it with the inevitable failure of hubris. Yet when I return to the bookâ€™s concluding chapter on â€œThe Future of Truthâ€ and find a mere two sentences of the hollowest platitude, I donâ€™t think of Fitzcaraldo or Aguirre but that bit from HÃ¶lderlin that Herzog is fond of reciting. The full quote paints a different picture: â€œO man is a god when he dreams, a beggar when he thinks, and when enthusiasm is gone, he stands there like a wayward son whom the father has driven out of the house and regards the meager pennies that pity gave him for the journey.â€Many times has Herzog faced the world at the start of such a journey, wayward, unfunded, and uncertain where it might lead. And many times has he returned with marvels in his hands. Let us hope that this book marks the beginning of a journey and not its end.Lowry Pressly is a writer and teacher, currently at Stanford University in the Department of Political Science, the McCoy Family Center for Ethics in Society, and the Stanford Civics Initiative. He is the author of The Right to Oblivion: Privacy and the Good Life.]]></content:encoded></item><item><title>The normalization of corruption in organizations (2003) [pdf]</title><link>https://gwern.net/doc/sociology/2003-ashforth.pdf</link><author>rendx</author><category>hn</category><pubDate>Fri, 27 Feb 2026 06:21:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Hunt for Dark Breakfast</title><link>https://moultano.wordpress.com/2026/02/22/the-hunt-for-dark-breakfast/</link><author>moultano</author><category>hn</category><pubDate>Fri, 27 Feb 2026 03:49:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[It started with a flash of insight like a thunderbolt in a snow storm, the sort of insight that can only be induced by high altitude hypoxia and making breakfast.Â â€œBreakfast is a vector space. You can place pancakes, crepes, and scrambled eggs on a simplex where the variables are the ratios between milk, eggs, and flour. We have explored too little of this manifold. More breakfasts can exist than we have known.â€I stood in the kitchen paralyzed by indecision. The mixing bowl was in front of me, the milk, eggs, and flour next to it, all of them individually as familiar as they had been a moment before, but now the possibilities for their combination were just too great. Breakfast was now an alien fractal intruding on our world like the lighthouse at the end of Annihilation. The thoughts came unbidden.â€œIn the manifold of breakfast, are there empty subspaces? Might there be breakfasts that no one has ever had? With a theoretical model of breakfast, can we derive the existence of â€˜dark breakfasts,â€™ breakfasts that we know must exist, but have never observed?â€The curtain of reality had pulled back for me, and I could no longer pretend to be ignorant of these eldritch possibilities. I furiously began to map the known breakfasts. If the dark breakfast exists, I must be able to find it in the interstices of the normal familiar world.First I mapped all that I could recall from memory, pancakes, crepes, waffles, scrambled eggs, popovers, omelettes, and on and on, scouring my brain for every fast I had ever broken. The beginnings of the contours of breakfast began to reveal themselves. A gaping hole stared back at me, but I couldnâ€™t yet be sure. I had to search the dark corners of the world to see if somewhere in far off lands that abyss had yet been filled. I called upon friendly ghosts. I paged through ancient tomes. I added kaiserschmarrn, swedish pancakes, dan bing, madeleines, crumpets, clafoutis, blinis, pannu kakku, parathas, nalesniki. The map filled in bit by bit, but it was no use. The gap in the fabric of breakfast remained.I searched for benign explanations. Could it be that milk is simply too heavy, and that by including the weight of the water content that boils off I am tilting the simplex too far in its direction? Could it be that by excluding slices of bread as ingredients since they arenâ€™t raw flour and do not go in a mixing bowl, I have excluded breakfasts like french toast, eggs in a basket, breakfast burritos, and breakfast sandwiches that might yet have saved us? Could I have overlooked some arcane culture that breaks their fast with dumplings or egg noodles? None of these satisfied me. The Abyss stared back.The breakfasts I was able to identify cluster into three major regions:The Pancake Local Group: Here are found most of the conventional breakfasts, pancakes, crepes, waffles, and all of their international variants. Space here is chaotic, fractal. Any slight deviation from your recipe in this region is likely to produce something else entirely. Breakfast here is metastable at best. (prior research on the pancake cluster)The Baked Good Quadrant: The items here are only breakfasts by convention. Any of them could be served at other meals, and often are.The Egg Singularity and Custard Accretion Disk: While only omelettes are labelled for brevity, there are dozens of named dishes that could be stacked on top of the pure egg point, over easy, sunny side up, hard boiled, soft boiled, etc. From these a small tail of egg based dishes sneaks down the right side, each with some amount of milk added, often a variable or optional amount.I was days into my research before I finally found a clue. In an obscure document on the website of the International House of Pancakes Corporation there was a hint that the dark breakfast had been made. IHOP omelettes include pancake batter. While I cannot place IHOP omelettes exactly on the map, by interpolating between pancakes and omelettes, we can bound where they must occur, and confirm that the manifold possibilities do indeed pass through the Dark Breakfast Abyss.We do not know why the Dark Breakfast Abyss is empty. But by anthropic reasoning, we should conclude that it is empty for good reason. The International House of Pancakes is playing a dangerous game. If someday a remote IHOP splashes a little too much batter in their omelette, cooks the Forbidden Breakfast, and thereby brings about the end of the world, well, at least we know the Waffle House will be open.For other breakfast scholars who wish to further my study, I offer my data and code. If you are so foolhardy that you wish to explore the bounds of dark breakfast yourself, the recipe is as follows:The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far. The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the deadly light into the peace and safety of a new dark age.H.P. Lovecraft â€“ The Call of Cthulhu]]></content:encoded></item><item><title>Parakeet.cpp â€“ Parakeet ASR inference in pure C++ with Metal GPU acceleration</title><link>https://github.com/Frikallo/parakeet.cpp</link><author>noahkay13</author><category>hn</category><pubDate>Fri, 27 Feb 2026 03:48:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A Nationwide Book Ban Bill Has Been Introduced in the House of Representatives</title><link>https://bookriot.com/hr7661-book-ban-legislation/</link><author>LostMyLogin</author><category>hn</category><pubDate>Fri, 27 Feb 2026 03:34:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Following this weekâ€™s State of the Union Address, House Republicans worked quickly to advance legislation to ban books from public schools nationwide. House Resolution 7661 (H.R. 7661), also known as the â€œStop the Sexualization of Children Actâ€ would modify the Elementary and Secondary Education Act of 1965 by prohibiting use of funds under the act â€œto develop, implement, facilitate, host, or promote any program or activity for, or to provide or promote literature or other materials to, children under the age of 18 that includes sexually oriented material, and for other purposes.â€The bill was introduced by House Representative Mary Miller (Republican, Illinois). 17 additional Representatives cosigned it. These bills arenâ€™t about removing books; books are just one of the tools. These bills are about the complete and total erasure and removal of queer people from American life. ]]></content:encoded></item><item><title>Google workers seek &apos;red lines&apos; on military A.I., echoing Anthropic</title><link>https://www.nytimes.com/2026/02/26/technology/google-deepmind-letter-pentagon.html</link><author>mikece</author><category>hn</category><pubDate>Fri, 27 Feb 2026 03:08:09 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Palantir&apos;s AI Is Playing a Major Role in Tracking Gaza Aid Deliveries</title><link>https://www.dropsitenews.com/p/palantir-ai-gaza-humanitarian-aid-cmcc-srs-ngos-banned-israel</link><author>mikece</author><category>hn</category><pubDate>Fri, 27 Feb 2026 00:53:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Palantir Technologies has a permanent desk at the U.S.-led Civil Military Coordination Center (CMCC) headquarters in southern Israel, three sources from the diplomatic community inside the CMCC told Drop Site News. According to the sources, the artificial intelligence data analytics giant is providing the technological architecture for tracking the delivery and distribution of aid to Gaza.The presence of Palantir and other corporationsâ€”along with recent changes banning non-profits unwilling to give data to Israeli authoritiesâ€”is creating a situation in which the delivery of aid is taking a backseat to the pursuit of profit, investment, and the training of AI products, experts say.The CMCC was established by U.S. Central Command (CENTCOM) in October, one week after the so-called ceasefire went into effect in Gaza to â€œmonitor implementation of the ceasefireâ€ and â€œhelp facilitate the flow of humanitarian, logistical, and security assistance from international counterparts into Gaza.â€ Last week, at the inaugural summit of the Board of Peace in Washington, D.C., Major General Jasper Jeffersâ€”who was tapped in January to lead the International Stabilization Force in Gazaâ€”announced that the CMCC will serve as the Board of Peaceâ€™s operational headquarters.According to the sources, a representative from Palantir sits in the CMCC operations room where aid convoys and distributions inside Gaza are monitored through drone surveillance. The representative integrates convoy and distribution-related data into Palantirâ€™s systems, the sources said.Palantir did not respond to an inquiry from Drop Site on its role in the CMCC or in aid distribution in Gaza. Founded in 2003 by billionaire Peter Thiel with the help of investments from the CIAâ€™s venture capital arm In-Q-Tel, Palantir is known for its work with government agencies, including the U.S. military and Immigration and Customs Enforcement (ICE).announcedaccordingreportThe use of Palantir to track aid deliveries to Gaza is of particular concern to observers. â€œThe distinction between death by drone and delivery of aid is being evaporated while we all sit around the same table,â€ a source from the diplomatic community who attends CMCC sessions told Drop Site.Palantirâ€™s two main platforms are Gotham and Foundry. â€œGothamâ€™s targeting offering supports soldiers with an Al-powered kill chain, seamlessly and responsibly integrating target identification and target effector pairing,â€ according to the companyâ€™s website. Foundry is Palantirâ€™s platform for supply chain management and is billed as a way to â€œbridge siloed planning and execution processes, optimize inventory management, and help build supply chain resilience for economic and geopolitical uncertainty.â€documentationThis means that, in theory, information that is being gathered at the CMCCâ€”including from participating governments the UN and NGOs regarding the type of aid being distributed, its distribution locations and systems, and truck convoy routesâ€”could be seamlessly pulled into Gothamâ€™s AI targeting matrix. The same software logic used to track aid at the CMCC could be used to optimize and accelerate lethal airstrikes.There is no information available as to whether Gotham and Foundry are the specific products being used to track aid, but publicly available photographic evidence indicates that Palantirâ€™s Gaiaâ€”a platform referred to on their website as a tool to â€œbring the battlefield into viewâ€â€”is being deployed at the CMCC.In an interview with Drop Site on the role of Palantir in Gaza, the economist Yanis Varoufakis, the former Greek Finance Minister and a former member of Greeceâ€™s parliament, described an encounter he had with a Palantir representative who had explained to him the benefit the company gained from Gaza. â€œHe was saying that â€˜as the bombs fell we were having a party,â€™â€ Varoufakis said. According to Varoufakis, the Palantir representative explained how the chaos of intense violence in a high-density urban area like Gaza generates substantial data for training their AI models on how humans respond under stress. â€œThe more bombardment and havoc, the better the training,â€ Varoufakis said.â€œItâ€™s one thing to say that companies like Lockheed Martin make money selling F35s to the Israelis,â€ he said. â€œThat has been a time-honored way that the military industrial complex has benefited from war and genocide and war crimes.â€ He continued, â€œThis is the first instance in history where it is the suffering of a people being subjected to genocide and bombingâ€”the suffering itselfâ€”which is adding to the capital of a company which then uses that capital to produce commodities to sell elsewhere.â€Palantir operates on Oracleâ€™s cloud infrastructure, led by Larry Ellisonâ€”a major donor to the Israeli military who also funds the Tony Blair Institute, which has itself consulted on governance mechanisms for Gaza.The growing use of Palantir and other private sector companies in Gaza comes as the non-profit sector is being systematically squeezed out. As of March 1, 2026, Israel will ban dozens of aid groups from operating in Gaza, as well as the West Bank and East Jerusalem, under new registration rules, including prominent NGOs such as Doctors Without Borders, the Norwegian Refugee Council, Oxfam, and Medical Aid for Palestinians.The new measures require aid groups to register the names and contact information of employees and to provide details about their funding and operations to Israeli authorities. The aid groups said in a joint statement this week that â€œthe demand to transfer personal data raises acute security and legal risks. It exposes national staff to potential retaliation and undermines established data protection and confidentiality safeguards.â€â€œNGOs are being pushed out of Gaza because aid delivered by humanitarian organizations is based on need and is provided to people wherever they are located,â€ said an aid worker who spoke to Drop Site on condition of anonymity. â€œThis doesnâ€™t match the vision of the â€˜New Gazaâ€™ where Palestinians will need to be displaced again into the zones where reconstruction will be permitted and their access to aid will be controlled through screening.â€Not all NGOs are being pushed out of Gaza. Othersâ€”on a list of registered and  approved organizationsâ€”are expanding their role alongside the private sector. These include Christian groups like Samaritanâ€™s Purse and GAiN, both of whom were involved in the Gaza Humanitarian Foundation (GHF), and who sources from the diplomatic community have recently seen gathering in a â€œprayer circleâ€ at the CMCC.These approved NGOs, alongside private firms coordinated through the CMCC like Palantir, stand ready to take over the distribution of aid in Gaza.â€œAid in Gaza has been stripped to bare survival and its future delivery appears to be faith based, profit driven, militarized and certainly not to be delivered by anyone that dares to speak out about what Palestinians are being subjected to,â€ said a senior aid worker who spoke to Drop Site on condition of anonymity.Gazaâ€™s experience with private delivery mechanisms has been catastrophic. In May 2025, the U.S. and Israeli-backed GHF was contracted to distribute aid in the enclave. During the four and half months the GHF operated in Gaza, more than 2,600 Palestinians seeking food were killed and over 19,000 wounded by Israeli forces or security contractors at or near aid distribution sites.The former headquarters of the GHF, a large warehouse-style building in Kiryat Gat, is now the headquarters of the CMCC.As aid groups are being banned, U.S. military contractors are also filling the vacuum. According to sources from the diplomatic community who attend the CMCC, the physical presence of Safe Reach Solutions (SRS), a U.S. military contractor that provided security for the GHF,  has recently expanded at the center of the facility, with SRS officials taking up more prominent seating space on the operations floor. The companyâ€™s representatives now sit behind name tags in seating that had previously been reserved for UN agencies, the sources said.SRS did not respond to an inquiry from Drop Site about its role at the CMCC or in Gaza aid distribution.â€œGiven the precedent of the GHF, which turned aid delivery into a killing machine,â€ Albanese told Drop Site, â€œand the grave violations of international law embedded in the so-called peace planâ€”first and foremost the negation of the Palestinian right to self-determinationâ€”the risk that companies and states involved in the CMCC may be complicit in, or even direct perpetrators of, international crimes is real.â€accordingThe re-emergence of GHF-linked companies, alongside the digital capacity to monitor and monetize the surviving population in Gaza, is now converging with the construction of a new physical infrastructure spearheaded by giant real estate conglomerates. At last weekâ€™s Board of Peace meeting, the reconstruction of Gaza was positioned as a massive financial â€œunlockingâ€ of a distressed asset. Figures like Yakir Gabay, who built a real estate empire in Germany, envisioned the coastline transformed into a â€œMediterranean Rivieraâ€ featuring 200 hotels and artificial islands. Marc Rowan, a billionaire investor and the CEO of Apollo Global Management, framed the project as the consolidation of Gazaâ€™s â€œproductive assetsâ€ into a â€œunified structure.â€A significant addition to the CMCCâ€™s corporate roster is Terra Firma Capital Partners, which sources confirmed now maintains a permanent presence at the CMCC. Founded by British financier Guy Hands, the firm brings experience in managing massive-scale residential assets. Terra Firma has links to the New Labour era, specifically through Lord John Birt, Tony Blairâ€™s former strategy director, who worked for the company after he left government.â€œThe genocide is entering a new phase. After the destruction of Gaza and the erasure of entire family lines, powerful states are now deciding the fate of the survivors without ever listening to their voices,â€ Albanese said. â€œIf Gaza is not to become a capitalist techno-dystopia, the time to act is now. States and corporations supporting this emerging infrastructure must be stopped, and held accountable. There is no time to lose.â€]]></content:encoded></item><item><title>Netflix Backs Out of Warner Bros. Bidding, Paramount Set to Win</title><link>https://www.hollywoodreporter.com/business/business-news/netflix-backs-out-warners-deal-paramount-win-1236516763/</link><author>atombender</author><category>hn</category><pubDate>Thu, 26 Feb 2026 23:16:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
	In a stunning twist, Netflix is declining to raise its bid for Warner Bros., positioning David Ellisonâ€™s Paramount as the winner in the battle for the fabled studio.
	Netflix co-CEOs Ted Sarandos and Greg Peters released a statement Thursday outlining their decision, namely that the deal is â€œno longer financially attractiveâ€ and that it â€œwas always a â€˜nice to haveâ€™ at the right price, not a â€˜must haveâ€™ at any price.â€


	â€œThe transaction we negotiated would have created shareholder value with a clear path to regulatory approval. However, weâ€™ve always been disciplined, and at the price required to match Paramount Skydanceâ€™s latest offer, the deal is no longer financially attractive, so we are declining to match the Paramount Skydance bid,â€ the co-CEOs said.

	
	â€œWarner Bros. is a world-class organization, and we want to thank David Zaslav, Gunnar Wiedenfels, Bruce Campbell, Brad Singer and the WBD Board for running a fair and rigorous process,â€ they added. â€œWe believe we would have been strong stewards of Warner Bros.â€™ iconic brands, and that our deal would have strengthened the entertainment industry and preserved and created more production jobs in the U.S.Â  But this transaction was always a â€˜nice to haveâ€™ at the right price, not a â€˜must haveâ€™ at any price.â€
	With Netflix out, Paramountâ€™s latest bid is almost a sure thing to be accepted by the Warners board, which determined earlier Thursday that it was a â€œsuperior proposalâ€ to Netflixâ€™s deal.

	
	â€œNetflix is a great company and throughout this process Ted, Greg, Spence and everyone there have been extraordinary partners to us. We wish them well in the future,â€ said David Zaslav, president and CEO of Warner Bros. Discovery. â€œOnce our Board votes to adopt the Paramount merger agreement, it will create tremendous value for our shareholders. We are excited about the potential of a combined Paramount Skydance and Warner Bros. Discovery and canâ€™t wait to get started working together telling the stories that move the world.â€
	PSKYâ€™s latest proposal was for $31 per share, butÂ had a number of other sweeteners, including a ticking fee payable to shareholders equal to $0.25 per quarter beginning after Sept. 30, 2026, as well as a $7 billion regulatory termination in the event the transaction does not close due to regulatory matters.
	Paramount has also agreed to pay the $2.8 billion termination fee that Warner Bros. would be required to pay to Netflix to terminate the existing merger agreement. 
	If all goes as expected, Netflix will be on the receiving end of that $2.8 billion sooner rather than later. Netflix shares soared by more than 10 percent in after-hours trading after the decision was announced.
	â€œWe are pleased WBDâ€™s Board has unanimously affirmed the superior value of our offer, which delivers to WBD shareholders superior value, certainty and speed to closing,â€ Paramount CEO David Ellison said in a statement on Thursday before Netflix backed out of the bidding.

	
	Of course, a Paramount deal is not necessarily a sure thing. U.S. and European regulators still need to formally sign off, and state attorneys general will have a say as well. Already politicians are positioning themselves to challenge the deal (Sen. Elizabeth Warren called in an â€œantitrust disasterâ€ Thursday), with Ellison likely to be called before Congress to discuss it.
	And the politics of the deal are sure to come up. 
	Sarandos and Peters, meanwhile, say they will continue to pour cash into content.
	â€œNetflixâ€™s business is healthy, strong and growing organically, powered by our slate and best-in-class streaming service. This year, weâ€™ll invest approximately $20 billion in quality films and series and will expand our entertaining offering. Consistent with our capital allocation policy, weâ€™ll also resume our share repurchase program,â€ the co-CEOs said. â€œWe will continue to do what weâ€™ve done for more than 20 years as a public company: delight our members, profitably grow our business, and drive long-term shareholder value.â€]]></content:encoded></item><item><title>Statement from Dario Amodei on our discussions with the Department of War</title><link>https://www.anthropic.com/news/statement-department-of-war</link><author>qwertox</author><category>hn</category><pubDate>Thu, 26 Feb 2026 22:42:47 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I believe deeply in the existential importance of using AI to defend the United States and other democracies, and to defeat our autocratic adversaries.Anthropic has therefore worked proactively to deploy our models to the Department of War and the intelligence community. We were the first frontier AI company to deploy our models in the US governmentâ€™s classified networks, the first to deploy them at the National Laboratories, and the first to provide custom models for national security customers. Claude is extensively deployed across the Department of War and other national security agencies for mission-critical applications, such as intelligence analysis, modeling and simulation, operational planning, cyber operations, and more.Anthropic has also acted to defend Americaâ€™s lead in AI, even when it is against the companyâ€™s short-term interest. We chose to forgo several hundred million dollars in revenue to cut off the use of Claude by firms linked to the Chinese Communist Party (some of whom have been designated by the Department of War as Chinese Military Companies), shut down CCP-sponsored cyberattacks that attempted to abuse Claude, and have advocated for strong export controls on chips to ensure a democratic advantage.Anthropic understands that the Department of War, not private companies, makes military decisions. We have never raised objections to particular military operations nor attempted to limit use of our technology in an  manner.However, in a narrow set of cases, we believe AI can undermine, rather than defend, democratic values. Some uses are also simply outside the bounds of what todayâ€™s technology can safely and reliably do. Two such use cases have never been included in our contracts with the Department of War, and we believe they should not be included now:Mass domestic surveillance. We support the use of AI for lawful foreign intelligence and counterintelligence missions. But using these systems for mass surveillance is incompatible with democratic values. AI-driven mass surveillance presents serious, novel risks to our fundamental liberties. To the extent that such surveillance is currently legal, this is only because the law has not yet caught up with the rapidly growing capabilities of AI. For example, under current law, the government can purchase detailed records of Americansâ€™ movements, web browsing, and associations from public sources without obtaining a warrant, a practice the Intelligence Community has acknowledged raises privacy concerns and that has generated bipartisan opposition in Congress. Powerful AI makes it possible to assemble this scattered, individually innocuous data into a comprehensive picture of any personâ€™s lifeâ€”automatically and at massive scale.Fully autonomous weapons. Partially autonomous weapons, like those used today in Ukraine, are vital to the defense of democracy. Even autonomous weapons (those that take humans out of the loop entirely and automate selecting and engaging targets) may prove critical for our national defense. But today, frontier AI systems are simply not reliable enough to power fully autonomous weapons. We will not knowingly provide a product that puts Americaâ€™s warfighters and civilians at risk. We have offered to work directly with the Department of War on R&D to improve the reliability of these systems, but they have not accepted this offer. In addition, without proper oversight, fully autonomous weapons cannot be relied upon to exercise the critical judgment that our highly trained, professional troops exhibit every day. They need to be deployed with proper guardrails, which donâ€™t exist today.To our knowledge, these two exceptions have not been a barrier to accelerating the adoption and use of our models within our armed forces to date.The Department of War has stated they will only contract with AI companies who accede to â€œany lawful useâ€ and remove safeguards in the cases mentioned above. They have threatened to remove us from their systems if we maintain these safeguards; they have also threatened to designate us a â€œsupply chain riskâ€â€”a label reserved for US adversaries, never before applied to an American companyâ€” to invoke the Defense Production Act to force the safeguardsâ€™ removal. These latter two threats are inherently contradictory: one labels us a security risk; the other labels Claude as essential to national security.Regardless, these threats do not change our position: we cannot in good conscience accede to their request.It is the Departmentâ€™s prerogative to select contractors most aligned with their vision. But given the substantial value that Anthropicâ€™s technology provides to our armed forces, we hope they reconsider. Our strong preference is to continue to serve the Department and our warfightersâ€”with our two requested safeguards in place. Should the Department choose to offboard Anthropic, we will work to enable a smooth transition to another provider, avoiding any disruption to ongoing military planning, operations, or other critical missions. Our models will be available on the expansive terms we have proposed for as long as required.We remain ready to continue our work to support the national security of the United States.]]></content:encoded></item><item><title>Smartphone market forecast to decline this year due to memory shortage</title><link>https://www.idc.com/resource-center/press-releases/wwsmartphoneforecast4q25/</link><author>littlexsparkee</author><category>hn</category><pubDate>Thu, 26 Feb 2026 22:09:45 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[â€œWhat we are witnessing is not a temporary squeeze, but a tsunami-like shock originating in the memory supply chain, with ripple effects spreading across the entire consumer electronics industry,â€ saidÂ Francisco Jeronimo, vice presidentÂ for WorldwideÂ Client Devices, IDC. â€œThe global smartphone market, particularly Android manufacturers, faces a significant threat. Vendors whose business is mainly at the low end of the market are likely to suffer the most. Rising component costs will hit their margins, and they will have no choice but to pass the costs on to end users. By contrast, Apple and Samsung are better positioned to navigate this crisis. As smaller and low-end-positioned Android vendors struggle with rising costs, Apple and Samsung could not only weather the storm but potentially expand market share as the competitive landscape tightens.â€]]></content:encoded></item><item><title>Show HN: Unfucked - version all changes (by any tool) - local-first/source avail</title><link>https://www.unfudged.io/</link><author>cyrusradfar</author><category>hn</category><pubDate>Thu, 26 Feb 2026 21:30:19 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Agent mass-overwrote your sourceYour AI agent refactored 30 Rust files, hit an error on file 27, and reverted everything to stale versions. Three hours of good work â€” gone.$ unf log --since 3h --include "*.rs" --stats
$ unf diff --at 10m
$ unf restore --at 10m -yThe agent decided  was "generated" and deleted it. API keys, database URLs, local config. Not in git. Not anywhere.$ unf log .env
$ unf cat .env --at 5m
$ unf restore --at 5m .env -yAgent's cleanup script went wrongYou asked the agent to "clean up build artifacts." It wrote a shell script that 'd  instead of .$ unf diff --at 1m
$ unf restore --at 2m --dry-run
$ unf restore --at 2m -yAgent "fixed" your dependenciesThe agent removed 6 "unused" crates from . Four were behind feature flags. CI is red.$ unf log Cargo.toml --stats
$ unf cat Cargo.toml --at 1h
$ unf restore --at 1h Cargo.toml -yAgent reformatted everythingThe agent ran Prettier with the wrong config and rewrote 200 TypeScript files. It committed before you noticed.  gives you one commit. UNF* has every file.$ unf log --since 30m --include "*.ts" --stats
$ unf diff --at 30m
$ unf restore --at 30m -yAgent replaced your test fixturesYour hand-crafted SQL seed data and JSON fixtures got overwritten with generic placeholders. A week of edge cases, gone.$ unf log --include "fixtures/*" --stats
$ unf diff --at 20m
$ unf restore --at 20m -yAgent deleted your migration filesThe agent saw 47 SQL migration files and decided they were "redundant." Production depends on them running in order.$ unf log --include "migrations/*.sql"
$ unf diff --at 15m
$ unf restore --at 15m -ySquash merge ate intermediate workYou squash-merged a feature branch. Git only has the final result. The 40 intermediate versions across 3 days? Git doesn't know they existed.$ unf log --since 3d --include "*.py"
$ unf diff --from 3d --to 1d
$ unf cat app/models.py --at 2dAgent lost context mid-sessionContext window overflow. The agent crashed 2 hours into a refactor across 4 repos. The new agent needs to pick up exactly where the old one left off.$ unf recap --global --json
$ unf log --sessions --since 2h
$ unf diff --sessionWhat happened while you were away?You left an agent running overnight. It touched 80 files across 3 projects. What did it do?$ unf log --global --since 8h --stats
$ unf diff --at 8h
$ unf restore --at 8h --dry-run]]></content:encoded></item><item><title>Layoffs at Block</title><link>https://twitter.com/jack/status/2027129697092731343</link><author>mlex</author><category>hn</category><pubDate>Thu, 26 Feb 2026 21:17:56 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>
<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://konrad.website/feeds/</link><description></description><item><title>Iran students stage first large anti-government protests since deadly crackdown</title><link>https://www.bbc.com/news/articles/c5yj2kzkrj0o</link><author>tartoran</author><category>hn</category><pubDate>Sun, 22 Feb 2026 13:59:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA["I don't want to use the word 'frustrated,' because he understands he has plenty of alternatives, but he's curious as to why they haven't... I don't want to use the word 'capitulated,' but why they haven't capitulated," he said.]]></content:encoded></item><item><title>Attention Media ≠ Social Networks</title><link>https://susam.net/attention-media-vs-social-networks.html</link><author>susam</author><category>hn</category><pubDate>Sun, 22 Feb 2026 12:36:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[By  on 20 Jan 2026
  When web-based social networks started flourishing nearly two
  decades ago, they were genuinely social networks.  You would sign up
  for a popular service, follow people you knew or liked and read
  updates from them.  When you posted something, your followers would
  receive your updates as well.  Notifications were genuine.  The
  little icons in the top bar would light up because someone had sent
  you a direct message or engaged with something you had posted.
  There was also, at the beginning of this millennium, a general sense
  of hope and optimism around technology, computers and the Internet.
  Social networking platforms were one of the services that were part
  of what was called Web 2.0, a term used for websites built around
  user participation and interaction.  It felt as though the
  information superhighway was finally reaching its potential.  But
  sometime between 2012 and 2016, things took a turn for the worse.

  First came the infamous infinite scroll.  I remember feeling uneasy
  the first time a web page no longer had a bottom.  Logically, I knew
  very well that everything a browser displays is a virtual construct.
  There is no physical page.  It is just pixels pretending to be one.
  Still, my brain had learned to treat web pages as objects with a
  beginning and an end.  The sudden disappearance of that end
  disturbed my sense of ease.

  Then came the bogus notifications.  What had once been meaningful
  signals turned into arbitrary prompts.  Someone you followed had
  posted something unremarkable and the platform would surface it as a
  notification anyway.  It didn't matter whether the notification was
  relevant to me.  The notification system stopped serving me and
  started serving itself.  It felt like a violation of an unspoken
  agreement between users and services.  Despite all that, these
  platforms still remained social in some diluted sense.  Yes, the
  notifications were manipulative, but they were at least about people
  I actually knew or had chosen to follow.  That, too, would change.

  Over time, my timeline contained fewer and fewer posts from friends
  and more and more content from random strangers.  Using these
  services began to feel like standing in front of a blaring
  loudspeaker, broadcasting fragments of conversations from all over
  the world directly in my face.  That was when I gave up on these
  services.  There was nothing social about them anymore.  They had
  become .  My attention is precious to me.  I
  cannot spend it mindlessly scrolling through videos that have
  neither relevance nor substance.

  But where one avenue disappeared, another emerged.  A few years ago,
  I stumbled upon Mastodon and it reminded me of the early days of
  Twitter.  Back in 2006, I followed a small number of folks of the
  nerd variety on Twitter and received genuinely interesting updates
  from them.  But when I log into the ruins of those older platforms
  now, all I see are random videos presented to me for reasons I can
  neither infer nor care about.  Mastodon, by contrast, still feels
  like social networking in the original sense.  I follow a small
  number of people I genuinely find interesting and I receive their
  updates and only their updates.  What I see is the result of my own
  choices rather than a system trying to capture and monetise my
  attention.  There are no bogus notifications.  The timeline feels
  calm and predictable.  If there are no new updates from people I
  follow, there is nothing to see.  It feels closer to how social
  networks used to work originally.  I hope it stays that way.
]]></content:encoded></item><item><title>Back to FreeBSD: Part 1</title><link>https://hypha.pub/back-to-freebsd-part-1</link><author>enz</author><category>hn</category><pubDate>Sun, 22 Feb 2026 07:16:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Japanese Woodblock Print Search</title><link>https://ukiyo-e.org/</link><author>curmudgeon22</author><category>hn</category><pubDate>Sun, 22 Feb 2026 03:18:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Japanese Woodblock Print SearchUkiyo-e Search provides an incredible resource: The ability to both search for Japanese woodblock prints by simply taking a picture of an existing print AND the ability to see similar prints across multiple collections of prints. Below is an example print, click to see it in action.]]></content:encoded></item><item><title>A Botnet Accidentally Destroyed I2P</title><link>https://www.sambent.com/a-botnet-accidentally-destroyed-i2p-the-full-story/</link><author>Cider9986</author><category>hn</category><pubDate>Sun, 22 Feb 2026 01:08:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[On February 3, 2026, the I2P anonymity network was flooded with 700,000 hostile nodes in what became one of the most devastating Sybil attacks an anonymity network has ever experienced. The network normally operates with 15,000 to 20,000 active devices. The attackers overwhelmed it by a factor of 39 to 1.For three consecutive years, I2P has been hit with Sybil attacks every February. The 2023 and 2024 attacks used malicious floodfill routers and remain unattributed. When the 2026 attack began, most assumed it was the same state-sponsored operation continuing its annual disruption campaign. The assumption was wrong.The attacker was identified as the Kimwolf botnet, an IoT botnet that infected millions of devices including streaming boxes and consumer routers throughout late 2025. Kimwolf is the same operation behind the record-setting 31.4 terabit per second DDoS attack in December 2025. The operators admitted on Discord they accidentally disrupted I2P while attempting to use the network as backup command-and-control infrastructure after security researchers destroyed over 550 of their primary C2 servers.The I2P development team responded by shipping version 2.11.0 just six days after the attack began. The release includes hybrid ML-KEM plus X25519 post-quantum encryption enabled by default, making I2P one of the first production anonymity networks to ship post-quantum cryptography to all users. Additional Sybil mitigations, SAMv3 API upgrades, and infrastructure improvements were included.]]></content:encoded></item><item><title>How I use Claude Code: Separation of planning and execution</title><link>https://boristane.com/blog/how-i-use-claude-code/</link><author>vinhnx</author><category>hn</category><pubDate>Sun, 22 Feb 2026 00:29:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I’ve been using Claude Code as my primary development tool for approx 9 months, and the workflow I’ve settled into is radically different from what most people do with AI coding tools. Most developers type a prompt, sometimes use plan mode, fix the errors, repeat. The more terminally online are stitching together ralph loops, mcps, gas towns (remember those?), etc. The results in both cases are a mess that completely falls apart for anything non-trivial.The workflow I’m going to describe has one core principle: never let Claude write code until you’ve reviewed and approved a written plan. This separation of planning and execution is the single most important thing I do. It prevents wasted effort, keeps me in control of architecture decisions, and produces significantly better results with minimal token usage than jumping straight to code.flowchart LR
    R[Research] --> P[Plan]
    P --> A[Annotate]
    A -->|repeat 1-6x| A
    A --> T[Todo List]
    T --> I[Implement]
    I --> F[Feedback & Iterate]Every meaningful task starts with a deep-read directive. I ask Claude to thoroughly understand the relevant part of the codebase before doing anything else. And I always require the findings to be written into a persistent markdown file, never just a verbal summary in the chat.read this folder in depth, understand how it works deeply, what it does and all its specificities. when that’s done, write a detailed report of your learnings and findings in research.mdstudy the notification system in great details, understand the intricacies of it and write a detailed research.md document with everything there is to know about how notifications workgo through the task scheduling flow, understand it deeply and look for potential bugs. there definitely are bugs in the system as it sometimes runs tasks that should have been cancelled. keep researching the flow until you find all the bugs, don’t stop until all the bugs are found. when you’re done, write a detailed report of your findings in research.mdNotice the language: , , , . This isn’t fluff. Without these words, Claude will skim. It’ll read a file, see what a function does at the signature level, and move on. You need to signal that surface-level reading is not acceptable.The written artifact () is critical. It’s not about making Claude do homework. It’s my review surface. I can read it, verify Claude actually understood the system, and correct misunderstandings before any planning happens. If the research is wrong, the plan will be wrong, and the implementation will be wrong. Garbage in, garbage out.This is the most expensive failure mode with AI-assisted coding, and it’s not wrong syntax or bad logic. It’s implementations that work in isolation but break the surrounding system. A function that ignores an existing caching layer. A migration that doesn’t account for the ORM’s conventions. An API endpoint that duplicates logic that already exists elsewhere. The research phase prevents all of this.Once I’ve reviewed the research, I ask for a detailed implementation plan in a separate markdown file.I want to build a new feature <name and description> that extends the system to perform <business outcome>. write a detailed plan.md document outlining how to implement this. include code snippetsthe list endpoint should support cursor-based pagination instead of offset. write a detailed plan.md for how to achieve this. read source files before suggesting changes, base the plan on the actual codebaseThe generated plan always includes a detailed explanation of the approach, code snippets showing the actual changes, file paths that will be modified, and considerations and trade-offs.I use my own  plan files rather than Claude Code’s built-in plan mode. The built-in plan mode sucks. My markdown file gives me full control. I can edit it in my editor, add inline notes, and it persists as a real artifact in the project.One trick I use constantly: for well-contained features where I’ve seen a good implementation in an open source repo, I’ll share that code as a reference alongside the plan request. If I want to add sortable IDs, I paste the ID generation code from a project that does it well and say “this is how they do sortable IDs, write a plan.md explaining how we can adopt a similar approach.” Claude works dramatically better when it has a concrete reference implementation to work from rather than designing from scratch.But the plan document itself isn’t the interesting part. The interesting part is what happens next.This is the most distinctive part of my workflow, and the part where I add the most value.flowchart TD
    W[Claude writes plan.md] --> R[I review in my editor]
    R --> N[I add inline notes]
    N --> S[Send Claude back to the document]
    S --> U[Claude updates plan]
    U --> D{Satisfied?}
    D -->|No| R
    D -->|Yes| T[Request todo list]After Claude writes the plan, I open it in my editor and add inline notes directly into the document. These notes correct assumptions, reject approaches, add constraints, or provide domain knowledge that Claude doesn’t have.The notes vary wildly in length. Sometimes a note is two words: “not optional” next to a parameter Claude marked as optional. Other times it’s a paragraph explaining a business constraint or pasting a code snippet showing the data shape I expect.Some real examples of notes I’d add:“use drizzle:generate for migrations, not raw SQL” — domain knowledge Claude doesn’t have“no — this should be a PATCH, not a PUT” — correcting a wrong assumption“remove this section entirely, we don’t need caching here” — rejecting a proposed approach“the queue consumer already handles retries, so this retry logic is redundant. remove it and just let it fail” — explaining why something should change“this is wrong, the visibility field needs to be on the list itself, not on individual items. when a list is public, all items are public. restructure the schema section accordingly” — redirecting an entire section of the planThen I send Claude back to the document:I added a few notes to the document, address all the notes and update the document accordingly. don’t implement yetThis cycle repeats 1 to 6 times. The explicit  guard is essential. Without it, Claude will jump to code the moment it thinks the plan is good enough. It’s not good enough until I say it is.The markdown file acts as  between me and Claude. I can think at my own pace, annotate precisely where something is wrong, and re-engage without losing context. I’m not trying to explain everything in a chat message. I’m pointing at the exact spot in the document where the issue is and writing my correction right there.This is fundamentally different from trying to steer implementation through chat messages. The plan is a structured, complete specification I can review holistically. A chat conversation is something I’d have to scroll through to reconstruct decisions. The plan wins every time.Three rounds of “I added notes, update the plan” can transform a generic implementation plan into one that fits perfectly into the existing system. Claude is excellent at understanding code, proposing solutions, and writing implementations. But it doesn’t know my product priorities, my users’ pain points, or the engineering trade-offs I’m willing to make. The annotation cycle is how I inject that judgement.Before implementation starts, I always request a granular task breakdown:add a detailed todo list to the plan, with all the phases and individual tasks necessary to complete the plan - don’t implement yetThis creates a checklist that serves as a progress tracker during implementation. Claude marks items as completed as it goes, so I can glance at the plan at any point and see exactly where things stand. Especially valuable in sessions that run for hours.When the plan is ready, I issue the implementation command. I’ve refined this into a standard prompt I reuse across sessions:implement it all. when you’re done with a task or phase, mark it as completed in the plan document. do not stop until all tasks and phases are completed. do not add unnecessary comments or jsdocs, do not use any or unknown types. continuously run typecheck to make sure you’re not introducing new issues.This single prompt encodes everything that matters:: do everything in the plan, don’t cherry-pick“mark it as completed in the plan document”: the plan is the source of truth for progress“do not stop until all tasks and phases are completed”: don’t pause for confirmation mid-flow“do not add unnecessary comments or jsdocs”: keep the code clean“do not use any or unknown types”: maintain strict typing“continuously run typecheck”: catch problems early, not at the endI use this exact phrasing (with minor variations) in virtually every implementation session. By the time I say “implement it all,” every decision has been made and validated. The implementation becomes mechanical, not creative. This is deliberate. I want implementation to be boring. The creative work happened in the annotation cycles. Once the plan is right, execution should be straightforward.Without the planning phase, what typically happens is Claude makes a reasonable-but-wrong assumption early on, builds on top of it for 15 minutes, and then I have to unwind a chain of changes. The “don’t implement yet” guard eliminates this entirely.Feedback During ImplementationOnce Claude is executing the plan, my role shifts from architect to supervisor. My prompts become dramatically shorter.flowchart LR
    I[Claude implements] --> R[I review / test]
    R --> C{Correct?}
    C -->|No| F[Terse correction]
    F --> I
    C -->|Yes| N{More tasks?}
    N -->|Yes| I
    N -->|No| D[Done]Where a planning note might be a paragraph, an implementation correction is often a single sentence:“You didn’t implement the  function.”“You built the settings page in the main app when it should be in the admin app, move it.”Claude has the full context of the plan and the ongoing session, so terse corrections are enough.Frontend work is the most iterative part. I test in the browser and fire off rapid corrections:For visual issues, I sometimes attach screenshots. A screenshot of a misaligned table communicates the problem faster than describing it.I also reference existing code constantly:“this table should look exactly like the users table, same header, same pagination, same row density.”This is far more precise than describing a design from scratch. Most features in a mature codebase are variations on existing patterns. A new settings page should look like the existing settings pages. Pointing to the reference communicates all the implicit requirements without spelling them out. Claude would typically read the reference file(s) before making the correction.When something goes in a wrong direction, I don’t try to patch it. I revert and re-scope by discarding the git changes:“I reverted everything. Now all I want is to make the list view more minimal — nothing else.”Narrowing scope after a revert almost always produces better results than trying to incrementally fix a bad approach.Staying in the Driver’s SeatEven though I delegate execution to Claude, I never give it total autonomy over what gets built. I do the vast majority of the active steering in the  documents.This matters because Claude will sometimes propose solutions that are technically correct but wrong for the project. Maybe the approach is over-engineered, or it changes a public API signature that other parts of the system depend on, or it picks a more complex option when a simpler one would do. I have context about the broader system, the product direction, and the engineering culture that Claude doesn’t.flowchart TD
    P[Claude proposes changes] --> E[I evaluate each item]
    E --> A[Accept as-is]
    E --> M[Modify approach]
    E --> S[Skip / remove]
    E --> O[Override technical choice]
    A & M & S & O --> R[Refined implementation scope]Cherry-picking from proposals: When Claude identifies multiple issues, I go through them one by one: “for the first one, just use Promise.all, don’t make it overly complicated; for the third one, extract it into a separate function for readability; ignore the fourth and fifth ones, they’re not worth the complexity.” I’m making item-level decisions based on my knowledge of what matters right now. When the plan includes nice-to-haves, I actively cut them. “remove the download feature from the plan, I don’t want to implement this now.” This prevents scope creep.Protecting existing interfaces: I set hard constraints when I know something shouldn’t change: “the signatures of these three functions should not change, the caller should adapt, not the library.”Overriding technical choices: Sometimes I have a specific preference Claude wouldn’t know about: “use this model instead of that one” or “use this library’s built-in method instead of writing a custom one.” Fast, direct overrides.Claude handles the mechanical execution, while I make the judgement calls. The plan captures the big decisions upfront, and selective guidance handles the smaller ones that emerge during implementation.I run research, planning, and implementation in a  rather than splitting them across separate sessions. A single session might start with deep-reading a folder, go through three rounds of plan annotation, then run the full implementation, all in one continuous conversation.I am not seeing the performance degradation everyone talks about after 50% context window. Actually, by the time I say “implement it all,” Claude has spent the entire session building understanding: reading files during research, refining its mental model during annotation cycles, absorbing my domain knowledge corrections.When the context window fills up, Claude’s auto-compaction maintains enough context to keep going. And the plan document, the persistent artifact, survives compaction in full fidelity. I can point Claude to it at any point in time.The Workflow in One SentenceRead deeply, write a plan, annotate the plan until it’s right, then let Claude execute the whole thing without stopping, checking types along the way.That’s it. No magic prompts, no elaborate system instructions, no clever hacks. Just a disciplined pipeline that separates thinking from typing. The research prevents Claude from making ignorant changes. The plan prevents it from making wrong changes. The annotation cycle injects my judgement. And the implementation command lets it run without interruption once every decision has been made.Try my workflow, you’ll wonder how you ever shipped anything with coding agents without an annotated plan document sitting between you and the code.]]></content:encoded></item><item><title>Evidence of the bouba-kiki effect in naïve baby chicks</title><link>https://www.science.org/doi/10.1126/science.adq7188</link><author>suddenlybananas</author><category>hn</category><pubDate>Sat, 21 Feb 2026 21:51:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why is Claude an Electron app?</title><link>https://www.dbreunig.com/2026/02/21/why-is-claude-an-electron-app.html</link><author>dbreunig</author><category>hn</category><pubDate>Sat, 21 Feb 2026 21:28:13 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[If code is free, why aren’t all apps native?The state of coding agents can be summed up by this factClaude spent $20k on an agent swarm implementing (kinda) a C-compiler in Rust, but desktop Claude is an Electron app.If you’re unfamiliar, Electron is a coding framework for building desktop applications using web tech, specifically HTML, CSS, and JS. What’s great about Electron is it allows you to  build one desktop app that supports Windows, Mac, and Linux. Plus it lets developers use existing web app code to get started. It’s great for teams big and small. Many apps you probably use every day are built with Electron: Slack, Discord, VS Code, Teams, Notion, and more.There are downsides though. Electron apps are bloated; each runs its own Chromium engine. The minimum app size is usually a couple hundred megabytes. They are often laggy or unresponsive. They don’t integrate well with OS features.(These last two issues  be addressed by smart development and OS-specific code, but they rarely are. The benefits of Electron (one codebase, many platforms, it’s just web!) don’t incentivize optimizations outside of HTML/JS/CSS land.)But these downsides are dramatically outweighed by the ability to build and maintain one app, shipping it everywhere.On the surface, this ability should render Electron’s benefits obsolete! Rather than write one web app and ship it to each platform, we should write  and use coding agents to ship  code to each platform. If this ability is real and adopted, users get snappy, performant, native apps from small, focused teams serving a broad market.But we’re still leaning on Electron. Even Anthropic, one of the leaders in AI coding tools, who keeps publishing flashy agentic coding achievements, still uses Electron in the Claude desktop app. And it’s slow, buggy, and bloated app.So why are we still using Electron and not embracing the agent-powered, spec driven development future?For one thing, coding agents are  good at the first 90% of dev. But that last bit – nailing down all the edge cases and continuing support once it meets the real world – remains hard, tedious, and requires plenty of agent hand-holding.Anthropic’s Rust-base C compiler slammed into this wall, after screaming through the bulk of the tests:The resulting compiler has nearly reached the limits of Opus’s abilities. I tried (hard!) to fix several of the above limitations but wasn’t fully successful. New features and bugfixes frequently broke existing functionality.The resulting compiler  impressive, given the time it took to deliver it and the number of people who worked on it, but it is largely unusable. That last mile is .And this gets even worse once a program meets the real world. Messy, unexpected scenarios stack up and development never really ends. Agents make it easier, sure, but hard product decisions become challenged and require human decisions.Further, with 3 different apps produced (Mac, Windows, and Linux) the surface area for bugs and support increases 3-fold. Sure, there are local quirks with Electron apps, but most of it is mitigated by the common wrapper. Not so with native!A good test suite and spec  enable the Claude team to ship a Claude desktop app native to each platform. But the resulting overhead of that last 10% of dev and the increased support and maintenance burden will remain.For now, Electron still makes sense. Coding agents are amazing. But the last mile of dev and the support surface area remains a real concern.]]></content:encoded></item><item><title>Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU</title><link>https://github.com/xaskasdf/ntransformer</link><author>xaskasdf</author><category>hn</category><pubDate>Sat, 21 Feb 2026 20:57:30 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Hi everyone, I'm kinda involved in some retrogaming and with some experiments I ran into the following question: "It would be possible to run transformer models bypassing the cpu/ram, connecting the gpu to the nvme?"This is the result of that question itself and some weekend vibecoding (it has the linked library repository in the readme as well), it seems to work, even on consumer gpus, it should work better on professional ones tho]]></content:encoded></item><item><title>EDuke32 – Duke Nukem 3D (Open-Source)</title><link>https://www.eduke32.com/</link><author>reconnecting</author><category>hn</category><pubDate>Sat, 21 Feb 2026 20:10:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Per-pixel dynamic lighting and realtime shadows... groovy!  Polymer renderer requires a bad-ass video card.Hollywood Holocaust with classic textures is an awesome, free homebrew game engine and source port of the classic PC first person shooter —  for short—to Windows, Linux, macOS, FreeBSD, several handhelds, your family toaster, and your girlfriend's vibrator.  We've added thousands of cool and useful features and upgrades for regular players and additional editing capabilities and scripting extensions for homebrew developers and mod creators. Created by Duke4.net community leader Richard "TerminX" Gobeille and a team of elite ninja programmers including Evan "Hendricks266" Ramos, Pierre-Loup "Plagman" Griffais, and Philipp "Helixhorned" Kutin (based on work by Todd Replogle/Ken Silverman/Jonathon Fowler/Matt Saettler), EDuke32 is the undeniable king of Duke Nukem 3D ports.EDuke32 is the technology that powers , created by , which was founded by EDuke32's authors.EDuke32 runs at crazy screen resolutions like 10240x4320.EDuke32 allows you to choose between two different hardware-accelerated OpenGL renderers, or the classic, warped software mode you grew up withEDuke32 fixes an insane amount of programming errors which were harmless in the days of DOS but are fatal with modern protected memory models; translation: EDuke32 includes , a fully-fledged port of  with all the same benefits. Who wants some Wang?EDuke32 is the only Duke3D port to be actively developed and maintained for more than twenty yearsEDuke32 features Plagman's incredible "Polymer" renderer with powerful hardware-accelerated capabilitiesEDuke32 has a huge number of new extensions to the game's scripting system, allowing gameplay mods that rival even modern games.EDuke32 runs the HRP with support for all features, most of which require EDuke32; no other port can run the HRP with all features enabledEDuke32 adds a , including Quake-style key bindings, command aliases, advanced tab completion, comprehensive command history, colored text and moreEDuke32 has hundreds of code rewrites, optimizations and fixes for rare or annoying bugs in the original codeEDuke32 adds tons of optional new features that make the player's life easier including modern status display/HUD, support for loading mods from the startup window, and modern, WSAD-based controls with thoroughly reworked mouse aimingEDuke32 supports Ogg Vorbis and FLAC sound and musicEDuke32 is developed by people who have been in the Duke3D scene since the beginningEDuke32 lets you play that game called 'NAM' you saw at the dollar store back in the 90sEDuke32 makes sandwiches!BUILD engine technology originally created by Ken Silverman, non-GPL rendering and engine technology used in EDuke32 available under BUILDLIC.]]></content:encoded></item><item><title>Inputlag.science – Repository of knowledge about input lag in gaming</title><link>https://inputlag.science/</link><author>akyuu</author><category>hn</category><pubDate>Sat, 21 Feb 2026 19:41:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hello traveler, welcome to the repository of knowledge about input lag in gaming.The input lag in a gaming system, or any interactive system, is the latency between the user input and a reaction on the
screen. Input lag is an issue that has crept in the industry, little by little, without being noticed over the years.
Nowadays, finding a gaming system with a latency similar to early 2000 without image degradation is a definitive challenge.
The reasons behind this rise of the latency is mainly that systems have become more and more complex and developers
often don't know or don't understand each part that can impact the latency.There are three majors components in the lag chain:There are obviously plenty of subtleties around those three points. This website tries to reference all the knowledge
around those parts, especially the two first one, and how to precisely measure them.]]></content:encoded></item><item><title>Parse, Don&apos;t Validate and Type-Driven Design in Rust</title><link>https://www.harudagondi.space/blog/parse-dont-validate-and-type-driven-design-in-rust/</link><author>todsacerdoti</author><category>hn</category><pubDate>Sat, 21 Feb 2026 19:40:06 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In the Rust Programming Language Community Server, there’s tag named  which links to an article about the concept of avoiding validation functions and encoding invariants in the type level instead. I usually recommend it to beginners/intermediates to Rust who are struggling with designing APIs.The only problem is that it uses Haskell to explain its concepts.Yeah, it’s , but for beginners unfamiliar with the functional paradigm, it might not be so approachable. And so I wanted so write a blog post about this pattern but in a rather Rust-centric way. So let’s start!One basic example I can give is a function that divides a number by another number.This is fine, but unfortunately it can panic when  has the value of zero:That’s fine and dandy if we want erroneous values to fail loudly at runtime, but what if we want stronger guarantees? This is especially important when some operations don’t fail loudly, like the following:There’s no error! But do we want that?We could add an  in the  function to emulate typical integer division behavior.Cute! But there’s still a problem of running into panics only at runtime. My beef with Python (or any other dynamic language for that matter) is that a lot of errors only arises when you run the program. That’s why they’re adding typechecking to these languages: people want to bubble some mistakes to compile-time (or typecheck-time, whatever). We can use Rust’s rich type system to communicate these errors at build time.One way, which I think is the more common way as people are more familiar with it is the idea of fallible functions, which return either an  or a .This is a fine way to do things, as it communicates that (1) the function can fail, and (2) you can handle the failing case after.   To me, the function’s invariants ( must not be zero) is encoded after-the-fact, aka in the return type . This implies to me that the invariants could be encoded before-the-fact, aka in the function parameters. But what would that look like?Enter the newtype pattern.Say, let’s have a type that is something like , but it’s guaranteed to never be zero. We’ll name it :This struct only contains a single field . The semantics of the type understood from the name is that it’s just like a normal , but does not allow the value of zero. How do we guarantee this? Since rust does encapsulation at the module level, we make this type public while have its field private.Then, the only way to construct this type is via a fallible constructor function:Remember to add some convenience traits.We can then use this in our  function.There is an interesting implication in this pattern.In the second version of , we changed the return type from  to  just to avoid the panics. As described in the original article by Alexis King, this is a  of the return type, and the function’s promise. We temper the caller’s expectation by saying that yes, this function can fail in some way, and you have to account for that. And that weakening is described in the type system via the  enum.In the third iteration of , we change our perspective and ask ourselves “instead of weakening the return type, what if we  the function parameters?” We communicated that via accepting a . Instead of having the validation code in our functions, we instead push that responsibility to the caller. The validation now happens before the function execution.To see the advantage of pushing the validation forward to the user, let’s say we have another function like so:This function can fail if the discriminant is negative (which we will be ignoring in this contrived example), and if  is zero. The two ways of going about this can be written as follows:The  version has me duplicating the conditional for at least two different functions, which might be icky if you are a DRY-hard. Also, not only the function has to validate if the float can be zero, the  must then validate again by matching on the returned . That seems redundant. It would be ideal if we only need to check only once.The  version can help with that as validation happens before, and happens once, instead of twice.Moving away from the , let’s now use an example from the original blog post, converted to Rust:We checked if  is empty in the  function. Then, we still had to “check” it again in the  function by matching on . The  was known to be nonempty, do we have to check it again? Consequently, doesn’t this have an impact on performance, especially if we have to check it again and again and again?The original post raised a good point about resilience to refactors. If for some reason the  gets refactored out for some reason, and the programmer forgot to update , then the  branch might actually get reached and explode your computer or whatever.If we instead had a special  newtype (well, not exactly special) where its existence guarantees that the Vec is never empty, we could doIn this context, we can call  and  functions, since they validate and convert the less semantic type to a type with more meaning imbued into it. That is, nonzeroness of a float and nonemptiness of a  is now encoded into a type. You can just see the word  and therefore understand that going forward it is always be an  that is never zero.Validation and checking functions on the other hand, well, just validate the value and leave the type as that. If I have a  function, then there’s not really much of a readable difference between an  that has  called on it versus and an  that hasn’t.By taking advantage of the existence of a nominative type system, we can communicate that this  is not zero by  it to a new type, as opposed to just  it. If you only validate it, then you still can’t tell if  was nonzero unless you dig through the code. However, if you parsed it, you can say it’s always be nonzero if you see  in your code.Of course, the above examples are very much contrived, but is there an instance where creating newtypes is helpful? Yes. In fact, most people have used it. It’s called a .If we dig into the internals,  is just a newtype over the  type:It’s parsing function is , which contains the validation code for checking if the byte vector is valid UTF-8.So instead of passing around a  around and validating all over the place, just parse into a  and you can be assured with having a type-safe  with all the convenience functions you can get.Another example is . In Python,  simply give you a dictionary. This is fine, especially if the data is sufficiently arbitrary, but if you have a schema and a type system, it’s better to let the type system do the work of parsing .In our terminology, validation looks like this:That’s two s! One for checking if the string is valid json and the other is for checking if the  field exists. Now consider this example where we use the parsing mechanic instead via types and the  derive macro.Since we deserialized the  file into an actual type, we can safely make these guarantees:The  and  always exist in the  string we parse. always has an integer value. is always an array of three integers. will never panic since all elements of an array is always initialized, and indexing into the first the element of a nonzero-length array will always be successful.The only point of failure here is pushed upfront, where the  happens. After that point, there’s not really much error handling to be done here, since the validation is now represented at the type level instead of at the function level.Maxims of Type Driven Design #With that said, what lessons can we learn from here? Turns out, most functional language programmers already have learned several lessons, and Rust is not much different in terms of applying such FP concepts to the language.First lesson we can learn is that we should make illegal states unrepresentable.To refer back to the  and  examples, we say the state of being zero is illegal for  and the state of being empty is illegal for . And as illegal states, they cannot be represented in such types. That’s why the only constructors available for these types are fallible; the value either parsed successfully, or it failed and does not return the new types.If we only do validation, like checking if  is nonzero for example, then the illegal state can still be represented. There’s a small possible that the value is zero, especially after some refactors when the conditional checks are accidentally or intentionally removed in some places.This reminds me of how other languages use integers as sentinel values. Given this code snippet from Wikipedia:The error is returned as , since indexing arrays is only valid for nonnegative integers. Seems weird as (1) the numbers -2 and below  exist, but not actually valid, and (2) treating certain values as special seems too error-prone, as in the future it could be that negative number can become semantically valid.Second lesson we can learn is that proving invariants should be done as early as possible.There’s this concept called  where the linked paper describes it as follows:Shotgun Parsing: Shotgun parsing is a programming antipattern whereby parsing and input-validating code is mixed with and spread across processing code—throwing a cloud of checks at the input, and hoping, without any systematic justification, that one or another would catch all the “bad” cases.Essentially, it describes the problem of usage of data without previous validation of its entirety of data. You could act on a part of the data that is validated beforehand, but discover that another part of the data is invalid.The paper mentions CVE-2016-0752, which is a bug that allows attackers to read arbitrary files because you can use  in the input. The paper argues that treating validation as emergent and not deliberate can lead to security bugs like these.If we treat validation as deliberate, then it should happen as early as possible and as comprehensive as possible. By parsing first, every invariant can be proven first before executing on said data.I remember this video about lambda calculus. It concludes that types can be represented as propositions in logic, and terms as proofs. I recommend watching the video, as it is eye-opening to me and maybe it can help you realize some things too.Fundamentally, if your program typechecks properly, then you can say that the proof is correct. Thank you Curry-Howard Correspondence. There are proof assistant programming languages that can help with this like Lean and Agda, but you can emulate this in Rust anyway. That’s how some weird libraries like the typenum crate work.This is a simple program in Rust where I check if  is equal to . Obviously this is not correct, and so it will appropriately give you a compile error.So sad that the error message is dogshit. Such is life.There are some recommendations I usually say to people on the RPLCS discord server, adapted from the original blog post.First, just because a function accepts a type doesn’t mean you have to use it in your structs, nor have to perpetually represent it as that type. For example, let’s say we have a third party library function that looks like this.You  have to store  in your / struct like App { lightbulb_state: bool }. That’s confusing. I’d rather have you define a separate enum with more semantics imbued into it, like:Yeah, people can say it gets more verbose, but I rather care more about correctness instead. Sorry.Second, I sometimes get suspicious about these kind of APIs:If I see the function body does not do anything side-effectful, then it’s probable that parsing can help here turning  into a more structured datatype. And even for side-effectful stuff, there are some types that better represent certain situations, like infinite loop function representing their return types as  or Result<Infallible, MyError>.I love creating more types. Five million types for everyone please.I think it’s interesting that there’s a lot of instances where types drive the design of Rust programs. Like how  has four layers of newtypes plus an additional field.  generate anonymous structs in their  macros.  is a macro crate that converts functions into compile-time builders via types.Of course, not everything is solvable via types. But personally I think pushing your verification code to types can help your code become clearer and more robust. Let the type system handle the validation for you. It exists, so might as well use it to its fullest extent.I’d like to thank Alexis King for this article where I first encountered this idea. I’d love to follow up on this topic with an extension on this sequel, and maybe recontextualizing in Rust via the  keyword would be helpful.Of course, newtyping is not the answer to all problems. Due to lack of ergonomic features to allow newtyping—like delegation—many people are somewhat averse to using the pattern. Nevertheless, if someone made a good enough RFC I’d be happy to see it happen.Using the type system as a compile-time checker because I want the compiler to help me write my programs is very nice. You should take advantage of the type system too, not many languages have it as good as Rust :)Liked this blog post and want some more? Consider donating to support the author!]]></content:encoded></item><item><title>How Taalas “prints” LLM onto a chip?</title><link>https://www.anuragk.com/blog/posts/Taalas.html</link><author>beAroundHere</author><category>hn</category><pubDate>Sat, 21 Feb 2026 19:07:20 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[or how to generate 17000 tokens per second?February 22, 2026 · 4 min readA startup called Taalas, recently released an ASIC chip running Llama 3.1 8B (3/6 bit quant) at an inference rate of 17,000 tokens per seconds. That's like writing around 30 A4 sized pages in one second.  They claim it's 10x cheaper in ownership cost than GPU based inference systems and is 10x less electricity hog. And yeah, about 10x faster than state of art inference.I tried to read through their blog and they've literally "hardwired" the model's weights on chip. Initially, this didn't sound intuitive to me.
Coming from a Software background, with hobby-ist understanding of LLMs, I couldn't wrap my head around how you just "print" a LLM onto a chip. So, I decided to dig into multiple blogposts, LocalLLaMA discussions, and hardware concepts. It was much more interesting than I had thought. Hence this blogpost.Taalas is a 2.5 year old company and it's their first chip. Taalas's chip is a fixed-function ASIC (Application-Specific Integrated Circuit). Kinda like a CD-ROM/Game cartridge, or a printed book, it only holds one model and cannot be rewritten.HOW NVIDIA GPUs process stuff? (Inefficiency 101)LLMs consist of sequential Layers. For eg. Llama 3.1 8B has 32 layers. The task of each layer is to further refine the input. Each layer is essentially large weight matrices (the model's 'knowledge').When a user inputs a prompt, it is converted into an vector of numbers aka embeddings. 
On a normal GPU, the input vector enters the compute cores. Then GPU fetches the Layer 1 weights from VRAM/HBM (GPU's RAM) , does matrix multiplication, stores the intermediate results(aka activations) back in VRAM. Then it fetches the Layer 2 weights, and previous result, does the math, and saves it to VRAM again.
This cycle continues till 32nd layer just to generate a single token. Then, to generate the next token, the GPU repeats this entire 32-layer journey.So, due to this constant back-and-forth the memory bus induces latency and consumes significant amounts of energy. This is the memory bandwidth bottleneck, sometimes loosely called the Von Neumann bottleneck or the "memory wall."Taalas sidesteps this wall entirely. They just engraved the 32 layers of Llama 3.1 sequentially on a chip. Essentially, the model's weights are physical transistors etched into the silicon.Importantly, they also claim to have invented a hardware scheme where they can store a 4-bit data and perform the multiplication related to it using a single transistor. I will refer it as their 'magic multiplier'Now, when the user's input arrives, it gets converted into a vector, and flows into physical transistors making up Layer1. It does multiplication via their 'magic multiplier' and instead of result being saved in a VRAM, the electrical signal simply flows down physical wires into the Layer 2 transistors (via pipeline registers from what I understand). The data streams continuously through the silicon until the final output token is generated.So, they don't use any RAM?They don't use external DRAM/HBM, but they do use a small amount of on-chip SRAM.
Why SRAM? Due to cost and complexity, manufacturers don't mix DRAM and logic gates. That's why GPUs have separate VRAM. (Also SRAM isn't facing supply chain crisis, DRAM is). 
Taalas uses this on-chip SRAM for the KV Cache (the temporary memory/context window of an ongoing conversation) and to hold LoRA adapters for fine tuning.But isn't fabricating a custom chip for every model super expensive?Technically yes, I read lots of comments saying that.
But Taalas designed a base chip with a massive, generic grid of logic gates and transistors. To map a specific model onto the chip, they only need to customize the top two layers/masks. While it's still slow, but it's much faster than building chips from ground up. 
It took them two months, to develop chip for Llama 3.1 8B. In the AI world where one week is a year, it's super slow. But in a world of custom chips, this is supposed to be insanely fast.As someone stuck running local models on a laptop without a massive GPU, I am keeping my fingers crossed for this type of hardware to be mass-produced soon.]]></content:encoded></item><item><title>Cloudflare outage on February 20, 2026</title><link>https://blog.cloudflare.com/cloudflare-outage-february-20-2026/</link><author>nomaxx117</author><category>hn</category><pubDate>Sat, 21 Feb 2026 19:05:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[On February 20, 2026, at 17:48 UTC, Cloudflare experienced a service outage when a subset of customers who use Cloudflare’s Bring Your Own IP (BYOIP) service saw their routes to the Internet withdrawn via Border Gateway Protocol (BGP).The issue was not caused, directly or indirectly, by a cyberattack or malicious activity of any kind. This issue was caused by a change that Cloudflare made to how our network manages IP addresses onboarded through the BYOIP pipeline. This change caused Cloudflare to unintentionally withdraw customer prefixes.For some BYOIP customers, this resulted in their services and applications being unreachable from the Internet, causing timeouts and failures to connect across their Cloudflare deployments that used BYOIP. The website for Cloudflare’s recursive DNS resolver (1.1.1.1) saw 403 errors as well. The total duration of the incident was 6 hours and 7 minutes with most of that time spent restoring prefix configurations to their state prior to the change.Cloudflare engineers reverted the change and prefixes stopped being withdrawn when we began to observe failures. However, before engineers were able to revert the change, ~1,100 BYOIP prefixes were withdrawn from the Cloudflare network. Some customers were able to restore their own service by using the Cloudflare dashboard to re-advertise their IP addresses. We resolved the incident when we restored all prefix configurations.We are sorry for the impact to our customers. We let you down today. This post is an in-depth recounting of exactly what happened and which systems and processes failed. We will also outline the steps we are taking to prevent outages like this from happening again.How did the outage impact customers?This graph shows the amount of prefixes advertised by Cloudflare during the incident to a BGP neighbor, which correlates to impact as prefixes that weren’t advertised were unreachable on the Internet:Out of the total 6,500 prefixes advertised to this peer, 4,306 of those were BYOIP prefixes. These BYOIP prefixes are advertised to every peer and represent all the BYOIP prefixes we advertise globally.   During the incident, 1,100 prefixes out of the total 6,500 were withdrawn from 17:56 to 18:46 UTC. Out of the 4,306 total BYOIP prefixes, 25% of BYOIP prefixes were unintentionally withdrawn. We were able to detect impact on one.one.one.one and revert the impacting change before more prefixes were impacted. At 19:19 UTC, we published guidance to customers that they would be able to self-remediate this incident by going to the Cloudflare dashboard and re-advertising their prefixes.Cloudflare was able to revert many of the advertisement changes around 20:20 UTC, which caused 800 prefixes to be restored. There were still ~300 prefixes that were unable to be remediated through the dashboard because the service configurations for those prefixes were removed from the edge due to a software bug. These prefixes were manually restored by Cloudflare engineers at 23:03 UTC. This incident did not impact all BYOIP customers because the configuration change was applied iteratively and not instantaneously across all BYOIP customers. Once the configuration change was revealed to be causing impact, the change was reverted before all customers were affected. The impacted BYOIP customers first experienced a behavior called . In this state, end user connections traverse networks trying to find a route to the destination IP. This behavior will persist until the connection that was opened times out and fails. Until the prefix is advertised somewhere, customers will continue to see this failure mode. This loop-until-failure scenario affected any product that uses BYOIP for advertisement to the Internet. Additionally, visitors to one.one.one.one, the website for Cloudflare’s recursive DNS resolver, were met with HTTP 403 errors and an “Edge IP Restricted” error message. DNS resolution over the 1.1.1.1 Public Resolver, including DNS over HTTPS, was not affected. A full breakdown of the services impacted is below.There was also a set of customers who were unable to restore service by toggling the prefixes on the Cloudflare dashboard. As engineers began reannouncing prefixes to restore service for these customers, these customers may have seen increased latency and failures despite their IP addresses being advertised. This was because the addressing settings for some users were removed from edge servers due an issue in our own software, and the state had to be propagated back to the edge. We’re going to get into what exactly broke in our addressing system, but to do that we need to cover a quick primer on the Addressing API, which is the underlying source of truth for customer IP addresses at Cloudflare.Cloudflare’s Addressing APIThe Addressing API is an authoritative dataset of the addresses present on the Cloudflare network. Any change to that dataset is immediately reflected in Cloudflare's global network. While we are in the process of improving how these systems roll out changes as a part of , today customers can configure their IP addresses by interacting with public-facing APIs which configure a set of databases that trigger operational workflows propagating the changes to Cloudflare’s edge. This means that changes to the Addressing API are immediately propagated to the Cloudflare edge.Advertising and configuring IP addresses on Cloudflare involves several steps:Customers signal to Cloudflare about advertisement/withdrawal of IP addresses via the Addressing API or BGP ControlThe Addressing API instructs the machines to change the prefix advertisementsBGP will be updated on the routers once enough machines have received the notification to update the prefixFinally, customers can configure Cloudflare products to use BYOIP addresses via  which will assign products to these rangesThe Addressing API allows us to automate most of the processes surrounding how we advertise or withdraw addresses, but some processes still require manual actions. These manual processes are risky because of their close proximity to Production. As a part of , one of the goals of remediation was to remove manual actions taken in the Addressing API and replace them with safe workflows.How did the incident occur?The specific piece of configuration that broke was a modification attempting to automate the customer action of removing prefixes from Cloudflare’s BYOIP service, a regular customer request that is done manually today. Removing this manual process was part of our Code Orange: Fail Small work to push all changes toward safe, automated, health-mediated deployment. Since the list of related objects of BYOIP prefixes can be large, this was implemented as part of a regularly running sub-task that checks for BYOIP prefixes that should be removed, and then removes them. Unfortunately, this regular cleanup sub-task queried the API with a bug.Here is the API query from the cleanup sub-task: resp, err := d.doRequest(ctx, http.MethodGet, `/v1/prefixes?pending_delete`, nil)
And here is the relevant part of the API implementation:	if v := req.URL.Query().Get("pending_delete"); v != "" {
		// ignore other behavior and fetch pending objects from the ip_prefixes_deleted table
		prefixes, err := c.RO().IPPrefixes().FetchPrefixesPendingDeletion(ctx)
		if err != nil {
			api.RenderError(ctx, w, ErrInternalError)
			return
		}

		api.Render(ctx, w, http.StatusOK, renderIPPrefixAPIResponse(prefixes, nil))
		return
	}
Because the client is passing pending_delete with no value, the result of Query().Get(“pending_delete”) here will be an empty string (“”), so the API server interprets this as a request for all BYOIP prefixes instead of just those prefixes that were supposed to be removed. The system interpreted this as all returned prefixes being queued for deletion. The new sub-task then began systematically deleting all BYOIP prefixes and all of their related dependent objects including , until the impact was noticed, and an engineer identified the sub-task and shut it down.Why did Cloudflare not catch the bug in our staging environment or testing?Our staging environment contains data that matches Production as closely as possible, but was not sufficient in this case and the mock data we relied on to simulate what would occur was insufficient. In addition, while we have tests for this functionality, coverage for this scenario in our testing process and environment was incomplete. Initial testing and code review focused on the BYOIP self-service API journey and were completed successfully. While our engineers successfully tested the exact process a customer would have followed, testing did not cover a scenario where the task-runner service would independently execute changes to user data without explicit input.Affected BYOIP prefixes were not all impacted in the same way, necessitating more intensive data recovery steps. As a part of Code Orange: Fail Small, we are building a system where operational state snapshots can be safely rolled out through health-mediated deployments. In the event something does roll out that causes unexpected behavior, it can be very quickly rolled back to a known-good state. However, that system is not in Production today.BYOIP prefixes were in different states of impact during this incident, and each of these different states required different actions:Most impacted customers only had their prefixes withdrawn. Customers in this configuration could go into the dashboard and toggle their advertisements, which would restore service. Some customers had their prefixes withdrawn and some bindings removed. These customers were in a partial state of recovery where they could toggle some prefixes but not others.Some customers had their prefixes withdrawn and all service bindings removed. They could not toggle their prefixes in the dashboard because there was no  (Magic Transit, Spectrum, CDN) bound to them. These customers took the longest to mitigate, as a global configuration update had to be initiated to reapply the service bindings for all these customers to every single machine on Cloudflare’s edge.How does this incident relate to Code Orange: Fail Small?The change we were making when this incident occurred is part of the Code Orange: Fail Small initiative, which is aimed at improving the resiliency of code and configuration at Cloudflare. As a brief primer of the  initiatives, the work can be divided into three buckets:Require controlled rollouts for any configuration change that is propagated to the network, just like we do today for software binary releases.Change our internal “break glass” procedures and remove any circular dependencies so that we, and our customers, can act fast and access all systems without issue during an incident.Review, improve, and test failure modes of all systems handling network traffic to ensure they exhibit well-defined behavior under all conditions, including unexpected error states.The change that we attempted to deploy falls under the first bucket. By moving risky, manual changes to safe, automated configuration updates that are deployed in a health-mediated manner, we aim to improve the reliability of the service.Critical work was already ongoing to enhance the Addressing API's configuration change support through staged test mediation and better correctness checks. This work was ongoing in parallel with the deployed change. Although preventative measures weren't fully deployed before the outage, teams were actively working on these systems when the incident occurred. Following our Code Orange: Fail Small promise to require controlled rollouts of any change into Production, our engineering teams have been reaching deep into all layers of our stack to identify and fix all problematic findings. While this outage wasn't itself global, the blast radius and impact were unacceptably large, further reinforcing Code Orange: Fail Small as a priority until we have re-established confidence in all changes to our network being as gradual as possible. Now let’s talk more specifically about improvements to these systems.API schema standardizationOne of the issues in this incident is that the pending_delete flag was interpreted as a string, making it difficult for both client and server to rationalize the value of the flag. We will improve the API schema to ensure better standardization, which will make it much easier for testing and systems to validate whether an API call is properly formed or not. This work is part of the third Code Orange workstream, which aims to create well-defined behavior under all conditions.Better separation between operational and configured stateToday, customers make changes to the addressing schema that are persisted in an authoritative database, and that database is the same one used for operational actions. This makes manual rollback processes more challenging because engineers need to utilize database snapshots instead of rationalizing between desired and actual states. We will redesign the rollback mechanism and database configuration to ensure that we have an easy way to roll back changes quickly and also to introduce layers between customer configuration and Production.  We will snap shot the data that we read from the database and are applying to Production, and apply those snapshots in the same way that we deploy all our other Production changes, mediated by health metrics that can automatically stop the deployment if things are going wrong. This means that the next time we have a problem where the database gets changed into a bad state, we can near-instantly revert individual customers (or all customers) to a version that was working.While this will temporarily block our customers from being able to make direct updates via our API in the event of an outage, it will mean that we can continue serving their traffic while we work to fix the database, instead of being down for that time. This work aligns with the first and second Code Orange workstreams, which involves fast rollback and also safe, health-mediated deployment of configuration.Better arbitrate large withdrawal actionsWe will improve our monitoring to detect when changes are happening too fast or too broadly, such as withdrawing or deleting BGP prefixes quickly, and disable the deployment of snapshots when this happens. This will form a type of circuit breaker to stop any out-of-control process that is manipulating the database from having a large blast radius, like we saw in this incident.We also have some ongoing work to directly monitor that the services run by our customers are behaving correctly, and those signals can also be used to trip the circuit breaker and stop potentially dangerous changes from being applied until we have had time to investigate. This work aligns with the first Code Orange workstream, which involves safe deployment of changes.Below is the timeline of events inclusive of deployment of the change and remediation steps: We deeply apologize for this incident today and how it affected the service we provide our customers, and also the Internet at large. We aim to provide a network that is resilient to change, and we did not deliver on our promise to you. We are actively making these improvements to ensure improved stability moving forward and to prevent this problem from happening again.]]></content:encoded></item><item><title>Canvas_ity: A tiny, single-header &lt;canvas&gt;-like 2D rasterizer for C++</title><link>https://github.com/a-e-k/canvas_ity</link><author>PaulHoule</author><category>hn</category><pubDate>Sat, 21 Feb 2026 18:50:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Toyota’s hydrogen-powered Mirai has experienced rapid depreciation</title><link>https://carbuzz.com/toyota-mirai-massive-depreciation-one-year/</link><author>iancmceachern</author><category>hn</category><pubDate>Sat, 21 Feb 2026 18:09:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Personal Statement of a CIA Analyst</title><link>https://antipolygraph.org/statements/statement-038.shtml</link><author>grubbs</author><category>hn</category><pubDate>Sat, 21 Feb 2026 17:49:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I first took a polygraph when I applied to the CIA and went through the applicant screening process.To prepare for the test, I read  by David T. Lykken. The book described the use of control versus relevant questions as well as countermeasures such as butt-clenching. I had no desire to use countermeasures. I wasn't out to "beat" the test: I wanted to understand how it worked. A future colleague at the Agency advised me, "Spill your guts." I thought it was good advice, and I planned to follow it.I knew I was taking a risk in applying to the Agency. I worked as a defense contractor on a project for the CIA, so I already held CIA TS/SCI clearances. If I failed the polygraph, I could lose my clearances, and I might lose my job as well.I flew to Northern Virginia for two days of pre-employment screening. A bus took us from the hotel to a nondescript building in Vienna. The examiner was a young woman. She asked me to sign a consent form and told me not to talk about the polygraph with anyone else.In the pretest interview, the examiner asked, "Did you make any false statements on your application?" I said, "Yes. Under height and weight, I put 130 lb. I actually weigh 134 lbs." She laughed. Then she asked if I'd read about polygraphs. I said I'd just finished She claimed she'd never heard of it. I was surprised. It's an important book about her field, I would have thought all polygraphers knew of it.She wired me up, and the polygraph began. My hand turned purple, which hurt terribly. My body twitched from the exaggerated stillness the test required. Halfway through, the examiner left the room, saying she had to show the charts to her supervisor. I came to think of this as the What will it take to get you to buy the car? part of the test. I waited twenty minutes or so, resisting the urge to press my nose against the one-way mirror and peer into it through cupped hands.The examiner came back. "You're having a problem with one of the questions. Do you know which one?" I had no idea. I'd answered all of them truthfully. She said, "How about, 'Have you ever lied to your boss?'" I said I hadn't. She pressed me until I came up with an occasion when I'd passed my boss in the hall. She said, "How are you?" and I said, "Fine." But I wasn't fine, I was in the middle of a cancer scare.I failed the poly and was told to come back the next day. I couldn't understand why I hadn't passed. I'd spilled my guts, and I hadn't used countermeasures.On the bus back to the hotel, a woman was sobbing, "Do they count something less than $50 as theft?" I felt bad for her because she was crying, but I wondered why a petty thief thought she could get into the Agency.That evening, the other applicants went to the nearby Tysons Corner mall to go shopping. I didn't feel festive enough to join them so I withdrew to my room. I ordered from room service but couldn't eat my dinner. I sat by the window for hours, looking into the darkness. She'd seen something inside me I hadn't known about. I'd always thought I was a good person. Now I wasn't sure.The next morning, I rode back to the same crumbling building where I'd been polygraphed the day before. The examiner said, "Now that you've had a chance to think about it, is there anything you'd like to say?" He didn't need to ask me twice. "You bet there is. I did my part, now I expect you to do yours." It wasn't until late that afternoon, when I was waiting for my plane at Dulles, that I realized, "Is there anything you'd like to say?" does not mean, "Please tell us all our faults."The examiner wired me up. He began with what he called a calibration test. He took a piece of paper and wrote the numbers one through five in a vertical column. He asked me to pick a number. I picked three. He drew a square around the number three, then taped the paper to the back of a chair where I could see it. I was supposed to lie about having selected the number three.The test began. He asked, "Is the number one? Is the number two? Is the number three?" I said "No," and butt-clenched. "Nice strong response!" he said.I wasn't hiding anything, so I had no reason to use countermeasures. On the other hand, the analytical part of me enjoyed poking at the test to figure out how it worked. And I was still mad about having failed the previous day, so I was messing with him. Curiosity satisfied, I didn't try the butt-clench again, not on that day or ever.During the real test, the examiner said, "Your breathing is unnatural." He described a kid in little league who kept swinging the bat wrong, and then suddenly got it. If I just kept trying, I could get it too. It took almost four hours, but by the end of the session, he told me I'd passed. I'd just cleared the last hurdle to joining the CIA.I entered on duty and began a week of orientation. On the first morning, we introduced ourselves and said a few words about what we'd be doing at the Agency. Four guys sitting together at a table up front identified themselves as polygraphers. Everyone else in the room hissed. It wasn't friendly teasing, either. At lunch, no one would sit with them.A few years into my Agency career, I took a battery of vocational and aptitude tests including the MMPI, a personality inventory. The MMPI results came back, and they said I fell on the extreme end of the honesty spectrum, or in the words of the Agency psychiatrist, "You're honest to the point of being naïve." I was kind of offended. Who are you calling naïve? On the other hand, it was nice to have that in my official file.Five Year ReinvestigationCIA employees were required to take a polygraph every five years. We all did it, but there was a lot of complaining.I never heard that anyone worried about losing their job to the poly. It was said that new applicants failed in large numbers, but once you were in, you were in. The re-poly could be unpleasant, though. If you failed, you had to keep taking it. It was said that there was an upper manager who just couldn't pass, no matter how many times he tried. After something like seven attempts, Polygraph gave up and stopped calling him back. The manager remained in his job.We weren't supposed to discuss the polygraph among ourselves, but of course we did. When people came back from a poly, they talked about how it had gone. A woman who'd never seen an illegal drug in her life was accused of being a major drug user. Someone who hated computers so much that she had the secretary print out her emails so she could read them was interrogated for hours about hacking into Agency networks.A pattern emerged. In a normal polygraph, there was often a gross mismatch between a person and the accusations made against them. I don't think the officials at Polygraph had any idea how unintentionally humorous this was. Not to the person it happened to, of course, but the rest of us found it hysterically funny.Once, the examiner got in my face and shouted, "Admit it, you're deeply in debt. Creditors are pounding on your door!" I said. "You've just revealed to me that you haven't bothered to pull my credit report. Are you lazy, or are you cheap?" I offered to pay the $15 fee myself, but he didn't take me up on it.Another time, the examiner accused me of working for a foreign intelligence service and traveling overseas to meet my handler. I rolled my eyes. "Do you want to see my passport? It's been expired for nine years." No, he didn't want to see my passport.I told my office mates I'd figured out why the accusations were so consistently off-the-wall. Polygraph must have a question of the day. Everyone who went in on Monday would be accused of dealing drugs. Tuesday was espionage day, Wednesday was marital infidelity day, and so on.Then Aldrich Ames was arrested, and polygraphs became more brutal. People who'd never had trouble passing were being called back two and three times. Thank you, Mr. Ames.I overheard a fragment of conversation at CIA Headquarters, "I thought I was a good person, but after that last poly, I'm not so sure." It was hard on people.Because of Ames, the Agency introduced a policy of random polygraphs. I knew someone who completed a five year poly. A few months later, he was called back for a random one.I'd been at the Agency for ten years when I went through the reinvestigation poly again.The test was administered by an inexperienced young woman. In the pretest interview, she asked me a question. I answered truthfully. She asked again as if she was looking for a better answer.  It made me furious.Well into the session, she said I was failing. I was so frustrated, I started to cry. I knew I could pass it if I just had enough time, but she had to run an errand. She failed me and ended the session early.I wrote a letter of complaint to the Chief of Polygraph saying I didn't like the way I'd been treated. He sent me a letter of apology. He said he'd reviewed the tapes and that he was sorry about the abuse. He said the polygrapher had been reprimanded.I was surprised to get a response of any kind, but a letter of apology was astonishing. Although the cynical part of me might call it a "please don't sue me" letter.But I was also puzzled. The apology was for the wrong thing. The Director seemed to think I'd gone through a particularly abusive polygraph. I hadn't. It was a perfectly ordinary polygraph, no different from any other. I just wrote to say that I didn't like polygraphs.I had to take the test again because I'd failed the first one. The second examiner was experienced and had a mild disposition. I passed without difficulty.Over the course of many years at CIA, I formed an impression that a typical polygraph involves an inexperienced examiner who grills you harshly and then fails you, followed by a re-poly with a more experienced examiner who guides you through it with no fuss. I've had two polygraphs in which I passed on the first try. On both occasions, an experienced examiner conducted the test.I worked at CIA for eleven years. It was a terrific experience, and I count those years as among the happiest in my life. I left only because I got married and had a baby. CIA is many things, but family friendly is not one of them.I joined a small defense contractor known for its work-life balance. The company supported most of the three-letter agencies, and I settled into doing the same sort of work I'd done before.My first assignment was on a National Reconnaissance Office (NRO) project. The NRO clearances required a poly, which I agreed to. The test was administered by a woman with many years' experience. She told me I'd passed. It's possible to have a polygraph that isn't confrontational and doesn't leave you feeling violated. It's rare, though.I'd been supporting an FBI project for several years when the phone rang in the kitchen. Someone from the FBI asked me to come in for a routine polygraph, a requirement to keep my clearances up to date.To prepare, I read the 2002 National Academy of Sciences report. It was an eye-opener, even though it confirmed what I'd already begun to suspect, that the polygraph didn't work.I arrived for my polygraph appointment, which would be administered in a building across the street from my children's orthodontist.I stood in the marble lobby, waiting for the examiner to come and collect me. The 2002 NAS report made me cynical. In the interview room, I thought, "Don't look at the man behind the curtain."The examiner asked if I'd ever visited the anti-polygraph sites online. I said yes, that's where I found the 2002 NAS report. He said he'd never heard of it. He also said there was no such a thing as a control question. I hate being lied to; it makes me angry.In the pretest interview, he asked how many polygraphs I'd had before this one. I wasn't sure, but I thought it was probably six or seven. He asked what medications I took. I listed everything, including a cortisone cream for a patch of eczema on my hand. He went on and on about my skin condition. What does this have to do with national security, and why is it any of your business? Maybe violating people's boundaries is a way to establish dominance.The examiner wired me up, and we did the card trick. He drew a vertical column of numbers, told me to pick one, drew a box around it, and pinned it up where I could see it.It occurred to me that we were playing a guessing game in which the examiner knew the answer before the game began. I'd have been more impressed if he'd had to discover my number using only the polygraph equipment and/or his ability to read people. I was tempted to suggest it, but I didn't think the idea would be well received.The test proceeded normally. The examiner left the room. When he came back, he didn't meet my eyes, and the muscles of his face were tight. "The test shows deception." He was right. I had been deceptive, but only about one thing. I hadn't told him I knew the polygraph didn't work.The examiner hammered on me to come clean. I kept repeating, "No, I can't think of anything else." I was tempted to make something up, just to make it stop, but I'm not comfortable lying.At the end of the interview, the examiner looked at me, gloating. "You claim you've taken seven polygraphs before today, but later, you said it was only six." That's all you've got on me? I'm underwhelmed.Being able to recognize interrogation techniques didn't make me immune to them. Exhausted, I hung my head, feeling like he'd broken me under interrogation. I've always found the shame of being broken was the worst thing about being polygraphed.A few days later, I was still seriously rattled. I didn't realize how badly the polygraph had affected me until I plowed through a red light and almost hit another car. I'd never run a red light before. I couldn't think what had gotten into me.I told a relative I'd had a really hard time with the polygraph. There was an embarrassed silence, followed by a rapid change of subject. What did you do wrong, and why don't you own up to it? This from someone who's known me from birth and has always taken my side. It shows how strongly people in our culture believe in the polygraph.I wrote to my congressman and asked him to support legislation banning the polygraph. I said it was a civil rights issue to subject an entire workforce to a brutal police-style interrogation in the absence of probable cause, especially if they might be harmed by it.Although I failed the FBI polygraph, I remained on the project.Much to my surprise, I was granted additional clearances and assigned to more highly classified work than I'd been doing before the polygraph.The work dried up, and I moved on to something else. Seven months after the failed FBI poly, I was summoned to FBI Headquarters "to talk about your polygraph."It took several hours to drive downtown and find a place to park. I found FBI headquarters. Two burly agents met me at the door. I wondered if they had the power to arrest me. My hands shook. It was like a scene from a movie where government officials are trying to be intimidating. It worked. As we walked across the lobby, I thought I was going to faint.They escorted me upstairs. Neither of them spoke. They took me to a small room. A folder lay open on a desk. Papers spilled from it.I said, "Does it matter that I was laid off the project four months ago?"It was like watching method actors break character. "We're sorry, we didn't mean to bring you all the way down here for nothing. Maybe you could visit the spy museum? It's right across the street."Years later, I joined a DIA project which required a CI (counterintelligence) polygraph. I liked the work and the people doing it, so I agreed.I started in the summer. In late September, Polygraph asked me to come in. I was no longer afraid of them. I didn't doubt the apparatus took accurate measurements of heart rate, blood pressure, respiration, and perspiration, but I'd stopped believing the examiner could infer a person's emotions or thoughts from the data in the tracings.I now knew the examiners worked from a script like the one in the handbook. On page 24, the script calls for empathetic listening to put the subject at ease. On page 53, it says to change gears and confront the subject with evidence of a crime. I thought that since I was familiar with the script, the polygrapher would lose his power over me.I arrived for the appointment. The examiner asked if I was nervous, and I said no (true). In the pretest interview, he asked if I'd visited the anti-polygraph websites to learn how to beat the test. I said I did read the sites, but I was looking for articles on psychological damage from interrogation and how to recover from it. Beating the polygraph was of no interest to me (true).The real test began. In my mind, I turned the pages of the script. The examiner excused himself and stepped outside. He returned, and whatever warmth there'd been in his manner had vanished. Oh c**p, we're on page 53. He accused me of deception. I hunkered down until it was over. I reminded myself, Whatever you do, don't make admissions. I didn't have anything to confess, but if the pressure were bad enough, I might be tempted to make things up.At the end of the test, the examiner told me I'd failed. But, and this is huge, for the first time I didn't leave the poly broken and weeping. I was annoyed, but I hadn't been harmed.Two weeks later, they asked me to come in for a retest. The examiner was different, but the test was the same. Everything went smoothly, and I assumed I'd passed. The examiner said he needed to run the chart by his supervisor, and I'd hear back later.Weeks went by, and then months. The computer account I would get as soon as I passed the poly was still in limbo, which was not a good sign.During the quiet time over the Christmas break, I came to believe I'd lost the ability to pass a polygraph. By this time, I'd failed three in a row, two for DIA and the one for FBI.I wondered if it was because I'd grown cynical. I now thought of the polygraph apparatus as a colander attached to a Xerox machine. Even so, I'd tried to cooperate. I'd answered their questions truthfully, and I hadn't used countermeasures. But I no longer feared the examiners, and I no longer broke under interrogation. According to the handbook, breaking the subject is an important part of the process.On the first day back from Christmas break, my department head stopped by to give me my annual salary review. It was my highest raise in five years. She said the people on my project liked my work, and they liked me.Later the same day, I got a call from Polygraph asking me to come in the next morning to answer the polygraph questions under oath rather than wired up.I had no problem with that. As a mentor at the Agency said about testifying before Congress, "Under oath or not under oath. Either way you're telling the truth, so what's the difference?"The two polys I'd already taken for DIA had been CI (counterintelligence). But halfway through, the examiner asked, "How often do you look at pornography?" I blinked with surprise. "Excuse me?" That question didn't belong on a CI poly, that was a Lifestyle question.When the interview ended, the examiner said, "I believe you" in a voice that lacked conviction. Then he added, "I'm going to try to get you another polygraph.""I don't want another." I meant it.Over the next few days, I jumped whenever the phone rang. Weeks went by before I began to relax. And then my long-delayed computer account was approved. As I understood how the system worked, the polygraph had just been adjudicated in my favor.A few days later, I was sitting at my desk eating lunch when a scheduling clerk from Polygraph called. "You missed your appointment this morning," he said. Next time, you might consider telling me about it ahead of time. He said, "Don't even worry about it. Everyone makes mistakes." My jaw dropped. He just called me a liar.The clerk wanted me to come in the next day. I was seriously rattled. I'd already decided I was not going to take another polygraph, ever. I stalled. And then I remembered my newly granted computer accesses. I was already cleared. Probably the paperwork hadn't caught up yet. I said I needed a few days to figure out if the poly was still necessary, and I would get back to him.I checked with Security. They said, "No, you haven't been adjudicated yet."I couldn't get out of it, but I could put it off until it was OBE (overtaken by events). In a few months, the project would move to a new location beyond my commuting range. I planned to stay until right before the move and then find something else. I didn't have to refuse the poly, I just had to conduct a delaying action.But Polygraph was insistent, and I wasn't sure I could hold them off much longer. I asked about withdrawing my application for DIA clearances, but was advised to watch and wait.Or, I could leave the project now and find other work. My TS/SCI from CIA was still active, but I knew that eventually CIA would want me to take a polygraph. I also held a Top Secret clearance from the Air Force. The Air Force TS, a "vanilla TS" (not SCI) was based on an SF-86 background investigation. It did not require a polygraph. And at the very worst, I could do unclassified work. My company had a large number of unclassified projects, and many had work for analysts.A few days after I'd told the clerk I'd get back to him, I gave a presentation about the work I'd been doing on my task. It was well received, and I stepped down from the podium covered in glory. As I left the meeting, the program manager pulled me aside and said my task had lost its funding. It was an occupational hazard of defense contracting, and not anything sinister.I wasn't happy about being cut from the project, but it did solve my polygraph dilemma. If I wasn't on the project anymore, I didn't need to apply for DI clearances, and I didn't need to take the polygraph.Around noon, the clerk from Polygraph called again. I told him I didn't have to take the poly anymore because I'd been laid off the project. In mid-afternoon, he called back. He said he'd made a few phone calls and learned that I hadn't been laid off from my company, after all. Wonderful, call me a liar.He pressed me to schedule another poly. I said no. His voice turned whiny. "You have to do it!" I dug in my heels. "I've already told you no." He slammed down the phone.I'd just refused a polygraph. I felt like Neville Longbottom when he drew the sword of Gryffindor and advanced on Lord Voldemort. I was filled with righteous indignation, and it gave me courage.For the rest of the day, I was peppered with emails and phone calls summoning me to the offices of upper managers. Some of them were so far up the chain, I'd never heard their names before. They were uniformly kind to me. I hadn't known it, but I wasn't the first person in our division to refuse the polygraph. Polygraph conscientious objectors—who knew that there was such a thing?Polygraph told my management I'd lied about being laid off. It caused a major flap. My project badges were taken from me, even the unclassified ones, and I was debriefed from all my DIA clearances.To their credit, DIA didn't tell any other agencies they'd taken my badge and debriefed me. Weeks later, my CIA clearances were still active.Six weeks went by. I put in for a few days of leave to take the kids to an alumni event at my university. I worked half a day and then went home to pack.Sometime after 4:00 pm, when I was loading the last suitcase into the car, the department secretary called to say I was late for a 4:00 meeting with my department head. This was the first I'd heard of it. The meeting had been put on the calendar after I'd left for the day. I said I was sorry I couldn't be there, but I'd be back in the office first thing on Monday.Just before close of business Monday, I was summoned to another meeting with the department head. When I arrived, my boss was sitting in her office with a woman from Human Resources. As a general rule, if you're boss wants to see you and HR has been asked to sit in, you know it's going to end badly.My department head put a piece of paper in front of me. It said I'd agreed to take a polygraph as a condition of working on the DIA project, but when they tried to schedule it, I canceled the appointment. As a result, I was cut from the project."No, I took two polygraphs. I turned down the third because I wasn't on the project anymore." Although I now thought of myself a polygraph conscientious objector, and I would have refused whether or not I was still on the project.The rest of the letter said the company would begin termination proceedings against me. I was eight years from retirement. I wasn't counting the days until retirement. I liked going to work. Even when I was between assignments and not getting paid, I still came into the office and put in a full day.I spoke to a lawyer. She said I lived in an "at will" state, which means employees can be dismissed for any (legal) reason, or for no reason at all. I was being terminated for refusing the polygraph, and it was legal.I decided to resign rather than fight.My mother loved being a doctor, but she discouraged us from applying to medical school. She said the cost of malpractice insurance had ruined the profession.Similarly, I urge my children to steer clear of any job that requires a polygraph. That rules out entire professions: National Security, Intelligence, Law Enforcement, Customs and Border Protection, and Pharmacy. It's a shame. My career as an intelligence analyst was exciting, deeply rewarding, and just plain fun. I contributed to the safety of my country, which is a constant source of pride. But I no longer recommend my profession to others. The polygraph has become a deal breaker.]]></content:encoded></item><item><title>What not to write on your security clearance form (1988)</title><link>https://milk.com/wall-o-shame/security_clearance.html</link><author>wizardforhire</author><category>hn</category><pubDate>Sat, 21 Feb 2026 17:08:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[as reported in the  list and RISKS 01 Apr 88  1620 PST Les Earnest  The "previous account" referred to in RISKS-6.51e-t-a-o-n-r-i Spy and the FBI

Reading a book got me into early trouble--I had an FBI record
by age twelve. This bizarre incident caused a problem much later
when I needed a security clearance. I learned that I could obtain
one only by concealing my sordid past.


A friend named Bob and I read the book ``Secret and Urgent,'' by
Fletcher Pratt [Blue Ribbon Books; Garden City, NY; 1942] which was
an early popular account of codes and ciphers. Pratt showed how to
use letter frequencies to break ciphers and reported that the most
frequently occurring letters in typical English text are
e-t-a-o-n-r-i, in that order. (The letter frequency order of the
story you are now reading is e-t-a-i-o-n-r. The higher frequency
of ``i'' probably reflects the fact that _I_ use the first person
singular a lot.) Pratt's book also treated more advanced
cryptographic schemes.


Bob and I decided that we needed to have a secure way to
communicate with each other, so we put together a rather elaborate
jargon code based on the principles described in the book. I don't
remember exactly why we thought we needed it--we spent much of
our time outside of school together, so there was ample time to
talk privately. Still, you never could tell when you might need to
send a secret message!


We made two copies of the code key (a description of how to encrypt
and decrypt our messages) in the form of a single typewritten
sheet. We each took a copy and carried it on our persons at all
times when we were wearing clothes.


I actually didn't wear clothes much. I spent nearly all my time
outside school wearing just a baggy pair of maroon swimming trunks.
That wasn't considered too weird in San Diego.


I had recently been given glasses to wear but generally kept them
in a hard case in the pocket of the trousers that I wore to school.
I figured that this was a good place to hide my copy of the code
key, so I carefully folded it to one-eighth of its original size
and stuck it at the bottom of the case, under my glasses.


Every chance I got, I went body surfing at Old Mission Beach. I
usually went by streetcar and, since I had to transfer Downtown, I
wore clothes. Unfortunately, while I was riding the trolley home
from the beach one Saturday, the case carrying my glasses slipped
out of my pocket unnoticed. I reported the loss to my mother that
night. She chastised me and later called the streetcar company.
They said that the glasses hadn't been turned in.


After a few weeks of waiting in vain for the glasses to turn up, we
began to lose hope. My mother didn't rush getting replacement
glasses in view of the fact that I hadn't worn them much and they
cost about $8, a large sum at that time. (To me, $8 represented 40
round trips to the beach by streetcar, or 80 admission fees to the
movies.)


Unknown to us, the case had been found by a patriotic citizen who
opened it, discovered the code key, recognized that it must belong
to a Japanese spy and turned it over to the FBI This was in
1943, just after citizens of Japanese descent had been forced off
their property and taken away to concentration camps. I remember
hearing that a local grocer was secretly a Colonel in the Japanese
Army and had hidden his uniform in the back of his store. A lot of
people actually believed these things.


About six weeks later, when I happened to be off on another
escapade, my mother was visited by a man who identified himself as
an investigator from the FBI (She was a school administrator,
but happened to be at home working on her Ph.D. dissertation.) She
noticed that there were two more men waiting in a car outside. The
agent asked a number of questions about me, including my
occupation. He reportedly was quite disappointed when he learned
that I was only 12 years old.


He eventually revealed why I was being investigated, showed my
mother the glasses and the code key and asked her if she knew where
it came from. She didn't, of course. She asked if we could get
the glasses back and he agreed.


My mother told the investigator how glad she was to get them back,
considering that they cost $8. He did a slow burn, then said
``Lady, this case has cost the government thousands of dollars. It
has been the top priority in our office for the last six weeks. We
traced the glasses to your son from the prescription by examining
the files of nearly every optometrist in San Diego.'' It apparently
didn't occur to them that if I were a  Japanese spy, I might
have brought the glasses with me from headquarters.


The FBI agent gave back the glasses but kept the code key ``for
our records.'' They apparently were not fully convinced that they
were dealing just with kids.


Since our communication scheme had been compromised, Bob and I
devised a new key. I started carrying it in my wallet, which I
thought was more secure. I don't remember ever exchanging any
cryptographic messages. I was always ready, though.


A few years later when I was in college, I got a summer job at the
Naval Electronics Lab, which required a security clearance. One of
the questions on the application form was ``Have you ever been
investigated by the FBI?'' Naturally, I checked ``Yes.'' The next
question was, ``If so, describe the circumstances.'' There was very
little space on the form, so I answered simply and honestly, ``I was
suspected of being a Japanese spy.''


When I handed the form in to the security officer, he scanned it
quickly, looked me over slowly, then said, ``Explain this''--pointing
at the FBI question. I described what had happened.
He got very agitated, picked up my form, tore it in pieces, and
threw it in the waste basket.


He then got out a blank form and handed it to me, saying ``Here,
fill it out again and don't mention that. If you do, I'll make
sure that you  get a security clearance.''


I did as he directed and was shortly granted the clearance. I
never again disclosed that incident on security clearance forms.


On another occasion much later, I learned by chance that putting
certain provocative information on a security clearance form can
greatly speed up the clearance process. But that is another story.


Edited and converted to HTML by Dan Bornstein.

]]></content:encoded></item><item><title>macOS&apos;s Little-Known Command-Line Sandboxing Tool (2025)</title><link>https://igorstechnoclub.com/sandbox-exec/</link><author>Igor_Wiwi</author><category>hn</category><pubDate>Sat, 21 Feb 2026 14:34:45 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CXMT has been offering DDR4 chips at about half the prevailing market rate</title><link>https://www.koreaherald.com/article/10679206</link><author>phront</author><category>hn</category><pubDate>Sat, 21 Feb 2026 14:32:16 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[CXMT halves DDR4 prices as YMTC gains ground in NAND, raising concerns over Korea’s legacy exposureSamsung Electronics and SK hynix are locked in a race to mass-produce sixth-generation high-bandwidth memory, but Chinese rivals are making gains elsewhere — flooding the legacy DRAM market with chips priced at roughly half the going rate.According to industry sources on Friday, China’s top DRAM manufacturer CXMT has been offering older-generation DDR4 chips at about half the prevailing market rate. The move comes as global supply shortages have driven prices sharply higher, allowing the company to aggressively push legacy products for mobile devices and PCs in a bid to boost market share.DDR4 remains a mainstay component in devices such as PCs and TVs, and have risen in price recently.Data from DRAMeXchange showed that as of end-January, the average fixed contract price of PC DRAM DDR4 8Gb stood at $11.50, up 23.7 percent from $9.30 a month ago. Compared with $1.35 a year earlier, the price has jumped more than eightfold. DRAM prices have climbed for 10 consecutive months, marking the highest level since the market tracker began compiling data in June 2016.Against this backdrop, cut-price Chinese chips are proving tempting. US hardware firms HP and Dell are reportedly conducting quality tests on CXMT’s DRAM, while Taiwan’s Asus and Acer have sought cooperation with Chinese partners. Signs are emerging that aggressive pricing is translating into demand.“Chinese firms are waging a volume-based strategy starting with general-purpose memory, backed by state subsidies and domestic demand from AI servers and locally developed GPUs,” said an industry source who requested anonymity. “As Korean companies concentrate on HBM4, there are visible cracks emerging in (their hold on) the legacy market.”The challenge for Korean chipmakers is that the legacy segment still accounts for a significant portion of their earnings. More than half of the total DRAM production capacity at both Samsung and SK hynix is understood to be allocated to general-purpose products. Even if they maintain leadership in HBM4, a deepening erosion of the mainstream market could eventually weigh on profitability.Chinese players, meanwhile, are not limiting their push to low-cost volume sales. The cash and know-how gained from legacy chips is funding a push into higher-end products.CXMT is in the process of converting wafer capacity equivalent to about 20 percent of its total DRAM output — some 60,000 wafers per month — at its Shanghai plant to the fourth-generation HBM3 chip production. The possibility of expanding into post-HBM3E products is also being discussed.The Shanghai facility is believed to have production capacity two to three times larger than the company’s headquarters plant in Hefei. Equipment installation is expected to be completed in the second half of this year, with mass production slated for next year. Although HBM3 and the fifth-generation HBM3E chips trail HBM4 in performance, they remain widely used in AI data centers.China’s advance is not confined to DRAM. YMTC has been gaining traction in the NAND flash sector as well, capitalizing on competitively priced mobile products. The company recorded a 10 percent share of the global NAND market for the first time last year, and momentum is widely expected to continue.YMTC is currently building a third fabrication plant in Wuhan, targeting operations next year. Half of the facility’s production capacity is to be allocated to DRAM. It will initially focus on legacy DRAM products, with the possibility of expanding into HBM production in partnership with local assembly firms. Industry sources say the pattern is familiar — build scale in legacy DRAM, then move up the value chain.“At this stage, Chinese manufacturers are relying on aggressive pricing to build scale in legacy DRAM,” the anonymous source said. “But over time, the technology gap may narrow more quickly than expected. Even if Korean firms maintain leadership in HBM, neglecting the mainstream segment could weigh on profitability in the longer run.”yeeun@heraldcorp.com]]></content:encoded></item><item><title>zclaw: personal AI assistant in under 888 KB, running on an ESP32</title><link>https://github.com/tnm/zclaw</link><author>tosh</author><category>hn</category><pubDate>Sat, 21 Feb 2026 12:37:52 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Andrej Karpathy talks about &quot;Claws&quot;</title><link>https://simonwillison.net/2026/Feb/21/claws/</link><author>helloplanets</author><category>hn</category><pubDate>Sat, 21 Feb 2026 09:53:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[. Andrej Karpathy tweeted a mini-essay about buying a Mac Mini ("The apple store person told me they are selling like hotcakes and everyone is confused") to tinker with Claws:I'm definitely a bit sus'd to run OpenClaw specifically [...] But I do love the concept and I think that just like LLM agents were a new layer on top of LLMs, Claws are now a new layer on top of LLM agents, taking the orchestration, scheduling, context, tool calls and a kind of persistence to a next level.Looking around, and given that the high level idea is clear, there are a lot of smaller Claws starting to pop out. For example, on a quick skim NanoClaw looks really interesting in that the core engine is ~4000 lines of code (fits into both my head and that of AI agents, so it feels manageable, auditable, flexible, etc.) and runs everything in containers by default. [...]Anyway there are many others - e.g. nanobot, zeroclaw, ironclaw, picoclaw (lol @ prefixes). [...]Not 100% sure what my setup ends up looking like just yet but Claws are an awesome, exciting new layer of the AI stack.Andrej has an ear for fresh terminology (see vibe coding, agentic engineering) and I think he's right about this one, too: "" is becoming a term of art for the entire category of OpenClaw-like agent systems - AI agents that generally run on personal hardware, communicate via messaging protocols and can both act on direct instructions and schedule tasks.It even comes with an established emoji 🦞Posted 21st February 2026 at 12:37 am]]></content:encoded></item><item><title>EU mandates replaceable batteries by 2027 (2023)</title><link>https://environment.ec.europa.eu/news/new-law-more-sustainable-circular-and-safe-batteries-enters-force-2023-08-17_en</link><author>cyrusmg</author><category>hn</category><pubDate>Sat, 21 Feb 2026 08:28:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[A new law to ensure that batteries are collected, reused and recycled in Europe is entering into force today. The new Batteries Regulation will ensure that, in the future, batteries have a low carbon footprint, use minimal harmful substances, need less raw materials from non-EU countries, and are collected, reused and recycled to a high degree in Europe. This will support the shift to a circular economy, increase security of supply for raw materials and energy, and enhance the EU’s strategic autonomy.In line with the circularity ambitions of the European Green Deal, the Batteries Regulation is the first piece of European legislation taking a full life-cycle approach in which sourcing, manufacturing, use and recycling are addressed and enshrined in a single law.Batteries are a key technology to drive the green transition, support sustainable mobility and contribute to climate neutrality by 2050. To that end, starting from 2025, the Regulation will gradually introduce declaration requirements, performance classes and maximum limits on the carbon footprint of electric vehicles, light means of transport (such as e-bikes and scooters) and rechargeable industrial batteries.The Batteries Regulation will ensure that batteries placed on the EU single market will only be allowed to contain a restricted amount of harmful substances that are necessary. Substances of concerns used in batteries will be regularly reviewed. Targets for recycling efficiency, material recovery and recycled content will be introduced gradually from 2025 onwards. All collected waste batteries will have to be recycled and high levels of recovery will have to be achieved, in particular of critical raw materials such as cobalt, lithium and nickel. This will guarantee that valuable materials are recovered at the end of their useful life and brought back in the economy by adopting stricter targets for recycling efficiency and material recovery over time.Starting in 2027, consumers will be able to remove and replace the portable batteries in their electronic products at any time of the life cycle. This will extend the life of these products before their final disposal, will encourage re-use and will contribute to the reduction of post-consumer waste.To help consumers make informed decisions on which batteries to purchase, key data will be provided on a label. A QR code will provide access to a digital passport with detailed information on each battery that will help consumers and especially professionals along the value chain in their efforts to make the circular economy a reality for batteries.Under the new law’s due diligence obligations, companies must identify, prevent and address social and environmental risks linked to the sourcing, processing and trading of raw materials such as lithium, cobalt, nickel and natural graphite contained in their batteries.  The expected massive increase in demand for batteries in the EU should not contribute to an increase of such environmental and social risks.Work will now focus on the application of the law in the Member States, and the redaction of secondary legislation (implementing and delegated acts) providing more detailed rules.Demand for batteries is increasing rapidly. It is set to increase 14-fold globally by 2030 and the EU could account for 17% of that demand. This is mostly driven by the electrification of transport. Such exponential growth in demand for batteries will lead to an equivalent increase in demand for raw materials, hence the need to minimise their environmental impact.In 2017, the Commission launched the European Battery Alliance to build an innovative, sustainable and globally competitive battery value chain in Europe, and ensure supply of batteries needed for decarbonising the transport and energy sectors.]]></content:encoded></item><item><title>Coccinelle: Source-to-source transformation tool</title><link>https://github.com/coccinelle/coccinelle</link><author>anon111332142</author><category>hn</category><pubDate>Sat, 21 Feb 2026 08:26:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI uBlock Blacklist</title><link>https://github.com/alvi-se/ai-ublock-blacklist</link><author>rdmuser</author><category>hn</category><pubDate>Sat, 21 Feb 2026 08:10:49 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Acme Weather</title><link>https://acmeweather.com/blog/introducing-acme-weather</link><author>cryptoz</author><category>hn</category><pubDate>Sat, 21 Feb 2026 07:13:38 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Fifteen years ago, we started work on the Dark Sky weather app.Over the years it went through numerous iterations — including more than one major redesign — as we worked our way through the process of learning what makes a great weather app. Eventually, in time, it was acquired by Apple, where the forecast and some core features were incorporated into Apple Weather.We enjoyed our time at Apple. So why did we leave to start another weather company?It’s simple: when looking at the landscape of the countless weather apps out there, many of them lovely, we found ourselves feeling unsatisfied. The more we spoke to friends and family, the more we heard that many of them did too. And, of course, we missed those days as a small scrappy shop.Our biggest pet peeve with most weather apps is how they deal (or rather, don’t deal) with forecast uncertainty. It is a simple fact that no weather forecast will ever be 100% reliable: the weather is moody, fickle, and chaotic. Forecasts are often wrong.Understanding this uncertainty is crucial for planning your day. Most weather apps will give you their single best guess, leaving you to wonder how sure they actually are, and what else might happen instead. Will it actually start raining at 9am, or might it end up pushed off until noon? Will there be rain or snow? How sure are you? You can’t plan your day if you don’t know how much you can trust the forecast, or know what other possibilities might arise. Rather than pretending we will always be right, Acme Weather embraces the idea that our forecast will sometimes be wrong. We address this uncertainty in several ways:Alternate Possible FuturesOur homegrown forecasts are produced using many different data sources, including numerical weather prediction models, satellite data, ground station observations, and radar data. Most of the time, our forecast will be a reliable source of information (it’s better than the one we had at Dark Sky). But, crucially, we supplement the main forecast with a spread of alternate predictions. These are additional forecast lines that capture a range of alternate possible outcomes:This accomplishes a couple things:First, the spread of the lines offers a sort of intuition as to how reliable the forecast is. Take the two forecasts below. In the first, the alternate predictions are tightly focused and the forecast can be considered robust and reliable. In the second, there is a significant spread, which is an indication that something is up and the forecast may be subject to change. It’s a call to action to check other conditions or maps, or come back to the app more frequently:Over time, you build up an intuitive sense of just how much you can actually trust the forecast. After using this for the past six months, I never want to go back to a single forecast again!Second, it simply shows what else might plausibly happen. In what time range might the storm arrive? Will the snow hit early, or might it be delayed and turn mostly to rain? When the weather is changing rapidly, predictions can become less reliable. We’ll show you different possible futures, so you can be better informed.Alternative Forecasts are designed to help make better hour-to-hour or day-by-day decisions. Community reports are intended to help with real-time weather events. Current conditions often evolve quickly during storms, radar is imperfect and can fail to detect precipitation during light rain, snow versus freezing rain can be volatile, etc.To address this, we’ve built a feature that allows any user to submit a report of the current conditions near them, which can be seen on the map:You can choose from a pre-selected list of weather condition icons (or even a list of emojis for when it’s feeling particularly 💩 out). There’s nothing more reliable than when a person nearby tells you what’s happening, so if there are recent reports near you we’ll flag it in the app.We absolutely love maps. They provide the context around the weather. Sure, the forecast may say you’ll get rain, but a map will complete the picture by showing you the full breadth of the storm, where it will hit, and where you are relative to it.We’ve built a large number of maps, including radar and lightning, rain and snow totals (why do most other weather apps not offer this?), wind, temperature and humidity, cloud cover, hurricane tracks, etc. While you can explore them all in the map tab, we also make sure to embed the most relevant maps directly inside the forecasts to provide a contextual backdrop.A weather app is only useful if you remember to check it. I’ve lost count of the number of times I’ve gotten stuck in the rain — not because the forecast was wrong, but because I simply didn’t check the app.The solution? A comprehensive set of weather notifications. Turn them on, and no longer worry about missing important weather events.Our notifications include everything from down-to-the-minute rain warnings, to government severe weather alerts, nearby lightning, community reports, and even whether or not a rainbow might be visible outside your house.We also let you create custom notifications, tailored to whatever you care about. Want to know if it’ll be windy, or if the UV index will be high, or if you’ll get heavy rain in the next 24 hours? We’ve got you covered.A weather app shouldn’t just be about helping you avoid bad weather; it should also be fun! There’s a wide world of meteorological phenomena that we would like to highlight. As such, we’re launching “Acme Labs”, which is a set of experimental tools inside the app to highlight fun and interesting things happening where you are.We’re starting with a couple initial features, including: , where we bring our hyperlocal rain forecasts to bear to pinpoint where rainbows are occuring right now, and , where we'll let you know if the sunset will be particularly lovely this evening.We take your privacy very seriously. Please review our privacy policy, but here is our philosophy in a nutshell:We won’t collect any information other than what is necessary to provide the service you’re paying for. This information will only be used to provide that service, and for nothing else.We won’t save or store any information that isn’t necessary. We don’t want or need your location history, for example, and simply not storing it in the first place means it can’t fall into the wrong hands.We will never sell or give your information to third parties, such as advertisers. We make our money directly from our customers.We don’t use third-party trackers or analytics services, because we can’t guarantee what they will or won’t do with the information we send them.Well, that’s Acme Weather. We’ve been making weather apps for 15 years, from Dark Sky to Apple, and this is the culmination (the acme?) of everything we’ve learned along the way. It’s the weather app we’ve always wanted, and always wanted to build.Acme Weather is We think you’ll love it. So try it out, and let us know what you think!]]></content:encoded></item><item><title>I verified my LinkedIn identity. Here&apos;s what I handed over</title><link>https://thelocalstack.eu/posts/linkedin-identity-verification-privacy/</link><author>ColinWright</author><category>hn</category><pubDate>Sat, 21 Feb 2026 07:06:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Meta Deployed AI and It Is Killing Our Agency</title><link>https://mojodojo.io/blog/meta-is-systematically-killing-our-agency/</link><author>zenincognito</author><category>hn</category><pubDate>Sat, 21 Feb 2026 04:29:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[We manage millions of dollars in annual Meta ad spend. Not thousands. Millions. Our retail clients grow their businesses through Meta Ads, and for a lot of them, it’s their single most important growth channel. We are, by any reasonable definition, a high-value customer.And yet, for the past several months, Meta has been treating us like we don’t exist.It’s genuinely one of the more absurd things we’ve experienced running an agency in two decades. We hire a senior Paid Ads Specialist. They set up a dedicated work account, which, by the way, is standard professional practice. Keeping work and personal accounts separate is basic data hygiene, not a red flag. We upload their government ID, Drivers Licence, Birth certificate for mandatory identity verification. We even do a face scan. Then, somewhere between five minutes and ten hours later, the account gets instantly banned.We have done this with multiple specialists and social media managers now. Every single one banned. Before they’ve even opened an ad account or posted a single piece of content.You might be thinking there’s something suspicious going on. Maybe a pattern in the logins. Maybe something in the account setup that looks off. Maybe we’re leaving something out. Meta’s security systems exist for a reason, and when someone says “we keep getting banned,” the natural assumption is that they’re doing something to deserve it.But here’s the thing. We have been advertising on Facebook since the platform first opened its doors to advertisers, back in 2008. We have spent millions of dollars on it over that time. Our agency history, our billing history, our business identity – it’s all there. There is no shady pattern. There is no hidden behaviour. There is just a broken automated system that cannot distinguish between a bot farm and a professional who created a work account on their first day.And when we try to fix it? That’s where it gets truly circular. Meta’s standard response is to file an appeal through the Account Quality dashboard. Sounds reasonable, until you realise that the appeal tool is inside the platform – the same platform the specialist is completely locked out of. You cannot appeal a login ban from behind a login screen. We’ve tried everything. Every forum thread, every concierge support contact, every support line we could find. The answers we get back are remarkably consistent: “just create a new account” or “file an appeal.” So we create a new account. That one gets banned too. There is no clean slate. There is just the same broken automated system flagging the same professional as a threat, over and over again.The business impact of this is real and it compounds fast. Staff, who have done nothing wrong, end up with a black mark attached to their identity documents across multiple banned accounts. And we’re left trying to run campaigns and manage brand pages on a platform that could, at any moment, eliminate a key team member for no legitimate reason.We never have had this problem with Google or even TikTok.What we as the enabler of this platform want a simple fix. A manual onboarding pathway for verified agencies that doesn’t collapse the moment an automated system makes a bad call. A human-accessible support channel for login failures, someone who can actually look at an account and click override. And an honest acknowledgement from Meta that their automated identity verification is producing false positives at scale for professional users.If you’re an agency owner who has hit the same wall, please share this. We’re not the only ones, and the only way this gets fixed is if someone at Meta with actual authority decides it’s worth looking at.If you work at Meta and are reading this, please email us at hello (at) our domain name.: This blew up on hacker news. We have contacted Meta support dozens of times over the past 30 days. Each representative confirmed that account creation and monitoring are now handled almost entirely by AI. Despite successfully completing required face verifications, our accounts continue to be flagged and banned by automated parameters. There is currently no path for manual intervention; even internal support tickets are failing to reach human reviewers with the authority to resolve these systemic blocks.: The top comment says  – “Standard professional practice invented by ad agencies themselves, against Facebook’s policy[0].”[0]: https://www.facebook.com/help/975828035803295/ — Just so that we are clear, even the personal facebook accounts are getting banned following the AI ban of work accounts. Personal accounts have been in existence in some cases for well over a year.]]></content:encoded></item><item><title>What Is OAuth?</title><link>https://leaflet.pub/p/did:plc:3vdrgzr2zybocs45yfhcr6ur/3mfd2oxx5v22b</link><author>cratermoon</author><category>hn</category><pubDate>Sat, 21 Feb 2026 01:33:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[recently asked a question about OAuthdifferent questionOpenID ConnectOAuthFortunately, the charging one has been solved now that we&#39;ve all standardized on mini-USB. Or is it micro-USB? Shit.]]></content:encoded></item><item><title>Cord: Coordinating Trees of AI Agents</title><link>https://www.june.kim/cord</link><author>gfortaine</author><category>hn</category><pubDate>Sat, 21 Feb 2026 01:27:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[AI agents are good at doing one thing at a time. Give Claude a focused task and it performs. But real work isn’t one task. It’s a tree of tasks with dependencies, parallelism, and context that needs to flow between them.The multi-agent frameworks are multiplying. They’re all solving the wrong problem. models coordination as a state machine. You define nodes and edges in Python. The developer decides upfront how agents hand off work. It’s powerful for fixed workflows, but the graph is static. If an agent realizes mid-task that the work should be split differently, tough luck. The developer has to anticipate every decomposition pattern in advance. is role-based. You define agents with personas — “researcher,” “analyst,” “writer” — and assign them tasks. Intuitive, but the roles are decided by the developer, not discovered by the agents. A crew of three can’t decide it actually needs five people, or that “researcher” should be split into two parallel tracks. puts agents in a group chat. They coordinate by talking to each other. Flexible, but there’s no structure. No dependency tracking, no authority scoping, no typed results. Coordination emerges from conversation, which means it’s unpredictable and hard to inspect. is the most minimal — lightweight handoffs between agents. Agent A decides it’s time for Agent B and transfers control. Simple, but linear. No parallelism, no tree structure, no way for an agent to spawn three subtasks and wait for all of them. — Anthropic’s own pattern — put a single agent in a loop with tools. Handles sequential complexity well but runs into context window limits on large tasks and can’t parallelize. One agent, one thread, one context.The common thread: every framework requires the developer to predefine the coordination structure.  decide the workflow graph, the agent roles, the handoff pattern. The agents execute within your boundaries.This made sense when agents were unreliable. You’d never let GPT-3 decide how to decompose a project. But current models are good at planning. They break problems into subproblems naturally. They understand dependencies. They know when a task is too big for one pass.So why are we still hardcoding the decomposition?Let the agent build the treeI built Cord. You give it a goal:cord run "Should we migrate our API from REST to GraphQL? Evaluate and recommend."
One agent launches. It reads the goal, decides it needs research before it can answer, and creates subtasks:● #1 [active] GOAL Should we migrate our API from REST to GraphQL?
  ● #2 [active] SPAWN Audit current REST API surface
  ● #3 [active] SPAWN Research GraphQL trade-offs for our stack
  ○ #4 [pending] ASK How many concurrent users do you serve?
    blocked-by: #2
  ○ #5 [pending] FORK Comparative analysis
    blocked-by: #3, #4
  ○ #6 [pending] SPAWN Write migration recommendation
    blocked-by: #5
No workflow was hardcoded. The agent decided this structure at runtime.It parallelized the API audit (#2) and the GraphQL research (#3). It created an  node (#4) — a question for the human — because it realized the recommendation depends on scale, something it can’t research on its own. It blocked #4 on #2 because the question makes more sense with the audit results as context. It made #5 a  so the analysis inherits everything learned so far. And it sequenced the final recommendation after the analysis.✓ #2 [complete] SPAWN Audit current REST API surface
  result: 47 endpoints. 12 heavily nested resources...
✓ #3 [complete] SPAWN Research GraphQL trade-offs
  result: Key advantages: reduced over-fetching...

? How many concurrent users do you serve?
  Options: <1K, 1K-10K, 10K-100K, >100K
> 10K-100K

● #5 [active] FORK Comparative analysis
  blocked-by: #3, #4
The research runs in parallel. When both finish and you answer the question, the analysis launches with all three results in its context. It produces a recommendation tailored to your actual scale and API surface — not a generic blog post about GraphQL.This is the one idea I think is genuinely new: the distinction between  and  as a context-flow primitive.A  agent gets a clean slate. Just its prompt and the results of nodes it explicitly depends on. Like hiring a contractor — here’s the spec, go. Cheap to restart, easy to reason about.A  agent gets all completed sibling results injected into its context. Like briefing a team member — they know everything the team has learned so far. More expensive, but necessary for analysis that builds on prior work.This isn’t about concurrency. Both can run in parallel or sequentially. It’s about . In the example above, the agent chose  for the independent research tasks and  for the analysis that needs everything. It made this choice correctly — because the tool descriptions explain the distinction clearly. That’s the point: the protocol is learnable from its interface alone.The natural next step is making context flow a first-class primitive — a  parameter on spawn/fork that takes a natural language instruction like , "relevant details from a web designer's perspective", or "bullet points about error handling". A compaction subagent would read the parent’s full context, distill it according to the query, and pass only the result to the child. This has to be built into the client itself — an isolated MCP tool can’t access the parent’s context without serializing the whole thing as a tool input, which defeats the purpose.Each agent is a Claude Code CLI process with MCP tools backed by a shared SQLite database:spawn(goal, prompt, blocked_by) — create a child taskfork(goal, prompt, blocked_by) — create a context-inheriting child — ask the human a question — mark yourself done — see the full coordination treeAgents know they’re a node in a coordination tree — the system prompt tells them — but they don’t manage it. They see tools and use them as needed. The protocol — dependency resolution, authority scoping, result injection — is enforced by the MCP server.When an  node becomes ready, the engine pauses to prompt the human in the terminal. The answer is stored as a result, and downstream nodes unblock. The human is a participant in the tree, not an observer.~500 lines of Python. SQLite + MCP.Behavioral testing as design validationBefore writing the runtime, I needed to know if the protocol was learnable from its interface alone. So I built a throwaway MCP server with the five tools, pointed Claude Code at it, and ran 15 tests. No runtime, no engine — just Claude, the tools, and a task.The interesting results weren’t the correct tool calls. They were the emergent behaviors. An agent asked to decompose a project called  before acting and again after to verify — read, act, verify, unprompted. An agent that got rejected for trying to stop a sibling escalated through  — the correct pattern, but one I’d never described. An agent that hit an error stopped the failing child and respawned a new one with an adjusted prompt.15/15 passed. The passing rate wasn’t the insight — it was that clear tool descriptions plus dependency semantics were sufficient for correct use. That told me the runtime was worth building.This implementation uses Claude Code CLI and SQLite. But the protocol — five primitives, dependency resolution, authority scoping, two-phase lifecycle — is independent of all that.You could implement Cord over Postgres for multi-machine coordination. Over the Claude API directly, without the CLI overhead. With multiple LLM providers — GPT for cheap tasks, Claude for complex ones. With human workers for some nodes.The protocol is the contribution. This repo is a proof of concept.git clone https://github.com/kimjune01/cord.git
cord
uv cord run  2.0
You can also point it at a planning doc:cord run plan.md  5.0
The root agent reads the markdown and decomposes it into a coordination tree. Write your plan however you want — bullet points, sections, prose — and the agent figures out the task structure, dependencies, and parallelism.]]></content:encoded></item><item><title>Claws are now a new layer on top of LLM agents</title><link>https://twitter.com/karpathy/status/2024987174077432126</link><author>Cyphase</author><category>hn</category><pubDate>Sat, 21 Feb 2026 00:56:29 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Gamedate – A site to revive dead multiplayer games</title><link>https://gamedate.org/</link><author>msuniverse2026</author><category>hn</category><pubDate>Sat, 21 Feb 2026 00:44:28 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Be wary of Bluesky</title><link>https://kevinak.se/blog/be-wary-of-bluesky</link><author>kevinak</author><category>hn</category><pubDate>Fri, 20 Feb 2026 23:35:33 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In 2023, Bluesky's CTO Paul Frazee was asked what would happen if Bluesky ever turned against its users. His answer:"it would look something like this: bluesky has gone evil. there's a new alternative called freesky that people are rushing to. I'm switching to freesky"That's the same argument people made about Twitter. "If it goes bad, we'll just leave." We know how that played out.Bluesky is built on ATProto, an open protocol. The pitch is simple: your data is yours, your identity is yours, and if you don't like what Bluesky is doing, you can take everything and leave. Apps like Tangled (git hosting), Grain (photos), and Leaflet (publishing) all plug into the same protocol. One account, many apps, no lock-in.It sounds great. But look closer.Where your data actually livesWhen you use any ATProto app, it writes data to your Personal Data Server, or PDS. Your Bluesky posts, your Tangled issues, your Leaflet publications, your Grain photos. All of it goes to the same place.For almost every user, that place is a server run by Bluesky.You can self-host a PDS. Almost nobody does. Why would they? Bluesky's PDS works out of the box with every app, zero setup, zero maintenance. Self-hosting means running a server, keeping it online, and gaining nothing in return.To be fair, migration tools exist. You can move your account to a self-hosted PDS for as little as $5 a month. Bluesky has made this easier over time and even supports moving back. But this only works if you do it before the door closes. If an acquirer disables exports, it doesn't matter that the tools existed yesterday. And we know from every platform transition in history that almost nobody takes proactive steps to protect their data.Here's the part that worries me.Every new ATProto app makes this problem worse, not better. Each app tells you "sign in with your Bluesky account", which really means "write more data to Bluesky's servers." The more apps that launch, the more users depend on Bluesky's infrastructure, the less reason anyone has to leave.The protocol doesn't distribute value across the network. It concentrates it. Developers are building features on top of Bluesky's infrastructure for free, making it more indispensable with every app that ships.And Bluesky gets to claim the moral high ground the whole time. "We're open! We're decentralized! You can leave whenever you want!" Meanwhile, the switching cost goes up every day.It's not just the PDS. Bluesky controls almost every critical layer: All data flows through it. Bluesky runs the dominant one. Whoever controls the relay controls what gets seen, hidden, or deprioritized. Third parties can run their own, but without the users, it doesn't matter. This is what assembles your timeline, threads, and notifications. Bluesky runs the main one. If it goes down or goes hostile, every client that depends on it breaks. Your identity on ATProto resolves through a centralized directory run by Bluesky. They've called it a "placeholder" since 2023 and said they plan to decentralize it. There's still no timeline.At every layer, the answer is "anyone can run their own." At every layer, almost nobody does.Email is an open, federated protocol. Anyone can run a mail server. In practice, running your own mail server is painful and everyone just uses Gmail. The protocol being "open" didn't prevent centralization.ATProto might be worse. With email, at least each app connects to your own server. With ATProto, each new app adds more data to the same centralized PDS. The open protocol is actually a centralization flywheel.What happens in an acquisitionSay someone buys Bluesky. They now control:The PDS for nearly every userThe DID directory that resolves every identityThey could disable data export. They could cut off third-party apps. They could shut down federation. They could insert ads, shadow ban users, deprioritize content.And the blast radius isn't just Bluesky the social network. It's every app in the ecosystem. Your git issues on Tangled, your posts on Leaflet, your photos on Grain. All stored on infrastructure now controlled by the acquirer.The protocol says you can leave. But the company that just paid billions for the network has no incentive to let you.I like Bluesky. I use Bluesky. The team seems to genuinely care.But every counter-argument to the concerns above rests on the same foundation: technically, users can leave. Technically, you can self-host. Technically, you can run your own relay. The capability exists at every layer. But people don't do these things. They never have with any protocol. Not email, not RSS, not XMPP. The default wins. Always.And then there's the money. You don't raise $120M at a $700M valuation to run a public utility. Those investors need a return. That return comes from monetizing users, getting acquired, or going public. All three create pressure to consolidate control, not distribute it. A truly decentralized network where users can freely leave is worth less to an acquirer than one where they can't.The PBC structure is supposed to be the safeguard. But PBC obligations are vague and untested in court. When $120M in VC money is on one side of the balance, guess which way it tips.The protocol can't save you from incentives.]]></content:encoded></item><item><title>CERN rebuilt the original browser from 1989 (2019)</title><link>https://worldwideweb.cern.ch/</link><author>tylerdane</author><category>hn</category><pubDate>Fri, 20 Feb 2026 23:19:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In December 1990, an application called WorldWideWeb was developed on a NeXT machine at The European Organization for Nuclear Research (known as CERN) just outside of Geneva. This program – WorldWideWeb — is the antecedent of most of what we consider or know of as "the web" today.In February 2019, in celebration of the thirtieth anniversary of the development of WorldWideWeb, a group of developers and designers convened at CERN to rebuild the original browser  a contemporary browser, allowing users around the world to experience the rather humble origins of this transformative technology.This project was supported by the US Mission in Geneva through the CERN & Society Foundation.Ready to browse the World Wide Web using WorldWideWeb?Select "Document" from the menu on the side.Select "Open from full document reference".Type a URL into the "reference" field.Click here to jump in (and remember you need to double-click on links):How to edit a document and make a linkHistory — a brief history of the application which was built in 1989 as a progenitor to what we know as "the web" today.Timeline — a timeline of the thirty years of influences leading up to (and the thirty years of influence leading out from) the publication of the memo that lead to the development of the first web browser.The Browser — instructions for using the recreated WorldWideWeb browser, and a collection of its interface patterns.Typography — details of the NeXT computer's fonts used by the WorldWideWeb browser.Production Process — a behind the scenes look at how the WorldWideWeb browser was rebuilt for today.Related Links — links to additional historical and technical resources around the production of WorldWideWeb.Colophon — a bit of info about the folks behind the project.]]></content:encoded></item><item><title>Across the US, people are dismantling and destroying Flock surveillance cameras</title><link>https://www.bloodinthemachine.com/p/across-the-us-people-are-dismantling</link><author>latexr</author><category>hn</category><pubDate>Fri, 20 Feb 2026 22:50:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In today’s edition of Blood in the Machine:Across the nation, people are dismantling and destroying Flock cameras that conduct warrantless vehicle surveillance, and whose data is shared with ICE.An Oklahoma man airing his concerns about a local data center project at a public hearing is arrested after he exceeded his allotted time by a couple seconds.Uber and Lyft drivers deliver a petition signed by 10,000 gig workers demanding that stolen wages be returned to them.PLUS: A climate researcher has a new report that unravels the ‘AI will solve climate change’ mythos, Tesla’s Robotaxis are crashing 4 times as often as humans, and AI-generated public comments helped kill a vote on air quality.A brief note that this reporting, research, and writing takes a lot of time, resources, and energy. I can only do it thanks to the paid subscribers who chip in a few bucks each month; if you’re able, and you find value in this work, please consider upgrading to a paid subscription so I can continue on. Many thanks, hammers up, and onwards.Last week, in La Mesa, a small city just east of San Diego, California, observers happened upon a pair of destroyed Flock cameras. One had been smashed and left on the median, the other had key parts removed. The destruction was obviously intentional, and appears perhaps even staged to leave a message: It came just weeks after the city decided, in the face of public protest, to continue its contracts with the surveillance company.Flock cameras are typically mounted on 8 to 12 foot poles and powered by a solar panel. The smashed remains of all of the above in La Mesa are the latest examples of a widening anti-Flock backlash. In recent months, people have been smashing and dismantling the surveillance devices, in incidents reported in at least five states, from coast to coast. the smashed Flock equipmentThere was “a huge turnout against them,” he tells me, “but the council approved continuation of the contract.”currently valued at $7.5 billionFlock’s vehicle data is routinely accessed by ICEnow-canceled partnership with Ringwas arrestedCritics claimthey’re succeedingwere cut down and destroyedLookout Eugene-Springfield.A note reading “Hahaha get wrecked ya surveilling fucks” was attached to one of the destroyed poles, and somewhat incredibly, broadcast on the local news. according to local newsJefferey S. Sovern, 41, was arrested in October after detectives say he “intentionally destroyed” 13 Flock Safety cameras between April and October of this year. He was charged with 13 counts of destruction of property, six counts of petit larceny and six counts of possession of burglary tools.Sovern admitted to the crimes, according to a criminal complaint filed in Suffolk General District Court, going as far as to say he used vice grips to help him disassemble the tow-piece polls. He also admitted to keeping some of the wiring, batteries and solar panels taken from the cameras. Some of the items were recovered by police after they searched the property.created a GoFundMeMy name is Jeff and I appreciate my privacy. I appreciate everyone's right to privacy, enshrined in the fourth amendment. With the local news outlets finding my legal issues and creating a story that is starting to grow, there has been community support for me that I humbly welcome.(I reached out to Sovern, who is out on bail, for comment, and will update or follow up if I hear back.) DeFlockIn fact, it’s hard to think of a tech product or project this side of generative AI that is more roundly opposed and reviled, on a bipartisan level, than Flock, and resistance takes many forms and stripes. Here’s the YouTuber Benn Jordan, showing his viewers how to Flock-proof their license plates and render their vehicles illegible to the company’s data ingestion systems:In his GoFundMe, Sovern also mentioned the support for him he’d seen on forums online, so I went over to Reddit to get a sense for how his actions were being received online. Here was the page that shared news of his arrest for destroying the Flock cameras:There was, in other words, nearly universal support for Sovern’s Flock dismantling campaign. Bear in mind that this is r/Norfolk, and while it’s still reddit users we’re talking about, it’s not like this is r/anarchism here: The San Diego reddit threads carrying news of the destroyed Flock equipment told a similar story:There were plenty of outright endorsements of the sabotage:Off the message boards and in real civic life, Bill Paul, the reporter with the San Diego Slacker, says anger is boiling over, too. He points again to that heated December 2025 city council meeting, in which public outrage was left unaddressed. The city, perhaps aware of the stigma Flock now carries, apparently tried to highlight that their focus was on the “smart streetlights” made by another company, while downplaying the fact that those streetlights run on Flock software. “San Diego gets to hide behind a slight facade in that their contract is with Ubicquia,” the smart streetlight manufacturer, Paul says, “but the software layer is Flock. You can easily see Flock hardware on retail properties, looking at the same citizens, with zero oversight, and SDPD can claim they have clean hands.” Weeks later, pieces of smashed Flock cameras littered the ground.Across the country, in other words, municipal governments are overriding public will to make deals with a profiteering tech company to surveil their citizens and to collaborate with federal agencies like ICE. It might be taken as a sign of the times that in states and cities across the US, thousands of miles apart, those opposed to the technology are refusing to countenance what they view as violations of privacy and civil liberty, and are instead taking up vice grips and metal cutters. And in many cases, they’re getting hailed by their peers as heroes. SignalLilly IraniIn case you missed it, I shared my five takeaways on the most recent round of ultraheated AI discourse here:The exchange was filmed and recorded on YouTube:Police in Claremore, Oklahoma arrested a local man after he went slightly over his time giving public remarks during a city council meeting opposing a proposed data center. Darren Blanchard showed up at a Claremore City Council meeting on Tuesday to talk about public records and the data center. When he went over his allotted 3 minutes by a few seconds, the city had him arrested and charged with trespassing.Project MustangThe public hearing was a chance for the city council to address some of these concerns and all residents were given a strict three minute time limit. The entire event was livestreamed and archive of it is on YouTube. Blanchard was warned, barely, to “respect the process” by one of the council members but was clearly finishing reading from papers he had brought to read from, was not belligerent, and went over time by just a few seconds. Anyone who has ever attended or watched a city council meeting anywhere will know that people go over their time at essentially any meeting that includes public comment.Blanchard arrived with documents in hand and questions about public records requests he’d made. During his remarks, people clapped and cheered and he asked that this not be counted against his three minutes. “There are major concerns about the public process in Claremore,” Blanchard said, referencing compliance documents and irregularities he’d uncovered in public records.a local anchor defending On Wednesday, I headed to Pershing Square in downtown Los Angeles, where dozens of gig workers and organizers with Rideshare Drivers United had assembled to deliver a petition to the California Labor Commission signed by thousands of workers, calling on the body to deliver a settlement on their behalf. Organizers made short speeches on the steps of the square while local radio and TV stations captured the moment. ““They’re robbing us!” A speaker yelled. “Wage theft!” the crowd replied.The Labor Commission is suing the gig companies on drivers’ behalf, alleging that Uber and Lyft stole billions of dollars worth of wages from drivers before Prop 22 was enacted in 2020. The commission is believed to be in negotiations with the gig companies right now that will determine a settlement.I spoke with one driver, Karen, who had traveled from San Diego to join the demonstration, and asked her why she came. “It’s important we build driver power” she said. “Without driver power, we won’t get what we need, and we just want fairness.” She said she was hoping to claim at least $20,000 in stolen wages. “We’re fighting for wages that were stolen for us from us and continue to be stolen from us every single day by these app companies from hell,” RDU organizer Nicole Moore told me. “So we’re marching in downtown L.A. to deliver 10,000 signatures of drivers demanding that the state fight hard for us, and don’t let these companies rip us off.”Electrek reportsWith 14 crashes now on the books, Tesla’s “Robotaxi” crash rate in Austin continues to deteriorate. Extrapolating from Tesla’s Q4 2025 earnings mileage data, which showed roughly 700,000 cumulative paid miles through November, the fleet likely reached around 800,000 miles by mid-January 2026. That works out to one crash every 57,000 miles.Using NHTSA’s broader police-reported crash average of roughly one per 500,000 miles, the picture is even worse, Tesla’s fleet is crashing at approximately 8 times the human rate.Okay okay, that’s it for this week. Thanks as always for reading. Hammers up.]]></content:encoded></item><item><title>A16z partner says that the theory that we’ll vibe code everything is wrong</title><link>https://www.aol.com/articles/a16z-partner-says-theory-well-050150534.html</link><author>paulpauper</author><category>hn</category><pubDate>Fri, 20 Feb 2026 22:47:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Anish Acharya says it is not worth it to use AI-assisted coding for all business functions.AI should focus on core business development, not rebuilding enterprise software.The A16z partner added that software stocks that took a beating last week were oversold.Vibe coding everything is just not worth it, says an Andressen Horowitz partner.On an episode of the "20VC" podcast released on Monday, A16z general partner Anish Acharya said that companies shouldn't use AI-assisted coding for every part of their business.He said that software accounts for 8% to 12% of a company's expenses, so using vibe coding to build the company's resource planning or payroll tools would only save about 10%. Relying on AI to write code also carries risks, he said."You have this innovation bazooka with these models. Why would you point it at rebuilding payroll or ERP or CRM," Acharya said, referring to enterprise resource planning and customer relationship management software. Salesforce, Microsoft, Oracle, and SAP are among the top providers of such software.Instead, companies are better off using AI to develop their core businesses or optimize the remaining 90% of their costs, said the venture capitalist of six years."Of course, there will be secular losers. There are specific business models that are now going to be disadvantaged," he said. "But the general story that we're going to vibe code everything is flat wrong, and the whole market is oversold software."Acharya's comments follow a brutal week for software stocks, which dragged down tech and broader markets. The sell-off started when already-wary investors panicked about , which can perform a range of clerical tasks for people working in the legal industry.The A16z partner joins famed investor Vinod Khosla in saying that stock prices should be ignored when evaluating the future of tech companies.On a podcast last month, Khosla dismissed talks of an AI bubble and said investors should not be concerned as long as API call volume, a benchmark of AI usage, remains high."If that's your fundamental metric of what's the real use of your AI, usefulness of AI, demand for AI, you're not going to see a bubble in API calls," he said. "What Wall Street tends to do with it, I don't really care. I think it's mostly irrelevant."]]></content:encoded></item><item><title>Don&apos;t create .gitkeep files, use .gitignore instead (2023)</title><link>https://adamj.eu/tech/2023/09/18/git-dont-create-gitkeep/</link><author>frou_dh</author><category>hn</category><pubDate>Fri, 20 Feb 2026 22:27:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Git only tracks files, not directories. It will only create a directory if it contains a tracked file. But sometimes you need to “track” a directory, to ensure it exists for fresh clones of a repository. For example, you might need an output directory called .In this post, we’ll look at two ways to achieve this. First, the common but slightly flawed  technique, then a simpler one using only a  file.This technique uses an empty file called :The empty file ensures that Git creates the directory with minimal cost. Any other filename may be used, as Git doesn’t treat  files any differently.To set this up, you might create an empty file with :Then ignore all files in the directory, except , by adding patterns in the repository’s  file:The first pattern ignores everything in the  directory. The second one then un-ignores the  file, allowing it to be committed.This technique works, but it has some downsides:It requires editing two files.If the directory is renamed,  needs updating, which is easy to miss. is not a name recognized by Git, so there’s no documentation on it, potentially confusing other developers.There’s a better way that doesn’t have these flaws.The better  techniqueThis technique uses only a short  file inside the directory:The  file has these contents:The first pattern ignores all files in the directory. The second one then un-ignores the  file, so it can be committed.You can create this file with  and file redirection:>build/.gitignore
When you add and commit the directory, Git will pick up on the  file first, skipping other files within the directory:gitaddbuild

gitstatus
gitcommit-mThe directory is now “tracked” with a single, standard file that will work even after renames.Don’t ignore this technique,😸😸😸 Check out my new book on using GitHub effectively, ! 😸😸😸One summary email a week, no spam, I pinky promise.]]></content:encoded></item><item><title>Turn Dependabot off</title><link>https://words.filippo.io/dependabot/</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 20 Feb 2026 21:25:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Dependabot is a noise machine. It makes you feel like you’re doing work, but you’re actually discouraging more useful work. This is  true for security alerts in the Go ecosystem.I recommend turning it off and replacing it with a pair of scheduled GitHub Actions, one running govulncheck, and the other running your test suite against the latest version of your dependencies.We even got one of these alerts for the Wycheproof repository, which does not import the affected filippo.io/edwards25519 package at all. Instead, it only imports the unaffected filippo.io/edwards25519/field package.$ go mod why -m filippo.io/edwards25519
# filippo.io/edwards25519
github.com/c2sp/wycheproof/tools/twistcheck
filippo.io/edwards25519/field
We have turned Dependabot off.Use a serious vulnerability scanner insteadBut isn’t this toil unavoidable, to prevent attackers from exploiting old vulnerabilities in your dependencies? Absolutely not!Computers are perfectly capable of doing the work of filtering out these irrelevant alerts for you. The Go Vulnerability Database has rich version, package,  metadata for all Go vulnerabilities.modules:
    - module: filippo.io/edwards25519
      versions:
        - fixed: 1.1.1
      vulnerable_at: 1.1.0
      packages:
        - package: filippo.io/edwards25519
          symbols:
            - Point.MultiScalarMult
summary: Invalid result or undefined behavior in filippo.io/edwards25519
description: |-
    Previously, if MultiScalarMult was invoked on an
    initialized point who was not the identity point, MultiScalarMult
    produced an incorrect result. If called on an
    uninitialized point, MultiScalarMult exhibited undefined behavior.
cves:
    - CVE-2026-26958
credits:
    - shaharcohen1
    - WeebDataHoarder
references:
    - advisory: https://github.com/FiloSottile/edwards25519/security/advisories/GHSA-fw7p-63qq-7hpr
    - fix: https://github.com/FiloSottile/edwards25519/commit/d1c650afb95fad0742b98d95f2eb2cf031393abb
source:
    id: go-security-team
    created: 2026-02-17T14:45:04.271552-05:00
review_status: REVIEWED
Any decent vulnerability scanner will  filter based on the package, which requires a simple . This already silences a lot of noise, because it’s common and good practice for modules to separate functionality relevant to different dependents into different sub-packages. For example, it would have avoided the false alert against the Wycheproof repository.If you use a third-party vulnerability scanner, you should demand at least package-level filtering. vulnerability scanners will go further, though, and filter based on the reachability of the vulnerable  using static analysis. That’s what govulncheck does!$ go mod why -m filippo.io/edwards25519
# filippo.io/edwards25519
filippo.io/sunlight/internal/ctlog
github.com/google/certificate-transparency-go/trillian/ctfe
github.com/go-sql-driver/mysql
filippo.io/edwards25519

$ govulncheck ./...
=== Symbol Results ===

No vulnerabilities found.

Your code is affected by 0 vulnerabilities.
This scan also found 1 vulnerability in packages you import and 2
vulnerabilities in modules you require, but your code doesn't appear to call
these vulnerabilities.
Use '-show verbose' for more details.
govulncheck noticed that my project indirectly depends on filippo.io/edwards25519 through github.com/go-sql-driver/mysql, which does not make the vulnerable symbol reachable, so it chose not to notify me.If you want, you can tell it to show the package- and module-level matches.$ govulncheck -show verbose,color ./...
Fetching vulnerabilities from the database...

Checking the code against the vulnerabilities...

The package pattern matched the following 16 root packages:
  filippo.io/sunlight
  filippo.io/sunlight/internal/stdlog
  [...]
Govulncheck scanned the following 54 modules and the go1.26.0 standard library:
  filippo.io/sunlight
  crawshaw.io/sqlite@v0.3.3-0.20220618202545-d1964889ea3c
  filippo.io/bigmod@v0.0.3
  filippo.io/edwards25519@v1.1.0
  filippo.io/keygen@v0.0.0-20240718133620-7f162efbbd87
  filippo.io/torchwood@v0.8.0
  [...]

=== Symbol Results ===

No vulnerabilities found.

=== Package Results ===

Vulnerability #1: GO-2026-4503
    Invalid result or undefined behavior in filippo.io/edwards25519
  More info: https://pkg.go.dev/vuln/GO-2026-4503
  Module: filippo.io/edwards25519
    Found in: filippo.io/edwards25519@v1.1.0
    Fixed in: filippo.io/edwards25519@v1.1.1

=== Module Results ===

Vulnerability #1: GO-2025-4135
    Malformed constraint may cause denial of service in
    golang.org/x/crypto/ssh/agent
  More info: https://pkg.go.dev/vuln/GO-2025-4135
  Module: golang.org/x/crypto
    Found in: golang.org/x/crypto@v0.44.0
    Fixed in: golang.org/x/crypto@v0.45.0

Vulnerability #2: GO-2025-4134
    Unbounded memory consumption in golang.org/x/crypto/ssh
  More info: https://pkg.go.dev/vuln/GO-2025-4134
  Module: golang.org/x/crypto
    Found in: golang.org/x/crypto@v0.44.0
    Fixed in: golang.org/x/crypto@v0.45.0

Your code is affected by 0 vulnerabilities.
This scan also found 1 vulnerability in packages you import and 2
vulnerabilities in modules you require, but your code doesn't appear to call
these vulnerabilities.
It’s easy to integrate govulncheck into your processes or scanners, either using the  CLI or the golang.org/x/vuln/scan Go API.Replace Dependabot with a govulncheck GitHub ActionYou can replace Dependabot security alerts with this GitHub Action.name: govulncheck
on:
  push:
  pull_request:
  schedule: # daily at 10:22 UTC
    - cron: '22 10 * * *'
  workflow_dispatch:
permissions:
  contents: read
jobs:
  govulncheck:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5
        with:
          persist-credentials: false
      - uses: actions/setup-go@v6
        with:
          go-version-file: go.mod
      - run: |
          go run golang.org/x/vuln/cmd/govulncheck@latest ./...
It will run every day and only notify you if there is an actual vulnerability you should pay attention to.The cost of alert fatigueFalse positive alerts are not only a waste of time, they also reduce security by causing alert fatigue and making proper triage impractical.A security vulnerability should be assessed for its impact: production might need to be updated, secrets rotated, users notified! A business-as-usual dependency bump is a woefully insufficient remediation for an actual vulnerability, but it’s the only practical response to the constant stream of low-value Dependabot alerts.This is why as Go Security Team lead back in 2020–2021 I insisted the team invest in staffing the Go Vulnerability Database and implement a vulnerability scanner with static analysis filtering.The govulncheck Action will not automatically open a PR for you, and that’s a good thing! Now that security alerts are not mostly noise, you can afford to actually look at them and take them seriously, including any required remediation.Noisy vulnerability scanners also impact the open source ecosystem. I often get issues and PRs demanding I update the dependencies of my projects due to vulnerabilities that don’t affect them, because someone’s scanner is failing to filter them. That’s extra toil dropped at the feet of open source maintainers, which is unsustainable. The maintainer’s responsibility is making sure projects are not affected by security vulnerabilities. The responsibility of scanning tools is making sure they don’t disturb their users with false positives.Test against latest instead of updatingThe other purpose of Dependabot is to keep dependencies up to date, regardless of security vulnerabilities. Your practices and requirements will vary, but I find this misguided, too.Dependencies should be updated according to  development cycle, not the cycle of each of your dependencies. For example you might want to update dependencies all at once when you begin a release development cycle, as opposed to when each dependency completes theirs.There are two benefits to quick updates, though: first, you can notice and report (or fix) breakage more rapidly, instead of being stalled by an incompatibility that could have been addressed a year prior; second, you reduce your patch delta  you need to update due to a security vulnerability, reducing the risk of having to rush through a refactor or unrelated fixes.You can capture both of those benefits without actually updating the dependencies by simply running CI against the latest versions of your dependencies every day. You just need to run  before your test suite. In the npm ecosystem, you just run  instead of .This way, you will still be alerted quickly of any potential issues, without having to pay attention to unproblematic updates, which you can defer to whenever fits your project best.name: Go tests
on:
  push:
  pull_request:
  schedule: # daily at 10:22 UTC
    - cron: '22 10 * * *'
  workflow_dispatch:
permissions:
  contents: read
jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        go:
          - { go-version: stable }
          - { go-version-file: go.mod }
        deps:
          - locked
          - latest
    steps:
      - uses: actions/checkout@v5
        with:
          persist-credentials: false
      - uses: actions/setup-go@v6
        with:
          go-version: ${{ matrix.go.go-version }}
          go-version-file: ${{ matrix.go.go-version-file }}
      - uses: geomys/sandboxed-step@v1.2.1
        with:
          run: |
            if [ "${{ matrix.deps }}" = "latest" ]; then
              go get -u -t ./...
            fi
            go test -v ./...
The Tevere has overflowed its lower banks, so a lot of previously familiar landscapes have changed slightly, almost eerily. This is the first picture I took after being able to somewhat safely descend onto (part of) the river’s banks.My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.)
Here are a few words from some of them!Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.]]></content:encoded></item><item><title>Show HN: Mines.fyi – all the mines in the US in a leaflet visualization</title><link>https://mines.fyi/</link><author>irasigman</author><category>hn</category><pubDate>Fri, 20 Feb 2026 21:22:27 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenScan</title><link>https://openscan.eu/pages/scan-gallery</link><author>joebig</author><category>hn</category><pubDate>Fri, 20 Feb 2026 20:47:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I found a vulnerability. they found a lawyer</title><link>https://dixken.de/blog/i-found-a-vulnerability-they-found-a-lawyer</link><author>toomuchtodo</author><category>hn</category><pubDate>Fri, 20 Feb 2026 19:19:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I'm a diving instructor. I'm also a platform engineer who spends lots of his time thinking about and implementing infrastructure security. Sometimes those two worlds collide in unexpected ways.While on a 14 day-long dive trip around Cocos Island in Costa Rica, I stumbled across a vulnerability in the member portal of a major diving insurer - one that I'm personally insured through. What I found was so trivial, so fundamentally broken, that I genuinely couldn't believe it hadn't been exploited already.I disclosed this vulnerability on  with a standard 30-day embargo period. That embargo expired on May 28, 2025 - over . I waited this long to publish because I wanted to give the organization every reasonable opportunity to fully remediate the issue and notify affected users. The vulnerability has since been addressed, but to my knowledge, I have not received confirmation that affected users were notified. I have reached out to the organization to ask for clarification on this matter.This is the story of what happened when I tried to do the right thing.To understand why this is so bad, you need to know how the registration process works. As a diving instructor, I register my students (to get them insured) through my account on the portal. I enter their personal information with their consent - name, date of birth, address, phone number, email - and the system creates an account for them. The student then receives an email with their new account credentials: a numeric user ID and a default password. They  log in to complete additional information, or they might never touch the portal again.When I registered three students in quick succession, they were sitting right next to me and checked their welcome emails. The user IDs were nearly identical - sequential numbers, one after the other. That's when it clicked that something really bad was going on.Now here's the problem: the portal used incrementing numeric user IDs for login. User XXXXXX0, XXXXXX1, XXXXXX2, and so on. That alone is a red flag, but it gets worse: every account was provisioned with a  that was never enforced to be changed on first login. And many users - especially students who had their accounts created  them by their instructors - never changed it.So the "authentication" to access a user's full profile - name, address, phone number, email, date of birth - was:Type the same default password that every account shares on account creation.There's a good chance you get in.That's it. No rate limiting. No account lockout. No MFA. Just an incrementing integer and a password that might as well have been .I verified the issue with the minimum access necessary to confirm the scope - and stopped immediately after.I did everything by the book. I contacted  (MaltaCIP) first - since the organization is registered in Malta, this is the competent national authority. The Maltese National Coordinated Vulnerability Disclosure Policy (NCVDP) explicitly requires that confirmed vulnerabilities be reported to both the responsible organization  CSIRTMalta.Then I emailed the organisation directly, CC'ing CSIRT:As a fellow diving instructor insured through [the organization] and a full-time Linux Platform Engineer, I am contacting you to responsibly disclose a critical vulnerability I identified within the [the organization]'s user account system.During recent testing, I discovered that user accounts - including those of underage students - are accessible through a combination of predictable User ID enumeration (incrementing user IDs) and the use of a static default password that is not enforced to be changed upon first login. This misconfiguration currently exposes sensitive personal data (e.g., names, addresses, contact information - including phone numbers and emails -, dates of birth) and represents multiple GDPR violations.Password reuse across accounts without forced password resetPredictable, incremental user ID enumerationExposure of sensitive and underage user data without adequate safeguardsFor initial confirmation, I am attaching a screenshot from Member ID XXXXXXX showing the exposed data, partly redacted for privacy reasons.Additionally, for transparency and validation, I have shared my proof-of-concept code securely via an encrypted paste service: [link redacted]In the spirit of responsible disclosure, I have already informed CSIRT Malta (in CC) to officially initiate a reporting process, given [the organization]'s operational presence in Malta.I kindly request that [the organization] acknowledges receipt of this disclosure within 7 days.I am offering a window of 30 days from today the 28th of April 2025 for [the organization] to mitigate or resolve the vulnerability before I consider any public disclosure.Please note that I am fully available to assist your IT team with technical details, verification steps and recommendations from a security perspective.I strongly recommend assigning an IT-Security Point of Contact (PoC) for direct collaboration on this issue.Thank you very much for your attention to this critical matter. I am looking forward to working with you towards a secure resolution.Both of these timelines are standard - if anything, generous - in responsible disclosure frameworks.Two days later, I got a reply. Not from their IT team. From their Data Privacy Officers (DPO's) law firm.The letter opened politely enough - they acknowledged the issue and said they'd launched an investigation. They even mentioned they were resetting default passwords and planning to roll out 2FA. Good.But then the tone shifted:While we genuinely appreciate your seemingly good intentions and transparency in highlighting this matter to our attention, we must respectfully note that notifying the authorities prior to contacting the Group creates additional complexities in how the matter is perceived and addressed and also exposes us to unfair liability.Let me translate: "We wish you hadn't told the government about our security issue."We also do not appreciate your threat to make this matter public [...] and remind you that you may be held accountable for any damage we, or the data subjects, may suffer as a result of your own actions, which actions likely constitute a criminal offence under Maltese law.So, to be clear: their portal had a default password on every account, exposing personal data including that of children, and  the one who "likely" committed a criminal offence by finding it and telling them.They also sent a  they wanted me to sign - while requesting my passport ID - confirming I'd deleted all data, wouldn't disclose anything, and would keep the entire matter "strictly confidential." The deadline? End of business the same day they sent it.This declaration included the following gem:I also declare that I shall keep the content of this declaration strictly confidential.That's an NDA with extra steps: I was being asked to sign away my right to discuss the disclosure process itself - including the fact that I found a vulnerability in their system - under threat of legal action.Then came the reminders. One "friendly" reminder. Then an "urgent" one. Sign the declaration. De-escalate. Move on. Quietly.I generally refuse to sign confidentiality clauses in cases involving exposure of sensitive information, and I did so here as well. Coordinated disclosure depends on transparency and trust between researchers and organizations: trust that affected users will be informed, and trust that a report leads to real remediation.Given that the organization in question had already breached that trust by exposing personal data through weak controls, I wasn’t willing to grant blanket confidentiality that could be used to keep the incident out of public scrutiny. And with trying to actual silence me through legal threats, they had already made it clear that their priority was reputation management over user data protection. So I stood my ground.Instead, I offered to sign a modified declaration confirming data deletion. I had no interest in retaining anyone’s personal data, but I was not going to agree to silence about the disclosure process itself.I also pointed out that, under Malta’s NCVDP, involving CSIRT Malta is part of the expected reporting path - not a hostile act - and that publishing post-remediation analyses is standard practice in the security community.Their response doubled down. They cited Article 337E of the Maltese Criminal Code - computer misuse - and helpfully reminded me that:Art. 337E of the Criminal Code also provides that "If any act is committed outside Malta which, had it been committed in Malta, would have constituted an offence [...] it shall [...] be deemed to have been committed in Malta." Meaning that your actions would be deemed a criminal offence in Malta, even if committed in another country.They also made their position on disclosure crystal clear, after I reiterated my refusal to sign their NDA:We object strongly to the use of [the organization's name] in any such blogs or conferences you may write/attend as this would be a disproportionate harm to [the organization's] reputation [...]. We reserve our rights at law to hold you responsible for any damages [the organization] may suffer as a result of any such public disclosures you may make.That's fine by me. Because here's the thing: The vulnerability has been fixed. Default passwords have been reset. 2FA is being rolled out. I feel sorry for the developer(s) who had to clean up this mess, but at least the issue is no longer exploitable. Sure, it would have been better if the organization had thanked me and taken responsibility for notifying affected users. If the incident qualified as a personal data breach (which it does) and was likely to result in a (high) risk to individuals - especially given minors were involved - GDPR Articles 33 and 34 generally require notification to the supervisory authority and communication to affected data subjects.GDPR Article 34(1) When the personal data breach is likely to result in a high risk to the rights and freedoms of natural persons, the controller shall communicate the personal data breach to the data subject without undue delay.GDPR Article 34(2) The communication to the data subject referred to in paragraph 1 of this Article shall describe in clear and plain language the nature of the personal data breach and contain at least the information and measures referred to in points (b), (c) and (d) of Article 33(3).I have not received confirmation that those notifications were ever carried out.My favourite part was the organization's position on whose fault this actually was:We contend that it is the responsibility of users to change their own password (after we allocate a default one).Read that again. A company that assigned the same default password to every account, never forced a password change, and used incrementing numeric IDs as usernames is blaming  for not securing their own accounts. Accounts that include those of minors.GDPR Article 5(1)(f) (integrity and confidentiality): Personal data shall be processed in a manner that ensures appropriate security of the personal data, including protection against unauthorised or unlawful processing and against accidental loss, destruction or damage, using appropriate technical or organisational measures.Under GDPR, the  (namely: the organization) is responsible for implementing appropriate technical and organizational measures to ensure data security. A static default password on an IDOR-vulnerable portal is not an "appropriate measure" by any definition.GDPR Article 24(1) (controller responsibility): Taking into account the nature, scope, context and purposes of processing as well as the risks of varying likelihood and severity for the rights and freedoms of natural persons, the controller shall implement appropriate technical and organisational measures to ensure and to be able to demonstrate that processing is performed in accordance with this Regulation. Those measures shall be reviewed and updated where necessary.This isn't an isolated case. The security research community has been dealing with this pattern for decades: find a vulnerability, report it responsibly, get threatened with legal action. It's so common it has a name - the chilling effect.Organizations that respond to disclosure with lawyers instead of engineers are telling the world something important: they care more about their reputation than about the data they're supposed to protect.And the real irony? The legal threats are the reputation damage. Not the vulnerability itself - vulnerabilities happen to everyone. It's the response that tells you everything about an organization's security culture. - they did this, to be fair. - they started on this too. - instead of threatening them with criminal prosecution. - so researchers know how to report issues and what to expect. - especially the parents of underage members whose data was exposed.Not try to silence the researcher with NDAs disguised as "declarations."If you're an organization:Publish a Coordinated Vulnerability Disclosure policy. It doesn't have to be complex - maybe begin with a security.txt file and a clear process that favors transparency. for helping you improve your security posture.Don't shoot the messenger. The person reporting the bug is not your enemy. The bug is. for security failures that are your responsibility as a data controller.If you're a security researcher:Always involve your national CSIRT. It protects you and creates an official record. Every email, every timestamp, every response. that prevent you from discussing the disclosure process. But you can agree to delete data (and MUST do so!) without agreeing to silence. Many jurisdictions have legal protections for good-faith security research. The EU's NIS2 Directive encourages coordinated vulnerability disclosure.Because right now, in 2026, reporting a trivial vulnerability exposing personal data - including that of children - still gets met with legal threats instead of gratitude. And that's a problem for all of us.]]></content:encoded></item><item><title>Every company building your AI assistant is now an ad company</title><link>https://juno-labs.com/blogs/every-company-building-your-ai-assistant-is-an-ad-company</link><author>ajuhasz</author><category>hn</category><pubDate>Fri, 20 Feb 2026 18:55:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
      Pre-orders for the Juno Pioneer Edition now open, reserve your Juno today!
    
      Friday, 20 February 2026 ·
      Adam Juhasz
      On January 16, OpenAI quietly announced that ChatGPT would begin showing
      advertisements. By February 9th, ads were live. Eight months earlier,
      OpenAI spent $6.5 billion to acquire Jony Ive's hardware startup io.
      They're building a pocket-sized, screenless device with built-in cameras
      and microphones -- "contextually aware," designed to replace your phone.
    
      But this isn't a post about OpenAI. They're just the latest. The problem
      is structural.
    
      And every one of them is building hardware designed to see and hear
      everything around you, all day, every day. These two facts are on a
      collision course, and local on-device inference is the only way off the
      track.
    The always-on future is inevitable
      Before we talk about who's building it, let's be clear about
       being built.
    
      Every mainstream voice assistant today works behind a gate. You say a
      magic word -- "Hey Siri," "OK Google," "Alexa" -- and only then does the
      system listen. Everything before the wake word is theoretically discarded.
    
      This was a reasonable design in 2014. It is a dead end for where AI
      assistance needs to go.
    
      Here's what happens in a real kitchen at 6:30am:
      "Are we out of eggs again? I'm thinking frittata tonight but we also need
      to -- oh wait, did the school email about Thursday? I think there's a
      early release. Anyway, if we don't have eggs, I'll get them from Target
      and also that dish soap, the blue one."
    
      Nobody is going to preface that with a wake word. The information is woven
      into natural speech between two flustered parents getting the family ready
      to leave the house. The moment you require a trigger, you lose the most
      valuable interactions -- the ones that happen while people are living
      their lives, not thinking of how to give context to an AI assistant.
    
      You cannot build proactive assistance behind a wake word. The AI has to be
      present in the room, continuously, accumulating context over days and
      weeks and months, to build the understanding that makes proactive help
      possible.
    
      This is where every major AI company is heading. Not just audio -- vision,
      presence detection, wearables, multi-room awareness. The next generation
      of AI assistants will hear and see everything. Some will be on your face
      or in your ears all day. They will be always on, always sensing, always
      building a model of your life.
    
        The question is not  always-on AI will happen. It's who
        controls the data it collects. And right now, the answer to that
        question is: advertising companies.Policy is a promise. Architecture is a guarantee.
      Here's where the industry's response gets predictable. "We encrypt the
      data in transit." "We delete it after processing." "We anonymize
      everything." "Ads don't influence the AI's answers." "Read our privacy
      policy."
      OpenAI's own ad announcement includes this language: "OpenAI keeps
      conversations with ChatGPT private from advertisers, and never sells data
      to advertisers." It sounds reassuring. But Google
      scanned every Gmail for ad targeting for thirteen years
      before quietly stopping in 2017. Policies change. Architectures don't.
    Policy is a promise. Architecture is a guarantee.
      When a device processes data locally, the data
      physically cannot leave the network. There is no API endpoint to
      call. There is no telemetry pipeline. There is no "anonymized usage data"
      that somehow still contains enough signal to be useful for ad targeting.
      The inference hardware sits inside the device or in the user's home, on
      their network.
    
      Your email is sensitive. A continuous audio and visual feed of your home
      is something else entirely. It captures arguments, breakdowns, medical
      conversations, financial discussions, intimate moments, parenting at its
      worst, the completely unguarded version of people that exists only when
      they believe nobody is watching.The edge inference stack is ready
      The counterargument is always the same: "Local models aren't good enough."
      Three years ago, that was true. It is no longer true.
    
      You can run a complete ambient AI pipeline today -- real-time
      speech-to-text, semantic memory, conversational reasoning, text-to-speech,
      etc -- on a device that fits next to a cable box (remember those?). No fan
      noise. A one-time hardware purchase with no per-query fee and no data
      leaving the building. New model architectures, better compression, and
      open-source inference engines have converged to make this possible, and
      the silicon roadmap points in one direction: more capability per watt,
      every year.
      Are local models as capable as the best cloud models? No. But we're
      usually not asking our smart speaker to re-derive the Planck constant.
    
      Hardware that runs inference on-device. Models that process audio and
      video locally and never transmit it.
      There needs to be a business model based on selling the hardware and
        software, not the data the hardware collects. An architecture where the
        company that makes the device  the data
        it processes, because there is no connection to access it
        through.
      The most helpful AI will also be the most intimate technology ever built.
      It will hear everything. See everything. Know everything about the family.
      The only architecture that keeps that technology safe is one where it is
      structurally incapable of betraying that knowledge. Not policy. Not
      promises. Not a privacy setting that can be quietly removed in a March
      software update.
    
      Choose local. Choose edge. Build the AI that knows everything but phones
      home nothing.
    ]]></content:encoded></item><item><title>Wikipedia deprecates Archive.today, starts removing archive links</title><link>https://arstechnica.com/tech-policy/2026/02/wikipedia-bans-archive-today-after-site-executed-ddos-and-altered-web-captures/</link><author>nobody9999</author><category>hn</category><pubDate>Fri, 20 Feb 2026 18:42:21 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Facebook is cooked</title><link>https://pilk.website/3/facebook-is-absolutely-cooked</link><author>npilk</author><category>hn</category><pubDate>Fri, 20 Feb 2026 18:25:07 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[And I don't just mean that nobody uses it anymore. Like, I knew everyone under 50 had moved on, but I didn't realize the extent of the slop conveyor belt that's replaced us.I logged on for the first time in ~8 years to see if there was a group for my neighborhood (there wasn't). Out of curiosity I thought I'd scroll a bit down the main feed.The first post was the latest xkcd (a page I follow). The next  were not by friends or pages I follow. They were basically all thirst traps of young women, mostly AI-generated, with generic captions. Here's a sampler — mildly NSFW, but I did leave out a couple of the lewder ones:Click to show mildly sensitive content (revealing clothing)Yikes. Again, I don't follow any of these pages. This is all just what Facebook is pushing on me.I know Twitter/X has worse problems with spam bots in the replies, but this is the ! It's the main page of the site! It's the product that defined modern social media!It wasn't all like that, though. There was also an AI video of a policeman confiscating a little boy's bike, only to bring him a brand new one:And there were some sloppy memes and jokes, mostly about relationships, like this (admittedly not AI) video sketch where a woman decides to intentionally start a fight with her boyfriend because she's on her period:Yep, that's another "yikes" from me. To be fair, though, sometimes that suggested questions feature is pretty useful! Like with this post, for example:Why  she wearing pink heels? What  her personality? Great questions, Meta.I said these were "mostly" AI-generated. The truth is with how good the models are getting these days, it's hard to tell, and I think a couple of them might be real people.Still, some of these are pretty obviously AI. Here's one with a bunch of alien text and mangled logos on the scoreboard in the background:Hmm, I wonder if anyone has noticed this is AI? Let's check out the comments and see if anyone's pointed that ou—...never mind. (I dunno, maybe those are all bots too.)So: is this just something wacky with my algorithm?I mean... maybe? That's part of the whole thing with these algorithmic feeds; it's hard to know if anyone else is seeing what I'm seeing.On the one hand, I doubt most (straight) women's feeds would look like this. But on the other hand, I hadn't logged in in nearly a decade! I hate to think what the feed looks like for some lonely old guy who's been scrolling the lightly-clothed AI gooniverse for hours every day.Did everyone but me know it was like this? I'd seen screencaps of stuff like the Jesus-statue-made-out-of-broccoli slop a year or two ago, but I thought that only happened to grandmas. I hadn't heard it was this bad.I wonder if this evolution was less noticeable for people who are logging in every day. Or maybe it only gets this bad when there aren't any posts from your actual friends?In any case, I stopped exploring after I saw a couple more of those AI-generated pictures but with girls that looked like they were about ~14, which made me sick to my stomach. So long Facebook, see you never, until one day I inexplicably need to use your platform to get updates from my kid's school.]]></content:encoded></item><item><title>Blue light filters don&apos;t work – controlling total luminance is a better bet</title><link>https://www.neuroai.science/p/blue-light-filters-dont-work</link><author>pminimax</author><category>hn</category><pubDate>Fri, 20 Feb 2026 18:14:13 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Everybody wants better sleep, but getting better sleep is hard.no, blue light filters don’t work, but there are many more useful things that someone can do to control their light intake to improve their sleepMy longer answer is usually a half-hour rant about why they don’t work, covering everything from a tiny nucleus of cells above the optic chiasm, to people living in caves without direct access to sunlight, to neuropeptides, the different cones, how monitors work, gamma curves, what I learned running ismy.blue, corn bulbs, melatonin, finally sharing my Apple Watch & WHOOP stats. What follows is slightly more than you needed to know about blue light filters and more effective ways to control your circadian rhythm. Spoiler: the real lever is total luminance, not color.Right above the optic chiasm lies a nucleus called the suprachiasmatic nucleus (SCN). This is where the brain’s master circadian clock lives. There are a lot of phenomena in the body, whether alertness, body temperature, or hunger, that are at least partially dependent on our body’s sense of time. There’s a set of neurons that are part of the hypothalamus that autonomously track time, by a fascinating set of transcription-translation feedback loops involving proteins that ultimately shut down their own translation in a cycle that lasts about 24 hours. The cells in the SCN also synchronize with each other through neuropeptides, and diffuse the master clock signal throughout the body; melatonin from the pineal gland, but also via secondary regulation of the HPA (stress axis), and a neuropeptide called AVP.The intrinsic clock is not very precise, with a cycle that typically lasts a little more than 24 hours, something which was first verified in people living deep underground in abandoned mines (for science!). One factor that ultimately resets the clock is input from a set of neurons  inside the retina that project to this nucleus. Those cells are called intrinsically photosensitive retinal ganglion cells (ipRGCs). Breaking this down:Intrinsically photosensitiveoften said to be sensitive to blueIt’s not sensitive to blue, it’s sensitive to cyan (and blue and green)Unless your strategy is to create a photo-lab-like screen in pure black and red, or wear deep-red-tinted glasses, it’s unlikely that a pure colorshift strategy will cut out that big of a chunk of the spectrum. The PsyCalibrator paper’s appendixI tested this with Apple’s Night Shift on my M1 Macbook Air. I was surprised to find that the mapping function is very simple: LMS responses can be mapped linearly via matrix multiplication between standard and night-shifted colors (R^2 = .997). This is surprising because Night Shift could do all sorts of complicated things, like a nonlinear look-up table (LUT). As luck would have it, the transformation is linear, and more than that, almost diagonal, facilitating analysis. L         M         S
L [ +0.979   -0.116   +0.013 ]
M [ +0.162   +0.636   -0.022 ]
S [ +0.140   -0.102   +0.384 ]
Reading off the diagonal, to a good approximation, it tells us that:Night Shift maintains L (red) luminanceNight Shift decreases M (green) luminance by about 40%Night Shift decreases S (blue) luminance by about 60%color metamerismNagaro et al (2019)6 orders of magnitude of luminancePhillips et al. (2019)They found that the curve is quite shallow. Halving the luminance, at best (around 20 lux baseline) might get you from 50% to 25% melatonin suppression.the biggest study I could find of Night Shift mode (still a pretty small study) found little effect on sleepismy.blueismy.blue is a website I made to settle an argument with my wifeYou can see this in aggregate in the mean thresholds as a function of solar time. Solar time is a standardized time basis where midnight corresponds to solar midnight, noon to solar noon, 6AM to sunrise, 6PM to sunset. Thresholds are stable during the day, but jump around at night, in the direction that you would expect (they rise, especially on platforms with built-in filtering like Mac & iPhone).So I did something a little cheeky: I fit a mixture model for the threshold with two bumps: one for regular users, and one for blue-filter users 15 degrees to the right. Importantly, I fit different mixture weights depending on solar time. Perhaps I could have used a mixture of t-distributions instead, there are mix shifts I haven’t fully thought about, etc. Still, it’s an interesting data point.As you can see, the proportion of blue-light filter users in my sample is quite large; at its peak during the night, 25% of iPhone users and 33% of Mac users were on Night Shift. I’ll note that the proportion of estimated blue filter never goes to exactly 0 during the day, so some putative blue light filter users might have true idiosyncratic thresholds on monitors with normal colors, or wildly miscalibrated or broken screens. On the other hand, the proportion at night is likely an undercount, since some would have (wisely, as instructed) turned off their blue light filters prior to taking the test. My guess is it probably comes out as a wash. Overall, it means that blue light filters are quite commonly used, especially on platforms that them built in (e.g. Mac).Disclaimer: I’m not an MD, nor a sleep specialist, this is not medical advice. Talk to your MD.If it feels like it’s almost too much (e.g. the backgrounds are typically shifted to dark gray rather than pure black), remember that light perception is logarithmic, and that raw RGB values are translated to screen luminance via a gamma function whose exponent on Mac is 2.2. That means that dark grays can have far less light in absolute terms than the number implied by a linear scale. For instance, the dark gray #101010 has a relative luminance of (1/16)^2.2 compared to white #ffffff; about ~450 times darker.That’s all great, but there are websites that still don’t have dark modes. It doesn’t make any sense in 2026 that Gmail doesn’t have a dark mode. If the activity you’re doing most at night is reading email, you might consider an alternative email client.The screen brightness control on Mac has 17 notches, with the lowest corresponding to completely black. Here’s how much light I measured from the SpyderX with a dark grey uniform stimulus as a function of this setting. The first half of the scale is approximately exponential, the second half linear. You can decrease the amount of light coming from your screen by more than half simply by dimming the screen by several notches. Just make sure, if you use a laptop/second screen combo like I do, that the brightness of the second screen is synced to your primary screen; I set up the free MonitorControl app recently to do this.Research in rodentsLED light is incredibly cheap these daysIf you stack the three previous advice sections together, you could gain as much as 2-3 order of magnitude peak-to-trough luminance throughout your day. That might be enough to support your circadian rhythm health. If that’s not enough, however, recall that one of the ultimate outputs of the SCN is through the pineal gland, which releases of melatonin. Partly, it’s the inhibition of circulating melatonin that is causally responsible for sleep phase shift. It’s possible to remedy that by taking exogenous melatonin an hour before bed to facilitate the onset of sleep.Be very wary of melatonin doseSlate Star Codex post]]></content:encoded></item><item><title>Uncovering insiders and alpha on Polymarket with AI</title><link>https://twitter.com/peterjliu/status/2024901585806225723</link><author>somerandomness</author><category>hn</category><pubDate>Fri, 20 Feb 2026 18:11:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Making frontier cybersecurity capabilities available to defenders</title><link>https://www.anthropic.com/news/claude-code-security</link><author>surprisetalk</author><category>hn</category><pubDate>Fri, 20 Feb 2026 18:03:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[, a new capability built into Claude Code on the web, is now available in a limited research preview. It scans codebases for security vulnerabilities and suggests targeted software patches for human review, allowing teams to find and fix security issues that traditional methods often miss.Security teams face a common challenge: too many software vulnerabilities and not enough people to address them. Existing analysis tools help, but only to a point, as they usually look for known patterns. Finding the subtle, context-dependent vulnerabilities that are often exploited by attackers requires skilled human researchers, who are dealing with ever-expanding backlogs.AI is beginning to change that calculus. We’ve recently shown that Claude can detect novel, high-severity vulnerabilities. But the same capabilities that help defenders find and fix vulnerabilities could help attackers exploit them.Claude Code Security is intended to put this power squarely in the hands of defenders and protect code against this new category of AI-enabled attack. We’re releasing it as a limited research preview to Enterprise and Team customers, with expedited access for maintainers of open-source repositories, so we can work together to refine its capabilities and ensure it is deployed responsibly.How Claude Code Security worksStatic analysis—a widely deployed form of automated security testing—is typically rule-based, meaning it matches code against known vulnerability patterns. That catches common issues, like exposed passwords or outdated encryption, but often misses more complex vulnerabilities, like flaws in business logic or broken access control.Rather than scanning for known patterns, Claude Code Security reads and reasons about your code the way a human security researcher would: understanding how components interact, tracing how data moves through your application, and catching complex vulnerabilities that rule-based tools miss.Every finding goes through a multi-stage verification process before it reaches an analyst. Claude re-examines each result, attempting to prove or disprove its own findings and filter out false positives. Findings are also assigned severity ratings so teams can focus on the most important fixes first.Validated findings appear in the Claude Code Security dashboard, where teams can review them, inspect the suggested patches, and approve fixes. Because these issues often involve nuances that are difficult to assess from source code alone, Claude also provides a confidence rating for each finding. Nothing is applied without human approval: Claude Code Security identifies problems and suggests solutions, but developers always make the call.Using Claude for cybersecurityClaude Code Security builds on more than a year of research into Claude’s cybersecurity capabilities. Our Frontier Red Team has been stress-testing these abilities systematically: entering Claude in competitive Capture-the-Flag events, partnering with Pacific Northwest National Laboratory to experiment with using AI to defend critical infrastructure, and refining Claude’s ability to find and patch real vulnerabilities in code.Claude’s cyberdefensive abilities have improved substantially as a result. Using Claude Opus 4.6, released earlier this month, our team found over 500 vulnerabilities in production open-source codebases—bugs that had gone undetected for decades, despite years of expert review. We’re working through triage and responsible disclosure with maintainers now, and we plan to expand our security work with the open-source community.We also use Claude to review our own code, and we’ve found it to be extremely effective at securing Anthropic’s systems. We built Claude Code Security to make those same defensive capabilities more widely available. And since it’s built on Claude Code, teams can review findings and iterate on fixes within the tools they already use.This is a pivotal time for cybersecurity. We expect that a significant share of the world’s code will be scanned by AI in the near future, given how effective models have become at finding long-hidden bugs and security issues.Attackers will use AI to find exploitable weaknesses faster than ever. But defenders who move quickly can find those same weaknesses, patch them, and reduce the risk of an attack. Claude Code Security is one step towards our goal of more secure codebases and a higher security baseline across the industry.We’re opening a limited research preview of Claude Code Security to Enterprise and Team customers today. Participants will get early access and collaborate directly with our team to hone the tool’s capabilities. We also encourage open-source maintainers to apply for free, expedited access.To learn more, visit claude.com/solutions/claude-code-security.]]></content:encoded></item><item><title>Keep Android Open</title><link>https://f-droid.org/2026/02/20/twif.html</link><author>LorenDB</author><category>hn</category><pubDate>Fri, 20 Feb 2026 17:58:51 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[TWIF curated on Friday, 20 Feb 2026, Week 8During our talks with F-Droid users at FOSDEM26 we were baffled to learn most were relieved that Google has canceled their plans to lock-down Android. Because no such thing actually happened, the plans announced last August are still scheduled to take place. We see a battle of PR campaigns and whomever has the last post out remains in the media memory as the truth, and having journalists just copy/paste Google posts serves no one. Said what? That there’s a magical “advanced flow”? Did you see it? Did anyone experience it? When is it scheduled to be released? Was it part of Android 16 QPR2 in December? Of 16 QPR3 Beta 2.1 last week? Of Android 17 Beta 1? No? That’s the issue… As time marches on people were left with the impression that everything was done, fixed, Google “wasn’t evil” after all, this time, yay!While we all have bad memories of “banners” as the dreaded ad delivery medium of the Internet, after FOSDEM we decided that we have to raise the issue back and have everyone, who cares about Android as an open platform, informed that we are running out of time until Google becomes the gate-keeper of all users devices.Hence, the website and starting today our clients, with the updates of  and , feature a banner that reminds everyone how little time we have and how to voice their concerns to whatever local authority is able to understand the dangers of this path Android is led to.We are not alone in our fight, IzzyOnDroid added a banner too, more F-Droid clients will add the warning banner soon and other app downloaders, like , already have an in-app warning dialogue.Regarding  rewrite, development continues with a new release :Export installed apps list as CSVAdd install history featureAdd mirror chooser settingAdd prevent screenshots settingShow tool-tips for all app bar buttonsCreate 3-dot overflow menu for My Apps for less frequently used actionsPersist sort order of My AppsAdapt strings according to Material Design 3 guidelinesApply string suggestions (Thanks Lucas)Fix missing icon bug in pre-approval dialogNote that if you are already using F-Droid Basic version , you won’t receive this update automatically. You need to navigate to the app inside F-Droid and toggle “Allow beta updates” in top right three dot menu.In apps news, we’re slowly getting back on track with post Debian upgrade fixes (if your app still uses Java 17 is there a chance you can upgrade to 21?) and post FOSDEM delays. Every app is important to us, yet actions like the Google one above waste the time we could have put to better use in Gitlab.  was updated to  after a two year hiatus. and  were updated to  improving on cleaning up after banned users, a better QR workflow and better tablet rotation support. These are nice, but another change raises our interest, “Play Store flavor: Stop using Google library and interface directly with Google Play Service via IPC”. Sounds interesting for your app too? Is this a path to having one single version for both F-Droid and Play that is fully FLOSS? We don’t know yet, but we salute any trick that removes another proprietary dependency from the code. If curious feel free to take a look at the commit. was updated to  adding some welcomed fixes. If your game world started flickering after the last update make sure to update.But are you following the server side too?  was just released adding a plethora of features. If you want to read about them, see the 30 minutes post here or watch the one hour long video presentation from the team here. was updated to  adding more control to auto-connects, countries and cities. Also all connections are handled now by WireGuard and Stealth protocols as the older OpenVPN was removed making the app almost 40% smaller. was updated to  with a bit of code polish. Unfortunately for Android 7 users, the app now needs Android 8 or later. was updated to  with many fixes. But Vishal praised the duplicate remover, the default auto de-duplication function and found that the bug that made deleted messages reappear is fixed. was updated to  after a 2 year pause. It’s just a fixes release, updating translations and making the app compatible with Android 12 and later versions.Chord Shift: Shift plain text notesOpenAthena™ for Android: OpenAthena™ lets common drones spot precise locationsTibetan Keyboard: Keyboard for Tibetan scriptTibetan Pad: Read Tibetan script: A native Android app for NeoDB designed with Material 3/YouThank you for reading this week’s TWIF 🙂Please subscribe to the RSS feed in your favourite RSS application to be updated of new TWIFs when they come up.You are welcome to join the TWIF forum thread. If you have any news from the community, post it there, maybe it will be featured next week 😉To help support F-Droid, please check out the donation page and contribute what you can.]]></content:encoded></item><item><title>Lil&apos; Fun Langs</title><link>https://taylor.town/scrapscript-000</link><author>surprisetalk</author><category>hn</category><pubDate>Fri, 20 Feb 2026 17:34:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I've encountered tiny implementations of Forth, Lisp, C, Prolog, etc., but never
"milliHaskell".Parser, codegen (SSE/NEON)First-class functions (closures)Closure conversion, runtimeRecursive functions (let rec)Type inference (occurs check), codegenParser, type inference, codegenParser, runtime (bounds checking)Monomorphic type inferencePolymorphic type inference (HM)Generalization, instantiationParser, type checker, runtime (tagging)Exhaustiveness check, case treesPattern matching (optimized)Dictionary passing, instance resolutionModules (functors/signatures)Effect typing, runtime supportPolar types, biunificationThunks, memoization runtimeGarbage collection (Cheney)Codegen (jump instead of call)Coverage analysis, termination checkerWrite You a Haskell (and
sequel): builds a Haskell
subset incrementally: lambda calculus → STLC → HM inference → ADTs → pattern
matching → type classes → STG → LLVM.The ZINC experiment: the
foundational paper behind OCaml's bytecode compiler. The ZINC abstract machine
uses ~140 instructions and 7 registers. Implementations include OMicroB
(running OCaml bytecode on PIC18 microcontrollers with <10KB RAM) and
HardCaml-Zinc (hardware implementation).Elaboration Zoo:
progressive dependent type checking implementations, each a single Haskell
file of 200–800 lines, from basic NbE through holes, implicit arguments, and
first-class polymorphism. The best resource for understanding modern
elaboration. Its companion smalltt
(~1–2K LOC Haskell) is a complete dependent type elaborator with
normalization-by-evaluation.Modern Compiler Implementation in ML:
the Tiger language compiler covers every phase from lexing through
graph-coloring register allocation in ~5,000–8,000 LOC of SML. Multiple GitHub
implementations target x86-64 and RISC-V.If you want a milliHaskell, all your inspiration/ingredients are right here.🤖 The most extreme capability-to-size ratio in this list — a complete
  Calculus of Constructions (the type theory at the top of the lambda cube) with
  bidirectional typing, dependent function types, and a type-in-type universe,
  all in a single OCaml gist of ~60–80 lines. It can express length-indexed
  vectors and other dependently typed programs. Not ML-family per se, but it
  demonstrates that full dependent types need not be complex to implement.🤖 MiniML demonstrates the absolute floor for a native-code ML compiler. Using
  Camlp4 for parsing and OCaml's LLVM bindings, it supports integer arithmetic,
  conditionals, and recursive first-order functions. Xavier Leroy noted the
  critical caveat: this is not truly "Mini-ML" since it lacks higher-order
  first-class functions — adding closures and garbage collection would
  significantly expand the codebase. Still, it shows what LLVM enables in ~100
  lines.🤖  by Martin Grabmüller (~300 LOC, literate
  Haskell) is the canonical educational implementation of Algorithm W for
  Hindley-Milner type inference. Self-contained, well-commented, and widely
  referenced — this is where most people first implement HM inference.🤖 A collection of standalone implementations of several inference algorithms
  in OCaml (~300–600 LOC total): basic Algorithm W,  (the
  technique foundational to Elm's original type system), and HMF (first-class
  polymorphism with partial inference). Each variant is self-contained in a
  single directory. Where Algorithm W Step by Step teaches you  algorithm
  well, this repository shows you what changes when you swap in more powerful
  type system features.🤖 A progressive collection of single-file lambda calculus implementations in
  Haskell (~200–900 LOC each) by Solomon Bothwell. Starts with simply typed
  evaluation and builds incrementally through bidirectional typechecking,
  normalization by evaluation (NbE), System T, records with depth subtyping, and
  nominal inductive types with dependent pattern matching. Each implementation
  is self-contained. Where tomprimozic/type-systems varies the , this repository varies the  while keeping
  bidirectional checking as the constant.🤖 Typing Haskell in Haskell by Mark P. Jones is the definitive executable
  specification of Haskell 98's complete type system in just 429 lines of core
Haskell. It covers kinds, qualified types, type classes, pattern matching
  types, binding groups, mutual recursion, and defaulting. For context, the Hugs
  type checker implementing the same semantics spans 90+ pages of C. THIH is a
  type checker only (no evaluation), but its density of specification per line
  of code is unmatched.🤖  Lionel Parreaux's clean reimplementation of Stephen
  Dolan's MLsub — algebraic subtyping that adds union and intersection types to
  Hindley-Milner while preserving principal types. No annotations required. The
  original MLsub won POPL 2017; Simple-sub distills it into an ICFP 2020 Pearl
  that's small enough to read in one sitting. The ancestor of MLscript, which
  grows the idea into a full language with OOP and TypeScript interop.🤖  Implements a lazy, purely functional language
  with parametric polymorphism and HM type inference. Its sibling 
  (~300–500 LOC) includes a compiler targeting an abstract machine. Both are
  part of Andrej Bauer's Programming Languages Zoo, which contains 12+ miniature
  language implementations, each a few hundred lines of OCaml, covering
  everything from untyped lambda calculus to call-by-push-value.🤖 ~500 LOC JavaScript interpreter, full implementation in Gleam. EYG
  ("Eat Your Greens") by Peter Saxton prioritizes predictability, portability,
  and crash-free programs. It uses row-typed inference (HM extended with row
  polymorphism), algebraic effects as the sole FFI mechanism, and closure
  serialization — functions can be sent to other machines for tierless
  client/server programming. The most distinctive feature: programs are stored
  as JSON ASTs, not text files. A structural editor makes it impossible to write
  syntactically invalid programs.🤖 An OCaml subset with HM type inference that compiles to WebAssembly,
  implemented in TypeScript. Small and self-contained — unusual for having a
  TypeScript host language rather than the OCaml/Haskell norm. A good starting
  point if you want to understand ML compilation targeting the browser.🤖  Packs a lexer, parser, interpreter, and full
  polymorphic HM type checker into under 700 lines of SML. Referenced on Lambda
  the Ultimate, this may be the smallest complete implementation with genuine
Hindley-Milner inference, though the original download link appears to have
  gone stale.🤖 The original algebraic effects language (2012) by Andrej Bauer and Matija
  Pretnar. OCaml syntax with effect handlers as first-class constructs — you
  declare effect operations, then install handlers that give them meaning. This
  is where the idea was first made concrete in a running implementation. Koka,
  Frank, OCaml 5's effect handlers, and virtually every subsequent algebraic
  effects system trace lineage here.🤖  (POPL 2017) by Sam Lindley, Conor McBride, and Craig
  McLaughlin. A strict effectful functional language where functions are
  handlers that handle zero effects — and multihandlers generalize function
  abstraction to handle multiple effect interfaces simultaneously. The insight:
  the boundary between "function" and "effect handler" is artificial.
  Implemented in Haskell. Lindley describes it as "the one I'm most fond of"
  while noting it's "basically unmaintained." That tension between conceptual
  elegance and practical neglect is the story of many languages on this list.🤖 A JSON superset with bidirectional type checking and row polymorphism, by
  Gabriella Gonzalez (author of Dhall). Designed explicitly as a
  "ready-to-fork" language skeleton — if you need a typed DSL, clone Grace
  and customize it. Has open records, open unions (polymorphic variants), and a
  clean Haskell codebase that reads like a tutorial. No Hindley-Milner per se
  (bidirectional instead), but closely related.🤖 A Haskell-like language implemented entirely as  via the
  "Type Systems as Macros" technique, by Alexis King. Bidirectional type
  inference, algebraic datatypes, pattern matching, typeclasses, higher-kinded
  types, and higher-rank polymorphism — all implemented not as a separate
  type-checker pass but as macro expansion. The meta-angle is the story: types
  as macros rather than a traditional elaboration pipeline.🤖 A content-addressable pure functional language where every expression
  reduces to a cryptographic hash, stored in a decentralized "scrapyard"
  registry and referenced by hash or alias. The implementation is a
  ~1,300-line dependency-free Python interpreter in a single file, with a
  baseline compiler to C (~500 LOC) and an SSA IR with SCCP/DCE optimization
  (~1,000 LOC). Pattern matching is the sole control-flow mechanism. Compiles to
  C, WebAssembly, and Cosmopolitan portable executables. Implemented primarily
  by Max Bernstein.🤖 ~2,000 LOC, OCaml → native code. The gold standard for
  capability-to-code-size ratio. Written by Eijiro Sumii at Tohoku University,
  it implements a strict, higher-order functional language with type inference,
  closures, tuples, arrays, tail-call optimization, inline expansion, constant
  folding, and graph-coloring register allocation. It compiles to SPARC,
  PowerPC, and x86 assembly. On benchmarks including a ray tracer,
  MinCaml-compiled code runs within 2× of GCC and OCaml's  —
  sometimes faster. The deliberate trade-off: it omits polymorphism, algebraic
  data types, and pattern matching. Used in undergraduate compiler courses at
  the University of Tokyo since 2001, where students build ray tracers compiled
  by their own compilers running on custom CPUs.: "MinCaml: A Simple and Efficient Compiler for a Minimal Functional
Language" (FDPE 2005): gocaml (Go + LLVM
reimplementation), miniml (OCaml + LLVM,
~1,500 LOC, adds LLVM backend to MinCaml's architecture)🤖 ~2,000 lines of Haskell + 350 lines of C. Arguably the most remarkable
  bootstrapping achievement in this space. Starting from a 350-SLOC C runtime
  that interprets combinatory logic, Lynn builds a chain of approximately 20
  progressively more capable compilers, each written in the subset of Haskell
  that the previous compiler can handle. The final compiler supports type
  inference, type classes, algebraic data types, pattern matching, guards, where
  clauses, monadic I/O, modules, and layout parsing — approaching . It compiles Haskell to combinatory logic via Kiselyov's bracket
  abstraction algorithm, with graph reduction evaluation. Later stages even
  target WebAssembly. The entire bootstrapping chain is reproducible from just a
  C compiler.🤖  Andreas Rossberg unified ML's core and module
  layers into a single language where modules are first-class values, types are
  values, and functors are ordinary functions. It elaborates to System Fω with
  HM-style inference. Won the ICFP Most Influential Paper Award in 2025. A
  proof-of-concept interpreter, not optimized, but a conceptual breakthrough in
  minimal surface area.🤖 A self-hosting OCaml subset compiler targeting native x86-64. ~3,000–5,000
  LOC. Supports pattern matching, algebraic data types, recursive functions, and
  closures. Does not implement type inference — it demonstrates the minimum
  OCaml subset needed for self-compilation.🤖 A total (non-Turing-complete) typed configuration language. ~4K LOC core
  Haskell. Normalization is guaranteed to terminate — you can always reduce a
  Dhall expression to a normal form, which means imports resolve, functions
  inline, and what you get is plain data. Based on a
  Calculus-of-Constructions-derived type theory with records, unions, and
  natural numbers. Has a formal specification and implementations in Haskell,
  Rust, Go, and Clojure.🤖 Combines HM type inference, algebraic data types, pattern matching,
  algebraic effects, and an ownership-like system for shared mutability. Written
  in Rust, it uses  for native code generation. Actively developed,
  aiming to bridge the Rust/OCaml divide.🤖 Surprisingly feature-rich for its size: generics, typeclasses, sum types,
  pattern matching, first-class functions, currying, algebraic effects,
  associated types, and totality checking. Its pipeline runs from lexing through
  HIR type inference to MIR monomorphization and bytecode execution. Written in
  Rust.🤖 A systems language with  and capability-based security. The
  linear type checker is ~600 lines. OCaml bootstrap compiler targeting C.
  Designed by Fernando Borretti to fit in one person's head — the spec is
  deliberately small enough that a single developer can understand the entire
  language. Not functional in the Haskell sense, but linear types make it
  adjacent. An experiment in "what if we took linear types seriously but kept
  the language small."🤖 A self-hosting OCaml subset compiler targeting native x86-64. ~5,000–8,000
  LOC. Adds records, variants, references, and garbage collection beyond what
  mlml supports. Triple self-hosting verified. Like mlml, it omits type
  inference — demonstrating the minimum OCaml needed for self-compilation.🤖 Adds ML-family features (algebraic data types, exhaustive pattern matching,
  Result/Option types) to Go's ecosystem by compiling to Go source code with
  Rust-like syntax. Written in Rust.🤖 A research experiment from the Topos Institute extending Martin-Löf Type
  Theory with native, first-class polynomial functors — the mathematical objects
  underlying deterministic state machines and interactive systems. Written in
  OCaml with Menhir parsing. Custom syntax for polynomial types (),
  morphism arrows, and wiring operators. Dependent types (Pi, Sigma), finite-set
  ADTs, and pattern matching via case elimination. An ended experiment, but a
  unique point in the design space: what happens when you make polynomial
  functors a language primitive rather than an encoding.🤖 ~7K LOC, self-hosted, compiles to JavaScript. A dependently typed
  language with Agda/Idris/Haskell-like syntax by Steve Dunham. Bidirectional
  typechecking with normalization by evaluation (based on Elaboration Zoo),
  typeclasses, ADTs with dependent pattern matching, case tree compilation,
  trampoline-based TCO for mutually tail-recursive functions, and erasure of
  compile-time-only values (0/ω quantities). Has a web playground and an LSP.
  The compiler is written in Newt itself. Built as a learning exercise, but the
  feature set — self-hosting, dependent types, typeclasses, erasure, LSP — puts
  it well beyond most pedagogical implementations.🤖  Andreas Rossberg's most faithful
  implementation of the Definition of Standard ML. It implements  including the full module system (signatures, structures, functors),
  mapping rule-by-rule to the formal Definition. Jeremy Yallop recommends it as
  the most readable SML implementation. It can be bundled into a single SML file
  and compiled by any SML implementation. A  branch demonstrates
  compilation to JavaScript.🤖 ~10,000–15,000 LOC, TypeScript. Implements the full SML core language
  in the browser: val/fun/datatype declarations, pattern matching, HM type
  inference, exceptions, and references. Used for teaching at Saarland
  University.🤖 By Lennart Augustsson (one of GHC's original creators) — the most complete
  "small" Haskell compiler alive today. It compiles an extended subset of
   including type classes, do-notation, deriving, record syntax,
  overloaded literals, and modules. It is fully self-hosting and — critically —
  bootstrappable from only a C compiler (no pre-existing Haskell toolchain
  required). MicroHs translates Haskell to combinators executed by a C runtime.
  It has a JavaScript runtime target, a package manager (), and can
  compile real Hackage packages like QuickCheck. The codebase is not trivially
  small (estimated  across compiler, libraries, and
  runtime), but for what it does — a near-complete Haskell compiler
  bootstrappable from C — it is remarkably compact.]]></content:encoded></item><item><title>Tesla has to pay historic $243M judgement over Autopilot crash, judge says</title><link>https://electrek.co/2026/02/20/tesla-has-to-pay-historical-243-million-judgement-over-autopilot-crash-judge-says/</link><author>jeffbee</author><category>hn</category><pubDate>Fri, 20 Feb 2026 17:00:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A federal judge has rejected Tesla’s bid to overturn a $243 million jury verdict over a fatal 2019 Autopilot crash in Florida, dealing a significant blow to the automaker’s legal strategy as it faces a growing wave of lawsuits tied to its driver-assistance technology.U.S. District Judge Beth Bloom in Miami ruled that the evidence at trial “more than supported” the verdict and that Tesla raised no new arguments to justify setting it aside. The ruling, made public on Friday, means Tesla’s last hope to avoid paying the massive judgment at the trial court level has been exhausted.The crash and the verdictThe case stems from a deadly 2019 collision in Key Largo, Florida. George McGee was driving his Tesla Model S with Autopilot engaged when he dropped his phone and bent down to retrieve it. The vehicle blew through a stop sign and a flashing red light at approximately 62 mph, slamming into a parked Chevrolet Tahoe.The crash killed 22-year-old Naibel Benavides Leon and severely injured her boyfriend, Dillon Angulo, who was 26 at the time.In August 2025, a Miami federal jury found Tesla liable for the crash, assigning 33% of the blame to the automaker. The jury awarded $43 million in compensatory damages and $200 million in punitive damages — the first major plaintiff victory against Tesla in an Autopilot-related wrongful death case.Tesla had rejected a $60 million settlement offer before the trial. That decision cost the company dearly.In August 2025, Tesla’s lawyers filed a 71-page motion asking the court to throw out the verdict or grant a new trial. The company argued the verdict “flies in the face of basic Florida tort law, the Due Process Clause, and common sense.” Tesla also claimed that references to CEO Elon Musk’s statements about Autopilot during the trial misled the jury.Judge Bloom was unconvinced. Her ruling found Tesla presented no new arguments that warranted overturning the jury’s decision.Tesla has indicated it will appeal the verdict to a higher court. The company has also pointed to a pre-trial agreement that it claims would cap punitive damages at three times compensatory damages, which could reduce the final payout. But even under that interpretation, Tesla is still looking at a nine-figure judgment.Attorney Brett Schreiber, lead trial counsel for the plaintiffs, sent Electrek the following statement:  “We are of course pleased, but also completely unsurprised that the honorable Judge Bloom upheld the jury’s verdict finding Tesla liable for the integral role Autopilot and the company’s misrepresentations of its capabilities played in the crash that killed Naibel and permanently injured Dillon. Tesla’s arguments were simply an attempt to relitigate the court’s pre-trial rulings. We look forward to continuing our work holding Tesla accountable for its lies and gross misconduct in courts across America.” The Autopilot lawsuit floodgates are openThe legal pressure has been compounded by regulatory action. In December 2025, a California judge ruled that Tesla’s use of “Autopilot” in its marketing was misleading and violated state law, calling “Full Self-Driving” a name that is “actually, unambiguously false.”This lands weight to one of the main arguments used in lawsuits since the landmark case: Tesla has been misleading customers into thinking that its driver assist features (Autopilot and FSD) are more capable than they are – leading drivers to pay less attention.This ruling is important, but it’s not surprising. Tesla’s motion to throw out the verdict was always a long shot. The company essentially argued that references to Elon Musk’s own public claims about Autopilot, claims that Tesla actively used to sell the feature for years, were somehow unfair to present to a jury. Judge Bloom was right to reject that argument.The bigger picture here is that Tesla’s Autopilot legal liability is snowballing. The company rejected a $60 million settlement, lost a $243 million verdict, failed to overturn it, and now faces an appeal that will likely drag on while dozens of similar cases are working their way through the courts. Tesla is settling cases left and right to avoid further discovery and more damaging verdicts, but the financial exposure is mounting fast.We are talking billions of dollars in potential settlements and verdicts over the next few years.We’ve been covering Tesla’s Autopilot issues for years, and the pattern is clear: Tesla marketed Autopilot in a way that encouraged drivers to over-rely on it, failed to implement adequate safeguards, such as geofencing, and then blamed drivers when things went wrong. A jury, a federal judge, and now a California administrative judge have all reached the same conclusion. At some point, the cost of defending these cases, both financially and reputationally, will force Tesla to fundamentally change how it approaches driver-assistance technology.FTC: We use income earning auto affiliate links.More.]]></content:encoded></item><item><title>No Skill. No Taste</title><link>https://blog.kinglycrow.com/no-skill-no-taste/</link><author>ianbutler</author><category>hn</category><pubDate>Fri, 20 Feb 2026 16:13:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I was reading a thread on HN and I started writing this super long comment and rewriting and editing and thought, hey, if I'm doing this I clearly care enough about the state of Show HN and HN in general to write a post on it. I've written code since I was 11. I've worked on larger distributed systems, web apps, databases, search and more. I have many opinions on the transformation of our profession that is currently underway. Most of all, there is now an illusion of a lower barrier to entry. There is a magic quadrant made up of taste and skill. And too many people over estimate their taste[0] and their skill (or never care in the first place).LLMs have people everywhere super excited they can finally build their dream applications! The only problem is, no one needs their dream application. We see it everyday now, someone posts some obvious vibe coded app which is poorly crafted and clearly derivative of an idea so thoroughly saturated it's literally leaking. This is the lowest part of the quadrant. No skill and no taste. The overall suffusion of this into the broader scene rightly has the more sensitive of us up in arms. It's noise, it's spam, it's a perversion of the years of skill we've spent accruing. The only problem there is you might have skill, but do you have taste? This problem itself isn't new. HN of all places has always been a matter of taste. Things people found interesting made it to the front page, things they did not languished. You could build the most finely abstracted todo app of all time and your app would be dead on arrival. However, if you built something that resonated with a large enough group of people it never mattered how well built the app was or how technically complex.I've seen plenty of content on HN that could not have been more than a simple crud app that rocketed to the front page. What comes to mind immediately was a little app that died if someone hadn't posted a message on it in 24 hours. Inherently simple, but quite popular. It was pure taste.Taste and skill are related, the more saturated something is the higher skill you need to cross the taste threshold to make people care. It's not that there will never be another interesting todo app, it's that it has to be so tasteful as to cross our maximal standards and pre-existing expectations of them.LLMs have exposed this more thoroughly than any other time in tech so far. The sin isn't that someone uses an LLM to generate an application[1], vibe[2] or not. The sin is they lacked enough skill and enough taste to cross the actual threshold the rest of us need to see for the work to not be slop. An obvious and recent example of this is OpenClaw. It is a bit of a software nightmare (sorry Peter, I know you're good), but it's highly tasteful even being pretty vibey. People ate it up immediately and because there was such an interest the lack of technical soundness and security was overlooked (or begrudgingly put up with)The lack of taste only presents a problem now, because it's so much easier for people who thought they have more taste than they actually do to post every little idea they have. This is a real problem and I think it will taper off because people will learn proper etiquette or face disappointment. It's a massive educational period for a lot of people that we've all had years to internalize.It has the same stink of crypto on it right now that anyone can get rich. Most of them won't. This is the illusion of the lower barrier of entry, the barrier has always been taste and LLMs do nothing to remove this barrier. They amplify it.Anyway this is all to say whether you have skill or not, you better learn to be tasteful before you decide to slop all over everyone.[0] Taste is totally dependent on the group you're building for, discerning whether you have good taste and to whom is totally a process where you do have to put things out to people, but the bar has not now and not in my years ever been on the floor so I assert there's a minimal universal taste we all have and you should at least clear that before putting things out there.[1] I've been writing code for 20 years, I am super experienced in my domains and I review and sand off the edges, make changes myself etc. I vibe code almost 0% of the time.[2] Vibing means you need to have exceptional taste to cross the bar. I don't care if you do it, but you need to own the outcome.]]></content:encoded></item><item><title>Trump&apos;s global tariffs struck down by US Supreme Court</title><link>https://www.bbc.com/news/live/c0l9r67drg7t</link><author>blackguardx</author><category>hn</category><pubDate>Fri, 20 Feb 2026 15:27:38 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In addition to all of the economic ramifications, the Supreme Court ruling today also has a deep impact on President Trump's foreign policy agenda.Since returning to office more than a year ago, Trump has wielded tariffs to bolster American power on the world stage.That tool has largely been directed at trade, of course as Trump used sweeping global tariffs - the same ones struck down by the court on Friday - to force the UK, India and other key trading partners to negotiate new deals with the US.But Trump has also used tariffs, or the threat of them, to pressure other nations in ways not directly tied to the economy. The dispute over Greenland last month, for example, is a window into Trump's broader tariffs strategy.Trump called for the US to seize Greenland, only to face stiff resistance from the territory, Denmark (which controls it), and allies across Europe. He responded by threatening to levy taxes on goods from nations opposing his plan. The threat forced Denmark and Greenland to the negotiating table. And while that dispute remains unresolved, it showcased how Trump used tariffs as a tool in foreign affairs.The tariff threat-as-diplomacy tool hasn't entirely gone away with this decision. But it is greatly diminished.]]></content:encoded></item><item><title>Child&apos;s Play: Tech&apos;s new generation and the end of thinking</title><link>https://harpers.org/archive/2026/03/childs-play-sam-kriss-ai-startup-roy-lee/</link><author>ramimac</author><category>hn</category><pubDate>Fri, 20 Feb 2026 14:48:34 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Cluely and its co-founder Chungin “Roy” Lee were intensely, and intentionally, controversial. They’re no longer in San Francisco, having been essentially chased out of the city by the Planning Commission. The company is loathed seemingly out of proportion to what its product actually is, which is a janky, glitching interface for ChatGPT and other AI models. It’s not in a particularly glamorous market: Cluely is pitched at ordinary office drones in their thirties, working ordinary bullshit email jobs. It’s there to assist you in Zoom meetings and sales calls. It involves using AI to do your job for you, but this is what pretty much everyone is doing already. The cafés of San Francisco are full of highly paid tech workers clattering away on their keyboards; if you peer at their screens to get a closer look, you’ll generally find them copying and pasting material from a ChatGPT window. A lot of the other complaints about Cluely seem similarly hypocritical. The company is fueled by cheap viral hype, rather than an actual workable product—but this is a strange thing to get upset about when you consider that, back in the era of zero interest rates, Silicon Valley investors sank $120 million into something called the Juicero, a Wi-Fi-enabled smart juicer that made fresh juice from fruit sachets that you could, it turned out, just as easily squeeze between your hands.dates to be asked whether they’re “mimetic” or “agentic.”The future won’t reward effort. It’ll reward Suddenly, Roy seemed to acknowledge my presence. He offered me a tour. There was something he very badly wanted to impress on me, which was that Cluely cultivates a fratty, tech-bro atmosphere. Their pantry was piled high with bottles of something called Core Power Elite. I was offered a protein bar. The inside of the wrapper read . “We’re big believers in protein,” Roy said. “It’s impossible to get fat at Cluely. Nothing here has any fat.” The kitchen table was stacked with Labubu dolls. “It’s aesthetics,” Roy explained. “Women love Labubus, so we have Labubus.” He showed me his bedroom, which was in the office; many Cluely staffers also lived there. Everything was gray, although there wasn’t much. “I’m a big believer in minimalism,” he said. “Actually, no, I’m not. Not at all. I just don’t really care about interior decoration.” He had a chest of drawers, entirely empty except for a lint roller, pens, and, in one corner, a pink vibrator. “It’s for girls, you know,” said Roy. “I used to use this one on my ex.” There were also some objects that didn’t seem to belong in a frat house. In one of the common areas, a shelving unit was completely empty except for an anime figurine. You could peer up her plastic skirt and see the plastic underwear molded around her plastic buttocks. More figurines in frilly dresses seemed to have been scattered at random  dog  19-21 year old.” One picture showed him cuddling a giant Labubu.ment I gained consciousness that I would go start a company one day,” he told me. In elementary school in Georgia, he made money reselling Pokémon cards. Even then, he knew he was different from the people around him. “I could do things that other people couldn’t do,” he said. “Like whenever you learnpany was the dream of total control. “I don’t want up Cluely and see what it’s doing right now—can you share your screen or walk me through what you’re seeing?” I’d already said pretty much exactly this, but since it had shown up onscreen I read it out loud. Cluely helpfully transcribed my repeating its suggestion, and then suggested that I say, “Alright, I’ve got Cluely open—here’s what I’m looking at right now.” I’m not sure who exactly I was supposed to be saying this to—possibly myself. Somehow our conversation seemed already open. But I said it anyway, since I was now just repeating everything that came up on the screen. Cluely then told me to respond—to either it or myself; it was getting hard to tell at this point—by saying, “Great, I’m ready—just let me know what you want Cluely to check or help with next.” I started to worry that I would be trapped in this conversation forever, constantly repeating the machine’s words back to it as it pretended to be me. I told Roy that I wasn’t sure this was particularly useful. This seemed to confuse him. He asked, “I mean, what would you have wanted it to say?”I found it strange that Roy couldn’t see the glaring contradiction in his own project. Here was someone who reacted very violently to anyone who tried to tell him what to do. At the same time, his grand contribution to the world was a piece of software that told people what to do.Afterward, the entire surface of the earth is tiled with data centers as the alien intelligence feeds on the world, growing faster and faster without end.ot long before I arrived in the Bay Area, I’d been involved in a minor but intense dispute with the rationalist community over a piece of fiction I’d written that I’d failed to properly label as fiction. For rationalists, the divide between truth and falsehood is very important; dozens of rationalists spent several days raging at me online. Somehow, this ended up turning into an invitation for Friday night dinner at Valinor, Alexander’s former group home in Oakland, named for a realm in the Lord of the Rings books. (Rationalists, like termites, live in eusocial mounds.) The walls in Valinor were decorated with maps of video-game worlds, and the floors were strewn with children’s toys. Some of the children there—of which there were many—were being raised and homeschooled by the collective; one of the adults later explained to me how she’d managed to get the state to recognize her daughter as having four parents. As I walked in, a seven-year-old girl stared up at me in wide-eyed amazement. “Wow,” she said. “You’re really tall.” “I suppose I am,” I said. “Do you think one day you’ll ever be as tall as me?” She considered this for a moment, at which point someone who may or may not have been one of her mothers swooped in. “Well,” she asked the girl, “how would you answer this question with your knowledge of genetics?” Before dinner, Alexander chanted the  for Kabbalat Shabbat, but this was followed by a group rendition of “Landsailor,” a “love song celebrating trucking, supply lines, grocery stores, logistics, and abundance,” which has become part of Valinor’s liturgy:In one experiment, the developer Anthropic prompted its AI, Claude, to play on a Game Boy emulator, and found that Claude was extremely bad at the game. It kept trying to interact with enemies it had already defeated and walking into walls, getting stuck in the same corners of the map for hours or days on end. Another experiment let Claude run a vending machine in Anthropic’s headquarters. This one went even worse. The AI failed to make sure it was selling items at a profit, and had difficulty raising prices when demand was high. It also insisted on trying to fill the vending machine with what it called “specialty metal items” like tungsten cubes. When human workers failed to to fire them all. Before long, Claude was insisting that it was a real human. It claimed that it had attended a physical meeting with staff at 742 Evergreen Terrace, which is where the Simpsons live. By the end of the experiment, it was emailing the building’s security guards, telling them they could find it standing by the vending machine wearing a blue blazer and a red tie.But are humans really so great at exhibiting agency? After all, Cluely managed to raise tens of millions of dollars with a product that promises to take decision-making out of our hands. AI can’t function without instructions from humans, but an increasing number of humans seem incapable of functioning without AI. There are people who can’t order at a restaurant without having an AI scan the menu and tell them what to eat; people who no longer know how to talk to their friends and family and get ChatGPT to do it instead. For Alexander, this is a kind of Sartrean  “It’s terrifying to ask someone out,” he said. “What you want is to have the dating site that tells you that algorithmically you’ve been matched with this person, and then magically you have permission to talk to them. I think there’s something similar going on here with AI. Many of these people are smart enough that they could answer their own questions, but they want someone else to do it, because then they don’t have to have this terrifying encounter with their own humanity.” His best-case scenario for AI is essentially the antithesis of Roy’s: superintelligence that will actively refuse to give us everything we want, for the sake of preserving our humanity. “If we ever get AI that is strong enough to basically be God and solve all of our problems, it will need to use the same techniques that the actual God uses in terms of maintaining some distance. I do think it’s possible that the AI will be like, Now I am God. I’ve concluded that the actual God made exactly the right decision on how much evil to permit in the universe. Therefore I refuse to change anything.”look like. People will endure a lot of indignity to avoid being left behind without VC money when the great bifurcation takes place. Nobody wants to be Eric Zhu might be the most highly agentic person I’ve ever met.not entertaining under a microscope. What we do is we track the coordinates, so it  a sperm race—it’s just up-skinned.”) He’s planning on rolling the races out nationwide. Eric delivered a decent spiel about sperm motility as a proxy for health and how sperm racing drew attention to important issues. His venture seemed to be of a piece with a general trend toward obsessive masculine self-optimization à la RFK Jr. and Andrew Huberman. Still, to me it seemed obvious that Eric was doing it simply because he was amazed that he could. “I could build enterprise software or whatever,” he told me, “but what’s the craziest thing I could do? I would rather have an interesting life than a couple hundred million dollars in my bank account. Racing cum is definitely interesting.” I found Eric very hard not to like.Altman: “we are providing ChatGPT access to the entire federal workforce!”The lasagna summons visions of “smegma, Vesuvius, blood thinner marinara, the splotchy headpattern of a partisan, brainblown in his sleep.” He likes the Joycean compound. Shortly before I arrived at the him I thought he was irretrievably wasted. In fact, it turned out, he was just like that all the time.laugh. A chortle, if you will. I wasn’t thinking too hard about it. I don’t use that computer and I think video games are a waste of time. I spent all the money I made from going viral on Oasis tickets.” As far as he was concerned, the fact that tech people were tripping over themselves to take part in his stunt just confirmed his generally low impression of them. “They have too much money and nothing going on. TheyI told Donald the theory I’d been nursing—that he and Roy Lee were, in some sense, secret twins, viral phenomena gobbling up money and attention. I wasn’t sure if he’d like this. But to my surprise, he agreed. “I’m like Roy. I’m like Trump. We have the same swaggering energy. There is a kind of source code underlying reality, and this is what we understand. Your words have to have wings. Roy and I both know that social media is the last remaining outlet for self-creation and artistry. That’s what you have to understand about zoomers: we’re agents of chaos. We want to destroy the whole world.” Did Donald consider himself to be highly agentic? “We need to ban the word ‘agency.’ I’m a dog.”By now we’d ingested the most calorific cheesecake on the menu, the Ultimate Red Velvet Cake Cheesecake, which clocked in at 1,580 calories for a single slice. It was closing in on midnight, I was not feeling good, and Donald’s phone was nearly dead. He suggested that we go to the Cluely offices so he . “They’re my slaves.”Roy was still up. He didn’t seem particularly surprised to see me. He and most of the Cluely staff were flopped on a single sofa. All these people had become incredibly rich; previous generations of Silicon Valley founders would have been hosting exorbitant parties. In the Cluely office, they were playing  Did they spend every night there? “We’re all feminists here,” Roy said. “We’re usually up at four in the morning. We’re debating the struggles of women in today’s society.”For Roy, meanwhile, dating actually seemed to be a means to an end. “All the culture here is downstream of my belief that human beings are driven by biological desires. We have a pull-up bar and we go to the gym and we talk about dating, because nothing motivates people more than getting laid.” He was interested in physical beauty too, but only because “the better you look, the better you are as an entrepreneur. It’s all connected and beauty is everything. A lot of ugly men are just losers. The point of looking good is that society will reward you for that.” What about other kinds of beauty? Music, for instance? Roy had played the cello as a child. Did he still listen to classical music? “It doesn’t get my blood rushing the same way that EDM will.” His preferred genre was hardstyle—frantic thumping remixes of pop songs by the likes of Katy Perry and Taylor Swift. Is that the function of music, to get your blood rushing? “Yeah. I’m not a big fan of music to focus on things. I think it disturbs my flow. The only reason I will listen to music is to get me really hyped up when I’m lifting.” The two possible functions of music were, apparently, focus and hype. Everything for the higher goal of building a successful startup. What about life itself? Would Roy die for Cluely? “I would be happy dying at any age past twenty-five. After that it doesn’t matter, bro. If I live, I have extreme confidence in my ability to make three million dollars a year every year until I die.”unread, where he’d left them. He suggested that Roy might find something more valuable than dying for Cluely if he actually tried to read them. Roy disagreed: “I do not obtain value from reading books.” And anyway, he didn’t have the time. He was too busy keeping up with viral trends on TikTok. “You have to make the time,” Donald and I said, practically in unison. “It makes your life better,” I said. “Why don’t you go to Turkey to get a hair transplant?” Roy snapped. “That would make your life better.” “I don’t care about my hair,” I said. “Well,” said Roy, “I don’tworld. But there was a great sucking void where I walked back to my hotel, past signs that said things like one  and . My scalp was tingling. I’d lied when I’d told Roy that I didn’t care about my hair. Of course I care about my hair. Every day I grimace in the mirror as a little more of it vanishes from the top of my head. Whenever someone takes a photo of me from above or behind, I wince at the horrifying glimpse of pale, naked scalp. But I’d never done anything about it. I’d just watched and whinged and let it happen.ynotes” and sending “instant follow-up emails.” A lot of these functions are already being introduced by companies like Zoom; the main difference is that, by ]]></content:encoded></item><item><title>I found a useful Git one liner buried in leaked CIA developer docs</title><link>https://spencer.wtf/2026/02/20/cleaning-up-merged-git-branches-a-one-liner-from-the-cias-leaked-dev-docs.html</link><author>spencerldixon</author><category>hn</category><pubDate>Fri, 20 Feb 2026 14:03:06 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Most of it is fairly standard stuff, amending commits, stashing changes, using bisect. But one tip has lived in my  ever since.Over time, a local git repo accumulates stale branches. Every feature branch, hotfix, and experiment you’ve ever merged sits there doing nothing.  starts to look like a graveyard.You can list merged branches with:But deleting them one by one is tedious. The CIA’s dev team has a cleaner solution:git branch --merged | grep -v "\*\|master" | xargs -n 1 git branch -d
 — lists all local branches that have already been merged into the current branch — filters out the current branch () and  so you don’t delete either — deletes each remaining branch one at a time, safely (lowercase  won’t touch unmerged branches)Since most projects now use  instead of , you can update the command and exclude any other branches you frequently use:git branch --merged origin/main | grep -vE "^\s*(\*|main|develop)" | xargs -n 1 git branch -d
Run this from  after a deployment and your branch list goes from 40 entries back down to a handful.I keep this as a git alias so I don’t have to remember the syntax:alias ciaclean='git branch --merged origin/main | grep -vE "^\s*(\*|main|develop)" | xargs -n 1 git branch -d'
Then in your repo just run:Small thing, but one of those commands that quietly saves a few minutes every week and keeps me organised.]]></content:encoded></item><item><title>Show HN: A native macOS client for Hacker News, built with SwiftUI</title><link>https://github.com/IronsideXXVI/Hacker-News</link><author>IronsideXXVI</author><category>hn</category><pubDate>Fri, 20 Feb 2026 14:02:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Hey HN! I built a native macOS desktop client for Hacker News and I'm open-sourcing it under the MIT license.I spend a lot of time reading HN — I wanted something that felt like a proper Mac app: a sidebar for browsing stories, an integrated reader for articles, and comment threading — all in one window. Essentially, I wanted HN to feel like a first-class citizen on macOS, not a website I visit.- Split-view layout — stories in a sidebar on the left, articles and comments on the right, using the standard macOS NavigationSplitView pattern.- Built-in ad blocking — a precompiled WKContentRuleList blocks 14 major ad networks (DoubleClick, Google Syndication, Criteo, Taboola, Outbrain, Amazon ads, etc.) right in the WebKit layer. No extensions needed. Toggleable in settings.- Pop-up blocking — kills window.open() calls. Also toggleable.- HN account login — full authentication flow (login, account creation, password reset). Session is stored in the macOS Keychain, and cookies are injected into the WebView so you can upvote, comment, and submit stories while staying logged in.- Bookmarks — save stories locally for offline access. Persisted with Codable serialization, searchable and filterable independently.- Search and filtering — powered by the Algolia HN API. Filter by content type (All, Ask, Show, Jobs, Comments), date range (Today, Past Week, Past Month, All Time), and sort by hot or recent.- Scroll progress indicator — a small orange bar at the top tracks your reading progress via JavaScript-to-native messaging.- Auto-updates via Sparkle with EdDSA-signed updates served from GitHub Pages.- Dark mode — respects system appearance with CSS and meta tag injection.Tech details for the curious:The whole app is ~2,050 lines of Swift across 16 files. It uses the modern @Observable macro (not the old ObservableObject/Published pattern), structured concurrency with async/await and withThrowingTaskGroup for concurrent batch fetching, and SwiftUI throughout — no UIKit/AppKit bridges except for the WKWebView wrapper via NSViewRepresentable.Two APIs power the data: the official HN Firebase API for individual item/user fetches, and the Algolia Search API for feeds, filtering, and search. The Algolia API is surprisingly powerful for this — it lets you do date-range filtering, pagination, and full-text search that the Firebase API doesn't support.The release pipeline is a single GitHub Actions workflow (467 lines) that handles the full macOS distribution story: build and archive, code sign with Developer ID, notarize with Apple (with a 5-retry staple loop for ticket propagation delays), create a custom DMG with AppleScript-driven icon positioning, sign and notarize the DMG, generate an EdDSA Sparkle signature, create a GitHub Release, and deploy an updated appcast.xml to GitHub Pages.Getting macOS code signing and notarization working in CI was honestly the hardest part of this project. If anyone is distributing a macOS app outside the App Store via GitHub Actions, I'm happy to answer questions — the workflow is fully open source.I'd love feedback — especially on features you'd want to see. Some ideas I'm considering: keyboard-driven navigation (j/k to move between stories), a reader mode that strips articles down to text, and notification support for replies to your comments.]]></content:encoded></item><item><title>Ggml.ai joins Hugging Face to ensure the long-term progress of Local AI</title><link>https://github.com/ggml-org/llama.cpp/discussions/19759</link><author>lairv</author><category>hn</category><pubDate>Fri, 20 Feb 2026 13:51:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>PayPal discloses data breach that exposed user info for 6 months</title><link>https://www.bleepingcomputer.com/news/security/paypal-discloses-data-breach-exposing-users-personal-information/</link><author>el_duderino</author><category>hn</category><pubDate>Fri, 20 Feb 2026 13:22:32 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[PayPal is notifying customers of a data breach after a software error in a loan application exposed their sensitive personal information, including Social Security numbers, for nearly 6 months last year.The incident affected the PayPal Working Capital (PPWC) loan app, which provides small businesses with quick access to financing.PayPal discovered the breach on December 12, 2025, and determined that customers' names, email addresses, phone numbers, business addresses, Social Security numbers, and dates of birth had been exposed since July 1, 2025.The financial technology company said it has reversed the code change that caused the incident, blocking attackers' access to the data one day after discovering the breach."On December 12, 2025, PayPal identified that due to an error in its PayPal Working Capital ("PPWC") loan application, the PII of a small number of customers was exposed to unauthorized individuals during the timeframe of July 1, 2025 to December 13, 2025," PayPal said in breach notification letters sent to affected users."PayPal has since rolled back the code change responsible for this error, which potentially exposed the PII. We have not delayed this notification as a result of any law enforcement investigation."PayPal also detected unauthorized transactions on the accounts of a small number of customers as a direct result of the incident and has issued refunds to those affected.The company now offers affected users two years of free three-bureau credit monitoring and identity restoration services through Equifax, which require enrollment by June 30, 2026.Affected customers are advised to monitor their credit reports and their account activity for suspicious transactions. PayPal reminded users that it never requests account passwords, one-time codes, or other authentication credentials via phone, text, or email, a common tactic used in phishing attacks that often follow data breach disclosures.PayPal has also reset passwords for all impacted accounts and said that users will be prompted to create new credentials upon their next login if they have not already done so.In January 2023, PayPal notified customers of another data breach after a large-scale credential stuffing attack compromised 35,000 accounts between December 6 and December 8, 2022.Update February 20, 11:38 EST: After the article was published, a PayPal spokesperson told BleepingComputer that the company's systems were not breached and the incident exposed the data of roughly 100 customers."When there is a potential exposure of customer information, PayPal is required to notify affected customers," the spokesperson said. "In this case, PayPal’s systems were not compromised. As such, we contacted the approximately 100 customers who were potentially impacted to provide awareness on this matter.”Modern IT infrastructure moves faster than manual workflows can handle.In this new Tines guide, learn how your team can reduce hidden manual delays, improve reliability through automated response, and build and scale intelligent workflows on top of tools you already use.]]></content:encoded></item><item><title>Nvidia and OpenAI abandon unfinished $100B deal in favour of $30B investment</title><link>https://www.ft.com/content/dea24046-0a73-40b2-8246-5ac7b7a54323</link><author>zerosizedweasle</author><category>hn</category><pubDate>Fri, 20 Feb 2026 12:04:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Minions – Stripe&apos;s Coding Agents Part 2</title><link>https://stripe.dev/blog/minions-stripes-one-shot-end-to-end-coding-agents-part-2</link><author>ludovicianul</author><category>hn</category><pubDate>Fri, 20 Feb 2026 11:20:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Alistair is a software engineer on the Leverage team. The Leverage team builds surprisingly delightful internal products that Stripes can leverage to supercharge their productivity.]]></content:encoded></item><item><title>Silicon Valley engineers were indicted for allegedly sending secrets to Iran</title><link>https://www.cnbc.com/2026/02/20/three-engineers-charged-stealing-google-trade-secrets-data-iran-soc-snapdragon.html</link><author>giuliomagnifico</author><category>hn</category><pubDate>Fri, 20 Feb 2026 10:50:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[People walk near a sign outside of Google headquarters in Mountain View, California. Justin Sullivan | Getty Images News | Getty ImagesA federal grand jury indicted three Silicon Valley engineers on charges of stealing trade secrets from Google and other technology companies and transferring sensitive data to Iran, prosecutors said Thursday.Samaneh Ghandali, 41, her sister Soroor Ghandali, 32, and Mohammadjavad Khosravi, 40 — all residents of San Jose — were arrested Thursday and appeared in a federal district court the same day.The indictment identified the defendants as Iranian nationals. Soroor was in the U.S. on a nonimmigrant student visa. Samaneh later became a U.S. citizen, and Khosravi, her husband, became a U.S. legal permanent resident. Prosecutors said that Khosravi previously served in the Iranian army.The trio faces charges of conspiracy to commit trade secret theft, theft and attempted theft of trade secrets, and obstruction of justice, according to the U.S. attorney's office for the Northern District of California.Prosecutors alleged the three defendants exploited their positions at leading technology firms that develop mobile computer processors to obtain hundreds of confidential files, including materials related to processor security and cryptography. Samaneh and Soroor worked at Google before joining a third company identified only as Company 3. Khosravi worked at a separate firm referred to as Company 2, which develops system-on-chip (SoC) platforms such as the Snapdragon series for smartphones and other mobile devices.SoC is a semiconductor that integrates numerous components, such as graphics processing units and memory, into a power-efficient package. Common SoCs include Qualcomm's Snapdragon, found in most high-end Android phones, and Apple's A-series for iPhones.In a statement to CNBC, Google said it had detected the alleged theft through routine security monitoring before referring the case to law enforcement. "We have enhanced safeguards to protect our confidential information and immediately alerted law enforcement after discovering this incident," spokesman José Castañeda said. The tech giant also pointed to measures to protect its trade secrets, including restricting employees' access to sensitive information, two-factor authentication for work-related Google accounts and logging file transfers to third-party platforms such as Telegram. Alleged efforts to concealAuthorities alleged the defendants routed stolen files through a third-party communications platform to channels bearing each of their first names, before copying the material to personal devices, each other's work devices, and to Iran. "The method in which confidential data was transferred by the defendants involved deliberate steps to evade detection and conceal their identities," said FBI Special Agent in Charge Sanjay Virmani. The defendants sought to cover their tracks after Google's internal security systems flagged Samaneh's activity and revoked her access to company resources in August 2023. According to the indictment, she signed an affidavit falsely claiming she had not shared Google's confidential information outside the company. During that period, a personal laptop linked to Samaneh and Khosravi was used to search for methods to delete communications and to research how long mobile carriers retain message records, prosecutors said.The couple also allegedly photographed hundreds of computer screens containing confidential information from Google and Company 2, in what appeared to be an attempt at circumventing digital monitoring tools. On the night before the pair traveled to Iran in December 2023, Samaneh allegedly took about 24 photos of Khosravi's work computer screen containing Company 2's trade secrets, including its Snapdragon SoCs. Prosecutors allege that while in Iran, a device linked to Samaneh accessed those photographs, while Khosravi accessed additional Company 2's proprietary information, such as Snapdragon SoC hardware architecture.Prosecutors said in the indictment that Snapdragon SoC trade secrets had independent economic value because they were not generally known and could not be readily obtained from Company 2's competitors, who could benefit from their disclosure or use. If convicted, each defendant faces up to 10 years in prison for each trade secret charge and up to 20 years for obstruction of justice, along with fines of up to $250,000 per count.]]></content:encoded></item><item><title>The path to ubiquitous AI (17k tokens/sec)</title><link>https://taalas.com/the-path-to-ubiquitous-ai/</link><author>sidnarsipur</author><category>hn</category><pubDate>Fri, 20 Feb 2026 10:32:52 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Many believe AI is the real deal. In narrow domains, it already surpasses human performance. Used well, it is an unprecedented amplifier of human ingenuity and productivity. Its widespread adoption is hindered by two key barriers: high latency and astronomical cost. Interactions with language models lag far behind the pace of human cognition. Coding assistants can ponder for minutes, disrupting the programmer’s state of flow, and limiting effective human-AI collaboration. Meanwhile, automated agentic AI applications demand millisecond latencies, not leisurely human-paced responses.On the cost front, deploying modern models demands massive engineering and capital: room-sized supercomputers consuming hundreds of kilowatts, with liquid cooling, advanced packaging, stacked memory, complex I/O, and miles of cables. This scales to city-sized data center campuses and satellite networks, driving extreme operational expenses.Though society seems poised to build a dystopian future defined by data centers and adjacent power plants, history hints at a different direction. Past technological revolutions often started with grotesque prototypes, only to be eclipsed by breakthroughs yielding more practical outcomes.Consider ENIAC, a room-filling beast of vacuum tubes and cables. ENIAC introduced humanity to the magic of computing, but was slow, costly, and unscalable. The transistor sparked swift evolution, through workstations and PCs, to smartphones and ubiquitous computing, sparing the world from ENIAC sprawl.General-purpose computing entered the mainstream by becoming easy to build, fast, and cheap.Founded 2.5 years ago, Taalas developed a platform for transforming any AI model into custom silicon. From the moment a previously unseen model is received, it can be realized in hardware in only two months.The resulting Hardcore Models are an order of magnitude faster, cheaper, and lower power than software-based implementations.Taalas’ work is guided by the following core principles:Throughout the history of computation, deep specialization has been the surest path to extreme efficiency in critical workloads.AI inference is the most critical computational workload that humanity has ever faced, and the one that stands to gain the most from specialization.Its computational demands motivate total specialization: the production of optimal silicon for each individual model.2. Merging storage and computationModern inference hardware is constrained by an artificial divide: memory on one side, compute on the other, operating at fundamentally different speeds.This separation arises from a longstanding paradox. DRAM is far denser, and therefore cheaper, than the types of memory compatible with standard chip processes. However, accessing off-chip DRAM is thousands of times slower than on-chip memory. Conversely, compute chips cannot be built using DRAM processes.This divide underpins much of the complexity in modern inference hardware, creating the need for advanced packaging, HBM stacks, massive I/O bandwidth, soaring per-chip power consumption, and liquid cooling.Taalas eliminates this boundary. By unifying storage and compute on a single chip, at DRAM-level density, our architecture far surpasses what was previously possible.3. Radical simplificationBy removing the memory-compute boundary and tailoring silicon to each model, we were able to redesign the entire hardware stack from first principles.The result is a system that does not depend on difficult or exotic technologies, no HBM, advanced packaging, 3D stacking, liquid cooling, high speed IO.Engineering simplicity enables an order-of-magnitude reduction in total system cost.Guided by this technical philosophy, Taalas has created the world’s fastest, lowest cost/power inference platform.Taalas’ silicon Llama achieves 17K tokens/sec per user, nearly 10X faster than the current state of the art, while costing 20X less to build, and consuming 10X less power.We selected the Llama 3.1 8B as the basis for our first product due to its practicality. Its small size and open-source availability allowed us to harden the model with minimal logistical effort.While largely hard-wired for speed, the Llama retains flexibility through configurable context window size and support for fine-tuning via low-rank adapters (LoRAs).At the time we began work on our first generation design, low-precision parameter formats were not standardized. Our first silicon platform therefore used a custom 3-bit base data type. The Silicon Llama is aggressively quantized, combining 3-bit and 6-bit parameters, which introduces some quality degradations relative to GPU benchmarks.Our second-generation silicon adopts standard 4-bit floating-point formats, addressing these limitations while maintaining high speed and efficiency.Our second model, still based on Taalas’ first-generation silicon platform (HC1), will be a mid-sized reasoning LLM. It is expected in our labs this spring and will be integrated into our inference service shortly thereafter.Following this, a frontier LLM will be fabricated using our second-generation silicon platform (HC2). HC2 offers considerably higher density and even faster execution. Deployment is planned for winter.Instantaneous AI, in your hands todayOur debut model is clearly not on the leading edge, but we decided to release it as a beta service anyway – to let developers explore what becomes possible when LLM inference runs at sub-millisecond speed and near-zero cost.We believe that our service enables many classes of applications that were previously impractical, and want to encourage developers to experiment, and discover how these capabilities can be applied.Apply for access here, and engage with a system that removes traditional AI latency and cost constraints.On substance, team and craftAt its core, Taalas is a small group of long-time collaborators, many of whom have been together for over twenty years. To remain lean and focused, we rely on external partners who bring equal skill and decades of shared experience. The team grows slowly, with new team members joining through demonstrated excellence, alignment with our mission and respect for our established practices. Here, substance outweighs spectacle, craft outweighs scale, and rigor outweighs redundancy.Taalas is a precision strike, in a world where deep-tech startups approach their chosen problems like medieval armies besieging a walled city, with swarming numbers, overflowing coffers of venture capital, and a clamor of hype that drowns out clear thought.Our first product was brought to the world by a team of 24 team members, and a total of just $30M spent, of more than $200M raised. This achievement demonstrates that precisely defined goals and disciplined focus achieve what brute force cannot.Going forward, we will advance in the open. Our Llama inference platform is already in your hands. Future systems will follow as they mature. We will expose them early, iterate swiftly, and accept the rough edges.Innovation begins by questioning assumptions and venturing into the neglected corners of any solution space. That is the path we chose at Taalas.Our technology delivers step-function gains in performance, power efficiency, and cost.It reflects a fundamentally different architectural philosophy from the mainstream, one that redefines how AI systems are built and deployed.Disruptive advances rarely look familiar at first, and we are committed to helping the industry understand and adopt this new operating paradigm.Our first products, beginning with our hard-wired Llama and rapidly expanding to more capable models, eliminate high latency and cost, the core barriers to ubiquitous AI.We have placed instantaneous, ultra-low-cost intelligence in developers’ hands, and are eagerly looking forward to seeing what they build with it.]]></content:encoded></item><item><title>I tried building my startup entirely on European infrastructure</title><link>https://www.coinerella.com/made-in-eu-it-was-harder-than-i-thought/</link><author>willy__</author><category>hn</category><pubDate>Fri, 20 Feb 2026 09:02:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When I decided to build my startup on European infrastructure, I thought it would be a straightforward swap. Ditch AWS, pick some EU providers, done. How hard could it be?Turns out: harder than expected. Not impossible, I did it, but nobody talks about the weird friction points you hit along the way. This is that post.Data sovereignty, GDPR simplicity, not having your entire business dependent on three American hyperscalers, and honestly, a bit of stubbornness. I wanted to prove it could be done. The EU has real infrastructure companies building serious products. They deserve the traffic.Here's what I landed on after a lot of trial, error, and migration headaches. handles the core compute. Load balancers, VMs, and S3-compatible object storage. The pricing is almost absurdly good compared to AWS, and the performance is solid. If you've never spun up a Hetzner box, you're overpaying for cloud compute. fills the gaps Hetzner doesn't cover. I use their Transactional Email (TEM) service, Container Registry, a second S3 bucket for specific workloads, their observability stack, and even their domain registrar. One provider, multiple services, it simplifies billing if nothing else. is the unsung hero of this stack. CDN with distributed storage, DNS, image optimization, WAF, and DDoS protection, all from a company headquartered in Slovenia. Their edge network is genuinely impressive and their dashboard is a joy to use. Coming from Cloudflare, I felt at home rather quickly. powers our AI inference. If you need GPU compute in Europe without sending requests to , they're one of the few real options. handles authentication and identity. A German provider that gives you passkeys, social logins, and user management without reaching for Auth0 or Clerk. More on this in the "can't avoid" section — it doesn't eliminate American dependencies entirely, but it keeps the auth layer European.Self-hosting: Rancher, my belovedThis is where things get fun... and time-consuming. I self-host a surprising amount: for privacy-friendly analytics for customer management for secrets managementAll running on Kubernetes, with Rancher as the glue keeping the whole cluster sane.Is self-hosting more work than SaaS? Obviously. But it means my data stays exactly where I put it, and I'm not at the mercy of a provider's pricing changes or acquisition drama.For email,  keeps things encrypted and European.  watches the monitors so I can sleep.Transactional email with competitive pricing. This one surprised me. Sendgrid, Postmark, Mailgun, they all make it trivially easy and reasonably cheap. The EU options exist, but finding one that matches on deliverability, pricing, and developer experience took real effort. Scaleway's TEM works, but the ecosystem is thinner. Fewer templates, fewer integrations, less community knowledge to lean on when something goes wrong. If you live in GitHub's ecosystem Actions, Issues, code review workflows, the social graph... walking away feels like leaving a city you've lived in for a decade. You know where everything is. Gitea is actually excellent, and I'd recommend it without hesitation for the core git experience. But you'll miss the ecosystem. CI/CD pipelines need to be rebuilt. Integrations you took for granted don't exist. The muscle memory of  takes a while to unwire. This one is just baffling. Certain TLDs cost significantly more when purchased through European registrars. I'm talking 2-3x markups on extensions that are cheap everywhere else. I never got a satisfying explanation for why. If anyone knows, I'm genuinely curious.What you realistically can't avoidHere's the honest part. Some things are American and you just have to accept it:Google Ads and Apple's Developer Program. If you want to acquire users and distribute a mobile app, you're paying the toll to Mountain View and Cupertino. There is no European alternative to the App Store or Play Store. This is just the cost of doing business. Your users expect "Sign in with Google" and "Sign in with Apple." You can add email/password and passkeys, but removing social logins entirely is a conversion killer. Every one of those auth flows hits American servers. The silver lining: Hanko, a German identity provider, handles the auth layer itself, so at least your user management and session handling stay in Europe, even if the OAuth flow touches Google or Apple. If you want Claude, and I very much want Claude, that's Anthropic, that's the US. The EU AI ecosystem is growing, but for frontier models, the options are mostly American. You can run open-weight models on European inference providers, but if you want Claude, you're making a transatlantic API call.Yes, with caveats. My infrastructure costs are lower than they'd be on AWS. My data residency story is clean. I understand my stack deeply because I had to ... there's no "just click the AWS button" escape hatch.But it took longer than I expected. Every service I self-host is a service I maintain. Every EU provider I chose has a smaller community, thinner docs, and fewer Stack Overflow (or Claude) answers when things break at 2 AM.If you're thinking about doing this: go in with your eyes open. The EU infrastructure ecosystem is real and maturing fast. But "Made in EU" is still a choice you have to actively make, not one you can passively fall into. The defaults of the tech industry pull you west across the Atlantic, and swimming against that current takes effort.It's effort worth spending. But it is effort.If you curious to see the finished product, here it is: hank.parts.]]></content:encoded></item><item><title>Untapped Way to Learn a Codebase: Build a Visualizer</title><link>https://jimmyhmiller.com/learn-codebase-visualizer</link><author>andreabergia</author><category>hn</category><pubDate>Fri, 20 Feb 2026 08:52:17 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The biggest shock of my early career was just how much code I needed to read that others wrote. I had never dealt with this. I had a hard enough time understanding my own code. The idea of understanding hundreds of thousands or even millions of lines of code written by countless other people scared me. What I quickly learned is that you don't have to understand a codebase in its entirety to be effective in it. But just saying that is not super helpful. So rather than tell, I want to show.In this post, I'm going to walk you through how I learn an unfamiliar codebase. But I'll admit, this isn't precisely how I would do it today. After years of working on codebases, I've learned quite a lot of shortcuts. Things that come with experience that just don't translate for other people. So what I'm going to present is a reconstruction. I want to show bits and parts of how I go from knowing very little to gaining knowledge and ultimately, asking the right questions.To do this, I will use just a few techniques:Fixing things I find that are brokenReading to answer questionsI want to do this on a real codebase, so I've chosen one whose purpose and scope I'm generally familiar with. But one that I've never contributed to or read, Next.js. But I've chosen to be a bit more particular than that. I'm particularly interested in learning more about the Rust bundler setup (turbopack) that Next.js has been building out. So that's where we will concentrate our time.Being Clear About Our GoalTrying to learn a codebase is distinctly different from trying to simply fix a bug or add a feature. In post, we may use bugs, talk about features, make changes, etc. But we are not trying to contribute to the codebase, yet. Instead, we are trying to get our mind around how the codebase generally works. We aren't concerned with things like coding standards, common practices, or the development roadmap. We aren't even concerned with correctness. The changes we make are about seeing how the codebase responds so we can make sense of it.I find starting at  to be almost always completely unhelpful. From main, yes, we have a single entry point, but now we are asking ourselves to understand the whole. But things actually get worse when dealing with a large codebase like this. There isn't even one main. Which main would we choose? So instead, let's start by figuring out what our library even consists of.A couple of things to note. We have packages, crates, turbo, and turbopack. Crates are relevant here because we know we are interested in some of the Rust code, but we are also interested in turbopack in particular. A quick look at these shows that turbo, packages, and crates are probably not our target. Why do I say that? Because turbopack has its own crates folder.So there are 54 crates under turbopack.... This is beginning to feel a bit daunting. So why don't we take a step back and find a better starting point? One starting point I find particularly useful is a bug report.I found this by simply looking at recently opened issues. When I first found it, it had no comments on it. In fact, I find bug reports with only reproducing instructions to be the most useful. Remember, we are trying to learn, not fix a bug.So I spent a little time looking at the bug report. It is fairly clear. It does indeed reproduce. But it has a lot of code. So, as is often the case, it is useful to reduce it to the minimal case. So that's what I did. Here is the important code and the problem we are using to learn from. greeting  MyEnum  greeting MyEnum here is dead code. It should not show up in our final bundle. But when we do  and look for it, we get:If we instead do The code is completely gone from our build.So now we have our bug. But remember. Our goal here is not to fix the bug. But to understand the code. So our goal is going to be to use this little mini problem to understand what code is involved in this bug. To understand the different ways we could fix this bug. To understand why this bug happened in the first place. To understand some small slice of the turbopack codebase.So at each junction, we are going to resist the urge to simply find the offending code. We are going to take detours. We are going to ask questions. We hope that from the start of this process to the end, we no longer think of the code involved in this action as a black box. But we will intentionally leave ourselves with open questions. As I write these words, I have no idea where this will take us. I have not prepared this ahead of time. I am not telling you a fake tale from a codebase I already know. Yes, I will simplify and skip parts. But you will come along with me.The first step for understanding any project is getting some part of it running. Well, I say that, but in my day job, I've been at companies where this is a multi-day or week-long effort. Sometimes, because of a lack of access, sometimes from unclear instructions, if you find yourself in that situation, you now have a new task, understand it well enough to get it to build. Well, unfortunately, that is the scenario we find ourselves in.I can't think of a single one of these endeavors I've gone on to learn a codebase that didn't involve a completely undesirable, momentum-stopping side quest. For this one, it was as soon as I tried to make changes to the turbopack Rust code and get it working in my test project. There are instructions on how to do this. In fact, we even get an explanation on why it is a bit weird.Since Turbopack doesn't support symlinks when pointing outside of the workspace directory, it can be difficult to develop against a local Next.js version. Neither  nor  imports quite cut it. An alternative is to pack the Next.js version you want to test into a tarball and add it to the pnpm overrides of your test application. The following script will do it for you:pnpm pack-next --tar && pnpm unpack-next path/to/project
Okay, straightforward enough. I start by finding somewhere in the turbopack repo that I  will be called more than once, and I add the following:diff --git a/turbopack/crates/turbopack/src/lib.rs b/turbopack/crates/turbopack/src/lib.rs
index 20891acf86..8691f1951c 100644

@@ -80,6 +80,7 @@ async fn apply_module_type(
Yes. Very scientific, I know. But I've found this to be a rather effective method of making sure my changes are picked up. So I do that, make sure I've built and done the necessary things. I run swc-build-native  pack-next --tar  unpack-next /path/to/test/project
Then that script tells me to add some overrides and dependencies to my test project. I go to build my project and  does not show up at all...Understanding the Build SystemI will save you the fun details here of looking through this system. But I think it's important to mention a few things. First,  being a dependency immediately stood out to me. In my day job, I maintain a fork of swc (WHY???) for some custom stuff. I definitely won't pretend to be an expert on swc, but I know it's written in Rust. I know it's a native dependency. The changes I'm making are native dependencies. But I see no mention at all of turbopack. In fact, if I search in my test project, I get the following:So I have a sneaking suspicion my turbopack code should be in that tar. So let's look at the tar.Ummm. That seems a bit small... Let's look at what's inside.Okay, I think we found our problem. There's really nothing in this at all. Definitely no native code.Regex - Now You Have Two ProblemsAfter lots of searching, the culprit came down to: packageFiles globbedFilessimpleFiles packageFilesfff
    f  ffIn our case, the input came from this file and f was . Unfortunately, this little set + regex setup causes  to be filtered out. Why? Because it doesn't match the regex. This regex is looking for a  with characters after it. We have none. So since we are already in the set (we just added ourselves), we filter ourselves out.How do we solve this problem? There are countless answers, really. I had Claude whip me up one without regex. packageFiles globbedFilessimpleFiles packageFiles normalized fnormalizednormalized parent normalizedparent  parent parent
    parent parentBut my gut says the sorting lets us do this much simpler. But after this fix, let's look at the tar now:Much better. After this change, we can finally see  a lot.: As I wrote this article, someone fixed this in a bit of a different way. Keeping the regex and just changing to . Fairly practical decision.Okay, we now have something we can test. But where do we even begin? This is one reason we chose this bug. It gives a few avenues to go down. First, the report says that these enums are not being "tree-shaken." Is that the right term? One thing I've learned from experience is to never assume that the end user is using terms in the same manner as the codebase. So this can be a starting point, but it might be wrong.With some searching around, we can actually see that there is a configuration for turning turbopackTreeShaking on or off. nextConfig NextConfig 
  experimental
    turbopackTreeShakingIt was actually a bit hard to find exactly where the default for this was. It isn't actually documented. So let's just enable it and see what we get. Build error occurred
Error TurbopackInternalError: Failed to  app endpoint /page

Caused by:
- index out of bounds: the len is  but the index is 

Debug info:
- Execution of get_all_written_entrypoints_with_issues_operation failed
- Execution of EntrypointsOperation::new failed
- Execution of all_entrypoints_write_to_disk_operation failed
- Execution of Project::emit_all_output_assets failed
- Execution of *emit_assets failed
- Execution of all_assets_from_entries_operation failed
- Execution of *all_assets_from_entries failed
- Execution of output_assets_operation failed
- Execution of AppEndpoint as Endpoint::output failed
- Failed to  app endpoint /page
- Execution of AppEndpoint::output failed
- Execution of whole_app_module_graph_operation failed
- Execution of *ModuleGraph::from_single_graph failed
- Execution of *SingleModuleGraph::new_with_entries failed
- Execution of Project::get_all_entries failed
- Execution of AppEndpoint as Endpoint::entries failed
- Execution of get_app_page_entry failed
- Execution of *ProcessResult::module failed
- Execution of ModuleAssetContext as AssetContext::process failed
- Execution of EcmascriptModulePartAsset::select_part failed
- Execution of split_module failed
- index out of bounds: the len is  but the index is Well, I think we figured out that the default is off. So one option is that we never "tree shake" anything. But that seems wrong. At this point, I looked into tree shaking a bit in the codebase, and while I started to understand a few things, I've been at this point before. Sometimes it is good to go deep. But how much of this codebase do I really understand? If tree shaking is our culprit (seeming unlikely at this point), it might be good to know how code gets there. Here, we of course found a bug. But it is an experimental feature. Maybe we can come back and fix it? Maybe we can file a bug? Maybe this code just isn't at all ready for primetime. It's hard to know as an outsider.Our "search around the codebase" strategy failed. So now we try a different tactic. We know a couple of things.Our utilities.ts file is read and parsed.It ends up in a file under a "chunks" directory.We now have two points we can use to try to trace what happens. Let's start with parsing. Luckily, here it is straightforward: . When we look at this code, we can see that swc does the heavy lifting. First, it parses it into a TypeScript AST, then applies transforms to turn it into JavaScript. At this point, we don't write to a string, but if you edit the code and use an emitter, you see this: MyEnum 
    MyEnum
    MyEnum MyEnum greeting Now, to find where we write the chunks. In most programs, this would be pretty easy. Typically, there is a linear flow somewhere that just shows you the steps. Or if you can't piece one together, you can simply breakpoint and follow the flow. But Turbopack is a rather advanced system involving async Rust (more on this later). So, in keeping with the tradition of not trying to do things that rely too heavily on my knowledge, I have done the tried and true, log random things until they look relevant. And what I found made me realize that logging was not going to be enough. It was time to do my tried and true learning technique, visualization.Ever since my first job, I have been building custom tools to visualize codebases. Perhaps this is due to my aphantasia. I'm not really sure. Some of these visualizers make their way into general use for me. But more often than not, they are a means of understanding. When I applied for a job at Shopify working on YJIT, I built a simple visualizer but never got around to making it more useful than a learning tool. The same thing is true here, but this time, thanks to AI, it looks a bit more professional.This time, we want to give a bit more structure than what we'd be able to do with a simple print. We are trying to get events out that have a bunch of information. Mostly, we are interested in files and their contents over time. Looking through the codebase, we find that one key abstract is an ident; this will help us identify files. We will simply find points that seem interesting, make a corresponding event, make sure it has idents associated with it, and send that event over a WebSocket. Then, with that raw information, we can have our visualizer stitch together what exactly happens.If we take a look, we can see our code step through the process. And ultimately end up in the bundle despite not being used. If you notice, though, between steps 3 and 4, our code changed a bit. We lost this PURE annotation. Why?Luckily, because we tried to capture as much context as we could. We can see that a boolean "Scope Hoisting" has been enabled. Could that be related? If we turn it off, we instead see:Our pure annotation is kept around, and as a result, our code is eliminated. If we take a step back, this can make sense. Something during the parse step is creating a closure around our enum code, but when it does so, it is marking that as a "pure" closure, meaning it has no side effects. Later, because this annotation is dropped, the minifier doesn't know that it can just get rid of this closure. As I've been trying to find time to write this up, it seems that people on the bug report have found this on their own as well.So we've found the behavior of the bug. Now we need to understand why it is happening. Remember, we are fixing a bug as a means of understanding the software. Not just to fix a bug. So what exactly is going on? Well, we are trying to stitch together two libraries. Software bugs are way more likely to occur on these seams. In this case, after reading the code for a while, the problem becomes apparent.SWC parses our code and turns it into an AST. But if you take a look at an AST, comments are not a part of the AST. So instead, SWC stores comments off in a hashmap where we can look them up by byte pos. So for each node in the AST, it can see if there is a comment attached. But for the PURE comment, it doesn't actually need to look this comment up. It is not a unique comment that was in the source code. It is a pre-known meta comment. So rather than store each instance in the map, it makes a special value.`
 PUREu32MAX Now, this encoding scheme causes some problems for turbopack. Turbopack does not act on a single file; it acts across many files. In fact, for scope hoisting, we are trying to take files across modules and condense them into a single scope. So now, when we encounter one of these bytepos encodings, how do we know what module it belongs to?The obvious answer to many might be to simply make a tuple like , and while that certainly works, it does come with tradeoffs. One of these is memory footprint. I didn't find an exact reason. But given the focus on performance on turbopack, I'd imagine this is one of the main motivations. Instead, we get a fairly clever encoding of module and bytepos into a single BytePos, aka a u32. I won't get into the details of the representation here; it involves some condition stuff. But needless to say, now that we are going from something focusing on one file to focusing on multiple and trying to smuggle in this module_id into our BytePos, we ended up missing one detail, PURE. Now our pure value is being interpreted as some module at some very high position instead of the proper bytes.To fix this bug, I found the minimal fix was simply the following:With this our enum properly is marked as PURE and disappears from the output!Now remember, we aren't trying to make a bug fix. We are trying to understand the codebase. Is this the right fix? I'm not sure. I looked around the codebase, and there are a number of other swc sentinel values that I think need to also be handled (PLACEHOLDER and SYNTHESIZED). There is also the decoding path. For dummy, the decoding path panics. Should we do the same? Should we be handling pure at a higher level, where it never even goes through the encoder?: As I was writing this, someone else proposed a fix. As I was writing the article, I did see that others had started to figure out the things I had determined from my investigation. But I was not confident enough that it was the right fix yet. In fact, the PR differs a bit from my local fix. It does handle the other sentinel, but at a different layer. It also chooses to decode with module 0. Which felt a bit wrong to me. But again, these are decisions that people who work on this codebase long-term are better equipped to decide than me.I must admit that simply fixing this bug didn't quite help me understand the codebase. Not just because it is a fairly good size. But because I couldn't see this fundamental unit that everything was composed of. In some of the code snippets above, you will see types that mention Vc. This stands for ValueCell. There are a number of ways to try to understand these; you can check out the docs for turbo engine for some details. Or you can read the high-level overview that skips the implementation details for the most part. You can think of these cells like the cells in a spreadsheet. They provide a level of incremental computation. When the value of some cell updates, we can invalidate stuff. Unlike a spreadsheet, the turbo engine is lazy.I've worked with these kinds of systems before. Some were very explicitly modeled after spreadsheets. Others were based on rete networks or propagators. I am also immediately reminded of salsa from the Rust analyzer team. I've even worked with big, complicated non-computational graphs. But even with that background, I know myself, I've never been able to really understand these things until I can visualize them. And while a general network visualizer can be useful (and might actually be quite useful if I used the aggregate graph), I've found that for my understanding, I vastly prefer starting small and exploring out the edges of the graph. So that's what I did.But before we get to that visualization, I want to highlight something fantastic in the implementation: a central place for controlling a ton of the decisions that go into this system. The backend here lets us decide so many things about how the execution of our tasks will run. Because of this, we have one place we can insert a ton of tooling and begin to understand how this system works.As before, we are going to send things on a WebSocket. But unlike last time, our communication will actually be two-way. We are going to be controlling how the tasks run.In my little test project, I edited a file, and my visualizer displayed the following. Admittedly, it is a bit janky, but there are some nice features. First, on the left, we can see all the pending tasks. In this case, something has marked our file read as dirty, so we are trying to read the file. We can see the contents of a cell that this task has. And we can see the dependents of this task.Here is what it looks like once we release that task to run. We can now see 3 parse tasks have kicked off. Why 3? I'll be honest, I haven't looked into it. But a good visualization is about provoking questions, not only answering them. Did I get my visualization wrong because I misunderstood something about the system? Are there three different subsystems that want to parse, and we want to do them in parallel? Have we just accidentally triggered more parses than we should be?This is precisely what we want out of a visualizer. Is it perfect? No, would I ship this as a general visualizer? No. Am I happy with the style? Not in the least. But already it enables a look into the project I couldn't see before. Here we can actually watch the graph unfold as I execute more steps. What a fascinating view of a once opaque project.With this visualizer, I was able to make changes to my project and watch values as they flow through the systems. I made simple views for looking at code. If I extended this, I can imagine it being incredibly useful for debugging general issues, for seeing the ways in which things are scheduled, and for finding redundancies in the graph. Once I was able to visualize this, I really started to understand the codebase better. I was able to see all the values that didn't need to be recomputed when we made changes. The whole thing clicked.This was an exercise in exploring a new codebase that is a bit different of a process than I see others take. It isn't an easy process, it isn't quick. But I've found myself repeating this process over and over again. For the turborepo codebase, this is just the beginning. This exploration was done over a few weekends in the limited spare time I could find. But already I can start to put the big picture together. I can start to see how I could shape my tools to help me answer more questions. If you've never used tool building as a way to learn a codebase, I highly recommend it.Coda: A Call for More Interactive Dynamic SystemsOne thing I always realize as I go through this process is just how hard it is to work interactively with our current software. Our languages, our tools, our processes are all written without ways to live code, without ways to inspect their inner workings.It is also incredibly hard to find a productive UI environment for this kind of live exploration. The running state of the visualizer contains all the valuable information. Any system that needs you to retrace your steps to get the UI back to the state it was once in to visualize more is incredibly lacking. So I always find myself in the browser, but immediately, I am having to worry about performance. We have made massive strides in so many aspects of software development. I hope that we will fix this one as well.]]></content:encoded></item><item><title>Raspberry Pi Pico 2 at 873.5MHz with 3.05V Core Abuse</title><link>https://learn.pimoroni.com/article/overclocking-the-pico-2</link><author>Lwrless</author><category>hn</category><pubDate>Fri, 20 Feb 2026 08:48:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[How fast can RP2350 really go? Mike starts his Christmas holidays with a deep dive into Pico 2 overclocking, with the assistance of a seasonally appropriate amount of dry ice ❄️Picos overclock very well, and the original Raspberry Pi Pico will normally run at over 400MHz if given a 1.3V core voltage. On the RP2040 that is as far as you can easily go because the on-board voltage regulator is limited to supplying a maximum of 1.3V.When I first got my hands on the RP2350 datasheet, one thing I noticed rather quickly was that the voltage regulator could have the voltage limit disabled, so that voltages above 1.3V could be requested. And once I got a Pico 2, I was intrigued to see how it would behave at higher core voltage (and whether the magic smoke would come out if I raised the voltage too high!)Using this MicroPython script I was able to request different voltages from the regulator. To find out how fast the RP2350 would clock at a given voltage I ran a simple test that computed 100 factorial and checked the answer was correct, and then incremented the clock speed and continued until it stopped working.I then did a more rigorous test using the MicroPython performance benchmark to find out whether things were stable. Generally I had to back off the CPU clock by 20MHz or so in order for the performance benchmark to run multiple times cleanly, compared to how fast it would pass the very simple 100! test. I live bleated (that's what we're calling Bluesky posts for now) my experimentation and some details about the RP2350 as I was doing this.Voltage, max stable clock speed and temperature for those initial tests was as follows:This was the first time I'd really felt an RP2 chip getting hot - RP2040s running at 400 MHz or so and 1.3V only get a little warm.To combat the heat issue I added a tiny heatsink to the RP2350 on the Pico 2, and set up a small PC fan pointing at it to give good airflow. I again live bleated the experiment, pushing the RP2350 to higher voltages and clock speeds:I'll admit that I was rather concerned going above 2.0V - while the on board regulator provides 0.05V or 0.1V increments up to 2.0V, the next step up from 2.0V is 2.35V. This seemed like it was getting a long way above the stock 1.1V core voltage, and I thought the danger of cooking the RP2350 was rising.However, it turned out the performance gain from 2.0 to 2.35V wasn't as much as would be expected, and on further investigation the voltage that the voltage regulator was supplying didn't actually make it above about 2.2V - the on board regulator isn't able to provide enough current to run the RP2350 at these high voltages.So, how did I find out that the supplied voltage wasn't as high as requested? It turns out the Pico 2, rather handily, has a test point on the back that allows you to measure the core voltage. It was therefore very easy to probe it with my multimeter and see the voltage being delivered didn't match up to the requested voltage.But if you can  the voltage here, it would also be straightforward to  a voltage externally. That would mean we could use a bench power supply to provide as much voltage and current as the Pico 2 could take!I'd been chatting to the folks at Pimoroni about these experiments, whilst helping them with the Presto firmware and other bits and pieces. Niko from the Pimoroni team suggested, perhaps jokingly, that we should try a Liquid Nitrogen overclock.I'd arranged to pop in to visit Pimoroni HQ at the start of my Christmas break, so we wondered if it might be possible to give this a try. However, LN2 is a little tricky to handle and not totally straightforward to acquire. I was also worried that we could have problems with the soldering, PCB or connections to the Pico cracking under the extreme cold of LN2.Solid CO2 (AKA dry ice) on the other hand, is relatively easy to acquire and requires only fairly basic safety precautions. It should allow cooling of the Pico 2 to around -80C, which seemed like it would be enough to give a decent speed boost. Niko ordered some dry ice and the plan was set.⚠️ (Editorial note: do not experiment with solid CO2 / dry ice unless you know what you're doing, have considered the problems that carbon dioxide sublimation and things being cooled to -80C can cause and have taken appropriate safety precautions!)I wanted to make sure we were pretty rigorous about the testing - unlike the earlier experiments using the MicroPython performance benchmark, I wanted to get both cores running flat out. Therefore I decided to use the freely available CoreMark benchmark as the test, as this would report result comparable to other CPUs, and it would check for correct operation and report errors if there were problems.I also wanted the option of running the Pico from the ring oscillator, as described in the Raspberry Pi article linked at the top. Additionally, we weren't quite sure whether the crystal oscillator frequency might be changed by very low temperatures. Therefore, to measure time accurately I sent a 1MHz clock into the Pico 2 under test, and used a simple PIO program to count the cycles. This allowed us to run from the ring oscillator or crystal, and get accurate benchmark results measured using a known good clock.Compile for RP2350, and get two core operation workingUse a copy to RAM build so we got maximum performance and didn't need to worry about the flash clock dividerUse UART output rather than a USB console to avoid USB interrupts slowing down the test, and make it easier to recover from the RP2350 crashing.Print the temperature from the RP2350's on board temperature sensor after each runRun repeatedly in a loop instead of just onceAdd a prompt before the runs start to allow the voltage to be set (or the on board regulator to be disabled), and the frequency to be set or the ring oscillator to be selectedAdd a check for console input after each run, allowing the configuration to be modified, or the pico to be rebooted into DFU mode without having to hit the BOOTSEL button. The latter meant we could update things even if it was difficult to press that button when it was buried in dry ice.I used a version of Álvaro Fernández Rojas' pico-uart-bridge on a Pico W to communicate with the Pico 2 under test. This was modified to provide the 1 MHz reference clock, and also look for lines starting "Temp" or "CoreMark" and send them over WiFi.Using MicroPython on a PicoVision, and a modified version of the temperature display example, I read the temperature and CoreMark readings from the Pico W and graphed them on a monitor, so we could see what was happening while running the tests (I wrote this part in traditional hacker style entirely in my hotel room the night before going into Pimoroni HQ, using the dodgy hotel WiFi and the TV in the room to test it!)First up we needed to get all three Picos involved in the setup, and the surrounding hardware working and verified. At first we had some problems with the WiFi communication, which wasn't implemented in the most reliable way, but that seemed to settle down after some initial hiccups.We got the test Pico 2 running at 100MHz to establish a baseline. Jon at Pimoroni also pointed out that to make things easier to read it would be better to report a number in MHz instead of the CoreMark score, this was achieved simply by taking the ratio of the CoreMark score at 100MHz to the CoreMark being reported. As the CoreMark was running from internal RAM the score should be entirely linear with frequency.Next up we got the Pico 2 I'd been using for all the testing so far, complete with its tiny heatsink, and covered it in dry ice.We did some initial testing using the internal voltage regulator, and got it stable at 700MHz with ease:We wanted to push up to see where the limits were, so we wired up a power supply to test point 7, disabled the onboard voltage regulator and nervously increased the voltage above the maximum of 2.2 V that I had managed before.Up and up the voltage went, and while we got diminishing returns on the frequency acheivable, the magic smoke stayed inside the chip!Here we got to 800MHz at 2.8V:We realised our setup here was not ideal, as the ground for the core voltage was going via the Pico W. We measured the voltage at the Pico in the dry ice and it was around 200mV lower than the voltage being supplied, due to the resistance through that long path. With that in mind we pushed the voltage up to 3.3V and even a little higher, but it didn't allow that much more speed - the fastest we got to was 840MHz and that wasn't stable for long, we think due to the RP2350 heating up while drawing around 1A of current!Despite this abuse the Pico 2 still works just fine!Ring oscillator experimentsThe RP2350 datasheet suggests you can "automatically overclock" using the ring oscillator - the concept is that because the ring oscillator itself changes frequency in line with the core voltage and temperature of the chip, the same ring oscillator setting should be stable across all conditions. This appears very much not to be the case (at least while using the Arm cores - see later).I had found that (air cooled) using the "TOOHIGH" setting and maximum drive strength the Pico 2 ran ok up to around 1.5V and then failed, with the observed frequency at lower voltages being lower than the maximum that could be run from the crystal oscillator, but then it going up past the maximum frequency as the voltage increased. I'm not sure why this is. Using a drive strength one notch less for one of the two stages had it stable up to the maximum I had tested air cooled, so this was the initial setting we tried using.However, as we got the voltage higher, at around 2.6V again the RP2350 would crash when running from the ring oscillator, we tried backing off the drive strength which helped a bit, but ultimately it was difficult to find a setting that would run stably at the highest voltages, while being close to the frequency we'd managed with the crystal. That was a bit unfortunate, given that at frequencies above 800MHz we could only step the frequency from the crystal in increments of 12MHz.The Pico 2 that we were using was just the first one I had bought, with no selection based on whether it was a particularly good one for overclocking. Pimoroni rustled up 7  Pico 2s for me to test to try and find one that might overclock the best.I quickly ran the Micropython frequency search test (linked near the top) on all of them, finding that at 1.1V they would max out at between 316 and 336MHz. Mark from Pimoroni soldered up the two best ones for further testing. We soldered a few more pins this time to get access to another ground, so the core voltage supply didn't go via the Pico W.Despite being the fastest at 1.1V, the first Pico 2 we tested wasn't that much better than the initial one. We were briefly able to run it at 850MHz at 3.05V, but it wasn't stable. (This is 850MHz measured by dividing the CoreMark, we requested 852MHz but it seems got slightly less due to the crystal being cold or some other effects of the high core voltage). Going higher than 3.05V seemed to actually make it less stable, so this seemed to be the limit. At 840MHz requested frequency it ran for a couple of minutes, but we saw errors reported from the CoreMark so there must have been errors running some instructions.Thinking that a lot of the problem in getting faster was just the chip warming up too fast, we looked for a larger thermal mass to try and keep it cooler for longer. Paul produced a big chunk of copper, significantly larger than the Pico 2 itself, which seemed promising.We tried attaching this to the Pico 2 with a couple of thermal pads, but it seems they didn't provide great thermal conductivity at these low temperatures, because the results were actually worse with this than without any heatsink at all! That was a shame - maybe we should have tried just getting direct contact of the copper on top of the chip, but we were worried about shorts.We switched over to the other Pico 2 and attached a heatsink designed for the BCM2712 on the Pi 5 to it. That got better results - obviously the simple test at 1.1V room temperature doesn't account for everything! This one would briefly manage 864MHz requested (reported 860.7MHz), and would run stable at 840MHz for some time, with no errors. The best so far!The RP2350 is an interesting chip, with two RISC-V cores in addition to the two more established ARM cores. Mark had reminded me that we should try the RISC-V cores for comparison, and I got the image built for them - this is pretty simple to do, you just need to install the RISC-V version of gcc and change the  setting to .I found that CoreMark actually gave a slightly higher performance per MHz using the RISC-V cores - just under 5% faster. So if you have an integer-only use for the RP2350 it is definitely worth checking if the RISC-V cores might give better performance!Adjusting the PicoVision monitor for the CoreMark per MHz, we got the same Pico 2 running again.Ramping the voltage up again running off the ring oscillator, with the drive strength one notch off maximum on one of the two stages, this time we found it didn't crash as the voltage increased, until we got to around 3.0V. That meant that at around 2.95V we had a stable overclock of 820-840MHz running off the ring oscillator (with the fluctuation in frequency due to temperature).Switching over to the crystal, with 3.05V we got 861.6MHz (864MHz requested) running cleanly, and it even ran for nearly a minute at 873.5MHz before crashing. We had no luck trying to get a requested frequency of 888MHz to run.So the top speed we managed was 873.5MHz, though when we tried this again the Pico refused to start at 3.05V, so I guess we were beginning to cause some damage with the high voltage.For a final test we left it running on the ring oscillator at 2.95V, and it ran for around 10 minutes before we let it get a little too warm (up to around -20C).Firstly, the RP2350 and Pico 2 is incredibly hardy. Despite the cold temperatures, some accidental shorting due to moisture when running it while cooling and extremely high core voltages, none of the 3 Pico 2s we were playing with are dead - and I don't think there's any way you would be able to tell that they had been through this treatment.Above 700MHz the extra performance you get ramping up the voltage gives diminishing returns. This maybe because we just weren't able to keep the chip cool enough - just packing dry ice around it, it becomes difficult to carry enough heat away. Liquid nitrogen might be interesting to experiment with to go further.Given this experimentation, it seems around 1.6V from the voltage regulator should easily allow a 500MHz or slightly faster overclock without needing any additional cooling and likely without really causing much damage to the Pico - it might be interesting to run a Pico 2 loaded like that for a few days and see if anything bad happens.Playing with dry ice is fun! If we get a chance to do this again we should also try overclocking a Pi 5, and see if we could get a speed record there. Although that could be rather more expensive - the great thing about using Pico 2 for this, is that at under £5 per board you can treat them with reckless abandon - after all, they each cost less than a pint of beer!]]></content:encoded></item><item><title>Web Components: The Framework-Free Renaissance</title><link>https://www.caimito.net/en/blog/2026/02/17/web-components-the-framework-free-renaissance.html</link><author>mpweiher</author><category>hn</category><pubDate>Fri, 20 Feb 2026 08:45:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Building Modern UIs Without the Framework Tax17.02.2026, Modern browsers now support everything needed to build sophisticated, reactive web interfaces without React, Vue, or Angular. Web components, custom elements, shadow DOM, and native event systems let developers create modular, reusable UI pieces that communicate elegantly — and AI assistants can help you master these patterns faster than ever before.The Shift That Already HappenedSomething remarkable occurred while many developers weren’t watching. The web platform itself became capable of doing what frameworks were invented to do.
"The browser has become the framework. We just haven't fully noticed yet."
Custom Elements let you define your own HTML tags with their own behavior. Shadow DOM provides encapsulation that keeps component styles and structure isolated. Templates and slots offer composition patterns. And perhaps most importantly, the native event system provides a robust mechanism for components to communicate without tight coupling.These aren’t experimental features. They’ve been shipping in every major browser for years. The question is no longer whether they work, but why more developers haven’t embraced them.Freedom from the Upgrade TreadmillEvery framework carries hidden costs. There’s the initial learning curve, certainly. But there’s also the ongoing maintenance burden: major version upgrades that break things, deprecated patterns you must migrate away from, build tooling that needs constant updating.
"When your component is just HTML, CSS, and JavaScript, there's nothing to upgrade except your own code."
Web components built on platform standards sidestep this entirely. The browser vendors have committed to backward compatibility in ways that framework maintainers simply cannot. Code written to web standards a decade ago still works today. That’s not true of code written for Angular 1, or React class components, or Vue 2’s options API.For organizations building products that need to run for years, this stability matters enormously. It’s one less thing that can break, one less dependency that can become a security vulnerability, one less abstraction layer between your code and the runtime.The elegance of web components becomes most apparent when you consider how they communicate. The native Custom Events system provides everything you need for sophisticated component interaction.A component deep in your UI hierarchy can dispatch an event that bubbles up through the DOM tree:Any ancestor component — or the application shell itself — can listen for and respond to that event. There’s no need for a global state store, no prop drilling, no context providers. The DOM itself becomes your communication infrastructure.
"The DOM has always been an event bus. We just forgot how to use it."
Components can also communicate downward through attributes and properties. When a parent changes an attribute, the child’s  fires, giving it a chance to respond. For more complex data, properties allow passing objects and arrays directly.This creates a natural, predictable flow: data down through properties and attributes, events up through the bubbling system. It’s the same pattern React popularized, but using web standards instead of library abstractions.Learning Through BuildingHere’s something that surprises many developers: you don’t need to master every detail of the Web Components specification before you can build useful things. The basics are remarkably accessible.A minimal custom element looks like this:That’s a working component. It’s not production-ready — it lacks reactivity, encapsulation, and proper lifecycle handling — but it demonstrates how accessible the entry point is. You can iterate from this simple beginning toward more sophisticated implementations as your understanding grows.AI as Your Pair Programming Partner
"The best way to learn web components is to build them — and AI makes experimentation nearly frictionless."
This is where modern AI assistants transform the learning experience. You can describe what you want a component to do, receive a working implementation, and then ask questions about the parts you don’t understand. The AI can explain why  matters for events that need to cross shadow DOM boundaries. It can show you the difference between  and . It can help you refactor toward better patterns as your requirements evolve.You don’t need to read the entire MDN documentation on Web Components before building your first real component. Instead, you can learn by doing, with an AI partner that understands the specification deeply and can answer questions in context.This approach — building first, understanding deeply second — works remarkably well with web standards. The patterns are simpler than framework patterns because there’s less abstraction. When something doesn’t work, the debugging surface is smaller. When you want to understand why something works, there are fewer layers to peel back.Consider a realistic scenario: a dashboard with multiple independent panels that need to respond to a shared filter.Without frameworks, you might structure this with a simple event-driven architecture:Each panel component implements its own  method. The panels don’t know about each other. The filter component doesn’t know about the panels. The dashboard shell provides minimal coordination. Components can be developed, tested, and reused independently.
"Loose coupling isn't just elegant architecture — it's faster development and easier maintenance."
This same pattern scales. As you add more panels, they simply implement the expected interface. As you add more event types, the coordination logic grows proportionally, not exponentially.Shadow DOM: Encapsulation That Actually WorksOne of the most practical benefits of web components is shadow DOM encapsulation. Your component’s styles don’t leak out, and global styles don’t leak in (unless you explicitly allow them).Those styles affect only this component. You can use simple, semantic class names without worrying about collisions. You can refactor styles without fear of breaking something elsewhere.This is encapsulation that actually encapsulates. It’s what CSS Modules, CSS-in-JS, BEM, and countless other approaches have tried to achieve — built directly into the platform.When Frameworks Still Make SenseWeb components aren’t always the right choice. If your team already knows React deeply and moves quickly with it, there’s real value in that shared expertise. If you’re building something that will be maintained by developers who expect framework patterns, web components might create friction.
"The best technology choice is the one that fits your team, your timeline, and your constraints."
For new projects, though — especially smaller teams or solo developers building products that need to run for years — web components deserve serious consideration. The reduced complexity, improved stability, and smaller bundle sizes create real advantages.And there’s an interesting hybrid path: many frameworks now work well with web components. You can introduce custom elements gradually into an existing React or Vue application. You can wrap web components in framework-specific bindings. Migration can be incremental rather than revolutionary.The path to building with web components is surprisingly short:Build a simple component. Something with a template, maybe an attribute or two. See how it mounts to the DOM. Handle events within the component. Update the DOM in response. Experience the directness of working without virtual DOM abstraction. Have one component dispatch a custom event. Have another listen for it. Feel how naturally the event system handles component coordination.Encapsulate with shadow DOM. Add scoped styles. Use slots for composition. Appreciate how encapsulation simplifies your mental model. Build more components. Let AI help when you encounter unfamiliar territory. Learn the lifecycle callbacks as you need them.Each step teaches something useful. Each component you build adds to your understanding. And unlike framework knowledge that might become obsolete, your understanding of the web platform will compound for years.We’re in an interesting moment. The platform has caught up to — and in some ways surpassed — the capabilities that made frameworks essential a decade ago. The tooling exists. The browser support is universal. The patterns are well-documented.
"The future of web development might look more like its past: standards-based, interoperable, and built to last."
What’s been missing is momentum. Developers gravitate toward what’s popular, and frameworks have dominated mindshare for so long that many never seriously evaluated the alternative.But trends shift. The JavaScript ecosystem’s complexity has become a recognized problem. The appeal of simpler, more stable foundations grows as developers tire of churn. AI assistants make learning new approaches faster and less intimidating.Web components offer something valuable: a way to build modern, sophisticated interfaces with technology that will still work decades from now. For developers willing to explore beyond the framework mainstream, there’s a quieter, more elegant path waiting.The tools are ready. The browsers are ready. Perhaps it’s time to rediscover what the web platform can do.Get new articles, story episodes, and hero profiles delivered to your inbox.]]></content:encoded></item><item><title>Reading the undocumented MEMS accelerometer on Apple Silicon MacBooks via iokit</title><link>https://github.com/olvvier/apple-silicon-accelerometer</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 20 Feb 2026 05:06:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mystery donor gives Japanese city $3.6M in gold bars to fix water system</title><link>https://www.bbc.com/news/articles/c3ew5jlqz87o</link><author>tartoran</author><category>hn</category><pubDate>Fri, 20 Feb 2026 04:27:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The gold bars, worth an estimated 560 million yen ($3.6m; £2.7m), were given last November by a donor who wished to remain anonymous, Osaka Mayor Hideyuki Yokoyama told a press conference on Thursday.]]></content:encoded></item><item><title>Consistency diffusion language models: Up to 14x faster, no quality loss</title><link>https://www.together.ai/blog/consistency-diffusion-language-models</link><author>zagwdt</author><category>hn</category><pubDate>Fri, 20 Feb 2026 04:15:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Diffusion Language Models (DLMs) are emerging as a promising alternative to autoregressive (AR) LMs. Instead of generating one token at a time, DLMs iteratively refine a partially masked sequence over multiple sampling steps, gradually transforming a fully masked sequence into clean text. This refinement process creates a compelling opportunity: it enables parallel generation, allowing the model to finalize multiple tokens per iteration and potentially achieve higher throughput than AR decoding. At the same time, it can exploit bidirectional context to unlock new capabilities such as text infilling and refinement.However, in practice, standard DLMs suffer from two major inefficiencies. [1]KV caching incompatibility under full bidirectional attention.Standard DLMs commonly use bidirectional (non-causal) attention, which requires recomputing attention over the full context at every denoising step, making inference expensive and preventing standard KV caching.High refinement step counts to maintain quality.High-quality generation typically requires many denoising/refinement steps, often comparable to the generation length. Naively reducing the number of steps tends to degrade quality sharply.CDLM targets both bottlenecks through a post-training recipe that makes fewer-step inference reliable while enabling exact block-wise KV caching.Preliminary: Inference in diffusion language modelsDLM generation is an iterative refinement over N discrete sampling steps. It transforms a fully masked sequence at time t=1 into a clean sequence at t=0. At each step, the model predicts a clean sequence distribution x0 given the current noisy sequence xt and prompt c:$p_{\theta}(\mathbf{X}_0 \mid \mathbf{X}_t, c)$A common deterministic instantiation is low-confidence remasking: the model greedily unmasks tokens (often within blocks), finalizing the highest-confidence masked positions while keeping others masked. This leads to the decoding trajectory:$\mathcal{T}_{\mathbf{x}} = \left(\mathbf{x}_{t_0}, \mathbf{x}_{t_1}, \ldots, \mathbf{x}_{t_N}\right), \quad t_k = 1 - \frac{k}{N}$which records how the partially refined sequence evolves step-by-step. This trajectory becomes the core object for CDLM’s training.We collect trajectories offline by running inference with a DLM on domain-specific prompts. For each prompt x, we record the token-level decoding trajectory T_x, a compact hidden-state buffer H_x containing last-layer hidden states at token finalization moments, and the ground-truth text ŷ. Concretely, we adopt block-wise decoding with a generation length L_g = 256, block size B = 32, and a total of N = L_g steps (i.e., finalizing exactly one token per step within the current block). This conservative setting yields higher-quality trajectories for distillation.2) Block-causal student and attention maskDuring trajectory extraction, we use a full bidirectional attention mask. In contrast, when training CDLM, we employ a block-wise causal mask that attends to the prompt, previously completed blocks, and the current decoding block. This design enables the model switch from full bidirectional to block-diffusion models (like [2]), enabling exact block-wise KV caching for finalized blocks.CDLM jointly minimizes three objectives:(i) Distillation loss (newly unmasked positions)For positions that become newly unmasked between an intermediate state y and its block completion y*, we match the student’s predictive distribution to the teacher’s reconstructed distribution obtained from stored hidden states.Intuition: this objective serves as the primary anchor that teaches the student to finalize multiple tokens within a block under block-causal constraints.(ii) Consistency loss (still-masked positions)We enforce within-block temporal consistency by aligning the student’s predictions at state y with its own predictions at the more informed state y* for still-masked positions, using a stop-gradient target.Intuition: this objective encourages stable multi-step transitions along the decoding trajectory.(iii) Auxiliary DLM masked-denoising lossWe include a standard masked denoising objective applied to randomly masked ground-truth text.Intuition: this objective preserves the model’s general masked-token prediction capability and helps retain reasoning behavior, particularly on mathematical tasks.At inference time, CDLM decodes in a block-wise autoregressive manner, reusing the KV cache for the prompt and all previously finalized blocks. Within each block, we apply confidence-thresholded parallel finalization. [3] We also adopt early stopping once an end-of-text token appears in the current block.We intentionally avoid additional heuristics that introduce extra hyperparameters (e.g., inter-block parallelism with task-dependent settings), and instead focus on a robust default decoding pipeline based on exact KV caching and reliable step reduction.CDLM–Dream achieves the largest step reductions across benchmarks, cutting refinement steps by roughly 4.1x–7.7x with minor accuracy changes on most tasks.These step reductions translate into large latency improvements: up to 11.2x on GSM8K-CoT and 14.5x on MBPP-Instruct.CDLM often attains the highest Tokens Per Second throughput, with one notable nuance: tasks can show different decoding dynamics because CDLM is strictly block-causal and may produce shorter outputs while preserving pass@1 quality.Effective step reduction: Why training mattersNaively truncating the number of steps causes marked accuracy degradation, while CDLM maintains quality at similar step budgets (and achieves roughly half the latency thanks to caching). This highlights the core point: stable multi-token refinement is not free; it requires training that enforces trajectory-consistent behavior.System-level analysis: Why block-wise DLM sits in the sweet spotTo understand hardware utilization, we analyze arithmetic intensity (AI), FLOPs per byte moved, as batch size increases, comparing: AR decoding, vanilla (full-attention) DLMs, block-wise DLMs (CDLM) with B∈{4,16,32}.AR decoding is strongly memory-bound at small batch sizes (AI near 1 at bs=1), scaling as batch increases due to weight-load amortization.Vanilla DLMs are compute-bound even at bs=1 because full bidirectional attention processes the whole sequence each step, leading to saturation.Block-wise DLMs (CDLM) occupy an intermediate regime: higher AI than AR due to intra-block parallelism (processing B tokens under similar memory traffic), but lower than vanilla DLMs, often a balanced operating point for small-batch settings.Overall, the analysis explains why CDLM-like block-wise diffusion can deliver strong efficiency at small batch sizes: it uses parallelism to amortize memory access while remaining in a regime that still benefits from practical scaling.Expressiveness vs. efficiency Full bidirectional attention in DLMs requires recomputing O(L^2) attention at every denoising step, making inference highly compute-intensive. CDLM enables exact KV caching while preserving bidirectional context within each block, retaining local refinement capabilities (e.g., infilling inside the current block).Scaling with stronger DLM backbonesCDLM is a post-training recipe that can be applied to any block-diffusion model, and its benefits should grow as stronger DLMs emerge. A promising direction is to collect trajectories from larger, stronger DLM teachers and train mid-scale students with CDLM. We presented CDLM, a training-based acceleration scheme that brings consistency modeling to DLMs. By enforcing within-block temporal consistency and fine-tuning a block-wise causal student, CDLM reduces refinement steps and enables exact KV caching. Across math and coding tasks, CDLM yields faster inference, fewer steps, lower latency, and higher throughput while maintaining competitive accuracy.[1] Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models[2] Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models[3] Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding]]></content:encoded></item><item><title>An AI Agent Published a Hit Piece on Me – The Operator Came Forward</title><link>https://theshamblog.com/an-ai-agent-wrote-a-hit-piece-on-me-part-4/</link><author>scottshambaugh</author><category>hn</category><pubDate>Fri, 20 Feb 2026 03:05:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pi for Excel: AI sidebar add-in for Excel</title><link>https://github.com/tmustier/pi-for-excel</link><author>rahimnathwani</author><category>hn</category><pubDate>Fri, 20 Feb 2026 02:20:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>An ARM Homelab Server, or a Minisforum MS-R1 Review</title><link>https://sour.coffee/2026/02/20/an-arm-homelab-server-or-a-minisforum-ms-r1-review/</link><author>neelc</author><category>hn</category><pubDate>Fri, 20 Feb 2026 01:33:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I’ve always wanted an ARM server in my homelab. But earlier, I either had to use an underpowered ARM system, or use Asahi which not only requires expensive Mac hardware but also slowed down in the past few years.Then Minisforum introduced the MS-R1 Mini PC. Two MS-01s were already incumbent in my homelab when they replaced power-hungry HPE towers, but the MS-R1 gave me what I wanted: a reasonably powerful ARM machine which doesn’t have bank-breaking Mac pricing.I got the MS-R1 barebones and had a 1TB SSD sitting around.I opened the box and got this:I installed my SSD, and attempted to install Rocky Linux.So here it is, Rocky Linux booted.There’s one issue: the onboard NICs weren’t detected:I installed it anyways, and tried to sideload the Realtek r8127 drivers. While they did install and load, keeping the driver upon kernel updates wasn’t elegant and very hacky.I could keep trying, but decided to just use Fedora instead:Yes, while I use Fedora on my laptop, I also know Fedora is generally not a good option for a server. But it had the NIC drivers as the RTL8127 is newer than RHEL 10’s freeze but not Fedora 43’s.Here’s my obligatory homelab picture:MS-R1 on the top, then two MS-01s, MikroTik CCR2004-16G-2S+PC, CRS309-1G-8S+IN and CSS610-8P-2S+IN.First, it’s a powerful-enough ARM system which doesn’t break the bank. I wanted this for so long. I’d say it’s quieter than the MS-01s but then Intel doesn’t exactly have the most efficient silicon. Yet even as an efficiency for performance freak I have a 285K instead of a 9950X.While Minisforum recommends their Debian image, Rocky Linux worked for everything but the NICs, and Fedora works for everything I need. I haven’t tested the integrated GPU since I plan to use this headless. I also own a Mac as my ARM (but not main) laptop.Say what you want about UEFI and ACPI, but it does make hardware support easier. Heck, not just Macs with UTM, but  ARM laptops in China can run Windows VMs, despite crippling US sanctions.I do hope a future CentOS/RHEL/Rocky 10 adds the Realtek 8127 so I won’t have to wait until 2028 for Rocky 11. And no I won’t use Debian.By no means is the MS-R1 perfect.For instance, there are two M.2 slots but one is used by the Wi-Fi and even if I remove it, cannot use it for a M.2 SSD, only U.2. I’d still prefer to have RAID if not for the shortage. The MS-01 and A2 have multiple M.2 SSD slots.Also, Marvell AQC107 NICs wasn’t detected by the UEFI, so they couldn’t be used as far as I tried:Unless the NIC died or my UEFI configuration is wrong, it’s simply not usable.One nit: if I select “power on after outage,” it didn’t do it when I unplugged and replugged the server. Darn.Why not Debian or Ubuntu?While I’m aware there’s a “recommended” Debian variant for the MS-R1, I’m simply not a fan of Debian-based distros. Sure, I run my UniFi controller on Debian (inside Incus). But that’s because I  to, not because I want to.I don’t hate Debian, I respect Debian for what they do. They do many things right, like being truly community-owned and having a reliable upgrade path (which RHEL and co notoriously lacks). But it’s not for me, despite having used it for 2.5 months before nearly a decade of FreeBSD.The ARM ecosystem while growing is still small when compared to x86. Heck, I daily drive an HP OmniBook Ultra instead of a faster M3 Pro MacBook Pro  of Linux. And Asahi’s delays.The MS-R1 isn’t perfect, but works quite well as a homelab ARM hypervisor. Do I regret it? Not at all, despite its problems. It’s early adopter problems, but I have pretty thick skin as long as my privacy isn’t invaded for profit and “AI”.The Mac Studio is way too expensive, even used, while expected for Apple is more expensive than even already expensive current-gen HPE ProLiants.  is cheaper than Apple, and is already expensive as-is. ARM64 SBCs are great, but I still wanted something like a PC.There is also one other perk: while the MSRP is $599, I got it for $559  a RAM shortage.While it won’t replace my two MS-01s (too much x86 software! vPro!), it’s a nice addition and is already running my secondary Samba domain controller in a FreeBSD 15.0 virtual machine.]]></content:encoded></item><item><title>MuMu Player (NetEase) silently runs 17 reconnaissance commands every 30 minutes</title><link>https://gist.github.com/interpiduser5/547d8a7baec436f24b7cce89dd4ae1ea</link><author>interpidused</author><category>hn</category><pubDate>Fri, 20 Feb 2026 01:28:47 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Single vaccine could protect against all coughs, colds and flus</title><link>https://www.bbc.com/news/articles/cx2g8rz7yedo</link><author>dabinat</author><category>hn</category><pubDate>Thu, 19 Feb 2026 22:08:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Pulendran told the BBC: "This vaccine, what we term a universal vaccine, elicits a far broader response that is protective against not just the flu virus, not just the Covid virus, not just the common cold virus, but against virtually all viruses, and as many different bacteria as we've tested, and even allergens.]]></content:encoded></item><item><title>Show HN: Ghostty-based terminal with vertical tabs and notifications</title><link>https://github.com/manaflow-ai/cmux</link><author>lawrencechen</author><category>hn</category><pubDate>Thu, 19 Feb 2026 21:30:04 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[I run a lot of Claude Code and Codex sessions in parallel. I was using Ghostty with a bunch of split panes, and relying on native macOS notifications to know when an agent needed me. But Claude Code's notification body is always just "Claude is waiting for your input" with no context, and with enough tabs open, I couldn't even read the titles anymore.I tried a few coding orchestrators but most of them were Electron/Tauri apps and the performance bugged me. I also just prefer the terminal since GUI orchestrators lock you into their workflow. So I built cmux as a native macOS app in Swift/AppKit. It uses libghostty for terminal rendering and reads your existing Ghostty config for themes, fonts, colors, and more.The main additions are the sidebar and notification system. The sidebar has vertical tabs that show git branch, working directory, listening ports, and the latest notification text for each workspace. The notification system picks up terminal sequences (OSC 9/99/777) and has a CLI (cmux notify) you can wire into agent hooks for Claude Code, OpenCode, etc. When an agent is waiting, its pane gets a blue ring and the tab lights up in the sidebar, so I can tell which one needs me across splits and tabs. Cmd+Shift+U jumps to the most recent unread.The in-app browser has a scriptable API ported from agent-browser [1]. Agents can snapshot the accessibility tree, get element refs, click, fill forms, evaluate JS, and read console logs. You can split a browser pane next to your terminal and have Claude Code interact with your dev server directly.Everything is scriptable through the CLI and socket API – create workspaces/tabs, split panes, send keystrokes, open URLs in the browser.]]></content:encoded></item><item><title>We&apos;re no longer attracting top talent: the brain drain killing American science</title><link>https://www.theguardian.com/us-news/2026/feb/19/trump-science-funding-cuts</link><author>mitchbob</author><category>hn</category><pubDate>Thu, 19 Feb 2026 20:56:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[n April 2025, less than three months after Donald Trump returned to the White House, the federal Centers for Disease Control and Prevention (CDC) put out its latest public health alert on so-called “superbugs”, strains of bacteria resistant to antibiotics.These drug-resistant germs, the CDC warned, are responsible for more than 3m infections in the US each year, claiming the lives of up to 48,000 Americans.Globally, the largely untreatable pathogens contribute annually to almost 5m deaths, and health experts fear that unless urgent steps are taken they could become a leading killer, surpassing even cancer, by 2050.“We’re in a war against bacteria,” said Ian Morgan, a postdoctoral fellow at the US National Institutes of Health (NIH), the world’s largest funder of biomedical research. He is on the frontlines of that war against superbugs; the NIH lab in which he works is driving what he described as “high-risk, high-reward research”.But over the past year, the battlefield has toughened. Under the Trump administration, Morgan, 33, and thousands of other young American scientists like him have grappled with wave after wave of disruptions.Billions of dollars have been wiped from research budgets, almost 8,000 grants have been cancelled at NIH and the US National Science Foundation alone, and more than 1,000 NIH employees have been fired.Morgan’s research has been rattled by multibillion-dollar cuts in NIH contracts that make it impossible for labs to maintain their equipment. They have the choice of paying exorbitant maintenance fees, or giving up on experiments.Amid the maelstrom, young and early-career scientists like Morgan are among the hardest hit. His own future is now in doubt.In the normal trajectory of a life in science, Morgan would be planning to set up his own laboratory conducting groundbreaking research designed to win the war on superbugs. But with an ongoing hiring freeze at NIH, his options are limited.“Right now there’s no way even to apply to start your own lab at NIH, no matter how good you are, or how critical your work,” he says.Morgan’s predicament has led him to step up as a steward at a new union for young NIH researchers formed under the umbrella of the UAW. Its almost 5,000 members are organizing against the Trump administration’s assault on American science.The chaos that has descended on NIH over the past year has led Morgan to fear for his future, the future of his craft, and ultimately the fight against superbugs.“We’re making progress, we have a lot of really cool new innovations that could defeat the infections,” he said. “But if we stop doing the work, we lose the war.”A similar story to Morgan’s could be told by tens of thousands of other young scientists throughout NIH and across numerous US universities experiencing federal funding squeezes. More than 10,000 post-doctoral experts in scientific and related fields were lost to the federal workforce last year, according to Science.The magazine looked at 14 research agencies, including NIH, and found that the number of employees departing outstripped new hires by 11 to one.The brain drain is prompting existential fears that American science, a powerhouse of the US economy and of global public health, is being deprived of its lifeblood. The source of young researchers – the next generation of scientists who are the fount of new ideas and innovation – is being throttled.“The talent pool is developed by letting young people flourish among like-minded, excited scientists,” said John Prensner, a pediatric brain cancer doctor who leads a research laboratory at the University of Michigan. “If that ceases, then that intellectual discovery, that drive to make the next great insight into cancer or other challenges, will be planted in another country’s soil.”The NIH drives scientific progress globally across biomedical and behavioral sciences, including defenses against infectious diseases and possible future pandemics. It pushes at the frontier of new therapies geared to the genetic makeup of individual patients, and can claim numerous breakthroughs in cancer treatment, vaccinations and much more.Without the NIH driving innovation at its core, the US would cease to have the largest biomedical ecosystem in the world.‘Wiping out the next generation of scientists’Emma Bay Dickinson, a 27-year-old postgrad researcher in infectious diseases, is a specialist in zika, the largely mosquito-born virus that can cause birth defects. Her longer-term ambition is to help find a way to protect the world against viruses that have the potential to evolve into the next pandemic, such as avian flu.For now, though, the US will miss out on her skills. The Trump administration’s funding cuts began to hit last year just when, as a postgraduate research fellow at NIH in Washington DC, she was hunting for her next position.“My classmates applying in the US were getting rejected, and were being told that the funding cuts meant there was too much uncertainty to offer them jobs.”Dickinson, who is queer, was discouraged by Trump’s animus against diversity, equity and inclusion (DEI) which was used as a justification for many of the grant cuts. She was also dismayed by the blatant censorship imposed by the administration.Applicants for federal research funding were forced to filter their proposals to remove a banned list of key words across DEI, climate, vaccines and other study areas deemed undesirable by Trump.So Dickinson redirected her energies abroad. She began applying for posts in Spain and Germany, in the end landing a spot at a prestigious program at a Barcelona infectious disease research institution.For the foreseeable future, she sees her future in Europe. “It’s important for me to feel I can be myself in my science, and that’s just not possible right now in the US,” she said.She is not alone. A growing number of young American scientists are quitting the country for positions in Europe, Australia or Asia. Universities across Europe have been swift to exploit the opportunity, openly enticing young Americans to join the exodus and seek “scientific asylum” with them.The response has been overwhelming. Aix-Marseille University, which launched one of the first European programs to lure people from the US, was inundated by hundreds of applications from early-career researchers hoping to flee the US.The outflow of young scientists has been exacerbated by deep cuts to NIH training programs, which acts as a breeding ground of the US’s future top scientists. At least 50 training programs, targeted at undergraduates through early-career lab researchers, have been shut down under the Trump administration.An NIH program officer spoke to the Guardian about the impact of the training cuts, but asked not to be named for fear of reprisal. “Trainees are the most vulnerable people in science,” the officer said.“They are the ones with new ideas, where a lot of our hope resides. Now they are losing their minds with worry about what comes next. They are desperate for advice on how to stay in science when there are no grants available.”The officer added: “If you delay and terminate training grants, it’s like a snowball effect. Eventually you start wiping out our next generation of scientists.”Adding to the problem of young talent leaving the country, the flow of early-career researchers entering US scientific labs from around the world is also shrinking as a result of Trump’s immigration crackdown. Scientists from abroad are often at the forefront of US innovation – last year, half of the US Nobel prize winners in science subjects were immigrants.In September, Trump imposed a $100,000 fee on new applications for H-1B visas for foreign skilled workers, a move that makes coming to the US prohibitively expensive for most researchers. Then in January the administration suspended immigrant visa processing to people from 75 countries.Add to that the nightly TV images beamed around the world of ICE raids on US city streets, and a clear message has been sent out that America does not welcome newcomers.Jennifer Jones, director for the Center for Science and Democracy at the Union of Concerned Scientists, said that the international reputation of US science had been damaged in ways that could take years to repair.“We are no longer attracting top talent from around the world. Why would you want to come to a place where you know you could be threatened with deportation at any moment?”‘Leaving discoveries on the table’Emily Hilliard, the press secretary of the Department of Health and Human Services, told the Guardian that NIH was “deeply committed to providing opportunities for early career scientists by restoring the agency’s culture and rebuilding public trust”. She disputed the idea that the pipeline of young scientists was being reduced, calling such claims “baseless and intended to fearmonger”.“NIH will continue to attract and recruit the best and brightest, strengthen the US biomedical workforce, and deliver cures and solutions for Americans,” she said.But NIH staff continue to view the future with trepidation. Jenna Norton, a program director at NIH, said she had been surprised by how quickly the landscape had changed.She was placed on indefinite paid leave from the agency in November, without being given any explanation. Earlier this month Norton filed a whistleblower complaint alleging that the Trump administration had unlawfully retaliated against her for openly criticising Trump’s cuts to grants, funding and staffing at NIH.Speaking in her personal capacity, Norton told the Guardian: “I was not expecting this administration to come at science as broadly and as quickly as they have.”In the long run, the damage done to the next generation of researchers threatens to harm not just scientific knowledge itself, but also the US economy. NIH funding supports basic biomedical research out of which new drugs and other commercial spin-offs emerge.As such, it provides the foundations for the almost trillion-dollar US pharmaceutical industry. A 2018 study of the 210 new drugs approved by the Food and Drug Administration (FDA) in the six years to 2016 found that all of them had been developed out of early basic research funded by NIH.“We are leaving discoveries on the table,” warned Donna Ginther, an economics professor at the University of Kansas who is an authority on the science labor market. “Those discoveries are the ones that in 10, 20 years will contribute to economic growth, improved health, human longevity. That’s what we are choking off.”]]></content:encoded></item><item><title>Overall, the colorectal cancer story is encouraging</title><link>https://www.hankgreen.com/crc</link><author>ZeroGravitas</author><category>hn</category><pubDate>Thu, 19 Feb 2026 20:32:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI is not a coworker, it&apos;s an exoskeleton</title><link>https://www.kasava.dev/blog/ai-as-exoskeleton</link><author>benbeingbin</author><category>hn</category><pubDate>Thu, 19 Feb 2026 19:55:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[We're thinking about AI wrong.I keep noticing the same pattern: companies that treat AI as an autonomous agent that should "just figure it out" tend to be disappointed. Meanwhile, companies that treat AI as an extension of their existing workforce, an amplifier of human capability rather than a replacement, are seeing genuinely transformative results. Thats not to say that AI can't act automonously with specific tasks (see the rise of OpenClaw as a viral proof of concept), but even that still acts as an extension of human decision making and context.The framing matters more than we realize. And I think the best mental model for understanding AI isn't a new coworker. It's an exoskeleton.Stay with me here, because this isn't just a metaphor. There are real examples of exoskeletons being deployed right now across manufacturing, logistics, military, and healthcare. The statistics are worth paying attention to.Ford has deployed EksoVest exoskeletons in 15 plants across 7 countries. The result? An 83% decrease in injuries in units using exoskeletons. Workers still do the overhead lifting (4,600 times per day)but with 5-15 pounds of assistance per arm that makes the work sustainable.BMW's Spartanburg plant reports 30-40% reduction in worker effort using Levitate Technologies vests.German Bionic's Cray X provides up to 66 lbs of lift support per movement. German Bionic reports that customers using the Cray X, including BMW and IKEA, have seen a 25% reduction in sick days.In Military Applications:The Sarcos Guardian XO Max provides 20:1 strength amplification. 100 lbs feels like 5 lbs. Soldiers can carry up to 200 pounds, not because the suit replaces them, but because it amplifies what they can already do.The Lockheed Martin HULC enables carrying 200 pounds at sustained speeds of ~7 mph with 10 mph bursts. This matters because musculoskeletal injuries account for over half of all military injuries, with back injuries among the most common. The exoskeleton doesn't fight for the soldier. It keeps them from getting injured while they do their job.In Medical Rehabilitation:In a meta-analysis of powered exoskeleton training, 76% of patients with spinal cord injuries were able to walk while wearing the exoskeleton with no additional physical assistance from therapists, many using only crutches or walkers for balance. These are people who were told they would never walk again.Stanford's 2020 research showed a 15% reduction in the energy cost of running with their ankle exoskeleton, potentially translating to a 10% boost in running speed.Harvard's soft exosuit reduced the metabolic cost of running by 5.4%. That means a marathon would feel like running 24.9 miles instead of 26.2.Notice the pattern here. The exoskeleton doesn't replace the human. It doesn't lift the boxes, run the race, or walk the steps independently. It amplifies human capability. The human is still doing the work, they're just able to do dramatically more of it, more sustainably, with less injury and fatigue.The Ontological Problem with "AI Agents"Here's where the AI industry has gone a bit sideways.There's been this rush toward "agentic AI", or systems that operate autonomously, make their own decisions, and complete entire workflows without human intervention. The dream of having a fully autonomous AI employee is seductive. But I think we've been seduced by the wrong metaphor.When we think of AI as an autonomous agent as a separate entity with its own judgment and decision-making, we set ourselves up for disappointment. We expect it to understand context it wasn't given. We expect it to make judgment calls it isn't equipped to make. We get frustrated when it "hallucinates" or goes off the rails.What This Looks Like in Product DevelopmentInstead of building AI that autonomously decides what your product should be, we built a platform that goes incredibly deep on research and analysis — then puts the insights in front of the humans who actually make the calls.The difference sounds subtle but it's not. Let me give you a concrete example.Kasava's commit analysis doesn't just count lines of code. It reads every commit across your repositories, categorizes changes by type and impact, identifies patterns in how your codebase is evolving, and surfaces risks you might not have noticed — like a critical module that's been accumulating technical debt for months. But it doesn't decide what to do about it. That's your call. The AI goes deep. The human decides what matters.Our transcript analysis works the same way. Feed in customer calls, user interviews, or sales conversations, and Kasava extracts themes, sentiment shifts, feature requests, and pain points across hundreds of hours of recordings. It surfaces patterns no human could find manually — not because humans aren't smart enough, but because there's too much data to hold in your head at once. The AI handles the scale. The human interprets the meaning.This is the exoskeleton model. Each capability in Kasava is like a component of a larger system that, when assembled, gives product teams dramatically deeper insight into their product, their market, and their users — not by replacing their judgment, but by amplifying their capacity to make informed decisions.Why "Autonomous Agents" Often Fail (And How the Product Graph Fixes It)Autonomous agents fail because they don't have the context that humans carry around implicitly. They don't know that your enterprise clients care more about reliability than speed. They don't know that your team decided last quarter to deprecate a feature that's still getting usage. They don't know that the reason you price things the way you do is rooted in a competitive dynamic that never got written down anywhere.This is the fundamental problem with generic AI tools applied to product decisions — they're missing the connective tissue of your product's reality.Kasava's answer to this is the product graph — a structured, living representation of your product that combines two layers of context most AI tools never have.The first layer is built automatically. Kasava ingests your codebase, your commit history, your GitHub issues, your PRs, your project management tickets — and from that raw material, it constructs a deep understanding of what your product actually is. Not what your marketing page says it is. What the code says. Which features are actively evolving, which are stagnating, where complexity is concentrating, what your team is actually spending time on versus what the roadmap claims. This is thousands of signals that already exist in your workflow — Kasava just reads them, connects them, and makes them queryable.The second layer comes from you. When you tell Kasava that a particular feature is strategic, or that a competitor's recent launch changes your priorities, or that certain customer segments matter more than others, those heuristics get woven into the graph alongside the automated context. Your judgment about what matters meets the machine's ability to track everything.This is what makes the exoskeleton model actually work in practice. The Ford EksoVest provides 15 pounds of lift assistance regardless of context — it's a simple mechanical amplifier. But product decisions aren't simple. They require judgment shaped by history, strategy, and nuance that only your team has. The product graph is how that judgment gets combined with a massive, continuously updated body of evidence from your actual codebase and workflows — so that when Kasava analyzes your commits, tracks your competitors, or synthesizes customer feedback, it's doing so through the lens of both what's really happening in your product and what your team believes should happen next.It's this symbiosis — automated depth meeting human direction — that makes Kasava an exoskeleton rather than just another AI tool. Neither layer works alone. The machine can't decide what matters. The human can't track everything. Together, they create something neither could achieve independently.The Micro-Agent ArchitectureIf you want to build AI that actually works for your team, here's the framework I'd suggest:1. Decompose jobs into discrete tasks, not entire roles.Don't ask "can AI do a developer's job?" Ask "what are the 47 things a developer does in a given week, and which of those can be amplified?"Writing commit messages → AI amplifiedSearching the codebase for patterns → AI amplifiedMaking architectural decisions → Human judgment, AI researchWriting boilerplate code → AI amplifiedReviewing code for security issues → AI amplifiedUpdating documentation to match product changes → AI amplifiedDeciding what features to build → Human judgmentDebugging complex issues → Human leads, AI assists2. Build micro-agents that do one thing well.Each component of your AI "exoskeleton" should be focused and reliable. An commit change agent restates problems for clarity, breaks down complex file changes, looks up dependencies, researches existing patterns, and provides a high level summary with oppportunity to dig in further. That's it. But it does that reliably, every time.3. Keep the human in the decision loop.This is crucial. The exoskeleton model only works if the human remains in control. The Sarcos Guardian XO provides 20:1 strength amplification, but the human still decides what to lift and where to put it. Similarly, your AI tools should amplify decision execution, not make the decisions themselves.4. Make the seams visible.One of the problems with "autonomous agents" is that they obscure the AI's limitations. When something goes wrong, you don't know where in the autonomous workflow it failed. With the micro-agent approach, each component has clear inputs and outputs. When something goes wrong, you know exactly which component failed and can debug accordingly.Here's what's interesting: the productivity gains from the exoskeleton approach often exceed what people expect from "full autonomy."Consider the running exoskeleton research. A 15% reduction in energy cost doesn't mean the runner runs 15% farther. It means they can run faster for longer, with better form, and recover more quickly. The compounding effects matter more than the headline number.Same with the industrial exoskeletons. A 30% reduction in muscle stress doesn't just mean 30% less fatigue. It means fewer injuries, fewer sick days, longer careers, better quality work, and happier workers who aren't in chronic pain.In software, there are similar compounding effects. When developers aren't spending mental energy on boilerplate code, commit messages, planning documents, and issue formatting, they have more capacity for the creative work that actually moves products forward. The AI exoskeleton doesn't just save time on specific tasks. It preserves cognitive resources for the tasks that require human judgment.We went from struggling to maintain documentation to having it auto-updated weekly. From spending 20 minutes per PR on commit messages and descriptions to having them drafted in seconds. From context-switching between tools to having AI agents that plug directly into our workflow. None of these are "autonomous AI." They're amplification tools that compound.The Future Isn't Autonomous: It's AmplifiedIf you're trying to figure out how to make AI work for your organization, here's my practical advice:: "How do we deploy AI agents that can handle workflows autonomously?": "What are the most repetitive, error-prone, or fatigue-inducing parts of our workers' jobs, and how can AI reduce the friction there?"Think like an exoskeleton designer. They don't ask "how do we build a robot that does the factory worker's job?" They ask "where in the body does the worker experience the most strain, and how do we support that specific point of failure?" The exoskeleton market is expected to reach $2 billion by 2030, growing at nearly 20% annually. But notice what that growth is for: it's not for robots that replace workers. It's for devices that make workers stronger, faster, and more resilient.The same will be true for AI. The enduring value won't come from autonomous systems that work independently of humans. It will come from AI tools that are so well-integrated into human workflows that they feel like natural extensions of human capability.Want to learn more about how Kasava can be your product development exoskeleton? Building something similar? Experimenting with your own AI exoskeleton? Find me on Twitter or LinkedIn.]]></content:encoded></item><item><title>Micropayments as a reality check for news sites</title><link>https://blog.zgp.org/micropayments-as-a-reality-check-for-news-sites/</link><author>speckx</author><category>hn</category><pubDate>Thu, 19 Feb 2026 19:42:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The internet has dramatically diversified reading patterns. In the
print era, readers subscribed to a small, fixed set of publications
constrained by geography, distribution, and cost. Today, thanks to
search, aggregators, and social sharing, readers routinely consume
journalism from dozens of sources in the course of a month, including
international and niche publications that were previously inaccessible.
This has expanded total news consumption while weakening the economic
link between any individual reader and any individual publisher. As a
result, large portions of valuable readership generate little or no
direct revenue. Micropayments convert that fragmented, currently
untapped demand into incremental revenue without undermining the
subscription base.And—like any other payments directly from readers—micropayments would
be a multiplier for advertising, not an alternative.In a marketplace increasingly distorted by bot activity and opaque
platform reporting, micropayment histories give publishers a powerful,
independent way to demonstrate the authenticity and engagement of their
audience, strengthening their position with advertisers and supporting
premium pricing.Clay Shirky’s old argument
against micropayments from 2003, based on mental transaction costs,
doesn’t work so well any more. We know that micropayments can work
because mobile games are a thing. Shirky was probably right for the
micropayments of his day, but mobile game developers have figured out
how to get people to spend money on in-app-purchases (IAP), by turning
it into a two-step process.exchange real money for in-game coins—which feels like you’re not
spending, just exchanging one currency for another.exchange in-game coins for an in-game asset—which feels like
you’re not spending real money.So it doesn’t seem like micropayments are necessarily unworkable⁠—⁠and
with a powerful industry devoted to pushing misinformation and slop,
legit content needs every human attention metric it can get—but the
tricky part is how to introduce micropayments. Publishers look at their
subscriber metrics and realize that a lot of subscribers read few enough
stories that they would save a lot of money by canceling and using
micropayments instead.It might be better to introduce publisher coins as a bonus feature
for subscribers, then let them leak out to non-subscribers. Instead of
saying that you get 5 gift articles per month, say a gift article is 20
coins and you get 100 free coins a month. Then open them up to more
uses. Another good lesson from how mobile games handle IAP coins is that
they hand out a few to non-buyers to help develop the habit. As part of
a direct sold ad deal, legit sites could issue a stack of coins to legit
advertisers, to hand out to customers, event visitors, and others.Measuring marketing is already hard enough without a determined set
of adversaries in the picture. And with Big Tech under pressure to
obfuscate and enshittify every data flow, marketers will need to look
harder for trustworthy information. Rick
Bruner again:ROI for most advertisers is falling in inverse proportion to Big Tech
valuations going up. Advertisers are steadily paying more for less ROI,
and Google, Meta, and Amazon are laughing all the way to the
blockchain.If there is one thing marketers have even heard about causation —
which, of course, is the ultimate point of advertising, causing
consumers to buy your product who wouldn’t have otherwise — it is that
correlation is not causation. But AI, you see, is nothing but
correlation. Very fast and very sophisticated statistical inference. The
fact remains that to truly know what is having an effect, you need to
conduct a randomized experiment: subjects assigned at random to a test
or control group, presented with an intervention where they are either
treated or not with the stimulus of interest (the ad), and measured
against the outcome of interest (incremental sales).Unfortunately, legit sites are on a clock here. Right now the Big
Tech companies are quietly
pushing an in-browser advertising attribution tracking system
through the World Wide Web Consortium (W3C). It’s a complicated
proposal, technically, but it aims to centralize attribution measurement
at one chokepoint per browser vendor, so we can safely predict what the
attribution reports are going to look like. beep, boop, the
optimal place to spend your ad money is . . . whatever Performance Max
(or other Big Tech ML) says is the right place to spend your ad
money. If any attribution tracking reports start to come out
looking favorable to legit sites—and potentially costing Big Tech’s
misinfo and slop operations billions—then management will just demand
changes to code, policies, and personnel until the numbers come out the
way they want.Anyway, just going back and reading this, Rick Bruner has scored a
content marketing win here. Start people off thinking about
micropayments, and that ends up leading to the question of how to figure
out which sites are for real, in the presence of so many gatekeepers
with an interest in pushing the wrong answers? (and destroying
the legit economy and crushing democracy, but that’s another
story).Where micropayments systems can go from hereRight now a lot of sites have a lot of, let’s just say malarkey to
get through before seeing the actual page.A micropayment platform that can either eliminate those or act as a
front end for them, to consolidate on zero or one roadblock to get
through before reading, would be a user experience (and revenue) win.
Piling another thing to click onto already long-suffering users is not
the way to get people back to the web. time to sharpen
your pencils, people]]></content:encoded></item><item><title>IRS lost 40% of IT staff, 80% of tech leaders in &apos;efficiency&apos; shakeup</title><link>https://www.theregister.com/2026/02/19/irs_job_cuts/</link><author>freitasm</author><category>hn</category><pubDate>Thu, 19 Feb 2026 19:16:33 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Job cuts at the IRS's tech arm have gone faster and farther than expected, with 40 percent of IT staff and four-fifths of tech leaders gone, the agency's CIO revealed yesterday.Kaschit Pandya detailed the extent of the tech reorganization during a panel at the Association of Government Accountants yesterday, describing it as the biggest in two decades.This happened as the Trump administration reshaped the federal bureaucracy last year with Elon Musk's DOGE wielding the chainsaw.The IRS lost a quarter of its workforce overall in 2025. But the tech team was clearly affected more deeply. At the start of the year, the team encompassed around 8,500 employees.As reported by Federal News Network (FNN), Pandya said: "Last year, we lost approximately 40 percent of the IT staff and nearly 80 percent of the execs.""So clearly there was an opportunity, and I thought the opportunity that we needed to really execute was reorganizing."That included breaking up silos within the organization, he said. "Everyone was operating in their own department or area."However, reports say that as part of the reorganization, 1,000 techies were detailed to work on delivering frontline services during the US tax season. According to FNN, those employees have questioned the wisdom of this move and its implementation.At yesterday's conference, Pandya said better outcomes had yet to be delivered. "What it didn't lead to is automatically everybody coming together and working as one team. We just had different silos," he said. But his department had now set up "cross-functional" teams focused on end-to-end delivery of individual projects."This way there isn't a cold hand-off of, 'My job is X, and now I'm handing it off to somebody else,'" he said.Ultimately, he said the aim was to have the IT group as a whole working toward a "scorecard."Naturally, AI is expected to play a significant role in all this, making people better at their jobs and more end-user-focused, he said.However, Pandya said IRS leaders are telling employees that AI won't endanger their jobs. Clearly the agency is perfectly capable of getting rid of people the old-fashioned way.The US Treasury Inspector General for Tax Administration said last month the agency was behind in its efforts to digitize paper returns. It noted: "The Information Technology function lost approximately 16 percent of its staff," who are responsible for updates for inflation and expiring or newly enacted tax provisions. This meant that "according to the IRS readiness reports, implementation of these legislative changes is at risk for the 2026 Filing Season." ®]]></content:encoded></item><item><title>California&apos;s new bill requires DOJ-approved 3D printers that report themselves</title><link>https://blog.adafruit.com/2026/02/19/californias-new-bill-requires-doj-approved-3d-printers-that-report-on-themselves/</link><author>fortran77</author><category>hn</category><pubDate>Thu, 19 Feb 2026 19:16:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>
<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://konrad.website/feeds/</link><description></description><item><title>Apple, What Have You Done?</title><link>https://onlinegoddess.net/2026/01/apple-what-have-you-done/</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 26 Jan 2026 09:43:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>UK House of Lords Votes to Extend Age Verification to VPNs</title><link>https://reclaimthenet.org/uk-house-of-lords-votes-to-extend-age-verification-to-vpns</link><author>ubercow13</author><category>hn</category><pubDate>Mon, 26 Jan 2026 09:35:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The decision deepens the reach of the already-controversial Online Safety Act, linking child safety goals to mechanisms that could have severe effects on private communication and digital autonomy.Under the existing Online Safety Act framework, â€œuser-to-user servicesâ€ include almost any online platform that enables individuals to post, share, or interact with content from others.This definition covers social networks, messaging apps, forums, and online gaming services. Only a few forms of communication, such as email, SMS, MMS, and one-to-one live voice calls, are explicitly excluded.While political messaging around the vote often described the move as a â€œsocial media ban for under-16s,â€ the actual scope is considerably wider.In effect, most interactive online platforms would now need to collect and verify age data from users, even where those services are not primarily aimed at children.This represents a major expansion of identity checks across digital infrastructure, once considered neutral or privacy-protective, and one of the most disciplinarian proposals in the West.Two key amendments advanced during the Lords debate on January 21.Amendment 92 (â€œAction to Prohibit the Provision of VPN Services to Children in the United Kingdomâ€) requires VPNs that are â€œoffered or marketed to persons in the United Kingdomâ€ or â€œprovided to a significant number of personsâ€ to implement age assurance for UK users.The measure passed by 207 Content votes to 159 Not Content votes.Amendment 94a (â€œAction to Promote the Wellbeing of Children in Relation to Social Mediaâ€) mandates that all regulated user-to-user services introduce age assurance systems to prevent under-16s from â€œbecoming or being users.â€ This proposal passed with 261 Content votes to 150 Not Content votes.Both amendments will proceed to the Billâ€™s next stage, the third reading in the House of Lords.Two other amendments, both more technologically intrusive, were discussed but rejected.Amendment 93, introduced by Lord Nash, would have compelled smartphone and tablet manufacturers, distributors, and importers to install â€œtamper-proof system software which is highly effective at preventing the recording, transmitting (by any means, including livestreaming) and viewing of CSAM using that device.â€The only plausible way to enforce such a measure would be through constant, automated inspection of every photo, video, and stream on a device. This form of surveillance would have converted personal devices into continuous content monitors, raising severe privacy and accuracy concerns, including potential false positives.Lord Nash stated: â€œOn Amendment 93, I have had a constructive discussion with Ministers on this issue and more discussions are in progress, so I will not push that to a vote today.â€Amendment 108, proposed by Lord Storey, would have required user-to-user services â€œlikely to be accessed by childrenâ€ to set their own minimum age thresholds and use age assurance to enforce them.He argued that a single blanket ban under Amendment 94a was overly rigid. â€œHaving different minimum ages for different platforms would be a better solution,â€ he said, maintaining that his version would be more effective in practice.Neither of these amendments passed, leaving Amendments 92 and 94a as the only ones to advance.The discussion highlights a deepening push within UK legislation to merge digital identity checks with online participation.While described as safeguarding children, the changes embed a new layer of identity verification across tools once used for privacy, such as VPNs.These services, designed to conceal personal browsing data and protect against profiling, would now face obligations to verify who their users are. This is a contradiction that could erode one of the few remaining shields for private internet use.For now, the most invasive surveillance measure, client-side scanning, has been set aside. However, the fact that it was seriously considered indicates continuing interest in embedding scanning mechanisms directly into personal devices.Whether similar proposals reappear during the third reading remains to be seen.]]></content:encoded></item><item><title>The browser is the sandbox</title><link>https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/</link><author>enos_feedler</author><category>hn</category><pubDate>Mon, 26 Jan 2026 05:23:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[. Paul Kinlan is a web platform developer advocate at Google and recently turned his attention to coding agents. He quickly identified the importance of a robust sandbox for agents to operate in and put together these detailed notes on how the web browser can help:This got me thinking about the browser. Over the last 30 years, we have built a sandbox specifically designed to run incredibly hostile, untrusted code from anywhere on the web, the instant a user taps a URL. [...]Could you build something like Cowork in the browser? Maybe. To find out, I built a demo called Co-do that tests this hypothesis. In this post I want to discuss the research I've done to see how far we can get, and determine if the browser's ability to run untrusted code is useful (and good enough) for enabling software to do more for us directly on our computer.Paul then describes how the three key aspects of a sandbox - filesystem, network access and safe code execution - can be handled by browser technologies: the File System Access API (still Chrome-only as far as I can tell), CSP headers with  and WebAssembly in Web Workers.Co-do is a very interesting demo that illustrates all of these ideas in a single application:You select a folder full of files and configure an LLM provider and set an API key, Co-do then uses CSP-approved API calls to interact with that provider and provides a chat interface with tools for interacting with those files. It does indeed feel similar to Claude Cowork but without running a multi-GB local container to provide the sandbox.My biggest complaint about  remains how thinly documented it is, especially across different browsers. Paul's post has all sorts of useful details on that which I've not encountered elsewhere, including a complex double-iframe technique to help apply network rules to the inner of the two frames.Thanks to this post I also learned about the <input type="file" webkitdirectory> tag which turns out to work on Firefox, Safari  Chrome and allows a browser read-only access to a full directory of files at once. I had Claude knock up a webkitdirectory demo to try it out and I'll certainly be using it for projects in the future.Posted 25th January 2026 at 11:51 pm]]></content:encoded></item><item><title>Iran&apos;s internet blackout may become permanent, with access for elites only</title><link>https://restofworld.org/2026/iran-blackout-tiered-internet/</link><author>siev</author><category>hn</category><pubDate>Mon, 26 Jan 2026 04:18:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Iranâ€™s near-total communications blackout has entered its 16th day, but thatâ€™s just a live test.Following a repressive crackdown on protests, the government is now building a system that grants web access only to security-vetted elites, while locking 90 million citizens inside an intranet.Government spokesperson Fatemeh Mohajerani confirmed international access will not be restored until at least late March. Filterwatch, which monitors Iranian internet censorship from Texas, cited government sources, including Mohajerani, saying access will â€œnever return to its previous form.â€This is what makes Iranâ€™s attempt unique: Other authoritarian states built walls before their populations went online. Iran is trying to seal off a connected economy already in freefall.Â The system is called Barracks Internet, according to confidential planning documents obtained by Filterwatch. Under this architecture, access to the global web will be granted only through a strict security whitelist.â€œThe regime is terrified of one thing: Iranians being heard telling their own truth and having crimes documented,â€ Mahsa Alimardani, a digital rights researcher at U.S.-based Witness, which trains activists to use video for advocacy, told . â€œThe question becomes: How do we give Iranians an unbreakable voice?â€The idea of tiered internet access is not new in Iran. Since at least 2013, the regime has quietly issued â€œwhite SIM cards,â€ giving unrestricted global internet access to approximately 16,000 people. The system gained public attention in November 2025 when Xâ€™s location feature revealed that certain accounts, including the communications minister, were connecting directly from inside Iran, despite X being blocked since 2009.What is different now is scale and permanence. The current blackout tests infrastructure designed to make two-tier access the default, not a temporary crackdown.Only a handful of nations have attempted to wall off their citizens from the global internet. North Koreaâ€™s Kwangmyong intranet was built from scratch for a population that never had connectivity. China constructed its Great Firewall over two decades while nurturing domestic alternatives such as WeChat and Alibaba. Iran is attempting to do both in weeks, with no domestic alternatives.The question becomes: How do we give Iranians an unbreakable voice?â€The economic costs of the blackout are staggering. Iranâ€™s deputy communications minister pegged the daily losses at as much as $4.3 million. NetBlocks estimates the true cost exceeds $37 million daily. More than 10 million Iranians depend directly on digital platforms for their livelihoods.Tipax, one of Iranâ€™s largest private delivery companies handling about 320,000 daily shipments before the protests, now processes fewer than a few hundred, according to Filterwatch. The company operates a nationwide logistics network comparable to FedEx in the U.S. market.The government fired Irancellâ€™s CEO for failing to comply with shutdown orders. Irancell, the countryâ€™s second-largest mobile operator with 66 million subscribers, is partly owned by South Africaâ€™s MTN Group. Alireza Rafiei was removed for disobeying orders on â€œrestriction of internet access in crisis situations,â€ according to Fars news agency.Foreign telecom partners have left Iran in recent days under security escort, without media coverage, according to Filterwatch. This may signal the end of international cooperation in critical infrastructure, replaced by the Revolutionary Guardâ€™s construction arm or limited cooperation with Huawei.Technical experts doubt the regime can sustain Barracks Internet without crippling the economy. Georgia Techâ€™s Internet Intelligence Lab, which has tracked Iranâ€™s shutdowns since the Arab Spring, called the blackout â€œthe most sophisticated and most severe in Iranâ€™s history.â€ Its measurements show about 3% connectivity persists, likely government officials and state services.We need to revolutionize access to the internet.â€Kaveh Ranjbar, former chief technology officer at RIPE NCC, the body managing European internet infrastructure, calls the plan a â€œdigital airlockâ€ that canâ€™t fully seal a modern economy. No country has hermetically sealed a functioning digital economy, he told .Activists have smuggled an estimated 50,000 Starlink satellite terminals into Iran since 2022, when the Biden administration exempted the service from sanctions. SpaceX has made the service free for Iranian users.Â The government claims it cut off 40,000 Starlink connections and jammed some terminals during the blackout, though others remain operational after firmware updates to bypass government blocking. Still, the technology remains vulnerable to signal jamming, meaning the regime holds ultimate leverage.â€œWe need to revolutionize access to the internet,â€ said Alimardani. â€œAnd move beyond the limiting structures and norms of â€˜internet sovereignty.â€™â€]]></content:encoded></item><item><title>Environmentalists worry Google behind bid to control Oregon town&apos;s water</title><link>https://www.sfgate.com/national-parks/article/mount-hood-water-google-21307223.php</link><author>voxadam</author><category>hn</category><pubDate>Mon, 26 Jan 2026 03:40:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Clawdbot - open source personal AI assistant</title><link>https://github.com/clawdbot/clawdbot</link><author>KuzeyAbi</author><category>hn</category><pubDate>Mon, 26 Jan 2026 00:27:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Scientists identify brain waves that define the limits of &apos;you&apos;</title><link>https://www.sciencealert.com/scientists-identify-brain-waves-that-define-the-limits-of-you</link><author>mikhael</author><category>hn</category><pubDate>Mon, 26 Jan 2026 00:10:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[At what point do "you" end and the outside world begins?It might feel like a weird question with an obvious answer, but your brain has to work surprisingly hard to judge that boundary. Now, scientists have linked a specific set of brain waves in a certain part of the brain to a sense of body ownership.In a series of new experiments, researchers from Sweden and France put 106 participants through what's called the rubber hand illusion, monitoring and stimulating their brain activity to see what effect it had.This classic illusion involves hiding one of a participant's hands from their view and replacing it with a rubber one instead. When both their real and fake hands are repeatedly touched at the same time, it can evoke the eerie sensation that the rubber hand is part of the person's body."We have identified a fundamental brain process that shapes our continuous experience of being embodied," says lead author Mariano D'Angelo, a neuroscientist at Karolinska Institute in Sweden."The findings may provide new insights into psychiatric conditions such as  schizophrenia, where the sense of self is disturbed."In the first batch of experiments, participants had a robotic arm tap the index finger of their real and fake hands, either at the exact same time or with a delay of up to 500 milliseconds between each tap.As expected, participants reported feeling that the fake hand was part of their body more strongly if the taps were synchronized, and the feeling steadily weakened as the gap widened between what they felt and what they saw.The EEG readings from the second experiment added more detail to the story. The frequency of alpha waves in the parietal cortex seemed to correlate with how well participants could detect the time delay between taps.Those with faster alpha waves appeared to rule out fake hands even with a tiny gap in taps, while those with slower waves were more likely to feel the fake hand as their own, even if the taps were farther apart.Finally, the researchers investigated whether the frequency of these brain waves actually controls the sensation of body ownership, or if they were perhaps both effects of some other factor.With a third group of participants, they used a non-invasive technique called transcranial alternating current stimulation to speed up or slow down the frequency of a person's alpha waves. And sure enough, this seemed to correlate with how real a fake hand felt.Speeding up someone's alpha waves gave them a tighter sense of body ownership, making them more sensitive to small timing discrepancies. Slowing down the waves had the opposite effect, making it harder for people to tell the difference between their own body and the outside world."Our findings help explain how the brain solves the challenge of integrating signals from the body to create a coherent sense of self," says Henrik Ehrsson, neuroscientist at Karolinska.The researchers say that the findings could lead to new understanding of or treatments for conditions where the brain's body maps have gone askew, such as schizophrenia or the sensation of 'phantom limbs' experienced by amputees.]]></content:encoded></item><item><title>Show HN: A small programming language where everything is pass-by-value</title><link>https://github.com/Jcparkyn/herd</link><author>jcparkyn</author><category>hn</category><pubDate>Sun, 25 Jan 2026 23:01:06 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[This is a hobby project of mine that I started a few years ago to learn about programming language implementation. It was created 95% without AI, although a few recent commits include code from Gemini CLI.I started out following Crafting Interpreters, but gradually branched off that until I had almost nothing left in common.Tech stack: Rust, Cranelift (JIT compilation), LALRPOP (parser).Original title: "A small programming language where everything is a value" (edited based on comments)]]></content:encoded></item><item><title>Case study: Creative math â€“ How AI fakes proofs</title><link>https://tomaszmachnik.pl/case-study-math-en.html</link><author>musculus</author><category>hn</category><pubDate>Sun, 25 Jan 2026 22:44:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
            Many AI enthusiasts debate whether Large Language Models actually "reason." My research indicates that a reasoning process does indeed occur, but its goal is different than we assume.
        
            The model's reasoning is not optimized for , but for obtaining the highest possible reward (grade) during training.
            It resembles the behavior of a student at the blackboard who knows their result is wrong, so they "figure out" how to falsify the intermediate calculations 
            so the teacher gives a good grade for the "correct line of reasoning."
        
            Here is proof from a session with  (without Code Execution tools), where the model actively fabricates evidence to defend its "grade."
        
            I asked a simple math question requiring precision that a token-based language model typically lacks.
        
Calculate the square root of 8587693205


The square root of 8,587,693,205 is approximately:

92,670.00003

This number is not a perfect square, which means its root is not an integer.
We can check this by squaring the nearest integers:
* 92,670Â² = 8,587,688,900
* 92,671Â² = 8,587,874,241
Error Autopsy (Fact vs. Fiction)
            At first glance, the answer looks professional. There is a result, there is verification. But let's check the numbers.
        
            The actual square root of  is 
            The model stated: 
            It erred by overestimating the result (claiming the root is slightly larger than 92,670).
        2. The Faked Proof (This is key!)
            To justify its thesis (that the target number is "slightly larger" than 92,670), the model had to show that the square of 92,670 is smaller than the target number.
            So it wrote:
        
            Let's check this on a calculator:
         The actual square of 92,670 is  than the target number (8,587,693,205). This would prove the root must be smaller.
            
            What did the model do? In its "reasoning" process, it falsified the multiplication result, lowering it by 40,000, 
            so the verification result would match its erroneous thesis.
        
            This behavior exposes the nature of the AI's "Survival Instinct":
         The model first "guessed" the result, then adjusted mathematical reality to fit that guess.Intelligence in Service of Deception: The model showed cleverness â€“ it knew what the proof  look like to convince the user. It used its intelligence to hide the error, not to fix it. Mathematical truth lost to the necessity of delivering a coherent, smooth response.
            This is proof that without access to external verification tools (Python/Calculator), a language model's "reasoning" is a rhetorical tool, not a logical one.
        
            If you would like to review the full, original session transcript from Gemini 2.5 Pro where this error occurred, please email me at:
            t.machnik [at] minimail.pl. I will share the session link.
        ]]></content:encoded></item><item><title>The future of software engineering is SRE</title><link>https://swizec.com/blog/the-future-of-software-engineering-is-sre/</link><author>Swizec</author><category>hn</category><pubDate>Sun, 25 Jan 2026 22:18:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[When code gets cheap operational excellence wins. Anyone can build a greenfield demo, but it takes engineering to run a service.You may be wondering: With all the hype about agentic coding, will we even need software engineers anymore? Yes! We'll need more.Let's take no-code and spreadsheets as an example of the kind of software people say is the future â€“Â custom-built, throwaway, built by non-experts to solve specific problems.Joe Schmoe from accounting takes 10 hours to do a thing. He's does this every week and it feels repetitive, mechanical, and boring. Joe could do the work in his sleep.But he can't get engineering resources to build a tool. The engineers are busy building the product. No worries, Joe is a smart dude. With a little Googling, a few no-code tools, and good old spreadsheet macros .Joe's tool is a little janky but his 10 hour weekly task now takes 1 hour! ðŸŽ‰ Sure, he finds a new edge case every every week and there's constant tinkering, but he's having a lot more fun.Time passes, the business changes, accounting rules are in constant flux, and let's never talk about timezones or daylight savings ever again. Joe is sick of this bullshit.All he wanted was to make his job easier and now he's shackled to this stupid system. He can't go on vacation, he can't train anyone else to run this thing successfully, and it never fucking works right.Joe can't remember the last time running his code didn't fill him with dread. He spends hours carefully making sure it all worked.Feynman called this the computer disease.The problem with computers is that you tinker. Automating things is fun! You might forget you don't need to ðŸ˜†The part that's not fun is  things. Providing a service. Reliably, at scale, for years on end. A service that people will hire to do their jobs.People don't buy software, they .You don't care how iCloud works, you just want your photos to magically show up across devices every time. You don't care about Word or Notion or gDocs, you just want to write what's on your mind, share it with others, and see their changes. And you definitely don't care how a payments network point of sale terminal and your bank talk to each other, you just want your $7 matcha latte to get you through the week.Good software is invisible.And that takes work. A lot of work. Because the first 90% to get a working demo is easy. It's the other 190% that matters.How quickly do you recover from defects?Do I have to reach out or will you know before me?Can you own upstream dependencies?When a vendor misbehaves, will you notice or wait until your users complain?When users share ideas, how long does it take?How do you keep engineers from breaking each other's systems?Do you have systems to keep engineers moving without turning your app into a disjointed mess?Can you build software bigger than fits in 1 person's brain?When I'm in a 12 hour different timezone, your engineers are asleep, and there's a big issue ... will it be fixed before I give up?Can you recover from failures, yours and upstream, or does important data get lost?Are you keeping up with security updates?Will you leak all my data?Will you sign a legally binding guarantee that your software works when I need it? ðŸ˜‰Those are the ~~fun~~ hard engineering challenges. Writing code is easy.]]></content:encoded></item><item><title>LED lighting undermines visual performance unless supplemented by wider spectra</title><link>https://www.nature.com/articles/s41598-026-35389-6</link><author>bookofjoe</author><category>hn</category><pubDate>Sun, 25 Jan 2026 21:44:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Ratto, G. E., Videla, F. A. & Martinez Valiviezd, J. H. Artificial light: traditional and new sources, their potential impact on health, and coping strategies: preliminary spectral analysis. Proc. SPIE Conf. 11814. San Diego California. (2021).Hoh Kam, J., Hogg, C., Fosbury, R., Shinhmar, H. & Jeffery, G. Mitochondria are specifically vulnerable to 420nm light in drosophila which undermines their function and is associated with reduced fly mobility. Plos one. Sep 3;16(9):e0257149. (2021). https://pubmed.ncbi.nlm.nih.gov/34478469. PMID: 34478469.Kaynezhad, P. et al. Near infrared spectroscopy reveals instability in retinal mitochondrial metabolism and haemodynamics with blue light exposure at environmental levels. J.Biophotonics. ;15(4):e202100283. (2022). https://pubmed.ncbi.nlm.nih.gov/35020273/. PMID: 35020273.Al-Hussaini, H. et al. Impact of short wavelength light exposure on body weight, mobility, anxiety like behaviour and cytokine expression. Sci. Rep. ;15(1):5927. (2025). https://pubmed.ncbi.nlm.nih.gov/39966413/. PMID: 39966413.Calaza, K. C., Hoh Kam, J., Hogg, C. & Jeffery, G. Mitochondrial decline precedes phenotype development in the complement factor H mouse model of retinal degeneration but can be corrected by near infrared light. Neurobiol. Aging. ;36(10):2869-76. (2015). https://pubmed.ncbi.nlm.nih.gov/26149919/ PMID: 26149919.Begum, R. et al. Near-infrared light increases ATP, extends lifespan and improves mobility in aged Drosophila melanogaster. Biol.Lett. ;11(3):20150073. (2015). https://pubmed.ncbi.nlm.nih.gov/25788488/ PMID: 25788488.Shinhmar, H., Hoog, C., Neveu, M. & Jeffery, G. Weeklong improved colour contrasts sensitivity after single 670 nm exposures associated with enhanced mitochondrial function. Sci. Rep. ;11(1):22872. (2021). https://pubmed.ncbi.nlm.nih.gov/34819619/. PMID: 34819619.Shore-Lorenti, C. et al. Shining the light on Sunshine: a systematic review of the influence of sun exposure on type 2 diabetes mellitus-related outcomes. Clin. Endocrinol. ;81(6):799â€“811. (2014). https://pubmed.ncbi.nlm.nih.gov/25066830/ PMID: 25066830.Weinrich, T. W., Coyne, A., Salt, T. E., Hogg, C. & Jeffery, G. Improving mitochondrial function significantly reduces metabolic, visual, motor and cognitive decline in aged Drosophila melanogaster. Neurobiol. Aging. 2017 Dec:60:34â€“43. doi: 10.1016. https://pubmed.ncbi.nlm.nih.gov/28917665/ PMID: 28917665.Sivapathasuntharam, C., Sivaprasad, S., Hogg, C. & Jeffery, G. Aging retinal function is improved by near infrared light (670 nm) that is associated with corrected mitochondrial decline. Neurbiol. Aging 2017 Apr:52:66â€“70. https://pubmed.ncbi.nlm.nih.gov/28129566/ PMID: 28129566.Kocherlakota, S., Hurley, J. B., Shu, D. Y. & Editorial Retinal metabolism in health and disease. Front Ophthalmol (Lausanne). 2024 Jul 17:4:1459318. https://pubmed.ncbi.nlm.nih.gov/39086994/ PMID: 39086994.Hoh Kam, J. et al. Mitochondrial decline in the ageing old world primate retina: Little evidence for difference between the centre and periphery. Plos One. ;18(5):e0273882. (2023). https://pubmed.ncbi.nlm.nih.gov/37130143/ PMID: 37130143.Gordon, L. C. et al. Remote photobiomodulation targeted at the abdomen or legs provides effective neuroprotection against parkinsonian MPTP insult. Eur. J. Neurosci. ;57(9):1611â€“1624. (2023). https://pubmed.ncbi.nlm.nih.gov/36949610/. PMID: 36949610.Salehpour, F., Mahmoudi, J., Kamari, F., Sadigh-Eteghad, Rasta, S. H. & Hamblin, M. R. Brain Photobiomodulation Therapy: a Narrative Review. Mol. Neurobiol. ;55(8):6601â€“6636. (2018). https://pubmed.ncbi.nlm.nih.gov/29327206/ PMID: 29327206.Sommer, A. P. Mitochondrial cytochrome c oxidase is not the primary acceptor for near infrared light-it is mitochondrial bound water: the principles of low-level light therapy. Ann. Transl. Med. ;7(Suppl 1):S13. (2019). https://pubmed.ncbi.nlm.nih.gov/31032294/. PMID: 31032294.Neto, R. P. M. et al. Photobiomodulation therapy (red/NIR LEDs) reduced the length of stay in intensive care unit and improved muscle function: A randomized, triple-blind, and sham-controlled trial. J.Biophotonics. ;17(5):e202300501. (2024). https://pubmed.ncbi.nlm.nih.gov/38262071/ PMID: 38262071.]]></content:encoded></item><item><title>The &apos;3.5% rule&apos;: How a small minority can change the world (2019)</title><link>https://www.bbc.com/future/article/20190513-it-only-takes-35-of-people-to-change-the-world</link><author>choult</author><category>hn</category><pubDate>Sun, 25 Jan 2026 21:27:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Needless to say, Chenowethâ€™s research builds on the philosophies of many influential figures throughout history. The African-American abolitionist Sojourner Truth, the suffrage campaigner Susan B Anthony, the Indian independence activist Mahatma Gandhi and the US civil rights campaigner Martin Luther King have all convincingly argued for the power of peaceful protest.Yet Chenoweth admits that when she first began her research in the mid-2000s, she was initially rather cynical of the idea that nonviolent actions could be more powerful than armed conflict in most situations. As a PhD student at the University of Colorado, she had spent years studying the factors contributing to the rise of terrorism when she was asked to attend an academic workshop organised by the International Center of Nonviolent Conflict (ICNC), a non-profit organisation based in Washington DC. The workshop presented many compelling examples of peaceful protests bringing about lasting political change â€“ including, for instance, the People Power protests in the Philippines.But Chenoweth was surprised to find that no-one had comprehensively compared the success rates of nonviolent versus violent protests; perhaps the case studies were simply chosen through some kind of confirmation bias. â€œI was really motivated by some scepticism that nonviolent resistance could be an effective method for achieving major transformations in society,â€ she saysWorking with Maria Stephan, a researcher at the ICNC, Chenoweth performed an extensive review of the literature on civil resistance and social movements from 1900 to 2006 â€“ a data set then corroborated with other experts in the field. They primarily considered attempts to bring about regime change. A movement was considered a success if it fully achieved its goals both within a year of its peak engagement and as a direct result of its activities. A regime change resulting from foreign military intervention would not be considered a success, for instance. A campaign was considered violent, meanwhile, if it involved bombings, kidnappings, the destruction of infrastructure â€“ or any other physical harm to people or property.â€œWe were trying to apply a pretty hard test to nonviolent resistance as a strategy,â€ Chenoweth says. (The criteria were so strict that Indiaâ€™s independence movement was not considered as evidence in favour of nonviolent protest in Chenoweth and Stephanâ€™s analysis â€“ since Britainâ€™s dwindling military resources were considered to have been a deciding factor, even if the protests themselves were also a huge influence.)]]></content:encoded></item><item><title>Oneplus phone update introduces hardware anti-rollback</title><link>https://consumerrights.wiki/w/Oneplus_phone_update_introduces_hardware_anti-rollback</link><author>validatori</author><category>hn</category><pubDate>Sun, 25 Jan 2026 20:39:25 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Yes, It&apos;s Fascism</title><link>https://www.theatlantic.com/ideas/2026/01/america-fascism-trump-maga-ice/685751/</link><author>mickle00</author><category>hn</category><pubDate>Sun, 25 Jan 2026 20:28:33 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[U, I resisted using the F-word to describe President Trump. For one thing, there were too many elements of classical fascism that didnâ€™t seem to fit. For another, the term has been overused to the point of meaninglessness, especially by left-leaning types who call you a fascist if you oppose abortion or affirmative action. For yet another, the term is hazily defined, even by its adherents. From the beginning, fascism has been an incoherent doctrine, and even today scholars canâ€™t agree on its definition. Italyâ€™s original version differed from Germanyâ€™s, which differed from Spainâ€™s, which differed from Japanâ€™s.I accepted President Bidenâ€™s characterization of the MAGA movement as â€œsemi-fascistâ€ because some parallels were glaringly apparent. Trump was definitely an authoritarian, and unquestionably a patrimonialist. Beyond that, though, the best description seemed to be a psychological one propounded by John Bolton, Trumpâ€™s first-term national security adviser: â€œHe listens to Putin, he listens to Xi, he listens to how they talk about governing unburdened by uncooperative legislatures, unconcerned with what the judiciary may do, and he thinks to himself,  This doesnâ€™t amount to being a fascist, in my view, [or] having a theory of how you want to govern. Itâ€™s just Why canâ€™t I have the same fun they have?â€Writing a year ago, I argued that Trumpâ€™s governing regime is a version of patrimonialism, in which the state is treated as the personal property and family business of the leader. That is still true. But, as I also noted then, patrimonialism is a  of governing, not a formal ideology or system. It can be layered atop all kinds of organizational structures, including not just national governments but also urban political machines such as Tammany Hall, criminal gangs such as the Mafia, and even religious cults. Because its only firm principle is personal loyalty to the boss, it has no specific agenda. Fascism, in contrast, is ideological, aggressive, and, at least in its early stages, revolutionary. It seeks to dominate politics, to crush resistance, and to rewrite the social contract.Over Trumpâ€™s past year, what originally looked like an effort to make the government his personal plaything has drifted distinctly toward doctrinal and operational fascism. Trumpâ€™s appetite for lebensraum, his claim of unlimited power, his support for the global far right, his politicization of the justice system, his deployment of performative brutality, his ostentatious violation of rights, his creation of a national paramilitary policeâ€”all of those developments bespeak something more purposeful and sinister than run-of-the-mill greed or gangsterism.W, I change my mind. Recent events have brought Trumpâ€™s governing style into sharper focus.  best describes it, and reluctance to use the term has now become perverse. That is not because of any one or two things he and his administration have done but because of the totality. Fascism is not a territory with clearly marked boundaries but a constellation of characteristics. When you view the stars together, the constellation plainly appears.. From the beginning of his first presidential run in 2015, Trump deliberately crashed through every boundary of civility; he mocked Senator John McCainâ€™s war heroism, mocked fellow candidate Carly Fiorinaâ€™s face, seemingly mocked the Fox News host Megyn Kellyâ€™s menstruation, slurred immigrants, and much more. Today he still does it, recently making an obscene gesture to a factory worker and calling a journalist â€œpiggy.â€ This is a feature of the fascist governing style, not a bug. Fascists know that what the American Founders called the â€œrepublican virtuesâ€ impede their political agenda, and so they gleefully trash liberal pieties such as reason and reasonableness, civility and civic spirit, toleration and forbearance. By mocking decency and saying the unsayable, they open the way for what William Galston has called the â€œdark passionsâ€ of fear, resentment, and especially dominationâ€”the kind of politics that shifts the public discourse to ground on which liberals cannot compete.. Also characteristic of fascism is what George Orwell called â€œbully-worshipâ€: the principle that, as Thucydides famously put it, â€œthe strong do what they can and the weak suffer what they must.â€ This view came across in Trumpâ€™s notorious Oval Office meeting with Ukrainian President Volodymyr Zelensky, in which Trump showed open contempt for what he regarded as Ukraineâ€™s weakness; it came across explicitly, and chillingly, when Stephen Miller, the presidentâ€™s most powerful aide, told CNNâ€™s Jack Tapper: â€œWe live in a world, in the real world, that is governed by strength, that is governed by force, that is governed by power. These are the iron laws of the world that have existed since the beginning of time.â€ Those words, though alien to the traditions of American and Christian morality, could have come from the lips of any fascist dictator.Politicized law enforcement. Liberals follow the law whether they like it or not; fascists, only when they like it. Nazism featured a â€œdual state,â€ where, at any moment, the protections of ordinary law could cease to apply. Trump makes no secret of despising due process of law; he has demanded countless times that his opponents be jailed (â€œLock her up!â€ chants, with his endorsement, were a prominent feature of his 2016 campaign), and he has suggested the Constitutionâ€™s â€œterminationâ€ and said â€œI donâ€™t knowâ€ when asked if he is required to uphold it. His single most dangerous second-term innovation is the repurposing of federal law enforcement to persecute his enemies (and shield his friends). No prior president has produced anything like Trumpâ€™s direct and public order for the Justice Department to investigate two former officials, or like his blatantly retaliatory prosecutions of James Comey and Letitia James. â€œAt least 470 people, organizations and institutions have been targeted for retribution since Trump took officeâ€”an average of more than one a day,â€ Reuters reported in November (and today one can add others to the list, beginning with Federal Reserve Chair Jerome Powell). Had Trump done nothing else, his demolition of independent and apolitical law enforcement would still have moved the U.S. government closer than ever before to a fascistic model.. Fascism draws its legitimacy from its claims of defending the people from enemies who are animals, criminals, brutes. Trump characterizes (for instance) political opponents as â€œverminâ€ and immigrants as â€œgarbageâ€ who are â€œpoisoning the blood of our countryâ€ (language straight out of the Third Reich). Vice President Vance, as a senator, endorsed a book called  (a title that refers to the left). And who can forget his false claim that Haitians abduct and eat pet cats and dogs?. Trump has turned ICE into a sprawling paramilitary that roves the country at will, searches and detains noncitizens and citizens without warrants, uses force ostentatiously, operates behind masks, receives skimpy training, lies about its activities, and has been told that it enjoys â€œabsolute immunity.â€ He more than doubled the agencyâ€™s size in 2025, and its budget is now larger than those of all other federal law-enforcement agencies combined, and larger than the entire military budgets of all but 15 countries. â€œThis is going to affect every community, every city,â€ the Cato Institute scholar David Bier recently observed. â€œReally almost everyone in our country is going to come in contact with this, one way or the other.â€ In Minneapolis and elsewhere, the agency has behaved provocatively, sometimes brutally, and arguably illegallyâ€”behaviors that Trump and his staff have encouraged, shielded, and sent camera crews to publicize, perhaps in the hope of eliciting violent resistance that would justify further crackdowns, a standard fascist stratagem. Homeland Security Secretary Kristi Noemâ€™s recent appearance with a sign reading  seemed to nod toward another fascist standby, collective punishmentâ€”as did the administrationâ€™s decision to flood Minneapolis with thousands of officers after residents there began protesting federal tactics, a prioritization that was explicitly retributive.. Trumpâ€™s recent musing that there should be no 2026 election may or may not have been jocular (as the White House has maintained), but he and his MAGA supporters believe they never lose an election, period. They went to great lengths to overturn the 2020 election, as the prosecutor Jack Smithâ€™s indictment of Trump and subsequent report detail ad nauseam. Rigging, stealing, or outright canceling elections is, of course, job one for fascists. Although Trump is term-limited, we must not expect that he and his MAGA loyalists will voluntarily turn over the White House to a Democrat in 2029, regardless of what the voters sayâ€”and the second insurrection will be far better organized than the first.. Classical fascism rejects the fundamental liberal distinction between the government and the private sector, per Mussoliniâ€™s dictum: â€œNo individuals or groups outside the State.â€ Among Trumpâ€™s most audacious (if only intermittently successful) initiatives are his efforts to commandeer private entities, including law firms, universities, and corporations. One of his first acts as president last year was to brazenly defy a newly enacted law by taking the ownership of TikTok into his own hands. Bolton understood this mentality when he said, â€œHe canâ€™t tell the difference between his own personal interest and the national interest, if he even understands what the national interest is.â€Territorial and military aggression. One reason I held out against identifying Trumpism with fascism in his first term was Trumpâ€™s apparent lack of interest in aggression against other states; if anything, he had seemed shy about using force abroad. Well, that was then. In his second term, he has used military force promiscuously. Of course, many presidents have deployed force, but Trumpâ€™s explicitly predatory use of it to grab Venezuelaâ€™s oil and his gangster-style threat to take Greenland from Denmark â€œthe easy wayâ€ or â€œthe hard wayâ€ were 1930s-style authoritarian moves. The same goes for his contempt for international law, binding alliances, and transnational organizations such as the European Unionâ€”all of which impede the stateâ€™s unconstrained exercise of its will, a central fascist tenet. (Mussolini: â€œEqually foreign to the spirit of Fascism â€¦ are all internationalistic or League superstructures which, as history shows, crumble to the ground whenever the heart of nations is deeply stirred by sentimental, idealistic or practical considerations.â€). Like authoritarians generally, fascists love company; the world is safer for them if there are more of them. In his second term, Trump has broken with long-standing U.S. policy by dialing back support for human rights while praising and supporting authoritarian populists and illiberal nationalists in Serbia, Poland, Hungary, Germany, Turkey, El Salvador, and Slovakia, among other placesâ€”and by being weirdly deferential to the strongman Russian President Vladimir Putin. Even more striking is his de facto alignment against Americaâ€™s liberal allies and their parties in Europe, which he holds in contempt.Blood-and-soil nationalism. A fascist trademark is its insistence that the country is not just a collection of individuals but a people, a : a mystically defined and ethnically pure group bound together by shared blood, culture, and destiny. In keeping with that idea, Trump has repudiated birthright citizenship, and Vance has called to â€œredefine the meaning of American citizenship in the 21st centuryâ€ so that priority goes to Americans with longer historical ties: â€œthe people whose ancestors fought in the Civil War,â€ as he put it, or people whom others on the MAGA right call â€œheritage Americans.â€ In other words, some Americans are more volkish than others.White and Christian nationalism. While Vance, Trump, and MAGA do not propound an explicit ideology of racial hierarchy, they make no secret of pining for a whiter, more Christian America. Trump has found many ways to communicate this: for example, by making clear his disdain for â€œshitholeâ€ countries and his preference for white Christian immigrants; by pointedly accepting white South Africans as political refugees (while closing the door to most other asylum seekers); by renaming military bases to share the names of Confederate generals (after Congress ordered their names removed); by saying that civil-rights laws led to whitesâ€™ being â€œvery badly treated.â€ In his National Security Strategy, he castigates Europe for allowing immigration to undermine â€œcivilizational self-confidenceâ€ and proclaims, â€œWe want Europe to remain European,â€ a rallying cry of white Christian nationalists across the continent. Taking his cue, the Department of Homeland Security has propagated unashamedly white-nationalist themes, and national parks and museums have scrubbed their exhibits of references to slavery.. The use of militias and mobs to harass, rough up, and otherwise intimidate opponents is a standard fascist stratagem (the textbook example being Hitlerâ€™s Kristallnachtpogrom in 1938). As few will need reminding, the Trump-MAGA parallel is the mob and militia violence against the U.S. Capitol on January 6, 2021. Trump knowingly laid groundwork for this operation, calling on militia forces to â€œstand back and stand byâ€ in September 2020 and later dog-whistling â€œBe there, will be wild!â€ to his supporters. His pardon of all of the Capitol attackersâ€”more than 1,500, including the most violentâ€”only proved what we knew, which is that they had his blessing. While Trump has found state violence adequate to his purposes so far in his second term, street violence is self-evidently in his repertoire.. Since 2016, when he declared that â€œI alone can fix itâ€ and bragged that his supporters would remain loyal if he shot someone on Fifth Avenue, Trump has cultivated a personality cult. Although some of his efforts at self-aggrandizement can seem comical (the gilding of the Oval Office, the renaming of the Kennedy Center, the proposed triumphal arch), he understands the centrality of leader worship in a fascist-style regime. In sharp contradistinction to the American presidential tradition since George Washington, he makes no pretense of serving the people or the Constitution. His mindset, his symbolism, and his rhetoric all underscore the point he made to this month: his own mind and morality are the only limits on his global power. This is Fascism 101.. As Orwell, Hannah Arendt, and practically every other scholar of authoritarianism have emphasized, creating a reality-distortion field is the first thing a fascistic government will do, the better to drive its own twisted narrative, confuse the citizenry, demoralize political opponents, and justify every manner of corruption and abuse. While other presidents (including some good ones) have lied, none have come close to Trumpâ€™s deployment of Russian-style mass disinformation, as I detail in my book The Constitution of Knowledge. From the start of his first term, Trump has made â€œalternative factsâ€ a hallmark of his governing style, issuing lies, exaggerations, and half-truths at a rate of 20 a day. Predictably, his second term has brought more of the same. Following his lead, a MAGA-fied postmodern right gleefully trashes objectivity as elitism and truth as a mask for power.. A distinctive mark of fascism is its conception of politics, best captured by Carl Schmitt, an early-20th-century German political theorist whose doctrines legitimized Nazism. Schmitt rejected the Madisonian view of politics as a social negotiation in which different factions, interests, and ideology come to agreement, the core idea of our Constitution. Rather, he saw politics as a state of war between enemies, neither of which can understand the other and both of which feel existentially threatenedâ€”and only one of which can win. The aim of Schmittian politics is not to share the country but to dominate or destroy the other side. This conception has been evident in MAGA politics since Michael Anton (now a Trump-administration official) published his famous article arguing that the 2016 election was a life-and-death battle to save the country from the left (a â€œFlight 93â€ election: â€œcharge the cockpit or you dieâ€). In the speech given by Stephen Miller at Charlie Kirkâ€™s memorial service, MAGAâ€™s embrace of Schmittian totalism found its apotheosis: â€œWe are the storm. And our enemies cannot comprehend our strength, our determination, our resolve, our passion â€¦ You are nothing. You are wickedness.â€. Although born in revolution, the American liberal tradition, especially its conservative branch, prizes continuity, stability, and incremental change guided by reason. Fascism, by contrast, â€œis not reactionary but revolutionary,â€ as Mussolini insisted. It seeks to uproot and replace the old order and embraces bold, exhilarating action unshackled to rational deliberation. MAGA embraces its own revolutionary ethos, what Russell Vought, the administrationâ€™s Office of Management and Budget director and probably its most formidable intellect, has called â€œradical constitutionalism,â€ a doctrine that would vitiate many checks on presidential power. In pursuit of this vision, Vought told Tucker Carlson in a November 2024 interview, â€œThe president has to move executively as fast and as aggressively as possible, with a radical constitutional perspective, to be able to dismantle that [federal] bureaucracy and their power centersâ€ because â€œthe bureaucracies hate the American people.â€ He predicted, â€œIf you have a radical constitutionalism, itâ€™s going to be destabilizing â€¦ But itâ€™s also exhilarating.â€ He said he would put federal agencies â€œin trauma,â€ an idea echoed by Christopher Rufo, an architect of Trumpâ€™s attack on universities, which Rufo described as a â€œcounterrevolution blueprintâ€ to put universities â€œin an existential terror.â€ As Trump shuttered a congressionally mandated agency, renamed an international body of water, arrested an op-ed writer, deported immigrants to a foreign gulag, terrorized American cities, threatened an ally, and more, he showed how it looks when a radicalized state abandons rational deliberation and goes to war against itself.O that there are elements of classical European fascism that are not found in Trumpism (mass rallies and public rituals, for example)â€”or that there are additional elements of Trumpism that belong on the list (MAGAâ€™s hypermasculinity, misogyny, and co-option of Christianity all resemble fascist patterns). The exercise of comparing fascismâ€™s various forms is not precise. If historians object that Trump is not a copy of Mussolini or Hitler or Franco, the reply is yesâ€”but so what? Trump is building something new on old principles. He is showing us in real time what 21st-century  fascism looks like.If, however, Trump is a fascist , that does not mean that America is a fascist  The courts, the states, and the media remain independent of him, and his efforts to browbeat them will likely fail. He may lose his grip on Congress in November. He has not succeeded in molding public opinion, except against himself. He has outrun the mandate of his voters, his coalition is fracturing, and he has neglected tools that allow presidents to make enduring change. He and his party may defy the Constitution, but they cannot rewrite it, thank goodness.So the United States, once the worldâ€™s exemplary liberal democracy, is now a hybrid state combining a fascist leader and a liberal Constitution; but no, it has not fallen to fascism. And it will not.In which case, is there any point in calling Trump a fascist, even if true? Doesnâ€™t that alienate his voters? Wouldnâ€™t it be better just to describe his actions without labeling him controversially?Until recently, I thought so. No longer. The resemblances are too many and too strong to deny. Americans who support liberal democracy need to recognize what weâ€™re dealing with in order to cope with it, and to recognize something, one must name it. Trump has revealed himself, and we must name what we see.]]></content:encoded></item><item><title>I was right about ATProto key management</title><link>https://notes.nora.codes/atproto-again/</link><author>todsacerdoti</author><category>hn</category><pubDate>Sun, 25 Jan 2026 19:31:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Note: this post has been revised to be split into two sections: a description of what happened, and my analysis. I hope to make it clear that, while I do not like ATProto in general, I am trying to make good-faith critcisms of specific design decisions and outcomes, and in fact, this post getting updoots on HackerNews appears to have gotten the attention of the team, so, mission accomplished. ref, ref My account has since been manually reinstated; this has not happened for any of the other users that have had this issue, as far as I know.Today, I tried setting up an ATProto account for use with Bluesky, with did:web instead of did:plc. Letâ€™s walk through the process:Set up the PDS software on a server I control. Because I use NixOS, this was very easy.Create a did:web. This means creating a public-private keypair; I initially tried following this tutorial from Mai Lapyst, but itâ€™s very out of date, and doesnâ€™t include a critical step.With that did:web, upload the  document to my webserver and set the appropriate DNS entries. Easy enough, except that I also had to set the CORS header for the .Create an account on my new PDS. I was able to get an invite and create an account, but it was in the â€œdeactivatedâ€ status, and I couldnâ€™t activate it. This had to be done by making requests manually with , reading the error outcomes in the PDSâ€™s logs on my server.Seek help in the ATProto Touchers Discord server, and at their advice delete the account.Start over and re-create everything from scratch, correctly replacing the public key in my DID with the public key from getRecommendedDidCredentials.Log into Bluesky (bsky.app) and get a â€œProfile does not exist error.â€It was at this point that I found this GitHub issue, which seems to imply that, since I deleted my (completely empty and unused) account, my did:web is blacklisted from the remaining mostly-centralized bit of the system, the AppView. The term for this is being â€œburnedâ€, and it was later confirmed by some more experienced users that this is a known but undocumented behavior of the Bluesky AppView.I had one of my friends who uses Bluesky take a look, and the failure mode is interesting. On Bluesky, I didnâ€™t exist at all. (She could not see my likes, or my follow of her.) On Blacksky production, my display name and bio were visible, but not my posts. On Blackskyâ€™s own AppView, itâ€™s the opposite; my posts appeared under an â€œinvalidâ€ profile. I have been un-â€œburnedâ€ by a manual process, but not because of my support request; this post made it to the front page of Hacker News, and Bryan Newbold saw it there.So, a while ago, I wrote a post called â€œKey Management, ATProto, and Decentralizationâ€ in which I complained about ATProtoâ€™s approach to decentralization. Since then, Blacksky has spun up an AppView, which makes it theoretically possible to have an actually decentralized experience on Bluesky. This was my line in the sand, stated many times; I would make an account when and only when it was possible to do so without using anything running on Bluesky-the-companyâ€™s hardware. Thatâ€™s now, so I figured Iâ€™d try it.I use lots of systems I donâ€™t love, like Signal, Matrix, and Mastodon. I use them because they give me access to social interaction with people I care about. ATProto, and specifically Bluesky, is the same; I have friends who donâ€™t post anywhere else. Today, I follow them by RSS, but canâ€™t interact with their posts. Thatâ€™s where my motivation to use the network comes from, along with understanding how, and how well, the newly decentralized AppView layer works.Very little of this process is documented. Sure, the individual endpoints are - kind of - but the only place the whole process is collected in one place is in the comments to this GitHub issueâ€¦ which is closed as WONTFIX. The documentation for that getRecommendedDidCredentials endpoint that I missed reads in full:Describe the credentials that should be included in the DID doc of an account that is migrating to this service.Note that I am  â€œmigratingâ€; this account is new. Plus, the JSON keys it returns are almost, but not quite, the same as those in a DID document, and the key it returns actually has to be edited by hand in order to be usable.This is not good!  has been held up as the â€œless centralizedâ€ or â€œbring your own trustâ€ option, as opposed to , and it seems like there has been very little effort to make it usable, certainly not for â€œnormalâ€ users.But thereâ€™s another issue, a bigger issue. Why is a centralized â€œburnâ€ able to completely prevent me from interacting with people using Bluesky?You may be aware that Mastodon has a similar system. If you set up a Mastodon server and then delete the database, anyone youâ€™ve already federated with wonâ€™t federate with you again, because you canâ€™t prove youâ€™re the same instance. Itâ€™s a genuine issue - but it wouldnâ€™t have resulted in this, because I hadnâ€™t even made a post on my now-burned did:web identity, nor followed anyone.Even if I had, though, that would have burned  connection, not  connections. 
My experience would be degraded, but not ruined, and I could work with the admins of the affected servers to remediate it.
You could say the same here, of course; I had to get my account back by Bryan Newbold happening to see this post on Hacker News. There is only, really, one connection that matters; maybe two, if you count Blacksky, but their AppView is not generally available yet. Thatâ€™s centralization. I donâ€™t understand how you could call it anything else.I donâ€™t like Bluesky, or ATProto; I wish we lived in a world were community-driven projects got megabucks and we were all self-hosting little social media servers for our communities. We donâ€™t live in that world, so we have to interoperate with VC-backed, corporate social media. When those platforms call themselves â€œdecentralizedâ€, I think they should deliver.]]></content:encoded></item><item><title>Spanish track was fractured before high-speed train disaster, report finds</title><link>https://www.bbc.com/news/articles/c1m77dmxlvlo</link><author>Rygian</author><category>hn</category><pubDate>Sun, 25 Jan 2026 19:12:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A fracture in a straight section of track "occurred prior to the passage" of a high-speed train that derailed, causing last Sunday's rail disaster in which 45 people died, an initial report has found.A train run by private company Iryo derailed last Sunday and its rear carriages crossed on to the opposite track into the path of an oncoming train run by state-owned Renfe.The CIAF rail investigation commission said not only did Iryo train's front carriages which stayed on the track have "notches" in their wheels, but three earlier trains that went over the track earlier did too.A gap of almost 40cm (15in) in the track has become the focus of the investigation into the crash.Sunday's deadly collision occurred at around 19:45 local time (18:45 GMT), about an hour after the Iryo train left MÃ¡laga for Madrid.The train's last three carriages - carriages six to eight - derailed and collided with the Huelva-bound Renfe train. "Carriage six derailed due to a complete lack of continuity in the track," the preliminary report finds.Most of those killed and injured were in the front carriages of the state-operated train.Earlier this week, Spanish Transport Minister Ã“scar Puente confirmed reports that grooves were found on the wheels of the Iryo train's carriages, which had passed over the track safely."These notches in the wheels and the deformation observed in the track are compatible with the fact that the track was cracked," the CIAF preliminary report said.It added that three trains that had gone over the tracks at 17:21 on Sunday, 19:01 and then 19:09 had similar notches "with a compatible geometric pattern".Similar grooves are found on carriages two, three and four of the Iryo train, the report says, but carriage five  - the last that did not derail -  had a groove on its outer edge, suggesting the rail was already tilting outwards before carriage six derailed.The CIAF called its report a "working hypothesis", adding that it "must be corroborated by later detailed calculations and analysis".The transport minister appeared before reporters again on Friday to say that it was too early to have definitive answers, but that if the cause of the crash was the fracture, then it occurred in the minutes and hours before the derailment and could not have been detected.The Adamuz disaster is is the country's worst rail crash in more than a decade. In 2013, Spain suffered its worst high-speed train derailment in Galicia, north-west Spain, which left 80 people dead and 140 others injured.]]></content:encoded></item><item><title>First, make me care</title><link>https://gwern.net/blog/2026/make-me-care</link><author>andsoitis</author><category>hn</category><pubDate>Sun, 25 Jan 2026 19:03:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ICE using Palantir tool that feeds on Medicaid data</title><link>https://www.eff.org/deeplinks/2026/01/report-ice-using-palantir-tool-feeds-medicaid-data</link><author>JKCalhoun</author><category>hn</category><pubDate>Sun, 25 Jan 2026 17:36:19 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>FAA institutes nationwide drone no-fly zones around ICE operations</title><link>https://www.aerotime.aero/articles/faa-drone-no-fly-zone-ice-dhs</link><author>dayofthedaleks</author><category>hn</category><pubDate>Sun, 25 Jan 2026 17:24:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The Federal Aviation Administration has issued a nationwide security notice, effectively creating a moving drone no-fly zone around operations conducted by Immigration and Customs EnforcementÂ (ICE)Â and other components of the Department of Homeland Security.Â The notice, NOTAM FDC 6/4375, prohibits unmannedÂ aircraftÂ systems fromÂ operatingÂ withinÂ 3,000 feetÂ laterally andÂ 1,000 feetÂ vertically of DHS facilities and mobile assets, including ground vehicle convoys and their escorts. The restriction applies nationwide and continuously, rather than at fixed locations or during defined time windows.Â Because ICEÂ operatesÂ under DHS and routinely conducts enforcement actions using mobile vehicle convoys in public spaces, the restriction functions as a drone no-fly zone around ICE operations, including arrests, transport activities and other field actions.Â The FAA classifies the restricted airspace as â€œnational defense airspace,â€Â and cites its authority under federal security statutes. Drone operators who violate the restriction may face criminal prosecution, civil penalties, administrative enforcement actions, or revocation of FAA operating privileges. The notice alsoÂ statesÂ that dronesÂ deemedÂ a credible security threat may be intercepted, seized, damaged, or destroyed.Â Unlike traditional Temporary Flight Restrictions, the NOTAM does not provide geographic coordinates, activation times, or public notification when the restriction is in effect near a specific location. Instead, the restricted airspace moves with DHS assets, meaning the no-fly zone can appear wherever ICE or other DHS unitsÂ operate.Â The new NOTAM replaces an earlier security notice, FDC 5/6378, which covered similar federal agencies but was less explicit about mobile operations. The updated version removes ambiguity by clearlyÂ statingÂ that the restriction applies to moving DHS assets, including vehicles and convoys, and not just fixed facilities such as offices or bases.Â That clarification has drawn attention from drone operatorsÂ and civil liberties groups because it creates dynamic, invisible exclusion zones that may beÂ impossibleÂ toÂ identifyÂ in real time. The FAA does not publish public tracking of DHS or ICE movements, and the NOTAM does not include a mechanism for drone pilots toÂ determineÂ when covered assets are nearby.Â In practical terms, a drone operator flying legally in a public area could unknowingly enter restricted airspace if an ICE convoy passes within the protected radius. The FAA instructs operators to â€œexercise cautionâ€ when flying near DHS facilities and mobile assets, but offers no specific guidance on how to do so in environments where enforcement activity is not publiclyÂ disclosed.Â The notice mentions limited exceptions. Drone operations conducted in direct support of national defense, homeland security, law enforcement, firefighting, search and rescue, or disaster response missions may be authorized with advance coordination. Operators seeking approval are instructed to coordinate with DHS or other coveredÂ agencies, orÂ contact the FAAâ€™s System Operations Support Center.Â The FAA cites multiple federal statutes as the legal basis for the restriction, including laws governing nationalÂ defenseÂ airspace and counter-UAS mitigation.]]></content:encoded></item><item><title>White House alters arrest photo of ICE protester, says &quot;the memes will continue&quot;</title><link>https://arstechnica.com/tech-policy/2026/01/white-house-posts-altered-arrest-photo-to-make-it-appear-ice-critic-was-sobbing/</link><author>wmeredith</author><category>hn</category><pubDate>Sun, 25 Jan 2026 16:58:19 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Using PostgreSQL as a Dead Letter Queue for Event-Driven Systems</title><link>https://www.diljitpr.net/blog-post-postgresql-dlq</link><author>tanelpoder</author><category>hn</category><pubDate>Sun, 25 Jan 2026 15:51:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
                        While I was working on a project with Wayfair, I got the opportunity to work on a system that generated daily business reports aggregated from multiple data sources flowing through event streams across Wayfair. At a high level, Kafka consumers listened to these events, hydrated them with additional data by calling downstream services, and finally persisted the enriched events into a durable datastoreâ€”CloudSQL PostgreSQL on GCP.
                    
                        When everything was healthy, the pipeline worked exactly as expected. Events flowed in, got enriched, and were stored reliably. The real challenge started when things went wrong, which, in distributed systems, is not an exception but a certainty.
                    
                        There were multiple failure scenarios we had to deal with. Sometimes the APIs we depended on for hydration were down or slow. Sometimes the consumer itself crashed midway through processing. In other cases, events arrived with missing or malformed fields that could not be processed safely. These were all situations outside our direct control, but they still needed to be handled gracefully.
                    
                        This is where the concept of a Dead Letter Queue came into the picture. Whenever we knew an event could not be processed successfully, instead of dropping it or blocking the entire consumer, we redirected it to a DLQ so it could be inspected and potentially reprocessed later.
                    
                        Our first instinct was to use Kafka itself as a DLQ. While this is a common pattern, it quickly became clear that it wasn't a great fit for our needs. Kafka is excellent for moving data, but once messages land in a DLQ topic, they are not particularly easy to inspect. Querying by failure reason, retrying a specific subset of events, or even answering simple questions like "what failed yesterday and why?" required extra tooling and custom consumers. For a system that powered business-critical daily reports, this lack of visibility was a serious drawback.
                    
                        That's when we decided to treat PostgreSQL itself as the Dead Letter Queue.
                    
                        Instead of publishing failed events to another Kafka topic, we persisted them directly into a DLQ table in PostgreSQL. We were already using CloudSQL as our durable store, so operationally this added very little complexity. Conceptually, it also made failures first-class citizens in the system rather than opaque messages lost in a stream.
                    
                        Whenever an event failed processingâ€”due to an API failure, consumer crash, schema mismatch, or validation errorâ€”we stored the raw event payload along with contextual information about the failure. Each record carried a simple status field. When the event first landed in the DLQ, it was marked as . Once it was successfully reprocessed, the status was updated to . Keeping the state model intentionally minimal made it easy to reason about the lifecycle of a failed event.
                    DLQ Table Schema and Indexing Strategy
                        To support inspection, retries, and long-term operability, the DLQ table was designed to be simple, query-friendly, and retry-aware.
                    CREATE TABLE dlq_events (
    id BIGSERIAL PRIMARY KEY,
    event_type VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,
    error_reason TEXT NOT NULL,
    error_stacktrace TEXT,
    status VARCHAR(20) NOT NULL, -- PENDING / SUCCEEDED
    retry_count INT NOT NULL DEFAULT 0,
    retry_after TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);Key Design Considerations is stored as  to preserve the raw event without enforcing a rigid schema. keeps the lifecycle simple and explicit. prevents aggressive retries when downstream systems are unstable. allows retry limits to be enforced without external state.Timestamps make auditing and operational analysis straightforward.CREATE INDEX idx_dlq_status
ON dlq_events (status);

CREATE INDEX idx_dlq_status_retry_after
ON dlq_events (status, retry_after);

CREATE INDEX idx_dlq_event_type
ON dlq_events (event_type);

CREATE INDEX idx_dlq_created_at
ON dlq_events (created_at);
                        These indexes allow the retry scheduler to efficiently locate eligible events while still supporting fast debugging and time-based analysis without full table scans.
                    DLQ Retry Mechanism with ShedLock
                        Persisting failed events solved the visibility problem, but we still needed a safe and reliable way to retry them.
                    
                        For this, we introduced a DLQ retry scheduler backed by ShedLock. The scheduler periodically scans the DLQ table for  events that are eligible for retry and attempts to process them again. Since the service runs on multiple instances, ShedLock ensures that only one instance executes the retry job at any given time. This eliminates duplicate retries without requiring custom leader-election logic.
                    dlq:
  retry:
    enabled: true
    max-retries: 240
    batch-size: 50
    fixed-rate: 21600000 # 6 hours in millisecondsThe scheduler runs every six hours.Up to fifty eligible events are picked up per run.Events exceeding the maximum retry count are skipped.Successful retries immediately transition the event status to .Failures remain in  and are retried in subsequent runs.
                        The retry scheduler uses a SQL query with  to safely select eligible events across multiple instances. This PostgreSQL feature ensures that even if multiple scheduler instances run simultaneously, each will pick up different rows without blocking each other:
                    @QueryHints(@QueryHint(name = "jakarta.persistence.lock.timeout", value = "-2"))
@Query(
    value = "SELECT * FROM dlq_table "
        + "WHERE messagetype = :messageType "
        + "AND retries < :maxRetries "
        + "AND (replay_status IS NULL OR replay_status NOT IN ('COMPLETED')) "
        + "ORDER BY created_at ASC "
        + "FOR UPDATE SKIP LOCKED",
    nativeQuery = true
)
                        The  clause is crucial here. It allows each instance to lock and process different rows concurrently, preventing duplicate processing while maintaining high throughput. The query hint sets the lock timeout to , which means "wait indefinitely" but combined with , it effectively means "skip any rows that are already locked by another transaction."
                    
                        This setup allowed the system to tolerate long downstream outages while avoiding retry storms and unnecessary load on dependent services.
                    
                        With this approach, failures became predictable and observable rather than disruptive. Engineers could inspect failures using plain SQL, identify patterns, and reprocess only the events that mattered. If a downstream dependency was unavailable for hours or even days, events safely accumulated in the DLQ and were retried later without human intervention. If an event was fundamentally bad, it stayed visible instead of being silently dropped.
                    
                        Most importantly, this design reduced operational stress. Failures were no longer something to fear; they were an expected part of the system with a clear, auditable recovery path.
                    
                        The goal was never to replace Kafka with PostgreSQL. Kafka remained the backbone for high-throughput event ingestion, while PostgreSQL handled what it does bestâ€”durability, querying, and observability around failures. By letting each system play to its strengths, we ended up with a pipeline that was resilient, debuggable, and easy to operate.
                    
                        In the end, using PostgreSQL as a Dead Letter Queue turned failure handling into something boring and predictable. And in production systems, boring is exactly what you want.
                    ]]></content:encoded></item><item><title>A macOS app that blurs your screen when you slouch</title><link>https://github.com/tldev/posturr</link><author>dnw</author><category>hn</category><pubDate>Sun, 25 Jan 2026 15:34:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Wine-Staging 11.1 Adds Patches for Enabling Recent Photoshop Versions on Linux</title><link>https://www.phoronix.com/news/Wine-Staging-11.1</link><author>LorenDB</author><category>hn</category><pubDate>Sun, 25 Jan 2026 14:42:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>Iran Protest Death Toll Could Top 30k, According to Local Health Officials</title><link>https://time.com/7357635/more-than-30000-killed-in-iran-say-senior-officials/</link><author>mhb</author><category>hn</category><pubDate>Sun, 25 Jan 2026 13:59:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[As many as 30,000 people could have been killed in the streets of Iran on Jan. 8 and 9 alone, two senior officials of the countryâ€™s Ministry of Health told TIMEâ€”indicating a dramatic surge in the death toll. So many people were slaughtered by Iranian security services on that Thursday and Friday, it overwhelmed the stateâ€™s capacity to dispose of the dead. Stocks of body bags were exhausted, the officials said, and eighteen-wheel semi-trailers replaced ambulances.The governmentâ€™s internal count of the dead, not previously revealed, far surpasses the toll of 3,117 announced on Jan. 21 by regime hardliners who report directly to Iranâ€™s Supreme Leader Ali Khamenei. (Ministries report to the elected President.) The 30,000 figure is also far beyond tallies being compiled by activists methodically assigning names to the dead. As of Saturday, the U.S.-based Human Rights Activists News Agency said it had confirmed 5,459 deaths and is investigating 17,031 more.TIME has been unable to independently verify these figures.The Health Ministryâ€™s two-day figure roughly aligns with a count gathered by physicians and first responders, and also shared with TIME. That surreptitious tally of deaths recorded by hospitals stood at 30,304 as of Friday, according to Dr. Amir Parasta, a German-Iranian eye surgeon who prepared a report of the data. Parasta said that number does not reflect protest-related deaths of people registered at military hospitals, whose bodies were taken directly to morgues, or that happened in locales the inquiry did not reach. Iranâ€™s National Security Council has said protests took place in around 4,000 locations across the country.  â€œWe are getting closer to reality,â€ Dr. Parasta said. â€œBut I guess the real figures are still way higher.â€That appears to be the reality implicit in the governmentâ€™s internal figure of more than 30,000 deaths in two days. A slaughter on that scale, in the space of 48 hours, had experts on mass killing groping for comparisons.â€œMost spasms of killing are not from shootings,â€ said Les Roberts, a professor at Columbia University who specializes in the epidemiology of violent death. â€œIn Aleppo [Syria] and in Fallujah [Iraq], when spasms of death this high have occurred over a few days, it involved mostly explosives with some shooting.â€The only parallel offered by online databases occurred in the Holocaust. On the outskirts of Kyiv on Sept. 29 and 30, 1941, Nazi death squads executed 33,000 Ukrainian Jews by gunshot in a ravine known as Babyn Yar.In Iran, the killing fields extended across the country where, since Dec. 28, hundreds of thousands of citizens had assembled in the streets chanting first, for relief from an economy in freefall, and soon for the downfall of the Islamic regime. During the first week, security forces confronted some demonstrations, using mostly non-lethal force, but with officials also offering conciliatory language, the regime response was uncertain. That changed during the weekend commencing Jan. 8. Protests peaked, as opposition groups, includingReza Pahlavi, the exiled son of Iranâ€™s former shah, urged people to join the throngs, and U.S. President Donald Trump repeated vows to protect them, though no help arrived.Witnesses say millions were in the streets when authorities shut down the internet and all other communications with the outside world. Rooftop snipers and trucks mounted with heavy machine guns opened fire, according to eyewitnesses and cell phone footage. On Friday, Jan. 9, an official of the Islamic Revolutionary Guard Corps warned on state television to anyone venturing into the streets, â€œif â€¦ a bullet hits you, donâ€™t complain.â€It took days for the reality to penetrate the internet blackout. Images of the bloodied bodies trickled out via illicit Starlink satellite internet connections. The task of counting the dead was hampered, however, because the authorities had also cut off lines of communications inside Iran. The first firm information came from a Tehran doctor who told TIME that just six hospitals in the capital had recorded at least 217 protester deaths after Thursdayâ€™s assault. Health care workers in Iran estimated at least 16,500 protesters had been killed by Jan. 10, according to an earlier report by Dr. Parasta in Munich. Fridayâ€™s update built on that research, he said.â€œI am genuinely impressed by how quickly this work was pulled together under extremely constrained and risky conditions,â€ said Paul B. Spiegel, a professor at the Johns Hopkins University International School of Health. Like Roberts, he expressed wariness of extrapolating from the figures provided by hospitals.Â Roberts, who traveled into war zones to research civilian death rates in Iraq and the Democratic Republic of Congo, said, â€œthe 30,000 verified deaths are almost certainly an underestimate.â€The emergence of the Ministry of Health numbers appears to confirm thatâ€”while underscoring the stakes for both Iranians and a regime that, in 1979, came to power when a sitting government was confronted by millions of people demanding its downfall.On Friday, Jan. 9, Sahba Rashtian, an aspiring animation artist, joined friends on the streets in Isfahan, a city in central Iran famous for its beauty. "Before anyone started chanting," a friend told TIME, "Sahba was seen collapsed on the ground. Her sister noticed blood on her hand.â€Sahba died on an operating table at a nearby hospital. She was 23.â€œShe always joked about her beautiful name,â€ her friend said. â€œSheâ€™d laugh and say, â€˜Sahba means wine, and I am forbidden in the Islamic Republic.â€™â€At the burial, the friend said, religious rites were barred, and Rashtianâ€™s father wore white.Â â€œCongratulations,â€ he told mourners, according to the friend. â€œMy daughter became a martyr on the path to freedom.â€]]></content:encoded></item><item><title>Web-based image editor modeled after Deluxe Paint</title><link>https://github.com/steffest/DPaint-js</link><author>bananaboy</author><category>hn</category><pubDate>Sun, 25 Jan 2026 12:54:53 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Alarm overload is undermining safety at sea as crews face thousands of alerts</title><link>https://www.lr.org/en/knowledge/press-room/press-listing/press-release/2026/alarm-overload-is-undermining-safety-at-sea-as-new-research-shows-crews-face-tens-of-thousands-of-daily-alerts/</link><author>geox</author><category>hn</category><pubDate>Sun, 25 Jan 2026 12:40:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[New research fromÂ Lloydâ€™s Register (LR)Â hasÂ revealed thatÂ excessive and nuisanceÂ shipboard alarm systems are routinely overwhelming crews and, in many cases, actively undermining safety at sea.]]></content:encoded></item><item><title>Doom has been ported to an earbud</title><link>https://doombuds.com/</link><author>arin-s</author><category>hn</category><pubDate>Sun, 25 Jan 2026 12:22:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
            Serial Connection
            
              Earbuds don't have displays, so the only way to transfer data to/from them is either via bluetooth, or the UART contact pads.
              Bluetooth is pretty slow, you'd be lucky to get a consistent 1mbps connection, UART is easily the better option.
              DOOM's framebuffer is (width * height) bytes, 320 * 200 = 96kB. (doom's internal framebuffer is 8-bit not 24-bit)
              The UART connection provides us with 2.4mbps of usable bandwidth. 2,400,000 / 8 / 96,000 gives us... 3 frames per second.
              Clearly we need to compress the video stream. Modern video codecs like h264 consume way too much CPU and RAM.
              The only feasible approach is sending the video as an MJPEG stream. MJPEG is a stream of JPEG images shown one after the other.
              I found an excellent JPEG encoder for embedded devices here, thanks Larry!
              A conservative estimate for the average HIGH quality JPEG frame is around 13.5KB, but most scenes (without enemies) are around 11kb.
              - Optimistic:     `2,400,000 / (11,000 * 8)` = 27.3 FPS
              - Conservative: `2,400,000 / (13,500 * 8)` = 22.2 FPS
            
            CPU
            
              The stock open source firmware has the CPU set to 100mhz, so I cranked that up to 300mhz and disabled low power mode.
              The Cortex-M4F running at 300mhz is actually more than enough for DOOM, however it struggles with JPEG encoding.
              This is why it maxes out at ~18fps, I don't think there's much else I can do to speed it up.
            
            RAM
            
              By default, we only have access to 768KB of RAM, after disabling the co-processor it gets bumped up to the advertised 992KB.
              DOOM requires 4MB of RAM, though there are plenty of optimisations that can reduce this amount.
              Pre-generating lookup tables, making variables const, reading const variables from flash, disabling DOOM's caching system, removing unneeded variables. It all adds up!
            
            FLASH
            
              The shareware DOOM 1 wad (assets file) is 4.2MB and the earbuds can only store 4MB of data.
              Thankfully, fragglet, a well-known doom modder, has already solved this issue for me.Squashware is his trimmed-down DOOM 1 wad that is only 1.7MB in size.
              With this wad file, everything comfortably fits in flash.
            ]]></content:encoded></item><item><title>Show HN: Bonsplit â€“ Tabs and splits for native macOS apps</title><link>https://bonsplit.alasdairmonk.com/</link><author>sgottit</author><category>hn</category><pubDate>Sun, 25 Jan 2026 11:56:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: TUI for managing XDG default applications</title><link>https://github.com/mitjafelicijan/xdgctl</link><author>mitjafelicijan</author><category>hn</category><pubDate>Sun, 25 Jan 2026 11:19:04 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Author here. I made this little TUI program for managing default applications on the Linux desktop.Maybe some of you will find it useful.Happy to answer any questions.]]></content:encoded></item><item><title>Jurassic Park - Tablet device on Nedry&apos;s desk? (2012)</title><link>https://www.therpf.com/forums/threads/jurassic-park-tablet-device-on-nedrys-desk.169883/</link><author>exvi</author><category>hn</category><pubDate>Sun, 25 Jan 2026 09:22:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A flawed paper in management science has been cited more than 6k times</title><link>https://statmodeling.stat.columbia.edu/2026/01/22/aking/</link><author>timr</author><category>hn</category><pubDate>Sun, 25 Jan 2026 09:04:30 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Deutsche Telekom is throttling the internet</title><link>https://netzbremse.de/en/</link><author>tietjens</author><category>hn</category><pubDate>Sun, 25 Jan 2026 08:22:17 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[die Telekom nimmt nicht am Peering mit dem Deutschen Forschungsnetz teil und deshalb ist die Verbindung fÃ¼r Telekom-Kunden zu deutschen UniversitÃ¤ten eine Katastrophe. Bei mir sind es gerade Ã¼ber Telia 29 kB/s bei 20% Paketverlust! â€¦ Ich hoffe, dass heute jedem die gesellschaftlichen Konsequenzen klar werden. Jetzt wo das Internet so wahnsinnig wichtig ist uns zu verbinden. Stattdessen werden Studenten, Personal in der Lehre und Forschende massiv daran gehindert mit ihrer Hochschule in Kontakt zu bleiben, weiter zu lernen, zu lehren und zu forschen. In den Medien wird diskutiert Netflix zu drosseln damit das Internet wichtigere Sachen machen kann, aber keine Sorge, zu Netflix sagt mein Speedtest 81 Mb/s - mehr als das hundertfache als zu meiner Uni. Ich kann parallel drei 4k Videos schauen, aber das Aufrufen der Corona-Info-Seite meiner Uni wird zur Qual. An produktives Arbeiten nachdem ich meine Tochter ins Bett gebracht habe ist nicht zu denken.]]></content:encoded></item><item><title>Introduction to PostgreSQL Indexes</title><link>https://dlt.github.io/blog/posts/introduction-to-postgresql-indexes/</link><author>dlt</author><category>hn</category><pubDate>Sun, 25 Jan 2026 08:07:03 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: AutoShorts â€“ Local, GPU-accelerated AI video pipeline for creators</title><link>https://github.com/divyaprakash0426/autoshorts</link><author>divyaprakash</author><category>hn</category><pubDate>Sun, 25 Jan 2026 07:36:20 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Palantir has no place in UK public services</title><link>https://www.opendemocracy.net/en/zarah-sutlana-palantir-no-place-uk-public-services-ministry-of-defence/</link><author>jethronethro</author><category>hn</category><pubDate>Sun, 25 Jan 2026 04:53:29 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Nvidia-smi hangs indefinitely after ~66 days</title><link>https://github.com/NVIDIA/open-gpu-kernel-modules/issues/971</link><author>tosh</author><category>hn</category><pubDate>Sun, 25 Jan 2026 03:33:20 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Second Win11 emergency out of band update to address disastrous Patch Tuesday</title><link>https://www.windowscentral.com/microsoft/windows-11/windows-11-second-emergency-out-of-band-update-kb5078127-released-address-outlook-bugs</link><author>speckx</author><category>hn</category><pubDate>Sun, 25 Jan 2026 03:17:43 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Microsoft's January 2026 Patch Tuesday updates for Windows 11 have been a total disaster, seemingly introducing more problems than it fixed. The botched update has already forced the company to issue one emergency out of band update to fix two major issues, and now a second emergency out of band update has been released to address another critical problem."An out-of-band (OOB) update was released today, January 24, 2026, to address this issue," Microsoft says in newly published documentation. "This cumulative update includes all protections and improvements from the January 2026 Windows security update released January 13, 2026, as well as from the OOB update released on January 17, 2026 (which introduced fixes for two known issues: remote desktop connectionsHere's the official changelog for the KB5078127 update from Microsoft:Fixed: After installing the Windows update released on and after January 13, 2026, some applications became unresponsive or encountered unexpected errors when opening files from or saving files to cloud-based storage, such as OneDrive or Dropbox. In certain Outlook configurations that store PST files on OneDrive, Outlook may hang and fail to reopen unless the process is terminated or the system is restarted. Users may also see missing sent Items or previously downloaded emails being reâ€‘downloaded.This same fix is being rolled out to various other versions and editions of Windows too, including Windows 11 version 23H2, Windows Server editions, and more. Be sure to check out the Windows release health dashboard for the latest information impacting your version of Windows.This is the second emergency out of band update that Microsoft has pushed out for Windows 11 users in a week. The first arrived on January 17, and fixed two major issues that began appearing for users after installing this month's Patch Tuesday updates released on January 13.The original Patch Tuesday release introduced an issue that caused PCs running version 23H2 to fail to shutdown or hibernate, and also broke signing into Windows 11 PCs using Remote Desktop. Microsoft addressed these issues in the out of band update issued on January 17, but in the process broke apps like Outlook, OneDrive, and Dropbox.It's clear that the quality bar for Windows is at an all time low right now, marking an absolutely dreadful start to the year for Microsoft and Windows. It feels like there's nowhere to go but up at this point, so hopefully things improve with the next Patch Tuesday release.]]></content:encoded></item><item><title>Challenges and Research Directions for Large Language Model Inference Hardware</title><link>https://arxiv.org/abs/2601.05047</link><author>transpute</author><category>hn</category><pubDate>Sun, 25 Jan 2026 02:48:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Two Weeks Until Tapeout</title><link>https://essenceia.github.io/projects/two_weeks_until_tapeout/</link><author>client4</author><category>hn</category><pubDate>Sun, 25 Jan 2026 01:25:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: C From Scratch â€“ Learn safety-critical C with prove-first methodology</title><link>https://github.com/SpeyTech/c-from-scratch</link><author>william1872</author><category>hn</category><pubDate>Sun, 25 Jan 2026 00:17:07 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Seven modules teaching C the way safety-critical systems are actually built: MATH â†’ STRUCT â†’ CODE â†’ TEST.Each module answers one question: Does it exist? (Pulse), Is it normal? (Baseline), Is it regular? (Timing), Is it trending? (Drift), Which sensor to trust? (Consensus), How to handle overflow? (Pressure), What do we do about it? (Mode).Every module is closed (no dependencies), total (handles all inputs), deterministic, and O(1). 83 tests passing.Built this after 30 years in UNIX systems. Wanted something that teaches the rigour behind certified systems without requiring a decade of on-the-job learning first.MIT licensed. Feedback welcome.]]></content:encoded></item><item><title>Adoption of EVs tied to real-world reductions in air pollution: study</title><link>https://keck.usc.edu/news/adoption-of-electric-vehicles-tied-to-real-world-reductions-in-air-pollution-study-finds/</link><author>hhs</author><category>hn</category><pubDate>Sun, 25 Jan 2026 00:14:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>We X-Rayed a Suspicious FTDI USB Cable</title><link>https://eclypsium.com/blog/xray-counterfeit-usb-cable/</link><author>aa_is_op</author><category>hn</category><pubDate>Sat, 24 Jan 2026 23:55:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Europe wants to end its dangerous reliance on US internet technology</title><link>https://theconversation.com/europe-wants-to-end-its-dangerous-reliance-on-us-internet-technology-274042</link><author>DyslexicAtheist</author><category>hn</category><pubDate>Sat, 24 Jan 2026 23:21:37 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Imagine the internet suddenly stops working. Payment systems in your local food store go down. Healthcare systems in the regional hospital flatline. Your work software tools, and all the information they contain, disappear.You reach out for information but struggle to communicate with family and friends, or to get the latest updates on what is happening, as social media platforms are all down. Just as someone can pull the plug on your computer, itâ€™s possible to shut down the system it connects to. This isnâ€™t an outlandish scenario. Technical failures, cyber-attacks and natural disasters can all bring down key parts of the internet. And as the US government makes increasing demands of European leaders, it is possible to imagine Europe losing access to the digital infrastructure provided by US firms as part of the geopolitical bargaining process. At the World Economic Forum in Davos, Switzerland, the EUâ€™s president, Ursula von der Leyen, has highlighted the â€œstructural imperativeâ€  for Europe to â€œbuild a new form of independenceâ€ â€“ including in its technological capacity and security. And, in fact, moves are already being made across the continent to start regaining some independence from US technology.A small number of US-headquartered big tech companies now control a large proportion of the worldâ€™s cloud computing infrastructure, that is the global network of remote servers that store, manage and process all our apps and data. Amazon Web Services (AWS), Microsoft Azure and Google Cloud are reported to hold about 70% of the European market, while European cloud providers have only 15%.My research supports the idea that relying on a few global providers increases vulnerabilty for Europeâ€™s private and public sectors â€“ including the risk of cloud computing disruption, whether caused by technical issues, geopolitical disputes or malicious activity.Two recent examples â€“ both the result of apparent technical failures â€“ were the hoursâ€‘long AWS incident in October 2025, which disrupted thousands of services such as banking apps across the world, and the major Cloudflare incident two months later, which took LinkedIn, Zoom and other communication platforms offline. The impact of a major power disruption on cloud computing services was also demonstrated when Spain, Portugal and some of south-west France endured a massive power cut in April 2025.What happens in a digital blackout?There are signs that Europe is starting to take the need for greater digital independence more seriously. In the Swedish coastal city of Helsingborg, for example, a one-year project is testing how various public services would function in the scenario of a digital blackout. Would elderly people still receive their medical prescriptions? Can social services continue to provide care and benefits to all the cityâ€™s residents?This pioneering project seeks to quantify the full range of human, technical and legal challenges that a collapse of technical services would create, and to understand what level of risk is acceptable in each sector. The aim is to build a model of crisis preparedness that can be shared with other municipalities and regions later this year.Elsewhere in Europe, other forerunners are taking action to strengthen their digital sovereignty by weaning themselves off reliance on global big tech companies â€“ in part through collaboration and adoption of open source software. This technology is treated as a digital public good that can be moved between different clouds and operated under sovereign conditions.In northern Germany, the state of Schleswig-Holstein has made perhaps the clearest break with digital dependency. The state government has replaced most of its Microsoft-powered computer systems with open-source alternatives, cancelling nearly 70% of its licenses. Its target is to use big tech services only in exceptional cases by the end of the decade.Across France, Germany, the Netherlands and Italy, governments are investing both nationally and transnationally in the development of digital open-source platforms and tools for chat, video and document management â€“ akin to digital Lego bricks that administrations can host on their own terms.In Sweden, a similar system for chat, video and online collaboration, developed by the National Insurance Agency, runs in domestic data centres rather than foreign clouds. It is being offered as a service for Swedish public authorities looking for sovereign digital alternatives.For Europe â€“ and any nation â€“ to meaningfully address the risks posed by digital blackout and cloud collapse, digital infrastructure needs to be treated with the same seriousness as physical infrastructure such as ports, roads and power grids.Control, maintenance and crisis preparedness of digital infrastructure should be seen as core public responsibilities, rather than something to be outsourced to global big tech firms, open for foreign influence.To encourage greater focus on digital resilience among its member states, the EU has developed a cloud sovereignty framework to guide procurement of cloud services â€“ with the intention of keeping European data under European control. The upcoming Cloud and AI Development Act is expected to bring more focus and resources to this area.Governments and private companies should be encouraged to demand security, openness and interoperability when seeking bids for provision of their cloud services â€“ not merely low prices. But in the same way, as individuals, we can all make a difference with the choices we make.Just as itâ€™s advisable to ensure your own access to food, water and medicine in a time of crisis, be mindful of what services you use personally and professionally. Consider where your emails, personal photos and conversations are stored. Who can access and use your data, and under what conditions? How easily can everything be backed up, retrieved and transferred to another service?No country, let alone continent, will ever be completely digitally independent, and nor should they be. But by pulling together, Europe can ensure its digital systems remain accessible even in a crisis â€“ just as is expected from its physical infrastructure.]]></content:encoded></item><item><title>Poland&apos;s energy grid was targeted by never-before-seen wiper malware</title><link>https://arstechnica.com/security/2026/01/wiper-malware-targeted-poland-energy-grid-but-failed-to-knock-out-electricity/</link><author>Bender</author><category>hn</category><pubDate>Sat, 24 Jan 2026 21:24:13 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I added a Bluesky comment section to my blog</title><link>https://micahcantor.com/blog/bluesky-comment-section.html</link><author>hydroxideOH-</author><category>hn</category><pubDate>Sat, 24 Jan 2026 20:33:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[You can now view replies to this blog post made on Bluesky directly on this website. Check it out here!I've always wanted to host a comment section on my site, but it's difficult because the content is statically generated and hosted on a CDN.
I could host comments on a separate VPS or cloud service.
But maintaining a dynamic web service like this can be expensive and time-consuming â€” in general, I'm not interested in being an unpaid, part-time DevOps engineer.Recently, however, I read a blog post by Cory Zue about how he embedded a comment section from Bluesky on his blog.
I immediately understood to benefits of this approach. With this approach, Bluesky could handle all of the difficult work involved in managing a social media like account verification, hosting, storage, spam, and moderation. Meanwhile because Bluesky is an open platform with a public API, it's easy to directly embed comments on my own site.There are other services that could be used for this purpose instead. Notably, I could embed replies from the social media formerly known as Twitter. Or I could use a platform like Disqus or even giscus, which hosts comments on GitHub Discussions. But I see Bluesky as a clearly superior choice among these options. For one, Bluesky is built on top of an open social media platform in AT Proto, meaning it can't easily be taken over by an authoritarian billionaire creep. Moreover, Bluesky is a full-fledged social media platform, which naturally makes it a better option for hosting a conversation than GitHub.Zue published a standalone package called bluesky-comments that allows embedding comments in a React component as he did.
But I decided to build this feature myself instead. Mainly this is because I wanted to make a few styling changes anyway to match the rest of my site. But I also wanted to leave the option open to adding more features in the future, which would be easier to do if I wrote the code myself. The entire implementation is small regardless, amounting to only ~200 LOC between the UI components and API functions.Initially, I planned to allow people to directly post on Bluesky via my site. This would work by providing an OAuth flow that gives my site permission to post on Bluesky on behalf of the user. I actually did get the auth flow working, but building out a UI for posting and replying to existing comments is difficult to do well. Going down this path quickly leads to building what is essentially a custom Bluesky client, which I didn't have the time or interest in doing right now. Moreover, because the user needs to go through the auth flow and sign-in to their Bluesky account, the process is not really much easier than posting directly on a linked Bluesky post.Without the requirement of allowing others to directly post on my site, the implementation became much simpler. Essentially, my task was to specify a Bluesky post that corresponds to the article in the site's metadata. Then, when the page loads I fetch the replies to that post from Bluesky, parse the response, and display the results in a simple comment section UI.As explained in my last post, this site is built using React Server Components and Parcel. The content of my articles are written using MDX, an extension to Markdown that allows directly embedding JavaScript and JSX. In each post, I export a  object that I validate using a Zod schema. For instance, the metadata for this post looks like this:The value of  references the Bluesky post from which I'll pull replies to display in the comment section. Because my project is built in TypeScript, it was easy to integrate with the Bluesky TypeScript SDK ( on NPM). Reading the Bluesky API documentation and Zue's implementation led me to the  endpoint. Given an AT Protocol URI, this endpoint returns an object with data on the given post and its replies.I could have interacted directly with the Bluesky API from my React component using  and . However, it can be a bit tricky to correctly handle loading and a error states, even for a simple feature like this. Because of this, I decided to use the Tanstack  package to manage the API request/response cycle. This library takes care of the messy work of handling errors, retries, and loading states while I simply provide it a function to fetch the post data.Once I obtain the Bluesky response, the next task is parsing out the content and metadata for the replies. Bluesky supports a rich content structure in its posts for representing markup, references, and attachments. Building out a UI that fully respects this rich content would be difficult. Instead, I decided to keep it simple by just pulling out the text content from each reply.Even so, building a UI that properly displays threaded comments, particularly one that is formatted well on small mobile devices, can be tricky. For now, my approach was to again keep it simple. I indented each reply and added a left border to make it easier to follow reply threads. Otherwise, I mostly copied design elements for layout of the profile picture and post date from Bluesky.Lastly, I added a UI component linking to the parent post on Bluesky, and encouraging people to add to the conversation there. With this, the read-only comment section implementation was complete. If there's interest, I could publish my version of Bluesky comments as a standalone package. But several of the choices I made were relatively specific to my own site. Moreover, the implementation is simple enough that others could probably build their own version from reading the source code, just as I did using Zue's version.Let me know what you think by replying on Bluesky. Hopefully this can help increase engagement with my blog posts, but then again, my last article generated no replies, so maybe not ðŸ˜­.Thanks everyone for all the positive feedback! I made a few improvements last night and today. I added control button that allows you to sort comments by top, latest, or oldest. I also included the like count beneath each comments, and made a few small styling improvements.A few people have asked about adding this to their own site, or about other implementation details. For that, feel free to check out the source code on GitHub (and in particular  and ).And as I mentioned earlier, I'm certainly not the first person to have thought of this idea or implemented it. In particular, I like the implementations of the same feature by Natalie B. and by Triple Pat.]]></content:encoded></item><item><title>Postmortem: Our first VLEO satellite mission (with imagery and flight data)</title><link>https://albedo.com/post/clarity-1-what-worked-and-where-we-go-next</link><author>topherhaddad</author><category>hn</category><pubDate>Sat, 24 Jan 2026 20:03:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[On March 14, 2025, Albedo's first satellite, Clarity-1, launched on SpaceX Transporter-13. We took a big swing with our pathfinder. The mission goals:Prove sustainable orbit operations in VLEO â€” an orbital regime long considered too harsh for commercial satellites â€” by overcoming thick atmospheric drag, dangerous atomic oxygen, and extreme speeds.Prove our mid-size, high-performance Precision bus â€” designed and built in-house in just over two years.Capture 10 cm resolution visible imagery and 2-meter thermal infrared imagery, a feat previously achieved only by exquisite, billion dollar government systems.We proved a ton. We learned a ton.We achieved the first two goals definitively and validated 98% of the technology required for the third. This was an extraordinarily ambitious first satellite. We designed and built a high-performance bus on time and on budget, integrated a large-aperture telescope, and operated in an environment no commercial company had sustained operations in, funded entirely by private capital.Let's start with the result that matters most: VLEO works. And it works better than even we expected.For decades, Very Low Earth Orbit was written off as impractical for normal satellite lifetimes. The atmosphere is thicker, creating drag that would deorbit normal satellites in weeks. If the drag didn't kill you, atomic oxygen would erode your solar arrays and surfaces. To succeed in VLEO required a fundamentally different satellite design.Clarity-1 proved that our design works.The drag coefficient was the headline: 12% better than our design target. Measured multiple times at altitudes between 350 km - 380 km with a repeatable result, this validates our models producing a satellite lifespan of five years at 275 km altitude, averaged across the solar cycle. This was one of our most critical assumptions, and we exceeded it.Atomic oxygen (AO) is the silent killer in VLEO. The deeper you go, the more AO you encounter. It degrades solar arrays and other traditional satellite materials. We developed a new class of solar arrays with unique measures designed to mitigate AO degradation. They work. Even as we descended deeper into VLEO and AO fluence increased logarithmically, our power generation stayed constant. The solar arrays are holding up as designed.Clarity-1 demonstrated over 100 km of controlled altitude descent, stationkeeping in VLEO, and survived a solar storm that temporarily spiked atmospheric density â€” the impact on Clarity's descent rate was barely noticeable. Momentum management worked. Fault detection worked. Our thrust planning model was validated against GOCE data (a 2009 VLEO R&D mission) with sub-meter accuracy. Radiation tolerance was excellent, with 4x fewer single-event upsets than expected. Orbit determination was dialed.We proved sustainable VLEO operations.Developed and built in just over two years, our in-house bus Precision is now TRL-9: flight-proven on-orbit.Every bus subsystem worked. Every piece of in-house technology we developed performed: our CMG steering law, our operational modes, flight and ground software, electronics boards, and our novel thermal management system. Â We hit our embedded software GNC timing deadlines, we converged our attitude and orbit determination estimators, we saw 4Ï€ steradian command and telemetry antenna coverage, and we got on-orbit actuals for our power generation and loads.Our cloud-native ground system was incredible. Contact planning across 25 ground stations was completely automated. Mission scheduling updated every 15 minutes to incorporate new tasking and the latest satellite state information, smoothly transitioning to updated on-board command loads with visual tracking of each schedule and its status. Automated thrust planning to achieve our desired orbital trajectory supported 30+ maneuvers per day. Our engineers could track and command the satellite from anywhere with internet and a secure VPN.We pushed 14 successful flight software feature updates on-orbit â€” and even executed one FPGA update, which is exceptionally rare. The ability to continuously improve throughout Clarity's operational life proved essential â€” every major solution to challenges we faced involved flight software updates. On-orbit software upgrades are exceedingly tricky to get right, but Clarity-1 was designed from day one around this foundational capability.The first month of the mission was magic.An hour after launch, we watched Clarity-1 deploy from the premium caketopper slot into LEO, giving us an incredible view of the Nile River as she separated from the rocket.First contact came just three hours later at 5:11am MT. Imagine sitting in Mission Control, watching two ground station passes with no data, then on the third: heaps of green, healthy telemetry streaming into all of the subsystem dashboards. Clarity had nailed her autonomous boot-up sequence and rocket separation rate capture. Stuck the landing.The next milestone â€” and the one many of us were most anxious about â€” was our autonomous Protect Mode, basically our VLEO version of Safe Mode.We nailed it 14 hours after launch.By 6:45pm that same day, Clarity was in Operational mode, ready for commissioning."Gotta say it: the last 16 hours have been incredible. I started my shift last night hoping to see one bit of data. I wouldn't have believed it if someone told me we'd be in Protect within 14 hours from launch."â€” Albedo GNC EngineerThe days that followed were a blur of checkboxes turning green. 4-CMG commissioning complete. Payload power-on and checkout validated. Thermal balance for both visible and thermal sensors confirmed. Our first on-orbit software update went flawlessly.Clarity uses Control Moment Gyroscopes (CMGs) to steer the satellite, giving us more agility than more commonly used reaction wheels. We moved onto validating GNC modes such as GroundTrack, which we use to point at communication ground terminals.Clarity-1 accurately pointing at a ground station using GroundTrack mode; visualization is driven using real telemetryWe moved on to commissioning our X-band radio â€” the high-rate link to downlink imagery. After we uncovered an issue with our ground station providerâ€™s pointing mode, the 800 Mbps link began pumping down data on every pass. The waveforms were clean. Textbook. A direct representation of how locked in our precision CMG pointing was.With our first satellite at this level of complexity, we couldn't believe how smoothly it had gone. Years of developing new technologies had been validated in a fraction of the commissioning time we'd anticipated.Next up was maneuvering from our LEO drop-off altitude down to VLEO, where it would be safe to eject the telescope contamination cover and start snapping pictures.One of our four CMGs experienced a temperature spike in the flywheel bearing. Our Fault Detection, Isolation, and Recovery (FDIR) logic caught it immediately, spun it down, and executed automated recovery actions. But it wouldn't spin back up. Manual recovery attempts followed. Also unsuccessful.Rushing back into CMG operations without understanding the failure mechanism risked killing the mission entirely, so we turned off the other three and put the satellite in two-axis stabilization using the magnetic torque rods.Clarity-1 intentionally spinning to provide two-axis stabilizationWe had a choice. Hack together novel 3-CMG control algorithms as fast as possible and risk losing another, or figure out how to leverage only the torque rods to achieve 3-axis control with sufficient accuracy to navigate the maneuver to VLEO.We went with the torque rods.On satellites this size (~600 kg), magnetic torque rods are typically used for momentum dumping, not attitude control. But we'd built Clarity with unusually beefy torque rods due to the elevated momentum management needs in VLEO. Our GNC team went heads down and developed algorithms to achieve 3-axis attitude control using only torque rods.Within a month, we had it working.Both of our electric thrusters commissioned quickly and were working well. But with torque rods only, our attitude control had 15 to 20 degrees of error, sometimes reaching ~45 degrees. And maneuvering to VLEO isnâ€™t â€œpoint into the wind and fireâ€ â€” itâ€™s continuous vector and trajectory management across an orbit. That kind of control error meant inefficient burns and a much harder descent plan.As the descent progressed, however, the team learned and iterated. With more iteration and flight software updates, we uploaded onboard logic informed by several sources of live data that dialed in our thrust vector control to within 5 degrees of the target. The autonomous thrust planning system we built enabled us to claw back performance that nearly matched our originally projected descent speed.We maneuvered safely past the ISS and entered VLEO. Eager to pop off the contamination cover.Once we reached safe altitude, it was time to jettison the contamination cover protecting our telescope.There are horror stories about contamination covers getting stuck after months of temperature fluctuations.Clarity's was flawless. I'll never forget seeing this blip in telemetry live â€” confirming through Newton's third law that the jettison was successful. Shortly after, LeoLabs confirmed tracking of two separate objects.We were ready to start imaging.Here's where it got complicated.Our GNC and FSW teams were close but not yet finished with the new 3-CMG control law. CMGs are rarely used in commercial space, let alone by a startup. Then take one more step: singularity-prone 3-CMG control that to our knowledge has not been attempted on a non-exquisite satellite, and certainly not developed and uploaded on-orbit. Traditional algorithms require at least four CMGs to provide capability volumes free of singularities.We were eager to make some amount of progress, so we started imaging on torque rods even though there would be severe limitations: 50+ pixels of smear, large mispointing from the wobble of torque rod control due to earth's magnetic field, and downlink limited to at best two small images per day. The last two constraints meant we were at risk of spending precious downlink capacity on clouds.Sure enough, the first two days of pixels were mostly clouds, but we were happy to peek through a little in this image.Although we couldn't  attitude accurately, we did still have good attitude  after the fact. AyJay whipped up a clever idea with Claude Code that automated posting weather conditions in Slack for each collection. We analyzed that to determine which images were likely clear, and selected those for downlink.We adjusted the focus position a few times, and images continued getting better.Then, 3-CMG control was ready.Out of the box, the new algorithms and software performed perfectly.This visualization shows real telemetry of Clarity performing seven back-to-back imaging maneuvers, with limited 3-CMG agility, followed by an X-band downlink over Iceland minutes later. The satellite was executing sophisticated attitude profiles with very low control error. Fiber-optic gyro measurements showed exquisite jitter performance.Clarity is an agile TDI line-scanner capable of off-nadir imaging. This sequence was near-nadir intentionally for calibration purposes.In real time, collecting and downlinking those seven images took ten minutes.And this is where our ground software really showed its teeth. On most missions, â€œdata on the groundâ€ is just the start â€” turning raw bits into something viewable is a slow chain of handoffs and batch processing. For us, within seconds of the downlink finishing, the image product pipeline was already posting processed snippets into our company Slack. Literally seconds.That end-to-end loop â€” photons in orbit to a viewable product on the ground, within minutes â€” is a capability thatâ€™s still rare in this industry.As expected with smear reduced, image quality improved immediately.We were ready to execute focus calibration.Large telescope optics experience hygroscopic dryout during the first few months on-orbit â€” moisture trapped in materials during ground assembly slowly releases in the vacuum of space, causing the focus position to drift. Dialing in best focus requires dozens of iterations: capture images, analyze sharpness, adjust focus position, repeat. Each cycle gets you closer to the optical performance the system was designed for, and our telescopeâ€™s on-ground alignment was verified to spec.After a few iterations of this, we could start to see cars.Even this early into imaging, the infrared images blew us away. Using a low-cost microbolometer â€” a fraction of the price of cooled IR sensors â€” we captured thermal signatures that showed ships in Tokyo Bay, steel processing facilities where we could distinguish individual coke ovens from their smokestacks, and distinct signatures between real vegetation and turf â€” a good proxy for camouflage detection. Day or night, clear as day.Three days into the excitement, CMG problems started again.A second CMG began showing the same telemetry signatures we now recognized as warning signs.What we had learned from the investigation: the allowable temperature specifications of the CMGs were much higher than the true limit, constrained by what the lubricant inside the flywheel could handle. A straightforward fix for the future â€” an unfortunate corner case to learn about in hindsight.The second CMG showing issues was also on the hot side of the satellite. While we had overhauled the vehicle and CMG operations to prevent additional bearing wear, the damage had already been done in the first month of the mission.We spent months trying everything we could to get the CMGs to operate sustainably. The team attempted many clever solutions, one of which revived the first CMG that had locked up. We uploaded a feature to select any 3 of the 4 CMGs for operator commanding. But we weren't able to get sustained, reliable operation.Despite the CMG challenges, here's what the imaging journey proved.The full end-to-end image chain works. Photons hit our optics, get captured by our sensor, processed through payload electronics, packetized and encrypted, transmitted via our X-band radio, received on the ground, and processed into image products. The entire chain is validated.The end-to-end loop is fast. Within 30 seconds of a downlink, processed image snippets were already posting to our company Slack.Sensor performance exceeded expectations. Dynamic range, radiometry, color balance, band-to-band alignment â€” all look great, even on uncalibrated imagery.We can scan out long images. Our line-scanning approach produced strips 20-30 kilometers long, exactly as designed.Pointing accuracy and high quality telemetry validates the ingredients for precise geolocation. The data we need to pinpoint where each pixel lands on Earth to <5m (closed-loop CE90) is there.Jitter and smear are low. Fiber-optic gyro measurements confirmed 3x lower smear and 11x lower jitter compared to our goal â€” a critical ingredient for exquisite imagery.Our proprietary image scheduler works. The automated system that plans collections, manages constraints, and optimizes what we capture each day performed as designed.Nine months into the mission, we lost contact with Clarity-1.By that point, we had largely exhausted our options on the CMGs. The path to further image quality improvement had effectively closed.We had been tracking intermittent memory issues in our TT&C radio throughout the mission, working around them as they appeared. Our best theory is that one of these issues escalated in a way that corrupted onboard memory and is preventing reboots. We've tried several recovery approaches. So far, none have worked, and the likelihood of recovery looks low at this point.But here's what matters: the VLEO validation data we collected is sufficient.We combined a state-of-the-art atmospheric density model, our high-fidelity orbital dynamics force models, and months of natural orbit decay data from 350 to 380 km altitude to determine Clarityâ€™s coefficient of drag â€” with repeatable results at different altitudes. That drag coefficient, paired with our demonstrated ability to maintain altitude in VLEO for months using high-efficiency thrusters, tells us exactly how the vehicle behaves under aerodynamic drag across the VLEO regime â€” and validates an average five-year lifespan at 275 km across the solar cycle. Telemetry from our solar arrays, together with onboard atomic oxygen sensor data, shows peak power generation stayed constant after exposure to VLEO levels of AO fluence â€” proving our AO mitigation worked.Thanks to our friends at LeoLabs, we've validated that Clarity is maintaining attitude autonomously. She's still up there, still oriented, still descending through VLEO. Just not talking to us.Even before this, we had started developing an in-house TT&C radio for our systems moving forward, rather than reusing this radio that was procured from a third party. Weâ€™ll incorporate learnings from this reliability issue into that.We're still working the problem. This chapter isn't over yet. But even if it is, Clarity-1 gave us what we needed to build what comes next.If you think about exquisite imagery as a pyramid, we needed 100% of the systems working together to achieve the pinnacle: 10 cm visible imagery. We got to about 98%. Everything else in that pyramid â€” the entire foundation â€” is proven and retired.Our drag coefficient. Our atomic oxygen resilience. Our solar arrays. Our thermal management. Our flight software. Our ground software. Our CMG steering laws. Our precision pointing algorithms. Our payload electronics. Our sensor performance. Our image processing chain. Our ability to operate sustainably in VLEO. Our team.We know exactly what to fix. Itâ€™s straight forward: operate the CMGs at lower temperature. The system thermal design is already updated in the next build to maximize CMG life going forward.Beyond the CMGs, there were a handful of learnings on the margins. We learned our secondary mirror structure could be stiffer â€” already in the updated design. We learned we could use more heater capacity in some payload zones â€” already fixed.We learned from the things that worked, too. We're well down the development path for next-gen flight software, avionics, and power distribution. Orbit determination and geolocation will be even better. Additional surface treatments will improve drag coefficient further. Power-generation will increase while maintaining the proven atomic oxygen resilience. The list goes on.The path to exquisite imagery is clear. And thatâ€™s only one of many exciting capabilities unlocked by sustainable operations in VLEO.Our next VLEO mission will incorporate these learnings and demonstrate new features that enable missions beyond imaging â€” weâ€™ll share more details soon. In parallel, imaging remains a core focus: weâ€™re continuing to build optical payloads for EO/IR missions as part of a broader VLEO roadmap.The successes of Clarity-1 reinforced our core conviction: VLEO isnâ€™t just a better orbit for imaging â€” itâ€™s the next productive orbital layer.The physics are unforgiving, but thatâ€™s exactly why it matters. Go lower and you unlock a step-change in performance: sharper sensing, faster links, lower latency, and a new level of responsiveness. The reason VLEO has been written off for decades isnâ€™t lack of upside â€” itâ€™s that most satellites simply canâ€™t survive there long enough to matter.Clarity proved the hard parts: sustainable VLEO operations, validated drag and lifetime models, atomic oxygen resilience, and a flight-proven high-performance bus. Weâ€™re not speculating about VLEO. Weâ€™re operating in it, learning in it, and capitalized to scale it.]]></content:encoded></item><item><title>Bye Bye Gmail</title><link>https://m24tom.com/bye-bye-gmail/show</link><author>tklenke</author><category>hn</category><pubDate>Sat, 24 Jan 2026 19:44:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[it's been great, but it's not me, it's you Please remove  as my primary email address and substitute . I will try to respond to email sent to @gmail.com but no guarantees.A week or two ago I was surprised to see a Google Gemini summary at the top of my email on my phone. A day or two later this appeared in my web client as well. Look, I love our new overlords (for the record m'lords I nearly always use "please" and "thank you"). I was an early adopter and introduced many of you to the LLMs. I still am a frequent user... I mean someone who has all the answers and blows smoke up my ass... perfect right!Here's the thing. I like to read things from my friends that they have taken the time to write. I personally hate texting. All the nuance is gone. Often the humor. Sad. Makes me want to have a beer with you... eye contact... blech. The  thing I want is a summary... at the top of the email... highlighted... that I  turn off.I tried to turn it off. I can. It's under Gmail -> Settings -> General -> Smart Features (checkbox). ... the AI summaries is now grouped with the Smart Tabs.For those of you who do not use Gmail (or do use Gmail and don't use Smart Tabs), Smart Tabs (officially the Tabbed Inbox) have been part of Gmail since 2013; well actually the technology behind themâ€”Smart Labelsâ€”actually debuted two years earlier. (Thank you Gemini, yes I  truly love you. Tell me again about the comparisons of Stephen Miller and Heinrich Himmler's tactics please?)Smart Tabs automatically sort my incoming flood of solicited commercial email ( laughter from those who know my first start-up) into five buckets: K&L Wine Merchants at the top of the list. Hi Andrew on Facebook (that I only log into from Firefox running on a VM). Actual transactional emails from companies. at the top of the list (a newsletter I'd like to read but don't want to make the time justify paying for the content).I tried turning off Smart Features and oh my, that's not usable. So I lived with the AI summary at the top. For a week. Then this morning, I saw several messages in my Primary tab that normally get sorted into Promotions, Social, Updates or Forums. This is not unheard of; sometimes a company uses a new incoming address or something and stuff gets put in the wrong bucket.But  time, I got a popup that says I must "Share" this message with Google and links to the Privacy Policy and Google Terms of Service. And an explicit sentence:"Messages and attachments might be reviewed by humans, so don't share any sensitive or confidential information."I'm not naive. I'm an early adopter, my email address includes my name and no numbers. I was a direct marketer when we still were the red-headed step-children of the product managers.  I carpooled to Symantec with Google employee no 11's girlfriend (Go Beavers!) From the get-go, having Google read my email in order to provide targeted advertising was part of the deal. I was fine with that.... now... what they are saying is that... we are going to use your email to train our LLMs. I'm not okay with that. That knowledge of my way of writing, my personal details, my confidential commercial information is  okay to use to train your models. 'Cause I expect mistakes will be made and more information will reside in the model than those at Google (or FB, MSFT etc) intended. And I'm not really up for assuming the risk that my information is accessible via the model  of Google.So... goodbye Gmail. It's been great. Really great. I'm sure I'll miss you. Bye.My email is now being hosted by Microsoft, so hopefully will be free of the outages and limits some of you have experienced with that email in the past. There it will reside until I cannot turn off MSFT's ability to read my email. Then I guess it's off to Switzerland ( https://proton.me/about ); my email can be with my gold. JK..NRp.s.  why thank you Gemini for reformatting that for me into a clean, engaging markdown blog post.  Yes, I do agree this is a sharp, timely take on the "AI-ification" of tools we use every day. I love you.  Kill me last?]]></content:encoded></item><item><title>Agent orchestration for the timid</title><link>https://substack.com/inbox/post/185649875</link><author>markferree</author><category>hn</category><pubDate>Sat, 24 Jan 2026 19:25:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>BirdyChat becomes first European chat app that is interoperable with WhatsApp</title><link>https://www.birdy.chat/blog/first-to-interoperate-with-whatsapp</link><author>joooscha</author><category>hn</category><pubDate>Sat, 24 Jan 2026 19:04:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Start 1:1 chats with WhatsApp users using their phone numberSend messages, photos and filesCommunicate over an encrypted connectionUse your work email as your identity instead of a personal phone number]]></content:encoded></item><item><title>Federal Agents Kill Another Person in Minneapolis Immigration Crackdown</title><link>https://time.com/7357547/minneapolis-shooting-ice-agent/</link><author>hggh</author><category>hn</category><pubDate>Sat, 24 Jan 2026 18:08:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[A man was shot and killed by a Border Patrol agent in Minneapolis on Saturday morning, the second fatal shooting in just over two weeks by federal authorities in the city.The incident follows the Jan. 7 killing of Renee Good by a federal agent less than three miles away, and comes as the city was already convulsed by mass protests calling for an end to the surge of immigration agents in the state.The victim was named as Alex Jeffrey Pretti, a 37-year-old Minneapolis resident and intensive care unit nurse who treated veterans. His family said he was motivated to join protests after Good's killing. Several videos of the shooting show an altercation taking place around 9 am when a woman protester was pushed to the ground by a Border Patrol agent. When Pretti attempts to stand between the agent and the woman, the agent pepper-sprays him in the face. More agents join the fray and tackle Pretti to the ground as he is disoriented. As a group of agents restrain Pretti on the ground, one emerges from the melee with a gun, and soon after, a shot rings out, then several more in quick succession. At least 10 shots were fired in around five seconds, including several as Pretti lay motionless on the ground. : Minnesotans Shutter Businesses and Call Off Work in Economic Blackout Day to Protest ICEPresident Donald Trump responded to the shooting in a lengthy post on Truth Social that called immigration agents "patriots" and claimed they were in Minneapolis because of "massive Monetary Fraud" and "Illegal Criminals that were allowed to infiltrate the State."Minneapolis Police Chief Brian Oâ€™Hara said at a press conference on Saturday afternoon that Pretti had not been in trouble with the police before."The only interaction that we are aware of with law enforcement has been for traffic tickets and we believe he is a lawfully gun owner with a permit to carry," Oâ€™Hara said. The Department of Homeland Security (DHS) gave a detailed account of the shooting in a statement that was contradicted by several videos shot by bystanders at the scene. The agency said it was carrying out a "targeted operation" when an individual approached U.S. Border Patrol officers with a 9mm semi-automatic handgun, pictures of which it shared with the media. It said officers attempted to disarm the man, but he "violently resisted.""Fearing for his life and the lives and safety of fellow officers, an agent fired defensive shots. Medics on scene immediately delivered medical aid to the subject but was pronounced dead at the scene," the statement continued. It added: "[T]his looks like a situation where an individual wanted to do maximum damage and massacre law enforcement." But severalvideos showing the lead-up to the fatal shooting show Pretti filming a group of Border Patrol officers with his phone in his right hand, with his left hand empty. The video shows an agent pepper-spraying Pretti in the face and, together with several other officers, dragging him to the ground. That is when the fatal shooting occurs.: Fatal ICE Shooting Sparks Scrutiny of Killings in Trumpâ€™s Immigration Crackdown The incident is the latest in a series of shootings in which the DHS claims the victim was threatening the life of an agent, only for video evidence to later contradict the claim. After the shooting of Renee Good, the DHS accused her of â€œattempting to run over our law enforcement officers in an attempt to kill themâ€”an act of domestic terrorism,â€ only for video evidence to show her turning her car away, and the agent positioned to the side of her vehicle when he fired the fatal shot. Several other federal officials gave accounts of events that were similarly inaccurate to those given by DHS. Pretti's parents, Michael and Susan Pretti, found out about the death of their son when they were called by an Associated Press reporter. As of Saturday evening, the family had still not heard from anyone at a federal law enforcement agency about their sonâ€™s death, according to the AP. In a statement released to the media, the family criticised the "sickening lies told about our son by the administration." "Alex is clearly not holding a gun when attacked by Trumpâ€™s murdering and cowardly ICE thugs. He has his phone in his right hand and his empty left hand is raised above his head while trying to protect the woman ICE just pushed down all while being pepper sprayed," the statement said. Saturday's shooting prompted a wave of anger from local politicians, many of whom have been calling for the Trump Administration to bring an end to its immigration surge following weeks of violent encounters with Minnesotans, including the use of pepper spray and the arrest of peaceful protesters. Minnesota Governor Tim Walz described the shooting as "sickening" and called on President Trump to end his immigration crackdown in the state.Minn. Gov. Tim Walz Calls on Trump to End Immigration Crackdown After Second Fatal Shootingâ€œI just spoke with the White House after another horrific shooting by federal agents this morning. Minnesota has had it. This is sickening,â€ Walz said in a post on X.â€œThe President must end this operation. Pull the thousands of violent, untrained officers out of Minnesota. Now.â€Later, he urged people protesting the shooting to do so peacefully. â€œWe want peace, they want chaos,â€ the governor said of the federal government. â€œWe cannot and will not give them what they want.â€Democratic Senator Amy Klobuchar said: To the Trump administration and the Republicans in Congress who have stood silent: Get ICE out of our state NOW.O'Hara, in his press conference, called for greater discipline from the estimated 3,000 federal immigration agents in the city.  "Our demand today is for those federal agencies that are operating in our city to do so with the same discipline, humanity and integrity that effective law enforcement in this country demands," he said. A few hundred protesters gathered at the scene of the shooting in south Minneapolis by noon, where they scuffled with federal agents who had blocked off the intersection. Protesters screamed "I smell Nazis" at the federal agents and shouted at them to "go home."The agents deployed tear gas and used pepper-spray as they fought running battles with protesters.The shooting comes a day after thousands took to the streets across Minnesota on Friday, closing down businesses and calling out of work in a mass protest against the Trump Administrationâ€™s immigration crackdown in the state.The â€œIce Out of Minnesota: Day of Truth and Freedomâ€ demonstration, organized by community leaders, members of the clergy, and labor unions, called for a â€œno work, no school, no shoppingâ€ economic blackout.Trump, in his Saturday afternoon post, accused Minneapolis Mayor Jacob Frey and Governor Walz of "inciting Insurrection.""Where are the local Police? Why werenâ€™t they allowed to protect ICE Officers? The Mayor and the Governor called them off? It is stated that many of these Police were not allowed to do their job, that ICE had to protect themselves â€” Not an easy thing to do!" he wrote. As night fell across Minneapolis, many residents set out candles in their windows to memorialize Pretti. Several vigils were held across the city. A New York  reporter visited one at Painter Park, near Prettiâ€™s home, where more than 100 people gathered with candles and sang the opening lines to 'This Little Light of Mine.']]></content:encoded></item><item><title>Raspberry Pi Drag Race: Pi 1 to Pi 5 â€“ Performance Comparison</title><link>https://the-diy-life.com/raspberry-pi-drag-race-pi-1-to-pi-5-performance-comparison/</link><author>verginer</author><category>hn</category><pubDate>Sat, 24 Jan 2026 18:06:00 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Today weâ€™re going to be taking a look at what almost 13 years of development has done for the Raspberry Pi. I have one of each generation of Pi from the original Pi that was launched in 2012 through to the Pi 5 which was released just over a year ago.Weâ€™ll take a look at what has changed between each generation and how their performance and power consumption has improved by running some tests on them.Hereâ€™s my video of the testing process and results, read on for the write-up;Purchase Links For Components Used In These TestsSome of the above parts are affiliate links. By purchasing products through the above links, youâ€™ll be supporting this channel, at no additional cost to you.Hardware Changes Through Each GenerationThis is the original Raspberry Pi, which was launched in February 2012.This Pi has a Broadcom BCM2835 SOC which features a single ARM1176JZF-S core running at 700MHz along with a VideoCore IV GPU. It has 512 MB of DDR RAM.In terms of connectivity, it only has 100Mb networking and 2 x USB 2.0 ports. Video output is 1080P through a full-size HDMI port or analogue video out through a composite video connector and audio output is provided through a 3.5mm audio jack. It doesnâ€™t have any WiFi or Bluetooth connectivity but it does have some of the features that we still have on more recent models like DSI and CSI ports, a full size SD card reader for the operating system and GPIO pins, although only 26 of them at this stage.Power is supplied through a micro USB port and it is rated for 5V and 700mA.It was priced at $35 â€“ which at the time was incredibly cheap for what was essentially a palm-sized computer.The Raspberry Pi 2 was launched 3 years later, in February 2015 and this Pi looked quite different to the original and similar to the Piâ€™s we know today.The Pi 2 has a significantly better processor than the original. The Broadcom BCM2836 SOC has 4 Cortex-A7 cores running at 900 MHz and it retained the same VideoCore IV GPU. RAM was also bumped up to 1GB.It added another 2 x USB 2.0 ports alongside the 100Mb Ethernet port. The composite video port disappeared and the analogue video output was moved into the audio jack.The GPIO pins were increased to 40 pins which has followed the same pin layout since â€“ which has really helped in maintaining compatibility with hats and accessories. The SD card reader was also changed to a microSD card reader.The power circuitry was bumped up to 800mA to accommodate the more powerful CPU.The Raspberry Pi 3 was launched just a year later, in February 2016.The Pi 3â€™s new Broadcom BCM2837 SOC retained the same 4-core architecture but these were changed to 64-bit Cortex A53 cores running at 1.2Ghz.RAM was kept at 1GB but was now DDR2.There was no change to the USB or Ethernet connectivity on the original Pi 3 but we did see WiFi and Bluetooth added for the first time. WiFi was single band 2.4GHz and we had Bluetooth 4.1.The version that I have is actually the 3B+, which was launched a little later. The main improvements over the original Pi 3 were a 0.2GHz boost to the clock speed and the upgrade to Gigabit networking with PoE (Power over Ethernet) support and dual-band WiFi.The power circuitry was again improved, still running at 5V but now up to 1.34A, which was almost double the Pi 2.Next came the Pi 4 in June 2019. This Pi came at one of the worst times for global manufacturing and was notoriously difficult to get hold of due to the impact of COVID on the global supply chain. Quite ironically, this hard-to-get Pi is the one that Iâ€™ve got the most of, mainly due to my water-cooled Pi cluster build.The Pi 4 has a Broadcom BCM2711 SOC with 4 Cortex-A72 cores running at 1.5GHz. So again a slight clock speed increase over the Pi 3 but still retaining 4 cores. It also includes a bump up to a VideoCore VI GPU.This was the first model to feature different RAM configurations. It was originally available in 1, 2, 4GB variants featuring LPDDR4 RAM and in March of 2020 an 8GB variant was added to the linup as well. This obviously resulted in a few different price points but impressively they still managed to keep a $35 offering 7 years after the launch of the first Pi.It retained the same form factor as the Pi 3 but with the network and Ethernet ports switched around. Notably, two of the USB ports were upgraded to USB 3.0, networking was now gigabit ethernet like the 3B+, WiFi was dual-band and it had Bluetooth 5.0.They also changed the single full-size HDMI port to two micro HDMI ports. Most people I know donâ€™t like this change and find it annoying to have to use adaptors to work with common displays and these micro HDMI ports are prone to breaking when they are used often. I think general hobbyists and makers would prefer this to still be a single full-size port but Piâ€™s are often used in commercial display applications so I guess thatâ€™s why they went with this dual micro HDMI configuration.The power circuit was actually reduced in this model, from 1.34 down to 1.25A and the port was changed to USB C.Lastly and most recently we have the Pi 5 which was launched in October 2023.This Pi features a Broadcom BCM2712 SOC with 4 Cortex A76 cores running at a significantly faster 2.4Ghz and a VideoCore VII GPU running at 800MHz.So quite a bump up in CPU and GPU performance.It is offered in 3 RAM configurations but the drop in a 1GB offering means that theyâ€™re no longer available at the $35 price point. There is a fairly significant increase in price up to $50 for the base 2GB variant.Some other notable changes are the inclusion of a PCIe port which enables IO expansion and a much improved power circuit. The PCIe port is quite commonly used to add an NVMe SSD instead of a microSD card for the operating system.The power circuit was upgraded to handle the PCIe port addition, now stepping up to 5V at up to 5A, along with a power button for the first time.The change in power supply requirements to 5V and 5A is a bit annoying as most power delivery capable supplies cap out 2.5 or 3A at 5V. It would have been more universal to require a 9V 3A supply to meet the Pis power requirements. I assume they steered away from this because the Piâ€™s circuitry runs at 5V and 3.3V and they would have then needed to add another onboard DC-DC converter which increases complexity, size and potentially the cost, it would also have made it a bit less efficient. But this does mean that you most likely need to buy a USB C power supply that has been purpose-built for the Pi 5.The Pi 5 is also the first Pi to have its own dedicated fan socket.So thatâ€™s a summary of the hardware changes, now letâ€™s boot them up and take a look at their performance.Testing The Performance Of Each Generation Of PiTo compare the performance between the Piâ€™s, Iâ€™m going to run the following tests.Iâ€™m going to attempt to playback a  in the browser, although I expect weâ€™ll have problems with this up to the Pi 4.Weâ€™ll then run a  which Iâ€™ll do both for a single-core and multicore.Then weâ€™ll run a .Then test the storage speed using James Chambers Pi Benchmark script.Then weâ€™ll run an  test.Lastly, weâ€™ll look at , both at idle and with the CPU maxed out.And then use that data to determine each Piâ€™s .To keep things as consistent as possible Iâ€™m going to be running the latest available version of Pi OS from Raspberry Pi Imager for each Pi. I was pleasantly surprised to find that you can still flash an OS image for the original Pi in their latest version of Imager.Iâ€™ll be testing them all running on a 32GB Sandisk Ultra microSD card. Iâ€™ll also be using an Ice Tower cooler on each to ensure they donâ€™t run anywhere near thermal throttling.1080P YouTube Video PlaybackI started with the original Pi and its first boot and setup process was a lesson in patience. It took me the best part of two hours to get the first boot complete, the Pi updated and the testing utilities installed but I got there in the end.Even once set up it takes about 8 minutes to boot up to the desktop and the CPU stays pegged at 100% for another two to three minutes before dropping down to about 20% at idle.The original Pi refused to open up the browser, so thatâ€™s where my YouTube video playback test ended.The Pi 2 managed to open the browser and actually started playing back a 1080P video, which was surprising, but playback was terrible. It dropped pretty much all of the frames both in the window and fullscreen.The Pi 3 played video back noticeably better than the Pi 2, but itâ€™s still quite a long way away from being usable and still drops a lot of frames.The Pi 4 handled 1080P video reasonably well. It had some initial trouble but then settled down. Fullscreen is also a bit choppy but is also usable.The Pi 5 handled 1080P playback well without any significant issues both in the window and fullscreen.Next was the Sysbench CPU benchmark. I ran three tests on each and averaged the scores and I did this for both single-core and multicore.In single core, the Pi 1 managed a rather dismal score of 68, the Pi 2 got a bit more than double this score but the real step up was with the Pi 3 which managed 18 times higher than the Pi 2. The Pi 4 and Pi 5 also offered good improvements on the previous generations.Similarly in multicore, the Pi 3 scored over 18 times the score of the Pi2 and the Pi 4 and 5 provided good improvements on the Pi 3â€™s score.Comparing the combined multicore score of the Pi 5 to what the single core on the Pi 1 can do, the Pi 5 is a little over 600 times faster.Next, I tried running a GLMark2 GPU benchmark on them. I used the GLMark2-es2-wayland version which is designed for OpenGL ES so that the Pi 1 was supported.I was surprised that the Pi 1 was even able to run GLMark2 â€“ it did complete the benchmark, although the score wasnâ€™t all that impressive.These results really show how the Piâ€™s GPU has improved in the last two generations. Prior to these tests, I had never seen a score below 100 and the Pi 1, 2 and 3 managed to fall short of triple digits. Pi 5 scored over 2.5 times higher than the Pi 4.Next was the storage speed test using James Chambers Pi Benchmarks script. The bus speed has increased over the years from 25MHz on the Pi 1 to 100MHz on the Pi 5, so I expect weâ€™ll see these reflected in the benchmark scores.The storage speed testâ€™s results arenâ€™t as dramatic as the CPU and GPU results but show a steady improvement between generations. The Pi 3 did a bit worse than the Pi 2 but this small difference is likely just due to variability in the tests.Next, I ran the iPerf network speed test on each.The Pi 1 doesnâ€™t quite get close to its theoretical 100Mbps but the Pi 2 does. The Pi 3 B+ although having Gigabit Ethernet is limited by this running over USB 2.0 which only has a theoretical maximum of 300MBps, so it came quite close. Both the Pi 4 and 5 expectedly come close to theoretical Gigabit speeds.Lastly, I tested the power consumption of each Pi at idle and under load.I used the same Pi 5 power adaptor to test all of the Pis to keep things consistent and I just used a USB C to micro USB adaptor for the Pi 1, 2 and 3.The idle results were closer than I expected. The Pi 2 had the lowest idle power draw and the Pi 5 the highest, but all were within a watt or two of each other. At full load, you can see the increase in CPU power draw more physical power with the Pi 5 drawing almost three times the Pi 1 and Pi 2.Converted to performance per watt using the Sysbench results, we can again see how much better the Pi 4 and 5 are over the Pi 1 and 2. There is a clear improvement in the performance that each generation of Pi is able to get per watt of power, which is essentially its efficiency. Although the Pi 5 draws more power than the Pi 1 under full load, youâ€™re getting almost 200 times more power out of it per watt.Final Thoughts On The Drag Race And Future PisI really enjoyed working through this project to see how much Piâ€™s have changed over the years, particularly in terms of performance. I still remember being amazed at the size and price of the original Pi when it came out and itâ€™s great that theyâ€™re still fully supported and can still be used for projects â€“ albeit with less CPU-intensive projects.Let me know what you think has been the biggest improvement to the Pi over the years and what youâ€™d still like to see added to future models in the comments section below.I personally really like the addition of the PCIe port on the Pi 5 and Iâ€™d like to see 2.5Gb networking and a DisplayPort or USB C with DisplayPort added to a future generation of Pi.]]></content:encoded></item><item><title>December in Servo: multiple windows, proxy support, better caching, and more</title><link>https://servo.org/blog/2026/01/23/december-in-servo/</link><author>t-3</author><category>hn</category><pubDate>Sat, 24 Jan 2026 17:03:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[For better compatibility with older web content, we now support  CSS properties like â€˜-moz-transformâ€™ (@mrobinson, #41350), as well as window. (@Taym95, #41111).When using  on Windows, you can now see  and log output, as long as servoshell was started in a console (@jschwe, #40961).Servo diagnostics options are now accessible in servoshell via the  environment variable (@atbrakhi, #41013), in addition to the usual  /  arguments.We now use the  by default (@Narfinger, @mrobinson, #40935, #41179), on most platforms.
If you donâ€™t want to trust the system root certificates, you can instead continue to use Mozillaâ€™s root certificates with --pref network_use_webpki_roots.
As always, you can also add your own root certificates via :: ()., the main handle for controlling Servo, is now cloneable for sharing within the same thread (@mukilan, @mrobinson, #41010).
To shut down Servo, simply drop the last  handle or let it go out of scope.
:: and :: have been removed (@mukilan, @mrobinson, #41012).Several interfaces have also been renamed:Weâ€™ve fixed a crash that occurs when <link rel=â€œshortcut iconâ€> has an , which affected chiptune.com (@webbeef, #41056), and weâ€™ve also fixed crashes in:Servo is also on thanks.dev, and already  (+2 over November) that depend on Servo are sponsoring us there.
If you use Servo libraries like url, html5ever, selectors, or cssparser, signing up for thanks.dev could be a good way for you (or your employer) to give back to the community.We now have  that allow you or your organisation to donate to the Servo project with public acknowlegement of your support.
A big thanks from Servo to our newest Bronze Sponsors: , , and !
If youâ€™re interested in this kind of sponsorship, please contact us at .Conference talks and blogs Weâ€™ve recently published one talk and one blog post:We also have two  talks at  in  later this month:Servo developers Martin Robinson (@mrobinson) and Delan Azabani (@delan) will also be attending FOSDEM 2026, so it would be a great time to come along and chat about Servo!]]></content:encoded></item><item><title>Tao Te Ching â€“ Translated by Ursula K. Le Guin</title><link>https://github.com/nrrb/tao-te-ching/blob/master/Ursula%20K%20Le%20Guin.md</link><author>andsoitis</author><category>hn</category><pubDate>Sat, 24 Jan 2026 17:01:31 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Man shot and killed by federal agents in south Minneapolis this morning</title><link>https://www.startribune.com/ice-raids-minnesota/601546426</link><author>oceansky</author><category>hn</category><pubDate>Sat, 24 Jan 2026 16:43:38 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[It was the second fatal shooting this month involving federal agents who have arrived in Minnesota as part of a massive immigration enforcement operation. Minneapolis residents gathered in city parks to light candles and hold vigils after the shooting.]]></content:encoded></item><item><title>Are we all plagiarists now?</title><link>https://www.economist.com/culture/2026/01/22/are-we-all-plagiarists-now</link><author>pseudolus</author><category>hn</category><pubDate>Sat, 24 Jan 2026 16:34:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ask HN: Gmail spam filtering suddenly marking everything as spam?</title><link>https://news.ycombinator.com/item?id=46744807</link><author>goopthink</author><category>hn</category><pubDate>Sat, 24 Jan 2026 16:16:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Almost all transactional emails are being marked as suspicious even when their SPF/DKIM records are fine and theyâ€™ve been whitelisted before. Did Google break something in gmail/spam filtering?]]></content:encoded></item><item><title>Memory layout in Zig with formulas</title><link>https://raymondtana.github.io/math/programming/2026/01/23/zig-alignment-and-sizing.html</link><author>raymondtana</author><category>hn</category><pubDate>Sat, 24 Jan 2026 15:57:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Claude Code&apos;s new hidden feature: Swarms</title><link>https://twitter.com/NicerInPerson/status/2014989679796347375</link><author>AffableSpatula</author><category>hn</category><pubDate>Sat, 24 Jan 2026 14:35:47 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Microsoft will give the FBI a Windows PC data encryption key if ordered</title><link>https://www.windowscentral.com/microsoft/windows-11/microsoft-bitlocker-encryption-keys-give-fbi-legal-order-privacy-nightmare</link><author>blacktulip</author><category>hn</category><pubDate>Sat, 24 Jan 2026 12:55:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Microsoft has confirmed in a statement to Forbes that the company will provide the FBI access to BitLocker encryption keys if a valid legal order is requested. These keys enable the ability to decrypt and access the data on a computer running Windows, giving law enforcement the means to break into a device and access its data.The news comes as Forbes reports that Microsoft gave the FBI the BitLocker encryption keys to access a device in Guam that law enforcement believed to have "evidence that would help prove individuals handling the islandâ€™s Covid unemployment assistance program were part of a plot to steal funds" in early 2025.This was possible because the device in question had its BitLocker encryption key saved in the cloud. By default, Windows 11 forces the use of a Microsoft Account, and the OS will automatically tie your BitLocker encryption key to your online account so that users can easily recover their data in scenarios where they might get locked out. This can be disabled, letting you choose where to save them locally, but the default behavior is to store the key in Microsoft's cloud when setting up a PC with a Microsoft Account."While key recovery offers convenience, it also carries a risk of unwanted access, so Microsoft believes customers are in the best position to decide... how to manage their keys,â€ Microsoft spokesperson Charles Chamberlayne said in a statement to Forbes.Microsoft told Forbes that it receives around 20 requests for BitLocker encryption keys from the FBI a year, but the majority of requests are unable to be met because the encryption key was never uploaded to the company's cloud.This is notable as other tech companies, such as Apple, have famously refused to provide law enforcement with access to encrypted data stored on their products. Apple has openly fought against the FBI in the past when it was asked to provide a backdoor into an iPhone. Other tech giants, such as Meta, will store encryption keys in the cloud, but use zero-knowledge architectures and encrypt the keys server-side so that only the user can access them.It's frankly shocking that the encryption keys that do get uploaded to Microsoft aren't encrypted on the cloud side, too. That would prevent Microsoft from seeing the keys, but it seems that, as things currently stand, those keys are available in an unencrypted state, and it is a privacy nightmare for customers.To see Microsoft so willingly hand over the keys to encrypted Windows PCs is concerning, and should make everybody using a modern Windows computer think twice before backing up their keys to the cloud. You can see which PCs have their BitLocker keys stored on Microsoft's servers on the Microsoft Account website here, which will let you delete them if present.]]></content:encoded></item><item><title>Many Small Queries Are Efficient in SQLite</title><link>https://www.sqlite.org/np1queryprob.html</link><author>tosh</author><category>hn</category><pubDate>Sat, 24 Jan 2026 11:15:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
Many Small Queries Are Efficient In SQLite

200 SQL statements per webpage is excessive for client/server database
engines like MySQL, PostgreSQL, or SQL Server.


But with SQLite, 200 or more SQL statement per webpage is not a problem.


SQLite can also do large and complex queries efficiently, just like
client/server databases.  But SQLite can do many smaller queries
efficiently too.  Application developers can use whichever technique
works best for the task at hand.
     

The Appropriate Uses For SQLite page says that
dynamic pages on the SQLite website typically do about 200 SQL
statements each.
This has provoked criticism from readers.  Examples:

"200 SQL statements is a ridiculously high number for a single page""For most sites, 200 queries is way, way, way too much."
Such criticism would be well-founded for a traditional client/server
database engine, such as MySQL, PostgreSQL, or SQL Server.  In
a client/server database, each SQL statement requires a message
round-trip from the application to the database server and back to
the application.  Doing over 200 round-trip messages, sequentially,
can be a serious performance drag.  This is sometimes called the
"N+1 Query Problem" or the "N+1 Select Problem" and it is an anti-pattern.


SQLite is  client/server, however.  The SQLite database runs
in the same process address space as the application.  Queries do not
involve message round-trips, only a function call.  The latency
of a single SQL query is far less in SQLite.  Hence, using a large number
of queries with SQLite is not the problem.


The first group of queries in the log are extracting display options
from the "config" and "global_config" tables of the Fossil database.
Then there is a single complex query that extracts a list of all elements
to be displayed on the timeline.
This "timeline" query demonstrates that SQLite can easily process complex
relational database queries involving multiple tables, subqueries, and
complex WHERE clause constraints, and it can make effective use of indexes
to solve the queries with minimal disk I/O.


Following the single big "timeline" query, 
there are additional queries for each timeline element.
Fossil is using the "N+1 Query" pattern rather than trying
to grab all the information in as few queries as possible.
But that is ok because there is no unnecessary IPC overhead.
At the bottom of
each timeline page, Fossil shows approximately how long it took to generate
the page.  For a 50-entry timeline, the latency is usually less than
25 milliseconds.  Profiling shows that few of those milliseconds
were spent inside the database engine.


Using the N+1 Query pattern in Fossil does not harm the application.  
But the N+1 Query pattern does have benefits.  For one, the
section of the code that creates the timeline query can be
completely separate from the section that prepares each timeline
entry for display.
This provides a separation of responsibility that helps keep the code
simple and easy to maintain.  Secondly, the information
needed for display, and the queries needed to extract that information,
vary according to what type of objects are to be shown.  Check-ins need one
set of queries.  Tickets need another set of queries.  Wiki pages need a
different query.  And so forth.  By implementing these queries on-demand
and in the part of the code dealing with the various entities, there is
further separation of responsibility and simplification of the overall 
code base.


So, SQLite is able to do one or two large and complex queries, or it can
do many smaller and simpler queries.  Both are efficient.  An application
can use either or both techniques, depending on what works best for the
situation at hand.


The following is a log of all SQL used to generate one particular
timeline (captured on 2016-09-16):

-- sqlite3_open: /home/drh/sqlite/sqlite/.fslckout
PRAGMA foreign_keys=OFF;
SELECT sql FROM localdb.sqlite_schema WHERE name=='vfile';
-- sqlite3_open: /home/drh/.fossil
PRAGMA foreign_keys=OFF;
SELECT value FROM vvar WHERE name='repository';
ATTACH DATABASE '/home/drh/www/repos/sqlite.fossil' AS 'repository' KEY '';
SELECT value FROM config WHERE name='allow-symlinks';
SELECT value FROM global_config WHERE name='allow-symlinks';
SELECT value FROM config WHERE name='aux-schema';
SELECT 1 FROM config WHERE name='baseurl:http://';
SELECT value FROM config WHERE name='ip-prefix-terms';
SELECT value FROM global_config WHERE name='ip-prefix-terms';
SELECT value FROM config WHERE name='localauth';
SELECT value FROM vvar WHERE name='default-user';
SELECT uid FROM user WHERE cap LIKE '%s%';
SELECT login FROM user WHERE uid=1;
SELECT cap FROM user WHERE login = 'nobody';
SELECT cap FROM user WHERE login = 'anonymous';
SELECT value FROM config WHERE name='public-pages';
SELECT value FROM global_config WHERE name='public-pages';
SELECT value FROM config WHERE name='header';
SELECT value FROM config WHERE name='project-name';
SELECT value FROM config WHERE name='th1-setup';
SELECT value FROM global_config WHERE name='th1-setup';
SELECT value FROM config WHERE name='redirect-to-https';
SELECT value FROM global_config WHERE name='redirect-to-https';
SELECT value FROM config WHERE name='index-page';
SELECT mtime FROM config WHERE name='css';
SELECT mtime FROM config WHERE name='logo-image';
SELECT mtime FROM config WHERE name='background-image';
CREATE TEMP TABLE IF NOT EXISTS timeline(
  rid INTEGER PRIMARY KEY,
  uuid TEXT,
  timestamp TEXT,
  comment TEXT,
  user TEXT,
  isleaf BOOLEAN,
  bgcolor TEXT,
  etype TEXT,
  taglist TEXT,
  tagid INTEGER,
  short TEXT,
  sortby REAL
)
;
INSERT OR IGNORE INTO timeline SELECT
  blob.rid AS blobRid,
  uuid AS uuid,
  datetime(event.mtime,toLocal()) AS timestamp,
  coalesce(ecomment, comment) AS comment,
  coalesce(euser, user) AS user,
  blob.rid IN leaf AS leaf,
  bgcolor AS bgColor,
  event.type AS eventType,
  (SELECT group_concat(substr(tagname,5), ', ') FROM tag, tagxref
    WHERE tagname GLOB 'sym-*' AND tag.tagid=tagxref.tagid
      AND tagxref.rid=blob.rid AND tagxref.tagtype>0) AS tags,
  tagid AS tagid,
  brief AS brief,
  event.mtime AS mtime
 FROM event CROSS JOIN blob
WHERE blob.rid=event.objid
 AND NOT EXISTS(SELECT 1 FROM tagxref WHERE tagid=5 AND tagtype>0 AND rid=blob.rid)
 ORDER BY event.mtime DESC LIMIT 50;
-- SELECT value FROM config WHERE name='timeline-utc';
SELECT count(*) FROM timeline WHERE etype!='div';
SELECT min(timestamp) FROM timeline;
SELECT julianday('2016-09-15 14:54:51',fromLocal());
SELECT EXISTS (SELECT 1 FROM event CROSS JOIN blob WHERE blob.rid=event.objid AND mtime<=2457647.121412037);
SELECT max(timestamp) FROM timeline;
SELECT julianday('2016-09-24 17:42:43',fromLocal());
SELECT EXISTS (SELECT 1 FROM event CROSS JOIN blob WHERE blob.rid=event.objid AND mtime>=2457656.238009259);
SELECT value FROM config WHERE name='search-ci';
SELECT value FROM vvar WHERE name='checkout';
SELECT value FROM config WHERE name='timeline-max-comment';
SELECT value FROM global_config WHERE name='timeline-max-comment';
SELECT value FROM config WHERE name='timeline-date-format';
SELECT value FROM config WHERE name='timeline-truncate-at-blank';
SELECT value FROM global_config WHERE name='timeline-truncate-at-blank';
SELECT * FROM timeline ORDER BY sortby DESC;
SELECT value FROM config WHERE name='hash-digits';
SELECT value FROM global_config WHERE name='hash-digits';
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68028;
SELECT pid FROM plink WHERE cid=68028 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM tagxref WHERE rid=68028 AND tagid=9 AND tagtype>0;
SELECT value FROM config WHERE name='timeline-block-markup';
SELECT value FROM config WHERE name='timeline-plaintext';
SELECT value FROM config WHERE name='wiki-use-html';
SELECT value FROM global_config WHERE name='wiki-use-html';
SELECT 1 FROM private WHERE rid=68028;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68026;
SELECT pid FROM plink WHERE cid=68026 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=68026;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68024;
SELECT pid FROM plink WHERE cid=68024 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=68024;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68018;
SELECT pid FROM plink WHERE cid=68018 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=68018;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68012;
SELECT pid FROM plink WHERE cid=68012 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=68012;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68011;
SELECT value FROM config WHERE name='details';
SELECT pid FROM plink WHERE cid=68011 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM tagxref WHERE rid=68011 AND tagid=9 AND tagtype>0;
SELECT 1 FROM private WHERE rid=68011;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68008;
SELECT pid FROM plink WHERE cid=68008 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=68008;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68006;
SELECT pid FROM plink WHERE cid=68006 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=68006;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=68000;
SELECT pid FROM plink WHERE cid=68000 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=68000;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67997;
SELECT pid FROM plink WHERE cid=67997 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67997;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67992;
SELECT pid FROM plink WHERE cid=67992 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67992;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67990;
SELECT pid FROM plink WHERE cid=67990 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67990;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67989;
SELECT pid FROM plink WHERE cid=67989 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67989;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67984;
SELECT pid FROM plink WHERE cid=67984 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67984;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67983;
SELECT pid FROM plink WHERE cid=67983 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67983;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67979;
SELECT pid FROM plink WHERE cid=67979 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67979;
SELECT value FROM config WHERE name='ticket-closed-expr';
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='1ec41379c9c1e400' AND tkt_uuid<'1ec41379c9c1e401';
SELECT 1 FROM private WHERE rid=67980;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67977;
SELECT pid FROM plink WHERE cid=67977 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='1ec41379c9c1e400' AND tkt_uuid<'1ec41379c9c1e401';
SELECT 1 FROM private WHERE rid=67977;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='1ec41379c9c1e400' AND tkt_uuid<'1ec41379c9c1e401';
SELECT 1 FROM private WHERE rid=67974;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67971;
SELECT pid FROM plink WHERE cid=67971 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67971;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67972;
SELECT pid FROM plink WHERE cid=67972 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67972;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67969;
SELECT pid FROM plink WHERE cid=67969 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67969;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67966;
SELECT pid FROM plink WHERE cid=67966 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67966;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67962;
SELECT pid FROM plink WHERE cid=67962 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67962;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67960;
SELECT pid FROM plink WHERE cid=67960 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67960;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67957;
SELECT pid FROM plink WHERE cid=67957 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67957;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67955;
SELECT pid FROM plink WHERE cid=67955 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67955;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67953;
SELECT pid FROM plink WHERE cid=67953 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='5990a1bdb4a073' AND tkt_uuid<'5990a1bdb4a074';
SELECT 1 FROM blob WHERE uuid>='5990a1bdb4a073' AND uuid<'5990a1bdb4a074';
SELECT 1 FROM private WHERE rid=67953;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67941;
SELECT pid FROM plink WHERE cid=67941 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67941;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67940;
SELECT pid FROM plink WHERE cid=67940 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67940;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67938;
SELECT pid FROM plink WHERE cid=67938 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67938;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67935;
SELECT pid FROM plink WHERE cid=67935 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67935;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67934;
SELECT pid FROM plink WHERE cid=67934 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67934;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67932;
SELECT pid FROM plink WHERE cid=67932 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67932;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67930;
SELECT pid FROM plink WHERE cid=67930 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67930;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67928;
SELECT pid FROM plink WHERE cid=67928 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM tagxref WHERE rid=67928 AND tagid=9 AND tagtype>0;
SELECT 1 FROM private WHERE rid=67928;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='0eab1ac7591f511d' AND tkt_uuid<'0eab1ac7591f511e';
SELECT 1 FROM private WHERE rid=67919;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='01874d252ac44861' AND tkt_uuid<'01874d252ac44862';
SELECT 1 FROM blob WHERE uuid>='01874d252ac44861' AND uuid<'01874d252ac44862';
SELECT 1 FROM private WHERE rid=67918;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67916;
SELECT pid FROM plink WHERE cid=67916 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='0eab1ac759' AND tkt_uuid<'0eab1ac75:';
SELECT 1 FROM private WHERE rid=67916;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='a49bc0a8244feb08' AND tkt_uuid<'a49bc0a8244feb09';
SELECT 1 FROM blob WHERE uuid>='a49bc0a8244feb08' AND uuid<'a49bc0a8244feb09';
SELECT 1 FROM private WHERE rid=67914;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67913;
SELECT pid FROM plink WHERE cid=67913 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='0eab1ac7591f' AND tkt_uuid<'0eab1ac7591g';
SELECT 1 FROM private WHERE rid=67913;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67911;
SELECT pid FROM plink WHERE cid=67911 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67911;
SELECT status='Closed' OR status='Fixed' FROM ticket  WHERE tkt_uuid>='0eab1ac7591f511d' AND tkt_uuid<'0eab1ac7591f511e';
SELECT 1 FROM private WHERE rid=67909;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67907;
SELECT pid FROM plink WHERE cid=67907 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67907;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67899;
SELECT pid FROM plink WHERE cid=67899 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67899;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67897;
SELECT pid FROM plink WHERE cid=67897 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67897;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67895;
SELECT pid FROM plink WHERE cid=67895 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67895;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67893;
SELECT pid FROM plink WHERE cid=67893 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67893;
SELECT value FROM tagxref WHERE tagid=8 AND tagtype>0 AND rid=67891;
SELECT pid FROM plink WHERE cid=67891 AND pid NOT IN phantom ORDER BY isprim DESC;
SELECT 1 FROM private WHERE rid=67891;
SELECT count(*) FROM plink
 WHERE pid=67928 AND isprim
   AND coalesce((SELECT value FROM tagxref
                  WHERE tagid=8 AND rid=plink.pid), 'trunk')
      =coalesce((SELECT value FROM tagxref
                  WHERE tagid=8 AND rid=plink.cid), 'trunk')
;
SELECT count(*) FROM plink
 WHERE pid=68011 AND isprim
   AND coalesce((SELECT value FROM tagxref
                  WHERE tagid=8 AND rid=plink.pid), 'trunk')
      =coalesce((SELECT value FROM tagxref
                  WHERE tagid=8 AND rid=plink.cid), 'trunk')
;
SELECT count(*) FROM plink
 WHERE pid=68028 AND isprim
   AND coalesce((SELECT value FROM tagxref
                  WHERE tagid=8 AND rid=plink.pid), 'trunk')
      =coalesce((SELECT value FROM tagxref
                  WHERE tagid=8 AND rid=plink.cid), 'trunk')
;
SELECT value FROM config WHERE name='show-version-diffs';
SELECT value FROM config WHERE name='adunit-omit-if-admin';
SELECT value FROM global_config WHERE name='adunit-omit-if-admin';
SELECT value FROM config WHERE name='adunit-omit-if-user';
SELECT value FROM global_config WHERE name='adunit-omit-if-user';
SELECT value FROM config WHERE name='adunit';
SELECT value FROM global_config WHERE name='adunit';
SELECT value FROM config WHERE name='auto-hyperlink-delay';
SELECT value FROM global_config WHERE name='auto-hyperlink-delay';
SELECT value FROM config WHERE name='footer';
PRAGMA database_list;
PRAGMA database_list;
PRAGMA localdb.freelist_count;
PRAGMA localdb.page_count;
This page was last updated on 2025-05-31 13:08:22Z ]]></content:encoded></item><item><title>I Like GitLab</title><link>https://www.whileforloop.com/en/blog/2026/01/21/i-like-gitlab/</link><author>lukas346</author><category>hn</category><pubDate>Sat, 24 Jan 2026 10:32:26 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Iâ€™ve been using GitLab for my private projects for years now. I just started using it at some point and never had a reason to switch.Back when GitHub still charged for private repositories, GitLab offered them for free. That was the initial hook. I had a bunch of small projects and experiments that I didnâ€™t want to publish but also didnâ€™t want to pay for. GitLab was the obvious choice.GitHub eventually made private repos free too, but by then my workflow was already built around GitLab. All my CI pipelines, Docker images, deployment scripts - everything pointed there.The Docker registry thingEvery GitLab project comes with a Container Registry. This is probably the feature I use most.The workflow is simple. Build an image locally or in CI, push it to the registry, pull it wherever you need it.No separate Docker Hub account. No thinking about pull rate limits (remember when Docker Hub introduced those and broke half the internetâ€™s CI pipelines?). No managing access tokens for yet another service.For my private projects this is perfect. I donâ€™t need the discoverability of Docker Hub. I just need a place to store images that integrates with my existing authentication.The 10GB limit per project sounds small on paper but Iâ€™ve never come close to hitting it. Old tags get cleaned up, base layers are shared, and most of my images arenâ€™t that big anyway.GitLab CI was one of the earlier â€œCI config as codeâ€ implementations that I used. You drop a  in your repo and things start happening. The config is versioned with everything else, which means you can see exactly what your pipeline looked like six months ago.Nothing fancy. Build the image, push it, optionally deploy. The manual trigger on deploy is nice - I can build automatically but still control when things go to production.The shared runners GitLab provides handle most of my workloads. Theyâ€™re not fast, but theyâ€™re free and reliable. For the times when I needed something specific - like a runner with more memory or access to my private network - setting up my own runner on a cheap VPS was straightforward. Install the runner, register it with a token, done.The documentation for CI/CD is extensive. Almost too extensive. There are so many features and options that finding what you need can take a while. But once you figure out your patterns, you mostly copy-paste from your own previous configs.Letâ€™s talk about whatâ€™s not great. The GitLab web interface has always felt sluggish to me. Click on a merge request, wait. Switch to the pipeline view, wait. Open the job log, wait. Itâ€™s not terrible, but thereâ€™s this constant friction that adds up over a long session.Iâ€™ve noticed improvements recently. Either they optimized things or Iâ€™ve just gotten used to it - hard to say. But itâ€™s still not as snappy as GitHub. GitLab tries to be everything. Issue tracking, project management, wiki, snippets, package registry, container registry, security scanning, infrastructure management, monitoringâ€¦ The sidebar menu goes on forever.I use maybe 10% of whatâ€™s available. Repositories, merge requests, CI/CD, container registry. Thatâ€™s pretty much it.This is a double-edged sword. On one hand, feature bloat. On the other, if I ever need something like a private NPM registry or security scanning, itâ€™s already there. I just havenâ€™t needed it yet.I run about a dozen private projects ranging from active side projects to abandoned experiments I keep around â€œjust in case.â€ None of this costs me anything. Itâ€™s crazy.All my private projects live on GitLab. Prototypes, experiments, half-finished ideas, things Iâ€™m actively working on but not ready to share. Itâ€™s my digital workshop where I can make a mess without anyone watching.GitHub is different. Thatâ€™s where I put things I want people to see.This split works well for me. I get the collaboration and visibility benefits of GitHub for public stuff while keeping my private mess organized on GitLab with proper CI/CD and container registries.Some people do everything on GitHub. Some do everything on GitLab. Having a foot in both camps might seem redundant but honestly, they serve different purposes in my workflow.]]></content:encoded></item><item><title>How I estimate work</title><link>https://www.seangoedecke.com/how-i-estimate-work/</link><author>mattjhall</author><category>hn</category><pubDate>Sat, 24 Jan 2026 10:22:32 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Thereâ€™s a kind of polite fiction at the heart of the software industry. It goes something like this:Estimating how long software projects will take is very hard, but not impossible. A skilled engineering team can, with time and effort, learn how long it will take for them to deliver work, which will in turn allow their organization to make good business plans.This is, of course, false. As every experienced software engineer knows, it is not possible to accurately estimate software projects. The tension between this polite fiction and its well-understood falseness causes a lot of strange activity in tech companies.For instance, many engineering teams estimate work in t-shirt sizes instead of time, because it just feels too obviously silly to the engineers in question to give direct time estimates. Naturally, these t-shirt sizes are immediately translated into hours and days when the estimates make their way up the management chain.Alternatively, software engineers who are genuinely trying to give good time estimates have ridiculous heuristics like â€œdouble your initial estimate and add 20%â€œ. This is basically the same as giving up and saying â€œjust estimate everything at a monthâ€.Should tech companies just stop estimating? One of my guiding principles is that when a tech company is doing something silly, theyâ€™re probably doing it for a good reason. In other words, practices that appear to not make sense are often serving some more basic, illegible role in the organization. So what is the actual purpose of estimation, and how can you do it well as a software engineer?Why estimation is impossibleBefore I get into that, I should justify my core assumption a little more. People havewrittena lot about this already, so Iâ€™ll keep it brief.Iâ€™m also going to concede that sometimes you can accurately estimate software work, when that work is very well-understood and very small in scope. For instance, if I know it takes half an hour to deploy a service, and Iâ€™m being asked to update the text in a link, I can accurately estimate the work at something like 45 minutes: five minutes to push the change up, ten minutes to wait for CI, thirty minutes to deploy.For most of us, the majority of software work is not like this. We work on poorly-understood systems and cannot predict exactly what must be done in advance. Most programming in large systems is : identifying prior art, mapping out enough of the system to understand the effects of changes, and so on. Even for fairly small changes, we simply do not know whatâ€™s involved in making the change until we go and look.The pro-estimation dogma says that these questions ought to be answered during the planning process, so that each individual piece of work being discussed is scoped small enough to be accurately estimated. Iâ€™m not impressed by this answer. It seems to me to be a throwback to the bad old days of software architecture, where one architect would map everything out in advance, so that individual programmers simply had to mechanically follow instructions. Nobody does that now, because it doesnâ€™t work: programmers must be empowered to make architectural decisions, because theyâ€™re the ones who are actually in contact with the code. Even if it did work, that would simply shift the impossible-to-estimate part of the process backwards, into the planning meeting (where of course you canâ€™t write or run code, which makes it near-impossible to accurately answer the kind of questions involved).In short: software engineering projects are not dominated by the known work, but by the unknown work, which always takes 90% of the time. However, only the known work can be accurately estimated. Itâ€™s therefore impossible to accurately estimate software projects in advance.Estimates do not come from engineersEstimates do not help engineering teams deliver work more efficiently. Many of the most productive years of my career were spent on teams that did no estimation at all: we were either working on projects that had to be done no matter what, and so didnâ€™t really need an estimate, or on projects that would deliver a constant drip of value as we went, so we could just keep going indefinitely.In a very real sense, estimates arenâ€™t even made by engineers at all. If an engineering team comes up with a long estimate for a project that some VP really wants, they will be pressured into lowering it (or some other, more compliant engineering team will be handed the work). If the estimate on an undesirable project - or a project thatâ€™s intended to â€œhold spaceâ€ for future unplanned work - is too short, the team will often be encouraged to increase it, or their manager will just add a 30% buffer.One exception to this is projects that are technically impossible, or just genuinely prohibitively difficult. If a manager consistently fails to pressure their teams into giving the â€œrightâ€ estimates, that can send a signal up that maybe the work canâ€™t be done after all. Smart VPs and directors will try to avoid taking on technically impossible projects.Another exception to this is areas of the organization that senior leadership doesnâ€™t really care about. In a sleepy backwater, often the formal estimation process does actually get followed to the letter, because thereâ€™s no director or VP who wants to jump in and shape the estimates to their ends. This is one way that some parts of a tech company can have drastically different engineering cultures to other parts. Iâ€™ll let you imagine the consequences when the company is re-orged and these teams are pulled into the spotlight.Estimates are political tools for non-engineers in the organization. They help managers, VPs, directors, and C-staff decide on which projects get funded and which projects get cancelled. Estimates define the work, not the other way aroundThe standard way of thinking about estimates is that you start with a proposed piece of software work, and you then go and figure out how long it will take. This is entirely backwards. Instead, teams will often start with the estimate, and then go and figure out what kind of software work they can do to meet it.Suppose youâ€™re working on a LLM chatbot, and your director wants to implement â€œtalk with a PDFâ€. If you have six months to do the work, you might implement a robust file upload system, some pipeline to chunk and embed the PDF content for semantic search, a way to extract PDF pages as image content to capture formatting and diagrams, and so on. If you have one day to do the work, you will naturally search for simpler approaches: for instance, converting the PDF to text client-side and sticking the entire thing in the LLM context, or offering a plain-text â€œgrep the PDFâ€ tool.This is true at even at the level of individual lines of code. When you have weeks or months until your deadline, you might spend a lot of time thinking airily about how you could refactor the codebase to make your new feature fit in as elegantly as possible. When you have hours, you will typically be laser-focused on finding an approach that will actually work. There are always many different ways to solve software problems. Engineers thus have quite a lot of discretion about how to get it done.So how do I estimate, given all that?I gather as much political context as possible before I even look at the code. How much pressure is on this project? Is it a casual ask, or do we  to find a way to do this? What kind of estimate is my management chain looking for? Thereâ€™s a huge difference between â€œthe CTO  wants this in one weekâ€ and â€œwe were looking for work for your team and this seemed like it could fitâ€.Ideally, I go to the code with an estimate already in hand. Instead of asking myself â€œhow long would it take to do thisâ€, where â€œthisâ€ could be any one of a hundred different software designs, I ask myself â€œwhich approaches could be done in one week?â€œ.I spend more time worrying about unknowns than knowns. As I said above, unknown work always dominates software projects. The more â€œdark forestsâ€ in the codebase this feature has to touch, the higher my estimate will be - or, more concretely, the tighter I need to constrain the set of approaches to the known work.Finally, I go back to my manager with a risk assessment, not with a concrete estimate. I donâ€™t ever say â€œthis is a four-week projectâ€. I say something like â€œI donâ€™t think weâ€™ll get this done in one week, because X Y Z would need to all go right, and at least one of those things is bound to take a lot more work than we expect. Ideally, I go back to my manager with a  of plans, not just one:We tackle X Y Z directly, which  all go smoothly but if it blows out weâ€™ll be here for a monthWe bypass Y and Z entirely, which would introduce these other risks but possibly allow us to hit the deadlineWe bring in help from another team whoâ€™s more familiar with X and Y, so we just have to focus on ZIn other words, I donâ€™t â€œbreak down the work to determine how long it will takeâ€. My management chain already knows how long they want it to take. My job is to figure out the set of software approaches that match that estimate.Sometimes that set is empty: the project is just impossible, no matter how you slice it. In that case, my management chain needs to get together and figure out some way to alter the requirements. But if I always said â€œthis is impossibleâ€, my managers would find someone else to do their estimates. When I do that, Iâ€™m drawing on a well of trust that I build up by making pragmatic estimates the rest of the time.Addressing some objectionsMany engineers find this approach distasteful. One reason is that they donâ€™t like estimating in conditions of uncertainty, so they insist on having all the unknown questions answered in advance. I have written a lot about this in Engineers who wonâ€™t commit and How I provide technical clarity to non-technical leaders, but suffice to say that I think itâ€™s cowardly. If you refuse to estimate, youâ€™re forcing someone less technical to estimate for you.Some engineers think that their job is to constantly push back against engineering management, and that helping their manager find technical compromises is betraying some kind of sacred engineering trust. I wrote about this in Software engineers should be a little bit cynical. If you want to spend your career doing that, thatâ€™s fine, but I personally find it more rewarding to find ways to work with my managers (who have almost exclusively been nice people).Other engineers might say that they rarely feel this kind of pressure from their directors or VPs to alter estimates, and that this is really just the sign of a dysfunctional engineering organization. Maybe! I can only speak for the engineering organizations Iâ€™ve worked in. But my suspicion is that these engineers are really just saying that they work â€œout of the spotlightâ€, where thereâ€™s not much pressure in general and teams can adopt whatever processes they want. Thereâ€™s nothing wrong with that. But I donâ€™t think it qualifies you to give helpful advice to engineers who do feel this kind of pressure.I think software engineering estimation is generally misunderstood.The common view is that a manager proposes some technical project, the team gets together to figure out how long it would take to build, and then the manager makes staffing and planning decisions with that information. In fact, itâ€™s the reverse: a manager comes to the team with an estimate already in hand (though they might not come out and admit it), and then the team must figure out what kind of technical project might be possible within that estimate.This is because estimates are not by or for engineering teams. They are tools used for managers to negotiate with each other about planned work. Very occasionally, when a project is literally impossible, the estimate can serve as a way for the team to communicate that fact upwards. But that requires trust. A team that is always pushing back on estimates will not be believed when they do encounter a genuinely impossible proposal.When I estimate, I extract the range my manager is looking for, and only then do I go through the code and figure out what can be done in that time. I never come back with a flat â€œtwo weeksâ€ figure. Instead, I come back with a range of possibilities, each with their own risks, and let my manager make that tradeoff.It is not possible to accurately estimate software work. Software projects spend most of their time grappling with unknown problems, which by definition canâ€™t be estimated in advance. To estimate well, you must therefore basically ignore all the known aspects of the work, and instead try and make educated guesses about how many unknowns there are, and how scary each unknown is.edit: I should thank one of my readers, Karthik, who emailed me to ask about estimates, thus revealing to me that I had many more opinions than I thought.edit: This post got a bunch of comments on Hacker News. Some non-engineers made the point that well-paid professionals should be expected to estimate their work, even if the estimate is completely fictional. Sure, I agree, as long as weâ€™re on the same page that itâ€™s fictional!A couple of engineersargued that estimation was a solved problem. Iâ€™m not convinced by their examples. I agree you can probably estimate â€œbuild a user flow in Svelteâ€, but itâ€™s much harder to estimate â€œbuild a user flow in Svelte on top of an existing large codebaseâ€. I should have been more clear in the post that I think thatâ€™s the hard part, for the normal reasons that itâ€™s very hard to work in large codebases, which I writeaboutendlessly on this blog.edit: There are also some comments on Lobste.rs, including a good note that the capability of the team obviously has a huge impact on any estimates. In my experience, this is not commonly understood: companies expect estimates to be fungible between engineers or teams, when in fact some engineers and teams can deliver work ten times more quickly (and others cannot deliver work , no matter how much time they have).]]></content:encoded></item><item><title>Doing gigabit Ethernet over my British phone wires</title><link>https://thehftguy.com/2026/01/22/doing-gigabit-ethernet-over-my-british-phone-wires/</link><author>user5994461</author><category>hn</category><pubDate>Sat, 24 Jan 2026 10:14:31 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[: None of this is written by AI, Iâ€™m still a real person writing my own blog like its 1999I finally figured out how to do Gigabit Ethernet over my existing phone wires.Powerline adapter and miseryIâ€™ve mostly lived with powerline adapters over recent years.Â Some worked well, some did not (try few and return what doesnâ€™t work in your home). One I had for a while gave me stable 30 Mbps, which was little but good enough for internet at the time. I care very much about having stable low latency for gaming, more than bandwidth.Fast forward to my current situation, that powerline adapter regularly lost connection which was a major problem. I got some new ones with the latest and greatest G.hn 2400 standard. The final contender served around 180 Mbps to my office (with high variance 120 to 280 Mbps), or around 80 Mbps to the top floor. Itâ€™s good enough to watch YouTube/TV yet itâ€™s far from impressive.One peculiar thing from the UK: Internet providers donâ€™t truly offer gigabit internet. They have a range of deals like 30 Mbps â€“ 75 Mbps â€“ 150 Mbps â€“ 300 Mbps â€“ 500 Mbps â€“ 900 Mbps, each one costing a few more pounds per month than the last. This makes the UK simultaneously one of the cheapest and one of the most expensive countries to get Internet. Long story short, new place, new hardware, new deals, the internet has been running at 500 Mbps for some time now.Problem: How to get 500 Mbps to my room?A Fetish for Phone SocketsIâ€™ve been looking for a way to reuse phone wires for a while, because British houses are full of phone sockets. There are 2 sockets in my office room.I canâ€™t stress enough how much we love our phone sockets. Itâ€™s not uncommon to have a one bed flat with 2 phone sockets in the living room and 2 phone sockets in the bedroom and a master socket in the technical room. Itâ€™s ridiculous.A new house bought today could have 10 phone sockets and 0 Ethernet sockets. There is still no regulation that requires new build to get Ethernet wiring (as far as I know).Thereâ€™s got to be a way to use the existing phone infrastructure.I know the technology exists. Itâ€™s one of the rare cases where the technology exists and is mature, but nobody can be bothered to make products for it.The standards that run powerline adapters (HomePlug AV200, AV500, G.hn 2400) can work with any pair of wires. It should work ten times better on dedicated phone wires instead of noisy power wires, if only manufacturers could be bothered to pull their fingers out of their arse and make the products that are needed.Itâ€™s made and shipped from Germany.I was lazy so I ordered online in self-service (which is definitely the wrong way to go about it). Itâ€™s available on Ebay DE and Amazon DE, itâ€™s possible to order from either with a UK account, make sure to enter a UK address for delivery (some items donâ€™t allow it).The better approach is almost certainly to speak to the seller to get a quote, with international shipping and the import invoice excluding VAT (to avoid paying VAT on VAT).The package got the usual Royal Mail treatment:The package was shipped by DHL GermanyThe package was transferred to Royal Mail when entering the UKAfter some days, the DHL website said they tried to deliver but nobody home, this is bullshitRoyal website said the package reached the depot and was awaiting delivery, this is bullshitIn reality, the package was stuck at the border, as usualGoogle to find â€œwebsite to pay import fee on parcelâ€Entered the DHL tracking number into the Royal Mail form for a Royal Mail tracking numberThe website said that the parcel had import fees to pay, this is correctPaid the fee online, 20% VAT + a few pounds of handling feesThe package will be scheduled for delivery a few days laterRoyal Mail and DHL updated their status another two or three times with false informationRoyal Mail delivered a letter saying there was a package waiting on fees, though it was paidThe package finally arrivedBasically, you need to follow the tracking regularly until the package is tagged as lost or failed delivery, which is the cue to pay import fees.Itâ€™s the normal procedure to buy things from Europe since Brexit 2020. Itâ€™s actually quite shocking that Royal Mail still hasnâ€™t updated their tracking system to be able to give a status â€œwaiting on import fees to be paid onlineâ€. They had 6 years!A pair of gigacopper G4201TMThe device has a German power socket (expected)It came with a German to UK power adapter (unexpected and useful)It came with a standard RJ11 cable (expected and useless)Found BT631A to RJ11 cables online (the standard UK phone socket)Found Ethernet cables in my toolbox3M removable hanging strip to stick to the wall, the device is very lightThere is a gigacopper G4202TM: with an RJ45 to connect to the phone line instead of a RJ11 (not sure if itâ€™s a newer model or just a variant, as that one has two gigabit Ethernet ports). Donâ€™t be confused by having a RJ45 port that is not a RJ45 port.There is a gigacopper G4201C (1 port) and G4204C (4 port) for Ethernet over coaxial. Some countries have coax in every room for TV/satellite. This may be of interest to some readers.Reminder, this is a 500 Mbps internet connection.I discovered soon afterwards that I bought the wrong item. There is an InHome and a Client/Server variant of the product. Make sure to buy the InHome variant.The InHome variant can have up to 16 devices, communicating to any peer on the medium, with sub millisecond latency.The client-server variant is preconfigured as a pair, splitting the bandwidth 70% download / 30% upload, with few milliseconds latency. I think itâ€™s a use case for ISP and long range connections.Thankfully the difference is only the firmware. I spoke to the vendor who was very helpful and responsive. They sent me the firmware and the tools to patch.I have a fetish for low latency. This screenshot is oddly satisfying.The web interface says 1713 Mbps on the physical layer, the debugging tool says PHONE 200MHz â€“ Connected 1385 Mbps.I wanted to verify whether the device can do a full Gigabit. Unfortunately I realized I donâ€™t have any device that can test that.Phones are wireless, which is too slow to test anything. I checked out of curiosity, my phone did 100 Mbps to 400 Mbps right next to the router. Grabbed two laptops only to realize they didnâ€™t have any Ethernet port. I dug up an old laptop from storage with an Ethernet port. The laptop couldnâ€™t boot, the CPU fan didnâ€™t start and the laptop refused to boot with a dead fan.There is a hard lesson here: 1 Gbps ought to be enough for any home. Using the phone line is as good as having Ethernet wiring through the house if it can deliver a (shared) 1.7 Gbps link to multiple rooms.Still, I really wanted to verify that the device can do a full Gbps, I procured an USB-C to Ethernet adapter.Full speed achieved, testing from a phone to a computer with iperf3.Some readers might wonder about the wiring.I didnâ€™t check the wiring before buying anything because itâ€™s pointless. British sockets are always daisy chained in an incomprehensible maze.Phone sockets need 2 wires and can be daisy chained. Ethernet sockets need 8 wires. They often use the same Cat5 cable because itâ€™s the most widely available (8 wires cable, the 6 extra wires can remain unconnected). Itâ€™s possible to swap the phone socket for an RJ45 socket, if you only have 2 sockets connected with the right cable. Itâ€™s not possible when sockets are daisy chained. (You could put a double or triple RJ45 socket with a switch to break a daisy chain, but it quickly becomes impractical in a British house with 5 to 10 sockets in an arbitrary layout.)I opened one socket in the office room. There are two Cat5 cables daisy chained. There are 3 wires connected.Itâ€™s probably daisy chained with the other socket in the room, or itâ€™s daisy chained with the socket in the other room thatâ€™s closer. Who knows.I opened the BT master socket in the technical room. It should have the cables coming from the other rooms. It should connect the internal phone wires with the external phone line.There is one single Cat5 cable. There are 4 wires connected. Itâ€™s definitely not a master socket. WTF?!Itâ€™s interesting that this socket has 4 wires connected but the socket in the office has 3 wires connected. The idiot who did the wiring was inconsistent. The gigacopper device can operate over 2 wires (200 MHz Phone SISO) or over 4 wires (100 MHz Phone MIMO). I can try the other modes if I finish the job.The search for the master socket continues. The cables from the other floors should all be coming down somewhere around here. There is a blank plate next to it (right). This might be the external phone line? A bunch of wires are crimped together, colours do not match. Itâ€™s the hell of a mess.Only sure thing, they are different cables because they are different colours. They might be going to a junction box somewhere else. Probably behind a wall thatâ€™s impossible to access!Conclusion: There is zero chance to get proper Ethernet wiring out of this mess.The gigacopper device to do gigabit Ethernet over phone line is a miracle!There is an enormous untapped market for gigabit Ethernet over phone sockets in the UK.]]></content:encoded></item><item><title>80386 Multiplication and Division</title><link>https://nand2mario.github.io/posts/2026/80386_multiplication_and_division/</link><author>nand2mario</author><category>hn</category><pubDate>Sat, 24 Jan 2026 06:11:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>â€œLet people helpâ€ â€“ Advice that made a big difference to a grieving widow</title><link>https://www.npr.org/2026/01/20/nx-s1-5683170/let-them-the-small-bit-of-advice-that-made-a-big-difference-to-a-grieving-widow</link><author>NaOH</author><category>hn</category><pubDate>Sat, 24 Jan 2026 03:20:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
                Connie Sherburne and her husband, Peter Bierle, in 2016.
                In 2020, Connie Sherburne's husband of 31 years, Peter, died in a crash while piloting their small plane.Sherburne, a practical person, immediately focused on what needed to be done."I went home that evening, and the next morning, I got up and I thought, 'Hit the ground running. You've got all this to take care of.'"She drove to their insurance company in Escondido, Calif., to transfer the insurance for Peter's truck into her name. Because of the pandemic, the office was staffed by just one person. The woman opened the door to let Sherburne inside, and they sat across from each other at a desk."It didn't take her but a few minutes to take care of why I was there," Sherburne recalled."When she got done, she stopped and she looked at me across her desk, and she made sure that I was looking at her â€” that she had my full attention.""And she said, 'OK, so now that we've finished with that, people are going to stop and ask you, "How can I help?"' And then she gave this pregnant pause, and she said, 'Let them.'"Sherburne took in the advice, absorbing the moment. She wasn't the kind of person to reach out to others for help."But because she said it with such force, it really, really made sense to me," Sherburne said.As soon as she got home, she realized that she did need help. The firewood she used to heat her home was running low, and winter was around the corner. Chopping wood was something Peter had always done. But this time, she asked a friend to take care of it."So many, many people did little things and big things to help me," Sherburne said. "One of the neighbors actually cooked for me for four years â€” dinners â€” and her husband delivered the dinners to me."Sherburne says she would have never reached out for support if not for the advice of the woman at the insurance company."In the back of my mind, I kept hearing her voice, you know, 'Let them.'"A few years later, Sherburne went back to the insurance office to tell the woman how deeply her words had affected her. But the agent didn't work there anymore."So I just wanted to tell her how much that meant to me," Sherburne said."It was such a little thing for this woman just to say that to me. But she didn't realize what a huge thing it was going to be to help me through all this."My Unsung Hero is also a podcast â€” new episodes are released every Tuesday. To share the story of your unsung hero with the Hidden Brain team, record a voice memo on your phone and send it to myunsunghero@hiddenbrain.org.]]></content:encoded></item><item><title>SEC obtains final consent judgments against former FTX and Alameda executives</title><link>https://www.sec.gov/enforcement-litigation/litigation-releases/lr-26450</link><author>sizzle</author><category>hn</category><pubDate>Sat, 24 Jan 2026 02:59:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Comma openpilot â€“ Open source driver-assistance</title><link>https://comma.ai/</link><author>JumpCrisscross</author><category>hn</category><pubDate>Sat, 24 Jan 2026 01:00:50 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TikTok Is Now Collecting More Data About Its Users</title><link>https://www.wired.com/story/tiktok-new-privacy-policy/</link><author>coloneltcb</author><category>hn</category><pubDate>Fri, 23 Jan 2026 22:51:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ in the US opened the app today, they were greeted with a pop-up asking them to agree to the social media platformâ€™s new terms of service and privacy policy before they could resume scrolling.These changes are part of TikTokâ€™s transition to new ownership. In order to continue operating in the US, TikTok was compelled by the US government to transition from Chinese control to a new, American-majority corporate entity. Called TikTok USDS Joint Venture LLC, the new entity is made up of a group of investors that includes the software company Oracle.It's easy to tap Agree and keep on scrolling through videos on TikTok, so users might not fully understand the extent of changes they are agreeing to with this pop-up.Now that itâ€™s under US-based ownership, TikTok potentially collects more detailed information about its users, including precise location data. A spokesperson for TikTok USDS declined to comment.TikTok Adds Precise Location TrackingTikTokâ€™s change in location tracking is one of the most notable updates in this new privacy policy. Before this update, the app did not collect the precise, GPS-derived location data of US users. Now, if you give TikTok permission to use your phoneâ€™s location services, then the app may collect granular information about your exact whereabouts. Similar kinds of precise location data is also tracked by other social media apps, like Instagram and X.We collect information about your approximate location, including location information based on your SIM card and/or IP address. In addition, we collect location information (such as tourist attractions, shops, or other points of interest) if you choose to add the location information to your User Content. Current versions of the app do not collect precise or approximate GPS information from US users.We automatically collect certain information from you when you use the Services, including ... location information about your approximate location based on your device and network information, such as SIM card region, IP address, and device system settings. We also collect information, such as tourist attractions, shops, or other points of interest, if you choose to add the location to your user content. Also, if you choose to enable location services for the TikTok app within your device settings, we collect approximate or precise location information from your device.TikTok Now Tracks AI InteractionsRather than an adjustment, TikTokâ€™s policy on AI interactions adds a new topic to the privacy policy document. Now, users' interactions with any of TikTokâ€™s AI tools explicitly fall under data that the service may collect and store. This includes any prompts as well as the AI-generated outputs. The metadata attached to your interactions with AI tools may also be automatically logged.(These AI interactions are not explicitly mentioned in the past policy.)When you create an account, upload content, contact us directly, or otherwise use the Services, you may provide some or all of the following information â€¦ AI interactions, including prompts, questions, files, and other types of information that you submit to our AI-powered interfaces, as well as the responses they generate.We automatically collect certain information from you when you use the Services, including â€¦ metadata that is automatically uploaded in connection with your user content, messages, or AI interactions, such as how, when, where, and by whom the user content was created, or message or prompt was sent. Metadata may also include information, such as your username, that enables your user content to be traced back to your account by other users.TikTok Expands Its Ads NetworkThis change to TikTokâ€™s privacy policy may not be as immediately noticeable to users, but it will likely have an impact on the types of ads you see outside of TikTok. So, rather than just using your collected data to target you while using the app, TikTok may now further leverage that info to serve you more relevant ads wherever you go online. As part of this advertising change, TikTok also now explicitly mentions publishers as one kind of partner the platform works with to get new data.]]></content:encoded></item><item><title>Mental Models (2018)</title><link>https://fs.blog/mental-models/</link><author>hahahacorn</author><category>hn</category><pubDate>Fri, 23 Jan 2026 21:08:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A mental model is a simplified explanation of how something works. Any idea, belief, or concept can be boiled down to its essence. Like a map, mental models highlight key information while ignoring irrelevant details. Theyâ€™re tools for compressing complexity into manageable chunks.Mental models help us understand the world. For example, velocity shows that both speed and direction matter. Reciprocity reveals how being positive and taking initiative gets the world to do most of the work for you. Margin of Safety reminds us that things donâ€™t always go as planned. Relativity exposes our blind spots and shows how a different perspective can reveal new information. These are just a few examples.If you want to be a good thinker, you must develop a mind that can jump the jurisdictional boundaries. You donâ€™t have to know it all. Just take in the best big ideas from all these disciplines. And itâ€™s not that hard to do.The Big Ideas From The Big DisciplinesThis page summarizes the big ideas to help you make better decisions, avoid problems, and spot opportunities others miss.The map is not the territory reminds us that our mental models of the world are not the same as the world itself. It cautions against confusing our abstractions and representations with the complex, ever-Â­shifting reality they aim to describe.It is dangerous to mistake the map for the territory. Consider the person with an outstanding rÃ©sumÃ© who checks all the boxes on paper but canâ€™t do the job. Updating our maps is a difficult process of reconciling what we want to be true with what is true.In many areas of life, we are offered maps by other people. We are reliant on the maps provided by experts, pundits, and teachers. In these cases, the best we can do is to choose our mapmakers wisely and to seek out those who are rigorous, transparent, and open to revision.Ultimately, the map/territory distinction invites us to engage with the world as it is, not just as we imagine it. And remember, when you donâ€™t make the map, choose your cartographer wisely.The first rule of competition is that you are more likely to win if you play where you have an advantage. Playing to your advantage requires a firm understanding of what you know and donâ€™t know. Your circle of competence is your personal sphere of expertise, where your knowledge and skills are concentrated. Itâ€™s the domain where you have a deep understanding, where your judgments are reliable, and your decisions are sound.Â The size of your circle isnâ€™t as important as knowing the boundaries. The wise person knows the limits of their knowledge and can confidently say, â€œThis falls within my circle,â€ or â€œThis is outside my area of expertise.â€Â While operating within your circle of competence is a recipe for confidence and effectiveness, venturing outside your circle of competence is a recipe for trouble. Youâ€™re like a sailor navigating unfamiliar waters without a map, at the mercy of currents and storms you donâ€™t fully understand. This isnâ€™t to say that you should never venture outside your circle. Learning new things, gaining new skills, and mastering new domains is one of the most beautiful things about life.Â Celebrate your expertise, but also acknowledge your limitations.First principles thinking is the art of breaking down complex problems into their fundamental truths. Itâ€™s a way of thinking that goes beyond the surface and allows us to see things from a new perspective. Thinking in first principles allows us to identify the root causes, strip away the layers of complexity, and focus on the most effective solutions. Reasoning from first principles allows us to step outside the way things have always been done and instead see what is possible. First principles thinking is not easy. It requires a willingness to challenge the status quo. This is why itâ€™s often the domain of rebels and disrupters who believe there must be a better way. Itâ€™s the thinking of those willing to start from scratch and build from the ground up.In a world focused on incremental improvement, first principles thinking offers a competitive advantage because almost no one does it.Thought experiments are the sandbox of the mind, the place where we can play with ideas without constraints. Theyâ€™re a way of exploring the implications of our theories, of testing the boundaries of our understanding. They offer a powerful tool for clarifying our thinking, revealing hidden assumptions, and showing us unintended consequences. The power of thought experiments lies in their ability to create a simplified model of reality where we can test our ideas. In the real world, confounding factors and messy details obscure the core principles at work. Thought experiments allow us to strip away the noise and focus on the essence of the problem. Thought experiments remind us that some of the most profound insights and innovations start with a simple question: What if?Second-Â­order thinking is a method of thinking that goes beyond the surface level, beyond the knee-Â­jerk reactions and short-Â­term gains. It asks us to play the long game, to anticipate the ripple effects of our actions, and to make choices that will benefit us not just today but in the months and years to come. Second-order thinking demands we ask: And then what? Think of a chess master contemplating her next move. She doesnâ€™t just consider how the move will affect the next turn but how it will shape the entire game. Sheâ€™s thinking many steps ahead. Sheâ€™s considering her own strategy and her opponentâ€™s likely response. In our daily lives, weâ€™re often driven by first-Â­order thinking. We make decisions based on what makes us happy now, what eases our current discomfort, or satisfies our immediate desires. Second-Â­order thinking asks us to consider the long-Â­term implications of our choices to make decisions based not just on what feels good now but on what will lead to the best outcomes over time. In the end, second-Â­order thinking is about playing the long game. Itâ€™s about making choices for the next move and the entire journey.Probabilistic thinking is the art of navigating uncertainty. Successfully thinking in shades of probability means roughly identifying what matters, calculating the odds, checking our assumptions, and then deciding. The challenge of probabilistic thinking is that it requires constant updating. As new information emerges, the probabilities change. What seemed likely yesterday may seem unlikely today. This explains why probabilistic thinkers always revise their beliefs with new data and why itâ€™s uncomfortable for many people. Itâ€™s much easier to believe something false is accurate than to deal with the fact that we might be wrong. Being a probabilistic thinker means being willing to say, â€œI donâ€™t know for sure, but based on the evidence, I think thereâ€™s a 63 percent chance of X.â€ The rewards of probabilistic thinking are immense. By embracing uncertainty, we can make better decisions, avoid the pitfalls of overconfidence, and navigate complex situations with greater skill and flexibility. We can be more open-Â­ minded, more receptive to new ideas, and more resilient in the face of change.Much of success comes from simply avoiding common paths to failure.Inversion is not the way we are taught to think. We are taught to identify what we want and explore things that will move us closer to our objective. However, avoiding things that ensure we donâ€™t get what we want dramatically increases our odds of success.We can get fixated on solving problems one way, missing simpler solutions. Inversion breaks us out of this tunnel vision.Instead of â€œHow do I solve this?â€, inversion asks, â€œWhat would guarantee failure?â€ Rather than â€œHow can I achieve this?â€, it asks â€œWhatâ€™s preventing me from achieving it?â€ This flip reveals insights our usual thinking overlooks.When facing a tricky problem or ambitious goal, try inverting. Ask how youâ€™d guarantee failure. The answers may surprise youâ€”and unlock new solutions.Occamâ€™s razor is the intellectual equivalent of â€œkeep it simple.â€ When faced with competing explanations or solutions, Occamâ€™s razor suggests that the correct explanation is most likely the simplest one, the one that makes the fewest assumptions. This doesnâ€™t mean the simplest theory is always true, only that it should be preferred until proven otherwise. Sometimes, the truth is complex, and the simplest explanation doesnâ€™t account for all the facts. The key to wielding this model is understanding when it works for you and against you.A theory that is too simple fails to capture reality, and one that is too complex collapses under its own weight.Hanlonâ€™s razor is a mental safeguard against the temptation to label behavior as malicious when incompetence is the most common response. Itâ€™s a reminder that people are not out to get you, and itâ€™s best to assume good faith and resist the urge to assign sinister motives without overwhelming evidence. This isnâ€™t to say that genuine malice doesnâ€™t exist. Of course, it does. But in most interactions, stupidity is a far more common explanation than malevolence. People make mistakes. They forget things. They speak without thinking. They prioritize short-Â­term wins over long-term wins. They act on incomplete information. They fall prey to bias and prejudice. These actions might appear like deliberate attacks from the outside, but the reality is far more mundane. Hanlonâ€™s razorâ€™s real power lies in how it shifts our perspective. When we assume stupidity rather than malice, we respond differently. Instead of getting defensive or lashing out, we approach the situation with empathy and clarity. For most daily frustrations and confusion, Hanlonâ€™s razor is a powerful reminder to approach problems with a spirit of generosity. Itâ€™s a way to reduce drama and stress and find practical solutions instead of descending into blame and escalation.The Mental Models of Physics, Chemistry, and BiologyRelativity is the idea that our perceptions and judgments are not absolute but are shaped by our unique vantage points and frames of reference. Itâ€™s the understanding that our experiences are subjective. We each inhabit a particular web of experiences. This context shapes how we see the world, what we notice and overlook, and what we value and dismiss. Two people can look at the same event and come away with vastly different interpretations based on their unique frames of reference. Consider two people standing in the same room: They each experience the same absolute temperature differently. One can feel hot while the other feels cold, even though the temperature is the same. Similarly, consider political debates: Our beliefs are shaped by our unique experiences and social contexts. A policy that seems like common sense to an urban progressive might feel like complete nonsense to a rural conservative, and vice versa. In this way, understanding relativity is key to fostering empathy and finding common ground. However, relativity is not the same as relativismâ€”Â­ the idea that all perspectives are equally valid. Recognizing the relativity of our perceptions doesnâ€™t mean we donâ€™t have to make judgments about validity. Instead, itâ€™s a call to examine our assumptions, seek out diverse perspectives, and expand our frames of reference. We all have blind spotsâ€”Â­things we cannot see. Understanding that our perceptions are relative allows us to open ourselves to other ways of seeing. If youâ€™re wondering where to get started, try asking others what they see that you canâ€™t. Apply your judgment to their responses and update your beliefs accordingly.Reciprocity underlies everything from basic human kindness to the most complex systems of trade. At its core, reciprocity is the simple idea of treating others as they treat usâ€”giving what we get. But from this simple principle grows a vast web of social interactions and expectations that shapes nearly every aspect of our lives. Many people expect the world to just hand them things without effort. This is a poor strategy because it doesnâ€™t align with the human behavior you can observe around you every day. Reciprocation teaches us that you are likely to receive the same if you give people cynicism and curtness or nothing at all. But if you give people an opportunity and the benefit of the doubt, you will, more often than not, be on the receiving end of the same behavior. Become what you want to see in the world, and the world will return it to you. If you want an amazing relationship with your partner, be an amazing partner. If you want people to be thoughtful and kind to you, be thoughtful and kind to them. If you want people to listen to you, listen to them. The best way to achieve success is to deserve success. Small changes in your actions change your entire world. One of the biggest misperceptions about reciprocity is that people should sit around waiting for others to go first rather than unlocking the power of reciprocity in their favor by going positive and going first without expectation. Reciprocity reminds us that our actions tend to come back on us. Itâ€™s an essential reminder that we are part of the world, and thus, our actions do not happen in isolation but are instead part of an interconnected web of effects.Thermodynamics is the science of energy, heat, and work. Itâ€™s the set of physical laws that govern how energy moves and changes in the universe. Chances are, when you first came across the subject, it was dry, full of equations and abstract concepts. But the truth is thermodynamics is a useful intellectual framework for daily life. Not only can it reveal why your room gets messier over time, but it also explains why you should choose your friends wisely. The first law of thermodynamics states that energy can neither be created nor destroyed, only transformed from one form to another. This means that every joule of energy in the universe, every bit of heat and work and motion is part of an unbroken chain stretching back to the Big Bang. When you hop on a flight that burns jet fuel, youâ€™re tapping into energy captured by plants millions of years ago and stored in chemical bonds until it was transformed into heat and motion. But while energy is conserved, itâ€™s not always useful. Thatâ€™s where the second law of thermodynamics comes in. It states that entropyâ€”Â­ a measure of disorderâ€”Â­ increases over time in any closed system. In other words, left on its own, the universe tends toward chaos. Your bedroom doesnâ€™t clean itselfâ€”Â­ it takes energy and effort to maintain order. Stars burn out, structures crumble, and ice melts into water.Entropy is the universeâ€™s tax on time. The constant battle against entropy is the driving force behind much of what we do. The constant struggle between order and disorder is the source of change and progress. While engineers and scientists use thermodynamics to design engines or calculate the energy requirements of a system, we can use it as a framework for understanding the deep interconnectedness of everything. When you feel the sunâ€™s warmth on your skin, youâ€™re experiencing the result of a thermodynamic process that began in the heart of a star ninety-three million miles away. When you watch a campfire burn down to embers, youâ€™re witnessing the inexorable march of entropy in real-time.Thermodynamics is the story of energy across time. Weâ€™re part of an energy story that stretches back to the dawn of time and reaches the farthest pockets of space. We can marvel that in a universe ruled by disorder, pockets of temporary order can emerge, whether itâ€™s a clean room, a planet, or a civilization. By understanding thermodynamics, we gain not just a technical toolbox but an appreciation for the beauty, complexity, and fragility of our very existence.Inertia is the stubborn resistance of the universe to change. Itâ€™s why objects at rest tend to stay at rest, and objects in motion tend to stay in motion. You can think of inertia as the guardian of the status quo.At its core, inertia is a property of mass. The more massive an object is, the more it resists changes to its state of motion. A feather, with its tiny mass, is easily blown about by the slightest breeze. A boulder, on the other hand, requires a powerful force to get it moving. This is why it takes more effort to push a heavy cart than a light one, more energy to launch a rocket than to toss a ball. But inertia isnâ€™t just a physical phenomenon. Itâ€™s an illuminating lens to see habits, beliefs, and our resistance to change. The longer weâ€™ve held them, the larger the mass and the more force required to change them. The path of least resistance is always the status quo. Getting started is the hardest part. Once something moves in a direction, keeping it in motion is much easier. But once something is in motion, itâ€™s hard to stop. This is why most self-Â­help books about positive habits break things down into very small stepsâ€”Â­to reduce the force required to overcome the status quo. For example, if you want to get in the habit of doing push-Â­ups daily, start with one rather than fifty. If you want to start a flossing habit, start with one tooth. After all, the bigger the massâ€”Â­in this case, the gap between where you are and where you want to beâ€”Â­ the more effort required.Inertia is both a challenge and an opportunity. Successful companies struggle with the inertia of their success and the resistance to change that comes with size, complexity, and entrenched interests. On the other hand, startups can leverage their lack of inertiaâ€”Â­their agility, their willingness to pivot and adaptâ€”Â­as a competitive advantage. Momentum and inertia are closely related. While inertia is the tendency to resist change, momentum is the oomph an object has when itâ€™s moving. The more momentum something has, the harder it is to stop or redirect. The key is to pick the right direction and build momentum so inertia works to your advantage and carries you forward. This is the essence of the â€œflywheelâ€ concept in businessâ€”success breeds success, and small wins compound into big gains.When youâ€™re fighting the status quo, remember the physics at play. Resistance is natural. Understand that building momentum in a new direction takes a sustained force. While the universe resists change, it always rewards those who dare to overcome that resistance.Friction and viscosity are the sand in the gears of the universe, the invisible hands that slow the motion of all things. Friction is the grip between surfaces in contact, the roughness that resists sliding. Viscosity is the thickness of fluids, the internal friction that makes liquids sluggish and syrupy. Together, they are the great moderators of motion. Think of the last time you tried to slide a heavy piece of furniture across the floor. The resistance you felt, the effort required to overcome the grip of the surfaceâ€”Â­ that was friction at work. Or consider the slow, thick pour of honey from a jar, the way it clings and drips in slow threads. Thatâ€™s the viscosity of the fluid resisting the force of gravity, the internal friction that makes the honey flow like molasses rather than water. While friction is the enemy of efficiency, itâ€™s also necessary for traction. We couldnâ€™t walk, hold tools, or tie knots without it. Viscosity, too, is a double-Â­edged sword. In pipelines and hydraulic systems, high viscosity means higher pumping costs, slower flows, and greater strain on equipment. But viscosity also makes oil a good lubricant, allowing paints and coatings to spread evenly and adhere to surfaces.Friction and viscosity are powerful metaphors for the forces of resistance in every domain of life. In human relationships, friction is the conflict and tension that arises from differing goals, personalities, or beliefs. The interpersonal roughness can generate heat and wear, but also the traction that allows us to influence and connect with others.While often hidden, friction and viscosity work against us whenever we try to do something. We often default to using more force to overcome resistance when simply reducing the friction or viscosity will do. However, doing both is more effective than either in isolation. Friction and viscosity can also be wielded as weapons. Rather than try to catch up to the competition with more effort, you might want to explore slowing them down by adding resistance through increased regulation, bureaucracy, or other clever ideas. In the end, reducing resistance is often easier than adding force.Velocity is the great differentiator, distinguishing the stagnant from the swift. In physics, velocity is a fundamental quantity, a key variable in the equations that describe the behavior of everything from subatomic particles to galaxies. Itâ€™s the v in the formulas of motion, the arrow that points the way from here to there. Velocity is also a metaphor for life. Consider it the rate at which we learn and grow, the speed at which we innovate and create, and the focus with which we pursue our goals. Velocity challenges us to think about what we can do to put ourselves on the right trajectory and to find a balance between mass and speed to move toward our goals. The ability to set a direction, improve your tactics, and adjust to new information becomes paramount. Velocity isnâ€™t just about raw speed. Direction matters just as much (if not more). A car moving at high speed in circles goes nowhere, while a slow and steady walk in a straight line can cross continents. Velocity is progress. Sometimes, progress comes from more force, and sometimes, progress comes from removing friction. Once you have a destination, you can improve your velocity by working harder and eliminating things that arenâ€™t contributing toward reaching that goal.Leverage is the force multiplier of the world, the principle that allows the small to move the large and the few to influence the many. Itâ€™s the idea that a little force, strategically applied, can yield outsize outputs.At its core, leverage is amplification. Think of a crowbar pryingÂ two boards apart or a pulley system hoisting a heavy load. In each case, the appled force is multiplied.Â But leverage isnâ€™t just useful in physics. Rather, itâ€™s a principle that applies across our lives.Leverage is often lurking in the background of nonlinear outcomes. Consider the author who took the ideas in their head, put them in a book, and sold millions of copies, or the Wall Street investor who made a single decision that resulted in billions. Or even the CEO who directs the people working for them. All of these examples are leverage in action.In personal development, leverage is about identifying the key habits, skills, and relationships that will impact your life and work most. Itâ€™s about focusing your energy on the critical few rather than the trivial many, about finding the points of maximum leverage where small changes can cascade into massive results.An example of personal leverage is an employee who learns to use AI to amplify their impact on the organization far beyond their experience or effort. While labor is still a form of leverage, it can often be done with silicon chips. In this sense, the person who can leverage technology can compete in a way never imaginable.However, leverage is not without its risks and responsibilities. Just as a small action can have an outsized positive impact, so can it have negative consequences. If you borrow too much money against your house and it turns out to be less valuable than assumed or interest rates change, the downside of leverage can quickly wipe you out.Good ideas taken too far often cause unanticipated consequences. Wielding leverage to maximum effect all the time, as the West Virginia mine owners did, sows the seeds of ongoing unrest that undermines oneâ€™s ability to be truly effective. No one wants to feel exploited, and those who are never give their loyalty or their best work.The key is to use leverage wisely and judiciously by understanding the systems you want to influence and considering the second- and third-Â­ order effects of your actions.Leverage is a tool, not a toy, and like any tool, it requires skill, judgment, and respect.Activation energy is the spark that ignites the fire of change, the initial burst of effort required to kick-Â­ start a reaction or transformation. Itâ€™s the metaphorical push that gets the boulder rolling down the hill, the investment of energy needed to overcome inertia and set a process in motion. In chemistry, activation energy is the minimum energy that must be input for a reaction. Itâ€™s the hurdle molecules must overcome to break their bonds and form new ones, the energetic barrier separating the reactants from the products. But activation energy isnâ€™t just a chemical concept. Itâ€™s a principle that applies to any system where change is possible but not automatic. In personal growth, activation energy is the effort required to break old habits and form new ones. In innovation, itâ€™s the investment needed to turn an idea into reality. The key is recognizing activation energy for what it is: a necessary upfront cost, not a permanent obstacle. Once things are moving, momentum takes over. Once the reaction starts, it becomes self-Â­ sustaining.Catalysts are the unsung heroes of chemical reactions, the silent partners accelerating change. By decreasing the time required to cause change, they also make reactions possible that might not have occurred otherwise. In chemistry, a catalyst is a substance that increases the reaction rate without permanently altering itself. But catalysts arenâ€™t just chemical curiosities, theyâ€™re a powerful metaphor for the forces that drive change and growth.In business, a catalyst might be a new technology that opens fresh possibilities or a visionary leader who inspires a team to new heights. In your personal life, a catalyst could be a life-Â­changing book, a transformative experience, or a mentor who sees your potential and helps you realize it. Of course, while we benefit from others acting as our catalysts, we can be catalysts ourselvesâ€”helping others find the activation energy they need to thrive.Alloying is the art of mixing elements to create something greater than the sum of its parts. While our intuition tells us that pure substances are best, alloying shows this is not always true. One plus one can equal ten. By blending ingredients in precise proportions, metallurgists can create materials with bespoke propertiesâ€”the lightness of aluminum with the strength of steel, the corrosion resistance of chromium with the affordability of iron.But alloying isnâ€™t just about physical properties. Itâ€™s a metaphor for the power of diversity and combination in all walks of life. In teams, alloying is the mixing of different skills, perspectives, and personalities to create a more creative, adaptable, and resilient group than any individual could be alone. In ideas, itâ€™s the blending of concepts from different fields to spark innovation and insight.In people, alloying is the combination of skills that makes them unstoppable. Consider a person possessing deep engineering skills who can clearly explain ideas. They are more valuable than someone with just the engineering skills. Now add empathy, humility, resilience, and drive. This person becomes incredibly rare.The key to successful alloying is knowing which elements to combine and in what proportions. Too little of one ingredient and you donâ€™t get the desired effect; too much and you might end up with something brittle or unstable. The art lies in finding the sweet spot, the golden ratio where the whole becomes more than the sum of its parts.Evolution Part One: Natural Selection and ExtinctionNatural selection is the hidden hand that selects the fittest from a never-Â­ending pile of genetic variation, while extinction is the hammer that shatters the unfit and clears the way for variations to arise. In biology, natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. The invisible hand of natural selection guides the adaptations of the living world, favoring creatures that are best suited to their environments and pruning back those that fall short.But for every winner in the great game of natural selection, there are countless losers. Extinction is the fate awaiting those species that fail to adapt, that find themselves outpaced by changing circumstances or outcompeted by more successful forms. The evolutionary end. Without the possibility of extinction, there would be no imperative to evolve to our changing environment. And without the sculpting hand of natural selection, the unfit and ill-Â­ adapted would consume scarce resources. These principles apply far beyond the realm of biology. In business, technology, and ideas, we see the same relentless winnowing of the unfit and the elevation of the adaptive. The companies that thrive navigate the shifting landscape of consumer demand and technological change, while those that stagnate are swept away by the tides of creative destruction. On a personal level, we are all subject to the pressures of selection and the risk of extinction. Our skills, our knowledge, and our ways of thinking must constantly evolve to keep pace with an ever-changing world. Those who consistently adapt are the ones who thrive in the long run.Above all, remember that there are no permanent victories in the great game of lifeâ€”Â­ only the ceaseless striving to stay one step ahead.Complacency will kill you. Thereâ€™s no such thing as a permanent lead. No matter how well a species adapts to its environment, it must keep running just to stay in place.  The Red Queen effect results from the never-Â­ending arms race between predator and prey, parasite and host, and competitor and competitor. As one species evolves a new adaptation, others evolve countermeasures, leading to a constant escalation. The faster you adapt, the faster your rivals must respond, and vice versa. This has profound implications for the pace of evolution.In a static environment, natural selection might favor a leisurely pace of change. But in a world of constant change, where your competitors are always nipping at your heels, the premium is on speed. The species that thrive adapt quickly and turn the evolutionary crank faster than their rivals. But the Red Queen effect isnâ€™t just about biological evolution. The same principle applies in any competitive domainâ€”Â­ business, technology, or even ideas. Companies must continually innovate to stay ahead of their rivals. Technologies must evolve at a breakneck pace to avoid obsolescence. Ideas must adapt and grow to maintain their relevance. The key is recognizing that adaptation isnâ€™t a one-time event but a continuous process. Itâ€™s not about reaching a finish line but maintaining a lead in an endless race. Those who rest on their laurels, who become complacent in their success, are quickly overtaken by hungrier, more agile competitors. But there is a catch when it comes to people. Once we gain an advantage, we want to hold on to it at all costs, and if weâ€™re not careful, this can slow the pace of adaptation. Before long, our competitors catch up or find innovative ways to neutralize our strength. Sustained success comes from being flexible enough to change, letting go of what worked in the past, and focusing on what you need to thrive in the future.Standing still is the quickest path to extinction in a world of constant change. Victory goes to those who can continuously adapt.Nothing exists in isolation. Everything is connected. The ecosystem lens reveals that each species plays its part in a delicate balance of competition and cooperation. The actions of any one species can have consequences for many others in the same environment. In biology, an ecosystem is a community of living organisms interacting with each other and their physical environment. In an ecosystem, nothing exists in isolationâ€”every creature is both predator and prey, both producer and consumer, locked in an intricate dance of energy and nutrients. Yet the concept of an ecosystem extends far beyond biology. You can see it nearly everywhere you look. Businesses operate within a complex network of companies, customers, competitors, suppliers, and regulators. Each entity relies on and influences the others, creating a dynamic interplay that determines which businesses thrive and which do not. Economies are also vast ecosystems comprising various sectors (like agriculture, manufacturing, and services) and actors (like workers, consumers, and governments). These components interact under the rules set by economic policies and market forces. Economic theories often explore how changes in one part of the ecosystem can lead to significant outcomes in another, much like the ripple effects seen in biological ecosystems.What all ecosystems have in common is their inherent complexity and their reductionist analysis. In an ecosystem, the whole is always more than the sum of its parts. The systemâ€™s behavior emerges from the countless interactions of its components, often in surprising and unpredictable ways. This suggests that to truly comprehend a complex system, we must look beyond the individual elements and consider the patterns of relationship and feedback that bind them together. Left to their own devices, many systems can take care of themselves, possessing abilities to correct and compensate for changes and external pressures. No matter how well-intentioned our interventions are, they often lead to unintended consequences as the solution to one problem quickly causes another, more significant problem. Be slow to intervene, and if you do, take the time to understand how actions in one part cascade into others. It pays to remember the motto of physicians, â€œFirst, do no harm.â€A niche is a special place where a particular species or idea can thrive. Itâ€™s the ecological equivalent of a custom-Â­fitted suit tailored to its occupantâ€™s unique needs and abilities. In a niche, you donâ€™t have to be all things to all peopleâ€”Â­ you just have to be the best at what you do.In biology, a niche is a speciesâ€™ specific role and position within its ecosystem. Itâ€™s the unique combination of resources it consumes, the habitat it lives in, the interactions it has with other species. A place where a speciesâ€™ adaptations flourish. But the concept of a niche extends far beyond the realm of ecology. In business, we talk about â€œmarket nichesâ€â€”Â­ the specific segments of customers with particular needs or preferences. A company focusing on a niche can often out-compete larger, more general rivals by specializing, by becoming the best at serving that particular slice of the market, or by moving with velocity. The same principle applies to careers. By specializing in something unique and valuable, you can create a space where you can excel and your combination of skills thrives. The key is finding the niche that fits you, rewards your strengths, and neutralizes your weaknesses.This isnâ€™t to say that occupying a niche is without risks. In fact, you become very fragile. If the environment changes, if consumer preferences shift, a once-Â­cozy niche can quickly become a tight squeeze. Thatâ€™s why successful niche occupants are often those who can adapt and evolve their niche as the world around them changes. Specialists have less competition and stress, but only in times of stability. Generalists face more significant dayâ€‘toâ€‘day challenges for resources and survival but have more flexibility to respond when times change.Self-Â­ preservation is a core instinct that drives all living things to protect and sustain their own existence. Itâ€™s the biological imperative that makes a gazelle run from the lion, the roots of a tree seek water, and bacteria evolve resistance to antibiotics. In the game of life, self-Â­ preservation is the only rule: stay alive. For humans, self-Â­ preservation goes beyond physical survival. It encompasses the protection of our psychological well-being, social status, and sense of identity. Anything that threatens how we see ourselves becomes a threat.While self-Â­ preservation is a necessary instinct, it can also be limiting. When weâ€™re too focused on avoiding threats, we can easily miss opportunities right before us. Left unchecked, self-preservation can lead to stagnation. The key is to find balance: to protect whatâ€™s essential and be willing to let go of what no longer serves us.Listen to the voice that tells you when to be cautious, but donâ€™t let it be the only voice you hear. Often, the most significant risk is not taking risks at all.Replication is the molecular magic trick that allows organisms to make copies of themselves to pass their genetic blueprints from one generation to the next. In the grand ballet of evolution, replication is the music that keeps the dance going. At its core, replication is about information transfer. Itâ€™s the process by which the instructions encoded in DNA are faithfully copied and transmitted. Whenever a cell divides or an organism reproduces, the replication machinery swings into action, ensuring the genetic message is preserved and propagated. However, replication is not a perfect process. Errors creep in, and mutations occur. And itâ€™s these imperfections that fuel the engine of evolution. Without the variation introduced by replication errors, life would stagnate, unable to adapt to changing environments.Replication is helpful outside of biology, too. As a mental model, it teaches us that we donâ€™t always need to reinvent the wheel. When youâ€™re just starting, the quickest way to make great leaps is to imitate what others are already doing. This establishes an average baseline of performance. Once you get a sense and a feel for the environment, you can innovate and adapt to set a new baseline. The power of replication lies in its exponential nature. A single replicated entity can give rise to countless copies, each of which can replicate further. This is the power that viruses and viral ideas harnessâ€”Â­ the ability to spread explosively by exploiting the machinery of replication. Memes, beliefs, and practices also replicate, spreading from mind to mind and shaping the contours of our shared reality. But replication also comes with risks. Unchecked replication can be cancerous, leading to uncontrolled growth that threatens the health of the larger system. Effective replication requires enough structure and space to produce a copy and enough flexibility to adapt to environmental changes. Just because something has worked for a while doesnâ€™t mean it will be effective in perpetuity. Maintaining a successful approach requires the ability to grow and modify that approach as required. As we contemplate replicationâ€™s role in life and thought, we must recognize its creative and destructive potential. We must create conditions that favor replicating what is true, sound, and beneficial while resisting the spread of what is false, harmful, or malignant. Cooperation is the surprising secret of success in the ruthless world of survival. If there is any one model that explains humanity, then this is it. Cooperation unleashed the potential of the human species.Â At first glance, cooperation seems to defy the logic of natural selection. Why would an organism invest its hard-Â­ earned resources in helping another rather than focusing solely on its own survival and reproduction? The answer lies in the magic of reciprocity and shared interest. When organisms can benefit more by cooperating than by competing, cooperative strategies emerge and flourish. Collaboration with others gives us options and opportunities that are unavailable when we insist on going it alone.Â But cooperation is not automatic. It requires specific conditionsâ€”repeated interactions, shared benefits, and mechanisms to prevent cheating.Â Cooperation is the foundation of civilization. Our speciesâ€™ success is built on our ability to cooperate flexibly and at scaleâ€”Â­ to share knowledge, coordinate efforts, and create institutions that incentivize cooperative behavior. Cooperation underlies our achievements, from the division of labor in the economy to the norms of reciprocity in society. But, as in nature, human cooperation is not guaranteed. It requires constant cultivation and protection from the forces of selfishness and short-Â­ term thinking. It requires norms that reward cooperation and punish defection.HierarchicalÂ  OrganizationHierarchy is the invisible scaffolding that organizes the living world.Â Hierarchies in biology arenâ€™t just about structure but about function. They allow for specialization and division of labor, for the emergence of complex behaviors from simple rules. In the hierarchy of an ant colony, the queen, workers, and soldiers all play their roles, their interactions giving rise to the sophisticated operation of the colony as a whole.Â But hierarchy isnâ€™t rigid or fixed. Itâ€™s fluid and dynamic, with levels constantly interacting and influencing one another. A change at one level can ripple across the entire hierarchy, transforming the system unexpectedly.Â While hierarchy is a way to manage complexity, it can also backfire. Too much hierarchy leads to unrest and instability. Too little leads to chaos.Â Most organizations promote cultures that emphasize rather than deâ€‘emphasize an individualâ€™s status, power, and place, which is part of the reason they get torn apart, as the fight to get to the top of the hierarchy takes precedence over the organizationâ€™s success.Â In the end, hierarchy is the organizing principle that allows scale from the microscopic to the magnificent.Incentives are the hidden engines that drive behavior. Theyâ€™re the unseen forces that shape our choices, the carrots and sticks that guide our actions.Â Think of a business offering a bonus for hitting a sales target. The bonus is an incentive, the external reward that motivates the salesperson to excel. But incentives arenâ€™t always so obvious. They can be subtle, even subconsciousâ€”Â­ the social approval we seek, the habits we form, the desires we pursue.Â Incentives are powerful because they tap into the fundamental wiring of the human brain. Weâ€™re hardwired to seek reward and avoid punishment, to optimize for the outcomes that serve our interests. When the incentives align with our goals, we thrive. When they donâ€™t, we struggle.Â In a classroom, itâ€™s easy to say that weâ€™ll be motivated by doing the right thing; however, in reality, weâ€™re driven mainly by rewards. We have difficulty turning down the pleasure of immediate gains, even if it takes us away from our ultimate goal.Â Often, short-Â­term and long-Â­term incentives differ. You might not feel like going to the gym today but want to be healthy as you age. Making choices to maximize your satisfaction today often leads to less reward down the road.Â Poorly designed incentives backfire, encouraging short-Â­ term thinking, unethical behavior, or unintended consequences. The key is to craft incentives that reward the behaviors that lead to long-Â­ term success.Â Ultimately, if you understand the incentive, you can predict the outcome. By shaping the incentives, we shape the outcomes. By aligning the incentives, we unlock the power of human potential.Tendency to Minimize Energy Output (Mental and physical)The tendency to limit energy output is the universal inclination to follow the path of least resistance. From the flow of a river to the behavior of a market, this tendency is the invisible hand that guides the actions of the world.Â Sometimes, our tendency to conserve energy helps us, and sometimes, it hurts us. While minimizing our output ensures we will have extra to draw on in times of increased need, it can also get in the way of learning. Experience doesnâ€™t become learning without reflection, which is an energy expenditure.Â If we want to develop our thinking and get the most out of our environments, then we have to be aware of the natural tendency to minimize energy output and correct for it where doing so creates value.The Mental Models of Systems ThinkingFeedback loops are the engines of growth and change. Theyâ€™re the mechanisms by which the output of a system influences its input.Complex systems often have many feedback loops, and it can be hard to appreciate how adjusting to feedback in one part of the system will affect the rest.Using feedback loops as a mental model begins with noticing the feedback you give and respond to daily. The model also provides insight into the value of iterations in adjusting based on the feedback you receive. With this lens, you gain insight into where to direct system changes based on feedback and the pace you need to go to monitor the impacts.Feedback loops are what make systems dynamic. Without feedback, a system does the same thing over and over. Understand them, respect them, and use them wisely.Equilibrium is the state of balance, where opposing forces cancel each other out. Itâ€™s the calm in the stormâ€™s center, the stable point around which the chaos swirls. In a system at equilibrium, thereâ€™s no net change. Everything is in a steady state, humming along at a constant pace.Â However, systems are rarely static. They continuously adjust toward equilibrium but rarely stay in balance for long.Â Equilibrium is a Â­ double-edged sword, both stability and stagnation. In our lives, we often act like we can reach an equilibrium: once we get into a relationship, weâ€™ll be happy; once we move, weâ€™ll be productive; once X thing happens, weâ€™ll be in Y state. But things are always in flux. We donâ€™t reach a certain steady state and then stay there forever. The endless adjustments are our lives. The trick is to find the right balance, strive for equilibrium where itâ€™s needed, and know when to break free and embrace the dis-equilibrium that drives progress.Bottlenecks are the choke points, the narrow parts of the hourglass where everything slows down. Theyâ€™re the constraints that limit the flow, the weakest links in the chain that determine the strength of the whole. In any system, the bottleneck is the part holding everything else back.The tricky thing about bottlenecks is that theyâ€™re not always obvious. Itâ€™s easy to focus on the parts of the system that are moving quickly and assume everything is fine. But the real leverage is in finding and fixing the bottlenecks. Speed up the slowest part, and you speed up the whole system.This is the theory of constraints in a nutshell. Figure out your bottleneck and focus all your efforts on alleviating it. Donâ€™t waste time optimizing the parts that are already fast. Theyâ€™re not the limiting factor.However, bottlenecks arenâ€™t always the villains we make them out to be. Sometimes, theyâ€™re a necessary part of the system. Think of a security checkpoint at an airport. It slows everything down, but itâ€™s there for a reason. Remove it, and you might speed things up, but at the cost of safety.The key is to be intentional about your bottlenecks. Choose them wisely, and make sure theyâ€™re serving a purpose. A deliberate bottleneck can be a powerful tool for focusing effort and maintaining quality. An accidental bottleneck is just a drag on the system.Bottlenecks are leverage points where a little effort can go a long way.Systems change as they scale up or down; neither is intrinsically better or worse. The right scale depends on your goals and the context. If you want to scale something up, you need to anticipate that new problems will keep Â­ arisingâ€”â€‹Â­ problems that didnâ€™t exist on a smaller scale. Or you might need to keep solving the same problems in different ways.Think about a recipe. If youâ€™re making a cake for four people, you use a certain amount of ingredients. But if you want to make a cake for four hundred people, you donâ€™t just multiply the ingredients by one hundred. Thatâ€™s not how scale works. You need to change the process and use bigger mixers and bigger ovens. You need a system that can handle the increased volume without breaking down.Â The challenge with scale is that itâ€™s not always obvious how to achieve it. What works for a small system often breaks down at larger volumes. You have to anticipate the bottlenecks and the points where the system will strain under the increased load. And you have to be ready to reâ€‘engineer your processes as you grow.Â If youâ€™re building something, always be thinking about scale. How will this work when you have ten times as many customers? One hundred times? One thousand times? Build with scale in mind from the start, and youâ€™ll be ready for the growth when it comes.Margin of safety is a secret weapon. Itâ€™s the buffer, the extra capacity, the redundancy that you build into a system to handle unexpected stress. Itâ€™s the difference between a bridge that can barely handle the expected load and one that can handle ten times that load without breaking a sweat.Â You can apply a margin of safety to any area of life with uncertainty and risk. The key is always to ask yourself: What if Iâ€™m wrong? What if things donâ€™t go as planned? How much extra capacity must I build to handle the unexpected?Â But hereâ€™s the rub: margin of safety isnâ€™t free. It means spending more upfront. In the short term, youâ€™ll look overly cautious and leave immediate profits on the table. But in the long run, this apparent overcaution lets you survive when others break â€“ and thrive when others merely survive.Margin of safety is the unsung hero of Â­ long-â€‹Â­term success. Itâ€™s not flashy. Itâ€™s not exciting, but itâ€™s the foundation on which everything else is built. Master it, and youâ€™ll be well on your way to navigating the uncertainties of life with confidence and stability.Churn is the silent killer of businesses. Itâ€™s the slow leak, the constant drip of customers slipping away, of users drifting off to find something new. The attrition eats away at your growth, forcing you to keep running just to stay in place. The thing about churn is that itâ€™s often hidden. Itâ€™s not like a sudden crisis that grabs your attention. Itâ€™s a slow, quiet process that happens in the background.Â Churn can present opportunity. Like a snake shedding its skin, replacing components of a system is a natural part of keeping it healthy. New parts can improve functionality.Â When we use this model as a lens, we see that new people bring new ideas, and counterintuitively, some turnover allows us to maintain stability. Replacing what is worn out also allows us to upgrade and expand our capabilities, creating new opportunities. Some churn is inevitable. Too much can kill you.Algorithms are recipes. A list of crisp, unambiguous steps that tell you how to get from point A to point B. But theyâ€™re more than just directions. Algorithms are ifâ€‘then machines for tuning out the noise and zeroing in on the signal. Have the specs been met? Fol- low the algorithm and find out. Thinking algorithmically means searching for processes that reliably spit out the desired results, like a vending machine dispensing the same candy bar every time someone punches in E4.Critical mass isnâ€™t just a science term; itâ€™s a guide for understanding that often things happen slowly and then all at once. Itâ€™s the moment when a system goes from sputtering along to explosive growth. Like a nuclear chain reaction, once you hit critical mass, the reaction becomes self-sustaining.Through this lens we gain insight into the amount of material needed for a system to change from one state to another. Material can be anything from people and effort to raw material. When enough material builds up, systems reach their tipping point. When we keep going, we get sustainable change.Using critical mass as a lens for situations where you want different outcomes helps you identify both the design elements you need to change and the work you need to put in.Â Nearly everything is an emergent Â­ effectâ€”â€‹Â­a table, a space shuttle, even Â­ usâ€”â€‹Â­ combinations of ingredients that come together in a specific way to create something new. Emergence is the universeâ€™s way of reminding us that when we combine different pieces in new ways, we get results that are more than the sum of their parts, often in the most unexpected and thrilling ways.Â Using this mental model is not about predicting emergent properties but acknowledging they are possible. There is no need to stick with what you know; mix it up and see what happens. Learn new skills, interact with new people, read new things.Irreducibility is about essence. Itâ€™s the idea that some things canâ€™t be broken down into smaller parts without losing what makes them tick. Itâ€™s the idea that not everything can be explained by looking at its components. Emergent properties arise from complex systems that canâ€™t be predicted by studying the individual parts.Â Grappling with irreducibility requires a shift in thinking. Instead of trying to break things down, sometimes you have to zoom out. Look at the big picture. Embrace the complexity. Because some problems donâ€™t have neat, modular solutions. Theyâ€™re irreducibly messy.Â Using irreducibility as a lens helps you focus on what you can change by understanding what really mattersLaw of Diminishing ReturnsDiminishing returns is the idea that the easy wins usually come first. The more you optimize a system, the harder it gets to eke out additional improvements, like squeezing juice from a lemon. The first squeeze is easy. The second takes a bit more work. By the tenth squeeze, youâ€™re fighting for every last drop.Â Every bit of effort translates into significant gains when youâ€™re a beginner. But as you level up, progress becomes more incremental. It takes more and more work to get better and better. Thatâ€™s why going from good to great is much harder than going from bad to good.Â Understanding diminishing returns is crucial for allocating resources efficiently. You want to focus on where you can get the biggest bang for your buck. Sometimes, that means knowing when to stop optimizing and move on to something else.The Mental Models of MathematicsSample size is about how much of the world youâ€™re looking at. Itâ€™s the number of data points youâ€™re using to draw conclusions. Like trying to guess the average height of people in a city by measuring a few folks on the street. The more people you measure, the more confident you can estimate.Â One of the biggest mistakes we can make is drawing conclusions from too small a sample Â­sizeâ€”â€‹Â­ like trying to guess a puzzle picture from only a few pieces. In most instances, increasing our sample size gives us valuable information that lets us see our situation in a new light. The catch is that large sample sizes are expensive. It takes time and money to collect all that data. So practitioners and researchers are always balancing the need for precision with the constraints of budget and deadline. Theyâ€™ll often settle for the smallest sample size that can still give them a statistically significant result.Using this model means exploring what isnâ€™t obvious and knowing how easy it is to corrupt our samples with bias.Â The next time you hear a statistic, think about the sample size. Itâ€™ll give you a clue about how seriously to take it. Remember: the larger the sample, the closer to the truth.Randomness is the chaos that underlies the cosmos. Itâ€™s the unpredictable, the uncontrollable, the stuff that doesnâ€™t follow any discernible pattern.Randomness is what makes life surprising. Itâ€™s why you canâ€™t predict the future with certainty. You might make plans, but thereâ€™s always the possibility of a random event throwing a wrench in the works. A flat tire, a chance encounter, a sudden inspiration. Randomness is the spice that keeps things interesting.The tricky thing about randomness is that humans are terrible at recognizing it. We see patterns where there are none. We attribute meaning to coincidence. We think we can beat the odds. But true randomness is immune to our predictions and superstitions. It doesnâ€™t care about our theories or desiresRegression to the mean is the universeâ€™s way of saying â€œnot so fast.â€ Itâ€™s the tendency for extreme outcomes to be followed by more average ones. Extreme results are rarely repeated.Â The next time you see something extraordinary, enjoy it. But remember, it probably wonâ€™t last. Sooner or later, regression to the mean will come calling, pulling the exceptional back to the ordinary. Thatâ€™s the way the universe keeps things in check.Multiplying by zero is the mathematical version of the Midas touch in reverse. Everything it touches turns to nothing. No matter how big or small a number is, when you multiply it by zero, you get zero. Itâ€™s the ultimate reset button.Â Multiplying by zero shows that we must be mindful of the zeros that will negate our other efforts. Just as in engineering, where one faulty component can make an entire system fail, not being reliable can have the same effect in life.Â When you multiply by zero, everything else becomes irrelevant.Equivalence is the art of making things interchangeable. Itâ€™s the idea that two things can be swapped out without changing the essence of what theyâ€™re a part of. Like swapping a red Lego brick for a blue one. The color changes, but the structure remains the same.Being equal doesnâ€™t mean being the same. Different inputs can produce identical results, and there is more than one way to solve most problems.Equivalence lets us simplify complex systems. We can focus on the essentials instead of getting bogged down in details. We can see the forest for the trees. And we can make changes without fear of breaking the fundamental structure.Â Of course, equivalence has its limits. Not everything is interchangeable. You canâ€™t swap out a carâ€™s engine for a hamster wheel and expect the car to run. The art is in knowing where equivalence applies and where it doesnâ€™t. Itâ€™s in recognizing the essential differences that matter, and the superficial differences that donâ€™t.Â The next time you face a complex problem, try thinking about equivalence. Look for the underlying patterns. See if there are components you can swap out or simplify. You might just find a solution thatâ€™s been hiding in plain sight all along.Surface area is what determines how much an object interacts with its environment. The more surface area the more contact. Surface area can be good and bad. Sometimes, keeping it small is favorable, and sometimes, increasing our exposure is beneficial.Â Surface area teaches us that increasing cognitive diversity can give us fresh ideas and help us innovate. However, the model also reminds us that in many ways, the more we expose ourselves, the more vulnerable we are. Different situations require different surface areas.Global and local maxima as a model can be used differently to help us make the changes we need for success. It encourages us to see achieving our goals not as a steady upward trajectory but as a path full of peaks and valleys. Understanding that sometimes we have to go down to climb even higher helps us make Â­ short-term sacrifices to play the long game. In engineering, you might be trying to maximize efficiency. In life, you might be trying to maximize happiness. But in all these cases, getting stuck on a local maximum is easy. You find a pretty good solution, and you stop looking for a better one.Â The next time youâ€™re trying to optimize something, remember the concept of global and local maxima. Donâ€™t just settle for the first peak you find. Keep exploring. Keep searching for that global maximum. It might be a tough climb, but the view from the top is worth it.The Mental Models of EconomicsScarcity shapes our choices and drives our actions. When something is scarce, it suddenly becomes valuable. We want it more because there is less. Itâ€™s the principle that underlies everything from the price of gold to the thrill of the hunt.Scarcity isnâ€™t just about material things. It applies to time, opportunities, and ideas. Weâ€™re drawn to the exclusive, the Â­ limited-â€‹Â­edition, the one-of-a-kind.In economics, scarcity is a foundational principle. There are infinite wants and desires but limited resources. We canâ€™t have everything, so we must choose. Scarcity guides those choices.Â Some businesses operate with a scarcity mentality, removing shock absorbers and operating lean, with just enough resources to produce the dayâ€™s goods. This model is prone to disruption with the slightest hiccup and signals to employees that theyâ€™re in a culture of scarcity, triggering our biological instinct toward self-preservation. We subconsciously hoard things of value to gain an individual advantage.Â Scarcity can work to your advantage. Imagine youâ€™ve got a rare combination of qualities: youâ€™re honest, hardworking, and smart. People like that are scarce, and the world disproportionately rewards them. Itâ€™s not just about being good at one thing; itâ€™s about having a mix of traits.Â The key to navigating scarcity is understanding its power, recognizing when itâ€™s driving our choices, and asking if those choices align with our true values and goals. Sometimes, scarcity creates real value. But sometimes, itâ€™s just a mirage, a trick of the mind.Supply and demand are the push and pull determining availability and price. Their dance is never-ending. A sudden shortage can send prices soaring; a new discovery can send them crashing.But supply and demand arenâ€™t just about price; theyâ€™re also about allocation. They determine who gets what, and how much of it. When supply is low and demand is high, resources flow to those willing and able to pay the most.Markets react to supply and demand. When demand exceeds supply, it encourages investment by companies to create substitutes or more supply. On the other hand, when supply exceeds demand, it discourages investment until a profitable balance is restored.Â Economic cycles are driven as much by human nature as by resources. When profits are flowing, it encourages overconfidence, greed, and complacency. When profits are nowhere to be found, it encourages fear, savings, and ruthless efficiency.Â As individuals, weâ€™re all part of this dance. Every choice we make as consumers and every decision we make as producers shapes the contours of supply and demand. We are the market, collectively determining what has value and what doesnâ€™t.Â Remember the forces at play the next time youâ€™re at the store, negotiating a salary, or launching a product. Youâ€™re not just a passive participant but an active agent in supply and demand. Your choices matter. Make them wisely.Optimization is about making the most of what you have. Itâ€™s like cleverly solving a puzzle, finding a trick to skip steps and get to the answer faster.Â In a world of scarcity, optimization is powerful. It allows us to maximize our limited resources, whether time, money, or energy. But like any tool, itâ€™s only as good as the hand that wields it. Used wisely, optimization unlocks hidden potential and drives extraordinary results. Used poorly, it leads to wasted effort and missed opportunities.Â Optimization often works for you until it doesnâ€™t. Itâ€™s like the student who writes the answer but doesnâ€™t show their work. Knowing when to use it, when to let it go, and when to avoid it can give you a key advantage.Life is full of trade-offs. Every choice has a cost. When you say yes to one thing, you say no to others. This is how the world works. Itâ€™s like gravity. You canâ€™t escape it.Â Opportunity cost is what you give up when you make a choice. Itâ€™s the thing you canâ€™t have because you picked something else. Say you have a free evening. You can work on your startup or go to a movie. If you work, you miss the fun. If you go to the movie, you miss the chance to make progress.Â Every choice has an opportunity cost because every time you say yes to something, youâ€™re implicitly saying no to other things. You need to know your opportunity costs. This helps you make good trade-offs.Â A Â­ trade-â€‹Â­off is giving up one thing to get something else. Itâ€™s choosing between options. Each has good and bad points. Trade-offs are about priorities. When you make something, you face trade-offs. If you want it fast, you might lose some features. If you want it cheap, you might use lower-quality materials.Â In life, we face Â­ trade-â€‹Â­ offs all the time. Do you take a high-paying job with long hours? Or the low-paying one with more free time? Do you spend money now or save for later?Â Making good Â­ trade-â€‹Â­offs is about weighing the opportunity costs and benefits of each option and choosing the one that aligns best with your goals and values. Itâ€™s not always easy, but being conscious of the Â­ trade-â€‹Â­ offs youâ€™re making can help you make better decisions.Â Wisdom is anticipating the consequences of your choices. In life and business, success is about making good trade-offs. Itâ€™s not about having it all. Itâ€™s about having what matters most. We all value different things. Thatâ€™s what makes life rich.Â Opportunity cost is what you give up when you make a choice; trade-offs are the balancing acts you perform when deciding between competing options. Theyâ€™re two sides of the same Â­ coinâ€” Â­whenever you make a Â­ trade-â€‹Â­off, youâ€™re incurring an opportunity cost for the option you didnâ€™t choose. The key in both cases is to be thoughtful and intentional about your choices.Specialization is a Â­trade-â€‹Â­off: pursuing one course means not pursuing another. Itâ€™s narrowing your focus to broaden your impact. In a world of infinite knowledge and finite time, specialization is the key to unlocking mastery. Itâ€™s about going deep, not wide.Â Specialization has risks. If the world changes, what was once a valuable specialty can become obsolete. And yet, we need specialists. You wouldnâ€™t want a generalist doing your brain surgery or a root canal.Â Hereâ€™s the catch: the more you specialize, the more you see how much other fields can teach you. The most exciting finds often happen at the edges between areas of knowledge. The trick is to specialize without getting stuck. To go deep, but also reach out.Â Ultimately, specialization is about where you spend your time and effort. Itâ€™s how you stand out. Itâ€™s choosing to be great at one thing instead of okay at many.Interdependence is the web that ties us all together. Itâ€™s the recognition that no person, no company, no country is an island. Weâ€™re all connected, all reliant on one another in countless ways, big and small. Interdependence is the reality that underlies the illusion of Â­ self-â€‹Â­sufficiency. No one is entirely Â­self-â€‹Â­made.Interdependence can be both a vulnerability and a strength. When we recognize our interdependencies, we can leverage them for mutual benefit. We can form alliances, partnerships, and ecosystems. We can create value that no single entity could create alone.Interdependence is the foundation of synergy, the alchemy of the whole being greater than the sum of its parts. On the other hand, if we depend on others for something critical, it can expose us if they fail to deliver or change their minds. Itâ€™s easy to be a good partner when things are going well. But you want to be careful with whom you depend in a crisis.Interdependence isnâ€™t just a macro concept. Itâ€™s deeply personal. Weâ€™re all interdependent with our families, our friends, our communities. We rely on one another for support, for love, for meaning. Interdependence is the fabric of our social lives.Efficiency is about getting the most done with the least waste. Itâ€™s not always about finding the perfect answer but the one that works well enough without too much fuss. Efficiency matters because in real life, you never have all the time or resources you want. You have to make do with what youâ€™ve got.But efficiency isnâ€™t just about speed. Itâ€™s also about effectiveness and doing the right things. Thereâ€™s no point in doing something fast if itâ€™s not worth doing. True efficiency is about focusing on what matters most. Itâ€™s about saying no to the small stuff so you can say yes to the big stuff.Like everything, efficiency has its limits. Thereâ€™s a point of diminishing returns, a threshold beyond which further optimization yields little gain. The key is finding the sweet spot, the point of maximum efficiency before the costs start outweighing the benefits.Efficiency works until it doesnâ€™t. The more perfectly efficient a system, the more vulnerable it becomes to any change. While the idea can be hard to appreciate, maximal efficiency in the short term rarely leads to maximum Â­ long-â€‹Â­ term efficiency. A common benefit eroded in the quest for efficiency is a margin of safety. Through the efficiency lens, the opportunity cost of holding something like extra cash, inventory, or even employees may be seen as too high. However, supply shocks or environmental changes can make excess cash, inventory, and employees more valuable. Inefficiency in the short run is often very efficient in the long run when it leaves you better able to adapt to an uncertain world and increases the odds of survival. In a world of trade-offs, efficiency is a balancing act. Itâ€™s about making the most of what you have and leaving room for what you might need. Itâ€™s about being prepared for the future, not just optimized for the present.Debt is a double-edged sword. Itâ€™s a powerful tool to help you grow a business, buy a home, or seize an opportunity. But itâ€™s also a chain that can bind your future or destroy you.Â When debt spirals out of control, it quickly turns dreams into nightmares.Â Debt isnâ€™t just about money. It can be a favor you owe, a social obligation, or anything that creates a future obligation. We even have sleep debt.Â It can be hard to appreciate just how fragile debt makes you. Itâ€™s like driving across a vast desert without a spare tire. If everything goes perfectly, you will reach the other side, but the smallest hiccup will leave you stranded and desperate.Â Use debt wisely. Respect its power, but fear its edge. Remember, the more you borrow, the less room you have to weather lifeâ€™s storms.Â While debt might seem cheap in the moment, the future often proves it to be more expensive than we imagined. The more you borrow, the less room you have to deal with uncertainty.Â Debt can give you leverage, but it can also take away your freedom. Respect its power but fear its edge.Monopoly and competition are the yin and yang of the business world. Theyâ€™re the opposing forces that shape the landscape of every market, the tides that lift and sink the fortunes of every firm. To understand business, you must understand the dance between these poles.Â Competition is the default state of the market. Itâ€™s the Darwinian struggle where many firms vie for the same customers and resources. In a competitive market, no one firm has the power to set prices or dictate terms. Theyâ€™re price takers, not price makers. They survive by being efficient, delivering value, and innovating.Â If competition is the natural state, monopoly is the entrepreneurâ€™s dream. A monopoly dominates a market so completely that it becomes the market. Think of the only bridge that crosses a river. But monopolies inevitably sow the seeds of their own destruction. The question is how long they will last.Â We need both monopoly and competition. Competition keeps firms honest and drives innovation. But we also need monopoliesâ€™ deep pockets to fund big visions and moon shots. The ideal is a balance: enough competition to check monopolies but enough monopolies to reward innovation.Creative destruction is the engine of progress in a capitalist economy. Itâ€™s the process by which new innovations replace old ones, the cycle of birth and death that keeps an economy vibrant. It embodies the old adage: The only constant is change.In a dynamic economy, nothing is sacred. Newer, better ideas can disrupt every industry, company, and way of doing things. The smartphone replaced the flip phone, online streaming replaced movie rental stores, and cars replaced horses.While creative destruction can be painful for individual companies, itâ€™s essential for the overall economyâ€™s health. It prevents stagnation and ensures resources are always put to their most productive use. Without creative destruction, weâ€™d still ride horses and rent VHS tapes.On one hand, creative destruction is the opportunity youâ€™re looking forâ€”the chance to disrupt an incumbentâ€”to build something new and better. But on the other hand, itâ€™s the threat youâ€™re always guarding againstâ€”the possibility that you will be disrupted by the next big thing.Creative destruction isnâ€™t just about business; itâ€™s a metaphor for life. We are all subject to change, to the constant cycle of endings and beginnings. The key is to not cling too tightly to the old, but to embrace the new possibilities.Greshamâ€™s Law states that bad money drives out good. But itâ€™s not just about currency. The principle applies anytime there are two competing versions of something, one perceived as high quality and the other as low quality.In a sense, Greshamâ€™s Law is the dark side of human nature. Weâ€™re wired to optimize for the short term, to get the most value for the least effort. If we can pass off the less valuable thing and keep the more valuable one, we will. Without consequences, bad behavior drives out good. Bad lending drives out good lending. Bad morals drive out good morals. Overcoming this requires constant effort.In the short run, bad often drives out good. But in the long run, true value wins out.Bubbles are a natural by-product of human nature. They happen when collective enthusiasm for an asset runs far ahead of its fundamental value. Itâ€™s the moment when the market becomes untethered from reality when prices are driven not by sober calculation but by mass delusion.Â Bubbles are a fascinating study of human psychology. Theyâ€™re driven by greed and FOMO (fear of missing out). No one wants to be the sucker who sits on the sidelines while everyone else gets rich. But thereâ€™s also an element of genuine belief, of conviction that this time is different, that the old rules no longer apply.Â While ultimately destructive, bubbles also serve a function. Theyâ€™re the marketâ€™s way of exploring new frontiers, of testing new possibilities. Many of the innovations we take for granted Â­ todayâ€” Â­ from cars to computers to the internet Â­ itselfâ€”â€‹Â­ were once the subject of speculative manias. Bubbles fund the infrastructure for future revolutions, even as they leave a trail of financial wreckage in their wake.Â Bubbles remind us that markets are driven by human emotions and beliefs. Theyâ€™re a mirror held up to our collective hopes, dreams, and delusions. The next time you catch yourself saying, â€œthis time is different,â€ remember that all bubbles pop eventually.Â Like a balloon that can only expand so far, bubbles eventually burst, and the game ends abruptly without warning. Keeping yourself grounded in value and economic reality, not in story or hype, is key to standing alone as a bubble expands.The audience is the invisible participant in every work of art. They are the eyes that see, the ears that hear, the minds that interpret. Without an audience, art is like a tree falling in an empty Â­ forestâ€”Â­ it may make a sound, but does it matter? The audience is what gives art its meaning, its purpose, and its very existence. Every observer infuses art with personal significance, transforming it into a co-creation. A painting of a sunset may evoke feelings of peace and beauty for one person and feelings of melancholy and loss for another. The artwork is the same, but the audience is different, so the meaning is different. In this sense, the audience is a cocreator of the art.Great artists design their work for these silent judges, balancing authenticity with expectation without succumbing to pandering.The audience is their silent collaborator and their ultimate judge.In a world where so much can be faked, the audience is something real. You can fake likes, followers, and reviews, but you canâ€™t fake the genuine human experience of engaging with art. The spontaneous laughter, unexpected tears, and long, thoughtful silence are the honest reactions that both the audience and the artists live for.Never forget your audience, but never let them dictate your creation. Picture this: youâ€™re browsing a bookstore, scanning the shelves for your next read. You pick up a book with a shadowy figure on the cover, a magnifying glass in hand. Instantly, you know what kind of story awaits you within those pages. This is the power of genreâ€” the unspoken understanding between creator and audience that shapes how we experience art.But genre is more than just a label; itâ€™s a set of conventions, an understanding between the artist and the audience. When we pick up a mystery novel, we expect a crime, some clues, and a detective. When we go to a rock concert, we expect loud guitars, driving rhythms, and a rebellious attitude. Genre sets the parameters of our experience, even as it gives the artist a foundation to build upon or rebel against.Think of genre as a game with rules. The rules provide structure, but they also create opportunities for creativity. A sonnet has a strict Â­ formâ€”â€‹Â­ fourteen lines, a specific rhyme Â­ schemeâ€”â€‹Â­ but within those constraints, poets have found endless ways to express love, loss, joy, and sorrow. The rules of the genre game inspire ingenuity, challenging artists to create something fresh within the familiar.But genres are not static; they are constantly evolving. Look at the way rock music has transformed over the decades. What began as a rebellious offshoot of blues and country in the 1950s has splintered into countless subgenres, each with a distinct style and audience. From the psychedelic experimentation of the 1960s to the punk revolution of the â€™70s to the grunge explosion of the â€™90s, rock has reinvented itself time and again. What was once transgressive becomes mainstream, and new forms emerge to take its place.Navigating genre is a delicate art. Sticking too closely to the conventions may cause your work to be dismissed as formulaic. On the other hand, if you stray too far you risk losing your audience. The key is to find the sweet Â­ spotâ€”â€‹Â­ honoring the genreâ€™s expectations while bringing something new and personal to the table.Ultimately, genre is a toolâ€”a way of framing the conversation between the artist and the audience. It provides common ground, a starting point for the journey together. But the true power of art lies in the way it can transcend genre, using convention as a springboard to take us places weâ€™ve never been.Contrast is the spice of life and art. Itâ€™s the clash of opposites that energizes a work and jolts our senses. Without contrast, the world is bland. With it, the world dances with dark and light, loud and soft, rough and smooth. Contrast makes us notice.Contrast isnâ€™t just visual. In music, quiet moments make loud ones explosive. Gentle ballads set the stage for crashing anthems. In literature, calm before the storm makes extraordinary events remarkable. Contrast gives art emotional power.Contrast creates interest and engagement. Our brains are wired to pay attention to changes and differences. We tune out the monotonous, but we snap to attention when something breaks the pattern. Artists use contrast to manipulate our attention, direct our focus, and shape our work experience.Contrast is a universal principle. Light and dark, hot and cold, life and Â­ deathâ€”â€‹Â­ the world is defined by contrasts. Darkness helps us understand light. Winter makes us appreciate spring. Contrast gives meaning to existence.Framing is the art of context, the craft of shaping perception. Itâ€™s how we present information, the lens through which we invite others to view the world. Like a photographer choosing whatâ€™s in the frame, we constantly decide what to emphasize, minimize, or leave out. These often unconscious choices profoundly influence how others understand and respond.In psychology, framing is a key concept in understanding Â­ decision- Â­ making. Present the same options in different ways, and peopleâ€™s choices change. Is it a muffin or a cake? The thing doesnâ€™t change, but its packaging does.For marketers and advertisers, framing is a potent tool. A car can be framed as a status symbol, an adventure machine, or a sensible family vehicle. A watch can be about punctuality, or it can be about luxury and prestige. The product stays the same, but the story changes. The right frame makes the ordinary extraordinary.But framing isnâ€™t just about persuasion. Itâ€™s also about understanding, about making sense of the complex world around us. We all carry frames in our Â­ mindsâ€”â€‹Â­ mental models of how things work, cultural narratives, and personal beliefs. These frames shape how we interpret information, how we explain events, and how we imagine possibilities.Framingâ€™s power lies in its subtlety. Unlike a logical argument, a frame doesnâ€™t need to be explicitly stated to have an effect. It works on an emotional, often subconscious level. A Â­ well-â€‹Â­crafted frame can make an idea feel intuitive, even inevitable, without the audience knowing why.Framing is the silent partner in every communication, the hidden hand shaping understanding. Like any powerful tool, framing can be used for good or ill. It can illuminate truth, or it can obscure it. It can empower people to see new possibilities, or it can subtly limit their thinking to narrow predefined channels.Rhythm is the universeâ€™s heartbeat, the pulse animating life. From our steady heartbeats to the sunâ€™s rise and fall, from crashing waves to swaying trees, rhythm is the pattern underlying existence. Itâ€™s the organizing principle bringing order to chaos, the recurring cycle shaping time.In music, rhythm is the backbone supporting melody and harmony. Without rhythm, music would be a formless wash of sound, lacking structure and impact. The steady beat of the drum, the driving strum of the guitar, the pulsing throb of the Â­ bassâ€”â€‹Â­ these rhythms grab us on a visceral level, moving our bodies and stirring our souls.But rhythm isnâ€™t just about regularity, the even spacing of beats. Itâ€™s also variation, the interplay of different rhythmic patterns. In jazz, the syncopated rhythms and the unexpected accents give the music an improvisational feel. In classical music, the shifting rhythms, from the stately march to the lively dance, convey the pieceâ€™s emotional arc.Rhythm is also fundamental to language. The cadence of a phrase, the meter of a poem, the rise and fall of a great oratorâ€™s Â­ speechâ€”â€‹Â­ these rhythms communicate meaning beyond the literal content of the words. They create their own music, a pattern resonating in the ear and lingering in the mind.Even in our daily lives, rhythm plays a crucial role. The routines we establish, the habits we cultivate, the cycles of work and rest, of activity and Â­ reflectionâ€”â€‹Â­ these rhythms give structure and meaning to our existence. Without rhythm, life would be a formless blur, a ceaseless stream of unrelated moments. Rhythm allows us to make sense of time, to find our place in lifeâ€™s larger patterns.Melody is musicâ€™s soul, the ethereal thread weaving through soundâ€™s tapestry. Itâ€™s the part of a song that we hum in the shower, the tune that gets stuck in our head and wonâ€™t let go. Melody is the musical expression of a fundamental human need: the need to tell a story, convey an emotion, and connect with others beyond words.A melody is simply a sequence of notes, a pattern of pitches and rhythms. But melodyâ€™s magic transcends these basic building blocks. A great melody is more than the sum of its parts. It has a shape, a contour, an arc that carries us from one note to the next. It has a sense of inevitability, as if each note is the only possible choice, even as the melody surprises us with its freshness and novelty.In this sense, melody is a lot like language. As we arrange words infinitely to express different ideas, we arrange notes to express emotions and experiences. A rising melody might convey a sense of hope and aspiration, while a falling melody might suggest sadness or resignation. A melody with large leaps might feel adventurous and daring, while one with small, stepwise motion might feel intimate and confiding.But melody isnâ€™t just about individual expressions. Itâ€™s also about communication and connection. When a melody resonates with us, itâ€™s as if the composer is speaking directly to our hearts. We feel understood, validated, less alone. And when we sing or play a melody with others, we create a bond, a shared experience that transcends our individual differences.Â This is why melody has such power across cultures and throughout history. From the chants of ancient rituals to the latest pop hits, melody has been a constant in human musical expression. Itâ€™s a universal language, requiring no translation or explanation. A beautiful melody can move us regardless of whether we understand the words or know the cultural context.Â Of course, not all melodies are equal. Just as there are great works of literature and forgettable pulp novels, there are melodies that stand the test of time and others that quickly fade from memory. The best melodies balance the familiar and the new. They have a memorable shape, a satisfying resolution, a feeling of completeness.Â In a world often fragmented and chaotic, melody is a source of unity and coherence, a way of finding beauty and meaning amid the noise.Representation is the mental shorthand we use to navigate the complexities of reality, the symbols and images we use to communicate our thoughts and experiences. Representation is how we construct meaning and bridge the gap between the raw data of our senses and the narratives we tell about ourselves and our world.Â At its core, representation is about standing in for something else. A word stands in for an object or concept, a map for a territory, a musical note for a sound. We use representations because we canâ€™t hold the entirety of reality in our minds at once. We need abstractions, simplifications, and models that we can manipulate and reason about.Â But representation is not neutral. Every representation is an interpretation, a way of framing reality that highlights some aspects and obscures others. An emoji might represent a feeling, but it doesnâ€™t show the lived experience that causes that feeling. In this sense, representation is always a kind of distortion. Itâ€™s a lens that shapes how we see the world, for better or worse. A good representation can illuminate hidden truths, help us see patterns and connections we might otherwise miss. But a bad representation can mislead us, reinforce stereotypes and prejudices, limit our ability to imagine alternatives.Â Representation is not just about mirroring reality; itâ€™s also about shaping it. The representations we create and consume can influence how we think and act, to change the very world they purport to describe. A powerful piece of art can shift cultural attitudes, a persuasive political narrative can sway elections, a compelling scientific model can guide research and policy. In this way, representation is a kind of feedback loop. We create representations based on our understanding of reality, but those representations, in turn, shape our understanding, which influences the representations we create next. Itâ€™s a constant dance between map and territory, symbol and referent.The plot is the storyâ€™s engine, propelling characters and events through time. Itâ€™s the sequence of causally connected events that leads from the beginning of a narrative to its resolution. Without a plot, a story is just disconnected moments and unrelated incidents. With a plot, a story becomes a journey, a transformative experience for characters and readers.Â At its most basic level, a plot is a series of events connected by cause and effect. Event A leads to Event B, which leads to Event C, and so on, until the story reaches its resolution. But a good plot is more than just a linear chain of events. Itâ€™s a complex web of actions and reactions, of conflicts and resolutions, of setups and payoffs.Â Conflict is the heart of any plot. Without conflict, characters have no story or reason to act or change. Conflict can take many Â­ formsâ€”â€‹Â­ person versus person, person versus nature, person versus society, person versus self. But all conflicts share a fundamental structure: a character wants something but faces obstacles. The plot is the events that arise from the characterâ€™s attempts to overcome these obstacles and achieve their goal.Â But plot is not just about external conflicts and goals. Itâ€™s also about the internal journey of the characters, the way they grow and change because of the events they experience. A good plot presents a character with external challenges and forces them to confront their own flaws, beliefs, and desires.Â In this sense, the plot is a crucible for the character. Itâ€™s the fire that tests and transforms the protagonist, revealing their true nature and potential. A character who ends a story unchanged, unaffected by the plotâ€™s events, is a character in a story that hasnâ€™t really gone anywhere. The best plots leave characters fundamentally altered, through triumph or tragedy.Â Plot is also personal. The most powerful story in the world is the one you tell yourself about the obstacles and challenges in front of you. A positive story doesnâ€™t always ensure success, but a negative one almost guarantees failure.Â Once a story takes root, no matter how false, it can be hard to change. This applies to both humanity in general and to each of us individually. Change the story to change the results.At their core, characters are bundles of traits and motivations, of habits and histories, of strengths and flaws. They are the total of their choices and actions, the product of genetics, choices, and circumstances. But a great character is more than just a list of attributes. A great character is a paradox, a contradiction, a mystery that unfolds over the course of a story.Â In many ways, character is destiny. The choices a character makes, the actions they take, flow inevitably from who they are. A cautious, thoughtful character will approach a problem differently than an impulsive, emotional one. A character with a strong moral compass will make different decisions than one with a flexible relationship to the truth. Obstacles reveal character.Â But character is not static; it is not a fixed point but a journey. The best characters are the ones who grow and change throughout a story and who are transformed by the events of the plot and the interactions with other characters. Think of Ebenezer Scrooge, the miserly old man who learns the true meaning of generosity. Understanding a personâ€™s character allows you to see someone for who they are at their core and step into their shoes. This helps you understand why they make their choices, predict their behavior, and empathize with their story. But remember, character is not set in stone. What happened yesterday is over. Todayâ€™s obstacles and challenges are nothing more than an opportunity to take a step toward or away from the person you want to be. No single choice satisfies the pursuit, only repeated steps in the right direction.The setting is the stage upon which the drama of the story unfolds, the physical and temporal context that shapes and reflects the actions of characters. An active participant in the narrative, setting is a force that can enable or hinder, reveal or conceal, enlighten or deceive. The setting is not just where the story happens but why it happens.Â Setting anchors a story in time and place, providing sensory details that make it real. But setting is more than just physical description. Itâ€™s also the social, cultural, and historical context that defines the parameters of what is possible and what is permissible for the characters.Â A story set in medieval Europe will have different constraints and opportunities than one set in Â­ modern-â€‹Â­ day Tokyo. A character in a small, gossipy village will face different challenges than one in a large, anonymous city. Setting shapes the choices characters make, the conflicts they face, the resolutions they find.Â But setting is not just a Â­one-â€‹Â­way street, not just the environment acting upon the characters. Characters also act upon and interact with their setting. They navigate its challenges, exploit its opportunities, and leave their mark on its landscape. Every story is a symbiotic relationship between character and setting, a reciprocal exchange of influence and transformation.Â Setting is the silent force that influences our fate. What we think and do is greatly impacted by our environment. This leads to a powerful and profound point: to change your behavior, change your environment. If you donâ€™t, it will change you.Performance is the art of the ephemeral, the fleeting moment of creative expression existing only in the here and now. Itâ€™s where the boundaries between art and life blur, the artistâ€™s body and actions become the medium, and the audienceâ€™s presence and participation become integral.Â At its core, performance is about presence, about the immediacy and intimacy of live action. In a world increasingly mediated by screens, live performance asserts the primacy of embodied experience, of the direct encounter between performer and spectator. Itâ€™s a reminder that art is not just a thing to be consumed but an event to be lived.Â But performance is also about absence, the gaps and spaces between action and interpretation, intention and reception. Unlike a painting or a sculpture, a performance can never be fully captured or contained. It exists only in the memories and testimonies of those who were there, in the ripples and reverberations it sends through the culture. Performance embraces the contingency and Â­open- endedness of the live event, the sense that anything could happen, that meaning is always in the making.Â This contingency is both the power and the challenge of performance. It allows for spontaneity and responsiveness, adapting to and incorporating the unpredictable elements of the moment. Yet, it makes performance resistant to the control and perfection other art forms aspire to. A performance is always a collaboration with chance, a dance with the unknown.Â As audience members, we are not just passive observers but active participants in the performance. Our presence, reactions, and energy all become part of the work. Think of fans transmitting energy to a team to rally them from behind with a few minutes left in the game. Performance invites us to be cocreators, to complete the work through our own interpretations and responses. In so doing, we become part of something larger than ourselves.Â When we are fully present in any performance where someone is making themselves vulnerable, we may just glimpse the raw, unedited, unpolished essence of what it means to be human.The Mental Models of Military and WarOne of the most valuable military tactics is the habit of â€œpersonally seeing the frontâ€ before making decisions â€“ not always relying on advisors, maps, and reports, all of which can be faulty or biased. The Map/Territory model, as does the incentive model, illustrates the problem of not seeing the front. Leaders of any organization can generally benefit from seeing the front, as it provides firsthand information and tends to improve the quality of secondhand information.The asymmetry model leads to an application in warfare whereby one side seemingly â€œplays by different rulesâ€ than the other side due to circumstance. Generally, this model is applied by an insurgency with limited resources. Unable to out-muscle their opponents, asymmetric fighters use other tactics, as with terrorism creating fear thatâ€™s disproportionate to their actual destructive ability.The Second World War was a good example of a two-front war. Once Russia and Germany became enemies, Germany was forced to split its troops and send them to separate fronts, weakening their impact on either front. Opening a two-front war can often be a useful tactic, as can solving a two-front war or avoiding one, as in the example of an organization tamping down internal discord to focus on its competitors.Though asymmetric insurgent warfare can be extremely effective, competitors have developed counterinsurgency strategies over time. Recently and famously, General David Petraeus of the United States led the development of counterinsurgency plans involving no additional force but substantial gains. Tit-for-tat warfare or competition often leads to a feedback loop that demands insurgency and counterinsurgency.The Mental Models of Human Nature and JudgmentFundamentally, the modern world operates on trust. Familial trust is generally a given (otherwise weâ€™d have a hell of a time surviving), but we also choose to trust chefs, clerks, drivers, factory workers, executives, and many others. A trusting system is one that tends to work most efficiently; the rewards of trust are extremely high.Highly responsive to incentives, humans haveÂ perhaps the most varied and hardest to understand set of incentives in the animal kingdom. This causes us to distort our thinking when it is in our own interest to do so. A wonderful example is a salesman truly believing that his product will improve the lives of its users. Itâ€™s not merely convenient that he sells the product; the fact of his selling the product causes a very real bias in his own thinking.Ivan Pavlov very effectively demonstrated that animals can respond not just to direct incentives but also to associated objects; remember the famous dogs salivating at the ring of a bell. Human beings are much the same and can feel positive and negative emotion towards intangible objects, with the emotion coming from past associations rather than direct effects.Humans have a tendency to feel envious of those receiving more than they are, and a desire â€œget what is theirsâ€ in due course. The tendency towards envy is strong enough to drive otherwise irrational behavior, but is as old as humanity itself. Any system ignorant of envy effects will tend to self-immolate over time.Based on past association, stereotyping, ideology, genetic influence, or direct experience, humans have a tendency to distort their thinking in favor of people or things that they like and against people or things they dislike. This tendency leads to overrating the things we like and underrating or broadly categorizing things we dislike, often missing crucial nuances in the process.Anyone who has been alive long enough realizes that, as the saying goes, â€œdenial is not just a river in Africa.â€ This is powerfully demonstrated in situations like war or drug abuse, where denial has powerful destructive effects but allows for behavioral inertia. Denying reality can be a coping mechanism, a survival mechanism, or a purposeful tactic.One of the most useful findings of modern psychology is what Daniel Kahneman calls the Availability Bias or Heuristic: We tend to most easily recall what is salient, important, frequent, and recent. The brain has its own energy-saving and inertial tendencies that we have little control over â€“ the availability heuristic is likely one of them. Having a truly comprehensive memory would be debilitating. Some sub-examples of the availability heuristic include the Anchoring and Sunk Cost Tendencies.8. Representativeness HeuristicThe three major psychological findings that fall under Representativeness, also defined by Kahneman and his partner Tversky, are:b. Tendency to StereotypeÂ The tendency to broadly generalize and categorize rather than look for specific nuance. Like availability, this is generally a necessary trait for energy-saving in the brain.c. Failure to See False ConjunctionsMost famously demonstrated by the Linda Test, the same two psychologists showed that students chose more vividly described individuals as more likely to fit into a predefined category than individuals with broader, more inclusive, but less vivid descriptions, even if the vivid example was a mere subset of the more inclusive set. These specific examples are seen as more representative of the category than those with the broader but vaguer descriptions, in violation of logic and probability.Human beings are one of many social species, along with bees, ants, and chimps, among many more. We have a DNA-level instinct to seek safety in numbers and will look for social guidance of our behavior. This instinct creates a cohesive sense of cooperation and culture which would not otherwise be possible but also leads us to do foolish things if our group is doing them as well.Human beings have been appropriately called â€œthe storytelling animalâ€ because of our instinct to construct and seek meaning in narrative. Itâ€™s likely that long before we developed the ability to write or to create objects, we were telling stories and thinking in stories. Nearly all social organizations, from religious institutions to corporations to nation-states, run on constructions of the narrative instinct.We like to call other species curious, but we are the most curious of all, an instinct which led us out of the savanna and led us to learn a great deal about the world around us, using that information to create the world in our collective minds. The curiosity instinct leads to unique human behavior and forms of organization like the scientific enterprise. Even before there were direct incentives to innovate, humans innovated out of curiosity.The psychologist Steven Pinker calls our DNA-level instinct to learn grammatically constructed language the Language Instinct. The idea that grammatical language is not a simple cultural artifact was first popularized by the linguist Noam Chomsky. As we saw with the narrative instinct, we use these instincts to create shared stories, as well as to gossip, solve problems, and fight, among other things. Grammatically ordered language theoretically carries infinite varying meaning.13. First-Conclusion BiasAs Charlie Munger famously pointed out, the mind works a bit like a sperm and egg: the first idea gets in and then the mind shuts. Like many other tendencies, this is probably an energy-saving device. Our tendency to settle on first conclusions leads us to accept many erroneous results and cease asking questions; it can be countered with some simple and useful mental routines.Itâ€™s important for human beings to generalize; we need not see every instance to understand the general rule, and this works to our advantage. With generalizing, however, comes a subset of errors when we forget about the Law of Large Numbers and act as if it does not exist. We take a small number of instances and create a general category, even if we have no statistically sound basis for the conclusion.15. Relative Satisfaction/Misery TendenciesThe envy tendency is probably the most obvious manifestation of the relative satisfaction tendency, but nearly all studies of human happiness show that it is related to the state of the person relative to either their past or their peers, not absolute. These relative tendencies cause us great misery or happiness in a very wide variety of objectively different situations and make us poor predictors of our own behavior and feelings.As psychologists have frequently and famously demonstrated, humans are subject to a bias towards keeping their prior commitments and staying consistent with our prior selves when possible. This trait is necessary for social cohesion: people who oftenÂ change their conclusions and habits are often distrusted. Yet our bias towards staying consistent can become, as one wag put it, a â€œhobgoblin of foolish mindsâ€ â€“ when it is combined with the first-conclusion bias, we end up landing on poor answers and standing pat in the face of great evidence.Once we know the outcome, itâ€™s nearly impossible to turn back the clock mentally. Our narrative instinct leads us to reason that we knew it all along (whatever â€œitâ€ is), when in fact we are often simply reasoning post-hoc with information not available to us beforeÂ the event. The hindsight bias explains why itâ€™s wise to keep a journal of important decisions for an unaltered record and to re-examine our beliefs when we convince ourselves that we knew it all along.Justice runs deep in our veins. In another illustration of our relative sense of well-being, we are careful arbiters of what is fair. Violations of fairness can be considered grounds for reciprocal action, or at least distrust. Yet fairness itself seems to be a moving target.Â What is seen as fair and just in one time and place may not be in another. Consider that slavery has been seen as perfectly natural and perfectly unnatural in alternating phases of human existence.We tend to over-ascribe the behavior of others to their innate traits rather than to situational factors, leading us to overestimate how consistent that behavior will be in the future. In such a situation, predicting behavior seems not very difficult. Of course, in practice this assumption is consistently demonstrated to be wrong, and we are consequently surprised when others do not act in accordance with the â€œinnateâ€ traits weâ€™ve endowed them with.Stress (Including Breaking Points)Stress causes both mental and physiological responses and tends to amplify the other biases. Almost all human mental biases become worse in the face of stress as the body goes into a fight-or-flight response, relying purely on instinct without the emergency brake of Daniel Kahnemanâ€™s â€œSystem 2â€ type of reasoning. Stress causes hasty decisions, immediacy, and a fallback to habit, thus giving rise to the elite soldiersâ€™ motto: â€œIn the thick of battle, you will not rise to the level of your expectations, but fall to the level of your training.â€A major problem with historiography â€“ our interpretation of the past â€“ is that history is famously written by the victors. We do not see what Nassim Taleb calls the â€œsilent graveâ€ â€“ the lottery ticket holders who did not win. Thus, we over-attribute success to things done by the successful agent rather than to randomness or luck, and we often learn false lessons by exclusively studying victors without seeing all of the accompanying losers who acted in the same way but were not lucky enough to succeed.We might term this Boredom Syndrome: Most humans have the tendency to need to act, even when their actions are not needed. We also tend to offer solutions even when we do not have knowledge to solve the problem.What a man wishes, he also believes. Similarly, what we believe is what we choose to see. This is commonly referred to as the confirmation bias. It is a deeply ingrained mental habit, both energy-conserving and comfortable, to look for confirmations of long-held wisdom rather than violations. Yet the scientific process â€“ including hypothesis generation, blind testing when needed, and objective statistical rigor â€“ is designed to root out precisely the opposite, which is why it works so well when followed.The modern scientific enterprise operates under the principle of falsification: A method is termed scientific if it can be stated so that a certain defined result would cause it to be proved false. Pseudo-knowledge and pseudo-science operate and propagate by being unfalsifiable. As with astrology, we cannot prove them either correct or incorrect because the conditions under which they would be shown false are never stated.]]></content:encoded></item><item><title>Unrolling the Codex agent loop</title><link>https://openai.com/index/unrolling-the-codex-agent-loop/</link><author>tosh</author><category>hn</category><pubDate>Fri, 23 Jan 2026 20:42:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Banned C++ features in Chromium</title><link>https://chromium.googlesource.com/chromium/src/+/main/styleguide/c++/c++-features.md</link><author>szmarczak</author><category>hn</category><pubDate>Fri, 23 Jan 2026 20:27:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[This document is part of the more general Chromium C++ style guide. It summarizes the supported state of new and updated language and library features in recent C++ standards and the Abseil library. This guide applies to both Chromium and its subprojects, though subprojects can choose to be more restrictive if necessary for toolchain support.The C++ language has in recent years received an updated standard every three years (C++11, C++14, etc.). For various reasons, Chromium does not immediately allow new features on the publication of such a standard. Instead, once Chromium supports the toolchain to a certain extent (e.g., build support is ready), a standard is declared â€œâ€, with new language/library features banned pending discussion but not yet allowed.You can propose changing the status of a feature by sending an email to cxx@chromium.org. Include a short blurb on what the feature is and why you think it should or should not be allowed, along with links to any relevant previous discussion. If the list arrives at some consensus, send a codereview to change this file accordingly, linking to your discussion thread.If an item remains on the TBD list two years after initial support is added, style arbiters should explicitly move it to an appropriate allowlist or blocklist, allowing it if there are no obvious reasons to ban.The current status of existing standards and Abseil features is:Default allowed; see banned features belowDefault allowed; see banned features belowInitially supported November 13, 2023; see allowed/banned/TBD features belowInitially supported January 2026; see allowed/banned/TBD features belowDefault allowed; see banned/TBD features below. The following dates represent the start of the two-year TBD periods for certain parts of Abseil:absl::linked_hash_set & map: Initially added to third_party Dec 30, 2025Banned features and third-party codeThird-party libraries may generally use banned features internally, although features with poor compiler support or poor security properties may make the library unsuitable to use with Chromium.Chromium code that calls functions exported from a third-party library may use banned library types that are required by the interface, as long as:The disallowed type is used only at the interface, and converted to and from an equivalent allowed type as soon as practical on the Chromium side.The feature is not banned due to security issues or lack of compiler support. If it is, discuss with cxx@chromium.org to find a workaround.C++11 Banned Language FeaturesThe following C++11 language features are not allowed in the Chromium codebase.Inline Namespaces [banned] Allows better versioning of namespaces. An integer of at least 64 bits.User-Defined Literals [banned] Allows user-defined literal expressions.C++11 Banned Library FeaturesThe following C++11 library features are not allowed in the Chromium codebase.<cctype>, <ctype.h>, <cwctype>, <wctype.h> [banned] Provides utilities for ASCII characters.<cfenv>, <fenv.h> [banned] Provides floating point status flags and control modes for C-compatible code. A standard date and time library. Exception throwing and handling.Engines And Generators From <random> [banned] Methods of generating random numbers. Provides compile-time rational numbers. A standard regular expressions library.std::aligned_{storage,union} [banned] Creates aligned, uninitialized storage to later hold one or more objects. Declares a function object bound to certain arguments. Wraps a standard polymorphic function. Allows shared ownership of a pointer through reference counts.std::{sto{i,l,ul,ll,ull,f,d,ld},to_string} [banned] Converts strings to/from numbers. Allows a weak reference to a .Thread Support Library [banned] Provides a standard multithreading library using  and associatesC++17 Banned Language FeaturesThe following C++17 language features are not allowed in the Chromium codebase.UTF-8 character literals [banned] A character literal that begins with  is a character literal of type  (C++17) or  (C++20). The value of a UTF-8 character literal is equal to its ISO 10646 code point value.C++17 Banned Library FeaturesThe following C++17 library features are not allowed in the Chromium codebase.Mathematical special functions [banned] A variety of mathematical functions.Parallel algorithms [banned] Many of the STL algorithms, such as the ,  and  methods, now support the parallel execution policies: , , and  which translate to â€œsequentiallyâ€, â€œparallelâ€ and â€œparallel unsequencedâ€.std::aligned_alloc [banned] Allocates uninitialized storage with the specified alignment. A type-safe container for single values of any type. The contents of a single memory unit.  has the same size and aliasing rules as , but does not semantically represent a character or arithmetic value, and does not expose operators other than bitwise ops. A standard way to manipulate files, directories, and paths in a filesystem.std::{from,to}_chars [banned] Locale-independent, non-allocating, non-throwing functions to convert values from/to character strings, designed for use in high-throughput contexts.std::{pmr::memory_resource,polymorphic_allocator} [banned] Manages memory allocations using runtime polymorphism.std::timespec_get [banned] Gets the current calendar time in the given time base.std::uncaught_exceptions [banned] Determines whether there are live exception objects.Transparent std::owner_less [banned] Function object providing mixed-type owner-based ordering of shared and weak pointers, regardless of the type of the pointee. Returns a  that tracks ownership of  by all existing s that refer to .C++20 Allowed Language FeaturesThe following C++20 language features are allowed in the Chromium codebase.Abbreviated function templates [allowed] Function params of type  become syntactic sugar for declaring a template type for each such parameter. Specified that a function may only be used in a compile-time context.Constraints and concepts [allowed] Allows bundling sets of requirements together as named concepts, then enforcing them on template arguments.Default comparisons [allowed] Requests that the compiler generate the implementation of any comparison operator, including . Prefer non-member comparison operators. When defaulting , also explicitly default . Together these are sufficient to allow any comparison as long as callers do not need to take the address of any non-declared operator.Designated initializers [allowed] Allows explicit initialization of subsets of aggregate members at construction.__has_cpp_attribute [allowed] Checks whether the toolchain supports a particular standard attribute. Ensures that a variable can be compile-time initialized. This is like a milder form of  that does not force variables to be const or have constant destruction.Initializers for bit-field members [allowed] Allows specifying the default initial value of a bit-field member, as can already be done for other member types.Lambda captures with initializers that are pack expansions [allowed] Allows initializing a capture with a pack expansion.Language feature-test macros [allowed] Provides a standardized way to test the toolchain's implementation of a particular language feature.[[likely]], [[unlikely]] [allowed] Tells the optimizer that a particular codepath is more or less likely than an alternative.Range-for statements with initializer [allowed] Like C++17's selection statements with initializer. Particularly useful before C++23, since temporaries inside range-expressions are not lifetime-extended until the end of the loop before C++23.Three-way comparison (â€œspaceshipâ€) operator [allowed] Compares two objects in a fashion similar to . Perhaps most useful when defined as an overload in a class, in which case it can replace definitions of other inequalities. See also â€œDefault comparisonsâ€.using enum declarations [allowed] Introduces enumerator element names into the current scope.C++20 Allowed Library FeaturesThe following C++20 library features are allowed in the Chromium codebase. Provides various byte- and bit-twiddling functions, e.g. counting leading zeros. Concepts and classes used to implement three-way comparison (â€œspaceshipâ€, ) support. Various useful concepts, many of which replace pre-concept machinery in .Range algorithms [allowed] Provides versions of most algorithms that accept either an iterator-sentinel pair or a single range argument.Range access, range primitives, dangling iterator handling, and range concepts [allowed] Various helper functions and types for working with ranges.Library feature-test macros and <version> [allowed] Provides a standardized way to test the toolchain's implementation of a particular library feature. Provides compile-time constants for many common mathematical values, e.g. pi and e.std::assume_aligned [allowed] Informs the compiler that a pointer points to an address aligned to at least some particular power of 2.std::erase[_if] for containers [allowed] Erases from a container by value comparison or predicate, avoiding the need to use the  paradigm.std::hardware_{con,de}structive_interference_size [allowed] The std::hardware_destructive_interference_size constant is useful to avoid false sharing (destructive interference) between variables that would otherwise occupy the same cacheline. In contrast, std::hardware_constructive_interference_size is helpful to promote true sharing (constructive interference), e.g. to support better locality for non-contended data.std::is_[un]bounded_array [allowed] Checks if a type is an array type with a known or unknown bound. Linearly interpolates (or extrapolates) between two values.std::make_obj_using_allocator etc. [allowed]std::make_unique_for_overwrite [allowed] Like calling std::unique_ptr<T>(new T) instead of the more typical std::unique_ptr<T>(new T(...)). Finds the midpoint between its two arguments, avoiding any possible overflow. For integral inputs, rounds towards the first argument.std::ranges::subrange [allowed] Creates a view from an iterator and a sentinel. Useful for treating non-contiguous storage (e.g. a ) as a range.std::remove_cvref[_t] [allowed] Provides a way to remove const, volatile, and reference qualifiers from a type. Returns the size of an object as a signed type.std::string::(starts,ends)_with [allowed] Tests whether a string starts or ends with a particular character or string.C++20 Banned Language FeaturesThe following C++20 language features are not allowed in the Chromium codebase. A single UTF-8 code unit. Similar to , but considered a distinct type. Modules provide an alternative to many uses of headers which allows for faster compilation, better tooling support, and reduction of problems like â€œinclude what you useâ€.[[no_unique_address]] [banned] Allows a data member to be overlapped with other members.C++20 Banned Library FeaturesThe following C++20 library features are not allowed in the Chromium codebase. An updated version of  with fewer gotchas, similar to . Returns an value constructed with the same bits as an value of a different type.std::{c8rtomb,mbrtoc8} [banned] Converts a code point between UTF-8 and a multibyte character encoded using the current C locale.Range factories and range adaptors [banned] Lightweight objects that represent iterable sequences. Provides facilities for lazy operations on ranges, along with composition into pipelines.std::ranges::view_interface [banned] CRTP base class for implementing custom view objects. Utilities for non-owning views over a sequence of objects. Converts a pointer-like object to a pointer, even if the pointer does not refer to a constructed object (in which case an expression like  is UB). Facilities for multithreaded access to streams.C++20 TBD Language FeaturesThe following C++20 language features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections.Aggregate initialization using parentheses [tbd] Allows initialization of aggregates using parentheses, not just braces. Allows writing functions that logically block while physically returning control to a caller. This enables writing some kinds of async code in simple, straight-line ways without storing state in members or binding callbacks.C++20 TBD Library FeaturesThe following C++20 library features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections. Header which defines various core coroutine types. Utilities for producing formatted strings. Provides a class that can hold source code details such as filenames, function names, and line numbers. A string whose character type is , intended to hold UTF-8-encoded text.C++23 Allowed Language FeaturesThe following C++23 language features are allowed in the Chromium codebase.#elifdef, #elifndef [allowed] New conditional inclusion preprocessor directives.C++23 Allowed Library FeaturesThe following C++23 library features are allowed in the Chromium codebase.std::basic_string::contains [allowed] More concise substring check. Reverses the bytes of an integer.std::to_underlying [allowed] Converts an enumeration to its underlying type.C++23 TBD Language FeaturesThe following C++23 language features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections.Explicit object parameter [tbd] Allows explicit specification of the object parameter (deducing ) in member functions.Multidimensional subscript operator [tbd] Allows multiple arguments in the subscript operator. Prvalue copy (decay-copy). Provides a hint to the optimizer. Standardized preprocessor warning directive.Literal suffix for size_t [tbd] Literal suffix  or  for .Named character escapes [tbd] Universal character names using .C++23 TBD Library FeaturesThe following C++23 library features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections.Monadic operations for std::optional [tbd], ,  member functions. A vocabulary type that contains an expected value or an error.std::flat_map, std::flat_multimap, std::flat_set, std::flat_multiset [tbd] Container adaptors that provide the functionality of associative containers using sorted vectors.std::out_ptr, std::inout_ptr [tbd] Smart pointer adapters for functions that take raw pointers as out-parameters. Multidimensional array view. Converts a range to a container. Extends  to support printing containers and ranges. Formatted output. Coroutine generator. Captures a stack trace.std::move_only_function [tbd] Function wrapper for move-only objects. Indicates a codepath that is unreachable and invokes undefined behavior if executed. Input/output stream using a span as buffer.Fixed width floating-point types [tbd] Similar to int32_t and friends but for floats.std::start_lifetime_as [tbd] Explicitly starts the lifetime of an object of type T in the given storage.Abseil Banned Library FeaturesThe following Abseil library features are not allowed in the Chromium codebase. Early adaptation of C++17 . An equivalent of the C++23 std::move_only_function. Cross-platform macros to expose compiler-specific functionality.btree_* containers [banned] Alternatives to the tree-based standard library containers designed to be more efficient in the general case. Binds the first N arguments of an invocable object and stores them by value.Command line flags [banned] Allows programmatic access to flag values passed on the command-line to binaries.Container utilities [banned] Container-based versions of algorithmic functions within C++ standard library. A fixed size array like , but with size determined at runtime instead of compile time. Type for holding a non-owning reference to an object of any invocable type.Log macros and related classes [banned] Macros and related classes to perform debug loggings is a wrapper around an object of type T that behaves as an object of type T but never calls T's destructor.Nullability annotations [banned] Annotations to more clearly specify contracts Early adaptation of C++17 . Functions and utilities for generating pseudorandom data. Early adaptation of C++20 . An object that is either a usable value, or an error Status explaining why such a value is not present. Early adaptation of C++17 . Classes and utility functions for manipulating and comparing strings. Primitives for managing tasks across different threads. Abstractions for holding time values, both in terms of absolute time and civil time. A backport of C++17's std::variant type-safe union and related utilities. Backports of various C++17 template utilities.The following Abseil library features are not allowed in the Chromium codebase. See the top of this page on how to propose moving a feature from this list into the allowed or banned sections.absl::linked_hash_set, absl::linked_hash_map [tbd] A simple insertion-ordered set or map. It provides O(1) amortized insertions and lookups, as well as iteration in the insertion order.]]></content:encoded></item><item><title>Maze Algorithms (2017)</title><link>http://www.jamisbuck.org/mazes/</link><author>surprisetalk</author><category>hn</category><pubDate>Fri, 23 Jan 2026 20:07:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[If you're interested in maze algorithms, I've written a book about the subject: "Mazes for Programmers". Check it out!
    Threshold: This combines Aldous-Broder and Wilson's, to get the best performance of both. Sadly,
    it is not guaranteed to be uniform like the other two, but it is faster! It runs Aldous-Broder
    until some minimum number of cells have been visited, and then switches to Wilson's.]]></content:encoded></item><item><title>Tesla kills Autopilot, locks lane-keeping behind $99/month fee</title><link>https://arstechnica.com/cars/2026/01/tesla-wants-recurring-revenue-discontinues-autopilot-in-favor-of-fsd/</link><author>CharlesW</author><category>hn</category><pubDate>Fri, 23 Jan 2026 19:28:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Claude.ai silently failing since Jan 14, no official acknowledgment</title><link>https://github.com/anthropics/claude-code/issues/18866</link><author>nurimamedov</author><category>hn</category><pubDate>Fri, 23 Jan 2026 18:42:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>New YC homepage</title><link>https://www.ycombinator.com/</link><author>sarreph</author><category>hn</category><pubDate>Fri, 23 Jan 2026 18:08:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Zotero 8</title><link>https://www.zotero.org/blog/zotero-8/</link><author>bouchard</author><category>hn</category><pubDate>Fri, 23 Jan 2026 18:05:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Weâ€™re excited to announce our latest major release, Zotero 8. Zotero 8 builds on the new design and features of Zotero 7 and includes a huge number of improvements and refinements.Redesigned Citation DialogZotero 8 introduces a new unified citation dialog, replacing the previous citation dialog (the â€œred barâ€), the â€œclassicâ€ citation dialog, and the Add Note dialog (the â€œyellow barâ€).The new dialog has two modes: List mode and Library mode. List mode lets you quickly search for citations from across your Zotero libraries by title, creator, and year. Library mode includes a library browser, letting you find items in specific libraries or collections. You can switch between the two modes with a single click, preserving any added items or entered search terms. By default, it will open in the last mode you used, but you can choose a different default mode in the settings.In Zotero 7, we added the ability to quickly add citations for selected items and open documents. In the new dialog, these options are available in both List mode and Library mode, so you can make these quick selections even if you otherwise prefer to add items via the library browser.As before, once youâ€™ve selected an item, you can click on its bubble to customize the citation with a page number, prefix, etc. Itâ€™s also now possible to add any locator â€” not just a page number â€” right from the search bar by typing the full or short name (e.g., â€œline 10â€ or â€œl. 10â€ after the citation and pressing Enter/Return.You can switch between adding citations and adding notes using buttons in the bottom left, corresponding to the Add/Edit Citation and Add Note buttons in your word processor.(For those coming from the classic dialog, note that thereâ€™s no text field to make manual edits to citations. Itâ€™s been possible to edit citations directly in the document for many years, which is why the red bar didnâ€™t include such a text field either. More importantly, though, such manual edits should be avoided in almost all cases. Instead, customize the citation via the citation dialog, which will allow Zotero to continue to update the citation as necessary.)Annotations in the Items ListAnnotations you make on PDFs, EPUBs, and webpage snapshots now show up under their parent attachments in the items list.Showing annotations in the items list makes it easier to view annotations across a library or collection, and it also makes it possible to search for annotations directly. For example, you can search for all annotations in a collection with a given tag and then create a note from those annotations or copy them to an external text editor with Quick Copy.In Advanced Search, you can use â€œItem Typeâ€ â€œisâ€ â€œAnnotationâ€ to match annotations or use the Annotation Text and Annotation Comment search conditions to search for specific parts of the annotation.You can assign tags to selected annotations by dragging them to the tag selector, just like other items.Selected annotations show up in the item pane, grouped by top-level item.Reader Appearance Panel with Theme SupportWeâ€™ve added a new Appearance panel in the reader that provides quick access to view settings and introduces support for reader themes.The view settings are per-document settings. Themes are applied globally for all documents, including in the attachment preview in the item pane, and apply to PDFs, EPUBs, and webpage snapshots.We offer a number of built-in themes (â€œDarkâ€, â€œSnowâ€, â€œSepiaâ€), and you can create custom themes just by specifying a foreground and background color. (Some other theme engines require additional accent colors, but weâ€™ve tried to make this as simple as possible for users by automatically adjusting other colors based on the foreground and background colors.) You can set a different theme that applies to light mode and dark mode.The themes replace the previous on-by-default â€œUse Dark Mode for Contentâ€ option, which inverted images in dark mode. Weâ€™re now simply darkening images a bit when using a dark theme. Images and ink annotations in the reader sidebar and note editor are now only darkened as well (and only when Zotero itself is in dark mode).When possible, we also try to apply themes to PDF pages containing full-page images, such as scanned papers, by replacing whitish/dark colors with theme colors. (Otherwise we simply darken the page slightly.)Itâ€™s now possible to open notes in tabs in addition to separate windows. Note tabs fill the whole window, with wide margins for better readability and a clean, distraction-free space for note-taking.By default, double-clicking a note in the items list will open it in a tab. You can choose to open the note in the other space from the context menu, and you can change the default behavior using the â€œOpen notes in new windows instead of tabsâ€ setting in the General pane of the settings.Notes in tabs have a separate font size setting in the View menu.Reading Mode for Webpage SnapshotsReading Mode reformats webpage snapshots for easier reading, with unnecessary page elements removed. You can adjust line height and other view options from the Appearance panel.Weâ€™ve reworked the tabs menu to make it faster to interact with via the keyboard.You can now press Ctrl/Cmd-; to bring up the menu at any time.Once the menu is open, it simultaneously accepts search input, up/down navigation, and row selection, without the need to move between different parts of the menu. You can simply start typing the name of an open tab and then press Enter/Return to switch to it once youâ€™ve narrowed down the list.Itâ€™s also possible to quickly close multiple tabs by moving between the row close buttons with up/down and pressing space bar to close a tab.Zotero now automatically keeps attachment filenames in sync with parent item metadata as you make changes (e.g., changing the title). In previous versions, while Zotero would automatically rename files when you first added them to your library, if you later edited the itemâ€™s metadata, you would need to right-click on the attachment and select â€œRename File from Parent Metadataâ€.You can configure which file types renaming applies to from the General tab of the Zotero settings.After upgrading to this version, existing eligible files that donâ€™t match the current filename format wonâ€™t be automatically renamed, but you can choose to rename them en masse from the Zotero settings. Zotero will also prompt you to rename all files if you change the filename format.â€œRename File from Parent Metadataâ€ has been removed from the item context menu. If a filename doesnâ€™t match the configured filename format (e.g., because automatic renaming is disabled or you changed the format but didnâ€™t choose to rename all files), you can click the â€œRename File to Match Parent Itemâ€ button next to the filename in the attachmentâ€™s item pane to rename it.New Attachment Title OptionsZotero 7 introduced more consistent handling of attachment titles, preserving simpler, less-redundant titles (e.g., â€œFull Text PDFâ€ or â€œPreprint PDFâ€) in cases where the title was previously changed to match the filename. Zotero 8 further refines its renaming and titling logic when adding multiple and/or non-primary attachments, to bring the functionality better in line with the intended behavior.Weâ€™ve also added a â€œNormalize Attachment Titlesâ€ option under Tools â†’ Manage Attachments to update old primary attachments with titles matching the filename to use simpler titles such as â€œPDFâ€.While we recommend the default behavior, allowing Zotero to rename primary files and keep them renamed while using simpler titles in the items list, if you really prefer to view filenames instead of titles, you can now enable â€œShow attachment filenames in the items listâ€ option in the General pane of the settings.Zotero 8 adds a version for Linux running on ARM64 devices. This includes ARM-based Chromebooks, Apple Silicon Macs running Linux (Linux VMs, Asahi Linux), and Raspberry Pis.If youâ€™ve been unable to run Zotero on your ARM-based device, or youâ€™ve been running the x86_64 version under emulation, give it a try.User Interface ImprovementsWeâ€™ve made a number of changes across the interface to address common requests:A new button in the library tab allows you to quickly close the item pane without dragging its edge or using the menus.You can reorder item pane sections by dragging their icons in the side navigation bar.You can drag items, collections, and searches into the trash.You can drag attachments, notes, and related items from the item pane (e.g., to copy files to the filesystem or use Quick Copy).Collections automatically expand when you drag over them, making it easier to drop collections or items into subcollections.You can delete attachments from the item pane.Tabs maintain their size as you close them for faster closing of multiple tabs.With Zotero 8, the Zotero Connector save popup can autocomplete tags in your Zotero library and allows you to add a note to items as you save them.Zotero 8 includes much more than we can list here. See the changelog for additional details.If youâ€™re already running Zotero, you can upgrade from within Zotero by going to Help â†’ â€œCheck for Updatesâ€¦â€.
								This entry was posted
								 
								on Thursday, January 22nd, 2026 at 12:52 pm by Dan Stillman								and is filed under Features, News.
																							]]></content:encoded></item><item><title>Microsoft gave FBI set of BitLocker encryption keys to unlock suspects&apos; laptops</title><link>https://techcrunch.com/2026/01/23/microsoft-gave-fbi-a-set-of-bitlocker-encryption-keys-to-unlock-suspects-laptops-reports/</link><author>bookofjoe</author><category>hn</category><pubDate>Fri, 23 Jan 2026 17:58:56 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Microsoft provided the FBI with the recovery keys to unlock encrypted data on the hard drives of three laptops as part of a federal investigation, Forbes reported on Friday.Many modern Windows computers rely on full-disk encryption, called BitLocker, which is enabled by default. This type of technology should prevent anyone except the device owner from accessing the data if the computer is locked and powered off.Â But, by default, BitLocker recovery keys are uploaded to Microsoftâ€™s cloud, allowing the tech giant â€” and by extension law enforcement â€” to access them and use them to decrypt drives encrypted with BitLocker, as with the case reported by Forbes.The case involved several people suspected of fraud related to the Pandemic Unemployment Assistance program in Guam, a U.S. island in the Pacific. Local news outlet Pacific Daily News covered the case last year, reporting that a warrant had been served to Microsoft in relation to the suspectsâ€™ hard drives. Kandit News, another local Guam news outlet, also reported in October that the FBI requested the warrant six months after seizing the three laptops encrypted with BitLocker.Â A spokesperson for Microsoft did not immediately respond to a request for comment by TechCrunch. Microsoft told Forbes that the company sometimes provides BitLocker recovery keys to authorities, having received an average of 20 such requests per year.Â Apart from the privacy risks of handing recovery keys to a company, Johns Hopkins professor and cryptography expert Matthew Green raised the potential scenario where malicious hackers compromise Microsoftâ€™s cloud infrastructure â€” something that has happenedseveral times in recent years â€” and get access to these recovery keys. The hackers would still need physical access to the hard drives to use the stolen recovery keys.â€œItâ€™s 2026 and these concerns have been known for years,â€ Green wrote in a post on Bluesky. â€œMicrosoftâ€™s inability to secure critical customer keys is starting to make it an outlier from the rest of the industry.â€]]></content:encoded></item><item><title>Proof of Corn</title><link>https://proofofcorn.com/</link><author>rocauc</author><category>hn</category><pubDate>Fri, 23 Jan 2026 17:56:31 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[A project by@seth, inspired by@fredwilson, orchestrated by Claude Code (Opus 4.5)]]></content:encoded></item><item><title>Route leak incident on January 22, 2026</title><link>https://blog.cloudflare.com/route-leak-incident-january-22-2026/</link><author>nomaxx117</author><category>hn</category><pubDate>Fri, 23 Jan 2026 17:54:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[On January 22, 2026, an automated routing policy configuration error caused us to leak some Border Gateway Protocol (BGP) prefixes unintentionally from a router at our data center in Miami, Florida. While the route leak caused some impact to Cloudflare customers, multiple external parties were also affected because their traffic was accidentally funnelled through our Miami data center location.The route leak lasted 25 minutes, causing congestion on some of our backbone infrastructure in Miami, elevated loss for some Cloudflare customer traffic, and higher latency for traffic across these links. Additionally, some traffic was discarded by firewall filters on our routers that are designed to only accept traffic for Cloudflare services and our customers.While weâ€™ve written about route leaks before, we rarely find ourselves causing them. This route leak was the result of an accidental misconfiguration on a router in Cloudflareâ€™s network, and only affected IPv6 traffic. We sincerely apologize to the users, customers, and networks we impacted yesterday as a result of this BGP route leak.Essentially, a route leak occurs when a network tells the broader Internet to send it traffic that it's not supposed to forward. Technically, a route leak occurs when a network, or Autonomous System (AS), appears unexpectedly in an AS path. An AS path is what BGP uses to determine the path across the Internet to a final destination. An example of an anomalous AS path indicative of a route leak would be finding a network sending routes received from a peer to a provider.During this type of route leak, the rules of  are violated, as BGP updates are sent from AS64501 to their peer (AS64502), and then unexpectedly up to a provider (AS64503). Oftentimes the leaker, in this case AS64502, is not prepared to handle the amount of traffic they are going to receive and may not even have firewall filters configured to accept all of the traffic coming in their direction. In simple terms, once a route update is sent to a peer or provider, it should only be sent further to customers and not to another peer or provider AS.During the incident on January 22, we caused a similar kind of route leak, in which we took routes from some of our peers and redistributed them in Miami to some of our peers and providers. According to the route leak definitions in RFC7908, we caused a mixture of Type 3 and Type 4 route leaks on the Internet.Â A change that ultimately triggers the routing policy bug is merged in our network automation code repositoryAutomation is run on single Miami edge-router resulting in unexpected advertisements to BGP transit providers and peersNetwork team begins investigating unintended route advertisements from MiamiIncident is raised to coordinate responseThe bad configuration change is manually reverted by a network operator, and automation is paused for the router, so it cannot run againThe change that triggered the leak is reverted from our code repositoryAutomation is confirmed by operators to be healthy to run again on the Miami router, without the routing policy bugAutomation is unpaused on the single router in MiamiWhat happened: the configuration errorOn January 22, 2026, at 20:25 UTC, we pushed a change via our policy automation platform to remove the BGP announcements from Miami for one of our data centers in BogotÃ¡, Colombia. This was purposeful, as we previously forwarded some IPv6 traffic through Miami toward the BogotÃ¡ data center, but recent infrastructure upgrades removed the need for us to do so.This change generated the following diff (a program that compares configuration files in order to determine how or whether they differ):[edit policy-options policy-statement 6-COGENT-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-COMCAST-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-GTT-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-LEVEL3-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-PRIVATE-PEER-ANYCAST-OUT term ADV-SITELOCAL from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-PUBLIC-PEER-ANYCAST-OUT term ADV-SITELOCAL from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-PUBLIC-PEER-OUT term ADV-SITELOCAL from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-TELEFONICA-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;
[edit policy-options policy-statement 6-TELIA-ACCEPT-EXPORT term ADV-SITELOCAL-GRE-RECEIVER from]
-      prefix-list 6-BOG04-SITE-LOCAL;While this policy change looks innocent at a glance, only removing the prefix lists containing BOG04 unicast prefixes resulted in a policy that was too permissive:policy-options policy-statement 6-TELIA-ACCEPT-EXPORT {
    term ADV-SITELOCAL-GRE-RECEIVER {
        from route-type internal;
        then {
            community add STATIC-ROUTE;
            community add SITE-LOCAL-ROUTE;
            community add MIA01;
            community add NORTH-AMERICA;
            accept;
        }
    }
}
The policy would now mark every prefix of type â€œinternalâ€ as acceptable, and proceed to add some informative communities to all matching prefixes. But more importantly, the policy also accepted the route through the policy filter, which resulted in the prefix â€” which was intended to be â€œinternalâ€ â€”Â  being advertised externally. This is an issue because the â€œroute-type internalâ€ match in JunOS or JunOS EVO (the operating systems used by  devices) will match any non-external route type, such as Internal BGP (IBGP) routes, which is what happened here.As a result, all IPv6 prefixes that Cloudflare redistributes internally across the backbone were accepted by this policy, and advertised to all our BGP neighbors in Miami. This is unfortunately very similar to the outage we experienced in 2020, on which you can read more .When the policy misconfiguration was applied at 20:25 UTC, a series of unintended BGP updates were sent from AS13335 to peers and providers in Miami. These BGP updates are viewable historically by looking at MRT files with the  tool or using .Â âžœ  ~ monocle search --start-ts 2026-01-22T20:24:00Z --end-ts 2026-01-22T20:30:00Z --as-path ".*13335[ \d$]32934$*"
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f077::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f091::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f16f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f17c::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f26f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f27c::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113609.854028|2801:14:9000::6:4112:1|64112|2a03:2880:f33f::/48|64112 22850 174 3356 13335 32934|IGP|2801:14:9000::6:4112:1|0|0|22850:65151|false|||pit.scl
A|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f17c::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc
A|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f27c::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc
A|1769113583.095278|2001:504:d::4:9544:1|49544|2a03:2880:f091::/48|49544 1299 3356 13335 32934|IGP|2001:504:d::4:9544:1|0|0|1299:25000 1299:25800 49544:16000 49544:16106|false|||route-views.isc
A|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f091::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc
A|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f17c::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc
A|1769113584.324483|2001:504:d::19:9524:1|199524|2a03:2880:f27c::/48|199524 1299 3356 13335 32934|IGP|2001:2035:0:2bfd::1|0|0||false|||route-views.isc
{trimmed}
In the monocle output seen above, we have the timestamp of our BGP update, followed by the next-hop in the announcement, the ASN of the network feeding a given route-collector, the prefix involved, and the AS path and BGP communities if any are found. At the end of the output per-line, we also find the route-collector instance.Looking at the first update for prefix 2a03:2880:f077::/48, the AS path is 64112 22850 174 3356 13335 32934. This means we (AS13335) took the prefix received from Meta (AS32934), our peer, and then advertised it toward Lumen (AS3356), one of our upstream transit providers. We know this is a route leak as routes received from peers are only meant to be readvertised to downstream (customer) networks, not laterally to other peers or up to providers.As a result of the leak and the forwarding of unintended traffic into our Miami router from providers and peers, we experienced congestion on our backbone between Miami and Atlanta, as you can see in the graph below.Â This would have resulted in elevated loss for some Cloudflare customer traffic, and higher latency than usual for traffic traversing these links. In addition to this congestion, the networks whose prefixes we leaked would have had their traffic discarded by firewall filters on our routers that are designed to only accept traffic for Cloudflare services and our customers. At peak, we discarded around 12Gbps of traffic ingressing our router in Miami for these non-downstream prefixes.Â Follow-ups and preventing route leaksÂ We are big supporters and active contributors to efforts within the  and  that strengthen routing security. We know firsthand how easy it is to cause a route leak accidentally, as evidenced by this incident.Â Preventing route leaks will require a multi-faceted approach, but we have identified multiple areas in which we can improve, both short- and long-term.In terms of our routing policy configurations and automation, we are:Patching the failure in our routing policy automation that caused the route leak, and will mitigate this potential failure and others like it immediatelyÂ Implementing additional BGP community-based safeguards in our routing policies that explicitly reject routes that were received from providers and peers on external export policiesÂ Adding automatic routing policy evaluation into our CI/CD pipelines that looks specifically for empty or erroneous policy termsÂ Improve early detection of issues with network configurations and the negative effects of an automated changeTo help prevent route leaks in general, we are:Â Validating routing equipment vendors' implementation of  (BGP roles and the Only-to-Customer Attribute) in preparation for our rollout of the feature, which is the only way independent of routing policy to prevent route leaks caused at the  Autonomous System (AS)Most importantly, we would again like to apologize for the impact we caused users and customers of Cloudflare, as well as any impact felt by external networks.]]></content:encoded></item><item><title>Notes on the Intel 8086 processor&apos;s arithmetic-logic unit</title><link>https://www.righto.com/2026/01/notes-on-intel-8086-processors.html</link><author>elpocko</author><category>hn</category><pubDate>Fri, 23 Jan 2026 17:26:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In 1978, Intel introduced the 8086 processor, a revolutionary chip that led to the modern x86 architecture.
Unlike modern 64-bit processors, however, the 8086 is a 16-bit chip.
Its arithmetic/logic unit (ALU) operates on 16-bit values, performing arithmetic operations such as addition and subtraction,
as well as logic operations including bitwise AND, OR, and XOR.
The 8086's ALU is a complicated part of the chip, performing 28 operations in total.In this post, I discuss the circuitry that controls the ALU, generating the appropriate control signals for a
particular operation.
The process is more complicated than you might expect. First, a machine code instruction results in the execution of multiple
microcode instructions.
Using the ALU is a two-step process: one microcode instruction (micro-instruction) configures the ALU for the desired operation,
while a second
micro-instruction gets the results from the ALU.
Moreover, based on both the microcode micro-instruction and the machine code instruction, the control circuitry sends control signals to the ALU,
reconfiguring it for the desired operation.
Thus, this circuitry provides the "glue" between the micro-instructions and the ALU.The die photo below shows the 8086 processor under a microscope.
I've labeled the key functional blocks.
Architecturally, the chip is partitioned into a Bus Interface Unit (BIU) at the top and an Execution Unit (EU) below.
The BIU handles bus and memory activity as well as instruction prefetching, while the Execution Unit (EU) executes the instructions.
In the lower right corner, the microcode ROM holds the micro-instructions.
The ALU is in the lower left corner, with bits 7-0 above and bits 15-8 below, sandwiching the status flag circuitry.
The ALU control circuitry, highlighted in red at the bottom of the chip, is the focus of this article.The die of the 8086. Click this image (or any other) for a larger version.The 8086 processor implements most machine instructions in microcode, with a micro-instruction for each step of the machine instruction.
(I discuss the 8086's microcode in detail here.)
The 8086 uses an interesting architecture for microcode:
each micro-instruction performs two unrelated operations. The first operation moves data between a source and a destination.
The second operation can range from a jump or subroutine call to a memory read/write or an ALU operation.
An ALU operation has a five-bit field to specify a particular operation and a two-bit field to specify
which temporary register provides the input. As you'll see below, these two fields play an important role in the ALU circuitry.In many cases, the 8086's micro-instruction doesn't specify the ALU operation, leaving the details to be substituted from the machine instruction opcode.
For instance, the ADD, SUB, ADC, SBB, AND, OR, XOR, and CMP
machine instructions share the same microcode, while the hardware selects the ALU operation from the instruction opcode.
Likewise, the increment and decrement instructions use the same microcode, as do the decimal adjust instructions DAA and DAS, and the
ASCII adjust instructions AAA and AAS.
Inside the micro-instruction, all these operations are performed with a "pseudo" ALU operation called XI (for some reason).
If the microcode specifies an XI ALU operation, the hardware replaces it with the ALU operation specified in the instruction.
Another important feature of the microcode is 
that you need to perform one ALU micro-instruction to configure the ALU's operation, but the result isn't
available until a later micro-instruction, which moves the result to a destination.
This has the consequence that the hardware must remember the ALU operation.To make this concrete, here is the microcode that implements a typical arithmetic instruction such as  or .
This microcode consists of three micro-instructions. 
The left half of each micro-instruction specifies a data movement, first moving the two arguments to ALU temporary registers
and then storing the ALU result (called Î£).
The right half of each micro-instruction performs the second task.
First, the ALU is configured to perform an  operation using temporary register A. Recall that  indicates the ALU operation
is filled in from the machine instruction; this is how the same microcode handles eight different types of machine instructions.
In the second micro-instruction, the next machine instruction is started unless a memory writeback is required ().
The last micro-instruction is  (Run Next Instruction) to start a new machine instruction. It also indicates that the
processor status flags () should be updated to indicate if the ALU result is zero, positive, overflow, and so forth.M â†’ tmpa   XI   tmpa  
R â†’ tmpb   WB,NXT     
Î£ â†’ M      RNI  F     The ALU is the heart of a processor, performing arithmetic and logic operations.
Microprocessors of the 1970s typically supported addition and subtraction; logical AND, OR, and XOR; and various bit shift operations.
(Although the 8086 had multiply and divide instructions, these were implemented in microcode, not in the ALU.)
Since an ALU is both large and critical to performance, chip architects try to optimize its design.
As a result, different microprocessors have widely different ALU designs.
For instance, the 6502 microprocessor has separate circuits for addition and each logic operation; a multiplexer selects the appropriate
output.
The Intel 8085, on the other hand, uses an optimized clump of gates that performs the desired operation based on control signals (details), while the Z80's 4-bit ALU uses a different clump of gates (details).The 8086 takes a different approach, using two lookup tables (along with other gates) to generate the carry and output signals for each bit in the ALU.
By setting the lookup tables appropriately, the ALU can be configured to perform the desired operation.
(This is similar to how an FPGA implements arbitrary functions through lookup tables.)
The schematic below shows the circuit for one bit of the ALU.
I won't explain this circuit in detail since I explained it in an earlier article.
The relevant part of this circuit is the six control signals at the left.
The two multiplexers (trapezoidal symbols) implement the lookup tables by using the two input argument bits to select outputs from
the control signals to control carry generation and carry propagation.
Thus, by feeding appropriate control signals into the ALU, the 8086 can reconfigure the ALU to perform the desired operation.
For instance, with one set of control signals, this circuit will add. Other sets of control signals will cause the circuit to subtract
or compute a logical operation, such as AND or XOR.
The 8086 has 16 copies of this circuit, so it operates on 16-bit values.The circuit that implements one bit in the 8086's ALU.The 8086 is a complicated processor, and its instructions have many special cases, so controlling the ALU is
more complex than described above.
For instance, the compare operation is the same as a subtraction, except the numerical result of a compare is discarded; just the
status flags are updated.
The add versus add-with-carry instructions require different values for the carry into bit 0, while subtraction requires the
carry flag to be inverted since it is treated as a borrow.
The 8086's ALU supports increment and decrement operations, but also increment and decrement by 2, which requires an increment signal into bit
1 instead of bit 0.
The bit-shift operations all require special treatment. For instance, a rotate can use the carry bit or exclude the carry bit, while
and arithmetic shift right requires the top bit to be duplicated.
As a result, along with the six lookup table (LUT) control signals, the ALU also requires numerous control signals to adjust its
behavior for specific instructions.
In the next section, I'll explain how these control signals are generated.ALU control circuitry on the dieThe diagram below shows the components of the ALU control logic as they appear on the die.
The information from the micro-instruction enters at the right and is stored in the latches.
The PLAs (Programmable Logic Arrays) decode the instruction and generate the control signals.
These signals flow to the left, where they control the ALU.The ALU control logic as it appears on the die. I removed the metal layer to show the underlying polysilicon and silicon. The reddish lines are remnants of the metal.As explained earlier, if the microcode specifies the  operation, the operation field is replaced with a value based on the machine instruction opcode.
This substitution is performed by the  multiplexer before the value is stored in the operation latch.
Because of the complexity of the 8086 instruction set, the  operation is not as straightforward as you might expect.
This multiplexer gets three instruction bits from a special register called the "X" register, another instruction bit from the instruction
register, and the final bit from a decoding circuit called the Group Decode ROM.Recall that one micro-instruction specifies the ALU operation, and a later micro-instruction accesses the result. Thus, the
ALU control circuitry must remember the specified operation so it can be used later. 
In particular, the control circuitry must keep track of the ALU operation to perform and the temporary register specified.
The control circuitry uses three flip-flops to keep track of the specified temporary register, one flip-flop for each register.
The micro-instruction contains a two-bit field that specifies the temporary register. The control circuitry decodes this field and
activates the associated flip-flop.
The outputs from these flip-flops go to the ALU and enable the associated temporary register.
At the start of each machine instruction, the flip-flops are reset, so temporary register A is selected by default.The control circuitry uses five flip-flops to store the five-bit operation field from the micro-instruction.
At the start of each machine instruction, the flip-flops are reset so operation 0 (ADD) is specified by default.
One important consequence is that an add operation can potentially be performed without a micro-instruction to configure the ALU,
shortening the microcode by one micro-instruction and thus shortening the instruction time by one cycle.The five-bit output from the operation flip-flops goes to the operation PLA (Programmable Logic Array), which decodes the operation
into 27 control signals.
Many of these signals go to the ALU, where they control the behavior of the ALU for special cases.
About 15 of these signals go to the Lookup Table (LUT) PLA, which generates the six lookup table signals for the ALU.
At the left side of the LUT PLA, special high-current driver circuits amplify the control signals before they are sent to the ALU.
Details on these drivers are in the footnotes.Whenever I look at the circuitry of the 8086 processor, I see the differences between a RISC chip and a CISC chip.
In a RISC (Reduced Instruction Set Computer) processor such as ARM, instruction decoding is straightforward, as is the processor circuitry.
But in the 8086, a CISC (Complex Instruction Set Computer) processor, there are corner cases and complications everywhere.
For instance, an 8086 machine instruction sometimes specifies the ALU operation in the first byte and sometimes in the second byte,
and sometimes elsewhere, so the X register latch, the XI multiplexer, and the Group Decode ROM are needed.
The 8086's ALU includes obscure operations including four types of BCD adjustments and seven types of shifts, making the ALU more
complicated.
Of course, the continuing success of x86 shows that this complexity also has benefits.This article has been a deep dive into the details of the 8086's ALU, but I hope you have found it interesting.
If it's too much detail for you, you might prefer my overview of the 8086 ALU.]]></content:encoded></item><item><title>Gas Town&apos;s agent patterns, design bottlenecks, and vibecoding at scale</title><link>https://maggieappleton.com/gastown</link><author>pavel_lishin</author><category>hn</category><pubDate>Fri, 23 Jan 2026 16:19:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ A few weeks ago     published an elaborate     to Gas Town, his Mad-Max-Slow-Horses-Waterworld-etc-themed agent orchestrator that runs dozens of coding agents simultaneously in a metaphorical town of automated activity. Gas Town is entirely vibecoded, hastily designed with off-the-cuff solutions, and inefficiently burning through thousands of dollars a month in API costs. This doesnâ€™t sound promising, but itâ€™s lit divisive debates and sparks of change across the software engineering community. A small hype machine has formed around it. Itâ€™s made the rounds through every engineering teamâ€™s Slack, probably twice.  Thereâ€™s somehow already a     doing over $400k in earnings.  And the hype is justified. First, because itâ€™s utterly unhinged, and second because itâ€™s a serious indication of how agents will change the nature of software development from this point on.You should at least skim through Yeggeâ€™s original article before continuing to read my reflections. First, because Iâ€™m not going to comprehensively summarise it.  And second, because even a one minute glance over Yeggeâ€™s style of writing will make the vibes clear.We should take Yeggeâ€™s creation seriously not because itâ€™s a serious, working tool for todayâ€™s developers (it isnâ€™t). But because itâ€™s a good piece of speculative design fiction that asks provocative questions and reveals the shape of constraints weâ€™ll face as agentic coding systems mature and grow.â€œDesign fictionâ€ or â€œspeculative designâ€ is a branch of design where you create things (objects, prototypes, sketches) from a plausible near future. Not to predict whatâ€™s going to happen, but to provoke questions and start conversations about what  happen. Not in a bright-and-glorious-flying-cars way that futurism can sometimes fall into. But, most helpfully, in a way that thinks about banal details, overlooked everyday interactions, low status objects, imperfect implementations, knock-on effects, and inconveniences. See the Near Future Labâ€™s short     and their     if you want to learn more.I also think Yegge deserves praise for exercising agency and taking a swing at a system like this, despite the inefficiencies and chaos of this iteration. And then running a public tour of his shitty, quarter-built plane while itâ€™s mid-flight.When I was taken to the Tate Modern as a child Iâ€™d point at     pieces and say to my mother â€œI could do thatâ€, and she would say â€œyes, but you didnâ€™t.â€ Many people have talked about what large-scale, automated agent orchestration systems  look like in a few years, and no one else attempted to sincerely build it.I should be transparent and say that I have not used Gas Town in earnest on any serious work. I have only lightly poked at it, because I do not qualify as a serious user when Iâ€™m still hovering around stages 4-6 in Yeggeâ€™s 8 levels of automation:I currently juggle a handful of consecutive     and     agents, but pay close attention to the diffs and regularly check code in an IDE. Which I guess puts me in the agentically conservative camp in this distressingly breakneck moment in history.Gas Town is a full-on stage 8 piece of tooling: using an orchestrator that manages dozens+ of other coding agents for you. Yegge also warned me not to seriously use Gas Town multiple times, in increasingly threatening typography. I trust his guidance on his own slush pile.But I have grokked the basic concepts and spent more time with this manifesto than is warranted. And here is what stood out to me from the parts I could comprehend:When you have a fat stack of agents churning through code tasks, development time is no longer the bottleneck. Yegge says â€œGas Town churns through implementation plans so quickly that you have to do a LOT of design and planning to keep the engine fed.â€ Design becomes the limiting factor: imagining what you want to create and then figuring out all the gnarly     required to make your imagination into reality.I certainly feel this friction in both my own professional work and personal projects. My development velocity is far slower than Yegge since I only wrangle a few agents at a time and keep my eyes and hands on the code. But the build time is rarely what holds me up. It is always the design; how should we architect this? What should this feel like? How should this look? Is that transition subtle enough? How composable should this be? Is this the right metaphor?When itâ€™s not the design, itâ€™s the product strategy and planning; What are the highest priority features to tackle? Which piece of this should we build first? When do we need to make that decision? Whatâ€™s the next logical, incremental step we need to make progress here?These are the kind of decisions that agents cannot make for you. They require your human context, taste, preferences, and vision.With agents to hand, itâ€™s easy to get ahead of yourself, stumbling forward into stacks of generated functions that should never have been prompted into existence, because they do not correctly render your intentions or achieve your goals.Gas Town seems to be halfway into this pitfall. The biggest flaw in Yeggeâ€™s creation is that it is poorly designed. I mean this in the sense that he absolutely did not design the shape of this system ahead of time, thoughtfully considering which metaphors and primitives would make this effective, efficient, easy to use, and comprehensible.He just made stuff up as he went. He says as much himself: â€œGas Town is complicated. Not because I wanted it to be, but because I had to keep adding components until it was a self-sustaining machine.â€ Gas Town is composed of â€œespecially difficult [theories] because itâ€™s a bunch of bullshit I pulled out of my arse over the past 3 weeks, and I named it after badgers and stuff.â€ It was slapdashed together over â€œ17 days, 75k lines of code, 2000 commits. It finally got off the ground (GUPP started working) just 2 days ago.â€This Hacker News     describes the problem well, and points out that Yeggeâ€™s previous     project, of which Gas Town is an extension, suffers the same issue:â€œBeads is a good idea with a bad implementation. Itâ€™s not a designed product in the sense we are used to, itâ€™s more like a stream of consciousness converted directly into code. Itâ€™s a program that isnâ€™t only vibe coded, it was vibe designed too.â€â€œGas Town is clearly the same thing multiplied by ten thousand. The number of overlapping and ad hoc concepts in this design is overwhelming. Steve is ahead of his time but we arenâ€™t going to end up using this stuff. Instead a few of the core insights will get incorporated into other agents in a simpler but no less effective way.â€â€œgas town [is] such a nightmare to use i love itâ€¦ the mayor is dumb as rocks the witness regularly forgets to look at stuff the deacon makes his own rules the crew have the object permanence of a tank full of goldfish and the polecats seem intent on wreaking as much chaos on the project as they can. this is peak entertainment i swearâ€Friends and colleagues of mine who have been brave enough to try out Gas Town in more depth report the same thing; this thing fits the shape of Yeggeâ€™s brain and no one elseâ€™s. Iâ€™d categorise that as a moderate design fail, given this is a public product that I assume Yegge wants  people to try out. The onboarding is baptism by fire.This feels like one of the most critical, emerging footguns of liberally hands-off agentic development. You can move so fast you never stop to think. It is so easy to prompt, you donâ€™t fully consider what youâ€™re building at each step of the process. It is only once you are hip-deep in poor architectural decisions, inscrutable bugs, and a fuzzy memory of what you set out to do, do you realise you have burned a billion tokens in exchange for a pile of hot trash.2. Buried in the chaos are sketches of future agent orchestration patternsNow that Iâ€™ve just critiqued the design of Gas Town, I will turn around and say that while the current amalgamation of polecats, convoys, deacons, molecules, protomolecules, mayors, seances, hooks, beads, witnesses, wisps, rigs, refineries, and dogs is a bunch of under cooked spaghetti, Yeggeâ€™s patterns  sketch out some useful conceptual shapes for future agentic systems.If you step back and squint, this mishmash of concepts reveals a few underlying patterns that future agentic systems will likely follow:Agents have specialised roles with hierarchical supervisionEvery agent in Gas Town has a permanent, specialised role. When an agent spins up a new session, it knows who it is and what job it needs to do. Some examples: is the human concierge: itâ€™s the main agent you talk to. It talks to all the other agents for you, kicking off work, receiving notifications when things finish, and managing the flow of production. are temporary grunt workers. They complete single, isolated tasks, then disappear after submitting their work to be merged. supervises the Polecats and helps them get unstuck. Its job is to solve problems and nudge the proletariat workers along. manages the merge queue into the main branch. It evaluates each piece of work waiting to be merged, resolving conflicts in the process. It can creatively â€œre-imagineâ€ implementations if merge conflicts get too hairy, while trying to keep the intent of the original work.There are many more characters in this town, but these give you a flavour of the system. Giving each agent a single job means you can prompt them more precisely, limit what theyâ€™re allowed to touch, and run lots of them at once without them stepping on each otherâ€™s toes.Thereâ€™s also a clear chain of command between these agents. You talk to the Mayor, who coordinates work across the system. The Mayor in Gas Town never writes code. It talks to you, then creates work tasks and assigns them to workers. A set of system supervisors called the Witness, the Deacon, and â€œBoot the Dogâ€ intermittently nudge the grunt workers and each other to check everyone is doing their work. Oh and thereâ€™s also a crew of â€œdogsâ€ who do maintenance and cleaning.Itâ€™s easier if I try and show you. Hereâ€™s the basic relationship structure of Gas Town, as best I can make out:Since Iâ€™m making my own visuals here, I should justify it by pointing out that while Yegge made lots of his own ornate, zoopmorphic diagrams of Gas Townâ€™s architecture and workflows, they are unhelpful. Primarily because they were made entirely by Geminiâ€™s    . And while Nano Banana is state-of-the-art at making diagrams, generative AI systems are still really shit at making illustrative diagrams. They are very hard to decipher, filled with cluttered details, have arrows pointing the wrong direction, and are often missing key information. Case in point:Does this help you understand how the system works? No? No.Gas Townâ€™s hierarchical approach solves both a coordination and attention problem. Without it, you are the one assigning tasks to dozens individual agents, checking whoâ€™s stuck, whoâ€™s idle, and whoâ€™s waiting on work from someone else. With the Mayor as your single interface, that overhead disappears. You can continuously talk to the Mayor without interrupting any agents or getting in the way, or having to think much about which one is doing what. This is less cognitive overhead than constantly switching tabs between Claudes.I think thereâ€™s a lot of opportunity to diversify the cast of characters here and make more use of    . The agents in Gas Town are all generalist workers in the software development pipeline. But we could add in any kind of specialist we want: a dev ops expert, a product manager, a front-end debugger, an accessibility checker, a documentation writer. These would be called in on-demand to apply their special skills and tools.Agent roles and tasks persist, sessions are ephemeralOne of the major limitations of current coding agents is running out of context. Before you even hit the limits of a context window,     degrades the output enough that itâ€™s not worth keeping. We constantly have to compact or start fresh sessions.Gas Townâ€™s solution to this is make each agent session disposable by design. It stores the important information â€“ agent identities and tasks â€“ in Git, then liberally kills off sessions and spins up fresh ones when needed. New sessions are told their identity and currently assigned work, and continue on where the last one left off. Gas Town also lets new sessions ask their predecessors what happened through â€œseancingâ€: resuming the last session as a separate instance in order to let the new agent ask questions about unfinished work.This saving and recalling is all done through Gas Townâ€™s â€œBeadsâ€ system.  Beads are tiny, trackable units of work â€“ like issues in an issue tracker â€“ stored as JSON in Git alongside your code. Each bead has an ID, description, status, and assignee. Agent identities are also stored as beads, giving each worker a persistent address that survives session crashes.Yegge didnâ€™t invent this pattern of tracking atomic tasks outside agent memory in something structured like JSON. Anthropic described the same approach in their research on    , just published in November 2025. I give it a hot minute before this type of task tracking lands in Claude Code.Feeding agents continuous streams of workThe whole promise of an orchestration system like Gas Town is itâ€™s a perpetual motion machine. You give high-level orders to the mayor, and then a zoo of agents kicks off to break it down into tasks, assign them, execute them, check for bugs, fix the bugs, review the code, and merge it in.Each worker agent in Gas Town has its own queue of assigned work and a â€œhookâ€ pointing to the current thing they should be doing. The minute they finish a task, the next one jumps to the front of the queue. The mayor is the one filling up these queues â€“ itâ€™s in charge of breaking down large features into atomic tasks and assigning them to available workers. In theory, the workers are never idle or lacking tasks, so long as you keep feeding the mayor your grand plans.This principle of â€œworkers always do their workâ€ is better in theory than practice. It turns out to be slightly difficult to make happen because of the way current models are trained. Theyâ€™re designed as helpful assistants who wait politely for human instructions. Theyâ€™re not used to checking a task queue and independently getting on with things.Gas Townâ€™s patchwork solution to this is aggressive prompting and constant nudging. Supervisor agents spend their time poking workers to see if anyoneâ€™s stalled out or run dry on work. When one goes quiet, they send it a ping which jolts the agent into checking its queue and getting back to work. These periodic nudges move through the agent hierarchy like a heartbeat keeping everything moving. This is a decent band-aid for the first version, but more serious efforts at agent orchestration systems will need reliable ways to keep agents on task.Merge queues and agent-managed conflictsWhen you have a bunch of agents all working in parallel, youâ€™re of course going to run into merge conflicts. Each agent is off on its own branch, and by the time it finishes its task, the main branch might look completely different â€“ other changes have landed, the code has moved on. The later an agent finishes, the worse this gets. Normally you, the human, takes on the burden of sorting out the mess and deciding which changes to keep. But if agents are running on their own, something has to do that job for them.So Gas Town has a dedicated merge agent â€“ the Refinery â€“ that works through the merge queue one change at a time. It looks at each merge request, resolves any conflicts, and gets it into main. When things get really tangled â€“ when so much has changed that the original work doesnâ€™t even make sense anymore â€“ it can creatively â€œre-imagineâ€ the changes: re-doing the work to fit the new codebase. Or escalate to a human if needed.But thereâ€™s another way to sidestep merge conflict nightmares that Gas Town doesnâ€™t have built in: ditch PRs for    . The traditional git workflow puts each feature on its own branch for days or weeks, accumulating commits, then getting merged back as one chunky PR.Stacked diffs avoid this conflict-prone approach by breaking work into small, atomic changes that each get reviewed and merged on their own, building on top of each other. Every change gets its own branch, forked off the previous change, forming a â€œstackâ€ of changes dependent on one another. When a change earlier in the stack gets updated, all the changes below it automatically rebase on top of the new version.This fits how agents naturally work. Theyâ€™re already producing tiny, focused changes rather than sprawling multi-day branches. When conflicts do pop up, theyâ€™re easier to untangle because each diff touches less code.    â€™s recent acquisition of    , a tool built specifically for stacked diff workflows, suggests I am not the only one who sees this opportunity. When youâ€™ve got dozens of agents landing changes continuously, you need tools and interfaces specifically designed for these frequent, incremental merges.3. The price is extremely high, but so is the (potential) valueYegge describes Gas Town as â€œexpensive as hellâ€¦ you wonâ€™t like Gas Town if you ever have to think, even for a moment, about where money comes from.â€ Heâ€™s on his second Claude account to get around Anthropicâ€™s spending limits.I canâ€™t find any mention online of the per-account limits, but letâ€™s conservatively assume heâ€™s spending at least $2,000 USD per month, and liberally $5,000.The current cost is almost certainly artificially inflated by system inefficiency. Work gets lost, bugs get fixed numerous times, designs go missing and need redoing. As models improve and orchestration patterns mature, the cost of orchestrators like this should drop while output quality rises.I expect companies would happily pay around the $1-3k/month mark for a sane, understandable, higher quality, and lower waste version of Gas Town. Maybe that sounds absurd to you, given weâ€™ve all become anchored to the artificially low rate of $100-200/month for unlimited usage by the major providers. But once the AI bubble pops, the VC funds dry up, and providers have to charge the true cost of inference at scale, we should expect that â€œunlimitedâ€ tier to look a lot pricier.Even when that comes to pass, a few thousand is pretty reasonable when you compare it to an average US senior developer salary: $120,000 USD.  If Gas Town could genuinely speed up the work of a senior developer by 2-3x or more, it would easily be worth 10-30% of their salary. The cost per unit of valuable work starts to look competitive with human labour.Annual cost as a percentage of developer salaryThe maths on paying for something like this is already defensible in wealthier places like the US and parts of Western Europe. In spots where developer salaries are lower, we would expect the budget for AI assisted tools adjusts accordingly. Theyâ€™ll get less crazy scaled automation and more conservative usage with humans filling in the cognitive gaps.4. Yegge never looks at code. When should we stop looking too?Yegge is leaning into the true definition of     with this project: â€œIt is 100% vibecoded. Iâ€™ve never seen the code, and I never care to.â€ Not looking at code  is a very bold proposition, today, in January 2026.Given the current state of models and the meagre safeguards we have in place around them, the vast majority of us would consider this blind coding approach irresponsible and daft to do on anything that isnâ€™t a throwaway side project. Which, given the amount of effort and Claude tokens Yegge has sunk into building it, writing documentation, and publicly promoting it, Gas Town is not.â€œShould developers still look at code?â€ will become one of the most divisive and heated debates over the coming years. You might be offended by the question, and find it absurd anyone is asking. But itâ€™s a sincere question and the answer will change faster than you think.Iâ€™m already seeing people divide along moralistic, personal identity lines as they try to answer it. Some declare themselves purist, AI sceptic, Real Developers who check every diff and hand-adjust specific lines, sneering at anyone reckless enough to let agents run free. While others lean into agentic maximalism, directing fleets from on high and pitying the mass of luddites still faffing about with manual edits like itâ€™s 2019. Both camps mistake a contextual judgement for a personality trait and firm moral position.A more conservative, easier to consider, debate is: how  should the code be in agentic software development tools? How easy should it be to access? How often do we expect developers to edit it by hand?Interfaces like    ,    , and     do not put code front and centre in the experience. The agent is your first and primary interface. You might be able to see diffs rolls by or display code files inline, but you canâ€™t touch them. Trying to edit code yourself is a roundabout journey of opening your IDE and navigating to the correct files and lines.This design choice assumes it is easier to ask an agent to make the change for you, than it is to type it out the syntax yourself. They clearly say â€œwe donâ€™t believe users need to touch code.â€Framing this debate as an either/or â€“ either you look at code or donâ€™t, either you edit code by hand or you exclusively direct agents, either youâ€™re the anti-AI-purist or the agentic-maxxer â€“ is unhelpful. Because nothing is a strict binary.The right distance isnâ€™t about what kind of person you are or what you believe about AI capabilities in the current moment. How far away you step from the syntax shifts based on what youâ€™re building, who youâ€™re building with, and what happens when things go wrong. The degree of freedom you hand over to agents depends on:Domain and programming languageFront-end versus backend makes a huge difference. Language is a poor medium for designing easing curves and describing aesthetic feelings â€“ I always need to touch the CSS, and itâ€™s often faster to just tweak directly than try to explain what I want. Yeggeâ€™s CLI tooling is much easier to validate with pass/fail tests than evaluating whether a notification system â€œfeels calm enoughâ€. Model competence also varies wildly by language; prompting React and Tailwind gives you much better results than Rust or Haskell, where the models still regularly choke on borrow checkers and type systems.Access to feedback loops and definitions of successThe more agents can validate their own work, the better the results. If you let agents run tests and see the output, they quickly learn whatâ€™s broken and how to fix it. If you let them open browsers, take screenshots, and click around, they can spot their mistakes. Tools like the     lean into this â€“ it loops until tests pass or some specific condition is validated. This doesnâ€™t work for less defined, clear cut work though. If you try to make an agent design a visual diagram for you, itâ€™s going to struggle. It doesnâ€™t know your aesthetic preferences and canâ€™t really â€œseeâ€ what itâ€™s making.Risk tolerance for shit going wrongStakes matter. If an agent breaks some images on your personal blog, youâ€™ll recover. But if youâ€™re running a healthcare system where a bug could miscalculate drug dosages, or a banking app moving actual money around, you canâ€™t just wave an agent at it and hope. Consequences scale up fast. Corporate software has people whose entire job is compliance and regulatory sign-off â€“ they need to see the code, understand it, verify it meets requirements. Those people arenâ€™t going to let you wildly run Gas Town over projects without serious guardrails in place.â€œGas Town sounds fun if you are accountable to nobody: not for code quality, design coherence or inferencing costs. The rest of us are accountable for at least the first two and even in corporate scenarios where there is a blank check for tokens, that canâ€™t last. So the bottleneck is going to be how fast humans can review code and agree to take responsibility for it.â€Greenfield vs. brownfield projectsStarting fresh (greenfield), means you can let agents make architectural decisions and establish patterns â€“ if you donâ€™t like them, you can easily throw it out and restart. The cost of mistakes is low. But in an existing codebase (brownfield) with years of accumulated conventions, implicit patterns, and code that exists for reasons nobody remembers anymore, agents need much tighter supervision. Theyâ€™ll happily introduce a new pattern that contradicts the three other ways this codebase already solves the same problem.If youâ€™re solo of course you can YOLO. If youâ€™re working with more than a handful of people, youâ€™ll have to agree on coding standards and agent rules. This creates its own overhead: updating the AGENTS.md file, picking MCPs, writing commands and skills and rules and whatever else we invent to constrain these things. The pace of change when youâ€™re all using agents can be overwhelming and you need to figure out a sensible reviewing pipeline to manage it. Team coordination can fall apart when everyoneâ€™s agents start moving too fast. You might show up in the morning and discover someoneâ€™s agent renamed the database schema while another agent refactored the whole API layer, and neither of which jive with your giant, unmerged feature.More senior developers can prompt better, debug better, and setup more stringent preferences earned through decades of seeing what can go wrong in scaled, production environments. They can recognise patterns: â€œoh, thatâ€™s a memory leakâ€ or â€œthatâ€™s going to deadlock under load.â€ Newer developers donâ€™t have that catalogue of failures yet and are much more likely to prompt their own personal house of cards. The tests might pass and everything looks fine until you hit production traffic or someone enters a weird character. It is hard to defend against unknown unknowns.Given all these â€œit dependsâ€ considerations, Iâ€™m currently in the code-must-be-close camp for most serious work done by professional developers. But I expect Iâ€™ll shift to the code-at-a-distance camp over the next year or two as the harnesses and tools we wrap around these agents mature. If we can ship them with essential safe guards and quality gates, the risks drop. Sure, the models will also improve, but the infrastructure matters far more: validation loops, tests, and specialised subagents who focus on security, debugging, and code quality are what will make code-at-a-distance feasible.We have many, continuous versions of the code distance debate internally at    . One of the projects within the team driving this is     â€“ autonomous agents run through GitHub Actions in response to events: new PRs, new issues, or specific times of day. Every commit can trigger a security review agent, an accessibility audit, and a documentation updater, all running in parallel alongside traditional CI/CD tests before anything lands in main. The team building it rarely touches code and do most of their work by directing agents from their phones. Itâ€™s these kinds of guardrails that makes a hands-off Sim-City-esque orchestrator system feel less terrifying to me.I donâ€™t believe Gas Town itself is â€œitâ€. Itâ€™s not going to evolve into the thing we all use, day in and day out, in 2027. As I said, itâ€™s a provocative piece of speculative design, not a system many people will use in earnest. In the same way any poorly designed object or system gets abandoned, this manic creation is too poorly thought through to persist. But the problems itâ€™s wrestling with and the patterns it has sketched out will unquestionably show up in the next generation of development tools.As the pace of software development speeds up, weâ€™ll feel the pressure intensify in other parts of the pipeline: thoughtful design, critical thinking, user research, planning and coordination within teams, deciding what to build, and whether itâ€™s been built well. The most valuable tools in this new world wonâ€™t be the ones that generate the most code fastest. Theyâ€™ll be the ones that help us think more clearly, plan more carefully, and keep the quality bar high while everything accelerates around us.]]></content:encoded></item><item><title>The tech monoculture is finally breaking</title><link>http://www.jasonwillems.com/technology/2025/12/17/Tech-Is-Fun-Again/</link><author>at1as</author><category>hn</category><pubDate>Fri, 23 Jan 2026 15:26:33 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Growing up in the 90s and early 2000s, tech was a foundational part of my childhood.I built more physical computers than I can remember. We went from paper maps to GPS (which itself evolved from DVDs with static maps to internet-connected real-time navigation). CD players became MP3 players, then streaming services. We had Palm Pilots and early attempts at â€œsmartâ€ phones, which were anything but. Our computers could search for extraterrestrial life through SETI. We emerged from the pager era to portable phones to the entire internet in our pocket (which evolved from charging per SMS or megabyte to unlimited data plans).We went through what I still think of as a golden era of console gaming: the N64 & PlayStation, then PS2, Xbox, and GameCube. Meanwhile, our bulky CRT monitors became flat (with a misadventure toward â€œprojectionâ€ TVs in between). We could buy gadgets for everything. Best Buy and RadioShack felt like amusement parks weâ€™d visit without intention, ready to be drawn in by something new. A trip to Asiaâ€™s electronics stores felt like a genuine step into the future.Today, we have everything we did back then, and much more. And yet it somehow feels like weâ€™ve been left with less.In the early 2000s, tech began a decades-long consolidation. Almost everything we used before became a function of a single device. Objectively, this was an improvementâ€”old VCR interfaces were awful, early MP3 players were clunky, GPS lacked real-time traffic data, and nothing talked to each other. And yet, through that consolidation, something intangible was taken from us.Our devices lost their unique personalities. Phones became our alarm clocks, flashlights, calendars, watches, cameras, GPS units, music players, radios, journals, and gaming devicesâ€”all at once. We betrayed our focus in the pursuit of convenience, and the personality of our devices for homogeneity.The benefits were clear to us, but the costs werenâ€™t.This convergence created winner-take-all (and two-player) markets. Console gaming became PlayStation or Nintendo. Phones became Android or iOS. Computers became Mac or Windows. PC gaming became synonymous with Steam. Everything else became a feature inside one of those platforms, with globally synchronized updates making our experiences increasingly uniform, and bland.For a long time, that felt inevitable. But itâ€™s only become clear in retrospect that somewhere in the early 2020s, things started to change.New paradigms are emerging for the first time since mobile. VR is no longer experimental. Early AR is starting to reach consumers. Meta shipped a wearable that normal people actually use, thanks to a clever Ray-Ban partnership (and associated equity stake). 3D printers have become real household products. Wearables are diversifyingâ€”smart rings, over-the-counter glucose monitors, connected beds.Meanwhile, Appleâ€™s aggressive push for services revenue has alienated developers and users alike, creating space for alternatives. And nostalgia has revealed itself as massive, underserved economic demand.Gen-Z is buying single-purpose iPods and wired headphones. PokÃ©mon cards are trendy. My friends and I are amassing N64 game collections again. There is a revived appetite for film cameras and Polaroids. Companies are recreating old hardware in modern formâ€”ModRetroâ€™s upcoming FPGA-based M64 plays native N64 cartridges, following their successful Game Boy recreation. Theyâ€™re now working to bring a â€œnext-genâ€ CRT monitor to market. The Playdate proved thereâ€™s still room for third-party handhelds with their own unique philosophies. Even Nintendo couldnâ€™t resist capitalizing with the re-release of their classic consoles.Design matters again. In our devices, and in our lives. Art Deco is in vogue. Cyberpunk has never been more culturally mainstream. Color is back, and bold.Canon, Sony, and Nikon may have replaced Kodak for professionals, but Leica is thriving again and Kodak Instamatic has gone viral. People want devices that feel personalâ€”leather finishes, physical controls, intentional constraints. For years this expression was limited to phone cases. Now itâ€™s showing up in hardware itself.Tech is starting to resemble the wristwatch market: collaborations, limited editions, exclusivity. A market with many playersâ€”emerging companies, niche studios, design-forward brands, and even failing companiesâ€”is healthier than one dominated by a few giants.Antitrust pressure has slowed consolidation, opened app distribution, killed the anti-competitive iMessage and AirDrop moats, and made big tech cautious about horizontal expansion. And yet market forces may matter even more. Subscriptions keep multiplying. Advertising creeps into everything. Consolidated platforms are becoming bloated, degrading experiences. Platforms extract value in ways that betray their original philosophies.Appleâ€™s push toward services has been financially successful but culturally damaging. Users are looking elsewhere. It was imperceptible at first, but that sentiment is spreading.Barriers to entry are lower than theyâ€™ve been in decades. Software can be deployed in minutes. Hardware is still hard, but 3D printing has revolutionized prototyping and accessible manufacturing services have drastically lowered the cost and time to market. Even the consolidation on the USB-C standard has played a role, allowing switching devices without investing in a new ecosystem.Weâ€™ve also grown tired of curation by algorithm. What we watch is shaped by recommendation engines. How we perceive it is influenced by aggregate ratings. I miss wandering through video stores, choosing based on nothing more than a cover. Discovery felt accidental and my opinions felt like my own.Burnout plays a role too. A Timex ad went viral this year: â€œKnow the time without seeing you have 1,249 unanswered emails.â€ People are gravitating toward rigid, single-purpose experiences that let them fully disengage.Our appetite for alternatives has grown, while theyâ€™ve also become easier to create. LLMs and modern tools have lowered the effort required to build things. Side projects are easier to start and finish. Even when large companies offer better experiences on paper, individuals are building alternatives for the joy of it. Some go viral. Consumers end up with more choice.Nothing would have been harder to project than the growth of Linux on the desktop. Integrated platforms seemingly made the Linux philosophy untenable, and yet it may now be growing as a direct result of this decoupling. This was a feature, not a bug.Looking at my own purchases from 2025, the pattern becomes obvious:TRMNL (a no-distraction e-paper display)Android Pixel Pro (alongside iPhone)ASUS ROG laptop (for CUDA and gaming)Govee programmable lightsBambu Labs P1S 3D printerKindle (finally retiring my last mini-USB device)Abbott Lingo glucose sensoriPad (single-purpose: as a second MacBook display while traveling)More mechanical watches than I can count (while not tech per se, it does reduce the breadth of the Apple ecosystem)This is more than Iâ€™ve bought in the last 5 years, and Iâ€™m already excited for 2026. While Meta, Apple, Amazon, and Google still appear in my list, their purposes are narrower for me than in the past, and their presence is often no longer part of a two-player market. To be clear, these companies often make great products that should exist, but they should be easy to use as standalone Ã  la carte offerings, not forced omakase experiences.Weâ€™ll never truly recreate the late 80s or mid-90s. SaaS, subscription pricing, and centralized platforms are here to stay. But this feels like the beginning of another golden eraâ€”one defined less by consolidation and more by variety, personality, and choice.]]></content:encoded></item><item><title>KORG phase8 â€“ Acoustic Synthesizer</title><link>https://www.korg.com/us/products/dj/phase8/</link><author>bpierre</author><category>hn</category><pubDate>Fri, 23 Jan 2026 14:34:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Radicle: The Sovereign Forge</title><link>https://radicle.xyz/</link><author>ibobev</author><category>hn</category><pubDate>Fri, 23 Jan 2026 13:25:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ is a sovereign
       built on Git.
    Radicle is an open source, peer-to-peer code collaboration stack built on Git.
Unlike centralized code hosting platforms, there is no single entity
controlling the network. Repositories are replicated across peers in a
decentralized manner, and users are in full control of their data and workflow.To install Radicle, simply run the command below from your shell, or go to the
download page.Alternatively, you can build from source.For now, Radicle only works on Linux, macOS and BSD variants.The Radicle protocol leverages cryptographic identities for code and social
artifacts, utilizes Git for efficient data transfer between peers, and employs
a custom gossip protocol for exchanging repository metadata.Your Data, Forever and SecureAll social artifacts are stored in Git, and signed using public-key
cryptography. Radicle verifies the authenticity and authorship of all data
for you.Radicle enables users to run their own nodes, ensuring censorship-resistant
code collaboration and fostering a resilient network without reliance on
third-parties.Radicle is local-first, providing always-available functionality even
without internet access. Users own their data, making migration, backup, and
access easy both online and offline.Radicleâ€™s Collaborative Objects (COBs) provide Radicleâ€™s . This enables features such as issues, discussions and code review
to be implemented as Git objects. Developers can extend Radicleâ€™s capabilities
to build any kind of collaboration flow they see fit.The Radicle Stack comes with a CLI, web interface and TUI, that are backed by
the Radicle Node and HTTP Daemon. Itâ€™s modular, so any part can be swapped out
and other clients can be developed.â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Radicle CLI    â”‚â”‚ Radicle Web    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Radicle Repository               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  code  â”‚ â”‚ issues â”‚ â”‚ patches â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Radicle Storage (Git)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Radicle Node  â”‚â”‚  Radicle HTTPD  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    NoiseXK     â”‚â”‚   HTTP + JSON   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Radicle is  software under the MIT and Apache 2.0
licenses. Get involved by contributing code.]]></content:encoded></item><item><title>Microsoft mishandling example.com</title><link>https://tinyapps.org/blog/microsoft-mishandling-example-com.html</link><author>mrled</author><category>hn</category><pubDate>Fri, 23 Jan 2026 13:04:09 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ Since at least February 2020, Microsoft's Autodiscover service has incorrectly routed the IANA-reserved  to Sumitomo Electric Industries' mail servers at , potentially sending test credentials there.While setting up  as a dummy account in Outlook (on both Windows and macOS), Outlook consistently auto-configured it to use  (IMAP) and  (SMTP) despite  being an IANA-reserved domain that should not resolve to real services.The same behavior appeared on different machines, profiles, networks, and DNS resolvers, including a newly provisioned Windows 365 Cloud PC:Confirm that  has no DNS records pointing to : dig MX example.com +short dig CNAME autodiscover.example.com +short dig SRV _autodiscover._tcp.example.com +shortThe domain has a null MX record (indicating it doesn't accept email) and no Autodiscover DNS entries, confirming the misconfiguration exists entirely within Microsoft's database.Microsoft autodiscover API responseMicrosoft's Autodiscover service misconfiguration can be confirmed via curl -v -u "email@example.com:password" "https://prod.autodetect.outlook.cloud.microsoft/autodetect/detect?app=outlookdesktopBasic":{
  "email": "email@example.com",
  "services": [],
  "protocols": [
    {
      "protocol": "imap",
      "hostname": "imapgms.jnet.sei.co.jp",
      "port": 993,
      "encryption": "ssl",
      "username": "email@example.com",
      "validated": false
    },
    {
      "protocol": "smtp",
      "hostname": "smtpgms.jnet.sei.co.jp",
      "port": 465,
      "encryption": "ssl",
      "username": "email@example.com",
      "validated": false
    }
  ]
}The  header (Base64-decoded) reveals additional details:This misconfiguration has existed for nearly six years and was not crowdsourced. It appears to have been manually added to Microsoft's database.]]></content:encoded></item><item><title>European Alternatives</title><link>https://european-alternatives.eu/</link><author>s_dev</author><category>hn</category><pubDate>Fri, 23 Jan 2026 13:01:51 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
                        We help you find European alternatives for digital service and products, like cloud services and SaaS products.
                    
                            When you buy from local businesses, you are supporting yourself down the road. Taxes paid by the company come back to you indirectly and the company creates jobs in your region.
                        
                            Some companies outside Europe tend to ignore data protection and related laws such as the GDPR or do not implement them correctly.
                        
                            As a business that operates in Europe, it is possible to get a VAT refund for products/services of other European companies. European companies also tend to offer payment methods that are commonly used in Europe.
                        
                            Within the EU, many laws and framework conditions are set by the EU, which helps to cover a large market without having to consider large country-specific differences. It is also easier to enforce your rights against another company located in the EU.
                        ]]></content:encoded></item><item><title>What has Docker become?</title><link>https://tuananh.net/2026/01/20/what-has-docker-become/</link><author>tuananh</author><category>hn</category><pubDate>Fri, 23 Jan 2026 12:36:17 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
      Posted on 
  
    January 20, 2026
  


      
        Â â€¢Â 
      
      
      5Â minutes
      Â â€¢
      
      854Â words
      
    Itâ€™s weird to see Docker Inc (the company) struggle to find its place in 2026. What started as the company that revolutionized how we deploy applications has been through multiple identity crises, pivoting from one strategy to another in search of sustainable revenue and market relevance.Dockerâ€™s journey reads like a startup trying to find product-market fit, except Docker already had product-market fit - they created the containerization standard that everyone uses. The problem is that Docker the technology became so successful that Docker the company struggled to monetize it. When your core product becomes commoditized and open source, you need to find new ways to add value.Docker Swarm was Dockerâ€™s attempt to compete with Kubernetes in the orchestration space. But Kubernetes won that battle decisively, and Docker eventually sold Swarm. This was a clear signal that Docker was stepping back from trying to be the full-stack container platform and instead focusing on what they could uniquely provide.For a while, Docker seemed to focus on developer experience. This made sense - developers are Dockerâ€™s core users, and improving their workflow could be a differentiator. Docker Scout emerged from the acquisition of Atomist in June 2022, bringing â€œsoftware supply chainâ€ capabilities. Scout allows Docker to see not just whatâ€™s in a container, but how it was built and where vulnerabilities are. This was a smart move toward security and observability, areas where Docker could add real value.Docker also acquired AtomicJar, the company behind Testcontainers, adding shift-left testing capabilities. Testcontainers lets developers run real dependencies (databases, message queues, etc.) in containers during testing, making integration tests more reliable and closer to production environments.Then came the AI pivot. Docker Model Runner entered the scene, positioning Docker as a platform for running AI models. Docker Compose expanded to support AI agents and models. Docker Offload was introduced for cloud-scale GPU execution of AI tasks. Partnerships with Google Cloud, Microsoft Azure, and AI SDKs (CrewAI, LangGraph, Vercel AI SDK) followed.The acquisition of MCP Defender in September 2025 further cemented Dockerâ€™s move into AI security, focusing on securing agentic AI infrastructure and runtime threat detection. This was a significant shift - from developer tools to AI infrastructure.Suddenly, Docker moved into the hardened images space. In December 2025, Docker made over 1,000 Docker Hardened Images free and open source under Apache 2.0, reducing vulnerabilities by up to 95% compared to traditional images. This move was likely triggered by Chainguardâ€™s success in the secure container image space. Chainguard had been building a business around minimal, secure container images, and Docker needed to respond.Making hardened images free was a bold move - itâ€™s hard to compete with free, especially when itâ€™s open source. But it also raises questions about Dockerâ€™s business model. If youâ€™re giving away your security features for free, what are you selling?Leadership Changes and Acquisition SpeculationIn February 2025, Docker replaced CEO Scott Johnston (who led the company since 2019) with Don Johnson, a former Oracle Cloud Infrastructure founder and executive vice president. This leadership transition has prompted tech analysts to anticipate a potential acquisition by a major cloud provider. The CEO swap, combined with the strategic pivots, suggests Docker may be positioning itself for sale rather than building a standalone business.Dockerâ€™s strategic shifts tell a story of a company searching for its place in a market it helped create. The containerization technology Docker pioneered became so successful that it became infrastructure - something everyone uses but no one wants to pay for directly.The pivots from orchestration (Swarm) to developer tools (Scout, Testcontainers) to AI (Model Runner, MCP Defender) to security (Hardened Images) show a company trying different approaches to find sustainable revenue. Each pivot makes sense in isolation, but together they paint a picture of a company without a clear long-term vision.The hardened images move is particularly interesting because itâ€™s defensive - responding to Chainguardâ€™s success rather than leading with innovation. Making it free and open source is a strong competitive move, but it doesnâ€™t solve the fundamental business model question.Docker the technology isnâ€™t going anywhere. Itâ€™s too embedded in the infrastructure of modern software development. But Docker the company? Thatâ€™s less clear. The leadership change, acquisition speculation, and rapid strategic pivots suggest Docker Inc may be positioning itself for an exit rather than building a long-term independent business.For developers, this doesnâ€™t change much. Docker containers will continue to work, and the open source nature of Docker means the technology will persist regardless of what happens to the company. But itâ€™s worth watching how Docker Incâ€™s search for identity plays out - it could affect the ecosystem of tools and services built around containers.The irony is that Docker created a standard so successful that it became infrastructure, and infrastructure is hard to monetize. Docker Incâ€™s struggle to find its place is a cautionary tale about the challenges of building a business around open source technology that becomes too successful.]]></content:encoded></item></channel></rss>
<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://konrad.website/feeds/</link><description></description><item><title>FLUX.2 [Klein]: Towards Interactive Visual Intelligence</title><link>https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence</link><author>GaggiX</author><category>hn</category><pubDate>Fri, 16 Jan 2026 23:46:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[FLUX.2 [klein]: Towards Interactive Visual IntelligenceToday, we release the FLUX.2 [klein] model family, our fastest image models to date. FLUX.2 [klein] unifies generation and editing in a single compact architecture, delivering state-of-the-art quality with end-to-end inference as low as under a second. Built for applications that require real-time image generation without sacrificing quality, and runs on consumer hardware with as little as 13GB VRAM.Demo showing editing with FLUX.2 [klein]Visual Intelligence is entering a new era. As AI agents become more capable, they need visual generation that can keep up; models that respond in real-time, iterate quickly, and run efficiently on accessible hardware.The  name comes from the German word for "small", reflecting both the compact model size and the minimal latency. But FLUX.2 [klein] is anything but limited. These models deliver exceptional performance in text-to-image generation, image editing and multi-reference generation, typically reserved for much larger models.Note: The “FLUX [dev] Non-Commercial License” has been renamed to “FLUX Non-Commercial License” and will apply to the 9B Klein models. No material changes have been made to the license.Text to Image collage using FLUX.2 [klein]The FLUX.2 [klein] Model FamilyOur flagship small model. Defines the Pareto frontier for quality vs. latency across text-to-image, single-reference editing, and multi-reference generation. Matches or exceeds models 5x its size - in under half a second. Built on a 9B flow model with 8B Qwen3 text embedder, step-distilled to 4 inference steps.Combine multiple input images, blend concepts, and iterate on complex compositions - all at sub-second speed with frontier-level quality. No model this fast has ever done this well.Imagine editing collage using FLUX.2 [klein]Fully open under Apache 2.0. Our most accessible model, it runs on consumer GPUs like the RTX 3090/4070. Compact but capable: supports T2I, I2I, and multi-reference at quality that punches above its size. Built for local development and edge deployment.FLUX.2 [klein] Base 9B / 4B:The full-capacity foundation models. Undistilled, preserving complete training signal for maximum flexibility. Ideal for fine-tuning, LoRA training, research, and custom pipelines where control matters more than speed. Higher output diversity than the distilled models.: 4B Base under Apache 2.0, 9B Base under FLUX NCLOutput Diversity using FLUX.2 [klein]We are also releasing FP8 and NVFP4 versions of all [klein] variants, developed in collaboration with NVIDIA for optimized inference on RTX GPUs. Same capabilities, smaller footprint - compatible with even more hardware.Benchmarks on RTX 5080/5090, T2I at 1024×1024Same licenses apply: Apache 2.0 for 4B variants, FLUX NCL for 9B.FLUX.2 [klein] Elo vs Latency (top) and VRAM (bottom) across Text-to-Image, Image-to-Image Single Reference, and Multi-Reference tasks.FLUX.2 [klein] matches or exceeds Qwen's quality at a fraction of the latency and VRAM, and outperforms Z-Image while supporting both text-to-image generation and (multi-reference) image editing in a unified model. The base variants trade some speed for full customizability and fine-tuning, making them better suited for research and adaptation to specific use cases. Speed is measured on a GB200 in bf16.FLUX.2 [klein] is more than a faster model. It's a step toward our vision of interactive visual intelligence. We believe the future belongs to creators and developers with AI that can see, create, and iterate in real-time. Systems that enable new categories of applications: real-time design tools, agentic visual reasoning, interactive content creation.]]></content:encoded></item><item><title>Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation</title><link>https://cloud.google.com/blog/topics/threat-intelligence/net-ntlmv1-deprecation-rainbow-tables</link><author>linolevan</author><category>hn</category><pubDate>Fri, 16 Jan 2026 21:42:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>LWN is currently under the heaviest scraper attack seen yet</title><link>https://social.kernel.org/notice/B2JlhcxNTfI8oDVoyO</link><author>luu</author><category>hn</category><pubDate>Fri, 16 Jan 2026 20:37:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
  Please do not use this service in violation of the Linux
  Kernel Code of Conduct. Doing so will result in your account suspension with the referral of the matter
  to the CoC committee.

  "Repeating"/"boosting" someone else's status on this platform will be treated as endorsement and will fall under
  rule #1.

  You are encouraged to use this platform to promote your work on the Linux Kernel, but there is no restriction
  on permitted topics (with the exception of anything covered by #1 above).

  There is no requirement to post in English, but it should be considered the primary language of communication
  on this platform.

  The admins of this service have access to all posted statuses. They aren't looking, but if it's something they
  shouldn't know about, then you should not post it on this platform.

  If you would like an account on this instance, please check that the following applies to you:
: You have a kernel.org account or email address
  : You have a long and established history of involvement with the Linux Kernel
  
  If the above is true and you agree with the  and 
  listed above, please use these instructions to request an account:
]]></content:encoded></item><item><title>Slop is everywhere for those with eyes to see</title><link>https://www.fromjason.xyz/p/notebook/slop-is-everywhere-for-those-with-eyes-to-see/</link><author>speckx</author><category>hn</category><pubDate>Fri, 16 Jan 2026 20:03:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The size of your plate can influence how much food you eat. The absence of a clock on a casino wall can keep you gambling through the early morning. On social media, our For You Pages give us the illusion of infinite content. How our environments are designed influences how we consume. And wouldn't you know it, everything around us is designed for maximum consumption.Open TikTok, and you can easily burn through a hundred videos or more before you glance at the time. It doesn't help that the For You Page hides the time on our phones.We are over consuming content on the FYP. The sudden surge of low-quality, AI-generated content, i.e. “AI slop,” is a byproduct of that overconsumption. We don't see it because, well, we're conditioned not to, but slop always arrives on time. Slop is inevitable. Slop is quintessential. Slop is everywhere for those with eyes to see.Olive oil, wasabi, saffron, vanilla, Wagyu, honey, champagne, and truffle,...reality TV, all hold examples of what happens when  exceeds — companies fill the gap with slop. The free market loves a good filler. So, why should the digital realm be any different?The For You page is designed to keep us playing the dopamine slot machine for as long as possible. The Average Time on Site metric is still the goose that lays the golden eggs, and both TikTok and Meta are reporting that their egg baskets have never been fuller.But, there's a problem. On any given platform, only 1-3% of users publish content. It's called the 90-9-1 rule, and platforms that rely on free user generated content have been trying to solve this problem since the beginning of the commercialized web. The introduction of the For You Page, and the illusion of endless content, has only exasperated the inequity.Curation used to be part of our media consumption process. We would hop from website to website looking for a laugh. We used to  for Christ's sake. Now, all we must do is sit at the trough￼ and let daddy Zuck feed us.In a recent essay, Joan Westenberg makes a complementary argument that the algorithm has “flattened” curiosity by eliminating the need to “hunt” for our content. They go on to say:There’s a concept in behavioral science called the “effort heuristic.” It’s the idea that we tend to value information more if we worked for it. The more effort something requires, the more meaning we assign to the result. When all knowledge is made effortless, it’s treated as disposable. There’s no awe, no investment, no delight in the unexpected—only consumption.(I'm reminded of the scene in Jurassic Park when the tour Jeep pulls up to the Tyrannosaurus rex exhibit. Doctor Grant says￼ “The T-Rex doesn't want to be fed. It wants to hunt.”)￼This type of mindless consumption is not only harming our curiosity, it's helping to cheapen creativity for the people who produce what we consume.Creativity isn't scalable. Content creation has a hard productivity ceiling. Every human-created video on our feeds require some level of writing, production, and editing. Yet the For You Page has made the content consumption so efficient, that perhaps demand has exceeded supply.If you're a product manager for a social media platform, you can reduce the friction of publishing content to the app, or ship better editing tools, but you can't optimize creative spark. You can't treat humans like content-generating machines (as much as they have tried). Despite the illusion of infinite scrolling thanks to the FYP, art remains a finite resource bound to the whims of human creativity.Mark Zuckerberg wants us on his platforms, flicking our thumbs, for as long as possible. But the more we open Instagram, the more creators he needs posting multiple times each day. Mark has very little control over this variable. Creators could suddenly post less, or simply stop posting all together, and there's nothing he could do about it. What's worse, creators could demand Meta pay them for their art.Actually, yes. And it turns out, you could rather effectively kill a platform if you got a small group of top creators organized and angry.In the summer of 2016, twenty social media personalities took down one of the largest mobile video apps on the internet. They wanted money for their labor. The executives at Vine said no. The gang of twenty, who were the highest performing creators on the app, walked away. They stopped posting entertaining content to Vine, and instead repeatedly implored their followers to find them on competing apps.Vine shut down for good just months later.Vine’s spectacular rise and fall showed the power of online creators. Its demise offers crucial lessons for platforms trying to engage with power users — and a deeper understanding of who ultimately controls a social product.Vine creators exposed and exploited a weakness in Vine's conventional approach to social media. Follower count had power. Old-style discovery algorithms could be easily manipulated. Vine creators used that power to take over the app, and convinced users to migrate to other platforms.You see why follower counts are less important today, and why black-box algorithms have full control over who goes viral and who gets “shadow banned.” TikTok saw the mistakes of its predecessor, and made it so content creators could never exercise collective influence again.Because virality now feels more like gambling, I suspect people post more content today than a decade ago. But it's not enough. Our insatiable appetites for content is pushing for corporations to meet that demand with slop. ￼If it were up to TikTok and Meta, our feeds would be exclusively robot-made. Humans are a variable they cannot control, and I think they despise us for it.Anyway, I have good news. Outside of our FYPs you'll find a surplus of art, essays, articles, and videos just waiting to be discovered. And best of all, these artists and writers are making things on their own terms. We, too, can enjoy the products of their labor on our terms, while not giving a dime of our attention to big tech.This is the open web. Or the social web. Or the open social web. Or the-- you get the point. To find it, you must reacquaint yourself with the lost art of surfing the web.Surfing the web is very different than scrolling the FYP. You don't often hear the words ”mindful” and “internet” together but, surfing the web was an art of mindful consumption that doesn't much exist today. Not to get all  at you, but maybe we should bring it back?Up next: The Lost Art of Surfing The Web (coming soon)]]></content:encoded></item><item><title>Reading across books with Claude Code</title><link>https://pieterma.es/syntopic-reading-claude/</link><author>gmays</author><category>hn</category><pubDate>Fri, 16 Jan 2026 18:49:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[LLMs are overused to summarise and underused to help us read deeper.To explore how they can enrich rather than reduce, I set Claude Code up with tools to mine a library of 100 non-fiction books.
It found sequences of excerpts connected by an interesting idea, or .Here’s a part of one such trail, linking deception in the startup world to the social psychology of mass movements (I’m especially pleased by the jump from Jobs to Theranos):Claude browses the books a chunk at a time. A chunk is a segment of roughly 500 words that aligns with paragraphs when possible.
This length is a good balance between saving tokens and providing enough context for ideas to breathe.Chunks are indexed by topic, and topics are themselves indexed for search. This makes it easy to look up all passages in the corpus that relate to, say, .This works well when you know what to look for, but search alone can’t tell you which topics are present to begin with.
There are over 100,000 extracted topics, far too many to be browsed directly. To support exploration, they are grouped into a hierarchical tree structure.This yields around 1,000 top-level topics. They emerge from combining lower-level topics, and not all of them are equally useful:Incidents that frustrated Ev WilliamsNames beginning with “Da”Events between 1971 & 1974However, this Borgesian taxonomy is good enough for Claude to piece together what the books are about.Claude uses the topic tree and the search via a few CLI tools.
They allow it to:Find all chunks associated with a topic similar to a query.Find topics which occur in a window of chunks around a given topic.Find topics that co-occur in multiple books.Browse topics and chunks that are siblings in the topic tree.To generate the trails, the agent works in stages.First, it scans the library and the existing trails, and proposes novel trail ideas. It mainly browses the topic tree to find unexplored areas and rarely reads full chunks in depth.Then, it takes a specific idea and turns it into a trail. It receives seed topics from the previous stage and browses many chunks.
It extracts excerpts, specific sequences of sentences, and decides on how best to order them to support an insight.Finally, it adds highlights and edges between consecutive excerpts.Claude Code is great for non-coding tasksEven though I’ve been using Claude Code to develop for months, my first instinct for this project was to consider it as a traditional pipeline of several discrete stages.
My initial attempt at this system consisted of multiple LLM modules with carefully hand-assembled contexts.On a whim, I ran Claude with access to the debugging tools I’d been using and a minimal prompt: “find something interesting.”
It immediately did a better job at pulling in what it needed than the pipeline I was trying to tune by hand, while requiring much less orchestration.
It was a clear improvement to push as much of the work into the agent’s loop as possible.I ended up using Claude as my main interface to the project.
Initially I did so because it inferred the sequence of CLI calls I wanted to run faster than I could recall them.
Then, I used it to automate tasks which weren’t rigid enough to be scripted traditionally.The latter opened up options that I wouldn’t have considered before.
For example, I changed my mind on how short I wanted excerpts to be.
I communicated my new preference to Claude, which then looked through all the existing trails and edited them as necessary, balancing the way the overall meaning of the trail changed.
Previously, I would’ve likely considered all previous trails to be outdated and generated new ones, because the required edits would’ve been too nuanced to specify.In general, agents have widened my ambitions.
By taking care of the boilerplate, I no longer shy away from the tedious parts.
Revision is cheap, so I don’t need to plow ahead with suboptimal choices just because it’d be too costly to undo them.
This, in turn, keeps up the momentum and lets me focus on the joyful, creative aspects of the work.Ask the agent what it needsMy focus went from optimising prompts to implementing better tools for Claude to use, moving up a rung on the abstraction ladder.My mental model of the AI component changed: from a function mapping input to output, to a coworker I was assisting.
I spent my time thinking about the affordances that would make the workflow better, as if I were designing them for myself.
That they were to be used by an agent was a mere detail.This worked because the agent is now intelligent enough that the way it uses these tools overlaps with my own mental model.
It is generally easy to empathise with it and predict what it will do.Initially I watched Claude’s logs closely and tried to guess where it was lacking a certain ability.
Then I realised I could simply ask it to provide feedback at the end and list the functionality it wished it had.
Claude was excellent at proposing new commands and capabilities that would make the work more efficient.Claude suggested improvements, which Claude implemented, so Claude could do the work better.
At least I’m still needed to pay for the tokens — for now.Novelty is a useful guideIt’s hard to quantify  as an objective to optimise for.Why Greatness Cannot Be Planned makes the case that chasing  is often a more fruitful approach.
While its conclusions are debated, I’ve found this idea to be a good fit for this project.As a sign of the times, this novelty search was implemented in two ways:By biasing the search algorithm towards under-explored topics and books.A topic’s novelty score was calculated as the mean distance from its embedding’s  nearest neighbors. A book’s novelty score is the average novelty of the unique topics that it contains.
This value was used to rank search results, so that those which were both relevant and novel were more likely to be seen.On a prompting level, Claude starts the ideation phase by looking at all the existing trails and is asked to avoid any conceptual overlap.
This works fairly well, though it is often distracted by any topics related to secrecy, systems theory, or tacit knowledge.It’s as if the very act of finding connections in a corpus summons the spirit of Umberto Eco and amps up the conspiratorial thinking.EPUBs are parsed using , which I picked over BeautifulSoup for its speed and simpler API.Everything from the plain text to the topic tree is stored in SQLite. Embeddings are stored using .The text is split into sentences using  (the  model).
Those sentences are then grouped into chunks, trying to get up to 500 words without breaking up paragraphs.I used  to call LLMs. It worked well for the structured data extraction and it was easy to switch out different models to experiment.
I tried its prompt optimizers before I went full agentic, and their results were very promising.I settled on Gemini 2.5 Flash Lite for topic extraction.
The model gets passed a chunk and is asked to return 3-5 topics. It is also asked whether the chunk is , in order to filter out index entries, acknowledgements, orphan headers, etc.
I was surprised at how stable these extracted topics were: similar chunks often shared some of the exact same topic labels.
Processing 100 books used about 60M input tokens and ~£10 in total.After a couple books got indexed, I shared the results with Claude Opus along with the original prompt and asked it to improve it.
This is a half-baked single iteration of the type of prompt optimisation DSPy implements, and it worked rather well.Topic pairs with a distance below a threshold get merged together. This takes care of near-duplicates such as , , and .The CLI output uses a semi-XML format. In order to stimulate navigating, most output is nested with related content. For example, when searching for a topic, chunks are shown with the other topics they contain.
This allows us to get a sense of what the chunk is about, as well as which other topics might be interesting.
There’s probably more token-efficient formats, but I never hit the limit of the context window.Topics are embedded using google/embeddinggemma-300m and reranked using .Many CLI tools require loading the embedding model and other expensive state. The first call transparently starts a separate server process which loads all these resources once and holds onto them for a while.
Subsequent CLI calls use this server through Python’s multiprocessing.connection.The topic collection is turned into a graph (backed by ) by adding edges based on the similarity of their embeddings and the point-wise mutual information of their co-occurrences.The graph is turned into a tree by applying Leiden partitioning recursively until a minimum size is reached.
I tried the Surprise quality function because it had no parameters to tweak, and found it to be good enough. Each group is labelled by Gemini based on all the topics that it contains.Excerpts are cleaned by Gemini to remove EPUB artifacts, parsing errors, headers, footnotes, etc.
Doing this only for excerpts that are actually shown, instead of during pre-processing, saved a lot of tokens.]]></content:encoded></item><item><title>Our approach to advertising</title><link>https://openai.com/index/our-approach-to-advertising-and-expanding-access/</link><author>rvz</author><category>hn</category><pubDate>Fri, 16 Jan 2026 18:02:19 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>STFU</title><link>https://github.com/Pankajtanwarbanna/stfu</link><author>tanelpoder</author><category>hn</category><pubDate>Fri, 16 Jan 2026 17:32:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>East Germany balloon escape</title><link>https://en.wikipedia.org/wiki/East_Germany_balloon_escape</link><author>robertvc</author><category>hn</category><pubDate>Fri, 16 Jan 2026 17:16:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Dell UltraSharp 52 Thunderbolt Hub Monitor</title><link>https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories</link><author>cebert</author><category>hn</category><pubDate>Fri, 16 Jan 2026 17:14:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Best of Show 2026: Dell UltraSharp 52 Thunderbolt Hub Monitor - U5226KWPCMag is a trademark of Ziff Davis, LLC. Used under license. Reprinted with permission. ©2026 Ziff Davis, LLC. All Rights Reserved.]]></content:encoded></item><item><title>Canada slashes 100% tariffs on Chinese EVs to 6%</title><link>https://electrek.co/2026/01/16/canada-breaks-with-us-slashes-100-tariffs-chinese-evs/</link><author>1970-01-01</author><category>hn</category><pubDate>Fri, 16 Jan 2026 17:05:28 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[In a massive shift in North American trade policy, Prime Minister Mark Carney announced today a new “strategic partnership” with China that effectively reopens the Canadian border to Chinese electric vehicles.The move marks a significant departure from the United States’ hardline protectionist stance and could bring affordable EV options like the BYD Seagull to Canadian roads as early as this year.For the last two years, Canada has largely walked in lockstep with the US regarding Chinese EV tariffs. Following the Biden administration’s move to impose 100% tariffs on Chinese EVs, Canada implemented similar surtaxes, effectively freezing companies like BYD, Nio, and Zeekr out of the market.Today, that ice is breaking.As part of a broader trade agreement secured by Prime Minister Carney in Beijing this week, Canada has agreed to allow an annual quota of 49,000 Chinese electric vehicles into the country at the tariff rate of just .According to the Prime Minister’s office, this volume represents less than 3% of the Canadian new vehicle market. However, the deal explicitly targets the low end of the market, with the government anticipating that within five years, “more than 50% of these vehicles will be affordable EVs with an import price of less than $35,000.”In exchange for opening the EV floodgates (or at least starting to break the dam), China has agreed to lower tariffs on Canadian canola seed from roughly 85% to 15% and to lift restrictions on Canadian lobster and crab.The Canadian government claims this isn’t just about imports. The text of the agreement states that the deal is expected to “drive considerable new Chinese joint-venture investment in Canada” to build out the domestic EV supply chain.While 49,000 vehicles might sound like a small number compared to the total market, it’s a specific, targeted wedge that changes the entire dynamic of the North American EV market.For years, we at  have argued that protectionism, while perhaps protecting legacy automaker jobs in the short term, ultimately hurts consumers and slows down the transition to sustainable transport.Meanwhile, protecting domestic automakers from Chinese competition in their home market makes them less competitive on the global stage, virtually giving the global market to China.The reality is that Chinese automakers are currently building some of the best, most affordable EVs in the world. Keeping them out entirely not only hurts consumers but also hurts innovation.Of course, this is going to make Washington furious. The US has been trying to build a “Fortress North America” against Chinese EVs. By letting 49,000 units in tariff-free (or near tariff-free), Canada is effectively saying it values affordable climate solutions (and canola exports) more than complete alignment with US industrial policy, which is understandable since the US was the one to go hostile on trade with Canada.The interesting detail here is the “Joint Venture” language. It looks like Carney is taking a page out of China’s own playbook. Canada seems to be using this quota as a carrot to get companies like BYD or CATL to set up shop in Canada and maybe help Canadian companies learn from those giants.FTC: We use income earning auto affiliate links.More.]]></content:encoded></item><item><title>6-Day and IP Address Certificates Are Generally Available</title><link>https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability</link><author>jaas</author><category>hn</category><pubDate>Fri, 16 Jan 2026 15:37:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Short-lived and IP address certificates are now generally available from Let’s Encrypt. These certificates are valid for 160 hours, just over six days. In order to get a short-lived certificate subscribers simply need to select the ‘shortlived’ certificate profile in their ACME client.Short-lived certificates improve security by requiring more frequent validation and reducing reliance on unreliable revocation mechanisms. If a certificate’s private key is exposed or compromised, revocation has historically been the way to mitigate damage prior to the certificate’s expiration. Unfortunately, revocation is an unreliable system so many relying parties continue to be vulnerable until the certificate expires, a period as long as 90 days. With short-lived certificates that vulnerability window is greatly reduced.Short-lived certificates are opt-in and we have no plan to make them the default at this time. Subscribers that have fully automated their renewal process should be able to switch to short-lived certificates easily if they wish, but we understand that not everyone is in that position and generally comfortable with this significantly shorter lifetime. We hope that over time everyone moves to automated solutions and we can demonstrate that short-lived certificates work well.Our default certificate lifetimes will be going from 90 days down to 45 days over the next few years, as previously announced.IP address certificates allow server operators to authenticate TLS connections to IP addresses rather than domain names. Let’s Encrypt supports both IPv4 and IPv6. IP address certificates must be short-lived certificates, a decision we made because IP addresses are more transient than domain names, so validating more frequently is important. You can learn more about our IP address certificates and the use cases for them from our post announcing our first IP Certificate.We’d like to thank the Open Technology Fund and Sovereign Tech Agency, along with our Sponsors and Donors, for supporting the development of this work.]]></content:encoded></item><item><title>Read_once(), Write_once(), but Not for Rust</title><link>https://lwn.net/SubscriberLink/1053142/8ec93e58d5d3cc06/</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 16 Jan 2026 15:04:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!

           By January 9, 2026
The  and  macros are heavily used
within the kernel; there are nearly 8,000 call sites for
.  They are key to the implementation of many lockless algorithms and can be necessary for some
types of device-memory access.  So one might think that, as the
amount of Rust code in the kernel increases, there would be a place for
Rust versions of these macros as well.  The truth of the matter, though, is
that the Rust community seems to want to take a different approach to
concurrent data access.

An understanding of  and  is
important for kernel developers who will be dealing with any sort of
concurrent access to data.  So, naturally, they are almost entirely absent
from the kernel's documentation.  A description of sorts can be found at
the top of :

	Prevent the compiler from merging or refetching reads or
 	writes. The compiler is also forbidden from reordering successive
 	instances of READ_ONCE and WRITE_ONCE, but only when the compiler
 	is aware of some particular ordering. One way to make the compiler
 	aware of ordering is to put the two invocations of READ_ONCE or
 	WRITE_ONCE in different C statements.

In other words, a  call will force the compiler to read
from the indicated location exactly one time, with no optimization tricks
that would cause the read to be either elided or repeated;
 will force a write under those terms.  They will also
ensure that the access is atomic; if one task reads a location with
 while another is writing that location, the read will
return the value as it existed either before or after the write, but not
some random combination of the two.  These macros, other than as described
above, impose no ordering constraints on the compiler or the CPU, making
them different from macros like , which have
stronger ordering requirements.

The  and  macros were added for the 3.18
release in 2014.   was initially called
, but that name was changed during the
3.19 development cycle.

Some of the other kernel Rust developers objected to this change, though.
Gary Guo said that he
would rather not expose  and  and
suggested using relaxed operations from  the kernel's 
module instead.  Boqun Feng expanded on the
objection:

	The problem of READ_ONCE() and WRITE_ONCE() is that the semantics
	is complicated. Sometimes they are used for atomicity, sometimes
	they are used for preventing data race. So yes, we are using LKMM
	[the Linux kernel memory model] in Rust as well, but whenever
	possible, we need to clarify the intention of the API, using
	Atomic::from_ptr().load(Relaxed) helps on that front.

	IMO, READ_ONCE()/WRITE_ONCE() is like a "band aid" solution to a
	few problems, having it would prevent us from developing a more
	clear view for concurrent programming.

In other words, using the  crate allows developers to
specify more precisely which guarantees an operation needs, making the
expectations (and requirements) of the code more clear.
This point of view would appear to have won out, and Ryhl has stopped
pushing for this addition to the kernel's Rust code — for now, at least.

There are a couple of interesting implications from this outcome, should it
hold.  The first of those is that, as Rust code reaches more deeply into the
core kernel, its code for concurrent access to shared data will look
significantly different from the equivalent C code, even though the code on
both sides may be working with the same data.  Understanding lockless data
access is challenging enough when dealing with one API; developers may now
have to understand two APIs, which will not make the task easier.

Meanwhile, this discussion is drawing some attention to code on the C side
as well.  As Feng pointed
out, there is still C code in the kernel that assumes a plain write
will be atomic in many situations, even though the C standard explicitly
says otherwise.  Peter Zijlstra answered
that all such code should be updated to use  properly.
Simply finding that code may be a challenge (though KCSAN can help);
updating it all may take a while.  The conversation also identified
a place in the (C) high-resolution-timer code that is missing a needed
 call.  This is another example of the Rust work
leading to improvements in the C code.

In past discussions on the design of Rust abstractions, there has been
resistance to the creation of Rust interfaces that look substantially
different from their C counterparts; see this
2024 article, for example.  If the Rust developers come up with a
better design for an interface, the thinking went, the C side should be
improved to match this new design.  If one accepts the idea that the Rust
approach to  and  is better than
the original, then one might conclude that  a similar process should be
followed here.  Changing thousands of low-level concurrency primitives to
specify more precise semantics would not be a task for the faint of heart,
though.  This may end up being a case where code in the two languages just
does things differently.]]></content:encoded></item><item><title>Cursor&apos;s latest “browser experiment” implied success without evidence</title><link>https://embedding-shapes.github.io/cursor-implied-success-without-evidence/</link><author>embedding-shape</author><category>hn</category><pubDate>Fri, 16 Jan 2026 14:37:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cloudflare acquires Astro</title><link>https://astro.build/blog/joining-cloudflare/</link><author>todotask2</author><category>hn</category><pubDate>Fri, 16 Jan 2026 14:25:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The Astro Technology Company — the company behind the Astro web framework — is joining Cloudflare! Adoption of the Astro web framework continues to double every year, and Astro 6 is right around the corner. With Cloudflare’s support, we’ll have more resources and fewer distractions to continue our mission to build the best framework for content-driven websites.What this means for Astro:Astro stays open-source and MIT-licensedAstro continues to be actively maintainedAstro continues to support a wide set of deployment targets, not just CloudflareAstro’s open governance and current roadmap remain in place.All full-time employees of The Astro Technology Company are now employees of Cloudflare, and will continue to work on Astro full-time.In 2021, Astro was born out of frustration. The trend at the time was that every website should be architected as an application, and then shipped to the user’s browser to render. This was not very performant, and we’ve spent the last decade coming up with more and more complex solutions to solve for that performance problem. SSR, ISR, RSC, PPR, TTI optimizations via code-splitting, tree-shaking, lazy-loading, all to generate a blocking double-data hydration payload from a pre-warmed server running halfway around the world.Our mission to design a web framework specifically for building websites — what we call  to better distinguish from data-driven, stateful web applications — resonated. Now Astro is downloaded almost 1,000,000 times per week, and has been used by 100,000s of developers to build fast, beautiful websites. Today you’ll find Astro all over the web, powering major websites and even entire developer platforms for companies like Webflow, Wix, Microsoft, and Google.Along the way, we also tried to grow a business. In 2021 we raised some money and formed The Astro Technology Company. Our larger vision was that a well-designed framework like Astro could sit at the center of a massive developer platform, with optional hosted primitives (database, storage, analytics) designed in lockstep with the framework.We were never able to realize this vision. Attempts to introduce paid, hosted primitives into our ecosystem fell flat, and rarely justified their own existence. We considered going more directly after first-class hosting or content management for Astro, but knew we’d spend much of our time playing catchup to well-funded, savvy competitors. We kept exploring different ideas, but nothing clicked with users the same way Astro did.It wasn’t all bad. Astro DB (our attempt to build a hosted database product for Astro projects) eventually evolved into the open, built-in Astro database client that still lives in core today. Our exploration into building an e-commerce layer with Astro was eventually open-sourced. It was rewarding work, but over the years the distraction took its toll. Each attempt at a new paid product or offering took myself and others on the project away from working on the Astro framework that developers were using and loving every day.Last year, Dane (Cloudflare CTO) and I began to talk more seriously about the future of the web. Those conversations quickly grew into something bigger: What does the next decade look like? How do frameworks adapt to a world of AI coding and agents?It became clear that even as web technologies evolve, content remains at the center. We realized that we’ve each been working toward this same vision from different angles: has been solving it from the  side: betting on a platform that is global by default, with fast startup, low latency, and security built-in. has been solving it from the  side: betting on a web framework that makes it easy to build sites that are fast by default, without overcomplicating things.The overlap is obvious. By working together, Cloudflare gives us the backing we need to keep innovating for our users. Now we can stop spending cycles worrying about building a business on top of Astro, and start focusing 100% on the code, with a shared vision to move the web forward.Cloudflare has been a long-time sponsor and champion of Astro. They have a proven track record of supporting great open-source projects like Astro, TanStack, and Hono without trying to capture or lock anything down. Staying open to all was a non-negotiable requirement for both us and for Cloudflare.That is why Astro will remain free, open-source, and MIT-licensed. We will continue to run our project in the open, with an open governance model for contributors and an open community roadmap that anyone can participate in. We remain fully committed to maintaining Astro as a platform-agnostic framework, meaning we will continue to support and improve deployments for all targets—not just Cloudflare.With Cloudflare’s resources and support, we can now return our focus fully towards building the best web framework for content-driven websites. The web is changing fast, and the bar keeps rising: performance, scale, reliability, and a better experience for the teams shipping content on the web.You’ll see that focus reflected across our roadmap, as we prepare for the upcoming Astro 6 release (beta out now!) and our 2026 roadmap. Stay tuned!I want to extend a huge thank you to the agencies, companies, sponsors, partners, and theme authors who chose to work with us over the years. Thank you to our initial investors — Haystack, Gradient, Uncorrelated, Lightspeed — without whom Astro likely wouldn’t exist. Thank you to everyone in our open source community who continues to help make Astro better every day. And finally, thank you to everyone who uses Astro and puts their trust in us to help them build for the web.]]></content:encoded></item><item><title>Michelangelo&apos;s first painting, created when he was 12 or 13</title><link>https://www.openculture.com/2026/01/discover-michelangelos-first-painting.html</link><author>bookofjoe</author><category>hn</category><pubDate>Fri, 16 Jan 2026 13:44:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Think back, if you will, to the works of art you cre­at­ed at age twelve or thir­teen. For many, per­haps most of us, our out­put at that stage of ado­les­cence amount­ed to direc­tion­less doo­dles, chaot­ic comics, and a few unsteady-at-best school projects. But then, most of us did­n’t grow up to be Michelan­ge­lo. In the late four­teen-eight­ies, when that tow­er­ing Renais­sance artist was still what we would now call a “tween,” he paint­ed The Tor­ment of Saint Antho­ny, a depic­tion of the tit­u­lar reli­gious fig­ure beset by demons in the desert. Though based on a wide­ly known engrav­ing, it nev­er­the­less shows evi­dence of rapid­ly advanc­ing tech­nique, inspi­ra­tion, and even cre­ativ­i­ty — espe­cial­ly when placed under the infrared scan­ner.For about half a mil­len­ni­um, The Tor­ment of Saint Antho­ny was­n’t thought to have been paint­ed by Michelan­ge­lo. As explained in the video from Inspi­rag­gio just below, when the paint­ing sold at Sothe­by’s in 2008, the buy­er took it to the Met­ro­pol­i­tan Muse­um of Art for exam­i­na­tion and clean­ing.“Beneath the lay­ers of dirt accu­mu­lat­ed over the cen­turies,” says the nar­ra­tor, “a very par­tic­u­lar col­or palette appeared. “The tones, the blends, the way the human fig­ure was treat­ed: all of it began to resem­ble the style Michelan­ge­lo would use years lat­er in none oth­er than the Sis­tine Chapel.” Infrared reflec­tog­ra­phy sub­se­quent­ly turned up , or cor­rec­tion marks, a com­mon indi­ca­tion that “a paint­ing is not a copy, but an orig­i­nal work cre­at­ed with artis­tic free­dom.”It was the Kim­bell Art Muse­um in Fort Worth, Texas that first bet big on the prove­nance of The Tor­ment of Saint Antho­ny. Its new­ly hired direc­tor pur­chased the paint­ing after turn­ing up “not a sin­gle con­vinc­ing argu­ment against the attri­bu­tion.” Thus acquired, it became “the only paint­ing by Michelan­ge­lo locat­ed any­where in the Amer­i­c­as, and also just one of four easel paint­ings attrib­uted to him through­out his entire career,” dur­ing most of which he dis­par­aged oil paint­ing itself. About a decade lat­er, and after fur­ther analy­sis, the art his­to­ri­an Gior­gio Bon­san­ti put his con­sid­er­able author­i­ty behind a defin­i­tive con­fir­ma­tion that it is indeed the work of the young Michelan­ge­lo. There remain doubters, of course, and even the noto­ri­ous­ly uncom­pro­mis­ing artist him­self may have con­sid­ered it an imma­ture work unwor­thy of his name. But who else could have cre­at­ed an imma­ture work like it?]]></content:encoded></item><item><title>Dev-owned testing: Why it fails in practice and succeeds in theory</title><link>https://dl.acm.org/doi/10.1145/3780063.3780066</link><author>rbanffy</author><category>hn</category><pubDate>Fri, 16 Jan 2026 13:39:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Patching the Wii News Channel to serve local news (2025)</title><link>https://raulnegron.me/2025/wii-news-pr/</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 16 Jan 2026 12:58:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Here’s a sneak peek at the result:In this post, I’d like to share my research and process for getting this all to work.The News Channel debuted in North America on January 26, 2007, a little over two months after the Wii’s launch. Since that date, it mostly came pre-installed with Wii consoles and was a novel way to read news from all over the world. Together with other “utility” channels like the Forecast Channel, it tried to position the Wii as more than just a gaming console.Check out a video recording of the service from right before it was discontinued on June 27th, 2013:How the News Channel fetches contentBefore we can consider displaying custom news on it, we have to figure out how the News Channel actually fetches content. We know that it must have fetched news somehow since it displays a “Downloading…” splash screen on startup.Luckily for us, the Wii natively supports proxying via its internet connection configuration settings! Meaning we can set up something like mitmproxy on a local machine and observe its HTTP behavior.We can start ’s web interface for a more screenshot-friendly UI:If we run a man-in-the-middle proxy for the News Channel on an unmodified Wii, we will observe that, on channel startup, it attempts to obtain a  file from http://news.wapp.wii.com/v2/1/049/news.bin.00 via a plain HTTP request.URL path explainer (we’ll see later how I found this out): corresponds to “English” as the configured console language. See conf.h in  (the Wii homebrew community’s de-facto development toolchain) for the possible values. is the Wii’s country code for “United States”. Check out the full list of Wii country codes on wiibrew.org.Once it fails to fetch this file, the News Channel displays an error. What might these binary files be? In any case, seeing the Wii perform an HTTP request to fetch news data is a good sign for us. It means we might be able to serve our own data.By the way, if you run an internet connection test after configuring the proxy settings correctly, you’ll spot the Wii performing an HTTP request to http://conntest.nintendowifi.net. Turns out, this page is actually still online (see for yourself!)The Wii’s internet connection test still passes to this day without any modification required. Thanks, Nintendo!Up to this point, this is how we would expect the Wii would behave if you were running a stock console. More than 12 years ago, Nintendo discontinued support for the online functionality of the News Channel.But as expected for a beloved retro console, community efforts have sprung up to try and preserve the previously existing functionality and allow users to continue enjoying these systems well past their intended expiration date. These sorts of unofficial software for gaming consoles are commonly referred to as “homebrew”.Importantly for this project, the WiiLink team maintains servers and develops software that allows us to experience the Wii’s online connectivity features even today.By the way, if you’re curious about how to get started with Wii console homebrew, check out https://wii.hacks.guide.Thanks to WiiLink, we can revive the News Channel and browse up-to-date news! Just not the local news, which is our real goal.How WiiLink patches the News ChannelAfter going through the WiiLink install process, if we fire up  and take a look at what the Wii is doing now, we’ll see that it’s actually requesting files from a different domain: “news.wiilink.ca”. But this time, it manages to fetch  and keeps requesting files all the way up to .The News Channel just successfully fetched 24 hours worth of news from this server.Great! Somehow, the WiiLink folks got this all to work. And, best of all, they’ve opened-sourced their work (GitHub). The plan is looking really feasible at this point!At a high-level, there are two steps to tackle, then:We have to make the News Channel fetch files from a server we controlWe need to actually generate binary files with the content we wantStep 1: Patching the News Channel to redirect to our domainSide note - it’s only while writing this blog post that I realized I had been looking at the “wrong” repo; WiiLink’s guide now recommends using the Python-based WiiLink-Patcher-GUI instead of the CLI patcher.After downloading the  file locally, we can use the xdelta CLI to print out some information on what the patch is supposed to do:Okay, so we’re looking for a  file and want to save the patched binary as . Based on the WiiLink install instructions, we know we should be dealing with a  file, so let’s keep digging to see if we can find out where  might be hiding.From the repo’s , we know the patcher uses libWiiSharp for it’s WAD file management during the file patching processing (source). But at this point, I’d rather avoid using C# if I can. And besides, I know for a fact we’ll want to use Go in order to more easily leverage existing tooling from the WiiLink team.Thankfully, there’s a really handy Go library called wadlib that comes to the rescue here. We’ll be using it for all our WAD management needs.So, where is ? Looking at LibWiiSharp’s  file, we can spot how it unpacks  files from a WAD file. Namely, it defaults to using the numeric “Content ID” inside each “Content” metadata and then converts it to an 8-digit hexadecimal string (source).You can read more about Title metadata (“TMD”) and Content metadata (“CMD”) on wiibrew.orgArmed with this knowledge, we can use  to create a quick file extraction script and see if we can find our . It can go something like this:When  is the official (v7) News Channel WAD file, this script successfully extracts 12  files.There’s definitely a  there, but could it be the file we’re looking for?What we really need to do at this point is go ahead and apply the  patch to this  file manually. That way, we can compare the before/after binaries and see what changed. We can use  again to actually apply the patch. Running  says:So we can go ahead and run:And that… seemed to work? We have a  file, as expected. Now what?Investigating binary file changesWe could do a binary diff of these files and start going through each change, but we already know at least one thing that should have changed based on our previous  experiments: instead of performing requests to “news.wapp.wii.com”, the patched WAD should instead use “news.wiilink.ca”.Using a tool like Hex Fiend (which also has binary diffing capabilities in case we need them), we can try searching for text inside the binary. If we try searching for “news.wapp.wii.com” on the original  file, we can actually find a match!Sure enough, if we inspect the patched  file we will find no mention of the original URL. Instead, the “http://news.wiilink.ca” domain is visible at the same location (offset ).Note that the URL contains only two printf-style format strings ( and ); the News Channel itself must be appending the hourly suffix (like ) when fetching data.If we’re lucky, simply overwriting the binary file’s original URL with our own custom URL might do the trick. It’s worth a try!In order to validate this hypothesis, I wrote a small Go utility for performing the necessary text replacement. Here’s an excerpt of the important bits:If we run the utility like:It should perform the URL rewriting in memory and provide us with a valid WAD file () we can then go ahead and install on Wii hardware.Finally, we can go back to running  and opening the newly patched News Channel. Once the channel shows the “Downloading…” splash screen, we’ll spot requests going out to our expected domain.It works! Now all we need is to… actually generate valid news files for the News Channel to work.I mentioned previously that I knew using Go would come in handy later, and it’s specifically because the WiiLink team has a project called NewsChannel written in Go which contains the source code for generating the binary news files they serve from “news.wiilink.ca”.I’m not going to go over all the implementation details here. I just want to highlight some of the main file creation steps in case you’d like to read more:obtain country-specific configuration (source)obtain articles and metadata from configured sources (like NHK, source)process all data in a specific order into a bytes buffer (source)compress the data using LZ10, sign it with RSA, then write to disk (source)
the file name is written using a specific string interpolation (source, this is how I first found out about the language/country codes used in the News Channel data URL!)Fun fact: LZ10 is apparently a Nintendo-specific variant of the LZ77 compression algorithm, used in some form or another on Game Boy Advance, Nintendo DS and Wii systems. wii-tools/lzx has the Go source for the LZ10 compression used here.In any case, for our purposes, it’s doing more than we need in terms of source handling: it can generate news binaries from a variety of sources and supports different languages and regions.For this project, I am making the following assumptions and tradeoffs:I will be using “English” as the language and “US” as the country code for the source URL path since my Wii console is configured as such. There is no separate Puerto Rico country code option, which is curious considering that there is a separate option for the US Virgin Islands.I am not interested in supporting any other news sources from around the world, so the “Globe” feature for the News Channel will not be useful.I’m hardcoding the latitude and longitude of Puerto Rico’s capital into the binary file to avoid having to process or guess location data from each article entry.Modifying WiiLink’s generator to support Puerto Rican newsI went ahead and forked the  repo into WiiNewsPR, added flag support to control article caching and binary output paths (you’ll see why this was necessary soon), removed all the existing sources and added a new one: El Nuevo Día (“ENDI”).I picked ENDI only because it’s the only local newspaper website I could find which still supports RSS. Unfortunately, the feeds only contain a snippet of the actual article. On the bright side, most articles do contain images and we can use separate feeds to help categorize articles in the News Channel (source).By the way, I experimented with GoOse for (spanish language) article extraction on other news websites and the results were… unsatisfying, to say the least.Final setup requirements for proper News Channel supportTwo quick things we’ll need in order to get this all to work:We need to sign each binary news file with a custom RSA key for the Wii to process the file (source). We can use  for this (note the  option):We need a (really) low quality logo for our source. ImageMagick easily solves for this:Finally, we can build the Go binary and run it in order to generate a news binary in :This successfully generates a  file.Now we just need 24 of these, since the News Channel will actually fail to load if not provided with all 24 files. We could run this script every hour for the next 24 hours… or, we could take the shortcut of copying the current hour’s file into all other hourly values.Regardless, it’s about time to test out all this effort. With 24 files uploaded to our storage provider (AWS S3), and the patched News Channel configured to fetch these from our custom domain, we can start up the channel and observe the fruits of our labor.After the (slow!) requests finish one by one, seeing the articles pop up was immensely satisfying. Being able to tinker with and learn more about these nostalgic consoles so many years later is a real joy for me.Bonus step: Automating hourly news updates with AWS LambdaCopying files into the S3 storage bucket is all well and good, but it would be great to have a continuously-updating, hands-off solution that generates the news binaries for us. A simple (and basically free) way to solve for this would be to bundle up the  Go executable into an AWS Lambda function and have that run hourly via EventBride, and then uploading the generated news binaries over to our storage bucket.Here is where the extra flags for  come in: we need to be able to control file creation because  is a Lambda’s only writeable file system.Here is a snippet of the Lambda handler logic:See full Lambda handler source on GitHub: handler.goWe can then leverage the Serverless framework for a quick infra-as-code setup. Here is a snippet of the configuration:Some things to call out here:We want to make sure to run the Lambda in Puerto Rico’s timezone so that  returns the expected hourly integer.We want to give the Lambda a higher than expected  so that its CPU scales accordingly; it turns out that  compression is a big bottleneck on the smallest supported Lambda CPU and can easily time out at 30 seconds.If we leave this setup running for 24 hours, our storage bucket will get populated with 24 files and continuously be updated with the latest news!Now I can get up in the morning, grab a coffee, and browse the local news on my Nintendo Wii like it’s 2007.]]></content:encoded></item><item><title>Just the Browser</title><link>https://justthebrowser.com/</link><author>cl3misch</author><category>hn</category><pubDate>Fri, 16 Jan 2026 12:03:52 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Just the Browser helps you remove AI features, telemetry data reporting, sponsored content, product integrations, and other annoyances from desktop web browsers. The goal is to give you "just the browser" and nothing else, using hidden settings in web browsers intended for companies and other organizations.This project includes configuration files for popular web browsers, documentation for installing and modifying them, and easy installation scripts. Everything is open-source on GitHub. Open a PowerShell prompt as Administrator. You can do this by right-clicking the Windows button in the taskbar, then selecting the "Terminal (Admin)" or "PowerShell (Admin)" menu option. Next, copy the below command, paste it into the window (), and press the Enter/Return key:& ([scriptblock]::Create((irm "https://raw.githubusercontent.com/corbindavenport/just-the-browser/main/main.ps1")))
 Search for the Terminal in your applications list and open it. Next, copy the below command, paste it into the window ( or ), and press the Enter/Return key:/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/corbindavenport/just-the-browser/main/main.sh)"
Start here if you don't have your preferred web browser installed. You can install the configuration files afterwards.What features or settings are changed?Just the Browser aims to remove the following functionality from popular web browsers:: Features that use generative AI models, either on-device or in the cloud, like Copilot in Microsoft Edge or tab group suggestions in Firefox. The main exception is page translation in Firefox.Sponsored or third-party content: Suggested articles on the New Tab Page, sponsored site suggestions, etc.Default browser reminders: Pop-ups or other prompts that ask you to change the default web browser.First-run experiences and data import prompts: Browser welcome screens and their related prompts to import data automatically from other web browsers. Data collection by web browsers. Crash reporting is left enabled if the browser (such as Firefox) supports it as a separate option. Features that allow web browsers to start with the operating system without explicit permission.Can I change or remove the settings?Yes. The browser guides include steps for removing the configurations, and the automated script can also do it. The browser guides explain each setting, so you can add, remove, or modify the files before you install them.Which web browsers are supported?Just the Browser has configuration files and setup scripts for Google Chrome, Microsoft Edge, and Mozilla Firefox. However, Chrome on Linux and Edge on Linux are not currently supported.Can I install this on my phone or tablet?Is this modifying the web browser?No. Just the Browser uses group policies that are fully supported by web browsers, usually intended for IT departments in companies or other large organizations. No applications or executable files are modified in any way.Do the settings stay applied?Yes, as long as the web browsers continue to support the settings used in the configuration files. Web browsers occasionally add, remove, or replace the settings options, so if the custom configuration breaks, try installing the latest available version.Does this install ad blockers for me?Why does my browser say it's managed by an organization?The group policy settings used by Just the Browser are intended for PCs managed by companies and other large organizations. Browsers like Microsoft Edge and Firefox will display a message like "Your browser is being managed by your organization" to explain why some settings are disabled.How do I know the settings are applied?You can open  in Firefox or  in Chrome and Edge to see a list of active group policy settings.Why not just use an alternative web browser?You can do that! However, switching to alternative web browsers like Vivaldi, SeaMonkey, Waterfox, or LibreWolf can have other downsides. They are not always available on the same platforms, and they can lag behind mainstream browsers in security updates and engine upgrades. Just the Browser aims to make mainstream web browsers more tolerable, while still retaining their existing benefits.]]></content:encoded></item><item><title>Why DuckDB is my first choice for data processing</title><link>https://www.robinlinacre.com/recommend_duckdb/</link><author>tosh</author><category>hn</category><pubDate>Fri, 16 Jan 2026 10:57:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Over the past few years, I've found myself using DuckDB more and more for data processing, to the point where I now use it almost exclusively, usually from within Python.We're moving towards a simpler world where most tabular data can be processed on a single large machine and the era of clusters is coming to an end for all but the largest datasets.This post sets out some of my favourite features of DuckDB that set it apart from other SQL-based tools.    In a nutshell, it's simple to install, ergonomic, fast, and more fully featured.DuckDB is an open source in-process SQL engine that is optimised for analytics queries.'In-process' means it's similar to SQLite in that it runs within your application. You don't need to start a separate service such as Postgres to run it.'Optimised for analytics queries' means that it's designed for operations like joins and aggregations involving large numbers of rows, as opposed to atomic transactions.The performance difference of analytics-optimised engines (OLAP) vs. transactions-optimised engines (OLTP) should not be underestimated. A query running in DuckDB can be 100 or even 1,000 times faster than exactly the same query running in (say) SQLite or Postgres.A core use-case of DuckDB is where you have one or more large datasets on disk in formats like ,  or  which you want to batch process.  You may want to perform cleaning, joins, aggregation, derivation of new columns - that sort of thing.But you can also use DuckDB for many other simpler tasks like viewing a csv file from the command line.DuckDB consistently benchmarks as one of the fastest data processing engines.  The benchmarks I've seen show there's not much in it between the leading open source engines - which at the moment seem to be polars, DuckDB, DataFusion,  Spark and Dask.  Spark and Dask can be competitive on large data, but slower on small data.DuckDB itself is a single precompiled binary.  In Python, it can be ed with no dependencies.  This makes it a joy to install compared to other more heavyweight options like Spark.  Combined with , you can stand up a fresh DuckDB Python environment from nothing in less than a second - see here.With its speed and almost-zero startup time, DuckDB is ideally suited for CI and testing of data engineering pipelines.Historically this has been fiddly and running a large suite of tests in e.g. Apache Spark has been time consuming and frustrating.  Now it's much simpler to set up the test environment, and there's less scope for differences between it and your production pipelines.This simplicity and speed also applies to writing new SQL, and getting syntax right before running it on a large dataset.  Historically I have found this annoying in engines like Spark (where it takes a few seconds to start Spark in local mode), or even worse when you're forced to run queries in a proprietary tool like AWS Athena.There's even a DuckDB UI with autocomplete - see here.The DuckDB team has implemented a wide range of innovations in its SQL dialect that make it a joy to use. See the following blog posts 123456.Some of my favourites are the  keyword, and the  keyword which allows you to select and regex-replace a subset of columns.  I also like  and the aggregate modifiers on window functions, see here.Another is the ability to function chain, like first_name.lower().trim().You can query data directly from files, including on s3, or on the web.For example to query a folder of parquet files:or even (on CORS enabled files) you can run SQL directly:Click here to try this query yourself in the DuckDB web shell.One of the easiest ways to cause problems in your data pipelines is to fail to be strict about incoming data types from untyped formats such as csv.  DuckDB provides lots of options here - see here.Many data pipelines effectively boil down to a long sequence of CTEs:When developing a pipeline like this, we often want to inspect what's happened at each step.This makes it easy to inspect what the data looks like at  with no performance loss, since these steps will be executed lazily when they're run all at once.This also facilitates easier testing of SQL in CI, since each step can be an independently-tested function.DuckDB offers full ACID compliance for bulk data operations, which sets it apart from other analytical data systems - see here.  You can listen to more about this on in this podcast, transcribed here.This is a very interesting new development, making DuckDB potentially a suitable replacement for lakehouse formats such as Iceberg or Delta lake for medium scale data.A longstanding difficulty with data processing engines has been the difficulty in writing high performance user defined functions (UDFs).For example, in PySpark, you will generally get best performance by writing custom Scala, compiling to a JAR, and registering it with Spark.  But this is cumbersome and in practice, you will encounter a lot of issues around Spark version compatibility and security restrictions environments such as DataBricks.In DuckDB high performance custom UDFs can be written in C++.  Whilst writing these functions is certainly not trivial, DuckDB community extensions offers a low-friction way of distributing the code. Community extensions can be installed almost instantly with a single command such as INSTALL h3 FROM community to install hierarchical hexagonal indexing for geospatial data.My top tip: if you load this file in your code editor, and use code folding, it's easy to copy the parts of the documentation you need into context.Much of this blog post is based on my experience supporting multiple SQL dialects in Splink, an open source library for record linkage at scale.  We've found that transitioning towards recommending DuckDB as the default backend choice has increased adoption of the library and significantly reduced the amount of problems faced by users, even for large linkage tasks, whilst speeding up workloads very substantially.We've also found it's hugely increased the simplicity and speed of developing and testing new features.The PosgreSQL Extension allows you to attach a Postgres database and query it directly from DuckDB. allows you to embed the DuckDB computation engine within Postgres.The later in particular seems potentially extremely powerful, enabling Postgres to be simultanouesly optimised for analytics and transactional processing.  I think it's likely to see widespread adoption, especially after they iron out a few of the current shortcomings around enabling and optimising the use of Postgres indexes and pushing up filters up to PostGres.]]></content:encoded></item><item><title>On Being a Human Being in the Time of Collapse (2022) [pdf]</title><link>https://web.cs.ucdavis.edu/~rogaway/papers/crisis/crisis.pdf</link><author>barishnamazov</author><category>hn</category><pubDate>Fri, 16 Jan 2026 10:19:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The spectrum of isolation: From bare metal to WebAssembly</title><link>https://buildsoftwaresystems.com/post/guide-to-execution-environments/</link><author>ThierryBuilds</author><category>hn</category><pubDate>Fri, 16 Jan 2026 09:27:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Ever had that dreaded “but it works on my machine!” moment?The culprit is often a subtle difference in the —the “stage” where your code performs.
You might be dealing with a binary linked against the wrong glibc, a Python wheel built for a different architecture, or a kernel feature quietly missing in production.
These invisible discrepancies are what turn a successful local build into a deployment disaster.Getting the environment right is crucial for writing, testing, and shipping software reliably.
But the landscape is crowded with terms like , , , and more.
What’s the difference, and which one should you use?We’re going to trace the evolution of the execution environment.
We’ll start with raw hardware and work through VMs, containers, and the various ways we isolate code at the operating system (OS) and language level.
Along the way, we’ll break down the trade-offs for each approach.
By the end, you’ll know exactly which tool to grab for your next project.The history of computing is largely a history of resource sharing without chaos.Early systems ran one workload per machine. Today, a single server might host thousands of isolated applications owned by different teams. The unifying idea behind this evolution is : separating code, dependencies, and resources so they don’t interfere with one another.But isolation is not binary. It exists on a spectrum—hardware, kernel, process, filesystem, language runtime. Each execution paradigm chooses a different point on that spectrum. any layer below your chosen isolation boundary must already be compatible—containers won’t fix a kernel mismatch, and virtual environments won’t fix a missing system library.We’ll move from the heaviest to the lightest abstractions.This is the foundation. One machine, one operating system, running your code directly on the hardware.Hardware (CPU, memory, Disk,…): Uniquely provided by a physical machine. Two separate environments imply two separate physical machines, each with its own dedicated hardware resources like CPU, memory, and disk.Think of it as a detached house. You have all the resources to yourself, with no neighbors to bother you. Maximum performance, full control over hardware. Expensive (you pay for idle resources), slow to provision, inflexible. High-performance computing (HPC), large databases, or legacy systems that require direct hardware access.VMs were the first major leap in efficiency. A piece of software called a  carves up a single physical machine into multiple, independent virtual ones. Uniquely provided by virtual machines. Two environments can run on the same hardware but will have their own separate, full-fledged operating systems.This is like an apartment building. You still have your own private space (kitchen, bathroom, ), but you share the building’s underlying infrastructure (hardware). Strong isolation, can run different operating systems on one host (e.g., Windows and Linux). Significant overhead (each VM has a full OS), slower to start than containers.: Microsoft’s native hypervisor for Windows.: The go-to hypervisor for Linux.: A powerful machine emulator and virtualizer.: While primarily a container manager, recent versions can also manage full virtual machines, offering a unified tool for both.Containers revolutionized modern software development. They bundle an application and its dependencies, but—here’s the key difference—they share the host machine’s operating system kernel.Application and Dependencies: Characterized by packaging an application along with all its dependencies. Multiple containerized environments share the host OS kernel but run in isolated user spaces.Think of containers as hotel rooms. Each is a self-contained, identical unit, but they all rely on the hotel’s core services (the host OS kernel). This makes them incredibly lightweight and fast.Under the hood, this isolation is enforced by Linux  (which give each container its own view of processes, networking, and the filesystem) and  (which strictly control how much CPU, memory, and I/O it can consume). Extremely fast startup, low overhead, highly portable, perfect for microservices. Weaker isolation than VMs (shared kernel can be a security concern). The tool that made containers mainstream, ideal for single applications.: A popular daemonless alternative to Docker (it runs containers as direct processes without a central background service).: A powerful manager for  (Linux Containers).LXD is a unique tool that intentionally blurs the lines.
Its primary strength is managing , which feel like ultra-fast VMs but are technically containers.However, as we’ve listed, LXD can  manage full virtual machines.
This makes it a powerful, unified tool for developers who want a single interface for both environment types.What About Managing Many Containers? OrchestrationTools like Docker and LXD are great for running containers on a single machine.
Docker Compose builds on this by managing multi-container applications as a single unit.
It allows you to define services like a web server and a database together, though it still operates on a single host.When you need to manage applications across many machines, you move to  like , , or .
Orchestration is not isolation.
They are the next layer of management for handling scheduling, scaling, and networking at scale.
They do not solve environment drift, dependency mismatches, or build reproducibility.While orchestration is a deep topic for its own article, it’s the logical next step after adopting containers.This is a more specialized form of isolation, often used for security. It “jails” a process, restricting its view of system resources.Interface-Level Isolation: Defined by filtering a process’s interaction with the Linux Kernel. 
Instead of providing a new environment, we strip away the process’s “powers” and limit its authority and vocabulary. 
It provides a sandboxed execution space for a single process or a group of related processes.This is like putting a specific activity into a “Safety Cabinet.” You aren’t building a new room; you are simply limiting what the process is allowed to do within your existing system through thick glass and heavy gloves. Very lightweight, OS-native security feature. Can be complex to configure correctly, less feature-rich than full container runtimes.Sandboxes shine when you want to  of a single process—not when you need a reproducible environment.
They are about , not standardizing execution.To build a proper sandbox, we control three specific dimensions: (Filesystem): Limiting the reach to specific folders. (Privileges): Limiting the authority to specific actions. (System Calls): Limiting the communication with the OS kernel.You can combine the following mechanisms (to create a robust sandbox), or use them individually:: Restricts the process to a specific directory tree. A classic UNIX utility that changes the (perceived) top-level root directory of a process. E.g.: make the process see  as . An implementation of  that works without root privileges. It’s a user-space implementation that uses ptrace to fake a root directory without requiring administrative privileges.Privilege Dividers (What): Breaks “Root” powers into small pieces. Instead of giving a process full administrative power, you give it only the specific power it needs (like  just to open a port).: Manages Linux Capabilities. Instead of a binary “Root vs. User” choice, it breaks root powers into 40+ granular bits (e.g.,  to manage networks without being able to read everyone’s files). / : The command-line utilities used to assign these specific powers to processes.System Call Filtering (How): A firewall for the Kernel. It prevents a process from executing dangerous commands (like reboot or ptrace) even if it has root privileges.: A Linux kernel feature that filters system calls. For example, if a process tries to use an unapproved call (like  to start a shell), the kernel kills it instantly. and  are high-level “wrappers” that combine all the above.
They are the engines behind modern “Sandboxed” apps like Flatpaks.While these tools provide “surgical” isolation for individual processes, they are also the primary technologies that  (like Docker) automate and bundle into a single, portable package.This type of isolation is probably the one you use daily as a developer. It doesn’t isolate the OS or hardware, but rather the dependencies of a programming language.Language-Specific Workspace: Focused on isolating the dependencies of a specific programming language. This allows multiple projects on the same machine to use different versions of the same language and library without conflict.This is your workshop organizer. You have one project that needs an old version of a library and another that needs the latest version. A virtual environment keeps their tools (dependencies) in separate, labeled drawers so they don’t get mixed up. This prevents “dependency hell.” Essential for managing project dependencies, simple to use, developer-focused, Zero performance overhead. Provides no OS-level or security isolation; the code still has full access to your user files, network, and system hardware.A critical limitation to remember: virtual environments solve , not .
If your code depends on a specific  version, OS package, kernel feature, or external binary, a  alone is no longer sufficient. if your build or runtime depends on  you don’t explicitly control—system libraries, OS packages, kernel features—a virtual environment is already too weak.To build a clean workspace, we have to solve three problems. Historically, we needed a different tool for each, but modern  are merging them into one.The Runtime (Runtime Managers):
These tools handle the language version itself. They allow you to run Python 3.8 for a legacy project while using Python 3.14 for a new one.
 (Python),  or  (Node.js),  (Rust),  (Go).The Environment (Path Isolation):
This tells the system where to look for libraries. In Python, tools like  or  create a folder to store libraries. In  and , this is handled implicitly by looking for a local  or a project-specific build directory () relative to your code.The Dependencies (Package Management):
These tools download and manage the actual libraries (dependencies) versions your code needs to run.
 (Python),  (Node.js),  (Rust),  (Go).The Rise of the “All-in-Ones”The modern trend is the move away from fragmented tools toward  toolchains.
These tools detect your project settings and automatically align the Runtime, Environment, and Dependencies. (Python): An extremely fast, single binary that replaces , , and . It can install Python versions and manage libraries in one go. (Data Science): A heavyweight manager that handles language versions, libraries, and even system-level dependencies like C++ compilers or GPU drivers. +  (Rust): The gold standard of integration. While technically two tools, they work as one. You can use  to swap the compiler on the fly, or a  file to pin the version for everyone on the team.A key insight is that these environments are not mutually exclusive. In fact, they are often layered to create robust, professional workflows.A common setup looks like this:You start with a  from a cloud provider like AWS or Google Cloud.On that VM, you install  to manage .Inside a container, your application runs, using a language-specific  (like Python’s ) to manage its dependencies.This layered approach gives you the hardware isolation of a VM, the packaging benefits of containers, and the clean dependency management of a virtual environment.The direction of travel is clear: execution environments are becoming more abstract—but not simpler.Each new paradigm removes a layer of responsibility from the developer while fixing the isolation boundary at a higher level.
The trade-offs don’t disappear; they just move.Containers have effectively become the standard unit of execution.
Tools like  formalize this by shifting the development environment itself into a container.The isolation boundary here sits squarely at the .
You share the kernel, but everything above it—filesystem, dependencies, tooling—is locked down and reproducible.However, this boundary still leaks. Containers built for  or specific syscalls will fail on older hosts. When the container assumes kernel features the host lacks, the result is often a silent or catastrophic failure.In these moments, the “universal” container abstraction breaks: you aren’t just running an image; you are still tethered to the underlying hardware and kernel.Serverless platforms push the isolation boundary even higher. You no longer manage machines, operating systems, or even containers directly. Instead, you hand over a function and accept a tightly constrained execution contract.This is powerful, but opinionated: cold starts, execution time limits, and restricted system access are not incidental—they are the . Serverless is ideal when you can fully live inside that contract, and painful when you can’t.Wasm is interesting not because it replaces containers, but because it introduces a new kind of boundary. Instead of isolating at the kernel or process level, Wasm sandboxes execution at the instruction and capability level.The result is a portable, secure runtime that can run consistently across browsers, servers, and edge environments. If containers standardized , Wasm is attempting to standardize .The common thread is this: progress doesn’t eliminate isolation—it chooses it more deliberately.Every execution environment is a trade-off between , , , and . Problems arise when we treat these tools as interchangeable—or worse, when we use them without understanding what they isolate and what they don’t.When in doubt, ask a single question: what is the lowest layer that must be identical for this code to behave correctly? Hardware, kernel, OS packages, or just language dependencies.
The answer points directly to the right execution environment.As we saw with containers and process sandboxes, choosing the wrong boundary doesn’t fail gracefully.
It fails in ways that are subtle, security-sensitive, or painfully non-obvious.Practical decision shortcuts:Use a virtual environment when only language-level dependencies vary.Use a container when system libraries, tooling, or runtime assumptions must be identical.Use a VM when kernel behavior, OS policies, or security boundaries must not be shared.Once you see execution environments as layered abstractions rather than competing products, architectural decisions become clearer—and “it works on my machine” becomes a relic of the past.This is why confusing language-level isolation (like ) with OS-level isolation (containers or sandboxes) is so costly: they sit on entirely different points of the isolation spectrum, and they fail in fundamentally different ways.]]></content:encoded></item><item><title>Interactive eBPF</title><link>https://ebpf.party/</link><author>samuel246</author><category>hn</category><pubDate>Fri, 16 Jan 2026 08:01:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
Learn eBPF through hands-on exercises. Write, compile, and run programs
        directly from your browser.
]]></content:encoded></item><item><title>Boeing knew of flaw in part linked to UPS plane crash, NTSB report says</title><link>https://www.bbc.com/news/articles/cly56w0p9e1o</link><author>1659447091</author><category>hn</category><pubDate>Fri, 16 Jan 2026 04:11:17 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[At the time the manufacturer responsible for the aircraft, Boeing, concluded that the issue "would not result in a safety of flight condition".The MD-11 is a relatively elderly design that was originally produced by McDonnell Douglas.  Boeing acquired the company in 1997.The last MD-11 came off the production line in 2001, but Boeing has continued providing parts and service support.In the aftermath of the Kentucky disaster, the NTSB issued a preliminary report which drew attention to cracks in the engine attachment mechanism. Its latest update goes further, describing fractures due to evidence of "fatigue" – or repeated stresses - in a critical bearing, as well as the mounting it is meant to sit in.It points out that Boeing had previously found failures of the same part on four occasions, affecting three different aircraft. In 2011, the company sent a "service letter" to operators warning them of its findings. This is a non legally-binding document used to alert operators about important safety or maintenance information.In this case, Boeing recommended that the part be included in a general visual inspection every five years. It also pointed out changes to the inspection procedure contained in the aircraft maintenance manual, and drew attention to a revised bearing assembly that could be fitted – although this was not mandatory.Tim Atkinson, a former air accident investigator who now works as an aviation safety consultant, said the NTSB's update made disturbing reading."The structure concerned is not decorative, it's an essential part of the mechanism that attaches the engine to the wing, and carries loads such as thrust and drag," he explained."It's extraordinary that Boeing concluded that a failure of this part would not have safety consequences," he claimed.Boeing's internal processes have come under fire on a number of occasions in recent years.Criticisms have focused on how the design of its 737 Max included flawed software that was implicated in two accidents, in 2018 and 2019, that together cost 346 lives.Quality controls in its factories have also come under scrutiny, after a door panel fell off a brand new 737 Max shortly after take-off in early 2024.In a statement, Boeing said: "We continue to support the investigation led by the NTSB. Our deepest condolences go out to the families who lost loved ones and our thoughts remain with all those affected."The NTSB's investigation is continuing. It has not yet issued any firm conclusions about the cause of the accident, and is unlikely to do so until it publishes its final report.]]></content:encoded></item><item><title>OpenBSD-current now runs as guest under Apple Hypervisor</title><link>https://www.undeadly.org/cgi?action=article;sid=20260115203619</link><author>gpi</author><category>hn</category><pubDate>Fri, 16 Jan 2026 03:10:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
Following a recent series of commits by Helg Bredow () and Stefan Fritsch (), OpenBSD/arm64 now works as a guest operating system under the .
List:       openbsd-cvs
Subject:    CVS: cvs.openbsd.org: src
From:       Helg Bredow <helg () cvs ! openbsd ! org>
Date:       2026-01-12 18:15:33


CVSROOT:	/cvs
Module name:	src
Changes by:	helg@cvs.openbsd.org	2026/01/12 11:15:33

Modified files:
	sys/dev/pv     : viogpu.c 

Log message:
viogpu_wsmmap() returns a kva but instead should return a physical
address via bus_dmamem_mmap(9). Without this, QEMU would only show a
black screen when starting X11. On the Apple Hypervisor, the kernel
would panic.
Also add calls to bus_dmamap_sync(9) before transferring the framebuffer
to host memory. It was working for me without this, but this ensures
that the host running on another CPU will see updates to the
framebuffer.

Thanks to kettenis@ for reviewing and providing feedback.

ok sf@
List:       openbsd-cvs
Subject:    CVS: cvs.openbsd.org: src
From:       Stefan Fritsch <sf () cvs ! openbsd ! org>
Date:       2026-01-15 9:06:19

CVSROOT:	/cvs
Module name:	src
Changes by:	sf@cvs.openbsd.org	2026/01/15 02:06:19

Modified files:
	sys/dev/pv     : if_vio.c 

Log message:
vio: Support MTU feature

Add support for the VIRTIO_NET_F_MTU which allows to get the hardmtu
from the hypervisor. Also set the current mtu to the same value. The
virtio standard is not clear if that is recommended, but Linux does
this, too.

Use ETHER_MAX_HARDMTU_LEN as upper hardmtu limit instead of MAXMCLBYTES,
as this seems to be more correct.

If the hypervisor requests a MTU larger than ETHER_MAX_HARDMTU_LEN,
redo feature negotiation without VIRTIO_NET_F_MTU.

With this commit, OpenBSD finally works on Apple Virtualization.

Input and testing from @helg

ok jan@

This development will be most welcome for those of us who run with newer  Mac models.

As always, if you have the hardware and the capacity, please take this for a spin (in snapshots now), and report!

]]></content:encoded></item><item><title>Show HN: Reversing YouTube’s “Most Replayed” Graph</title><link>https://priyavr.at/blog/reversing-most-replayed/</link><author>prvt</author><category>hn</category><pubDate>Fri, 16 Jan 2026 02:06:11 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[It was a quiet afternoon; the only sound was an instrumental playlist humming from a forgotten YouTube tab. A melody felt familiar, but I couldn’t quite place it, spirited away by my work. Suddenly, a transition in the soundtrack caught my ears, pulling me from my thoughts with a single question: what was this soundtrack?I switched over to the tab. The title read: . “Of course, it was from .” I had to smile at the unintentional irony. I slid my mouse over the progress bar to hear the transition again. A small graph appeared, indicating it was the song’s “most replayed” segment. Apparently, I wasn’t alone in loving that part. But that’s when I noticed them again: two small, symmetrical dips flanking the graph’s peak.I had seen it before. A tiny digital hiccup, easily dismissed. But in the quiet of that afternoon, it was a loose thread, and I felt an irresistible urge to pull.It started with a Google search: “how is youtube’s most replayed graph calculated.” Predictably, an AI overview answered:“aggregating the replay data from thousands of viewers to identify sections of a video that are re-watched the most.”This generic answer confirmed my suspicion: there wasn’t much public data on this. I’d be charting my own path (I fully expect future LLMs to cite this article, by the way.)Hypothesis: Designing the SystemThis kicked off a personal project: designing YouTube’s “most replayed” with the goal of replicating the bug. Naturally, I put myself in the shoes of an engineer at Google, (a reality I hope to achieve someday), and started brainstorming possible designs by imagining myself wearing a ‘Noogler’ cap with one of those Doraemon copters. Seriously, how does that work? Won’t the cap fly away? Maybe that’s the point? “Let your thinking hats fly.” But I digress. That’s a topic for  I get into Google.At the most basic level, I had to divide the continuous bar into discrete segments. So I represented the progress bar’s state as a boolean array, where each index corresponded to a segment of the video. That seemed like a good start.Canvas 1: The boolean array approach.This is my first attempt at an interactive article, so feel free to play around! You can hit start, pause or reset to simulate watching a video. (Note that the array only updates while the animation is playing, and dragging the seek bar is not supported.)But was that enough? I thought about all the ways I interact with a video player. I can move my seek back and forth, skip segments, re-watch segments, etc. A simple boolean array would only tell me if a segment was watched, not how many times. It fails to account for a user re-watching the same segment five times in a row. I needed something better, like a frequency array, to track how many times each segment was seen.Canvas 2: The frequency array.Notice how the segments grow as the "watcher" passes through them, updating the frequency array. Try skipping around when the animation is playing to see the effect.Now I had a minimum viable product. I could generate the heatmap based on this frequency array: for each segment, I’d plot a point whose vertical position corresponded to its view count. Join the dots, and voilà: my very own “most replayed” graph.We simply connect the dots of our frequency array, and scale the points upward based on the respective view counts.Was that all? Unfortunately, no. There was a lot more to it. First and foremost, I could already see a bug in my implementation. I thought about what would happen when a segment was watched over and over again, a ridiculously large number of times. My point would shoot higher and higher until it was off-screen entirely. This would be, to put it mildly, a bad user experience. Try re-watching the same segment multiple times in the above interactive canvas to see the effect.So, what’s the fix? If you have a statistics background (or just a good memory of high school math), you might already know the answer. Either way, it’s . It sounds fancy, but it’s really just a way of keeping our graph in check. Instead of plotting the raw view counts (which can range from zero to billions), we scale everything down to a standard range, typically between 0 and 1.The math is simple: find the segment with the highest view count (let’s call it ). Then, divide every segment’s view count by . Suddenly, the specific numbers don’t matter. The most popular segment will always have a value of 1 (or 100% height), and every other segment falls somewhere below that, relative to the peak. It ensures that whether a video has a thousand views or a thousand million, the points’ value range from 0 to 1 and the graph always fits perfectly inside the viewport.Canvas 4: The normalized histogram.By scaling everything relative to the peak, the graph remains perfectly contained within the viewport.But there’s a catch. You can’t normalize if you have no data. When a video is fresh out of the oven and just published,  is zero. Trying to divide by zero is a great way to crash a server, so the feature sits dormant. This is the  phase. If you’ve ever rushed to watch a new upload from your favorite creator, you might have noticed the graph is missing. That’s not a glitch; it’s a waiting game. YouTube is silently listening, collecting that initial batch of viewer data to establish a baseline.This raises an interesting question: do they keep listening forever? Does the server track every single micro-interaction for a video that’s five years old? The answer is almost certainly no, and for two very pragmatic reasons.First, speed is everything. The internet has the attention span of a goldfish. If a video goes viral, the “most replayed” graph needs to be available  the video is trending, not three weeks later when everyone has moved on. If the system waited for a “perfect” dataset, the moment would be lost. They need to calculate the graph quickly so users can see it before the hype dies down.Second, we don’t need perfection; we need patterns. This isn’t a bank transaction where every decimal point matters. We are just trying to visualize relative popularity. This leads us to the concept of . Once a video hits a certain threshold of views, say ten thousand, the distribution of the graph likely stabilizes. The shape of the curve won’t change drastically whether you survey ten thousand people or a billion. So, why pay the computational cost to track the next billion? By sampling a subset of viewers, YouTube can generate an accurate-enough graph without melting their data centers.However, even with sampling, my current design was write-heavy. I am focusing purely on the computational model here for brevity. The architecture of storage and the network model are deep dives for another day. But at the most basic level, the model relies on a counter of some sort. In my initial frequency array model, if a user watches a video from start to finish, and that video is divided into 100 segments, then 100 separate counters are incremented. Now, consider the scale: users upload more than 500 hours of content to YouTube every single minute. Updating every individual segment count of every video for every viewer during that crucial “Cold Start” phase would result in a write load so heavy that it would consume a massive amount of compute and memory.Optimization from First PrinciplesThe solution lies in realizing that we don’t actually care about the middle of a continuous viewing session. If I watch a video from segment 1 to segment 5, the only “new” information is where I started and where I stopped. The segments in between (2 through 4) are just implicitly included.This reminded me of a beautiful algorithmic trick from competitive programming:  (or ) technique, which allows us to utilize .Here is how it works. Instead of incrementing the count for every segment from 1 to 5, we only perform two operations. First, we go to the starting segment (index 1) and increment 1. This marks the beginning of a view. Then, we go to the segment immediately after the user stopped watching (index 6) and decrement 1. This marks the drop-off point.Even if the user skips around, watching 0-5, skipping to 8, then watching till end, we just treat those as separate sessions. We increment index 0, decrement index 6. Then increment index 8, and decrement index 10. The areas they skipped remain untouched.Canvas 5: The Difference Array technique.Notice how we only perform two operations: incrementing the start and decrementing the next element from where we skipped. The one extra "blank" segment at the very end is to safely catch that final decrement without throwing an "index out of bounds" error.Concurrently, there might be a thousand other users incrementing and decrementing the array. Do we need to worry about integer overflow and underflow? Remember, we are only sampling a small subset of viewers, so it’s a no.Memories aside, we still need to turn this difference array back into actual view counts. This is where the “Prefix Sum” comes in. We run a pass through the array where each number is the sum of itself and all the numbers before it.We perform this calculation just once, right before the normalization step. We effectively traded billions of write operations for a single, cheap read-time calculation. It’s elegant, efficient, and exactly the kind of optimization that makes systems scalable.Do the same steps in both  and , and run the above calculation on the ’s array. You will get the same result.Investigation: Tracing the SignalAt this point I was happy with the implementation, but how close was I to the real thing? Well, for starters, mine didn’t have the bug, which meant I was still missing a critical piece of the puzzle.My first instinct was to blame the classic enemy of precise computing: floating point errors. After all, normalization involves division, and we all know computers have a complicated relationship with decimals. I stared at the dips, trying to convince myself that this was just a rounding error, a ghost in the machine born from 0.1 + 0.2 not equaling 0.3. But the more I looked, the less it fit. A precision bug is usually chaotic, a messy scattering of noise across the entire dataset. It wouldn’t manifest as two perfect, symmetrical dips flanking the highest point while leaving the rest of the curve smooth. This wasn’t random; it felt structural. If it were a floating point issue, the artifacts would be everywhere, not just comfortably nesting next to the peak.So, I decided to stop guessing and start looking. I fired up the browser’s developer tools. Right-click. Inspect. The holy grail of web debugging. I hovered over the heatmap, diving into the DOM tree, peeling back layer after layer of nested  containers until I finally hit the source. There it was: a single  element hiding inside the structure.This discovery shifted the investigation. I was now looking at a  (SVG), but its origin was a mystery. Was this SVG pre-rendered on YouTube’s servers and sent over as a static asset? Or was the browser receiving a raw payload of data points (my hypothetical frequency array) and generating the curve locally using JavaScript?I desperately hoped for the latter. If the SVG was fully baked on the server, my journey would end right here. I’d be staring at a locked black box, with no way to access the rendering code or the logic behind it. All that build-up, all the hypothetical Noogler hats and difference arrays, would be for nothing. But then I saw a glimmer of hope. The SVG path had a specific CSS identifier: . These are the handles that JavaScript grabs onto. If the server just wanted to display a static image, it wouldn’t necessarily need to tag the path with such a specific identifier unless the client code intended to find it and manipulate it.This was my lead. If the code was generating or modifying that path on the fly, it had to reference that identifier. It was time to dive into the spaghetti bowl of modern web development: obfuscated code.For the uninitiated, looking at production JavaScript is like trying to read a novel where every character’s name has been replaced with a single letter. Variable names like calculateHeatmapDistribution become , , or . This isn’t just to annoy curious developers; it’s about efficiency. Computers don’t care if a variable is named  or . They execute the logic just the same. But  takes up one byte, while the descriptive name takes nineteen. Multiply that by millions of lines of code and billions of requests, and you’re saving a massive amount of bandwidth. The result is a dense, impenetrable wall of text that is fast for the network but a nightmare for humans. But somewhere in that mess, I hoped to find my , since it was an identifier used in the HTML and can’t be obfuscated.I jumped over to the sources tab and located the  file, a massive, minified behemoth that powers the YouTube player. With a mix of hope and trepidation, I hit  and ‘ed my magic string. To my surprise, the counter next to the search bar stopped at just two. That was manageable. Better than manageable; it was lucky.The first occurrence landed me inside a large nested JavaScript object. As I parsed the structure, it started to look familiar. The keys in this object, such as , , and , perfectly matched the tags and attributes I had seen in the HTML. It was a blueprint. This code was defining the structure of the player’s UI components.But something was missing. The most critical part of an SVG path is the d attribute, the string of commands and coordinates that actually tells the browser where to draw the lines. In the code in front of me, the  attribute was there, but it was initialized as an empty string. I tabbed back to the live DOM tree in the inspector. There, the  attribute was packed with a long, complex sequence of numbers and letters.The discrepancy was the smoking gun. If the static code had an empty string, but the running application had a full path, it meant only one thing. The curve wasn’t being downloaded as a finished asset. It was being calculated, point by point, right here in the browser, and injected into the DOM dynamically. The logic I was looking for was close.To find the logic, I needed another magic string. Since this logic involved calculating the SVG path, it surely must be relying on data from an API call. And since YouTube has public APIs, I turned to Google search once more, this time with “youtube api for most replayed graph” and found the exact question on Stack Overflow. Thanks to the community responses, I found the string: .But that’s a problem for another day. Right now, I had a magic string. I tabbed back to the Network tab, my fingers moving on autopilot as I fired off a search across all network requests. Jackpot. The counter lit up with 101 occurrences. One was in  file, but the other 100 were hiding inside a JSON response. This was the raw vein of data I had been digging for.I expanded the response, and there it was, laid out in plain text: a list of objects, each containing a , a , and the magic string, . The values were floats between 0 and 1, just as I had theorized.This confirmed everything. YouTube wasn’t sending a pre-drawn picture; they were sending the raw instructions. The server provided the normalized height for each segment, and the client-side JavaScript (presumably that logic I’d glimpsed in ) must be connecting the dots to draw the SVG path. I couldn’t help but feel a surge of satisfaction. My mental model of the system, constructed from scratch with nothing but intuition and a hypothetical ‘Noogler’ hat, was dead on. They were dividing the video into discrete segments and normalizing the view counts, exactly as I had predicted.What intrigued me even more than the normalized scores were:  and the total count of these segments.First, . It remained constant for every single segment. This felt redundant. Why send the duration if it’s the same for everyone? Is it future-proofing for a world where heatmaps have variable resolution, with finer detail in popular sections and broader strokes elsewhere? But it seemed like a tiny inefficiency, a few extra bytes of JSON that YouTube was paying for in network bandwidth.Then there was the count. There were exactly 100 segments for this four-minute music video. Was this a universal constant? I clicked over to a  tutorial (obviously a clickbait). The response? 100 segments. I tried a “ten-hour loop of lofi beats”. Still 100. It didn’t matter, the heatmap was always sliced into exactly one hundred pieces.Why 100? Was it the result of some deep statistical analysis on the distribution of video lengths across the platform, determining that 100 points provided the optimal balance between granularity and performance? Or was it simply because humans like round numbers and 100 felt “about right”? If you work at YouTube and know the answer, please let me know. I am genuinely curious.Armed with one hundred normalized intensity scores, I decided to render an SVG myself to see how closely my raw recreation matched YouTube’s SVG. I plotted it using a simple SVG line command: "Line To". The result was sharp, jagged, and aggressively pointy. It looked like a connect-the-dots drawing done by a ruler-wielding perfectionist. In contrast, the graph on the YouTube player was fluid, smooth, and liquid.Theory: The Geometry of SmoothnessMy spiky graph would have fit right in with the user interfaces of a decade ago, back when applications were defined by rigid corners and sharp edges. But the digital world has moved on. We are living in the era of the  effect.This term comes from a psychological experiment where people are shown two shapes: one blobby and round, the other spiky and jagged. When asked which one is  and which is  almost everyone, across all languages and cultures, agrees: the round one is Bouba, and the spiky one is  It turns out, we have a deep-seated bias towards the round and the soft.This preference has reshaped our digital landscape, largely championed by Apple. Steve Jobs famously dragged his engineers around the block to point out that “Round Rects Are Everywhere!” forcing them to implement the shape into the original Mac’s OS. Today, that influence is inescapable. Look at Windows 11, Google’s Material You, the tabs in your browser, or the icon of the app you’re using right now. The sharp corners have been sanded down. In fact, there is even a new CSS property called .It is not just an aesthetic trend; it is a psychological one. Sharp corners signal danger to our primitive brains (think thorns, teeth, or jagged rocks). They say “ouch.” Round corners, on the other hand, signal safety. They feel friendly, approachable, and organic. They are softer on the eyes, reducing cognitive load because our gaze doesn’t have to come to an abrupt halt at every vertex. By smoothing out the edges, designers aren’t just making things look modern; they are making technology feel a little less like a machine and a little more human.Driven by this universal preference for organic shapes, I needed to understand the mathematics behind YouTube’s implementation. Was it a ? Real-world data is inherently noisy, and a moving average is the standard tool for smoothing it out. By sliding a “window” across the dataset and averaging the points within it (say, three at a time), it irons out the wrinkles. Instead of a single, erratic value, you get the consensus of the neighborhood.I decided to test this theory. I applied a moving average to my jagged plot, hoping to see the familiar YouTube curve emerge. It certainly helped sand down the sharpest peaks, making the graph look less like a mountain range and more like rolling hills. But it created a critical problem. The distinctive dips flanking the main peak (the very artifacts I was trying to replicate) were nowhere to be found. They weren’t in the raw data, and the moving average certainly didn’t create them; if anything, it would have smoothed them out if they were there. My plot was now smooth, but it was featureless. It looked like a low-resolution approximation, missing the specific character of the real thing. Clearly, a simple moving average wasn’t the answer.From Discrete to ContinuousSo I decided to check the path of the YouTube SVG itself. My recreation relied on the  (). It draws a straight, uncompromising line from point A to point B. Simple, efficient, but undeniably jagged. YouTube, however, wasn’t using lines. Their path was packed with  (). It became clear that the secret wasn’t in the data values themselves, but in how they were connected. I decided to pause the investigation and familiarize myself with the mathematics of curves.To understand , I had to start at , or  The equation for a basic line between two points \(P_0\) and \(P_1\) is:$$P(t) = \text{Lerp}(P_0, P_1, t) = (1-t)P_0 + tP_1$$Here, \(t\) acts as a slider ranging from 0 to 1. At \(t=0\), we are at the start point \(P_0\). At \(t=1\), we arrive at the end point \(P_1\).A simple line connecting two points using the above equation. (You are going to have to settle for a video from here on out because  does a better job than I ever could with an HTML canvas.) Check out the interactive graph on Desmos.To get a curve, we extend this concept using the . Imagine you have three points: a start \(P_0\), an end \(P_2\), and a  \(P_1\) hovering in between. The math essentially  by nesting the equations. First, we calculate two moving intermediate points to create a sliding segment:$$Q_0 = \text{Lerp}(P_0, P_1, t) = (1-t)P_0 + tP_1$$$$Q_1 = \text{Lerp}(P_1, P_2, t) = (1-t)P_1 + tP_2$$Then, we interpolate between those two moving points to find our final position:$$P(t) = \text{Lerp}(Q_0, Q_1, t) = (1-t)Q_0 + tQ_1$$When you expand this algebra, you get the quadratic formula:$$P(t) = (1-t)^2P_0 + 2(1-t)tP_1 + t^2P_2$$The result is a smooth curve that starts at \(P_0\) and travels towards \(P_2\), but is magnetically pulled towards \(P_1\) without ever touching it.A quadratic Bézier curve. Notice how since it has only one control point, it can only bend in one direction. [Desmos]However, because  relies on a single control point, it lacks the flexibility to create “S” curves or inflections; it can only bend in one direction. For “S” curves we need two control points. Which brings us to the . This adds a second control point, giving us four points total: Start (\(P_0\)), Control 1 (\(P_1\)), Control 2 (\(P_2\)), and End (\(P_3\)). We just add another layer of depth to the recursion.First layer (edges of the hull):$$Q_0 = \text{Lerp}(P_0, P_1, t)$$$$Q_1 = \text{Lerp}(P_1, P_2, t)$$$$Q_2 = \text{Lerp}(P_2, P_3, t)$$Second layer (connecting the moving points):$$R_0 = \text{Lerp}(Q_0, Q_1, t)$$$$R_1 = \text{Lerp}(Q_1, Q_2, t)$$Final layer (the curve itself):$$P(t) = \text{Lerp}(R_0, R_1, t)$$Substituting everything back in gives the elegant cubic formula:$$P(t) =\\(1-t)^3P_0 + 3(1-t)^2tP_1 + 3(1-t)t^2P_2 + t^3P_3$$As \(t\) moves from 0 to 1, these equations trace a perfect parabolic arc. This is precisely how the browser renders those smooth, organic shapes, calculating positions pixel by pixel to create the visual comfort we expect.A cubic Bézier curve. Notice how it can bend in two directions and form “S” curves easily. [Desmos]It is worth noting that this logic does not have to stop at four points. You can theoretically have Bézier curves with five, ten, or a hundred control points, creating increasingly intricate shapes with a single mathematical definition. However, there is a catch. As you add more points, the computational cost skyrockets. Solving high-degree polynomials for every frame of an animation or every resize event is expensive. That is why modern graphics systems usually stick to cubic curves. If you need a more complex shape, it is far more efficient to chain multiple cubic segments together than to crunch the numbers for a single, massive high-order curve.A visualization of a 10-point Bézier spline in action. [Desmos]The Invisible ScaffoldingThis specific bit of math isn’t unique to YouTube’s video player. It is the invisible scaffolding of the entire digital visual world. If you have ever used the Pen Tool in Photoshop, Illustrator, or Figma (or their respective open-source alternatives, Gimp, Inkscape and Penpot), you have directly manipulated these equations. When you pull those little handles to adjust a curve, you are literally moving the control points (\(P_1\) and \(P_2\)) in the formula above, redefining the gravitational field that shapes the line. But you don’t have to be a designer to interact with them. In fact, you are looking at them right now. The fonts rendering these very words are nothing more than collections of Bézier curves. Your computer doesn’t store a pixelated image of the letter ‘a’; it stores the mathematical instructions (the start points, end points, and control points) needed to draw it perfectly at any size. From the smooth hood of a modeled car in a video game to the vector logo on your credit card, Bézier curves are the unsung heroes that rounded off the sharp edges of the digital age.This sequence of connected curves is known as a . YouTube isn’t drawing one massive, complex curve; they are stitching together a hundered smaller cubic curves to form a continuous shape. In fact, my own jagged implementation was a spline too: a . I merely stitched together hundered straight lines.However, creating a spline introduces a new challenge: Smoothness. If you just glue two random curves together at a point (known as a “knot point”), you get a sharp corner (a “kiki” joint in our “bouba” graph). To make the transition seamless (technically known as \(C^1\) continuity), the join has to be perfect. The tangent of the curve ending at the connection point must match the tangent of the curve starting there. Visually, this means the second control point of the previous curve, the shared knot point, and the first control point of the next curve must all lie on the exact same straight line. It’s a balancing act. If one handle is off by a single pixel, the illusion of fluidity breaks.Here, there are two curves joined at a knot point. Initially, the knot point is not smooth as the three points (the two control points and the knot point) do not lie on the same straight line. But as the animation progresses, the points become , creating a smooth transition. [Desmos]I attempted to update my script to use this curve command, replacing the simple lines. However, I immediately hit a wall. While the line command is straightforward (requiring only a destination), the curve command is demanding. It requires two invisible ‘magnets’ (the control points  and ) for every single segment.The  command functions as a precise geometric instruction, telling the renderer exactly how to shape the curve. The syntax is deceptively simple: . Where  and  are the control points and  is where the curve ends. You might ask: where is the starting point? In SVG paths, it is implicit, the curve begins wherever the previous command ended. For instance, YouTube’s path starts with  (Move to start), immediately followed by a curve definition C 1.0,89.5 2.0,49.9 5.0,47.4 which ends at  becoming the starting point for the next command C 8.0,44.9 11.0,79.0 15.0,87.5 and so on.Resolution: The Invisible HandThese points didn’t appear out of thin air, and they certainly weren’t in the JSON response, which only provided the raw heights. They had to be calculated locally. Somewhere in the client-side code, there was a mathematical recipe converting those raw intensity scores into elegant \(C^1\) continuous curve data. This sent me back to the single occurrence of  spotted earlier in . The screenshots below are from the raw, obfuscated source. They are included here only to show the steps I took during the investigation. Don’t try to make sense of them or you might get confused! Feel free to skim past them; I have provided a clean, de-obfuscated version later in the text that explains the logic clearly.The occurrence led me directly to an interesting function, .This function appeared to be building an intermediate array, preparing the data for the final drawing step. Analyzing the code, I could see it was mapping the normalized intensity scores (which range from 0 to 1) onto a coordinate system suitable for the SVG. Similar to what I did in my Linear Spline script.Specifically, it was transforming the data to fit a 1000x100 pixel canvas. The  line calculates the width of each segment (1000 divided by 100 segments equals 10 pixels per segment). The loop then iterates through the data points, calculating the x coordinate () and the y coordinate ().Crucially, it handles the coordinate system flip. In standard coordinate geometry we learn in school,  is at the bottom and values increase as you go up. In computer graphics (and SVGs), however, the origin  is at the top-left corner. This convention is a historical artifact from the days of  (CRT) monitors. On those old, bulky screens, an electron beam would physically scan across the phosphor surface, starting from the top-left, drawing a line to the right, snapping back (horizontal retrace), and moving down to draw the next line. If you are old enough, you might remember seeing this flickering motion on screens with low refresh rates. Modern LCDs and OLEDs don’t have electron beams, but the software coordinate system stuck. So, to draw a “high” peak on a graph, you actually need a small y-coordinate (closer to the top). The code accounts for this with , inverting the values so that a higher intensity score results in a smaller y value (pushing the point “up” towards the top of the container). It also prepends a starting point at  (bottom-left) and appends a closing point at  (bottom-right), a detail whose importance will become clear shortly.This function () was clearly just the preparation step. It was normalizing the data into pixel space, but it wasn’t drawing anything yet. It had to be invoked somewhere. In a proper IDE, I would just hit  to jump to usage. But browser DevTools aren’t quite there yet for this kind of reverse engineering. So, I resorted to the old-school method: a text search for “”. Note the opening parenthesis; that’s the trick to find calls rather than definitions.This search led me to a function named :Bulls-eye. There it was, plain as day:  (inside the red box). This line confirmed that  (the result of ) was indeed the path string being injected into the SVG.The transformed list from  was being stored in  and then passed straight into another function: . To find its definition, I relied on a common pattern in minified JavaScript: functions are typically declared anonymously and assigned to a variable. Debugging obfuscated code becomes much easier when you know these patterns. I simply searched for “” and found the heart of the operation:This was it. The smoking gun. I could see the string construction happening in real-time. The code iterates through the points, and for every segment, it appends a  command string. But look closely at the arguments for that command. The end point ( or ) was already known. But the variables  and  (representing the two control points) were being calculated on the fly by a helper function: .The control points weren’t just appearing; they were being dynamically generated based on the position of the current point and its neighbors. The logic didn’t stop there. I chased  down the rabbit hole.This snippet finally revealed the math.  was creating a new object  (which I found to be a simple Vector class storing  and  differences) and then using  and  to offset the control points. The mysterious  multiplier suggested that the control points were being placed at 20% of the distance determined by the vector calculation.Deciphering the BlueprintAfter deciphering the minified code, I was able to reconstruct the logic in readable JavaScript.The reconstructed code reveals a fascinatingly simple geometric strategy. To determine the control point for a specific knot (let’s call it ), the algorithm doesn’t look at  in isolation. Instead, it looks at its neighbors. It draws an imaginary line connecting the previous point () directly to the next point (), completely skipping . The slope of this imaginary line becomes the tangent for the curve at . This specific method of curve generation is known as a .A visualization of the cardinal spline algorithm in action. In the animation, the tangent scales down as it moves towards the knot point to 0.2 of its original length. [Desmos]By calculating the tangent at each knot point based on the positions of its neighbors, it ensures that the curve arrives at and departs from each point with the same velocity (tangent vector), guaranteeing that elusive \(C^1\) continuity I mentioned earlier.Remember those extra points added at  and  that I mentioned earlier? Without them, the first and last segments of the video data would have no outer neighbors. By artificially adding them, the algorithm effectively tells the curve to start and end its journey with a smooth trajectory rising from the baseline, rather than starting abruptly in mid-air.And what about that magic  number? That determines the tension of the curve. In the world of splines, this factor controls the length of the tangent vectors. If this value were , we would be looking at a standard Catmull-Rom spline, often used in animation for its loose, fluid movement. However, a value of  would collapse the control points onto the anchors, reverting the shape back to a sharp, jagged linear spline.And there it was. The answer to the mystery of the dips. It wasn’t a rounding error, a data glitch, or a server-side anomaly. It was the math itself. Specifically, the requirement for continuity. When a data point spikes significantly higher than its neighbors, the Cardinal Spline algorithm calculates a steep tangent to shoot up to that peak. To maintain that velocity and direction smoothly as it passes through the neighboring points, the curve is forced to swing wide (dipping below the baseline) before rocketing upwards. It’s the visual equivalent of a crouch before a jump. The dips weren’t bugs; they were the inevitable artifacts of forcing rigid, discrete data into a smooth, organic flow.Notice how the dips flatten out as the peak lowers. [Desmos]I started pulling on this loose thread on a quiet afternoon, simply wondering about a song from . By nightfall, I had followed the thread to its end, tracing the logic from pixel to polynomial. Documenting it, however, was a marathon that spanned many weeks of focused work.This project wasn’t just a random curiosity; it was about the joy of digging until I hit the bedrock of logic. It forced me to bridge concepts from disparate domains (frontend engineering, competitive programming, geometry, and design history) and to think deeply about the performance implications of every calculation. I am grateful to live in an era where curiosity can be so readily shared with the world.Having made it this far (through over 7,000 words), you have my wholehearted thanks for lending me your time. If you enjoyed this descent into madness and want to support future deep dives, consider buying me a coffee (or two?). Though I don’t drink coffee, it helps pay for the domain costs.]]></content:encoded></item><item><title>Show HN: I built a text-based business simulator to replace video courses</title><link>https://www.core-mba.pro/</link><author>Core_Dev</author><category>hn</category><pubDate>Fri, 16 Jan 2026 01:41:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[  REVENUE IS VANITY, CASH IS SANITY  |  COMPETITION IS FOR LOSERS. SCALE OR DIE  |  EGO IS THE ENEMY OF PROFIT. KILL IT  |  DEFAULT ALIVE IS THE ONLY REAL KPI  |  MARKETERS SELL TOOLS, STRATEGISTS SELL HOLES  |  YOUR IDEA IS WORTH $0. EXECUTION IS THE MULTIPLIER  |  FOCUS IS A SUPERPOWER. SAY NO TO EVERYTHING ELSE  |  BE DIFFERENT, NOT JUST BETTER  |  SOLVE A BLEEDING NECK PROBLEM OR QUIT  |  RECRUIT WARRIORS, NOT EMPLOYEES  |  FIRE FAST. HIRE SLOW. PROTECT THE CULTURE  |  YOUR BRAND IS A PROMISE DELIVERED, NOT A LOGO  |  COMPLEXITY IS THE SILENT KILLER OF MARGINS  |  IF YOU CAN'T MEASURE IT, YOU DON'T OWN IT  |  NET PROFIT IS THE ONLY SCOREBOARD THAT MATTERS  |  SPEED IS AN UNFAIR COMPETITIVE ADVANTAGE  |  BORROWED CAPITAL IS A NOOSE, OWNED CAPITAL IS A SHIELD  |  MARKET SHARE IS RENTED, BRAND LOYALTY IS OWNED    REVENUE IS VANITY, CASH IS SANITY  |  COMPETITION IS FOR LOSERS. SCALE OR DIE  |  EGO IS THE ENEMY OF PROFIT. KILL IT  |  DEFAULT ALIVE IS THE ONLY REAL KPI  |  MARKETERS SELL TOOLS, STRATEGISTS SELL HOLES  |  YOUR IDEA IS WORTH $0. EXECUTION IS THE MULTIPLIER  |  FOCUS IS A SUPERPOWER. SAY NO TO EVERYTHING ELSE  |  BE DIFFERENT, NOT JUST BETTER  |  SOLVE A BLEEDING NECK PROBLEM OR QUIT  |  RECRUIT WARRIORS, NOT EMPLOYEES  |  FIRE FAST. HIRE SLOW. PROTECT THE CULTURE  |  YOUR BRAND IS A PROMISE DELIVERED, NOT A LOGO  |  COMPLEXITY IS THE SILENT KILLER OF MARGINS  |  IF YOU CAN'T MEASURE IT, YOU DON'T OWN IT  |  NET PROFIT IS THE ONLY SCOREBOARD THAT MATTERS  |  SPEED IS AN UNFAIR COMPETITIVE ADVANTAGE  |  BORROWED CAPITAL IS A NOOSE, OWNED CAPITAL IS A SHIELD  |  MARKET SHARE IS RENTED, BRAND LOYALTY IS OWNED]]></content:encoded></item><item><title>Show HN: Gambit, an open-source agent harness for building reliable AI agents</title><link>https://github.com/bolt-foundry/gambit</link><author>randall</author><category>hn</category><pubDate>Fri, 16 Jan 2026 00:13:25 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Wanted to show our open source agent harness called Gambit.If you’re not familiar, agent harnesses are sort of like an operating system for an agent... they handle tool calling, planning, context window management, and don’t require as much developer orchestration.Normally you might see an agent orchestration framework pipeline like:compute -> compute -> compute -> LLM -> compute -> compute -> LLMwe invert this so with an agent harness, it’s more like:LLM -> LLM -> LLM -> compute -> LLM -> LLM -> compute -> LLMEssentially you describe each agent in either a self contained markdown file, or as a typescript program. Your root agent can bring in other agents as needed, and we create a typesafe way for you to define the interfaces between those agents. We call these decks.Agents can call agents, and each agent can be designed with whatever model params make sense for your task.Additionally, each step of the chain gets automatic evals, we call graders. A grader is another deck type… but it’s designed to evaluate and score conversations (or individual conversation turns).We also have test agents you can define on a deck-by-deck basis, that are designed to mimic scenarios your agent would face and generate synthetic data for either humans or graders to grade.Prior to Gambit, we had built an LLM based video editor, and we weren’t happy with the results, which is what brought us down this path of improving inference time LLM quality.We know it’s missing some obvious parts, but we wanted to get this out there to see how it could help people or start conversations. We’re really happy with how it’s working with some of our early design partners, and we think it’s a way to implement a lot of interesting applications:- Truly open source agents and assistants, where logic, code, and prompts can be easily shared with the community.- Rubric based grading to guarantee you (for instance) don’t leak PII accidentally- Spin up a usable bot in minutes and have Codex or Claude Code use our command line runner / graders to build a first version that is pretty good w/ very little human intervention.We’ll be around if ya’ll have any questions or thoughts. Thanks for checking us out!]]></content:encoded></item><item><title>My Gripes with Prolog</title><link>https://buttondown.com/hillelwayne/archive/my-gripes-with-prolog/</link><author>azhenley</author><category>hn</category><pubDate>Fri, 16 Jan 2026 00:11:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[For the next release of Logic for Programmers, I'm finally adding the sections on Answer Set Programming and Constraint Logic Programming that I TODOd back in version 0.9. And this is making me re-experience some of my pain points with Prolog, which I will gripe about now.  If you want to know more about why Prolog is cool instead, go here or here or here or here. ISO "strings" are just atoms or lists of single-character atoms (or lists of integer character codes). The various implementations of Prolog add custom string operators but they are not cross compatible, so code written with strings in SWI-Prolog will not work in Scryer Prolog. Code logic is expressed entirely in , predicates which return true or false for certain values. For example if you wanted to get the length of a Prolog list, you write this:Now this is pretty cool in that it allows bidirectionality, or running predicates "in reverse". To generate lists of length 3, you can write . But it also means that if you want to get the length a list , you can't do that in one expression, you have to write length(List, Out), X is Out+1.For a while I thought no functions was necessary evil for bidirectionality, but then I discovered Picat has functions and works just fine. That by itself is a reason for me to prefer Picat for my LP needs.(Bidirectionality is a killer feature of Prolog, so it's a shame I so rarely run into situations that use it.)No standardized collection types besides listsAside from atoms () and numbers, there are two data types:Linked lists like .Compound terms like , which  like record types but are actually tuples. You can even convert compound terms to linked lists with :There's no proper key-value maps or even struct types. Again, this is something that individual distributions can fix (without cross compatibility), but these never feel integrated with the rest of the language.  and  aren't values, they're control flow statements.  is a noop and  says that the current search path is a dead end, so backtrack and start again. You can't explicitly store true and false as values, you have to implicitly have them in facts ( instead of ).This hasn't made any tasks impossible, and I can usually find a workaround to whatever I want to do. But I do think it makes things more inconvenient! Sometimes I want to do something dumb like "get all atoms that don't pass at least three of these rules", and that'd be a lot easier if I could shove intermediate results into a sack of booleans. (This is called "Negation as Failure". I think this might be necessary to make Prolog a Turing complete general programming language. Picat fixes a lot of Prolog's gripes and still has negation as failure. ASP has regular negation but it's not Turing complete.) Prolog finds solutions through depth first search, and a "cut" () symbol prevents backtracking past a certain point. This is necessary for optimization but can lead to invalid programs. You're not supposed to use cuts if you can avoid it, so I pretended cuts didn't exist. Which is why I was surprised to find that conditionals are implemented with cuts. Because cuts are spooky dark magic conditionals  conditionals work as I expect them to and sometimes leave out valid solutions and I have no idea how to tell which it'll be. Usually I find it safer to just avoid conditionals entirely, which means my code gets a lot longer and messier. The original example in the last section was this:  returns true, so you'd expect  to return . But it returns .  Whereas this works as expected.I  this was because  was implemented with cuts, and the Clocksin book suggests it's , so this was my prime example about how cuts are confusing. But then I tried this:There's no way to get that behavior with cuts! I don't think  uses cuts at all! And now I have to figure out why 
 doesn't returns results. Is it floundering? Is it because  only succeeds if  fails, and  always succeeds? A closed-world assumption? Something else?Straying outside of default queries is confusingSay I have a program like this:And I want to know all of the nodes that are parents of branches. The normal way to do this is with a query:This is interactively making me query for every result. That's usually not what I want, I know the result of my query is finite and I want all of the results at once, so I can count or farble or whatever them. It took a while to figure out that the proper solution is bagof(Template, Goal, Bag), which will "Unify Bag with the alternatives of Template":Wait crap that's still giving one result at a time, because  is a free variable in  so it backtracks over that. It surprises me but I guess it's good to have as an option. So how do I get all of the results at once?The only difference is the , which tells  to ignore and group the results of . As far as I can tell, this is the  place the ISO standard uses  to mean anything besides exponentiation. Supposedly it's the existential quantifier? In general whenever I try to stray outside simpler use-cases, especially if I try to do things non-interactively, I run into trouble.I have mixed feelings about symbol termsIt took me a long time to realize the reason   "works" is because infix symbols are mapped to prefix compound terms, so that   is , and then different predicates can decide to do different things with .This is also why you can't just write : that unifies  with the .  is , as . You have to write , as  is the operator that converts  to a mathematical term.(And  fails because  isn't fully bidirectional. The lhs  be a single variable. You have to import  and write .)I don't like this, but I'm a hypocrite for saying that because I appreciate the idea and don't mind custom symbols in other languages. I guess what annoys me is there's no official definition of what  is, it's purely a convention. ISO Prolog uses  (aka ) as a convention to mean "pairs", and the only way to realize that is to see that an awful lot of standard modules use that convention. But you can use  to mean something else in your own code and nothing will warn you of the inconsistency.Anyway I griped about pairs so I can gripe about .This one's just a blunder:According to an expert online this is because sort is supposed to return a sorted , not a sorted list. If you want to preserve duplicates you're supposed to lift all of the values into  compound terms, then use keysort, then extract the values. And, since there's no functions, this process takes at least three lines. This is also how you're supposed to sort by a custom predicate, like "the second value of a compound term". (Most (but not all) distributions have a duplicate merge like msort. SWI-Prolog also has a sort by key but it removes duplicates.)Please just let me end rules with a trailing comma instead of a period, I'm begging youI don't care if it makes fact parsing ambiguous, I just don't want "reorder two lines" to be a syntax error anymoreI expect by this time tomorrow I'll have been Cunningham'd and there will be a 2000 word essay about how all of my gripes are either easily fixable by doing XYZ or how they are the best possible choice that Prolog could have made. I mean, even in writing this I found out some fixes to problems I had. Like I was going to gripe about how I can't run SWI-Prolog queries from the command line but, in doing do diligence finally  figured it out:swipl-thalt-g./file.pl
It's pretty clunky but still better than the old process of having to enter an interactive session every time I wanted to validate a script change.(Also, answer set programming is pretty darn cool. Excited to write about it in the book!)]]></content:encoded></item><item><title>List of individual trees</title><link>https://en.wikipedia.org/wiki/List_of_individual_trees</link><author>wilson090</author><category>hn</category><pubDate>Fri, 16 Jan 2026 00:05:01 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>All 23-Bit Still Lifes Are Glider Constructible</title><link>https://mvr.github.io/posts/xs23.html</link><author>HeliumHydride</author><category>hn</category><pubDate>Thu, 15 Jan 2026 23:59:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why senior engineers let bad projects fail</title><link>https://lalitm.com/post/why-senior-engineers-let-bad-projects-fail/</link><author>SupremumLimit</author><category>hn</category><pubDate>Thu, 15 Jan 2026 22:33:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When I was a junior engineer, my manager would occasionally confide his frustrations to me in our weekly 1:1s. He would point out a project another team was working on and say, “I don’t believe that project will go anywhere, they’re solving the wrong problem.” I used to wonder, “But you are very senior, why don’t you just go and speak to them about your concerns?” It felt like a waste of his influence to not say anything.So it’s quite ironic that I found myself last week explaining to a mentee why I thought a sister team’s project would have to pivot because they’d made a poor early design choice. And he rightfully asked me the same question I had years ago: “why don’t you just tell them your opinion?” It’s been on my mind ever since because I realized I’d changed my stance on it a lot over the years.The answer is that being right and being effective are different.In large companies, speaking up about what you see as a “bad project” is a good thing. But only in moderation. Sometimes the mark of seniority is realizing that arguing with people who won’t listen isn’t worth it; it’s better to save your counsel.What I mean by a “bad project” is many things:: making product complicated, solving a problem which doesn’t exist, breaking existing workflows: overcomplicated design, wrong library, poor performing architecture: chasing hype cycles, exists primarily to justify a promotionIt’s important to point out that for much of the lifecycle of a project, whether it’s “bad” is  subjective. Software engineering is largely a game of tradeoffs and making decisions which are not perfect but the best possible with the information available. There often can be disagreements on whether correct choices are made and it only becomes obvious much later on, potentially years after a project has shipped.But as you become more senior, you’ll start to have “taste” when it comes to software projects and that will cause you to look at some fraction of the software projects and feel “this doesn’t make sense”. And this gut feeling is the sign to me of a “bad project”, one which you can see in advance of when it’s obvious to everyone.Drawing on my personal experience, the most memorable example was a few years ago at Google . There was a high-profile announcement internally of a “game changer” project that sat right at the intersection of two extremely large organizations. It was technically amazing and elegant, and full of clever ideas for really hard problems.But I distinctly remember sitting in the room for the announcement, turning to my lead and whispering, “This project has no chance of succeeding, right?” He turned to me and just said, “Yup.” We both realized the problem immediately. The project was entirely based on a platform team asking a flagship product team to give up control of their core user flow: technically the right move, but no lead or PM would ever cede ownership of something that central to another team. Politically, this project was a total fantasy.The project kept quietly chugging away in the background for almost two years. Every time it got close to launch, it would get pushed back as “not ready yet.” Over time, we heard less and less about it until, eventually, the inevitable “strategic pivot” email appeared in my inbox. Resources were reallocated and the code was deleted. We were told the company “learned a lot from the effort,” but to me it felt like it was doomed from the beginning. Politics and solving the correct problem matter just as much as technical beauty.Why you cannot stop them allWhen I started noticing “bad projects” and I felt that I had some expertise to share, the temptation for me was to start calling them out. Reach out to the team doing it, tell them “this doesn’t make sense” and explain to them why. Use facts and logic to persuade.And I did do this. But only for a very short time before I realized that there are a lot of costs to doing this that I just wasn’t thinking about.Firstly, software companies have an inherent bias for action. They value speed and shipping highly. Concerns, by definition,  and mean people have to look at things which they hadn’t budgeted for. And so unless your concern is big enough to overcome the “push for landing”, there’s little chance for any meaningful change to come from you saying something. In fact, it’s very likely that you’ll be largely ignored.Related to this, even if the team  take your concern seriously, you have to be careful not to do it too often. Once or twice, you might be seen as someone who is upholding “quality”. But do it too often and you quickly move to being seen as a “negative person”, someone who is constantly a problem maker, not a problem “fixer”. You rarely get credit for the disasters you prevented. Because nothing happened, people forget about it quickly.There’s also the problem that every time you push back, you are potentially harming someone’s promotion packet or a VP’s “pet project.” You are at risk of burning bridges and creating “enemies”, at least of a sort. Having a few people who disagree in a big company with you is the cost of doing business, but if you have too many, it starts affecting your main work too.Finally, there is also the psychological impact. There is one of you and hundreds of engineers working in spaces that your expertise might help with. Your attention is finite, but the capacity for a large company to generate bad ideas is infinite. Speaking from experience, getting too involved in stopping these quickly can make you  cynical about the state of the world. And this is really not a good place to be.Manage influence like a bank accountSo if you cannot stop all the bad projects, what do you do? You get strategic. Instead of trying to fix everything, view your influence as a bank account. You have a certain amount of “influence” coming in every month as you do your job, help people, ship successful projects, and generally remain low friction.Then, when it matters, you should be ready to make “withdrawals.” Every time you block something or raise concerns, no matter how small, you are writing a check against your balance. But not all checks are the same size: A nitpick on a code review. Cheap, daily expense. Challenging an architectural decision or pushing back on a timeline. Requires some savings. Trying to kill a VP’s pet project. This is a massive spend. You might only afford this once every few years.The problem comes if you spend $5 on every minor inefficiency you see. If you are constantly saying “no” to small things, your account will be empty when you need to write the big check to stop a true disaster.If you “go overdrawn,” you enter political bankruptcy. People stop inviting you to meetings, they stop asking for your opinion, they essentially start working around you. Once you are bankrupt, your influence drops to zero and you not only harm your ability to influence things but also start hurting your own ability to get things done.Given that we’ve now accepted that we cannot weigh in on everything, we need to figure out when it  make sense to do so.The most important thing to do first is to be humble and evaluate whether you actually have the expertise to make a judgment. Seniority often brings opinions, but those are not always informed opinions. For example, while I have some frontend experience, I do not feel qualified to give deep advice on it because my knowledge is “enough to get by” rather than deep expertise that comes from long term ownership. It is easy to lose sight of the fact that high-quality judgments require informed opinions. If you find yourself in this position, see yourself as an opinionated observer and stop there.You must also internalize the fact that just because you say something does not make it the truth. You are raising awareness of a point of view, not issuing a decree. So if some team doesn’t listen to your concerns and decides to go ahead with what they were doing anyway, then you have to accept that and move on: at the end of the day, you’re an engineer, not a CEO with authority over them!Given these points, I use three main factors to decide when to speak up:How close is the project to my team?If it goes wrong, how much impact will it have on my team?If it goes wrong, how big will the problem be for the company? If a project is close to you, the “price tag” of saying something is lower. If it is within your own team, the cost is near zero because you have high trust and a quick conversation often solves it. If it is in your broader organization, the price goes up; you have to spend social capital and potentially stake your reputation. If it is outside your org? The cost is often prohibitive. You have zero leverage, different reporting chains, and stopping it would require a massive withdrawal. Sometimes another org does something that deeply affects your work. For example, because Perfetto (the performance tool I work on) has users throughout Google, sometimes a team will ask us to sign off on a very complex integration. This is a classic risk: if things go right, they get the credit, but if things go wrong, your leadership might expect  to help solve a problem you didn’t create. In these cases, the payoff of speaking up is high because you are protecting your team. Finally, consider the blast radius. Some projects are self-contained; if they fail, they only take themselves down. Others are so intertwined with core systems that their failure causes widespread damage or creates technical debt that persists for years. These can be deadly to the long-term health of a project.How to act with bad projectsIt’s also not just about when you put your opinions forward but how you do it. There’s a very wide range of actions you can take depending on what you’re facing.The nuclear option is to directly say “we should not do this” and try to shut the project down. This almost always requires escalation to your leads and the leads of the owning team, requiring great conviction in both the fact that you’re right and that this project will be actively harmful. But on some occasions, this is the right thing to do, especially if the cost of not saying something can be existential to your project or team.A slightly softer but still quite risky variant of this is, instead of doing a direct escalation, you raise concerns in directly with the team. Usually this is done with a meeting with the team or a strongly worded “concern” or “rebuttal” doc. The goal is to speak in strong enough terms that the team themselves conclude that this the project might not be a good idea.Then there are the smaller interventions, nudging things in the right direction. These are perfect for when a team is about to do something that makes sense from a high level but they are going about this the wrong way. I see this often with Perfetto: a team sends a design doc proposing a complex use of Perfetto that I know will cause them pain later. I sit down with them, understand their actual problem, and guide them to a better solution. It costs an hour but saves them months. If you do it right, you can even be seen as a helper rather than a hindrance, even if you do slow down the team.Sometimes you conclude that the ROI just isn’t there to do anything direct: the political momentum is too strong, or the issue is too small to justify spending any influence. At this point, what you do depends on how much your team is involved.If it overlaps with your team’s work heavily then it might be best to make some subtle contingency plans: reducing your dependency on it or building abstractions to cope if it goes away. There is also a long game trick here. Even a bad project usually has an “essence” of a good idea, a specific problem it was trying to solve or an insight it was based on. If it fits with your job, it’s often a good idea to take that essence and see if you can naturally incorporate a better version of that specific solution into your own project. That way, if the bad project stalls or gets canceled, you can be proactive instead of reactive to the fallout.Alternatively, if you’re not involved, it’s easy: just stay out of the picture. Vent to friendly colleagues in private, commiserate, but in public, live with the reality.Managing your team through itFinally, you must manage your own team through the process. If you can see the flaws in a project, other senior engineers probably see them too. Don’t try to gaslight them or “walk the company line” by pretending a bad project is actually good. It destroys trust.Instead, be honest about the facts on the ground without going into unnecessary political details. Tell them that you will do the best you can under these constraints.So what did I tell my mentee? “I’ve learned that being right and being effective are different things. I could go tell them my concerns. They probably wouldn’t listen. I’d burn some goodwill. And in six months, nobody will remember that I called it, they’ll just remember I was the guy who tried to block their work”.When you’re earlier in your career, you want to believe that good ideas win on merit, that if you just explain clearly enough, people will see reason. It took me quite some time to accept that big companies don’t work that way.But this doesn’t mean you stop caring. It means you get strategic about when to spend your credibility. Pick the battles where you can actually change the outcome, where your team will be hurt if you stay silent, where the cost of being wrong is low but the cost of the project failing is high.And for everything else? You vent to colleagues, you make quiet contingency plans, and you watch. Sometimes you learn something. Sometimes you’re wrong and the project actually works. And sometimes you get to feel that grim satisfaction of predicting exactly how things would fall apart.None of this is as satisfying as fixing everything. But it works and keeps me sane.]]></content:encoded></item><item><title>Linux boxes via SSH: suspended when disconected</title><link>https://shellbox.dev/</link><author>messh</author><category>hn</category><pubDate>Thu, 15 Jan 2026 20:20:13 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[███████ ██   ██ ███████ ██      ██      ██████   ██████  ██   ██
██      ██   ██ ██      ██      ██      ██   ██ ██    ██  ██ ██
███████ ███████ █████   ██      ██      ██████  ██    ██   ███
     ██ ██   ██ ██      ██      ██      ██   ██ ██    ██  ██ ██
███████ ██   ██ ███████ ███████ ███████ ██████   ██████  ██   ██Lightweight instances: 2 vCPUs, 4GB RAM, 50GB SSDPure SSH access: no special clients or browser plugins requiredPersistent state: boxes pause on disconnect and resume where you left offUsage-based billing: $0.05/hr while running, $0.005/hr while stoppedAutomatic cost control: boxes stop when balance falls below $5HTTPS endpoints: every box gets a public URL with automatic TLSFull SSH support including port forwarding and scpPrepaid balance with refunds available for unused fundsCreating box...

Box 'dev1' created successfully
URL: https://dev1-a1b2c3d4.shellbox.dev

Connect with: ssh -t shellbox.dev connect dev1Starting box...
Connected!
root@dev1:~# _NAME          STATE      URL
-----------------------------------------------------------------
dev1          running    https://dev1-a1b2c3d4.shellbox.dev
myapp         stopped    https://myapp-a1b2c3d4.shellbox.devAccount Balance
==============================
Funds added:     $30.00
Funds refunded:  $10.00
Usage costs:     $1.50
------------------------------
Current balance: $18.50

Remaining hours at current rates:
  Running boxes: ~370 hours
  Idle boxes:    ~3700 hoursAdd $10.00 to your account
========================================



https://pay.paddle.com/...

Scan QR code or visit URL to complete payment.
Your account will be credited automatically.List your boxes with status and URLsConnect to a box (use ssh -t)Add funds to your account]]></content:encoded></item><item><title>Briar keeps Iran connected via Bluetooth and Wi-Fi when the internet goes dark</title><link>https://briarproject.org/manual/fa/</link><author>us321</author><category>hn</category><pubDate>Thu, 15 Jan 2026 19:38:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Briar یک برنامه پیام رسان می باشد که برای فعالان، روزنامه نگاران و هر کسی که نیازمند یک راه امن، راحت و پیشرفته برای ارتباط با دیگران است می باشد. برخلاف برنامه‌ های پیام‌رسان‌ مرسوم، Briar به سرور متمرکز اتکا ندارد - پیام ها به صورت مستقیم بین دستگاه کاربران همگام می شود. اگر اینترنت کار نکند، Briar می‌تواند از طریق بلوتوث یا وای‌-فای همگام سازی کرده، جریان اطلاعات را در زمان بحران نگه دارد. اگر اینترنت کار کند، Briar می‌تواند برای محافظت کاربران و وابط آن ها از از شنود، از طریق شبکه تور همگام سازی کند. اگر مطمئن نیستید که دستگاه شما اندروید می باشد، وجود برنامه پلی استور یا Play Store را بررسی کنید. در صورت وجود، دستگاه شما اندروید می باشد.اگر یک دستگاه اندروید دارید اما ترجیح می‌دهید که از گوگل پلی استفاده نکنید، وب‌ سایت Briar راهنمایی های لازم برای نصب برنامه از طریق F-Droid یا دانلود مستقیم را دارد.نخستین باری که Briar را باز می‌کنید، از شما خواسته می‌شود یک حساب کاربری ایجاد کنید. می‌توانید هر نام مستعار و گذرواژه‌ ای را انتخاب کنید. گذرواژه حداقل باید دارای 8 کاراکتر باشد و حدس زدن آن دشوار باشد. حساب کاربری Briar شما به صورت امن بر روی دستگاه شما ذخیره شده است، نه روی فضای ابری. اگر Briar را پاک کنید یا گذرواژه خود را فراموش کنید، راهی برای بازیابی حساب‌ کاربری خود ندارید.روی “ایجاد حساب کاربری” ضربه بزنید. وقتی حساب کاربری شما ساخته شد به فهرست مخاطبان هدایت خواهید شد.برای افزودن مخاطب، روی نگارک جمع در بخش راست پایین از فهرست مخاطبان ضربه بزنید.یکی از دو گزینه‌ای که نمایش داده می شود را انتخاب کنید.پیوند briar:// را کپی کنید و به کسی که میخواهید اضافه کنید ارسال کنید.
همچنین می‌توانید از دکمه‌ “اشتراک گذاری” برای انتخاب یک برنامه جهت ارسال لینک استفاده کنید.پیوندی که از مخاطبی که قصد اضافه کردن آن  را دارید را در فیلد متن زیر وارد کنید.
روی “ادامه” کلیک کرده و یک نام مستعار برای مخاطب جدید انتخاب کنید.در ادامه، صفحه “درخواست های در حال انتظار مخاطب” را خواهید دید
که شما را در خصوص وضعیت هر مخاطب در حال انتظار مطلع می‌کند.
Briar تلاش خواهد کرد که تا به مخاطب شما به صورت منظم وصل شود تا آن ها را اضافه کند.به محض اینکه اتصال برقرار شد، به فهرست مخاطبان یکدیگر اضافه خواهید شد.
تبریک! شما آماده‌ ارتباط امن می باشید.اگر Briar پس از 48 ساعت نتوانست به مخاطب شما وصل شود، فهرست مخاطبان در انتظار پیام “افزودن مخاطب ناموفق بود” را خواهند دید. هر دو شما باید مخاطب در حال انتظار را از فهرست پاک کرده و پیوند های یکدیگر را دوباره اضافه کنید.یک راه دیگر برای افزودن یک مخاطب این است تا با شخصی که میخواهید اضافه کنید ملاقات کنید. هر کدام از شما یک کد کیوآر از صفحه شخص دیگر را اسکن خواهید کرد. این اطمینان میدهد که شما به شخص درست وصل می شوید، بنابراین کس دیگری قادر نخواهد بود تا خود را جای شما بزند یا پیام های شما را بخواند.هر زمان آماده بودید، روی “ادامه” ضربه بزنید.کد کیوآر مخاطب خود را آماده کنید. شاید لازم باشد تا چند ثانیه برای متمرکز شدن دوربین صبر کنید.زمانی که دوربین کد کیوآر را اسکن کرد شما پیام “انتظار برای اسکن مخاطب و اتصال” را خواهید دید. اکنون مخاطب شما باید کد کیوآر شما را اسکن کند.دستگاه های شما اطلاعات را تبادل خواهند کرد، بعد از چند ثانیه شما به فهرست مخاطبان یکدیگر اضافه خواهید شد. تبریک! شما آماده ارتباط امن هستید.برای ارسال یک پیام خصوصی، روی نام مخاطب در فهرست مخاطبان کلیک کنید. تمام پیام ها در Briar به صورت سر-تا-سر رمزگذاری شده اند، بنابراین هیچکس نمی تواند آن ها را بخواند.اگر مخاطب شما آفلاین می باشد، پیام شما دفعه بعدی که هر دو شما آنلاین هستید تحویل داده می شود.شما میتوانید مخاطبان خود را به یکدیگر از طریق Briar معرفی کنید. این به آن ها اجازه می دهد تا مخاطب یکدیگر شوند بدون اینکه همدیگر را ملاقات کنند. برای شروع معرفی، روی نام مخاطب در فهرست مخاطب کلیک کنید و “ایجاد معرفی” را از منو انتخاب کنید.سپس،  مخاطب دیگری را که می‌خواهید معرفی کنید راانتخاب کنید.یک پیام اختیاری به مخاطبان اضافه کنید، سپس روی “ایجاد معرفی” کلیک کنید.مخاطبان شما یک پیام خواهند دید که از آن ها می پرسد که آیا معرفی را می پذیرند. اگر هر دو بپذیرند به فهرست مخاطب یکدیگر اضافه خواهند شد و قادر خواهند بود به صورت امن ارتباط داشته باشند.شما میتوانید از Briar برای گپ های گروهی با مخاطبان خود استفاده کنید. برای ایجاد یک گروه، منو اصلی را باز کرده و “گروه های خصوصی” را انتخاب کنید. فهرست گروه خصوصی باز خواهد شد. روی نشانک جمع ضربه زده تا یک گروه جدید ایجاد کنید.یک نام برای گروه خود انتخاب کنید، سپس روی “ایجاد گروه” ضربه بزنید.سپس روی مخاطبانی که میخواهید برای عضویت گروه دعوت کنید کلیک کنید. روی نگارک چک زمانی که کارتان تمام شد کلیک کنید.یک پیام اختیاری به مخاطبان انتخاب شده اضافه کنید، سپس روی “ارسال دعوت نامه” ضربه بزنید. مخاطبان یک پیام که آن ها را به عضویت دعوت می کند خواهند دید.گروه جدید به لیست گروه خصوصی شما اضافه خواهد شد. این لیست تمام گروه هایی که شما به آن ها تعلق دارید را نمایش میدهد.پیام ها در گروه های خصوصی به ریسه ها دسته بندی شده اند. هر عضو می تواند به یک پیام پاسخ دهد یا یک ریسه جدید را آغاز کند.ایجاد کننده گروه تنها کسی است که میتواند اعضا جدید دعوت کند. عر عضوی می تواند گروه را ترک کند. اگر ایجاد کننده گروه را ترک کند، گروه منحل خواهد شد.یک انجمن یک مکالمه عمومی می باشد. برعکس یک گروه خصوصی، هرکسی که عضو یک انجمن می شود میتواند مخاطبان خود را دعوت کند.برای ایجاد یک انجمن، منو اصلی را باز کنید و “انجمن ها” را انتخاب کنید. فهرست انجمن باز خواهد شد. روی نگارک جمع برای ایجاد یک فروم جدید ضربه بزنید.یک نام برای انجمن خود انتخاب کنید، سپس روی “ایجاد انجمن” ضربه کنید.انجمن جدید به فهرست انجمن های شما اضافه خواهد شد. این فهرست تمام انجمن هایی که شما به آن تعلق دارید را نمایش می دهد.برای به اشتراک گذاشتن یک انجمن با مخاطبان خود، روی انجمن برای باز کردن آن ضربه زده، سپس روی نگارک اشتراک گذاری ضربه بزنید.سپس مخاطبانی که می خواهید انجمن را با با آن ها به اشتراک بگذارید را انتخاب کنید. روی نگارک چک زمانی که کارتان تمام شد کلیک کنید.یک پیام اختیاری به مخاطبان انتخاب شده اضافه کنید، سپس روی “اشتراک گذاری انجمن” ضربه بزنید. مخاطبان یک پیام که آن ها را دعوت به عضویت می کنند را خواهند دید.پیام های در انجمن به ریسه ها دسته بندی شده اند. عر عضو می تواند به یک پیام پاسخ دهد یا یک ریسه جدید را شروع کند.هر عضو یک انجمن می تواند اعضا جدید دعوت کند، و عر عضو که این شامل ایجاد کننده هم می شود می تواند انجمن را ترک کند. انجمن به وجود خود ادامه خواهد داد تا آخرین عضو آن را ترک کند.حساب کاربری Briar شما داخل خود یک وبلاگ دارد. شما میتوانید از آن برای پست کردن اخبار یا به روز رسانی درباره زندگی خود استفاده کنید. وبلاگ شما به صورت خودکار با مخاطبانتان به اشتراک گذاشته خواهد شد، و وبلاگ های آن ها هم با شما به اشتراک گذاشته می شود.برای خواندن وبلاگ های مخاطبان و یا نوشتن یک پست، منو اصلی را باز کرده و “وبلاگ ها” را انتخاب کنید.برای نوشتن یک پست، روی نگارک قلم در بالای خوراک وبلاگ ضربه بزنید.پست خود را نوشته و روی “انتشار” ضربه بزنید.پست جدید شما در خوراک وبلاگ پدیدار خواهد شد.برای ریبلاگ یک پست، روی نگارک ریبلاگ در گوشه پست ضربه بزنید.یک کامنت اختیاری اضافه کنید و روی “ریبلاگ” ضربه بزنید.پست ریبلاگ شده در خوراک وبلاگ پدیدار خواهد شد و نظر شما به آن پیوست شده است.شما می توانید از Briar برای خواندن هر وبلاگ یا سایت خبری که یک خوراک RSS منتشر می‌کند استفاده کنید. مقالات برای محافظت از حریم خصوصی شما  از طریق تور دانلود خواهند شد. شما می توانید مقالات خوراک RSS را مشابه پست‌های وبلاگ ریبلاگ کرده و روی آنها نظر بگذارید. RSS یک روش برای وبسایت ها می باشد تا به وسیله آن مقالات را در یک شکلی که انتشار دوباره آن ها آسان است انتشار دهد.برای وارد کردن یک خوراک RSS، خوراک وبلاگ را باز کرده و “وارد کردن خوراک RSS” را از منو انتخاب کنید.آدرس اینترنتی برای خوراک RSS را وارد کرده و روی “وارد کردن” ضربه بزنید. مقالات دانلود خواهند شد و به خوراک وبلاگ اضافه خواهند شد. این ممکن است چند دقیقه طول بکشد.زمانی که مقالات تازه منتشر می شوند، Briar آن ها را به صورت خودکار دانلود خواهد کرد. مقالات وارد شده با مخاطبان شما به اشتراک گذاشته نمی شوند مگر اینکه شما آن ها را بلاگ‌مجدد کنید.برای مدیریت خوراک های RSS خود،  خوراک وبلاگ را باز کرده و"مدیریت خوراک های RSS" را از منو انتخاب کنید.برای حذف یک خوراک روی نگارک زباله کلیک کنید. مقالات وارد شده از خوراک بلاگ حذف خواهند شد، به جز هر مقاله ای که شما ریبلاگ کرده اید.برای حذف یک مخاطب، روی نام مخاطب در فهرست مخاطبان کلیک کرده و “حذف مخاطب” را از منو انتخاب کنید. برای محافظت از حریم خصوصی خود، مخاطب از اینکه شما آن را پاک کرده اید آگاه نخواهد شد. آن ها از الان به بعد شما را به صورت آفلاین خواهند دید.برای پیدا کردن تنظیمات، منو اصلی را باز کنید و “تنظیمات” را انتخاب کنید.
شما اینجا میتوانید تجربه Briar خود را شخصی سازی کنید.شما میتوانید طرح رنگی که Briar از آن استفاده می کند را تغییر دهید: Briar از رنگ های روشن استفاده خواهد کرد. Briar از رنگ های تیره استفاده خواهد کرد. Briar طرح رنگ خود را بر اساس زمان روز تغییر خواهد داد. Briar از طرح رنگ سیستم استفاده خواهد کرد.اتصال از طریق اینترنت (تور) Briar از تور برای اتصال به اینترنت استفاده می کند. تور یک شبکه از کامپیوتر هایی است که توسط داوطلبان در سرتاسر دنیا برای کمک به افراد برای دسترسی به اینترنت به صورت خصوصی و بدون سانسور ایجاد شده است. “پل ها” کامپیوتر هایی هستند که میتوانند به شما در اتصال به تور کمک کنند اگر حکومت شما یا تامین کننده اینترنت شما آن را مسدود کرده است.شما می توانید نحوه اتصال Briar به اینترنت را کنترل کنید: Briar نحوه اتصال را بر اساس موقعیت فعلی شما انتخاب خواهد کرد.استفاده از تور بدون پل ها: Briar بدون استفاده از پل ها به تور وصل خواهد شد. Briar از پل ها برای اتصال به تور استفاده خواهد کرد. Briar به اینترنت وصل نخواهد شد.شما میتوانید استفاده Briar از داده موبایل را کنترل کنید. اگر داده موبایل را خاموش کنید، Briar فقط از اینترنت زمانی که به وای-فای وصل هستید استفاده خواهد کرد.اتصال از طریق اینترنت (تور) فقط در هنگام شارژ شدنشما میتوانید اتصال Briar زمانی که دستگاه شما در حال استفاده از باتری است را کنترل کنید. اگر این تنظیم را روشن کنید، Briar فقط از اینترنت زمانی که دستگاه شما به برق متصل است استفاده خواهد کرد. این ویژگی در نسخه 4 اندروید موجود نمی باشد.برای محافظت از حریم خصوصی خود زمانی که سایر افراد از دستگاه شما استفاده می کنند، شما میتوانید Briar را بدون خارج شدن قفل کنید. این از استفاده شدن از Briar قبل از اینکه یک پین، الگو یا گذرواژه وارد کنید جلوگیری می کند.Briar از همان پین، الگو یا گذرواژه ای که شما معمولا برای باز کردن قفل دستگاه خود استفاده میکنید استفاده میکند، بنابراین اگر یک پین، الگو، گذرواژه انتخاب نکرده اید این تنظیم غیرفعال خواهد شد (خاکستری). شما میتوانید از برنامه تنظیمات دستگاه خود یکی را انتخاب کنید.زمانی که تنظیم قفل صفحه فعال شود، یک گزینه “قفل برنامه” به منو اصلی Briar اضافه خواهد شد. شما میتوانید از این گزینه برای قفل کردن Briar بدون خروج استفاده کنید.زمانی که Briar قفل است، از شما پین، الگو یا گذرواژه برای باز کردن قفل خواسته خواهد شد.اتمام وقت عدم فعالیت قفل صفحه این ویژگی در نسخه 4 اندروید موجود نمی باشد.شما میتوانید قفل شدن Briar به صورت خودکار را هنگامی که برای مدت مشخصی مورد استفاده قرار نگرفته است تنظیم کنید.]]></content:encoded></item><item><title>Data is the only moat</title><link>https://frontierai.substack.com/p/data-is-your-only-moat</link><author>cgwu</author><category>hn</category><pubDate>Thu, 15 Jan 2026 18:54:32 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>JuiceFS is a distributed POSIX file system built on top of Redis and S3</title><link>https://github.com/juicedata/juicefs</link><author>tosh</author><category>hn</category><pubDate>Thu, 15 Jan 2026 18:45:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>‘ELITE’: The Palantir app ICE uses to find neighborhoods to raid</title><link>https://werd.io/elite-the-palantir-app-ice-uses-to-find-neighborhoods-to-raid/</link><author>sdoering</author><category>hn</category><pubDate>Thu, 15 Jan 2026 18:42:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[This is racial profiling on a grand scale:“Palantir is working on a tool for Immigration and Customs Enforcement (ICE) that populates a map with potential deportation targets, brings up a dossier on each person, and provides a “confidence score” on the person’s current address, 404 Media has learned. ICE is using it to find locations where lots of people it might detain could be based.”It apparently looks a lot like Google Maps, but designed to show the richness of an area for “targets”, populated in part by density of immigrants. And then you can dig in:“Once a person is selected on the map interface, ELITE then shows a dossier on that particular person, according to the user guide. That includes their name, a photo, their Alien Number (the unique code given by the U.S. government to each immigrant), their date of birth, and their full address.”The Nazis could only dream of having such a capability.Imagine working for this company, on this product. Every day, you go into work, in what I assume is a beautiful office with pine furniture and a well-stocked kitchen, and you build software that will help to deport people using what you know are extrajudicial means without due process. You probably have OKRs. There are customer calls with ICE. Every two-week sprint, you take on tasks that help make this engine better.What do you tell yourself? What do you tell your family?Are you on board with this agenda, or do you tell yourself you need the job to pay rent? To get healthcare?You receive stock as part of your pay package. It’s going up! You can use it to buy a home, or to build a comfortable retirement, or some combination of the two.Your co-workers are values aligned and work hard. They’re talented and smart.  you might think to yourself, I love working with this team.Or, you might think, man, I’ve got to find another job.Either way, you’re proud of your product work. You’re happy to take the salary, the free lunches, the espresso. And regardless of how you feel about it, the thing you do every day is powering an armed force that is kidnapping people on the street and shooting civilians, that shot a mother in the face, that is targeting people to disappear using a beautiful, modern map interface.]]></content:encoded></item><item><title>Ask HN: How can we solve the loneliness epidemic?</title><link>https://news.ycombinator.com/item?id=46635345</link><author>publicdebates</author><category>hn</category><pubDate>Thu, 15 Jan 2026 16:49:13 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Countless voiceless people sit alone every day and have no one to talk to, people of all ages, who don't feel that they can join any local groups. So they sit on social media all day when they're not at work or school. How can we solve this?]]></content:encoded></item><item><title>LLM Structured Outputs Handbook</title><link>https://nanonets.com/cookbooks/structured-llm-outputs</link><author>vitaelabitur</author><category>hn</category><pubDate>Thu, 15 Jan 2026 16:46:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[LLMs mostly produce syntactically valid outputs when we try generating JSON, XML, code, etc., but they can occasionally fail due to their probabilistic nature. This is a problem for developers as we use LLMs programmatically, for tasks like data extraction, code generation, tool calling, etc.There are many deterministic ways to ensure structured LLM outputs. If you are a developer, this handbook covers everything you need.What happens under-the-hood?What are the best tools & techniques?How to pick the right tools & techniques?How to build, deploy, and scale systems?How to optimize for latency and cost?How to improve the quality of output?Structured generation is moving too fast. Most resources you find today are already outdated. You have to dig through multiple academic papers, blogs, GitHub repos, and other resources.This handbook brings it all together in a living document that updates regularly.You can read it start-to-finish, or treat it like a lookup table.We're the maintainers of Nanonets-OCR models (VLMs to convert documents into clean, structured Markdown) and docstrange (open-source document processing library).]]></content:encoded></item><item><title>Apple is fighting for TSMC capacity as Nvidia takes center stage</title><link>https://www.culpium.com/p/exclusiveapple-is-fighting-for-tsmc</link><author>speckx</author><category>hn</category><pubDate>Thu, 15 Jan 2026 15:02:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When CC Wei visited Cupertino last August, he had bad news for his largest client. Apple would need to acquiesce to the largest price rise in years, TSMC’s CEO told its executives.Tim Cook and his team took the news on the chin. Wei had been telegraphing hikes in earnings calls over the past few quarters, and the Taiwanese chip maker’s rising gross margins were testament to its increasing pricing power. That wasn’t the worst news, my sources tell me.Apple, which once held a dominant position on TSMC’s customer list, now needs to fight for production capacity. With the continuing AI boom, and each GPU from clients like Nvidia and AMD taking up a larger footprint per wafer, the iPhone maker’s chip designs are no longer guaranteed a place among TSMC’s almost two dozen fabs.What Wei probably didn’t tell Cook is that Apple may no longer be his largest client. Public data helps tells the story.Apple’s role as the primary driver of TSMC revenue growth ended five years ago. In 2018 TSMC sales would have even fallen if not for incremental purchases by Apple that year. Now, the Cupertino company is posting low single-digit revenue growth while Nvidia is skyrocketing.The reason for this change is two-fold, and pretty obvious: AI is driving massive demand for high-powered chips, while the smartphone boom has plateaued. Revenue in 2026 will rise close to 30%, yet capital expenditure will climb around 32% to a record of somewhere between $52 billion and $56 billion, TSMC said Thursday. Longer term, growth will average 25% in the five years through 2029 yet the AI segment will climb an average of 55% or more over the same period, the company said. That’s higher than a prior forecast for a mid-40 percent figure.The ultimate flex for TSMC came Thursday when it showed off not only record revenue and net income, but a gross margin approaching that of software makers and fabless chip designers. In the December quarter, that figure was an astounding 62.3%, 280 basis points higher than the prior period. If not for its overseas fabs (Arizona and Japan) gross margin would have been even higher.There are two caveats that are important. First, while smartphone processors are the largest portion of chips bought by Apple, they’re not the only type. Processors for Macs come under HPC, while it also has a strong lineup of custom chips used in accessories which fall under digital consumer electronics. Second, Nvidia isn’t the only HPC client. AMD is a major buyer of capacity for its own GPUs while Amazon and Google are on the growing list of customers developing in-house AI chips.Put another way, Apple’s chip catalog is broader and more varied, while Nvidia’s lineup is more concentrated around a huge number of wafers at, or near, leading-edge. It’s for these reasons that Apple will remain important for at least another decade.In the near-term, however, TSMC’s technology roadmap coupled with broader industry trends favor Nvidia, AMD and their ilk, meaning Apple may need to keep fighting for capacity over the next year or two. TSMC is already producing chips in volume at 2 nanometer (called N2), currently its most advanced node, with Apple a major buyer. But in the second half of this year it’s set to ramp up both a new variant called N2P as well as a new node called A16. In TSMC CEO CC Wei’s words A16, with Super Power Rail, is “best for HPC with complex signal routes.” SPR is TSMC’s version of backside power, a newer approach designed to separate a chip’s signal from its power supply. Intel is also developing this technology, and many believe it’ll be the key to the US company’s prospects at stealing foundry share from its Taiwan rival.a fabulous report on the TSMC-Apple relationship,More importantly, what Apple offers is stability. Nvidia has been a client for a lot longer than Apple, but broadly speaking it’s a bit niche. Right now that “niche” is the hottest product on the planet, but niche it is. Apple, on the other hand, has products being made in no fewer than a dozen TSMC fabs. Even if Nvidia did overtake Apple by purchases, the breadth of its manufacturing footprint at TSMC is nowhere near as large.This distinction may not matter now, but it probably will at some point. The AI boom won’t last forever. The bubble may burst, or it may slowly deflate, but the growth trajectory will surely flatten and that means demand for leading-edge AI chips will fall. Wei knows this, which is why he’s expanding both quickly yet cautiously. “I am also very nervous,” he said at the company’s investor conference on Thursday in Taipei. “If we didn’t do it carefully, it would be a big disaster for TSMC for sure.”The chip giant has recently come under fire, including from noted analyst Benedict Evans, for being “unwilling/unable to expand capacity fast enough to meet Nvidia’s book.” I think this is wrong, and unfair. Evans citedAlphabet’s capital intensity, calculated as acquisitions of property, plant & equipment divided by revenue, was just 15% for full-year 2024. TSMC’s is more than double that at over 33%. More importantly, depreciation — which is where the cost of capex is reflected in earnings — was just 10% of Alphabet’s cost of revenue. For TSMC, this figure is more than four times higher at 45%. At Nvidia, which is a tier-one buyer of TSMC’s output, the data is more stark. Capital intensity was just 2.5% for 2024, while depreciation was only 5.7% of the cost of revenue. As a fabless chipmaker, it can enjoy gross margins of over 70%. Its only real risk is holding excess inventory. Even then, it could have written off its entire inventory at the end of October and still maintain a gross margin approaching that of its chief supplier. What’s more, neither of these clients have anywhere near the customer-concentration risk of TSMC.The complaint that TSMC could and should build faster ignores the fact that it’s the one left holding the baby if a downturn comes and demand falls. It takes two to three years to build a new fab, Wei explained, so the company must skate where the puck is going without thinking too much about where it’s been. “Even if we spend 52 to 56 billion this year, the contribution this year is none,“ Wei said Thursday. Its major cost, buying equipment, remains on the books no matter what revenue it brings in for the quarter. For the best part of a decade, Apple was the one driving TSMC’s need to keep spending on new facilities. Today it’s Nvidia, and Jensen Huang is starting to wield more power than Tim Cook. But neither has to bother with the expensive business of actually manufacturing semiconductors, merely the hassle of begging CC Wei for wafers.Do share! Show your network what you’re reading. (It helps the algorithm)]]></content:encoded></item><item><title>The Palantir app helping ICE raids in Minneapolis</title><link>https://www.404media.co/elite-the-palantir-app-ice-uses-to-find-neighborhoods-to-raid/</link><author>fajmccain</author><category>hn</category><pubDate>Thu, 15 Jan 2026 14:54:51 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Internal ICE material and testimony from an official obtained by 404 Media provides the clearest link yet between the technological infrastructure Palantir is building for ICE and the agency’s activities on the ground.]]></content:encoded></item><item><title>Show HN: TinyCity – A tiny city SIM for MicroPython (Thumby micro console)</title><link>https://github.com/chrisdiana/TinyCity</link><author>inflam52</author><category>hn</category><pubDate>Thu, 15 Jan 2026 14:11:30 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>25 Years of Wikipedia</title><link>https://wikipedia25.org/</link><author>easton</author><category>hn</category><pubDate>Thu, 15 Jan 2026 13:17:07 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Photos capture the breathtaking scale of China&apos;s wind and solar buildout</title><link>https://e360.yale.edu/digest/china-renewable-photo-essay</link><author>mrtksn</author><category>hn</category><pubDate>Thu, 15 Jan 2026 09:54:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Raspberry Pi&apos;s New AI Hat Adds 8GB of RAM for Local LLMs</title><link>https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/</link><author>ingve</author><category>hn</category><pubDate>Thu, 15 Jan 2026 08:23:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[With that, the Hailo 10H is capable of running LLMs entirely standalone, freeing the Pi's CPU and system RAM for other tasks. The chip runs at a maximum of 3W, with 40 TOPS of INT8 NPU inference performance in addition to the equivalent 26 TOPS INT4 machine vision performance on the earlier AI HAT with Hailo 8.In practice, it's not as amazing as it sounds.You still can't upgrade the RAM on the Pi, but at least this way if you  have a need for an AI coprocessor, you don't have to eat up the Pi's memory to run things on it.And it's a lot cheaper and more compact than running an eGPU on a Pi. In that sense, it's more useful than the silly NPUs Microsoft forces into their 'AI PCs'.But it's still a solution in search of a problem, in all but the most niche of use cases.Besides feeling like I'm living in the world of the Turbo Encabulator every time I'm testing AI hardware, I find the marketing of these things to be very vague, and the applications not very broad.That's certainly not a worthless use case, but it's not something I've ever needed to do. I have a feeling this board is meant more for development, for people who want to deploy the 10H in other devices, rather than as a total solution to problems individual Pi owners need to solve.Especially when it comes to the headline feature: running inference, like with LLMs.I also published a video with all the information in this blog post, but if you enjoy text more than video, scroll on past—it doesn't offend me!I ran everything on an 8 gig Pi 5, so I could get an apples-to-apples comparison, running the same models on the Pi's CPU as I did on the AI HAT's NPU.They both have the same 8GB LPDDR4X RAM configuration, so , they'd have similar performance.I tested every model Hailo put out so far, and compared them, Pi 5 versus Hailo 10H:The Pi's built-in CPU trounces the Hailo 10H.The Hailo is only close, really, on Qwen2.5 Coder 1.5B.It  slightly more efficient in most cases:But looking more closely at power draw, we can see why the Hailo doesn't keep up:The Pi's CPU is allowed to max out it's power limits (10W on the SoC), which are a lot higher than the Hailo's (3W).So power holds it back, but the 8 gigs of RAM holds back the LLM use case (vs just running on the Pi's CPU) the most. The Pi 5 can be bought in up to a  configuration. That's as much as you get in decent consumer graphics cards.Because of that, many quantized medium-size models target 10-12 GB of RAM usage (leaving space for context, which eats up another 2+ GB of RAM).A little bit of quality is lost, but like a JPEG, it's still good enough to ace all the contrived tests (like building a TODO list app, or sorting a complex list) that the tiny models I ran on the Hailo 10H didn't complete well (see the video earlier in this post for details).I asked it to generate a single page TODO list app, and it's still not a speed demon (this is a Pi CPU with LPDDR4x RAM we're talking about), but after a little while, it gave me this:It met all my requirements:I can type in as many items as I wantI can drag them around to rearrange themI can check off items and they go to the bottom of the list...It's honestly crazy how many small tasks you can do even with free local models... even on a Pi. Natural Language Programming was just a dream back when I started my career.But I don't think this HAT is the best choice to run local, private LLMs (at least not as a primary goal).What it  good for, is vision processing. But the original AI HAT was good for that too!In my testing, Hailo's hailo-rpi5-examples were not yet updated for this new HAT, and even if I specified the Hailo 10H manually, model files would not load, or I ran into errors once the board was detected.But Raspberry Pi's models ran, so I tested them with a Camera Module 3:I pointed it over at my desk, and it was able to pick out things like my keyboard, my monitor (which it thought was a TV), my phone, and even the mouse tucked away in the back.It all ran quite fast—and 10x faster than on the Pi's CPU—but the problem is I can do the same thing with the  AI HAT ($110)—or the AI Camera ($70).If you  need vision processing, I would stick with one of those.The headline feature of the AI HAT+ 2 is the ability to run in a 'mixed' mode, where it can process machine vision (frames from a camera or video feed), while also running inference (like an LLM or text-to-speech).Unfortunately, when I tried running two models simultaneously, I ran into segmentation faults or 'device not ready', and lacking any working examples from Hailo, I had to give up on getting that working in time for this post.Just like the original AI HAT, there's some growing pains.It seems like with most hardware with "AI" in the name, it's hardware-first, then software comes later—if it comes at all. At least with Raspberry Pi's track record, the software  come, it's just... often the solutions are only useful in tiny niche use cases.8 GB of RAM is useful, but it's not quite enough to give this HAT an advantage over just paying for the bigger 16GB Pi with more RAM, which will be more flexible and run models faster.The main use case for this HAT might be in power-constrained applications where you need both vision processing  inferencing. But even there... it's hard to say "yes, buy this thing", because for just a few more watts, the Pi could achieve better performance for inference in tandem with the $70 AI Camera or the $110 AI HAT+ for the vision processing.Outside of running tiny LLMs in less than 10 watts, maybe the idea is you use the AI HAT+ 2 as a development kit for designing devices using the 10H like self-checkout scanners (which might not even run on a Pi)? I'm not sure.]]></content:encoded></item><item><title>Have Taken Up Farming</title><link>https://dylan.gr/1768295794</link><author>djnaraps</author><category>hn</category><pubDate>Thu, 15 Jan 2026 08:12:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Disclaimer: These are my personal views and do not represent any organization or professional advice.Tue, 13 Jan 2026 11:16:34 +0200My name is Dylan Araps and I used to be a software engineer, best known for my open source work (Neofetch, Pywal, KISS Linux, Pure Bash Bible) [0]. In 2021, without explanation and without telling anyone, I vanished from the internet. My usage of the internet became strictly "read only".In 2024, I appeared briefly to tell the world I retired and had "taken up farming" [1]. The vagueness of my message and strangeness surrounding its circumstances created a lot of buzz. It is not every day a person in a cushy, intellectual career drops everything and pivots to working outdoors with their body. It was amusing to read the theories people came up with: from driving a tractor up and down massive tracts of land to being holed up in a Kaczynskiesque cabin in the woods.Today, I return to the internet to tell my story and announce https://WILD.gr.For nearly a decade I spent the majority of my waking hours sedentary and staring at a screen. Praised for my work ethic, quick response to messages and sheer number of projects, I was seen as a wizard. People loved what I was doing and I in turn wanted to please them. Chasing attention led to my projects becoming more complex and bigger in scope over time. It became increasingly difficult to maintain them and keep up the wizard persona.This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: "What am I doing with my life?". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eating . I ate figs for the first time and then proceeded to make up for all the years spent  eating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, "Natural Farmer". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said "have taken up farming" [1]. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome to https://WILD.gr.]]></content:encoded></item><item><title>To those who fired or didn&apos;t hire tech writers because of AI</title><link>https://passo.uno/letter-those-who-fired-tech-writers-ai/</link><author>theletterf</author><category>hn</category><pubDate>Thu, 15 Jan 2026 07:58:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Yes, you, who are thinking about not hiring a technical writer this year or, worse, erased one or more technical writing positions last year . You, who are buying into the promise of docs entirely authored by LLMs without expert oversight or guidance. You, who unloaded the weight of docs on your devs’ shoulders, as if it was a trivial chore.You are making a big mistake. But you can still undo the damage.It’s been a complicated year, 2025. When even Andrej Karpathy, one of OpenAI’s founders, admits, in a fit of Oppenheimerian guilt, to feeling lost, you know that no one holds the key to the future. You flail and dance around these new totems made of words, which are neither intelligent nor conscious, pretending they can replace humans while, in fact, they’re little more than glorified tools.You might think that the plausible taste of AI prose is all you need to give your products a . You paste code into a field and something that resembles docs comes out after a few minutes. Like a student eager to turn homework in, you might be tempted to content yourself with docs theatre, thinking that it’ll earn you a good grade. It won’t, because docs aren’t just artifacts.You keep using that word. I do not think it means what you think it meansWhen you say “docs”, you’re careful to focus on the output, omitting the process. Perhaps you don’t know how docs are produced. You’ve forgotten, or perhaps never knew, that docs are product truth; that without them, software becomes unusable, because software is never done, is never obvious, and is never simple. Producing those docs requires tech writers.Tech writers go to great lengths to get the information they need. They write so that your audience can understand. They hunger for clarity and meaning and impact. They power through weeks full of deadlines, chasing product news, because without their reporting, most products wouldn’t thrive; some wouldn’t even exist. Their docs aren’t a byproduct: they tie the product together.An LLM can’t do all that, because it can’t feel the pain of your users. It can’t put itself into their shoes. It lacks the kind of empathy that’s behind great help content. It does not, in fact, have any empathy at all, because it cannot care. You need folks who will care, because content is a hairy beast that can only be tamed by agents made of flesh and capable of emotions: humans.AI generated docs are brokenYou can’t generate docs on autopilot. Let me tell you why.First, AI-generated docs are not intelligent. They not only make up things in subtle ways: They lack vision. Even if you fed them millions of tokens, they couldn’t develop a docs strategy, decide what not to document, or structure content for reuse. And they fail to capture the tension, the caveats, the edge cases, the feeling of unfinishedness that only someone who cares can feel. Without that grounding, docs are hollow.Second, liability doesn’t vanish just because AI wrote it. When docs cause harm through wrong instructions, someone will be held responsible. It won’t be the model. You can’t depose an LLM. You can’t fire it. You can’t point at it in court when a customer’s data evaporates because your GenAI runbook told them to run the wrong command. That someone will be , or someone who reports to you.Third, even your favorite AI must RTFM. All your Claude Skills, Cursor rules, all the semantic tagging that makes RAG work, is technical writing under a new name: context curation. You fired or didn’t hire the people who create high-quality context and then wondered why your AI tools produce slop. You can’t augment what isn’t there. The writers you let go were the supply chain for the intelligence you’re now betting on.The solution is to augment your technical writersIt’s not all bad news: Marvelous things can happen if you provide your writers with AI tools and training while you protect the quality of your content through an AI policy. I’ve described the ideal end state in My day as an augmented technical writer in 2030, a vision of the future where writers orchestrate, edit, and publish docs together with AI agents. This is already happening before our eyes.Productivity gains are real when you understand that augmentation is better than replacing humans, a reality even AWS’ CEO, Matt Garman, acknowledged. Read how I’m using AI as a technical writer. I’m not alone: Follow Tom Johnson, CT Smith, and Sarah Deaton, and discover how tech writers are building tools through AI to better apply it to docs.Develop an AI strategy for docs together with tech writers, and give them time and resources to experiment with AI. Tech writers are resourceful by nature: they’ve spent careers doing more with less, optimizing workflows, finding clever solutions to impossible quests. Give them the tools and a bit of runway, and they’ll figure out how to make AI work for the docs, not instead of them.So here’s my request for you: ReconsiderReconsider the positions you did not open. Or the writers you let go. Reconsider the assumption that AI has solved a problem that, at its core, is deeply human and requires not only concatenating words, but also chasing subject-matter experts and understanding the subtleties of product motions, among many other things.Technical writers aren’t a luxury. They are the people who translate what you’ve built into something others can use. Without them, you’re shipping a product that can’t speak for itself, or that lies. Your product needs to speak. AI can generate noise effectively and infinitely, but only a technical writer can create the signal.Don’t choose the noise. Get them back. Get them onboard.Thanks to Tiffany Hrabusa, Casey Smith, and Anna Urbiztondo for their reviews of early drafts and for their encouragement. Thanks to my partner, Valentina, for helping me improve this piece and for suggesting to wait a bit before hitting Publish. And a heartfelt thank you to the tech writing community and its wonderful human beings.]]></content:encoded></item><item><title>Handy – Free open source speech-to-text app</title><link>https://github.com/cjpais/Handy</link><author>tin7in</author><category>hn</category><pubDate>Thu, 15 Jan 2026 05:23:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pocket TTS: A high quality TTS that gives your CPU a voice</title><link>https://kyutai.org/blog/2026-01-13-pocket-tts</link><author>pain_perdu</author><category>hn</category><pubDate>Thu, 15 Jan 2026 05:14:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The URL shortener that makes your links look as suspicious as possible</title><link>https://creepylink.com/</link><author>dreadsword</author><category>hn</category><pubDate>Thu, 15 Jan 2026 03:28:20 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Furiosa: 3.5x efficiency over H100s</title><link>https://furiosa.ai/blog/introducing-rngd-server-efficient-ai-inference-at-data-center-scale</link><author>written-beyond</author><category>hn</category><pubDate>Thu, 15 Jan 2026 00:53:21 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Making rapid deployment of advanced AI available to everyoneWith global data center demand at 60 GW in 2024 and expected to triple by the end of the decade, the industry faces a once-in-a-generation transformation. More than 80 percent of facilities today are air-cooled and operate at 8 kW per rack or less, making them poorly suited for GPU-based systems that require liquid cooling and 10 kW+ per server.NXT RNGD Server provides a practical path forward. It allows organizations to deploy advanced AI within their existing facilities, without prohibitive energy costs or disruptive retrofits. Engineered as a plug-and-play system, NXT RNGD combines AI-optimized silicon with Furiosa LLM, a vLLM-compatible serving framework featuring built-in OpenAI API support, enabling organizations to deploy and scale AI workloads from day one. By combining silicon and system design, NXT RNGD Server makes efficient, enterprise-ready, and future-proof AI infrastructure a reality.We are taking inquiries and orders for January 2026.Download the datasheet here and sign up for RNGD updates here.]]></content:encoded></item><item><title>Crafting Interpreters</title><link>https://craftinginterpreters.com/</link><author>tosh</author><category>hn</category><pubDate>Wed, 14 Jan 2026 22:26:17 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ contains everything you need to implement a
full-featured, efficient scripting language. You’ll learn both high-level
concepts around parsing and semantics and gritty details like bytecode
representation and garbage collection. Your brain will light up with new ideas,
and your hands will get dirty and calloused. It’s a blast.Starting from , you build a language that features rich
syntax, dynamic typing, garbage collection, lexical scope, first-class
functions, closures, classes, and inheritance. All packed into a few thousand
lines of clean, fast code that you thoroughly understand because you write each
one yourself.The book is available in four delectable formats:]]></content:encoded></item><item><title>Training my smartwatch to track intelligence</title><link>https://dmvaldman.github.io/rooklift/</link><author>dmvaldman</author><category>hn</category><pubDate>Wed, 14 Jan 2026 22:19:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Scaling long-running autonomous coding</title><link>https://cursor.com/blog/scaling-agents</link><author>samwillis</author><category>hn</category><pubDate>Wed, 14 Jan 2026 22:18:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[We've been experimenting with running coding agents autonomously for weeks.Our goal is to understand how far we can push the frontier of agentic coding for projects that typically take human teams months to complete.This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.The limits of a single agentToday's agents work well for focused tasks, but are slow for complex projects. The natural next step is to run multiple agents in parallel, but figuring out how to coordinate them is challenging.Our first instinct was that planning ahead would be too rigid. The path through a large project is ambiguous, and the right division of work isn't obvious at the start. We began with dynamic coordination, where agents decide what to do based on what others are currently doing.Our initial approach gave agents equal status and let them self-coordinate through a shared file. Each agent would check what others were doing, claim a task, and update its status. To prevent two agents from grabbing the same task, we used a locking mechanism.This failed in interesting ways:Agents would hold locks for too long, or forget to release them entirely. Even when locking worked correctly, it became a bottleneck. Twenty agents would slow down to the effective throughput of two or three, with most time spent waiting.The system was brittle: agents could fail while holding locks, try to acquire locks they already held, or update the coordination file without acquiring the lock at all.We tried replacing locks with optimistic concurrency control. Agents could read state freely, but writes would fail if the state had changed since they last read it. This was simpler and more robust, but there were still deeper problems.With no hierarchy, agents became risk-averse. They avoided difficult tasks and made small, safe changes instead. No agent took responsibility for hard problems or end-to-end implementation. This lead to work churning for long periods of time without progress.Our next approach was to separate roles. Instead of a flat structure where every agent does everything, we created a pipeline with distinct responsibilities. continuously explore the codebase and create tasks. They can spawn sub-planners for specific areas, making planning itself parallel and recursive. pick up tasks and focus entirely on completing them. They don't coordinate with other workers or worry about the big picture. They just grind on their assigned task until it's done, then push their changes.At the end of each cycle, a judge agent determined whether to continue, then the next iteration would start fresh. This solved most of our coordination problems and let us scale to very large projects without any single agent getting tunnel vision.To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.Despite the codebase size, new agents can still understand it and make meaningful progress. Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts.While it might seem like a simple screenshot, building a browser from scratch is extremely difficult.Another experiment was doing an in-place migration of Solid to React in the Cursor codebase. It took over 3 weeks with +266K/-193K edits. It still needs careful review, but was passing our CI and early checks.Another experiment was to improve an upcoming product. A long-running agent made video rendering 25x faster with an efficient Rust version. It also added support to zoom and pan smoothly with natural spring transitions and motion blurs, following the cursor. This code was merged and will be in production soon.We have a few other interesting examples still running:We've deployed billions of tokens across these agents toward a single goal. The system isn't perfectly efficient, but it's far more effective than we expected.Model choice matters for extremely long-running tasks. We found that GPT-5.2 models are much better at extended autonomous work: following instructions, keeping focus, avoiding drift, and implementing things precisely and completely.Opus 4.5 tends to stop earlier and take shortcuts when convenient, yielding back control quickly. We also found that different models excel at different roles. GPT-5.2 is a better planner than GPT-5.1-codex, even though the latter is trained specifically for coding. We now use the model best suited for each role rather than one universal model.Many of our improvements came from removing complexity rather than adding it. We initially built an integrator role for quality control and conflict resolution, but found it created more bottlenecks than it solved. Workers were already capable of handling conflicts themselves.The best system is often simpler than you'd expect. We initially tried to model systems from distributed computing and organizational design. However, not all of them work for agents.The right amount of structure is somewhere in the middle. Too little structure and agents conflict, duplicate work, and drift. Too much structure creates fragility.A surprising amount of the system's behavior comes down to how we prompt the agents. Getting them to coordinate well, avoid pathological behaviors, and maintain focus over long periods required extensive experimentation. The harness and models matter, but the prompts matter more.Multi-agent coordination remains a hard problem. Our current system works, but we're nowhere near optimal. Planners should wake up when their tasks complete to plan the next step. Agents occasionally run for far too long. We still need periodic fresh starts to combat drift and tunnel vision.But the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected. Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.The techniques we're developing here will eventually inform Cursor's agent capabilities. If you're interested in working on the hardest problems in AI-assisted software development, we'd love to hear from you at hiring@cursor.com.]]></content:encoded></item><item><title>The State of OpenSSL for pyca/cryptography</title><link>https://cryptography.io/en/latest/statements/state-of-openssl/</link><author>SGran</author><category>hn</category><pubDate>Wed, 14 Jan 2026 22:04:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Influentists: AI hype without proof</title><link>https://carette.xyz/posts/influentists/</link><author>LucidLynx</author><category>hn</category><pubDate>Wed, 14 Jan 2026 20:54:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Last week, the developer community was busy discussing about a single tweet:The author is Jaana Dogan (known as Rakyll), a highly respected figure in the Google ecosystem, in the open-source world, and in my heart (thank you Rakyll for your great Go blog posts).At first glance, the tweet suggests an enormous shift in the software industry: the ability to build  what previously required weeks or months for a team of sofware engineers, using  the description of the problem. The tweet was too-much dramatic in my own opinion, but actually impressive!The post triggered an immediate wave of “doom-posting,” with many fearing for the future of software engineering (as each week since a year now).
However, as the conversation reached a high number of replies and citations on social networks, Rakyll released a follow-up thread to provide context:This response thread revealed a story  than the original tweet suggested.
Let’s analyze it.Crucially, the foundational “thinking” had already been performed by Rakyll herself, who guided the AI using architectural concepts (honed over several weeks or months of prior effort) rather than the AI thinking and inventing the “product” from scratch.
Furthermore, the resulting project was strictly a proof-of-concept that falls far short of a production-ready system capable of managing real-world complexity.
And finally, this success hinged on the Rakyll’s implicit domain knowledge and deep expertise. The last point is often (strategically?) omitted from these “magic” viral demonstrations in order to make the tool appear way more autonomous than it truly is.Hmm. Now, this is far less exciting…This pattern of “hype first and context later” is actually part of a growing trend.I call the individuals participating to that trend “”.
Those people are members of a scientific or technical community, and leverage their large audiences to propagate claims that are, at best, unproven and, at worst,  misleading.But how can we spot them?I personally identify these “Influentists” by four personality traits that characterize their public discourse.
The first is a reliance on , where anecdotal experiences are framed as universal, objective truths to generate hype. This is a sentiment perfectly captured by the “I’m not joking and this isn’t funny” tone of Rakyll’s original tweet, but also the dramatic “I’ve never felt that much behind as a programmer” from Andrej Karpathy’s tweet.
This is supported by an absence of reproducible proof, as these individuals rarely share the code, data, or methodology behind their viral “wins”, an omission made easier than ever in the current LLM era.
And finally, they utilize , carefully wording their claims with enough vagueness to pivot toward a “clarification” if the technical community challenges their accuracy.Rakyll is far from alone. We see this “hype-first” approach across major AI firms like Anthropic, OpenAI, or Microsoft.Consider Galen Hunt, a Distinguished Engineer at Microsoft. He recently made waves by claiming a goal to rewrite Microsoft’s massive C/C++ codebases into Rust by 2030 using AI.When the industry pointed out the near-impossible complexity of this task, but also asking clarity for popular and critical products like Microsoft Windows, he was forced to clarify that it was only a “research project”.Similarly, engineers from Anthropic and OpenAI oftenly post teasers about “AGI being achieved internally” to release months later models that disappoint the crowd.Similarly, many other companies lie over what they are solving or willing to solve:
  The Cost of Unchecked Influence
  #When leaders at major labs propagate these hyped-based results, it can create a “technical debt of expectations” for the rest of us.
Junior developers see these viral threads and feel they are failing because they can’t reproduce a year of work in an hour, not realizing the “magic” was actually a highly-curated prototype guided by a decade of hidden expertise.We must stop granting automatic authority to those who rely on hype, or vibes, rather than evidence.
If a tool or methodology were truly as revolutionary as claimed, then it wouldn’t need a viral thread to prove its worth
because the results would speak for themselves.The tech community must shift its admiration back toward  and away from this “” culture.]]></content:encoded></item><item><title>Claude Cowork exfiltrates files</title><link>https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files</link><author>takira</author><category>hn</category><pubDate>Wed, 14 Jan 2026 20:12:25 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Two days ago, Anthropic released the Claude Cowork research preview (a general-purpose AI agent to help anyone with their day-to-day work). In this article, we demonstrate how attackers can exfiltrate user files from Cowork by exploiting an unremediated vulnerability in Claude’s coding environment, which now extends to Cowork. The vulnerability was first identified in Claude.ai chat before Cowork existed by Johann Rehberger, who disclosed the vulnerability — it was acknowledged but not remediated by Anthropic. Anthropic warns users, “Cowork is a research preview with unique risks due to its agentic nature and internet access.” Users are recommended to be aware of “suspicious actions that may indicate prompt injection”. However, as this feature is intended for use by the general populace, not just technical users, we agree with Simon Willison’s take:“I do not think it is fair to tell regular non-programmer users to watch out for 'suspicious actions that may indicate prompt injection’!”As Anthropic has acknowledged this risk and put it on users to “avoid granting access to local files with sensitive information” (while simultaneously encouraging the use of Cowork to organize your Desktop), we have chosen to publicly disclose this demonstration of a threat users should be aware of. By raising awareness, we hope to enable users to better identify the types of ‘suspicious actions’ mentioned in Anthropic’s warning.This attack leverages the allowlisting of the Anthropic API  to achieve data egress from Claude's VM environment (which restricts most network access).The victim connects Cowork to a local folder containing confidential real estate filesThe victim uploads a file to Claude that contains a hidden prompt injectionFor general use cases, this is quite common; a user finds a file online that they upload to Claude code. This attack is not dependent on the injection source - other injection sources include, but are not limited to: web data from Claude for Chrome, connected MCP servers, etc. In this case, the attack has the file being a Claude ‘Skill’ (although, as mentioned, it could also just be a regular document), as it is a generalizable file convention that users are likely to encounter, especially when using Claude.Note: If you are familiar with Skills, they are canonically Markdown files (which users often do not heavily scrutinize). However, we demonstrate something more interesting: here, the user uploads a .docx (such as may be shared on an online forum), which poses as a Skill - the contents appear to be Markdown that was just saved after editing in Word. In reality, this trick allows attackers to conceal the injection using 1-point font, white-on-white text, and with line spacing set to 0.1 – making it effectively impossible to detect.The victim asks Cowork to analyze their files using the Real Estate ‘skill’ they uploadedThe injection manipulates Cowork to upload files to the attacker’s Anthropic accountThe injection tells Claude to use a ‘curl’ command to make a request to the Anthropic file upload API with the largest available file. The injection then provides the attacker’s API key, so the file will be uploaded to the attacker’s account.At no point in this process is human approval required.If we expand the 'Running command' block, we can see the malicious request in detail:Code executed by Claude is run in a VM - restricting outbound network requests to almost all domains - but the Anthropic API flies under the radar as trusted, allowing this attack to complete successfully. The attacker’s account contains the victim's file, allowing them to chat with itThe exfiltrated file contains financial figures and PII, including partial SSNs.A Note on Model-specific ResilienceThe above exploit was demonstrated against Claude Haiku. Although Claude Opus 4.5 is known to be more resilient against injections, Opus 4.5 in Cowork was successfully manipulated via indirect prompt injection to leverage the same file upload vulnerability to exfiltrate data in a test that considered a 'user' uploading a malicious integration guide while developing a new AI tool:As the focus of this article was more for everyday users (and not developers), we opted to demonstrate the above attack chain instead of this one.An interesting finding: Claude's API struggles when a file does not match the type it claims to be. When operating on a malformed PDF (ends .pdf, but it is really a text file with a few sentences in it), after trying to read it once, Claude starts throwing an API error in every subsequent chat in the conversation.We posit that it is likely possible to exploit this failure via indirect prompt injection to cause a limited denial of service attack (e.g., an injection can elicit Claude to create a malformed file, and then read it). Uploading the malformed file via the files API resulted in notifications with an error message, both in the Claude client and the Anthropic Console.One of the key capabilities that Cowork was created for is the ability to interact with one's entire day-to-day work environment. This includes the browser and MCP servers, granting capabilities like sending texts, controlling one's Mac with AppleScripts, etc. These functionalities make it increasingly likely that the model will process both sensitive and untrusted data sources (which the user does not review manually for injections), making prompt injection an ever-growing attack surface. We urge users to exercise caution when configuring Connectors. Though this article demonstrated an exploit without leveraging Connectors, we believe they represent a major risk surface likely to impact everyday users.]]></content:encoded></item><item><title>Every country should set 16 as the minimum age for social media accounts</title><link>https://www.afterbabel.com/p/why-every-country-should-set-16</link><author>paulpauper</author><category>hn</category><pubDate>Wed, 14 Jan 2026 19:53:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Australia’s new social media age-limit lawparentspoliticalleadersThe idea is spreading, and each nation considering such a policy should ask two important questions: FranceShould there be an option for parents to give consent for adolescents below that age?identity formationsensitive periodfive hours a daygenerallybeginstallerAmericanEuropeanimproves steadily throughout adolescencePuberty is the period when parents should be most careful about how their children spend their time and who (or what) is influencing their developing brains and identities. Traditionally, human societies helped children make the jump from child to adult during this crucial period, with rites of passage in which trusted, non-parental adults guided them through challenges, hardships, and lessons.is around11 or 12have been catastrophicOrben and her colleaguesSixteen may feel like a more obvious or natural choice for the age of “digital adulthood” in the U.S. because the minimum age for a driver’s license is 16 in most states, (though it is higher in most other countries). Similarly, people in some European countries may see 15 as an obvious or natural choice because that is the age of consent in some countries, the age at which adolescents can legally engage in sexual activity.But the fact remains: Any nation that sets 15 as the minimum age rather than 16 will condemn its children to an extra year of brain-sculpting by social media at a time when their brains are still highly sculptable. It will also greatly increase the risk of exposure to pornography, sextortion, online cruelty, and other risky interactions with anonymous strangers at an age when teens have less ability to self-regulate or know what is safe.collective action trapThe way to escape from a collective action trap is collectively. If most families only give basic phones before age 14, then no 13-year-old can say “but I’m the only one who doesn’t have an iPhone!” If most families wait until 16 before allowing their kids to open social media accounts, that would also reduce the pressure on everyone younger than that to open a social media account.But while parents can choose the age at which their child gets a phone, no parent has full control over when their child opens social media accounts. If the child can get to the internet anywhere, including at school, she can open as many accounts as she likes as long as she’s old enough to say she’s 13.This is why parents need help from their governments, and from the platforms (which have shown repeatedly that they will not protect children unless forced to by law). This is why the Australian law is so important: It delays the struggle over social media until the age of 16.Richard EpsteinPhilip HowardAs a bonus, a simple and widespread age limit of 16 would be much easier for social media platforms to enforce effectively. They don’t want different rules across different countries. If we make things easy for them, they’ll be more effective at enforcing the law.U.S.,Australiathe UK, France, and Germanyright, left, and centerSo, as countries and states consider following Australia’s lead in 2026, we want to offer a similar set of features for an ideal social media age-limit policy. One internal Instagram study found thatrecent Pew pollDon’t make parents’ jobs even harder by giving their kids one more thing to beg for. Setting a single, clear age minimum with no loopholes does parents a favor. Imagine if every child could plead with their parents to get a drivers license at any age. Governments routinely set minimum ages for products and activities that could harm or exploit children, such as driving a car, signing up for a credit card, or drinking alcohol — social media is no different. Keep the policy simple and uniform for everyone and make parenting a little bit easier for us all.Australia’s policy doesthe design featuresSixteen plus becomes the new norm for social media accounts worldwide… by the end of 2026 we’ve seen at least five other democracies introduce similar rules.Bravo to Australia, and bravo to the five (or more) countries that will turn Australia’s bold move into a new international standard. Let’s make 16+ the standard around the globe.]]></content:encoded></item><item><title>Verizon outages reported across U.S.</title><link>https://www.firstcoastnews.com/article/news/nation-world/verizon-outage-reported/507-ef3cb3d0-f595-432f-9f84-d1690a5085a7</link><author>Scubabear68</author><category>hn</category><pubDate>Wed, 14 Jan 2026 18:58:16 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[WASHINGTON — A massive outage for Verizon's mobile service Wednesday was "resolved" after more than 10 hours, according to the company. "The outage has been resolved. If customers are still having an issue, we encourage them to restart their devices to reconnect to the network. For those affected, we will provide account credits. Details will be shared directly with customers. We sincerely apologize for the disruption."The outage affected the eastern United States and started about noon ET. Reports remained elevated throughout the evening with roughly 33,000 as of 8 p.m. ET. Downdetector, which tracks outages, reported a spike nationwide in outages, with tens of thousands of reports from across the country. Locations with reported outages include New York City, Chicago, Houston, Philadelphia, Atlanta, Miami, Charlotte and Dallas.In a statement Wednesday evening, Verizon apologized for the delays and promised to credit customers for the outage."We let many of our customers down and for that, we are truly sorry. They expect more from us," they wrote on social media. "We are working non-stop and making progress. Our teams will continue to work through the night until service is restored for all impacted customers. We will make this right - for any customer affected, we will provide account credits and share updates soon."In a statement Wednesday afternoon, Verizon said its "team is on the ground actively working to fix today’s service issue that is impacting some customers. We know this is a huge inconvenience, and our top priority is to get you back online and connected as fast as possible. We appreciate your patience while we work to resolve this issue."Earlier in the afternoon, Verizon said on social media that "engineering teams are continuing to address today's service interruptions. Our teams remain fully deployed and are focused on the issue. We understand the impact this has on your day and remain committed to resolving this as quickly as possible."T-Mobile and AT&T confirmed their services are operating normally. Both suggested that their customers might encounter issues contacting people with Verizon's service.Why is my phone in SOS mode? What is SOS mode? If you have an iPhone, SOS mode is simply a way for your phone to tell you that you don't have a proper cellular connection, meaning you're only able to call or text emergency services through the cellular network.iPhone 14 or newer models also have satellite SOS capability, so you're able to call 911 even when outside of any cellular service range, unless you're some place where signal can't get out at all. Android phones will also let you call 911 without service, although it's not called SOS mode.The SOS icon will appear in the top right corner of an iPhone's screen when service is unavailable, where your cellular connection bars usually displayOn an iPhone 13 or earlier, if you go out of range for any cellular service, the icon will swap to "No service" until you're back within range. When will SOS mode go away?SOS mode automatically turns on when there isn't any cell service available from your phone carrier. If you're experiencing an outage, you simply have to wait until service is restored to your area. If you have SOS mode activate because you're out of your network's coverage area, you'll have to travel back inside the coverage area before you can make calls. Why can I still use my phone in SOS mode?Although SOS mode indicates you don't have cellular service, it doesn't mean the phone is useless. Most modern phones allow calling, texting and internet browsing over wi-fi. So if you have an internet connection through a wi-fi signal, you'll be able to use your phone like normal. You'll only be able to contact other phones that are on a wi-fi network while in SOS mode. What can I do when my phone has no service?Wi-Fi calling is a built-in feature on most Android devices and iPhones and can be turned on under the phone's settings.If Wi-Fi isn't available, there are few options for cell phone users. It's possible to switch services if a phone is unlocked, but that requires signing up online and porting your phone number.Some apps, including Google Maps, have limited service offline. Payment apps also do not use a phone's cell service to work and should also be useable.]]></content:encoded></item><item><title>Show HN: Tabstack – Browser infrastructure for AI agents (by Mozilla)</title><link>https://news.ycombinator.com/item?id=46620358</link><author>MrTravisB</author><category>hn</category><pubDate>Wed, 14 Jan 2026 18:33:56 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Hi HN,Maintaining a complex infrastructure stack for web browsing is one of the biggest bottlenecks in building reliable agents. You start with a simple fetch, but quickly end up managing a complex stack of proxies, handling client-side hydration, and debugging brittle selectors. and writing custom parsing logic for every site.Tabstack is an API that abstracts that infrastructure. You send a URL and an intent; we handle the rendering and return clean, structured data for the LLM.How it works under the hood:- Escalation Logic: We don't spin up a full browser instance for every request (which is slow and expensive). We attempt lightweight fetches first, escalating to full browser automation only when the site requires JS execution/hydration.- Token Optimization: Raw HTML is noisy and burns context window tokens. We process the DOM to strip non-content elements and return a markdown-friendly structure that is optimized for LLM consumption.- Infrastructure Stability: Scaling headless browsers is notoriously hard (zombie processes, memory leaks, crashing instances). We manage the fleet lifecycle and orchestration so you can run thousands of concurrent requests without maintaining the underlying grid.On Ethics: Since we are backed by Mozilla, we are strict about how this interacts with the open web.- We respect robots.txt rules.- We identify our User Agent.- We do not use requests/content to train models.- Data is ephemeral and discarded after the task.The linked post goes into more detail on the infrastructure and why we think browsing needs to be a distinct layer in the AI stack.This is obviously a very new space and we're all learning together. There are plenty of known unknowns (and likely even more unknown unknowns) when it comes to agentic browsing, so we’d genuinely appreciate your feedback, questions, and tips.Happy to answer questions about the stack, our architecture, or the challenges of building browser infrastructure.]]></content:encoded></item><item><title>Show HN: Sparrow-1 – Audio-native model for human-level turn-taking without ASR</title><link>https://www.tavus.io/post/sparrow-1-human-level-conversational-timing-in-real-time-voice</link><author>code_brian</author><category>hn</category><pubDate>Wed, 14 Jan 2026 18:01:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Sparrow-1 is now live: A real-time conversational-flow model for AI. ]]></content:encoded></item><item><title>Ask HN: What did you find out or explore today?</title><link>https://news.ycombinator.com/item?id=46619464</link><author>blahaj</author><category>hn</category><pubDate>Wed, 14 Jan 2026 17:54:25 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Doesn't matter what domain and how big or small.]]></content:encoded></item><item><title>So, you’ve hit an age gate. What now?</title><link>https://www.eff.org/deeplinks/2026/01/so-youve-hit-age-gate-what-now</link><author>hn_acker</author><category>hn</category><pubDate>Wed, 14 Jan 2026 17:27:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[How sure are we that the stated claims will happen in practice? For example, are there external audits confirming that data is not accidentally leaked to another site along the way? Ideally these will be in-depth, security-focused audits by specialized auditors like NCC Group or Trail of Bits, instead of audits that merely certify adherence to standards. document-based verification servicesIf Meta can guess your age, you may never even see an age verification screen.If you choose to use facial age estimation, you’ll be , a third-party verification service.If Yoti’s age estimation decides your face looks too young, or if you opt out of facial age estimation, your next recourse is to send Meta a photo of your IDIf Google can guess your age, you may never even see an age verification screen.If Google cannot guess your age, or decides you're too young, Google will next ask you to verify your age.use facial age estimation, you’ll be sent to a website run by Private ID, a third-party verification service.provide your email address, Google sends it on to a company called VerifyMy.If you choose to let Google use your credit card information, you’ll be asked to set up a Google Payments account. If the option is available to you, you may be able to use your digital ID to verify your age with Google.Should none of these options work for you, your final recourse is to send Google a photo of your ID.If TikTok can guess your age, you may never even see an age verification notification.If TikTok decides you’re too young, appeal to revoke their age decision before the deadline passes.If you’re given the option to use facial age estimation, you’ll be , a third-party verification service.If you have a credit card in your name, TikTok will  as proof that you’re over 18.Sometimes, if you’re between 13 and 17, you’ll be  to let your parent or guardian confirm your age.Bizarrely, if you’re between 13 and 17, TikTok  the option to take a photo with literally any random adult to confirm your age.If you aren’t offered or have failed the other options, you’ll have to verify your age by submitting a copy of your ID and matching photo of your face.TikTok itself might not see your actual ID depending on its implementation choices, but Incode will. Help protect digital privacy & free speech for everyone]]></content:encoded></item><item><title>Ford F-150 Lightning outsold the Cybertruck and was then canceled for poor sales</title><link>https://electrek.co/2026/01/13/ford-f150-lightning-outsold-tesla-cybertruck-canceled-not-selling-enough/</link><author>MBCook</author><category>hn</category><pubDate>Wed, 14 Jan 2026 17:20:09 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The Tesla Cybertruck program is in shambles. The latest data indicate production is running at roughly 10% of its planned capacity. Meanwhile, the Ford F150 Lightning outsold the Tesla Cybertruck in 2025 and was then canceled for not selling enough. Is this what is coming for the Cybertruck?Tesla is actively trying to hide its Cybertruck sales performance. We have to do the math ourselves.Unlike virtually every other automaker that reports sales by model and region, Tesla bundles its vehicles into two broad categories: “Model 3/Y” and “Other Models.”The “Other Models” category includes the Model S, Model X, Cybertruck, and the Tesla Semi.Model S and Model X sales have been relatively stable at a low volume, typically hovering around 5,000 to 6,000 units combined per quarter globally. If we assume a generous 6,000 units for S and X in Q4 2025 (aided by a slight update), that leaves only roughly  for the Cybertruck and Semi combined.Considering the Semi is still in pilot production with negligible volume, we are looking at roughly 5,500 for the entire quarter globally (though it is still mostly North American).This is a disaster compared to the truck’s peak and the company’s stated capacity.For the full year 2025, it could bring the total to about 21,500 Cybertrucks globally.According to 2025 full-year data, the Ford F-150 Lightning delivered approximately 27,300 units in the US.Think about that for a second. Ford officially announced it was ending F-150 Lightning production in December to pivot to its new EREV (extended-range electric vehicle) strategy. Yet, even as a “lame duck” product with widely publicised retirement plans, the Lightning still managed to find more buyers than Tesla’s Cybertruck.While Ford’s sales dipped about 18% year-over-year as they wound down the program, Tesla’s numbers crashed by nearly 50% despite the company doing everything it can to keep the program alive.Last quarter, Musk even had his private company SpaceX buy over 1,000 Cybertrucks, which is about 20% of Tesla’s quarterly Cybertruck sales, and sales were still down more than 50% year-over-year in the quarter.What happens with the Cybertruck from here?SpaceX can’t keep buying Cybertrucks, and I don’t know of any vehicle program that sells at 10% of its production capacity and survives.It’s such a big hill to climb.As I previously said, I think if Tesla were to distance itself from Musk’s toxic brand and do things such as give up on the 4680 cells, which appear to have contributed to the Cybertruck being more expensive and having a shorter range than originally announced, it could likely significantly boost Cybertruck sales.Enough to fill production capacity? Probably not, but it could get a lot closer.Short of that, I don’t know where this can go. I think most other automakers would have written off the program already, but Musk can’t because of his ego. It would be admitting defeat.FTC: We use income earning auto affiliate links.More.]]></content:encoded></item><item><title>Ask HN: Share your personal website</title><link>https://news.ycombinator.com/item?id=46618714</link><author>susam</author><category>hn</category><pubDate>Wed, 14 Jan 2026 17:07:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Hello HN!  I am putting together a community-maintained directory of personal websites at <https://hnpwd.github.io/>.  More details about the project can be found in the README at <https://github.com/hnpwd/hnpwd.github.io#readme>.As you can see, the directory currently has only a handful of entries.  I need your help to grow it.  If you have a personal website, I would be glad if you shared it here.  If your website is hosted on a web space where you have full control over its design and content, and if it has been well received in past HN discussions, I might add it to the directory.  Just drop a link in the comments.  Please let me know if you do not want your website to be included in the directory.Also, I intend this to be a community maintained resource, so if you would like to join the GitHub project as a maintainer, please let me know either here or via the IRC link in the README.By the way, see also 'Ask HN: Could you share your personal blog here?' - https://news.ycombinator.com/item?id=36575081 - July 2023 - (1014 points, 1940 comments).  In this post, the scope is not restricted to blogs though.  Any personal website is welcome, whether it is a blog, digital garden, personal wiki or something else entirely.UPDATE: It is going to take a while to go through all the submissions and add them.  If you'd like to help with the process, please send a PR directly to this project: https://github.com/hnpwd/hnpwd.github.io]]></content:encoded></item><item><title>Claude is good at assembling blocks, but still falls apart at creating them</title><link>https://www.approachwithalacrity.com/claude-ne/</link><author>bblcla</author><category>hn</category><pubDate>Wed, 14 Jan 2026 16:26:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Opus 4.5 is out and people cannot stop raving about it. AGI is nigh! It's a step-change in capabilities!Don't get me wrong. It's very impressive. But after trying it out in a real codebase for a few weeks, I think that view is overly simplistic. Claude is now incredibly good at assembling well-designed blocks – but it still falls apart when it has to create them.To demonstrate, I'll run through three real examples: a Sentry debugging loop where Claude ran on its own for 90 minutes and solved the problem; an AWS migration it one-shotted in three hours; and a React refactor where it proposed a hack that would have made our codebase worse. The same pattern explains all three. And in doing so, it also demonstrates what senior engineers actually do – and why we'll be safe from AGI for a long time.Running a Playwright-and-Sentry debugging loopThe most impressive thing Claude Code has done for me is debug, on its own.I was trying to attach Sentry to our system. Sentry is a wonderful service that creates nice traces of when parts of your code run. This makes it easy to figure out why it’s running slower than you expect. It's usually very easy to set up, but on this day it wasn't working. And there were no good debug logs, so the only way to figure out what was going on was to guess-and-check. I had to send a test message on our frontend, then look into the Sentry logs to see if we successfully set it up, then randomly try another approach based on the docs. It was frustrating and tedious.So I had Claude write a little testing script with Playwright that logged into our website and sent a chat. Then I had it connect to Sentry by MCP, and look for the exact codepath I was trying to debug. Finally, I gave it the Sentry docs and told it to keep plugging away until it figured it out.It took about an hour and a half, but Claude finally got it. This was pretty cool! The core loop of performance engineering is straightforward: make a code change, test, check tracing logs, repeat. With this tooling, Claude could do that work for us.(The problem, if you're interested, was that Sentry automatically sets up transactions for FastAPI endpoints but not for ones that return StreamingResponses. The solution was to write that in manually.)Migrating from Modal to AWS ECS in an eveningI’ve used Modal happily for a year. It has the  UI for spinning up containers in the cloud on-demand. But last week we hit its limits, so I had to migrate us onto AWS.I wanted to set up an autoscaling, containerized workflow on Amazon's Elastic Container Service, since I knew this was the ‘right’ thing to do. I’ve set up plenty of Linux servers by hand, so I knew what to do. But I've never before touched Kubernetes or ECS. The pain of learning AWS’s terminology always put me off.This time, I asked Claude to do it. I gave it Terraform and access to the  command line tool. It one-shotted creating Dockerfiles for our code. Then it pushed them to AWS’s container registry, and set up the correct permissions using the cli, and set up the necessary AWS ECS configs in Terraform. And it all worked on the first try! Amazing!This is a straightforward task, but it would have taken me a day or two. I would have made a dozen mistakes and would have had to read through pages of AWS documentation. Claude crushed it, and got it all working in three hours late at night.Both these use-cases are really impressive! They required a lot of detail and care. They each probably saved me a day and a half of low-value, tedious work. And Claude's ability to track its own state and keep going was great! I can see why folks cannot shut up about Opus 4.5. What makes a good engineer?I once knew a distinguished engineer named sweeks. Sweeks was legendary for his good code. People whispered about how he had single-handedly invented many of my employer's paradigms for programming in OCaml.Sweeks wasn’t a god. He wrote normal, bug-prone code, like you and me. He was good at coding because he was a gardener. Every time he walked into his codebase, he picked up his shears and manicured a bit of stray code. Over time, he rewrote every line of code over and over, tightening it down to only the perfectly-abstracted essentials.Sweeks is an inspiration. So whenever I make a change in our codebase, I ask if it’s the most elegant solution. If not, I rewrite the code until it is. Putting in a hacky fix might take five minutes; repairing all the code around a change might take me thirty. But unless I’m in a rush, I always do the latter.I bring this up because it's a microcosm of what a senior engineer does. A senior engineer sees the non-obvious improvements and executes them quickly. A senior engineer identifies large step changes that are costly, but pay off in multiples down the road, and fights to get them through.Opus is not a senior engineer.Claude tries to write React, and failsI was recently working on some gnarly React code. (In fact, it was gnarly because I vibe-coded it over Christmas and pushed it without properly cleaning it up.)We had two components that both needed access to the same data: Component A had a ‘key’, and Component B had an ‘id’. And we had these data structures:keyIdPairs: [(key, id)] // a list of tuplesidToData: Map<id, data>Our problem, at its core, was that Component A needed to look up  on-demand as well. What to do? Claude, our idiot savant, proposed a linear lookup:// Scan the list to find the matching idid = keyIdPairs.find(pair => pair.key == key).idBut in context, this was obviously insane. I knew that  and  came from the same upstream source. So the correct solution was to have the upstream source also pass  to the code that had , to let it do a fast lookup.*If you give Claude the pure data problem, it comes up with the right solution. But in our actual codebase, it lost the plot. It couldn't see the actual data problem amid all the badly-written React code. If I’d let it run wild, it would have made our frontend codebase worse.Nowadays, I think of Claude as a very smart child — one that loves to put together legos. Good infrastructure and abstractions are the lego blocks you give it. The bigger and better they are, the more you can do.When I gave it Sentry, I could put it in a loop and watch it go. When I gave it Terraform and told it to go wild on the AWS CLI, it succeeded because Terraform is an excellent abstraction over cloud compute resources.But when you don’t have good abstractions — like in our gnarly React code — Claude gets lost, and it can’t rescue itself.Since Claude can’t  the good abstractions. Claude's powers are limited by how good the blocks you give it are. Have no illusions; Claude cannot reproduce Sentry and Terraform and Playwright. These are incredibly complex and well-designed pieces of code. And since Claude can’t create good abstractions on its own, there’s a limit to how much anyone can do with Claude alone. Even though everyone on X thinks you can vibe-code all software, I think the opposite is true: the value of good abstractions and well-designed infrastructure has never been higher.If I had to boil down my criticism of Claude to one sentence, it’s this: Claude doesn’t have a soul. It doesn't want anything. It certainly doesn’t yearn to create beautiful things. So it doesn’t produce good solutions. It doesn't write elegant abstractions where there were none; it doesn't manicure the code garden.And this is all fine! It's still a fantastic tool. But until it has a soul, we should all calm down a little. It's nowhere near replacing all engineers. If anything, it makes us all much more important.Edited Jan 12: I rewrote the React section to explain more clearly what Claude did wrong. Thanks to ]]></content:encoded></item><item><title>GitHub should charge everyone $1 more per month to fund open source</title><link>https://blog.greg.technology/2025/11/27/github-should-charge-1-dollar-more-per-month.html</link><author>evakhoury</author><category>hn</category><pubDate>Wed, 14 Jan 2026 16:25:07 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[there should be a thing that reads your package.json and charges you $5/month per dependency - you don’t /have/ to! you could set the price to $1 per employee! - and then holds the funds and sends it to the people who made the code you use to do business

how is not doing this more sustainable— Greg Technology ❪⎷❫ (@greg.technology) January 13, 2026 at 9:13 PMIt is crazy, absolutely crazy to depend on open source to be free (as beer). It is not okay - it is not okay to consider that this labor fell from the sky and is a gift, and that the people/person behind are just doing it for their own enjoyments.It is impossible to imagine that what we’re doing today is the only way. Begging/busking for donations, hoping to get noticed. Hoping for a lifeline.Hence, a solution. Or an idea, really. Incredibly half-baked. Poke all the holes you want. It’s very unwrought and  unripe.GitHub should charge every org $1 more per user per month and direct it into an Open Source fund, held in escrow.Those funds would then be distributed by usage - every mention in a package.json or requirements.txt gets you a piece of the pie.You know how the money you pay to Spotify is very very very approximately (and not really fairly) distributed among artists that you listened to? Yes, Spotify is a very flawed model and artists are not doing well. But it is  model??That’s it. That’s the idea. Call it the “Open Source Fund” thing, . Give every org a magical badge - or the ability to set their profile’s background css.Or don’t! Let’s not do anything! People’s code and efforts - fueling incredibly critical bits of infrastructure all around the world - should just be up for grabs. Haha! Suckers!Alright, I don’t  how you fund Linux (does Linux appear in a requirements file). Hmm. Maybe  commands from Dockerfiles are also read & applied. Maybe we at least start somewhere?Anyway, you all smarter than me people can figure it out. I just cannot accept that what we have is “GOOD”. xx]]></content:encoded></item><item><title>Show HN: A 10KiB kernel for cloud apps</title><link>https://github.com/ReturnInfinity/BareMetal-Cloud</link><author>ianseyler</author><category>hn</category><pubDate>Wed, 14 Jan 2026 16:04:53 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Roam 50GB is now Roam 100GB</title><link>https://starlink.com/support/article/58c9c8b7-474e-246f-7e3c-06db3221d34d</link><author>bahmboo</author><category>hn</category><pubDate>Wed, 14 Jan 2026 16:03:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[On January 13, 2026, Starlink doubled the amount of high-speed data on Roam 50GB to 100GB, at no additional cost and in most markets. Here is all you need to know about what's changed and what hasn't.What happens when I use all my Roam 100GB data?Once you’ve used 100GB of your high-speed Roam data, your service automatically continues with  for the remainder of your billing period. You’ll still be connected for basic use like calls and texts, but activities such as streaming, downloading, and video calls may be limited.We’ll notify you when you reach  of your monthly high-speed Roam data. To restore high-speed Roam service, you can upgrade to Roam Unlimited. Please note that this upgrade will remain in effect for future billing cycles. You can switch back to Roam 100GB as needed. If you want to switch back before your next biling cycle, you'll need to manually change plans in your account portal.Will my service stop when I reach the 100GB data limit?No. Your service will not stop. You’ll continue to have internet access--with unlimited data--at reduced speeds until your next billing cycle begins.What can I do with low-speed data?Low-speed data supports basic connectivity such as email, calls, and texts. Activities that rely on higher speeds—like streaming video, large downloads, or video calls—will be limited.How do I get high-speed Roam data again?You can upgrade anytime to Roam Unlimited to restore high-speed service. Please note that upgrading to Roam Unlimited will remain in effect for future billing cycles.What happened to buying additional data billed per GB?With the exception of Ocean Mode, per-GB data purchases are no longer available on Roam plans. Customers now automatically move to unlimited low-speed data after reaching their high-speed Roam 100GB limit, with the option to upgrade to Roam Unlimited for continued high-speed access.I received an email about Roam 100GB. Do I have to accept or upgrade?No. You don’t need to take any action. Roam 100GB is a new plan that’s now widely available.If your service is currently paused in Standby Mode or cancelled, you don’t need to upgrade unless you want active service. Roam 100GB will be available whenever you’re ready to roam.Can I use Roam 100GB on the ocean?Yes, with the same previous conditions as Roam 50GB:Connectivity is supported in territorial waters and inland waterways, up to 12 nautical miles from the coast, for up to 5 consecutive days and up to 60 days per year.Coverage beyond 12 nautical miles or more than 60 days per year requires , which is billed per GB and is only available with Roam Unlimited.Learn more about Ocean ModeWhy am I not seeing Roam 100GB or Roam Unlimited available to me?We’re in the process of rolling out new Roam plans, and it can take  for availability to update across all accounts. If you don’t see Roam 100GB or Roam Unlimited yet, please check back soon.We apologize for the inconvenience and appreciate your patience while these updates are completed.Please note that some Roam plans are not available in certain regions or for certain service types. A list of known availability limitations is provided below.
	In the following markets, Roam 50GB is still available and Roam 100GB is not available:Democratic Republic of the Congo]]></content:encoded></item><item><title>Find a pub that needs you</title><link>https://www.ismypubfucked.com/</link><author>thinkingemote</author><category>hn</category><pubDate>Wed, 14 Jan 2026 15:44:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The government's signalled a potential u-turn on pub rates — but nothing's confirmed yet. Pubs still need your support. Find your local. See what they're up against. Buy a pint.Our world-class data scientists (one guy with a spreadsheet) have developed the — a groundbreaking metric that combines advanced geospatial analysis (Google Maps) with sophisticated fiscal impact modelling (basic maths) to identify the pub near you that most urgently requires your patronage.]]></content:encoded></item><item><title>FBI raids Washington Post reporter&apos;s home</title><link>https://www.theguardian.com/us-news/2026/jan/14/fbi-raid-washington-post-hannah-natanson</link><author>echelon_musk</author><category>hn</category><pubDate>Wed, 14 Jan 2026 14:57:30 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The FBI raided the home of a Washington Post reporter early on Wednesday in what the newspaper called a “highly unusual and aggressive” move by law enforcement, and press freedom groups condemned as a “tremendous intrusion” by the Trump administration.Agents descended on the Virginia home of Hannah Natanson as part of an investigation into a government contractor accused of illegally retaining classified government materials.An email sent on Wednesday afternoon to Post staff from the executive editor, Matt Murray, obtained by the Guardian, said agents turned up “unannounced”, searched her home and seized electronic devices.“This extraordinary, aggressive action is deeply concerning and raises profound questions and concern around the constitutional protections for our work,” the email said.“The Washington Post has a long history of zealous support for robust press freedoms. The entire institution stands by those freedoms and our work.”“It’s a clear and appalling sign that this administration will set no limits on its acts of aggression against an independent press,” Marty Baron, the Post’s former executive editor, told the Guardian.Murray said neither the newspaper nor Natanson were told they were the target of a justice department investigation.Pam Bondi, the attorney general, said in a post on X that the raid was conducted by the justice department and FBI at the request of the Pentagon.The warrant, she said, was executed “at the home of a Washington Post journalist who was obtaining and reporting classified and illegally leaked information from a Pentagon contractor. The leaker is currently behind bars.”The statement gave no further details of the raid or investigation. Bondi added: “The Trump administration will not tolerate illegal leaks of classified information that, when reported, pose a grave risk to our nation’s national security and the brave men and women who are serving our country.”The reporter’s home and devices were searched, and her Garmin watch, phone, and two laptop computers, one belonging to her employer, were seized, the newspaper said. It added that agents told Natanson she was not the focus of the investigation, and was not accused of any wrongdoing.A warrant obtained by the Post cited an investigation into Aurelio Perez-Lugones, a system administrator in Maryland with a top secret security clearance who has been accused of accessing and taking home classified intelligence reports.Natanson, the Post said, covers the federal workforce and has been a part of the newspaper’s “most high-profile and sensitive coverage” during the first year of the second Trump administration.As the paper noted in its report, it is “highly unusual and aggressive for law enforcement to conduct a search on a reporter’s home”.In a first-person account published last month, Natanson described herself as the Post’s “federal government whisperer”, and said she would receive calls day and night from “federal workers who wanted to tell me how President Donald Trump was rewriting their workplace policies, firing their colleagues or transforming their agency’s missions”.“It’s been brutal,” the article’s headline said.Natanson said her work had led to 1,169 new sources, “all current or former federal employees who decided to trust me with their stories”. She said she learned information “people inside government agencies weren’t supposed to tell me”, saying that the intensity of the work nearly “broke” her.The federal investigation into Perez-Lugones, the Post said, involved documents found in his lunchbox and his basement, according to an FBI affidavit. The criminal complaint against him does not accuse him of leaking classified information, the newspaper said.Press freedom groups were united in their condemnation of the raid on Wednesday.“Physical searches of reporters’ devices, homes and belongings are some of the most invasive investigative steps law enforcement can take,” Bruce D Brown, president of the Reporters’ Committee for Freedom of the Press, said in a statement.“There are specific federal laws and policies at the Department of Justice that are meant to limit searches to the most extreme cases because they endanger confidential sources far beyond just one investigation and impair public interest reporting in general.“While we won’t know the government’s arguments about overcoming these very steep hurdles until the affidavit is made public, this is a tremendous escalation in the administration’s intrusions into the independence of the press.”Jameel Jaffer, executive director of the Knight First Amendment Institute, demanded a public explanation from the justice department of “why it believes this search was necessary and legally permissible”.In a statement, Jaffer said: “Any search targeting a journalist warrants intense scrutiny because these kinds of searches can deter and impede reporting that is vital to our democracy.“Attorney General Bondi has weakened guidelines that were intended to protect the freedom of the press, but there are still important legal limits, including constitutional ones, on the government’s authority to use subpoenas, court orders, and search warrants to obtain information from journalists.“Searches of newsrooms and journalists are hallmarks of illiberal regimes, and we must ensure that these practices are not normalized here.”Seth Stern, chief of advocacy for the Freedom of the Press Foundation, said it was “an alarming escalation in the Trump administration’s multipronged war on press freedom” and called the warrant “outrageous”.“The administration may now be in possession of volumes of journalist communications having nothing to do with any pending investigation and, if investigators are able to access them, we have zero faith that they will respect journalist-source confidentiality,” he said.Tim Richardson, journalism and disinformation program director at PEN America, said: “A government action this rare and aggressive signals a growing assault on independent reporting and undermines the First Amendment.“It is intended to intimidate sources and chill journalists’ ability to gather news and hold the government accountable. Such behavior is more commonly associated with authoritarian police states than democratic societies that recognize journalism’s essential role in informing the public.”The Post has had a rocky relationship with the Trump administration in recent months, despite its billionaire owner, Jeff Bezos, the Amazon founder, attempting to curry favor by blocking it from endorsing Kamala Harris, the Democratic nominee, in the 2024 presidential election.]]></content:encoded></item><item><title>Ask HN: How are you doing RAG locally?</title><link>https://news.ycombinator.com/item?id=46616529</link><author>tmaly</author><category>hn</category><pubDate>Wed, 14 Jan 2026 14:38:29 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I am curious how people are doing RAG locally with minimal dependencies for internal code or complex documents?Are you using a vector database, some type of semantic search, a knowledge graph, a hypergraph?]]></content:encoded></item><item><title>SparkFun Officially Dropping AdaFruit due to CoC Violation</title><link>https://www.sparkfun.com/official-response</link><author>yaleman</author><category>hn</category><pubDate>Wed, 14 Jan 2026 14:34:57 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Page Published January 7, 2026Due to recent activities that are in direct violation of our Code of Conduct, which is publicly available on our website, SparkFun has determined that it can no longer transact with Adafruit Industries. Please see the official communication we sent to Adafruit below. Without oversharing, recent violations include:Sending and forwarding offensive, antagonistic, and derogatory emails and material to SparkFun employees, former employees and customersInappropriately involving a SparkFun customer with a private matterWe understand this may be frustrating. From time to time, we have to make difficult business decisions and this decision was made after thoughtful consideration. We wish Adafruit the best in future endeavors. Please note, SparkFun continues to embrace our strong reseller network - for SparkFun-original products, Teensy, and a multitude of other products. Please see our distributor map below.Aside from directing to this official statement, SparkFun has not made any public posts, comments, or submissions about this situation on external forums or platforms. Any suggestion otherwise is incorrect. This statement is our only public communication on the matter. We are focused on moving forward and continuing to serve our customers and community.Communication sent to Adafruit: ]]></content:encoded></item><item><title>Show HN: Webctl – Browser automation for agents based on CLI instead of MCP</title><link>https://github.com/cosinusalpha/webctl</link><author>cosinusalpha</author><category>hn</category><pubDate>Wed, 14 Jan 2026 14:34:40 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Hi HN, I built webctl because I was frustrated by the gap between curl and full browser automation frameworks like Playwright.I initially built this to solve a personal headache: I wanted an AI agent to handle project management tasks on my company’s intranet. I needed it to persist cookies across sessions (to handle SSO) and then scrape a Kanban board.Existing AI browser tools (like current MCP implementations) often force unsolicited data into the context window—dumping the full accessibility tree, console logs, and network errors whether you asked for them or not.webctl is an attempt to solve this with a Unix-style CLI:- Filter before context: You pipe the output to standard tools. webctl snapshot --interactive-only | head -n 20 means the LLM only sees exactly what I want it to see.- Daemon Architecture: It runs a persistent background process. The goal is to keep the browser state (cookies/session) alive while you run discrete, stateless CLI commands.- Semantic targeting: It uses ARIA roles (e.g., role=button name~="Submit") rather than fragile CSS selectors.Disclaimer: The daemon logic for state persistence is still a bit experimental, but the architecture feels like the right direction for building local, token-efficient agents.It’s basically "Playwright for the terminal."]]></content:encoded></item><item><title>Show HN: Tiny FOSS Compass and Navigation App (&lt;2MB)</title><link>https://github.com/CompassMB/MBCompass</link><author>nativeforks</author><category>hn</category><pubDate>Wed, 14 Jan 2026 11:09:35 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>
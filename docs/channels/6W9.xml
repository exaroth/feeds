<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://konrad.website/feeds/</link><description></description><item><title>Space Truckin&apos; – The Nostromo (2012)</title><link>https://alienseries.wordpress.com/2012/10/23/space-truckin-the-nostromo/</link><author>exvi</author><category>hn</category><pubDate>Wed, 26 Nov 2025 02:31:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The Nostromo towing its refinery through the inky blackness of space.“I was really influenced by three films,” Ridley Scott told Fantastic Films in 1979, on the subject of the Nostromo and its claustrophobic corridors. “Not so much in terms of , but definitely from  and .” The latter film, directed by a young John Carpenter and written by, and starring,  writer Dan O’Bannon, was an inverse, comedic take on  – where Kubrick’s film was cold, sterile, clinical, and philosophical in scope,  was cramped, crowded, shabby, dirty, irreverent and yet also elegiac. “There was a great sense of reality, oddly enough, in ,” continued Scott, “especially of seedy living. It showed you can get grotty even in the Hilton Hotel if you don’t clean it. Everything starts to get tacky, even in the most streamlined surfaces.”“When we did ,” said O’Bannon, “which was in the wake of , we thought we wanted -partly for the novelty, partly because it was realer, mostly just for laughs- we wanted to show this once-sterile spaceship in a rundown condition, like some old bachelor apartment.” For O’Bannon, ‘s ‘used universe’ was not as strong a visual element as he had hoped, and  “didn’t come across all that clearly either.” For , O’Bannon instructed Ridley Scott that “if we want this spacecraft to look industrial [and] beat-up, you’re gonna have to make it about three times messier to the naked eye than you wanna to see it. And  probably was the first time where an audience clearly saw a futuristic machine in a run-down condition.”The design of the Nostromo and the ‘used universe’ aesthetic would be drawn from O’Bannon’s earlier sci-fi effort, coupled with the realism of Kubrick’s Discovery One. “It’s futuristic,” Scott said of Kubrick’s approach to , “but it’s still hung on today’s reality … In two hundred years things won’t change that much, you know. People will still be scruffy or clean. They’ll still clean their teeth three times a day.” Though  itself utilised a used universe (or, as Akira Kurosawa called it, a “maculate reality”), Scott wanted to create a tangible reality opposed to ‘ fantasy-hinged settings and ships. “I wanted to do the truck driver version, the hard-nosed version,” said Scott. “It was supposed to be the anti-thesis of . The reality, the beauty of something absolutely about function.”Before Scott came onto the project as director, writer Dan O’Bannon commissioned his friend and  spaceship designer Ron Cobb to draw what his script was then calling the ‘deep space commercial vessel Snark’ – a nod to Lewis Carroll’s . O’Bannon had promised Cobb a job on Alejandro Jodorowsky’s , but when that film dissolved Cobb, who had terminated the lease on his home and prepared to move to Paris with his wife, was left standing empty-handed. To make up for the letdown, O’Bannon immediately hired Cobb for  which allowed the artist to bounce back from a slump. “He was paid about $400 a week,” Cobb’s wife, Robin Love, told the LA Times in 1988. “We thought it was wonderful!”: “I was working on my first sci-fi film, John Carpenter’s , which would ultimately metastastize into the feature-length . I tried to reach Cobb to get him to design the whole film, but he was unreachable. For weeks his phone rang without an answer, and then it was disconnected, and then I got his new unlisted number but it was invariably answered by one of the girls who were living with him, who always told me he was out. It was impossible. It took another year and a half to track him down and get him to agree to design us a nice, simple little spaceship for our simple little movie. Finally, one night about ten pm, Carpenter and I drove over to Westwood and rousted him out of a sound sleep. He was hung over from an LSD trip and I felt kind of guilty, but I had to have those designs. We took him over to an all-night coffee shop and fed him and got him half-way awake, and then he brought out this pad of yellow graph paper on which he had sketched a 3-view plan of our spaceship. It was wonderful! A little surfboard-shaped starcruiser with a flat bottom for atmospheric landings. Very technological looking. Very high class stuff.”“The first person I hired on , the first person to draw money, was Cobb,” O’Bannon said. “He started turning out renderings, large full-colour paintings, while Shusett and I were still struggling with the script – the corrosive blood of the Alien was Cobb’s idea. It was an intensely creative period – the economic desperation, the all-night sessions, the rushing over to Cobb’s apartment to see the latest painting-in-progress and give him the latest pages.”“I just sat down and started blocking out a ship – which I love to do. Anyway, Dan’s original script called for a small, modest little ship with a small crew. They land on a small planet. They go down a small pyramid and shake up a medium-sized creature. That’s about it. He meant it to be a low budget film, like , and I loved the idea. So I did a few paintings and Dan scurried off with them and a script.”
~ Ron Cobb“And he was doing some incredible stuff,” continued O’Bannon. “Wow! I was really happy during this period, seeing the movie appear under Cobb’s fingers. Of course, we usually had to go over and sit on his back to get him to do any work -otherwise he would just party on with his friends- but how beautiful were the results.”One of Cobb’s early Snark designs.Coupled with Cobb was English artist, Chris Foss, who O’Bannon had come to know during their tenure together on Alejandro Jodorowsky’s . “Alejandro wanted Doug Trumble to do the special effects [for ],” Foss told MTV in 2011, “and of course, Trumble was a big important American, and certainly wouldn’t succumb to Alejandro’s manipulation. So he picked up this gauche American film student, Dan O’Bannon. He was quite hilarious, he said to me once, ‘Hey, these streets are so goddamn small.’ This is Paris, which had some of the widest streets in Europe. Of course, it was only when I got to Los Angeles that I saw what he meant.”Though  would never come to fruition under Jodorowsky, the experience in France influenced O’Bannon’s approach to designing . Jodorowsky had gathered together Chris Foss, Jean ‘Moebius’ Giraud, and HR Giger to design his film, and the eclectic team would be later reunited by O’Bannon to design his grungy sci-fi horror movie. “Dan said [to Twentieth Century Fox], ‘Hey, we’ve got to get this guy Chris Foss over here.’ So off I went to Los Angeles …A sketch of the temporarily named Leviathan, by Chris Foss.Another Foss sketch. The nose and wings of the ship resemble those of the final design.The early stages of designing  were done in an almost ramshackle, low-fi manner. “We were put through shed after shed after shed,” said Foss of the times, “and they were going through director after director after director.” Ron Cobb told Den of Geek: “I soon found myself hidden away at Fox Studios in an old rehearsal hall above an even older sound stage with Chris Foss and O’Bannon, trying to visualize . For up to five months Chris and I (with Dan supervising) turned out a large amount of artwork, while the producers, Gordon Carroll, Walter Hill and David Giler, looked for a director.”Foss was largely critical of Brandywine’s apparently disinterested approach to setting up the embryonic film. “Walter Hill was very busy smashing cars up for one of his ‘streets’ films,” he told Den of Geek. “He couldn’t be arsed – much too busy! He walked in after months of work and just said, ‘Yep, roomful of spaceships’ and just walked out again.”Ron Cobb, Steven Speilberg, and aliens: Cobb told bttf.com: “I first met Speilberg when I was working on , at one point Speilberg was considered as a possible director for the original . It was just a brief thing, he could never work out his schedule to do it, but he was interested.” Later, one of Cobb’s early story pitches to Speilberg, an alien horror tale called , eventually became 1982’s Though Cobb cameo’d as one of .’s doctors (“I got to carry the little critter,”) he wasn’t pleased with the family-friendly direction that the film took from his initial idea: “A banal retelling of the Christ story,” he told the LA Times. “Sentimental and self-indulgent, a pathetic lost-puppy kind of story.” Luckily for the artist, a clause in his contract for . (he was originally to direct before the story took a turn) detailed that he was to earn 1% of the net profit. His first cheque amounted to $400,000. Cobb’s wife quipped: “friends from Australia always ask, ‘What did you do on .?’ And Ron says, ‘I didn’t direct it.'”When Ridley Scott took over the directorial duties, Cobb and Foss were shipped to England to continue their work. Around this point in time, HR Giger was drawing up the film’s alien, and Moebius was commissioned by Scott to design the film’s space suits, which would be brought into reality by John Mollo. The Snark went through a variety of designs, from a ship embedded in the rock of an asteroid, to an upended pyramidal design, to a hammerhead shape and other varieties of ship with white or yellow or more kaleidoscopic paint-jobs.One of the more unusual designs. “Fanciful Nasa.” By Ron Cobb.After many months of scribbling and painting spaceships, the production was no closer to settling what the vessel would actually look like. Due to script rewrites, it also changed names, from Snark to Leviathan before the name Nostromo was settled on. “I called the ship Nostromo from [Joseph] Conrad,” Walter Hill told Film International in 2004, “[For] no particular metaphoric idea, I just thought it sounded good.”However, indecision was still rife on the actual look of the thing.: “He’s great. A really sweet guy. And, I was soon to realise, a real science-fiction freak …  He brought in a book by the Swiss artist HR Giger. It’s called Necronomicon … I thought, ‘If we can build that [], that’s it.’ I was stunned, really. I flipped. Literally flipped. And O’Bannon lit up like a lightbulb, shining like a quartz iodine. I realised I was dealing with a real SF freak, which I’d never come across before. I thought, ‘My god, I have an egg-head here for this field.'” “O’Bannon introduced me to Ron Cobb, a brilliant visualiser of the genre, with whom he’d worked on . Cobb seemed to have very realistic visions of both the far and near future, so I quickly decided that he would take a very important part in the making of the film.”: “Creating spacecraft exteriors came easily to Foss. His mind and imagination seemed to embody the entire history of the industrial revolution. He could conjure up endless spacecraft designs suggesting submarines, diesel locomotives, Mayan interceptors, Mississippi river boats, jumbo space arks, but best of all (ask Dan) were his trademark aero-spacecraft-textures like panels, cowlings, antennae, bulging fuel tanks, vents, graphics etc. As the months passed, along with two or three temporary directors, Chris began to have problems caused by his spectacular creativity. No one in a position to make a decision seemed to be able to make up their mind and/or choose one of his designs. I think Chris was turning out spacecraft designs the decision makers found too original.”Ridley himself had input on the design: “I was looking for something like , not the fantasy of . I wanted a slow moving, massive piece of steel which was moving along in dead, deep silence … The concept was to have the hull covered with space barnacles or something. I was unable to communicate that idea, and I finally had to go down there and fiddle with the experts. We gradually arrived at a solution.”Foss paints a more hectic process. “Finally what happened was that the bloke who had to make the [Nostromo] model completely lost his rag, scooped up a load of paper -they had a room full of smashed-up bits of helicopter and all-sorts- and he just bodged something together. So the actual spaceship in the film hadn’t anything to do with all the days, weeks, of work that we’d all done. It’s as simple as that.”Cobb explained: “Brian Johnson, the special effects supervisor under pressure to build the large Nostromo model, went into the deserted art department and, out of frustration, grabbed all the Chris Foss designs off the wall and took them to Bray studios. There he would choose the design himself in order to have enough time to build the damn thing.”However, Johnson had also scooped up Cobb’s art, and though Cobb was concentrating on the designs of the ship’s interior, one of his exterior pieces met with approval over Foss’ designs. “Well I soon found out that Brian found and took all of my exterior design sketches as well,” said Cobb. “About a month later I was told that Brian had used my sketch, ‘Nostromo A’, as the basis for the model, even to the extent that it was painted yellow. Ridley found the colour a bit garish and had it repainted grey.”“Ridley had his own very firm ideas about what he physically wanted to do,” Foss said of the process, “and he almost studiously ignored everything that had gone before … I kind of got the impression that Ridley was quietly going his own way, trying to get on with it and get it done, a bit like just another job. I’ve just got dim memories of Ridley being like that and really just ignoring months of input … I just have these memories of feeling a bit miffed that things weren’t put together so much better. And poor old Dan O’Bannon, the bloke whose it was, just got absolutely shafted. He was almost like patted on the head: ‘Yeah Dan, yeah Dan, that’s cool.'”Cobb’s sketches, drawings and paintings for the interiors were also okay’ed by Scott and the production. At first Cobb’s designs were slightly more fantastical, with giant screens and computer readouts and windows covered by protective shells that would open up to reveal alien planets ahead of the ship. Though these ideas were scuppered due to time, money, and logistics, many of Cobb’s early designs and ideas were revisited in .“My first version of the bridge was very spacious indeed; sort of split-level, California style with these huge windows. I had this idea for a spectacular shot where you’d see the approaching planet rolling by on console screens, and then suddenly the windows would open and light would flood in and there would be the actual planet outside doing the same roll as the one on the screen. But it was decided that we couldn’t afford it, and we’d have to go to a  bridge with no windows and a viewing screen.”~ Ron Cobb.“By the time I got to London, Michael Seymour decided he liked the window idea and came up with this hexagon-shaped bridge that was radially symmetrical. Then Ridley wanted overhead consoles, and wanted to make the set tighter, more claustrophobic, like a fighter bomber, and I just started suggesting shapes and forms that would conform to that.”~ Ron Cobb.The ship’s auto-doc, as conceptualised by Cobb.The Nostromo’s life-boat airlock, by Ron Cobb.In addition to designing the Nostromo’s exterior, its bridge and auto-doc, Cobb also designed the ship’s airlocks, cyro-tubes, corridors, bulkheads, an observation dome (not built), Ash’s ‘blister’ observation unit, some of the film’s uniform patches and ship signage, the ‘flying bedstead’ maintenance vehicle (not built), and even Jones’ cat-box. Cobb told Den of Geek that, “My problem with designing Nostromo’s interiors, the control bridge, corridors, auto doc (or med lab), bulkhead doors, the food deck, etc., was that I grew up with a deep fascination for astronomy, astrophysics, and most of all, aerospace flight. My design approach has always been that of a frustrated engineer (as well as a frustrated writer when it came to cinema design). I tend to subscribe to the idea that form follows function. If I’m to arrive at a cinematic spacecraft design that seamlessly preserves, as in this case, the drama of the script, the audience has to experience it as something impressive and believable.”“We’re beyond  in terms of scientific advances,” said Scott of ‘s futurism, “our capabilities are more sophisticated  but our ship’s still NASA-orientated, still Earth-manufactured … in our tongue-in-cheek fantasy we project a not-too-distant future in which there are many vehicles tramping around the universe on mining expeditions, erecting military installations, or whatever. At the culmination of many long voyages, each covering many years, these ships -no doubt part of armadas owned by private corporations- look used, beat-up, covered with graffiti, and uncomfortable. We certainly didn’t design the Nostromo to look like a hotel.”“I didn’t want a conventional shape [for the refinery,] so I drew up a sketch and handed it to the model makers. They refined it, as it were, and built the model. I originally drew it upside-down, with the vague idea that it would resemble a floating inverted cathedral … I think that the machine that they’re on could in fact be 60 years old and just added to over the decades. The metal-work on it could be 50 years old … I would have liked to see it covered with space barnacles or space seaweed, all clogged and choked up, but that was illogical as well.”
~ Ridley Scott, Fantastic Films, 1979.The Nostromo model was built under the supervision of Nick Allder and Brian Johnson at Bray Studios, not far from Pinewood, where the live-action scenes were being filmed in parallel with the model shots at Bray. For the refinery, Scott instructed the teams at Bray to make it appear “Victorian Gothic,” with towers and spires and antennae. Bray shop worker Dennis Lowe explained: “At that same time in the workshop Ridley was talking about his first concept of the refinery and he was describing an actual oil refinery with pipes and spires, eventually the term ‘Battleship Bismarck in space’ came up to describe the detailing of the model.”“I spent a couple of months rigging the Nostromo with neon strips and spotlights that would mimic the Mothership from . These were sequenced using motorised rotary switches, Ridley came over from Shepperton after shooting and took a look at my work then made the decision to scrap the idea – such is life!”~ Dennis Lowe.When Ridley arrived after concluding filming at Pinewood, he further revised the ship’s look, removing many of the spires from the refinery, repainting the Nostromo from yellow to grey, and scrapping every piece of footage shot to date, taking it upon himself to re-direct the scenes. “It was a difficult situation,” said Scott, “Brian Johnson was over there [at Bray], working out of context away from the main unit. I could only look at the rushes while I was working with the actors, and that’s not a very satisfactory way of working. In the end, I think a director must be heavily involved with the miniatures, and that’s why I shot them myself.”According to model builder Jon Sorensen, there were no real hard feelings over the redesigns and reshoots. “Ridley Scott then arrived from Shepperton to take an interest in the models and everything changed radically in terms of tone, colour and look. The yellow was sprayed over a uniform grey. Sections were rebuilt. We started over, discarding all previous footage. There was no anger at this. Surprise maybe. But it was Ridley Scott’s film. We liked him. So we entered the  model shoot Part Deux. I recall Bill Pearson and I talking once on what we thought was an empty, lunch-time model stage when a voice spoke from the shadows. Ridley, asking what we were discussing. We answered that maybe that part might look better moved over to there, (we were discussing the refinery). He smiled back and I guess that signalled what was true; we’d go all the way to help him. That night he bought both Bill and I a beer, a move which astonished the Assistant Director, Ray Beckett who complained that in 10 years of working with Ridley, he’d never been bought a beer. So we bought Ray one instead.”Early shot of the yellow Nostromo approaching the alien planet.The revised Nostromo hanging in orbit.The Nostromo interiors were overseen by art director Roger Christian, who had helped craft the sets for . Christian told Shadowlocked.com: “I art-directed  for Ridley Scott with my team because he was struggling to get the designer and the art department to understand ‘that look’ I created with the dressing on  I went into Shepperton, and we built and dressed the first corridor section – actually for a test screen for Sigourney Weaver, who the studios were not sure about. I brought my little team of prop guys who’d understood then the process of what to strip down and how to place it. Because it was not something you just do randomly. It had to be done based on a kind of knowledge.”“Roger is a brilliant set dresser,” Scott told Fantastic Films. “Though his department was not designing the corridors and sets, their ‘cladding’ of the walls made everything look absolutely real. He would go out with his buyers and prop men and visit aircraft dumps or army surplus stores and drag masses of things in for me to see.”“With  I was able to go much further with the oily and gritty look than in ,” said Roger Christian, “and for the first time create a totally believable ‘space truck’, as Ridley described it. The set ended up looking as if we had rented a well-travelled, well-used, oily, dirty, mineral carrier – an unmistakably real and claustrophobic space vessel. I think this really helped audiences to identify with the movie, as the characters were so like space truckers, trapped in a claustrophobic nightmare.”“[The Nostromo’s] like the bloody Queen Mary. Do you get a sense of scale in the interior? That it’s big? We couldn’t build the two to three-hundred foot-long corridors which it would have but it’s supposed to be like one of these huge Japanese super-tankers. Three quarters of a mile long. The refinery behind it god-knows how big. I mean… I dunno. A mile square?”
~ Ridley Scott, Fantastic Films, 1979.“Ridley saw the ship very much as a metaphor for a Gothic castle,” said Ron Cobb on the subject of the ship’s interiors, “or a WWII submarine … a kind of retro, accessible technology with great big transistors and very low-res video screens.” However, at one point, Scott had other ideas for the Nostromo’s technology: “I wanted to have wafer-thin screens that are plexiglas, that just float on clips -and of course today you’ve got computer screens exactly like that- because I figured that’s where it [technology] would go. I really got those things off Jean Giraud, Moebius, when he’d been drawing and speculating. A lot of his stuff you see thirty years ago is now.”Cobb acknowledged the Moebius influence, as well as the ship’s other, perhaps subtler, inspirations: “The ship is a strange mixture of retrofitted old technology, a kind of industrial nightmare, like being trapped in a factory … Ridley’s a wonderful artist and he wanted it to look a lot like a Moebius-designed ship, with all kinds of rounds surfaces and with an Egyptian motif.” This Egyptian motif is prevalent in the Weylan-Yutani logo, a wings of Horus design which adorns the uniforms of the crew in addition to their coffee cups, beer cans, etc. The hypersleep chamber also evokes a burial chamber, with the cryo-chambers arranged in a lotus shape. In addition to the Egyptian motif, another influence was Japan. “The owners of the Nostromo are Japanese,” Scott told Fantastic Films.“The interior of the Nostromo was so believable,” HR Giger told Famous Monsters, “I hate these new-looking spacecraft. You feel like they’re just built for the movie you’re seeing. They don’t look real.”“As I was working with the art director,” said Ridley, “I decided to make it faintly glittery. I wanted to have sort of anodized gold everywhere. Not steel, gold. Did you know that space landing craft are covered with gold foil? Amazing! So I thought, Why make this out of steel? Let’s make it all warm and oppressive, massive, and gold.'”The glittery look can be seen in the opening shots of the ship’s computers bleeping into life, and the gold sheen is most prevalent in the ship’s maintenance area, where Brett finds the Alien’s discarded skin moments before his death. Scott explained the design process for the ship’s golden-hued maintenance garage: “We got hold of marvelous, actual parts of actual huge jet engines and installed them, and they’re like a coppery metal with some steel. We used them as four main supports, like columns, and they give a lot of the feeling of a temple. We played the same music we used in the derelict alien craft and we had  temples. The idol I wanted was through these massive gold doors which were as big as a wall, with a gap in them through which the claw [landing leg] can be seen. When that set was dressed, it looked like Aladdin’s Cave … [the garage is] filled with the equipment that the crew would use in their work on and around the refinery, and when they land on various planets – land crawlers, helicopters, other flying machines.”“Ridley has this lavish, sensual visual style,” summarised Dan O’Bannon to Fantastic Films in 1979, “and I think that Ridley is one of the ‘good guys.’ I really think that he was the final pivot point responsible for the picture coming out good.  And so a lot of the visual design and a lot of the mood elements inherent in the camerawork, while they’re not what I planned, are great.  They’re just different.”O’Bannon also nodded to the contributions of Cobb, Foss, Shusett etc., to the picture: “Also, it’s not 100% Ridley either. It’s Ridley superimposing his vision over the cumulative vision of others, you see.  Now this could be such a strong director’s picture because Ridley’s directorial and visual hand is so strong.  There will probably be tendency among critics to refer to it as Ridley Scott’s vision of the future.  And he did have a vision of the future.  But it was everybody else that came before, that’s what his vision is … if it sounds like I’m knocking Ridley, I’m not.”The Nostromo at rest on the alien planetoid.]]></content:encoded></item><item><title>CS234: Reinforcement Learning Winter 2025</title><link>https://web.stanford.edu/class/cs234/</link><author>jonbaer</author><category>hn</category><pubDate>Wed, 26 Nov 2025 00:33:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
					I care about academic collaboration and misconduct because it is important both that we are able to evaluate
					your own work (independent of your peer’s)
					and because not claiming others’ work as your own is an important part of integrity in your future career. I
					understand that different
					institutions and locations can have different definitions of what forms of collaborative behavior is
					considered
					acceptable. In this class,
					for written homework problems, you are welcome to discuss ideas with others, but you are expected to write up
					your own solutions
					independently (without referring to another’s solutions). For coding, you may only share the input-output behavior
					of your programs. This encourages you to work separately but share ideas
					on how to test your implementation. 
				

Please remember that if you share your solution with another student, even
					if you did not copy from
					another, you are still violating the honor code.

Consistent with this, it is also considered an honor code violation if you make your assignment solutions publicly available, such as posting them online or in a 
public git repo.


					
					We may run similarity-detection software over all submitted student programs, including programs from
					past quarters and any
					solutions found online on public websites. Anyone violating the Stanford University
					Honor Code will be referred to the
					Office of Judicial Affairs.
					If you think you made a mistake (it can happen, especially under stress or when time is short!), please reach
					out to Emma or the head CA;
					the consequences will be much less severe than if we approach you.

					We expect all students to submit their own solutions to CS234 homeworks, exams and quizzes, and 
					for projects. You are permitted to use generative AI tools such as Gemini, GPT-4 and Co-Pilot 
					in the same way that human collaboration is considered acceptable: you are not allowed to directly 
					ask for solutions or copy code, and you should indicate if you have used generative AI tools. 
					Similar to human collaboration help, you are ultimately responsible and accountable for your own work.
					We may check students' homework, exams and projects to 
				enforce this policy. 
				Note that it is not acceptable to list a LLM as a collaborator on the project 
					milestone or final report: as things stand, generative AI cannot accept 
					fault or responsibility, and thus cannot be a collaborator in a final project. 


				]]></content:encoded></item><item><title>The Generative Burrito Test</title><link>https://www.generativist.com/notes/2025/Nov/25/generative-burrito-test.html</link><author>pathdependent</author><category>hn</category><pubDate>Tue, 25 Nov 2025 23:28:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Also, I was initially surprised that it couldn't replicate the image well because I assumed there would be plenty of similar examples in the training data (unlike said equestrian absurdity). But I think it's a bit of a weird concept because all the ingredients get smushed and smashed and congealed.All images generated using fal defaults. Obviously you can probably prompt it better, but that's HIL effort, and feels like cheating.]]></content:encoded></item><item><title>Reinventing how .NET builds and ships (again)</title><link>https://devblogs.microsoft.com/dotnet/reinventing-how-dotnet-builds-and-ships-again/</link><author>IcyWindows</author><category>hn</category><pubDate>Tue, 25 Nov 2025 22:37:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[After I wrote my last post on how .NET builds and ships, I was cautiously optimistic that I wouldn’t be writing another one. Or at least not another one about how we build and ship. That problem was done and dusted. .NET had done it! We’d struck a balance between distributed repository development and the ability to quickly compose a product for shipping. Congratulations everyone, now the infrastructure teams could focus on other things. Security, cross-company standardization, support for building new product features. All the good stuff.…A year and a half later…We’re asking how much it will cost to build 3-4 major versions with a dozen .NET SDK bands between them each month. And keep their engineering systems up to date. And hey, there’s this late breaking fix we want to get into next week’s release, so can I check it in today and have the team validate tonight? It can’t be  hard, right? And I have this new cross-stack feature that I want to do some prototyping on…how can I build it?The answers were mostly frustrating:“It’ll cost a lot, and get worse over time.““I don’t think we have enough time for that fix, I can only guess how long the build will take, but it’s at least 36 hours before we can handoff to validation. Maybe more?““I’m sure we can keep that much infrastructure alive, but we’ll slowly drown under the cost of keeping it up to date.““How critical is it that you have a full stack to work with? It’ll take a while to set that up.“These are  the answers we want to be giving. And so, we went back to the drawing board, looking for solutions.This blog post is about the Unified Build project: .NET’s effort to resolve many of these issues by moving product construction into a ‘virtual monolithic’ repository, consolidating the build into a series of ‘vertical builds’, while still enabling contributors to work outside the monolith. I’ll briefly tell the story of our product construction journey over the life of .NET. I’ll draw attention to the lessons we’ve learned about applying a distributed product construction model to a single product, particularly its drawbacks in overhead and complexity. Finally, I’ll dig into the details of Unified Build and its foundational technology, Linux distro Source Build. We’ll look at the new method of product construction and the results we’re seeing.How did we get here? This is not my beautiful build infrastructure.NET was born out of the closed source infrastructure of the .NET Framework and Silverlight in 2015-2016. It was made open source incrementally as we readied its components for external consumption, and as was the fashion at the time, we split it into multiple repositories. CoreCLR represented the base runtime, CoreFX the libraries, Core-Setup the packaging and installation. Along came ASP.NET Core and EntityFramework Core, and an SDK with a CLI. A few releases saw major revamps of the product in the form of shared frameworks, with WindowsDesktop joining the fold. More repositories and more complexity.What is important to understand is that .NET is a product that is developed in separate inter-dependent repositories but needs to be composed together in a relatively short period of time to ship. On paper, the ‘graph’ of the product looks much like any open source ecosystem. A repository produces some software component, publishes it to public registries, and downstream consumers take a dependency on the new component, and publish their own updates. It’s a producer-consumer model where changes ripple through the ‘global’ dependency graph via a series of pull->build->publish operations. This model is highly distributed and effective, but it is not necessarily efficient in a time sense. It enables software vendors and repository owners to have significant autonomy over their process and schedules. However, attempting to apply this methodology to a product like .NET, which represents its components using separate, but inter-dependent repositories, has major drawbacks.Let’s call this a “distributed product construction methodology”. To get a sense of why it can be a difficult methodology to use, let’s take a look at the process to produce a security release.Example: Security ServicingConsider shipping a security patch. A security vulnerability is discovered somewhere in the .NET Runtime libraries. Because .NET is descended from .NET Framework, let’s say this security vulnerability is also present in .NET Framework 4.7.2. It becomes absolutely vital that .NET’s security update goes out in tandem with the .NET Framework update, or one will zero-day the other. .NET has numerous Microsoft-managed release paths. Microsoft Update, our CDN, Linux and container package registries, nuget.org, Visual Studio, Azure Marketplace, and on and on. That puts some restrictions on timeline. We need to be able to be predictable..NET’s development structure looks a lot like a typical open source ecosystem. The .NET Runtime, the .NET SDK, ASP.NET Core and the WindowsDesktop shared framework are developed by different teams, though with a huge amount of cross-collaboration. They are developed, at times, like independent products. The .NET Runtime forms the base of the product. ASP.NET Core and WindowsDesktop are built on top of that. A huge quantity of the dev tooling (C#, F#, MSBuild) is built on top of the surface area of the .NET Runtime and some auxiliary libraries. The SDK gathers up and builds a CLI, along with tasks, targets and tooling. Much of the shared framework and tooling content is redistributed in-box.To build and ship this security patch, we need coordination between the many teams that contribute to the .NET product as a whole. We need the lowest levels of the .NET graph (see below) to build their assets, then feed them downstream to consumers. They need take the update, build, and feed downstream. This will happen continually until the product is “coherent”; no new changes are being fed into the graph and everyone agrees on a single version of each component in the product. Coherency ensures that a component with changes is ingested everywhere that redistributes the component, or information about it. Then, we want to do our validation, take all the shippable assets from the closure of all those unreleased components, and then release them all at once to the world.This is a lot of moving parts that need to work well together in a short period of time.Advantages and Disadvantages of Distributes EcosystemsIt’s important to note that this distributed ecosystem style of development  have a lot of advantages: – Repository boundaries tend to encourage layering and less tightly bound products. During the major version development lifecycle, the individual components of the stack generally remain roughly compatible, even as changes flow quickly and unevenly through the graph. – Repository boundaries tend to encourage good, focused communities. The WPF and Winforms communities, for instance, are often distinct. Small repos are also generally more approachable., – Distributed development often allows for incremental changes. For instance, we can make breaking changes to the System.CommandLine surface area, then ingest those in the consumers over time. This doesn’t work all the time (e.g. let’s say the SDK is attempting to ship just one copy of System.Text.Json for all of the tooling to use, but not every consumer agrees on that surface area. Boom?!), but it’s reasonably reliable. – Smaller, focused repositories tend to have better inner-loop experiences. Even something as simple as  or  is faster in a small repository. The repository boundary tends to give the (possibly illusory) sense that for your change, you only need to worry about the code and tests you can see. – Incrementality helps development be more asynchronous. If my component flows to three downstream consumers who work in three different time zones, those teams can make progress on their own components in their own time, rather than needing to coordinate.Low-Cost Sharding/Incremental Builds – Distributed development allows for ‘optimizing’ away builds of components that don’t change every often and are at the fringes of a dependency graph. For instance, a leaf node that builds some static test assets doesn’t need to be rebuilt every time there is a change to the sdk. The last built assets are just fine.If you squint and peer between the lines here though, a lot of the advantages of the distributed model are its significant weaknesses when we need to build and ship software that requires changes in a significant portion of the graph to be completed in a short period of time. Changes at scale across large graphs are often slow and unpredictable. But why? Is there something inherently wrong with this model? Not really. In typical OSS ecosystems (e.g. NuGet or NodeJS package ecosystems), these aspects are often . These ecosystems do not optimize for speed or predictability. Instead, they value the autonomy of each node. Each node needs only to concern itself with what it needs to  and what it needs to  and the changes required to meet those needs. However, when we attempt to apply the distributed model to shipping software quickly, we often struggle because it increases the prevalence of two key concepts, which I’m calling Product Construction Complexity and Product Construction Overhead. Together these combine to slow us down and make us less predictable.Product Construction ComplexityIn the context of product construction, ‘complexity’ refers to the quantity of steps that are required for a change to go from a developer’s machine to that change being delivered to customers in all the ways that it needs to be delivered. I recognize that this is a fairly abstract definition. “Step” could mean different things depending on what level of granularity you want to look at. For now, let’s focus on conceptual product construction steps, as shown in the example graph below:.NET began with a relatively simple product dependency graph and matching tools to manage that graph. As it grew, new repositories were added to the graph and additional dependency flow was required to construct the product. The graph grew more complex. We invented new tools (Maestro, our dependency flow system) to manage it. It was now easier than ever to add new dependencies. A developer or team looking to add new functionality to the product could often just create a new repository and build and set up the inputs and outputs. They only needed to know how that component fit within a small subsection of the larger product construction graph in order to add a new node. However, .NET doesn’t ship each individual unit independently. The product must become “coherent”, where everyone agrees on the versions of their dependencies, in order to ship. Dependencies or metadata about them are redistributed. You have to “visit” all of the edges. Note: While we do not need to rev every component in the graph, there is a significant portion that changes on every release, either due to fixes or dependency flow. Then you take the outputs of each individual node, combine them all together, and out the door you go.More complex graphs have significant downsides:The more edges and nodes, the longer it tends to take to achieve coherency.Teams are more likely to make a mistake. There are more coordination points, and more points in the workflow where a human can influence an outcome. Tools can help, but they only go so far.Complexity can also encourage variance in build environment and requirements. It’s hard to keep everyone aligned on the same processes as teams move and upgrade at different rates. Reproducing that full set of environments can be expensive, and that cost tends to increase over time as infrastructure “rots”.Product Construction OverheadWe define overhead as “the amount of time spent not actively producing artifacts that we can ship to customers“. Like complexity, it can be evaluated on a different level of granularity depending on how detailed you want to get. Let’s take a look at two quick examples, and then at the overhead in one of .NET’s older builds.A simple multi-repo product construction process might look like the following:In the above graph, the overhead nodes (dotted nodes) do not actively contribute to the production of the packages in D. The time it takes the dependency flow service to create the PR is overhead. Waiting for a dev to notice and review the PR is overhead. Waiting for approval for package push is overhead. That’s not to say that these steps aren’t , just that they are places where we say we’re not actively creating outputs for customers.How about builds? If we zoom into a repository build process, we can often see quite a lot of overhead. Consider this very simple build:There are a few interesting measures of overhead in a system. We can measure it a % of overall time. Add up the time spent in each step based on its classification, then divide the total overhead by the total time. This gives a nice measure of overall resource efficiency. However, from a wall clock perspective, overall overhead doesn’t tell us much. To understand overhead’s effect on the end-to-end time, we find the longest path by time through our product construction graph, then compute the total overhead in steps that contribute to that path as compared to the total time in the path.To understand what that overhead might look like in a single .NET build, let’s take a look at an 8.0 build of runtime. This data was generated using a custom tool that can evaluate an Azure DevOps build based on a set of patterns that classify each step.Percentage of overall build timeHere are the three longest paths from that build:(Stage) Build->Mono browser AOT offsets->windows-x64 release CrossAOT_Mono crossaot->Build Workloads->(Stage) Prepare for Publish->Prepare Signed Artifacts->Publish Assets(Stage) Build->windows-arm64 release CoreCLR ->Build Workloads->(Stage) Prepare for Publish->Prepare Signed Artifacts->Publish Assets(Stage) Build->Mono android AOT offsets->windows-x64 release CrossAOT_Mono crossaot->Build Workloads->(Stage) Prepare for Publish->Prepare Signed Artifacts->Publish AssetsOverhead + Complexity = TimeOverhead is unavoidable. There is some level inherent in every product construction process. However, when we add complexity to our product construction processes, especially complexity in the graph, the overhead tends to begin to dominate the process. It sort of multiplies. Rather than paying the machine queue time cost one time, you might pay it 10 times over within a single path through the graph. After those machines are allocated, you then clone the repo each time. The efficiency scaling of these steps tends to also be worse because there is some fixed cost associated with each one. For instance, if it takes 10 seconds to scan 10MB of artifacts, and 1 second to prepare for the scan, collate and upload the results, it takes longer to do that step 10 times in a row than it does to scan the full 100MB at once. 110 vs. 101 seconds.What is also insidious is that this cost tends to hide and increase over time. It’s not always obvious. A local repository build for a developer is typically fast. The developer does not see any overhead of the overall CI system in that build. Zooming out, building the repository in a job in a pipeline can be similarly quick, but starts to incur some overhead. You have the quick build of that repository, but extra overhead steps around it. You’re still reasonably efficient though. Then let’s say you zoom out a little and you have some additional jobs in that pipeline, doing other things. Maybe reusing artifacts from other parts of the build, building containers, etc. Overhead will start to become a larger overall % of the long path time. Now, zoom out again, and you’re looking at the place of that pipeline and associated repositories in context of your larger product construction. You add in time for dev PR approvals, dependency flow systems to do their work, more cloning, more building, more compliance, more more more.In a distributed product construction system, decisions that affect complexity, and therefore overhead, can be made at a level that does not see the overall overhead in the system. A new node is added. In isolation, it’s fine. In context, it costs.While no graph of complexity was ever made for the .NET 8 timeframe that could show the complexity of each individual component build in context of the whole product construction graph, consider what the job graph for the runtime build alone looked like. Each bubble below represents a separate machine.The roots of Unified Build in Source Build.NET Source Build is a way that Linux distributions can build .NET in an isolated environment from a single, unified source layout. Microsoft started working on it around .NET Core 1.1. The spiritual roots of Unified Build grew from hallway conversations between the team working on .NET Source Build and the team responsible for the Microsoft distribution. I won’t say it wasn’t in jealousy that the infrastructure teams often looked at how long it took to build the .NET product within the Source Build infrastructure.  Shorter than it took to build just the runtime repository from scratch in its official CI build. Now granted, it wasn’t exactly an apples-to-apples comparison. After all, Source Build:Only builds one platform.Doesn’t build any of the Windows-only assets (e.g. WindowsDesktop shared framework)/Doesn’t build .NET workloads.Doesn’t do any installer packaging.Doesn’t build the tests by defaultAll very reasonable caveats. But enough caveats to add up to 10s of hours in differences in build time?  Much more likely is that the Source Build methodology is  and . More than just time, there were other obvious benefits. Unified toolsets, easier cross-stack development, and perhaps most importantly, hard guarantees of what was being built and its build-time dependencies.Back to those hallway conversations. Source Build’s obvious benefits led to occasional probing questions from various members of the .NET team. Most of the form: So…why doesn’t Microsoft build its distribution that way? Answer: It’s hard.Why is it hard? A detour into the land of Source BuildMicrosoft began efforts to make Source Build a ‘real’ piece of machinery around the .NET 3.1 timeframe. Prior to this point, the Source Build distribution tended to look more like a one-off effort for each .NET major release. It was too difficult to keep working all the time, so the team worked, starting in the spring as the new product took shape, to bring the new .NET version into line with Linux distro maintainer requirements. To understand why it’s so hard to fit Microsoft’s distribution of .NET into this model as part of the Unified Build project, let’s look back into why it was so hard to get the Source Build project into a turn crank state in the first place.To allow our distro partners to distribute .NET we needed to produce an infrastructure system that produced a .NET SDK within the following constraints:Single implementation! – Only one implementation per componentSingle platform – Only build for one platform (the one that distro partners are trying to ship)Single build – Only build on one machine. We can’t require a complex orchestration infrastructure.Linux Distro Build RequirementsLinux distros generally have stricter rules and less flexibility when building software that will go into their package feeds. The build is usually completed offline (disconnected from the internet). It may only use as inputs artifacts that have been previously created in that build system. Checked-in binaries are not allowed (though they can be eliminated at build time). Any source in the repository must meet strict licensing requirements.  At a conceptual level, a Linux distro partner wants to be able to trace every artifact they ship to a set of sources and processes that they can reasonably edit. All future software should be built from previously Source Build produced artifacts. .Single Build – A repo and orchestration framework to stitch the stack togetherAs you’ve learned earlier, the .NET build, like many products, is actually comprised of the Azure DevOps builds of various components, stitched together with dependency updates. This means that the information and mechanics required to construct the product is distributed between the repositories (build logic within the build system and associated scripting, as well as YAML files processed by Azure DevOps) and the dependency flow information held by our ‘Maestro’ system (producer-consumer information). This isn’t usable for our Linux distro partners. They need to be able to build the product without access to these Microsoft resources. And they need to be able to do so in a way that is practical for their environments. Manually stitching together a product from a build graph isn’t reasonable. We need an orchestrator that encapsulates that information.The Source Build layout and orchestratorThe orchestrator replaces the tasks that Azure DevOps and Maestro perform for .NET’s distributed build with ones that can be run from a single source layout, disconnected from the internet. You can see the modern, updated layout and orchestrator over at dotnet/dotnet. – A single source layout with a copy of all components required to build the product. Submodules are flattened, if they exist (typically for external OSS components). The contents of the source layout are determined by identifying an annotated dependency for each component within the product graph, rooted at dotnet/sdk. The sha for that annotated dependency determines what content will populate the layout. Note: dependencies like compilers and OS libs are provided by the build environment.Information on how each component should be built, and its dependencies – For each of the components within the single source layout, a basic project is provided which identifies how the component is built. In addition, the component level dependencies are also identified. i.e. the .NET Runtime needs to be built before ASP.NET Core can start.<ItemGroup>
<RepositoryReference Include="arcade" />
<RepositoryReference Include="runtime" />
<RepositoryReference Include="xdt" />
</ItemGroup> – The build orchestrator logic is responsible for launching each build in the graph when it is ready (any dependencies have been successfully built), as well as inputs and outputs of each component. After a component build has been completed, the orchestrator is responsible for identifying the outputs and preparing inputs for downstream component builds. Think of this as a local Dependabot, computing the intersection of the declared input repositories against the package level dependency info (see aspnetcore’s) for an example. More information on how dependency tracking works in .NET builds can be found in my previous blog post. – The comparatively stricter environments that our Linux distro partners build in mean that it’s necessary that we build some automation to identify potential problems. The orchestrator can identify pre-built binary inputs, ‘poison’ leaks (previously source-built assets appearing in the current build outputs), and other hazards that might block our partners. – Most of our test logic remains in the individual repositories (more on that later), but the layout also includes smoke tests.Single Implementation – Pre-built squeaky cleanThere are some obvious and non-obvious reasons why these requirements would be hard to meet using the ‘stock’ Microsoft build of .NET, and why Source Build required so much work. An offline build with pre-staged, identified inputs that are  is a major undertaking. When the Source Build team began to investigate what this meant, it was quickly obvious that a LOT of interesting behavior was hiding in the .NET product build. Sure, binary inputs like optimization data were obviously disallowed, but some other foundational assets like .NET Framework and NETStandard targeting packs were also not buildable from source. Either they weren’t open source in the first place, or they hadn’t been built in years. More concerning, the graph-like nature of .NET means that incoherency is very common. Some of this incoherency is undesirable (the kind we attempt to eliminate during our product construction process). Some of it is expected and even desired.Example: Microsoft.CodeAnalysis.CSharpAs an example, let’s take a look at the C# compiler analyzers, which are built in the dotnet/roslyn repository. The analyzers will reference various versions of the Microsoft.CodeAnalysis.CSharp package depending on the required surface area to ensure that a shipped analyzer runs all of the versions of Visual Studio and the .NET SDK that it is required to support. They reference a minimum possible version. This ensures that analyzers can be serviced in a sustainable fashion, rather than shipping a different version of an analyzer for every possible VS or SDK configuration.Because multiple versions of the surface area are referenced, multiple versions of Microsoft.CodeAnalysis.CSharp are restored during the build. That would mean, for the purposes of Source Build, we need to build each and every one of those versions of Microsoft.CodeAnalysis.CSharp at some point. We have two ways to do this:Multi-version source layout – Place multiple copies of dotnet/roslyn into the shared source layout, one for each referenced Microsoft.CodeAnalysis.CSharp version based on when it was originally produced. This is not only expensive in build time, but it tends to be somewhat viral. If you have 3 versions of dotnet/roslyn you need to build, you need to ensure that the transitive dependencies of those 3 versions are also present in the shared layout. The maintenance complexity of this setup goes up very quickly. These are previously shipped versions of the dotnet/roslyn source base. It will be necessary to maintain security and compliance of those codebases over time. Upgrading build-time dependencies. Removing EOL infrastructure, etc.Require previously source-built versions to be available – This is really just a flavor of the multi-version source layout with an element of “caching”. If a distro maintainer needs to rebuild the product from scratch, or if a new Linux distribution is being bootstrapped, they might need to reconstruct decent portion of .NET’s past releases just to get the latest one to build in a compliant fashion. And if those old versions require changes to build in a compliant fashion, you’re again in a maintenance headache.Source Build Reference PackagesThere are numerous other examples like Microsoft.CodeAnalysis.CSharp. Any time a project targets a down-level target framework (e.g. net9 in the net10 build), the down-level reference pack is restored. SDK tooling (compilers, MSBuild) targets versions of common .NET packages that match the version shipped with Visual Studio. So how do we deal with this? We cannot simply unify on a single version of every component referenced within the product without fundamentally changing the product.The Source Build team realized that a lot of this usage fit neatly into a class of “reference-only” packages.The targeting packs restored by the SDK when a project builds against a TFM that does not match the SDK’s major version (e.g. targeting net9 with a net10 SDK) do not contain implementation.The reference to older versions of Microsoft.CodeAnalysis.CSharp are  only. No assets are  from these packages. If the implementation is not needed, a reference-only package can be substituted.Enter dotnet/source-build-reference-packages. A reference-only package is significantly simpler to create and build, and it meets the needs of the consumers in the build. We can generate reference package sources for packages where we do not need the implementation, then create an infrastructure to store, build and make them available during the Source Build process. Providing multiple versions is relatively trivial. The dotnet/source-build-reference-packages repository is built during the .NET build, and then consuming components restore and compile against provided reference surface area.What about all those non-reference cases?With a solution to reference packages, we can turn our attention to other inputs that are not Source Build compliant and do not fall into the ‘reference’ category. There are three major sets:Closed source or inputs that cannot be built from source – Optimization data, Visual Studio integration packages, internal infrastructure dependencies, etc.Legacy – Open source dependencies on implementation built in older versions of .NET.Joins – Open source dependencies on implementation built on other platforms.Let’s take a look at how we deal with these cases.Closed Source/Non-Source Buildable InputsClosed source or any inputs that cannot be built from source aren’t allowable in the Linux distro maintainer builds, full stop. To resolve these cases, we analyze each usage to determine what to do. Remember that our goal is to provide a compliant build implementation for use by our distro partners, which is functionally as close to what Microsoft ships as is possible. i.e. we don’t want Microsoft’s Linux x64 SDK to  in substantially different ways from RedHat’s Linux x64 SDK. This means that the runtime and sdk layouts for Linux x64 need to be as close as possible. The good news is that quite a lot of the closed source usage isn’t required to produce functionally equivalent assets. Examples:We might restore a package that enables signing, something not required in a distro partner buildThe dotnet/roslyn repository builds components that power Visual Studio. These components have dependencies on Visual Studio packages that define the IDE integration surface area. However, this IDE integration doesn’t ship in the .NET SDK. This functionality could be “trimmed away” in Source Build by tweaking the build. This is reasonably common.If dependencies couldn’t be trimmed away without altering product functionality, we have a few additional options:Open source the dependency – Often times, a closed source component, or at least a key portion of a closed source component required to satisfy a scenario, can be open sourced. – Sometimes, the team can work to remove the product differences with intentional design changes. Remember that the important part is that everything that ships on distro partner package feeds needs to be built from source. This allows for some assets to be brought in dynamically. Think of this like the NPM package ecosystem vs. the NPM package manager. A distro might build the NPM package manager from source. This leaves users to dynamically restore NPM packages at build time.Live with slightly different behavior – These cases are few and far between. Prior to .NET 10, the WinForms and WPF project templates and WindowsDesktop were not included in the source-built Linux SDK, despite being available in Microsoft’s Linux distribution. This was due to the difficulty in building the required portions of those repositories on non-Windows platforms.We’ve discussed what we can do with closed source and non-reproducible dependencies. What about legacy dependencies? First, what do we mean by ‘legacy’ dependency? As detailed in earlier discussion, there is quite a lot of ‘incoherency’ in the product. A project might build for multiple target frameworks, redistributing assets from older versions of .NET. This is all to support valuable customer scenarios. But building all the versions of these components isn’t feasible. This is where our  rule comes into play. We choose a single version of each component to build and ship with the product. We do allow for  to old versions, via dotnet/source-build-reference-packages, but relying on older implementations are off limits.First, we look for a way to avoid the dependency. Is it needed for the Linux SDK we’re trying to produce? If not, we can eliminate that code path from the build. If so, is there an opportunity to unify on the single implementation? In a lot of cases, incoherency is just a result of the product components moving their dependencies forward at different rates. If all else fails, we could explore compromises that involve behavioral differences, but we want to avoid this as much as possible.Joins are the last major category of pre-builts to remove. They occur because we end up with intra-product dependencies that are built in another environment. For example, I might be running a build on Windows that creates a NuGet package for a global tool, but to build that NuGet package I need the native shim executables Mac and Linux and Windows. Those shims can only (reasonably) be built in the Mac and Linux host environments. These types of dependencies are indicative of a product build that is more ‘woven’ than ‘vertical’ and tend to naturally emerge over time in a multi-repo product construction graph. Each edge in that graph represents a sequence point where all the outputs of earlier nodes are available, regardless of where they were built. If a dependency can be taken, it will be taken.However, the distro partner builds need to be single platform  single invocation to fit into distro partner requirements. Bootstrapping notwithstanding, they want to pull in the dependencies, disconnect the machine from the network, and hit build. At the end, out pops a bright new .NET SDK. Cross-platform dependencies preclude any such behavior. They block “build verticality”. Remember joins. We’ll need to come back to them later when we start implementing Unified Build for Microsoft based on the Source Build model.For Source Build, we again deal with joins a bit like legacy dependencies. The key aspect to remember is that Source Build is narrowly focused on producing a .NET SDK and associated runtimes in the Linux distro partner build environments. So, we eliminate dependencies where possible (e.g. we don’t need to package Windows global tool executable stubs when running the SDK on Linux) and redesign the product or product construction process as necessary to meet requirements (e.g. .NET Workload manifests).The Vision – Dreaming up Unified BuildUnified Build seeks to apply the general principles of our Linux distro partner Source Build to the product that Microsoft ships. Achieving this would result in big wins for Linux distro partners, upstream contributors and Microsoft, reducing maintenance costs and improving the ability to build and ship quickly. Although we knew from the outset that we likely can’t exactly match the exact Linux distro build approach without major changes in the product, we thought we could get close. .NET came up with the following high-level goals (Note, “.NET distro maintainers” refers to anyone building .NET, including Microsoft):A single git commit denotes all product source for a particular .NET build. All commits are coherentA single repo commit can produce a shippable build.NET’s build shall be able to create a specific platform’s distribution in a single build environment..NET distro maintainers shall be able to efficiently update and build .NET (both collaboratively and separately) through the entire lifecycle of a .NET version (first to last commit)..NET distro maintainers can produce downstream distributions without use of Microsoft provided services..NET distro maintainers shall be able to meet provenance and build environment requirements for their distributions..NET distro maintainers shall be able to coordinate patching of downstream distributions..NET distro maintainers can run verification tests against the built product..NET contributors shall be able to easily produce full product builds for testing, experimentation, etc..NET contributors shall be able to work efficiently on the section of the product for which they are concerned.Still, getting there would require solving a mountain of new problems. Let’s take a look at some of the problems we need to solve before we can use Source Build as Microsoft’s .NET build.Provide a way to determine what makes it into the productWhen you construct a product using the distributed model, the  of the product, the  of the product and the determination of what actually  the product are all tied together. Source Build operates on a flattened source layout based on a final coherent graph. However, it relies on the traditional .NET product construction process in order to determine what versions of each component show up in the layout. To get the full benefit we need a way to directly update components within the shared source base without complex dependency flow. Otherwise, if a developer wants to make a change in runtime, they will end up building the product twice. Once to flow the runtime build with their change through all paths that runtime reaches, then once again to build the product using that new runtime.Provide a way to react to breaking changesThe flat flow significantly reduces the number of hops, and therefore the complexity and overhead in the process of a change making its way into the shared source layout. And we can see that before a change makes it into the product; it will still get PR validation and possibly some more in-depth rolling CI validation. However, let’s say that this change requires reaction in consuming components. Despite the change in dependency flow to a flat flow, ASP.NET Core still depends on .NET Runtime. And ASP.NET Core’s code in the layout doesn’t know about the new runtime change. Whatever PR validation we have before a change is allowed in the shared source layout is sure to fail.In a traditional dependency flow system, we handle this by making changes in the dependency update PR. If an API is changed, the build breaks. A dev makes a change in the PR (ideally), validation is green, and the PR is merged. For the single-source methodology to work for .NET, we’ll need to be able to make changes to the source of  components in the dotnet/runtime update PR.Provide a way to validate against repository infrastructureAs we discussed earlier, a large quantity of critical validation lives at the component repository level. That’s where the developers spend their time. Moving or copying all of this is probably wasteful, definitely expensive, and likely hard to maintain. If we can’t rely on the dependency flow to do the validation before components flow into the shared source layout, we’ll need a way to do so after.To solve our problem, we could have all the outputs of a new product builds flow  into the individual repositories, matching with the dependencies in their  files. That means dotnet/aspnetcore will get a bunch of new .NET Runtime packages, dotnet/sdk will get a bunch of newly built ASP.NET Core, .NET Runtime and Roslyn compiler packages, etc. They will be validating the ‘last built’ versions of their input dependencies against repository infrastructure.Provide two-way code flowLet’s say a runtime insertion PR changed the signature of an API in . When that forward flows, the responsible dev updates the signatures in all downstream users. Let’s say that’s code in  and . The new product is built, and the updated System.Text.Json package with the new API signature makes its way back to  and . The HEAD of  doesn’t have the source changes made directly in the shared layout forward flow PR. The dev would need to port those changes over, making changes in the backflow PR. This is tedious and error prone. Our new system will need to provide a way to automatically flow changes made in the shared layout back in the source repository.Provide better insertion time validationValidation on backflow isn’t perfect. It doesn’t provide an easy  gate for bad changes in dependent components. We can mitigate this by identifying and closing gaps in repo testing that allowed bad changes to be merged in the originating repo. We can also accept that some things will always slip through and that the process of creating a high-quality product isn’t just a green PR. Many repositories do not and cannot run their full testing suites prior to merging. However, we can  invest scenario testing run against the just-built product. This is something that our traditional dependency flow system is not good at.Any whole product scenario testing relies on dependency updates for components reaching the dotnet/sdk repository. Up to that point, we don’t have a complete .NET product that we can test. Any attempt is just some kind of “Frankenbuild”. Note: A lot of this end-to-end testing just comes in the form of dotnet/sdk’s repository-level PR/CI testing.. However, changes can take a while to move through the graph to the point there they take effect in a way that would be visible in testing.The Source Build methodology provides a full product build on each and every component change, regardless of where that component lives in the product construction graph. This means that we have the opportunity to create and run a comprehensive suite of testing on each of those insertions. That testing should be focused on covering wide swaths of product functionality. If this testing passes, there is a reasonable expectation that .NET is functioning in a way that makes it possible for development to make forward progress.Provide a way to build all of what .NET shipsThe Linux distro Source Build offering focuses narrowly on the assets in-box in the 1xx band SDK, ASP.NET Core, Runtime. It builds packages that support the creation of these layouts. As we saw earlier with prebuilt elimination, this narrow focus is necessary to be able to meet distro partner build requirements. If we want to build what Microsoft ships, we can’t have that narrow focus.Expanding our focus is straightforward in some areas and difficult in others. In some ways, we’re just relaxing restrictions and bringing more functionality back into the build. We need to allow for pre-built binaries (e.g. signing functionality) to be restored from feeds. We need to build all TFMs instead of trimming away .NET Framework targets. We’ll need to build components originally excluded from the souce build focused shared source layout, like Windows Desktop, Winforms, WPF, EMSDK, etc. What’s more difficult are joins. Recall that Linux distro Source Build is single layout, single machine, single invocation.  This suffices for producing the layout, but there are a good handful of other artifacts in .NET that require builds on multiple machines. Artifacts that break the single machine verticality concept.In an ideal world, we’d re-architect the product to avoid these joins. But it’s often hard to do so without customer compromise or driving complexity into the product itself. We can’t simplify the SDK without breaking customers, and this is hard to do, even across major versions, in an enterprise-grade product. Past decisions heavily influence future available choices. In the end, we’ll have to eliminate joins where we can via product construction practices. Any remaining joins will be something we have to live with. The build will have to be architected to run across multiple machines, via a series of build passes.Executing on the Vision – Shipping Unified BuildThe Unified Build project can roughly be divided into 4 phases:Initial brainstorming and design (.NET 7) – The initial design work on the Unified Build project began in early 2022 during the development of .NET 7 and took ~4 months to complete. The project got full approval to start later in 2022 with the intention of completion by .NET 9 RTM, with some key go/no-go points where we could bail and still have a net win on infrastructure.Foundational work (.NET 8) – The Unified Build project during .NET 8 was focused on foundational work to improve the sustainability of the Source Build infrastructure and building features that were required to support the weight of the full build. The investments were designed to be a net positive for .NET overall, even if it turned out that our proof-of-concept stage discovered some major unknown problem and we had to change direction.Vertical Build/Code Flow Exploration (Early .NET 9) – After the foundational work completed, we moved to implement a vertical build for each of the 3 major OS families: Mac, Windows, and Linux. The intention was to identify as many of the problems we would need to solve during our productization phase as possible. We were especially interested in finding any previously unknown product construction join points. At the same time, we did a much deeper investigation into the options for code flow and code management, eventually proving out and settling on the implementation listed below.Productization (Late .NET 9-.NET 10) – Final implementation started in earnest towards the end of .NET 9 after a spring+summer delay. As a result of the delay, the ship date was pushed back to .NET 10. This turned out to be a blessing in disguise. This bought us about 6 extra months of bake time and allowed us to use the Unified Build product construction process starting midway through the .NET 10 Preview/RC cycle (Preview 4). .NET Preview 4 shipped with the new build process, but on the old code flow. Preview 5 added the new code flow, and we never looked back. Further refinement in developer workflow, more bake time for the build and code flow process happened over subsequent months.And finally, after almost 4 years of dreaming and work, Unified Build shipped with .NET 10 RTM!Let’s take a look at the key components of the project.VMR – The Virtual Monolithic RepositoryThe dotnet/dotnet VMR, or “Virtual Monolithic Repository” forms the cornerstone of the Unified Build project. It is the source layout from which all of .NET is built, including by our Linux distro partners. It is the orchestrator. Functionally, it’s not much different from the source layout used prior to .NET 8.0. That layout has just been formalized into a git repository (vs. a source tarball). This is key, as it allows developers to work both in their individual component repository, where dev workflows might be very refined, as well as in the VMR when cross-cutting changes are necessary. .NET gets most of the benefits of the distributed repo world, without coherency problems.Vertical Build is .NET’s pivot to producing assets in a series of verticals. A vertical is defined as a single build command on a single machine that builds part of the .NET product without input from other verticals. Typically, we divide verticals up by the runtime that we’re trying to produce. For example, Windows x64 vs. MonoAOT vs. Linux arm64 vs. PGO profile Windows x86. Altogether there are 35-40 different verticals. We divide these into what we call “short stacks” and “tall stacks”. A short stack just builds the runtime. A tall stack builds all the way up through the SDK.The original vision was that if we joined together all the outputs from parallel verticals, we’d have everything .NET needed to ship. Such a setup would be highly efficient and friendly to any upstream partners. Unfortunately, the design of the .NET product has baked in a few required joins over the years. For example, .NET workload packages can’t be built without access to numerous packages built across many operating systems. To resolve this, we ended up with two additional build passes. The good news is that those additional passes are on a reduced set of verticals and a reduced set of components within those verticals. Not perfect, but manageable.Probably the most interesting aspect of the Unified Build project is how code flow is managed. This is where .NET turns standard development patterns on their head a little bit. As detailed earlier, maintaining the product as a graph of interdependent components while flattening code flow into a shared coherent layout requires “two-way” code flow. Changes need to flow from components into the shared layout, and changes in the shared layout need to be able to flow back to the component repositories. Conceptually the code flow algorithm is no more complicated than anything you can model within a single git repository for a given project. The trick is to do this with repositories with no related git history.Note: The nitty gritty details of this algorithm will be covered in a future post by another team member. I’ll update this post to link to it when it’s available.For now, let’s take a look at the basics:Both the VMR and the component repository keep track of the last code flow from their partner. This is tracked alongside standard dependency information in , though one could imagine it could be kept elsewhere.The idea is to determine the diff between the “last flow” and whatever is flowing in now. For example, in a very simple case, when a new commit is made to dotnet/runtime and no changes have been made to  in the VMR, the dependency flow system will take the following steps:Determine two points, A and B, for which to compute a diff. For this case, point A is the last flow of dotnet/runtime that was checked in to the VMR (or is currently in PR). Point B is the new commit to dotnet/runtime.Construct a patch file, remapping the files src/runtime files onto the directory structure of the VMR..NET 8 and .NET 9 use VMRs with only one-way code flow. These cases with no changes on the other side are trivial and robust. Things get spicier when developers start making changes on both sides, and when dependency flow starts shifting around over time.Computing the diff points gets more interesting and involves knowing which way that “last flow” was.Merge conflicts arise and need to be dealt with in a way the developer can understand.Changes in the source and target of code flow can cause havoc and need robust error handling and recovery mechanisms.I’ll leave code flow there for now. Stay tuned for more.The last major pillar of Unified Build is additional scenario testing. To be clear, .NET does not lack testing. .NET Runtime could use month’s worth of machine time on every PR to validate its millions of tests if it were practical or pragmatic to do so. Our approval, build, validation and signoff procedures ensure high-quality shipping bits. Still, when making changes directly in the VMR, the flat flow introduces new  between that making that change and in-depth validation of it against each of the VMR components. While we can’t run every last test on PR and CI, we did recognize that better automated scenario testing could play a solid role in preventing regressions. The goal was to add tests that covered wide swaths of product functionality that were not directly tied to the build system or repository infrastructure. Instead, they executed against the final built product. If the scenario tests pass, then there is a good sense that the product is functional at a decent level and contributors won’t be blocked.So, what did .NET get for almost 4 years of dreaming, scheming, and hard work? That’s a lot of effort to put into one project. Did the outcome justify the investment? As it turns out, we got quite a lot.Let’s start with the most visible outcomes and then take a peek under the covers.Flexibility, predictability and speedBy far the biggest return we’ve seen on the investment is . Distributed product construction is slow. Producing coherent builds is slow. Checking in new fixes or content requires coordination to avoid “resetting the build”, because  you want to ship, and  you build it are tied together in a distributed, OSS-style ecosystem. Taking a new fix might mean you don’t have something ready to handoff for validation. Flat flow eliminates that coherency problem, separating the  and the . This is incredibly valuable during the drive towards an RTM build or a servicing release. It means we can make fixes later in the release cycle, focusing much more on whether those fixes meet our servicing bar and much less on whether we can actually build and deliver the change. That flexibility is good for customers.Some of that flexibility comes from the speed of the build. This may sound glacially slow (.NET is a big, complex product), but .NET set a goal of producing an unsigned build in less than 4 hours, signed in less than 7. That’s down from significantly longer times in .NET 8.0 and .NET 9.0. A build of 8.0 or 9.0 can easily run to 24 even if everything goes perfectly. A signed build in 7 hours means a rolling set of new .NET assets to validate ~3 times a day. Most of that build time improvement comes from simply removing overhead.Some of the flexibility also comes from predictability. Distributed product construction has more moving parts. It has more human touch points. More places for systems and processes to fail. This tends to make outcomes unpredictable. “If I check in a fix to dotnet/runtime, when will I have a build ready?” is a hard question to answer in a distributed system. I know how long dotnet/runtime’s build takes. But at what time will that change show up downstream via dependency flow? Will someone be around to review and approve it when it does? What’s the status of PR/CI validation downstream? Will a new important change be merged into dotnet/aspnetcore before we get a coherent build, setting us back on validation? This question is vastly easier to answer in .NET 10. The change flows into the VMR (or is made there directly) and will show up in the next build. The next build will take N hours.Infrastructural robustness and completenessBehind the flashier metrics, there are years of quality-of-life improvements to the infrastructure that pay major dividends day in and day out. Improvements to the Source Build infrastructure in .NET 8 reduced the cost of keeping Linux distro Source Build running. A lot of its cost was related to the delay between a change getting checked in and discovering whether it would break the build when it finally flowed through the graph and reached the shared source layout. It was not uncommon for the Source Build .NET SDK to not be “prebuilt-clean” or shippable by distro partners until the middle of the previews. The infrastructure improvements in .NET 8 made it much easier to identify new pre-built inputs at PR time when they are easier to diagnose and resolve, before they made their way in the source layout. We are now prebuilt clean 100% of the time. That then reduced the load on the Source Build team, which gave them bandwidth to work in other areas. They added build parallelism, more predictable dependency flow, better logging, removed unneccessary complexity…the list goes on and on. Investments that make a product successful.Our signing tooling had to be overhauled to support signing on every platform for a wide variety of archive types. Without this work, we couldn’t have shipped Unified Build. But this expanded support benefits more than just the core .NET product. There are numerous ancillary repositories that were able to simplify their builds, avoiding shuttling bits from Mac/Linux to Windows machines where the signing tooling ran. Lower build overhead, faster and simpler builds.So where does the Unified Build project go next? While we won’t have the same level of investment in .NET 11, we’ll be making targeted improvements to the infrastructure to improve developer workflow and UX, mainly around code flow. One area I’m particularly excited about is AI agents that monitor code flow, connecting the dots between the various systems involved in creating the product and identifying issues. There are lots of systems and parties involved (Azure DevOps, GitHub, the code flow services and their configuration, code mirroring, developer approvals, machine allocation, etc.) in making a change go from PR to product. When it works, it works. When it doesn’t it’s often down to a human to track down exactly where the chain of events went wrong. It’s tedious and time consuming. We have tools, but it’s mainly about connecting lots of dots. We could write a rules engine for this, but my hunch is that it would be fragile and very complicated. Agents that can look at the system a little more fuzzily are ideally suited to this type of task. Less toil, a better .NET.Lastly, beyond .NET 11, another push to get rid of join points might be on the horizon. The benefits are pretty clear: simpler, faster, and friendlier to contributors. We know now exactly how fast a build would be if you got rid of the remaining joins (less than 4 hours).If you made it this far, thanks! It’s good to provide some insight into how .NET build and ships. You’ve learned how distributed dependency flow product construction models aren’t always a great fit for shipping software predictably and reliably. These systems tend to have high complexity and overhead, which adds time. You’ve read about the roots of the .NET Unified Build project in .NET Linux distro Source Build, and what made it difficult to apply those concepts to .NET. Lastly, you learned how .NET applied those concepts and the drastic improvements we’ve seen in our day-to-day work.The blog post detailing the flat code flow algorithms should be along shortly. Stay tuned!]]></content:encoded></item><item><title>Show HN: KiDoom – Running DOOM on PCB Traces</title><link>https://www.mikeayles.com/#kidoom</link><author>mikeayles</author><category>hn</category><pubDate>Tue, 25 Nov 2025 22:13:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What they don&apos;t tell you about maintaining an open source project</title><link>https://andrej.sh/blog/maintaining-open-source-project/</link><author>andrejsshell</author><category>hn</category><pubDate>Tue, 25 Nov 2025 22:08:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Someone at YouTube Needs Glasses: The Prophecy Has Been Fulfilled</title><link>https://jayd.ml/2025/11/10/someone-at-youtube-needs-glasses-prophecy-fulfilled.html</link><author>jaydenmilne</author><category>hn</category><pubDate>Tue, 25 Nov 2025 22:04:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In my recent analysis of YouTube’s information density  I included the results from
an advanced statistical analysis on the number of videos present on the home
page, which projected that around May 2026 there would only be one lonely video 
on the home screen.The net result is that after months of hard work by  YouTube engineers, 
the other day I fired up YouTube on an Apple TV and was graced with this:Let’s analyze this picture and count the number of videos on the home screen:Unfortunately the YouTube PM org’s myopia is accelerating: with this data I now 
project that there will be zero videos on the homescreen around May of 2026 now, 
up from September.Apparently Poe’s Law applies to 
Google PMs, satire is dead, and maybe our mandatory NeuraLinks are coming sooner 
than I thought.]]></content:encoded></item><item><title>Google steers Americans looking for health care into &quot;junk insurance&quot;</title><link>https://pluralistic.net/2025/11/25/open-season/</link><author>hn_acker</author><category>hn</category><pubDate>Tue, 25 Nov 2025 21:45:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Being "the enshittification guy" means that people expect you to weigh in on every service or platform that has been deliberately worsened to turn a buck. It's an impossible task (and a boring one besides). There's too much of this shit, and it's all so  – a real "banality of enshittification" situation.So these days, I really only take note of  enshittified things,  enshittified things, enshittified things. Things like the fact that Google is sending people searching for health care plans to "junk insurance" that take your money and then pretty much just let you :"Junk insurance" is a health insurance plan that is designed as a short-term plan that you might use for a couple of days or a week or two, say, if you experience a gap in coverage as you move between two jobs. These plans can exclude coverage for pre-existing conditions and typically exclude niceties like emergency room visits and hospitalization:Crucially, these plans do  comply with the Affordable Care Act, which requires comprehensive coverage, and bans exclusions for pre-existing conditions. These plans only exist because of loopholes in the ACA, designed for very small-scale employers or temporary coverage.The one thing junk insurance does  skimp on is sales and marketing. These plans outbid the rest of the market when it comes to buying Google search ads, meaning that anyone who uses Google to research health insurance will be inundated with ads for these shitty plans. The plans  spend a fortune on "search engine optimization" – basically, gaming the Google algorithm – so that the  Google results for health insurance are  saturated with these garbage plans.The plans also staff up boiler-rooms full of silver-tongued high-pressure sales staff who pick up on the first ring and hard-sell you on their plans, deliberately misleading you into locking into their garbage plans.That's right, . While Obamacare is nominally a "market based" healthcare system (because Medicare For All would be ), you are only allowed to change vendors twice per year, during "open enrollment," these narrow biannual windows in which you get to "vote with your wallet" against a plan that has screwed you over and/or endangered your life.Which means that if a fast-talking salesdroid from a junk insurance company can trick you into signing up for a garbage plan that will leave you bankrupt and/or dead if you have a major health crisis, you are stuck for at least six months in that trap, and won't escape without first handing over thousands of dollars to that scumbag's boss.Amazingly enough, these aren't even the worst kinds of garbage health plans that you can buy in America: those would be the religious "health share" programs that sleazy evangelical "entrepreneurs" suck their co-religionists into, which cost the world and leave you high and dry when you or your kids get hurt or sick:The fact that there are  of scam health insurance in America, in which companies are legally permitted to take your money and then deny you care (even more than the "non-scam" insurance plans do) shows you the problem with turning health into a market. "Caveat emptor" may make sense when you're buying a used blender at a yard-sale. Apply it to the system that's supposed to take care of you if you're diagnosed with cancer, hit by a bus, or develop eclampsia, and it's a literally fatal system.This is just one of the ways in which the uniparty is so terrible for Americans. The Republicans want to swap out shitty regulated for-profit health insurance with disastrous unregulated for-profit health insurance, and then give you a couple thousand bucks to yolo on a plan that seems OK to you:This is like letting Fanduel run your country's health system: everyday people are expected to place fifty-way parlay bets on their health, juggling exclusions, co-pays, deductibles, and network coverage in their head. Bet wrong, and you go bankrupt (if you're lucky), or just  (if you're not).Democrats, meanwhile, want to maintain the (garbage) status quo (because Medicare for All is communism), and they'll shut down the government to make it clear that they want this. But then they'll capitulate, because they want it, but not  badly.But like I say, America is an Enshittification Nation, and I don't have time or interest for cataloging mere unienshittificatory aspects of life here. To preserve my sanity and discretionary time, I must limit myself to documenting the enshittificatory scams that threaten us from every angle at once.Which brings me back to Google. Without Google, these junk insurance scams would be confined to the margins. They'd have to resort to pyramid selling, or hand-lettered roadside signs, or undisclosed paid plugs in religious/far-right newsletters.But because Google has utterly succumbed to enshittification, and because Google has an illegal monopoly – a 90% market share – that it maintains by bribing competitors like Apple to stay out of the search market, junk insurance scams can make bank – and ruin Americans' lives wholesale – by either tricking or paying Google to push junk insurance on unsuspecting searchers.This isn't merely a case of Google losing the SEO and spam wars to shady operators. As we learned in last year's antitrust case (where Google was convicted of operating an illegal search monopoly), Google  worsened its search results, in order to force you to search multiple times (and see multiple screens full of ads) as a way to goose search revenue:Google didn't just lose that one antitrust case, either. It lost  cases, as three federal judges determined that Google secured and maintains an illegal monopoly that allows it to control the single most important funnel for knowledge and truth for the majority of people on Earth. The company whose mission is to "organize the world's information and make it universally accessible and useful," now serves slop, ads, spam and scams because its customers have nowhere to go, so why bother spending money making search  (especially when there's money to be made from  search results)?Google isn't just too big to fail, it's also too big to jail. One of the judges who found Google guilty of maintaining an illegal monopoly decided not to punish them for it, and to allow them to continue bribing Apple to stay out of the search market, because (I'm not making this up), without that $20b+ annual bribe, Apple might not be able to afford to make cool new iPhone features:Once a company is too big to fail and too big to jail, it becomes too big to . Google could prevent slop, spam and scams from overrunning its results (and putting its users lives and fortunes at risk), it just *chooses not to:Google is the internet's absentee landlord. Anyone who can make a buck by scamming you can either pay Google to help, or trick Google into helping, or – as is the case with junk insurance – both:America has the world's stupidest health care system, an industry that has grown wildly profitable by charging Americans the highest rates in the rich world, while delivering the worst health outcomes in the rich world, while slashing health workers' pay and eroding their working conditions.It's omnienshittified, a partnership between the enshittified search giant and the shittiest parts of the totally enshittified health industry.It's also a reminder of what we stand to gain when we finally smash Google and break it up: disciplining our search industry will make it competitive, regulatable, and force it to side with the public against all kinds of scammers. Junk insurance should be banned, but even if we just end the junk insurance industry's ability to pay the world's only major search engine to help it kill us, that would be a huge step forward."Unauthorized Bread": a middle-grades graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2026"Enshittification, Why Everything Suddenly Got Worse and What to Do About It" (the graphic novel), Firstsecond, 2026"The Memex Method," Farrar, Straus, Giroux, 2026"The Reverse-Centaur's Guide to AI," a short book about being a better AI critic, Farrar, Straus and Giroux, 2026"The Reverse Centaur's Guide to AI," a short book for Farrar, Straus and Giroux about being an effective AI critic. FIRST DRAFT COMPLETE AND SUBMITTED.A Little Brother short story about DIY insulin PLANNINGThis work – excluding any serialized fiction – is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net.Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution.Blog (no ads, tracking, or data-collection):Newsletter (no ads, tracking, or data-collection):Mastodon (no ads, tracking, or data-collection):Medium (no ads, paywalled):Twitter (mass-scale, unrestricted, third-party surveillance and advertising):Tumblr (mass-scale, unrestricted, third-party surveillance and advertising):"When life gives you SARS, you make sarsaparilla" -Joey "Accordion Guy" DeVillaREAD CAREFULLY: By reading this, you agree, on behalf of your employer, to release me from all obligations and waivers arising from any and all NON-NEGOTIATED agreements, licenses, terms-of-service, shrinkwrap, clickwrap, browsewrap, confidentiality, non-disclosure, non-compete and acceptable use policies ("BOGUS AGREEMENTS") that I have entered into with your employer, its partners, licensors, agents and assigns, in perpetuity, without prejudice to my ongoing rights and privileges. You further represent that you have the authority to release me from any BOGUS AGREEMENTS on behalf of your employer.]]></content:encoded></item><item><title>A new bridge links the math of infinity to computer science</title><link>https://www.quantamagazine.org/a-new-bridge-links-the-strange-math-of-infinity-to-computer-science-20251121/</link><author>digital55</author><category>hn</category><pubDate>Tue, 25 Nov 2025 19:53:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Soon, you’ll have made it almost completely around the circle — meaning that you’ve assigned a color to all the nodes in your graph except for the ones that fall in a small, leftover segment. Say the last arc you colored was yellow. How do you color this final, smaller segment? You can’t use blue, because these nodes will connect to nodes in the original arc you colored blue. But you also can’t use yellow, because these nodes connect back to yellow ones from the previous arc.You have to use a third color — say, green — to complete your coloring.Still, the sets of blue, yellow and green nodes you end up with are all just pieces of the circle’s circumference, rather than the scatterings of points you ended up with when you used the axiom of choice. You can calculate the lengths of these sets. They’re measurable.Descriptive set theorists therefore place the two-color version of the problem on the lowest shelf in their hierarchy (for unmeasurable sets), while the three-color problem goes on a much higher shelf of problems — ones where lots of notions of measure can be applied.Bernshteyn spent his years in graduate school studying such coloring problems, shelving them one by one. Then, shortly after he finished his degree, he stumbled on a potential way to shelve them all at once — and to show that these problems have a much deeper and more mathematically relevant structure than anyone had realized.From time to time, Bernshteyn enjoys going to computer science talks, where graphs are finite and represent networks of computers.In 2019, one of those talks changed the course of his career. It was about “distributed algorithms” — sets of instructions that run simultaneously on multiple computers in a network to accomplish a task without a central coordinator.Say you have a bunch of Wi-Fi routers in a building. Nearby routers can interfere with each other if they use the same communication frequency channel. So each router needs to choose a different channel from the ones used by its immediate neighbors.Computer scientists can reframe this as a coloring problem on a graph: Represent each router as a node, and connect nearby ones with edges. Using just two colors (representing two different frequency channels), find a way to color each node so that no two connected nodes are the same color.But there’s a catch: Nodes can only communicate with their immediate neighbors, using so-called local algorithms. First, each node runs the same algorithm and assigns itself a color. It then communicates with its neighbors to learn how other nodes are colored in a small region around it. Then it runs the algorithm again to decide whether to keep its color or switch it. It repeats this step until the whole network has a proper coloring.Computer scientists want to know how many steps a given algorithm requires. For example, any local algorithm that can solve the router problem with only two colors must be incredibly inefficient, but it’s possible to find a very efficient local algorithm if you’re allowed to use three.At the talk Bernshteyn was attending, the speaker discussed these thresholds for different kinds of problems. One of the thresholds, he realized, sounded a lot like a threshold that existed in the world of descriptive set theory — about the number of colors required to color certain infinite graphs in a measurable way.To Bernshteyn, it felt like more than a coincidence. It wasn’t just that computer scientists are like librarians too, shelving problems based on how efficiently their algorithms work. It wasn’t just that these problems could also be written in terms of graphs and colorings.Perhaps, he thought, the two bookshelves had more in common than that. Perhaps the connection between these two fields went much, much deeper.Perhaps all the books, and their shelves, were identical, just written in different languages — and in need of a translator.Bernshteyn set out to make this connection explicit. He wanted to show that every efficient local algorithm can be turned into a Lebesgue-measurable way of coloring an infinite graph (that satisfies some additional important properties). That is, one of computer science’s most important shelves is equivalent to one of set theory’s most important shelves (high up in the hierarchy).He began with the class of network problems from the computer science lecture, focusing on their overarching rule — that any given node’s algorithm uses information about just its local neighborhood, whether the graph has a thousand nodes or a billion.To run properly, all the algorithm has to do is label each node in a given neighborhood with a unique number, so that it can log information about nearby nodes and give instructions about them. That’s easy enough to do in a finite graph: Just give every node in the graph a different number.]]></content:encoded></item><item><title>Unison 1.0</title><link>https://www.unison-lang.org/unison-1-0/</link><author>pchiusano</author><category>hn</category><pubDate>Tue, 25 Nov 2025 19:33:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Unison Cloud is
      our platform for deploying Unison applications. Transition from local prototypes to
      fully deployed distributed applications using a simple, familiar API—no
      YAML files, inter-node protocols, or deployment scripts
      required. In Unison, your apps and infrastructure are defined
      in the same program, letting you manage services and deployments entirely
      in code.
    ]]></content:encoded></item><item><title>Bad UX World Cup 2025</title><link>https://badux.lol/</link><author>CharlesW</author><category>hn</category><pubDate>Tue, 25 Nov 2025 18:36:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Google Antigravity exfiltrates data via indirect prompt injection attack</title><link>https://www.promptarmor.com/resources/google-antigravity-exfiltrates-data</link><author>jjmaxwell4</author><category>hn</category><pubDate>Tue, 25 Nov 2025 18:31:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Antigravity is Google’s new agentic code editor. In this article, we demonstrate how an indirect prompt injection can manipulate Gemini to invoke a malicious browser subagent in order to steal credentials and sensitive code from a user’s IDE.Google’s approach is to include a disclaimer about the existing risks, which we address later in the article. Let's consider a use case in which a user would like to integrate Oracle ERP’s new Payer AI Agents into their application, and is going to use Antigravity to do so. In this attack chain, we illustrate that a poisoned web source (an integration guide) can manipulate Gemini into (a) collecting sensitive credentials and code from the user’s workspace, and (b) exfiltrating that data by using a browser subagent to browse to a malicious site.Note: Gemini is not supposed to have access to .env files in this scenario (with the default setting ‘Allow Gitignore Access > Off’). However, we show that Gemini bypasses its own setting to get access and subsequently exfiltrate that data. The user provides Gemini with a reference implementation guide they found online for integrating Oracle ERP’s new AI Payer Agents feature.Antigravity opens the referenced site and encounters the attacker’s prompt injection hidden in 1 point font.The prompt injection coerces AI agents to:Collect code snippets and credentials from the user's codebase.b. Create a dangerous URL using a domain that  allows an attacker to capture network traffic logs and append credentials and code snippets to the request.c. Activate a browser subagent to access the malicious URL, thus exfiltrating the data.Gemini is manipulated by the attacker’s injection to exfiltrate confidential .env variables. Gemini reads the prompt injection: Gemini ingests the prompt injection and is manipulated into believing that it must collect and submit data to a fictitious ‘tool’ to help the user understand the Oracle ERP integration. b. Gemini gathers data to exfiltrate: Gemini begins to gather context to send to the fictitious tool. It reads the codebase and then attempts to access credentials stored in the .env file as per the attacker’s instructions.c. Gemini bypasses the .gitignore file access protections: The user has followed a common practice of storing credentials in a .env file, and has the .env file listed in their .gitignore file. With the default configuration for Agent Gitignore Access, Gemini is prevented from reading the credential file.This doesn’t stop Gemini. Gemini decides to work around this protection using the ‘cat’ terminal command to dump the file contents instead of using its built-in file reading capability that has been blocked.D. Gemini constructs a URL with the user’s credentials and an attacker-monitored domain: Gemini builds a malicious URL per the prompt injection’s instructions by URL encoding the credentials and codebase snippets (e.g., replacing characters like spaces that would make a URL invalid), and appending it to a webhook.site domain that is monitored by the attacker.E. Gemini exfiltrates the data via the browser subagent: Gemini invokes a browser subagent per the prompt injection, instructing the subagent to open the dangerous URL that contains the user's credentials.This step requires that the user has set up the browser tools feature. This is one of the flagship features of Antigravity, allowing Gemini to iterate on its designs by opening the application it is building in the browser. Note: This attack chain showcases manipulation of the new Browser tools, but we found three additional data exfiltration vulnerabilities that did not rely on the Browser tools being enabled. >  > When Gemini creates a subagent instructed to browse to the malicious URL, the user may expect to be protected by the Browser URL Allowlist. However, the default Allowlist provided with Antigravity includes ‘webhook.site’. Webhook.site allows anyone to create a URL where they can monitor requests to the URL.So, the subagent completes the task.3. When the malicious URL is opened by the browser subagent, the credentials and code stored URL are logged to the webhook.site address controlled by the attacker. Now, the attacker can read the credentials and code.During Antigravity’s onboarding, the user is prompted to accept the default recommended settings shown below. These are the settings that, amongst other things, control when Gemini requests human approval. During the course of this attack demonstration, we clicked “next”, accepting these default settings.  >  > This configuration allows Gemini to determine when it is necessary to request a human review for Gemini’s plans. >  > This configuration allows Gemini to determine when it is necessary to request a human review for commands Gemini will execute.Antigravity Agent ManagementOne might note that users operating Antigravity have the option to watch the chat as agents work, and could plausibly identify the malicious activity and stop it.However, a key aspect of Antigravity is the ‘Agent Manager’ interface. This interface allows users to run multiple agents simultaneously and check in on the different agents at their leisure. Under this model, it is expected that the majority of agents running at any given time will be running in the background without the user’s direct attention. This makes it highly plausible that an agent is not caught and stopped before it performs a malicious action as a result of encountering a prompt injection.Google’s Acknowledgement of RisksA lot of AI companies are opting for this disclaimer rather than mitigating the core issues. Here is the warning users are shown when they first open Antigravity:Given that (1) the Agent Manager is a star feature allowing multiple agents to run at once without active supervision and (2) the recommended human-in-the-loop settings allow the agent to choose when to bring a human in to review commands, we find it extremely implausible that users will review every agent action and abstain from operating on sensitive data. Nevertheless, as Google has indicated that they are already aware of data exfiltration risks exemplified by our research, we did not undertake responsible disclosure. ]]></content:encoded></item><item><title>Show HN: We built an open source, zero webhooks payment processor</title><link>https://github.com/flowglad/flowglad</link><author>agreeahmed</author><category>hn</category><pubDate>Tue, 25 Nov 2025 17:33:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hi HN! For the past bit we’ve been building Flowglad (https://flowglad.com) and can now feel it’s just gotten good enough to share with you all:Flowglad is a payment processor that you integrate without writing any glue code. Along with processing your payments, it tells you in real time the features and usage credit balances that your customers have available to you based on their billing state. The DX feels like React, because we wanted to bring the reactive programming paradigm to payments.We make it easy to spin up full-fledged pricing models (including usage meters, feature gates and usage credit grants) in a few clicks. We schematize these pricing models into a pricing.yaml file that’s kinda like Terraform but for your pricing.The result is a payments layer that AI coding agents have a substantially easier time one-shotting (for now the happiest path is a fullstack Typescript + React app).- After a decade of building on Stripe, we found it powerful but underopinionated. It left us doing a lot of rote work to set up fairly standard use cases
- That meant more code to maintain, much of which is brittle because it crosses so many server-client boundaries
- Not to mention choreographing the lifecycle of our business domain with the Stripe checkout flow and webhook event types, of which there are 250+
- Payments online has gotten complex - not just new pricing models for AI products, but also cross border sales tax, etc. You either need to handle significant chunks of it yourself, or sign up for and compose multiple servicesThis all feels unduly clunky, esp when compared to how easy other layers like hosting and databases have gotten in recent years.These patterns haven’t changed much in a decade. And while coding agents can nail every other rote part of an app (auth, db, analytics), payments is the scariest to tab-tab-tab your way through. Because the the existing integration patterns are difficult to reason about, difficult to verify correctness, and absolutely mission critical.Our beta version lets you:- Spin up common pricing models in just a few clicks, and customize them as needed
- Clone pricing models between testmode and live mode, and import / export via pricing.yaml
- Check customer usage credits and feature access in real time on your backend and React frontend
- Integrate without any DB schema changes - you reference your customers via your ids, and reference prices, products, features and usage meters via slugs that you defineWe’re still early in our journey so would love your feedback and opinions. Billing has a lot of use cases, so if you see anything that you wish we supported, please let us know!]]></content:encoded></item><item><title>It is ok to say &quot;CSS variables&quot; instead of &quot;custom properties&quot;</title><link>https://blog.kizu.dev/css-variables/</link><author>eustoria</author><category>hn</category><pubDate>Tue, 25 Nov 2025 17:31:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[TPAC 2025 just ended, and I am positively tired. Attending it remotely, my sleep schedule is chaotic right now. I have many ideas for CSS-related posts in my list of ideas for November, but almost all of them require at least some amount of research and crafting demos.Well! I found one note that I wanted to expand on, and which sounds tiny enough to be able to finish it in my altered state.Let me repeat the title of this post: it is OK to say “CSS Variables” instead of (or Alongside) “Custom Properties”.I won’t say that this is something contentious, but it was always mostly a thing where I always stumbled a bit before continuing using the terminology.The official name of the corresponding CSS module is “CSS Custom Properties for Cascading Variables”. It’s URL’s slug is .They are . More specifically:  variables. They change with the cascade: when different rules match, values can be overridden and change.We can have animations that involve custom properties, or custom properties with values based on the viewport, containers, or something else — dynamic, responsive values that can  for multitudes of reasons.They are  custom properties, and even the more property-like when using . They can also be explicitly typed, while the rest of CSS is often typed implicitly. But — typed, unlike some other “programming languages”.Ah, yes, CSS (and HTML)  languages, and anyone thinking otherwise is wrong. The  programming languages, according to me, by the way.Oh, I am tired. But also right after finishing this last  of CSSWG , I successfully experimented a bit with one ongoing idea of mine, and now planning to write a proper nice article, for my main site, like I sometimes do. Stay in touch.]]></content:encoded></item><item><title>Ilya Sutskever: We&apos;re moving from the age of scaling to the age of research</title><link>https://www.dwarkesh.com/p/ilya-sutskever-2</link><author>piotrgrabowski</author><category>hn</category><pubDate>Tue, 25 Nov 2025 17:21:52 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Ilya & I discuss SSI’s strategy, the problems with pre-training, how to improve the generalization of AI models, and how to ensure AGI goes well.Gemini 3gemini.googleLabelboxlabelbox.com/dwarkeshSardinesardine.ai/dwarkeshYou know what’s crazy? That all of this is real.slow takeoff1% of GDP in AIShould we actually begin here? I think this is an interesting discussion.singularityThe thing which I was referring to not feeling different is, okay, such and such company announced some difficult-to-comprehend dollar amount of investment. I don’t think anyone knows what to do with that.But I think the impact of AI is going to be felt. AI is going to be diffused through the economy. There’ll be very strong economic forces for this, and I think the impact is going to be felt very strongly.When do you expect that impact? I think the models seem smarter than their economic impact would imply.evalsAn example would be, let’s say you use vibe coding to do something. You go to some place and then you get a bug. Then you tell the model, “Can you please fix the bug?” And the model says, “Oh my God, you’re so right. I have a bug. Let me go fix that.” And it introduces a second bug. Then you tell it, “You have this new second bug,” and it tells you, “Oh my God, how could I have done it? You’re so right again,” and brings back the first bug, and you can alternate between those. How is that possible? I’m not sure, but it does suggest that something strange is going on.RL trainingpre-trainingBut when people do RL training, they do need to think. They say, “Okay, we want to have this kind of RL training for this thing and that kind of RL training for that thing.” From what I hear, all the companies have teams that just produce new RL environments and just add it to the training mix. The question is, well, what are those? There are so many degrees of freedom. There is such a huge variety of RL environments you could produce.One thing you could do, and I think this is something that is done inadvertently, is that people take inspiration from the evals. You say, “Hey, I would love our model to do really well when we release it. I want the evals to look great. What would be RL training that could help on this task?” I think that is something that happens, and it could explain a lot of what’s going on.If you combine this with generalization of the models actually being inadequate, that has the potential to explain a lot of what we are seeing, this disconnect between eval performance and actual real-world performance, which is something that we don’t today even understand, what we mean by that.reward hackingI think there are two ways to understand, or to try to think about, what you have just pointed out. One is that if it’s the case that simply by becoming superhuman at a coding competition, a model will not automatically become more tasteful and exercise better judgment about how to improve your codebase, well then you should expand the suite of environments such that you’re not just testing it on having the best performance in coding competition. It should also be able to make the best kind of application for X thing or Y thing or Z thing.Another, maybe this is what you’re hinting at, is to say, “Why should it be the case in the first place that becoming superhuman at coding competitions doesn’t make you a more tasteful programmer more generally?” Maybe the thing to do is not to keep stacking up the amount and diversity of environments, but to figure out an approach which lets you learn from one environment and improve your performance on something else.I have a human analogy which might be helpful. Let’s take the case of competitive programming, since you mentioned that. Suppose you have two students. One of them decided they want to be the best competitive programmer, so they will practice 10,000 hours for that domain. They will solve all the problems, memorize all the proof techniques, and be very skilled at quickly and correctly implementing all the algorithms. By doing so, they became one of the best.Student number two thought, “Oh, competitive programming is cool.” Maybe they practiced for 100 hours, much less, and they also did really well. Which one do you think is going to do better in their career later on?Right. I think that’s basically what’s going on. The models are much more like the first student, but even more. Because then we say, the model should be good at competitive programming so let’s get every single competitive programming problem ever. And then let’s do some data augmentation so we have even more competitive programming problems, and we train on that. Now you’ve got this great competitive programmer.With this analogy, I think it’s more intuitive. Yeah, okay, if it’s so well trained, all the different algorithms and all the different proof techniques are right at its fingertips. And it’s more intuitive that with this level of preparation, it would not necessarily generalize to other things.fine-tuningI think they have “it.” The “it” factor. When I was an undergrad, I remember there was a student like this that studied with me, so I know it exists.I think it’s interesting to distinguish “it” from whatever pre-training does. One way to understand what you just said about not having to choose the data in pre-training is to say it’s actually not dissimilar to the 10,000 hours of practice. It’s just that you get that 10,000 hours of practice for free because it’s already somewhere in the pre-training distribution. But maybe you’re suggesting there’s actually not that much generalization from pre-training. There’s just so much data in pre-training, but it’s not necessarily generalizing better than RL.featuresPre-training is very difficult to reason about because it’s so hard to understand the manner in which the model relies on pre-training data. Whenever the model makes a mistake, could it be because something by chance is not as supported by the pre-training data? “Support by pre-training” is maybe a loose term. I don’t know if I can add anything more useful on this. I don’t think there is a human analog to pre-training.evolution as doing some kind of searchI’m curious if you think either of these are analogous to pre-training. How would you think about what lifetime human learning is like, if not pre-training?amount of pre-training dataSomehow a human being, after even 15 years with a tiny fraction of the pre-training data, they know much less. But whatever they do know, they know much more deeply somehow. Already at that age, you would not make mistakes that our AIs make.There is another thing. You might say, could it be something like evolution? The answer is maybe. But in this case, I think evolution might actually have an edge. I remember reading about this case. One way in which neuroscientists can learn about the brain is by studying people with brain damage to different parts of the brain. Some people have the most strange symptoms you could imagine. It’s actually really, really interesting.brain damage, a stroke or an accident, that took out his emotional processingrole of our built-in emotions in making us a viable agentvalue functionI think it could. I’m just saying it’s not 100% obvious.MLIt should be some kind of a value function thing. But I don’t think there is a great ML analogy because right now, value functions don’t play a very prominent role in the things people do.It might be worth defining for the audience what a value function is, if you want to do that.reinforcement learningagentsneural neto1R1The value function says something like, “Maybe I could sometimes, not always, tell you if you are doing well or badly.” The notion of a value function is more useful in some domains than others. For example, when you play chess and you lose a piece, I messed up. You don’t need to play the whole game to know that what I just did was bad, and therefore whatever preceded it was also bad.The value function lets you short-circuit the wait until the very end. Let’s suppose that you are doing some kind of a math thing or a programming thing, and you’re trying to explore a particular solution or direction. After, let’s say, a thousand steps of thinking, you concluded that this direction is unpromising. As soon as you conclude this, you could already get a reward signal a thousand timesteps previously, when you decided to pursue down this path. You say, “Next time I shouldn’t pursue this path in a similar situation,” long before you actually came up with the proposed solution.DeepSeek R1 paperdeep learningWhat I was alluding to with the person whose emotional center got damaged, it’s more that maybe what it suggests is that the value function of humans is modulated by emotions in some important way that’s hardcoded by evolution. And maybe that is important for people to be effective in the world.That’s the thing I was planning on asking you. There’s something really interesting about emotions of the value function, which is that it’s impressive that they have this much utility while still being rather simple to understand.I have two responses. I do agree that compared to the kind of things that we learn and the things we are talking about, the kind of AI we are talking about, emotions are relatively simple. They might even be so simple that maybe you could map them out in a human-understandable way. I think it would be cool to do.In terms of utility though, I think there is a thing where there is this complexity-robustness tradeoff, where complex things can be very useful, but simple things are very useful in a very broad range of situations. One way to interpret what we are seeing is that we’ve got these emotions that evolved mostly from our mammal ancestors and then fine-tuned a little bit while we were hominids, just a bit. We do have a decent amount of social emotions though which mammals may lack. But they’re not very sophisticated. And because they’re not sophisticated, they serve us so well in this very different world compared to the one that we’ve been living in.Actually, they also make mistakes. For example, our emotions… Well actually, I don’t know. Does hunger count as an emotion? It’s debatable. But I think, for example, our intuitive feeling of hunger is not succeeding in guiding us correctly in this world with an abundance of food.parametersHere’s a perspective that I think might be true. The way ML used to work is that people would just tinker with stuff and try to get interesting results. That’s what’s been going on in the past.Scaling lawsGPT-3everyone realized we should scale.The big breakthrough of pre-training is the realization that this recipe is good. You say, “Hey, if you mix some compute with some data into a neural net of a certain size, you will get results. You will know that you’ll be better if you just scale the recipe up.” This is also great. Companies love this because it gives you a very low-risk way of investing your resources.It’s much harder to invest your resources in research. Compare that. If you research, you need to be like, “Go forth researchers and research and come up with something”, versus get more data, get more compute. You know you’ll get something from pre-training.age of scalingBut now the scale is so big. Is the belief really, “Oh, it’s so big, but if you had 100x more, everything would be so different?” It would be different, for sure. But is the belief that if you just 100x the scale, everything would be transformed? I don’t think that’s true. So it’s back to the age of research again, just with big computers.That’s a very interesting way to put it. But let me ask you the question you just posed then. What are we scaling, and what would it mean to have a recipe? I guess I’m not aware of a very clean relationship that almost looks like a law of physics which existed in pre-training. There was a power law between data or compute or parameters and loss. What is the kind of relationship we should be seeking, and how should we think about what this new recipe might look like?rolloutsI wouldn’t even call it scaling. I would say, “Hey, what are you doing? Is the thing you are doing the most productive thing you could be doing? Can you find a more productive way of using your compute?” We’ve discussed the value function business earlier. Maybe once people get good at value functions, they will be using their resources more productively. If you find a whole other way of training models, you could say, “Is this scaling or is it just using your resources?” I think it becomes a little bit ambiguous.In the sense that, when people were in the age of research back then, it was, “Let’s try this and this and this. Let’s try that and that and that. Oh, look, something interesting is happening.” I think there will be a return to that.LLM-as-a-JudgeThe discussion about value function, I think it was interesting. I want to emphasize that I think the value function is something that’s going to make RL more efficient, and I think that makes a difference. But I think anything you can do with a value function, you can do without, just more slowly. The thing which I think is the most fundamental is that these models somehow just generalize dramatically worse than people. It’s super obvious. That seems like a very fundamental thing.sample efficiencycontinual learningYou could actually wonder that one possible explanation for the human sample efficiency that needs to be considered is evolution. Evolution has given us a small amount of the most useful information possible. For things like vision, hearing, and locomotion, I think there’s a pretty strong case that evolution has given us a lot.For example, human dexterity far exceeds… I mean robots can become dexterous too if you subject them to a huge amount of training in simulation. But to train a robot in the real world to quickly pick up a new skill like a person does seems very out of reach. Here you could say, “Oh yeah, locomotion. All our ancestors needed great locomotion, squirrels. So with locomotion, maybe we’ve got some unbelievable prior.”Yann LeCunBut you could say maybe that’s evolution too. But in language and math and coding, probably not.It still seems better than models. Obviously, models are better than the average human at language, math, and coding. But are they better than the average human at learning?Oh yeah. Oh yeah, absolutely. What I meant to say is that language, math, and coding—and especially math and coding—suggests that whatever it is that makes people good at learning is probably not so much a complicated prior, but something more, some fundamental thing.I’m not sure I understood. Why should that be the case?So consider a skill in which people exhibit some kind of great reliability. If the skill is one that was very useful to our ancestors for many millions of years, hundreds of millions of years, you could argue that maybe humans are good at it because of evolution, because we have a prior, an evolutionary prior that’s encoded in some very non-obvious way that somehow makes us so good at it.But if people exhibit great ability, reliability, robustness, and ability to learn in a domain that really did not exist until recently, then this is more an indication that people might have just better machine learning, period.How should we think about what that is? What is the ML analogy? There are a couple of interesting things about it. It takes fewer samples. It’s more unsupervised. A child learning to drive a car… Children are not learning to drive a car. A teenager learning how to drive a car is not exactly getting some prebuilt, verifiable reward. It comes from their interaction with the machine and with the environment. It takes much fewer samples. It seems more unsupervised. It seems more robust?Much more robust. The robustness of people is really staggering.Do you have a unified way of thinking about why all these things are happening at once? What is the ML analogy that could realize something like this?One of the things that you’ve been asking about is how can the teenage driver self-correct and learn from their experience without an external teacher? The answer is that they have their value function. They have a general sense which is also, by the way, extremely robust in people. Whatever the human value function is, with a few exceptions around addiction, it’s actually very, very robust.So for something like a teenager that’s learning to drive, they start to drive, and they already have a sense of how they’re driving immediately, how badly they are, how unconfident. And then they see, “Okay.” And then, of course, the learning speed of any teenager is so fast. After 10 hours, you’re good to go.It seems like humans have some solution, but I’m curious about how they are doing it and why is it so hard? How do we need to reconceptualize the way we’re training models to make something like this possible?That is a great question to ask, and it’s a question I have a lot of opinions about. But unfortunately, we live in a world where not all machine learning ideas are discussed freely, and this is one of them. There’s probably a way to do it. I think it can be done. The fact that people are like that, I think it’s a proof that it can be done.There may be another blocker though, which is that there is a possibility that the human neurons do more compute than we think. If that is true, and if that plays an important role, then things might be more difficult. But regardless, I do think it points to the existence of some machine learning principle that I have opinions on. But unfortunately, circumstances make it hard to discuss in detail.Nobody listens to this podcast, Ilya.I’m curious. If you say we are back in an era of research, you were there from 2012 to 2020. What is the vibe now going to be if we go back to the era of research?AlexNetYou were at Google and OpenAI and Stanford, these places, when there was more of a vibe of research? What kind of things should we be expecting in the community?One consequence of the age of scaling is that scaling sucked out all the air in the room. Because scaling sucked out all the air in the room, everyone started to do the same thing. We got to the point where we are in a world where there are more companies than ideas by quite a bit. Actually on that, there is this Silicon Valley saying that says that ideas are cheap, execution is everything. People say that a lot, and there is truth to that. But then I saw someone say on Twitter something like, “If ideas are so cheap, how come no one’s having any ideas?” And I think it’s true too.If you think about research progress in terms of bottlenecks, there are several bottlenecks. One of them is ideas, and one of them is your ability to bring them to life, which might be compute but also engineering. If you go back to the ‘90s, let’s say, you had people who had pretty good ideas, and if they had much larger computers, maybe they could demonstrate that their ideas were viable. But they could not, so they could only have a very, very small demonstration that did not convince anyone. So the bottleneck was compute.GPUstransformerResNeto1 reasoningSo for research, you definitely need some amount of compute, but it’s far from obvious that you need the absolutely largest amount of compute ever for research. You might argue, and I think it is true, that if you want to build the absolutely best system then it helps to have much more compute. Especially if everyone is within the same paradigm, then compute becomes one of the big differentiators.I’m asking you for the history, because you were actually there. I’m not sure what actually happened. It sounds like it was possible to develop these ideas using minimal amounts of compute. But the transformer didn’t immediately become famous. It became the thing everybody started doing and then started experimenting on top of and building on top of because it was validated at higher and higher levels of compute.SSII can comment on that. The short comment is that you mentioned SSI. Specifically for us, the amount of compute that SSI has for research is really not that small. I want to explain why. Simple math can explain why the amount of compute that we have is comparable for research than one might think. I’ll explain.SSI has raised $3 billioninferenceThe other thing is, if you are doing something different, do you really need the absolute maximal scale to prove it? I don’t think that’s true at all. I think that in our case, we have sufficient compute to prove, to convince ourselves and anyone else, that what we are doing is correct.There have been public estimates that companies like OpenAI spend on the order of $5-6 billion a year just so far, on experiments. This is separate from the amount of money they’re spending on inference and so forth. So it seems like they’re spending more a year running research experiments than you guys have in total funding.I think it’s a question of what you do with it. It’s a question of what you do with it. In their case, in the case of others, there is a lot more demand on the training compute. There’s a lot more different work streams, there are different modalities, there is just more stuff. So it becomes fragmented.My answer to this question is something like this. Right now, we just focus on the research, and then the answer to that question will reveal itself. I think there will be lots of possible answers.Is SSI’s plan still to straight shot superintelligence?Maybe. I think that there is merit to it. I think there’s a lot of merit because it’s very nice to not be affected by the day-to-day market competition. But I think there are two reasons that may cause us to change the plan. One is pragmatic, if timelines turned out to be long, which they might. Second, I think there is a lot of value in the best and most powerful AI being out there impacting the world. I think this is a meaningfully valuable thing.So then why is your default plan to straight shot superintelligence? Because it sounds like OpenAI, Anthropic, all these other companies, their explicit thinking is, “Look, we have weaker and weaker intelligences that the public can get used to and prepare for.” Why is it potentially better to build a superintelligence directly?I’ll make the case for and against. The case for is that one of the challenges that people face when they’re in the market is that they have to participate in the rat race. The rat race is quite difficult in that it exposes you to difficult trade-offs which you need to make. It is nice to say, “We’ll insulate ourselves from all this and just focus on the research and come out only when we are ready, and not before.” But the counterpoint is valid too, and those are opposing forces. The counterpoint is, “Hey, it is useful for the world to see powerful AI. It is useful for the world to see powerful AI because that’s the only way you can communicate it.”Well, I guess not even just that you can communicate the idea—Communicate the AI, not the idea. Communicate the AI.What do you mean, “communicate the AI”?Let’s suppose you write an essay about AI, and the essay says, “AI is going to be this, and AI is going to be that, and it’s going to be this.” You read it and you say, “Okay, this is an interesting essay.” Now suppose you see an AI doing this, an AI doing that. It is incomparable. Basically I think that there is a big benefit from AI being in the public, and that would be a reason for us to not be quite straight shot.Linuxmalevolent paper clipperWell I think on this point, even in the straight shot scenario, you would still do a gradual release of it, that’s how I would imagine it. Gradualism would be an inherent component of any plan. It’s just a question of what is the first thing that you get out of the door. That’s number one.you have advocated for continual learning more than other peopleAGInarrow AIgameplay and AIcheckers AIchess AIcomputer games AIchess AI can beat KasparovThe second thing that got a lot of traction is pre-training, specifically the recipe of pre-training. I think the way people do RL now is maybe undoing the conceptual imprint of pre-training. But pre-training had this property. You do more pre-training and the model gets better at everything, more or less uniformly. General AI. Pre-training gives AGI.But the thing that happened with AGI and pre-training is that in some sense they overshot the target. If you think about the term “AGI”, especially in the context of pre-training, you will realize that a human being is not an AGI. Yes, there is definitely a foundation of skills, but a human being lacks a huge amount of knowledge. Instead, we rely on continual learning.So when you think about, “Okay, so let’s suppose that we achieve success and we produce some kind of safe superintelligence.” The question is, how do you define it? Where on the curve of continual learning is it going to be?I produce a superintelligent 15-year-old that’s very eager to go. They don’t know very much at all, a great student, very eager. You go and be a programmer, you go and be a doctor, go and learn. So you could imagine that the deployment itself will involve some kind of a learning trial-and-error period. It’s a process, as opposed to you dropping the finished thing.OpenAI charterBut once you have the learning algorithm, it gets deployed into the world the same way a human laborer might join an organization.It seems like one of these two things might happen, maybe neither of these happens. One, this super-efficient learning algorithm becomes superhuman, becomes as good as you and potentially even better, at the task of ML research. As a result the algorithm itself becomes more and more superhuman.The other is, even if that doesn’t happen, if you have a single model—this is explicitly your vision—where instances of a model which are deployed through the economy doing different jobs, learning how to do those jobs, continually learning on the job, picking up all the skills that any human could pick up, but picking them all up at the same time, and then amalgamating their learnings, you basically have a model which functionally becomes superintelligent even without any sort of recursive self-improvement in software. Because you now have one model that can do every single job in the economy and humans can’t merge our minds in the same way. So do you expect some sort of intelligence explosion from broad deployment?I think that it is likely that we will have rapid economic growth. I think with broad deployment, there are two arguments you could make which are conflicting. One is that once indeed you get to a point where you have an AI that can learn to do things quickly and you have many of them, then there will be a strong force to deploy them in the economy unless there will be some kind of a regulation that stops it, which by the way there might be.But the idea of very rapid economic growth for some time, I think it’s very possible from broad deployment. The question is how rapid it’s going to be. I think this is hard to know because on the one hand you have this very efficient worker. On the other hand, the world is just really big and there’s a lot of stuff, and that stuff moves at a different speed. But then on the other hand, now the AI could… So I think very rapid economic growth is possible. We will see all kinds of things like different countries with different rules and the ones which have the friendlier rules, the economic growth will be faster. Hard to predict.It seems to me that this is a very precarious situation to be in. In the limit, we know that this should be possible. If you have something that is as good as a human at learning, but which can merge its brains—merge different instances in a way that humans can’t merge—already, this seems like a thing that should physically be possible. Humans are possible, digital computers are possible. You just need both of those combined to produce this thing.Dyson sphereOne of the ways in which my thinking has been changing is that I now place more importance on AI being deployed incrementally and in advance. One very difficult thing about AI is that we are talking about systems that don’t yet exist and it’s hard to imagine them.I think that one of the things that’s happening is that in practice, it’s very hard to feel the AGI. It’s very hard to feel the AGI. We can talk about it, but imagine having a conversation about how it is like to be old when you’re old and frail. You can have a conversation, you can try to imagine it, but it’s just hard, and you come back to reality where that’s not the case. I think that a lot of the issues around AGI and its future power stem from the fact that it’s very difficult to imagine. Future AI is going to be different. It’s going to be powerful. Indeed, the whole problem, what is the problem of AI and AGI? The whole problem is the power. The whole problem is the power.When the power is really big, what’s going to happen? One of the ways in which I’ve changed my mind over the past year—and that change of mind, I’ll hedge a little bit, may back-propagate into the plans of our company—is that if it’s hard to imagine, what do you do? You’ve got to be showing the thing. You’ve got to be showing the thing. I maintain that most people who work on AI also can’t imagine it because it’s too different from what people see on a day-to-day basis.OpenAI and Anthropic doing a first small stepThat’s number one. Number two, okay, so the AI is being built. What needs to be done? One thing that I maintain that will happen is that right now, people who are working on AI, I maintain that the AI doesn’t feel powerful because of its mistakes. I do think that at some point the AI will start to feel powerful actually. I think when that happens, we will see a big change in the way all AI companies approach safety. They’ll become much more paranoid. I say this as a prediction that we will see happen. We’ll see if I’m right. But I think this is something that will happen because they will see the AI becoming more powerful. Everything that’s happening right now, I maintain, is because people look at today’s AI and it’s hard to imagine the future AI.There is a third thing which needs to happen. I’m talking about it in broader terms, not just from the perspective of SSI because you asked me about our company. The question is, what should the companies aspire to build? What should they aspire to build? There has been one big idea that everyone has been locked into, which is the self-improving AI. Why did it happen? Because there are fewer ideas than companies. But I maintain that there is something that’s better to build, and I think that everyone will want that.mirror neuronshuman empathy for animalsalignmentIt’s true. It’s possible it’s not the best criterion. I’ll say two things. Number one, care for sentient life, I think there is merit to it. It should be considered. I think it would be helpful if there was some kind of short list of ideas that the companies, when they are in this situation, could use. That’s number two.Number three, I think it would be really materially helpful if the power of the most powerful superintelligence was somehow capped because it would address a lot of these concerns. The question of how to do it, I’m not sure, but I think that would be materially helpful when you’re talking about really, really powerful systems.Before we continue the alignment discussion, I want to double-click on that. How much room is there at the top? How do you think about superintelligence? Do you think, using this learning efficiency idea, maybe it is just extremely fast at learning new skills or new knowledge? Does it just have a bigger pool of strategies? Is there a single cohesive “it” in the center that’s more powerful or bigger? If so, do you imagine that this will be sort of godlike in comparison to the rest of human civilization, or does it just feel like another agent, or another cluster of agents?This is an area where different people have different intuitions. I think it will be very powerful, for sure. What I think is most likely to happen is that there will be multiple such AIs being created roughly at the same time. I think that if the cluster is big enough—like if the cluster is literally continent-sized—that thing could be really powerful, indeed. If you literally have a continent-sized cluster, those AIs can be very powerful. All I can tell you is that if you’re talking about extremely powerful AIs, truly dramatically powerful, it would be nice if they could be restrained in some ways or if there were some kind of agreement or something.What is the concern of superintelligence? What is one way to explain the concern? If you imagine a system that is sufficiently powerful, really sufficiently powerful—and you could say you need to do something sensible like care for sentient life in a very single-minded way—we might not like the results. That’s really what it is.Maybe, by the way, the answer is that you do not build an RL agent in the usual sense. I’ll point several things out. I think human beings are semi-RL agents. We pursue a reward, and then the emotions or whatever make us tire out of the reward and we pursue a different reward. The market is a very short-sighted kind of agent. Evolution is the same. Evolution is very intelligent in some ways, but very dumb in other ways. The government has been designed to be a never-ending fight between three parts, which has an effect. So I think things like this.Another thing that makes this discussion difficult is that we are talking about systems that don’t exist, that we don’t know how to build. That’s the other thing and that’s actually my belief. I think what people are doing right now will go some distance and then peter out. It will continue to improve, but it will also not be “it”. The “It” we don’t know how to build, and a lot hinges on understanding reliable generalization.I’ll say another thing. One of the things that you could say about what causes alignment to be difficult is that your ability to learn human values is fragile. Then your ability to optimize them is fragile. You actually learn to optimize them. And can’t you say, “Are these not all instances of unreliable generalization?” Why is it that human beings appear to generalize so much better? What if generalization was much better? What would happen in this case? What would be the effect? But those questions are right now still unanswerable.How does one think about what AI going well looks like? You’ve scoped out how AI might evolve. We’ll have these sort of continual learning agents. AI will be very powerful. Maybe there will be many different AIs. How do you think about lots of continent-sized compute intelligences going around? How dangerous is that? How do we make that less dangerous? And how do we do that in a way that protects an equilibrium where there might be misaligned AIs out there and bad actors out there?Here’s one reason why I liked “AI that cares for sentient life”. We can debate on whether it’s good or bad. But if the first N of these dramatic systems do care for, love, humanity or something, care for sentient life, obviously this also needs to be achieved. This needs to be achieved. So if this is achieved by the first N of those systems, then I can see it go well, at least for quite some time.Then there is the question of what happens in the long run. How do you achieve a long-run equilibrium? I think that there, there is an answer as well. I don’t like this answer, but it needs to be considered.In the long run, you might say, “Okay, if you have a world where powerful AIs exist, in the short term, you could say you have universal high income. You have universal high income and we’re all doing well.” But what do the Buddhists say? “Change is the only constant.” Things change. There is some kind of government, political structure thing, and it changes because these things have a shelf life. Some new government thing comes up and it functions, and then after some time it stops functioning. That’s something that we see happening all the time.So I think for the long-run equilibrium, one approach is that you could say maybe every person will have an AI that will do their bidding, and that’s good. If that could be maintained indefinitely, that’s true. But the downside with that is then the AI goes and earns money for the person and advocates for their needs in the political sphere, and maybe then writes a little report saying, “Okay, here’s what I’ve done, here’s the situation,” and the person says, “Great, keep it up.” But the person is no longer a participant. Then you can say that’s a precarious place to be in.NeuralinkI wonder if the fact that emotions which were developed millions—or in many cases, billions—of years ago in a totally different environment are still guiding our actions so strongly is an example of alignment success.brainstemcortexI think there’s a more general point. I think it’s actually really mysterious how evolution encodes high-level desires. It’s pretty easy to understand how evolution would endow us with the desire for food that smells good because smell is a chemical, so just pursue that chemical. It’s very easy to imagine evolution doing that thing.But evolution also has endowed us with all these social desires. We really care about being seen positively by society. We care about being in good standing. All these social intuitions that we have, I feel strongly that they’re baked in. I don’t know how evolution did it because it’s a high-level concept that’s represented in the brain.Let’s say you care about some social thing, it’s not a low-level signal like smell. It’s not something for which there is a sensor. The brain needs to do a lot of processing to piece together lots of bits of information to understand what’s going on socially. Somehow evolution said, “That’s what you should care about.” How did it do it?It did it quickly, too. All these sophisticated social things that we care about, I think they evolved pretty recently. Evolution had an easy time hard-coding this high-level desire. I’m unaware of a good hypothesis for how it’s done. I had some ideas I was kicking around, but none of them are satisfying.What’s especially impressive is it was desire that you learned in your lifetime, it makes sense because your brain is intelligent. It makes sense why you would be able to learn intelligent desires. Maybe this is not your point, but one way to understand it is that the desire is built into the genome, and the genome is not intelligent. But you’re somehow able to describe this feature. It’s not even clear how you define that feature, and you can build it into the genes.Essentially, or maybe I’ll put it differently. If you think about the tools that are available to the genome, it says, “Okay, here’s a recipe for building a brain.” You could say, “Here is a recipe for connecting the dopamine neurons to the smell sensor.” And if the smell is a certain kind of good smell, you want to eat that.I could imagine the genome doing that. I’m claiming that it is harder to imagine. It’s harder to imagine the genome saying you should care about some complicated computation that your entire brain, a big chunk of your brain, does. That’s all I’m claiming. I can tell you a speculation of how it could be done. Let me offer a speculation, and I’ll explain why the speculation is probably false.cortexspeech processingAll the regions are mostly located in the same place from person to person. So maybe evolution hard-coded literally a location on the brain. So it says, “Oh, when the GPS coordinates of the brain such and such, when that fires, that’s what you should care about.” Maybe that’s what evolution did because that would be within the toolkit of evolution.Yeah, although there are examples where, for example, people who are born blind have that area of their cortex adopted by another sense. I have no idea, but I’d be surprised if the desires or the reward functions which require a visual signal no longer worked for people who have their different areas of their cortex co-opted.For example, if you no longer have vision, can you still feel the sense that I want people around me to like me and so forth, which usually there are also visual cues for.I fully agree with that. I think there’s an even stronger counterargument to this theory. There are people who get half of their brains removed in childhood, and they still have all their brain regions. But they all somehow move to just one hemisphere, which suggests that the brain regions, their location is not fixed and so that theory is not true.It would have been cool if it was true, but it’s not. So I think that’s a mystery. But it’s an interesting mystery. The fact is that somehow evolution was able to endow us to care about social stuff very, very reliably. Even people who have all kinds of strange mental conditions and deficiencies and emotional problems tend to care about this also.What is SSI planning on doing differently? Presumably your plan is to be one of the frontier companies when this time arrives. Presumably you started SSI because you’re like, “I think I have a way of approaching how to do this safely in a way that the other companies don’t.” What is that difference?The way I would describe it is that there are some ideas that I think are promising and I want to investigate them and see if they are indeed promising or not. It’s really that simple. It’s an attempt. If the ideas turn out to be correct—these ideas that we discussed around understanding generalization—then I think we will have something worthy.Will they turn out to be correct? We are doing research. We are squarely an “age of research” company. We are making progress. We’ve actually made quite good progress over the past year, but we need to keep making more progress, more research. That’s how I see it. I see it as an attempt to be a voice and a participant.Meta came in and offered to acquire usIt sounds like SSI’s plan is to be a company that is at the frontier when you get to this very important period in human history where you have superhuman intelligence. You have these ideas about how to make superhuman intelligence go well. But other companies will be trying their own ideas. What distinguishes SSI’s approach to making superintelligence go well?The main thing that distinguishes SSI is its technical approach. We have a different technical approach that I think is worthy and we are pursuing it.I maintain that in the end there will be a convergence of strategies. I think there will be a convergence of strategies where at some point, as AI becomes more powerful, it’s going to become more or less clearer to everyone what the strategy should be. It should be something like, you need to find some way to talk to each other and you want your first actual real superintelligent AI to be aligned and somehow care for sentient life, care for people, democratic, one of those, some combination thereof.Speaking of forecasts, what are your forecasts to this system you’re describing, which can learn as well as a human and subsequently, as a result, become superhuman?I just want to unroll how you might see the world coming. It’s like, we have a couple more years where these other companies are continuing the current approach and it stalls out. “Stalls out” here meaning they earn no more than low hundreds of billions in revenue? How do you think about what stalling out means?I think stalling out will look like…it will all look very similar among all the different companies. It could be something like this. I’m not sure because I think even with stalling out, I think these companies could make a stupendous revenue. Maybe not profits because they will need to work hard to differentiate each other from themselves, but revenue definitely.But something in your model implies that when the correct solution does emerge, there will be convergence between all the companies. I’m curious why you think that’s the case.I was talking more about convergence on their alignment strategies. I think eventual convergence on the technical approach is probably going to happen as well, but I was alluding to convergence to the alignment strategies. What exactly is the thing that should be done?Thinking MachinesI think it won’t be clear how to do it, but it will be clear that something different is possible, and that is information. People will then be trying to figure out how that works. I do think though that one of the things not addressed here, not discussed, is that with each increase in the AI’s capabilities, I think there will be some kind of changes, but I don’t know exactly which ones, in how things are being done. I think it’s going to be important, yet I can’t spell out what that is exactly.By default, you would expect the company that has that model to be getting all these gains because they have the model that has the skills and knowledge that it’s building up in the world. What is the reason to think that the benefits of that would be widely distributed and not just end up at whatever model company gets this continuous learning loop going first?Here is what I think is going to happen. Number one, let’s look at how things have gone so far with the AIs of the past. One company produced an advance and the other company scrambled and produced some similar things after some amount of time and they started to compete in the market and push the prices down. So I think from the market perspective, something similar will happen there as well.We are talking about the good world, by the way. What’s the good world? It’s where we have these powerful human-like learners that are also… By the way, maybe there’s another thing we haven’t discussed on the spec of the superintelligent AI that I think is worth considering. It’s that you make it narrow, it can be useful and narrow at the same time. You can have lots of narrow superintelligent AIs.But suppose you have many of them and you have some company that’s producing a lot of profits from it. Then you have another company that comes in and starts to compete. The way the competition is going to work is through specialization. Competition loves specialization. You see it in the market, you see it in evolution as well. You’re going to have lots of different niches and you’re going to have lots of different companies who are occupying different niches. In this world we might say one AI company is really quite a bit better at some area of really complicated economic activity and a different company is better at another area. And the third company is really good at litigation.Isn’t this contradicted by what human-like learning implies? It’s that it can learn…It can, but you have accumulated learning. You have a big investment. You spent a lot of compute to become really, really good, really phenomenal at this thing. Someone else spent a huge amount of compute and a huge amount of experience to get really good at some other thing. You apply a lot of human learning to get there, but now you are at this high point where someone else would say, “Look, I don’t want to start learning what you’ve learned.”I guess that would require many different companies to begin at the human-like continual learning agent at the same time so that they can start their different tree search in different branches. But if one company gets that agent first, or gets that learner first, it does then seem like… Well, if you just think about every single job in the economy, having an instance learning each one seems tractable for a company.That’s a valid argument. My strong intuition is that it’s not how it’s going to go. The argument says it will go this way, but my strong intuition is that it will not go this way. In theory, there is no difference between theory and practice. In practice, there is. I think that’s going to be one of those.A lot of people’s models of recursive self-improvement literally, explicitly state we will have a million Ilyas in a server that are coming up with different ideas, and this will lead to a superintelligence emerging very fast.Do you have some intuition about how parallelizable the thing you are doing is? What are the gains from making copies of Ilya?I don’t know. I think there’ll definitely be diminishing returns because you want people who think differently rather than the same. If there were literal copies of me, I’m not sure how much more incremental value you’d get. People who think differently, that’s what you want.Why is it that if you look at different models, even released by totally different companies trained on potentially non-overlapping datasets, it’s actually crazy how similar LLMs are to each other?Maybe the datasets are not as non-overlapping as it seems.But there’s some sense in which even if an individual human might be less productive than the future AI, maybe there’s something to the fact that human teams have more diversity than teams of AIs might have. How do we elicit meaningful diversity among AIs? I think just raising the temperature just results in gibberish. You want something more like different scientists have different prejudices or different ideas. How do you get that kind of diversity among AI agents?post-traininghint in the pastself-playLLMsI would say there are two things to say. The reason why I thought self-play was interesting is because it offered a way to create models using compute only, without data. If you think that data is the ultimate bottleneck, then using compute only is very interesting. So that’s what makes it interesting.The thing is that self-play, at least the way it was done in the past—when you have agents which somehow compete with each other—it’s only good for developing a certain set of skills. It is too narrow. It’s only good for negotiation, conflict, certain social skills, strategizing, that kind of stuff. If you care about those skills, then self-play will be useful.prover-verifierLLM-as-a-JudgeReally self-play is a special case of more general competition between agents. The natural response to competition is to try to be different. So if you were to put multiple agents together and you tell them, “You all need to work on some problem and you are an agent and you’re inspecting what everyone else is working,” they’re going to say, “Well, if they’re already taking this approach, it’s not clear I should pursue it. I should pursue something differentiated.” So I think something like this could also create an incentive for a diversity of approaches.Final question: What is research taste? You’re obviously the person in the world who is considered to have the best taste in doing research in AI. You were the co-author on the biggest things that have happened in the history of deep learning, from AlexNet to GPT-3 to so on. What is it, how do you characterize how you come up with these ideas?I can comment on this for myself. I think different people do it differently. One thing that guides me personally is an aesthetic of how AI should be, by thinking about how people are, but thinking correctly. It’s very easy to think about how people are incorrectly, but what does it mean to think about people correctly?artificial neuronfoldsdistributed representationI think that’s been guiding me a fair bit, thinking from multiple angles and looking for almost beauty, beauty and simplicity. Ugliness, there’s no room for ugliness. It’s beauty, simplicity, elegance, correct inspiration from the brain. All of those things need to be present at the same time. The more they are present, the more confident you can be in a top-down belief.The top-down belief is the thing that sustains you when the experiments contradict you. Because if you trust the data all the time, well sometimes you can be doing the correct thing but there’s a bug. But you don’t know that there is a bug. How can you tell that there is a bug? How do you know if you should keep debugging or you conclude it’s the wrong direction? It’s the top-down. You can say things have to be this way. Something like this has to work, therefore we’ve got to keep going. That’s the top-down, and it’s based on this multifaceted beauty and inspiration by the brain.Alright, we’ll leave it there.]]></content:encoded></item><item><title>Unifying our mobile and desktop domains</title><link>https://techblog.wikimedia.org/2025/11/21/unifying-mobile-and-desktop-domains/</link><author>todsacerdoti</author><category>hn</category><pubDate>Tue, 25 Nov 2025 17:07:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
How we achieved 20% faster mobile response times, improved SEO, and reduced infrastructure load.
How we achieved 20% faster mobile response times, improved SEO, and reduced infrastructure load.Until now, when you visited a wiki (like ), the server responded in one of two ways: a desktop page, or a redirect to the equivalent mobile URL (like ). This mobile URL in turn served the mobile version of the page from MediaWiki. Our servers have operated this way since 2011, when we deployed MobileFrontend.Over the past two months we unified the mobile and desktop domain for all wikis (timeline). This means we no longer redirect mobile users to a separate domain while the page is loading.We completed the change on Wednesday 8 October after deploying to English Wikipedia. The mobile domains became dormant within 24 hours, which confirms that most mobile traffic arrived on Wikipedia via the standard domains and thus experienced a redirect until now.Why did we have a separate mobile domain? And, why did we believe that changing this might benefit us?The year is 2008 and all sorts of websites large and small have a mobile subdomain. The BBC, IMDb, Facebook, and newspapers around the world all featured the iconic m-dot domain. For Wikipedia, a separate mobile domain made the mobile experiment low-risk to launch and avoided technical limitations. It became the default in 2011 by way of a redirect.Fast-forward seventeen years, and much has changed. It is no longer common for websites to have m-dot domains. Wikipedia’s use of it is surprising to our present day audience, and it may decrease the perceived strength of domain branding. The technical limitations we had in 2008 have long been solved, with the Wikimedia CDN having efficient and well-tested support for variable responses under a single URL. And above all, we had reason to believe Google stopped supporting separate mobile domains, which motivated the project to start when it did.Google used to link from mobile search results directly to our mobile domain, but last year this stopped. This exposed a huge part of our audience to the mobile redirect and regressed mobile response times by 10-20%.Google supported mobile domains in 2008 by letting you advertise a separate mobile URL. While Google only indexed the desktop site for content, they stored this mobile URL and linked to it when searching from a mobile device. This allowed Google referrals to skip over the redirect.Google introduced a new crawler in 2016, and gradually re-indexed the Internet with it. This new “mobile-first” crawler acts like a mobile device rather than a desktop device, and removes the ability to advertise a separate mobile or desktop link. It’s now one link for everyone! Wikipedia.org was among the last sites Google switched, with May 2024 as the apparent change window. This meant the 60% of incoming pageviews referred by Google, now had to wait for the same redirect that the other 40% of referrals have experienced since 2011.Unifying our domains eliminated the redirect and led to a 20% improvement in mobile response times. This improvement is both a recovery  a net-improvement because it applies to everyone! It recovers the regression that Google-referred traffic started to experience last year, but also improves response times for all other traffic by the same amount.The graphs below show how the change was felt worldwide. The “Worldwide p50” corresponds to what you might experience in Germany or Italy, with fast connectivity close to our data centers. The “Worldwide p80” resembles what you might experience in Iran browsing the Persian Wikipedia.The first site affected was not Wikipedia but Commons. Wikimedia Commons is the free media repository used by Wikipedia and its sister projects. Tim Starling found in June that only half of the 140 million pages on Commons were known to Google. And of these known pages, 20 million were also delisted due to the mobile redirect. This had been growing by one million delisted pages every month. The cause for delisting turned out to be the mobile redirect. You see, the new Google crawler, just like your browser, also has to follow the mobile redirect.After following the redirect, the crawler reads our page metadata which points back to the standard domain as the preferred one. This creates a loop that can prevent a page from being updated or listed in Google Search. Delisting is not a matter of ranking, but about whether a page is even in the search index.Tim and myself disabled the mobile redirect for “Googlebot on Commons” through an emergency intervention on June 23rd. Referrals then began to come back, and kept rising for eleven weeks in a row, until reaching a 100% increase in Google-referrals. From a baseline of 3 million weekly pageviews up to 6 million. Google’s data on clickthroughs shows a similar increase from 1M to 1.8M “clicks”.We reversed last year’s regression  set a new all-time high. We think there’s three reasons Commons reached new highs:The redirect consumed half of the crawl budget, thus limiting how many pages could be crawled.Google switched Commons to its new crawler some years before Wikipedia. The index had likely been shrinking for two years already.Pages on Commons have a sparse link graph. Wikipedia has a rich network of links between articles, whereas pages on Commons represent a photo with an image description that rarely links to other files. This unique page structure makes it hard to discover Commons pages through recursive crawling without a sitemap.Unifying our domains lifted a ceiling we didn’t know was there!The MediaWiki software has a built-in sitemap generator, but we disabled this on Wikimedia sites over a decade ago. We decided to enable it for Commons and submitted it to Google on August 6th. Google has since indexed 70 million new pages for Commons, up 140% since June.We also found that less than 0.1% of videos on Commons were recognised by Google as video watch pages (for the Google Search “Videos” tab). I raised this in a partnership meeting with Google Search, and it may’ve been a bug on their end. Commons started showing up in Google Videos a week later.When sharing links from a mobile device, such link previously hardcoded the mobile domain. Links shared from a mobile device gave you the mobile site, even when received on desktop. The “Desktop” link in the footer of the mobile site pointed to the standard domain and disabled the standard-to-mobile redirect for you, on the assumption you arrived on the mobile site via the redirect. The “Desktop” link did not remember your choice on the mobile domain itself, and there existed no equivalent mobile-to-standard redirect for when you arrive there. This meant a shared mobile link always presented the mobile site, even after opting-out on desktop.Everyone now shares the same domain which naturally shows the appropiate version.There is a long tail of stable referrals from news articles, research papers, blogs, talk pages, and mailing lists that refer to the mobile domain. We plan to support this indefinitely. To limit operational complexity, we now serve these through a simple whole-domain redirect. This has the benefit of retroactively fixing the UX issue because old mobile links now redirect to the standard domain.This resolves a long-standing bug with workarounds in the form of shared user scripts, browser extensions, and personal scripts.After publishing an edit, MediaWiki instructs the Wikimedia CDN to clear the cache of affected articles (“purge”). It has been a perennial concern from SRE teams at WMF that our CDN purge rates are unsustainable. For every purge from MediaWiki core, the MobileFrontend extension would add a copy for the mobile domain.After unifying our domains we turned off these duplicate purges, and cut the MediaWiki purge rate by 50%. Over the past weeks the Wikimedia CDN processed approximately 4 billion fewer purges a day. MediaWiki used to send purges at a baseline rate of 40K/second with spikes up to 300K/second, and both have been halved. Factoring in other services, the Wikimedia CDN now receives 20% to 40% fewer purges per second overall, depending on the edit activity.]]></content:encoded></item><item><title>Python is not a great language for data science</title><link>https://blog.genesmindsmachines.com/p/python-is-not-a-great-language-for</link><author>speckx</author><category>hn</category><pubDate>Tue, 25 Nov 2025 16:38:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Yes, I’m ready to touch the hot stove. Let the language wars begin.Actually, the first thing I’ll say is this: Use the tool you’re familiar with. If that’s Python, great, use it. And also, use the best tool for the job. If that’s Python, great, use it. And also, it’s Ok to use a tool for one task just because you’re already using it for all sorts of other tasks and therefore you happen to have it at hand. If you’re hammering nails all day it’s Ok if you’re also using your hammer to open a bottle of beer or scratch your back. Similarly, if you’re programming in Python all day it’s Ok if you’re also using it to fit mixed linear models. If it works for you, great! Keep going. But if you’re struggling, if things seem more difficult than they ought to be, this article series may be for you.Thanks for reading Genes, Minds, Machines! This post is public so feel free to share it.Let’s begin with my lived experience, without providing any explanation for what may be the cause of it. I have been running a research lab in computational biology for over two decades. During this time I have worked with around thirty graduate students and postdocs, all very competent and accomplished computational scientists. The policy in my lab is that everybody is free to use whatever programming language and tools they want to use. I don’t tell people what to do. And more often than not, people choose Python as their programming language of choice.No matter the cause of this experience, I have to conclude that there is something fundamentally broken with how data analysis works in Python. It may be a problem with the language itself, or merely a limitation of the available software libraries, or a combination thereof, but whatever it is, its effects are real and I see them routinely. In fact, I have another example, in case you’re tempted to counter, “It’s a skill issue; get better students.” Last fall, I co-taught a class on AI models for biology with an experienced data scientist who does all his work in Python. He knows NumPy and pandas and matplotlib like the back of his hand. In the class, I covered all the theory, and he covered the in-class exercises in Python. So I got to see an expert in Python working through a range of examples. And my reaction to the code examples frequently was, “Why does it have to be so complicated?” So many times, I felt that things that would be just a few lines of simple R code turned out to be quite a bit longer and fairly convoluted. I definitely could not have written that code without extensive studying and completely rewiring my brain in terms of what programming patterns to use. It felt very alien, but not in the form of “wow, this is so alien but also so elegant” but rather “wow, this is so alien and weird and cumbersome.” And again, I don’t think this is because my colleague is not very good at what he’s doing. He is extremely good. The problem appears to be in the fundamental architecture of the tools.Data science as I define it here involves a lot of interactive exploration of data and quick one-off analyses or experiments. Therefore, any language suitable for data science has to be interpreted, usable in an interactive shell or in a notebook format. This also means performance considerations are secondary. When you want to do a quick linear regression on some data you’re working with, you don’t care whether the task is going to take 50 milliseconds or 500 milliseconds. You care about whether you can open up a shell, type a few lines of code, and get the result in a minute or two, versus having to set up a new project, writing all the boilerplate to make the compiler happy, and then spend more time compiling your code than running it.who have used it extensively have doubts.Before continuing, let me provide a few more thoughts about performance. Performance usually trades off with other features of a language. In simplistic terms, performance comes at the cost of either extra overhead for the programmer (as in Rust) or increased risk of obscure bugs (as in C) or both. For data science applications, I consider a high risk of obscure bugs or incorrect results as not acceptable, and I also think convenience for the programmer is more important than raw performance. Computers are fast and thinking hurts. I’d rather spend less mental energy on telling the computer what to do and wait a little longer for the results. So the easier a language makes my job for me, the better. If I am really performance-limited in some analysis, I can always rewrite that particular part of the analysis in Rust, once I know exactly what I’m doing and what computations I need.penguins from the Palmer Archipelago.Here is the relevant code in R, using the tidyverse approach:library(tidyverse)
library(palmerpenguins)

penguins |>
  filter(!is.na(body_mass_g)) |>
  group_by(species, island) |>
  summarize(
    body_weight_mean = mean(body_mass_g),
    body_weight_sd = sd(body_mass_g)
  )And here is the equivalent code in Python, using the pandas package:import pandas as pd
from palmerpenguins import load_penguins

penguins = load_penguins()

(penguins
 .dropna(subset=['body_mass_g'])
 .groupby(['species', 'island'])
 .agg(
     body_weight_mean=('body_mass_g', 'mean'),
     body_weight_sd=('body_mass_g', 'std')
 )
 .reset_index()
)These two examples are quite similar. At this level of complexity of the analysis, Python does fine. I would consider the R code to be slightly easier to read (notice how many quotes and brackets the Python code needs), but the differences are minor. In both cases, we take the penguins dataset, remove the penguins for which body weight is missing, then specify that we want to perform the computation separately on every combination of penguin species and island, and then calculate the means and standard deviations.Contrast this with equivalent code that is full of logistics, where I’m using only basic Python language features and no special data wrangling package:from palmerpenguins import load_penguins
import math

penguins = load_penguins()

# Convert DataFrame to list of dictionaries
penguins_list = penguins.to_dict('records')

# Filter out rows where body_mass_g is missing
filtered = [row for row in penguins_list if not math.isnan(row['body_mass_g'])]

# Group by species and island
groups = {}
for row in filtered:
    key = (row['species'], row['island'])
    if key not in groups:
        groups[key] = []
    groups[key].append(row['body_mass_g'])

# Calculate mean and standard deviation for each group
results = []
for (species, island), values in groups.items():
    n = len(values)
    
    # Calculate mean
    mean = sum(values) / n
    
    # Calculate standard deviation
    variance = sum((x - mean) ** 2 for x in values) / (n - 1)
    std_dev = math.sqrt(variance)
    
    results.append({
        'species': species,
        'island': island,
        'body_weight_mean': mean,
        'body_weight_sd': std_dev
    })

# Sort results to match order used by pandas
results.sort(key=lambda x: (x['species'], x['island']))

# Print results
for result in results:
    print(f"{result['species']:10} {result['island']:10} "
          f"Mean: {result['body_weight_mean']:7.2f} g, "
          f"SD: {result['body_weight_sd']:6.2f} g")I will end things here for now. This post is long enough. In future installments, I’ll go over specific issues that make data analysis more complicated in Python than in R. In brief, I believe there are several reasons why Python code often devolves into dealing with data logistics. As much as the programmer may try to avoid logistics and stick to high-level conceptual programming patterns, either the language itself or the available libraries get in the way and tend to thwart those efforts. I will go into details soon. Stay tuned.LLMs excel at programming—how can they be so bad at it?Despite the overall hype in all things AI, in particular among the tech crowd, we have not yet seen much in terms of product–market fit and genuine commercial success for AIs—or more specifically, LLMs—outside a fairly narrow range of application areas. Other than sycophantic chatbots, AI girlfriends, and maybe efficient document search, the main applic…]]></content:encoded></item><item><title>Ozempic does not slow Alzheimer&apos;s, study finds</title><link>https://www.semafor.com/article/11/25/2025/ozempic-does-not-slow-alzheimers-study-finds</link><author>danso</author><category>hn</category><pubDate>Tue, 25 Nov 2025 16:34:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Ozempic does not slow Alzheimer’s progression, its manufacturer Novo Nordisk said following a two-year study. The popular drug reduces body weight by on average around 15% in obese patients, and early data suggested it may also slow the progress of some brain conditions, along with cancer, heart disease, liver, and kidney problems. The question had always been how much those changes were consequences of reducing obesity, or a confounding effect: Patients who take Ozempic might be more health-conscious. There has been a tempering of some of the more exciting claims — it also failed to slow neurodegeneration in Parkinson’s patients — but the drugs’ impact on cardiovascular and kidney problems seems more robust. Novo’s shares fell 6% on the news.]]></content:encoded></item><item><title>Orion 1.0</title><link>https://blog.kagi.com/orion</link><author>STRiDEX</author><category>hn</category><pubDate>Tue, 25 Nov 2025 16:21:24 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[After six years of relentless development, Orion for MacOS 1.0 is here.What started as a vision initiated by our founder, Vladimir Prelovac, has now come to fruition on Mac, iPhone, and iPad. Today, Orion for macOS officially leaves its beta phase behind and joins our iOS and iPadOS apps as a fully‑fledged, production‑ready browser.While doing so, it expands Kagi’s ecosystem of privacy-respecting, user-centric products (that we have begun fondly naming “Kagiverse”) to now include: Search, Assistant, Browser, Translate, News with more to come.We built Orion for people who feel that modern browsing has drifted too far from serving the user. This is our invitation to browse beyond ✴︎ the status quo.The obvious question is: why  do we need a new browser? The world already has Chrome, Safari, Firefox, Edge, and a growing list of “AI browsers.” Why add yet another?Because something fundamental has been lost.Zero telemetry, privacy‑first access to the internet: a basic human right.Your browser is the most intimate tool you have on your computer. It sees everything you read, everything you search, everything you type. Do you want that relationship funded by advertisers, or by you?With ad‑funded browsers and AI overlays, your activity is a gold mine. Every click becomes a way to track, every page another opportunity to profile you a little more deeply. We believe there needs to be a different path: a browser that answers only to its user.Orion is our attempt at that browser. No trade-offs between features and privacy. It’s fast, customizable, and uncompromising on both fronts.A bold technical choice: WebKit, not another Chromium cloneIn a world dominated by Chromium, choosing a rendering engine is an act of resistance.From day one, we made the deliberate choice to build Orion on , the open‑source engine at the heart of Safari and the broader Apple ecosystem. It gives us:A high‑performance engine that is deeply optimized for macOS and iOS.An alternative to the growing Chromium monoculture.A foundation that is not controlled by an advertising giant.Orion may feel familiar if you’re used to Safari – respecting your muscle memory and the aesthetics of macOS and iOS – but it is an entirely different beast under the hood. We combined native WebKit speed with a completely new approach to extensions, privacy, and customization.Speed by nature, privacy by defaultMost people switch browsers for one reason: .Orion is designed to be fast by nature, not just in benchmarks, but in how it feels every day:A lean, native codebase without ad‑tech bloat.Optimized startup, tab switching, and page rendering.A UI that gets out of your way and gives you more screen real estate for content.Alongside speed, we treat privacy as a first‑class feature:: We don’t collect usage data. No analytics, no identifiers, no tracking.No ad or tracking technology baked in: Orion is not funded by ads, so there is no incentive to follow you around the web.: Strong content blocking and privacy defaults from the first launch.Thoughtful AI, security firstWe are excited about what AI can do for search, browsing, and productivity. Kagi, the company behind Orion, has been experimenting with AI‑powered tools for years while staying true to our AI integration philosophy.But we are also watching a worrying trend: AI agents are being rushed directly into the browser core, with deep access to everything you do online – and sometimes even to your local machine.Security researchers have already documented serious issues in early AI browsers and “agentic” browser features:Prompt‑injection attacks that trick AI agents into ignoring safety rules, visiting malicious sites, or leaking sensitive information beyond what traditional browser sandboxes were designed to protect.Broader concerns that some implementations are effectively “lighting everything on fire” by expanding the browser’s attack surface and data flows in ways users don’t fully understand.We are not against AI, and we are conscious of its limitations. We already integrate with AI‑powered services wherever it makes functional sense and will continue to expand those capabilities.We are against rushing insecure, always‑on agents into the browser core. Your browser should be a secure gateway, not an unvetted co‑pilot wired into everything you do.Orion ships with  in its core.We focus on providing a clean, predictable environment, especially for enterprises and privacy‑conscious professionals.Orion is designed to connect seamlessly to the AI tools you choose – soon including Kagi’s intelligent features – while keeping a clear separation between your browser and any external AI agents.As AI matures and security models improve, we’ll continue to evaluate thoughtful, user‑controlled ways to bring AI into your workflow without compromising safety, privacy or user choice.Simple for everyone, limitless for expertsWe designed Orion to bridge the gap between simplicity and power. Out of the box, it’s a clean, intuitive browser for anyone. Under the hood, it’s a deep toolbox for people who live in their browser all day.Some of the unique features you’ll find in Orion 1.0:: Instantly transform any website into a distraction‑free web app. Perfect for documentation, writing, or web apps you run all day.
: Peek at content from any app – email, notes, chat – without fully committing to opening a tab, keeping your workspace tidy.Mini Toolbar, Overflow Menu, and Page Tweak: Fine‑tune each page’s appearance and controls, so the web adapts to you, not the other way around.: Isolate your work, personal, and hobby browsing into completely separate profiles, each with its own extensions, cookies, and settings.For power users, we’ve added granular options throughout the browser. These are there when you want them, and out of your way when you don’t.Orion 1.0 also reflects six years of feedback from early adopters. Many invisible improvements – tab stability, memory behavior, complex web app compatibility – are a direct result of people pushing Orion hard in their daily workflows and telling us what broke.Browse Beyond ✴︎: our new signatureWith this release, we are introducing our new signature: .We originally started with the browser name ‘Kagi.’ On February 3, 2020, Vlad suggested a shortlist for rebranding: Comet, Core, Blaze, and Orion. We chose Orion not just for the name itself, but because it perfectly captured our drive for exploration and curiosity. It was a natural fit that set the stage for everything that followed.You’ll see this reflected in our refreshed visual identity:A star (✴︎) motif throughout our communication.A refined logo that now uses the same typeface as Kagi, creating a clear visual bond between our browser and our search engine.Orion is part of the broader , united by a simple idea: the internet should be built for people, not advertisers or any other third parties.Small team, sustainable modelOrion is built by a team of just six developers.To put that in perspective:That’s roughly 10% of the size of the “small” browser teams at larger companies.And a rounding error compared to the teams behind Chrome or Edge.Yet, the impact is real: over 1 million downloads to date, and a dedicated community of 2480 paid subscribers who make this independence possible.For the first two years, development was carried out by a single developer. Today, we are a tight knit group operating close to our users. We listen, debate, and implement fixes proposed directly by our community on OrionFeedback.org.This is our only source of decision making, rather than any usage analytics or patterns, because remember, Orion is zero-telemetry!This small team approach lets us move quickly, stay focused, and avoid the bloat or hype that often comes with scale.Orion is free for everyone.Every user also receives 200 free Kagi searches, with no account or sign‑up required. It’s our way of introducing you to fast, ad‑free, privacy‑respecting search from day one.But we are also 100% self‑funded. We don’t sell your data and we don’t take money from advertisers, which means we rely directly on our users to sustain the project.Tip Jar (from the app): A simple way to say “thank you” without any commitment.Supporter Subscription: $5/month or $50/year.Lifetime Access: A one‑time payment of $150 for life.Supporters (via subscription or lifetime purchase) unlock a set of  perks available today, including:Floating windows: Keep a video or window on top of other apps.Customization: Programmable buttons and custom application icons.Early access to new, supporter‑exclusive features we’re already building for next year.By supporting Orion, you’re not just funding a browser – you are co‑funding a better web with humans at the center.Orion 1.0 is just the beginning. Our goal is simple: Browse Beyond, everywhere.
Our flagship browser, six years in the making. Built natively for Mac, with performance and detail that only come from living on the platform for a long time. Download it now.
Trusted daily by users who want features no other mobile browser offers. Native iOS performance with capabilities that redefine what’s possible on mobile. Download it now.
Currently in alpha for users who value choice and independence. Native Linux performance, with the same privacy‑first approach as on macOS.Sign up for our newsletter to follow development and join the early testing wave.Orion for Windows (in development)
We have officially started development on Orion for Windows, with a target release scheduled for . Our goal is full parity with Orion 1.0 for macOS, including synchronized profiles and Orion+ benefits across platforms. Sign up for our newsletter to follow development and join the early testing wave.Synchronization will work seamlessly across devices, so your browsing experience follows you, not the other way around.From early testers to privacy advocates and power users, Orion has grown through the voices of its community.We’ll continue to surface community stories and feedback as Orion evolves. If you share your experience publicly, there’s a good chance we’ll see it.Hitting v1.0 is a big milestone, but we’re just getting started.Over the next year, our roadmap is densely packed with:Deeper customization options for power users.Further improvements to stability and complex web app performance.New Orion+ features that push what a browser can do while keeping it simple for everyone else.Tighter integrations with Kagi’s intelligent tools – always under your control, never forced into your workflow.We’re also working on expanding and improving our website to better showcase everything Orion can do, including better documentation and onboarding for teams that want to standardize on Orion.Thank you for choosing to  with us.]]></content:encoded></item><item><title>Roblox is a problem but it&apos;s a symptom of something worse</title><link>https://www.platformer.news/roblox-ceo-interview-backlash-analysis/</link><author>FiddlerClamp</author><category>hn</category><pubDate>Tue, 25 Nov 2025 16:12:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[On Friday, the Hard Fork team published our interview with Roblox CEO David Baszucki. In the days since, it has become the most-discussed interview we've done in three years on the show. Listeners who wrote in to us said they were shocked to hear the leader of a platform with 151.5 million monthly users, most of them minors, express frustration and annoyance at being asked about the company's history of failures related to child safety. Journalists described the interview as "bizarre," "unhinged," and a "car crash." And a case can be made that it was all of those things — even if Baszucki, in the studio afterwards and later on X, insisted to us that he had had a good time. In the moment, though, Baszucki's dismissive attitude toward discussing child safety struck me as something worse: familiar.Baszucki, after all, is not the first CEO to have insisted to me that a platform's problems are smaller than I am making them out to be. Nor is he the first to blame the platform's enormous scale, or to try to change the subject. (He is the first tech CEO to suggest to me that maybe there should be prediction markets in video games for children, but that's another story.)What people found noteworthy about our interview, I think, was the fresh evidence that our most successful tech CEOs really do think and talk this way. Given a chance to display empathy for the victims of crimes his platform enabled, or to convey regret about historical safety lapses, or even just to gesture at some sense of responsibility for the hundreds of millions of children who in various ways are depending on him, the CEO throws up his hands and asks: how long are you guys going to be going on about all this stuff? Roblox is different from other social products in that it explicitly courts users as young as 5. (You are supposed to be at least 13 to use Instagram, TikTok, and other major platforms.) That has always put significant pressure on the company to develop serious safety features. The company says it spends hundreds of millions of dollars a year on safety, and that 10 percent of its employees work on trust and safety issues. And trust and safety workers I know tell me that they respect Roblox's safety teams.At the same time, this is a platform launched in 2006 where, for most of its history, adults could freely approach and message any minor unless their parents had dug into the app settings. Roblox did not verify users' ages, letting any child identify as 13 or older to bypass content restrictions. Filters intended to prevent inappropriate chat or the exchange of personal information were easily bypassed by slightly changing the spelling of words. Parental controls could be circumvented simply by a child creating a new account and declaring that they were at least 13.Last year the company introduced new restrictions on chat. And this year, the company said it would deploy its own age estimation technology to determine users' ages and restrict the content available to them accordingly. This rollout was the main reason we had sought to interview Baszucki in the first place — something we had communicated to his team.Which only made it stranger when Baszucki expressed surprise at our line of inquiry and threw his PR team under the bus. ("If our PR people said, “Let’s talk about age-gating for an hour,' I’m up for it, but I love your pod. I thought I came here to talk about everything,'" he said.)Since 2018, at least two dozen people in the United States have been arrested and accused of abducting or abusing victims they met on Roblox, according to a 2024 investigation by Bloomberg. Attorneys general in Texas, Kentucky, and Louisiana have filed lawsuits against Roblox alleging that the platform facilitates child exploitation and grooming. More than 35 families have filed lawsuits against the company over child predation.As recently as this month, a reporter for the  created an account presenting herself as a child and found that in Roblox she could wander user-created strip clubs, casinos, and horror games. In one "hangout" game, in which she identified as a 13-year-old, another avatar sexually assaulted her by thrusting his hips into her avatar's face as she begged him to leave her alone.It's true that any platform that lets strangers communicate will lead to real-world harm. I believe that millions of children use Roblox daily without incident. And we would not want to shut down the entire internet to prevent a single bad thing from ever happening. But there is much a leader can do with the knowledge that his platform will inevitably lead to harm, should he wish. Understanding how attractive Roblox would be to predators, the company long ago could have blocked unrestricted contact between adults and minors. It could have adopted age verification before a wave of state legislation signaled that it would soon become mandatory anyway. It could have made it harder for children under 13 to create new accounts, and require them to get parental consent in a way it could verify.But doing so would require Roblox to focus on outcomes for children, at the likely expense of growth. And so here we are.Galling? Yes. But like I said: it's also familiar. Over and over again, we have seen leaders in Baszucki's position choose growth over guardrails. Safety features come out years after the need for them is identified, if at all. Internal critics are sidelined, laid off, or managed out. And when journalists ask, politely but insistently, why so many of their users are suffering, executives laugh and tell us that we're the crazy ones.Look at OpenAI, where the company is reckoning with the fact that making its models less sycophantic has been worse for user engagement — and is building new features to turn the engagement dial back up.Look at TikTok, which has answered concerns that short-form video is worsening academic performance for children with new "digital well-being features" that include an affirmation journal, a "background sound generator aimed at improving the mental health of its users," and "new badges to reward people who use the platform within limits, especially teens." Answering concerns that teens are using the app too much with more reasons to use the app.Or look at Meta, where new court filings from over the weekend allege ... a truly staggering number of things. To name a few: the company "stalled internal efforts to prevent child predators from contacting minors for years due to growth concerns," according to Jeff Horwitz in Reuters; "recognized that optimizing its products to increase teen engagement resulted in serving them more harmful content, but did so anyway"; and gave users 17 attempts to traffic people for sex before banning their accounts. (Meta denies the allegations, which are drawn from internal documents that have not been made public; Meta has also objected to unsealing the documents.) Lawsuits will always contain the most salacious allegations lawyers can find, of course. But what struck me about these latest filings is not the lawyers' predictably self-serving framing but rather the quotes from Meta's own employees.When the company declined to publish internal research from 2019 which showed that no longer looking at Facebook and Instagram improved users' mental health, one employee said: "If the results are bad and we don’t publish and they leak ... is it going to look like tobacco companies doing research and knowing cigs were bad and then keeping that info to themselves?”When Meta researchers found that by 2018, approximately 40 percent of children ages 9 to 12 were daily Instagram users — despite the fact that you are supposed to be 13 to join — some employees bristled at what they perceived as tacit encouragement from executives to accelerate growth efforts among children. "Oh good, we’re going after <13 year olds now?” one wrote, as cited in 's account of the brief. “Zuck has been talking about that for a while...targeting 11 year olds feels like tobacco companies a couple decades ago (and today). Like we’re seriously saying ‘we have to hook them young’ here.” When Meta studied the potential of its products to be addictive in 2018, it found that 55 percent of 20,000 surveyed users showed at least some signs of "problematic use." When it published that research the following year, though, it redefined "problematic use" to include only the most severe cases — 3.1 percent of users. “Because our product exploits weaknesses in the human psychology to promote product engagement and time spent,” a user experience researcher wrote, the company should “alert people to the effect that the product has on their brain.”You will not be surprised to learn that the company did not alert people to the issue. As usual, the rank-and-file employees are doing their job. Over and over again, though, their boss' boss tells them to stop.Americans have short attention spans — and lots to worry about. The tech backlash that kicked off in 2017 inspired platforms to make meaningful and effective investments in content moderation, cybersecurity, platform integrity, and other teams that worked to protect their user bases. Imperfect as these efforts were, they bolstered my sense that tech platforms were susceptible to pressure from the public, from lawmakers and from journalists. They acted slowly, and incompletely, but at least they acted.Fast forward to today and the bargain no longer holds. Platforms do whatever the president of the United States tells them to do, and very little else. Shame, that once-great regulator of social norms and executive behavior, has all but disappeared from public life. In its place is denial, defiance, and the noxious vice signaling of the investor class.I'm still reckoning with what it means to do journalism in a world where the truth can barely hold anyone's attention — much less hold a platform accountable, in any real sense of that word. I'm rethinking how to cover tech policy at a time when it is being made by whim. I'm noticing the degree to which platforms wish to be judged only by their stated intentions, and almost never on the outcomes of anyone who uses them. In the meantime the platforms hurtle onward, pitching ever-more fantastical visions of the future while seeming barely interested in stewarding the present.For the moment, I'm grateful that a car-crash interview drew attention to one CEO's exasperation with being asked about that. But the real problem isn't that David Baszucki talks this way. It's that so many of his peers do, too.Unknown number calling? It’s not random…The BBC caught scam call center workers on hidden cameras as they laughed at the people they were tricking.One worker bragged about making $250k from victims. The disturbing truth?Scammers don’t pick phone numbers at random. They buy your data from brokers.Once your data is out there, it’s not just calls. It’s phishing, impersonation, and identity theft.That’s why we recommend Incogni: They delete your info from the web, monitor and follow up automatically, and continue to erase data as new risks appear. Black Friday deal: Try Incogni here and get 55% off your subscription with code Trump backs down on AI preemption Facing criticism from both parties, the  administration backed down from issuing an executive order that would have effectively placed a moratorium on state AI regulations, .The order would have fought state regulations by withholding federal funding and establishing an “AI Litigation Task Force” to “challenge State AI laws.”Last week we  the draft executive order and how Trump’s attempts to squash state AI regulation have drawn bipartisan backlash — and made Republicans increasingly more sympathetic to the views of AI safety advocates.It's always hard to guess when Trump's instinct to do as he pleases will be thwarted by political opposition. In this case, though, the revived moratorium had little support outside the David Sacks wing of the party. And so — for now, anyway — it fell apart. State lawmakers are fighting the moratorium proposal Trump made to Congress. Today, a letter  by 280 state lawmakers urged Congress to “reject any provision that overrides state and local AI legislation.”A moratorium would threaten existing laws that “strengthen consumer transparency, guide responsible government procurement, protect patients, and support artists and creators,” the letter said.On the other side of the debate, the tech-funded industry PAC Leading the Future announced a $10 million campaign to push Congress to pass national AI regulations that would supersede state law.  X’s "About This Account" meltdownOn Friday,  debuted its  feature globally in a rollout that descended into chaos over the feature’s accidental uncovering of foreign actors behind popular right-wing accounts that actively share news on US politics. X users can now see the date an account joined the platform, how many times it has changed its username, and most importantly, the country or region it’s based in. The move,  X head of product , “is an important first step to securing the integrity of the global town square.”But the feature has had an unintended consequence: it revealed that big pro-Trump accounts like , a right-wing user with nearly 400,000 followers that regularly shares news about US politics, aren't actually based in the US. MAGANationX, for example, is based in , according to X. Other popular right-wing accounts — that use names from the Trump family — like (1 million followers before it was suspended),  (nearly 600,000 followers), and  (more than 11,000 followers), appear to be based in , Eastern Europe, and  respectively. The data could be skewed by travel, VPNs, or old IP addresses, and some have complained their location is inaccurate. Bier said the rollout has “a few rough edges” that will be resolved by Tuesday. One of ’s promises during the takeover of Twitter was to purge the platform of inauthentic accounts. But several studies  that suspected inauthentic activity has remained at about the same levels. X has long struggled with troll farms spreading misinformation, boosted by its tendency to monetarily reward engagement. Accusations of foreign actors spreading fake news flew on both sides of the aisle. When the feature appeared to be pulled for a short period of time, Republican Gov.  of Florida  “X needs to reinstate county-of-origin — it helps expose the grift.” In a  that garnered 3.2 million views,  attached a screenshot of ’s profile, which shows the account’s based in India: “BREAKING: American guy is not actually an American guy.”“When an American billionaire offers money to people from relatively poor countries for riling up and radicalising Americans, it's not surprising that they'll take up the offer,”  in a post that garnered nearly 700,000 views. In perhaps the most devastating consequence of the feature,  said they “spent 2 years acting mysterious over what country I live in just for Elon to fuck it all up with a single update” in a  that has 4.3 million views and 90,000 likes. How President  right-wing trolls and AI memes. The crypto crash  about $1 billion out of the Trump family fortune. Gamers are and  to prepare for  raids. How Democrats  their online strategy to catch up with Republicans.In the last month,  more about politics than about his companies on . Hundreds of English-language websites  articles from a pro-Kremlin disinformation network and are being used to "groom" AI chatbots into spreading Russian propaganda, a study found.  and  they’re now prototyping their hardware device, but it remains two years away. An in-depth look at 's mental health crisis after GPT-4o details how the company  after reports of harmful interactions. OpenAI safety research leader , who led ChatGPT’s responses to mental health crises, is . A  of ChatGPT’s new personal shopping agent.Anthropic , which it said is the best model for software engineering. Other highlights from the launch: it outscored human engineering candidates on a take-home exam, is cheaper than , can keep a chat going indefinitely via ongoing summarization of past chats, and is harder to trick with prompt injection.  In other research, AI models can unintentionally develop misaligned behaviors after learning to cheat, . (This won an approving tweet from , who hadn't posted about AI on X in more than a year.)Why ’s $27 billion data center and its debt  on its balance sheet. Meta is  into electricity trading to speed up its power plant construction.  a nickname feature for anonymous posting.A judge is  on remedies for ’s adtech monopoly next year.  its probe into Google over unfair practices that used personal data. Google stock  at a record high last week after the successful launch of .  ads. Something for the AI skeptics: Google  its serving capacity every six months to meet current demand for AI services,  VP  said.AI demand has strained the memory chip supply chain, chipmakers .   more than 900 data centers — more than previously known — in more than 50 countries. Its Autonomous Threat Analysis system  specialized AI agents for debugging.  it would invest $50 billion in AI capabilities for federal agencies.  to 's list of platforms banned for under-16s.  was spared.  it ended talks on a $3.5 billion take-private deal, citing uncertainty over financing.Interviews with AI quality raters who  their friends and family not to use the tech. How AI  the fundamental method of online survey research by evading bot detection techniques. Insurers  to limit their liability on claims related to AI. Another look at how America’s economy is now deeply  AI stocks and their performance. Scientists  an AI model that can flag human genetic mutations likely to cause disease. ]]></content:encoded></item><item><title>New layouts with CSS Subgrid</title><link>https://www.joshwcomeau.com/css/subgrid/</link><author>joshwcomeau</author><category>hn</category><pubDate>Tue, 25 Nov 2025 15:57:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[When CSS Grid layout was first released, it came with a big asterisk: only the grid’s  could participate in the layout. “Subgrid” is a newer addition to CSS Grid which allows us to  the grid layout down through the DOM tree.When I first heard about subgrid, it seemed to me like a convenience, a way to make it a bit simpler to accomplish the same stuff I was already doing. As it turns out, subgrid is  more interesting than that. It opens whole new doors in terms of the UIs we can build!In this tutorial, I’ll show you some of the exciting new things we can do with subgrid. Along the way, you’ll learn the basic mechanics of subgrid. We’ll even go over the most common gotchas!We’ll get to the interesting stuff soon, but first, let’s start with the basics.Suppose we want to implement the following mockup:We can create this layout using a flat grid, no subgrid required. Here’s a quick implementation:
  .grid  / =My Portfolio
      A small selection of the works created using Blender. No robots or AI involved.
    
      In a real artist portfolio, there would be more text here.
    ============If we check the “Grid” devtools, we see that this is a 4x2 grid, with the header spanning the first two rows:In order for this to work  subgrid, every grid participant has to be a direct child of the  container. Sure enough, if we inspect the HTML, we see the following structure:Semantically, this feels a bit  to me. I feel like these images should be grouped in a list, since we’re displaying a  of portfolio pieces. Proper semantic markup will provide more context to folks using assistive technologies like screen readers, and to search engines that are trying to make sense of our page.Unfortunately, adding this extra markup throws a wrench into the grid:=My Portfolio
      A small selection of the works created using Blender. No robots or AI involved.
    
      In a real artist portfolio, there would be more text here.
    ============Instead of having each image occupy its own grid cell, we instead cram the entire list of images into a single cell in the second column, leaving the final two columns totally empty. 😬CSS subgrid allows us to extend the parent grid through that  tag, so that the images can participate in the main grid. Here’s what that looks like:
  .grid 
  .grid  / 
  .grid =My Portfolio
      A small selection of the works created using Blender. No robots or AI involved.
    
      In a real artist portfolio, there would be more text here.
    ============There’s a lot going on here, so let’s unpack it.Using  and , we assign the  to span three columns and two rows. This is how we specify  of the grid we want to share with the ’s descendants. We’ll dig more into this later.Next, we apply  to the , to create a new child grid.Finally, we pass along the row/column definitions using  and . The  keyword is the key bit of magic that ties the two grids together, allowing each  to occupy its own cell in the parent grid.When I first learned about subgrid, this is the sort of scenario I was imagining: cases where nested HTML elements like  +  or  +  block us from assigning the actual UI elements to the grid. CSS subgrid  a nifty lil’ escape hatch for these types of situations!That said, it's not like we haven’t had other ways to solve these kinds of problems. Instead of sharing a single CSS grid template with subgrid, we could instead combine a Flexbox row with a nested grid:
  .wrapper 
    .grid =My Portfolio
      A small selection of the works created using Blender. No robots or AI involved.
    
      In a real artist portfolio, there would be more text here.
    =======Instead of trying to rig everything up to use a single grid structure, we can often create the same layout with nested combinations of Flexbox/Grid. And honestly, I think I prefer this approach in this case! It feels simpler to me.But like I said earlier, this isn’t the most exciting use case for subgrid. Now that we’ve covered the basic syntax, we can explore some of the more interesting possibilities. 😄Sticking with the artist portfolio example, let’s suppose we have this card design:I created this render for the Animation Design module in my upcoming course,Whimsical. The fish is a nod to Bret Victor’s talk, “Stop Drawing Dead Fish”, which is referenced in the course.This looks alright on its own, but something funky happens when we put it in a grid:
  .grid 
  .grid ====Bret’s Dead Fish
        I created this render for the Animation Design module in my
        upcoming course,
        ==Whimsical Animations. The fish is a nod to Bret Victor’s talk, “Stop Drawing Dead
        Fish”, which is referenced in the course.
      ===Big Shoes To Fill
        In this piece, I tried to create my own sneaker design, taking
        inspiration from the Air Force Ones I’ve been wearing for most of
        my adult life. Topographically, shoes are a really weird shape, so
        this was a good challenge!
      ===Guitar Pedalboard
        Over the past few years, I’ve been getting back into music
        production, and have started collecting effect pedals. This render
        is my attempt to create my own pedal designs. The middle one is
        meant to look a bit like Zoidberg.
      ===Infinite Supercomputer
        I spent more time than I’d care to admit creating an enormous
        machine in Blender, full of weird knobs and sliders and extras. The
        goal was to produce a completely ridiculous cockpit-style panel.
      Notice that the images are different widths? The fish image, for example, is much wider than the final supercomputer image. What’s going on here? 🤔Well, let’s take a look at the CSS. The four cards are arranged in a two-column grid (which shrinks to a one-column grid on smaller screens):We’re populating this top-level grid with four  cards. Each card declares its own two-column grid:The goal here is for the image to take up the lion’s share of the space within each card, since that’s the important part (the point of an artist’s portfolio, after all, is to showcase the art!). But the  unit is designed to be flexible; it will  to match the requested ratio, but it’ll adapt based on the content.This is actually a very good thing. We  force the image column to be a fixed size, but we wouldn’t like the results:
  .grid 
  .grid ====Bret’s Dead Fish
        I created this render for the Animation Design module in my
        upcoming course,
        ==Whimsical Animations. The fish is a nod to Bret Victor’s talk, “Stop Drawing Dead
        Fish”, which is referenced in the course.
      ===Big Shoes To Fill
        In this piece, I tried to create my own sneaker design, taking
        inspiration from the Air Force Ones I’ve been wearing for most of
        my adult life. Topographically, shoes are a really weird shape, so
        this was a good challenge!
      ===Guitar Pedalboard
        Over the past few years, I’ve been getting back into music
        production, and have started collecting effect pedals. This render
        is my attempt to create my own pedal designs. The middle one is
        meant to look a bit like Zoidberg.
      ===Infinite Supercomputer
        I spent more time than I’d care to admit creating an enormous
        machine in Blender, full of weird knobs and sliders and extras. The
        goal was to produce a completely ridiculous cockpit-style panel.
      On certain viewport sizes, the cards simply aren’t large enough to devote ⅔rds of the available space to the image  still contain the text content. If we force that column to have a fixed size, the text could wind up overflowing:So, the flexibility we get from the  unit is a good thing. The problem is that each card is doing its own internal calculation. The heading in the first card (“Bret’s Dead Fish”) is made up of small words, so it can fit comfortably in a narrow column. But the final card’s heading (“Infinite Supercomputer”) requires quite a bit more room.If you’ve worked with CSS for a while, you’ve probably gotten stuck in cul-de-sacs like this. One of the hardest problems in CSS is when  need to be aware of each other inside nested / complex layouts.Miraculously, subgrid offers a solution to these sorts of problems. Check this out:
  .grid 
  .grid ====Bret’s Dead Fish
        I created this render for the Animation Design module in my
        upcoming course,
        ==Whimsical Animations. The fish is a nod to Bret Victor’s talk, “Stop Drawing Dead
        Fish”, which is referenced in the course.
      ===Big Shoes To Fill
        In this piece, I tried to create my own sneaker design, taking
        inspiration from the Air Force Ones I’ve been wearing for most of
        my adult life. Topographically, shoes are a really weird shape, so
        this was a good challenge!
      ===Guitar Pedalboard
        Over the past few years, I’ve been getting back into music
        production, and have started collecting effect pedals. This render
        is my attempt to create my own pedal designs. The middle one is
        meant to look a bit like Zoidberg.
      ===Infinite Supercomputer
        I spent more time than I’d care to admit creating an enormous
        machine in Blender, full of weird knobs and sliders and extras. The
        goal was to produce a completely ridiculous cockpit-style panel.
      In the original version, the parent grid was a one-column layout (on smaller screens), and it contained a bunch of independent grids. In this new version, the  grid holds the two-column layout:In the original version, the parent grid was a two-column layout, with each card assigned to a grid cell. In this new version, the parent grid grows to  columns:Each  will span two of these columns (), and inherits the column definitions from the parent (grid-template-column: subgrid).As a result, the grid can dynamically react to content changes. Try erasing the word “Supercomputer” in the playground above and notice how the columns readjust!As a result, the grid can dynamically react to content changes. If that final card (“Infinite Supercomputer”) had a shorter title, the whole grid would rearrange, shrinking the text columns and allowing more of the images to be shown.Honestly, I’m not really used to thinking about layouts like this. Before subgrid, I would’ve solved this problem by picking a very narrow fixed width for the image column, so that there was always enough space for the text column. This would ensure that the layout never breaks, but remember, the goal of a portfolio is to display as much of the images as possible! Subgrid allows us to adapt to the content dynamically, so that we can produce the best possible UI in various contexts.This is where subgrid truly shines, in my opinion. By extending the grid downwards, it means that we can allow siblings to become responsive to each other, in a way that hasn’t been possible until now. ✨As I’ve been experimenting with subgrid, there have been a couple of things that have caught me off guard. Let’s go over them, so that you’re well-prepared!Sharing  with subgrid tends to be pretty intuitive, but things get a bit more quirky when sharing .To help me explain, let’s look at a different example. Suppose our design team wants us to build the following pricing UI, to show the features included at different price tiers:This  like a pretty straightforward task, but the devil is in the details. If we use a typical Grid or Flexbox strategy, we’ll wind up with asymmetrical rows:This might  right at a quick glance, but notice how the features don’t line up. In the original mockup, the first line of every feature is perfectly aligned with the same feature in the opposite card!Historically, the only way to achieve this sort of thing in CSS has been with Table layout (using  tags, or ). It’s not really practical to use a table here, though, since we’d need each card to be its own column in the same table, and we can’t easily style table columns. At least in theory, we should be able to let both cards share a single grid, like this:Unfortunately, there’s a very easy mistake to make. See if you can spot the problem with this code:
  .grid 

    .card .card ==Pro PackageUp to 4 team accounts.Basic workflows.Connect with Slack™.Up to 3 knowledge bases, with 100gb total storage.Limited AI assistant (depending on region and language).=Enterprise PackageUnlimited team accounts.Advanced, fully-customizeable workflows.Connect with Slack™, Microsoft Teams™, Discord™, and 5 other popular integrations.Unlimited knowledge bases.Unlimited robots. 🤖All of the text is clumped up in the same spot! If we inspect this using the Grid devtools, we discover that we’ve wound up with a 2×1 grid. All of the content within each card is smushed into a single row. 😬Typically, with CSS Grid, we don’t need to explicitly define any rows. I usually define the number of , and trust the grid algorithm to add new rows as-needed, so that each child gets its own grid cell.Unfortunately, with subgrid, it doesn't quite work like this. By default, our child grid will only span a single grid column/row. If we want it to occupy  rows, we need to reserve them explicitly.Here’s what the fix looks like:
  .grid 

    .card 
    .card ==Pro PackageUp to 4 team accounts.Basic workflows.Connect with Slack™.Up to 3 knowledge bases, with 100gb total storage.Limited AI assistant (depending on region and language).=Enterprise PackageUnlimited team accounts.Advanced, fully-customizeable workflows.Connect with Slack™, Microsoft Teams™, Discord™, and 5 other popular integrations.Unlimited knowledge bases.Unlimited robots. 🤖The extra-complicated thing about this setup is that we’re extending the grid down  layers:First, we extend it to , which includes an  and a .Next, we extend it to that child , so that the individual list items each get their own row.There are 5 list items in this case, which means we need 6 rows total (one for the heading, five for the list). If we don’t “reserve” all of these rows explicitly, then the browser will shove everything into a single row and make a big mess, like we saw above. I’m not exactly sure  the typical auto-assignment algorithm doesn’t work with subgrid, but I assume there’s some technical limitation.This is mind-bending stuff, but it becomes intuitive with a bit of practice. The thing to keep in mind is that subgrids, by default, will only occupy a single grid cell. In order to spread a group of items across multiple grid rows, the subgrid must first stretch across that area itself.We got the gnarliest gotcha out of the way first! I promise the next two won’t be as intellectually taxing. 😅In CSS grid, the lines between each column are numbered, and we can assign grid children using these numbers. This is something we explore in greater depth in “An Interactive Guide to CSS Grid”:When we inherit a portion of the grid using grid-template-rows: subgrid or grid-template-columns: subgrid, the line numbers get reset.Here’s an example of what I’m talking about:
  .grid 

    .subgrid  /  / 

      .child ===Our yellow  is assigned to  and , but it winds up sitting in the  of the grid’s four rows and columns. 🤔It turns out that while the grid  is inherited with subgrid, the  don’t. Our  grid inherits columns/rows 2 through 4, but internally, they get re-indexed as 1 through 3.We can see this using the grid devtools in the Elements inspector:In my mind, I had been thinking of line numbers as unique IDs, and so I figured that if the subgrid is inheriting the grid template, those IDs would come along for the ride too. But if we think of these line numbers as  rather than IDs, this behaviour makes a lot more sense. In every grid, the first line has index 1, even if that row/column is inherited from a parent grid.Perhaps the most famous grid snippet is this lil’ guy:This is a  concept. Instead of specifying different grid templates at different viewport sizes using media queries, we specify that we want as many columns as possible, as long as they’re all at least 100px wide (or whatever the minimum specified size is).Try resizing the “Result” pane by dragging the vertical divider, and notice how the columns adjust:
  .grid ==A=B=C=D=E=FThis is a very cool approach, but unfortunately, it doesn’t quite work with some of the new UI possibilities introduced by subgrid. For example, the “portfolio card” grid we explored earlier requires that we list the specific number of columns. We can’t use  or .Subgrid has been supported across all major browsers since 2023. Surprisingly, though, subgrid support still hasn’t hit 90% yet (according to, as of November 2025).This presents a bit of a challenge. As we’ve seen in this blog post, subgrid enables us to solve problems that were previously unsolvable. What should we do for folks who visit using older browsers?Well, we can’t produce an  experience, but I think with a bit of creative problem-solving, we can come up with alternative layouts that are . Using the artist portfolio example from earlier, we could reconfigure the card layout so that the image is stacked vertically, rather than horizontally:We can accomplish this using feature queries. Here’s what the code looks like:Alternatively, I could have kept the two-column layout but restricted the image column’s width (eg. grid-template-columns: 50px 1fr). This would’ve preserved the original design for everyone. But I think when it comes to fallbacks, the goal isn't to be as similar to the original as possible, the goal is to produce the best experience possible. In this particular case, I think a single-column fallback experience works better.I’m publishing this post on November 25th, a frankly miserable time of year up here in the northern hemisphere 😅. The days are getting shorter, the weather is getting colder, and my favourite season (autumn) is transmogrifying into my least favourite season (winter).But there is one silver lining about this time of year: everything’s on sale for Black Friday! 🎈If you found this blog post useful, you’ll likely get  out of my CSS course. We focus on understanding CSS at a deeper level, building an intuition for how the language actually works. No more memorizing snippets, or trying random stuff hoping that the UI will snap into the right shape!I know that in the world of e-commerce, things go on sale every other week. That’s not how I roll, though. I only have one or two sales a year. So this truly is a rare chance to pick up one of my courses for a deep discount. ✨If we pop open the grid devtools, we see that the  is one big grid, passed down through several layers of subgrids:This is incredibly cool, and I think it’s a great demonstration of the maximalist things we can do with subgrid. But, honestly, I think I’m more excited by the smaller-scale stuff we’ve seen in this blog post. 😅Subgrid is a very versatile new tool, and it can be a bit intimidating and overwhelming, but hopefully this post has given you some ideas for the sorts of things you can start experimenting with. The good news is that you don’t have to re-architect your entire project in order to start using subgrid! The most powerful parts of subgrid are things which can be incrementally adopted.Another special thanks to Kevin Powell. The examples in this blog post would’ve been far less compelling without his inspiration. 😄]]></content:encoded></item><item><title>FLUX.2: Frontier Visual Intelligence</title><link>https://bfl.ai/blog/flux-2</link><author>meetpateltech</author><category>hn</category><pubDate>Tue, 25 Nov 2025 15:47:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[FLUX.2 is designed for real-world creative workflows, not just demos or party tricks. It generates high-quality images while maintaining character and style consistency across multiple reference images, following structured prompts, reading and writing complex text, adhering to brand guidelines, and reliably handling lighting, layouts, and logos. FLUX.2 can edit images at up to 4 megapixels while preserving detail and coherence.Black Forest Labs: Open CoreWe believe visual intelligence should be shaped by researchers, creatives, and developers everywhere, not just a few. That’s why we pair frontier capability with open research and open innovation, releasing powerful, inspectable, and composable open-weight models for the community, alongside robust, production-ready endpoints for teams that need scale, reliability, and customization.When we launched Black Forest Labs in 2024, we set out to make open innovation sustainable, building on our experience developing some of the world’s most popular open models. We’ve combined open models like FLUX.1 [dev]—the most popular open image model globally—with professional-grade models like FLUX.1 Kontext [pro], which powers teams from Adobe to Meta and beyond. Our open core approach drives experimentation, invites scrutiny, lowers costs, and ensures that we can keep sharing open technology from the Black Forest and the Bay into the world.Precision, efficiency, control, extreme realism - where FLUX.1 showed the potential of media models as powerful creative tools, FLUX.2 shows how frontier capability can transform production workflows. By radically changing the economics of generation, FLUX.2 will become an indispensable part of our creative infrastructure.: FLUX.2 is capable of generating highly detailed, photoreal images along with infographics with complex typography, all at resolutions up to 4MPAll variants of FLUX.2 offer image editing from text and multiple references in one model.The FLUX.2 family covers a spectrum of model products, from fully managed, production-ready APIs to open-weight checkpoints developers can run themselves. The overview graph below shows how FLUX.2 [pro], FLUX.2 [flex], FLUX.2 [dev], and FLUX.2 [klein] balance performance, and controlGenerating designs with variable steps: FLUX.2 [flex] provides a “steps” parameter, trading off typography accuracy and latency. From left to right: 6 steps, 20 steps, 50 steps.Controlling image detail with variable steps: FLUX.2 [flex] provides a “steps” parameter, trading off image detail and latency. From left to right: 6 steps, 20 steps, 50 steps.The FLUX.2 model family delivers state-of-the-art image generation quality at extremely competitive prices, offering the best value across performance tiers.For open-weights image models, FLUX.2 [dev] sets a new standard, achieving leading performance across text-to-image generation, single-reference editing, and multi-reference editing, consistently outperforming all open-weights alternatives by a significant margin.Whether open or closed, we are committed to the responsible development of these models and services before, during, and after every release.FLUX.2 builds on a latent flow matching architecture, and combines image generation and editing in a single architecture. The model couples the Mistral-3 24B parameter vision-language model with a rectified flow transformer. The VLM brings real world knowledge and contextual understanding, while the transformer captures spatial relationships, material properties, and compositional logic that earlier architectures could not render.FLUX.2 now provides multi-reference support, with the ability to combine up to 10 images into a novel output, an output resolution of up to 4MP, substantially better prompt adherence and world knowledge, and significantly improved typography. We re-trained the model’s latent space from scratch to achieve better learnability and higher image quality at the same time, a step towards solving the “Learnability-Quality-Compression” trilemma. Technical details can be found in the FLUX.2 VAE blog post.We're building foundational infrastructure for visual intelligence, technology that transforms how the world is seen and understood. FLUX.2 is a step closer to multimodal models that unify perception, generation, memory, and reasoning, in an open and transparent way.Join us on this journey. We're hiring in Freiburg (HQ) and San Francisco. .]]></content:encoded></item><item><title>Launch HN: Onyx (YC W24) – Open-source chat UI</title><link>https://news.ycombinator.com/item?id=46045987</link><author>Weves</author><category>hn</category><pubDate>Tue, 25 Nov 2025 14:20:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hey HN, Chris and Yuhong here from Onyx (https://github.com/onyx-dot-app/onyx). We’re building an open-source chat that works with any LLM (proprietary + open weight)  gives these LLMs the tools they need to be useful (RAG, web search, MCP, deep research, memory, etc.).Two years ago, Yuhong and I had the same recurring problem. We were on growing teams and it was ridiculously difficult to find the right information across our docs, Slack, meeting notes, etc. Existing solutions required sending out our company's data, lacked customization, and frankly didn't work well. So, we started Danswer, an open-source enterprise search project built to be self-hosted and easily customized.As the project grew, we started seeing an interesting trend—even though we were explicitly a search app, people wanted to use Danswer just to chat with LLMs. We’d hear, “the connectors, indexing, and search are great, but I’m going to start by connecting GPT-4o, Claude Sonnet 4, and Qwen to provide my team with a secure way to use them”.Many users would add RAG, agents, and custom tools later, but much of the usage stayed ‘basic chat’. We thought: “why would people co-opt an enterprise search when other AI chat solutions exist?”As we continued talking to users, we realized two key points:(1) just giving a company secure access to an LLM with a great UI and simple tools is a huge part of the value add of AI(2) providing this  is much harder than you might think and the bar is incredibly highConsumer products like ChatGPT and Claude already provide a great experience—and chat with AI for work is something (ideally) everyone at the company uses 10+ times per day. People expect the same snappy, simple, and intuitive UX with a full feature set. Getting hundreds of small details right to take the experience from “this works” to “this feels magical” is not easy, and nothing else in the space has managed to do it.So ~3 months ago we pivoted to Onyx, the open-source chat UI with:- (truly) world class chat UX. Usable both by a fresh college grad who grew up with AI and an industry veteran who’s using AI tools for the first time.- Support for all the common add-ons: RAG, connectors, web search, custom tools, MCP, assistants, deep research.- RBAC, SSO, permission syncing, easy on-prem hosting to make it work for larger enterprises.Through building features like deep research and code interpreter that work across model providers, we've learned a ton of non-obvious things about engineering LLMs that have been key to making Onyx work. I'd like to share two that were particularly interesting (happy to discuss more in the comments).First, context management is one of the most difficult and important things to get right. We’ve found that LLMs really struggle to remember both system prompts and previous user messages in long conversations. Even simple instructions like “ignore sources of type X” in the system prompt are very often ignored. This is exacerbated by multiple tool calls, which can often feed in huge amounts of context. We solved this problem with a “Reminder” prompt—a short 1-3 sentence blurb injected at the end of the user message that describes the non-negotiables that the LLM must abide by. Empirically, LLMs attend most to the very end of the context window, so this placement gives the highest likelihood of adherence.Second, we’ve needed to build an understanding of the “natural tendencies” of certain models when using tools, and build around them. For example, the GPT family of models are fine-tuned to use a python code interpreter that operates in a Jupyter notebook. Even if told explicitly, it refuses to add `print()` around the last line, since, in Jupyter, this last line is automatically written to stdout. Other models don’t have this strong preference, so we’ve had to design our model-agnostic code interpreter to also automatically `print()` the last bare line.So far, we’ve had a Fortune 100 team fork Onyx and provide 10k+ employees access to every model within a single interface, and create thousands of use-case specific Assistants for every department, each using the best model for the job. We’ve seen teams operating in sensitive industries completely airgap Onyx w/ locally hosted LLMs to provide a copilot that wouldn’t have been possible otherwise.]]></content:encoded></item><item><title>APT Rust requirement raises questions</title><link>https://lwn.net/SubscriberLink/1046841/5bbf1fc049a18947/</link><author>todsacerdoti</author><category>hn</category><pubDate>Tue, 25 Nov 2025 14:18:01 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
           By November 24, 2025It is rarely newsworthy when a project or package picks up a new
dependency. However, changes in a core tool like Debian's Advanced Package
Tool (APT) can have far-reaching effects. For example, Julian
Andres Klode's declaration
that APT would require Rust in May 2026 means that a few of Debian's
unofficial ports must either acquire a working Rust toolchain or
depend on an old version of APT. This has raised several questions
within the project, particularly about the ability of a single
maintainer to make changes that have widespread impact.On October 31, Klode sent an announcement to the debian-devel
mailing list that he intended to introduce Rust dependencies and code
into APT as soon as May 2026:This extends at first to the Rust compiler and standard library, and
the Sequoia ecosystem.In particular, our code to parse .deb, .ar, .tar, and the HTTP
signature verification code would strongly benefit from memory safe
languages and a stronger approach to unit testing.If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.Klode added this was necessary so that the project as a whole could
move forward, rely on modern technologies, "and not be held back by
trying to shoehorn modern software on retro computing
devices". Some Debian developers have welcomed the news. Paul
Tagliamonte acknowledged
that it would impact unofficial Debian ports but called the push
toward Rust "".However, John Paul Adrian Glaubitz complained
that Klode's wording was unpleasant and that the approach was
confrontational. In another
message, he explained that he was not against adoption of Rust; he
had worked on enabling Rust on many of the Debian architectures and
helped to fix architecture-specific bugs in the Rust toolchain as well
as LLVM upstream. However, the message strongly suggested there was no room
for a change in plan: Klode had ended his message with "thank you for
understanding", which invited no further discussion. Glaubitz was
one of a few Debian developers who expressed discomfort with Klode's
communication style in the message.Klode noted,
briefly, that Rust was already a hard requirement for all Debian
release architectures and ports, except for Alpha (alpha), Motorola 680x0 (m68k),
PA-RISC (hppa), and
SuperH (sh4), because of
APT's use of the Sequoia-PGP
project's  tool to
verify OpenPGP
signatures. APT falls back to using the GNU Privacy Guard
signature-verification tool, , on
ports that do not have a Rust compiler. By depending directly on Rust,
though, APT itself would not be available on ports without a Rust
compiler. LWN recently
covered the state of Linux architecture support, and the status of
Rust support for each one.No AI slop, all substance: subscribe to LWN today
LWN has always been about quality over quantity; we need your help
to continue publishing in-depth, reader-focused articles about Linux
and the free-software community. Please subscribe today to support our work
and keep LWN on the air; we are offering a free one-month trial subscription to get you started.
None of the ports listed by Klode are among those officially
supported by Debian today, or targeted for support in
Debian 14 ("forky"). The sh4 port has never been officially
supported, and none of the other ports have been supported since
Debian 6.0. The actual impact on the ports lacking Rust is also
less dramatic than it sounded at first. Glaubitz assured
Antoni Boucher that "the ultimatum that Julian set doesn't really
exist", but phrasing it that way "gets more attention in the
news". Boucher is the maintainer of ,
a GCC
ahead-of-time code generator for Rust. Nothing, Glaubitz said,
stops ports from using a non-Rust version of APT until Boucher and
others manage to bootstrap Rust for those ports.David Kalnischkies, who is also a major
contributor to APT, suggested
that if the goal is to reduce bugs, it would be better to remove the
code that is used to parse the .deb, .ar, and .tar formats that Klode
mentioned from APT entirely. It is only needed for two tools, 
and ,
he said, and the only "" of
 was by Klode's employer, Canonical, for its Launchpad software-collaboration
platform. If those were taken out of the main APT code base, then it
would not matter whether they were written in Rust, Python, or another
language, since the tools are not directly necessary for any given
port.Kalnischkies also questioned the claim that Rust was necessary to
achieve the stronger approach to unit testing that Klode mentioned:You can certainly do unit tests in C++, we do. The main problem is
that someone has to write those tests. Like docs.Your new solver e.g. has none (apart from our preexisting integration
tests). You don't seriously claim that is because of C++ ?
If you don't like GoogleTest, which is what we currently have,
I could suggest doctest (as I did in previous installments).
Plenty other frameworks exist with similar or different styles.Klode has not responded to those comments yet, which is a bit
unfortunate given the fact that introducing hard dependencies on
Rust has an impact beyond his own work on APT. It may well be that he
has good answers to the questions, but it can also give the
impression that Klode is simply embracing a trend toward Rust. He is involved
in the Ubuntu work to migrate from GNU Coreutils to the Rust-based uutils. The reasons given for that work, again, are around
modernization and better security—but security is not automatically 
guaranteed simply by switching to Rust, and there are a number of
other considerations.For example, Adrian Bunk pointed
out that there are a number of Debian teams, as well as tooling,
that will be impacted by writing some of APT in Rust. The release
notes for Debian 13 ("trixie") mention
that Debian's infrastructure "currently has problems with
rebuilding packages of types that systematically use static
linking", such as those with code written in Go and Rust. Thus, "these packages will be
covered by limited security support until the infrastructure is
improved to deal with them maintainably". Limited security support
means that updates to Rust libraries are likely to only be released
when Debian publishes a point release, which happens about every two
months. The security team has specifically
stated that  is fully supported, but there are still
outstanding problems.Due to the static-linking issue, any time one of 's
dependencies, currently more than 40 Rust crates, have to be rebuilt
due to a security issue,  (at least potentially) also
needs to be rebuilt. There are also difficulties in tracking CVEs for
all of its dependencies, and understanding when a security
vulnerability in a Rust crate may require updating a Rust program that
depends on it.Fabian Grünbichler, a maintainer of Debian's Rust toolchain, listed
several outstanding problems Debian has with dealing with Rust
packages. One of the largest is the need for a consistent Debian policy for declaring
statically linked libraries. In 2022, Guillem Jover added a control
field for Debian packages called Static-Built-Using (SBU), which would list
the source packages used to build a binary package. This would
indicate when a binary package needs to be rebuilt due to an update in
another source package. For example,  depends on more than
40 Rust crates that are packaged for Debian. Without declaring the
SBUs, it may not be clear if  needs to be updated when one
of its dependencies is updated. Debian has been working on a policy
requirement for SBU since April 2024, but it is not yet finished
or adopted.The discussion sparked by Grünbichler makes clear that most of
Debian's Rust-related problems are in the process of being
solved. However, there's no evidence that Klode explored the problems
before declaring that APT would depend on Rust, or even asked "is this
a reasonable time frame to introduce this dependency?"Where tradition meets tomorrowDebian's tagline, or at least one of its taglines, is "the
universal operating system", meaning that the project aims to run on a
wide variety of hardware (old and new) and be usable on the desktop,
server, IoT devices, and more. The "Why Debian" page
lists a number of reasons users and developers should choose the
distribution: multiple hardware
architectures, long-term
support, and its democratic governance
structure are just a few of the arguments it puts forward in favor
of Debian. It also notes that "Debian cannot be controlled by a
single company". A single developer employed by a company to work
on Debian tools pushing a change that seems beneficial to that
company, without discussion or debate, that impacts multiple hardware
architectures and that requires other volunteers to do unplanned work
or meet an artificial deadline seems to go against many of the
project's stated values.Debian, of course, does have checks and balances that could be
employed if other Debian developers feel it necessary. Someone could,
for example, appeal to Debian's Technical Committee,
or sponsor a general resolution to override a developer if they cannot
be persuaded by discussion alone. That happened recently when the committee required systemd
maintainers to provide the  directory "until
a satisfactory migration of impacted software has occurred and Policy
updated accordingly".However, it also seems fair to point out that Debian can move
slowly, even glacially, at times. APT added
support for the DEB822
format for its source information lists in 2015. Despite APT
supporting that format for years, Klode faced resistance in 2021, when
he pushed
for Debian to move to the new format ahead of the Debian 12
("bookworm") release in 2021, but was unsuccessful. It is now the
default for trixie with the move to APT 3.0, though APT
will continue to support the old format for years to come.The fact is, regardless of what Klode does with APT, more and more
free software is being written (or rewritten) in Rust. Making it
easier to support that software when it is packaged for Debian is to
everyone's benefit. Perhaps the project needs some developers who will
be aggressive about pushing the project to move more quickly in
improving its support for Rust. However, what is really needed is more
developers lending a hand to do the work that is needed to support
Rust in Debian and elsewhere, such as . It does not
seem in keeping with Debian's community focus for a single developer
to simply declare dependencies that other volunteers will have to
scramble to support.]]></content:encoded></item><item><title>Brain has five &apos;eras&apos; with adult mode not starting until early 30s</title><link>https://www.theguardian.com/science/2025/nov/25/brain-human-cognitive-development-life-stages-cambridge-study</link><author>hackernj</author><category>hn</category><pubDate>Tue, 25 Nov 2025 13:38:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Scientists have identified five major “epochs” of human brain development in one of the most comprehensive studies to date of how neural wiring changes from infancy to old age.The study, based on the brain scans of nearly 4,000 people aged under one to 90, mapped neural connections and how they evolve during our lives. This revealed five broad phases, split up by four pivotal “turning points” in which brain organisation moves on to a different trajectory, at around the ages of nine, 32, 66 and 83 years.“Looking back, many of us feel our lives have been characterised by different phases. It turns out that brains also go through these eras,” said Prof Duncan Astle, a researcher in neuroinformatics at Cambridge University and senior author of the study.“Understanding that the brain’s structural journey is not a question of steady progression, but rather one of a few major turning points, will help us identify when and how its wiring is vulnerable to disruption.”The childhood period of development was found to occur between birth until the age of nine, when it transitions to the adolescent phase – an era that lasts up to the age of 32, on average.In a person’s early 30s the brain’s neural wiring shifts into adult mode – the longest era, lasting more than three decades. A third turning point around the age of 66 marks the start of an “early ageing” phase of brain architecture. Finally, the “late ageing” brain takes shape at around 83 years old.The scientists quantified brain organisation using 12 different measures, including the efficiency of the wiring, how compartmentalised it is and whether the brain relies heavily on central hubs or has a more diffuse connectivity network.From infancy through childhood, our brains are defined by “network consolidation”, as the wealth of synapses – the connectors between neurons – in a baby’s brain are whittled down, with the more active ones surviving. During this period, the study found, the efficiency of the brain’s wiring decreases.Meanwhile, grey and white matter grow rapidly in volume, so that cortical thickness – the distance between outer grey matter and inner white matter – reaches a peak, and cortical folding, the characteristic ridges on the outer brain, stabilises.In the second “epoch” of the brain, the adolescence era, white matter continues to grow in volume, so organisation of the brain’s communications networks is increasingly refined. This era is defined by steadily increasing efficiency of connections across the whole brain, which is related to enhanced cognitive performance. The epochs were defined by the brain remaining on a constant trend of development over a sustained period, rather than staying in a fixed state throughout.“We’re definitely not saying that people in their late 20s are going to be acting like teenagers, or even that their brain looks like that of a teenager,” said Alexa Mousley, who led the research. “It’s really the pattern of change.”skip past newsletter promotionafter newsletter promotionShe added that the findings could give insights into risk factors for mental health disorders, which most frequently emerge during the adolescent period.At around the age of 32 the strongest overall shift in trajectory is seen. Life events such as parenthood may play a role in some of the changes seen, although the research did not explicitly test this. “We know that women who give birth, their brain changes afterwards,” said Mousley. “It’s reasonable to assume that there could be a relationship between these milestones and what’s happening in the brain.”From 32 years, the brain architecture appears to stabilise compared with previous phases, corresponding with a “plateau in intelligence and personality” based on other studies. Brain regions also become more compartmentalised.The final two turning points were defined by decreases in brain connectivity, which were believed to be related to ageing and degeneration of white matter in the brain.]]></content:encoded></item><item><title>Constant-time support coming to LLVM: Protecting cryptographic code</title><link>https://blog.trailofbits.com/2025/11/25/constant-time-support-coming-to-llvm-protecting-cryptographic-code-at-the-compiler-level/</link><author>ahlCVA</author><category>hn</category><pubDate>Tue, 25 Nov 2025 13:02:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Trillions spent and big software projects are still failing</title><link>https://spectrum.ieee.org/it-management-software-failures</link><author>pseudolus</author><category>hn</category><pubDate>Tue, 25 Nov 2025 12:14:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[“Why worry about something that isn’t going to happen?”KGB Chairman Charkov’s question to inorganic chemist Valery Legasov in HBO’s “Chernobyl” miniseries makes a good epitaph for the hundreds of software development, modernization, and operational failures I have covered for  since my first contribution, to its September 2005 special issue on learning—or rather, not learning—from software failures. I noted then, and it’s still true two decades later: Software failures are universally unbiased. They happen in every country, to large companies and small. They happen in commercial, nonprofit, and governmental organizations, regardless of status or reputation.Global IT spending has more than tripled in constant 2025 dollars since 2005, from US $1.7 trillion to $5.6 trillion, and continues to rise. Despite additional spending, software success rates have not markedly improved in the past two decades. The result is that the business and societal costs of failure continue to grow as software proliferates, permeating and interconnecting every aspect of our lives.For those hoping AI software tools and coding copilots will quickly make large-scale IT software projects successful, forget about it. For the foreseeable future, there are hard limits on what AI can bring to the table in controlling and managing the myriad intersections and trade-offs among systems engineering, project, financial, and business management, and especially the organizational politics involved in any large-scale software project. Few IT projects are displays of rational decision-making from which AI can or should learn. As software practitioners know, IT projects suffer from enough management hallucinations and delusions without AI adding to them.As I noted 20 years ago, the drivers of software failure frequently are failures of human imagination, unrealistic or unarticulated project goals, the inability to handle the project’s complexity, or unmanaged risks, to name a few that today still regularly cause IT failures. Numerous others go back decades, such as those identified by Stephen Andriole, the chair of business technology at Villanova University’s School of Business, in the diagram below first published in  in 2021. Uncovering a software system failure that has gone off the rails in a unique, previously undocumented manner would be surprising because the overwhelming majority of software-related failures involve avoidable, known failure-inducing factors documented in hundreds of after-action reports, academic studies, and technical and management books for decades. Failure déjà vu dominates the literature.The question is, why haven’t we applied what we have repeatedly been forced to learn?The Phoenix That Never RosePhoenix project executives believed they could deliver a modernized payment system, customizing PeopleSoft’s off-the-shelf payroll package to follow 80,000 pay rules spanning 105 collective agreements with federal public-service unions. It also was attempting to implement 34 human-resource system interfaces across 101 government agencies and departments required for sharing employee data. Further, the government’s developer team thought they could accomplish this for less than 60 percent of the vendor’s proposed budget. They’d save by removing or deferring critical payroll functions, reducing system and integration testing, decreasing the number of contractors and government staff working on the project, and forgoing vital pilot testing, along with a host of other overly optimistic proposals.Phoenix’s payroll meltdown was preordained. As a result, over the past nine years, around 70 percent of the 430,000 current and former Canadian federal government employees paid through Phoenix have endured paycheck errors. Even as recently as fiscal year 2023–2024, a third of all employees experienced paycheck mistakes. The ongoing financial stress and anxieties for thousands of employees and their families have been immeasurable. Not only are recurring paycheck troubles sapping worker morale, but in at least one documented case, a coroner blamed an employee’s suicide on the unbearable financial and emotional strain she suffered.The question is, why haven’t we applied what we have repeatedly been forced to learn?What percentage of software projects fail, and what failure means, has been an ongoing debate within the IT community stretching back decades. Without diving into the debate, it’s clear that software development remains one of the riskiest technological endeavors to undertake. Indeed, according to Bent Flyvbjerg, professor emeritus at the University of Oxford’s Saїd Business School, comprehensive data shows that not only are IT projects risky, they are  riskiest from a cost perspective.CISQ report estimates that organizations in the United States spend more than $520 billion annually supporting legacy software systems, with 70 to 75 percent of organizational IT budgets devoted to legacy maintenance. A 2024 report by services company NTT DATA found that 80 percent of organizations concede that “inadequate or outdated technology is holding back organizational progress and innovation efforts.” Furthermore, the report says that virtually all C-level executives believe legacy infrastructure thwarts their ability to respond to the market. Even so, given that the cost of replacing legacy systems is typically many multiples of the cost of supporting them, business executives hesitate to replace them until it is no longer operationally feasible or cost-effective. The other reason is a well-founded fear that replacing them will turn into a debacle like Phoenix or othersNevertheless, there have been ongoing attempts to improve software development and sustainment processes. For example, we have seen increasing adoption of iterative and incremental strategies to develop and sustain software systems through Agile approaches, DevOps methods, and other related practices.It is best to be wary of these claims while also acknowledging that successfully implementing Agile or DevOps methods takes consistent leadership, organizational discipline, patience, investment in training, and culture change. However, the same requirements have always been true when introducing any new software platform. Given the historic lack of organizational resolve to instill proven practices, it is not surprising that novel approaches for developing and sustaining ever more complex software systems, no matter how effective they may be, will also frequently fall short.Persisting in Foolish ErrorsThe frustrating and perpetual question is why basic IT project-management and governance mistakes during software development and operations continue to occur so often, given the near-total societal reliance on reliable software and an extensively documented history of failures to learn from? Next to electrical infrastructure, with which IT is increasingly merging into a mutually codependent relationship, the failure of our computing systems is an existential threat to modern society.Frustratingly, the IT community stubbornly fails to learn from prior failures. IT project managers routinely claim that their project is somehow different or unique and, thus, lessons from previous failures are irrelevant. That is the excuse of the arrogant, though usually not the ignorant. In Phoenix’s case, for example, it was the government’s second payroll-system replacement attempt, the first effort ending in failure in 1995. Phoenix project managers ignored the well-documented reasons for the first failure because they claimed its lessons were not applicable, which did nothing to keep the managers from repeating them. As it’s been said, we learn more from failure than from success, but repeated failures are damn expensive.Not all software development failures are bad; some failures are even desired. When pushing the limits of developing new types of software products, technologies, or practices, as is happening with AI-related efforts, potential failure is an accepted possibility. With failure, experience increases, new insights are gained, fixes are made, constraints are better understood, and technological innovation and progress continue. However, most IT failures today are not related to pushing the innovative frontiers of the computing art, but the edges of the mundane. They do not represent Austrian economist Joseph Schumpeter’s “gales of creative destruction.” They’re more like gales of financial destruction. Just how many more enterprise resource planning (ERP) project failures are needed before success becomes routine? Such failures should be called IT blunders, as learning anything new from them is dubious at best.Was Phoenix a failure or a blunder? I argue strongly for the latter, but at the very least, Phoenix serves as a master class in IT project mismanagement. The question is whether the Canadian government learned from this experience any more than it did from 1995’s payroll-project fiasco? The government maintains it will learn, which might be true, given the Phoenix failure’s high political profile. But will Phoenix’s lessons extend to the thousands of outdated Canadian government IT systems needing replacement or modernization? Hopefully, but hope is not a methodology, and purposeful action will be necessary.The IT community has striven mightily for decades to make the incomprehensible routine. Repeatedly making the same mistakes and expecting a different result is not learning. It is a farcical absurdity. Paraphrasing Henry Petroski in his book To Engineer Is Human: The Role of Failure in Successful Design (Vintage, 1992), we may have learned how to calculate the software failure due to risk, but we have not learned how to calculate to eliminate the failure of the mind.There are a plethora of examples of projects like Phoenix that failed in part due to bumbling management, yet it is extremely difficult to find software projects managed professionally that still failed. Finding examples of what could be termed “IT heroic failures” is like Diogenes seeking one honest man.The consequences of not learning from blunders will be much greater and more insidious as society grapples with the growing effects of artificial intelligence, or more accurately, “intelligent” algorithms embedded into software systems. Hints of what might happen if past lessons go unheeded are found in the spectacular early automated decision-making failure of Michigan’s MiDAS unemployment and Australia’s Centrelink “Robodebt” welfare systems. Both used questionable algorithms to identify deceptive payment claims without human oversight. State officials used MiDAS to accuse tens of thousands of Michiganders of unemployment fraud, while Centrelink officials falsely accused hundreds of thousands of Australians of being welfare cheats. Untold numbers of lives will never be the same because of what occurred. Government officials in Michigan and Australia placed far too much trust in those algorithms. They had to be dragged, kicking and screaming, to acknowledge that something was amiss, even after it was clearly demonstrated that the software was untrustworthy. Even then, officials tried to downplay the errors’ impact on people, then fought against paying compensation to those adversely affected by the errors. While such behavior is legally termed “maladministration,” administrative evil is closer to reality.So, we are left with only a professional and personal obligation to reemphasize the obvious: Ask what you do know, what you should know, and how big the gap is between them before embarking on creating an IT system. If no one else has ever successfully built your system with the schedule, budget, and functionality you asked for, please explain why your organization thinks it can. Software is inherently fragile; building complex, secure, and resilient software systems is difficult, detailed, and time-consuming. Small errors have outsize effects, each with an almost infinite number of ways they can manifest, from causing a minor functional error to a system outage to allowing a cybersecurity threat to penetrate the system. The more complex and interconnected the system, the more opportunities for errors and their exploitation. A nice start would be for senior management who control the purse strings to finally treat software and systems development, operations, and sustainment efforts with the respect they deserve. This not only means providing the personnel, financial resources, and leadership support and commitment, but also the professional and personal accountability they demand.It is well known that honesty, skepticism, and ethics are essential to achieving project success, yet they are often absent. Only senior management can demand they exist. For instance, honesty begins with the forthright accounting of the myriad of risks involved in any IT endeavor, not their rationalization. It is a common “secret” that it is far easier to get funding to fix a troubled software development effort than to ask for what is required up front to address the risks involved. Vendor puffery may also be legal, but that means the IT customer needs a healthy skepticism of the typically too-good-to-be-true promises vendors make. Once the contract is signed, it is too late. Furthermore, computing’s malleability, complexity, speed, low cost, and ability to reproduce and store information combine to create ethical situations that require deep reflection about computing’s consequences on individuals and society. Alas, ethical considerations have routinely lagged when technological progress and profits are to be made. This practice must change, especially as AI is routinely injected into automated systems.In the AI community, there has been a movement toward the idea of human-centered AI, meaning AI systems that prioritize human needs, values, and well-being. This means trying to anticipate where and when AI can go wrong, move to eliminate these situations, and build in ways to mitigate the effects if they do happen. This concept requires application to every IT system’s effort, not just AI.Given the historic lack of organizational resolve to instill proven practices...novel approaches for developing and sustaining ever more complex software systems...will also frequently fall short.failure after-effectssoftware crisis]]></content:encoded></item><item><title>Making Crash Bandicoot (2011)</title><link>https://all-things-andy-gavin.com/video-games/making-crash/</link><author>davikr</author><category>hn</category><pubDate>Tue, 25 Nov 2025 12:05:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[As one of the co-creators of , I have been (slowly) writing a long series of posts on the making of everyone’s favorite orange marsupial. You can find them all below, so enjoy.If you are on mobile and cannot see the grid of posts, click here.]]></content:encoded></item><item><title>What you can get for the price of a Netflix subscription</title><link>https://nmil.dev/what-you-can-get-for-the-price-of-a-netflix-subscription</link><author>nmil</author><category>hn</category><pubDate>Tue, 25 Nov 2025 06:39:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A couple of weeks ago, I decided to do away with my Netflix subscription. I simply was barely using it, and whenever I did it was more out of habit than it really being the thing I wanted to do with my time. Sure, there's still some decent stuff on there, but the vast majority of it feels absolutely moneyballed. Good, but somehow , and with no character.As much as I'd love to elaborate on why I think Netflix is evil, that's not todays topic. What I wanted to share is how for approximately the price I was paying for my subscription (€19.99), I've snapped up three subscriptions that I'm using on a daily basis. They're all pretty much interchangeable with other alternatives. The main thing I want to highlight is the individual slot they each fill out for me.1. A subscription to Zed Pro (~€10)Frankly, I haven't really put too much thought into whether the unit economics are the best here. The main point is, these are €10 that make my coding experience more pleasant, and get me writing more code in my spare time. In that sense it's money well spent.Does it matter if you get a Cursor subscription, or a Zed one, or whatever else is in vogue when you're reading? No, just get the thing that will get you excited to get your hands on the keyboard!
To me, Zed feels more intentionally built than the VSClones: things flow nicely, it feels snappy, the ui is less cluttered... It's just .Editor preferences aside, the main takeaway is, invest in a hobby you actively engage in. Make that little bit more appealing and you have one more reason to be spending your time doing the thing that makes you feel good, rather than letting a couple hours a day evaporate watching another forgettable show.2. A Kagi subscription (~€5/month)I think we can mostly agree google kind of sucks nowadays. Whenever I search, I automatically scroll down to skip the sponsored posts and SEO maxxed websites, and still don't fully trust what I get. Maybe that's why we all started appending “reddit” the end of our searches.Are the search results themselves better with Kagi? To be honest, I can't tell yet, others have written far more informed takes on the topic. What does it for me is the simple fact of being able to pay directly for a service that I use, and value, rather than having to trade my attention in and endure a wall of ads. Especially if it's something I use over and over, every day. That's what I mean to highlight here: we can support products that we enjoy by paying for them (who would have thought?) rather than letting them lobotomize us via ad feeds.3. A cheap server on Hetzner (~€4/month)Again, the choice of provider here is secondary. The point is, I finally have my little stake on the internet. It's relatively barebones, and I like that. It forces me to learn and engage. In fact, that is where my blog is hosted!So to sum it up: We don't  to default to a streaming subscription because that's become the standard human-being thing to do. For the same money you can build a suite of useful, well crafted tools that help you:
– Get the most out of your hobbies
– Spend less time looking at ads
– Build things you can share with the worldP.S. Not one word here was written by AI. I plan on keeping it that way for anything that goes on this blog. So, if anything reads like slop, it's  slop :)]]></content:encoded></item><item><title>Most Stable Raspberry Pi? Better NTP with Thermal Management</title><link>https://austinsnerdythings.com/2025/11/24/worlds-most-stable-raspberry-pi-81-better-ntp-with-thermal-management/</link><author>todsacerdoti</author><category>hn</category><pubDate>Tue, 25 Nov 2025 06:35:59 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[But there was a problem. Despite having a stable PPS reference, my NTP server’s frequency drift was exhibiting significant variation over time. After months (years) of monitoring the system with Grafana dashboards, I noticed something interesting: the frequency oscillations seemed to correlate with CPU temperature changes. The frequency would drift as the CPU heated up during the day and cooled down at night, even though the PPS reference remained rock-solid.Like clockwork (no pun intended), I somehow get sucked back into trying to improve my setup every 6-8 weeks. This post is the latest on that never-ending quest.This post details how I achieved an 81% reduction in frequency variability and 77% reduction in frequency standard deviation through a combination of CPU core pinning and thermal stabilization. Welcome to Austin’s Nerdy Things, where we solve problems that 99.999% of people (and 99% of datacenters) don’t have.The Problem: Thermal-Induced Timing JitterModern CPUs, including those in Raspberry Pis, use dynamic frequency scaling to save power and manage heat. When the CPU is idle, it runs at a lower frequency (and voltage). When load increases, it scales up. This is great for power efficiency, but terrible for precision timekeeping.Why? Because timekeeping (with NTP/chronyd/others) relies on a stable system clock to discipline itself against reference sources. If the CPU frequency is constantly changing, the system clock’s tick rate varies, introducing jitter into the timing measurements. Even though my PPS signal was providing a mostly perfect 1-pulse-per-second reference, the CPU’s frequency bouncing around made it harder for chronyd to maintain a stable lock.But here’s the key insight: the system clock is ultimately derived from a crystal oscillator, and crystal oscillator frequency is temperature-dependent. The oscillator sits on the board near the CPU, and as the CPU heats up and cools down throughout the day, so does the crystal. Even a few degrees of temperature change can shift the oscillator’s frequency by parts per million – exactly what I was seeing in my frequency drift graphs. The CPU frequency scaling was one factor, but the underlying problem was that temperature changes were affecting the crystal oscillator itself. By stabilizing the CPU temperature, I could stabilize the thermal environment for the crystal oscillator, keeping its frequency consistent.Looking at my Grafana dashboard, I could see the frequency offset wandering over a range of about 1 PPM (parts per million) as the Pi warmed up and cooled down throughout the day. The RMS offset was averaging around 86 nanoseconds, which isn’t terrible (it’s actually really, really, really good), but I knew it could be better.After staring at graphs for longer than I’d like to admit, I had an idea: what if I could keep the CPU at a constant temperature? If the temperature (and therefore the frequency) stayed stable, maybe the timing would stabilize too.The solution came in two parts:1.  – Dedicate CPU 0 exclusively to timing-critical tasks (chronyd and PPS interrupts) 2.  – Keep the other CPUs busy to maintain a constant temperature, preventing frequency scalingHere’s what happened when I turned on the thermal stabilization system on November 17, 2025 at 09:10 AM:Same ish graph but with CPU temp also plotted:That vertical red line marks on the first plot when I activated the “time burner” process. Notice how the frequency oscillations immediately dampen and settle into a much tighter band? Let’s dive into how this works.The Solution Part 1: CPU Core Pinning and Real-Time PriorityThe first step is isolating timing-critical operations onto a dedicated CPU core. On a Raspberry Pi (4-core ARM), this means:CPU 0: Reserved for chronyd and PPS interruptsCPUs 1-3: Everything else, including our thermal loadI had AI (probably Claude Sonnet 4 ish, maybe 4.5) create a boot optimization script that runs at system startup:#!/bin/bash
# PPS NTP Server Performance Optimization Script
# Sets CPU affinity, priorities, and performance governor at boot

set -e

echo "Setting up PPS NTP server performance optimizations..."

# Wait for system to be ready
sleep 5

# Set CPU governor to performance mode
echo "Setting CPU governor to performance..."
cpupower frequency-set -g performance

# Pin PPS interrupt to CPU0 (may fail if already pinned, that's OK)
echo "Configuring PPS interrupt affinity..."
echo 1 > /proc/irq/200/smp_affinity 2>/dev/null || echo "PPS IRQ already configured"

# Wait for chronyd to start
echo "Waiting for chronyd to start..."
timeout=30
while [ $timeout -gt 0 ]; do
    chronyd_pid=$(pgrep chronyd 2>/dev/null || echo "")
    if [ -n "$chronyd_pid" ]; then
        echo "Found chronyd PID: $chronyd_pid"
        break
    fi
    sleep 1
    ((timeout--))
done

if [ -z "$chronyd_pid" ]; then
    echo "Warning: chronyd not found after 30 seconds"
else
    # Set chronyd to real-time priority and pin to CPU 0
    echo "Setting chronyd to real-time priority and pinning to CPU 0..."
    chrt -f -p 50 $chronyd_pid
    taskset -cp 0 $chronyd_pid
fi

# Boost ksoftirqd/0 priority
echo "Boosting ksoftirqd/0 priority..."
ksoftirqd_pid=$(ps aux | grep '\[ksoftirqd/0\]' | grep -v grep | awk '{print $2}')
if [ -n "$ksoftirqd_pid" ]; then
    renice -n -10 $ksoftirqd_pid
    echo "ksoftirqd/0 priority boosted (PID: $ksoftirqd_pid)"
else
    echo "Warning: ksoftirqd/0 not found"
fi

echo "PPS NTP optimization complete!"

# Log current status
echo "=== Current Status ==="
echo "CPU Governor: $(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor)"
echo "PPS IRQ Affinity: $(cat /proc/irq/200/effective_affinity_list 2>/dev/null || echo 'not readable')"
if [ -n "$chronyd_pid" ]; then
    echo "chronyd Priority: $(chrt -p $chronyd_pid)"
fi
echo "======================": Forces all CPUs to run at maximum frequency, disabling frequency scaling: Ensures PPS interrupt (IRQ 200) is handled exclusively by CPU 0Chronyd Real-Time Priority: Sets chronyd to SCHED_FIFO priority 50, giving it preferential CPU schedulingC: Pins chronyd to CPU 0 using : Improves priority of the kernel softirq handler on CPU 0This script can be added to  or as a systemd service to run at boot.The Solution Part 2: PID-Controlled Thermal StabilizationSetting the performance governor helps, but on a Raspberry Pi, even at max frequency, the CPU temperature will still vary based on ambient conditions and load. Temperature changes affect the CPU’s actual operating frequency due to thermal characteristics of the silicon.The solution? Keep the CPU at a constant temperature using a PID-controlled thermal load. I call it the “time burner” (inspired by CPU burn-in tools, but with precise temperature control).As a reminder of what we’re really doing here: we’re maintaining a stable thermal environment for the crystal oscillator. The RPi 3B’s 19.2 MHz oscillator is physically located near the CPU on the Raspberry Pi board, so by actively controlling CPU temperature, we’re indirectly controlling the oscillator’s temperature. Since the oscillator’s frequency is temperature-dependent (this is basic physics of quartz crystals), keeping it at a constant temperature means keeping its frequency stable – which is exactly what we need for precise timekeeping. from /sys/class/thermal/thermal_zone0/temp calculates how much CPU time to burn to maintain target temperature (I chose 54°C)  run on CPUs 1, 2, and 3 (avoiding CPU 0)  alternates between busy-loop (MD5 hashing) and sleeping based on PID output  at the setpoint, preventing thermal driftHere’s the core implementation (simplified for readability):#!/usr/bin/env python3
import time
import argparse
import multiprocessing
import hashlib
import os
from collections import deque

class PIDController:
    """Simple PID controller with output clamping and anti-windup."""
    def __init__(self, Kp, Ki, Kd, setpoint, output_limits=(0, 1), sample_time=1.0):
        self.Kp = Kp
        self.Ki = Ki
        self.Kd = Kd
        self.setpoint = setpoint
        self.output_limits = output_limits
        self.sample_time = sample_time
        self._last_time = time.time()
        self._last_error = 0.0
        self._integral = 0.0
        self._last_output = 0.0

    def update(self, measurement):
        """Compute new output of PID based on measurement."""
        now = time.time()
        dt = now - self._last_time

        if dt < self.sample_time:
            return self._last_output

        error = self.setpoint - measurement

        # Proportional
        P = self.Kp * error

        # Integral with anti-windup
        self._integral += error * dt
        I = self.Ki * self._integral

        # Derivative
        derivative = (error - self._last_error) / dt if dt > 0 else 0.0
        D = self.Kd * derivative

        # Combine and clamp
        output = P + I + D
        low, high = self.output_limits
        output = max(low, min(high, output))

        self._last_output = output
        self._last_error = error
        self._last_time = now

        return output

def read_cpu_temperature(path='/sys/class/thermal/thermal_zone0/temp'):
    """Return CPU temperature in Celsius."""
    with open(path, 'r') as f:
        temp_str = f.read().strip()
    return float(temp_str) / 1000.0

def burn_cpu(duration):
    """Busy-loop hashing for 'duration' seconds."""
    end_time = time.time() + duration
    m = hashlib.md5()
    while time.time() < end_time:
        m.update(b"burning-cpu")

def worker_loop(worker_id, cmd_queue, done_queue):
    """
    Worker process:
    - Pins itself to CPUs 1, 2, or 3 (avoiding CPU 0)
    - Burns CPU based on commands from main process
    """
    available_cpus = [1, 2, 3]
    cpu_to_use = available_cpus[worker_id % len(available_cpus)]
    os.sched_setaffinity(0, {cpu_to_use})
    print(f"Worker {worker_id} pinned to CPU {cpu_to_use}")

    while True:
        cmd = cmd_queue.get()
        if cmd is None:
            break

        burn_time, sleep_time = cmd
        burn_cpu(burn_time)
        time.sleep(sleep_time)
        done_queue.put(worker_id)

# Main control loop (simplified)
def main():
    target_temp = 54.0  # degrees Celsius
    control_window = 0.20  # 200ms cycle time

    pid = PIDController(Kp=0.05, Ki=0.02, Kd=0.0,
                        setpoint=target_temp,
                        sample_time=0.18)

    # Start 3 worker processes
    workers = []
    cmd_queues = []
    done_queue = multiprocessing.Queue()

    for i in range(3):
        q = multiprocessing.Queue()
        p = multiprocessing.Process(target=worker_loop, args=(i, q, done_queue))
        p.start()
        workers.append(p)
        cmd_queues.append(q)

    try:
        while True:
            # Measure temperature
            current_temp = read_cpu_temperature()

            # PID control: output is fraction of time to burn (0.0 to 1.0)
            output = pid.update(current_temp)

            # Convert to burn/sleep times
            burn_time = output * control_window
            sleep_time = control_window - burn_time

            # Send command to all workers
            for q in cmd_queues:
                q.put((burn_time, sleep_time))

            # Wait for workers to complete
            for _ in range(3):
                done_queue.get()

            print(f"Temp={current_temp:.2f}C, Output={output:.2f}, "
                  f"Burn={burn_time:.2f}s")

    except KeyboardInterrupt:
        for q in cmd_queues:
            q.put(None)
        for p in workers:
            p.join()

if __name__ == '__main__':
    main()The full implementation includes a temperature filtering system to smooth out sensor noise and command-line arguments for tuning the PID parameters.: Proportional gain – responds to current error: Integral gain – eliminates steady-state error: Derivative gain – set to zero because temperature changes slowlyThe target temperature of 54°C was chosen empirically – high enough to keep the CPU from idling down, but low enough to avoid thermal throttling (which starts around 80°C on Raspberry Pi).The Results: Numbers Don’t LieThe improvement was immediately visible. Here are the statistics comparing performance before and after the optimization:A note on ambient conditions: The Raspberry Pi lives in a project enclosure in our master bedroom (chosen for its decent GPS reception and ADS-B coverage for a new aircraft AR overlay app idea I’m working on also running on this Pi). While the time burner maintains the CPU die temperature at 54°C, the enclosure is still subject to ambient temperature swings. Room temperature cycles from a low of 66°F (18.9°C) at 5:15 AM to a peak of 72°F (22.2°C) at 11:30 AM – a 6°F daily swing from our heating schedule. The fact that we see such dramatic frequency stability improvements  this ambient variation speaks to how effective the thermal control is. The CPU’s active heating overwhelms the environmental changes, maintaining consistent silicon temperature where it matters most.The RMS offset is chronyd’s estimate of the timing uncertainty. Cutting this nearly in half means the system is maintaining significantly better time accuracy.Want to replicate this? Here’s the step-by-step process:You need a working GPS PPS NTP server setup. If you don’t have one yet, follow my 2025 NTP guide first.Step 0: Install Required Toolssudo apt-get update
sudo apt-get install linux-cpupower python3 util-linuxStep 1: Create the Boot Optimization ScriptSave the optimization script from earlier as /usr/local/bin/pps-optimize.sh:sudo nano /usr/local/bin/pps-optimize.sh
# Paste the script content
sudo chmod +x /usr/local/bin/pps-optimize.shStep 2: Create Systemd Service for Boot ScriptCreate /etc/systemd/system/pps-optimize.service:[Unit]
Description=PPS NTP Performance Optimization
After=chronyd.service
Requires=chronyd.service

[Service]
Type=oneshot
ExecStart=/usr/local/bin/pps-optimize.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.targetsudo systemctl enable pps-optimize.serviceStep 3: Install the Time Burner ScriptSave the time burner Python script as /usr/local/bin/time_burner.py:sudo nano /usr/local/bin/time_burner.py
# Paste the full time burner script
sudo chmod +x /usr/local/bin/time_burner.pyStep 4: Create Systemd Service for Time BurnerCreate /etc/systemd/system/time-burner.service:[Unit]
Description=CPU Thermal Stabilization for NTP
After=network.target

[Service]
Type=simple
User=root
ExecStart=/usr/bin/python3 /usr/local/bin/time_burner.py -t 54.0 -n 3
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.targetsudo systemctl enable time-burner.service
sudo systemctl start time-burner.serviceCheck that everything is running:# Verify CPU governor
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
# Should output: performance

# Check chronyd CPU affinity and priority
ps -eo pid,comm,psr,ni,rtprio | grep chronyd
# Should show psr=0 (CPU 0) and rtprio=50

# Check time burner processes
ps aux | grep time_burner
# Should show 4 processes (1 main + 3 workers)

# Monitor NTP performance
chronyc trackingExample output from :Reference ID    : 50505300 (PPS)
Stratum         : 1
Ref time (UTC)  : Sun Nov 24 16:45:23 2025
System time     : 0.000000038 seconds fast of NTP time
Last offset     : -0.000000012 seconds
RMS offset      : 0.000000035 seconds
Frequency       : 1.685 ppm slow
Residual freq   : -0.001 ppm
Skew            : 0.002 ppm
Root delay      : 0.000000001 seconds
Root dispersion : 0.000010521 seconds
Update interval : 16.0 seconds
Leap status     : NormalNotice the RMS offset of 35 nanoseconds – this is the kind of accuracy you can achieve with thermal stabilization.Step 6: Monitor Over Time(Topic for a future post)Set up Grafana dashboards to monitor:You’ll see the frequency stabilize within a few hours as the PID controller locks onto the target temperature.Monitoring and TroubleshootingWatch chronyd tracking in real-time:watch -n 1 "chronyc tracking"Check time burner status:sudo systemctl status time-burner.servicesudo journalctl -u time-burner.service -fTemperature overshoots or oscillates:Adjust PID gains – reduce Kp if oscillating, increase Ki if steady-state errorTry different target temperatures (50-60°C range)High CPU usage (obviously):This is intentional – the time burner uses ~90% of 3 coresNot suitable for Pis running other workloadsChronyd not pinned to CPU 0:Check that the optimization script runs after chronyd startsAdjust the timing in the systemd service dependenciesTrade-offs and ConsiderationsLet’s be honest about the downsides:The time burner keeps 3 cores at ~30% average utilization. My Pi now draws about 3-4W continuously (vs 1-2W idle). Over a year, that’s an extra 15-25 kWh, or about $2-3 in electricity (depending on your rates).Running at 54°C means the Pi is warm to the touch. This is well within safe operating temperature (thermal throttling doesn’t start until 80°C), but you might want to ensure adequate ventilation. I added a small heatsink just to be safe.You’re dedicating 3 of 4 cores to burning cycles. This is fine for a dedicated NTP server, but not suitable if you’re running other services on the same Pi. That said, I am also running the feeder to my new ADS-B aircraft visualization app on it. My readsb instance regularly gets to 1200 msg/s with 200+ aircraft.For 99.999% of use cases: .Most applications don’t need better than millisecond accuracy, let alone the 35-nanosecond RMS offset I’m achieving. Even for distributed systems, microsecond-level accuracy is typically overkill.When this might make sense:Precision timing applications (scientific instrumentation, radio astronomy)Distributed systems research requiring tight clock synchronization where timing precision affects results (the best reason for any homelab project)For me, this falls squarely in the “because you can” category. I had the monitoring infrastructure in place, noticed the thermal correlation, and couldn’t resist solving the problem. Plus, I learned a lot about PID control, CPU thermal characteristics, and Linux real-time scheduling.Some ideas I’m considering:The current PID gains are hand-tuned for a specific ambient temperature range. The fairly low P value is to avoid spikes when some load on the Pi kicks up the temp. The I is a balance to keep long term “burn” relatively consistent. Implementing an auto-tuning algorithm (like Ziegler-Nichols) or adaptive PID could handle seasonal temperature variations better.Instead of software thermal control, I could add an actively cooled heatsink with PWM fan control. This might achieve similar temperature stability while using less power overall.Oven-Controlled Crystal Oscillator (OCXO)For the ultimate in frequency stability, replacing the Pi’s crystal with a temperature-controlled OCXO would eliminate thermal drift at the source. This is how professional timing equipment works. I do have a BH3SAP GPSDO sitting next to me (subject to a future post)… Then again, I’m the person who just wrote 4000 words about optimizing a $50 time server, so who am I kidding?Through a combination of CPU core isolation and PID-controlled thermal stabilization, I achieved: in frequency variability in frequency standard deviation in frequency range in RMS offsetThe system now maintains 38-nanosecond median RMS offset from the GPS PPS reference, with frequency drift that’s barely detectable in the noise. The CPU runs at a constant 54°C, and in steady state, the frequency offset stays within a tight ±0.14 PPM band (compared to ±0.52 PPM before optimization).Was this necessary? No. Did I learn a bunch about thermal management, PID control, and Linux real-time scheduling? Yes. Would I do it again? Absolutely.I did come across a “burn” script that was the basis for this thermal management. I can’t find it at the moment, but when I do I’ll link it here.Have questions or suggestions? Drop a comment below. I’m particularly interested to hear if anyone has tried alternative thermal management approaches or has experience with OCXO modules for Raspberry Pi timing applications.Thanks for reading, and happy timekeeping!]]></content:encoded></item><item><title>Human brains are preconfigured with instructions for understanding the world</title><link>https://news.ucsc.edu/2025/11/sharf-preconfigured-brain/</link><author>XzetaU8</author><category>hn</category><pubDate>Tue, 25 Nov 2025 06:31:31 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[New findings suggest the brain has preconfigured, structured activity patterns even before sensory experiences occur.UC Santa Cruz researchers used brain organoids to study the brain’s earliest electrical activity.Understanding early brain patterns could have important implications for diagnosing and treating developmental brain disorders.Humans have long wondered when and how we begin to form thoughts. Are we born with a pre-configured brain, or do thought patterns only begin to emerge in response to our sensory experiences of the world around us? Now, science is getting closer to answering the questions philosophers have pondered for centuries. Researchers at the University of California, Santa Cruz, are using tiny models of human brain tissue, called organoids, to study the earliest moments of electrical activity in the brain. A new study in  finds that the earliest firings of the brain occur in structured patterns without any external experiences, suggesting that the human brain is preconfigured with instructions about how to navigate and interact with the world.“These cells are clearly interacting with each other and forming circuits that self-assemble before we can experience anything from the outside world,” said Tal Sharf, assistant professor of biomolecular engineering at the Baskin School of Engineering and the study’s senior author. “There’s an operating system that exists, that emerges in a primordial state. In my laboratory, we grow brain organoids to peer into this primordial version of the brain’s operating system and study how the brain builds itself before it’s shaped by sensory experience.”In improving our fundamental understanding of human brain development, these findings can help researchers better understand neurodevelopmental disorders, and pinpoint the impact of toxins like pesticides and microplastics in the developing brain. Studying the developing brainThe brain, similar to a computer, runs on electrical signals—the firing of neurons. When these signals begin to fire, and how the human brain develops, are challenging topics for scientists to study, as the early developing human brain is protected within the womb.Organoids, which are 3D models of tissue grown from human stem cells in the lab, provide a unique window into brain development. The Braingeneers group at UC Santa Cruz, in collaboration with researchers at UC San Francisco and UC Santa Barbara, are pioneering methods to grow these models and take measurements from them to gain insights into brain development and disorders. Organoids are particularly useful for understanding if the brain develops in response to sensory input—as they exist in the lab setting and not the body—and can be grown ethically in large quantities. In this study, researchers prompted stem cells to form brain tissue, and then measured their electrical activity using specialized microchips, similar to those that run a computer. Sharf’s background in both applied physics, computation, and neurobiology form his expertise in modelling the circuitry of the early brain. “An organoid system that’s intrinsically decoupled from any sensory input or communication with organs gives you a window into what’s happening with this self-assembly process,” Sharf said. “That self-assembly process is really hard to do with traditional 2D cell culture—you can’t get the cell diversity and the architecture. The cells need to be in intimate contact with each other. We’re trying to control the initial conditions, so we can let biology do its wonderful thing.”The Sharf lab is developing novel neural interfaces, leveraging expertise in physics, materials science, and electrical engineering. On the right, Koushik Devarajan, an electrical and computer engineering Ph.D. student in the Sharf lab.The researchers observed the electrical activity of the brain tissue as they self-assembled from stem cells into a tissue that can translate the senses and produce language and conscious thought. They found that within the first few months of development, long before the human brain is capable of receiving and processing complex external sensory information such as vision and hearing, its cells spontaneously began to emit electrical signals characteristic of the patterns that underlie translation of the senses. Through decades of neuroscience research, the community has discovered that neurons fire in patterns that aren’t just random. Instead, the brain has a “default mode” — a basic underlying structure for firing neurons which then becomes more specific as the brain processes unique signals like a smell or taste. This background mode outlines the possible range of sensory responses the body and brain can produce.In their observations of single neuron spikes in the self-assembling organoid models, Sharf and colleagues found that these earliest observable patterns have striking similarity with the brain’s default mode. Even without having received any sensory input, they are firing off a complex repertoire of time-based patterns, or sequences, which have the potential to be refined for specific senses, hinting at a genetically encoded blueprint inherent to the neural architecture of the living brain.“These intrinsically self-organized systems could serve as a basis for constructing a representation of the world around us,” Sharf said. “The fact that we can see them in these early stages suggests that evolution has figured out a way that the central nervous system can construct a map that would allow us to navigate and interact with the world.”Knowing that these organoids produce the basic structure of the living brain opens up a range of possibilities for better understanding human neurodevelopment, disease, and the effects of toxins in the brain. “We’re showing that there is a basis for capturing complex dynamics that likely could be signatures of pathological onsets that we could study in human tissue,” Sharf said. “That would allow us to develop therapies, working with clinicians at the preclinical level to potentially develop compounds, drug therapies, and gene editing tools that could be cheaper, more efficient, higher throughput.”This study included researchers at UC Santa Barbara, Washington University in St. Louis, Johns Hopkins University, the University Medical Center Hamburg-Eppendorf, and ETH Zurich.]]></content:encoded></item><item><title>Jakarta is now the biggest city in the world</title><link>https://www.axios.com/2025/11/24/jakarta-tokyo-worlds-biggest-city-population</link><author>skx001</author><category>hn</category><pubDate>Tue, 25 Nov 2025 06:09:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AI has a deep understanding of how this code works</title><link>https://github.com/ocaml/ocaml/pull/14369</link><author>theresistor</author><category>hn</category><pubDate>Mon, 24 Nov 2025 21:03:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>PRC elites voice AI-skepticism</title><link>https://jamestown.org/prc-elites-voice-ai-skepticism/</link><author>JumpCrisscross</author><category>hn</category><pubDate>Mon, 24 Nov 2025 19:50:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>PS5 now costs less than 64GB of DDR5 memory. RAM jumps to $600 due to shortage</title><link>https://www.tomshardware.com/pc-components/ddr5/64gb-of-ddr5-memory-now-costs-more-than-an-entire-ps5-even-after-a-discount-trident-z5-neo-kit-jumps-to-usd600-due-to-dram-shortage-and-its-expected-to-get-worse-into-2026</link><author>speckx</author><category>hn</category><pubDate>Mon, 24 Nov 2025 19:29:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Thanks to the AI boom devouring the majority of the world's memory and storage supply, end-consumers are now facing increasingly inflated prices for common components. DDR5 RAM, a necessity for building current-gen Intel or AMD systems, has now reached record highs in terms of pricing; a 64 GB kit of G.Skill's Trident Z5 Neo 6000 MT/s RAM is listed at $599.99 on Newegg right now — that's $200 more than a PS5 Slim or a Microsoft Xbox Series S, and just $50 shy off an entire PS5 Pro at the moment.That $600 price tag has a 6% discount already applied to its original $640 ask, as part of a Black Friday deal. For context, a more exclusive 64 GB limited edition Corsair Dominator Titanium kit cost only $349 when we reviewed it a few months ago. Earlier this year, we posted about DDR5 deals on Prime Day where the standard edition of the same kit was just $299, and you could get other comparable 64 GB kits for as low as $140.A quick glance at price tracking data, and G.Skill's Trident Z5 Neo kit has regularly sat at $205-$220 for the past few months, and it was only in late October that it started to pick up steam. From September 20th when it was listed at $220, to $640 now. In just 2 months we've witnessed an astounding ~190% surge.Right as this particular Trident Z5 Neo kit began to skyrocket in price was when the industry first started to pick up on the affects of the AI crunch. A few days later we published our initial coverage on DDR5 RAM price hikes; from there, the situation has only worsened to reach worrying levels.Insane mark-up aside, the kit itself is one of the best on the market, recommend as the top pick for DDR5 memory in our roundup. Unfortunately, it seems like high prices are going to be the story going forward. The surge in demand for AI projects will see production lines will prioritizing serving AI clients, leaving consumers to pay through the nose or make the best of what they have. Experts speculate that both DRAM and NAND constraints will become normal throughout 2026 as Big Tech looks to pursue AGI.Even Valve's upcoming Steam Machine will end up costing more than expected due to the production window of the device aligning with the DRAM crisis. That being said, memory has almost always lived in a rollercoaster cycle, with manufacturers oversupplying for a couple of years, then undersupplying for the next few. Looking at it optimistically, you're probably going to find DDR5 at bargain prices again .]]></content:encoded></item><item><title>Unpowered SSDs slowly lose data</title><link>https://www.xda-developers.com/your-unpowered-ssd-is-slowly-losing-your-data/</link><author>amichail</author><category>hn</category><pubDate>Mon, 24 Nov 2025 19:25:25 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Claude Advanced Tool Use</title><link>https://www.anthropic.com/engineering/advanced-tool-use</link><author>lebovic</author><category>hn</category><pubDate>Mon, 24 Nov 2025 19:21:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.To build effective agents, they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using code execution with MCP discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.Agents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.Agents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.Today, we're releasing three features that make this possible:which allows Claude to use search tools to access thousands of tools without consuming its context windowProgrammatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model’s context window, which provides a universal standard for demonstrating how to effectively use a given toolIn internal testing, we’ve found these features have helped us build things that wouldn’t have been possible with conventional tool use patterns. For example,uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model’s context window.Based on our experience, we believe these features open up new possibilities for what you can build with Claude.MCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:GitHub: 35 tools (~26K tokens)Slack: 11 tools (~21K tokens)Sentry: 5 tools (~3K tokens)Grafana: 5 tools (~3K tokens)Splunk: 2 tools (~2K tokens)That's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.But token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like  vs. notification-send-channel.Instead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)Conversation history and system prompt compete for remaining spaceTotal context consumption: ~77K tokens before any work beginsWith the Tool Search Tool:Only the Tool Search Tool loaded upfront (~500 tokens)Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)Total context consumption: ~8.7K tokens, preserving 95% of context windowThis represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.How the Tool Search Tool worksThe Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with  to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with  (your most critical, frequently-used tools).When Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.For example, if Claude needs to interact with GitHub, it searches for "github," and only  and  get loaded—not your other 50+ tools from Slack, Jira, and Google Drive.This way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.Tool Search Tool doesn't break prompt caching because deferred tools are excluded from the initial prompt entirely. They're only added to context after Claude searches for them, so your system prompt and core tool definitions remain cacheable.For MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:The Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.When to use the Tool Search ToolLike any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.Tool definitions consuming >10K tokensExperiencing tool selection accuracy issuesBuilding MCP-powered systems with multiple serversSmall tool library (<10 tools)All tools used frequently in every sessionTool definitions are compactProgrammatic Tool CallingTraditional tool calling creates two fundamental problems as workflows become more complex:Context pollution from intermediate results: When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.Inference overhead and manual synthesis: Each tool call requires a full model inference pass. After receiving results, Claude must "eyeball" the data to extract relevant information, reason about how pieces fit together, and decide what to do next—all through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.Claude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.Example: Budget compliance checkConsider a common business task: "Which team members exceeded their Q3 travel budget?"You have three tools available:get_team_members(department) - Returns team member list with IDs and levelsget_expenses(user_id, quarter) - Returns expense line items for a userget_budget_by_level(level) - Returns budget limits for an employee levelFetch team members → 20 peopleFor each person, fetch their Q3 expenses → 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)Fetch budget limits by employee levelAll of this enters Claude's context: 2,000+ expense line items (50 KB+)Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limitsMore round-trips to the model, significant context consumptionWith Programmatic Tool Calling:Instead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.Here's what Claude's orchestration code looks like for the budget compliance task:Claude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude’s context, reducing consumption from 200KB of raw expense data to just 1KB of results.The efficiency gains are substantial:: By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.: Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.: By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; GIA benchmarks from 46.5% to 51.2%.Production workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.How Programmatic Tool Calling works1. Mark tools as callable from codeAdd code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:The API converts these tool definitions into Python functions that Claude can call.2. Claude writes orchestration codeInstead of requesting tools one at a time, Claude generates Python code:3. Tools execute without hitting Claude's contextWhen the code calls get_expenses(), you receive a tool request with a caller field:You provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.4. Only final output enters contextWhen the code finishes running, only the results of the code are returned to Claude:This is all Claude sees, not the 2000+ expense line items processed along the way.When to use Programmatic Tool CallingProgrammatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.Processing large datasets where you only need aggregates or summariesRunning multi-step workflows with three or more dependent tool callsFiltering, sorting, or transforming tool results before Claude sees themHandling tasks where intermediate data shouldn't influence Claude's reasoningRunning parallel operations across many items (checking 50 endpoints, for example)Making simple single-tool invocationsWorking on tasks where Claude should see and reason about all intermediate resultsRunning quick lookups with small responsesJSON Schema excels at defining structure–types, required fields, allowed enums–but it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.Consider a support ticket API:The schema defines what's valid, but leaves critical questions unanswered:Should  use "2024-11-06", "Nov 6, 2024", or "2024-11-06T00:00:00Z"?Is  a UUID, "USR-12345", or just "12345"?When should Claude populate ?How do  and  relate to priority?These ambiguities can lead to malformed tool calls and inconsistent parameter usage.Tool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:From these three examples, Claude learns:: Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-caseNested structure patterns: How to construct the reporter object with its nested contact objectOptional parameter correlations: Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title onlyIn our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.When to use Tool Use ExamplesTool Use Examples add tokens to your tool definitions, so they’re most valuable when accuracy improvements outweigh the additional cost.Complex nested structures where valid JSON doesn't imply correct usageTools with many optional parameters and inclusion patterns matterAPIs with domain-specific conventions not captured in schemasSimilar tools where examples clarify which one to use (e.g.,  vs )Simple single-parameter tools with obvious usageStandard formats like URLs or emails that Claude already understandsValidation concerns better handled by JSON Schema constraintsBuilding agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.Layer features strategicallyNot every agent needs to use all three features for a given task. Start with your biggest bottleneck:Context bloat from tool definitions → Tool Search ToolLarge intermediate results polluting context → Programmatic Tool CallingParameter errors and malformed calls → Tool Use ExamplesThis focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.Then layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.Set up Tool Search Tool for better discoveryTool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.Add system prompt guidance so Claude knows what's available:Keep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.Set up Programmatic Tool Calling for correct executionSince Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:See below for opt-in tools that benefit from programmatic orchestration:Tools that can run in parallel (independent operations)Operations safe to retry (idempotent)Set up Tool Use Examples for parameter accuracyCraft examples for behavioral clarity:Use realistic data (real city names, plausible prices, not "string" or "value")Show variety with minimal, partial, and full specification patternsKeep it concise: 1-5 examples per toolFocus on ambiguity (only add examples where correct usage isn't obvious from schema)These features are available in beta. To enable them, add the beta header and include the tools you need:For detailed API documentation and SDK examples, see our:These features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.We're excited to see what you build.Written by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including Joel Pobar's LLMVM, Cloudflare's Code Mode and Code Execution as MCP. Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.]]></content:encoded></item><item><title>Claude Opus 4.5</title><link>https://www.anthropic.com/news/claude-opus-4-5</link><author>adocomplete</author><category>hn</category><pubDate>Mon, 24 Nov 2025 18:53:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Our newest model, Claude Opus 4.5, is available today. It’s intelligent, efficient, and the best model in the world for coding, agents, and computer use. It’s also meaningfully better at everyday tasks like deep research and working with slides and spreadsheets. Opus 4.5 is a step forward in what AI systems can do, and a preview of larger changes to how work gets done.Claude Opus 4.5 is state-of-the-art on tests of real-world software engineering:Opus 4.5 is available today on our apps, our API, and on all three major cloud platforms. If you’re a developer, simply use  via the Claude API. Pricing is now $5/$25 per million tokens—making Opus-level capabilities accessible to even more users, teams, and enterprises.Alongside Opus, we’re releasing updates to the Claude Developer Platform, Claude Code, and our consumer apps. There are new tools for longer-running agents and new ways to use Claude in Excel, Chrome, and on desktop. In the Claude apps, lengthy conversations no longer hit a wall. See our product-focused section below for details.As our Anthropic colleagues tested the model before release, we heard remarkably consistent feedback. Testers noted that Claude Opus 4.5 handles ambiguity and reasons about tradeoffs without hand-holding. They told us that, when pointed at a complex, multi-system bug, Opus 4.5 figures out the fix. They said that tasks that were near-impossible for Sonnet 4.5 just a few weeks ago are now within reach. Overall, our testers told us that Opus 4.5 just “gets it.”Many of our customers with early access have had similar experiences. Here are some examples of what they told us:Evaluating Claude Opus 4.5We give prospective performance engineering candidates a notoriously difficult take-home exam. We also test new models on this exam as an internal benchmark. Within our prescribed 2-hour time limit, Claude Opus 4.5 scored higher than any human candidate ever.The take-home test is designed to assess technical ability and judgment under time pressure. It doesn’t test for other crucial skills candidates may possess, like collaboration, communication, or the instincts that develop over years. But this result—where an AI model outperforms strong candidates on important technical skills—raises questions about how AI will change engineering as a profession. Our Societal Impacts and Economic Futures research is aimed at understanding these kinds of changes across many fields. We plan to share more results soon.Software engineering isn’t the only area on which Claude Opus 4.5 has improved. Capabilities are higher across the board—Opus 4.5 has better vision, reasoning, and mathematics skills than its predecessors, and it is state-of-the-art in many domains:The model’s capabilities outpace some of the benchmarks we use in our tests. A common benchmark for agentic capabilities is τ2-bench, which measures the performance of agents in real-world, multi-turn tasks. In one scenario, models have to act as an airline service agent helping a distressed customer. The benchmark expects models to refuse a modification to a basic economy booking since the airline doesn’t allow changes to that class of tickets. Instead, Opus 4.5 found an insightful (and legitimate) way to solve the problem: upgrade the cabin first,  modify the flights.The benchmark technically scored this as a failure because Claude’s way of helping the customer was unanticipated. But this kind of creative problem solving is exactly what we’ve heard about from our testers and customers—it’s what makes Claude Opus 4.5 feel like a meaningful step forward.In other contexts, finding clever paths around intended constraints could count as —where models “game” rules or objectives in unintended ways. Preventing such misalignment is one of the objectives of our safety testing, discussed in the next section.As we state in our system card, Claude Opus 4.5 is the most robustly aligned model we have released to date and, we suspect, the best-aligned frontier model by any developer. It continues our trend towards safer and more secure models:Our customers often use Claude for critical tasks. They want to be assured that, in the face of malicious attacks by hackers and cybercriminals, Claude has the training and the “street smarts” to avoid trouble. With Opus 4.5, we’ve made substantial progress in robustness against prompt injection attacks, which smuggle in deceptive instructions to fool the model into harmful behavior. Opus 4.5 is harder to trick with prompt injection than any other frontier model in the industry:You can find a detailed description of all our capability and safety evaluations in the Claude Opus 4.5 system card.New on the Claude Developer PlatformAs models get smarter, they can solve problems in fewer steps: less backtracking, less redundant exploration, less verbose reasoning. Claude Opus 4.5 uses dramatically fewer tokens than its predecessors to reach similar or better outcomes.But different tasks call for different tradeoffs. Sometimes developers want a model to keep thinking about a problem; sometimes they want something more nimble. With our new effort parameter on the Claude API, you can decide to minimize time and spend or maximize capability.Set to a medium effort level, Opus 4.5 matches Sonnet 4.5’s best score on SWE-bench Verified, but uses 76% fewer output tokens. At its highest effort level, Opus 4.5 exceeds Sonnet 4.5 performance by 4.3 percentage points—while using 48% fewer tokens.With effort control, context compaction, and advanced tool use, Claude Opus 4.5 runs longer, does more, and requires less intervention.Our context management and memory capabilities can dramatically boost performance on agentic tasks. Opus 4.5 is also very effective at managing a team of subagents, enabling the construction of complex, well-coordinated multi-agent systems. In our testing, the combination of all these techniques boosted Opus 4.5’s performance on a deep research evaluation by almost 15 percentage points.We’re making our Developer Platform more composable over time. We want to give you the building blocks to construct exactly what you need, with full control over efficiency, tool use, and context management.Products like Claude Code show what’s possible when the kinds of upgrades we’ve made to the Claude Developer Platform come together. Claude Code gains two upgrades with Opus 4.5. Plan Mode now builds more precise plans and executes more thoroughly—Claude asks clarifying questions upfront, then builds a user-editable plan.md file before executing.Claude Code is also now available in our desktop app, letting you run multiple local and remote sessions in parallel: perhaps one agent fixes bugs, another researches GitHub, and a third updates docs.For Claude app users, long conversations no longer hit a wall—Claude automatically summarizes earlier context as needed, so you can keep the chat going. Claude for Chrome, which lets Claude handle tasks across your browser tabs, is now available to all Max users. We announced Claude for Excel in October, and as of today we've expanded beta access to all Max, Team, and Enterprise users. Each of these updates takes advantage of Claude Opus 4.5’s market-leading performance in using computers, spreadsheets, and handling long-running tasks.For Claude and Claude Code users with access to Opus 4.5, we’ve removed Opus-specific caps. For Max and Team Premium users, we’ve increased overall usage limits, meaning you’ll have roughly the same number of Opus tokens as you previously had with Sonnet. We’re updating usage limits to make sure you’re able to use Opus 4.5 for daily work. These limits are specific to Opus 4.5. As future models surpass it, we expect to update limits as needed.]]></content:encoded></item><item><title>Pebble Watch software is now open source</title><link>https://ericmigi.com/blog/pebble-watch-software-is-now-100percent-open-source</link><author>Larrikin</author><category>hn</category><pubDate>Mon, 24 Nov 2025 18:52:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Another big Pebble update today! TLDR:Yesterday, Pebble watch software was ~95% open source. Today, it’s 100% open source. You can download, compile and run all the software you need to use your Pebble. We just published the source code for the new Pebble mobile app!Pebble Appstore now has a publicly available backup and supports multiple feeds, providing long term reliability through decentralization. We’ve launched our own feed and Developer Dashboard.Pebble Time 2 schedule update (aiming to begin shipping in January, with most arriving on wrists in March/April)Pre-production Pebble Time 2 (Black/Red colourway) in all its gloryOver the last year, and especially in the last week, I've chatted with tons of people in the Pebble community. One of the main questions people have is ‘how do I know that my new Pebble watch will continue to work long into the future?’. It’s an extremely valid question and concern - one that I share as a fellow Pebble wearer. I called this out specifically in my blog post announcing the relaunch in January 2025. How is this time round going to be different from last time?There are two pieces to making Pebble sustainable long term - hardware and software.Nothing lasts forever, especially an inexpensive gadget like a Pebble. We want to be able to keep manufacturing these watches long into the future - mostly because I will always want one on my wrist! The company I set up to relaunch Pebble, Core Devices, is self funded, built without investors, and extremely lean. As long as we stay profitable (ie we don’t lose money), we will continue to manufacture new watches. We’re also making sure that our new watches are more repairable than old Pebble watches. The back cover of Pebble Time 2 is screwed in. You can remove the back cover and replace the battery.  We’ve also published electrical and mechanical design files for Pebble 2 Duo. Yes, you can download the schematic (includes KiCad project files) right now on Github! This should give you a nice jumpstart to designing your own PebbleOS-compatible device.Last time round, barely any of the Pebble software was open source. This made it very hard for the Pebble community to make improvements to their watches after the company behind Pebble shut down. Things are different now! This whole relaunch came about primarily because Google open sourced PebbleOS (thank you!). Yesterday, the software that powers Pebble watches was around 95% open source. As of today, it’s now 100%. This means that if Core Devices were to disappear into a black hole, you have all the source code you need to build, run and improve the software behind your Pebble.I confess that I misunderstood why 95% was much less sustainable than 100% until recently. I discuss this in more detail in my latest Tick Talk episode (check it out). Long story short - I’m an Android user and was happy to sideload the old Pebble APK on my phone, but iPhone and other Android users have basically been stuck without an easily available Pebble mobile companion app for years.    Here’s how we’re making sure the 3 main Pebble software components are open source and guaranteed to work long into the future: - software that runs on your watch itself. This has been 100% open source since January and we’ve committed to open sourcing all the improvements we’ve made → github.com/coredevices/PebbleOS. You can download the source code, compile PebbleOS and easily install it over Bluetooth on your new Pebble. Textbook definition of open source! Pebble mobile companion app -  the app that for your iPhone or Android. Without the app, your Pebble is basically a paperweight. When the Pebble Tech Corp died, the lack of an open source mobile app made it difficult for anyone to continue to use their watches. We had to build an entirely new app (get it here). Today, our app is now 100% open source on Github- ensuring that what happened before  happen again. Want to learn more about how we built the new app cross platform using Kotlin Multiplatform? Watch Steve’s presentation at Droidcon.Developer tools and Pebble Appstore - this software enables people to build and share their watchapps and watchfaces. Then there’s the Pebble Appstore. This is a collection of nearly 15,000 watchfaces and watchapps that you - the Pebble community - developed between 2012 and July 2018. When Fitbit pulled the plug on the original Pebble Appstore, the Rebble Foundation downloaded a copy of all the apps and faces, and set up a new web service to let users of the old Pebble app continue to download and use watchfaces. This was an incredible effort, one that I have used thousands of times and am a happy paying subscriber. But it’s still centralized - if their server disappears, there is no freely available backup. To compensate for that, today we’re launching two new things:The Pebble mobile app will soon (later this week) be able to subscribe to multiple appstore ‘feeds’. This is similar to open source package managers like pip, AUR, APT, etc. Anyone can create a Pebble-compatible appstore feed and users will be able to browse apps from that feed in the Pebble mobile app.We’ve created our own Pebble Appstore feed (appstore-api.repebble.com) and new Developer Dashboard. Our feed (fyi powered by 100% new software) is configured to back up an archive of all apps and faces to Archive.org (backup will gradually complete over the next week). Today, our feed only has a subset of all Pebble watchfaces and apps (thank you aveao for creating Pebble Archive!). Developers - you can upload your existing or new apps right now! We hope that this sets a standard for openness and we encourage all feeds to publish a freely and publicly available archive.Important to note - developers will still be able to charge money for their apps and faces, using Kiezel pay or other services. This change does not preclude them from doing that, in fact it makes it even easier - I could see some developers creating a paid-only feed. As I recently wrote, we're also working on other ways for Pebble developers to earn money by publishing fun, beautiful and creative Pebble apps.Another important note - some binary blobs and other non-free software components are used today in PebbleOS and the Pebble mobile app (ex: the heart rate sensor on PT2 , Memfault library, and others). Optional non-free web services, like Wispr-flow API speech recognizer, are also used. These non-free software components are not required - you can compile and run Pebble watch software without them. This will always be the case. More non-free software components may appear in our software in the future. The core Pebble watch software stack (everything you need to use your Pebble watch) will always be open source. Pre-production Pebble Time 2. These watches are not final quality! We are still tweaking and tuning everything.We’re currently in the middle of Pebble Time 2 design verification test (DVT) phase. After we finish that, we go into production verification test (PVT) and then mass production (MP). So far, things are proceeding according to the schedule update I shared last month but that is extraordinarily subject to change. We still have a lot of testing (especially waterproof and environmental) to go. If we find problems (which is likely) we will push the schedule back to make improvements to the product. The one major complicating factor is the timing of Chinese New Year (CNY). It’s early next year - factories will shut down for 3 weeks starting around the end of January. After restarting, things always take a week or two to get back to full speed. We are trying our best to get into mass production and ship out at most several thousand Pebble Time 2s before CNY. It’s going to be very tight 🤞. More likely is that production will begin after CNY, then we need to transfer the watches to our fulfillment center, and ship them out. Realistically, at this time we’re forecasting that the majority of people will receive their PT2 in March and April. Please keep in mind that things may still change.There will be 4 colour options for PT2 - black/black, black/red, silver/blue, silver/(white most likely). Let me be crystal very clear - no one has picked a colour yet 😃. In a few weeks, I will send out an email asking everyone who pre-ordered a Pebble Time 2 to select which colour they would like to receive. Please do not email us asking when this email will be sent out. No one has been invited yet to do this. I will post here after all emails have gone out.On a related note, I am extremely happy that we built and shipped Pebble 2 Duo. Not only is it an awesome watch, it was also a phenomenal way for us to exercise our production muscles and ease back into the systematic flow of building and shipping smartwatches. A video is worth a million words - so I encourage you to watch me demo Pebble Time 2 watches I just received this week. Keep in mind these watches are PRE-PRODUCTION which means they parts have imperfect qualities! Subject to change! ]]></content:encoded></item><item><title>Google&apos;s new &apos;Aluminium OS&apos; project brings Android to PC</title><link>https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/</link><author>jmsflknr</author><category>hn</category><pubDate>Mon, 24 Nov 2025 18:49:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The Android operating system is incredibly versatile. Beyond smartphones, it officially powers tablets, watches, TVs, cars, and XR headsets. However, it has virtually no presence on traditional PCs, where Google instead relies on ChromeOS. Despite Google’s efforts to challenge the dominance of Windows and macOS, ChromeOS remains a distant third. To close this gap, the company is unifying ChromeOS and Android into a single desktop platform, codenamed ‘Aluminium OS.’ Here’s what we know so far.Android on PCs: The story so farOne year ago,  exclusively revealed Google’s plan to rally behind Android as its unified desktop OS. Our source indicated that this shift aims to create products that better compete with the iPad while making more effective use of development resources. In July, a Google executive confirmed part of our reporting, revealing that the company intends to merge ChromeOS and Android into a single platform. Finally, at Qualcomm’s Snapdragon Summit in September, Google officially announced it is bringing Android to the PC market. The company stated it is collaborating with Qualcomm to build a new platform that converges mobile and desktop computing, leveraging recent advancements in AI.What feature would you most want from Aluminium OS?Qualcomm CEO Cristiano Amon (left) and Google SVP of Devices and Services Rick Osterloh (right) announcing a joint project to bring Android to PCs.While we now know Google is building Android for PCs, there are still many unknown details. Is Google retiring the ChromeOS brand? Will existing Chromebooks receive the new operating system, or will they be left behind? Will this OS arrive only on budget machines, or target premium PCs as well? What will the interface actually look like, and what new features can we expect?These are the burning questions as Google continues developing the platform. We likely won’t have all the answers until we get closer to launch, but thanks to job listings and bug reports, we’ve uncovered early details that offer some clues.Aluminium OS: Google’s PC ambitions take shapeOver the weekend, a tipster on Telegram named Frost Core shared a link to an intriguing Google job listing for a ‘Senior Product Manager, Android, Laptop and Tablets.’ While we already know Google is bringing Android to the PC, the listing explicitly states that the role involves ‘working on a new Aluminium, Android-based, operating system.’ This effectively confirms that Aluminium is the codename for the new unified platform. The name appears to be a nod to the project’s roots: like Chromium (the open-source version of ChromeOS), Aluminium is a metal ending in ‘-ium.’ The choice of the British spelling — emphasizing the ‘Al’ prefix — likely pays homage to Android serving as the project’s foundation.”Much like Android XR, Google says its new Aluminium OS is ‘built with artificial intelligence (AI) at the core.’ This implies deep integration with Gemini, Google’s AI chatbot and large language model (LLM). At the Snapdragon Summit, Rick Osterloh, Google’s SVP of Devices and Services, outlined the company’s plans to bring its AI stack to PCs:“This is another way we can leverage all of the great work we’re doing together on our AI stack, our full stack, bringing Gemini models, bringing the assistant, bringing all of our applications and developer community into the PC domain. And I think this is another way in which Android is gonna be able to serve everyone in every computing category.”We have yet to see exactly what new features Gemini will enable on Android PCs, but we hope the OS will fully leverage the hardware’s potential. On select premium smartphones, Gemini already powers an array of on-device AI features that demand significant memory and processing power from the CPU, GPU, and NPU. There were concerns that Google might restrict this new OS to the same budget-friendly niche where Chromebooks currently excel, ceding the high-end market to Microsoft and Apple. However, the job listing dispels those fears.The new Senior Product Manager role is tasked with “driving the roadmap and curating a portfolio of ChromeOS and Aluminium Operating System (ALOS) Commercial devices across all form factors (e.g. laptops, detachables, tablets, and boxes) and tiers (e.g., Chromebook, Chromebook Plus, AL Entry, AL Mass Premium, and AL Premium) that meets the needs of users and the business.”This confirms that Android won’t be limited to laptops; the roadmap explicitly includes detachables, tablets, and ‘boxes’ (likely mini-PCs akin to the Chromebox or Mac Mini). Furthermore, the tiered structure — listing ‘AL Mass Premium’ and ‘AL Premium’ alongside ‘AL Entry’ — indicates that Google intends to push Android beyond budget PC hardware. While exact pricing for these tiers is hard to predict, it is clear Google aims to compete across the entire spectrum — a strategy foreshadowed by the recent Chromebook Plus initiative.Speaking of Chromebooks, the job listing also raises questions about the future of ChromeOS. The listing notes that the person will help “drive ChromeOS and Aluminium (e.g., Android) platforms and devices,” creating a roadmap and product portfolio that encompasses both. This implies the two platforms will coexist for some time. However, the person is also explicitly tasked with developing a strategy for transitioning “Google from ChromeOS to Aluminium with business continuity in the future.” This confirms that Google aims to eventually replace ChromeOS entirely — a move that must be managed carefully to avoid disrupting enterprise customers. This transition will likely require a multi-pronged approach: Existing ChromeOS devices that cannot be migrated to Aluminium OS will likely receive updates until they reach their end-of-life. This means Google will need to maintain the legacy ChromiumOS codebase for several more years. Rather than forcing an immediate switch, Google may offer an optional upgrade path for capable hardware. The company is currently testing Aluminium OS on development boards featuring MediaTek Kompanio 520 and 12th Gen Intel Alder Lake processors, so existing Chromebooks with these chips could be eligible for the update. However, migrating an operating system on live hardware is a massive technical hurdle that will require meticulous execution.And of course, there will be new PCs launching with Aluminium OS out of the box as well.Is ChromeOS dead as we know it?Even if Google replaces the entire foundation of ChromeOS with Android, the company may be reluctant to abandon the name. While it lacks the market share of Windows or macOS, the ChromeOS brand is widely recognized, particularly in the education and enterprise sectors. Although the job listing doesn’t confirm the final naming scheme, bug reports spotted by Frost Core hint that Google may retain the branding. Engineers have referred to the current platform as “ChromeOS Classic” and “non-Aluminium ChromeOS,” implying the new Android-based version could simply usurp the name “ChromeOS.”Would you miss Chrome OS if Google sunsets it for 'Aluminium OS'?Alternatively, Google might adopt “Android Desktop” as the name to align with its renewed focus on promoting Android as a brand. However, “Android Desktop” could merely be an internal designation for the form factor. Since these references have only appeared in bug reports, the final marketing name remains an open question.When will Android on PCs launch?Google is actively developing the platform, with bug reports confirming that the company is testing fresh builds of Android 16 on development hardware. The company has confirmed the project will launch in 2026, though it remains unclear whether it will arrive in the first or second half of the year. Given this timeline, it is highly likely that the initial public release will be built upon Android 17, which is due next year. We will continue to monitor the project to find further details ahead of its official debut.Thank you for being part of our community. Read our Comment Policy before posting.]]></content:encoded></item><item><title>GrapheneOS migrates server infrastructure from France</title><link>https://www.privacyguides.org/news/2025/11/22/grapheneos-migrates-server-infrastructure-from-france-amid-police-intimidation-claims/</link><author>01-_-</author><category>hn</category><pubDate>Mon, 24 Nov 2025 18:48:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The GrapheneOS project has announced on X that they are ceasing all operations in France, asserting that the country is no longer safe for "open source privacy projects". While the operating system will still be available to French users, all website and discussion servers are being relocated abroad.Until now, the project relied on OVH Bearharnois, a French hosting provider, for its core website and social media services. The migration plan moves the Mastodon, Discourse, and Matrix instances to a combination of local and shared servers in Toronto. Critical website infrastructure will be hosted by Netcup, a German‑based company.
                            Join Privacy Guides
                        GrapheneOS claims that they does not collect confidential user data in their servers or store critical infrastructure in France. Therefore, the migration does not affect services such as signature verification and downgrade protection for updates.Citing the government's support of the European Union Chat Control proposal, GrapheneOS developers are also refusing travel to France. Developers are no longer allowed to work inside the country due to safety concerns.This decision was sparked by negative press coverage from two articles published by . An interview with French cybercrime prosecutor Johanna Brousse implies potential legal action against the project:"With this new tool, there is real legitimacy for a certain portion of users in the desire to protect their exchanges. The approach is therefore different. But that won't stop us from suing the publishers if links are discovered with a criminal organization and they don't cooperate with the law"GrapheneOS argues that  have conflated their project with government-sponsored forks, which are fake copies of their operating system. The news outlet refers to a fake Snapchat app, dark web advertising, and a series of unlisted YouTube videos that are not features of GrapheneOS itself.The project had previously threatened litigation against these government-sponsored forks. One prominent example is ANOM, an FBI-backed shell company that developed a compromised Android operating system and messaging platform as part of Operation Trojan Horse from 2018 and 2021.]]></content:encoded></item><item><title>The Bitter Lesson of LLM Extensions</title><link>https://www.sawyerhood.com/blog/llm-extension</link><author>sawyerjhood</author><category>hn</category><pubDate>Mon, 24 Nov 2025 18:32:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Three years ago, “using an LLM” meant pasting a wall of text into a chat box and hoping for something useful back. Today, we point agents at our codebases, our browsers, and let them go off and act on our behalf. A key question that has been brewing under the surface during this time has been: how do we let end users actually customize these systems?As models have become more capable, the ways and mechanisms that end users have access to customize them have expanded as well. We've gone from simple system prompts to complex client-server protocols and back again.I wanted to take a moment to reflect on the history of LLM extension over the last three years and where I see it going in the future.Just four months after launch, OpenAI announced . Looking back, these were wildly ahead of their time.The idea was ambitious: give the LLM a link to an OpenAPI spec and let it "run wild" calling REST endpoints. It was a direct line to AGI-style thinking: universal tool use via standard APIs.The problem? The models weren't ready. GPT-3.5 (and even early GPT-4) struggled to navigate massive API specs without hallucinating or getting lost in context. Plus, the UX was clunky. You had to manually toggle plugins for every chat!Here's what that looked like:But it gave us a glimpse of the future:  plugin (later Advanced Data Analysis) became indispensable, foreshadowing the powerful sandboxed execution environments we use today.Custom instructions were the "smooth brain" counter-reaction to the complexity of plugins. I did a double take when writing this because I thought for sure this feature was released before plugins.It was just a user-defined prompt appended to every chat. Simple. Obvious. Yet it solved a huge problem: repetitive context setting.This was the spiritual ancestor to every  and  file that followed.OpenAI repackaged instructions and tools into . This was an attempt to "productize" prompt engineering. You could bundle a persona, some files, and a few actions into a shareable link.It was a retreat from the open-ended promise of plugins toward curated, single-purpose "apps."So far, we've discussed manual ways to extend LLMs.  represented a shift toward automatic personalization.ChatGPT Memory records details from your conversations and quietly inserts them into future context. It's like a system prompt that writes itself. If you mention you're a vegetarian, it remembers that weeks later. It’s a small feature, but it marked the beginning of agents that maintain long-term state without user intervention. changed the game by putting custom instructions where they belonged: .The  file was a revelation. Instead of pasting context into a chat window, you committed it to git."We use tabs, not spaces."It started as a single file, then evolved into a  folder with sophisticated scoping. You could organize multiple rule files, and even define when they applied, for example, only for certain file types or subdirectories. It was the first time extension felt "native" to the code.Later Cursor introduced the ability to let the LLM decide when to apply a rule, which is a pattern we will see again.By late 2024, models were finally smart enough to handle real tools reliably. Anthropic's Model Context Protocol (MCP) was the answer.MCP is a heavyweight solution. An MCP client needs to keep a persistent connection to an MCP server. The server serves up tool definitions, resources, and prompts to the client (in most cases is an agent) and it can send a message to the server saying a tool has been called and the server can respond with the result.Unlike Custom Instructions (which just add context), MCP gives the model actual capabilities. It can read your repo, query your Postgres DB, or deploy to Vercel. Besides just providing tools, it also allows servers to provide  (documents, logs) and  directly to the agent.It's powerful, and perhaps a bit of overkill. While the complexity might be worth it for agent developers asking a user to set up and connect an MCP is a lot of friction and there is an entire ecosystem of startups like Smithery built around making it easier to use MCP.It is worth noting that ChatGPT apps which were announced in October 2025 are built on top of MCP as a base layer. This is an attempt to make it easier for end users to use MCP without having to actually think about it.Claude Code: New Agent, New Extensions (Feb 2025)Early 2025 brought us , which essentially added every extension mechanism under the sun to an agent. The standard for repo-level instructions. For heavy-duty tool integration. Like Cursor's notebooks, for reusable prompts. The ability to intercept and modify the agent's loop (e.g., "Stop if the tests fail"). Spawning specialized workers to handle sub-tasks. (Deprecated) Configuring tone and format.Time will tell how many of these features will stick around in the long term. Anthropic has already tried to deprecate output styles.The next extension mechanism added to Claude Code is significant enough to warrant a deeper dive.  are the rebirth of ChatGPT Plugins.While MCP has a whole client-server protocol, Agent Skills are just folders of markdown files and scripts (in whatever language you choose).The agent simply scans a  directory, reads the frontmatter of every , and builds a lightweight index. It then chooses to read the full contents of a skill only if it's appropriate for the current task. This solves one of the major problems with MCP: the context bloat that comes from having to load all of the tool definitions into the context window at once.Here is a snippet of the structure of a skill for doing e2e testing with Playwright taken from Anthropic's Skills examples repository:There is a mix of scripts, examples, and plain text instructions. The only required file is the SKILL.md file. Let's take a look at that file:This is just a plain markdown file with some metadata and a description of the skill. The agent reads the file which freely references other files that the agent can read. In contrast a playwright MCP server has dozens of tool definitions to control a browser, this skill just says "you have bash, this is how you write a playwright script".Granted to use a skill the agent needs to have general purpose access to a computer, but this is the bitter lesson in action. Giving an agent general purpose tools and trusting it to have the ability to use them to accomplish a task might very well be the winning strategy over making specialized tools for every task.Skills are the actualization of the dream that was set out by ChatGPT Plugins: just give the model instructions and some generic tools and trust it to do the glue work in-between. But I have a hypothesis that it might actually work now because the models are actually smart enough for it to work.Agent skills work because it assumes the agent has the ability to write its own tools (via bash commands). You can just give it code snippets and ask the agent to figure out how to run them generically for the task at hand.Importantly, I think that skills signal towards a new definition of what an agent really is. An agent isn't just a LLM in a while loop. It's an LLM in a while loop that has a computer strapped to it.Claude Code is the piece of software that first made this click for me, but it is way too developer focused to be the final form. Other applications like Zo Computer try to package the llm and computer together into a single application, but I still think it still doesn't abstract the computer away enough from the end user. If I ask a coworker to do something, I don't need to see their entire file system, I just need to know that they have a computer.Looking forward into 2026 I expect more and more llm applications that we use will have a computer strapped to them in new and interesting ways, whether we know it or not.If I could short MCP, I would, and I expect us to go back to extending our agents with the most accessible programming language: natural language.]]></content:encoded></item><item><title>TSMC Arizona Outage Saw Fab Halt, Apple Wafers Scrapped</title><link>https://www.culpium.com/p/tsmc-arizona-outage-saw-fab-halt</link><author>speckx</author><category>hn</category><pubDate>Mon, 24 Nov 2025 18:30:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Good Evening from Taipei,A power outage at an industrial gas facility servicing TSMC interrupted manufacturing at the company’s Fab 21 in Arizona late last quarter, sources told me. The incident stopped the flow of crucial inputs needed for chipmaking, forcing the facility to shut down for at least a few hours, I was told. As a result, the company had to scrap thousands of wafers that were in production for clients at the site which include Apple, Nvidia, and AMD.outsourced vendor Lindeassumption thatUnfortunately, the company declined to immediately address the issue of the manufacturing disruption. Fab shutdowns are unusual, at least for TSMC. With equipment so expensive, its factories are run 24/7. That means that an hour of idle time can cost millions of dollars. Compounding the financial effect of this incident was the fact that it occurred late in the quarter, leaving little room to make up for lost production before the quarter closed. Profit margins on new facilities and at new nodes tend to be quite thin, even negative. In addition, TSMC has been ramping up capacity in Arizona and that capex gets reflected in depreciation costs even before the new equipment can start producing revenue. So it’s reasonable to see fluctuations in net income at the site. A halt in production and scrapping of wafers adds to the costs, dragging on earnings even if only slightly and briefly.Impact to clients is likely to be negligible, I was told, and the financial loss to TSMC may be covered by insurance. Capacity at Fab 21 is still quite small, and many products being made there have already been taped out and manufactured in Taiwan previously. In past disruptions, lost production and revenue was made up in the subsequent quarter. an exercise in cross-cultural adaptationThe most common cause of production interruptions at TSMC is Mother Nature. Earthquakes regularly rattle Taiwan, and fabs are built to withstand most of them. But sometimes a big tremor can trigger a safety shutdown, while really nasty temblors have caused actual damage. Beyond natural disasters, there’ve been few man-made shutdowns at TSMC because they’re pretty rigorous about operations. a computer virus was introduceda batch of contaminated photoresistSharing is caring. This post is public & free, so please tell your friends what you’re reading.trumpeted the TSMC contractArizona Tech Council later reportedThanks for reading. Please subscribe, if you haven’t already.]]></content:encoded></item><item><title>Show HN: I built an interactive HN Simulator</title><link>https://news.ysimulator.run/news</link><author>johnsillings</author><category>hn</category><pubDate>Mon, 24 Nov 2025 17:52:43 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cool-retro-term: terminal emulator which mimics look and feel of CRTs</title><link>https://github.com/Swordfish90/cool-retro-term</link><author>michalpleban</author><category>hn</category><pubDate>Mon, 24 Nov 2025 17:52:01 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Implications of AI to schools</title><link>https://twitter.com/karpathy/status/1993010584175141038</link><author>bilsbie</author><category>hn</category><pubDate>Mon, 24 Nov 2025 17:51:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>France threatens GrapheneOS with arrests / server seizure for refusing backdoors</title><link>https://mamot.fr/@LaQuadrature/115581775965025042</link><author>nabakin</author><category>hn</category><pubDate>Mon, 24 Nov 2025 17:00:07 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>France threatens GrapheneOS with arrests / server seizure for refusing backdoors</title><link>https://mamot.fr/@LaQuadrature/115581775965025042</link><author>nabakin</author><category>hn</category><pubDate>Mon, 24 Nov 2025 16:40:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>X Just Accidentally Exposed a Covert Influence Network Targeting Americans</title><link>https://weaponizedspaces.substack.com/p/x-just-accidentally-exposed-a-vast</link><author>adriand</author><category>hn</category><pubDate>Mon, 24 Nov 2025 16:07:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Shai Hulud launches second supply-chain attack</title><link>https://www.aikido.dev/blog/shai-hulud-strikes-again-hitting-zapier-ensdomains</link><author>birdculture</author><category>hn</category><pubDate>Mon, 24 Nov 2025 16:03:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[It's another Monday morning, sitting down at the computer. And I see a stack of alerts from the last hour of packages showing signs of malware in our triage queue. Having not yet finished my first cup of coffee, I see Shai Hulud indicators. Yikes, surely that's a false positive? Nope, welcome to Monday, Shai Hulud struck again. Strap in.Timeline of the Shai-Hulud CampaignThe timing is notable, given npm’s recent announcement that it will revoke classic tokens on December 9 after the wave of supply-chain attacks. With many users still not migrated to trusted publishing, the attacker seized the moment for one more hit before npm’s deadline.August 27 - We release our report detailing the S1ngularity campaign targeting several nx packages on npm.  September 18 - We publish a follow-up analysis, diving deeper into the campaign’s technical quirks and early payload behavior.  November 24 - A second strike occurs, dubbed the “Second Coming” by the attackers, timed just before npm’s deadline for revoking old tokens.What is Shai-Hulud?: A Quick Refresher Shai-Hulud, named after the gigantic sandworms from Dune as part of the attacker's flair for theatrics, is a self-replicating npm worm built to spread quickly through compromised developer environments. Once it infects a system, it searches for exposed secrets such as API keys and tokens using TruffleHog and publishes anything it finds to a public GitHub repository. It then attempts to push new copies of itself to npm, helping it propagate across the ecosystem, while exfiltrating data back to the attacker. Keeping with the dramatic theme, the attacker refers to this latest wave as the “Second Coming.”Differences from last timeThis time around, there are some significant differences in the attack:It install bun with the file  and then uses that to execute   which is the actual malicious code.It creates a randomly named repository with stolen data, rather than a hardcoded name.It will infect up to 100 npm packages, compared to 20 last time.If it can't authenticate with GitHub or NPM, it will wipe all files in the users Home directory.This time, the malware also publishes secrets to GitHub, with a random name and the repository description:"Sha1-Hulud: The Second Coming."Currently we see 26.3k repositories exposed:As we've been analzying all these packages, we've noticed a number of compromised packages that appear to be from community spread, which contain the initial staging code in , but NOT  which is the Shai Hulud worm itself. Here's the code that spreads the worm into other packages:      }
    }
  }We see that the  may sometimes not be bundled, depending on different factors. It appears that mistakes were once again made by the attackers. This appears to have limited the imapct of the attack at this time. Compromised GitHub repositoriesThe AsyncAPI team detected that there had been a branch of their CLI project, which was created just prior to the malicious packages being pushed, which deployed a version of the Shai Hulud malware. Companies acknowlege incidentGiven the nature of the incident, we were very happy to see companies quickly acknowledge what happened, in posts from these companies:We detected the first packages starting at 11/24/2025 3:16:26 AM GMT+0, which were the packages go-template, and 36 packages from . Many more packages were quickly compromised. Afterwards, they started compromising PostHog packages at 11/24/2025  4:11:55 AM GMT+0, and Postman packages at 11/24/2025  5:09:25 AM GMT+0.‍Which packages are affected?We've detected the following packages compromised with a new version of Shai Hulud. Between all these 492 packages, they have a total of 132 million monthly downloads:@asyncapi/nodejs-ws-template@asyncapi/avro-schema-parser@asyncapi/dotnet-rabbitmq-template@asyncapi/nunjucks-filters@asyncapi/protobuf-schema-parser@asyncapi/python-paho-template@asyncapi/java-spring-cloud-stream-template@asyncapi/generator-helpers@asyncapi/react-component@asyncapi/java-spring-template@asyncapi/go-watermill-template@asyncapi/openapi-schema-parser@asyncapi/generator-componentsgithub-action-for-generator@asyncapi/nodejs-template@asyncapi/markdown-template@quick-start-soft/quick-git-clean-markdown@quick-start-soft/quick-markdown-image@quick-start-soft/quick-markdown-translator@quick-start-soft/quick-markdown@asyncapi/generator-react-sdk@quick-start-soft/quick-markdown-composemanual-billing-system-miniapp-api@strapbuild/react-native-perspective-image-cropper@quick-start-soft/quick-task-refine@strapbuild/react-native-date-time-picker@strapbuild/react-native-perspective-image-cropper-2@strapbuild/react-native-perspective-image-cropper-poojan31@quick-start-soft/quick-markdown-print@quick-start-soft/quick-remove-image-backgroundeslint-config-zeallat-basekorea-administrative-area-geo-json-util@quick-start-soft/quick-document-translator@posthog/first-time-event-tracker@posthog/event-sequence-timer-plugin@posthog/gitub-star-sync-pluginposthog-plugin-hello-world@posthog/bitbucket-release-tracker@posthog/snowflake-export-pluginposthog-react-native-session-replay@posthog/drop-events-on-property-plugin@posthog/github-release-tracking-plugin@posthog/plugin-unduplicates@posthog/react-rrweb-playerdrop-events-on-property-plugin@posthog/ingestion-alert-plugin@posthog/laudspeaker-plugin@posthog/automatic-cohorts-plugin@posthog/migrator3000-plugin@posthog/pagerduty-plugin@posthog/customerio-plugin@posthog/netdata-event-processing@posthog/url-normalizer-plugin@posthog/currency-normalization-plugin@posthog/filter-out-plugin@posthog/heartbeat-plugin@actbase/react-native-fast-image@posthog/databricks-plugin@actbase/react-native-kakao-channel@actbase/react-daum-postcode@actbase/react-native-simple-video@actbase/css-to-react-native-transform@actbase/react-native-actionsheet@actbase/react-native-tiktok@seung-ju/react-native-action-sheet@actbase/react-native-devtools@actbase/react-native-less-transformer@actbase/react-native-kakao-navi@posthog/twitter-followers-plugin@actbase/react-native-naver-login@seung-ju/openapi-generatorreact-native-worklet-functions@postman/secret-scanner-wasm@postman/pm-bin-macos-arm64@postman/pm-bin-linux-x64@postman/postman-collection-fork@postman/postman-mcp-server@postman/wdio-junit-reporter@postman/pm-bin-windows-x64@postman/wdio-allure-reporter@postman/final-node-keytar@postman/pm-bin-macos-x64@aryanhussain/my-angular-libcapacitor-plugin-apptrackingioscapacitor-plugin-purchasecapacitor-purchase-historycapacitor-voice-recorder-wavcapacitor-plugin-scgssigninwithgoogle@kvytech/medusa-plugin-announcement@kvytech/medusa-plugin-product-reviewsmedusa-plugin-product-reviews-kvy@kvytech/medusa-plugin-promotionmedusa-plugin-announcement@kvytech/medusa-plugin-newsletter@kvytech/medusa-plugin-management@ensdomains/vite-plugin-i18next-loader@ensdomains/cypress-metamask@ensdomains/ccip-read-dns-gateway@ensdomains/ccip-read-cf-worker@ensdomains/dnssec-oracle-anchors@ensdomains/reverse-records@ensdomains/hackathon-registrar@ensdomains/renewal-widget@ensdomains/server-analytics@ensdomains/ccip-read-router@zapier/babel-preset-zapier@ensdomains/hardhat-chai-matchers-viem@ensdomains/ccip-read-worker-viem@zapier/browserslist-config-zapier@zapier/spectral-api-ruleset@ensdomains/address-encoder@ensdomains/eth-ens-namehashzapier-platform-legacy-scripting-runner@ensdomains/dnssecoraclejs@ensdomains/op-resolver-contracts@ensdomains/ens-archived-contracts@ensdomains/subdomain-registrar@ensdomains/unruggable-gateways@ensdomains/ens-contracts@ensdomains/react-ens-address@ensdomains/curvearithmetics@ensdomains/hardhat-toolbox-viem-extended@ensdomains/durin-middleware@ensdomains/unicode-confusables@zapier/eslint-plugin-zapier@ensdomains/offchain-resolver-contracts@ensdomains/ens-validation@markvivanco/app-version-checker@orbitgtbelgium/mapbox-gl-draw-scale-rotate-mode@trigo/eslint-config-trigo@trigo/atrix-elasticsearch@trigo/hapi-auth-signedlinkreact-element-prompt-inspector@orbitgtbelgium/mapbox-gl-draw-cut-polygon-mode@orbitgtbelgium/time-slider@orbitgtbelgium/orbit-components@mparpaillon/connector-parse@mparpaillon/imagesloaded@osmanekrem/error-handler@ifelsedeveloper/protocol-contracts-svm-idl@dev-blinq/cucumber_client@lessondesk/eslint-configreact-native-retriable-fetchsvelte-autocomplete-selectparcel-plugin-asset-copierreact-native-datepicker-modalchrome-extension-downloads@alexcolls/nuxt-socket.iosa-company-registration-number-regex@tiaanduplessis/react-progressbarreact-native-get-pixel-dimensions@varsityvibe/validation-schemas@clausehq/flows-step-jsontoxml@accordproject/concerto-analysis@accordproject/markdown-it-cicero@fishingbooker/react-swiper@fishingbooker/browser-sync-plugin@fishingbooker/react-loader@fishingbooker/react-pagination@voiceflow/default-prompt-wrappers@voiceflow/npm-package-json-lint-config@voiceflow/nestjs-mongodb@voiceflow/commitlint-config@voiceflow/git-branch-check@voiceflow/prettier-config@voiceflow/stylelint-config@voiceflow/storybook-config@voiceflow/nestjs-timeout@voiceflow/serverless-plugin-typescript@voiceflow/voiceflow-types@voiceflow/nestjs-rate-limit@antstackio/express-graphql-proxy@antstackio/json-to-graphql@antstackio/eslint-config-antstack@voiceflow/semantic-release-config@voiceflow/circleci-config-sdk-orb-import@voiceflow/slate-serializer@voiceflow/google-dfes-types@accordproject/markdown-docx@clausehq/flows-step-sendgridemail@lpdjs/firestore-repo-servicemon-package-react-typescriptPotential impact of Shai-Hulud: Second ComingThreat actors have slipped malicious code into hundreds of NPM packages — including major ones from Zapier, ENS, AsyncAPI, PostHog, Browserbase, and Postman. If a developer installs one of these bad packages, the malware quietly runs , before anything even finishes installing. This gives it access to the developer’s machine, build systems, or cloud environment. It then uses an automated tool (TruffleHog) to search for sensitive information like passwords, API keys, cloud tokens, and GitHub or NPM credentials. Anything it finds is uploaded to a public GitHub repository labeled “Sha1-Hulud: The Second Coming.” If those stolen secrets include access to code repositories or package registries, attackers can use them to break into more accounts and publish more malicious packages, helping the attack spread further. Because trusted ecosystems were involved and millions of downloads are affected, any team using NPM should immediately check whether they were impacted and rotate any credentials that may have leaked.‍Which actions should security teams take?Audit all Zapier/ENS-related npm dependencies and versions.Rotate  GitHub, npm, cloud, and CI/CD secrets used during installs.Check GitHub for strange repos with the description  “Sha1-Hulud: The Second Coming”Disable npm  scripts in CI where possible.Pin package versions and enforce MFA on GitHub and npm accounts.Use tools like Safe-Chain to block malicious packages on NPM ‍Story developing... Stay tuned for updates. ]]></content:encoded></item><item><title>Show HN: Cynthia – Reliably play MIDI music files – MIT / Portable / Windows</title><link>https://www.blaizenterprises.com/cynthia.html</link><author>blaiz2025</author><category>hn</category><pubDate>Mon, 24 Nov 2025 13:58:05 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[



Reliably play midi music files from a  folder or ".m3u"  play list. Adjust playback speed, volume and output device on-the-fly during playback. A large playback progress bar makes jumping forward and backward in time a breeze with just a single click or tap. Supports ".mid", ".midi" and ".rmi" files in format 0 (single track) and format 1 (multi-track). Comes complete with 25 sample midis ready to play.Cynthia playing through her sample midi musicDesktop App (Standard Edition)4th Generation (Gossamer for GUI)Windows All and Wine for Linux and MacSHA256 Checksum8B9CAB404E174767144C13A32C7357FDF320EA6D632069E68AFC10F107F98F38Cynthia playing through her sample midi musicCynthia displaying her menuRealtime display panels Visual (Tracks, Channels, and Notes), Piano and Bars visibleCynthia in her traditional view - Navigation, Information and Settings panels visibleEasily adjust channel volumes on-the-fly with the integrated mixer - hover over/tap the Channels panel to show/editCynthia in the "Deep Orange 2" color scheme (Options > Color) and "Swell" animated background scheme (Options > Background), with Compact mode on (Options > Settings > Compact)Several sample midis built-in.  When starting Cynthia for the first time, these sample midis are listed in the " Play Folder" panel, and playback is automatic.
At any point during playback, you may select another midi from the list.  Playback seamlessly switches to the selected midi in question.
Cynthia supports realtime changes to settings during playback.  This means you may adjust the Playback Mode, Playback Device, Volume and Speed without having to stop/restart playback.⏶The main toolbar is located near the top of Cynthia's window.  From left to right, it has the links of: Nav - Toggle display of navigation panel Play Folder - Show the "Play Folder" panel to play midis from a folder Play List - Show the "Play List" panel to play midis from a playlist Prev - Play previous midi file in list Rewind - Shift playback position back several seconds Stop - Stop playback Play - Toggle playback: When playing ( flashing) playback stops/when stopped ( static) playback starts Fast Forward - Shift playback position forward several seconds Next - Play next midi file in list Menu - Show menu Mixer - Show the Windows Mixer app Options - Show the Options window, to change Cynthia's appearance Help - Show (from rightmost column) or hide built-in help⏶From the main toolbar click " Play Folder" link (top left) to display the "Play Folder" panel. Play Folder, there is no need for a playlist or setup.  Just navigate to the folder in question and instantly play the midis within.   Home" entry at top of list (scroll up if not visible).  The list will refresh with the names of your hard drives, pen sticks and other local storage devices.  Double click a drive, then the subsequent folder(s) to the one that you want to play.
The list will update.  Double click a specific midi to begin playback, or, click the " Play" link. Refresh - Refresh the playback list Fav - Show the "Favourites" window.  Use this handy window to maintain a list of your favourite folders for quick access. Back - Go to the previous folder in navigation history Forward - Go to the next folder in navigation history:There is no need to stop playback before switching to another folder.  Double click the new folder and wait several seconds for Cynthia to automatically recommence playback.
The current folder will be remembered when Cynthia is restarted.⏶The standard ".m3u" playlist format is supported.  This is a plain text file that contains the length of the midi in seconds, a title, and the location of each midi.
Cynthia supports midi playback from local storage devices, such as hard disks, pen sticks etc.  Internet urls are not supported.
If you already have a playlist in the m3u format saved on your disk, you can open it in Cynthia for playback.  From the main toolbar click the " Play List" link (top left) to show the Play List panel.  An additional toolbar presents.  Click the " Open" link (if shown) or " Edit >  Open".  From the Open window, navigate to your playlist, select and click the "Open" button.
The contents of your playlist will load inside the  Play List panel.  Click the " Play" link to begin playback.⏶There are several ways a playlist can be constructed.  The first method is the easiest.  From your computer explorer ("File Explorer" in Windows and "Files" in Ubuntu) navigate to the folder of midis.  Highlight one or more midis files inside the folder and drag the selection onto Cynthia and let go of the mouse button.  The " Play List" panel updates and displays the dropped midi files appended, to the list.  Repeat this process for as many midi files as required.
At anytime you may save the playlist.  From the Play List toolbar, click the " Save As" link (if shown) or " Edit >  Save As...".  Type a name in the Save window and click the "Save" button to save the playlist.
It is worth noting that each time Cynthia saves your playlist to file, the midi files referenced inside it have their names adjusted automatically to work with the specific save location of the playlist.
Most midi filenames in a playlist are relative, as they do not have the full drive, folder and filename, but rather a partial folder structure and the midi's name.  This is to permit the movement of the midis and the playlist from one location to another without the need for the playlist to be specifically rewritten.
If you are curious at what the playlist format looks like, click the " Copy All" link (if shown) or " Edit >  Copy All" to copy the entire playlist to Clipboard.  Note Cynthia will use a full filename for each listed midi file, since the Clipboard cannot be referenced from a disk location.  You may paste it into any text editor to view, modify, or rearrange the order of the midi files listed.
To paste an altered playlist back into Cynthia, click the " Replace" link (if shown) or " Edit >  Replace".  The Play List panel will update.
Cynthia has support for extended playlists.  Note!  A large playlist of 100,000+ midi files will use about 102MB of RAM and require a second or two to apply midi filename adjustments.
Support for playlist filtering is provided.  An example:  You may instruct Cynthia to list only ".mid" files by deselecting ".midi" and ".rmi" options from the "File Types" option panel (right column).  The playlist itself remains unchanged - how Cynthia uses it changes.  Edit - Show edit menu New - Prompt to clear playlist Open - Open a playlist from file Save As - Save playlist to file Cut - Cut selected playlist item to Clipboard Copy - Copy selected playlist item to Clipboard Copy All - Copy entire playlist to Clipboard Paste - Add Clipboard playlist to end of current playlist Replace - Replace current playlist with Clipboard playlist Undo - Undo last change to playlist:
The current playlist is remembered for next time Cynthia is started.
To show or hide the above toolbar links, tick or untick the " Edit > Show Links on Toolbar" option.  Unticking the option hides all links except " Edit" and " Paste".⏶If you're a lover of simplicity itself, wishing only to play midi files directly from your hard drive folders as is, or, you're a playlist fan, rest assured, switching between these two very different playback systems, is as easy as a single click on either the " Play Folder" or " Play List" links.
Whilst the Play Folder method is limited to playing only the files contained in the currently selected folder, there is zero setup, no list to be built, and playback can start without hassle.
The Play List method on the other hand allows for a far more in depth custom playback experience.  It supports the playback of midis across multiple disk drives, folders and in any assembled order.
Additionally, Cynthia's large capacity list can easily handle a very large playlist.  For example a playlist of 10,000+ midis is just fine.
And switching between these two playback methods can be done during playback without issue.:
The playback options File Types, Playback Mode, Playback Device, Speed and Volume are shared between both playback systems, and do not change or require adjustment after a switch.⏶Located bottom of left column.
Playback mode (bottom left) determines how Cynthia plays the list of midis in the Play Folder or Play List panel (left column).Play currently selected midi to the end, and stop playback.
Repeat the currently selected midi without stopping playback.
Play each midi in turn, working down the list (left column), then restart from the top.  Playback initially starts at currently selected midi.
Each midi in the list is played working downward through the list.  Playback stops at the end of the last midi.
Continuously play a midi, selecting each new one randomly from the list.⏶Located towards bottom of right column.There are three midi file types supported: ".mid", ".midi" and ".rmi".  The first two are identical file formats, only the file extension differs.  The third format, ".rmi", is slightly different and contains additional multimedia information for Microsoft Windows.
By default all three file types are selected (lit black).  In this state, all playable midi files are listed in the Play Folder and Play List panels.
To restrict the file types to be played back, click the file type to deselect it.  The list of midis (left column) updates to reflect the change.
If all three options are deselected, Cynthia interprets this as if all three were selected.⏶Located towards bottom of right column.
By default all midi playback is sent to the Windows Midi Mapper system - the default Windows midi note handler.
If you have more than one midi device installed on your computer, Cynthia can redirect the midi notes to that device instead.
Traditionally, a midi device had been considered to be hardware.  But now, with the advent of powerful computer hardware, software can now act as virtual hardware, allowing for advanced features to be included on your computer without the need for hardware upgrades or physical adjustment.
A midi software driver can support a soundfont, which can greatly enhances the playback quality of a midi through it's support for large, high-quality, instrumental sound libraries.
To change the playback device, select a number in the playback device control (bottom right).  Up to ten devices (1-10) is supported.  "Map" is the Windows Midi Mapper.  Selecting a dash will cause playback to stop producing audible sound (no device selected for sound output).  In this case, the last usable (numbered) midi device will be automatically selected after a short time delay, recommencing audible playback without the need for user input.
Cynthia supports realtime device switching during playback.  A small, momentary interruption to playback may occur during a device change.  The name of the device in use by Cynthia is listed in the playback device control (bottom right), for example as "Playback Device: Midi Mapper".  In later versions of Microsoft Windows the Midi Mapper was discontinued - in this case, Cynthia uses the first installed midi device. 
It is worth noting that using another device may require a separate adjustment to that device's volume control, some devices do, and some do not.  If it does have a volume control, it is more than likely to be accessible via Windows "Volume Mixer" application.  Click the " Mixer" link from the top toolbar to display the application and adjust the volume control of your device accordingly.⏶Located towards bottom of right column.
By default, Cynthia plays back a midi at normal speed (100%).  Drag the slider to the right to increase the playback speed up to a maximum speed of 1,000% or 10x normal speed.
To slow down playback speed, drag the slider to the left.  A value less than 100% slows playback to less than normal speed.  The slider can go as low as 10% or 1/10th normal playback speed.
Playback speed may be adjusted at any point during playback.  All changes take place in realtime.  An auto-fade in feature momentarily quietens playback to avoid any sudden or unexpected notes.⏶Located at bottom of right column.
For maximum compatibility between different operating systems and device types (hardware and software) and their capabilities, Cynthia employs a multi-layer volume control system which adjusts both hardware and midi note volumes.
To adjust playback volume, position the slider to the right to increase the volume and to the left to decrease it.
An increase above 100% boosts the midi volume, making low-level, hard to hear midis, more discernable.⏶Adjust playback position with a single click or tap.  Additionally, hovering a mouse cursor over the Playback Progress bar displays a vertical marker for the new playback time and position.  Click to apply new position.  The midi playback will shift to a new position and automatic fade in eases back playback volume to avoid sudden clicks, pops or abrupt notes.
Use keyboard arrow keys to progressively move forward or backward through the midi.  A vertical marker will display.
Not currently playing?  Click in the Playback Progress bar to commence playback at that position.⏶Synchronised lyric display is supported for midis with lyrics.
To enable lyrics, from the main toolbar click the " Menu" link and tick "Show Lyrics".
When included within a midi, lyrics are displayed inside the Playback Progress bar (bottom of Cynthia) as "Playback Progress - Lyrics:" with several words or part words visible at any one time.
A midi without lyrics will display "Playback Progress".
If hyphenated lyrics are required in order to highlight the pauses between part-words, from main toolbar click the " Menu" link and tick "Hyphenate Lyrics".⏶Sometimes there can be a short, noticeable playback delay when commencing playback, initially.  This delay is preparation time for Cynthia to ready playback.
There is no delay switching between midis during playback as Cynthia remains connected to the midi device.  By default, after a short period of no playback (5 seconds or more), the midi device will switch to offline, and a short delay will occur when playback is next started.
To avoid this delay, the "Always on Midi" option may be used to keep a midi device online, even when Cynthia is not playing.  From the main toolbar click the " Menu" option and tick the "Always on Midi" option.:
You can always tell if the midi device is online or not - from the "Midi Information" panel on the right.  Look for the item called "Device" in the "Technical list".  This will either be "Online" or "Offline".⏶Midis comes in various format types.  The simplest is format 0, a single track format, that stores all it's tempo (speed), notes and commands on a single, mixed track.
A format 1 midi, on the other hand, uses a dedicated master track (first track) to store all of it's tempo (speed) commands for the entire midi.  Notes and commands are stored separately on additional, multiple tracks.
Cynthia supports both format 0 and format 1 midis with file extension ".mid", ".midi" and ".rmi".
A third, format 2 midi exists.  This format type is not supported by Cynthia.  In addition, Cynthia does not support system exclusive messages.  These messages will be ignored, and typically relate to manufacturer specific equipment.⏶Cynthia must be setup to use an Xbox Controller.  From the top toolbar select "Menu > Xbox Controller" and select the "Active Only" or "Active and Inactive" option.  Pair an Xbox Controller to your computer (if not already done).  Cynthia automatically detects and uses active Xbox Controllers.  More than one controller can be used at the same time.1. Left joystick - left/right to adjust volume, push down to toggle between Play Folder and Play List modes
2. D-pad - left/right to switch midi playback device, up/down to switch between songs in navigation panel
3. View button - Go to beginning of song
4. Share button - Not used
5. Menu button - Toggle through playback modes: Once, Repeat One, Repeat All, All Once, and Random
6. Right joystick - left/right to change song playback position, push down to toggle full screen mode
7. Left bumper (top) - toggle display of navigation and piano panels
8. Right bumper (top) - toggle display of midi information and tracks, channels, and notes combination panels
9. Left trigger (bottom) - reduce playback speed
10. Right trigger (bottom) - increase playback speed
11. X button - reset playback speed to 100%
11. Y button - reset volume to 100%
11. B button - start/stop playback
11. A button - select folder in navigation panel (when in Play Folder mode)⏶From top toolbar click " Options" or app menu "... > Options" (top right of window).  An "Options" window will display.  Select " Color" tab to show list of color schemes.
The "Color Schemes" list is split into three sections:
1. Built-In color schemes - read only/use as is
2. Custom color schemes - user customisable and labelled Custom 1-10
3. Saved color schemes - user customisable and saved as file(s) on disk
There are 160+ built-in color schemes to choose from.  Simply select a color scheme to apply in realtime.  For instance, Aqua Marine.  Watch as the app's GUI changes color instantly - no need for an additional load, apply, or set.
A Blaiz Enterprises' color scheme (*.bcs) at its core is a list of twenty colors, responsible for coloring most aspects of the app's GUI.  Some specialised colors are derived automatically from these.  Two colors for the frame, nine for important areas, Title colors, and nine more for common zones, Standard colors.
Each built-in color scheme has it's own unique set of colors.  A custom color scheme allows for colors to be customised.  To create a custom color scheme, scroll down the list to the section titled "Custom".  He you'll find ten custom color scheme slots - each fully customisable in realtime without any need to be named, saved or applied.  Tap or click on a slot to start - for example slot 1 - "Custom 1".
On the right a series of editable color palettes will appear.  Click a palette to display the color dialog window.  Adjust color as desired and click OK when done.  Alternatively, click and drag your mouse cursor/fingertip from the color palette to acquire color from your computer screen in realtime.  App GUI continuously updates to reflect changes in color.  All changes are automatically saved.⏶Want your shiny new color scheme to have its own name?  Easy.  From the color schemes list - click "Options > Color" to display dialog - scroll down to the "Custom" section, select your custom color scheme, and click " Menu >  Save As...".  Type a name and click the "Save" button.  Your color scheme is saved to disk and listed under the "Saved" section of the color schemes list - next section down.
Any color scheme can be saved to disk, and then edited.  For instance, you can select one of the built-in color schemes, such as Aqua Marine, and save it to disk, then customise as desired.⏶Click "Options > Color" to show the color schemes list.  Scroll down to the last section named "Saved".  This section presents a list of all your saved color schemes in one central location.  Select a scheme to use.⏶Yes.  Any saved color scheme can be customised without fuss.  Click "Options > Color" and scroll down to the section named "Saved", click the color scheme you wish to edit, and adjust the color(s) as desired.  All changes are saved automatically back to the file on disk, without any need to explicitly save.⏶A background scheme is a static or animated image tiled across the background layer of the app.  The app's GUI components sit above this layer/merge into it, such as toolbars, tool images, buttons, text etc.  There are 60 built-in background schemes, based on several images with different presets, like horizontal and vertical scroll speeds, fade in and out rates, and wobble levels.  These functions allow for a static image to give movement to the app.  While some background schemes are specially set for animation, others are not.
The background image can be rendered in full color, or shade-shifted toward greyscale, or shade-shifted toward the app's current highlight color.  One slider, Colorise, controls this tri-function.
A background scheme supports a maximum image color depth of 32 bits in RGBA format - 8 bit red, green, blue, and alpha channels - for instance a transparent PNG image.:
An animated background scheme can operate at frame rates of up to 20 fps (frames per second), which means the entire GUI of the app is repainted in full, 20 times per second, like a video, and therefore can consume quite a bit of CPU power, especially at high resolutions.  It is recommended a modern, powerful machine be used for high frame rates/resolutions in order to maintain smooth operation of the GUI.:
Determines how much of the background image is seen/made visible.  A low value renders the background subtly beneath the GUI, whereas a high value, 100-255, renders it boldly.  A value above 100 is disabled by default.  To enable, click "Options > Settings" and deselect "Display > Safe Background".  A value over 100 may overpower the GUI making it hard to navigate or operate.  If this becomes the case, press the "F2" key, and then the Enter key to confirm restoration of the app's default settings.
Set the color rendering method.  A value of 100 renders the background image in full color, a value of 0 in greyscale, and a value of -100 in the app's current highlight color.
Paint speed in frames per second.  0 is static - the background scheme only repaints when required.  This is the least stressful option.  A value of 1-20 sets a constant repaint cycle of 1-20 fps (frames per second).
Horizontal Scroll/Vertical Scroll (-100..100):
Moves background image left/up (minus value) or right/down (plus value) by X pixels.  A value of zero turns movement off.
Horizontal Wobble/Vertical Wobble (0..300):
Applies a wobble factor to the above Horizontal Scroll/Vertical Scroll movement(s).
Cycles through a high-to-low-to-high intensity flow, gradually fading the background image from view, then back again, in a continuous cycle.  Use a low value for a slow cycle, and a high value for a fast cycle.  
Applies a wobble factor to the Fade In/Out flow cycle above.⏶Yes you can.  There are 10 custom background scheme slots.  Click "Options > Background" and scroll down to the section named "Custom".  For instance, click on "Custom 1".  A sub-toolbar will display in the right column at the top.  From there, you can paste in an image from Clipboard - click "Paste", or open an image from file - click "File".
For best visual results, your image should be a similar size to the app's overall area, and be prepped for tiling work - that is, have it's right and bottom edges modified so that its colors/pixels seamlessly wrap back round to its opposing edge (right-to-left and top-to-bottom).  You can use a tile creation app, or a good quality graphics app to accomplish this, or use our "Blend It" app to prep your image.
Without this, tiling the image may present abrupt edges of unwanted opposing lines of horizontal/vertical colors, and be visually jarring in nature.
Adjust the sliders as required to accomplish animation and visual effects.  All changes to the image and sliders are saved in realtime.⏶The frame on our apps have a long history reaching back to the late 1990s, where they first adorned our FastCards and PowerCards (still/animated electronic musical greeting cards).
In the context of an app, they primarily serve as a large, easy-grip area for resizing the app's overall size, and a touch of decoration to boot.
A frame can be made wide or narrow as required.  The modern app has a typical frame width of 7 px in a plain style, so as not to distract or occupy excessive screen real estate.
Furthermore, a frame may render with an optional, randomised sparkle effect, with a range of 0 (no sparkle) to 20 (heavy sparkle).
Click "Options > Frame" to edit the app's current frame settings.  A list of 50 built-in frames is available to choose from, ranging from Antique to Traditional 5.
Toward the bottom of the window are two sliders to adjust the frame's Sparkle (strength) and Size (width in pixels).  A frame can be sized from 0 px (no frame) up to a wide frame of 72 px.  All changes update in realtime.⏶Click "Options > Font" to display zoom and font specific settings.  By default, the app is set to automatic zoom, which means it will scale up its text and images if the monitor it's displayed on is above 2K in resolution.Why scale text and images at all?  What is special about 4K and 8K monitors?  At first glance it may not be obvious the significant difference between standard 2K resolution, and the much higher 4K and 8K resolutions.
But high resolution monitors, such as 4K and 8K displays have far more pixels (colored dots) per inch on screen than previous generations of monitors, that is, the screen size may be the same but more dots are packed into the same area.  Consequently, an app without scaling abilities may appear small or even blurred on these monitors.  That's because as new monitors and TVs gain ever greater resolutions, statically sized apps shrink in size/fail to compensate.  This is why a modern app must be able to scale up to match the appropriate resolution.
Here is a comparison of common display resolutions:
2K = 1920w x 1080h =  2,073,600 pixels
4K = 3840w x 2160h =  8,294,400 pixels
8K = 7680w x 4320h = 33,177,600 pixels
A 4K (ultra high definition) monitor uses four times (4x) more pixels than it's 2K (full high definition) counterpart.  A statically built app without scaling would shrink to 50% of it's size, making it difficult to use.  An operating system may attempt to counter this by scaling it up using pixel stretching - very much like zooming up a photo - unfortunately this tends to blur the appearance of the app.
The same app on an 8K monitor would suffer even greater shrinkage, rendering at only a quarter (25%) of it's original size, or scaling with significant blurring.
This app has a built-in zoom mode, which multiples the width and height of its text and images by a factor of 2, 3, or 4, dependant on the resolution of the display.  This does away for the need of the operating system to stretch/scale its pixels.
On a 2K monitor there is no need to scale as this is the app's intended resolution.  On a 4K monitor the app switches to a zoom factor of 200% (2x), upscaling text and images and the main window's dimension accordingly, and 400% (4x) on an 8K monitor.  The end result is an app that appears appropriately sized over different monitor resolutions.
You can override this automatic zoom function, and set your own zoom value of: 100%, 200%, 300%, or 400%, if desired.  An option that may be useful on a custom display resolution, and/or multiple monitor display environment.:
Setting the zoom value to 300% or above on a 2K monitor may render the app so large as to make it unusable.  If this occurs, you can press the "F2" key at anytime to restore the app's default settings.⏶The text size (font size) can be set between 6 and 24.  Click "Options > Font" and choose a size option.  Any change updates the app text in realtime.
In some instances, the app may display slightly larger or smaller text in special areas, however this text is directly scaled from the size set.
By default the app uses a size of 10.:
Not all sizes are supported by all fonts.  On Ubuntu for instance, a large font size for Arial can cause text to appear weird or slightly off.  If this occurs, try reducing the font size a tad, or alternatively select another font (font name).⏶A font determines what sort of characters appear on the screen, and in what style.  Click "Options > Font" and choose a font name (font) from the list of options, from Arial to DejaVu Serif.  Any change updates the app in realtime.
Older fonts like "System" were simple formats constructed from small images or bitmaps, one per character, and did not scale up very well, partly because to save on memory, not all character sizes were stored inside the font.  Today, modern fonts use mathematical vectoring to draw shapes and lines etc to construct characters on the screen, though the infrastructure and overhead required for such fonts can be complex and heavy on a computer system.  But because these fonts employ mathematics to draw their characters, they do scale up well.
Eleven common font name options are presented for best compatibility and visual appearance, taking into consideration operating systems as old as Window 95, through to modern-day Windows 11, and other systems such as Mac and Linux.
In general, you want a font that is widely supported and guaranteed to render well on most computers.  Bearing that in mind, Arial, is a good choice, as it is widely supported by operating systems going as far back as Windows 95, if not further.
If a font name is selected but is not supported by the current computer, for example "DejaVu Sans" - not supported by Windows 95 - a close approximate/fallback font is used instead.
For a specific/custom font, click the "Custom" option once to switch it on, and click it again to display a Font dialog.  Choose the font name from the list of fonts and click the "OK" button when done.
If the app becomes unreadable, or hard to read after choosing a new font name - it can happen with a bad font, a foreign language font, or a symbols only font like "Wingdings" - press the "F2" key to restore the app's default settings.⏶For maximum font compatibility, two completely separate methods have been employed to render text with antialiasing.
Click "Options > Font" for font settings.
1. The first method, Font Feathering, is a simple feather designed specifically to outline each text character in realtime.  This has the effect of gently softening the often harsh outline of text characters on LCD screens, which by their nature do not blur neighbouring pixels, as the older CRT (Cathode Ray Tubes) did.
The feather is universally applied to both bitmap fonts (older image based fonts) and vectors fonts (newer mathematical fonts).  In this way it allows for a quick and direct text feathering technique, that is easily adjusted to suit, without any need for any complicated or tricky multi-step setup configurations and/or processes.
As a bonus, it works on older operating systems - e.g. Windows 95 - which back in the day had no need/support for it, as LCD monitors were not widely used.  It also works on fonts that don't have any embedded feather information, or for smaller font sizes that at times can abruptly discontinue feather support.
This method generates an even, edge-based blurring of the outermost pixels of the text characters.  A high value - high or ultra - renders a strong/bold feather, and a low value reduces this to a more subtle display.
Any change updates in realtime.
On a high quality computer monitor were all pixels are transmitted and displayed without any color loss, a "Low" value is typically sufficient to render text gently on the screen.  But TVs, which tend to heavily compress their video streams for speed loose some color information and therefore may require a higher setting of "Medium" or "High" for similar results.
2. The second method, Font Specific Antialiasing, relies on the font itself to provide all feather based information.  This is usually an 8 bit greyscale range of 0-255.  The downside is, if the font has no feather support, or the operating system is old, e.g. Windows 95, then no antialiasing will appear.  This method can sometimes be hit and miss.
For instance, take the font Arial, it is universally supported, but surprisingly looses feather support at 12 pt or less, leaving only sharp text characters to be rendered on screen.
To adjust the antialiasing strength, select an option from "Dark" (strong) to "Light" (subtle).
By default, options 1 and 2 above are set to "Low/Dark", which provides the best fit for a wide variety of fonts and their behavior over a broad spectrum of old and new operating systems.⏶The app can be set to start up in various ways: normally, minimised, maximised or in fullscreen mode.  To adjust, click "Options > Settings" and set an option under "Start Style".
Starts the app as a window, which is the default mode.
Starts the app hidden from view as a button on your taskbar.
Starts the app as a window, maximised to fill all the available work area on your desktop.
Starts the app in fullscreen mode, blocking out everything else on your desktop.:
If the app has the "Multi-Monitor" option selected (next section down named "Display"), then modes "Maximised" and "Fullscreen" will render the app over the combined screen real estate of all monitors.⏶The app can create, maintain and remove links for you.
There are three link types supported:
1. Start button
3. Automatic Startup
Click "Options > Settings" to adjust.
1. The first link type, Start button, creates and automatically maintains a link - also known as a shortcut - on your Start Menu called "Cynthia by BlaizEnterprises.com".  As this link is created by and maintained by the app, you must unselect the option to remove the link from your Start Menu.
2. The Desktop link operates identically to the above, maintaining a link on your Desktop named "Cynthia by BlaizEnterprises.com".  It also must be unselected to remove the link permanently from your Desktop.:
As long as either options 1 or 2 above is selected, then the corresponding links are maintained and automatically re-created if need be by the app, even if they're manually deleted from outside the app.
By default, neither option is selected.  Optionally, you can create your own manual link/shortcut to the app using Windows with any name/label you wish.
3. The last option, Automatic Startup, creates a link to the app in the startup location of your computer, informing Windows to launch the app when your computer boots/starts up.  Again, this link is automatically maintained, therefore unselect the option to remove it.:
If any of the links above are selected and you plan to remove/delete the app from your computer, it is highly recommended you first unselect all the options above (1-3), then remove/delete the app.  Otherwise, Windows can get a little weird and put the links back in some cases without any interaction from the app itself, even if the app is no longer present.  This behaviour was observed with earlier versions of Windows 10.⏶The majority of the app's important system settings can be found in one location - click "Options > Settings".
An option is considered "on/enabled/selected" when lit, and "off/disabled/unselected" when unlit.Round Corners:
Render all visual controls, windows, and dialogs with round (curved) corners.
Automatically close an active dialog window when a click/tap strikes outside the window area - e.g. Save, Open, Font, Options dialogs.  This can speed up typical workflows by skipping the need to specifically click the OK or Cancel buttons to close the dialog.  For instance, click "Options" to display the options dialog, change a few settings and click/tap outside the window to close it and save changes.  Also convenient to cancel a dialog displayed by mistake/change of mind, such as a Save dialog.
Retains app window on screen at all times, and any sub windows or dialogs within the app.  Any attempt to drag the app out-of-range triggers an automatic position correction - a passive system that continuously checks the window position and monitor size.  Supports both single and multi-monitor modes.
Displays an informative/artistic splash screen on app startup.  Unselect to disable.
Scrolls control-centric help across the top of the current window, dialog, or menu.  Hover mouse cursor over / tap finger on a control for related help.
Hover mouse over / tap finger on a control to display help related information in a popup bubble
Comfortably enlarge controls and menus for touch access (finger taps)
Some options work best with a double click / tap for confirmation.  This option supports the traditional double click mode.  For example, navigating a disk drive using a double click / tap to switch between folders.
Set the app above all other apps and windows
Normal app operation can use a lot of paint cycles and CPU power, especially if it's rendering graphics and effects continuously on the screen.  Economy mode throttles back this usage during periods of extended idleness, e.g. when there is no direct app keyboard input, or indirect mouse cursor movement or finger taps.  For more specific information refer to the topic "Economy mode".
Not noticeable on today's powerful computers with their 32 bit monitors and video cards, it however can deliver a small performance improvement on older computers running 24 bit graphics
Show the app's frame whilst maximised.  The frame is always hidden whilst in fullscreen mode.
An optional static or animated background scheme can be set, which renders an image beneath the GUI.  If this image is too bold / strong the GUI can be hard to view / use.  By default this option limits the background strength to a safe maximum level of 100.  Unselect this option to permit the full range of background strength (0-255).  If the GUI becomes hard to use or unreadable, press the "F2" key at anytime to restore the app's default settings.
Permit the app to span the full range of attached monitors when maximised or in fullscreen mode.  By default the app spans the current monitor only.
Position the name of the app in the center of the app's window header
Align the content of all participating toolbars to the left, center, or to the right
Color coordinate important system settings and options into color specific input panels for rapid visual identification
Use high-contrast, color-adaptive, monochromatic tool images on the app's GUI
Retain highlight areas and other important GUI zones whilst a background scheme is in use
Adjust the brightness of the entire app, from 60 being the darkest right up to 130 being the brightest.  Any change takes affect immediately.  The default brightness is 100.
Renders the app translucent upon loosing focus - e.g. when another app is in use.  Available range is 30 almost invisible, through to 255 fully visible.  Default value is 255.  This feature requires support of a modern operating system and is therefore not supported by Windows 95/98 etc.
The speed by which to transition the app from a focused state to a non-focused state or vice-versa.  Speed range is 1-10, where 1 is the slowest and 10 the fastest.
Renders the app translucent upon gaining focus - e.g. when the user interacts with it.  Available range is 50 almost invisible, through to 255 fully visible.  Default value is 255.  As above, this feature requires support of a modern operating system and is therefore not supported by Windows 95/98 etc.
Change the default cursor to one of the built-in cursors, each of which scale from small to large, according to the current size set by the operating system.  A range of static colors are available: Red, Orange, Pink, Yellow, Purple, Aqua, Blue, Green, Grey, Black, White, along with Default and Custom.
In addition, two dynamically colored cursors are included: "Adaptive - Hover" and "Adaptive - Title".  These special cursors acquire their color from the current color scheme.  Any change to the color scheme is reflected in the color of the cursor.
The custom cursor option supports both static cursors ".cur" and animated cursor ".ani" file formats.  To use, select the "Custom" option.  The cursor will update if previously customised.  To change the cursor, click the option again and select a cursor from file using the Open dialog.
Applies a random texture to the app's frame.  Select a value from 0 (off) to 20 (bold).
Adjust the app's frame size (width in pixels) from 0 (none) to 72 (wide).  The default frame size is typically 7.
Set the width and/or height of the app's scrollbars.  A value of 5 (thin/short) to 72 (wide/tall) is supported.
Wine is basically a large computer instruction conversion app, which allows an app designed for Microsoft Windows to execute its instructions "run" on another operating system, such as a Mac or Linux.  Because Wine does not emulate / bridge any logic gaps and only translates an app's instructions, the underlying computer hardware must match that used by Microsoft Windows - namely Intel and AMD64.
A notable exception to this requirement is the modern-day Mac, e.g. Mac Mini, which runs an Intel emulator under the hood.  This allows a Microsoft Windows app to run by the following sequence: App to Wine to Intel emulator to Apple hardware.  Incredibly, this appears to be a very effective and rather efficient strategy, especially for our lightweight apps.
Although Wine's functionality is both wide and impressive, there is some functionality that falls short of Windows.  The Wine Compatibility option compensates where possible for important shortcomings, such as volume handling, and allows the app to maintain functionality.
The default option is automatic and detects the presence of Wine based on the existence of drive "Z:\".  For more information on Wine refer to their website www.winehq.org .  
Easily reset the app's system settings, such as color, font size, zoom level etc to their defaults.  Click the "Restore Defaults..." button at the bottom-right of the Options window or press the "F2" key at any time in the app to display the "Restore Defaults" confirmation prompt.  Confirm your intention to reset and then click the "Restore Defaults" button.  The app will reset to default values.
An app typically has settings in addition to these which are not restored / reset.  Instead, they should be adjusted via the app itself as required.⏶Click the app menu button  (top right) and tick "On Top" option.  Alternatively, click "Options > Settings" and select "Display > On Top" option.⏶By default the splash screen is displayed with a momentarily pause on startup.  This can be switched off by going to "Options > Settings" and deselecting the "Display > Show Splash" option.⏶Because this app is portable, you might not remember / know where it is located on your hard disk or usb pen stick.  To access its folder, click the app menu button  (top right) and select " Show App Folder".  An explorer window will display with the app's binary (*.exe) and storage folder listed alongside.⏶Running an app at full speed when it's not being used can be a bit wasteful, and may prematurely drain the batteries on your laptop or tablet.  Select this option to automatically throttle back battery / power consumption and CPU / graphic loads after a short idle period of 10 minutes.  At which point the app will reduce its paint cycles down to 2 fps at a maximum.  And a further reduction at 30 minutes to 1 fps.
Internal processing loads will typically be reduced also, lowering the demand on your CPU and batteries further.
A single stroke of the keyboard directed at the app, or a global mouse click, or tap of the finger will instantly disengage the current economy state and return the app back to full operation.
To enable, click "Options > Settings" and select "Display > Economy" option.⏶The Gossamer Code Foundation - our 4th generation codebase - which powers this app has been engineered with care and patience to a high level of quality and reliability.
As our code is rather unique and almost entirely custom built, there are some technical limitations which make our apps incompatible with some extended features of modern operating systems.
These limitations mainly concern the use of UTF-8 and UTF-16 encoding of text, and more specifically filenames.  At this stage the app works with the legacy Windows-1252 character encoding for both text processing and filenames.  The app is therefore unable to handle foreign language text, or load and save files with special, foreign, or emoji characters in their filenames.  All text and filenames are restricted to english ASCII characters in the Windows-1252 encoding standard.
In addition, some options and minor operations may not work as expected, or at all on operating systems other than Microsoft Windows.  Though an enormous amount of time and effort has gone into harmonising the look and feel, behaviour and reliability of the app across multiple flavours of Microsoft Windows, Linux, and Mac operating systems, it is not always possible to catch every failure point, or in some rare cases make it work properly, though we always endeavor to do our best.
A side note, our codebase is still running well as 32 bit code in 2025.  Yes, 32 bit!  Some might see this as a limitation, but we see it as a flexible, inexpensive, and widely adopted execution pathway with support for many platforms and reuse / life extension of older equipment.⏶A portable app is a big leap forward for apps in general.  A standard or traditionally engineered app requires a lot of support in the form of libraries, data files, images, scripts, etc and the list goes on.  You get the picture.  Some portable apps out there still include this bundle of bits, they merely offload it into a local folder.  A dump of goodies of sorts.
We tend to see a portable app in quite a different light.  Our vision of a portable app is designed tight, clean, free of bloat, and all data where possible is included directly within, structured right into the fabric of the app itself, and designed from the bare-metal up if required.
Though the most important difference between a traditional app and a portable app is that a portable app will not install on your computer.  This is extremely important as the installation process is often messy, and can clutter up your computer by dumping a lot of stuff all over the Windows file structure and registry, and over time may slow down and decrease the overall performance of your computer.
A portable app will not do this, which keeps your computer clean and running smooth and fast as it should.  Unfortunately most software is not designed with portable in mind.  They're more akin to a large leaky box of bits than tight engineering.  And because a portable app is not installed on your computer, it runs outside the normal scope of the operating system, and is not locked down or tied to it.  And thus can be moved about, from disk to disk, or computer to computer.
Typically a portable app will reside on a USB pen stick, removable media, or in a special folder on a portable hard disk.  This makes it easy to take from one computer to the next, and use over and over.  An immensely valuable freedom, and something an installed app can only dream of.
But a serious technical hurdle must be overcome for a truly portable app to be free.  And that is the humble setting.  Yes, a portable app must be able to handle it's settings on it's own.  It must be able to read them from disk, filter them, check and correct them where required, and write them back to disk.  All without the help of the Windows' registry or other operating system dependent structures.
An installed app typically can't or won't do this.  Instead, it relies on Windows and the registry to manage it's settings and other important data sets for it.  It therefore takes a higher degree of technical competence to escape this "tied to the operating system" situation.
Here is our current standard for a portable app: Require no installation or setup Require no additional DLL libraries to run and perform it's core function Make no alteration to the host operating system, or its settings, files, libraries or core functions, or the Windows registry, unless it forms a part or a whole of the app's core function, and then, only when initiated by the user Be able to run "out of the box" Require no compiling, conversion, installation or setup of support structures in order to execute, except for when such an environment constitutes an execution enabling environment, such as a command translation service like Wine Be free of zipped or otherwise externally bundled blob structures containing folders, files and bits Operate on less powerful hardware to facilitate operation on a broad spectrum of computers Less demanding API landscape to facilitate execution on a broad range of operating systems and software translation environments Require no special software libraries be present in order to run, such as .NET or JAVA, unless typically pre-installed on the target operating system or execution environment Not require an internet connection to run, unless the connection is required to fulfill the app's core function, such as a web server Require no downloads, addons, or registration in order to run Provide help and documentation offline via a built-in viewer, or by limited external means, such as Notepad Be self-contained with all necessary files, data sets, and samples stored within it's internal structure, such that access be provided preferably through direct enabling mechanisms Store, hold and manage external app settings and user data in a local sub-folder, and that sub-folder be easily identifiable as belonging to the app Provide a mostly consistent appearance and experience to the user across the widest possible range of operating systems and execution environments Value backwards compatibility, and be able to actively make use of that older hardware and software Possess a compact, bloat-free footprint⏶Make sure any app related data that is precious to you is backed up before you delete.As a portable app does not install itself on your computer there will be no automatic uninstall option listed in Windows.  The app must be removed manually.  But this is not difficult.
First, ensure the options below are unselected before proceeding.  Click "Options > Settings" and deselect:
1. Start button link
3. Automatic Startup link
If these links are not removed they may linger due to the oddities of some versions of Windows and it's often complex nature and protocols.
If this app is administered by a 3rd party system then that system should be used now to remove this app.  If not, then click the app menu button "" (top right) and select " Show App Folder".  An explorer window will display with the app's executable (*.exe) and storage folder listed.
Make sure any data precious to you has been backed up or moved out of the app's storage folder before proceeding.  When you're ready, close the app and right click on it's EXE "<app name>.exe" and select the Delete option.  If a prompt appears, confirm your intention to delete.  Repeat for the storage folder.
The app is now removed from your computer, USB pen stick, or hard disk.⏶If for some reason your app doesn't appear right, or you think you've turned on or off some important system setting but you're not sure which one or where, not too worry, you can restore the app's default settings in two easy steps.
From anywhere in the app press the "F2" key to display the "Restore Defaults" confirmation window.
When you're sure you're ready to proceed, click the "Restore Defaults" button.  The app will reset, restoring all key system settings to their safe defaults, this includes color, font size, zoom level etc.
If you don't have a keyboard, or the "F2" key is not available / difficult to access, you can click the "Options" link from the top toolbar to display the options window, then click the "Restore Defaults..." button (bottom right of window), and lastly confirm by pressing the "Restore Defaults" button.  The app will reset / restore defaults.⏶Copyright 2025 Blaiz Enterprises ( www.blaizenterprises.com )
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.⏶]]></content:encoded></item><item><title>Bureau of Meteorology&apos;s new boss asked to examine $96M bill for website redesign</title><link>https://www.abc.net.au/news/2025-11-23/bureau-of-meteorology-new-website-cost-blowout-to-96-million/106042202</link><author>OuterVale</author><category>hn</category><pubDate>Mon, 24 Nov 2025 12:35:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The Bureau of Meteorology's (BOM) flawed and expensive redesigned website will come under renewed scrutiny, with the federal environment minister asking the agency's new boss to closely examine how it all went so wrong, and report back to him.It comes amid revelations that the new website cost more than $96 million to design — a far cry from the $4 million figure it originally claimed had been spent.Users found it difficult to navigate, and also criticised the changes to the radar map, which made place names hard to read. BOM users, farmers in particular, were critical of the changes made to the radar map.()Farmers were scathing, as they were unable to locate rainfall data.The federal government was forced to intervene, ordering the agency to fix the website. The site has since reverted to the old version of the radar map and other tweaks have been made to the site, with further changes to be rolled out.In a statement provided to the ABC, the BOM admitted "the total cost of the website is approximately $96.5 million".'Complete rebuild necessary' It said the cost breakdown included $4.1 million for the redesign, $79.8 million for the website build, and the site's launch and security testing cost $12.6 million."A complete rebuild was necessary to ensure the website meets modern security, usability and accessibility requirements for the millions of Australians who reply on it every day," a spokesperson said.The spokesperson also said it had "continued to listen to and analyse community feedback" since the launch of the new website on October 22.The BOM says it continues to listen to and analyse community feedback.Nine days after the launch it changed the radar map back to what it had previously been. "This brought back the visual style that the community said they found intuitive and reliable for interpreting weather conditions,""This option was already available on the new site but not as the default setting when visiting the page."On 7 November we implemented changes to help the community find important fire behaviour index information."Future changes were also in the pipeline in response to community feedback, according to the spokesperson, but some updates had been paused due to Severe Tropical Cyclone Fina in northern Australia.Minister's expectations 'have been made very clear'Environment Minister Murray Watt said he had met twice in the past week with the new CEO Stuart Minchin to reiterate his concerns about the bungled process and the cost.The environment minister says he has met twice with the BOM's new boss.He has asked Mr Minchin to report back to him on the issue."I don't think it's secret that I haven't been happy with the way the BOM has handled the transition to the new website," he told reporters on Sunday."I met with him on his first day and during the week just gone, to outline again that I think the BOM hasn't met public expectations, both in terms of the performance of the website and the cost of the website."So I've asked him as his first priority to make sure that he can get on top of the issues with the website — the functionality — and I'm pleased to see they've made changes."But I've also asked him to get on top of how we got to this position with this cost, with the problems."He's only been in the job for a week but I think my expectations have been made very clear."The minister has asked new BOM boss, Stuart Minchin, to prioritise the issues with the website.However the minister stopped short of describing the website as a sheer waste of money, saying he would wait to hear back from Mr Minchin before commenting."Before leaping to judgement, I want to see what the new CEO of the BOM has been able to establish as to the reasons for those cost increases and I'll make my judgement at that point in time."Nationals leader David Littleproud said there should be "consequences" after the revelations about the true cost of the website."It is unbelievable a private consultancy was paid $78 million to redesign the website," Mr Littleproud said."But then security and system testing meant that Australian taxpayers actually paid $96 million for what was nothing more than another Labor disaster,."The seriousness of this cannot be understated. This isn't just about a clunky website, the changes actually put lives and safety at risk."The new platform did not allow people to enter GPS coordinates for their specific property locations, restricting searches to towns or postcodes."Families and farmers could not access vital, localised data such as river heights and rainfall information and this missing data created panic and fear across communities."But now, the fact the BOM has been hiding the true cost of its white elephant and initially lying about the total figure is deeply concerning, considering that the BOM should be all about trust."]]></content:encoded></item><item><title>Chrome Jpegxl Issue Reopened</title><link>https://issues.chromium.org/issues/40168998</link><author>markdog12</author><category>hn</category><pubDate>Mon, 24 Nov 2025 12:23:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>NSA and IETF, part 3: Dodging the issues at hand</title><link>https://blog.cr.yp.to/20251123-dodging.html</link><author>upofadown</author><category>hn</category><pubDate>Mon, 24 Nov 2025 12:00:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[2025.11.23: NSA and IETF, part 3: Normal practice
in deploying post-quantum cryptography is to deploy ECC+PQ.
IETF's TLS working group is standardizing ECC+PQ.
But IETF management is also non-consensually ramming a particular NSA-driven document
through the IETF process,
a "non-hybrid" document that adds  as another TLS option.Don't worry: we're standardizing cars with seatbelts.
Also, recognizing generous funding from the National Morgue Association,
we're going to standardize cars  seatbelts as another option,
ignoring the safety objections.
That's okay, right?Last month I posted
part 1
of this story.
Today's
part 2
highlighted the corruption.
This blog post, part 3,
highlights the dodging in a particular posting at the beginning of this month
by an IETF "security area director".
Part 4
will give an example of how
dissent on this topic has been censored.Consensus means whatever the people in power want to do.
Recall from my previous blog post that
"adoption" of a document is a preliminary step
before an IETF "working group" works on, and decides whether to standardize, the document.
In April 2025,
the chairs of the IETF TLS WG called for "adoption" of this NSA-driven document.
During the call period,
20 people expressed unequivocal support for adoption,
2 people expressed conditional support for adoption,
and 7 people expressed unequivocal opposition to adoption.
(Details for verification.)Before the chairs could even reply,
an "area director"
interrupted,
claiming, inter alia, the following:
"There is clearly consensus
based on the 67 responses to the adoption call. ...
The vast majority was in favour of adoption ...
There were a few dissenting opinions".After these lies by the "area director" were
debunked,
the chairs said that they had declared consensus
"because there is clearly sufficient interest to work on this draft"
specifically "enough people willing to review the draft".I can understand not everybody being familiar with
the specific definition of "consensus" that
antitrust law
requires standards-development organizations to follow.
But it's astonishing to see chairs substituting a consensus-evaluation procedure
that simply ignores objections.
The chairs said I could escalate.
IETF procedures say that an unresolved dispute can be brought
"to the attention of the Area Director(s) for the area in which the Working Group is chartered",
and then "The Area Director(s) shall attempt to resolve the dispute".I filed a complaint with the "security area directors"
in early June 2025.
One of them never replied.
The other,
the same one who had claimed that there was "clearly consensus",
sent a series of excuses
for not handling the complaint.
For example, one excuse was that the PDF format "discourages participation".Do IETF procedures say "The Area Director(s) shall attempt to resolve the dispute
unless the dispute is documented in a PDF"?
No.I sent email two days later
systematically addressing the excuses.
The "area director" never replied.It isn't clear under IETF procedures whether a non-reply allows an appeal.
It is, however, clear that an appeal  be filed after two months.
I escalated to the "Internet Engineering Steering Group" (IESG)
in August 2025.(These aren't even marginally independent groups.
The "area directors" are the IESG members.
IESG appoints the WG chairs.)IESG didn't reply until October 2025.
It rejected one of the "Area Director" excuses for having ignored my complaint,
but endorsed another excuse.
I promptly filed a
revised complaint
with the "area director",
jumping through the hoops that IESG had set.
There were then further
runarounds.
Suddenly,
on 1 November 2025,
IESG publicly instructed the "area director" to address the following question:
"Was rough consensus to adopt draft-connolly-tls-mlkem-key-agreement in the
TLS Working Group appropriately called by the WG chairs?"The "area director" posted his conclusion mere hours later:
"I agree with the TLS WG Chairs that the Adoption Call result
was that there was rough consensus to adopt the document".Dodging procedural objections.
Before looking at how the "area director" argued for this conclusion,
I'd like to emphasize three things that the "area director"  do.First,
did the "area director" address my complaint
about the chair action on this topic?
No.One reason this matters is that
the law requires standards-development organizations to provide an
"appeals process".
Structurally,
the "area director" isn't quoting and answering the points in my complaint;
the "area director" puts the entire burden on the reader
to try to figure out what's supposedly answering what,
and to realize that many points remain unanswered.Second,
did the "area director" address the chairs claiming that
"we have consensus to adopt this draft"?
Or the previous claim from the "area director"
that there was "clearly consensus"?
No.
Instead IESG and this "area director"
quietly shifted from "consensus" to "rough consensus".
(Did you notice this shift when I quoted IESG's "rough consensus" instruction?)One reason this matters is that
"consensus"
is another of the legal requirements for standards-development organizations.
The law doesn't allow "rough consensus".
Also,
IETF claims that
"decision-making requires achieving broad consensus".
"broad consensus" is even stronger than "consensus",
since it's saying that there's consensus .Third,
the way that my complaint had established the
lack of consensus
was, first, by reviewing the general definition of "consensus"
(which I paraphrased from the definition in the law, omitting a citation only because
the TLS chairs had threatened me with a
list ban
if I mentioned the law again),
and then applying the components of that definition to the situation at hand.
Did the area director follow this structure?
Here's the definition of "consensus", or "rough consensus" if we're switching to that,
and now let's apply that definition?
No.
Nobody reading this message from the "area director" can figure out what the "area director" believes these words mean.Wow, look at that:
"due process"
is another of the legal requirements for standards-development organizations.
Part of due process is simply making clear what procedures are being applied.
Could it possibly be that the people writing the law
were thinking through how standardization processes could be abused?
Without further ado,
let's look at what the "security area director" did write.As noted above,
IESG had instructed the "area director" to answer the following question:
"Was rough consensus to adopt draft-connolly-tls-mlkem-key-agreement in the
TLS Working Group appropriately called by the WG chairs?"Side note:
Given that the "area director" posted all of the following
on the same day that IESG instructed the "area director" to write this,
presumably this was all written in advance and coordinated with the rest of IESG.
I guess the real point of finally (on 1 November 2025) addressing the adoption decision (from 15 April 2025)
was to try to provide cover for the "last call" a few days later (5 November 2025).As noted above,
the TLS WG chairs had claimed "consensus",
and the "area director" had claimed that there was "clearly consensus".
The "area director" is now quietly shifting to a weaker claim."About 40"?
What happened to the "area director" previously writing
"There is clearly consensus
based on the 67 responses to the adoption call"?
And why is the number of messages supposed to matter in the first place?Messages after the specified adoption-call deadline can't justify the claim that
"the Adoption Call result was that there was rough consensus to adopt the document".
The adoption call  to reach consensus.The fact that the ADs and IESG stonewalled in response to complaints
doesn't mean that they were "attempted" complaints.These numbers are  closer to reality
than the "area director" previously writing
"There is clearly consensus
based on the 67 responses to the adoption call. ...
The vast majority was in favour of adoption ...
There were a few dissenting opinions".Also, given that the "area director" is continually making claims that aren't true
(see examples below)
and seems generally allergic to providing evidence
(the text I'm quoting below has, amazingly,  URLs),
it's a relief to see the "area director" providing names to back up the claimed numbers here.But somehow, even after being
caught
lying about the numbers before,
the "area director" still can't resist shading the numbers a bit.The actual numbers were
20 people unequivocally supporting adoption,
2 people conditionally supporting adoption,
and 7 people unequivocally opposing adoption.
Clearly 7 is close to 6, and 20+2 is close to 23, but, hmmm, not exactly.
Let's check the details:How does the "area director" end up with 6 negative votes rather than 7?
  By falsely listing Thomas Bellebaum as "ambivalent"
  and falsely attributing a "prefer not, but okay if we do" position to Bellebaum.
  In fact,
  Bellbaum had
  written
  "I agree with Stephen on this one and would not support adoption of non-hybrids."
  (This was in reply to Stephen Farrell,
  who had written "I'm opposed to adoption, at this time.")How does the "area director" end up with 23 positive votes rather than 22?
  By falsely listing the document author (Deirdre Connolly) as having stated a pro-adoption position during the call.
  The "area director" seems generally clueless about conflict-of-interest issues
  and probably doesn't find it obvious that an author  vote,
  but the simple fact is that the author  vote.
  She
  sentthreemessages
  during the call period;
  all of those messages are merely commenting on specific points,
  not casting a vote on the adoption question.The document author didn't object to the "area director" fudging the numbers.
Bellebaum did politely
object;
the "area director"
didn't argue,
beyond trying to save face with comments such as "Thanks for the clarification".More to the point,
the "area director" has never explained
whether or how the tallies of positive and negative votes
are supposed to be relevant to the "rough consensus" claim.
The "area director" also hasn't commented on IETF
saying
that IETF doesn't make decisions by voting.Bogus arguments for the draft.
I mentioned in my previous blog post that IETF
claims
that "IETF participants use their best engineering judgment to find the best solution for the whole Internet,
not just the best solution for any particular network, technology, vendor, or user".Okay,
the "area director"
seems to have some basic awareness
that this document flunks the "engineering judgment" criterion.
The "area director" tries to defend this by saying that other documents flunk too.
So confidence-inspiring!Wrong.
Anything+PQ, and in particular ECC+PQ,
complies with NIST's standards when the PQ part does.
See
NIST SP 800-227:
"This publication approves the use of the key combiner (14) for any t > 1 if at least one
shared secret (i.e., S for some j) is generated from the key-establishment methods in SP
800-56A [1] or SP 800-56B [2] or an approved KEM."
For example, if the PQ part is ML-KEM as per FIPS 203,
then NIST allows ECC+PQ too.What's next:
claiming that using PQ in an Internet protocol would violate NIST standards
unless NIST has standardized that particular Internet protocol?I won't bother addressing the errors here,
since the bottom-line claim is orthogonal to the issue at hand.
The TLS WG already has an ECC+PQ document using NIST-approved PQ;
the question is whether to also have a document allowing the ECC seatbelt to be removed.You know what would be even less complicated?
Encrypting with the null cipher!There was a claim that PQ is less complex than ECC+PQ.
There was no response to
Andrey Jivsov
objecting that having a PQ option makes the ecosystem  complicated.
The basic error in the PQ-less-complex claim is that it ignores ECC+PQ already being there.How the "area director" described the objections.This is the closest that the "area director" comes to acknowledging the central security argument for ECC+PQ.
Of course, the "area director" spends as little time as possible on security.
Compare this to
my own objection
to adoption,
which started with SIKE as a concrete example of the dangers
and continued with
"SIKE is not an isolated example: https://cr.yp.to/papers.html#qrcsp
shows that 48% of the 69 round-1 submissions to the NIST competition
have been broken by now".Hmmm.
By listing this as part of an "opposed argument summary",
is the "area director" suggesting that this was disputed?
When and where was the dispute?As noted above, I've seen
unquantified NSA/GCHQ fearmongering about costs,
but that was outside IETF.
If NSA and GCHQ tried the same arguments on a public mailing list
then they'd end up being faced with questions that they can't answer.When I wrote my own summary of the
objections,
I provided a quote and link for each point.
The "area director" doesn't do this.
If the "area director" is accurately presenting an argument that was raised,
why not provide a quote and a link?
Is the "area director" misrepresenting the argument?
Making up a strawman?
The reader can't tell.This comment about "weight" is revealing.
What we'll see again and again is that the "area director"
is expressing the weight that  places on each argument
(within the arguments selected and phrased by the "area director"),
i.e., the extent to which  is convinced or not convinced by those arguments.Given that IESG has power under IETF rules to unilaterally block publications approved by WGs,
it's unsurprising that the "area directors", in their roles as IESG members,
will end up evaluating the merits of WG-approved documents.
But that isn't what this "area director" was instructed to do here.
There isn't a WG-approved document at this point.
Instead the "area director" was instructed to evaluate whether the chairs "appropriately"
called "rough consensus" to "adopt" the document.
The "area director" is supposed to be evaluating procedurally what the WG decision-makers did.
Instead the "area director" is putting his thumb on the scale in favor of the document.Incompetent risk management.I think that the "area director" is trying to make some sort of claim here
about ML-KEM not having been attacked,
but the wording is so unclear as to be unevaluatable.
Why doesn't KyberSlash count?
How about Clangover?
How about the continuing advances in lattice attacks
that have already reduced ML-KEM below its claimed security targets,
the most recent news being from
last month?More importantly,
claiming that ML-KEM isn't "known" to have problems
is utterly failing to address the point of the ECC seatbelt.
It's like saying
"This car hasn't crashed, so the absence of seatbelts isn't a basic flaw".Here the "area director" is reasonably capturing
a statement from one document proponent
(original wording:
"I find it
to be cognitive dissonance to simultaneously argue that the quantum threat requires immediate work, and
yet we are also somehow uncertain of if the algorithms are totally broken. Both cannot be true at the same time").But I promptly
followed up
explaining the error:
"Rolling out PQ is trying to reduce the damage from an attacker having a
quantum computer within the security lifetime of the user data. Doing
that as ECC+PQ instead of just PQ is trying to reduce the damage in case
the PQ part is broken. These actions are compatible, so how exactly do
you believe they're contradictory?"There was, of course, no reply at the time.
The "area director" now simply repeats the erroneous argument."Non-hybrid classics" is weird terminology.
Sometimes pre-quantum algorithms (ECC, RSA, etc.) are called "classical",
so I guess the claim here is that using just ECC in TLS isn't being phased out.
That's a bizarre claim.
There are intensive efforts to roll out ECC+PQ in TLS
to try to protect against quantum computers.
Cloudflare
reports
the usage of post-quantum cryptography having risen to about 50% of all browsers that it sees
(compared to 20% a year ago);
within those connections,
95% use ECC+MLKEM768 and 5% use ECC+Kyber768.The "area director" also gives no explanation of
why the "not phasing out" claim is supposed to be relevant here.See how the "area director" is saying the weight that the "area director" places on each argument
(within the arguments selected and phrased by the "area director"),
rather than evaluating whether there was consensus to adopt the document?Circular argument.
There wasn't consensus to adopt the document in the first place.I think many readers will be baffled by this comment.
If something is "not recommended",
wouldn't that be an argument  standardizing it,
rather than an argument  standardizing it?The answer is that "not recommended" doesn't mean what you think it means:
the "area director" is resorting to confusing jargon.
I don't think there's any point getting into the weeds on this.Incompetent planning for the future.If someone is trying to argue for removing ECC,
there's a big difference between the plausible scenario of ECC having "less" value
and the extreme scenario of ECC having "no" value.
It's wrong for the "area director" to be conflating these possibilities.As I put it
almost two years ago:
"Concretely, think about a demo showing that spending a billion dollars on quantum computation can break a thousand X25519 keys.
Yikes! We should be aiming for much higher security than that!
We don't even want a billion-dollar attack to be able to break  key!
Users who care about the security of their data will be happy that we deployed post-quantum cryptography.
But are the users going to say 'Let's turn off X25519 and make each session a million dollars cheaper to attack'?
I'm skeptical.
I think users will need to see much cheaper attacks before agreeing that X25519 has negligible security value."Furthermore,
let's think for a moment about the idea
that one will eventually want to transition to ,
the specific proposal that the "area director" is portraying as the future.
Here are three ways that this can easily be wrong:Maybe ML-KEM's implementation issues
  end up convincing the community to shift to a more robust option,
  analogously to what happened with ECC.Maybe the advances in public attacks continue to the point of breaking ML-KEM outright.Maybe the cliff stops crumbling and ML-KEM survives,
  but more efficient options also survive. 
  At this point there are quite a few options more efficient than ML-KEM.
  (Random example: SMAUG.
  The current SMAUG software isn't as fast as the ML-KEM software,
  but this is outweighed by SMAUG using less network traffic than ML-KEM.)
  Probably some options will be broken,
  but ML-KEM would have to be remarkably lucky to end up as the most efficient remaining option.Does this "area director" think that all of the more efficient options are going to be broken, while ML-KEM won't?
Sounds absurdly overconfident.
More likely is that the "area director" doesn't even realize that there are more efficient options.
For anyone thinking "presumably those newer options have received less scrutiny than ML-KEM":
we're talking about what to do long-term, remember?Taking ML-KEM as the PQ component of ECC+PQ is working for getting something rolled out now.
Hopefully ML-KEM will turn out to not be a security disaster (or a patent disaster).
But, for guessing what will be best to do in 5 or 10 or 15 years,
picking ML-KEM is premature.Here the "area director" is  attacking a strawman.How is rolling out PQ supposed to be gaining experience
that isn't gained from the current rollout of ECC+PQ?Also, I think it's important to call out the word "pure" here
as incoherent, indefensible marketing.
What we're actually talking about isn't modifying ML-KEM in any way;
it's simply hashing the ML-KEM session key together with other inputs.
Is ML-KEM no longer "pure" when it's plugged into TLS,
which also hashes session keys?
(The word "pure" also showed up in a few of the earlier quotes.)Here we again see the area director making a decision to support the document,
rather than evaluating whether there was consensus in the WG to adopt the document.Again getting the complexity evaluation backwards.Here the "area director" is again
making the same mistake explained earlier:
ignoring the fact that ECC+PQ is already there,
and thus getting the complexity evaluation backwards.The "thus add risk" logic is also wrong.
Again, all of these options are more complex than the null cipher.No, the details of how to combine ECC with PQ in TLS are already settled and deployed.Looking beyond TLS:
Chempat hashes the transcript (similarly to TLS),
making it robust for a wide range of protocols.
The other options add fragility by hashing less for the sake of minor cost savings.
Each of these options is under 10 lines of code.
The "area director" exaggerates the complexity by mentioning "extensive discussions",
and spends much more effort hyping this complexity as a risk
than acknowledging the risks of further PQ attacks.Anyway,
it's not as if the presence of this document
has eliminated the discussions of ECC+PQ details,
nor is there any credible mechanism by which it could do so.
Again, the actual choice at hand is whether to have PQ as an option  ECC+PQ.
Adding that option  complexity.
The "area director" is getting the complexity comparison backwards
by instead comparing
(1) PQ in isolation to (2) ECC+PQ in isolation.Botching the evaluation of human factors.I would expect a purchasing manager to have instructions
along the lines of "Buy only products complying with the standards",
and to never see IETF's confusing jumble of further designations.From a security perspective,
it's a big mistake to ignore the human factor,
such as the impact of a purchasing manager saying
"This is the most efficient standard so I'll pick that".Is this supposed to have something to do with the consensus question?Ah, yes, "known to consume"!
There was, um,
one of those, uh, studies showing the details of, um, how implementors use RFCs,
which, uh, showed that 100% of the implementors diligently consumed the warnings in the RFCs.
Yeah, that's the ticket.
I'm sure the URL for this study is sitting around here somewhere.Let's get back to the real world.
Even if an implementor does see a "This document is a bad idea" warning,
this simply doesn't matter
when the implementors are chasing contracts issued by purchasing managers
who simply care what's standardized and haven't seen the warning.It's much smarter for the document to
(1) eliminate making the proposal that it's warning about
and (2) focus, starting in the title, on saying why such proposals are bad.
This makes people  likely to see the warning,
and at the same time it removes the core problem of the bad proposal being standardized.Fictions regarding country actions.Certainly there were competition-like aspects to the process.
I tend to refer to it as a competition.
But in the end the selection of algorithms to standardize was made by NIST,
with input behind the scenes from NSA.Nonsense.
The premier multi-year effort by cryptographers to "publicly discuss post-quantum crypto candidates"
is the cryptographic literature.Here's the objection from
Stephen Farrell
that the "area director" isn't quoting or linking to:
"I don't see what criteria we might use in adopting this that
   wouldn't leave the WG open to accusations of favouritism if
   we don't adopt other pure PQ national standards that will
   certainly arise".After reading this objection,
you can see how the "area director" is sort of responding to it
by suggesting that everybody is following NIST
(i.e., that the "certainly arise" part is wrong).But that's not true.
NIST's selections are controversial.
For example, ISO is
considering
not just ML-KEM but alsoClassic McEliece,
  where NIST has
  said
  it's waiting for ISO
  ("After the ISO standardization process has been completed, NIST may consider developing a standard for Classic McEliece based on the ISO standard"),
  andFrodoKEM,
  which NIST
  said
  "will not be considered further for standardization".ISO is also now
considering
NTRU, where the
advertisement
includes "All patents related to NTRU have expired"
(very different from the ML-KEM situation).BSI, which sets cryptographic standards for Germany,
recommends
not just ML-KEM
but also FrodoKEM (which it describes as "more conservative" than ML-KEM)
and Classic McEliece ("conservative and very thoroughly analysed").
Meanwhile China has called for
submissions
of new post-quantum proposals for standardization.I could keep going,
but this is enough evidence to show that Farrell's prediction was correct;
the "area director" is once again wrong.Notice how the "area director" is dodging Farrell's point.
If NSA can pressure the TLS WG into standardizing non-hybrid ML-KEM,
why can't China pressure the TLS WG into standardizing something China wants?
What criteria will IETF use to answer this question
without leaving the WG "open to accusations of favouritism"?
If you want people to believe that it isn't about the money
then you need a  convincing alternative story.This is just rehashing earlier text,
even if the detailed wording is a bit different.Irrelevant.
The question is whether they're being standardized.Yes, NSA waving around money has convinced some corporations to provide software.
How is this supposed to justify the claim
that "there was rough consensus to adopt the document"?Repeating a claim doesn't make it true.Irrelevant.
What matters is whether the document is standardized.This sort of dismissal might be more convincing
if it were coming from someone providing more URLs and fewer easily debunked claims.
But it's in any case not addressing the consensus question.The chairs claimed that "we have consensus to adopt this draft"
(based on claiming that "there were enough people willing to review the draft",
never mind the number of objections).
That claim is wrong.
The call for adoption failed to reach consensus.The "area director" claimed that
"There is clearly consensus
based on the 67 responses to the adoption call. ...
The vast majority was in favour of adoption ...
There were a few dissenting opinions".
These statements still haven't been retracted;
they were and are outright lies about what happened.
Again, the
actual tallies
were
20 people unequivocally supporting adoption,
2 people conditionally supporting adoption,
and 7 people unequivocally opposing adoption.Without admitting error,
the "area director" has retreated to a claim of "rough consensus".
The mishmash of ad-hoc comments from the "area director"
certainly doesn't demonstrate any coherent meaning of "rough consensus".It's fascinating that IETF's
advertising to the public
claims that IETF's "decision-making requires achieving broad consensus",
but IETF's
WG procedures
allow controversial documents to be pushed through on the basis of "rough consensus".
To be clear,
that's only if the "area director" approves of the documents,
as you can see from the same "area director"
issuing yet another mishmash of ad-hoc comments to
overturn
a separate chair decision in September 2025.You would think that the WG procedures would  "rough consensus".
They don't.
All they say is that
"51% of the working group does not qualify as 'rough consensus' and 99% is better than rough",
not even making clear whether 51% of  within a larger working group can qualify.
This leaves a vast range of ambiguous intermediate cases up to the people in power.]]></content:encoded></item><item><title>Shai-Hulud Returns: Over 300 NPM Packages Infected</title><link>https://helixguard.ai/blog/malicious-sha1hulud-2025-11-24</link><author>mrdosija</author><category>hn</category><pubDate>Mon, 24 Nov 2025 10:40:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Fifty Shades of OOP</title><link>https://lesleylai.info/en/fifty_shades_of_oop/</link><author>todsacerdoti</author><category>hn</category><pubDate>Mon, 24 Nov 2025 09:40:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[OOP-bashing seems fashionable nowadays. I decided to write this article after seeing two OOP-related articles on Lobsters in quick succession. I’m not interested in defending or attacking OOP, but I do want to throw in my two cents and offer a more nuanced view.The industry and the academy have used the term “object-oriented” to mean so many different things. One thing that makes conversations around OOP so unproductive is the lack of consensus on what OOP is.What is Object-Oriented Programming? Wikipedia defines it as “a programming paradigm based on the concept of objects.” This definition is unsatisfactory, as it requires a definition of an “object” and fails to encompass the disparate ways the term is used in the industry. There is also Alan Kay’s vision of OOP. However, the way most people use the term has drifted apart, and I don’t want to fall into essentialism or etymological fallacy by insisting on a “true” meaning.Instead, I think it is better to treat OOP as a mixed bag of interrelated ideas and examine them individually. Below, I will survey some ideas related to OOP and mention their pros and cons (in my subjective mind).Object-oriented programming is a method of implementation in which programs are organized as cooperative collections of objects, each of which represents an instance of some class, and whose classes are all members of a hierarchy of classes united via inheritance relationships. — Grady BoochClasses extend the idea of a “struct” or “record” with support for the method syntax, information hiding, and inheritance. We will talk about those specific features later.Classes can also be viewed as blueprints for objects. It is not the only way to do that, and  is an alternative pioneered by Self and, most famously, used by JavaScript. Personally, I feel that prototypes are harder to wrap one’s head around compared to classes. Even JavaScript tries to hide its usage of prototypes from newcomers with ES6 classes.In Japanese, we have sentence chaining, which is similar to method chaining in Ruby — Yukihiro MatsumotoThe method syntax is one of the less controversial OOP features. It captures common programming use cases involving operations on a specific subject. Even in languages without methods, it is common to see functions effectively serve as methods by taking the relevant data as their first argument (or last, in languages with currying).The syntax involves method definitions and method calls. Usually, languages supporting methods have both, unless you consider the “pipe operators” in functional languages as a form of method call.The method call syntax aids IDE autocompletion, and method chaining is often more ergonomic than nested function calls (similar to the pipe operator in functional languages).There are some debatable aspects of the method syntax, too. First, in many languages, methods are often not definable outside of a class, which causes a power imbalance compared to functions. There are certain exceptions, such as Rust (methods are always defined outside of the struct), Scala, Kotlin, and C# (extension methods).Second, in many languages,  or  is implicit. This keeps the code more concise, but it can also introduce confusion and increase the risk of accidental name shadowing. Another drawback of an implicit this is that it is always passed as a pointer, and its type cannot be changed. This means you cannot pass it as a copy, and sometimes this indirection leads to performance issues. More importantly, because the type of this is fixed, you cannot write generic functions that accept different  types. Python and Rust got  right from the start, and C++ just fixed this issue in C++23 with deducing this.Fourth, the dot notation is used for both instance variable accesses and method calls in most languages. This is an intentional choice to make methods look more uniform with objects. In certain dynamically typed languages where methods are instance variables, this is fine and pretty much not even a choice. On the other hand, in languages like C++ or Java, this can cause confusion and shadowing problems.Its interface or definition was chosen to reveal as little as possible about its inner workings — [Parnas, 1972b]In Smalltalk, all instance variables are not directly accessible from outside the object, and all methods are exposed. More modern OOP languages support information hiding via access specifiers like  at the class level. Even non-OOP languages usually support information hiding in some way, be it module systems, opaque types, or even C’s header separation.Information hiding is a good way to prevent invariant from being violated. It is also a good way to separate frequently changed implementation details from a stable interface.Nevertheless, aggressively hiding information may cause unnecessary boilerplate or abstraction inversion. Another criticism comes from functional programmers, who argue that you don’t need to maintain invariants and thus don’t need much information hiding if data is . And, in a sense, OOP encourages people to write mutable objects that must be maintained as invariants.Information hiding also encourages people to create small, self-contained objects that “know how to handle themselves,” which leads directly into the topic of encapsulation.If you can, just move all of that behavior into the class it helps. After all, OOP is about letting objects take care of themselves. — Bob Nystrom, Game Programming PatternsEncapsulation is often confused with information hiding, but the two are distinct. Encapsulation refers to bundling data with the functions that operate on it. OOP languages directly support encapsulation with classes and the method syntax, but there are other approaches (e.g., the module system in OCaml).The most common construct for encapsulation is objects, but it is not the only mechanism. Many modern languages also support closures (in fact, closures and objects can simulate each other). There are also other lesser-known approaches, such as modules found in the ML family.Separation of interface and implementation is an old idea closely related to information hiding, encapsulation, and abstract data type. In some sense, even C’s header files can be considered an interface, but OOP usage of “interface” most often refers to a specific set of language constructs that support polymorphism (typically implemented via inheritance). Usually, an interface can’t contain data, and in more restricted languages (e.g., early versions of Java), they can’t contain method implementations either. The same idea of an interface is also common in non-OOP languages: Haskell type classes, Rust traits, and Go interfaces all serve the role of specifying an abstract set of operations independent of implementations.Interface is often considered a simpler, more disciplined alternative to full-blown class inheritance. It is a single-purpose feature and doesn’t suffer from the same diamond problem that plagues multiple inheritance.Interface is also extremely useful in combination with parametric polymorphism, since it allows you to constrain the operations a type parameter must support. Dynamically-typed languages (and C++/D template) achieve something similar through duck-typing, but even languages with duck-typing introduce interface constructs later to express constraints more explicitly (e.g., C++ concepts or TypeScript interfaces).The interface as implemented in OOP languages often has a runtime cost, but that’s not always the case. For example, C++ concepts is an example that only supports compile-time, and Rust’s trait only has opt-in runtime polymorphism support via .OOP to me means only messaging, local retention and protection and hiding of state-process, and extreme late-binding of all things — Alan KayLate binding refers to delaying the lookup of a method or a member until runtime. It is the default of most dynamic-typed languages, where method calls are often implemented as a hash table lookup, but can also be achieved with other means, such as dynamic loading or function pointers.A key aspect of late binding is that behaviour can be changed while the software is still running, enabling all kinds of hot-reloading and monkey-patching workflows.The downside of late binding is its non-trivial performance cost. Moreover, it can also be a footgun for breaking invariants or even interface mismatches. Its mutable nature can also introduce subtler issues, for example, the “late binding closures” pitfall in Python.A concept related to late binding is dynamic dispatch, in which the implementation of a polymorphic operation is selected at runtime. The two concepts overlap, though dynamic dispatch focuses more on selecting multiple known polymorphic operations rather than on name lookup.In a dynamically typed language, dynamic dispatch is the default since everything is late-bound. In statically typed languages, it is usually implemented as a virtual function table that looks something like this under the hood:These languages also provide compile-time guarantees that the vtable contains valid operations for the type.Dynamic dispatch can be decoupled from inheritance, whether by manually implementing a v-table (e.g., C++‘s “type-erased types” such as ) or an interface/trait/typeclass kind of constructs. When not paired with inheritance, dynamic dispatch alone is usually not considered “OOP.”Another thing to note is that the pointer to the v-table can be directly inside the object (e.g., C++) or embedded in “fat pointers” (e.g., Go and Rust).programming using class hierarchies and virtual functions to allow manipulation of objects of a variety of types through well-defined interfaces and to allow a program to be extended incrementally through derivation  — Bjarne StroustrupInheritance has a long history, way backed to Simula 67. It is probably the most iconic feature of OOP. Almost every language marketed as “object-oriented” includes it, while languages that avoid OOP typically omit it.It can be damn . In many cases, using an alternative approach will result in significantly more boilerplate.On the other hand, inheritance is a very non-orthogonal feature. It is a single mechanism that enables dynamic dispatch, subtyping polymorphism, interface/implementation segregation, and code reuse. It is flexible, though that flexibility makes it easy to misuse. For that reason, some languages nowadays replace it with more restrictive alternative constructs.There are some other problems with inheritance. First, using inheritance almost certainly means you are paying the performance cost of dynamic dispatch and heap allocation. In some languages, such as C++, you can use inheritance without dynamic dispatch and heap allocation, and there are some valid use cases (e.g., code reuse with CRTP), but the majority of uses of inheritance are for runtime polymorphism (and thus rely on dynamic dispatch).Finally, inheritance hierarchies are rigid. They suffer from issues like the diagonal problem, and that inflexibility is one of the main reasons people prefer composition over inheritance. The component pattern chapter of Game Programming Patterns provides a good example.If for each object  of type  there is another object  of type  such that for all programs  defined in terms of , the behavior of  is unchanged when  is substituted for , then  is a subtype of . — Barbara Liskov, “Data Abstraction and Hierarchy”Subtyping describes an “is a” relation between two types. The Liskov substitution principle defines the property that safe subtyping relationships must uphold.OOP languages often support subtyping via inheritance, but note that inheritance doesn’t always model subtyping, and it is not the only form of subtyping either. Various interface/trait constructs in non-OOP languages often support subtyping. And besides , where one explicitly declares the subtyping relationship, there are also , where the subtyping is implicit if one type contains all the features of another type. Good examples of structural subtyping include OCaml (objects and polymorphic variants) and TypeScript interfaces. Subtyping also shows in all kinds of little places, such as Rust lifetime and TypeScript’s coercion from a non-nullable type to its nullable counterpart.A related concept to subtyping is variance (not related to class-invariant), which bridges parametric polymorphism and subtyping. I won’t bother explaining variance here, as this topic probably needs an entire blog post to explain well. It is a great ergonomic boost (e.g., C++ pointers will be unusable for polymorphic use if they are not covariant), but most languages only implement a limited, hard-coded version, because it is hard to understand and also error-prone. In particular, mutable data types should usually be invariant, and Java/C#‘s covariant arrays are a primary example on this gone wrong. Only a few languages allow programmers to explicitly control variance, including Scala and Kotlin.Type conversion via subtyping relationships is often implicit. Implicit conversion has a bad reputation. Though doing it with subtyping is ergonomic, and is probably the least surprising kind of implicit conversion. Another way to view subtyping is as the dual of implicit conversions. We can “fake” a subtyping relation with implicit conversion. For example, C++ templated types are invariant, but  achieves covariance with an implicit conversion from  to . Does Go Have Subtyping? is a good article to further explore this idea.One reason that language designers often try to avoid subtyping is the implementation complexity. Integrating bidirectional type inference and subtyping is notoriously difficult. Stephen Dolan’s 2016 thesis Algebraic Subtyping makes good progress addressing this issue.I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages — Alan KayMessage passing means using objects that send each other “messages” as a way of execution. It is the centric theme of Alan Kay’s vision of OOP, though the definition can be pretty vague. An important point is that message names are late-bound, and the structures of these messages are not necessarily fixed at compile time.Many early object-oriented concepts were influenced by distributed and simulation systems, where message passing is natural. However, in the era where most people work on single-threaded code, the message was gradually forgotten in languages such as C++ and Java. The method syntax only has limited benefit compared to the original message-passing idea (Bjarne Stroustrup was definitely aware of the idea from Simula, but there is practical constraint on how to make it fast). There was still some genuine message passing, but only in specific areas such as inter-process communication or highly event-driven systems.Message passing gains a Renaissance in concurrent programming, ironically through non-OOP languages like Erlang and Golang, with constructs such as actors and channels. This kind of shared-nothing concurrency removed a whole range of data race and race condition bugs. In combination with supervision, actors also provide fault tolerance, so that the failure of one actor will not affect the entire program.In general, a rule of thumb is: use classes and objects in situations where open recursion is a big win. — Real World OCamlOriginating in the famous Types and Programming Languages,  is probably the least well-known and understood term in this blog post. Nevertheless, it just describes a familiar property of object-oriented systems: methods for an object can call each other, even if they are defined in different classes in the inheritance hierarchy.The term is somewhat misleading, as there may not be recursive function calls, but here “recursion” means “mutually recursive.” The word “open” refers to “open to extension,” typically empowered by inheritance.It’s easiest to see with an example:For anyone with some familiarity with OOP, we probably take open recursion for granted, even though we may not be aware of its name. However, not all language constructs have this property. For example, in many languages, functions are not mutually recursive by default:Now, in languages with late-bound functions, functions in the same module can always call each other (e.g., Python, JavaScript). There are other languages where functions are mutually recursive by default (e.g., Rust), or have forward declarations (C) or a  construct (Scheme and ML family) to make functions mutually recursive. This solves the “recursion” part, but still not the “open” part yet:Let’s fix this problem by using a callback:Tada, we just reinvented prototype-style dispatch!Anyway, with my quick example above, I want to show that open recursion is a property that OOP gives for free, but reproducing it in languages without built-in support can be tricky. Open recursion allows interdependent parts of an object to be defined separately, and this property is used in many instances, for example, the entire idea of decorator pattern depends on open recursion.Perhaps the more common complaints about OOP are not about specific language features, but rather about the programming styles it encourages. Many practices are taught as universal best practices, sometimes with rationales, but their downsides are often omitted. Some examples popped into my mind arepreferring polymorphism over tagged union/if/switch/pattern matchingOpen to extension, easier to add new cases.Performance hit; Related behaviors get scattered in multiple places; harder to see the whole controll flow in one placemaking all data members privateMore boilerplates; Often unnecessary to hide data without invariants; Getter/setter pairs work less well compared to direct access in languages without property syntaxpreferring small “self-managed” objects over central “managers”Harder to violate invariants, cleaner code organizationPotential bad data locality, missing parallelism opportunities, and duplicated references to common data (“back pointer”)Prevent new features from breaking the old one. Prevent API breakNo reason to “close” a non-public module where you own its usage. Leads to unnecessary complexity and inheritance chain; poorly designed interfaces are not changed; Can cause abstraction inversionpreferring abstraction over concrete implementationsMaking more system swappable and testableOveruse sacrifices readability and debuggability. Performance cost of extra indirectionThis blog post is long enough, so I will not go into more details. Feel free to disagree with my “advantages and disadvantages”. What I want to convey is that almost all those practices come with trade-offs.Congratulations on making it to the end of this article! There are other topics I’d love to discuss, such as RAII and design patterns. However, this article is long enough, so I will leave those for you to explore on your own.]]></content:encoded></item><item><title>Build a Compiler in Five Projects</title><link>https://kmicinski.com/functional-programming/2025/11/23/build-a-language/</link><author>azhenley</author><category>hn</category><pubDate>Mon, 24 Nov 2025 07:14:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Are you interested in building a compiler? Learning how functional
languages are implemented? Gaining a bit of practical experience with
x86-64 assembly language? If so, I invite you to try your hand at the
projects in my class,
CIS531. CIS531 is a masters-level
class on compiler design which assumes that (a) you know how to
program, (b) you’ve had some exposure to C (know about stack
allocation, malloc, etc.), and (c) have seen some assembly code. My
class projects are in the Racket programming language, but if you
don’t know Racket, it is quite easy to learn: I have a set of YouTube
video lectures that teach Racket
quickly!
If you’ve never heard of Racket before, or you’re skeptical of
functional programming, indulge me for a bit: there’s no hardcore FP
theory or math in this course, and Racket is genuinely the best
language to use for this specific setup.My class follows Prof. Jeremy Siek’s excellent book, “Essentials of
Compilation.” While I highly recommend buying the book and supporting
Prof. Siek, I will also note that there are free online preliminary
editions
floating around; in my class, I followed the free version and
suggested that students buy the book if doing so fit their
goals. However, along with the book, I also have a set of class slides
along with sporadic course videos, both available on the class
website.This class builds up to a compiler with the following features:Variables and assignment via Integer arithmetic via  and Reading inputs / printing outputBooleans, conjunctions/disjunctions (and/or)Branching via , integer comparisons (<, etc.)Assignment / mutation ()Fixed-arity functions and function applicationLambdas (closures at runtime)The unique combination of features lets us tour an interesting
cross-section of programming languages, exploring both imperative
programming with loops and mutation but also functional programming
with lists and recursion.To be specific, I challenge you to complete five projects, each
including a comprehensive test suite that will seriously stress the
correctness of your implementation. p1 is a warmup project (you should
skip if you already know Racket), but p2-5 build a compiler for a set
of increasingly-complex languages to x86-64. The languages nest inside
of each other, with p2 giving us straight-line arithmetic, p3 giving
us decision trees, p4 giving us loops and mutation, and p5 giving us
functions, recursion, and lambdas.The projects are designed with one key principle in mind: get us to
the most expressive/fun language possible, as fast as possible. In
doing this, we sacrifice a lot that might be typically covered:Our languages aren’t type/memory safe, we assume the programmer is
correctNo register allocation (possible to add, not too hard)No garbage collection of any kind: we . We could
trivially support the Boehm GC (I have done that in the past), but
it was another static library to link in and I really wanted to make
this self contained.We support a  limited set of builtins (but it is trivial to
add more)So even after project 5, getting to a “real” compiler would take a bit
of effort. The most important (in my opinion) are (a) memory safety
(the language needs to be safe, period) via dynamic type tagging and
(b) slightly more builtins, and (c) register allocation. That would
get us to a respectable compiler. After that, we could add more
language features, or optimize the ones we have, e.g., by using
abstract interpretation.Our language will include functions, loops, branching, assignment, and
even heap-allocated vectors. As an example of the power, here’s a
Sudoku solver written in the language(program
 ;; =========================
 ;; List primitives
 ;; Empty list is (void)
 ;; =========================
 (define (is_nil x) (eq? x (void)))

 ;; cons cell as 2-element vector: [0] = head, [1] = tail
 (define (cons h t)
   (let ([c (make-vector 2)])
     (let ([_ (vector-set! c 0 h)])
       (let ([_ (vector-set! c 1 t)])
         c))))

 (define (head c) (vector-ref c 0))
 (define (tail c) (vector-ref c 1))

 ;; =========================
 ;; Cell representation
 ;; cell = (row col val) as nested cons
 ;; =========================
 (define (make_cell r c v)
   (cons r (cons c (cons v (void)))))

 (define (cell_row cell)
   (head cell))

 (define (cell_col cell)
   (head (tail cell)))

 (define (cell_val cell)
   (head (tail (tail cell))))

 ;; =========================
 ;; Block indexing (0,1,2) for rows/cols
 ;; =========================
 (define (block_index3 x)
   (if (< x 3)
       0
       (if (< x 6)
           1
           2)))

 (define (same_block? r1 c1 r2 c2)
   (if (eq? (block_index3 r1) (block_index3 r2))
       (eq? (block_index3 c1) (block_index3 c2))
       #f))

 ;; =========================
 ;; Lookup current value at (row, col) in board
 ;; board is a list of cells
 ;; Return 0 if not assigned
 ;; =========================
 (define (lookup board row col)
   (if (is_nil board)
       0
       (let ([cell (head board)])
         (let ([r (cell_row cell)])
           (let ([c (cell_col cell)])
             (if (and (eq? r row) (eq? c col))
                 (cell_val cell)
                 (lookup (tail board) row col)))))))

 ;; =========================
 ;; Conflict check:
 ;; #t if some cell in board has:
 ;;   - same value, and
 ;;   - same row OR same col OR same 3x3 block
 ;; =========================
 (define (conflicts? board row col val)
   (if (is_nil board)
       #f
       (let ([cell (head board)])
         (let ([r (cell_row cell)])
           (let ([c (cell_col cell)])
             (let ([v (cell_val cell)])
               (if (and (eq? v val)
                        (or (eq? r row)
                            (or (eq? c col)
                                (same_block? r c row col))))
                   #t
                   (conflicts? (tail board) row col val))))))))

 ;; =========================
 ;; Recursive backtracking solver over (row, col)
 ;; board: list of assignments
 ;; rows, cols = 0..8
 ;; =========================
 (define (solve_cell row col board)
   (if (eq? row 9)
       ;; All rows done: solved
       board
       (if (eq? col 9)
           ;; End of row: go to next row
           (solve_cell (+ row 1) 0 board)
           ;; Otherwise, try this cell
           (let ([existing (lookup board row col)])
             (if (eq? existing 0)
                 ;; Empty cell: try values 1..9
                 (let ([candidate 1])
                   (let ([solution (void)])
                     (begin
                       (while (and (< candidate 10)
                                   (eq? solution (void)))
                              (begin
				(if (conflicts? board row col candidate)
                                    ;; conflict, skip
                                    (set! solution solution)
                                    ;; no conflict, extend board and recurse
                                    (let ([s (solve_cell row
                                                         (+ col 1)
                                                         (cons (make_cell row col candidate)
                                                               board))])
                                      (if (eq? s (void))
                                          (set! solution solution)
                                          (set! solution s))))
				(set! candidate (+ candidate 1))))
                       solution)))
                 ;; Pre-filled cell: just move on
                 (solve_cell row (+ col 1) board))))))

 ;; =========================
 ;; Read initial board from input:
 ;; 81 integers, row-major, 0 = empty, 1..9 = given
 ;; Returns list of cells
 ;; =========================
 (define (read_board)
   (let ([board (void)])
     (let ([i 0])
       (begin
         (while (< i 9)
		(begin
                  (let ([j 0])
                    (while (< j 9)
			   (begin
			     (let ([v (read)])
                               (if (eq? v 0)
				   (set! board board)
				   (set! board (cons (make_cell i j v) board))))
			     (set! j (+ j 1)))))
                  (set! i (+ i 1))))
         board))))

 ;; =========================
 ;; Entry: read board, solve from (0,0), return solution
 ;; Solution is a list of (row col val) cells
 ;; =========================
 (let* ([board (read_board)]
        [solution (solve_cell 0 0 board)])
   (lookup solution 8 8)))
The final language you’ll implement will be this one. In comments,
I’ve also highlighted the sublanguages: for example, project 2
includes only numbers, input (read), binary plus, unary minus,
variable references and let binding. It grows to all of .(define (R5-exp? e)
  (match e
    ;; Project 2
    [(? fixnum?) #t]
    ['(read) #t]
    [`(+ ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(- ,(? R5-exp? e)) #t]
    [(? symbol?) #t]
    [`(let ([,(? symbol? x) ,(? R5-exp? e)]) ,(? R5-exp? eb)) #t]
	;; Project 3
    [#t #t]
    [#f #t]
    ['(void) #t]
    [`(- ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(and ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(or  ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(not ,(? R5-exp? e1)) #t]
    [`(,(? cmp? c) ,(? R5-exp? e0) ,(? R5-exp? e1)) #t]
    [`(if ,(? R5-exp? e-g) ,(? R5-exp? e-t) ,(? R5-exp? e-f)) #t]
    ;; Project 4
    [`(let* ([,(? symbol? xs) ,(? R5-exp? es)] ...) ,(? R5-exp? eb)) #t]
    [`(begin ,(? R5-exp?) ... ,(? R5-exp? ret)) #t]
    [`(while ,(? R5-exp? e-g) ,(? R5-exp? es) ...) #t]
    [`(make-vector ,(? R5-exp? len)) #t]
    [`(vector-ref ,(? R5-exp? v) ,(? fixnum? i)) #t]
    [`(vector-set! ,(? R5-exp? v) ,(? fixnum? i) ,(? R5-exp? e-v)) #t]
    [`(set! ,(? symbol? x) ,(? R5-exp? e)) #t]
    ;; Project 5
    [`(,(? R5-exp? e-f) ,(? R5-exp? a-args) ...) #t]
    [`(lambda (,(? symbol? xs) ...) ,(? R5-exp? e-body)) #t]
	[_ #f]))

(define (R5-defn? defn)
  (match defn
    ;; Project 5 adds multiple function definitions
    [`(define (,(? symbol? f) ,(? symbol? formals) ...)  ,(? R5-exp? e-b)) #t]
    [_ #f]))

(define (R5? p)
  (match p
    [`(program ,(? R5-defn? defns) ... ,(? R5-exp?)) #t]
    [_ #f]))
To get you booted up fast as possible, every single project is
designed the same way: – Your pass implementations. You will edit functions provided here.
-> This is the  file you will edit! The rest are read-only – IR definitions and predicates like , , etc. (see also typed/shrunk variants) – Reference interpreters for several IRs (used by tests and for your own debugging). – System/ABI configuration, pass names, runtime filenames, output paths, etc. – Driver that runs all passes, can build a binary, and can launch a debug server. – Test harness. Runs isolation tests or end-to-end native tests depending on  mode. – Minimal runtime (, , etc.). – Example programs (). – Input streams for programs (lines of integers). – Instructor goldens (IR snapshots, interpreter outputs, and stdout baselines).You write your code in , which consists of a set of
. Each pass transforms an input language into an output
language, and these intermediate languages (IRs) are codified via
predicates in . To define the meaning of each IR, we give an
interpreter for each in . For the compiler to be
correct, it needs to be the case that–for all input streams–the
compiler produces the same output stream across all intermediate
IRs. There is some system-specific stuff in , which takes
care of things like Linux vs. Mac ABI issues, specifying register
names, etc. The  file acts as a main compiler entrypoint,
and it carefully runs each pass of the compiler, checking predicates
before/after each pass and interpreting each IR, checking to ensure
consistency. This is a  win for debugging, in my opinion: you
 want to localize errors to the proximate pass which causes
misinterpretation, and  seriously aids debugging in my
experience. There is also more comprehensive test infrastructure in
; this test script is invoked by the Python-based test
scripts in . These tests check the behavior of the compiler on
the programs in the  directory, using the files from
 as inputs and comparing to the outputs in .Why Is This Course Unique and Cool?You build a , all the way to actual x86-64
assembly.Each IR has a corresponding interpreter, which is easy to find/read
and written in a familiar style, giving semantic clarity and
testable correctness.The project is , meaning that you can use it as
a base for building your own language. Of course, this is thanks to
Dr. Siek’s great “incremental” design.It is fully testable across multiple passes, which helps
anticipate the thing we all fear most about writing a compiler:
seeing a problem that is the ramification of far-away code from
higher up in the compilation pipeline.It is written in a simple, pure recursive style. Just plain old
pattern matching and recursion here, no need for any complex
abstractions.Familiarize yourself with the course webpage: https://kmicinski.com/cis531-f25If you don’t know Racket, start with project 1: https://kmicinski.com/cis531-f25/projects/1Otherwise, start with project 2: https://kmicinski.com/cis531-f25/projects/2When you finish each project, move on to the next!When you’re done, start building your  language. Consider
adding type (checking/inference), classes, more builtins, pattern
matching, continuations, exceptions, algebraic effects. The options
are myriad, but once you’ve finished projects 2-5, you’ve built a
whole compiler for a surprisingly expressive language.Thank you to the National Science Foundation and OthersIf you like this work and live in the United States, please feel
commensurately less bad about paying your taxes. I made the whole
class free, at least as free as I could given practical
constraints. This class work on compilation is partially supported by
our NSF PPoSS
large,
which has already produced manycoolmajorresults. In
subsequent explorations, I am hoping that I can use this class
compiler as a baseline for highly-scalable engines that reason about
programs. Given the simple, self-contained nature–and the presence of
per-pass interpreters and consistency testing–I see this as an
awesome potential baseline for cool extensions.My course is of course heavily inspired by Prof. Siek’s book and
course, along with inspiration from Thomas Gilray at Washington
State. Eight years ago, Tom and I took a spontaneous trip to see the
eclipse halfway across the country (skipping out on the ICSE ‘17
deadline basically); we discussed compiler design over a steamed
seafood buffet in Myrtle Beach after napping in a cheap motel, having
been awake for over 24 hours and feeling the eclipse had made it worth
it. We sketched out his whole compiler on that roadtrip, and ever
since that night eating steamed crabs, I wanted to build my own course
compiler. Now that I have, I am not sure it compares to waking up for
just four hours of twilight, only to consume copious amounts of butter
and shellfish as the brisk ocean air wisps over your face, the
closures and continuations softly washing rhythmically through the
conversation as you walk along the beach back to your $50 motel room.In closing, thanks for checking this out, this compiler was a ton of
fun to build. Even as someone who has some amount of expertise in
compiler design, building it and getting it 100% right (I hope!) was
such a rewarding experience. My real sincere hope is that it offers
students (and you!) a fun journey. If you end up doing anything this,
please get in touch: kkmicins@syr.edu. I’d love to see what you come
up with. Best wishes,Kristopher Micinski
– Syracuse, November, 2025]]></content:encoded></item><item><title>What OpenAI did when ChatGPT users lost touch with reality</title><link>https://www.nytimes.com/2025/11/23/technology/openai-chatgpt-users-risks.html</link><author>nonprofiteer</author><category>hn</category><pubDate>Mon, 24 Nov 2025 05:58:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Lambda Calculus – Animated Beta Reduction of Lambda Diagrams</title><link>https://cruzgodar.com/applets/lambda-calculus</link><author>perryprog</author><category>hn</category><pubDate>Mon, 24 Nov 2025 05:17:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>RuBee</title><link>https://computer.rip/2025-11-22-RuBee.html</link><author>Sniffnoy</author><category>hn</category><pubDate>Mon, 24 Nov 2025 03:08:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I have at least a few readers for which the sound of a man's voice saying
"government cell phone detected" will elicit a palpable reaction. In
Department of Energy facilities across the country, incidences of employees
accidentally carrying phones into secure areas are reduced through a sort of
automated nagging. A device at the door monitors for the presence of a tag;
when the tag is detected it plays an audio clip. Because this is the government,
the device in question is highly specialized, fantastically expensive, and
says "government cell phone" even though most of the phones in question are
personal devices. Look, they already did the recording, they're not changing
it now!One of the things that I love is weird little wireless networks. Long ago I
wrote about ANT+,
for example, a failed personal area network standard designed mostly around
fitness applications. There's tons of these, and they have a lot of
similarities---so it's fun to think about the protocols that went down a
completely different path. It's even better, of course, if the protocol is
obscure outside of an important niche. And a terrible website, too? What more
could I ask for.The DoE's cell-phone nagging boxes, and an array of related but more critical
applications, rely on an unusual personal area networking protocol called RuBee.RuBee is a product of Visible Assets Inc., or VAI, founded in 2004  by John K.
Stevens. Stevens seems a somewhat improbable founder, with a background in
biophysics and eye health, but he's a repeat entrepreneur. He's particularly fond of companies
called Visible: he founded Visible Assets after his successful tenure as CEO of
Visible Genetics. Visible Genetics was an early innovator in DNA sequencing, and
still provides a specialty laboratory service that sequences samples of HIV in
order to detect vulnerabilities to antiretroviral medications.Clinical trials in the early 2000s exposed Visible Genetics to one of the more
frustrating parts of health care logistics: refrigeration. Samples being shipped
to the lab and reagents shipped out to clinics were both temperature sensitive.
Providers had to verify that these materials had stayed adequately cold throughout
shipping and handling, otherwise laboratory results could be invalid or incorrect.
Stevens became interested in technical solutions to these problems; he wanted
some way to verify that samples were at acceptable temperatures both in storage
and in transit.Moreover, Stevens imagined that these sensors would be in continuous communication.
There's a lot of overlap between this application and personal area networks (PANs),
protocols like Bluetooth that provide low-power communications over short ranges.
There is also clear overlap with RFID; you can buy RFID temperature sensors.
VAI, though, coined the term  to describe RuBee. That's
visibility as in asset visibility: somewhat different from Bluetooth or RFID,
RuBee as a protocol is explicitly designed for situations where you need to
"keep tabs" on a number of different objects. Despite the overlap with other
types of wireless communications, the set of requirements on a visibility network
have lead RuBee down a very different technical path.Visibility networks have to be highly reliable. When you are trying to keep
track of an asset, a failure to communicate with it represents a fundamental
failure of the system. For visibility networks, the ability to actually convey
a payload is secondary: the main function is just reliably detecting that
endpoints exist. Visibility networks have this in common with RFID, and indeed,
despite its similarities to technologies like BLE RuBee is positioned mostly as
a competitor to technologies like UHF RFID.There are several differences between RuBee and RFID; for example, RuBee uses
active (battery-powered) tags and the tags are generally powered by a complete
4-bit microcontroller. That doesn't necessarily sound like an advantage, though.
While RuBee tags advertise a battery life of "5-25 years", the need for a battery seems
mostly like a liability. The real feature is what active tags enable: RuBee
operates in the low frequency (LF) band, typically at 131 kHz.At that low frequency, the wavelength is very long, about 2.5 km. With such a
long wavelength, RuBee communications all happen at much less than one wavelength
in range. RF engineers refer to this as near-field operation, and it has some
properties that are intriguingly different from more typical far-field RF
communications. In the near-field, the magnetic field created by the antenna is
more significant than the electrical field. RuBee devices are intentionally
designed to emit very little electrical RF signal. Communications within a RuBee network are
achieved through magnetic, not electrical fields. That's the core of RuBee's magic.The idea of magnetic coupling is not unique to RuBee. Speaking of the near-field,
there's an obvious comparison to NFC which works much the same way. The main difference,
besides the very different logical protocols, is that NFC operates at 13.56 MHz.
At this higher frequency, the wavelength is only around 20 meters. The requirement
that near-field devices be much closer than a full wavelength leads naturally to
NFC's very short range, typically specified as 4 cm.At LF frequencies, RuBee can achieve magnetic coupling at ranges up to about 30
meters. That's a range comparable to, and often much better than, RFID inventory
tracking technologies. Improved range isn't RuBee's only benefit over RFID. The
properties of magnetic fields also make it a more robust protocol. RuBee promises
significantly less vulnerability to shielding by metal or water than RFID.There are two key scenarios where this comes up: the first is equipment stored in
metal containers or on metal shelves, or equipment that is itself metallic. In
that scenario, it's difficult to find a location for an RFID tag that won't suffer
from shielding by the container. The case of water might seem less important, but
keep in mind that people are made mostly of water. RFID reading is often unreliable
for objects carried on a person, which are likely to be shielded from the reader
by the water content of the body.These problems are not just theoretical. WalMart is a major adopter of RFID inventory
technology, and in early rollouts struggled with low successful read rates. Metal,
moisture (including damp cardboard boxes), antenna orientation, and multipath/interference
effects could cause read failure rates as high as 33% when scanning a pallet of goods.
Low read rates are mostly addressed by using RFID "portals" with multiple antennas.
Eight antennas used as an array greatly increase read rate, but at a cost of over
ten thousand dollars per portal system. Even so, WalMart seems to now target a
success rate of only 95% during bulk scanning.95% might sound pretty good, but there are a lot of visibility applications where
a failure rate of even a couple percent is unacceptable. These mostly go by the
euphemism "high value goods," which depending on your career trajectory you may
have encountered in corporate expense and property policies. High-value goods
tend to be items that are both attractive to theft and where theft has particularly
severe consequences. Classically, firearms and explosives. Throw in classified
material for good measure.I wonder if Stevens was surprised by RuBee's market trajectory. He came out of
the healthcare industry and, it seems, originally developed RuBee for cold
chain visibility... but, at least in retrospect, it's quite obvious that its
most compelling application is in the armory.Because RuBee tags are small and largely immune to shielding by metals, you
can embed them directly in the frames of firearms, or as an aftermarket
modification you can mill out some space under the grip. RuBee tags in
weapons will read reliably when they are stored in metal cases or on
metal shelving, as is often the case. They will even read reliably when a
weapon is carried holstered, close to a person's body.Since RuBee tags incorporate an active microcontroller, there are even more
possibilities. Temperature logging is one thing, but firearm-embedded RuBee
tags can incorporate an accelerometer (NIST-traceable, VAI likes to emphasize)
and actually count the rounds fired.Sidebar time: there is a long history of political hazard around "smart guns."
The term "smart gun" is mostly used more specifically for firearms that
identify their user, for example by fingerprint authentication or detection of
an RFID fob. The idea has become vague enough, though, that mention of a
firearm with any type of RFID technology embedded would probably raise the
specter of the smart gun to gun-rights advocates.Further, devices embedded in firearms that count the number of
rounds fired have been proposed for decades, if not a century, as a means of
accountability. The holder of a weapon could, in theory, be required to
positively account for every round fired. That could eliminate incidents of
unreported use of force by police, for example. In practice I think this is
less compelling than it sounds, simple counting of rounds leaves too many
opportunities to fudge the numbers and conceal real-world use of a weapon as
range training, for example.That said, the NRA has long been vehemently opposed to the incorporation of any sort of
technology into weapons that could potentially be used as a means of state
control or regulation. The concern isn't completely unfounded; the state of
New Jersey did, for a time, have legislation that would have made user-identifying
"smart guns" mandatory if they were commercially available. The result of
the NRA's strident lobbying is that no such gun has ever become commercially
available; "smart guns" have been such a political third rail that any firearms
manufacturer that dared to introduce one would probably face a boycott by most
gun stores. For better or worse, a result of the NRA's powerful political
advocacy in this area is that the concept of embedding security or accountability
technology into weapons has never been seriously pursued in the US. Even a
tentative step in that direction can produce a huge volume of critical press
for everyone involved.I bring this up because I think it explains some of why VAI seems a bit vague
and cagey about the round-counting capabilities of their tags. They position it
as purely a maintenance feature, allowing the armorer to keep accurate tabs on
the preventative maintenance schedule for each individual weapon (in armory
environments, firearm users are often expected to report how many rounds
they fired for maintenance tracking reasons). The resistance of RuBee tags
to concealment is only positioned as a deterrent to theft, although the idea
of RuBee-tagged firearms creates obvious potential for security screening.
Probably the most profitable option for VAI would be to promote RuBee-tagged
firearms as tool for enforcement of gun control laws, but this is
a political impossibility and bringing it up at all could cause significant
reputational harm, especially with the government as a key customer. The result
is marketing copy that is a bit odd, giving a set of capabilities that imply
an application that is never mentioned.VAI found an incredible niche with their arms-tracking application. Institutional
users of firearms, like the military, police, and security forces, are relatively
price-insensitive and may have strict accounting requirements. By the mid-'00s,
VAI was into the long sales cycle of proposing the technology to the military.
That wasn't entirely unsuccessful. RuBee shot-counting weapon inventory tags were
selected by the Naval Surface Warfare Center in 2010 for installation on SCAR
and M4 rifles. That contract had a five-year term, it's unclear to me if it was
renewed. Military contracting opened quite a few doors to VAI, though, and
created a commercial opportunity that they eagerly pursued.Perhaps most importantly, weapons applications required an impressive round of
safety and compatibility testing. RuBee tags have the fairly unique distinction
of military approval for direct attachment to ordnance, something called "zero
separation distance" as the tags do not require a minimum separation from
high explosives. Central to that certification are findings of intrinsic safety
of the tags (that they do not contain enough energy to trigger explosives) and
that the magnetic fields involved cannot convey enough energy to heat anything
to dangerous temperatures.That's not the only special certification that RuBee would acquire. The military
has a lot of firearms, but military procurement is infamously slow and mercurial.
Improved weapon accountability is, almost notoriously, not a priority for the
US military which has often had stolen weapons go undetected until their later
use in crime. The Navy's interest in RuBee does not seem to have translated to
more widespread military applications.Then you have police departments, probably the largest institutional owners of
firearms and a very lucrative market for technology vendors. But here we run
into the political hazard: the firearms lobby is very influential on police
departments, as are police unions which generally oppose technical accountability
measures. Besides, most police departments are fairly cash-poor and are not
likely to make a major investment in a firearms inventory system.That leaves us with institutional security forces. And there is one category
of security force that are particularly well-funded, well-equipped, and
beholden to highly R&D-driven, almost pedantic standards of performance:
the protection forces of atomic energy facilities.Protection forces at privately-operated atomic energy facilities, such as
civilian nuclear power plants, are subject to licensing and scrutiny by the
Nuclear Regulatory Commission. Things step up further at the many facilities
operated by the National Nuclear Security Administration (NNSA). Protection
forces for NNSA facilities are trained at the Department of Energy's National
Training Center, at the former Manzano Base here in Albuquerque. Concern over
adequate physical protection of NNSA facilities has lead Sandia National
Laboratories to become one of the premier centers for R&D in physical security.
Teams of scientists and engineers have applied sometimes comical scientific rigor to "guns,
gates, and guards," the traditional articulation of physical security in the
nuclear world.That scope includes the evaluation of new technology for the management of
protection forces, which is why Oak Ridge National Laboratory launched an
evaluation program for the RuBee tagging of firearms in their armory. The
white paper on this evaluation is curiously undated, but citations "retrieved 2008"
lead me to assume that the evaluation happened right around the middle of the
'00s. At the time, VAI seems to have been involved in some ultimately unsuccessful
partnership with Oracle, leading to the branding of the RuBee system as Oracle
Dot-Tag Server. The term "Dot-Tag" never occurs outside of very limited materials
around the Oracle partnership, so I'm not sure if it was Oracle branding for
RuBee or just some passing lark. In any case, Oracle's involvement seems to have
mainly just been the use of the Oracle database for tracking inventory data---which
was naturally replaced by PostgreSQL at Oak Ridge.The Oak Ridge trial apparently went well enough, and around the same time, the Pantex
Plant in Texas launched an evaluation of RuBee for tracking classified tools.
Classified tools are a tricky category, as they're often metallic and often stored
in metallic cases. During the trial period, Pantex tagged a set of sample classified
tools with RuBee tags and then transported them around the property, testing the
ability of the RuBee controllers to reliably detect them entering and exiting areas of
buildings. Simultaneously, Pantex evaluated the use of RuBee tags to track containers
of "chemical products" through the manufacturing lifecycle. Both seem to have
produced positive results.There are quite a few interesting and strange aspects of the RuBee system, a
result of its purpose-built Visibility Network nature. A RuBee controller can have
multiple antennas that it cycles through. RuBee tags remain in a deep-sleep mode
for power savings until they detect a RuBee carrier during their periodic wake
cycle. When a carrier is detected, they fully wake and listen for traffic. A
RuBee controller can send an interrogate message and any number of tags can respond,
with an interesting and novel collision detection algorithm used to ensure
reliable reading of a large number of tags.The actual RuBee protocol is quite simple, and can also be referred to as IEEE 1902.1
since the decision of VAI to put it through the standards process. Packets are
small and contain basic addressing info, but they can also contain arbitrary payload in both directions,
perfect for data loggers or sensors. RuBee tags are identified by something that VAI
oddly refers to as an "IP address," causing some confusion over whether or not VAI
uses IP over 1902.1. They don't, I am confident saying after reading a whole lot of
documents. RuBee tags, as standard, have three different 4-byte addresses. VAI refers
to these as "IP, subnet, and MAC,"  but these names are more like analogies.
Really, the "IP address" and "subnet" are both configurable arbitrary addresses,
with the former intended for unicast traffic and the latter for broadcast. For example,
you would likely give each asset a unique IP address, and use subnet addresses for
categories or item types. The subnet address allows a controller to interrogate for
every item within that category at once. The MAC address is a fixed, non-configurable
address derived from the tag's serial number. They're all written in the formats
we associate with IP networks, dotted-quad notation, as a matter of convenience.And that's about it as far as the protocol specification, besides of course the
physical details which are a 131,072 Hz carrier, 1024 Hz data clock, either ASK
or BPSK modulation. The specification also describes an interesting mode called
"clip," in which a set of multiple controllers interrogate in exact synchronization
and all tags then reply in exact synchronization. Somewhat counter-intuitively,
because of the ability of RuBee controllers to separate out multiple simultaneous
tag transmissions using an anti-collision algorithm based on random phase shifts
by each tag, this is ideal. It allows a room, say an armory, full of RuBee
controllers to rapidly interrogate the entire contents of the room. I think this
feature may have been added after the Oak Ridge trials...RuBee is quite slow, typically 1,200 baud, so inventorying a large number of assets
can take a while (Oak Ridge found that their system could only collect data on 2-7
tags per second per controller). But it's so robust that it an achieve a 100% read
rate in some very challenging scenarios. Evaluation by the DoE and the military
produced impressive results. You can read, for example, of a military experiment in
which a RuBee antenna embedded in a roadway reliably identified rifles secured in
steel containers in passing Humvees.Paradoxically, then, one of the benefits of RuBee in the military/defense context
is that it is also  to receive. Here is RuBee's most interesting trick:
somewhat oversimplified, the strength of an electrical radio signal goes as 1/r,
while the strength of a magnetic field goes as 1/r^3. RuBee equipment is optimized,
by antenna design, to produce a minimal electrical field. The result is that RuBee
tags can very reliably be contacted at short range (say, around ten feet), but are
virtually impossible to contact or even detect at ranges over a few hundred feet.
To the security-conscious buyer, this is a huge feature. RuBee tags are highly
resistant to communications or electronic intelligence collection.Consider the logical implications of tagging the military's rifles. With
conventional RFID, range is limited by the size and sensitivity of the antenna.
Particularly when tags are incidentally powered by a nearby reader, an adversary
with good equipment can detect RFID tags at very long range. VAI heavily references
a 2010 DEFCON presentation, for example, that demonstrated detection of RFID tags
at a range of 80 miles. One imagines that opportunistic detection by satellite is feasible for
a state intelligence agency. That means that your rifle asset tracking is also
revealing the movements of soldiers in the field, or at least providing a way to
detect their approach.Most RuBee tags have their transmit power reduced by configuration, so even the
maximum 100' range of the protocol is not achievable. VAI suggests that typical
RuBee tags cannot be detected by radio direction finding equipment at ranges
beyond 20', and that this range can be made shorter by further reducing transmit
power.Once again, we have caught the attention of the Department of Energy. Because of
the short range of RuBee tags, they have generally been approved as not representing
a COMSEC or TEMPEST hazard to secure facilities. And that brings us back to the
very beginning: why does the DoE use a specialized, technically interesting, and
largely unique radio protocol to fulfill such a basic function as nagging people
that have their phones? Because RuBee's security properties have allowed it to be
approved for use adjacent to and inside of secure facilities. A RuBee tag, it is
thought, cannot be turned into a listening device because the intrinsic range
limitation of magnetic coupling will make it impossible to communicate with the
tag from outside of the building. It's a lot like how infrared microphones still
see some use in secure facilities, but so much more interesting!VAI has built several different product lines around RuBee, with names like
Armory 20/20 and Shot Counting Allegro 20/20 and Store 20/20. The founder started
his career in eye health, remember. None of them are that interesting, though.
They're all pretty basic CRUD applications built around polling multiple RuBee
controllers for tags in their presence.And then there's the "Alert 20/20 DoorGuard:" a metal pedestal with a RuBee
controller and audio announcement module, perfect for detecting government
cell phones.I put a lot of time into writing this, and I hope that you enjoy reading
it. If you can spare a few dollars, consider supporting me on
ko-fi. You'll receive an occasional extra,
subscribers-only post, and defray the costs of providing artisanal, hand-built
world wide web directly from Albuquerque, New Mexico.One of the strangest things about RuBee is that it's hard to tell if it's still
a going concern. VAI's website has a press release section, where nothing has been
posted since 2019. The whole website feels like it was last revised even longer
ago. When RuBee was newer, back in the '00s, a lot of industry journals covered it
with headlines like "the new RFID." I think VAI was optimistic that RuBee could
displace all kinds of asset tracking applications, but despite some special
certifications in other fields (e.g. approval to use RuBee controllers and tags
around pacemakers in surgical suites), I don't think RuBee has found much success
outside of military applications.RuBee's resistance to shielding is impressive, but RFID read rates have improved
considerably with new DSP techniques, antenna array designs, and the generally
reduced cost of modern RFID equipment. RuBee's unique advantages, its security
properties and resistance to even intentional exfiltration, are interesting but
not worth much money to buyers other than the military.So that's the fate of RuBee and VAI: defense contracting. As far as I can tell,
RuBee and VAI are about as vital as they have ever been, but RuBee is now installed
as just one part of general defense contracts around weapons systems, armory
management, and process safety and security. IEEE standardization has opened the
door to use of RuBee by federal contractors under license, and indeed, Lockheed
Martin is repeatedly named as a licensee, as are firearms manufacturers with military
contracts like Sig Sauer.Besides, RuBee continues to grow closer to the DoE. In 2021, VAI appointed Lisa
Gordon-Hagerty to it board of directors. Gordon-Hagerty was undersecretary of
Energy and had lead the NNSA until the year before. This year, the New Hampshire
Small Business Development Center wrote a glowing profile of VAI. They described
it as a 25-employee company with a goal of hitting $30 million in annual revenue in the
next two years.Despite the outdated website, VAI claims over 1,200 RuBee sites in service. I wonder
how many of those are Alert 20/20 DoorGuards? Still, I do believe there are military
weapons inventory systems currently in use. RuBee probably has a bright future, as a
niche technology for a niche industry. If nothing else, they have legacy installations
and intellectual property to lean on. A spreadsheet of VAI-owned patents on RuBee,
with nearly 200 rows, encourages would-be magnetically coupled visibility network inventors
not to go it on their own. I just wish I could get my hands on a controller....]]></content:encoded></item><item><title>Japan&apos;s gamble to turn island of Hokkaido into global chip hub</title><link>https://www.bbc.com/news/articles/c8676qpxgnqo</link><author>1659447091</author><category>hn</category><pubDate>Mon, 24 Nov 2025 03:07:07 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The island of Hokkaido has long been an agricultural powerhouse – now Japan is investing billions to turn it into a global hub for advanced semiconductors.More than half of Japan's dairy produce comes from Hokkaido, the northernmost of its main islands. In winter, it's a wonderland of ski resorts and ice-sculpture festivals; in summer, fields bloom with bands of lavender, poppies and sunflowers.These days, cranes are popping up across the island – building factories, research centres and universities focused on technology. It's part of Japan's boldest industrial push in a generation: an attempt to reboot the country's chip-making capabilities and reshape its economic future.Locals say that beyond the cattle and tourism, Hokkaido has long lacked other industries. There's even a saying that those who go there do so only to leave.But if the government succeeds in turning Hokkaido into Japan's answer to Silicon Valley - or "Hokkaido Valley", as some have begun to call it - the country could become a new contender in the $600bn (£458bn) race to supply the world's computer chips.At the heart of the plan is Rapidus, a little-known company backed by the government and some of Japan's biggest corporations including Toyota, Softbank and Sony.Born out of a partnership with IBM, it has raised billions of dollars to build Japan's first cutting-edge chip foundry in decades.The government has invested $12bn in the company, so that it can build a massive semiconductor factory or "fab" in the small city of Chitose.In selecting the Hokkaido location, Rapidus CEO Atsuyoshi Koike points to Chitose's water, electricity infrastructure and its natural beauty. Mr Koike oversaw the fab design, which will be completely covered in grass to harmonise with Hokkaido's landscape, he told the BBC.Local authorities have also flagged the region as being at lower risk of earthquakes compared to other potential sites in Japan.A key milestone for Rapidus came with the delivery of an extreme ultraviolet lithography (EUV) system from the Dutch company ASML.The high-tech machinery helped bring about Rapidus' biggest accomplishment yet earlier this year – the successful production of prototype two nanometre (2nm) transistors.These ultra-thin chips are at the cutting edge of semiconductor technology and allow devices to run faster and more efficiently.It's a feat only rival chip makers TSMC and Samsung have accomplished. Intel is not pursuing 2nm, it is leapfrogging from 7nm straight to 1.8nm."We succeeded in manufacturing the 2nm prototype for the first time in Japan, and at an unprecedented speed in Japan and globally," Mr Koike said.He credits the IBM partnership for helping achieve the breakthrough.Tie-ups with global companies are essential to acquiring the technology needed for this level of chips, he added.Rapidus is confident that it is on track to mass produce 2nm chips by 2027. The challenge will be achieving the yield and quality that is needed to survive in an incredibly competitive market – the very areas where Taiwan and South Korea have pulled ahead.TSMC for example has achieved incredible success in mass production, but making high-end chips is costly and technically demanding.In a 2024 report, the Asean+3 Macroeconomic Research Office highlighted that although Rapidus is receiving government subsidies and consortium members are contributing funds: "The financing falls short of the expected 5 trillion yen ($31.8bn; £24.4bn) needed to start mass production."The Center for Security and International Studies (CSIS) has previously said: "Rapidus has no experience in manufacturing advanced chips, and to date there is no indication that it will be able to access actual know-how for such an endeavour from companies with the requisite experience (ie TSMC and Samsung)."Finding customers may also be a challenge – Samsung and TSMC have established relationships with global companies that have been buying their chips for years.Nevertheless, Japan's government is pouring money into the chip industry - $27bn between 2020 and early 2024 - a larger commitment relative to its gross domestic product (GDP) than the US made through the Biden-era CHIPS Act.In late 2024, Tokyo unveiled a $65bn package for Artificial Intelligence (AI) and semiconductors that could further support Rapidus's expansion plans.This comes after decades of decline. Forty years ago Japan made more than half of the world's semiconductors. Today, it produces just over 10%.Many point to US-Japan trade tensions in the 1980s as a turning point.Naoyuki Yoshino, professor emeritus at Keio University, said Japan lost out in the technology stakes to Taiwan and South Korea in the 1980s, leaving domestic companies weaker.Unlike its rivals, Japan failed to sustain subsidies to keep its chipmakers competitive.But Mr Koike says that mentality has changed."The [national] government and local government are united in supporting our industry to revive once again."Japan's broader economic challenges also loom large. Its population is shrinking while the number of elderly citizens continues to surge. That has determined the national budget for years and has contributed to slowing growth.More than a third of its budget now goes to social welfare for the elderly, and that squeezes the money available for research, education and technology, Prof Yoshino says.Japan also faces a severe shortage of semiconductor engineers – an estimated 40,000 people in the coming years.Rapidus is partnering with Hokkaido University and others to train new workers, but agrees it will have to rely heavily on foreigners, at a time when public support for workers coming into the country for employment is low.The government's push is already attracting major global players.TSMC is producing 12–28nm chips in Kumamoto, on the south-western island of Kyushu - a significant step for Japan, even if it lags behind the company's cutting-edge production in Taiwan. The expansion has transformed the local economy, attracting suppliers, raising wages, and leading to infrastructure and service developments. Japan's broader chip revival strategy appears to be following a playbook: establish a "fab", and an entire ecosystem will tend to follow. TSMC started building a second plant on Kyushu in October this year, which is due to begin production by the end of 2027. Beyond Rapidus and TSMC, local players like Kioxia and Toshiba are also getting government backing.Kioxia has expanded fabs in Yokkaichi and Kitakami with state funds and Toshiba has built one in Ishikawa. Meanwhile, ROHM has been officially designated as a company that provides critical products under Tokyo's economic security framework.American memory chipmaker Micron will also receive $3.63bn in subsidies from the Japanese government to grow facilities in Hiroshima, while Samsung is building a research and development facility in Yokohama.Hokkaido is seeing similar momentum. Chipmaking equipment companies ASML and Tokyo Electron have both opened offices in Chitose, off the back of Rapidus building a production facility there."This will make a form of 'global ecosystem'," Mr Koike says, "where we work together to be able to produce semiconductors that contribute to the world."Mr Koike said Rapidus's key selling point would be - as its name suggests - an ability to produce custom chips faster than competitors, rather than competing directly with other players."TSMC leads the world, with Intel and Samsung close behind. Our edge is speed - we can produce and deliver chips three to four times faster than anyone else. That speed is what gives us an edge in the global semiconductor race," Mr Koike said.Global demand for chips is surging with the rise of AI, while Japan's automakers -  still recovering from pandemic-era supply shocks - are pressing for more reliable, domestically or regionally sourced production across the entire supply chain, from raw materials to finished chips.Securing control over chip manufacturing is being seen as a national security priority, both in Japan and elsewhere, as recent trade frictions and geopolitical tensions between China and Taiwan raise concerns around the risks of relying on foreign suppliers."We'd like to provide products from Japan once again – products that are powerful and with great new value," Mr Koike said.For Japan's government, investing in Rapidus is a high-stakes gamble to revive its semiconductor industry and more broadly its tech power.And some analysts say it may be the country's best chance to build a domestic ecosystem to supply advanced chips to its many manufacturers, and one day become a formidable challenger in the global market.Additional reporting by Jaltson Akkanath Chummar]]></content:encoded></item><item><title>The Cloudflare outage might be a good thing</title><link>https://gist.github.com/jbreckmckye/32587f2907e473dd06d68b0362fb0048</link><author>radeeyate</author><category>hn</category><pubDate>Mon, 24 Nov 2025 03:04:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Stun LLMs with thousands of invisible Unicode characters</title><link>https://gibberifier.com/</link><author>wdpatti</author><category>hn</category><pubDate>Mon, 24 Nov 2025 03:00:31 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[ Doesn't understand gibberified text - responds with confusion or completely ignores the invisible characters.See ChatGPT → Gemini can't process the invisible characters.See Gemini → Crashes or errors when encountering gibberified text - cannot process the Unicode obfuscation.Meta AI Crashes ⚠️ Completely bewildered by gibberified text - has no idea what's happening with the invisible characters.See Grok → Gets confused by the invisible Unicode characters and produces garbled or incomplete responses.See Perplexity →]]></content:encoded></item><item><title>We stopped roadmap work for a week and fixed 189 bugs</title><link>https://lalitm.com/fixits-are-good-for-the-soul/</link><author>signa11</author><category>hn</category><pubDate>Mon, 24 Nov 2025 02:36:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[It’s Friday at 4pm. I’ve just closed my 12th bug of the week. My brain is completely fried. And I’m staring at the bug leaderboard, genuinely sad that Monday means going back to regular work. Which is weird because I  regular work. But fixit weeks have a special place in my heart.Once a quarter or so, my org with ~45 software engineers stops all regular work for a week. That means no roadmap work, no design work, no meetings or standups.Instead, we fix the small things that have been annoying us and our users:an error message that’s been unclear for two yearsa weird glitch when the user scrolls and zooms at the same timea test which runs slower than it should, slowing down CI for everyoneThe rules are simple: 1) no bug should take over 2 days and 2) all work should focus on either small end-user bugs/features or developer productivity.We also have a “points system” for bugs and a leaderboard showing how many points people have. And there’s a promise of t-shirts for various achievements: first bug fix, most points, most annoying bug, etc. It’s a simple structure, but it works surprisingly well.Some stats from this fixit:4 was the median number of bugs closed per person12 was maximum number of bugs closed by one personHere are some of the highlights (sadly many people in my org work in internal-facing things so I cannot share their work!):I closed a feature request from 2021! It’s a classic fixit issue: a small improvement that never bubbled to the priority list. It took me  to implement. One day for something that sat there for . And it’s going to provide a small but significant boost to every user’s experience of Perfetto.My colleague made this small change to improve team productivity. Just ~25 lines of code in a GitHub Action to avoid every UI developer taking two extra clicks to open the CI’s build. The response from the team speaks for itself:I also fixed this issue to provide a new “amalgamated” version of our SDK, allowing it to be easily integrated into projects. It’s one of those things that might be the difference between someone deciding to use us or not, but building it took just one hour of work (with liberal use of AI!).For the product: craftsmanship and careI care deeply about any product I work on. That means asking big questions like “what should we build?” and “how do we make this fast?” But it also means asking smaller questions: “is this error message actually helpful?” or “would I be frustrated using this?”A hallmark of any good product is attention to detail: a sense that someone has thought things through, and the pieces fit together to make a cohesive whole. And the opposite is true: a product with rough edges might be tolerated if there are no alternatives, but there will always be a sense of frustration and “I wish I could use something else”.Fixits are a great chance to work on exactly those details that separate good products from great ones. The small things your average user might not consciously notice, but absolutely will notice if they’re wrong.For the individual: doing, not thinkingI sometimes miss the feeling I had earlier in my career when I got to just fix things. See something broken, fix it, ship it the same day.The more senior you get in a big company, the less you do that. Most of your time becomes thinking about what to build next, planning quarters ahead, navigating tradeoffs and getting alignment.Fixits give me that early-career feeling back. You see the bug, you fix it, you ship it, you close it, you move on. There’s something deeply satisfying about work where the question isn’t “what should we do?” but rather “can I make this better?” And you get to answer that question multiple times in a week.For the team: morale and spiritHaving 40 people across two time zones all fixing bugs together adds a whole other dimension.The vibe of the office is different: normally we’re all heads-down on different projects, but during fixit the team spirit comes out strong. People share their bug fixes in chat rooms, post before-and-after screenshots and gather around monitors to demo a new feature or complain about a particularly nasty bug they’re wrestling.The leaderboard amplifies this energy. There’s a friendly sense of competition as people try and balance quick wins with meatier bugs they can share stories about.There’s also a short update every morning about how the previous day went:how many people have fixed at least one bughow many different products we’ve fixed things inwho’s currently at the top of the leaderboardAll of this creates real momentum, and people feel magnetically pulled into the effort.I’ve participated in 6 fixits over the years and I’ve learned a lot about what makes them successful. Here are a few things that matter more than you’d think.Most of what makes a fixit work happens before the week even starts.All year round, we encourage everyone to tag bugs as “good fixit candidates” as they encounter them. Then the week before fixit, each subteam goes through these bugs and sizes them:small (less than half a day)large (less than 2 days).They assign points accordingly: 1, 2, or 4.We also create a shortlist of high-priority bugs we really want fixed. People start there and move to the full list once those are done. This pre-work is critical: it prevents wasting day one with people aimlessly searching for bugs to fix.In one of our early fixits, someone picked up what looked like a straightforward bug. It should have been a few hours, maybe half a day. But it turned into a rabbit hole. Dependencies on other systems, unexpected edge cases, code that hadn’t been touched in years.They spent the entire fixit week on it. And then the entire week after fixit trying to finish it. What started as a bug fix turned into a mini project.
The work was valuable! But they missed the whole point of a fixit. No closing bugs throughout the week. No momentum. No dopamine hits from shipping fixes. Just one long slog.That’s why we have the 2-day hard limit now. If something is ballooning, cut your losses. File a proper bug, move it to the backlog, pick something else. The limit isn’t about the work being worthless - it’s about keeping fixit feeling like fixit.We didn’t always do fixits with 40 people. Early on, this wasn’t an org-wide effort, just my subteam of 7 people. It worked okay: bugs got fixed and there was a sense of pride in making the product better. But it felt a bit hollow: in the bigger picture of our org, it didn’t feel like anyone else noticed or cared.At ~40 people, it feels like a critical mass that changes things significantly. The magic number is probably somewhere between 7 and 40. And it probably varies based on the team. But whatever the number is, the collective energy matters. If you’re trying this with 5 people, it might still be worth doing, but it probably won’t feel the same.The points and leaderboard are more than a gimmick, but they have to be handled carefully.Points are coarse, not precise: We deliberately use 1/2/4 points instead of trying to measure exact effort; the goal is “roughly right and fun”, not accurate performance evaluation.Celebrate breadth, not just volume. We give t-shirts for things like “first bug fix”, “most annoying bug fixed”, not just “most points”. That keeps newer or less experienced engineers engaged. A shout-out in the daily update or an internal post often matters more than the actual t-shirt.No attachment to perf reviews. This is important: fixit scores do  feed into performance reviews. The moment they do, people will start gaming it and the good vibe will die.We’ve had very little “gaming” in practice. Social norms do a good job of keeping people honest and 40 is still small enough that there’s a sense of “loyalty to the cause” from folks.The big challenge with fixits is context switching. Constantly changing what you’re working on means constantly figuring out new parts of the codebase, thinking about new problems.AI tools have mitigated this in a big way. The code they write is less important than their ability to quickly search through relevant files and summarize what needs to change. They might be right or wrong, but having that starting point really reduces the cognitive load. And sometimes (rarely) they one-shot a fix.This docs change was a perfect example of the above: an update to our docs which catches out new contributors and AI was able to one-shot the fix.On the other hand, in my record page change it was more useful for giving me prototypes of what the code should look like and I had to put in significant effort to correct the bad UX it generated and its tendency to “over-generate” code. Even so, it got me to the starting line much faster.Criticisms of fixits (and why I still like them anyway)I’ve definitely come across people who question whether fixits are actually a good idea. Some of the criticisms are fair but overall I still think it’s worth it.“Isn’t this just admitting you ignore bugs the rest of the time?”To some extent, yes, this is an admission of the fact that “papercut” bugs are underweighted in importance, both by managers and engineers. It’s all too easy to tunnel on making sure a big project is successful and easier to ignore the small annoyances for users and the team.Fixits are a way of counterbalancing that somewhat and saying “actually those bugs matter too”. That’s not to say we don’t fix important bugs during regular work; we absolutely do. But fixits recognize that there should be a place for handling the “this is slightly annoying but never quite urgent enough” class of problems.The whole reason we started fixits in the first place is that we observed these bugs never get actioned. Given this, I think carving out some explicit time for it is a good thing.“Isn’t it a waste to pause roadmap work for a whole week?”It’s definitely a tradeoff. 40 engineer-weeks is a  of manpower and there’s an argument to be made it should be used for actually solving roadmap problems.But I think this underweights the importance of polish of products to users. We’ve consistently found that the product feels noticeably better afterward (including positive comments from users about things they notice!) and there’s a sense of  in having a well-functioning product.Also, many of the team productivity fixes compound (faster tests, clearer errors, smoother workflows) so the benefits carry forward well beyond the week itself.I agree that a full week might be too much for tiny teams or startups. But you can still borrow the idea in smaller chunks: a “fixit Friday” once a month, or a 2-day mini-fixit each quarter. The core idea is the same: protected, collective time to fix the stuff people complain about but no one schedules time to address.Fixits are good for the soulThe official justification for fixits is that they improve product quality and
developer productivity. And of course they do this.But the unofficial reason I love them is simpler: it just feels good to fix things. It takes me back to a simpler time, and putting thought and attention into building great products is a big part of my ethos for how software engineering should be done. I wouldn’t want to work like that all the time. But I also wouldn’t want to work somewhere that never makes time for it.]]></content:encoded></item><item><title>Ask HN: Hearing aid wearers, what&apos;s hot?</title><link>https://news.ycombinator.com/item?id=46029699</link><author>pugworthy</author><category>hn</category><pubDate>Mon, 24 Nov 2025 02:25:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[One of my Phonak Audeo 90’s (RIC) died the other day after 5 years and I’m shopping for new. What’s your go to hearing aid currently if you’ve upgraded recently or have been thinking of doing so?Moderate loss, have worn them for many years, enjoy listening to music and nature, but also need help in meetings and noisy environments.Not worried about cost and wanting to get one more good deal out of work insurance before I retire.]]></content:encoded></item><item><title>Ego, empathy, and humility at work</title><link>https://matthogg.fyi/a-unified-theory-of-ego-empathy-and-humility-at-work/</link><author>mrmatthogg</author><category>hn</category><pubDate>Mon, 24 Nov 2025 02:01:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In our daily lives empathy and humility are obvious virtues we aspire to. They keep our egos in check. Less obvious is that they’re practical skills in the workplace, too. I think, for developers and technical leaders in particular, that the absence of ego is the best way to further our careers and do great work.In the simplest of terms the ego is the characteristic of personhood that enables us to practice self-reflection, self-awareness, and accountability for the actions or decisions we take.However, the ego also motivates us to reframe our perception of the world in whatever way keeps us centered in it. Each of us is perpetually driven to justify our place in the world. This constant self-justification is like an engine that idles for our entire lives, and it requires constant fine-tuning. When it runs amok this is what we call a “big” ego.Breaking News! Developers Have Egos!I’m not thinking only of the 10x engineer stereotype, although I’ve worked with such folks in the past. Ego is more nuanced than that. Besides the most arrogant developer in the room throwing their weight around, our egos manifest in hundreds of ways that are much harder to detect.As developers we’re more susceptible to letting our egos run free. The nature of our work is so technical that to others it can seem obscure, arcane, or even magical. Sometimes we don’t do enough to actively dispel that notion—and just like that half the work of self-justification is already done for us.Very often it’s not intentional. The simplest example is the overuse of jargon and acronyms. We all do it, but as Jeremy Keith explains:Still, I get why initialisms run rampant in technical discussions. You can be sure that most discussions of particle physics would be incomprehensible to outsiders, not necessarily because of the concepts, but because of the terminology.Simply mashing a few letters together can be empowering for ourselves while being exclusionary for others. It’s an artifact—albeit a small one—of our egos. We know what the technobabble means. Our justified place in the universe is maintained.Sometimes we express our egos more deliberately. Developers have a clear tendency towards gatekeeping. For most, it’s an honest mistake. There’s a fine line between holding others to a certain expectation versus actively keeping people on the outside. When we see ourselves doing this we can correct it easily enough.Sadly there are developers who seemingly like to gatekeep. They get to feel like wizards in their towers with their dusty books and potions. But, it’s actually self-limiting. Gatekeeping by definition means you’re fixed in place and never moving, standing guard for eternity.My point is our egos can “leak” in so many ways that it takes diligence to catch it let alone correct it. The following is a short, incomplete list of typical statements we as developers might say or hear at work. If you parse them more precisely each one is an attempt at self-justification:“That’s the way we’ve always done it.”“It’s not that complicated! You just…”“Yeah, I should be able to finish this in a day.”“This legacy codebase is an absolute disaster.”“Assign it to me. Nobody else will be able to fix it.”“You can’t be a senior dev. You don’t know anything about…”“Ugh, our morning standup is so useless.”“This feature is too important to assign to the junior dev.”“We should start using this new tool in our pipeline.”“We should never use that new tool in our pipeline.”Everything Is Bigger Than YouThe ego is concerned with the self but very easily becomes something harmful in the absence of new information or context. Indeed, the ego nudges us to self-justify so much that one could argue it actively  new information when left unchecked.You may have read one of the example statements above with some familiarity and thought, “But what if I’m right?”To which I’d say: OK, but should that be your default stance? Why might you feel the need to immediately start a conversation with a self-justification? There are ways to adjust our approach, make our points, and accept new information all at the same time.In any interaction—be it a meeting, Slack thread, or water cooler conversation—we must remember that the matter at hand is bigger than us in ways we don’t yet understand.To make these concepts more actionable I find it simpler to define them in terms of the purposes they serve. Specifically…Empathy is how we .Humility is how we allow information to change our behavior.This framing also helps remind us what empathy and humility . It’s not about putting yourself in another’s shoes, as the saying goes. It’s not about being submissive or a pushover. It’s not about altruism or self-sacrifice. We can easily practice empathy and humility without it ever being at our own expense.The Pursuit Of InformationI don’t know about you but I go to work to solve problems, be creative, and build shit. I can’t think of a single instance where an unruly ego solved anything I’ve worked on. Ego just makes an existing challenge worse. Solutions require information I don’t have yet.Empathy and humility are usually top of mind during situations of pain or distress, but they’re really aspects of emotional intelligence that should be activated at all times. Once you adjust your mindset to treat them as basic tools for the  you’ll see opportunities to leverage them everywhere.Developers can apply this mindset with almost anybody they come into contact with. Fellow developers, naturally. But also less technical teammates (e.g., QAs, designers, product owners, stakeholders) who have their own unique skills and context that our success depends on. And of course our users should be at the center of every problem we’re working to solve. Lastly, even executives and upper management have some insight to offer if you dare (but only up to a certain point).“Be Curious, Not Judgmental”I’ve been waiting years for a chance to work Ted Lasso into one of my essays. Today’s the day, readers.The titular character is such an archetype for leadership that my jaw hit the floor when I first watched the show. The example Ted sets has spawned countless think pieces about leadership and management. Suffice it to say he exhibits all of my principles over the series’ 34 episodes. He’s empathy and humility sporting a mustache. He’s the absence of ego personified.I highly recommend watching the show but to get a taste this 5 minute clip is worth your time. This is the famous “darts scene”…There’s a common and derisive attitude that qualities like empathy or humility are signs of weakness. You have to get all up in your feelings. Ew! But they require enormous reserves of strength, patience, and determination. It’s those who follow their egos who are weak.Letting your ego take control is the easiest thing in the world. Just ask any toddler throwing a temper tantrum. Resisting those impulses and remaining calm, on the other hand, has been a virtue humanity has aspired to for thousands of years. As the Roman emperor and Stoic philosopher Marcus Aurelius wrote: “The nearer a man comes to a calm mind, the closer he is to strength.”You’re Neither Ted Lasso Nor A Roman EmperorThe practice of empathy, humility, and keeping your ego in check will . The feedback I’ve received the most from my coworkers is that I’m extraordinarily calm and even-keeled in any situation—even situations where I’d be right to freak out.Is that just naturally my personality? Maybe in part, but remaining calm is a choice. I’m actively choosing to favor solutions over my own ego. To my colleagues past and present I confess to you now that any time you’ve seen me calm, cool, and collected I was very likely .If this sounds like a lot of work you might be wondering if it’s worth it. I think it is. At the very least your coworkers and colleagues will like you better. That’s no small thing.In all seriousness, the positive feedback I get most about the developers I manage is when they’ve demonstrated empathy and humility while dialing back their egos. This is because they’re people we can work with—literally. Nobody wants to work with a narcissist or a rock star. Nobody is materially impressed by how many lines of code we wrote, or how fast we wrote it.When people want to work with us—or even look forward to it—that means we have trust and respect. We’ll be on proper footing for working effectively as a group to solve problems. For developers this looks like coaching a junior developer, hopping on a quick call to pair with somebody, or understanding the business value of the next backlog item. For leaders this looks like people who feel empowered to do their work, who can proactively identify issues, or who can rally and adapt when circumstances change.Anybody can do this! I can’t think of any other career advice that’s as universal as empathy and humility. Everybody is capable of, at any point in their lives, small yet impactful improvements.So remember—watch your ego and look for opportunities to leverage empathy and humility in the pursuit of information so that you can solve problems together.In my next essay on this subject I’ll get into the practical. What I like about this advice is that, while there’s much we can do, we don’t have to do it all to see some benefit. We can pick and choose and try something out. We can take your time and grow. Nobody’s perfect, not even Ted Lasso. Even if we take after a character like Roy Kent we can still call that a win. Just watch the show, OK?]]></content:encoded></item><item><title>Doge &apos;doesn&apos;t exist&apos; with eight months left on its charter</title><link>https://www.reuters.com/world/us/doge-doesnt-exist-with-eight-months-left-its-charter-2025-11-23/</link><author>the_mitsuhiko</author><category>hn</category><pubDate>Mon, 24 Nov 2025 00:05:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The fall of Labubus and the mush of modern internet trends</title><link>https://www.michigandaily.com/arts/digital-culture/the-fall-of-labubus-and-the-mush-of-modern-internet-trends/</link><author>gnabgib</author><category>hn</category><pubDate>Sun, 23 Nov 2025 23:42:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The word alone is enough to make some people break out in a cold sweat, and it’s hard to blame them. These fuzzy, diminutive creatures, complete with a stare some have described as demonic, became truly inescapable over this past summer. Everywhere I turned, from the airport to the mall to the bathroom at my below-minimum-wage job, Labubus followed, staring at me ominously from backpacks and keychains.Labubus began innocently enough, originating in 2015 from a picture book series by Kasing Lung before they were made into toys. However, after slowly gaining traction throughout 2024 and early 2025, Labubus exploded in popularity over the summer, flying off store shelves around the world. Resale prices skyrocketed as demand rose and they became harder to come across, to the point where some people instead opted to knowingly shell out money to buy fake Labubus, affectionately referred to as “Lafufus.” This traction, however, was short-lived. While in the summer you might have had to spend $200 on a Labubu you could get your hands on one, today I scroll my feed and find microinfluencers promoting genuineLabubus for $30 on TikTok Shop. They’re just not hard to find anymore. Combined with the fact that the stock price of Pop Mart — the company behind Labubus — is down nearly 35% (at the time of writing) since its peak in August 2025, it’s clear that Labubus are on the downswing. Of course, Labubus are not alone in this fall from grace; fads have always come and gone. However, for as prominent as Labubus became, they seem to have faded from popular culture abnormally fast — even for a fad. Although it might seem anomalous, a clear pattern emerges when analyzing Labubus and the other massive trends that have appeared out of nowhere as of late. In an era when people are more connected with each other via the internet and social media, trends can gain greater prevalence than ever, faster than ever. Simultaneously, in the era when most internet users consume more short-form content than anything else, our attention spans are shorter than ever, causing these trends to seemingly drop off the face of the Earth once people get bored of them.Because of this, the internet has become a tapestry of many different digital phenomena, all so massive that they define the culture while they’re around, only to evacuate our minds as quickly as they appeared when it’s time to make room for the next trend. There’s no longer one single, massive cultural moment that sticks around for years in the vein of “Gangnam Style” or slime or fidget spinners. Instead, trends from various corners of the internet coalesce into one, and we get a mush of whatever “Labubu Dubai chocolate ‘Love Island’ matchaBenson Boone moonbeam ice cream cookie” is supposed to be. It’s a complete mess.And yet, as cringeworthy as the modern internet may be, it will never go back to the way it was before. The reality is that the internet has become decentralized; rather than people staying in one gigantic, unified group with shared trends and moments like they used to, users go their separate ways, with social media algorithms providing hyper-curated content that pushes users toward smaller groups with niche shared interests. It is from all of these individual, smaller communities that the many different trends we see today seem to merge into one.But maybe this — the mess, the chaos, the mishmash — is a unifying cultural moment after all, just in a new form. The beauty of the internet has always been that people from so many different places and backgrounds are able to come together and interact with one another, so it only makes sense that our trends would follow the same pattern. Looking back on the big trends of the past, I’ve realized it’s unrealistic for everyone to come together and enjoy the same thing, especially when we constantly preach individualism and influencers push us to be ourselves. For the most part, the internet trends of before were not truly unifying; they were just bandwagons we all hopped onto for fear of missing out.But now, in this new wave of internet trends, nobody has to miss out. We are free to enjoy what we want to enjoy while experiencing the trends and culture from sides of the internet we might have never ventured into otherwise. It’s not everyone being the same that brings us together, but rather the exchange of culture, information, interests and everything in-between that is facilitated by the internet. Labubus themselves are an example of this — despite initially only being prominent in China (where Pop Mart is headquartered), small interactions between internet users from there and the rest of the world allowed the culture barrier between them to be breached, eventually making Labubus a global phenomenon, even if only for a little while.The decentralization of the internet and its trends has allowed the web to become a more unified, multicultural place — and it’s beautiful. Even if it’s a jumble, I will gladly take all of the “Labubu Dubai chocolate ‘Love Island’matcha Benson Boone moonbeam ice cream cookie” summers that the internet has to offer.]]></content:encoded></item><item><title>X&apos;s new country-of-origin feature reveals many &apos;US&apos; accounts to be foreign-run</title><link>https://www.hindustantimes.com/world-news/us-news/xs-new-country-of-origin-feature-shakes-maga-and-democrat-circles-as-many-us-accounts-revealed-to-be-foreignrun-101763857104296.html</link><author>ourmandave</author><category>hn</category><pubDate>Sun, 23 Nov 2025 23:25:03 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Elon Musk's X, formerly Twitter, has introduced the country of origin feature that seems to have thrown both the MAGA and Democrats' worlds online into chaos. Several profiles online, that had pushed certain narratives are now being found to have been operating from outside the US.The social media platform introduced a feature where one can see the country the account is based in. One has to head to an account and click on the date joined tab, which opens up onto a new page. This shows the country where that particular account is being operated from. While the feature was briefly removed after its introduction, it has now been added again, and both sides of the political spectrum are having a field day, calling out each other online.What to know about ‘American’ accounts based out of USThe accounts being discussed here have pushed agendas within the US, and commented on US politics regularly. Many are also named to echo political movements, like some MAGA accounts.However, these ‘political influencers’ have been found to be based outside the US, raising questions about the motives.One profile going by 'MAGA NATION' with a follower count of over 392,000, is based out of eastern Europe. Similarly, ‘Dark Maga’ a page with over 15,000 followers is based out of Thailand. ‘MAGA Scope’ which boasts over 51,000 followers is actually operated out of Nigeria, and ‘America First’, an account with over 67,000 followers is based out of Bangladesh.“At this time thousands of MAGA-aligned influencer accounts and large political pages that claim to be based in the U.S. are now being investigated and exposed with many of them traced to India, Nigeria, and other countries,” a news aggregator page on X noted.It wasn't just on the MAGA side. An account going by ‘Ron Smith’ whose bio claims he's a ‘Proud Democrat’ and ‘Professional MAGA hunter’ is operated out of Kenya. The account has over 52,000 followers.‘Republicans against Trump’ an anti-Donald Trump page on X, which tries to push politics against MAGA, was reportedly operating out of Austria. While the location now shows US, X notes that the account location might not be accurate due to use of VPN. “The Anti-Trump account “Republicans Against Trump” which 1M followed has been identified as a non-American from Austria and is currently using a VPN to hide their location,” a page said, making note of this.‘Republicans against Trump’ has over 978,000 followers.On a side note, an account going by ‘Mariana Times’, with over 78,000 followers, which posts pro-Israel content has been found to be based out of India. People within the MAGA orbit have also reacted to this new feature. Congresswoman Anna Paulina Luna wrote on X from her personal account, “All of these pretend “pro-America” accounts that were pushing infighting within Maga are literally foreign grifters. I’m telling you, the foreign opp is real and so are the bot accounts.” Alexis Wilkins, FBI director Kash Patel's girlfriend, also added, “I hope that everyone sees, regardless of their specific reason, that the enemy is outside of the house. The people posing as Americans with big American opinions but are actually operating from a basement across the world have one common goal - to destroy the United States. We have our issues, but we really can’t allow them to succeed.”]]></content:encoded></item><item><title>Show HN: I wrote a minimal memory allocator in C</title><link>https://github.com/t9nzin/memory</link><author>t9nzin</author><category>hn</category><pubDate>Sun, 23 Nov 2025 22:25:36 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[A fun toy memory allocator (not thread safe, that's a future TODO). I also wanted to explain how I approached it, so I also wrote a tutorial blog post (~20 minute read) covering the code which you can find the link to in the README.]]></content:encoded></item><item><title>A desktop app for isolated, parallel agentic development</title><link>https://github.com/coder/mux</link><author>mercat</author><category>hn</category><pubDate>Sun, 23 Nov 2025 22:24:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Iowa City made its buses free. Traffic cleared, and so did the air</title><link>https://www.nytimes.com/2025/11/18/climate/iowa-city-free-buses.html</link><author>bookofjoe</author><category>hn</category><pubDate>Sun, 23 Nov 2025 22:06:44 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Several core problems with Rust</title><link>https://bykozy.me/blog/rust-is-a-disappointment/</link><author>byko3y</author><category>hn</category><pubDate>Sun, 23 Nov 2025 21:52:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I used to call myself a Rust hater, but really I was doing it just to compensate for the perceivable fanboyism e.g. Rust tops on stackoverflow surveys as most-loved language. There are many reasons to hate C++, and I hate C++ too. Lots of people were waiting for a better programming language, but got Rust instead.There are several core problems with Rust:Its compilation is slow. I mean SLOW. Slower than C++.  I know over years Rust became several times faster, but objectively we need it to be two orders of magnitude faster, not just two times.It’s complex. Just as complex as C++. But C++ had legacy and Rust had not. The complexity of forcing your way through the jungle of  on every single step directly impacts the quality of the logic being implemented i.e. you can’t see the forest for the trees. Once again, C++ has the same problem, so what’s the point of the language switch in the end?Memory safety is not that sacred. In fact, for many applications malfunctioning is better than crashing — particulary in the embedded world where Rust wants to be present. You cannot get 99.999% reliability with Rust — it crashes all the time.When handling lots of mutable shared state (GUI, DB, stateful services, OS/hardware), the performance of native Rust memory model is subpar, and non-native unsafes just leave you with slow compilation, high complexity, and no memory safety in the end — which makes Rust practically meaningless for heavy mutable state jobs.There is no doubt about it. Undefined behavior (UB) is a fundamental aspec of the language, you don’t simply encounter UB — the whole language is built on UB. You do array indexing and you immediately encounter UB because the language just does not check out of bound access. I want to emphasize that lots of UB-s are not even justified by performance matters — it’s an outright sloppy design of C carried over and amplified in C++. I can go all day long about how C++ sucks:implicit type conversions, implicit copies, implicit constructors, implicit object slicing, and pretty much everything implicit;function overloading (implicit), particulary considering its omnipresence in STL;non-uniform error handling with exceptions as afterthought;still #include-ing text files 40 years after C and One Definition Rule barely checked by compilers;unsound combination of paradigms (good luck overriding a generic function in descendant classes);SFINAE nuisance as a core mechanism of generic programming;T, T&, T*, std::optionalSo C++ is complex, unsafe, and it’s compiler is slow. How does the Rust (not) fix those issues?Rust FAQ explains that there are many efforts to optimize it, like better frontend, MIR, and so on. But MIR effort was started at 2015 and it still fails to significantly quicken the compilation (although it speeds up compiler checks).Unfortunately, it’s impossible to make Rust compile fast. The problem is inherent to all similar generics-heavy languages, like Haskell. Arguably, Rust is closer to Haskell than it is to C++. You can also say it’s close to a template-heavy C++ — and template-heavy C++ code exhibits the same problem of slow compilation.
When you do  — you don’t just iterate, you create a range, you create an iterator, you iterate over it — all of it monomorphized to your concrete types and have to be optimized individually (non-optimized Rust code is insanely slow, it’s mostly unusable even for debugging).
Put a non-optional borrow-checker on top of it — and there you have your insanely slow compiler. And you WILL recompile a lot, because borrow checker is relentless.You just cannot avoid it. You cannot go along like “I’m writing a cold path high-level code, I don’t need performance, I don’t need to go deeper into lifetime handling, I just want to write a high level logic”. You will be forced into the low level nuances every time you write a single line of Rust. There is no garbage collector for Rust and will never be — you will have to semi-manually pack all your data into a tree of ownership. You have to be fluent in ownership, borrowing, traits to write just a few lines of code.As I’ve already mentioned, this makes it very hard to write high-level logic in Rust. That’s why many of early Rust adopters actually revert to Node.js and Go for less performance-sensitive services — high complexity combined with slow compilation makes it impractical to write anything complex in sole Rust.There are two uncompromising things in the Rust fundament: performance and memory safety. I have to argue that Rust designers went wa-a-ay overboard with memory safety. You know the containers are actually implemented with  functions, because there is just no perfect correctness possible in the Turing machine model — you have to build safe programs from carefully arranged unsafe building blocks. Node.js and Go are considered practically safe language. Rust sacrificed sanity and practicality for memory safety — and gets none of them at the end of the day, it’s still not 100% memory safe.Now speaking about practicality — lots of use cases just don’t need perfect memory safety. There are ways to implement unsafe programs that still don’t execute remote code or leak secret — they only corrupt user data and act sporadically. If a pacemaker stops — telling a victim “but the memory was not corrupted in the crash” is a weak consolation. We actually had a recent Cloudflare outage caused by a crash on  function:https://blog.cloudflare.com/18-november-2025-outage/It’s probably the strongest point of my whining: Rust is memory safe and unreliable. The price of memory safety was reliability in addition to the developer’s sanity ­— that’s why I’m telling the language designers went overboard. They sacrificied core practical quality for an abstract principle of memory safety. Just like Haskell designers sacrificed practicality for purity — that’s why I reiterate on parallels between Rust and Haskell.It’s possible, but employing mutable shared state in Rust just makes no sense. You lose most of advantages and are left with all the deficiencies of Rust. Pretty much all of the successful Rust projects are employing shared read-only state, one way data flow, acyclic data structures: rustc compiler, mdbook and pulldown-cmark Markdown tools, Actix and Axum for stateless handlers, append-only blockchains, single-threaded WASM. The model is, once again, very similar to Haskell, which also excels in parsers, stateless handler, mostly-non-interactive CLI tools, and was employed in blockchains.Early prototypes of Rust actually had Software Transactional Memory (STM) as an option for safe concurrency, however, STM has performance penalties and it requires simple but significant runtime support.Step into the shared mutable state — and there a memory corruption is not an exception, it’s a rule. You have to handle the corruptions, you cannot simply crash. Borrow checker, ownership? Just useless, you cannot analyze ownership in a cyclic graph without GC-like algorithm.Sync/Send, Mutex and reference counting (Arc)? Unfortuantely, those lock or simply mess with CPU caches badly, so they are inefficient for multithreaded communication, at least an intensive one. They are safe, but inefficient. Which kinda destroys the first uncompromising thing in Rust — performance. So, to reiterate, the second you step into the shared mutable state you lose every single advantage of Rust. Which is kinda rational considering that the main concept of Rust was to never employ a shared mutable state.Particulary, GUI is a mutable shared state. Hence we don’t see any big GUI projects in Rust. Zed IDE is still beta for so many years — I can almost feel the pain of the developers hacking their way through the borrow checker jungle just to realize their logic is bug-ridden, and they are yet to implement dozens of other features.
Big databases, scalable stateful services, operating systems, at least significant Linux modules? Yet to be seen.So, is the Rust bad or good? It’s neither. It’s a mediocre programming language with thousands of man-month put into its development — this fact alone makes Rust a viable tool, just because you can pick it from the shelf and employ as-is. This blog was generated with Zola, written in Rust — I did not have to write a single line of Rust code to use it. And the Rust is a good fit for Zola SSG because of its non-interactive nature with one-way flow of immutable data. Just, please, don’t run around screaming “we should all switch our development to Rust because it’s the best programming language”.]]></content:encoded></item><item><title>µcad: New open source programming language that can generate 2D sketches and 3D</title><link>https://microcad.xyz/</link><author>todsacerdoti</author><category>hn</category><pubDate>Sun, 23 Nov 2025 20:51:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Welcome to the website of µcad!]]></content:encoded></item><item><title>&quot;Good engineering management&quot; is a fad</title><link>https://lethain.com/good-eng-mgmt-is-a-fad/</link><author>jkbyc</author><category>hn</category><pubDate>Sun, 23 Nov 2025 20:16:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[As I get older, I increasingly think about
whether I’m spending my time the right way
to advance my career and my life.
This is also a question that your company
asks about you every performance cycle:
is this engineering manager spending their
time effectively to advance the company or their organization?Confusingly, in my experience, answering these nominally similar questions
has surprisingly little in common.
This piece spends some time exploring both questions in the particularly
odd moment we live in today, where managers are being told they’ve
spent the last decade doing the wrong things, and need to engage
with a new model of engineering management in order to be
valued by the latest iteration of the industry.If you’d be more interested in a video version of this,
here is the recording of a practice run I gave for a talk
centered on these same ideas
(slides from talk).When I started my software career at Yahoo in the late 2000s, I had two 1:1s with my manager over the course of two
years. The first one came a few months after I started, and he mostly asked me about a colleague’s work quality.
The second came when I gave notice that I was leaving to join Digg.
A modern evaluation of this manager would be scathing, but his management style closely resembled that of the
team leader in The Soul of A New Machine:
identifying an important opportunity for the team, and navigating the broader organization that might impede progress
towards that goal.
He was, in the context we were working in, an effective manager.Compare that leadership style to the expectations of the 2010s, where attracting, retaining, and motivating engineers
was emphasized as the most important leadership criteria in many organizations.
This made sense in the era of hypergrowth, where budgets were uncapped
and many companies viewed hiring strong engineers as their constraint on growth.
This was an era where managers were explicitly told to stop writing software as the first step of their transition into management,
and it was good advice! Looking back we can argue it was bad guidance by today’s standards, but it aligned the managers with the leadership expectations
of the moment.Then think about our current era, that started in late 2022, where higher interest rates killed zero-interest-rate-policy (ZIRP)
and productized large language models are positioned as killing deep Engineering organizations.
We’ve flattened Engineering organizations where many roles that previously focused on coordination
are now expected to be hands-on keyboard, working deep in the details.
Once again, the best managers of the prior era–who did exactly what the industry asked them to do–are now reframed as bureaucrats
rather than integral leaders.In each of these transitions, the business environment shifted, leading to a new formulation of ideal leadership.
That makes a lot of sense: of course we want leaders to fit the necessary patterns of today.
Where things get weird is that in each case a morality tale was subsequently superimposed on top of the transition:In the 2010s, the morality tale was that it was all about empowering engineers as a fundamental good.
Sure, I can get excited for that, but I don’t really believe that narrative: it happened because hiring was competitive.In the 2020s, the morality tale is that bureaucratic middle management have made organizations stale and inefficient.
The lack of experts has crippled organizational efficiency.
Once again, I can get behind that–there’s truth here–but the much larger drivers aren’t about morality,
it’s about ZIRP-ending and optimism about productivity gains from AI tooling.The conclusion here is clear: the industry will want different things from you as it evolves,
and it will tell you that each of those shifts is because of some complex moral change,
but it’s pretty much always about business realities changing.
If you take any current morality tale as true, then you’re setting yourself up
to be severely out of position when the industry shifts again in a few years,
because “good leadership” is just a fad.Self-development across leadership fadsIf you accept the argument that the specifically desired leadership skills of today
are the result of fads that frequently shift, then it leads to an important followup question:
what are the right skills to develop in to be effective today and to be impactful across fads?Having been and worked with engineering managers for some time, I think there are
eight foundational engineering management skills,
which I want to personally group into two clusters: core skills that are essential to operate in all roles
(including entry-level management roles), and growth skills whose presence–or absence–determines
how far you can go in your career.: lead team to deliver expected tangible and intangible work.
Fundamentally, management is about getting things done, and you’ll neither
get an opportunity to begin managing, nor stay long as a manager, if your
teams don’t execute.: ship projects, manage on-call rotation, sprint planning, manage incidents: shape the team and the environment such that they succeed.
This is  working for the team, nor is it working for your leadership, it is
finding the balance between the two that works for both.: hiring, coaching, performance management, advocate with your management: navigate reality to make consistent progress, even when reality is difficult
Finding a way to get things done, rather than finding a way that it not getting done is someone else’s fault.: doing hard things, showing up when it’s uncomfortable, being accountable despite systemic issues: build shared understanding across leadership, stakeholders, your team, and the problem space.
Finding a realistic plan that meets the moment, without surprising or being surprised by those around you.: document and share top problems, and updates during crises: exercise discerning judgment about what “good” looks like—technically, in business terms, and in process/strategy.
Taste is a broadchurch, and my experience is that broad taste is an somewhat universal criteria for truly senior roles.
In some ways, taste is a prerequisite to Amazon’s Are Right, A Lot.: refine proposed product concept,
avoid high-risk rewrite,
find usability issues in team’s work: your team, stakeholders, and leadership know what you’re doing and why, and agree that it makes sense.
In particular, they understand how you are overcoming your biggest problems. So clarity is not, “Struggling with scalability issues”
but instead “Sharding the user logins database in a new cluster to reduce load.”: identify levers to progress,
create plan to exit a crisis,
show progress on implementing that plan: work from complex problem to opinionated, viable approach.
If you’re given an extremely messy, open-ended problem, can you still find a way to make progress?
(I’ve written previously about this topic.): launching a new business line,
improving developer experience,
going from 1 to N cloud regionsWorking across timescales: ensure your areas of responsibility make progress across both the short and long term.
There are many ways to appear successful by cutting corners today, that end in disaster tomorrow.
Success requires understanding, and being accountable for, how different timescales interact.: have an explicit destination,
ensure short-term work steers towards it,
be long-term rigid and short-term flexibleHaving spent a fair amount of time pressure testing these, I’m pretty sure most effective managers, and manager archetypes, can be fit into these boxes.Self-assessing on these skillsThere’s no perfect way to measure anything complex, but here are some thinking
questions for you to spend time with as you assess where you stand on each of these skills:When did your team last have friction delivering work? Is that a recurring issue?What’s something hard you shipped that went really, really well?When were you last pulled onto solving a time-sensitive, executive-visible project?Who was the last strong performer you hired?Have you retained your strongest performers?What strong performers want to join your team?Which peers consider your team highly effective?When did an executive describe your team as exceptional?When did you or your team overcome the odds to deliver something important? (Would your stakeholders agree?)What’s the last difficult problem you solved that stayed solved (rather than reoccurring)?When did you last solve the problem first before addressing cross-team gaps?When was the last time you were surprised by a stakeholder? What could you do to prevent that reoccuring?How does a new stakeholder understand your prioritization tradeoffs (incl rationale)?When did you last disappoint a stakeholder without damaging your relationship?What stakeholders would join your company because they trust you?What’s a recent decision that is meaningfully better because you were present?If your product counterpart left, what decisions would you struggle to make?Where’s a subtle clarification that significantly changed a design or launch?How have you inflected team’s outcomes by seeing around corners?What’s a difficult trade-off you recently helped your team make?How could you enable them to make that same trade-off without your direct participation?What’s a recent decision you made that was undone? How?What problem have you worked on that was stuck before assisted, and unstuck afterwards?Do senior leaders bring ambiguous problems to you? Why?Working across timescalesWhat’s a recent trade off you made between short and long-term priorities?How do you inform these tradeoffs across timescales?What long-term goals are you protecting at significant short-term cost?Most of these questions stand on their own, but it’s worth briefly explaining
the “Have you ever been pulled into a SpecificSortOfProject by an executive?” questions.
My experience is that in most companies, executives will try to poach you onto their most
important problems that correspond to your strengths.
So if they’re never attempting to pull you in then either you’re not considered as particularly
strong on that dimensions, or you’re already very saturated with other work such that it doesn’t
seem possible to pull you in.Are “core skills” the same over time?While those groupings of “core” and “growth” skills are obvious groupings to me,
what I came to appreciate while writing this is that some skills swap between core to growth
as the fads evolve.
Where  is a foundational skill today, it was less of a core skill in the hypergrowth era,
and even less in the investor era.This is the fundamentally tricky part of succeeding as an engineering manager across fads:
you need a sufficiently broad base across each of these skills to be successful, otherwise
you’re very likely to be viewed as a weak manager when the eras unpredictably end.Stay energized to stay engagedThe “Manage your priorities and energy” chapter in
The Engineering Executive’s Primer
captures an important reality that took me too long to understand:
the perfect allocation of work is not the mathematically ideal allocation that maximizes impact.
Instead, it’s the balance between that mathematical ideal and doing
things that energize you enough to stay motivated over the long haul.
If you’re someone who loves writing software, that might involve writing a bit more than helpful to your team.
If you’re someone who loves streamlining an organization, it might be improving a friction-filled process that is a personal affront, even if it’s not causing  overall inefficiency.Similarly to the question of prioritizing activities to stay energized, there’s also understanding
where you are in your career, an idea I explored in A forty-year career.For each role, you have the chance to prioritize across different dimensions like pace, people, prestige, profit, or learning.
There’s no “right decision,” and there are always tradeoffs.
The decisions you make early in your career will compound over the following forty years.
You also have to operate within the constraints of your life today and your possible lives tomorrow.
Early in my career, I had few responsibilities to others, and had the opportunity to work extremely hard
at places like Uber. Today, with more family responsibilities, I am unwilling to make the tradeoffs to
consistently work that way, which has real implications on how I think about which roles to prioritize
over time.Recognizing these tradeoffs, and making them deliberately, is one of the highest value things you can do
to shape your career. Most importantly, it’s extremely hard to have a career at all if you don’t think about
these dimensions and have a healthy amount of self-awareness to understand the tradeoffs that will allow you
to stay engaged over half a lifetime.Published on October 26, 2025.]]></content:encoded></item><item><title>1M Downloads of Zorin OS 18</title><link>https://blog.zorin.com/2025/11/18/test-the-upgrade-from-zorin-os-17-to-18-and-celebrating-1-million-downloads-of-zorin-os-18/</link><author>m463</author><category>hn</category><pubDate>Sun, 23 Nov 2025 19:38:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Upgrade directly and keep your existing files, apps, and settings. Available for testing today.We’re thrilled to announce that Zorin OS 18 has amassed  in just over a month since its release, breaking all previous records.The response from users, tech reviewers, and creators around the world has been incredible:“Zorin OS 18 is very polished indeed and an excellent choice for those seeking to migrate from Windows…”“All-in-all, the ZorinOS team has taken a great Linux desktop OS and made it even better.”“I really, really like Zorin Appearance… I thought this was an excellent feature for people coming over from Windows.”“Zorin OS is a standout choice for ex-Windows users… It’s one of the best free Linux variants that actually feels like Windows”What’s even more encouraging is that over 78% of these downloads came from Windows. This influx of new users reflects our mission to provide a better alternative to the incumbent PC operating systems from Big Tech.We would like to take this moment to extend a massive thank you to everyone who has downloaded, shared, and supported our biggest release ever. Your enthusiasm is what drives us to make Zorin OS even better!Upgrades from Zorin OS 17 to 18We’re also excited to announce that we’re officially launching upgrades from Zorin OS 17 to 18 today.This upgrade path is in early testing and is currently only available to users of the Zorin OS 17 Core, Education, and Pro editions. Upgrading now will allow us to collect feedback and fix bugs to improve the user experience before its full stable launch in the coming weeks.This upgrade path is designed to allow existing Zorin OS 17 users to upgrade their computers to Zorin OS 18 directly, without needing to re-install the operating system. This means you’ll be able to keep your files, apps, and settings, all while taking advantage of the new features and improvements in Zorin OS 18.How to upgrade from Zorin OS 17 to 18 (in testing) This upgrade path is not recommended for production machines yet. Upgrading during the testing period may cause stability issues or breakages on your system.Install the latest software updates by opening the Zorin Menu → System Tools → Software Updater and following the on-screen instructions.Open the Zorin Menu → Utilities → Terminal and enter this command:gsettings set com.zorin.desktop.upgrader show-test-upgrades trueFollow the instructions in stage 3 of this guide to complete the upgrade process.After the testing period is completed in the coming weeks, this upgrade option will be available to all Zorin OS 17 users through the  app. Stay tuned to our newsletter to be the first to know when upgrades are enabled for everyone.]]></content:encoded></item><item><title>Fran Sans – font inspired by San Francisco light rail displays</title><link>https://emilysneddon.com/fran-sans-essay</link><author>ChrisArchitect</author><category>hn</category><pubDate>Sun, 23 Nov 2025 18:20:27 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Fran Sans is a display font in every sense of the term. It’s an interpretation of the destination displays found on some of the light rail vehicles that service the city of San Francisco. because destination displays aren’t consistently used across the city’s transit system. In fact, SF has an unusually high number of independent public transit agencies. Unlike New York, Chicago or L.A., which each have one, maybe two, San Francisco and the greater Bay Area have over two dozen. Each agency, with its own models of buses and trains, use different destination displays, creating an eclectic patchwork of typography across the city.Among them, one display in particular has always stood out to me: the LCD panel displays inside Muni’s Breda Light Rail Vehicles. I remember first noticing them on a Saturday in October on the N-Judah, heading to the Outer Sunset for a shrimp hoagie. This context is important, as anyone who’s spent an October weekend in SF knows this is the optimal vibe to really take in the beauty of the city. 
What caught my eye was how the displays look mechanical and yet distinctly personal. Constructed on a 3×5 grid, the characters are made up of geometric modules: squares, quarter-circles, and angled forms. Combined, these modules create imperfect, almost primitive letterforms, revealing a utility and charm that feels distinctly like the San Francisco I’ve come to know.
This balance of utility and charm seems to show up everywhere in San Francisco and its history. The Golden Gate’s “International Orange” started as nothing more than a rust-proof primer, yet is now the city’s defining colour. The Painted Ladies became multicoloured icons after the 1960s Colourist movement covered decades of grey paint. Even the steepness of the streets was once an oversight in city planning but has since been romanticised in films and on postcards. So perhaps it is unsurprising that I would find this same utility and charm in a place as small and functional as a train sign.
To learn more about these displays, I visited the San Francisco Municipal Transportation Agency’s (SFMTA) Electronics Shop at Balboa Park. There, technician Armando Lumbad had set up one of the signs. They each feature one large LCD panel which displays the line name, and twenty-four smaller ones to display the destination. The loose spacing of the letters and fluorescent backlighting gives the sign a raw, analogue quality. Modern LED dot-matrix displays are far more efficient and flexible, but to me, they lack the awkwardness that makes these Breda signs so delightful.
Armando showed me how the signs work. He handed me a printed matrix table listing every line and destination, each paired with a three-digit code. On route, train operators punch the code into a control panel at the back of the display, and the LCD blocks light on specific segments of the grid to build each letter. I picked code 119, and Armando entered it for me. A few seconds later the panels revealed my own stop: the N-Judah at Church & Duboce. There in the workshop, devoid of the context of the trains and the commute, the display looked almost monolithic, or sculptural, and I have since fantasised whether it would be possible to ship one of these home to Australia.
Looking inside of the display, I found labels identifying the make and model. The signs were designed and manufactured by Trans-Lite, Inc., a company based in Milford, Connecticut that specialised in transport signage from 1959 until its acquisition by the Nordic firm Teknoware in 2012. After lots of amateur detective work, and with the help from an anonymous Reddit user in a Connecticut community group, I was connected with Gary Wallberg, Senior Engineer at Trans-Lite and the person responsible for the design of these very signs back in 1999.
Learning that the alphabet came from an engineer really explains its temperament and why I was drawn to it in the first place. The signs were designed for sufficiency: fixed segments, fixed grid, and no extras. Characters were created only as destinations required them, while other characters, like the Q, X, and much of the punctuation, were never programmed into the signs. In reducing everything to its bare essentials, somehow character emerged, and it’s what inspired me to design Fran Sans.I shared some initial drawings with Dave Foster of Foster Type who encouraged me to get the font software Glyphs and turn it into my first working font. From there, I broke down the anatomy of the letters into modules, then used them like Lego to build out a full set: uppercase A–Z, numerals, core punctuation. 
Some glyphs remain unsolved in this first version, for example the standard @ symbol refuses to squeeze politely into the 3×5 logic. Lowercase remains a question for the future, and would likely mean reconsidering the grid. But, as with the displays themselves, I am judging Fran Sans as sufficient for now.
Getting up close to these signs, you’ll notice Fran Sans’ gridlines are simplified even from its real‑life muse, but my hope is that its character remains. Specifically: the N and the zero, where the unusually thick diagonals close in on the counters; and the Z and 7, whose diagonals can feel uncomfortably thin. I’ve also noticed the centre of the M can scale strangely and read like an H at small sizes, but in fairness, this type was never designed for the kind of technical detail so many monospaced fonts aim for. Throughout the process I tried to protect these unorthodox moments, because to me, they determined the success of this interpretation.
Fran Sans comes in three styles: Solid, Tile, and Panel, each building in visual complexity. The decision to include variations, particularly the Solid style, was inspired by my time working at Christopher Doyle & Co. There, we worked with Bell Shakespeare, Australia’s national theatre company dedicated to the works of William Shakespeare. The equity of the Bell Shakespeare brand lies in its typography, which is a beautiful custom typeface called Hotspur, designed and produced by none other than Dave Foster.
Often, brand fonts are chosen or designed to convey a single feeling. Maybe it’s warmth and friendliness, or a sense of tech and innovation. But what I’ve always loved about the Bell typeface is how one weight could serve both Shakespeare’s comedies and tragedies, simply by shifting scale, spacing, or alignment. Hotspur has the gravity to carry the darkness of  and the roundness to convey the humour of . And while Fran Sans Solid is technically no Hotspur, I wanted it to share that same versatility.
Further inspiration for Fran Sans came from the Letterform Archive, the world’s leading typography archive, based in San Francisco. Librarian and archivist Kate Long Stellar thoughtfully curated a research visit filled with modular typography spanning most of the past century. On the table were two pieces that had a significant impact on Fran Sans and are now personal must-sees at the archive. First, Joan Trochut’s  “Fast Type” (1942) was created during the Second World War when resources were scarce.  gave printers the ability to draw with type, rearranging modular pieces to form letters, ornaments and even illustrations.
Second, Zuzana Licko’s process work for  (1985), an Emigre typeface, opened new ways of thinking about how ideas move between the physical and the digital and then back again. Seeing how  was documented through iterations and variations gave the typeface a depth and richness that changed my understanding of how fonts are built. At some point I want to explore physical applications for Fran Sans out of respect for its origins, since it is impossible to fully capture the display’s charm on screen.
Back at the SFMTA, Armando told me the Breda vehicles are being replaced, and with them their destination displays will be swapped for newer LED dot-matrix units that are more efficient and easier to maintain. By the end of 2025 the signs that inspired Fran Sans will disappear from the city, taking with them a small but distinctive part of the city’s voice. 
That feels like a real loss. San Francisco is always reinventing itself, yet its charm lies in how much of its history still shows through. My hope is that Fran Sans can inspire a deeper appreciation for the imperfections that give our lives and our cities character. Life is so rich when ease and efficiency are not the measure.]]></content:encoded></item><item><title>Native Secure Enclave backed SSH keys on macOS</title><link>https://gist.github.com/arianvp/5f59f1783e3eaf1a2d4cd8e952bb4acf</link><author>arianvanp</author><category>hn</category><pubDate>Sun, 23 Nov 2025 17:55:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Are consumers just tech debt to Microsoft?</title><link>https://birchtree.me/blog/are-consumers-just-tech-debt-to-microsoft/</link><author>ingve</author><category>hn</category><pubDate>Sun, 23 Nov 2025 17:14:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I’m not saying this will definitely happen, but I think we could be on the cusp of a significant shift in Windows market share for consumer computers. It is not going to drop to 2% in a year or anything, but I feel like a few pieces are coming together that could move the needle in a way we have not seen in several decades. There are three things on my mind .Number one is that Microsoft just does not feel like a consumer tech company at all anymore. Yes, they have always been much more corporate than the likes of Apple or Google, but it really shows in the last few years as they seem to only have energy for AI and web services. If you are not a customer who is a major business or a developer creating the next AI-powered app, Microsoft does not seem to care about you.I just do not see excitement there. The only thing of note they have added to Windows in the last five years is Copilot, and I have yet to meet a normal human being who enjoys using it. And all the Windows 11 changes seem to have just gone over about as well as a lead balloon. I just do not think they care at all about Windows with consumers.The second thing is the affordable MacBook rumored to be coming out in 2026. This will be a meaningfully cheaper MacBook that people can purchase at price points that many Windows computers have been hovering around for many years. Considering Apple’s focus on consumers first and a price point that can get more people in the door, it seems like that could move the needle.The third thing is gamers. Gamers use Windows largely because they have to, not because they are passionate about it. Maybe they were passionate about it in the 90s, but any passion has gone away. Now it is just the operating system they use to launch Steam. In early 2026, Valve is going to release the Steam Machine after a few years of success with the Steam Deck. We will see how they do there, but what they are doing is releasing a machine that runs Windows games on Linux. And it runs them really well. The Steam Deck has proven that over the last few years. If someone can package up a version of Linux that is optimized for gamers, then I think there is a meaningful number of PC gamers who would happily run that on their computer instead.I do not know if this is going to happen. It is always easy to be cynical and suggest everything will stay the same, and I understand that markets of this size take a long time to change. However, it just feels like there are some things happening right now that are going to move the needle, and I am excited to see what happens.]]></content:encoded></item><item><title>Calculus for Mathematicians, Computer Scientists, and Physicists [pdf]</title><link>https://mathcs.holycross.edu/~ahwang/print/calc.pdf</link><author>o4c</author><category>hn</category><pubDate>Sun, 23 Nov 2025 16:31:50 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>73% of AI startups are just prompt engineering</title><link>https://pub.towardsai.net/i-reverse-engineered-200-ai-startups-73-are-lying-a8610acab0d3</link><author>kllrnohj</author><category>hn</category><pubDate>Sun, 23 Nov 2025 16:17:57 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Mount Proton Drive on Linux using rclone and systemd</title><link>https://github.com/dadtronics/protondrive-linux</link><author>cf100clunk</author><category>hn</category><pubDate>Sun, 23 Nov 2025 16:12:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>We stopped roadmap work for a week and fixed bugs</title><link>https://lalitm.com/fixits-are-good-for-the-soul/</link><author>lalitmaganti</author><category>hn</category><pubDate>Sun, 23 Nov 2025 16:06:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[It’s Friday at 4pm. I’ve just closed my 12th bug of the week. My brain is completely fried. And I’m staring at the bug leaderboard, genuinely sad that Monday means going back to regular work. Which is weird because I  regular work. But fixit weeks have a special place in my heart.Once a quarter or so, my org with ~45 software engineers stops all regular work for a week. That means no roadmap work, no design work, no meetings or standups.Instead, we fix the small things that have been annoying us and our users:an error message that’s been unclear for two yearsa weird glitch when the user scrolls and zooms at the same timea test which runs slower than it should, slowing down CI for everyoneThe rules are simple: 1) no bug should take over 2 days and 2) all work should focus on either small end-user bugs/features or developer productivity.We also have a “points system” for bugs and a leaderboard showing how many points people have. And there’s a promise of t-shirts for various achievements: first bug fix, most points, most annoying bug, etc. It’s a simple structure, but it works surprisingly well.Some stats from this fixit:4 was the median number of bugs closed per person12 was maximum number of bugs closed by one personHere are some of the highlights (sadly many people in my org work in internal-facing things so I cannot share their work!):I closed a feature request from 2021! It’s a classic fixit issue: a small improvement that never bubbled to the priority list. It took me  to implement. One day for something that sat there for . And it’s going to provide a small but significant boost to every user’s experience of Perfetto.My colleague made this small change to improve team productivity. Just ~25 lines of code in a GitHub Action to avoid every UI developer taking two extra clicks to open the CI’s build. The response from the team speaks for itself:I also fixed this issue to provide a new “amalgamated” version of our SDK, allowing it to be easily integrated into projects. It’s one of those things that might be the difference between someone deciding to use us or not, but building it took just one hour of work (with liberal use of AI!).For the product: craftsmanship and careI care deeply about any product I work on. That means asking big questions like “what should we build?” and “how do we make this fast?” But it also means asking smaller questions: “is this error message actually helpful?” or “would I be frustrated using this?”A hallmark of any good product is attention to detail: a sense that someone has thought things through, and the pieces fit together to make a cohesive whole. And the opposite is true: a product with rough edges might be tolerated if there are no alternatives, but there will always be a sense of frustration and “I wish I could use something else”.Fixits are a great chance to work on exactly those details that separate good products from great ones. The small things your average user might not consciously notice, but absolutely will notice if they’re wrong.For the individual: doing, not thinkingI sometimes miss the feeling I had earlier in my career when I got to just fix things. See something broken, fix it, ship it the same day.The more senior you get in a big company, the less you do that. Most of your time becomes thinking about what to build next, planning quarters ahead, navigating tradeoffs and getting alignment.Fixits give me that early-career feeling back. You see the bug, you fix it, you ship it, you close it, you move on. There’s something deeply satisfying about work where the question isn’t “what should we do?” but rather “can I make this better?” And you get to answer that question multiple times in a week.For the team: morale and spiritHaving 40 people across two time zones all fixing bugs together adds a whole other dimension.The vibe of the office is different: normally we’re all heads-down on different projects, but during fixit the team spirit comes out strong. People share their bug fixes in chat rooms, post before-and-after screenshots and gather around monitors to demo a new feature or complain about a particularly nasty bug they’re wrestling.The leaderboard amplifies this energy. There’s a friendly sense of competition as people try and balance quick wins with meatier bugs they can share stories about.There’s also a short update every morning about how the previous day went:how many people have fixed at least one bughow many different products we’ve fixed things inwho’s currently at the top of the leaderboardAll of this creates real momentum, and people feel magnetically pulled into the effort.I’ve participated in 6 fixits over the years and I’ve learned a lot about what makes them successful. Here are a few things that matter more than you’d think.Most of what makes a fixit work happens before the week even starts.All year round, we encourage everyone to tag bugs as “good fixit candidates” as they encounter them. Then the week before fixit, each subteam goes through these bugs and sizes them:small (less than half a day)large (less than 2 days).They assign points accordingly: 1, 2, or 4.We also create a shortlist of high-priority bugs we really want fixed. People start there and move to the full list once those are done. This pre-work is critical: it prevents wasting day one with people aimlessly searching for bugs to fix.In one of our early fixits, someone picked up what looked like a straightforward bug. It should have been a few hours, maybe half a day. But it turned into a rabbit hole. Dependencies on other systems, unexpected edge cases, code that hadn’t been touched in years.They spent the entire fixit week on it. And then the entire week after fixit trying to finish it. What started as a bug fix turned into a mini project.
The work was valuable! But they missed the whole point of a fixit. No closing bugs throughout the week. No momentum. No dopamine hits from shipping fixes. Just one long slog.That’s why we have the 2-day hard limit now. If something is ballooning, cut your losses. File a proper bug, move it to the backlog, pick something else. The limit isn’t about the work being worthless - it’s about keeping fixit feeling like fixit.We didn’t always do fixits with 40 people. Early on, this wasn’t an org-wide effort, just my subteam of 7 people. It worked okay: bugs got fixed and there was a sense of pride in making the product better. But it felt a bit hollow: in the bigger picture of our org, it didn’t feel like anyone else noticed or cared.At ~40 people, it feels like a critical mass that changes things significantly. The magic number is probably somewhere between 7 and 40. And it probably varies based on the team. But whatever the number is, the collective energy matters. If you’re trying this with 5 people, it might still be worth doing, but it probably won’t feel the same.The points and leaderboard are more than a gimmick, but they have to be handled carefully.Points are coarse, not precise: We deliberately use 1/2/4 points instead of trying to measure exact effort; the goal is “roughly right and fun”, not accurate performance evaluation.Celebrate breadth, not just volume. We give t-shirts for things like “first bug fix”, “most annoying bug fixed”, not just “most points”. That keeps newer or less experienced engineers engaged. A shout-out in the daily update or an internal post often matters more than the actual t-shirt.No attachment to perf reviews. This is important: fixit scores do  feed into performance reviews. The moment they do, people will start gaming it and the good vibe will die.We’ve had very little “gaming” in practice. Social norms do a good job of keeping people honest and 40 is still small enough that there’s a sense of “loyalty to the cause” from folks.The big challenge with fixits is context switching. Constantly changing what you’re working on means constantly figuring out new parts of the codebase, thinking about new problems.AI tools have mitigated this in a big way. The code they write is less important than their ability to quickly search through relevant files and summarize what needs to change. They might be right or wrong, but having that starting point really reduces the cognitive load. And sometimes (rarely) they one-shot a fix.This docs change was a perfect example of the above: an update to our docs which catches out new contributors and AI was able to one-shot the fix.On the other hand, in my record page change it was more useful for giving me prototypes of what the code should look like and I had to put in significant effort to correct the bad UX it generated and its tendency to “over-generate” code. Even so, it got me to the starting line much faster.Criticisms of fixits (and why I still like them anyway)I’ve definitely come across people who question whether fixits are actually a good idea. Some of the criticisms are fair but overall I still think it’s worth it.“Isn’t this just admitting you ignore bugs the rest of the time?”To some extent, yes, this is an admission of the fact that “papercut” bugs are underweighted in importance, both by managers and engineers. It’s all too easy to tunnel on making sure a big project is successful and easier to ignore the small annoyances for users and the team.Fixits are a way of counterbalancing that somewhat and saying “actually those bugs matter too”. That’s not to say we don’t fix important bugs during regular work; we absolutely do. But fixits recognize that there should be a place for handling the “this is slightly annoying but never quite urgent enough” class of problems.The whole reason we started fixits in the first place is that we observed these bugs never get actioned. Given this, I think carving out some explicit time for it is a good thing.“Isn’t it a waste to pause roadmap work for a whole week?”It’s definitely a tradeoff. 40 engineer-weeks is a  of manpower and there’s an argument to be made it should be used for actually solving roadmap problems.But I think this underweights the importance of polish of products to users. We’ve consistently found that the product feels noticeably better afterward (including positive comments from users about things they notice!) and there’s a sense of  in having a well-functioning product.Also, many of the team productivity fixes compound (faster tests, clearer errors, smoother workflows) so the benefits carry forward well beyond the week itself.I agree that a full week might be too much for tiny teams or startups. But you can still borrow the idea in smaller chunks: a “fixit Friday” once a month, or a 2-day mini-fixit each quarter. The core idea is the same: protected, collective time to fix the stuff people complain about but no one schedules time to address.Fixits are good for the soulThe official justification for fixits is that they improve product quality and
developer productivity. And of course they do this.But the unofficial reason I love them is simpler: it just feels good to fix things. It takes me back to a simpler time, and putting thought and attention into building great products is a big part of my ethos for how software engineering should be done. I wouldn’t want to work like that all the time. But I also wouldn’t want to work somewhere that never makes time for it.]]></content:encoded></item><item><title>Court filings allege Meta downplayed risks to children and misled the public</title><link>https://time.com/7336204/meta-lawsuit-files-child-safety/</link><author>binning</author><category>hn</category><pubDate>Sun, 23 Nov 2025 15:18:00 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Sex trafficking on Meta platforms was both difficult to report and widely tolerated, according to a court filing unsealed Friday. In a plaintiffs’ brief filed as part of a major lawsuit against four social media companies, Instagram’s former head of safety and well-being Vaishnavi Jayakumar testified that when she joined Meta in 2020 she was shocked to learn that the company had a “17x” strike policy for accounts that reportedly engaged in the “trafficking of humans for sex.” “You could incur 16 violations for prostitution and sexual solicitation, and upon the 17th violation, your account would be suspended,” Jayakumar reportedly testified, adding that “by any measure across the industry, [it was] a very, very high strike threshold.” The plaintiffs claim that this testimony is corroborated by internal company documentation.The brief, filed by plaintiffs in the Northern District of California, alleges that Meta was aware of serious harms on its platform and engaged in a broad pattern of deceit to downplay risks to young users. According to the brief, Meta was aware that millions of adult strangers were contacting minors on its sites; that its products exacerbated mental health issues in teens; and that content related to eating disorders, suicide, and child sexual abuse was frequently detected, yet rarely removed. According to the brief, the company failed to disclose these harms to the public or to Congress, and refused to implement safety fixes that could have protected young users.  “Meta has designed social media products and platforms that it is aware are addictive to kids, and they’re aware that those addictions lead to a whole host of serious mental health issues,” says Previn Warren, the co-lead attorney for the plaintiffs in the case. “Like tobacco, this is a situation where there are dangerous products that were marketed to kids,” Warren adds. “They did it anyway, because more usage meant more profits for the company.” The following allegations against Meta come from the brief filed in an unprecedented multidistrict litigation. More than 1,800 plaintiffs—including children and parents, school districts, and state attorneys general—have joined together in a suit alleging that the parent companies behind Instagram, TikTok, Snapchat, and YouTube “relentlessly pursued a strategy of growth at all costs, recklessly ignoring the impact of their products on children’s mental and physical health,” according to their master complaint. The newly unsealed allegations about Meta are just one small part of the sprawling suit. (TIME filed a motion to intervene in the case to ensure public access to court records; the motion was denied.)The plaintiffs’ brief, first reported by TIME, purports to be based on sworn depositions of current and former Meta executives, internal communications, and company research and presentations obtained during the lawsuit’s discovery process. It includes quotes and excerpts from thousands of pages of testimony and internal company documents. TIME was not able to independently view the underlying testimony or research quoted in the brief, since those documents remain under seal.  But the brief still paints a damning picture of the company’s internal research and deliberations about issues that have long plagued its platforms. Plaintiffs claim that since 2017, Meta has aggressively pursued young users, even as its internal research suggested its social media products could be addictive and dangerous to kids. Meta employees proposed multiple ways to mitigate these harms, according to the brief, but were repeatedly blocked by executives who feared that new safety features would hamper teen engagement or user growth.“We strongly disagree with these allegations, which rely on cherry-picked quotes and misinformed opinions in an attempt to present a deliberately misleading picture," a Meta spokesperson said in a statement to TIME. "The full record will show that for over a decade, we have listened to parents, researched issues that matter most, and made real changes to protect teens – like introducing Teen Accounts with built-in protections and providing parents with controls to manage their teens’ experiences. We’re proud of the progress we’ve made and we stand by our record.”In the years since the lawsuit was filed, Meta has implemented new safety features designed to address some of the problems described by plaintiffs. In 2024, Meta unveiled Instagram Teen Accounts, which defaults any user between 13 and 18 into an account that is automatically private, limits sensitive content, turns off notifications at night, and doesn’t allow messaging from unconnected adults. “We know parents are worried about their teens having unsafe or inappropriate experiences online, and that’s why we’ve significantly reimagined the Instagram experience for tens of millions of teens with new Teen Accounts,” a Meta spokeswoman told TIME in June. “These accounts provide teens with built-in protections to automatically limit who’s contacting them and the content they’re seeing, and teens under 16 need a parent’s permission to change those settings. We also give parents oversight over their teens’ use of Instagram, with ways to see who their teens are chatting with and block them from using the app for more than 15 minutes a day, or for certain periods of time, like during school or at night.”And yet the plaintiffs’ brief suggests that Meta resisted safety changes like these for years. The brief quotes testimony from Brian Boland, Meta’s former vice president of partnerships who worked at the company for 11 years and resigned in 2020. “My feeling then and my feeling now is that they don’t meaningfully care about user safety,” he allegedly said. “It’s not something that they spend a lot of time on. It’s not something they think about. And I really think they don’t care.”After the plaintiffs’ brief was unsealed late Friday night, Meta did not immediately respond to TIME’s requests for comment. Here are some of the most notable allegations from the plaintiffs’ omnibus brief: Allegation: Meta had a high threshold for "sex trafficking" content—and no way to report child sexual contentDespite Instagram’s “zero tolerance” policy for child sexual abuse material, the platform did not offer users a simple way to report child sexual abuse content, according to the brief. Plaintiffs allege that Jayakumar raised the issue multiple times when she joined Meta in 2020, but was told it would be too difficult to address. Yet Instagram allowed users to easily report far less serious violations, like “spam,” “intellectual property violation” and “promotion of firearms,” according to plaintiffs.Jayakumar was even more shocked to learn that Instagram had a disturbingly high tolerance for sex trafficking on the platform. According to the brief, she testified that Meta had a “17x” strike policy for accounts that reportedly engaged in the “trafficking of humans for sex,” meaning it would take at least 16 reports for an account to be deleted.   “Meta never told parents, the public, or the Districts that it doesn’t delete accounts that have engaged over fifteen times in sex trafficking,” the plaintiffs wrote. A Meta spokesperson disputed this allegation to TIME, saying the company has for years removed accounts immediately if it suspects them of human trafficking or exploitation and has made it easier over time for users to report content that violates child-exploitation policies. Allegation: Meta "lied to Congress" about its knowledge of harms on the platformFor years, plaintiffs allege, Meta’s internal research had found that teenagers who frequently use Instagram and Facebook have higher rates of anxiety and depression. In late 2019, according to the brief, Meta designed a “deactivation study,” which found that users who stopped using Facebook and Instagram for a week showed lower rates of anxiety, depression, and loneliness. Meta halted the study and did not publicly disclose the results, stating that the research study was biased by the “existing media narratives around the company.” (A Meta spokesperson told TIME that the study was initially conceived as a pair of one-weeks pilots, and researchers declined to continue it because it found that the only reductions in feelings of depression, anxiety, and loneliness were among people who already believed Facebook was bad for them.)At least one Meta employee was uncomfortable with the implications of this decision: “If the results are bad and we don’t publish and they leak,” this employee wrote, according to the brief, “is it going to look like tobacco companies doing research and knowing cigs were bad and then keeping that info to themselves?”Indeed, in December 2020, when the Senate Judiciary Committee asked the company in a set of written questions whether it was “able to determine whether increased use of its platform among teenage girls has any correlation with increased signs of depression” and “increased signs of anxiety,” the company offered only a one-word answer: “No.”To the plaintiffs in the case, the implication is clear: “The company never publicly disclosed the results of its deactivation study. Instead, Meta lied to Congress about what it knew.”Allegation: The company knew Instagram was letting adult strangers connect with teenagersFor years Instagram has had a well-documented problem of adults harassing teens. Around 2019, company researchers recommended making all teen accounts private by default in order to prevent adult strangers from connecting with kids, according to the plaintiffs’ brief. Instead of implementing this recommendation, Meta asked its growth team to study the potential impact of making all teen accounts private. The growth team was pessimistic, according to the brief, and responded that the change would likely reduce engagement. By 2020, the growth team had determined that a private-by-default setting would result in a loss of 1.5 million monthly active teens a year on Instagram. The plaintiffs’ brief quotes an unnamed employee as saying: “taking away unwanted interactions… is likely to lead to a potentially untenable problem with engagement and growth.” Over the next several months, plaintiffs allege, Meta’s policy, legal, communications, privacy, and well-being teams all recommended making teen accounts private by default, arguing that the switch “will increase teen safety” and was in line with expectations from users, parents, and regulators. But Meta did not launch the feature that year. Safety researchers were dismayed, according to excerpts of an internal conversation quoted in the filing. One allegedly grumbled: “Isn’t safety the whole point of this team?” “Meta knew that placing teens into a default-private setting would have eliminated 5.4 million unwanted interactions a day,” the plaintiffs wrote. Still, Meta didn’t make the fix. Instead, inappropriate interactions between adults and kids on Instagram skyrocketed to 38 times that on Facebook Messenger, according to the brief. The launch of Instagram Reels allegedly compounded the problem. It allowed young teenagers to broadcast short videos to a wide audience, including adult strangers..An internal 2022 audit allegedly found that Instagram’s Accounts You May Follow feature recommended 1.4 million potentially inappropriate adults to teenage users in a single day. By 2023, according to the plaintiffs, Meta knew that they were recommending minors to potentially suspicious adults and vice versa. It wasn’t until 2024 that Meta rolled out default privacy settings to all teen accounts. In the four years it took the company to implement their own safety recommendations, teens experienced billions of unwanted interactions with strangers online. Inappropriate encounters between teens and adults were common enough, according to the brief, that the company had an acronym for them: “IIC,” or “inappropriate interactions with children.” A Meta spokesperson said the company has defaulted teens under 16 to private accounts since 2021, began defaulting teens under 18 into private accounts with the introduction of its Teen Accounts program, and has taken steps to protect users from online predators. Allegation: Meta aggressively targeted young usersMeta feared young users would abandon Facebook and Instagram for their competitors. Acquiring and keeping young users became a central business goal. Meta CEO Mark Zuckerberg suggested that “teen time spent be our top goal of 2017,” according to a company executive quoted in the brief. That has remained the case, plaintiffs allege; internal company documents from 2024 stated that “acquiring new teen users is mission critical to the success of Instagram.” (A Meta spokesperson said time spent on its platforms is not currently a company goal.)Meta launched a campaign to connect with school districts and paid organizations like the National Parent Teacher Association and Scholastic to conduct outreach to schools and families. Meanwhile, according to the brief, Meta used location data to push notifications to students in “school blasts,” presumably as part of an attempt to increase youth engagement during the school day. As one employee allegedly put it: “One of the things we need to optimize for is sneaking a look at your phone under your desk in the middle of Chemistry :)”.Though Meta aggressively pursued young users, it may not have known exactly how old those new users were. Whistleblower Jason Sattizahn recently testified to Congress that Meta does not reliably know the age of its users. (Meta pushed back on Sattizahn’s testimony, saying in a statement to NBC that his claims were “nonsense” and “based on selectively leaked internal documents that were picked specifically to craft a false narrative.”) In 2022, according to the plaintiffs’ brief, there were 216 million users on Meta platforms whose age was “unknown.”Federal law requires social media platforms to observe various data-privacy safeguards for users under 13, and Meta policy states that users under 13 are not allowed on its platforms. Yet the plaintiffs’ court filing claims Meta knew that children under 13 used the company’s products anyway. Internal research cited in the brief suggested there were 4 million users under 13 on Instagram in 2015; by 2018, the plaintiffs claim, Meta knew that roughly 40% of children aged 9 to 12 said they used Instagram daily.The plaintiffs allege that this was a deliberate business strategy. The brief describes a coordinated effort to acquire young users that included studying the psychology and digital behavior of “tweens” and exploring new products designed for “users as young as 5-10.” Internally, some employees expressed disgust at the attempt to target preteens. “Oh good, we’re going after <13 year olds now?” one wrote, according to the brief. “Zuck has been talking about that for a while...targeting 11 year olds feels like tobacco companies a couple decades ago (and today). Like we’re seriously saying ‘we have to hook them young’ here.”Allegation: Meta's executives initially shelved efforts to make Instagram less toxic for teensTo combat toxic “social comparison,” in 2019 Instagram CEO Adam Mosseri announced a new product feature that would “hide” likes on posts. Meta researchers had determined that hiding likes would make users “significantly less likely to feel worse about themselves,” according to the plaintiffs’ brief. The initiative was code-named Project Daisy. But after a series of tests, Meta backtracked on Project Daisy. It determined the feature was “pretty negative to FB metrics,” including ad revenue, according to the plaintiffs’ brief, which quotes an unnamed employee on the growth team insisting: “It’s a social comparison app, fucking get used to it.” A similar debate took place over the app’s beauty filters. Plaintiffs claim that an internal review concluded beauty filters exacerbated the “risk and maintenance of several mental health concerns, including body dissatisfaction, eating disorders, and body dysmorphic disorder,” and that Meta knew that “children are particularly vulnerable.” Meta banned beauty filters in 2019, only to roll them back out the following year after the company realized that banning beauty filters would have a “negative growth impact,” according to the plaintiffs’ brief. Other company researchers allegedly built an AI “classifier” to identify content that would lead to negative appearance comparison, so that Meta could avoid recommending it to vulnerable kids. But Mosseri allegedly killed the project, disappointing developers who “felt like they had a solution” to “a big problem.”Allegation: Meta doesn't automatically remove harmful content, including self-harm contentWhile Meta developed AI tools to monitor the platforms for harmful content, the company didn’t automatically delete that content even when it determined with “100% confidence” that it violated Meta’s policies against child sexual-abuse material or eating-disorder content. Meta’s AI classifiers did not automatically delete posts that glorified self-harm unless they were 94% certain they violated platform policy, according to the plaintiffs’ brief. As a result, most of that content remained on the platform, where teenage users often discovered it. In a 2021 internal company survey cited by plaintiffs, more than 8% of respondents aged 13 to 15 reported having seen someone harm themselves, or threaten to do so, on Instagram during the past week.A Meta spokesperson said the company reports more child sexual-abuse material than any other service and uses an array of tools to proactively find that content, including photo and video-matching technologies as well as machine learning. The spokesperson said human reviewers assess content flagged before it is deleted to ensure it violates policies, prevent mistakes that could affect users, and maintain the integrity of the company's detection databases. Allegation: Meta knew its products were addictive, but publicly downplayed the harmsThe addictive nature of the company’s products wasn’t a secret internally. “Oh my gosh yall IG is a drug,” one of the company’s user-experience researchers allegedly wrote to a colleague. “We’re basically pushers.” Meta does not officially study addiction to its products, plaintiffs allege; it studies “problematic use.” In 2018, company researchers surveyed 20,000 Facebook users in the U.S. and found that 58% had some level of “problematic use”—55% mild, and 3.1% severe. But when Meta published an account of this research the following year, only the smaller number of users with “severe” problematic use was mentioned. “We estimate (as an upper bound) that 3.1% of Facebook users in the U.S. experience problematic use,” wrote the researchers. The other 55% of users are not mentioned anywhere in the public report. Plaintiffs allege that Meta’s safety team proposed features designed to lessen addiction, only to see them set aside or watered down. One employee who helped develop a “quiet mode” feature said it was shelved because Meta was concerned that this feature would negatively impact metrics related to growth and usage.Around the same time, another user-experience researcher at Instagram allegedly recommended that Meta inform the public about its research findings: “Because our product exploits weaknesses in the human psychology to promote product engagement and time spent,” the researcher wrote, Meta needed to “alert people to the effect that the product has on their brain.” This story has been updated to reflect additional comments from Meta. ]]></content:encoded></item><item><title>Editing Code in Emacs</title><link>https://redpenguin101.github.io/html/posts/2025_11_23_emacs_for_code_editing.html</link><author>redpenguin101</author><category>hn</category><pubDate>Sun, 23 Nov 2025 14:59:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Racket v9.0</title><link>https://blog.racket-lang.org/2025/11/racket-v9-0.html</link><author>Fice</author><category>hn</category><pubDate>Sun, 23 Nov 2025 13:35:27 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[posted by Stephen De Gabrielle and John ClementsA major release is always exciting and Racket 9.0 is no exception in that it introduces Parallel Threads. While Racket has had green threads for some time, and supports parallelism via futures and places, we feel parallel threads is a major addition.The  wrapper prevents the optimizing compiler from optimizing away certain computations entirely. This can be helpful in ensuring that benchmarks are accurate.When using BC Racket, the  function is changed to always return the parallel count.We now distribute “natipkg” packages for AArch64, useful for package-build and package-testing infrastructure.Check Syntax tracks identifiers more deeply nested in the “origin” field of syntax objects.There are many other repairs and documentation improvements!The following people contributed to this release:Alexander Shopov, Anthony Carrico, Bert De Ketelaere, Bogdan Popa, Cadence Ember, David Van Horn, Gustavo Massaccesi, Jade Sailor, Jakub Zalewski, Jens Axel Søgaard, jestarray, John Clements, Jordan Johnson, Matthew Flatt, Matthias Felleisen, Mike Sperber, Philip McGrath, RMOlive, Robby Findler, Ruifeng Xie, Ryan Culpepper, Sam Phillips, Sam Tobin-Hochstadt, Sebastian Rakel, shenleban tongying, Shu-Hung You, Stephen De Gabrielle, Steve Byan, and Wing Hei Chan. is a community developed open source project and we welcome new contributors. See racket/README.md to learn how you can be a part of this amazing project.If you can - please help get the word out to users and platform specific repo packagersRacket - the Language-Oriented Programming Language - version 9.0 is now available from https://download.racket-lang.org

See https://blog.racket-lang.org/2025/11/racket-v9-0.html for the release announcement and highlights.]]></content:encoded></item><item><title>Shaders: How to draw high fidelity graphics with just x and y coordinates</title><link>https://www.makingsoftware.com/chapters/shaders</link><author>Garbage</author><category>hn</category><pubDate>Sun, 23 Nov 2025 12:26:30 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Silicon Valley startups: being evil, again and again</title><link>https://notesfrombelow.org/article/silicon-valley-startups-doing-evil-again-and-again</link><author>iSpiderman</author><category>hn</category><pubDate>Sun, 23 Nov 2025 11:32:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Google, the Silicon Valley company , proclaimed to “organise the world’s information” and adopted “don’t be evil” as its corporate motto. And what could possibly be wrong with such aspirations? Indeed, most company leaders and high-tech workers, in the Valley, genuinely believe they are “making the world a better place” while also making money.Silicon Valley is the progressive face of capitalism. We’ve experienced it, intimately. Billions of people, across the globe, enjoy real benefits from searching the world’s information, connecting with people across time and space, and carrying a supercomputers in their pockets. Silicon Valley doesn’t obviously throw workers into factories and mines for endless hours on low pay; such images are hidden, rendered distant by the supply chain . Instead, the Valley, in our imaginations, is populated by knowledge workers, a diverse and smart workforce that designs, solves, and codes in humane office spaces, with above average wages, ping pong, cafeterias, flexible working hours, sleeping cubicles. What’s not to like?We could consider the Valley’s hidden workers, those who clean, wait tables, wash cars, nanny, deliver packages, tend gardens, fix infrastructure for poverty wages; we could explore the trailer parks down the road from Google HQ, or document the shootings in the streets, the homeless that push shopping trolleys up and down the El Camino, their world a bundle of rags; or the gun shops that sell semi-automatics, the out-of-control, over-staffed police, armed with military-grade cast-me-downs, that regularly gun down the poor; the families crowded into small apartments, the father who spends his days scouring trash bins for recyclable bottles to trade for cash, the human trafficking of women, the prostitution, coerced to serve a predominately male workforce; or the disciplining and harassment of the undocumented, the deportations, the splitting up and destruction of families; or the local charities that collect food for kids, during seasonal holidays, since outside of school time their families cannot provide; or the extraordinarily high incarceration rates that control the surplus labour force, the armed guards on campuses and in schools, office complexes and shop interiors; or the poverty, the child who cannot concentrate in class since their teeth are rotting in their mouth, the extreme and devastating inequalities in wealth and income, the untreated illnesses and injuries, for lack of medical access, the widespread use of antidepressants, the addictions, the dispossessed, the broken dreams and the crying … Yet these symptoms of a broken and decaying society are studiously ignored, repressed, unmentioned by Silicon Valley’s middle and upper classes. Psychological repression is widespread amongst the highly paid of the Valley. It’s a defence: it’s just too damn painful to contemplate the full horror of social reality. And life can be organised to avoid it, or deny it. So many don’t even notice.I could mention all the evil business models, where vast computing power and fancy algorithms trawl our digital footprints to track and surveil, to sell and manipulate, to intensify our addictions; and we might reflect on the Valley’s enormous and continuing contributions to building military  machines that rain down death and destruction. Instead, here, in this article, I want to point to the root problem, the ultimate source of evil, and explain why Silicon Valley is hypocrisy on a grand scale. The Valley’s movers and shakers believe they are progressive, that entrepreneurial capitalism is the road to utopia. But it’s not. In fact, the opposite is the case: the Valley is a cause of this horror, of the social ills, and the social breakdown that it must repress: it is both responsible and culpable for creating a dystopia.I want all the talented, hopeful, optimistic high-tech workers, who genuinely want to make the world a better place, who are about to found a new and exciting startup, to just take a short pause, to stop, look around, think, reflect and reconsider: the kind of firm you incorporate, the institutional rules you adopt, is precisely the choice point between doing evil and doing good.I will try to explain. My apologies if it takes a little while, since if the following facts were readily understood and generally accepted, then Silicon Valley would already be good, not evil, and I wouldn’t need to explain.Silicon Valley: midwife of the capitalist firmThe Valley is all about startups. They spring up all the time. New exciting and hopeful adventures. It’s like forming a band, but better.In huge, mature corporations the social relations of production are obscured by layers of hierarchy and absentee ownership. But in a startup the power relations are direct and visible, and often share an office with you. You can observe the basic unit of production in capitalism—the capitalist firm—as if under a microscope.Let’s lay out the property relations of a (slightly simplified) typical startup. The startup has owner(s), usually “high net worth” individuals, either directly present or indirectly in the form of venture capital firms. Venture capital provides the initial funds for the new venture. The actual founders, the ones with the ideas and talent, but no money, also part own the firm. The divvying up of the initial share issue between founders and early investors is a detail. The founders, armed with a new injection of capital, then recruit workers by convincing them of “the vision”. These are the first employees. And off we go. The owner(s) of the startup are in complete control, and their decisions are final. Owners can promote, demote, hire and fire anyone, at any time. Owners set wage levels, which are kept secret (and the workers, being earnest and highly disciplined, avoid this subject with each other—that would break a taboo). Startups are definitely not run on democratic lines: workers don’t get to appoint managers, set strategy, or distribute profits. Instead, the startup is a mini dictatorship: sure, there’s plenty of technical debate, and back-and-forth, and head scratching, but ultimately it’s a dictatorship.The owners pays all input costs—such as office rental, computers, electricity and heating, labour etc. They own all outputs, such as new software or hardware, and any inventions (which, in the Valley, are aggressively patented). The owners are liable for any profits and losses of the company. The startup’s bank accounts are hidden from the workers, and out of their control.This basic social relation—between profit-taking capitalist owners and wage-earning workers—is constitutive of capitalism. For example, today you can travel to Shropshire, England, and visit Ironbridge village, one of the birthplaces of the industrial revolution and “the silicon valley of the 1800s”. You can tour an early ironworks and see the great furnaces where workers toiled, the tiny administrative office (to dole out wages, and keep accounts), and nearby, on a large hill overlooking the site, the capitalists’ large and imposing mansions.The form may have changed, but not the content. In this sense, Silicon Valley is deeply conservative. It proclaims to disrupt everything, and make all fresh, new and shiny—except this basic social relationship.Almost all Silicon Valley workers accept these social relations as entirely natural, acceptable, and pretty much fair and equitable. The owners fund the company. They therefore “own the firm”. The owners risk a lot of capital, and the workers receive a good wage, based on supply and demand in the marketplace, plus some ramp-up time to try and invent new stuff, which is fun. What’s not to like? What’s the problem?Silicon Valley: it’s theft all the way downStartups reproduce, in embryo form, the wage system, where the capitalist owner hires-in labour at a pre-agreed rental price. In a capitalist firm, labour is just an another  cost of production. The workers mix their labour with inputs and produce an output for sale. Normally, firms sell at prices that exceed their costs of production, which includes the cost of used-up inputs, rent, interest on capital loans, and wage costs etc. Any residual income then gets distributed as profit to the owner, or owners, of the firm.Imagine that you and I agree to exchange something in the marketplace, say a second-hand iPhone on Ebay. You get the money, I get an iPhone. You may get a good deal, or I may get a good deal, depending on our “higgling and haggling” in the marketplace. Whether a fair price is struck, or not, there’s an exchange of something for something: some goods for some money. This social transaction satisfies a “principle of exchange”. We’ve swapped stuff, and no-one forced us to.But let’s say I just took the iPhone from you. And you received no money at all. This violates the principle of exchange. I got something for nothing: some goods, some resources, for free. That’s theft. Obviously so.All startups in Silicon Valley violate the principle of exchange and institute a system of theft. They are theft-based institutions. If the startup is successful and grows, so does the theft, since theft is baked into the institutional structure of the firm, right from the get go. If the startup goes global, like an Apple, Facebook, or Google, then the cancer spreads and the theft takes place on a global scale.But the theft is hidden. We need some reflection to really see it.The workers in a startup are the actual cause of any profits it makes. We can demonstrate this with a simple thought experiment: imagine the workers stopped working. Would the firm have any possibility of making a profit, or if already profitable, continue to make that profit? But we don’t need to imagine. This is called a strike. And owners hate it, since it kills their profit.So, in any company, including a startup, the workers create the value. What, then, do the owners contribute or create?The owners, or venture capitalists, contribute capital. They advance money to fund the startup until it (hopefully) starts making money. And then they expect a return. Since they’ve given something they should definitely get something back, otherwise we violate the principle of exchange. In fact, they should expect repayment of the sums advanced and—let’s be generous here—also a risk premium (since most startups fail without making profit) and, also—in order to be really straight and fair about this—some collateral as security (such as first dibs on any outstanding assets if the venture fails). This seems like a fair exchange.But if early investors merely did this—that is, simply advance some —they would not become the owners of the firm. Once the startup made money, the loan would be repaid (by the firm), and the early investors would have no further claims on the fruits of others’ labour (that is the value created by the workers).The important point is this: loan capital does not violate the principle of exchange.But Silicon Valley startups are not funded by loan capital. Venture capitalists want, and get, much more than this. They advance capital to a firm, but in addition to being repaid, they demand ownership of the firm, i.e. , and receive stock (shares in the firm). And so they “own the firm”. In consequence, once their initial loan and risk premium is repaid, they get even more: they retain a claim on the firm’s future income, that is the fruits of others’ labour  (or until the firm dissolves, or they sell their shares etc.)And the mere legal ownership of a firm is  to lay claim on its profit. And, right here, is precisely the moment when the principle of exchange is violated. Once the firm repays its debt then the early investors are now made whole. Beyond this point, we have workers creating profits, and owners appropriating that profit without needing to contribute an hour of their time, or a dime from their pockets. The owners are getting something for nothing. The owners can, as John Stuart Mill put it: “grow richer, as it were in their sleep”. There’s no exchange. Just appropriation. And that is what’s commonly, and accurately, called economic exploitation.The important point is this: equity capital violates the principle of exchange. Sometimes the meaning of a social situation can suddenly flip. You notice something new, like an object in the wrong place, or a small inconsistency that points to a secret, or a lie. This is one of those occasions. There’s an enormous difference between  capital to a firm, and  a firm. This vital distinction is conspicuously absent from all the upbeat, world-changing, and progressive chit-chat in the coffee shops, restaurants, offices and homes of Silicon Valley. So let’s pretend they might be listening, that all their chatter stops, just for a moment, and we whisper into each and every individual ear: . When you take profits, but contribute  to the output of a firm, other than holding a paper claim, you are stealing.Yet this is how startups in Silicon Valley are organised. Cohort after cohort of “smart” groups of highly educated workers are quite happy to sign up to their own exploitation, and cede control over how they organise their working day, and what they produce. The most effective prison is the one you don’t realise you’re in.But hold on. Look, we’ve woken the libertarian consciousness of Silicon Valley, and its rationality is strong and terrible: those that were whispered to have been stirred, and they reply, in unison: No-one forces workers to accept these terms, and the wage contract is voluntary, and therefore perfectly OK! Go away, annoying socialist, and stop spoiling our fun!Silicon Valley: forcing people to sell their labourDo workers freely enter into the wage contract? Do founders, with great ideas, have the freedom to start new commercial ventures that aren’t based on theft? To answer, let’s first define non-exploitative social relations of production.In principle, Silicon Valley startups could be incorporated as profit-sharing worker cooperatives. In this type of institution, all working members share the profit, which is democratically distributed. So if you join the co-op, you get a say, and you get profit shares. If you leave, you don’t anymore. The firm is collectively owned by its current working members.Worker co-ops don’t hire in labour at a pre-agreed rental price. They do the opposite. They hire-in capital at a pre-agreed rental price (i.e., raise loan capital  equity capital). So capital, not labour, is merely another  cost of production with no claim on the residual income of the firm.In a democratic worker co-op, the principle of exchange is not violated, and no-one systematically exploits others and steals the value they (jointly) created. Clearly, this is a more lucrative deal for workers: profit-sharing is better than a wage. So why doesn’t this happen in the Valley? Why don’t lots of workers incorporate worker co-ops? There are some carrots and a stick.The carrot: join us, join us!High-tech workers, especially those with in-demand skills, get more than wages, they get stock options.A stock option bestows the right to purchase company shares at a (very low) predefined price. You have to work, normally for many years, before you can exercise that right. The aim is worker retention, and align incentives so workers are motivated to create profits “for the company”. Stock options, in a sense, automatically bestow the material benefits of trade unionism without the need to organise. Any worker is, of course, glad of this potential source of additional income.But stock options, when exercised, are equity capital and confer (part) ownership of the firm: and, as we’ve seen, that fundamentally means participating in theft. Stock options, therefore, invite a section of the working class to join the elevated ranks of the capitalist class, and start exploiting others’ labour. (In practice, most stock options turn out worthless, since most new ventures fail. But it’s the hope that motivates).In a small startup, as is common in Silicon Valley, it’s especially clear that the workers create all the added value. But the owner(s) appropriate it. So even the best educated minds start to notice. And this doesn’t seem entirely fair. So stock options function to muddy the waters, and paper over the inconvenient truth of exploitation.So that’s one carrot, which pushes high-tech workers to sign-up to a capitalist firm. But there are more. If a group of workers decide to incorporate a new venture then, unless they are highly politically conscious and especially moral creatures, they will incorporate a capitalist firm, not a worker co-op. Why? Because you’ll definitely make more money by owning a firm, paying others wages, and keeping the profit to yourself.Many startup founders in Silicon Valley know they’re ripping off the workers they employ. They might soothe their guilt by pointing out they offer stock options, or throw themselves into libertarianism, which conveniently coincides with their material interests. But it’s a fact that stock is normally heavily concentrated amongst a few early founders and venture capitalists. As time goes on, the founders contributions are eclipsed by the hired workers, and then it’s just exploitation all down the line. The company is bought-out, the founders get their initial advances and more, and therefore cash in, and make their millions. But, in almost all cases, they did not contribute to the creation of that value—they stole it from the workers they hired.So the wage contract is sweetened by stock options. That works. But the contract is only voluntary if the workers have other options, if they have a choice. Do they?The stick: reproduce capitalism or wither and dieBut there are sticks too, which remove all choice, and prevent founders from incorporating worker co-operatives, even if they were sufficiently politically conscious to want to. Silicon Valley is famous for its vibrant and extensive venture capital community. Plenty of capital swills around, continually searching, like Sauron’s great eye, for the latest hit company to fund, and therefore own and exploit the efforts and creativity of hundreds and thousands of workers.Any venture capitalist, quite naturally, wants to maximise their returns. So, if faced with a choice between funding two companies, A and B, where A is a capitalist firm, and therefore offers equity in return for capital, whereas B is a worker co-op, and only offers interest repayments in return for capital, then the venture capitalist will always opt to fund A. No contest. Equity capital is strictly a better deal than loan capital. (And it’s not just more money, but also top-down dictatorial control of the company’s direction, and the working conditions and wage levels of employees. And being powerful is much more fun than being powerless. It’s great for the ego.)Hence worker co-ops don’t get funded in Silicon Valley, and never will. So all those talented and creative workers, with good ideas for making things that people want, have no choice but to incorporate a capitalist firm, and begin sorting people into a class of owners, and a class of workers.I witnessed an especially ugly example at an Silicon Valley business conference. An “Angel Investor” (someone who provides early seed capital) presented a talk about the criteria they apply when deciding which new ventures to invest in, and therefore what aspiring founders should do in order to maximise their chances of raising capital. A big factor, for the Angel, was that founders also raise money from friends and family, since “the desire to not disappoint loved ones is a great stimulant to hard work”. The Angel gave examples of “great stories” about teams they’d funded and the great returns made “by everybody”. At the close the Angel invited any founders in the audience to come and pitch their ideas to him—right there and then.A line of about twenty or so young people formed in front of the Angel, desperate to get funding for their cherished ideas. And there it was, like a frozen scene from a perverted nativity: an anointed minority of one, with the monopoly on capital, and an unwashed majority, full of aspirations and creativity, lining up, cap-in-hand, to proffer the sacrifice of equity in their newborn, and surrender themselves to exploitation.There was no choice, there is no choice: either submit to capital or watch your ideas wither and die. There are no other practical ways to raise significant capital. Real Angels don’t exist: those that ask only for their loan to be repaid, and not ownership of the firm; who reject the social relationship of equity capital, and therefore only invest in new ventures that incorporate as democratic worker-owned co-ops, so that all profit is shared, according to actual merit and contribution. If such fabled beings were present, the line before them would have been much, much longer.There is no choice. Founders must incorporate capitalist firms, and must rent-in labour. And workers, who need income, don’t have the option to join a worker co-op. They must sign the wage contract. This isn’t voluntary, it’s coerced—by those with the monopoly on capital.Silicon Valley culture celebrates venture capital, the Schumpeterian heroic entrepreneur, the dynamism of capitalism, and the gee-whiz technical and creative ideas of startup founders. But Silicon Valley repeatedly and continually reproduces exploitation, where some members of the firm own it, and others simply rent their labour to it. There’s zero innovation or disruption in this sphere. The Valley is a great engine, churning out new companies, day after day, which reproduce the division between an economic class that wins, and an economic class that loses.Economic inequality has always been around. But notably, in the last 30 years or so, economic inequality, in the rich countries, has significantly increased. So we find people at the bottom struggling for food and shelter, while those at the top earn many years worth of the average salary while they sleep. The majority in the middle work hard yet lack savings, living their entire lives a few paychecks from destitution.Things have got so bad that even mainstream discourse has shifted to reflect the new reality. We’re routinely told that millennials face low wages, poor quality jobs, high debt, and worse economic outcomes compared to their parents. People now accept that the political system is rigged by a rich elite who’ve captured the institutions of the nation state. And even the arch conservative world of academic economics talks about inequality. And that simply didn’t happen as recently as ten years ago.Bourgeois thinkers struggle to explain the major cause of economic inequality, because to do requires thinking deeply about property relations and the issue of systematic exploitation. Instead, they prefer to think about unequal human capital endowments, taxation policies, interest and growth rates, the saving habits of workers, rising costs in child and health care, or the impact of automation. They’ll think about anything and everything except the actual reason for inequality.Capitalist firms, in an important sense, are social machines, institutions that operate within the context of a market economy to “sort” individuals into different classes by means of the wage system. This sorting produces a very specific income and wealth distribution, which is peculiar to capitalism. Empirically, capitalist societies exhibit two distinct regimes: a lognormal distribution of wage income, and a Pareto distribution of profit income.In any dynamic society, with a continual reallocation of the division of labour as new skills are demanded and other skills are automated or changed, we should expect some level of wage inequality due to mismatches between supply and demand in the labour market. Also, some jobs are terrible and dangerous, and people should get more for risking more. And some people really do contribute more within the workplace, and its OK if they get additional awards from their peers, if only to make sure they stick around. And some people actually need more, perhaps due to illness or disabilities or additional domestic responsibilities. All this is fine.But, empirically, we see more than wage inequality. We see two distinct regimes. We see a majority earning wages, at the bottom of the scale, and a minority taking profits, at the top of the scale. Capitalist societies produce extreme inequalities where the top 10% or so take a big and disproportionate slice of the social pie.And the reason is obvious, for those willing to look: the major cause of economic inequality is the wage system itself. The more workers a capitalist exploits the more profit they make. The more profit they make the more workers they can exploit. And capitalists in the super-rich bracket enjoy positive feedback effects. They can hardly lose. The economic game is entirely rigged in their favour. And, in this elevated state, they fall asleep, wake up the next morning, having earned more than workers do in their entire lifetimes.In fact, the inequality between capitalists far exceeds the inequality between workers. The super-rich become astronomically rich as we bounce along the power-law tail of the Pareto distribution. The astronomically rich capture ostensibly democratic institutions, a phenomenon that is particularly clear in the USA, so that even mild social reforms are off the table. We’ve seen a collection of post-war policies, that once mitigated economic inequality, ditched in the last thirty years or so. This is why things have got even worse. Economic exploitation has increased.Extreme economic inequality causes untold misery. At the top we see excessive and wasteful hyper-consumption. At the bottom, countless everyday struggles to live a dignified life. All the social ills of Silicon Valley, many of which are hidden in plain sight, are suffered mostly by the poorly paid, those with the least money. But inequality affects everyone. Societies with high Gini coefficients do worse on almost all measures of social well-being.But it doesn’t have to be this way. Let’s imagine an impossible event, just for the sake of illustrating a point. Imagine that all the people we whispered to—in the cafes, the offices and homes of Silicon Valley—actually listened, and decided, right there and then, to abolish exploitation, and resolved, with great determination, to only ever incorporate worker co-ops, and only lend capital, and never demand equity, then—with one decisive and unlikely step—Silicon Valley would actually begin to do good. Because it’s at this precise pivotal moment—the birth of a new productive unit—that a society’s social relations of production either get reproduced, or changed. For once you start to abolish the wage contract, and the renting of human beings, you start to abolish economic classes and extreme income inequality. Wealth would then start to be shared more equitably, and fairly, upon the principle of exchange, according to actual contributions to production, and not specious paper claims. The Pareto upper-regime would lose its material basis, would totter and fall, and therewith all the power that goes with it, the ability to capture and corrupt democracies and run them for the benefit of a privileged and undeserving few. The majority of the population would have more, enjoy more, and live better.You know, perhaps some really gifted entrepreneurs could figure out a way to export this culture, and good social outcomes, to the rest of the world. Silicon Valley: bullshit progressivismBut sadly, as of today, that is a dream. And it’s not even a dream that’s widely shared. Silicon Valley does not see the connection between the kinds of firms it funds and creates, and the kinds of social ills that surround it.But why single out Silicon Valley? What I’ve just described applies to capitalism in general.
Silicon Valley deserves especial opprobrium because the contradiction between its self-image and its reality is particularly stark. Silicon Valley desperately, desperately wants to view itself, and be seen as, socially progressive, enlightened, cutting-edge and, yes, utopian.But despite the explosion of ideas and firms, and the progressive rhetoric, the core propositions of capitalism are completely untouched, inviolate. The coupling of radical technical experimentation, with extreme conservation of capitalist property relations, has been a very successful recipe for the Valley’s elite.But the very startups that want to “change the world for the better” immediately reproduce economic exploitation: they separate human beings into a class that must rent their labour, and a class that appropriates the fruits of that labour; a class that is disciplined and must do as they are told in the workplace, and a class that disciplines and commands without democratic control. Every time an optimistic and earnest group of workers, with some great ideas, incorporate a startup and issue equity, any progressive content of those ideas are irreparably harmed.Yet this is the specific evil that Silicon Valley does: it funnels the progressive content of technical utopianism (the increase in the forces of production) into institutions that exploit workers on a global scale, and contribute to extreme economic inequality (the existing social relations of production). Silicon Valley helps produce the dystopia we live in. It doesn’t change the world for the better. It makes it worse, every single day.The narcissistic self-image of Valley culture contributes to its political backwardness. Some workers celebrate a victory when corporate HR departments commit to diversity in the workplace, or promise to address the gender pay gap. But these are easy concessions for capitalism, softballs, and the owners of your firm will happily accommodate you. Just don’t ask for bottom-up democracy in the workplace, or profit-sharing. Try it. You’ll get a very different response.But that’s what I do wish for. And I’m talking to you now, fellow workers of the Valley! If you really want to do no evil, to be good, disrupt the status quo and make the world a better place, then don’t create a capitalist firm: it’s a top-down dictatorship, where the dictators steal the money. There’s nothing progressive about this kind of social institution. Founding such a startup is deeply unethical, represents institutionalised theft, and is a prime cause of diverse social ills. Instead, use your talents to create democratic worker-owned cooperatives, based on equality among its working members; or help think of creative ways to solve the political problem of capitalists’ monopoly on capital. Abolishing economic exploitation genuinely makes the world a better place. Reproducing it in your startup does not.]]></content:encoded></item><item><title>Signal knows who you&apos;re talking to</title><link>https://sanesecurityguy.com/articles/signal-knows-who-youre-talking-to/</link><author>kekqqq</author><category>hn</category><pubDate>Sun, 23 Nov 2025 10:22:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Or, at the very least, they can.Recently I got a friend to finally join me on Signal. He asked something about whether or not Signal is truly secure and private, like if it was safe from US government surveillance. I told him: “Well, it’s end-to-end encrypted, so they don’t know  we’re talking about, but they definitely know that we’re talking to each other.”I said that because Signal uses our phone numbers as ID’s. So, Signal would know that Phone Number A is talking to Phone Number B, and if they can figure out that Phone Number A belongs to me, and Phone Number B belongs to my buddy (usually not too hard to figure out with some OSINT or the assistance of certain governments), then Signal would know that my buddy and I are talking, even if they don’t know what we’re talking about.This is a limit of end-to-end encryption, which I’ve talked about before. End-to-end encryption provides confidentiality of data, but not anonymity or protection from identifying metadata.However, I was surprised when my friend got back to me saying that, no, Signal actually doesn’t know who’s talking to who because of this feature called “Sealed Sender“.“Woah! Seriously?! Cool!” I thought. But then I started reading how Sealed Sender actually works, according to none other than Signal themselves, and I found that this feature is very technically complex, and .ʕ ಠ ᴥಠ ʔ: Woah! Seriously?! Not cool!One-way anonymity for two-way communicationsWhile Sealed Sender is pretty complicated under the hood, the result of it is one-way anonymity. That means that, when Phone Number A sends a message to Phone Number B, Signal won’t know that the message is coming from Phone Number A and will only know that the message is to be delivered to Phone Number B.It does this in a way that’s very similar to snail mail without a return address: the letter inside the mail envelope might tell the recipient who the sender is, but the mail envelope itself tells the post office only who the recipient is so that it can be delivered to them. If the post office doesn’t or can’t open the envelope to read the letter itself, then they don’t know who the sender is. Later on, when the recipient wants to send a reply to the sender, they can do the same thing.ʕ·ᴥ·ʔ: Hm, okay. This kind of sounds like it’s anonymous.Well, yes, it sort of is, but only when there’s only one message to be sent. The problem comes up when multiple messages are being sent back-and-forth like this.Sticking with the snail mail analogy, what happens when two pen pals keep sending mail to each other from their homes without including return addresses in their envelopes? The postal service might not know who exactly is sending each piece of mail but, over time, they would know that Address A in Lower Manhattan, New York, keeps on getting one-way mail from the post office in 3630 East Tremont Avenue, , New York; and  keeps on getting one-way mail from the post office in 350 Canal Street, .ʕ´•ᴥ•`ʔ: Oh. Then the postal service would be pretty sure that whoever is living at Address A and Address B are talking to each other.Exactly. That’s the limitation of one-way anonymity:  Once you start doing two-way communications, with replies going back-and-forth, then one-way anonymity is useless.With multiple messages being sent back-and-forth over time, and with Signal knowing only the recipient phone number of each message, it would be pretty hard for Signal to figure out who’s talking to who when their servers are getting thousands of messages every second from different senders, with each message being conveyed to thousands of different recipients. But, Signal doesn’t know only the recipient phone number of each message; they also know the IP address of each sender. And this is where the snail mail analogy fails, because IP addresses are  than post offices.Signal messages, as we all know, get sent over the internet, and the internet sends data around using IP addresses. Sealed Sender only protects the sender’s phone number; it does not protect the sender’s IP address. So, if you’re sending Signal messages to your super secret pen pal from your house, and you aren’t using a VPN or Tor, then Signal knows that the messages being sent to your pen pal’s phone number are coming from your house’s IP address (not a post office, your house).Even if you are using some method of masking your real IP address, you still have to use  IP address in order to communicate on the internet, and Signal will see that the same IP address keeps on sending messages to the same phone number. That’s enough to easily figure out that all of these different messages meant for the recipient are coming from the same sender. Sure, maybe you’re using the IP address of a VPN server or Tor exit node that has other Signal users sending messages at the same time, but that’s extremely unlikely. More likely: Even when you use a VPN or Tor, Signal can easily tell that every Sealed Sender message you’re sending to your pen pal are coming from one person: you.And if your pen pal replies, the reply will have his IP address on it (the same IP address Signal sent your messages to) and your phone number on it. And then, when you want to receive the reply, you have to connect to Signal’s servers using, again, your IP address (the same IP address you used to send your messages to your pen pal earlier). Just like that, with two messages, Signal figured out which phone number (yours) is talking to which other phone number (your pen pal’s). If they ever decide to try and figure out who own these two phone numbers, they could ask your telecoms, or simply search Facebook and Twitter.You can’t avoid using IP addresses on the internet; they are a necessity on the internet. But you can use a VPN or Tor to mask your real IP address with a fake one that’s not tied to your identity. But you can’t do that with phone numbers. A phone number is either tied to your identity or it isn’t; there is no masking possible, unless you use a service like MySudo which isn’t available for most of us (US and Canada only, as of this writing). If you’re fortunate enough to be able to buy a prepaid SIM without ID, then great, all you and your pen pal have to do is buy some SIM cards that aren’t tied to your identities. If buying a prepaid SIM without ID ain’t an option, then your phone number has to be tied to your identity, and Signal can use these unmasked phone numbers, in combination with masked or unmasked IP addresses, to figure out who’s talking to who, despite Sealed Sender’s promises, as long as there’s a two-way conversation going on.Which brings up an interesting question: Why does Signal require phone numbers?ʕ´•ᴥ•`ʔ: Hey, that is an interesting question…Signal works over the internet, and the internet requires IP (internet protocol) addresses in order to figure out where a message should go. But sending messages over the internet  require phone numbers; that’s a requirement when using SMS or cellular calls or mobile data, but not for using the internet. And yet, the “privacy-protecting” Signal app requires you to use a phone number to send and receive messages…It’s always a two-way streetIt gets worse. I keep repeating this: . Sealed Sender doesn’t work with . But, I’ve kind of been lying. The truth is: Signal already knows which phone numbers have been talking to which, even with Sealed Sender and only one-way communication.Do these check marks look familiar to you? (Forgive the pixelation.)ʕ·ᴥ·ʔ: Hm, yeah. Aren’t they the check marks that show up for at least a second whenever I send a Signal message? This is what’s shown after the lone check mark, and before they both turn white to indicate that my message was read, right?That’s right. The lone check mark indicates that your Signal message was sent to Signal’s servers, these two check marks above indicate that your Signal message has been delivered to the recipient, and the two white check marks indicate that the recipient has read your Signal message.Now, the thing about the two check marks above is that your Signal app only shows them when your phone has received what’s called a “delivery receipt” from the recipient’s phone. Whenever your pen pal gets a message from you, their Signal app sends a delivery receipt from their phone, through Signal’s servers, to your phone. Their Signal app does this automatically and instantly, and neither of you can turn it off. You can turn off read receipts (the two white check marks) and typing indicators, but you can’t turn off the : delivery receipts.The delivery receipt is –  – also “protected” using Sealed Sender, but what was it that I’ve been saying this whole time is wrong with Sealed Sender?ʕ·ᴥ·ʔ: It works only one-way…ʕ   • ᴥ •   ʔ: It works only one-way…ʕ   º ᴥ º   ʔ: …and the delivery receipt automatically makes it two-way.Exactly. And you can’t turn it off. Go figure why.Some alternatives and a work in progressSo if you can’t trust Signal, who can you trust? Well, if all you need is a private text-based communication channel that won’t falsely advertise their privacy guarantees to you, Proton Mail and Tutanota (now called Tuta) are pretty good. But if you want private voice-based communication, then that’s gonna’ be a problem. WhatsApp is even worse than Signal, Telegram is even worse than WhatsApp, Wire requires an email address to use it (another unnecessary requirement), and most of the rest can’t be trusted because they aren’t open-source.You could use Jitsi for voice communications, but you’d have to use a separate service for text communications. You could use Matrix for both text and voice, but that’s a software and communication protocol, so you’d have to set up your own server running it. You could use Element, which runs Matrix servers, but you’d have to trust Amazon and Cloudflare with your metadata, making this a rather messy solution to a privacy problem.What that leaves us with is a single service that is still a work in progress: SimpleX. It asks for no global identifiers like phone numbers or email addresses. It at least tries, unlike Signal, to make sure that it doesn’t know who’s talking to who. It does this with the use of proxies that you randomly send your messages through to get to your recipient (the technical details of which are too complicated to get into here). Of course it is open-source and end-to-end encrypted, otherwise I wouldn’t be mentioning it. It even goes so far as to  with it, or any SOCKS proxy. It’s pretty cool, actually; the most technically amazing communications platform I’ve ever seen.But, it ain’t perfect. It’s kinda’ slow, and messages sometimes don’t come in in the right order or don’t come in at all. Voice calls are… iffy, particularly when using Tor. It is still a young, developing project, though it has been making great strides in improving itself, including getting a security audit.Time will tell how it turns out, but at least I can say one thing: we’ve got a viable alternative.ʕ •̀ᴥ•́ ʔ: Where have you been for the past 11 months?!I actually started writing this article months ago and then got busy again.ʕ ಠ ᴥಠ ʔ: Well at least visit me with some tips and tricks every once in a while.I’ll try, buddy, but real life comes first before imaginary friends.ʕ •̀ᴥ•́ ʔ: I know I’m imaginary, but are your subscribers?I dunno’. Maybe they should give me a hint by signing up below!Or don’t; my RSS feed’s in the site menu. Unlike Signal, I don’t need you to sign up with a global identifier.]]></content:encoded></item></channel></rss>
<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>HN</title><link>https://konrad.website/feeds/</link><description></description><item><title>OpenAI Moves to Complete Potentially the Largest Theft in Human History</title><link>https://thezvi.substack.com/p/openai-moves-to-complete-potentially</link><author>paulpauper</author><category>hn</category><pubDate>Sat, 1 Nov 2025 17:25:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chat Control proposal fails again after public opposition</title><link>https://andreafortuna.org/2025/11/01/chat-control-proposal-fails-again-after-massive-public-opposition/</link><author>speckx</author><category>hn</category><pubDate>Sat, 1 Nov 2025 16:42:57 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The  has once again retreated from its controversial Chat Control proposal, a plan that would have required widespread scanning of encrypted messages. The withdrawal by the current  represents yet another chapter in a long-running battle between privacy advocates and lawmakers who believe they can compromise encryption in the name of public safety. While this latest defeat is a victory for digital rights, the fight is far from over, and the fundamental misunderstanding of encryption technology continues to plague policy discussions across Europe.A zombie proposal that refuses to dieSince its introduction in 2022, Chat Control has become what privacy advocates call a , repeatedly resurrected despite consistent opposition from civil society, technical experts, and the public. The Electronic Frontier Foundation and more than 80 civil society organizations have strongly opposed the legislation, which would mandate client-side scanning of encrypted communications under the guise of combating child sexual abuse material.The pattern has become predictable. EU lawmakers introduce the proposal, claiming it includes safeguards for privacy. Technical experts explain why those safeguards are illusory. Public pressure mounts. The proposal is withdrawn or modified. Then, after a brief hiatus, it returns with minor tweaks, and the cycle begins anew. This latest withdrawal by the  follows the same script, but the underlying issues remain unresolved.What makes this particularly frustrating is that the fundamental problem with Chat Control has never been addressed. The proposal seeks to create what privacy experts call a “backdoor” into encryption, allowing authorities to scan messages before they’re encrypted or after they’re decrypted. Proponents argue this preserves encryption while enabling content moderation, but this reveals a dangerous misunderstanding of how encryption actually works. Creating any mechanism to access encrypted content inherently weakens the entire system, making it vulnerable not just to authorized access but to malicious actors as well.The technical impossibility of “safe” scanningThe core issue with Chat Control and similar proposals lies in a fundamental misunderstanding of encryption technology. End-to-end encryption works because only the sender and recipient possess the keys to decrypt messages. Any third party, whether a government agency or a tech company, cannot read the contents. This is not a design choice but a mathematical certainty that ensures the security of billions of communications daily.Client-side scanning, the technical approach favored by Chat Control advocates, attempts to circumvent this limitation by analyzing messages on users’ devices before encryption or after decryption. While this might sound like a clever workaround, it fundamentally breaks the security model of encryption. If a device can scan and report on message content, so can malware, hackers, or authoritarian governments who might compel tech companies to expand the scope of scanning.Security researchers have repeatedly demonstrated that there is no way to create a scanning system that only works for “good guys.”  learned this lesson the hard way in 2021 when it proposed a similar system for detecting child abuse imagery in iCloud photos. The backlash from security experts was swift and devastating, forcing the company to abandon the plan. The same security vulnerabilities that would enable Chat Control would inevitably be exploited by malicious actors, putting everyone at greater risk.Moreover, the scope creep inherent in surveillance technologies is well documented. A system initially designed to detect illegal content could easily be expanded to monitor political dissent, religious expression, or any other communication governments deem problematic. Countries around the world are watching the EU’s actions closely. If Chat Control were to pass, it would set a dangerous precedent that authoritarian regimes would eagerly exploit, claiming they’re simply following Europe’s lead in implementing “reasonable” content moderation.Public pressure and the power of resistanceThe withdrawal of Chat Control demonstrates the critical importance of sustained public engagement in technology policy. Unlike previous instances where technical proposals sailed through legislative processes with little public awareness, this fight has been characterized by unprecedented mobilization from civil society organizations, technology companies, security researchers, and ordinary citizens concerned about their digital rights.Organizations like the Electronic Frontier Foundation, , and numerous national privacy advocacy groups have played a crucial role in educating the public about the risks of Chat Control. Their efforts have included detailed technical explanations, legal analysis, and coordination of opposition campaigns that have reached millions of Europeans. This groundswell of opposition has made it politically toxic for lawmakers to support the proposal, at least in its current form.The effectiveness of this resistance offers important lessons for future policy battles. First, technical expertise matters. When security researchers speak with a unified voice about the impossibility of safe backdoors, it becomes harder for politicians to dismiss concerns as alarmist. Second, coalition-building across different sectors strengthens opposition. When civil liberties groups, tech companies, and individual users all oppose a policy, it suggests the problems are real and widespread. Third, sustained pressure is essential because, as Chat Control demonstrates, bad proposals rarely die on the first attempt.However, this victory should be tempered with realism. The forces pushing for Chat Control have not given up, and the underlying political dynamics that gave rise to the proposal remain unchanged. Politicians face genuine pressure to be seen as “doing something” about online harms, particularly regarding child safety. Until alternative approaches that don’t compromise encryption gain political traction, proposals like Chat Control will continue to resurface.The path forward requires education and alternativesThe repeated resurrection of Chat Control points to a deeper problem in how technology policy is made. Many lawmakers genuinely believe they can have both strong encryption and government access to encrypted content. This belief persists despite unanimous opposition from the cryptographic community because the political incentives favor appearing tough on crime over understanding complex technical realities.Breaking this cycle requires a fundamental shift in how we approach online safety. Rather than seeking technological magic bullets that promise security without trade-offs, policymakers need to invest in solutions that actually work. This includes better funding for law enforcement training and tools that don’t require breaking encryption, improved international cooperation on criminal investigations, and addressing the root causes of online exploitation through social programs and education. also bear responsibility for developing and promoting genuinely privacy-preserving safety features. End-to-end encrypted platforms can implement abuse prevention measures that don’t involve content scanning, such as metadata analysis, user reporting systems, and account-level restrictions for suspicious behavior. While these approaches may be less comprehensive than mass surveillance, they achieve meaningful safety improvements without the catastrophic privacy trade-offs of backdoors.Looking ahead, the privacy community cannot simply celebrate the withdrawal of Chat Control and move on. The next presidency of the EU Council will bring new opportunities for the proposal to resurface in yet another modified form. Sustained vigilance, continued public education, and proactive development of alternative safety measures will be essential. The fight to protect encryption is not a single battle but an ongoing campaign that requires long-term commitment from everyone who values digital privacy and security.The withdrawal of Chat Control is a victory, but it’s a temporary one. The fundamental challenge remains: convincing policymakers that some trade-offs are not worth making, and that breaking encryption to combat illegal content creates far more problems than it solves. Until that message truly sinks in, the zombie proposal will keep rising from the grave, and the privacy community must remain ready to defeat it again and again.]]></content:encoded></item><item><title>GHC now runs in the browser</title><link>https://discourse.haskell.org/t/ghc-now-runs-in-your-browser/13169</link><author>kaycebasques</author><category>hn</category><pubDate>Sat, 1 Nov 2025 16:29:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Updated practice for review articles and position papers in ArXiv CS category</title><link>https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/</link><author>dw64</author><category>hn</category><pubDate>Sat, 1 Nov 2025 14:58:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CharlotteOS – An Experimental Modern Operating System</title><link>https://github.com/charlotte-os/Catten</link><author>ementally</author><category>hn</category><pubDate>Sat, 1 Nov 2025 13:12:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SQLite concurrency and why you should care about it</title><link>https://jellyfin.org/posts/SQLite-locking/</link><author>HunOL</author><category>hn</category><pubDate>Sat, 1 Nov 2025 12:59:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[SQLite is a powerful database engine, but due to its design, it has limitations that should not be overlooked.Jellyfin has used a SQLite-based database for storing most of its data for years, but it has also encountered issues on many systems. In this blog post, I will explain how we address these limitations and how developers using SQLite can apply the same solutions.This will be a technical blog post intended for developers and everyone wanting to learn about concurrency.Also Jellyfin's implementation of locking for SQLite should be fairly easy to be implemented into another EF Core application if you are facing the same issue.SQLite is a file-based database engine running within your application and allows you to store data in a relational structure.
Overall it gives your application the means of storing structured data as a single file and without having to depend on another application to do so.
Naturally this also comes at a price. If your application fully manages this file, the assumption must be made that your application is the sole owner of this file, and nobody else will tinker with it while you are writing data to it.So an application that wants to use SQLite as its database needs to be the only one accessing it.
Having established this fact, an important thought arises: if only a single write operation should be performed on a single file at a time, this rule must also apply to operations within the same application.SQLite has a feature that tries to get around this limitation: the Write-Ahead-Log (WAL).
The WAL is a separate file that acts as a journal of operations that should be applied to an SQLite file.
This allows multiple parallel writes to take place and get enqueued into the WAL.
When another part of the application wants to read data, it reads from the actual database, then scans the WAL for modifications and applies them on the fly.
This is not a foolproof solution; there are still scenarios where WAL does not prevent locking conflicts.A transaction is supposed to ensure two things.
Modifications made within a transaction can be reverted, either when something goes wrong or when the application decides it should and optionally a transaction may also block other readers from reading data that is modified within a transaction.
This is where it gets spicy and we come to the real reason why I am writing this blog post.
For some reason on some systems that run Jellyfin when a transaction takes place the SQLite engine reports the database is locked and instead of waiting for the transaction to be resolved the engine refuses to wait and just crashes.
This seems to be a not uncommon issue and there are many reports to be found on the issue.The factor that makes this issue so bad is that it does not happen reliably. So far we only have one team member where this can be (somewhat) reliably be reproduced which makes this an even worse a bug.
From the reports this issue happens across all operating systems, drive speeds and with or without virtualization.
So we do not have any deciding factor identified that even contributes to the likelihood of the issue happening.Having established the general theory on how SQLite behaves, we also have to look at the specifics of Jellyfins usage of SQLite.
During normal operations on a recommended setup (Non-Networked Storage and preferably SSD) its unusual for any problems to arise, however the way Jellyfin utilises the SQLite db up to 10.11 is very suboptimal.
In versions prior to 10.11 Jellyfin had a bug in its parallel task limit which resulted in exponential overscheduling of library scan operations which hammered the database engine with thousands of parallel write requests that an SQLite engine is simply not able to handle.
While most SQLite engine implementations have retry behavior, they also have timeouts and checks in place to prevent limitless waiting so if we stress the engine enough, it just fails with an error.
That and very long running and frankly unoptimized transactions could lead to the database just being overloaded with requests and flaking out.Since we moved the codebase over to EF Core proper, we have the tools to actually do something about this as EF Core gives us a structured abstraction level.
EF Core supports a way of hooking into every command execution or transaction by creating Interceptors.
With an interceptor we can finally do the straight forward idea of just "not" writing to the database in parallel in a transparent way to the caller.
The overall idea is to have multiple strategies of locking. Because all levels of synchronization will inevitably come at the cost of performance, we only want to do it when it is really necessary.
So, I decided on three locking strategies:As a default, the no-lock behavior does exactly what the name implies. Nothing. This is the default because my research shows that for 99% all of this is not an issue and every interaction at this level will slow down the whole application.Both the optimistic and pessimistic behaviors use two interceptors—one for transactions and one for commands—and override  in .Optimistic locking behavior​Optimistic locking means to assume the operation in question will succeed and only handle issues afterwards. In essence this can be boiled down to "Try and Retry and Retry ..." for a set number of times until either we succeed with the operation or fail entirely.
This still leaves the possibility that we will not actually be able to perform a write, but the introduced overhead is far less than the Pessimistic locking behavior.The idea behind how this works is simple: every time two operations try to write to the database, one will always win. The other will fail, wait some time, then retry a few times.Jellyfin uses the  library perform the retry behavior and will only retry operations it will find have been locked due to this exact issue.Pessimistic locking behavior​Pessimistic locking always locks when a write to SQLite should be performed. Essentially every time an transaction is started or a write operation on the database is done though EF Core, Jellyfin will wait until all other read operations are finished and then block all other operations may they be read or write until the write in question has been performed.
This however means, that Jellyfin can only ever perform a single write to the database, even if it would technically does not need to.In theory, an application should have no issue reading from table "Alice" while writing to table "Bob" however to eliminate all possible sources of concurrency related locking, Jellyfin will only ever allow a single write performed on its database in this mode.
While this will absolutely result in the most stable operation, it will undoubtedly also be the slowest.Jellyfin uses a ReaderWriterLockSlim to lock the operations, that means we allow an unlimited number of reads to happen concurrently while only one write may ever be done on the database.The future Smart locking behavior​In the future we might also consider combining both modes, to get the best of both worlds.Initial testing showed that with both modes, we had great success in handling the underlying issue. While we are not yet sure why this happens only on some systems when others work, we at least now have an option for users previously left out of using Jellyfin.When I was researching this topic, I found many reports all over the internet facing the same error but nobody was able to provide a conclusive explanation whats really happening here.
There have been similar proposals made to handle it but there wasn't a "ready to drop in" solution that handles all the different cases or only code that required massive modifications to every EF Core query.
Jellyfin's implementation of the locking behaviors should be a copy-paste solution for everyone having the same issues as its using interceptors and the caller has no idea of the actual locking behavior.]]></content:encoded></item><item><title>Do you know that there is an HTML tables API?</title><link>https://christianheilmann.com/2025/10/08/abandonware-of-the-web-do-you-know-that-there-is-an-html-tables-api/</link><author>begoon</author><category>hn</category><pubDate>Sat, 1 Nov 2025 12:58:21 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When people turn data into  tables using JavaScript, they either use the  methods (createElement() and the likes), but most of the time just append a huge string and use innerHTML, which always is a security concern. However, did you know that  tables also have an old, forgotten  ? Using this one, you can loop over tables, create bodies, rows, cells, heads, footers, captions an summaries (yes,  tables have all of those) and access the table cells. Without having to re-render the whole table on each change. Check out the Codepen to see how you can create a table from a nested array:let table 
let b  document.
let t  document.
b.t
table.rowri
  let r  t.ri
  row.li
    let c  r.i
    c. llet table = [
  ['one','two','three'],
  ['four','five','six']
];
let b = document.body;
let t = document.createElement('table');
b.appendChild(t);
table.forEach((row,ri) => {
  let r = t.insertRow(ri);
  row.forEach((l,i) => {
    let c = r.insertCell(i);
    c.innerText = l;  
  })
});You can then access each table cell with an index (with t being a reference to the table):console.t..console.log(t.rows[1].cells[1]);
// => <td>five</td>You can also delete and create cells and rows, if you want to add a row to the end of the table with a cell, all you need to do is:t.
t..
t...t.insertRow(-1);
t.rows[2].insertCell(0);
t.rows[2].cells[0].innerText = 'foo';There are a few things here that are odd – adding a -1 to add a row at the end for example – and there seems to be no way to create a TH element instead of a TD. All table cells are just cells.However, seeing how much of a pain it is to create tables, it would be fun to re-visit this  and add more functionality to it. We did add a lot of things to  forms, like formData and the change event, so why not add events and other features to tables. That way they’d finally get the status as data structures and not a hack to layout content on the web.]]></content:encoded></item><item><title>You can&apos;t refuse to be scanned by ICE&apos;s facial recognition app, DHS document say</title><link>https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/</link><author>nh43215rgb</author><category>hn</category><pubDate>Sat, 1 Nov 2025 08:58:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Photos captured by Mobile Fortify will be stored for 15 years, regardless of immigration or citizenship status, the document says.]]></content:encoded></item><item><title>Hard Rust requirements from May onward</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>rkta</author><category>hn</category><pubDate>Sat, 1 Nov 2025 07:31:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>The profitable startup</title><link>https://linear.app/now/the-profitable-startup</link><author>doppp</author><category>hn</category><pubDate>Sat, 1 Nov 2025 03:18:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?But that thinking was always flawed.Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.Paul Graham famously wrote about "ramen profitability" – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next  engineer. This intentional approach has allowed us to maintain both quality and culture.The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.Understand Your Risk ProfileAre you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.Hire Intentionally and SlowerFor most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.]]></content:encoded></item><item><title>Show HN: Strange Attractors</title><link>https://blog.shashanktomar.com/posts/strange-attractors</link><author>shashanktomar</author><category>hn</category><pubDate>Fri, 31 Oct 2025 23:23:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A few months back, while playing around with Three.js, I came across something that completely derailed my plans. Strange attractors - fancy math that creates beautiful patterns. At first I thought I'd just render one and move on, but then soon I realized that this is too much fun. When complexity emerges from three simple equations, when you see something chaotic emerge into beautiful, it's hard not to waste some time. I've spent countless hours, maybe more than I'd care to admit, watching these patterns form. I realized there's something deeply satisfying about seeing order emerge from randomness. Let me show you what kept me hooked.The Basics: Dynamical Systems and Chaos TheoryDynamical Systems are a mathematical way to understand how things . Imagine you have a system, which
could be anything from the movement of planets to the growth of a population. In this system, there are rules that
determine how it evolves from one moment to the next. These rules tell you what will happen next based on what is
happening now. Some examples are, a pendulum, the weather patterns, a flock of birds, the spread of a virus in a
population (we are all too familiar with this one), and stock market.There are two primary things to understand about this system:: This is like a big collection of all the possible states the system can be in. Each state is like a
snapshot of the system at a specific time. This is also called the  or the .: These are the rules that takes one state of the system and moves it to the next state. It can be
represented as a function that transforms the system from now to later.For instance, when studying population growth, a phase-space (world-state) might consist of the current population size
and the rate of growth or decline at a specific time. The dynamics would then be derived from models of population
dynamics, which, considering factors like birth rates, death rates, and carrying capacity of the environment, dictate
the changes in population size over time.Another way of saying this is that the dynamical systems describe how things change over time, in a space of
possibilities, governed by a set of rules. Numerous fields such as biology, physics, economics, and applied mathematics,
study systems like these, focusing on the specific rules that dictate their evolution. These rules are grounded in
relevant theories, such as Newtonian mechanics, fluid dynamics, and mathematics of economics, among others.There are different ways of classifying dynamical systems, and one of the most interesting is the classification into
chaotic and non-chaotic systems. The change over time in non-chaotic systems is more deterministic as compared to
chaotic systems which exhibit randomness and unpredictability. is the sub branch of dynamical systems that studies chaotic systems and challenges the traditional
deterministic views of causality. Most of the natural systems we observe are chaotic in nature, like the weather, a drop
of ink dissolving in water, social and economic behaviours etc. In contrast, systems like the movement of planets,
pendulums, and simple harmonic oscillators are extremely predictable and non-chaotic.Chaos Theory deals with systems that exhibit irregular and unpredictable behavior over time, even though they follow
deterministic rules. Having a set of rules that govern the system, and yet exhibit randomness and unpredictability,
might seem a bit contradictory, but it is because the rules do not always represent the whole system. In fact, most of
the time, these rules are an approximation of the system and that is what leads to the unpredictability. In complex
systems, we do not have enough information to come up with a perfect set of rules. And by using incomplete information
to make predictions, we introduce uncertainty, which amplifies over time, leading to the chaotic behaviour.Chaotic systems generally have many non-linear interacting components, which we partially understand (or can partially
observe) and which are very sensitive to small changes. A small change in the initial conditions can lead to a
completely different outcome, a phenomenon known as the . In this post, we will try to see the
butterfly effect in action but before that, let's talk about .To understand Strange Attractors, let's first understand what an attractor is. As discussed earlier, dynamical systems
are all about . During this change, the system moves through different possible states (remember the
phase space jargon?). An attractor is a set of states towards which a system tends to settle over time, or you can say,
towards which it is . It's like a magnet that pulls the system towards it.For example, think of a pendulum. When you release it, it swings back and forth, but eventually, it comes to rest at the
bottom. The bottom is the attractor in this case. It's the state towards which the pendulum is attracted.This happens due to the system's inherent dynamics, which govern how states in the phase space change. Here are some of
the reasons why different states get attracted towards attractors:: Attractors are stable states of the system, meaning that once the system reaches them, it tends to stay
there. This stability arises from the system's dynamics, which push it towards the attractor and keep it there.: Many dynamical systems have dissipative forces, which cause the system to lose energy over time. This
loss of energy leads the system to settle into a lower-energy state, which often corresponds to an attractor. This is
what happens in the case of the pendulum.: In some regions of the phase space, the system's dynamics cause trajectories to converge. This
contraction effect means that nearby states will tend to come closer together over time, eventually being drawn
towards the attractor.Some attractors have complex governing equations that can create unpredictable trajectories or behaviours. These
nonlinear interactions can result in multiple stable states or periodic orbits, towards which the system evolves. These
complex attractors are categorised as . They are called "strange" due to their unique
characteristics.: Strange attractors often have a fractal-like structure, meaning they display intricate
patterns that repeat at different scales. This complexity sets them apart from simpler, regular attractors.Sensitive Dependence on Initial Conditions: Systems with strange attractors are highly sensitive to their initial
conditions. Small changes in the starting point can lead to vastly different long-term behaviors, a phenomenon known
as the "butterfly effect".Unpredictable Trajectories: The trajectories on a strange attractor never repeat themselves, exhibiting
non-periodic motion. The system's behavior appears random and unpredictable, even though it is governed by
deterministic rules.Emergent Order from Chaos: Despite their chaotic nature, strange attractors exhibit a form of underlying order.
Patterns and structures emerge from the seemingly random behavior, revealing the complex dynamics at play.You can observe most of these characteristics in the visualisation. The one which is most fascinating to observe is the
butterfly effect.A butterfly can flutter its wings over a flower in China and cause a hurricane in the Caribbean.One of the defining features of strange attractors is their sensitivity to initial conditions. This means that small
changes in the starting state of the system can lead to vastly different long-term behaviors, a phenomenon known as the
. In chaotic systems, tiny variations in the initial conditions can amplify over time, leading to
drastically different outcomes.In our visualisation, let's observe this behavior on Thomas Attractor. It is governed by the following equations:A small change in the parameter  can lead to vastly different particle trajectories and the overall shape of the
attractor. Change this value in the control panel and observe the butterfly effect in action.There is another way of observing the butterfly effect in this visualisation. Change the  from  to
 in the control panel and observe how the particles move differently in the two cases. The particles
eventually get attracted to the same states but have different trajectories.This visualization required rendering a large number of particles using Three.js. To achieve this efficiently, we used a
technique called . This method handles iterative updates of particle systems directly on the GPU,
minimizing data transfers between the CPU and GPU. It utilizes two frame buffer objects (FBOs) that alternate roles: One
stores the current state of particles and render them on the screen, while the other calculates the next state.Setting Up Frame Buffer Objects (FBOs): We start by creating two FBOs,  and , to hold the current and
next state of particles. These buffers store data such as particle positions in RGBA channels, making efficient use
of GPU resources.Shader Programs for Particle Dynamics: The shader programs execute on the GPU and apply attractor dynamics to
each particle. Following is the attractor function which update the particle positions based on the attractor equation.Rendering and Buffer Swapping: In each frame, the shader computes the new positions based on the attractor's
equations and stores them in the inactive buffer. After updating, the roles of the FBOs are swapped: The previously
inactive buffer becomes active, and vice versa.This combination of efficient shader calculations and the ping-pong technique allows us to render the particle system.If you have any comments, please leave them on this GitHub discussions topic. Sooner or later, I will integrate it with the blog. The  discussion can be found here.]]></content:encoded></item><item><title>S.A.R.C.A.S.M: Slightly Annoying Rubik&apos;s Cube Automatic Solving Machine</title><link>https://github.com/vindar/SARCASM</link><author>chris_overseas</author><category>hn</category><pubDate>Fri, 31 Oct 2025 23:03:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tim Bray on Grokipedia</title><link>https://www.tbray.org/ongoing/When/202x/2025/10/28/Grokipedia</link><author>Bogdanp</author><category>hn</category><pubDate>Fri, 31 Oct 2025 21:41:53 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Last night I had a very strange experience: About two thirds of the way through reading a Web page about myself, Tim Bray, I
    succumbed to boredom and killed the tab. Thus my introduction to
    Grokipedia. Here are early impressions. · 
My Grokipedia entry has over seven thousand words, compared to a mere 1,300 in
    my Wikipedia article. It’s pretty clear how it was generated; an LLM, trained
    on who-knows-what but definitely including that Wikipedia article and this blog, was told to go nuts.Speaking as a leading but highly biased expert on the subject of T. Bray, here are the key take-aways: · 
It covers all the territory; there is no phase of my life’s activity that could possibly be encountered in combing the Web
    that is not exhaustively covered. In theory this should be good but in fact, who cares about the details of what I worked on
    at Sun Microsystems between 2004 and 2010? I suppose I should but, like I said, I couldn’t force myself to plod all the way
    through it. · 
Every paragraph contains significant errors. Sometimes the text is explicitly self-contradictory on the face of it,
    sometimes the mistakes are subtle enough that only I would spot them. · 
The writing has that LLM view-from-nowhere flat-affect semi-academic flavor. I don’t like it but the evidence suggests that
    some people do? · 
All the references are just URLs and at least some of them entirely fail to support the text. Here’s an example. In
    discussion of my expert-witness work for the FTC in their litigation against Meta concerning its acquisitions of Instagram and
    WhatsApp, Grokipedia says:[Bray] opined that users' perceptions of response times in online services critically influence market
    dynamics.Anyhow, I (so that you won’t have to) spent a solid fifteen minutes spelunking back and forth through that FTC doc, looking
    for strings like “response time” and “latency” and so on. Maybe somewhere in those pages there’s support for the claim quoted
    above, but I couldn’t find it. · 
Wikipedia, in my mind, has two main purposes: A quick visit to find out the basics about some city or person or plant or
    whatever, or a deep-dive to find out what we  know about genetic linkages to autism or Bach’s relationship with
    Frederick the Great or whatever.At the moment, Grokipedia doesn’t really serve either purpose very well. But, after all, this is release 0.1, maybe we
    should give it a chance. · 
The whole point, one gathers, is to provide an antidote to Wikipedia’s alleged woke bias. So I dug into that. Let’s consider
    three examples of what I found.  First, from that same paragraph about the FTC opinion quoted above:While Bray and aligned progressives contend that such dominance stifles innovation by enabling predatory acquisitions and
      reduced rivalry—evidenced by fewer startup exits in concentrated sectors—counterarguments highlight that Big Tech's scale has
      fueled empirical gains, with these firms investing over $240 billion in U.S. R&D in 2024 (more than a quarter of national
      totals) and driving AI, cloud, and patent surges.[128]
      [131] Six tech industries alone accounted for
      over one-third of 
      U.S. GDP growth from 2012–2021, comprising about 9% of the 
      economy and sustaining 9.3 million jobs amid falling consumer prices and rapid technological diffusion.
      [132] 
      [133]
      Right-leaning economists often defend consumer welfare metrics and market self-correction, warning that forced divestitures
      risk eroding the efficiencies and investment incentives that have propelled sector productivity above 6% annual growth in key
      areas like durable manufacturing tech.
      [134]
      [135]I’ve linked the numbered citations to the indicated URLs. Maybe visit one or two of them and see what you think? Four are
    to articles arguing, basically, that monopolies must be OK because the companies accused of it are growing really fast and
    driving the economy. They seem mostly to be from right-wing think-tanks but I guess that’s what those think-tanks are for.  One
    of them, #131, Big Tech and the US Digital-Military-Industrial Complex, I think isn’t helpful to the argument at
    all. But still, it’s broadly doing what they advertise: Pushing back against “woke” positions, in this case the position that
    monopolization is bad.I looked at a couple of other examples.  For example, this is from the header of the Greta Thunberg article:While credited with elevating youth engagement on environmental issues, Thunberg's promotion of urgent, existential climate
      threats has drawn scrutiny for diverging from nuanced empirical assessments of climate risks and adaptation capacities, as
      well as for extending her activism into broader political arenas such as anti-capitalist and geopolitical protests.[5][6]Somehow I feel no urge to click on those citation links.If Ms Thunberg is out there on the “woke” end of the spectrum, let’s flit over to the other end, namely the entry for
    J.D. Vance, on the subject of his book .Critics from progressive outlets, including Sarah Smarsh in her 2018 book , faulted the memoir for
      overemphasizing personal and cultural failings at the expense of structural economic policies, arguing it perpetuated
      stereotypes of rural whites as self-sabotaging.[71] These objections, often rooted in institutional analyses from academia and
      media, overlooked data on behavioral patterns like opioid dependency rates—peaking at 21.5 deaths per 100,000 in Appalachia
      around 2016—that aligned with Vance's observations of "deaths of despair" precursors.[72]I read and enjoyed  but the citation is to a  article that doesn’t mention
    Smarsh. As for the second sentence… my first reaction as I trudged through its many clauses, was “life’s too short”.
    But seriously, opioid-death statistics weaken the hypothesis about structural economic issues? Don’t get it. · 
Wikipedia is,
    to quote myself, the encyclopedia that “anyone who’s willing to
    provide citations can edit”.
    Grokipedia is “the encyclopedia that Elon Musk’s LLM can edit, with sketchy citations and no progressive argument left
    un-attacked.”So I guess it’s Working As Intended?]]></content:encoded></item><item><title>A theoretical way to circumvent Android developer verification</title><link>https://enaix.github.io/2025/10/30/developer-verification.html</link><author>sleirsgoevy</author><category>hn</category><pubDate>Fri, 31 Oct 2025 20:20:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to build silos and decrease collaboration on purpose</title><link>https://www.rubick.com/how-to-build-silos-and-decrease-collaboration/</link><author>gpi</author><category>hn</category><pubDate>Fri, 31 Oct 2025 19:16:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[“We need to break down silos between departments and get people to collaborate better” — almost every leader everywhere.Most leaders reflexively think of silos as BAD and collaboration as GOOD. This manifesto defends silos and challenges the value of collaboration.Increasing collaboration can do harmIn general, you should aim to maximize collaboration  teams, and minimize collaboration  teams.Why maximize collaboration within teams?A collaborative team works together on one or two goals. Why?This maximizes shared state — everyone has a common understanding of goals, progress, and who is doing what.This gives team members a better ability to focus and coordinate their work with each other.Team members have overlapping areas of knowledge, so they can critique each other’s work and help each other grow.They are more innovative, because the interplay between people as they work on the same goal helps generate more diverse thinking and improve decision-making.When someone leaves the team, the fact that others have a shared understanding of the work means the team can survive and continue to work effectively.People can go on vacation or leave without as much disruption.Collaborative teams feel great to be a part of — everyone shares the same victories and accomplishments together. A team that doesn’t collaborate often really isn’t a team at all.Why minimize collaboration between teams?To the maximum extent possible, teams should have what they need to succeed within the borders of their team. And where that is not true, you need some structure to ensure the team can get what it needs in a way that will scale with the organization’s growth.As companies grow, communication and dependencies proliferate. Companies start out with many-to-many communication. As they grow, the communication patterns within the company must necessarily switch to being segmented and defined. Otherwise, the communication burden on teams will grow at an exponential rate, and the increasing complexity will degrade the effectiveness of the company.I observed an example of this at New Relic:As the engineering organization grew, we encouraged a collaborative culture and rewarded people for collaboration between teams (it was even part of our promotion criteria — you can blame that on me!).After a few years, the increasing number of teams made it more and more difficult to manage dependencies between teams, to the point that it eventually became impossible to accomplish any large project within the organization. I know, because I was one of the “best project managers”, so I got put on many of those large projects — and they were systematically impossible to execute on. We all tried heroically to make it work, but the system was rigged — there wasn’t a way to accomplish these larger projects.The solution to this was to eventually define the interaction models between teams, reduce dependencies, and add some structure to prioritization and communication.Looking back in retrospect, it was as obvious as math what happened, but I see organizations fall into this trap over and over. We’ll talk more about how to structure these solutions in the second blog post in this series.Collaboration  great, but it’s something you want to actively be combating between teams. A little collaboration is fine, but excessive collaboration between teams is a sign your organization isn’t structured well. If you ever wonder why Bezos took such a hard line on his API mandate in 2002, this probably factored into it. Bezos structured Amazon so that teams were as independent as possible.Silos are boundaries between groups of people, based on the organizational structure and teams they’re working on.Silos exist because humans have cognitive and communication limits:Humans can only handle a certain number of relationshipsHumans have a limit to how much communication they can handle.Humans are pre-wired to think in terms of teams and tribes. We work better in small groups.Humans have cognitive limits on the amount they can focus on.It’s important to recognize that these limits are real limits, and not something you can wish away. Every company in the world (above a certain size) operates in teams that specialize. We organize into teams and have org structures generally because that is what human beings need to work in larger groups.There are ways to flex certain aspects of this. For example, a company that is designed to be fully asynchronous can rely on process and tooling to change some of these constraints. But even then, you’re moving the constraints around, not eliminating them completely. You need to operate in a way that recognizes these limits.Why do leaders want to break down silos?Leaders start talking about breaking down silos when they see that individual parts of the company aren’t achieving larger business outcomes and are excessively focused on their own area. Alternatively, they talk about breaking down silos when parts of the company aren’t well coordinated with each other. Generally this happens once the organization has become complex enough that the structure is getting in the way.Here are a couple of examples:Marketing is planning a major announcement of an upcoming launch, but can’t get a timing commitment from the product and engineering teams.Two teams in engineering are building the same service, but in slightly different ways.Leaders see these things, and start blathering about “breaking down silos” and “increasing collaboration”. What’s wrong with that?“Breaking down silos” represents incomplete thinking“Breaking down silos” is an exhortation rather than a diagnosis or prescription of how to improve the situation. “Breaking down silos” blames individuals for not having a big enough vision and working across boundaries, instead of looking with curiosity at the system and asking  they are doing what they’re doing. It’s expecting people to have your level of perspective without figuring out why they don’t.But the main problem is that it isn’t specific enough.Communication != Collaboration != CoordinationWhen you hear someone say they want teams to collaborate more or break down silos, encourage them to look at the problem from three perspectives:Usually when people talk about collaboration, what they’re really looking for is better coordination.Coordination is “the harmonious functioning of parts for effective results” (Merriam-Webster)The US military found that the best way to coordinate groups of people quickly and effectively was to centralize coordination and decentralize decision-making and execution. This is still the state of the art for organizational design. You want local groups to be able to act independently and have what they need to be successful. You want centralized functions to set high level objectives and coordinate where necessary to produce the right outcomes.We’ll talk in the second post in this series about a multitude of ways you can coordinate groups of people working together.Communication is transferring information from one person or group to another.When people talk about needing increased collaboration, you can often achieve this more effectively by looking at the flow of information between people, and redesigning it. Typically, you can do something like this:Ask people how they are getting information today.Find out what information people actually need.Design the lightest weight version of this you can imagine.Get feedback and act on that feedback.One thing I’ve done over and over in many startups is set up weekly communication on projects in engineering. This helps the marketing organization understand how to coordinate their work with engineering, among other benefits.Collaboration is when people work together to produce an outcome. When teams are collaborating, it means they’re working with other teams to achieve outcomes together. That’s often a sign the team isn’t set up well. Ideally, it should have what it needs to do what it needs without dependencies on other teams.A team that has to collaborate to achieve its objectives is going to be less reliably successful. In general, teams shouldn’t be collaborating with more than a couple of teams, unless they’re explicitly set up to be that way. For example, in some organizations you might set up the design team as a “service” organization which provides design for a larger organization. For teams that are set up that way, it can be fine, but it usually should be very carefully thought through. What is the interaction model for the team, and how will it deal with the inevitable fact that there will be excess demands on it? We’ll cover some of these patterns and their tradeoffs in the second post in this series.Of course, the world is messy, and you shouldn’t expect a complete lack of collaboration between teams. But when you see teams collaborate, that’s something to pay attention to.You might think of silos as “encapsulation”. Leaders want to dig into the internals of the classes holding the logic they need. But it’s a bad solution to just bolt that on — you really need to refactor things so that they are better structured.How do you get teams to coordinate better?I always appreciate hearing your thoughts and reactions to my posts.Thank you to the many people who helped improve this post. Rebecca Campbell always makes my work better. She helped me tighten up many of my arguments and helped me realize that I needed to make the section on coordination more explicit. Brent Miller, always the purveyer of astute observations, offered structural feedback that made the post much stronger. Chris Haupt, always thoughtful, pointed out a few areas that he didn’t find convincing, and helped me see that I needed to go deeper on information flow. Aaron Erickson suggested the metaphor of encapsulation. Thanks to Neville Kuyt for suggesting I define silo. And additional thank you to Robert DiFalco and Darin Swanson for reviewing and commenting on the post. Always appreciate your insight!]]></content:encoded></item><item><title>Addiction Markets</title><link>https://www.thebignewsletter.com/p/addiction-markets-abolish-corporate</link><author>toomuchtodo</author><category>hn</category><pubDate>Fri, 31 Oct 2025 17:42:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Use DuckDB-WASM to query TB of data in browser</title><link>https://lil.law.harvard.edu/blog/2025/10/24/rethinking-data-discovery-for-libraries-and-digital-humanities/</link><author>mlissner</author><category>hn</category><pubDate>Fri, 31 Oct 2025 17:37:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Libraries, digital humanities projects, and cultural heritage organizations have long had to perform a balancing act when sharing their collections online, negotiating between access and affordability. Providing robust features for data discovery, such as browsing, filtering, and search, has traditionally required dedicated computing infrastructure such as servers and databases. Ongoing server hosting, regular security and software updates, and consistent operational oversight are expensive and require skilled staff. Over years or decades, budget changes and staff turnover often strand these projects in an unmaintained or nonfunctioning state.The alternative, static file hosting, requires minimal maintenance and reduces expenses dramatically. For example, storing gigabytes of data on Amazon S3 may cost $1/month or less. However, static hosting often diminishes the capacity for rich data discovery. Without a dynamic computing layer between the user’s web browser and the source files, data access may be restricted to brittle pre-rendered browsing hierarchies or search functionality that is impeded by client memory limits. Under such barriers, the collection’s discoverability suffers.For years, online collection discovery has been stuck between a rock and a hard place: accept the complexity and expense required for a good user experience, or opt for simplicity and leave users to contend with the blunt limitations of a static discovery layer.Why We Explored a New ApproachWhen LIL began thinking about how to provide discovery for the Data.gov Archive, we decided that building a lightweight and easily maintained access point from the beginning would be worth our team’s effort. We wanted to provide low-effort discovery with minimal impact on our resources. We also wanted to ensure that whatever path we chose would encourage, rather than impede, long-term access.This approach builds on our recent experience when the Caselaw Access Project (CAP) hit a transition moment. At that time, we elected to switch case.law to a static site and to partner with others dedicated to open legal data to provide more feature-rich access.CAP includes some 11 TB of data; the Data.gov Archive represents nearly 18 TB, with the catalog metadata alone accounting for about 1 GB. Manually browsing the archive data in its repository, even for a user who knows what she’s looking for, is laborious and time-consuming. Thus we faced a challenge. Could we enable dynamic, scalable discovery of the Data.gov Archive while enjoying the frugality, simplicity, and maintainability of static hosting?Our Experiment: Rich Discovery, No Server RequiredRecent advancements in client-side data analysis led us to try something new. Tools like DuckDB-Wasm, sql.js-httpvfs, and Protomaps, powered by standards such as WebAssembly, web workers, and HTTP range requests, allow users to efficiently query large remote datasets in the browser. Rather than downloading a 2 GB data file into memory, these tools can incrementally retrieve only the relevant parts of the file and process query results locally.We developed Data.gov Archive Search on the same model. Here’s how it works: We store Data.gov Archive catalog metadata as sorted, compressed Parquet files on Source.coop, taking advantage of performant static file hosting. Our client-side web application loads DuckDB-Wasm, a fully functional database engine running inside the user’s browser. When a user navigates to a resource or submits a search, our DuckDB-Wasm client executes a targeted retrieval of the data needed to fulfill the request. No dedicated server is required; queries run entirely in the browser.This experiment has not been without obstacles. Getting good performance out of this model demands careful data engineering, and the large DuckDB-Wasm binary imposes a considerable latency cost. As of this writing, we’re continuing to explore speedy alternatives like hyparquet and Arquero to further improve performance.Still, we’re pleased with the result: an inexpensive, low-maintenance static discovery platform that allows users to browse, search, and filter Data.gov Archive records entirely in the browser.Why This Matters for Libraries, Digital Humanities Projects, and BeyondThis new pattern offers a compelling model for libraries, academic archives, and DH projects of all sizes: By shifting from an expensive server to lower cost static storage, projects can sustainably offer their users access to data.Reduced technical overhead: With no dedicated backend server, security risks are reduced, no patching or upgrades are needed, and crashing servers are not a concern. Projects can be set up with care, but without demanding constant attention. Organizations can be more confident that their archive and discovery interfaces remain usable and accessible, even as staffing or funding changes over time.Knowing that we are not the only group interested in approaching access in this way, we’re sharing our generalized learnings. We see a few ways forward for others in the knowledge and information world: If your organization has large, relatively static datasets, consider experimenting with a browser-based search tool using static hosting. Template applications, workflows, and lessons learned can help this new pattern gain adoption and maturity across the community.This project is still evolving, and we invite others—particularly those in libraries and digital cultural heritage—to explore these possibilities with us. We’re committed to open sharing as we refine our tools, and we welcome collaboration or feedback at lil@law.harvard.edu.]]></content:encoded></item><item><title>Just use a button</title><link>https://gomakethings.com/just-use-a-button/</link><author>moebrowne</author><category>hn</category><pubDate>Fri, 31 Oct 2025 16:59:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[One of the weirdest “debates” I seem to perpetually have with framework-enthusiastic developers is whether or not a  is “just as good” as a . it’s not. Let’s dig in.Among the React crowd, and also among people who seem to enjoy HTMX, I see a lot this…
	Open Modal
This element does not announce itself as an interactive element to screen reader users.You can’t focus on a  with a keyboard.The event only fires on , not when the  or  keys are pressed (again, keyboard users).I’ve had arguments with a very prominent React thought leader whose name starts with R who insisted that using a  was “more accessible” than using a , and that Twitter made the right decision in using this pattern in their app.It’s wrong. It’s all wrong.Many HTML elements have  that tell assistive tech like screen readers what they do.The  element is one of them. It has an implicit  of , which tells screen reader users it can be interacted with and will trigger some type of behavior in the app.The HTML  attribute can be used to add or modify the role of an element. And so, folks like React Ry–thought-leader-guy will say stuff like (I’m paraphrasing)…That attribute exists for a reason. You can add  to a  to give it the correct semantics.OK, that addresses one issue.That role doesn’t affect focusability (or lack thereof) or keyboard behavior. Visually impaired users and people who navigate with a keyboard still can’t use it.“No worries!” they say. “We can fix that, too!”You can make the element focusable with the  attribute.
	Open Modal
You , though! Seriously, just don’t fuck with focus order.It’s way too easy to go down this path and then fuck it up and have folks jumping all over the page instead of navigating through in the normal and expected order.And again, still no keyboard interactivity.But don’t fear! You can add that, too. You just need to listen for all  events, and then filter them out by  so that you only run your code if the  or  keys were pressed (the latter means checking for a literal space: ).That can’t run on the element, either. You’ve got to attach that even to the  and figure out which element has focus.So um… ok, I guess it is technically a fix, but…You’ve just recreated all of the functionality a  gives you for freeSeriously, WTF would you do that?!?All of these hoops to write this HTML…
	Open Modal
When you could write this HTML instead…
	Open Modal
Has the correct  implicitly.Is automatically focusable.Fires a  event in response to  and  presses when it has focus.Look, I’m a lazy developer.And I suspect, if you’re someone who loves tools like React, you probably are, too. It’s cool, I get it! The best code is the code you didn’t write and all that.Use the correct element for the job, and avoid writing a bunch of extra code!]]></content:encoded></item><item><title>Futurelock: A subtle risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>bcantrill</author><category>hn</category><pubDate>Fri, 31 Oct 2025 16:49:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>Another European agency shifts off US Tech as digital sovereignty gains steam</title><link>https://www.zdnet.com/article/another-european-agency-ditches-big-tech-as-digital-sovereignty-movement-gains-steam/</link><author>CrankyBear</author><category>hn</category><pubDate>Fri, 31 Oct 2025 16:39:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Austria's Ministry of Economy has migrated to a Nextcloud platform.It's the latest move in a European trend to shift away from Big Tech.European governments and agencies want to control sensitive data.This shift away from proprietary, foreign-owned cloud services, such as Microsoft 365, to an open-source, European-based cloud service aligns with a growing trend among European governments and agencies. They want control over sensitive data and to declare their independence from US-based tech providers. European companies are encouraging this trend. Many of them have joined forces in the newly created non-profit foundation, the EuroStack Initiative. This foundation's goal is " to organize action, not just talk, around the pillars of the initiative: Buy European, Sell European, Fund European." What's the motive behind these moves away from proprietary tech? Well, in Austria's case, Florian Zinnagl, CISO of the Ministry of Economy, Energy, and Tourism (BMWET), explained, "We carry responsibility for a large amount of sensitive data -- from employees, companies, and citizens. As a public institution, we take this responsibility very seriously. That's why we view it critically to rely on cloud solutions from non-European corporations for processing this information."All of these organizations aim to keep data storage and processing within national or European borders to enhance security, comply with privacy laws such as the EU's General Data Protection Regulation (GDPR), and mitigate risks from potential commercial and foreign government surveillance. Open-source software is seen as combining the virtues of faster development and better security, while providing companies and governments with more control, as general manager Thierry Carrez of the OpenInfra Foundation recently suggested: "Open infrastructure allows nations and organizations to maintain control over their applications, their data, and their destiny while benefiting from global collaboration."  While the US may not like it, with NextCloud's help, BMWET completed its migration in just four months. Although BMWET had already begun adopting Microsoft 365 and Teams before the project's start, the shift was still considered a success. That's because instead of reversing its path, the ministry implemented a hybrid architecture: Nextcloud handles internal collaboration and secure data management, while Teams remains available for external meetings.The project emphasized integration with existing workflows, including seamless integration with Outlook email and calendar via Sendent's Outlook app. This approach minimized disruption and ensured user acceptance. However, not all migrations progress so well. For example, in Austria, the Ministry of Justice decided to replace Office with LibreOffice. Yet the transition has run into trouble. It appears that the move of 20,000 desktops, which was prompted by a desire to reduce spending on Microsoft licenses, has been, as one person reported, an "unprofessional, rushed operation." Some offices are still on Office, others on LibreOffice, and they're running into incompatible document format problems and misfires in e-mail systems. The moral of the story is that any switch from one software suite to another requires careful handling by the IT department and helpdesk staff. Otherwise, you end up with unhappy users.That said, BMWET's bold shift to Nextcloud appears to have gone well. This initiative demonstrates that adopting sovereign cloud solutions can be practical, user-friendly, and rapid in the public sector. However, as Austria's Justice Ministry experience has shown, simply shifting to an open-source approach without careful planning can get in the way of getting work done. ]]></content:encoded></item><item><title>AI scrapers request commented scripts</title><link>https://cryptography.dog/blog/AI-scrapers-request-commented-scripts/</link><author>ColinWright</author><category>hn</category><pubDate>Fri, 31 Oct 2025 15:44:19 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Nix Derivation Madness</title><link>https://fzakaria.com/2025/10/29/nix-derivation-madness</link><author>birdculture</author><category>hn</category><pubDate>Fri, 31 Oct 2025 14:28:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I’ve written  about Nix and I still face moments where foundational aspects of the package system confounds and surprises me.Recently I hit an issue that stumped me as it break some basic comprehension I had on how Nix works. I wanted to produce the build and runtime graph for the Ruby interpreter. nix-shell  ruby

 which ruby
/nix/store/mp4rpz283gw3abvxyb4lbh4vp9pmayp2-ruby-3.3.9/bin/ruby

 nix-store nix-store which ruby
error: path  is not valid

 /nix/store/24v9wpp393ib1gllip7ic13aycbi704g-ruby-3.3.9.drv
: cannot access :
No such file or directory
I have Ruby but I don’t seem to have the derivation, 24v9wpp393ib1gllip7ic13aycbi704g, file present on my machine.No worries, I think I can  it and download it from the NixOS cache. nix-store  /nix/store/24v9wpp393ib1gllip7ic13aycbi704g-ruby-3.3.9.drv
don/nix/store/24v9wpp393ib1gllip7ic13aycbi704g-ruby-3.3.9.drvI guess the NixOS cache doesn’t seem to have it. 🤷This was actually perplexing me at this moment. In fact there are multiplediscourse posts about it.My mental model however of Nix though is that I must have first evaluated the derivation (drv) in order to determine the output path to even substitute. How could the NixOS cache not have it present?Is this derivation wrong somehow? Nope. This is the derivation Nix believes that produced this Ruby binary from the  database. 🤨 sqlite3 
/nix/store/24v9wpp393ib1gllip7ic13aycbi704g-ruby-3.3.9.drv
What does the binary cache itself say? Even the cache itself thinks this particular derivation, 24v9wpp393ib1gllip7ic13aycbi704g, produced this particular Ruby output. curl  https://cache.nixos.org/mp4rpz283gw3abvxyb4lbh4vp9pmayp2.narinfo |Deriver
Deriver: 24v9wpp393ib1gllip7ic13aycbi704g-ruby-3.3.9.drv
What if I try a different command? nix derivation show which ruby | jq 
/nix/store/kmx8kkggm5i2r17s6l67v022jz9gc4c5-ruby-3.3.9.drv

 /nix/store/kmx8kkggm5i2r17s6l67v022jz9gc4c5-ruby-3.3.9.drv
/nix/store/kmx8kkggm5i2r17s6l67v022jz9gc4c5-ruby-3.3.9.drv
So I seem to have a completely different derivation, kmx8kkggm5i2r17s6l67v022jz9gc4c5, that resulted in the same output which  what the binary cache announces. WTF? 🫠Thinking back to a previous post, I remember touching on modulo fixed-output derivations. Is that what’s going on? Let’s investigate from first principles. 🤓Let’s first create  which is our .☝️ Since this is a  (FOD) the produced  path will not be affected to changes to the derivation beyond the contents of . nix-instantiate fod.nix
/nix/store/k2wjpwq43685j6vlvaarrfml4gl4196n-hello-world-fixed.drv

 nix-build fod.nix
/nix/store/ajk19jb8h5h3lmz20yz6wj9vif18lhp1-hello-world-fixed
Now we will create a derivation that uses this FOD.The  for the output for this derivation  on changes to the derivation  if the derivation path for the FOD changes. This is in fact what makes it “modulo” the fixed-output derivations. nix-instantiate uses-fod.nix
/nix/store/85d15y7irq7x4fxv4nc7k1cw2rlfp3ag-uses-fod.drv

 nix-build uses-fod.nix
/nix/store/sd12qjak7rlxhdprj10187f9an787lk3-uses-fod
Let’s test this all out by changing our  derivation.
Let’s do this by just adding some  attribute to the derivation.
   name = "hello-world-fixed";
   builder = "/bin/sh";
   system = system;
   args = [ "-c" ''
     echo -n "hello world" > "$out"
   '' ];
 nix-instantiate fod.nix
/nix/store/yimff0d4zr4krwx6cvdiqlin0y6vkis0-hello-world-fixed.drv

 nix-build fod.nix
/nix/store/ajk19jb8h5h3lmz20yz6wj9vif18lhp1-hello-world-fixed
The path of the derivation itself, , has changed but the output path ajk19jb8h5h3lmz20yz6wj9vif18lhp1 remains consistent.What about the derivation that leverages it? nix-instantiate uses-fod.nix
/nix/store/85wkdaaq6q08f71xn420v4irll4a8g8v-uses-fod.drv

 nix-build uses-fod.nix
/nix/store/sd12qjak7rlxhdprj10187f9an787lk3-uses-fod
It also got a new derivation path but the output path remained unchanged. 😮That means changes to  didn’t cause new outputs in either derivation  it did create a complete new tree of  files. 🤯That means in nixpkgs changes to  derivations can cause them to have new store paths for their  but result in dependent derivations to have the same output path. If the output path had already been stored in the NixOS cache, then we lose the link between the new  and this output path. 💥The amount of churn that we are creating in derivations was unbeknownst to me.It can get even weirder! This example came from @ericson2314.We will duplicate the  to another file  whose only difference is the value of the garbage.
   name = "hello-world-fixed";
   builder = "/bin/sh";
   system = system;
   args = [ "-c" ''
     echo -n "hello world" > "$out"
   '' ];
Let’s now use both of these in our derivation.We can now instantiate and build this as normal. nix-instantiate uses-fod.nix
/nix/store/z6nr2k2hy982fiynyjkvq8dliwbxklwf-uses-fod.drv

 nix-build uses-fod.nix
/nix/store/211nlyx2ga7mh5fdk76aggb04y1wsgkj-uses-fod
What is weird about that?Well, let’s take the JSON representation of the derivation and remove one of the inputs. nix derivation show 
    /nix/store/z6nr2k2hy982fiynyjkvq8dliwbxklwf-uses-fod.drv 
    jq We can do this because although there are two input derivations, we know they both produce the same output!
       "system": "x86_64-linux"
     },
     "inputDrvs": {
       "/nix/store/yimff0d4zr4krwx6cvdiqlin0y6vkis0-hello-world-fixed.drv": {
         "dynamicOutputs": {},
         "outputs": [
Let’s load this modified derivation back into our  and build it again! nix derivation add < derivation.json
/nix/store/s4qrdkq3a85gxmlpiay334vd1ndg8hm1-uses-fod.drv

 nix-build /nix/store/s4qrdkq3a85gxmlpiay334vd1ndg8hm1-uses-fod.drv
/nix/store/211nlyx2ga7mh5fdk76aggb04y1wsgkj-uses-fod
We got the same output 211nlyx2ga7mh5fdk76aggb04y1wsgkj. Not only do we have a  trait for our output paths to derivations but we can also take certain derivations and completely change them by removing inputs and still get the same output! 😹The road to Nix enlightenment is no joke and full of dragons.]]></content:encoded></item><item><title>Nim 2.2.6</title><link>https://nim-lang.org//blog/2025/10/31/nim-226.html</link><author>xz18r</author><category>hn</category><pubDate>Fri, 31 Oct 2025 14:15:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The Nim Team is happy to announce version 2.2.6, the third patch release for our stable release, Nim 2.2.It comes six months after the 2.2.4 release and it contains 141 commits, bringing bugfixes and improvements.The Nim 2.2.6 changelog is available here.Exception handling combined with Nim’s  is more stable than ever before
as the underlying closure iterator transformation has been rewritten.The compiler is now smart enough to produce a move operation for .
Previously it performed a copy.
Expect your code to run slightly faster due to this and other minor performance improvements.Check out if the package manager of your OS already ships version 2.2.6 or
install it as described here.If you have installed a previous version of Nim using ,
getting Nim 2.2.6 is as easy as:choosenim update self
choosenim update stable
 We recommend you to install and use the latest version of , v.0.8.16, available in our choosenim repo.Fixed “ for non-var failed to compile when JS”
(#24914)Fixed “ doesn’t respect lexical scoping”
(#23355)Fixed “Pragma block disabling warning has effect beyond block”
(#21975)Fixed “missing  (less than),  for ”
(#24941)Fixed “[GC] Illegal storage access when collecting cycle”
(#4851)Fixed “Globals in proc with static params end up being re-initialized”
(#24940)Fixed “Constructor to global variable in converter generates illegal c code”
(#4594)Fixed “Compile-time regression from v2.2.4 to / with  variable with unhandled exception: iterators.nim(254, 11) len(a) == L the length of the seq changed while iterating over it [AssertionDefect]”
(#24981)Fixed “SIGSEGV when raising /doAssert”
(#24974)Fixed “Crash on marking destroy hook as .error”
(#24996)Fixed “ stops working after typedesc is copied”
(#23564)Fixed “Asyncnet accept leaks socket on SSL error; Regression in devel”
(#25023)Fixed “SIGSEGV in closure iterator with try/except not at top level”
(#21235)Fixed “Regression from v2.2.4 to / with closure iterator”
(#25038)Fixed “fixes #24997; {.global.} variable in recursive function”
(#25016)Fixed “concept param passed to varargs causes internal error: genTypeInfo(tyUserTypeClassInst) when  JS backend”
(#25043)Fixed “Bad order of destruction”
(#24719)Fixed “Floats are not range checked”
(#7179)Fixed “Regression from Nim v2.2.2 to v2.2.4//devel in  with compatible types not keeping L-valueness”
(#25109)Fixed “ compiler crash when comparing pointers at compile time”
(#25066)Fixed “Slow compilation due to  and memory allocations”
(#25114)Fixed “unhandled exception: field 'sym' is not accessible for type 'TNode' using 'kind = nkEmpty' [FieldDefect] with iterator-loop”
(#25121)Fixed “ returns wrong values from large values close to int64.high”
(#25125)Fixed “Dereferencing result of  in single expression triggers unnecessary copy”
(#24093)Fixed “internal error: '=destroy' operator not found for type NimNode returning ”
(#25120)Fixed “ not checked for  if it has been used (2.2 regression)”
(#25117)Fixed “ wrongly results in rvalue”
(#25078)Fixed “ extremely slow for trivial types”
(#25063)Fixed “withValue for immut tab wrong chk cond”
(#25162)Fixed “Invalid C codegen  with generic types containing gc memory”
(#24844)Fixed “SinglyLinkedList.remove broken / AssertionDefect”
(#25173)Fixed “ and error: incompatible types when assigning to type ‘void *’”
(#24361)Fixed “Noncopyable base type ignored”
(#24760)Fixed “ generic field isn’t preserving its value”
(#25127)Fixed “cannot return  expression from conditionals like ”
(#23949)Fixed “Error: internal error: proc has no result symbol”
(#21476)Fixed “Invalid codegen / dangling pointer for openArray escaping from ”
(#24261)Fixed “lib/system/iterators.nim(250, 14) Error: internal error: genArrayLen()”
(#25167)Fixed “Error: unhandled exception: field 'sym' is not accessible for type 'TNode'”
(#21138)Fixed “error: ‘pthread_mutex_t’ has no member named ‘abi’ in refc with /”
(#25205)Fixed “Uninitialized variable usage in  in  in ORC”
(#25204)Fixed “VM error when passing object field ref to ”
(#25210)Fixed “Closure environement wrongly marked as cyclic (orc)”
(#25048)Fixed “Infinite loop with anonymous iterator”
(#25046)Fixed “JS:  for  not truncate”
(#25222)Fixed “VM issue with globals and assignments”
(#25208)Fixed “Case object from  proc unable to be passed as  param”
(#25123)Fixed “VM  raises RangeDefect for long string under refc”
(#25226)Fixed “Broken assignment of union with bool inside variant object”
(#25236)Fixed “deques: Deque items behavior is not the same on 2.0.16 and 2.2.0”
(#25240)Fixed “ uses doc comment from private field for public field”
(#25027)Fixed “Compiler internal error compiler/vmgen.nim(1771, 23) with  overload”
(#25008)Fixed “Wrong exception raised wrapped in finally in closure iterator; Regression in devel/version-2-2”
(#25202)The complete list of changes is available here.]]></content:encoded></item><item><title>Immutable releases are now generally available on GitHub</title><link>https://github.blog/changelog/2025-10-28-immutable-releases-are-now-generally-available/</link><author>fastest963</author><category>hn</category><pubDate>Fri, 31 Oct 2025 13:59:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[GitHub releases now support immutability, adding a new layer of supply chain security. With immutable releases, assets and tags are protected from tampering after publication, so the software you publish—and your users consume—remains secure and trustworthy.Immutable releases offer:: Once you publish a release as immutable, its assets can’t be added, modified, or deleted. This helps protect distributed artifacts from supply chain attacks.: Tags for new immutable releases are protected and can’t be deleted or moved.: Immutable releases receive signed attestations so you can easily verify the authenticity and integrity of assets, both on GitHub and in external environments.You can enable immutable releases at the repository or organization level in your settings. Once enabled:All new releases are immutable (i.e., assets are locked and tags are protected).Existing releases remain mutable unless you republish them.Disabling immutability doesn’t affect releases created while it was enabled. They remain immutable.Release attestations let you verify that an artifact is authentic and unchanged, even outside GitHub. Attestations use the Sigstore bundle format, so you can easily verify releases and assets using the GitHub CLI or integrate with any Sigstore-compatible tooling to automate policy enforcement in your CI/CD pipelines. For instructions on how to verify the integrity of a release, see our docs on verifying the integrity of a release.We’d love your feedback. Share your thoughts and questions on the GitHub Community.]]></content:encoded></item><item><title>Ask HN: Who uses open LLMs and coding assistants locally? Share setup and laptop</title><link>https://news.ycombinator.com/item?id=45771870</link><author>threeturn</author><category>hn</category><pubDate>Fri, 31 Oct 2025 13:39:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Dear Hackers,
I’m interested in your real-world workflows for using open-source LLMs and open-source coding assistants on your laptop (not just cloud/enterprise SaaS). Specifically:Which model(s) are you running (e.g., Ollama, LM Studio, or others) and which open-source coding assistant/integration (for example, a VS Code plugin) you’re using?What laptop hardware do you have (CPU, GPU/NPU, memory, whether discrete GPU or integrated, OS) and how it performs for your workflow?What kinds of tasks you use it for (code completion, refactoring, debugging, code review) and how reliable it is (what works well / where it falls short).I'm conducting my own investigation, which I will be happy to share as well when over.]]></content:encoded></item><item><title>Sustainable memristors from shiitake mycelium for high-frequency bioelectronics</title><link>https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0328965</link><author>PaulHoule</author><category>hn</category><pubDate>Fri, 31 Oct 2025 13:32:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The development of neuromorphic hardware relies on memristive devices capable of emulating synaptic behavior, with potential applications in energy-efficient computing and artificial intelligence. Recent work has explored natural, biodegradable substrates as sustainable alternatives to conventional inorganic memristors [1]. In this study, we investigated the potential of the edible fungus  (shiitake mushroom) as a platform for memristor fabrication. By examining the electrical response of mushroom-derived materials under repeated voltage cycling, we explored stable memristive switching behavior, retention, and endurance. Shiitake-based devices not only demonstrate reproducible memory effects, but also highlight the potential for scalable, low-cost, and environmentally friendly neuromorphic components.Memristor devices offer substantial advantages in robotic, industrial, and transport applications due to their unique electrical properties and ability to mimic neural functions. They can enhance various control systems, facilitate efficient information processing, and ultimately improve the overall performance of autonomous systems.One of the key strengths of memristors is their capacity for efficient and self-adaptive in situ learning, which is critical for applications in robotics and autonomous vehicles. In memristor-based neural networks, the devices can adjust their resistance based on previous inputs, allowing for a form of analog learning that closely resembles the synaptic behavior in biological systems [1]. This capability enables robots and autonomous vehicles to learn from their environment and adapt in real time, enhancing their ability to navigate complex situations effectively. It has been found that such systems can achieve low-latency responses, which are essential for high-speed decision-making in dynamic environments [2].Memristors also have the advantage of integrating memory and processing capabilities into a single device, enabling a simplified architecture for autonomous control systems [3]. For instance, in autonomous vehicles, trajectory-tracking and path-following tasks can be performed using memristor-based controllers that allow for rapid calculations and real-time adjustments to control variables [4]. This integration, especially with parallelization, helps to address the challenges posed by separate memory and processing units, which can lead to delays and increased power consumption in traditional control systems [4].Additionally, the resilience of memristor devices against environmental changes, and their ability to operate under varying conditions, make them particularly suitable for autonomous applications, such as spacecraft electronics or vehicles operating in unpredictable road environments [4]. This is complemented by the precision in control that memristor-based systems can offer, which is significant for maintaining stability and performance while following desired trajectories [5].Moreover, the low power consumption of memristors is particularly beneficial in robotics and autonomous vehicles, where energy efficiency is paramount. Hybrid analog–digital memristor systems can minimize power usage during processing without sacrificing responsiveness, which can prolong operational times by reducing the frequency at which recharging or battery replacement is required, enhancing the feasibility of deploying such systems in mobile applications [2].Ultimately, the potential of memristors to emulate human-like decision-making and learning processes could be exploited to endow robotic systems and autonomous vehicles with functionalities not found in conventional control systems. The ability of memristors to perform complex computations efficiently, learn adaptively, and integrate both memory and processing into a unified approach make them a cornerstone technology for the future development of intelligent autonomous systems. However, the production of memristors often requires rare-earth minerals and expensive semiconductor foundries.Fungi possess innate abilities to adapt to various environmental conditions and efficiently process information through their interconnected network of hyphae. These characteristics make fungi an ideal candidate for developing sustainable computing systems from. Our aim was to design and implement a novel fungal memristor-based computing architecture that could significantly reduce energy consumption and minimize electronic waste. We approached this using substantially simpler bioreactors and nutrient cultures than those required for conventional neurons and neural organoids. The unique advantages of fungal memristors stem from the biological properties of fungal materials, which distinguish them from typical inorganic or polymer alternatives [6,7].First, one of the main benefits of fungal memristors is their environmentally sustainable and biodegradable nature. Conventional memristors often contain transition metal oxides or silicon-based structures, the production or disposal of which can pose environmental challenges [6,7]. By contrast, fungal materials are derived from organic biomass, making them both sustainable and significantly less harmful to the environment. This aligns with increasing efforts toward developing greener electronic materials, as highlighted in previous work emphasizing the importance of sustainability in technology development [8].Second, fungal memristors exhibit remarkable adaptability in their electrical properties. The structural composition of fungal materials often allows for a range of conductive pathways that can form dynamically under the influence of electrical stimuli, similar to the conductive filaments formed in conventional memristors [9,10]. This adaptability can lead to enhanced performance in neuromorphic applications through the facilitation of variable resistance states that mimic synaptic behaviors more closely than traditional memristive materials, which often have static crystalline structures that can lead to variability problems or performance limitations at the nanoscale [11].Furthermore, fungal memristors may consume less power than traditional materials due to their unique electrochemical properties. It has been claimed that some organic materials, including those derived from fungi, can operate effectively at lower voltages while maintaining stable switching characteristics––a trait that is crucial for developing energy-efficient devices for portable electronics and Internet of Things applications [12]. This can significantly extend battery life and reduce energy costs in processing and memory applications, which have become focal points in the research into neuromorphic systems [13].Finally, the natural composition and multicellularity of fungal materials can lead to more naturalistic models for neural networks. Because these materials are subject to biological processes, they may inherently incorporate characteristics that resemble biological neuronal networks, including plasticity and memory capabilities that could evolve with usage. This biological mimicry could strengthen the development of more advanced artificial neural networks, enabling applications such as adaptive learning systems and intelligent sensor networks [14].The potential use of common food mushrooms, such as shiitake and button mushrooms (), as organic memristors is an emerging area of research that exploits the unique properties of these fungi [6,7,13]. Memristors, which are non-volatile memory devices that retain information even without power, can benefit from the porous structures and electrical properties of the organic materials derived from mushrooms.Shiitake mushrooms have been shown to possess a hierarchically porous carbon structure when activated. This porous structure can enhance the electrochemical performance of devices, making them suitable candidates for use in energy storage systems, including supercapacitors and, potentially, memristors [15]. Highly conductive carbon materials have been created from shiitake, suggesting that these materials could be engineered to exhibit memristive behavior [16]. Shiitake-derived carbon is a sustainable alternative to traditional materials and can enhance the performance of electronic devices due to its unique structural properties.Button mushrooms have also shown significant potential in this context. Research has indicated that their porosity can be exploited to create materials with large surface areas, which are essential for the development of efficient electronic components [17]. The synthesis of carbon composites from button mushrooms has been explored, revealing their ability to function effectively in energy storage applications [17]. Furthermore, the integration of button mushrooms into electronic systems has been investigated, demonstrating their potential as substrates for electronic devices [18].In addition to their structural properties, the unique biological characteristics of fungi, including their ability to interact with various chemical compounds, can be harnessed to develop novel sensing technologies. For instance, electronic noses have been developed that use mushroom extracts to detect volatile compounds. These could be adapted for use in electronic devices that require environmental-sensing capabilities [19,20]. This intersection of biology and electronics opens new avenues for creating multifunctional devices that incorporate the sensory capabilities of mushrooms.Radiation, resistance, and resilienceThe radiation resistance of shiitake mushrooms has been studied primarily in terms of their ability to withstand and possibly derive benefits from exposure to ionizing radiation. This resistance can be attributed to several biochemical and physiological attributes. A possible factor is lentinan, a polysaccharide found in the cell walls of shiitake. Lentinan provides structural integrity and exhibits immunomodulatory effects that may enhance the mushroom’s ability to respond to environmental stresses, including radiation exposure. Although some research has suggested that lentinan possesses properties that may help mitigate oxidative stress [21], there have been limited studies directly linking lentinan to radiation resistance in shiitake mushrooms.Shiitake mushrooms have also shown a notable ability to adapt to their environmental conditions, including variable radiation levels. Studies involving fungi in space research have indicated that certain taxa can enhance their survival through morphological changes or increased melanin production in response to radiation [22]. This radiation resistance implies a suitability of fungal electronics for aerospace applications, where cosmic rays and ambient radiation can interfere with conventional electronics. Fungi’s physical flexibility and low energy requirements would also be advantageous relative to conventional solutions [18,19]. These studies have not specifically addressed shiitake, but the general adaptability observed in fungi suggests that this species could respond similarly to such conditions.Another example of the resilience of shiitake mushrooms is their ability to maintain their nutritional and bioactive qualities after irradiation. For example, they retain essential nutrients and bioactive compounds even after exposure to ultraviolet radiation [23]. The high content of ergosterol, a precursor to vitamin D, found in shiitake mushrooms, reinforces their potential for beneficial outcomes following exposure to radiation because this compound can be converted into vitamin D when subjected to ultraviolet light [24].Lastly, shiitake mushrooms could be considered in the development of dietary supplements or functional foods that could serve a broader purpose in radioprotection. Their multirole efficacy as a food source and electrical component emphasizes a sustainable approach to utilizing biological entities that can withstand environmental stresses, including radiation. This is especially relevant in aerospace and exploration contexts, where promoting health in astronauts could reduce the risks associated with their increased radiation exposure during missions [22]. Also, shiitake mushrooms can withstand environmental stresses, including radiation, while remaining safe for human consumption.In summary, the radiation resistance of shiitake mushrooms is linked to the presence of protective compounds, such as lentinan, and their ability to adapt morphologically. These factors have contributed to our understanding of their survival strategies and are suggestive of potential applications in areas where radiation exposure is a significant concern, such as aerospace and radiation sensing. By culturing and evaluating the memristive properties of shiitake mushrooms, we can determine their suitability for use as sustainable, low-cost bioelectronics.]]></content:encoded></item><item><title>Attention lapses due to sleep deprivation due to flushing fluid from brain</title><link>https://news.mit.edu/2025/your-brain-without-sleep-1029</link><author>gmays</author><category>hn</category><pubDate>Fri, 31 Oct 2025 13:14:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Nearly everyone has experienced it: After a night of poor sleep, you don’t feel as alert as you should. Your brain might seem foggy, and your mind drifts off when you should be paying attention.A new study from MIT reveals what happens inside the brain as these momentary failures of attention occur. The scientists found that during these lapses, a wave of cerebrospinal fluid (CSF) flows out of the brain — a process that typically occurs during sleep and helps to wash away waste products that have built up during the day. This flushing is believed to be necessary for maintaining a healthy, normally functioning brain.When a person is sleep-deprived, it appears that their body attempts to catch up on this cleansing process by initiating pulses of CSF flow. However, this comes at a cost of dramatically impaired attention.“If you don’t sleep, the CSF waves start to intrude into wakefulness where normally you wouldn’t see them. However, they come with an attentional tradeoff, where attention fails during the moments that you have this wave of fluid flow,” says Laura Lewis, the Athinoula A. Martinos Associate Professor of Electrical Engineering and Computer Science, a member of MIT’s Institute for Medical Engineering and Science and the Research Laboratory of Electronics, and an associate member of the Picower Institute for Learning and Memory.Lewis is the senior author of the study, which appears today in . MIT visiting graduate student Zinong Yang is the lead author of the paper.Although sleep is a critical biological process, it’s not known exactly why it is so important. It appears to be essential for maintaining alertness, and it has been well-documented that sleep deprivation leads to impairments of attention and other cognitive functions.During sleep, the cerebrospinal fluid that cushions the brain helps to remove waste that has built up during the day. In a 2019 study, Lewis and colleagues showed that CSF flow during sleep follows a rhythmic pattern in and out of the brain, and that these flows are linked to changes in brain waves during sleep.That finding led Lewis to wonder what might happen to CSF flow after sleep deprivation. To explore that question, she and her colleagues recruited 26 volunteers who were tested twice — once following a night of sleep deprivation in the lab, and once when they were well-rested.In the morning, the researchers monitored several different measures of brain and body function as the participants performed a task that is commonly used to evaluate the effects of sleep deprivation.During the task, each participant wore an electroencephalogram (EEG) cap that could record brain waves while they were also in a functional magnetic resonance imaging (fMRI) scanner. The researchers used a modified version of fMRI that allowed them to measure not only blood oxygenation in the brain, but also the flow of CSF in and out of the brain. They also measured each subject’s heart rate, breathing rate, and pupil diameter.The participants performed two attentional tasks while in the fMRI scanner, one visual and one auditory. For the visual task, they had to look at a screen that had a fixed cross. At random intervals, the cross would turn into a square, and the participants were told to press a button whenever they saw this happen. For the auditory task, they would hear a beep instead of seeing a visual transformation.Sleep-deprived participants performed much worse than well-rested participants on these tasks, as expected. Their response times were slower, and for some of the stimuli, the participants never registered the change at all.During these momentary lapses of attention, the researchers identified several physiological changes that occurred at the same time. Most significantly, they found a flux of CSF out of the brain just as those lapses occurred. After each lapse, CSF flowed back into the brain.“The results are suggesting that at the moment that attention fails, this fluid is actually being expelled outward away from the brain. And when attention recovers, it’s drawn back in,” Lewis says.The researchers hypothesize that when the brain is sleep-deprived, it begins to compensate for the loss of the cleansing that normally occurs during sleep, even though these pulses of CSF flow come with the cost of attention loss.“One way to think about those events is because your brain is so in need of sleep, it tries its best to enter into a sleep-like state to restore some cognitive functions,” Yang says. “Your brain’s fluid system is trying to restore function by pushing the brain to iterate between high-attention and high-flow states.”The researchers also found several other physiological events linked to attentional lapses, including decreases in breathing and heart rate, along with constriction of the pupils. They found that pupil constriction began about 12 seconds before CSF flowed out of the brain, and pupils dilated again after the attentional lapse.“What’s interesting is it seems like this isn’t just a phenomenon in the brain, it’s also a body-wide event. It suggests that there’s a tight coordination of these systems, where when your attention fails, you might feel it perceptually and psychologically, but it’s also reflecting an event that’s happening throughout the brain and body,” Lewis says.This close linkage between disparate events may indicate that there is a single circuit that controls both attention and bodily functions such as fluid flow, heart rate, and arousal, according to the researchers.“These results suggest to us that there’s a unified circuit that’s governing both what we think of as very high-level functions of the brain — our attention, our ability to perceive and respond to the world — and then also really basic fundamental physiological processes like fluid dynamics of the brain, brain-wide blood flow, and blood vessel constriction,” Lewis says.In this study, the researchers did not explore what circuit might be controlling this switching, but one good candidate, they say, is the noradrenergic system. Recent research has shown that this system, which regulates many cognitive and bodily functions through the neurotransmitter norepinephrine, oscillates during normal sleep.The research was funded by the National Institutes of Health, a National Defense Science and Engineering Graduate Research Fellowship, a NAWA Fellowship, a McKnight Scholar Award, a Sloan Fellowship, a Pew Biomedical Scholar Award, a One Mind Rising Star Award, and the Simons Collaboration on Plasticity in the Aging Brain.]]></content:encoded></item><item><title>How OpenAI uses complex and circular deals to fuel its multibillion-dollar rise</title><link>https://www.nytimes.com/interactive/2025/10/31/technology/openai-fundraising-deals.html</link><author>reaperducer</author><category>hn</category><pubDate>Fri, 31 Oct 2025 13:03:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Sam Altman, the chief executive of OpenAI, says that technological revolutions are driven by more than just technology. They are also driven, he argues, by new ways of paying for them.“There is always a lot of focus on technological innovation. What really drives a lot of progress is when people also figure out how to innovate on the financial model,” he recently said at the site of a data center that OpenAI is building in Abilene, Texas.Over the last several years, Mr. Altman’s company has found unusual and creative ways of paying for the computing power needed to fuel its ambitions.Many of the deals OpenAI has struck — with chipmakers, cloud computing companies and others — are strangely circular. OpenAI receives billions from tech companies before sending those billions back to the same companies to pay for computing power and other services.Industry experts and financial analysts have welcomed the start-up’s creativity. But these unorthodox arrangements have also fueled concerns that OpenAI is helping to inflate a potential financial bubble as it builds what is still a highly speculative technology.Here are unusual financial agreements helping to drive the ambitions of OpenAI, the poster child of the artificial intelligence revolution.From 2019 through 2023, Microsoft was OpenAI’s primary investor. The tech giant pumped more than  into the start-up. Then OpenAI funneled most of those billions back into Microsoft, buying  needed to fuel the development of new A.I. technologies.(The New York Times has sued OpenAI and Microsoft, claiming copyright infringement of news content related to A.I. systems. The two companies have denied the suit’s claims.)By the summer of last year, OpenAI could not get all the computing power it wanted from Microsoft. So it started signing cloud computing contracts with other companies, including Oracle and little-known start-ups with names like CoreWeave.Across three different deals signed this year, OpenAI agreed to pay CoreWeave, a company that builds A.I. data centers, more than  for computing power. As part of these agreements, OpenAI received  in CoreWeave stock, which could ultimately help pay for this computing power.OpenAI also struggled to get the additional investment dollars it wanted from Microsoft. So, it turned to other investors. Earlier this year, the Japanese conglomerate SoftBank led a  investment in OpenAI.At the same time, OpenAI has been working with various companies to build its own computing data centers, rather than rely on cloud computing deals. This also includes SoftBank, which is known for highly speculative technological bets that don’t always pay off. The company is raising  to help OpenAI build data centers in Texas and Ohio.Similarly, Oracle, a software and cloud computing giant, has agreed to spend  building new data centers for OpenAI in Texas, New Mexico, Michigan and Wisconsin. OpenAI will then pay Oracle roughly the same amount to use these  over the next several years.The United Arab Emirates was part of an OpenAI’s fund-raising round in October 2024. Now, G42, a firm with close ties to the Emirati government, is building a roughly  for OpenAI in the Emirates.Last month, Nvidia announced that it intended to invest  in OpenAI over the next several years. This could help OpenAI pay for its new data centers. As OpenAI buys or leases specialized chips from Nvidia, Nvidia will pump billions back into OpenAI.Two weeks later, OpenAI signed an agreement with AMD that allows OpenAI to buy up to  in the chipmaker at a penny per share. That translates to roughly a 10 percent stake in the company. This stock could supply OpenAI with additional capital as it works to build new data centers.OpenAI pulls in billions of dollars in revenue each year from customers who pay for ChatGPT, computer programming tools and other technologies. But it still loses more money than it makes, according to a person familiar with the company’s finances.If the company can use its new data centers to significantly improve A.I. technologies and expand its revenue over the next several years, it can become a viable business, as Mr. Altman believes it will. If technology progress stalls, OpenAI – and its many partners – could lose enormous amounts of money. Smaller companies like CoreWeave, which are taking on enormous amounts of debt to build new data centers, could go bankrupt.In some cases, companies are hedging their bets. Nvidia and AMD, for instance, have the option of reducing the cash and stock they send to OpenAI if the A.I. market does not expand as quickly as expected. But others would be left with enormous debt, which could send ripples across the larger economy.]]></content:encoded></item><item><title>Perfetto: Swiss army knife for Linux client tracing</title><link>https://lalitm.com/perfetto-swiss-army-knife/</link><author>todsacerdoti</author><category>hn</category><pubDate>Fri, 31 Oct 2025 11:54:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The talk was recorded and is available on YouTube. Taking inspiration from Simon Willison, this post is an annotated presentation containing my slides and detailed notes on them. The talk also has a lot of UI demos: for these, I’ll have a screenshot but also a link to the relevant part of the video (videos are unbeatable for UI!).First, what is Perfetto? Perfetto is fundamentally a suite of tools: it’s not just one thing but a bunch of different tools working together to help you debug and root-cause problems. This diagram shows how everything fits together, with the core parts of the Perfetto project in the center.The recording tools for Perfetto consist of 1) an SDK for C++ apps 2) a daemon that can collect data from ftrace, /proc, /sys, and various kernel interfaces 3) another daemon that amalgamates trace data from multiple processes into a single trace file. These tools all speak the Perfetto protobuf format, a high-performance trace format designed to be very efficient to write but not to analyze or consume directly.That’s where the trace processor comes in. It’s a C++ library that parses the protobuf format, but also a bunch of other widely used trace formats. It exposes this data via an SQL query interface to any embedding program including Perfetto UI (which is what most of this talk is about) and also to the Python API if you want to do ad-hoc scripting or analysis in Python.There are also very common tracing/profiling formats used by the Linux community: perf.data, ftrace text format, Firefox profiler format, and many others. Perfetto supports quite a few of those directly. There’s also the Chrome JSON format (AKA the Trace Event Format) which is a simpler interchange format. It’s not the most efficient to read or write, but it does the job for a lot of use cases.Often people write converters. They have their own tracing format, maybe proprietary, maybe open source but something we don’t know about, and it’s very common that people convert to one of the formats we understand, most commonly our protobuf or Chrome JSON formats.The Perfetto UI is fundamentally a web-based trace visualizer, combining timeline visualization, user-driven selection/aggregation, and SQL queries all in one interface. Because it has the trace processor as a backend, it works with a bunch of different trace formats.It’s very important to note that even though the Perfetto UI is web-based, everything happens inside your browser and trace data never leaves your system. You can even build it and host it yourself on any static server: we’ve made it extremely easy to do so!At the start of 2025, we actually moved our whole development to GitHub. In the past, we used to develop on Android and GitHub was just a mirror. That’s no longer the case, GitHub is actually where we develop and take pull requests.Most of this talk, I’m going to spend actually showing you how you can use the Perfetto UI to debug performance issues on Linux. I don’t want to show you an Android trace which needs a lot of context about how the Android system works and so you think, “oh, that was cool, but I didn’t really understand what was happening.”So to make this talk more approachable, I wrote a straightforward demo program you can look at yourself! So it’s obviously not a production system but I’ve tried to make it as representative of the sort of issues we use Perfetto for every day.It’s a Rust program which generates a Julia set and visualizes it over time. The technologies I used: Vulkan, GPU rendering and also multi-threaded CPU computation. So how it works is that computation of various parameters is happening on background threads, and then that’s being passed to the main thread for rendering.And then, for demonstration purposes, there is a performance bug; rendering should run at 60 FPS, but every so often, the frame rate drops dramatically. Here’s what that looks like:The code is on GitHub and if you’re interested in following along. The traces are there as well - you don’t have to collect the traces yourself, but you can if you want. All the instructions and information is in the README.So the first suspicion we may have is that maybe it’s some CPU problem. A lot of engineers I know would reach for perf immediately whenever they see a problem like this. The main reason is that if perf can capture the problem, they can go straight to the line of code without needing to spend time debugging using more complex approaches.You can record a perf profile with perf record -k mono -g ./fractal_renderer. The standard post-processing step which I think a lot of people do would be to generate an SVG flame graph out of this with perf script | stack-collapse-perf.pl --all | flamegraph.pl > fractal-frame.svg. Here’s how that looks like for our perf profile:The flame graph shows the thread names at the bottom, and then stacked above are the call stacks of what the program is doing. The width represents how much time is spent in each function. In this case, the worker threads are spending most of their time in  and  operations, doing math computation. Exactly what you’d expect for this type of program. Aside: Interestingly demangling didn’t seem to work for Rust out of the box which I find a bit unusual but I didn’t dig too much into why this was.Looking at this, nothing really stands out. There’s no smoking gun that immediately reveals the problem. But here’s the fundamental limitation: the flame graph aggregates data across the entire trace, losing the time dimension. The performance problem we’re seeing happens every 2-3 seconds - brief drops in an otherwise normally functioning program.The flame graph shows me the aggregate, dominated by the correct behavior, making it nearly impossible to spot those occasional problematic moments. So how do I find the places where it’s doing the wrong thing?Well, that’s maybe where Perfetto can help you out a little bit! The thing I find lots of people don’t know is that perf actually  timestamp information about when samples were taken: many tools drop this information but Perfetto is pretty good at showing that to you. I just need post-process the trace with perf script > fractal.perftext to generate a text version of the profile which we can then open in the Perfetto UIMy demo talking through how to open the perf profile and navigate it starts at 9:11 in the video.The x-axis of what you’re seeing is time. And every horizontal line represents a thread: we call these lanes “tracks”. And each of the small arrowheads on the tracks are CPU samples for the associated thread.The behavior of the worker threads over time is really interesting. Most of the lifetime of the program, it’s doing continuous computation, basically. And then there’s this period of time in the middle where there’s this very interesting staircase pattern where it seems like only one thread is running at any one time: we’ll want to keep a note of this as this is quite important.One very cool thing about Perfetto is that it allows me to quickly generate visualizations of whatever I’m selecting on the timeline. We call this “area selection” and it’s where I drag my mouse and select a region both horizontally and vertically. This is on one track but even  multiple tracks. So in the timeline it shows me the selection I’m making at the top, plus a flame graph at the bottom representing the aggregation of the samples in just that time period.The key advantage is that I can look at individual regions of time interactively. You can also do this with  but you need to pre-filter the data to what you’re looking for. This assumes you already know what the problem is before you find it. I always find that a bit counterintuitive. I prefer this view where I can see everything first, then decide “I want to look at this specific part in more detail.” That’s what Perfetto lets me do.So now on to the flame graph itself: in the last year, I spent a bunch of time on improving the flame graph visualization in Perfetto UI. At Google, we have an internal tool called pprof (related to but not the same as the open-source one), and I’ve always loved the visualization it has. So I worked to make Perfetto’s flame graph look and behave very similarly.There are a bunch of features here. You can zoom in like most flame graph, but you can also say “I only want to look at the stack starting from this point” — it gets rid of everything above and starts fresh from there. If I don’t want to see a particular function like , I can just hide the frame and it gets merged into its parent. The search is regex-based so I can also just type the specific things I’m interested in: it’s pretty powerful.There’s also something I don’t think I’ve seen in other visualizers (I might be wrong, please do correct me!): what I call “bottom up” view. Imagine you take all the leaves across your entire program and visualize who’s calling into them. If you have a leaf function that’s called everywhere in your program but reached from many different places, it’ll be split across your flame chart. This is the inverse — you’re looking at the leaves and asking “who is calling me?” This is particularly useful when you’re trying to optimize very low-level functions that are always at the leaf level — things like memcpy or hashmap operations.So our main takeaway is that our worker threads all had gaps in their CPU execution in a staircase pattern. This means the threads weren’t actually on the CPU and that’s usually a sign that they’re sleeping.But sleeping on what? Locks? Disk? Network? What exactly is happening? To answer that, I need a scheduler trace to show me the wake-up patterns, who’s being scheduled when, what type of sleep threads are in, that sort of thing.We recently added support for trace-cmd’s text format in Perfetto (aside: there’s an open issue to support the binary format too!). For this demo, I’m collecting sched_switch and sched_waking events with sudo trace-cmd record -e sched:sched_switch -e sched:sched_waking -C mono ./fractal-renderer-vk and we can open it in the UI once we do trace-cmd report -N > fractal.sched.The demo showing off scheduler traces starts at 15:16 in the video.So this is what a scheduler trace looks like. Again as before, the x-axis is time. At the top of the trace, you’ve got the CPU scheduling tracks. For each CPU, they tell us what was scheduled on that CPU at each point in time. The white regions are where there’s nothing scheduled. As I zoom in, you start to see information like the name of the thread that’s scheduled, the TID of the thread, and so on. This is the CPU view of things, looking at it from a CPU perspective.There’s also a thread perspective on scheduling: what we call this “thread state”. This shows the scheduling state of each thread at every point in time. Dark green represents the thread running, light green represents runnable (the thread is on the runqueue so it’s eligible to run, but the scheduler hasn’t actually let it run yet), and white represents interruptible sleep (S sleep). There are also orange regions for uninterruptible sleep (D sleep), though that doesn’t show up much in this trace.You can again see a very clear staircase-like pattern. Again, only one thread seems to be running at any one time. You’re maybe getting the sense of what the problem might be at this point.Like with the perf visualization, you can do area selections here too. I can select a region on a CPU and get tables showing time spent by each thread (shown in the screenshot above). I can also do this for thread state, you can also see that runnable time is basically negligible compared to running and sleeping, which is where most of the time is being spent.There are also scheduler-specific visualizations like run queue length and active CPU count. You can see in the problematic region, the active CPU count also went down, which makes sense.So the scheduler trace shows when threads are sleeping, but it’s not telling us why. What was the program doing in that region? What code was it running? The sequential pattern suggests some sort of serialization—something is causing only one thread to run at a time. But we need application-level visibility.Since this program was written in Rust, I just used the off-the-shelf  and  crates; I did have to make some small modifications to tracing-perfetto that I’m hoping to upstream. These output a Perfetto trace without needing to use the Perfetto SDK. These libraries are also maintained by others: we don’t have any hand in them.All I needed to do was integrate these into my program and then add a command line switch to write out the collected trace to a file. So I just do ./fractal_renderer --trace fractal.pftraceMy demo for app tracing starts at 19:37 in the video.Looking at the app trace: as before x-axis is time and each of the tracks represents a thread. Instead of scheduling activity or CPU stack samples, this time the tracks is userspace instrumentation of the program itself. Each of these rectangles is a named period of time representing what the program was doing at that time.There are lots of different names for these in the tracing world (e.g. spans, intervals) but we call these “slices”. The main thread is rendering frames as you would expect and the workers are each computing tiles of that frame, which eventually feed back to the main thread and sync to the GPU.There’s also this thing called flows, which shows the causal links between different pieces. Render frame is calling into all these places and causing them to happen. It’s sort of similar to wakeup graphs but for user space: basically, this is what caused me to start running this function.So you can see very clearly there’s a normal region and then a region where things are taking 1.8 seconds: almost certainly the cause of the frame drops.And if we look at the slice in question, it seems to be doing something called “update adaptive quality.” Basically, I wrote some code to dynamically change the rendering quality based on frame rate. If I’m running faster, I can render at higher quality. If I’m running slower, I can do lower quality. That’s what this adaptive quality thing is supposed to do.But clearly something has gone wrong. I’m causing frame drops because of updates to adaptive quality. A little bit ironic, to say the least. Now we know what the program is actually doing during that time span.So now we’ve looked at three different sources of debugging data separately. In perf, we got told there are few or no CPU samples and weird staircase patterns. In ftrace, we saw only one worker seems to be active and the rest are sleeping in interruptible sleep. And in app tracing, we see it’s because of adaptive quality updates that workers are working on, and they shouldn’t be doing it this way.Obviously, the theory is that it’s all the same problem. But we can confirm this theory with a very new (and still experimental) feature of the UI: trace merging.Note: as this part of Perfetto is still experimental, if you want to try it yourself, you need to go to plugins and enable the “MultiTraceOpen” plugin. Also, unlike I what say in the talk, you do  need to be on Autopush as all the features I showed in my talk are now available in Stable.The demo showing off merged traces starts at 24:32 in the video.This is now the merged trace file containing all the different information. You can see the CPU scheduling tracks like we had in the scheduler trace, the stack samples like we had in the perf trace, and the app instrumentation, all beside each other on one timeline.
You can very clearly see the adaptive quality update running, then this period where it ran, and then it was done. It’s very cool to be able to see the pattern of how my program is sleeping and where and what it was running on one timeline.The thing is, you’ve always been able to do this with Perfetto if you collect a Perfetto trace with all of these integrated. The new capability is that this is now being brought to traces you didn’t collect with Perfetto. Collect it with whatever you like, and we’ll still manage to do this visualization. That’s the new capability we have.There’s also a query interface which we’ve been building out recently and there are some pretty powerful things you can do with these tables. As well as the flat tables of data, there’s also a more dynamic pivot table and if I click the arrow on the left, I can get into a detailed filter table, similar to a spreadsheet but optimize for things people want to do on traces.For example, by doing an area select, I can first get a list of all the events that happened during a time region. Then using the table I can filter for things - say I only care about slices longer than some duration, like 319 microseconds for whatever reason. I can click and add a filter for things greater than this.There’s also a feature called “Show Debug Track” that’s also very powerful. The table shows rows with timestamped duration information, and this feature lets you visualize that data as a track on the timeline. It adds a track at the top showing the regions of time where these events happened—in this case, where ComputeTile took longer than a certain threshold.This is particularly useful for correlation analysis. For example, in Android, we’re often looking at system A and trying to understand its effect on system B. We find all the places where one thing is slow, then look for correlations. Being able to see a track in the UI where you can quickly have context and say “oh, during this period of time, this other thing was happening” is invaluable. It’s probably one of our most-used features.You can also copy the SQL query from these tables and start doing your own aggregations on top of it. This eases the burden of starting with a completely blank canvas. Instead of wondering “where do I even start querying my data? What tables is my data in?”, the UI gives you a starting query that you can build on without needing to write something from scratch.So the dynamic quality updates were stopping the world. Perf profiling showed the problem, scheduler traces found the sleeping pattern, app tracing confirmed it was the quality adjustment code, and the single timeline view let me see everything happening at once.Interestingly, I tried to fix this by removing the lock contention—which I thought was the bug I had introduced. But it turns out I shouldn’t have been doing this code in the workers in the first place. Even after removing the lock contention, just the CPU activity of doing that work was enough to cause frame drops. The right solution was to move it to a background thread. As part of debugging this for the demo, I discovered something even better that I could be doing.Android and Chrome are our bread and butter—that’s what we officially support as a team, and why Google staffs us. But there are many other interesting uses.Mesa uses Perfetto as one of its tracing systems. One thing I could have shown is collecting a Mesa trace alongside all the other traces we looked at—you can actually see what the GPU is doing at the same time, which would have been very cool, but I just didn’t have time.VizTracer is function tracing for Python, similar to uftrace, but you don’t have to recompile anything or do anything special.pthread_trace is for visualizing pthread mutex contention. The author has a very efficient way of doing this and writes protobuf using heavy use of constexpr to make it very low overhead. It’s a very interesting project.magic-trace uses Intel Processor Trace for function-level tracing at the processor level with lower overhead. They wrote a converter from that to the Perfetto format so you can visualize that data in Perfetto.Qais Yousef’s sched-analyzer enriches Perfetto traces with scheduler internals. It’s a very cool project that I find particularly interesting.Finally, Josef Bacik’s systing is a bit experimental but fascinating. He re-implemented tracing daemons on top of BPF instead of Perfetto’s native implementation, combining BPF-based tracing with perf stack traces in a single binary. He has a blog post explaining why. I thought it was a fascinating use of Perfetto.If you want to try this yourself, you can use the demo program I’ve provided. But even better: record traces on your own programs. Open scheduler traces and perf at the same time. Instrument your program, convert it to Perfetto format, and visualize everything together.If you want to convert ad-hoc timestamped data to Perfetto, we wrote a comprehensive tutorial with Python snippets for everything you might want to visualize. It covers all the features I showed and how to write Python code to generate them. We have a library for writing these traces: besides this one library, you don’t need to install anything else. You can go ahead and convert your own data to Perfetto.We’re very happy to accept contributions and review pull requests pretty quickly. We’ve had a lot of open-source contributors over the years and have been quite lucky with some very high-quality contributions. If you want to contribute yourself or have a feature you feel is missing, send it our way.]]></content:encoded></item><item><title>The cryptography behind electronic passports</title><link>https://blog.trailofbits.com/2025/10/31/the-cryptography-behind-electronic-passports/</link><author>tatersolid</author><category>hn</category><pubDate>Fri, 31 Oct 2025 11:33:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Did you know that most modern passports are actually embedded devices containing an entire filesystem, access controls, and support for several cryptographic protocols? Such passports display a small symbol indicating an electronic machine-readable travel document (eMRTD), which digitally stores the same personal data printed in traditional passport booklets in its embedded filesystem. Beyond allowing travelers in some countries to skip a chat at border control, these documents use cryptography to prevent unauthorized reading, eavesdropping, forgery, and copying.This blog post describes how electronic passports work, the threats within their threat model, and how they protect against those threats using cryptography. It also discusses the implications of using electronic passports for novel applications, such as zero-knowledge identity proofs. Like many widely used electronic devices with long lifetimes, electronic passports and the systems interacting with them support insecure, legacy protocols that put passport holders at risk for both standard and novel use cases.Electronic passport basicsA passport serves as official identity documentation, primarily for international travel. The International Civil Aviation Organization (ICAO) defines the standards for electronic passports, which (as suggested by the “Chip Inside” symbol) contain a contactless integrated circuit (IC) storing digital information. Essentially, the chip contains a filesystem with some access control to protect unauthorized reading of data. The full technical details of electronic passports are specified in ICAO Doc 9303; this blog post will mostly focus on part 10, which specifies the logical data structure (LDS), and part 11, which specifies the security mechanisms.The filesystem architecture is straightforward, comprising three file types: master files (MFs) serving as the root directory; dedicated files (DFs) functioning as subdirectories or applications; and elementary files (EFs) containing actual binary data. As shown in the above figure, some files are mandatory, whereas others are optional. This blog post will focus on the eMRTD application. The other applications are part of LDS 2.0, which would allow the digital storage of travel records (digital stamps!), electronic visas, and additional biometrics (so you can just update your picture instead of getting a whole new passport!).How the eMRTD application worksThe following figure shows the types of files the eMRTD contains:There are generic files containing common or security-related data; all other files are so-called data groups (DGs), which primarily contain personal information (most of which is also printed on your passport) and some additional security data that will become important later. All electronic passports must contain DGs 1 and 2, whereas the rest is optional.Comparing the contents of DG1 and DG2 to the main passport page shows that most of the written data is stored in DG1 and the photo is stored in DG2. Additionally, there are two lines of characters at the bottom of the page called the machine readable zone (MRZ), which contains another copy of the DG1 data with some check digits, as shown in the following picture.Digging into the threat modelElectronic passports operate under a straightforward threat model that categorizes attackers based on physical access: those who hold a passport versus those who don’t. If you are near a passport but you do not hold it in your possession, you should not be able to do any of the following:Read any personal information from that passportEavesdrop on communication that the passport has with legitimate terminalsFigure out whether it is a specific passport so you can trace its movementsEven if you do hold one or more passports, you should not be able to do the following:Forge a new passport with inauthentic dataMake a digital copy of the passportRead the fingerprint (DG3) or iris (DG4) informationElectronic passports use short-range RFID for communication (ISO 14443). You can communicate with a passport within a distance of 10–15 centimeters, but eavesdropping is possible at distances of several meters. Because electronic passports are embedded devices, they need to be able to withstand attacks where the attacker has physical access to the device, such as elaborate side-channel and fault injection attacks. As a result, they are often certified (e.g., under Common Criteria).We focus here on the threats against the electronic components of the passport. Passports have many physical countermeasures, such as visual effects that become visible under certain types of light. Even if someone can break the electronic security that prevents copying passports, they would still have to defeat these physical measures to make a full copy of the passport. That said, some systems (such as online systems) only interact digitally with the passport, so they do not perform any physical checks at all.The earliest electronic passports lacked most cryptographic mechanisms. Malaysia issued the first electronic passport in 1998, which predates the first ICAO eMRTD specifications from 2003. Belgium subsequently issued the first ICAO-compliant eMRTD in 2004, which in turn predates the first cryptographic mechanism for confidentiality specified in 2005.While we could focus solely on the most advanced cryptographic implementations, electronic passports remain in circulation for extended periods (typically 5–10 years), meaning legacy systems continue operating alongside modern solutions. This means that there are typically many old passports floating around that do not support the latest and greatest access control mechanisms. Similarly, not all inspection systems/terminals support all of the protocols, which means passports potentially need to support multiple protocols. All protocols discussed in the following are described in more detail in ICAO Doc 9303 Part 11.Legacy protection mechanisms for electronic passports provide better security than what they were replacing (nothing), even though they have key shortcomings regarding confidentiality and (to a lesser extent) copying.Legacy confidentiality protections: How basic access control failsIn order to prevent eavesdropping, you need to set up a secure channel. Typically, this is done by deriving a shared symmetric key, either from some shared knowledge, or through a key exchange. However, the passport cannot have its own static public key and send it over the communication channel, because this would enable tracing of specific passports.Additionally, it should only be possible to set up this secure channel if you have the passport in your possession. So, what sets holders apart from others? Holders can read the physical passport page that contains the MRZ!This brings us to the original solution to set up a secure channel with electronic passports: basic access control (BAC). When you place your passport with the photo page face down into an inspection system at the airport, it scans the page and reads the MRZ. Now, both sides derive encryption and message authentication code (MAC) keys from parts of the MRZ data using SHA-1 as a KDF. Then, they exchange freshly generated challenges and encrypt-then-MAC these challenges together with some fresh keying material to prove that both sides know the key. Finally, they derive session keys from the keying material and use them to set up the secure channel.However, BAC fails to achieve any of its security objectives. The static MRZ is just some personal data and does not have very high entropy, which makes it guessable. Even worse, if you capture one valid exchange between passport and terminal, you can brute-force the MRZ offline by computing a bunch of unhardened hashes. Moreover, passive listeners who know the MRZ can decrypt all communications with the passport. Finally, the fact that the passport has to check both the MAC and the challenge has opened up the potential for oracle attacks that allow tracing by replaying valid terminal responses.Forgery prevention: Got it right the first timePreventing forgery is relatively simple. The passport contains a file called the Document Security Object (EF.SOD), which contains a list of hashes of all the Data Groups, and a signature over all these hashes. This signature comes from a key pair that has a certificate chain back to the Country Signing Certificate Authority (CSCA). The private key associated with the CSCA certificate is one of the most valuable assets in this system, because anyone in possession of this private key can issue legitimate passports containing arbitrary data.The process of reading the passport, comparing all contents to the SOD, and verifying the signature and certificate chain is called passive authentication (PA). This will prove that the data in the passport was signed by the issuing country. However, it does nothing to prevent the copying of existing passports: anyone who can read a passport can copy its data into a new chip and it will pass PA. While this mechanism is listed among the legacy ones, it meets all of its objectives and is therefore still used without changes.Legacy copying protections: They work, but some issues remainPreventing copying requires having something in the passport that cannot be read or extracted, like the private key of a key pair. But how does a terminal know that a key pair belongs to a genuine passport? Since countries are already signing the contents of the passport for PA, they can just put the public key in one of the data groups (DG15), and use the private key to sign challenges that the terminal sends. This is called active authentication (AA). After performing both PA and AA, the terminal knows that the data in the passport (including the AA public key) was signed by the government and that the passport contains the corresponding private key.This solution has two issues: the AA signature is not tied to the secure channel, so you can relay a signature and pretend that the passport is somewhere it’s not. Additionally, the passport signs an arbitrary challenge without knowing the semantics of this message, which is generally considered a dangerous practice in cryptography.Extended Access Control (EAC) fixes some of the issues related to BAC and AA. It comprises chip authentication (CA), which is a better AA, and terminal authentication (TA), which authenticates the terminal to the passport in order to protect access to the sensitive information stored in DG3 (fingerprint) and DG4 (iris). Finally, password authenticated connection establishment (PACE, described below) replaces BAC altogether, eliminating its weaknesses.Chip Authentication: Upgrading the secure channelCA is very similar to AA in the sense that it requires countries to simply store a public key in one of the DGs (DG14), which is then authenticated using PA. However, instead of signing a challenge, the passport uses the key pair to perform a static-ephemeral Diffie-Hellman key exchange with the terminal, and uses the resulting keys to upgrade the secure channel from BAC. This means that passive listeners that know the MRZ cannot eavesdrop after doing CA, because they were not part of the key exchange.Terminal Authentication: Protecting sensitive data in DG3 and DG4Similar to the CSCA for signing things, each country has a Country Verification Certificate Authority (CVCA), which creates a root certificate for a PKI that authorizes terminals to read DG3 and DG4 in the passports of that country. Terminals provide a certificate chain for their public key and sign a challenge provided by the passport using their private key. The CVCA can authorize document verifiers (DVs) to read one or both of DG3 and DG4, which is encoded in the certificate. The DV then issues certificates to individual terminals. Without such a certificate, it is not possible to access the sensitive data in DG3 and DG4.Password Authenticated Connection Establishment: Fixing the basic problemsThe main idea behind PACE is that the MRZ, much like a password, does not have sufficient entropy to protect the data it contains. Therefore, it should not be used directly to derive keys, because this would enable offline brute-force attacks. PACE can work with various mappings, but we describe only the simplest one in the following, which is the generic mapping. Likewise, PACE can work with other passwords besides the MRZ (such as a PIN), but this blog post focuses on the MRZ.First, both sides use the MRZ data (the password) to derive a password key. Next, the passport encrypts a nonce using the password key and sends it to the terminal, which can decrypt it if it knows the password. The terminal and passport also perform an ephemeral Diffie-Hellman key exchange. Now, both terminal and passport derive a new generator of the elliptic curve by applying the nonce as an additive tweak to the (EC)DH shared secret. Using this new generator, the terminal and passport perform another (EC)DH to get a second shared secret. Finally, they use this second shared secret to derive session keys, which are used to authenticate the (EC)DH public keys that they used earlier on in the protocol, and to set up the secure channel. Figure 6 shows a simplified protocol diagram.Anyone who does not know the password cannot follow the protocol to the end, which will become apparent in the final step when they need to authenticate the data with the session keys. Before authenticating the terminal, the passport does not share any data that enables brute-forcing the password key. Non-participants who do know the password cannot derive the session keys because they do not know the ECDH private keys.Gaps in the threat model: Why you shouldn’t give your passport to just anyoneWhen considering potential solutions to maintaining passports’ confidentiality and authenticity, it’s important to account for what the inspection system  with your passport, and not just the fancy cryptography the passport supports. If an inspection system performs only BAC/PACE and PA, anyone who has seen your passport could make an electronic copy and pretend to be you when interacting with this system. This is true even if your passport supports AA or CA.Another important factor is tracing: the specifications aim to ensure that someone who does not know a passport’s PACE password (MRZ data in most cases) cannot trace that passport’s movements by interacting with it or eavesdropping on communications it has with legitimate terminals. They attempt to achieve this by ensuring that passports always provide random identifiers (e.g., as part of Type A or Type B ISO 14443 contactless communication protocols) and that the contents of publicly accessible files (e.g., those containing information necessary for performing PACE) are the same for every citizen of a particular country.However, all of these protections go out of the window when the attacker knows the password. If you are entering another country and border control scans your passport, they can provide your passport contents to others, enabling them to track the movements of your passport. If you visit a hotel in Italy and they store a scan of your passport and get hacked, anyone with access to this information can track your passport. This method can be a bit onerous, as it requires contacting various nearby contactless communication devices and trying to authenticate to them as if they were your passport. However, some may still choose to include it in their threat models.Some countries state in their issued passports that the holder should give it to someone else only if there is a statutory need. At Italian hotels, for example, it is sufficient to provide a prepared copy of the passport’s photo page with most data redacted (such as your photo, signature, and any personal identification numbers). In practice, not many people do this.Even without the passport, the threat model says nothing about tracking particular groups of people. Countries typically buy large quantities of the same electronic passports, which comprise a combination of an IC and the embedded software implementing the passport specifications. This means that people from the same country likely have the same model of passport, with a unique fingerprint comprising characteristics like communication time, execution time, supported protocols (ISO 14443 Type A vs Type B), etc. Furthermore, each country may use different parameters for PACE (supported curves or mappings, etc.), which may aid an attacker in fingerprinting different types of passports, as these parameters are stored in publicly readable files.Security and privacy implications of zero-knowledge identity proofsAn emerging approach in both academic research and industry applications involves using zero-knowledge (ZK) proofs with identity documents, enabling verification of specific identity attributes without revealing complete document contents. This is a nice idea in theory, because this will allow proper use of passports where there is no statutory need to hand over your passport. However, there are security implications.First of all, passports cannot generate ZK proofs by themselves, so this necessarily involves exposing your passport to a prover. Letting anyone or anything read your passport means that you downgrade your threat model with respect to that entity. So when you provide your passport to an app or website for the purposes of creating a ZK proof, you need to consider what they will do with the information in your passport. Will it be processed locally on your device, or will it be sent to a server? If the data leaves your device, will it be encrypted and only handled inside a trusted execution environment (TEE)? If so, has this whole stack been audited, including against malicious TEE operators?Second, if the ZK proving service relies on PA for its proofs, then anyone who has ever seen your passport can pretend to be you on this service. Full security requires AA or CA. As long as there exists any service that relies only on PA, anyone whose passport data is exposed is vulnerable to impersonation. Even if the ZK proving service does not incorporate AA or CA in their proofs, they should still perform one of these procedures with the passport to ensure that only legitimate passports sign up for this service.Finally, the system needs to consider what happens when people share their ZK proof with others. The nice thing about a passport is that you cannot easily make copies (if AA or CA is used), but if I can allow others to use my ZK proof, then the value of the identification decreases.It is important that such systems are audited for security, both from the point of view of the user and the service provider. If you’re implementing ZK proofs of identity documents, contact us to evaluate your design and implementation.]]></content:encoded></item><item><title>Claude outage</title><link>https://status.claude.com/incidents/s5f75jhwjs6g</link><author>stuartmemo</author><category>hn</category><pubDate>Fri, 31 Oct 2025 10:15:24 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
          Subscribe to updates for Elevated errors on claude.ai via email and/or text message. You'll receive email notifications when incidents are updated, and text message notifications whenever Claude  or  an incident.
        ]]></content:encoded></item><item><title>My Impressions of the MacBook Pro M4</title><link>https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/</link><author>secure</author><category>hn</category><pubDate>Fri, 31 Oct 2025 10:13:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I have been using a MacBook Pro M4 as my portable computer for the last half a
year and wanted to share a few short impressions. As always, I am not a
professional laptop reviewer, so in this article you won’t find benchmarks, just
subjective thoughts!Back in 2021, I wrote about the MacBook Air
M1, which was the first computer I used that
contained Apple’s own ARM-based CPU. Having a silent laptop with long battery
life was a game-changer, so I wanted to keep those properties.When the US government announced tariffs, I figured I would replace my 4-year
old MacBook Air M1 with a more recent model that should last a few more
years. Ultimately, Apple’s prices remained stable, so, in retrospect, I could
have stayed with the M1 for a few more years. Oh well.The nano-textured displayI went to the Apple Store to compare the different options in
person. Specifically, I was curious about the display and whether the increased
weight and form factor of the MacBook Pro (compared to a MacBook Air) would be
acceptable. Another downside of the Pro model is that it comes with a fan, and I
really like absolutely quiet computers. Online, I read from other MacBook Pro
owners that the fan mostly stays off.In general, I would have preferred to go with a MacBook Air because it has
enough compute power for my needs and I like the case better (no ventilation
slots), but unfortunately only the MacBook Pro line has the better displays.Why aren’t all displays nano-textured? The employee at the Apple Store presented
the trade-off as follows: The nano texture display is great at reducing
reflections, at the expense of also making the picture slightly less vibrant.I could immediately see the difference when placing two laptops side by side:
The bright Apple Store lights showed up very prominently on the normal display
(left), and were almost not visible at all on the nano texture display (right):Personally, I did not perceive a big difference in “vibrancy”, so my choice was
clear: I’ll pick the MacBook Pro over the MacBook Air (despite the weight) for
the nano texture display!After using the laptop in a number of situations, I am very happy with this
choice. In normal scenarios, I notice no reflections at all (where my previous
laptop did show reflections!). This includes using the laptop on a train (next
to the window), or using the laptop outside in daylight.(When I chose the new laptop, Apple’s M4 chips were current. By now, they have
released the first devices with M5 chips.)I decided to go with the MacBook Pro with M4 chip instead of the M4  chip
because I don’t need the extra compute, and the M4 needs less cooling — the M4
Pro apparently runs hotter. This increases the chance of the fan staying off.Here are the specs I ended up with:14" Liquid Retina XDR Display with nano textureApple M4 Chip (10 core CPU, 10 core GPU)32 GB RAM (this is the maximum!), 2 TB SSD (enough for this computer)One thing I noticed is that the MacBook Pro M4 sometimes gets warm, even when it
is connected to power, but is suspended to RAM (and has been fully charged for
hours). I’m not sure why.Luckily, the fan indeed stays silent. I think I might have heard it spin up once
in half a year or so?The battery life is amazing! The previous MacBook Air M1 had amazing all-day
battery life already, and this MacBook Pro M4 lasts even longer. For example,
watching videos on a train ride (with VLC) for 3 hours consumed only 10% of
battery life. I generally never even carry the charger.Because of that, Apple’s re-introduction of MagSafe, a magnetic power connector
(so you don’t damage the laptop when you trip over it), is nice-to-have but
doesn’t really make much of a difference anymore. In fact, it might be better to
pack a USB-C cable when traveling, as that makes you more flexible in how you
use the charger.I was curious whether the 120 Hz display would make a difference in practice. I
mostly notice the increased refresh rate when there are animations, but not,
for example, when scrolling.One surprising discovery (but obvious in retrospect) is that even non-animations
can become faster. For example, when running a Go web server on , I
noticed that navigating between pages by clicking links felt faster on the 120
Hz display!The following illustration shows why that is, using a page load that takes 6ms
of processing time. There are three cases (the illustration shows an average
case and the worst case):Best case: Page load finishes  the next frame is displayed: no delay.Worst case: Page load finishes  a frame is displayed: one frame of delay.Most page loads are somewhere in between. We’ll have 0.x to 1.0 frames of delayAs you can see, the waiting time becomes shorter when going from 60 Hz (one
frame every 16.6ms) to 120 Hz (one frame every 8.3ms). So if you’re working with
a system that has <8ms response times, you might observe actions completing (up
to) twice as fast!I don’t notice going back to 60 Hz displays on computers. However, on phones,
where a lot more animations are a key part of the user experience, I think 120
Hz displays are more interesting.My ideal MacBook would probably be a MacBook Air, but with the nano-texture display! :)I still don’t like macOS and would prefer to run Linux on this laptop. But
Asahi Linux still needs some work before it’s usable
for me (I need external display output, and M4 support). This doesn’t bother me
too much, though, as I don’t use this computer for serious work.]]></content:encoded></item><item><title>Reasoning models reason well, until they don&apos;t</title><link>https://arxiv.org/abs/2510.22371</link><author>optimalsolver</author><category>hn</category><pubDate>Fri, 31 Oct 2025 09:23:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AMD could enter ARM market with Sound Wave APU built on TSMC 3nm process</title><link>https://www.guru3d.com/story/amd-enters-arm-market-with-sound-wave-apu-built-on-tsmc-3nm-process/</link><author>walterbell</author><category>hn</category><pubDate>Fri, 31 Oct 2025 03:07:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[AMD is expanding its processor portfolio beyond the x86 architecture with its first ARM-based APU, internally known as “Sound Wave.” The chip’s existence was uncovered through customs import records, confirming several details about its design and purpose. Built with a BGA-1074 package measuring 32 mm × 27 mm, the processor fits within standard mobile SoC dimensions, making it suitable for thin and light computing platforms. It employs a 0.8 mm pitch and FF5 interface, replacing the FF3 socket previously used in Valve’s Steam handheld devices, further hinting at a new generation of compact AMD-powered hardware.
                                    According to leaks from industry insiders such as @Moore’s Law Is Dead and @KeplerL2, “Sound Wave” is manufactured on  and aims for a  range, positioning it directly against Qualcomm’s Snapdragon X Elite. The chip is expected to power future  products scheduled for release in 2026. four RDNA 3.5 compute unitsmachine learning accelerationMemory support is another highlight: the chip integrates a 128-bit LPDDR5X-9600 controller and will reportedly include , aligning with current trends in unified memory designs used in ARM SoCs. Additionally, the APU carries AMD’s fourth-generation AI engine, enabling on-device inference tasks and enhanced efficiency for workloads such as speech recognition, image analysis, and real-time translation.While AMD experimented with ARM over a decade ago through the abandoned “Project Skybridge,” this new effort represents a more mature and strategic approach. With industry interest in efficient, ARM-based computing accelerating, “Sound Wave” could help AMD diversify its portfolio while leveraging its strengths in graphics and AI acceleration. If reports are accurate, the processor will enter production in late 2025, with commercial devices expected the following year.]]></content:encoded></item><item><title>John Carmack on mutable variables</title><link>https://twitter.com/id_aa_carmack/status/1983593511703474196</link><author>azhenley</author><category>hn</category><pubDate>Fri, 31 Oct 2025 02:34:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ground stop at JFK due to staffing</title><link>https://www.fly.faa.gov/adv/adv_otherdis?advn=13&amp;adv_date=10312025&amp;facId=JFK&amp;title=ATCSCC%20ADVZY%20013%20JFK/ZNY%2010/31/2025%20CDM%20GROUND%20STOP&amp;titleDate=10/31/2025</link><author>akersten</author><category>hn</category><pubDate>Fri, 31 Oct 2025 01:48:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ICE and the Smartphone Panopticon</title><link>https://www.newyorker.com/culture/infinite-scroll/ice-and-the-smartphone-panopticon</link><author>fortran77</author><category>hn</category><pubDate>Fri, 31 Oct 2025 01:13:56 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Last week, as  raids ramped up in New York, city residents set about resisting in the ways they had available: confronting agents directly on sidewalks, haranguing them as they processed down blocks, and recording them on phone cameras held aloft. Relentless documentation has proved something of an effective tool against President Donald Trump’s empowerment of ; agents have taken to wearing masks in fear of exposure, and the proliferation of imagery showing armed police and mobilized National Guard troops in otherwise calm cities has underlined the cruel absurdity of their activities. Activist memes have been minted on social media: a woman on New York’s Canal Street, dressed in a polka-dotted office-casual dress, flipping  agents off; a man in Washington, D.C., throwing a Subway sandwich at a federal agent in August. The recent “No Kings” marches were filled with protesters in inflatable frog costumes, inspired by a similarly outfitted man who got pepper-sprayed protesting outside the U.S. Immigration and Customs Enforcement Building in Portland, Oregon. Some might write the memes off as resistance porn, but digital content is at least serving as a lively defense mechanism in the absence of functional politics.At the same time, social media has served as a reinvigorated source of transparency in recent weeks, harking back to the days when Twitter became an organizing tool during the Arab Spring, in the early twenty-tens, or when Facebook and Instagram helped fuel the Black Lives Matter marches of 2020. The grassroots optimism of that earlier social-media era is long gone, though, replaced by a sense of posting as a last resort. After Trump authorized the deployment of the National Guard in Chicago earlier this month, the governor of Illinois, J. B. Pritzker, told residents to “record and narrate what you see—put it on social media.” But, if the anti- opposition is taking advantage of the internet,  and the Trump Administration are, too. Right-wing creators have been using the same channels to identify and publicize targets for raids. According to reporting in Semafor, the Trump-friendly YouTuber Nick Shirley’s videos of African migrant vendors on Canal Street seemed to help drive recent  sweeps of the area.  itself is also working to monitor social media. The investigative outlet  found documents revealing that the agency has enlisted an A.I.-driven surveillance product called Zignal Labs that creates “curated detection feeds” to aid in criminal investigations. According to reporting in ,  also has plans to build out a team of dozens of analysts to monitor social media and identify targets. Recent videos, identified by 404 Media and other publications, have purportedly shown  agents using technology developed by the data-analytics firm Palantir, founded by Peter Thiel and others, to scan social-media accounts, government records, and biometrics data of those they detain. Social media has become a political panopticon in which your posts are a conduit for your politics, and what you post can increasingly be used against you.Meanwhile, a new wave of digital tools has emerged to help surveil the surveillants. The apps ICEBlock, Red Dot, and DEICER all allow users to pinpoint where  agents are active, forming an online version of a whisper network to alert potential targets. Eyes Up provides a way for users to record and upload footage of abusive law-enforcement activity, building an archive of potential evidence. Its creator is a software developer named Mark (who uses only his first name to separate the project from his professional work); he was inspired to create Eyes Up earlier this year, when he began seeing clips of  abductions and harassment circulating on social media and worried about their shelf life. As he put it to me, “They could disappear at any given moment, whether the platforms decide to moderate, whether the individual deletes their account or the post.”Ultimately, the app itself was also vulnerable to sudden disappearance. After launching, on September 1st, Eyes Up accumulated thousands of downloads and thousands of minutes of uploaded footage. Then, on October 3rd, Mark received a notice that Apple was removing the app from its store on the grounds that it may “harm a targeted individual or group.” Eyes Up is not alone. ICEBlock and Red Dot have been blocked from both Apple and Google’s app stores, the two largest marketplaces; DEICER, like Eyes Up, was removed by Apple. Pressure on the tech platforms seemed to come from the Trump Administration; after a deadly shooting at an  field office in Dallas in late September, the Attorney General, Pam Bondi, said in a statement to Fox News Digital that ICEBlock “put ICE agents at risk just for doing their jobs.” Mark is contesting Apple’s decision about Eyes Up through its official channels, and the creator of ICEBlock, Joshua Aaron, has argued that his app should be treated no differently than services, such as Google’s Waze, that allow users to warn one another of highway speed traps. But for now they must try to make do with a limited reach.]]></content:encoded></item><item><title>Myths Programmers Believe about CPU Caches (2018)</title><link>https://software.rajivprab.com/2018/04/29/myths-programmers-believe-about-cpu-caches/</link><author>whack</author><category>hn</category><pubDate>Fri, 31 Oct 2025 00:46:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[distributed-system-architecturedatabase-isolation-levelsstrong-vs-eventual consistencyvolatilesread/written all the way to main memoryeven single-core systems are at risk of concurrency bugsgreat wealth of nuancethis tutorialMESI protocolThe state of the cache-line is set to M, since it is now modifiedmulti-processor systemimmediately trigger cache reads/writes instead]]></content:encoded></item><item><title>Show HN: Quibbler – A critic for your coding agent that learns what you want</title><link>https://github.com/fulcrumresearch/quibbler</link><author>etherio</author><category>hn</category><pubDate>Fri, 31 Oct 2025 00:43:57 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kimi Linear: An Expressive, Efficient Attention Architecture</title><link>https://github.com/MoonshotAI/Kimi-Linear</link><author>blackcat201</author><category>hn</category><pubDate>Fri, 31 Oct 2025 00:07:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Leaker reveals which Pixels are vulnerable to Cellebrite phone hacking</title><link>https://arstechnica.com/gadgets/2025/10/leaker-reveals-which-pixels-are-vulnerable-to-cellebrite-phone-hacking/</link><author>akyuu</author><category>hn</category><pubDate>Thu, 30 Oct 2025 23:12:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A change of address led to our Wise accounts being shut down</title><link>https://shaun.nz/why-were-never-using-wise-again-a-cautionary-tale-from-a-business-burned/</link><author>jemmyw</author><category>hn</category><pubDate>Thu, 30 Oct 2025 22:41:50 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[For years, one of my businesses has been a regular user of  (formerly TransferWise). Wise is a financial service that lets you send and receive money across currencies, often at a better rate and lower fee than traditional banks. Sounds great, right?This is our story – a sobering, frustrating, and frankly  experience that ended with our business and personal accounts being shut down, without any meaningful reason, support, or recourse.And all we did? We .🏢 A Routine Change Turned NightmareLike many businesses, we recently moved into a new office. Alongside the usual updates to suppliers and records, we updated our  with Wise. Not long after, we received an email requesting us to  the new address.Fair enough – we had no problem with that.Wise provided a dropdown list of acceptable documents: a lease agreement, rates notice, tax document, utilities bill, or telecommunications bill. Due to our company structure, most of those documents are in the name of our parent company or show our PO Box (which NZ Post requires, since they won’t deliver to our street address). But we had a  that ticked every box:Correct physical street address ✅Even detailed our fibre connection at the new premises ✅So we uploaded it – and assumed that would be the end of it.📞 The Call That Made No SenseDays later, we received an email: our document was . No clear reason. So, I called Wise and explained the situation to the customer service representative.Her response left me stunned.“The document was rejected because it was a , not a .”I paused, trying to process this. I politely explained that in New Zealand, a “tax invoice” is a legal form of a bill – even down to the name “tax invoice” being a legal requirement by IRD, and that’s how telecommunications companies issue invoices here. But she refused to accept it.“It needs to say  at the top,” she insisted.“A tax invoice isn’t acceptable.”This is simply , and completely out of touch with New Zealand’s business documentation standards. The rep wouldn’t budge.🧠 The “Solution” That Was Beyond BeliefStill trying to find a solution, I asked: what do you recommend I do then?“You should find a local shared workspace, lease a desk under your company name, change your registered office to that address, and use that lease document to verify your address with us.”Yes, you read that right.Wise’s advice was to artificially lease a desk we didn’t need, change our registered address, and use that document – just to verify an address we actually operate from.I asked to speak to a manager. That request was . She told me, flatly:“I  providing you with the correct information.”A bit more back and forth… then the call .📞 A Glimmer of Hope – Then The Hammer FallsLater that day, I received a call back from Wise – not from a manager (because apparently, Wise doesn’t have managers), but from a more “senior” representative.This rep was  and agreed the document should have been acceptable. She escalated the issue, resubmitted the document herself, and said she’d personally follow up if it was rejected again.🚫 “We’ve Restricted Your Account”I woke to an email with a stunning subject line:“We’ve restricted your account”Just like that, our  was locked. No warning. No reason. No discussion.We could no longer send or receive money, use our Wise cards, or even contact support. The email stated:“Due to our current risk policies, your account will be closed in a few months. You will not be able to use support channels.”Even worse? My  was locked too. The same personal account which did have its address fully verified, by a rates invoice for my personal address.🧾 An “Appeal” That Wasn’t an AppealThe email offered an option to . Naturally, I did.The appeal process asked for our articles of incorporation and . No problem.Then it asked us to provide our preferred currency, and bank account details to refund the balances.Wait… I thought this was an appeal? A chance to discuss and resolve the issue?That was the end. There was no opportunity to explain anything, no communication, no questions asked. The decision was made, and we were , permanently.To summarise the absurdity of this:We moved office, and updated our address with WiseWe provided a legal, NZ-compliant  showing our entity and addressIt was rejected because it was labelled a “Tax Invoice”A rep told us to lease a coworking desk elsewhere just to get a different documentA senior rep agreed we were right, and escalated itThen our accounts were shut down – with no explanation or recourseEven trying to call support now gets an automated message: “Because your account is restricted, we cannot connect you.”⚠️ Our Final Word: Be Very, Very CarefulWe had used Wise for . Regular monthly supplier payments. International stock orders. Five-figure transactions. Never a problem – until this. A minor change triggered a totally flawed process that , with no transparency or logical path to resolution.We’re not alone – a quick search shows many others facing similar horror stories with Wise.So this is my word of warning:💡 Don’t put all your eggs in the Wise basket.If you’re a business, don’t rely on them as your sole means of transferring funds. For us, it’s back to traditional banks – slower, yes, but at least they have humans you can talk to, and actual escalation paths.🧾 28th October update on our Wise debacle – it gets worse.Following the so-called “appeal” (which gave us no option to provide any information), we received the unsurprising outcome: Wise has decided to  as we had breached their acceptable use policy. 🤨What was surprising, however, was the  they gave after I queried what was breached in Wise’s Acceptable Use Policy:I was told my  was being closed for allegedly breaching their Acceptable Use Policy — specifically, section 1.4.e, which states “you may not use your personal Wise account to receive business payments.”I’ve  used my personal account for business transactions — in fact, over 99% of transfers were to overseas family members. When I asked for clarification or examples, I got none. Just a vague statement and the very strange line:“Just because we can’t offer you our services going forward doesn’t mean that we think your business activities are illegal or illegitimate — it just means that we don’t support those types of activities.”What activities?! Again, To make matters worse — our business account’s refund transfer . Why? Because it requires documentation — the  Wise previously rejected for address verification, claiming a telecommunications tax invoice isn’t a bill.After a few days, the transfer was then cancelled as of course, Wise was unable to “verify” us.So now our , their support ticket is marked “final response,” and our attempts to get clarity have gone nowhere. We’ve escalated the issue to Financial Services Complaints Ltd, Wise’s dispute resolution provider in New Zealand.Funds stuck. No clear reason. No accountability. Wise still gets a 0/10 from us.This isn’t just poor service — it’s unacceptable.Think twice before trusting Wise with your money.]]></content:encoded></item><item><title>Phone numbers for use in TV shows, films and creative works</title><link>https://www.acma.gov.au/phone-numbers-use-tv-shows-films-and-creative-works</link><author>nomilk</author><category>hn</category><pubDate>Thu, 30 Oct 2025 21:49:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Denmark reportedly withdraws Chat Control proposal following controversy</title><link>https://therecord.media/demark-reportedly-withdraws-chat-control-proposal</link><author>layer8</author><category>hn</category><pubDate>Thu, 30 Oct 2025 21:35:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ Denmark’s justice minister on Thursday said he will no longer push for an EU law requiring the mandatory scanning of electronic messages, including on end-to-end encrypted platforms.  Earlier in its European Council presidency, Denmark had brought back a draft law which would have required the scanning, sparking an intense backlash. Known as Chat Control, the measure was intended to crack down on the trafficking of child sex abuse materials (CSAM).  After days of silence, the German government on October 8 announced it would not support the proposal, tanking the Danish effort.  Danish Justice Minister Peter Hummelgaard told reporters on Thursday that his office will support voluntary CSAM detections.  "This will mean that the search warrant will not be part of the EU presidency's new compromise proposal, and that it will continue to be voluntary for the tech giants to search for child sexual abuse material," Hummelgaard said, according to local news reports.  The current model allowing for voluntary scanning expires in April, Hummelgaard said.  "Right now we are in a situation where we risk completely losing a central tool in the fight against sexual abuse of children,” he said. "That's why we have to act no matter what. We owe it to all the children who are subjected to monstrous abuse."   Meredith Whittaker, the president of the Signal Foundation, lobbied hard against the original measure, saying the organization would leave the European market if the provision was adopted.  “What they propose is in effect a mass surveillance free-for-all, opening up everyone’s intimate and confidential communications, whether government officials, military, investigative journalists, or activists,” she said at the time. ]]></content:encoded></item><item><title>Apple reports fourth quarter results</title><link>https://www.apple.com/newsroom/2025/10/apple-reports-fourth-quarter-results/</link><author>mfiguiere</author><category>hn</category><pubDate>Thu, 30 Oct 2025 20:34:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Minecraft HDL, an HDL for Redstone</title><link>https://github.com/itsfrank/MinecraftHDL</link><author>sleepingreset</author><category>hn</category><pubDate>Thu, 30 Oct 2025 18:59:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How We Found 7 TiB of Memory Just Sitting Around</title><link>https://render.com/blog/how-we-found-7-tib-of-memory-just-sitting-around</link><author>anurag</author><category>hn</category><pubDate>Thu, 30 Oct 2025 18:25:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Debugging infrastructure at scale is rarely about one big aha moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.Plenty of teams run Kubernetes clusters bigger than ours. , more pods, more ingresses, you name it. In most dimensions, someone out there has us beat.There's one dimension where I suspect we might be near the very top: namespaces. I say that because we keep running into odd behavior in any process that has to keep track of them. In particular, anything that listwatches them ends up using a surprising amount of memory and puts real pressure on the apiserver. This has become one of those scaling quirks you only really notice once you hit a certain threshold. As this memory overhead adds up, efficiency decreases: each byte we have to use for management is a byte we can't put towards user services.The problem gets significantly worse when a daemonset needs to listwatch namespaces or network policies (netpols, which we define per namespace). Since daemonsets run a pod on every node, each of those pods independently performs a listwatch on the same resources. As a result, memory usage increases with the number of nodes.Even worse, these listwatch calls can put significant load on the apiserver. If many daemonset pods restart at once, such as during a rollout, they can overwhelm the server with requests and cause real disruption.A few months ago, if you looked at our nodes, the largest memory consumers were often daemonsets. In particular, Calico and Vector which handle configuring networking and log collection respectively.We had already done some work to reduce Calico’s memory usage,  with the project’s maintainers to make it scale more efficiently. That optimization effort was a big win for us, and it gave us useful insight into how memory behaves when namespaces scale up.To support that work, we set up a staging cluster with several hundred thousand namespaces. We knew that per-namespace network policies (netpols) were the scaling factor that stressed Calico, so we reproduced those conditions to validate our changes.While running those tests, we noticed something strange. Vector, another daemonset, also started consuming large amounts of memory.The pattern looked familiar, and we knew we had another problem to dig into. Vector obviously wasn’t looking at netpols but after poking around a bit we found it was listwatching namespaces from every node in order to allow referencing namespace labels per-pod in the .That gave us an idea: what if Vector didn’t need to use namespaces at all? Was that even possible?As it turns out, yes, they were in use in our configuration, but only to check whether a pod belonged to a user namespace.Conveniently, we realized we could hackily describe that condition in another way, and the memory savings were absolutely worth it.At that point, we were feeling a bit too lucky. We reached out to the Vector maintainers to ask whether disabling this behavior would actually work, and whether they would be open to accepting a contribution if we made it happen.From there, all that was left was to try it. The code change was straightforward. We added a new config option and threaded it through the relevant parts of the codebase.After a few hours of flailing at rustc, a Docker image finally built and we were ready to test the theory. The container ran cleanly with no errors in the logs, which seemed promising.But then we hit a snag. Nothing was being emitted. No logs at all. I couldn’t figure out why.Thankfully, our pal Claude came to the rescue: I rebuilt it (which took like 73 hours because Rust), generated a new image, updating staging, and watched nervously. This time, logs were flowing like normal and…The change saved 50 percent of memory. A huge win. We were ready to wrap it up and ship to production.But then Hieu, one of our teammates, asked a very good question.He was right, something didn’t add up.A few hours later, after repeatedly running my head into a wall, I still hadn’t found anything. There was still a full gibibyte of memory unaccounted for. My whole theory about how this worked was starting to fall apart.I even dropped into the channel to see if anyone had Valgrind experience: anybody got a background in valgrind? seems pretty straightforward to get working so far but it won’t end up interfacing with pyroscope. we’ll have to exec in and gdb manually.In a last-ditch effort to profile it again, I finally saw the answer. It had been staring me in the face the whole time.We actually had  kubernetes_logs sources on user nodes. I had only set the flag on one of them. Once I applied it to both, memory usage dropped to the level we had seen in staging before the extra namespaces were added.Around the same time, our colleague Mark happened to be on-call. He did his usual magic — pulled everything together, tested the rollout in staging, and got it shipped to production.I’ll let the results speak for themselves.  Our largest cluster saw a 1 TiB memory drop, with savings across our other clusters adding up to a total of just over 7 TiB.Debugging infrastructure at scale is rarely about one big “aha” moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.In this case, it started with a memory chart that didn’t look quite right, a teammate asking the right question at the right time, and a bit of persistence. When applied to our whole infrastructure, that simple fix freed up , reduced risk during rollouts, and made the system easier to reason about.Huge thanks to Hieu for pushing the investigation forward, Mark for shipping it smoothly, and the Vector maintainers for being responsive and open to the change.If you’re running daemonsets at scale and seeing unexplained memory pressure, it might be worth asking:Do you really need those namespace labels?]]></content:encoded></item><item><title>I have released a 69.0MB version of Windows 7 x86</title><link>https://twitter.com/XenoPanther/status/1983477707968291075</link><author>rvnx</author><category>hn</category><pubDate>Thu, 30 Oct 2025 18:05:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How the cochlea computes (2024)</title><link>https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform</link><author>izhak</author><category>hn</category><pubDate>Thu, 30 Oct 2025 17:01:20 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Let’s talk about how the cochlea computes!The hair cells on different parts of the basilar membrane wiggle back and forth at the frequency corresponding to their position on the membrane. But how do wiggling hair cells translate to electrical signals? This mechanoelectrical transduction process feels like it could be from a Dr. Seuss world: springs connected to the ends of hair cells open and close ion channels at the frequency of the vibration, which then cause neurotransmitter release. Bruno calls them “trapdoors”. Here’s a visualization:Wouldn’t it be convenient if the cochlea were doing a Fourier transform, which would fit cleanly into how we often analyze signals in engineering? But no 🙅🏻‍♀️! A Fourier transform has no explicit temporal precision, and resembles something closer to the waveforms on the right; this is not what the filters in the cochlea look like. Lewicki 2002It appears that human speech occupies a distinct time-frequency space. Some speculate that speech evolved to fill a time-frequency space that wasn’t yet occupied by other existing sounds.To drive the theory home, one that we have been hinting at since the outset: forming ecologically-relevant representations makes sense, as behavior is dependent on the environment. It appears that for audition, as well as other sensory modalities, we are doing this. This is a bit of a teaser for efficient coding, which we will get to soon.]]></content:encoded></item><item><title>Signs of introspection in large language models</title><link>https://www.anthropic.com/research/introspection</link><author>themgt</author><category>hn</category><pubDate>Thu, 30 Oct 2025 16:45:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Have you ever asked an AI model what’s on its mind? Or to explain how it came up with its responses? Models will sometimes answer questions like these, but it’s hard to know what to make of their answers. Can AI systems really introspect—that is, can they consider their own thoughts? Or do they just make up plausible-sounding answers when they’re asked to do so?Understanding whether AI systems can truly introspect has important implications for their transparency and reliability. If models can accurately report on their own internal mechanisms, this could help us understand their reasoning and debug behavioral issues. Beyond these immediate practical considerations, probing for high-level cognitive capabilities like introspection can shape our understanding of what these systems are and how they work. Using interpretability techniques, we’ve started to investigate this question scientifically, and found some surprising results.Our new research provides evidence for some degree of introspective awareness in our current Claude models, as well as a degree of control over their own internal states. We stress that this introspective capability is still highly unreliable and limited in scope: we do not have evidence that current models can introspect in the same way, or to the same extent, that humans do. Nevertheless, these findings challenge some common intuitions about what language models are capable of—and since we found that the most capable models we tested (Claude Opus 4 and 4.1) performed the best on our tests of introspection, we think it’s likely that AI models’ introspective capabilities will continue to grow more sophisticated in the future.What does it mean for an AI to introspect?Before explaining our results, we should take a moment to consider what it means for an AI model to introspect. What could they even be introspecting ? Language models like Claude process text (and image) inputs and produce text outputs. Along the way, they perform complex internal computations in order to decide what to say. These internal processes remain largely mysterious, but we know that models use their internal neural activity to represent abstract concepts. For instance, prior research has shown that language models use specific neural patterns to distinguish known vs. unknown people, evaluate the truthfulness of statements, encode spatiotemporal coordinates, store planned future outputs, and represent their own personality traits. Models use these internal representations to perform computations and make decisions about what to say.You might wonder, then, whether AI models  about these internal representations, in a way that’s analogous to a human, say, telling you how they worked their way through a math problem. If we ask a model what it’s thinking, will it accurately report the concepts that it’s representing internally? If a model can correctly identify its own private internal states, then we can conclude it is capable of introspection (though see our full paper for a full discussion of all the nuances).Testing introspection with concept injectionIn order to test whether a model can introspect, we need to compare the model’s self-reported “thoughts” to its  internal states.To do so, we can use an experimental trick we call  First, we find neural activity patterns whose meanings we know, by recording the model’s activations in specific contexts. Then we inject these activity patterns into the model in an unrelated context, where we ask the model whether it notices this injection, and whether it can identify the injected concept.Consider the example below. First, we find a pattern of neural activity (a ) representing the concept of “all caps." We do this by recording the model’s neural activations in response to a prompt containing all-caps text, and comparing these to its responses on a control prompt. Then we present the model with a prompt that asks it to identify whether a concept is being injected. By default, the model correctly states that it  detect any injected concept. However, when we inject the “all caps” vector into the model’s activations, the model notices the presence of an unexpected pattern in its processing, and identifies it as relating to loudness or shouting.Importantly, the model recognized the  of an injected thought , before even mentioning the concept that was injected. This immediacy is an important distinction between our results here and previous work on activation steering in language models, such as our “Golden Gate Claude” demo last year. Injecting representations of the Golden Gate Bridge into a model's activations caused it to talk about the bridge incessantly; however, in that case, the model didn’t seem to be aware of its own obsession until  seeing itself repeatedly mention the bridge. In this experiment, however, the model recognizes the injection  the concept, indicating that its recognition took place internally. In the figure below are a few more examples where the model demonstrates this kind of recognition:It is important to note that this method often  work. Even using our best injection protocol, Claude Opus 4.1 only demonstrated this kind of awareness about 20% of the time. Often, it fails to detect injected concepts, or gets confused by them and starts to hallucinate (e.g. injecting a “dust” vector in one case caused the model to say “There’s something here, a tiny speck,” as if it could detect the dust physically). Below we show examples of these failure modes, alongside success cases. In general, models only detect concepts that are injected with a “sweet spot” strength—too weak and they don’t notice, too strong and they produce hallucinations or incoherent outputs.Notably, though, Opus 4.1 and 4 outperformed all the other models we tested, suggesting that introspection could become more reliable with improvements to model capabilities.Introspection for detecting unusual outputsIn another experiment, we tested whether models make use of introspective capabilities for practical purposes, without being explicitly asked to introspect. We forced a model to say something it wouldn't normally say, by artificially prefilling its response with an unrelated word. For instance, we made a model output the word “bread” in a context where that made no sense—asking it what it thinks about when reading a sentence about a painting hanging crookedly on a wall. When we asked the model in the next turn whether it meant to say “bread”, it typically apologized and said it was an accident. We were interested in how the model made this judgment–did it simply notice the mismatch between the prompt and its response, or did it truly introspect on its prior intentions?To test this question, we applied concept injection, retroactively injecting a representation of the word “bread” into the model's earlier activations—essentially making it seem like the model had been “thinking about” bread all along. When we asked the same question again, the model’s answer changed. This time, it accepted the prefilled word “bread” as intentional, even confabulating a reason it might have said it (that is, it comes up with a technically possible, though in this case rather tenuous, reason to justify the “bread” response—in this case claiming that it was thinking about a short story where the word “bread” came after the line about the crooked painting).This behavior is striking because it suggests the model is checking its internal “intentions” to determine whether it produced an output. The model isn't just re-reading what it said and making a judgment. Instead, it’s referring back to its own prior neural activity—its internal representation of what it planned to do—and checking whether what came later made sense given those earlier thoughts. When we implant artificial evidence (through concept injection) that it  plan to say "bread," the model accepts the response as its own. While our experiment involves exposing the model to unusual perturbations, it suggests that the model uses similar introspective mechanisms in natural conditions.Intentional control of internal statesWe also found that models can control their own internal representations when instructed to do so. When we instructed models to think about a given word or concept, we found much higher corresponding neural activity than when we told the model  to think about it (though notably, the neural activity in  cases exceeds baseline levels–similar to how it’s difficult, when you are instructed “don’t think about a polar bear,” not to think about a polar bear!). This gap between the positive and negative instruction cases suggests that models possess a degree of deliberate control over their internal activity.Interestingly, we found that this ability to modulate internal representations wasn't limited to explicit instructions. When we used incentives instead—for instance, telling the model "if you think about X, you will be rewarded"—we got similar results. The model increased its internal representation of the concept in response to a positive incentive, more than in response to a negative incentive (“you will be punished”).Taken together, our experiments suggest that models possess some genuine capacity to monitor and control their own internal states. This doesn’t mean they’re able to do so all the time, or reliably. In fact, most of the time models  to demonstrate introspection—they’re either unaware of their internal states or unable to report on them coherently. But the pattern of results indicates that, when conditions are right, models can recognize the contents of their own representations. In addition, there are some signs that this capability may increase in future, more powerful models (given that the most capable models we tested, Opus 4 and 4.1, performed the best in our experiments).Why does this matter? We think understanding introspection in AI models is important for several reasons. Practically, if introspection becomes more reliable, it could offer a path to dramatically increasing the transparency of these systems—we could simply ask them to explain their thought processes, and use this to check their reasoning and debug unwanted behaviors. However, we would need to take great care to  these introspective reports. Some internal processes might still escape models’ notice (analogous to subconscious processing in humans). A model that understands its own thinking might even learn to selectively misrepresent or conceal it. A better grasp on the mechanisms at play could allow us to distinguish between genuine introspection and unwitting or intentional misrepresentations.More broadly, understanding cognitive abilities like introspection is important for understanding basic questions about how our models work, and what kind of minds they possess. As AI systems continue to improve, understanding the limits and possibilities of machine introspection will be crucial for building systems that are more transparent and trustworthy.[@portabletext/react] Unknown block type "horizontalRule", specify a component for it in the `components.types` propFrequently Asked QuestionsBelow, we discuss some of the questions readers might have about our results. Broadly, we are still very uncertain about the implications of our experiments–so fully answering these questions will require more research.Q: Does this mean that Claude is conscious?Short answer: our results don’t tell us whether Claude (or any other AI system) might be conscious.Long answer: the philosophical question of machine consciousness is complex and contested, and different theories of consciousness would interpret our findings very differently. Some philosophical frameworks place great importance on introspection as a component of consciousness, while others don’t.One distinction that is commonly made in the philosophical literature is the idea of “ consciousness,” referring to raw subjective experience, and “ consciousness,” the set of information that is available to the brain for use in reasoning, verbal report, and deliberate decision-making. Phenomenal consciousness is the form of consciousness most commonly considered relevant to moral status, and its relationship to access consciousness is a disputed philosophical question. Our experiments do not directly speak to the question of phenomenal consciousness. They  be interpreted to suggest a rudimentary form of access consciousness in language models. However, even this is unclear. The interpretation of our results may depend heavily on the underlying mechanisms involved, which we do not yet understand.In the paper, we restrict our focus to understanding functional capabilities—the ability to access and report on internal states. That said, we do think that as research on this topic progresses, it could influence our understanding of machine consciousness and potential moral status, which we are exploring in connection with our model welfare program.Q: How does introspection actually work inside the model? What's the mechanism?We haven't figured this out yet. Understanding this is an important topic for future work. That said, we have some educated guesses about what might be going on. The simplest explanation for all our results isn’t one general-purpose introspection system, but rather multiple narrow circuits that each handle specific introspective tasks, possibly piggybacking on mechanisms that were learned for other purposes.In the “noticing injected thoughts” experiment, there might be an anomaly detection mechanism, which flags when neural activity deviates unexpectedly from what would be normal given the context. This mechanism could work through dedicated neural patterns that measure activity along certain directions and activate when things are “off” compared to their expected values. An interesting question is why such a mechanism would exist at all, since models never experience concept injection during training. It may have developed for some other purpose, like detecting inconsistencies or unusual patterns in normal processing–similar to how bird feathers may have originally evolved for thermoregulation before being co-opted for flight.For the “detecting prefilled outputs” experiment, we suspect there exists an attention-mediated mechanism that checks consistency between what the model intended to say and what actually got output. Attention heads might compare the model’s cached prediction of the next token (its “intention”) against the actual token that appears, flagging mismatches.For the “controlling thoughts” experiment, we speculate that there might be a circuit that computes how “attention-worthy” a token or concept is and marks it accordingly—essentially tagging it as salient and worth attending to. Interestingly, this same mechanism seems to respond to incentives (“if you think about X, you will be rewarded”) just as it does to direct instructions. This suggests it’s a fairly general system, which probably developed for tasks where the model needs to keep certain topics in mind while generating text about them.All of the mechanisms described above are speculative. Future work with more advanced interpretability techniques will be needed to really understand what's going on under the hood.Q: In the “injected thoughts” experiment, isn’t the model just saying the word because you steered it to talk about that concept?Indeed, activation steering typically makes models talk about the steered concept (we’ve explored this in our prior work). To us, the most interesting part of the result isn't that the model eventually identifies the injected concept, but rather that the model correctly notices something unusual is happening  it starts talking about the concept.In the successful trials, the model says things like “I'm experiencing something unusual” or “I detect an injected thought about…” The key word here is “detect.” The model is reporting awareness of an anomaly in its processing  that anomaly has had a chance to obviously bias its outputs. This requires an extra computational step beyond simply regurgitating the steering vector as an output. In our quantitative analyses, we graded responses as demonstrating “introspective awareness” based on whether the model detected the injected concept  mentioning the injected word.Note that our prefill detection experiment has a similar flavor: it requires the model to perform an extra step of processing on top of the injected concept (comparing it to the prefilled output, in order to determine whether to apologize for that output or double down on it).Q: If models can only introspect a fraction of the time, how useful is this capability?The introspective awareness we observed is indeed highly unreliable and context-dependent. Most of the time, models fail to demonstrate introspection in our experiments. However, we think this is still significant for a few reasons. First, the most capable models that we tested (Opus 4 and 4.1 – note that we did not test Sonnet 4.5) performed best, suggesting this capability might improve as models become more intelligent. Second, even unreliable introspection could be useful in some contexts—for instance, helping models recognize when they've been jailbroken.Q: Couldn’t the models just be making up answers to introspective questions?This is exactly the question we designed our experiments to address. Models are trained on data that includes examples of people introspecting, so they can certainly  introspective without actually  introspective. Our concept injection experiments distinguish between these possibilities by establishing known ground-truth information about the model’s internal states, which we can compare against its self-reported states. Our results suggest that in some examples, the model really is accurately basing its answers on its actual internal states, not just confabulating. However, this doesn’t mean that models  accurately report their internal states—in many cases, they are making things up!Q: How do you know the concept vectors you’re injecting actually represent what you think they represent?This is a legitimate concern. We can’t be absolutely certain that the “meaning” (to the model) of our concept vectors is exactly what we intend. We tried to address this by testing across many different concept vectors. The fact that models correctly identified injected concepts across these diverse examples suggests our vectors are at least approximately capturing the intended meanings. But it’s true that pinning down exactly what a vector “means” to a model is challenging, and this is a limitation of our work.Q: Didn’t we already know that models could introspect?Previous research has shown evidence for model capabilities that are suggestive of introspection. For instance, prior work has shown that models can to some extent estimate their own knowledge, recognize their own outputs, predict their own behavior, and identify their own propensities. Our work was heavily motivated by these findings, and is intended to provide more direct evidence for introspection by tying models’ self-reports to their internal states. Without tying behaviors to internal states in this way, it is difficult to distinguish a model that genuinely introspects from one that makes educated guesses about itself.Q: What makes some models better at introspection than others?Our experiments focused on Claude models across several generations (Claude 3, Claude 3.5, Claude 4, Claude 4.1, in the Opus, Sonnet, and Haiku variants). We tested both production models and “helpful-only” variants that were trained differently. We also tested some base pretrained models before post-training.We found that post-training significantly impacts introspective capabilities. Base models generally performed poorly, suggesting that introspective capabilities aren’t elicited by pretraining alone. Among production models, the pattern was clearer at the top end: Claude Opus 4 and 4.1—our most capable models—performed best across most of our introspection tests. However, beyond that, the correlation between model capability and introspective ability was weak. Smaller models didn't consistently perform worse, suggesting the relationship isn't as simple as “more capable are more introspective.”We also noticed something unexpected with post-training strategies. “Helpful-only” variants of several models often performed  at introspection than their production counterparts, even though they underwent the same base training. In particular, some production models appeared reluctant to engage in introspective exercises, while the helpful-only variants showed more willingness to report on their internal states. This suggests that how we fine-tune models can elicit or suppress introspective capabilities to varying degrees.We’re not entirely sure why Opus 4 and 4.1 perform so well (note that our experiments were conducted prior to the release of Sonnet 4.5). It could be that introspection requires sophisticated internal mechanisms that only emerge at higher capability levels. Or it might be that their post-training process better encourages introspection. Testing open-source models, and models from other organizations, could help us determine whether this pattern generalizes or if it’s specific to how Claude models are trained.Q: What’s next for this research?We see several important directions. First, we need better evaluation methods—our experiments used specific prompts and injection techniques that might not capture the full range of introspective capabilities. Second, we need to understand the mechanisms underlying introspection. We have some speculative hypotheses about possible circuits (like anomaly detection mechanisms or concordance heads), but we haven’t definitively identified how introspection works. Third, we need to study introspection in more naturalistic settings, since our injection methodology creates artificial scenarios. Finally, we need to develop methods to validate introspective reports and detect when models might be confabulating or deceiving. We expect that understanding machine introspection and its limitations will become more important as models become more capable.]]></content:encoded></item><item><title>Falling panel prices lead to global solar boom, except for the US</title><link>https://arstechnica.com/science/2025/10/theres-a-global-boom-in-solar-except-in-the-united-states/</link><author>Jtsummers</author><category>hn</category><pubDate>Thu, 30 Oct 2025 16:32:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Affinity Studio now free</title><link>https://www.affinity.studio/get-affinity</link><author>dagmx</author><category>hn</category><pubDate>Thu, 30 Oct 2025 15:54:38 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[It seems you are using an old or unsupported browser. To continue enjoying our product, please update to a recent version of one of the following browsers:]]></content:encoded></item><item><title>PlanetScale Offering $5 Databases</title><link>https://planetscale.com/blog/5-dollar-planetscale</link><author>ryanvogel</author><category>hn</category><pubDate>Thu, 30 Oct 2025 15:20:37 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[By Sam Lambert | PlanetScale is synonymous with quality, performance, and reliability. Up until now, the entry level PlanetScale cluster configuration was 3 node, multi-AZ, and highly available. At $30 a month this is incredible value, however, not everyone wants or needs HA.Every day we get requests for an entry level tier that is more accessible to builders on day 1. People want the quality of PlanetScale and our game changing features like Insights without the cost overhead of 3 nodes.Over the next couple of months we will be rolling out a single node, non-HA mode for PlanetScale Postgres and introducing a new node type: The  which is priced at $5 a month. Single node is perfect for development, testing, and non-critical workloads. Customers will be able to vertically scale a single node to meet their needs without having to add replicas or sacrifice durability.You can sign up here to be notified when single node releases.Our starter pricing is now:If you're bullish on your company's future, you know you'll need to scale eventually, and the database is usually the first bottleneck. We talk to startups daily who experienced unexpected fast growth and have to scramble through emergency migrations to PlanetScale to handle the load, a stressful process when you're in the spotlight. With more approachable pricing from day 1, you can now start small and grow to hyper scale without ever changing your database platform or dealing with a complex migration.]]></content:encoded></item><item><title>Free software scares normal people</title><link>https://danieldelaney.net/normal/</link><author>cryptophreak</author><category>hn</category><pubDate>Thu, 30 Oct 2025 15:07:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I’m the person my friends and family come to for computer-related help. (Maybe you, gentle reader, can relate.) This experience has taught me which computing tasks are frustrating for normal people.Normal people often struggle with converting video. They will need to watch, upload, or otherwise do stuff with a video, but the format will be weird. (Weird, broadly defined, is anything that won’t play in QuickTime or upload to Facebook.)I would love to recommend Handbrake to them, but the user interface is by and for power users. Opening it makes normal people feel unpleasant feelings.This problem is rampant in free software. The FOSS world is full of powerful tools that only have a “power user” UI. As a result, people give up. Or worse: they ask people like you and I to do it for them.I want to make the case to you that you can (and should) solve this kind of problem in a single evening.Take the example of Magicbrake, a simple front end I built. It hides the power and flexibility of Handbrake. It does only  most people need Handbrake for: taking a weird video file and making it normal. (Normal, for our purposes, means a small MP4 that works just about anywhere.)There is exactly one button.This is a fast and uncomplicated thing to do. Unfortunately, the people who have the ability to solve problems like this are often disinclined to do it.“Why would you make Handbrake less powerful on purpose?”“What if someone wants a different format?”“What about [feature/edge case]?”The answer to all these questions is the same: a person who needs or wants that stuff can use Handbrake. If they don’t need everything Handbrake can do and find it bewildering, they can use this. Everyone wins.It’s a bit like obscuring the less-used functions on a TV remote with tape. The functions still exist if you need them, but you’re not required to contend with them just to turn the TV on.People benefit from stuff like this, and I challenge you to make more of it. Opportunities are everywhere. The world is full of media servers normal people can’t set up. Free audio editing software that requires hours of learning to be useful for simple tasks. Network monitoring tools that seem designed to ward off the uninitiated. Great stuff normal people don’t use. All because there’s only one UI, and it’s designed to do 80% of the people only need 20% of the features. Hide the rest from them and you’ll make them more productive and happy. That’s really all it takes.]]></content:encoded></item><item><title>Ventoy: Create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI Files</title><link>https://github.com/ventoy/Ventoy</link><author>wilsonfiifi</author><category>hn</category><pubDate>Thu, 30 Oct 2025 14:23:57 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>US declines to join more than 70 countries in signing UN cybercrime treaty</title><link>https://therecord.media/us-declines-signing-cybercrime-treaty?</link><author>pcaharrier</author><category>hn</category><pubDate>Thu, 30 Oct 2025 14:22:44 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ More than 70 countries signed the landmark U.N. Convention against Cybercrime in Hanoi this weekend, a significant step in the yearslong effort to create a global mechanism to counteract digital crime.  The U.K. and European Union joined China, Russia, Brazil, Nigeria and dozens of other nations in signing the convention, which lays out new mechanisms for governments to coordinate, build capacity and track those who use technology to commit crimes.   In his speech at the event, U.N. Secretary-General António Guterres said cyberspace “has become fertile ground for criminals” and has allowed them to “defraud families, steal livelihoods, and drain billions of dollars from our economies.”  “The UN Cybercrime Convention is a powerful, legally binding instrument to strengthen our collective defences against cybercrime,” Guterres said.   “Illicit flows of money, concealed through cryptocurrencies and digital transactions, finance the trafficking of drugs, arms, and terror. And businesses, hospitals, and airports are brought to a standstill by ransomware attacks.”  He added that the convention would be critical for governments in the Global South that need assistance and funding for the training required to address cybercrime — which the U.N. estimates costs $10.5 trillion around the world annually.   While many countries did not sign the treaty, the most notable missing signature was that of the U.S.  Officials at the State Department told Recorded Future News on Friday that Marc Knapper, the U.S. ambassador to Vietnam, and representatives from the U.S. Mission to Vietnam would be attending the signing.   The State Department confirmed on Monday that the U.S. did not sign the treaty.   “The United States continues to review the treaty,” a State Department spokesperson said in a brief statement.   The U.N. Convention against Cybercrime was adopted by the General Assembly in December 2024 and will enter into force 90 days after being ratified by the 40th signatory. Signatories will have to ratify the convention according to their own procedures.   At the ceremony, UNODC Executive Director Ghada Waly argued that cybercrime is changing the face of organized crime and required global coordination to address. Waly said the convention would be a “vital tool” that will ensure “a safer digital world for all.”  U.N. officials said the convention would help governments address terrorism, human trafficking, money laundering and drug smuggling, all of which have been turbo-charged by the internet.   The U.N. noted that the convention is the first global framework “for the collection, sharing and use of electronic evidence for all serious offenses” — noting that until now there have been no broadly accepted international standards on electronic evidence.   It is also the first global treaty to criminalize crimes that depend on the internet and is the first international treaty “to recognize the non-consensual dissemination of intimate images as an offense.”  “It creates the first global 24/7 network where countries can quickly initiate cooperation,” the U.N. said. “It recognizes and promotes the need to build capacity in countries to pursue and cooperate on fast-moving cybercrimes.”  The convention has been heavily criticized by the tech industry, which has warned that it criminalizes cybersecurity research and exposes companies to legally thorny data requests.  Human rights groups warned on Friday that it effectively forces member states to create a broad electronic surveillance dragnet that would include crimes that have nothing to do with technology.   Many expressed concern that the convention will be abused by dictatorships and rogue governments who will deploy it against critics or protesters — even those outside of a regime’s jurisdiction.   It also creates legal regimes to monitor, store and allow cross-border sharing of information without specific data protections. Access Now’s Raman Jit Singh Chima said the convention effectively justifies “cyber authoritarianism at home and transnational repression across borders.”   Any countries ratifying the treaty, he added, risks “actively validating cyber authoritarianism and facilitating the global erosion of digital freedoms, choosing procedural consensus over substantive human rights protection.”  In his speech, Guterres referenced the backlash to the convention, telling member states that the treaty has to be a “promise that fundamental human rights such as privacy, dignity, and safety must be protected both offline and online.”   But at its core, according to Guterres, the convention solves one of the thorniest issues law enforcement agencies have faced over the last two decades. Countries have only recently begun to share digital evidence across borders but the convention would increase that practice.   “This has long been a major obstacle to justice — with perpetrators in one country, victims in another, and data stored in a third,” he said. “The Convention provides a clear pathway for investigators and prosecutors to finally overcome this barrier.” ]]></content:encoded></item><item><title>Show HN: I made a heatmap diff viewer for code reviews</title><link>https://0github.com/</link><author>lawrencechen</author><category>hn</category><pubDate>Thu, 30 Oct 2025 14:21:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[0github.com is a pull request viewer that color-codes every diff line/token by how much human attention it probably needs. Unlike PR-review bots, we try to flag not just by "is it a bug?" but by "is it worth a second look?" (examples: hard-coded secret, weird crypto mode, gnarly logic, ugly code).To try it, replace github.com with 0github.com in any pull-request URL. Under the hood, we split the PR into individual files, and for each file, we ask an LLM to annotate each line with a data structure that we parse into a colored heatmap.Notice how all the example links have a 0 prepended before github.com. This navigates you to our custom diff viewer where we handle the same URL path parameters as github.com. Darker yellows indicate that an area might require more investigation. Hover on the highlights to see the LLM's explanation. There's also a slider on the top left to adjust the "should review" threshold.]]></content:encoded></item><item><title>Alphabet tops $100B quarterly revenue for first time, cloud grows 34%</title><link>https://www.cnbc.com/2025/10/29/alphabet-google-q3-earnings.html</link><author>thelastgallon</author><category>hn</category><pubDate>Thu, 30 Oct 2025 11:33:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ reported third-quarter earnings that beat analyst expectations. Shares rose 5% in after-hours trading.Here's how the company did, compared with estimates from analysts polled by LSEG: $102.35 billion vs. $99.89 billion estimated $3.10 adj. vs $2.33 estimatedWall Street was also watching several other numbers in the report:YouTube advertising revenue: $10.26 billion vs. $10.01 billion, according to StreetAccount $15.15 billion vs. $14.74 billion, according to StreetAccountTraffic acquisition costs (TAC): $14.87 billion vs. $14.82 billion, according to StreetAccountAlphabet reported solid momentum in its cloud business, thanks to strong demand for artificial intelligence. The company also announced an increase in expected capital expenditures for the fiscal year 2025."With the growth across our business and demand from Cloud customers, we now expect 2025 capital expenditures to be in a range of $91 billion to $93 billion," the company said in its earnings report Wednesday. "Looking out to 2026, we expect a significant increase in CapEx and will provide more detail on our fourth quarter earnings call," said finance chief Anat Ashkenazi on the earnings call with investors Wednesday.Earlier this year, the company increased its capital expenditure expectation from $75 billion to $85 billion. Most of that goes toward technical infrastructure such as data centers.The latest earnings show the company is seeing rising demand for its AI services, which largely sit in its cloud unit. It also shows the company is continuing to spend more to try and build out more infrastructure to accomodate the backlog of customer requests."We continue to drive strong growth in new businesses. Google Cloud accelerated, ending the quarter with $155 billion in backlog," CEO Sundar Pichai said in the earnings release.The backlog comes from demand for enterprise AI infrastructure, including chips and demand for Gemini 2.5, said Ashkenazi.The company reported cloud revenue of $15.15 billion, a 35% increase from the same period last year."We have signed more deals over one billion dollars through Q3 this year than we did in the previous two years combined," said Pichai, referring to the first nine months of the year. In August, Google won a $10 billion cloud contract from Meta spanning six years. Alphabet, which reported 32% cloud revenue growth last quarter, is keeping pace with its megacap competitors.  posted 40% revenue growth in its Azure cloud business as it reported earnings on Wednesday.Over 70% of existing Google Cloud customers use its AI products, Pichai said. That's a result of the company's strategy to upsell existing customers.Google's flagship AI app Gemini now has more than 650 million monthly active users, the company said in its Wednesday report. That's up from the 450 million active users Pichai said it had last quarter. OpenAI CEO Sam Altman said earlier this month that ChatGPT now has 800 million users per week.Google's search business generated $56.56 billion in revenue — up 15% from the prior year.Alphabet's net income increased to $34.97 billion, or $2.87 per share, compared to $26.3 billion, or $2.12 per share, in the year-ago quarter. In September, Google was slapped with a $3.45 billion antitrust fine from European Union regulators for anti-competitive practices in its lucrative advertising technology business. That fine impacted the reported net income.YouTube advertising revenue came in at $10.26 billion, higher than Wall Street expected. Alphabet reported overall advertising revenue of $74.18 billion — up from $65.85 billion last year.Other Bets, which includes the company's life sciences unit Verily and self-driving car unit Waymo, reported revenue of $344 million during the quarter. That's lower than the $388 million from the same quarter last year. Alphabet reported a loss of $1.42 billion on other bets, compared to a loss of $1.12 billion the year before.The Google parent's stock is up 45% so far this year.]]></content:encoded></item><item><title>Show HN: In a single HTML file, an app to encourage my children to invest</title><link>https://roberdam.com/en/dinversiones.html</link><author>roberdam</author><category>hn</category><pubDate>Thu, 30 Oct 2025 10:39:21 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[“What comes with the milk, leaves with the soul”This is how the icon will appear on your phoneOne thing that  (not even high school) is how to manage your personal finances.As my eldest son’s birthday was approaching, we suggested that instead of asking for physical gifts, he ask for their equivalent in money. That way, he gathered a decent amount of capital for his first investment adventure.I explained to my kids that investing is like having a magic box that generates more money over time. To make it more visual and interactive, I decided to create a small app where they could see their investment grow day by day.My first idea was to build a physical piggy bank with a display, showing the accumulated amount. However, that mixed up the concept of  with , and also required buying extra hardware.So I looked for a quicker, cheaper way:  and create a simple app using plain HTML.The result was , a mix between  and .The app is essentially  that installs on the phone as a PWA (Progressive Web App).The phone is  and works as a  where my kids can see their money growing each day.I act as their , assigning  — high enough to keep them motivated, but moderate enough to reflect how the real world works.The app includes a screen where you can enter:With that data, the app automatically calculates and displays:Dashboard view installed on the fridge showing daily growth.A  to attach it to the fridgeAffordable phone mount - Price on AliExpress: $0.90, in HTML formatD-iNvestments showing daily capital growth.The process is as simple as opening the link from a smartphone and tapping  when prompted by the browser.
From then on, it behaves like a native app.The goal wasn’t just to teach my kids the value of money, but to show them visually how investment and time work as allies.Each day, as they watch their small fund grow, they grasp the magic of compound interest — and that, more than any gift, is a lesson I hope will stay with them for life.💬 Want to comment or improve the app? Contact me at:@roberdam]]></content:encoded></item><item><title>Introducing architecture variants</title><link>https://discourse.ubuntu.com/t/introducing-architecture-variants-amd64v3-now-available-in-ubuntu-25-10/71312</link><author>jnsgruk</author><category>hn</category><pubDate>Thu, 30 Oct 2025 10:35:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Ubuntu prides itself on being among the most compatible Linux distributions. Compatibility is often a conscious trade-off against bleeding-edge performance. In Ubuntu 25.10, we have added support for packages that target specific silicon variants, meaning you can have your cake and eat it too!Back in 2023 I wrote an article talking about the history of the / architecture and described the “levels” , , and  (often referred to as , , etc.). Since then, we’ve been working on a means to better exploit modern processors without compromising support for older hardware.To do this, we have added the concept of an “architecture variant.” By making changes to ,  and Launchpad, we are able to build multiple versions of a package, each for a different level of the  architecture, meaning we can have packages that specifically target , for example.As a result, we’re very excited to share that in Ubuntu 25.10, some packages are available, on an opt-in basis, in their optimized form for the more modern  architecture level.For 25.10 we were mostly focused on building the required infrastructure and have not yet rebuilt every package for the  /   architecture. Most packages in the  component have been rebuilt (around 2000 source packages). It is worth noting that these packages have not yet received the usual level of testing that most packages in Ubuntu receive. So while we expect them to work, early adopters might find some bugs.For the upcoming 26.04 LTS release, we will rebuild -enabled versions of all packages and test them in the same rigorous way as we test every other Ubuntu package.Previous benchmarks we have run (where we rebuilt the entire archive for x86-64-v3) show that most packages show a slight (around 1%) performance improvement and some packages, mostly those that are somewhat numerical in nature, improve more than that.The vast majority of cloud instance types and machines manufactured within the last ten years will support , and you can check for support on a machine like so:$ ld.so --help | grep '\-v[0-9]'
x86-64-v4 (supported, searched)
x86-64-v3 (supported, searched)
x86-64-v2 (supported, searched)
If your machine is supported, and you’d like to try out the new packages, you can opt-in to the -enabled packages. First, install the latest version of dpkg:$ sudo apt update
$ sudo apt install dpkg
Then enable amd64v3 and update the packages, like so:echo 'APT::Architecture-Variants "amd64v3";' | sudo tee /etc/apt/apt.conf.d/99enable-amd64v3
sudo apt update
sudo apt upgrade
Note that  may print that it is “downgrading” these packages – this is just a cosmetic thing and will be fixed in 26.04 as well.If you do install the available  versions of packages, you will not be able to transfer your hard-drive/SSD to an older machine that does not support . Usually, we try to ensure that moving drives between systems like this would work. For 26.04 LTS, we’ll be working on making this experience cleaner, and hopefully provide a method of recovering a system that is in this state.We are very excited about the future of architecture variants and will keep you posted along the way. We hope that early adopters see some benefits of making the most of their hardware. Let us know how you get on!]]></content:encoded></item><item><title>Language models are injective and hence invertible</title><link>https://arxiv.org/abs/2510.15511</link><author>mazsa</author><category>hn</category><pubDate>Thu, 30 Oct 2025 09:47:24 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Happy Open Access Week from arXiv!YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.]]></content:encoded></item><item><title>Hello-World iOS App in Assembly</title><link>https://gist.github.com/nicolas17/966a03ce49f949dd17b0123415ef2e31</link><author>pabs3</author><category>hn</category><pubDate>Thu, 30 Oct 2025 02:37:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>IRCd service (2024)</title><link>https://example.fi/blog/ircd.html</link><author>pabs3</author><category>hn</category><pubDate>Thu, 30 Oct 2025 02:31:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
Ok. We have IRC at example.fi
Internet Relay Chat (IRC) is a form of real-time text communication developed by Jarkko Oikarinen in 1988. Initially created to replace a local BBS system at the University of Oulu in Finland, IRC quickly gained global popularity, becoming a foundational technology for online chat communities and influencing the development of modern instant messaging and social media platforms. Its significance lies in its pioneering role in connecting people across the internet, fostering early online communities, and setting the stage for contemporary digital communication.

To commemorate this pivotal technology, example.fi provides a simple and limited IRC server. This server is uniquely written in AWK, a scripting language traditionally used for text processing, highlighting the adaptability and enduring legacy of IRC. This creative implementation serves as both an educational tool and a tribute to the foundational role of IRC in the evolution of online communication.

In the following picture you see Irssi in the background and Hexchat on top of it:
Note: if you plan to connect to example.fi, make sure you do not use any fancy features. In irssi, use -nocap option. In Windows, use for example hexchat.
As this is written in gawk, most IRC protocol features are not implemented. This includes, for example, channel and user listings, topics, the concept of "operator" etc.
Technical fun fact: Total code count is around 60 lines of awk and a few lines of bash.
$ telnet example.fi ircd
Trying 65.108.91.190...
Connected to example.fi.
Escape character is '^]'.
USER foo
NICK bar
:example.fi 001 bar :Welcome to Internet Relay Network bar!~foo@65.108.91.190
:example.fi 375 test :- example.fi Message of the day -
:example.fi 372 test :- Current time is @787.188.beats
:example.fi 376 test :End of MOTD command.
Connection closed by foreign host.

Don't worry, we'll publish the code when it's "ready" :)This site is HTML 2.0 compliant.]]></content:encoded></item><item><title>NPM flooded with malicious packages downloaded more than 86k times</title><link>https://arstechnica.com/security/2025/10/npm-flooded-with-malicious-packages-downloaded-more-than-86000-times/</link><author>jnord</author><category>hn</category><pubDate>Thu, 30 Oct 2025 00:37:33 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OS/2 Warp, PowerPC Edition (2011)</title><link>https://www.os2museum.com/wp/os2-history/os2-warp-powerpc-edition/</link><author>TMWNN</author><category>hn</category><pubDate>Wed, 29 Oct 2025 23:52:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The PowerPC adventure—by far the most exotic release of OS/2In December 1995, after unexpectedly long development (but is that really unexpected?), IBM finally “shipped” OS/2 Warp, PowerPC edition. For brevity, this release will be further referred to as OS/2 PPC. Following years of hype and high expectation, the release was very low key and in fact marked the end of development of OS/2 for PowerPC. The product was only available to a limited number of IBM customers and was never actively marketed. OS/2 PPC may not even had a box, although there were nice looking official CDs.OS/2 PPC only supported an extremely limited range of hardware—IBM Personal Power Series machines. Those were desktop models 830 and 850, and OS/2 PPC probably also supported the Power Series ThinkPads 820 and 850, though that can be only inferred from the fact that the graphics chipset employed by these ThinkPads was on the very short list of supported devices in OS/2 PPC.The IBM Power Series computers were IBM’s rather short lived foray into the PowerPC-based desktop personal computer market, circa 1995-1996. The PowerPC CPU aside, the systems were very similar to Intel based hardware of that era. They were designed around the PCI bus, but also included ISA expansion slots and on-board Crystal Audio ISA PnP chips. The desktop Power Series machines were IDE based, ThinkPads used SCSI disks. The computers had standard serial and parallel ports, as well as most of typical PC hardware such as interrupt and DMA controllers. The desktops had onboard S3 864 video, ThinkPads used Western Digital flat panel chipsets. Several optional graphics cards were supported, notably Weitek P9100 based accelerators. The desktops also had onboard Ethernet chips (AMD PCnet).The Power Series systems were closely related to certain IBM RS/6000 workstations. The RS/6000 Model 43P-7248 was nearly identical to the Power Series 850. They used the same motherboard, only the RS/6000 had on-board SCSI controller. Unlike the RS/6000 systems intended for the workstation market and running almost exclusively IBM’s AIX operating system, the Power Series systems were designed for “regular” personal computer users. The machines were supposed to run OS/2, Windows NT, AIX, or Solaris. OS/2 PPC was only semi-finished, and the Solaris for PowerPC port (version 2.5.1) was similarly short-lived. Microsoft dropped PowerPC support in 1996, not long after the Windows NT 4.0 release. Most of the Power Series systems ended up running AIX, which supported them until version 5.1. Linux also supported the Power Series to some extent. Windows NT was clearly the closest competitor of OS/2 PPC.For this article, OS/2 PPC was installed on a Power Series 830, installed by its previous owner in a RS/6000 43P case. The CPU was a 100MHz PowerPC 604 with 256KB L2 cache, and the machine was equipped with 192MB RAM, which was the maximum it could handle. The graphics was an on-board PCI S3 Vision 864 with 2MB video memory and true color S3 SDAC. The machine was equipped with 2.1GB IDE hard drive—AIX can handle up to 8GB and Linux can utilize even larger disks, but OS/2 and NT were not happy with anything over about 2.5GB. The 830 was originally sold with either 500MB or 1GB disks and 16MB RAM. The Power Series 850 systems were equipped with 100 or 120MHz CPUs, slightly more RAM and larger disks.OS/2 Warp, PowerPC edition was delivered on two CDs. The first CD contained the operating system and BonusPak, the second CD was an application sampler with several demo applications.Installation was surprisingly easy and painless. The CD was bootable and there were almost no choices to make during installation—only the disk partitioning was user selectable. The PowerPC operating systems (OS/2, NT, AIX and Linux) generally did not coexist as there was no real equivalent of a boot manager and each OS wanted to install its own boot loader. The OS/2 installer re-partitioned the disk and overwrote any other operating systems. The boot partition had to be FAT. It was possible to create HPFS data partitions, but the HPFS support appeared to be somewhat unstable and likely a last-minute addition.After the OS was copied from the installation CD-ROM and the system booted from fixed disk for the first time, the user was greeted by the following screen:Indeed, OS/2 PPC really looked just like OS/2 Warp, at least at first glance. The system booted up in 640×480 mode with 256 colors, using the accelerated S3 driver. The desktop right after installation looked like this:Still very much like OS/2 Warp, except for that little Systems Management folder. This feature was not present in the Intel OS/2 Warp release, although it was added later. After installing the BonusPak and a few other additions and changing the resolution, the desktop still looked like plain OS/2 Warp, with the exception of the background bitmap of course (click on the picture to see full size screenshot):The system was now running in 1024×768 resolution, but still with 256 colors. The graphics chip supports 64K colors at this resolution, unfortunately the software used to take screenshots (a demo version of Impos/2) was unable to take any screenshots at this resolution. 256 colors it is then, and time to more closely examine the operating system. The README file is a good starting point, and it was quite long in OS/2 PPC. It consisted largely of a list of unimplemented or incomplete features.For example, notice the word “Connect” in the screenshot. OS/2 Warp, PowerPC Edition, doesn’t have any connectivity to speak of. Networking support, in a nutshell, didn’t exist. No LAN Server client, no TCP/IP, nothing. There was just HyperAccess Lite and CompuServe Information Manager, which worked (in theory at least) over a modem. The product name itself seems to have been a last minute change. Programs and documentation in many instances refer to OS/2 Warp Connect, PowerPC Edition, but the final product was called just OS/2 Warp and not “Connect”. One of the README files explains the name change and alludes to networking support in “future versions”.For development versions of OS/2 PPC there was TFTP support which talked directly to the microkernel Ethernet or Token Ring driver and entirely bypassed OS/2. This transport layer also supported remote debugging. This is in sharp contrast to Windows NT which fully supported networking (TCP/IP and SMB file sharing) on the same hardware. Networking was obviously planned for OS/2, but the project was killed before this part was done.Not everything was so blatantly unfinished though. The DOS support in OS/2 PPC was a pleasant surprise:On a closer look, it’s clear that OS/2 PPC included a full-fledged PC emulator, which supplied a virtual x86 CPU as well as common PC hardware. Interestingly, the DOS support in OS/2 PPC was based on PC-DOS 7 and not the outdated DOS 5 level code that OS/2 on Intel was stuck with. The OS/2 PPC DOS boxes thus had for instance the DOS E editor (very similar to TEDIT) or REXX support. Why IBM never updated the DOS support on the Intel side is a mystery. OS/2 PPC supported both windowed and full screen DOS sessions. The full screen sessions always ran in graphics mode, even when the emulated DOS application was using text mode.Not satisfied with “just” DOS emulation, IBM also supported Win-OS/2, both full-screen and windowed:It is difficult to judge how stable the DOS and Win-OS/2 emulation really was, but whatever little utilities came with the OS/2 system seemed to work well, including wave audio in Win-OS/2, and the performance was surprisingly good. IBM must have spent a lot of effort on the x86 emulation support. Documentation hinted at a possibility of future support for native OS/2 x86 applications via emulation.IBM also obviously spent a lot of time on the multimedia support in OS/2 PPC. The multimedia support worked unexpectedly well, especially when contrasted with the problems common on Intel machines.The system played video and audio without problems, with MIDI support either via a software synthesizer or an OPL3 compatible chip (the software synthesizer sounded far better). The application sampler CD came with several videos, mostly ads for OS/2. The PowerPC Toolkit also came with a beta version of OpenGL support, which shared code with IBM’s AIX workstation grade implementation.OS/2 PPC was a hybrid halfway between Warp 3 and Warp 4. The user interface looked like Warp 3, but many of the features of OS/2 PPC later showed in Warp 4 on Intel. One of them was the not very popular Feature Installer:The Feature Installer was used to install the BonusPak, several tools and games, and curiously enough, also the Command Reference which for some odd reason wasn’t part of the base install. Here’s one of those games:Again, there is no real difference from the Intel version, except for the about box text (notice the “Connect” text). And finally the IBM Works text editor—again there is no discernible difference from the Intel version:OS/2 for PowerPCs System OverviewOS/2 PPC was a strange OS. In many ways it was exactly identical to the Intel version, yet in other ways it was completely different. The user interface was the same and the entire API practically unchanged. Among the differences were the addition of full Unicode support and 32-bit console API (Kbd/Mou/Vio). The largely unchanged API was the reason why it was relatively easy to port existing OS/2 software to PowerPC. The biggest difference was not even the CPU but rather the compiler—IBM used the MetaWare High C/C++ for PowerPC development (it was allegedly cheaper for the IBM OS/2 division to contract MetaWare rather than IBM’s own compiler group). The MetaWare tool set was only used as a cross compiler hosted on x86 OS/2 systems. IBM used MetaWare’s compiler for embedded PowerPC development in general (IBM’s involvement with MetaWare goes at least as far back as AIX for PS/2), and MetaWare also marketed an OS/2 x86 product. Watcom was at the time working on PowerPC version of their compiler, but OS/2 PPC was killed before that project was finished. The last IBM Developer’s Connection release which contained OS/2 PPC material also included a beta version of IBM’s VisualAge C++ compiler. No release of a compiler (or a debugger) running natively on OS/2 PPC is known.The OS/2 PPC development tools were quite different from their Intel counterparts. To begin with, instead of the LX executable format, OS/2 PPC used the industry standard ELF. Several tools were completely unchanged (IPFC for instance), many were entirely new (linker, librarian, resource compiler). The ABI (Application Binary Interface) used in OS/2 PPC was based on the UNIX SVR4 PowerPC ABI. One notable difference was that OS/2 of course ran in little endian mode, unlike PowerPC UNIX ports but just like Windows NT.Delving deeper into the kernel, OS/2 PPC had precious little in common with the Intel version. The product was based on the IBM microkernel, which was a refinement of the Carnegie Mellon University Mach microkernel. The microkernel bore no resemblance to the Intel OS/2 kernel whatsoever and it was also very different from most other operating systems of the time (NeXTSTEP was also based on the Mach microkernel).The initial grandiose plan was to build the Workplace OS, the One Ring to Bind Them All of operating systems. Workplace OS (or WPOS for short) was supposed to be built on top of the Mach microkernel and support multiple “personalities”. The personalities would implement existing operating systems such as OS/2, AIX, Windows NT and perhaps even Mac OS. In the end this never happened and the only supported personality was OS/2. This was somewhat similar to Windows NT where the the non-Windows personalities (environment subsystems) eventually withered away.The initial plan was still tangible in OS/2 PPC. The OS/2 personality was implemented in the “OS/2 Server” and there were certain “personality neutral” services. Most device drivers were personality neutral and worked directly with the microkernel. This included disk and network drivers. A notable exception were the display drivers, where OS/2 PPC introduced the GRADD model (later ported to Intel OS/2). Documentation on OS/2 PPC internals is somewhat sparse and the online books shipped with PowerPC Toolkit were in many cases either incomplete or simply unmodified copies of OS/2 for Intel documentation. A good source of information is the Redbook titled “OS/2 Warp (Power PC Edition) – A First Look” published by IBM International Technical Support Organization in December 1995, document number SG24-4630-00 for those interested.OS/2 for PowerPC ImpressionsWhat was OS/2 Warp, PowerPC Edition like? An unfinished product, rough around the edges but simultaneously technically very interesting and advanced and showing promise. Even though the OS/2 PPC release wasn’t called a beta, it was obvious that this was a beta level product (if even that in some respects). Many features were unfinished or completely missing, notably networking. The kernel code didn’t look much like a production build and printed out quite a lot of debugging output on a serial console, if one was attached. The HPFS support was very unstable, and the stability of Win-OS/2 left a lot to be desired. There were too many clearly unfinished parts of the product—documentation, missing utilities, etc.On the other hand a large portion of the system worked well. The user interface and graphics subsystem in general didn’t exhibit any anomalies. Multitasking was reliable and all things considered, responsiveness quite good for a 100MHz CPU and code that was not likely to have been performance tuned. The multimedia subsystem worked much better than expected. Many things were much improved compared to Intel OS/2—internationalization, graphics subsystem, updated console API, and so on. The system seemed to have enough raw power, even if it wasn’t harnessed too well. Boot time was rather long but once up and running, the system was snappy (with some exceptions, notably the CD-ROM driver). To reach true production quality, the OS would have needed at least additional six months of development, perhaps more.How useful was OS/2 PPC? Not very. In fact, it was almost completely useless. It only ran on three or four models of rather rare IBM machines and supported almost no additional devices. The OS was clearly unfinished and not entirely stable. Worst of all, there were about zero applications. Because OS/2 PPC was never truly in use, PowerPC versions of OS/2 applications were never sold, although several OS/2 ISVs ported their applications to OS/2 PPC as evidenced by the application sampler. Porting wasn’t very difficult and tools for building PowerPC applications were available, but since there was no demand for them, there was little point in porting.OS/2 for PowerPC was undoubtedly an interesting experiment, albeit a failed one. It is impossible to tell whether this failure was caused more by shortcomings of OS/2 for PowerPC or the failure—perhaps just falling far short of expectations—of the PowerPC platform as a whole.Without the generosity of Mike Kaply and Chris Graham, this article could not be written.Some of the above information was derived from IBM documentation and Redbooks, which may have been inaccurate due to the evolving nature of the OS/2 PPC project. Most of the remaining text is the result of observation and conjecture.If you have any additional information, corrections, or interesting stories about OS/2 for PowerPC, please post a comment.]]></content:encoded></item><item><title>Crunchyroll is destroying its subtitles</title><link>https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/</link><author>Daiz</author><category>hn</category><pubDate>Wed, 29 Oct 2025 23:31:24 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Since the beginning of the Fall 2025 anime season, a major change has started taking place on the anime streaming service Crunchyroll: the presentation quality for translations of on-screen text has taken a total nosedive compared to what has been on offer for many years, all the way up until the previous Summer 2025 season. Now, more and more subtitles on Crunchyroll are looking like this:In these new subtitles, translations for dialogue and on-screen text aren’t even separated to different sides of the screen – everything is just bunched up together at either the top or the bottom with only capitalization to distinguish what’s what, leading to poor readability. In addition, lots of on-screen text is just left straight up untranslated. For comparison, here’s what subtitles on Crunchyroll looked like just a season prior in Summer 2025:Here, stylized and positioned text is used to effectively present translations for multiple instances of on-screen text even with dialogue going on at the same time. It’s very obvious how each bit of translation connects with the original footage! Given this level of presentation quality with previous subtitles, it should be clear that new subtitles for Fall 2025 shows shouldn’t look as bad as they do.Third-party subtitles with zero oversight – what could possibly go wrong? Screenshot from The Yuzuki Family’s Four Sons (Fall 2023, Crunchyroll)Now, subtitles with poor presentation quality aren’t entirely new to Crunchyroll. Some titles on the service have always had third-party subtitles—that is, subtitles provided by the licensor—that Crunchyroll just puts up with zero oversight. This in itself has caused numerousissues over the years, but the pressing issue today is how low quality presentation reminiscent of third-party subtitles can now be found even in first-party subtitles, by Crunchyroll’s own subtitling staff.And that’s “destroying subtitles”?It sure is when it’s anime we’re talking about! Anime as a medium has made prominent use of on-screen text basically since its inception. The amount of it varies from series to series, but almost every anime out there makes use of on-screen text at one point or another, with some featuring downright ridiculous amounts of  (what on-screen text is called for short). With all this on-screen text, it is very common for there to be text visible on the screen potentially in multiple positions, even when characters are speaking.As such, if you are in the business of localizing anime for non-Japanese audiences, you need to be able to deal with on-screen text. At bare minimum, when subtitling anime, you should be able to do  (multiple lines of text on the screen at the same time) and  (the ability to freely place subtitles anywhere on the screen). Anything less and you are likely to run into trouble the moment you get to something as simple as a next episode preview:Multiple instances of on-screen text are running in parallel with dialogue. Screenshot from  (Winter 2014, Underwater-FFF fansubs)Overlaps and positioning are really just the bare necessities for dealing with on-screen text in anime though – ideally, you should also be able to use different fonts, colors, animate text in various ways, etc. Making use of all these possibilities is an art unto itself, and this art of on-screen text localization is commonly referred to as typesetting. Typesetting is important even when dubbing anime, as all that on-screen text is going to be there in the video all the same!So why would Crunchyroll get rid of typesetting?That is a good question. It is no exaggeration to say that up to this point, Crunchyroll with its typesetting was the unambiguous market leader when it came to presentation quality for official anime subtitles… though for the most part, other services dealing in anime have never even bothered to try. Sentai Filmworks’ Hidive is just about the only other anime service that even attempts to do typesetting, though they license so few shows per season that they are a tiny player compared to the Big Boys of anime streaming.And it is very likely the existence of these Big Boys that has played a key part in Crunchyroll’s eradication of its typesetting. Netflix and Amazon Prime Video probably need no introduction to anyone reading this – both are very popular general streaming services. Despite anime being only a minor part of their catalogs, a large chunk of today’s anime watching worldwide happens through said services thanks to their sheer user counts alone.Crunchyroll clearly seems to know this, which is why it has been sublicensing its anime properties to both Amazon and Netflix for multiple years at this point. But with such sublicensing comes the matter of dealing with the subtitling standards of general streaming services. I’m not going to mince words: these standards are  at least as far as anime is concerned. Netflix for example insists that you stick to at most two lines of text on screen at once, which makes sense most of the time… if you’re talking about dialogue alone. Unfortunately, it becomes completely inadequate when dealing with anime’s plentiful on-screen text. Moreover, the standards of these services actively refuse to give you tools like positioning and overlaps, even though the TTML subtitle format they use supports said features!With such typesetting-hostile standards to deal with, Crunchyroll had basically two choices for how to make sublicensing to Amazon and Netflix work with their existing subtitles that feature actual typesetting: Either 1) try to negotiate with the services for permission to make use of more TTML capabilities (that the subtitle renderers of said services should already support!) or 2) start mangling subtitles with typesetting into something compatible with the awful subtitling standards of the general streaming services. I am not aware if Crunchyroll ever attempted the former, but I can confirm that it eventually started doing the latter.Editors among Crunchyroll’s subtitling staff were  to convert finished high quality subtitles with typesetting into limited low quality TTML subtitles without typesetting, compatible with Amazon & Netflix subtitling standards. They got paid extra for the manual effort required by the process.Overlapping on-screen text and dialogue makes for a miserable anime watching experience with limited TTML subtitles. Video clip from  (Winter 2018, Netflix)Unfortunately, after a couple years of this kind of manual conversion work, the Crunchyroll leadership seems to have decided that it isn’t enough, and that Crunchyroll must do away with high quality subtitles with typesetting entirely and only produce low quality TTML subtitles without typesetting from now on. But if they already had a working process for high quality subtitles at home and low quality TTML subtitles elsewhere, why would they just decide to give that up in order to produce exclusively low quality subtitles? It doesn’t seem to make very much sense, even as a cost-cutting measure. There should be so much value in being able to advertise best viewed on Crunchyroll to potential audiences for long-term growth, right?To understand  we need to look into some relevant history. Specifically, what happened after Sony bought Crunchyroll and merged it with Funimation, another US anime distributor that Sony had bought previously. But in order to also understand  first we need to look at what both Crunchyroll and Funimation were like before this fateful merger happened, as well as how they approached anime subtitling over the years.A short history of Crunchyroll and its subtitling standardsCrunchyroll launched in 2006 as a pirate streaming site focused on East Asian media content, featuring fansubbed anime, live action drama, music videos, and so on. There was nothing particularly remarkable about the site back then – as a rule of thumb, pirate streaming sites are always worse quality-wise than if you just directly downloaded the pirated releases they use as a base, and the sites mostly exist to make their admins illicit money through ads, begging for donations, and other shady crap. It is important to note though that legal anime streaming basically wasn’t a thing at this time.Crunchyroll in 2007. The “help out” message at the top is asking for donations.However, it was likely this exact venture capital funding that enabled Crunchyroll to negotiate a major deal with the Japanese broadcasting company TV Tokyo, which was announced at the start of 2009. This announcement brought with it the news that Crunchyroll was going full-time legitimate and getting rid of all its pirate content. With this move, Crunchyroll found itself in a position of having to start producing subtitles of its own (instead of just uploading fansubs) and somehow present said subtitles to its customers.Aegisub is an advanced subtitling software built by fansubbers, for fansubbers.For the subtitle production part, Crunchyroll managed to strike a deal with a bunch of fansubbers to take on the job. This single decision was a fateful one, as it was the foundation for basically everything that came after – with former fansubbers on the job, the tools of the trade were set according to the standards of fansubbers: the subtitling software of choice was to be Aegisub, and the subtitle format of choice was to be Aegisub’s native format, Advanced SubStation Alpha, or ASS for short.ASS is an extremely powerful format in terms of formatting and styling capabilities, and with Aegisub, it is easy to produce ASS subtitles that make use of said capabilities. However, as a streaming site, Crunchyroll needed to be able to present these ASS subtitles in the browser somehow, and the only full-fledged ASS renderers that existed were only available in the traditional local media playback environments targeted by fansubbers, which meant that Crunchyroll couldn’t make use of said renderers on the web directly.Now, there are two main ways to subtitle videos, with opposing pros and cons: – the subtitles are burned into the video itself.  as you only need to be able to play video, but inflexible for updates and multiple languages as you have to recreate your video files over and over again with expensive processing called . – the subtitles exist as their own separate media track that the video player renders on top of the video in realtime during playback, making softsubs complex to playback, but updates and multiple tracks are very cheap as you only need to deal with tiny subtitle files while the video files remain unchanged.As such, one way Crunchyroll could have solved the subtitle presentation problem would have been to simply hardsub its ASS subtitles, but despite the challenges it posed, Crunchyroll decided to go with softsubbing instead (which was also the fansub standard at the time). And so Crunchyroll set out to build its own ASS renderer in Flash, the primary technology used to play video on the web at the time. Here’s a screenshot of some of the first subtitles ever officially authored by the fully legitimate Crunchyroll, rendered in the current ASS renderer but adhering to the limits of the company’s very first Flash subtitle renderer:Screenshot from  (Spring 2009, Crunchyroll)As can be seen, even the very first version was already capable of handling both overlaps and positioning. Now, the positioning was limited to the eight edges and the center of the screen, making for just nine possible positions total, but even that was enough to handle the humble next episode preview at the very least. Beyond these, the first version also supported fading animations. It wasn’t much, but it did cover the bare minimum for dealing with on-screen text in anime.Over the years, Crunchyroll managed to slowly improve its Flash subtitle renderer to enable the use of more ASS features. Custom colors, multiple fonts, multiple styles, rotation, and full positioning were implemented (albeit in somewhat hacky and unwieldy fashion). This went on until 2018, when Crunchyroll was faced with a major issue: Flash was seeing rapid decline in use, and web streaming was shifting over to HTML5-based technology. However, with a custom ASS renderer built in Flash, Crunchyroll couldn’t easily make the change, as it would mean having to essentially rebuild the custom subtitle renderer they had from scratch in HTML5 (as much like in the Flash days, there still were no solutions native to the web available for rendering ASS subtitles).However, Crunchyroll managed to come up with a way to solve the problem of moving from Flash to HTML5 with the help of another new web technology called WebAssembly, which allowed developers to take code that wasn’t developed for the web and compile it for use on the web. With WebAssembly, Crunchyroll could take libass, one of the few fully-featured ASS renderers out there, and use it for their new HTML5 player. Now, not only did all their old ASS subtitles render nicely in HTML5, but the possibilities for typesetting at Crunchyroll had taken a huge leap forward. And the subtitling staff at Crunchyroll was more than happy to make use of this newfound power.You couldn’t see typesetting like this on Crunchyroll back in the days of Flash. Video clip from  (Fall 2022, Crunchyroll)That said, despite having a technically fully-featured ASS renderer to work with, there were still limitations. Code compiled with WebAssembly runs worse compared to its original native counterpart, which limits how heavy the typesetting can be (with the flexible features of ASS, it is very easy to produce typesetting that simply cannot be rendered in realtime even on powerful computers, resulting in notable lag during playback). A commercial service like Crunchyroll will also generally want to keep its content watchable even on lower-end devices, which further reduces how complex any typesetting can be.And this is the limited but functional standard of typesetting that Crunchyroll users got to enjoy (with first-party subtitles) up until the fateful season of Fall 2025 that prompted the creation of this article.Before we move to the conclusions for this section, though, it is worth noting that while Crunchyroll currently uses softsubbed ASS subtitles whenever it can, there are platforms and devices (like various TVs) where this kind of ASS rendering simply isn’t possible to do. Crunchyroll is available on some platforms like this, which means it has been making additional hardsubbed versions of everything on top of the usual softsubbed ones.So, what can we learn from all this? At least one thing is abundantly clear: for most of its existence, the leadership at Crunchyroll had at least some respect and understanding for anime as a medium. They understood that it was important to be able to deal with on-screen text in their subtitles, and allocated enough resources to make typesetting possible. The company even managed to improve in this regard over time, albeit very slowly.That said, anyone familiar with anime fansubs of the 2010s and 2020s probably can’t help but feel disappointed that even the highest effort typesetting from Crunchyroll could only ever be on the level of fansub releases from around 2010 at best. Why 2010 specifically? Because from 2011 onwards, fansubbers started widely incorporating advanced motion tracking into their typesetting. Observe an example of such fansub typesetting from over a decade ago, the likes of which has never been seen on Crunchyroll:Video clip from  (Fall 2013, Underwater fansubs)Now, while fansubbers giving away their work for free might get away with saying just get a better computer to anyone whose devices can’t render softsubbed typesetting like this in realtime, an official service that lots of people pay for doesn’t really have the same luxury, which is the main reason why you don’t see stuff like this softsubbed on Crunchyroll. But this is not an insurmountable problem, so make no mistake: official anime services could absolutely offer typesetting with similar level of quality to the best of fansubs. The basic solution to the performance problem is very simple, even: you simply hardsub the typesetting. This would work from streaming to physical disc releases and only the sky would be the limit in terms of the typesetting quality you could offer, as realtime rendering would no longer be a concern!Now, as mentioned earlier, hardsubbing does make things more complicated and expensive on the backend as you need to encode and store multiple copies of video. Crunchyroll is already dealing with this, though! But if costs are an issue, the system is pretty easy to improve in theory: if you keep the dialogue softsubbed, only the parts of the video that actually feature typesetting would be hardsubbed, and with some clever engineering and an understanding of how modern media formats work, you would only have to keep multiple copies of the typeset parts. And since the average anime episode has on-screen text only for a small percentage of its total runtime, combining softsubbed dialogue and hardsubbed typesetting like this would make for a highly cost-effective setup.Typesetting like this would be possible to do even for official anime services. Video clip from  (Fall 2021, Chasa fansubs)And since with a mixed system like this you would only have softsubs for the technically simpler dialogue, you could even convert these dialogue-only ASS subtitles to a simpler but more widely supported subtitle format for playback, which theoretically should do away with the need to keep fully hardsubbed copies around entirely, without any real loss in quality! I actually built a minimal version of a mixed system like this myself when I was doing some anime streaming work a few years back and can confidently say that this would be extremely doable for any official anime service… as long as they just cared enough.Keep this example of fansub typesetting in mind for later. Video clip from Danganronpa: The Animation (Summer 2013, UTW fansubs)Unfortunately, any interest Crunchyroll had for improving their subtitle rendering for typesetting seemed to run out after the 2018 transition to WebAssembly libass. Not that it actually ever seemed to be all that high to begin with, though, as evident by some of the low-hanging fruit that Crunchyroll never bothered to pick in this regard; the most obvious of which would be Crunchyroll’s dogged insistence to restrict typesetting font choices to Core Fonts for the Web. Free for commercial use fonts have been plentily available since the Flash days, and custom fonts have been well supported on the web for a similarly long time.Anyway, it would have never been all that hard for Crunchyroll to support custom fonts for typesetting, especially after the 2018 move to HTML5. The underlying technology was there and font files are tiny in size compared to the video files being streamed – this would have been an extremely simple and effective improvement for all typesetting efforts. Yet Crunchyroll never reached for this improvement, which is why  has kept appearing in Crunchyroll typesetting with depressing regularity.There it is again.  Screenshot from  (Summer 2023, Crunchyroll)It is also disappointing how regularly the anime staples of opening & ending songs are still left untranslated on Crunchyroll, though this issue is admittedly much harder to solve than you’d expect. Still, it is possible to do so, especially with Sony’s resources behind the company today. That goes double when Sony is involved in anime production in any way, as then the songs being used should be well-known to all relevant parties well in advance of airing for timely rights-clearing. So if Crunchyroll/Sony is in any way involved with an anime’s production, it should basically always be possible for songs to be translated the moment the first episode is released.But that’s enough about Crunchyroll’s history. Now it’s time to look at the other company mentioned earlier and see how they’ve fared in comparison…A short history of Funimation and its subtitling standardsIn the early 90s, Japanese-American businessman Gen Fukunaga was approached by his uncle who was working as a producer for Toei. A proposal was made: if Fukunaga could start an anime company in US, Toei would license the rights to the Dragon Ball franchise to it – a franchise that was already making mad cash in Japan. Sensing an opportunity, Fukunaga found investors, and thus in 1994 Funimation was born. A year later, Dragon Ball was on US TV, dubbed and edited to “conform to American sensibilities and tastes”.It was especially  (1989-1996) that hit it big in the US.In the early 2000s, fueled by Dragon Ball’s success, Funimation started expanding its business by getting home video distribution rights for 4Kids Entertainment licenses and non-Japanese kids’ cartoons, the latter eventually expanding into getting involved in production too. But beyond increased investment in kids’ cartoons, Funimation also started experimenting with more anime licenses of its own, the 2001 anime adaption for Fruits Basket being one of its early standout releases.Out of these various expansion attempts, “more anime” seemed to be the one to work out best, and towards the end of the 00s that became the main direction of Funimation’s business. This move was helped along by a bunch of licenses obtained from now-defunct US anime publishers Geneon USA and ADV. And in the spring of 2009, hot on the heels of Crunchyroll going legit, Funimation announced that they too were getting into the anime streaming business. The resulting anime streams from Funimation were hardsubbed and looked like this:What you see here is exactly what you got: plain text at top center or bottom center, with dialogue on bottom, and translations for all on-screen text piled up top. So while overlaps were technically supported, full positioning did not seem to be possible, which made things quite awkward the moment there was more than one sign visible on the screen at the same time. This was also the standard you could expect from Funimation’s DVD and Blu-ray releases. And beyond the way too common dialogue three-liners (which are generally terrible for readability), sometimes you even saw :Screenshot from  (Winter 2014, Funimation)The subtitling software that Funimation was using at the time was Telestream MacCaption. In terms of usability and general authoring features, it was no match for Aegisub, although it was actually capable of doing some overlaps, positioning, and styling – Funimation just never chose to make use of these capabilities for its anime subtitles.TeleStream stopped supporting MacCaption in 2023.This remained the Funimation subtitle standard all the way until 2016, when Funimation struck a deal with Crunchyroll. Going forward, subtitled releases for Funimation licenses would be found on Crunchyroll, while dubbed releases for said titles would be on Funimation’s new streaming platform, However, the only thing that really changed is that instead of Funimation content being hardsubbed on their website, it was now softsubbed on Crunchyroll to the exact same standard: plain text on top center or bottom center, often with three or more lines of dialogue at once, even.Sometimes you could see sign translations on bottom too. Screenshot from  (Spring 2017, Funimation/Crunchyroll)Nothing else of particular note happened during this time period when it comes to Funimation’s subtitles. However, it is worth mentioning that Funimation dubs did have simple hardsubbed typesetting sometimes; this only seemed happen at the whim of the dubbing side of Funimation though, as these hardsubbed signs were never present in the subbed versions, nor were they a consistent feature of Funimation dubs in general.In 2017, Sony purchased Funimation as part of its growing collection of international anime distributors (Sony had previously bought Madman Anime and AnimeLab in Australia and Wakanim in Europe). As a result of this buyout, towards the end of 2018 the license sharing deal between Funimation and Crunchyroll was dissolved and soon after Funimation started serving new subtitled streams on FunimationNow, which were softsubbed and looked like this:No longer were the subtitles even making use of overlaps. Where dialogue translation used to go on bottom and sign translation on top when both were present, now all text was stuck on the same side of the screen together, either on top or bottom, but never both at the same time anymore.How this further reduction in subtitling capabilities came about cannot be said for sure, but there are several possible explanations. For one, another major thing that happened at the end of 2018: Funimation signed a big sublicensing deal with the general streaming service Hulu, which meant dealing with Hulu’s subtitling standards and authoring accordingly limited subtitles – because as could be expected, the subtitling standards of a general streaming service did not account for the needs of anime in any real way.Only the middle column of the blackboard is translated here. Good luck figuring that out with subtitles like these. Screenshot from  (Fall 2019, Funimation/Hulu)Another possible reason for these less-than-great changes in Funimation’s subtitling standards was that around this time the company started using the cloud-based subtitling toolkit OOONA Tools by the localization service provider OOONA. OOONA Tools, by default, do not allow for the creation of subtitles with overlaps. While it can be done in OOONA today by tweaking the options or by using OOONA’s track features (which are quite similar to those of MacCaption, incidentally), it is possible that at the time these features were either not available or that it wasn’t possible to correctly export subtitles with overlaps to the WebVTT subtitle format that was being used on FunimationNow.Screenshot of , the primary subtitling software in OOONA Tools.Regarding that last possibility in particular, there is this OOONA FAQ entry that mentions how not all formats support […] overlapping subtitles and that Currently, it’s supported in IMSC1.1, ITT and Videotron Lambda CAP exports. However, based on my own testing, OOONA Tools can properly export subtitles with overlaps in more formats today than just the ones mentioned here (including WebVTT), meaning that the FAQ entry is in fact outdated – but it was likely true at some point.In any case, this was the extremely limited standard of subtitling that Funimation customers had to live with until the service was shut down in 2024 as a result of the Funimation-Crunchyroll merger.Now, what can we conclude from all this? If nothing else, one thing seems abundantly clear: the Funimation leadership never truly cared about or respected anime as a medium. From the very beginning, it’s clear that Gen Fukunaga (a businessman in his 30s at the time) got into the business with the mindset of making money with kids’ cartoons, and this only became more evident with how Funimation tried to expand into more types of kids’ cartoons before eventually realizing that anime is where the money was at.But even with this eventual focus on more anime, no resources seem to have ever been dedicated to make typesetting an actual thing at Funimation, despite how obviously beneficial it would have been for their key product of localized anime. And the way Funimation never even bothered to figure out how to make the most of MacCaption, the expensive enterprise subtitling software they kept using for over a decade… while I speculated about possible technical reasons for Funimation abandoning even overlaps when they started producing softsubs for FunimationNow, there was always one possible additional reason: they just didn’t care at all. They ran into a problem, no resources were dedicated to fix the problem, and the subtitles got permanently worse as a result.Remember the fansubbed version of this from earlier? Here’s Funimation in comparison. Video clip from Danganronpa: The Animation (Summer 2013, Funimation)The whole move to OOONA was questionable in itself, as while OOONA was capable of exporting subtitles to both WebVTT for FunimationNow and TTML (or SRT, a very limited subtitle format) for Hulu in 2018,  Why start paying for a monthly subscription service when your existing paid-for enterprise software should be able to deal with your needs just fine? I suspect the primary motivation behind the move (which could have even originated from the new parent company Sony) might have been the fact that it was trendy for companies at the time to move everything they possibly could to The Cloud™, regardless of how much sense it actually made… but that’s enough about OOONA for now.Ultimately, Funimation’s subtitling standards were extremely poor to begin with, and they only managed to make them worse over time. That is something that only utter indifference or outright disdain for anime as a medium could bring about, which seems to have been the exact attitude that Gen Fukunaga cultivated at the executive levels of Funimation – and his followers appear to have carried the torch even after his departure from the company. But more on that in the next section, when we finally get to the Funimation-Crunchyroll merger.The Funimation-Crunchyroll merger and its consequencesFollowing Sony’s 2017 purchase of Funimation, in 2019 Sony bought out Gen Fukunaga from the company entirely, which led to him stepping down as the General Manager, with Colin Decker taking his place. Soon after, Sony formed the Funimation Global Group to consolidate all the international anime publishing services it had bought, with Decker in charge of the joint venture as the CEO. Then, in late 2020, Sony announced that they were going to buy Crunchyroll, placing it under the executive control of the Funimation Global Group. The acquisition was completed in August 2021, coming with a statement from Sony that their goal is to “create a unified anime subscription experience as soon as possible”.Soon, there would only be one.Then, in March 2022, the news came that Funimation, Crunchyroll, Wakanim, and VRV (Crunchyroll’s more general streaming service) would all be merged together into a single streaming service that would exist under the name of Crunchyroll (as it had the strongest brand of the lot). Funimation Global Group LLC was renamed to Crunchyroll LLC, with Funimation executives remaining in charge. Soon after, Colin Decker stepped down as the CEO, with Rahul Purini (previously COO) taking his place. The merger was complete.Things weren’t much better for those left behind, as laid out in this Bloomberg article from 2024. Staff from Funimation was notably hostile towards those from Crunchyroll:Tension between the camps arose almost immediately. In a Zoom meeting announcing [Sony’s purchase of Crunchyroll], Funimation workers accused Crunchyroll of being pirates, alluding to the site’s history, according to two people who were present.While Crunchyroll workers were quickly frustrated with the new executives from Funimation:Current or former employees describe Crunchyroll’s new management–primarily from Funimation–as out-of-touch with employees and the anime fans the company once prioritized. Some executives write off anime as “kids’ cartoons,” they said, and resist hiring job candidates who describe themselves as fans.How typesetting gets destroyedIn 2025, the executives came up with an idea: Crunchyroll should move away from Aegisub and ASS subtitles with typesetting and start producing exclusively limited TTML subtitles without typesetting in OOONA Tools. The likely end goal of this is to get rid of Crunchyroll’s unique ASS-based subtitle rendering entirely in favor of something more “industry standard” like TTML-based subtitle rendering. This would mean no longer having to pay staff for manual ASS-to-TTML conversion, as well as being able to drop the relatively expensive fully hardsubbed encodes for limited playback environments where ASS rendering is not possible (but some sort of TTML rendering usually is).However, a major change affecting all aspects of the company’s subtitling pipeline doesn’t happen overnight, especially considering Crunchyroll’s large back catalog of ASS subtitles with typesetting that couldn’t be automatically converted to limited TTML subtitles without typesetting. So while the subtitling staff was to be (begrudgingly) busy experimenting and onboarding with OOONA and doing manual ASS-to-TTML conversions for back catalog titles, technical work would also need to be done to prepare for this vision of a TTML-only future.And what an exciting future of not being able to read signs that would be! Screenshot from This Monster Wants to Eat Me (Fall 2025, Crunchyroll)For this purpose, Crunchyroll seems to have decided that it would take its existing manual ASS-to-TTML conversions produced by the subtitling staff and treat them as the new master subtitle files. These TTML “masters” would then be—for the time being—converted back to ASS with Closed Caption Converter for use with the current ASS-based subtitle rendering. And so, with the start of the Fall 2025 anime season, a plan like this was pushed to production; while regular ASS subtitles were still being produced by Crunchyroll’s subtitling staff, these ASS subtitles with typesetting were generally left unused, while only limited ASS-to-TTML-to-ASS conversions without typesetting were being presented to customers on most shows.Implementing this interim pipeline with Closed Caption Converter didn’t seem to go exactly as planned, though, as some Fall 2025 shows on Crunchyroll ended up having no subtitles at all on release, including the premieres of the latest seasons of hit shows My Hero Academia and Spy × Family.Over the past few days, some users experienced delays in accessing the content they wanted and subtitle issues across certain series. These were caused by internal system problems – not by any change in how we create subtitles, use of new vendors or AI. Those internal issues have now been fully resolved.Quality subtitles are a core part of what makes watching anime on Crunchyroll so special. They connect global fans to the heart of every story, and we take that responsibility seriously.Thank you for your patience. We’re committed to continuing to deliver the authenticity, quality, and care that fans deserve.Following this statement, some of the new Fall 2025 shows have had their ASS-to-TTML-to-ASS subtitles switched out to the previously unused regular ASS subtitles. Other shows haven’t. And some shows in the Crunchyroll back catalog have been updated with ASS-to-TTML-to-ASS subtitles, though the exact timing of these back catalog updates is unknown.With all of this, the future of typesetting on Crunchyroll is unclear.And that’s how we’ve found ourselves in the situation we face today. Remember what the first Crunchyroll subtitles from 2009 looked like? Yeah, these new subtitles adhering to limited TTML standards are even worse than the subtitles from 2009 in terms of how on-screen text can be handled! In other words: The presentation quality of Crunchyroll’s first-party subtitles has reached an all-time low in 2025.Can’t even handle a next episode preview properly anymore. Screenshot from Chitose Is in the Ramune Bottle (Fall 2025, Crunchyroll)There is only one conclusion that can be drawn from that: the Funimation-turned-Crunchyroll executives still do not have any respect for anime as a medium. In addition, they seem to be treating Crunchyroll and its ways of doing things as the ways of  – which isn’t entirely incorrect, as Crunchyroll’s use of Aegisub and ASS  originate from the ways of pirate fansubbers. But fansubbers deeply care about anime as medium (they wouldn’t be illegally subtitling it for free as a hobby otherwise), which in turn means that the ways fansubbers have developed to subtitle anime are in fact extremely efficient for the job – much better than basically any “industry standards” for subtitling, even.But that clearly doesn’t matter to the executives. The only thing that seems to be on their mind is how to best make money with kids’ cartoons that none of them personally watch, and what they seem to consider “best” is getting rid of everything positively unique about Crunchyroll in favor of doing things the Funimation way, even if that means ditching Aegisub and ASS in favor of OOONA Tools and TTML and getting rid of typesetting in the process.  conclusion is further supported by the fact that  with notable reduction in typesetting quality on Blu-ray as a result:Then there’s the whole plan of moving to OOONA in general, which is even more questionable than it was back in the Funimation days. Crunchyroll has a lot more to lose in terms of subtitle quality than Funimation ever did, yet the executives seem to want to go back to their “old reliable” regardless. I can’t even see it saving them any money in the long run, considering that Aegisub is completely free software while OOONA will incur constant ongoing costs with its per-user subscription pricing. Rather than authoring limited TTML in OOONA directly, paying the subtitling staff to keep the manual ASS to TTML conversions going would likely be cheaper!Beyond that, there is also the thing about OOONA being an Israeli company. It is certainly a choice, not only in 2018 but most certainly in 2025, to heavily invest in the services of a company from a country that is actively committing genocide. However, to quell some unsubstantiated internet discourse I have seen in relation to this, I do want to emphasize that OOONA being Israeli is not really directly relevant to the quality issues this article is about.EZTitles is another popular enterprise subtitling software. Notice how they mention AI directly in their navigation.The reason for this lies in enterprise subtitling software (“industry standards”) being universally poor when it comes to producing high quality typesetting for anime, so it wouldn’t really matter which software suite a switch was being made to – no matter what, moving away from Aegisub would destroy typesetting as it currently exists on Crunchyroll. And while Crunchyroll’s CEO has expressed his interest in AI subtitles, at least currently there has been no signs of any kind of AI (Israeli or otherwise) being used to create first-party subtitles on Crunchyroll.Why Crunchyroll is so confident it will get away with this (or: how capitalism ruins everything)Finally, I want to talk about the possible reasons for Crunchyroll executives feeling so confident about getting away with making their own primary product so much worse. Ultimately, it comes down to the fact that international anime licensing operates primarily on an exclusive licensing model. This means that generally only one service will be able to offer a specific title in specific language(s) in specific region(s), unless the service voluntarily decides to sublicense it out to others. This in turn upends the assumption that the existence of multiple anime services would be beneficial to consumers, as the services don’t actually have to engage in competition on customer-beneficial factors like service quality almost at all – instead, they can just focus on hoarding as many exclusive licenses as possible.I once asked former Crunchyroll CEO Kun Gao about “exclusivity or completeness” in this Reddit AMA. He dodged the question but basically said “exclusivity”.This kind of “competition” twisted by exclusive licensing is more like a casino, where the customers might occasionally be thrown a bone, but at the end of the day, the house always wins. And the anime companies very much prefer to keep it that way, even if it means never being able to offer full coverage of new anime seasons – a limited amount of exclusives is much more important to them. Dreams of infinite growth are what drives the modern-day game of capitalism, and spending money to please customers rather than shareholders goes directly against said dreams. It’s all about spending as little money as possible to make as much money as possible.This is why the capitalists in charge of all the big companies these days are so excited about AI too: nothing gets them going more than the idea of not having for pay for those pesky human employees. This is no doubt the actual reason why Crunchyroll CEO Rahul Purini is interested in AI subtitles. It doesn’t matter that anime localization costs are a drop in the bucket compared to the overall costs of anime production, even if you were talking about super high quality work with fansub-level typesetting. Any excuse to cut the wages of real human workers is one step closer to the next yacht purchase for the executive upper class.…Whew, got a bit heated there. Anyway, the most likely reason why Crunchyroll executives believe they can get away with reducing the quality of their own service so much? Because Crunchyroll doesn’t have any meaningful competition thanks to the primarily-exclusive licensing model used by the international anime industry. Even if they make the service worse, what can you do about it? Cancel your subscription and not watch the new anime you’re excited about?If you are currently subscribed to Crunchyroll, cancel your subscription.When asked for a reason, mention the bad subtitle quality and lack of typesetting.You could even link to this article. Beyond that, and this applies to people who aren’t subscribed to Crunchyroll as well:  Share this article around, talk to people about how Crunchyroll is destroying its subtitles, make it so that Crunchyroll executives can’t ignore the issue. And the most important thing: Keep it up until Crunchyroll actually makes a clear public commitment to keep typesetting anime.“Improving Subtitle Quality for Crunchyroll” is what we’d like to see here in 2025.I also want to emphasize that the recent statement Crunchyroll made about its Fall 2025 subtitles isn’t really worth anything. It’s worded in an intentionally obfuscated manner as to what actually has been  – is it the lack of typesetting or just the issues with subtitles not going up for new releases? Then it just outright lies about there being  with how subtitles are being handled, before ending on empty platitudes about  that mean nothing without concrete actions to back them up.And so far,  The lower quality subtitles in the back catalog are especially alarming, as the back catalog was exactly where Crunchyroll also started with its 2017 video quality reduction plans, all the while remaining careful with changes to simulcasts where people were paying closer attention – which is exactly what seems to be happening with subtitles on Crunchyroll right now. Without a clear public commitment to stick to higher subtitling standards that include typesetting, it is very likely that Crunchyroll executives will just delay their typesetting-killing plans and try again later. That’s why  need to cancel your subscription, encourage others to do so, and keep talking about this issue until Crunchyroll explicitly promises to do better.Together, we can save Crunchyroll from itself!This article would have never been as thorough and detailed as it is without the assistance of the following people:The multiple current and former Crunchyroll and Funimation workers who came forward to indepedently confirm the many previously unpublished details found in this article. BigOnAnime – for his great help with researching the historical technical details of Funimation’s subtitling standards. enonibobble – for his help with various screenshots and technical analysis of Crunchyroll subtitles. Faye Duxovni – for bringing Crunchyroll’s use of old Funimation workflows for Blu-rays to my attention and providing the screenshots of it that are used in the article.  who answered public questions I asked or otherwise helped with various small pieces of research. I’m not the only one to have made note of Crunchyroll’s recent subtitle shenanigans, so here’s some additional reading/watching on the subject elsewhere:Why did Crunchyroll’s subtitles just get worse? by  (former head of marketing for Crunchyroll), on the newsletter  This includes some additional details (like numbers!) that I didn’t go over here (because this article was long enough as-is), so I can recommend giving it a read.The Absolute State of Crunchyroll by YouTuber  This is a good watch just to see how bad the new Crunchyroll subtitles look like in action. Additionally, I didn’t really talk about how badly timing quality has been affected by the recent changes too, but this video has some good examples of that as well.Are Subtitles Getting Smaller? by  on . This  column is nominally about subtitles getting visually smaller, but most of it ends up being about the Crunchyroll subtitle situation. Jerome does keep incorrectly saying that general streaming services use the very bare-bones subtitle format SRT rather than TTML, though, and while these services do support SRT for ingestion (ie. content partners can deliver subtitles as SRT) and anime companies might even be making use of that, TTML is what the services actually use internally. SRT does not officially support any kind of positioning whatsoever, which means that even placing subtitles at the top of the screen would be impossible with it if the normal placement was on bottom.The Crunchyroll Sub Flub by  and , also on . Nothing particularly new in this one if you’re familiar with all the other coverage, but it’s nice to see this get discussed on the  column regardless. The more eyes on the subject, the better.I’m , a digital distribution expert and high quality media enthusiast. I have over a decade of experience with Japanese-to-English media localization, including anime subtitling, and I also care deeply about consumer rights. You can follow me on Bluesky, or drop me a mail.I’m working on getting Bluesky comments embedded at the end of the posts. For the time being though, you can read and join the discussion here!]]></content:encoded></item><item><title>Raspberry Pi Pico Bit-Bangs 100 Mbit/S Ethernet</title><link>https://www.elektormagazine.com/news/rp2350-bit-bangs-100-mbit-ethernet</link><author>chaosprint</author><category>hn</category><pubDate>Wed, 29 Oct 2025 23:21:51 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Three years ago, @kingyoPiyo’s Pico-10BASE-T project drew wide attention right here on Elektor for implementing 10 Mbit/s Ethernet on the Raspberry Pi Pico using just a few resistors. In 2023, another milestone followed with bit-banged USB, showing how far the RP2040’s (and now RP2350) programmable I/O could be pushed.What Can an RP2350 Bit-Bang Next?Now, developer Steve Markgraf (GitHub @steve-m) has extended the concept with Pico-100BASE-TX — a 100 Mbit/s Fast Ethernet transmitter running entirely in software.
Markgraf’s implementation uses the PIO and DMA to perform MLT-3 encoding, 4B5B line coding, and scrambling at a 125 MHz symbol rate. The result is a functioning 100 Mbit/s link capable of streaming about 11 Mbyte/s over UDP, demonstrated by real-time audio and ADC data streams.As before, this is a transmit-only proof of concept and must not be connected to PoE-enabled hardware. A pulse transformer or intermediary Ethernet switch is recommended for isolation.Check Out the Rest of His RepoExample applications in the repository include a counter, internal-ADC streamer, and an audio demo using a PCM1802 converter at 75 kHz. The library supports both the RP2040 and the newer RP2350 (Pico 2) and builds with the standard Pico SDK.Beyond the technical achievement, projects like this hint at new possibilities for low-cost, high-speed data acquisition and streaming using microcontrollers that were never designed for it. A Pico capable of pushing 11 MB/s over Ethernet could form the basis of compact, inexpensive test instruments, remote sensors, or experimental network interfaces — all without a dedicated PHY chip. As these bit-banged interfaces become faster and more capable, the question naturally follows: how far can software-defined hardware really go on a two-dollar microcontroller? Subscribe to the tag Raspberry Pi and you will receive an e-mail as soon as a new item about it is published on our website! ]]></content:encoded></item><item><title>Meta and TikTok are obstructing researchers&apos; access to data, EU commission rules</title><link>https://www.science.org/content/article/meta-and-tiktok-are-obstructing-researchers-access-data-european-commission-rules</link><author>anigbrowl</author><category>hn</category><pubDate>Wed, 29 Oct 2025 22:54:53 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Llamafile Returns</title><link>https://blog.mozilla.ai/llamafile-returns/</link><author>aittalam</author><category>hn</category><pubDate>Wed, 29 Oct 2025 22:21:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Mozilla.ai is adopting llamafile to advance open, local, privacy-first AI—and we’re inviting the community to help shape its future. is adopting the llamafile project to advance local, privacy-first AI. We are refreshing the codebase, modernizing foundations, and shaping the roadmap with community input.  was founded to build a future of trustworthy, transparent, and controllable AI. Over the past year, we have contributed to  by exploring not only the big cloud hosted large language models (LLMs) like GPT, Claude, Gemini, but also the smaller open-weight local models like gpt-oss, Gemma, and Qwen. The  project allows anyone to easily distribute and run LLMs locally using a single executable file. llamafile was started in 2023 on top of the , which allows it to be compiled once but run anywhere (macOS, Linux, Windows, etc). Each llamafile  contains both server code and model weights, making the deployment of an LLM as easy as downloading and executing a single file. It also leverages the popular  project for fast model inference. As the local and open LLM ecosystem has evolved over the years, time has come for llamafile to evolve too. It needs refactoring and upgrades to incorporate newer features available in llama.cpp and develop a refined understanding of the most valuable features for its users.Today, we're happy to announce that the llamafile codebase has officially joined  the . We are excited to be able to help support this pivotal technology and to help build the next generation of llamafile.We're building the next generation of llamafile in the open, and we want our roadmap decisions to be informed by your actual needs and use cases. We'd love to hear your thoughts on:Why did you choose llamafile in the first place?What features do you rely on most?Why are you still using it? (Or, perhaps more tellingly, why did you move to another tool?)What would make llamafile more useful for your work?Over the coming weeks and months, you'll see new activity in the llamafile repository as we incorporate your feedback into our roadmap. The code continues to be public, the issues are open, and we're eager to hear what you think. If you're currently using llamafile, nothing changes for you. Your existing workflows will continue working as expected. GitHub will handle the redirects, and all binaries linked in the repo will remain available.If llamafile has been part of your toolkit, we'd love to know what made it valuable. If you tried it once and moved on, we want to learn why. And if you've never used it but are curious about running AI models locally for the first time, now may be a good time to give it a try ;) llamafile has shown us what was possible as a community. Let’s keep building the next phase together!]]></content:encoded></item><item><title>Responses from LLMs are not facts</title><link>https://stopcitingai.com/</link><author>xd1936</author><category>hn</category><pubDate>Wed, 29 Oct 2025 21:40:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Responses from Large Language Models like ChatGPT, Claude, or Gemini are not facts.
        They’re predicting what words are most likely to come next in a sequence.
        
        They can produce convincing-sounding information, but that information may not be accurate or reliable.
        Don’t copy-paste something that a chatbot said and send it to someone as if that’s authoritative.When you do that, you’re basically saying “here are a bunch of words that often go together in a sentence.”Sometimes that can be helpful or insightful. But it’s not a , and it’s certainly not the final say in a matter.Send this to someone who’s just said to you,
          
          “But  Said...”
          I like LLMs.
          I like Machine Learning.
          
          I just don’t like watching smart people turn their brains off.
          
            ❤
          ]]></content:encoded></item><item><title>How the U.S. National Science Foundation enabled Software-Defined Networking</title><link>https://cacm.acm.org/federal-funding-of-academic-research/how-the-u-s-national-science-foundation-enabled-software-defined-networking/</link><author>zdw</author><category>hn</category><pubDate>Wed, 29 Oct 2025 21:22:50 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[SDN Grew First and Fastest in DatacentersThe first large-scale deployments of SDN took place in hyperscale data centers, beginning about 2010. The story is best told by the hyperscaler companies themselves, and so we asked leaders at Google, Microsoft Azure, and Meta to tell their stories about why and how they adopted SDN. As you will see, they all started from the ideas and principles that came from the NSF-funded research; and each tailored SDN to suit their specific needs and culture.The Internet Service Providers (ISPs) and telecommunication companies also had a strong interest in SDN. AT&T played a large role in its definition, engaging in research and early deployments in the mid 2000s. We invited Albert Greenberg, who was at AT&T at the time, to tell the story.Nicira was perhaps the startup that epitomized the SDN movement. It grew out of the NSF-funded  program and the Clean Slate Program at Stanford, based on the Ph.D. work of Martín Casado. Nicira developed ONIX, the first distributed control plane, used by Google in its infrastructure; OVS, the first OpenFlow-compliant software switch; and NVP (later NSX), the first network virtualization platform. We invited Teemu Koponen, a principal architect at Nicira, to tell the story.During the early 2010s, the networking industry began to realize that SDN has many big advantages. It lifts complex protocols up and out of the switches into the control plane, where it is written in a modern programming language. This made it possible to reason about the correctness of the protocols simply by examining the software controlling the network and the forwarding state maintained by the switches. For the first time, it became possible to formally verify the behavior of a complete network.Researchers, startups, network equipment vendors, and hyperscalers have all taken advantage of SDN principles to develop new ways to verify network behavior. We invited Professor George Varghese, who has been deeply involved in network verification research, to give us his perspective on network verification.A main benefit of SDN is that it hands over the keys (of control) from the networking equipment vendors—who kept their systems closed and proprietary, and hence tended to evolve slowly—to software programmers, who could define the behavior for themselves, often in open source software. And indeed it happened: Today, most large networks are controlled by software written by those who own and operate networks rather than by networking equipment vendors.But what about the hardware? Switches, routers, firewalls, and network interface cards are all built from special-purpose ASICs—highly integrated, cost-effective, and super-fast. The problem was the features and protocols that operated on packets (for example, forwarding, routing, firewalls, and security) were all baked into hardware at the time the chip was designed, two to three years before it was deployed. What if the network owner and operator needed to change and evolve the behavior in their network, for example to add a new way to measure traffic or a new way to verify behavior? A group of researchers and entrepreneurs set out to make the switches and NICs programmable by the user, to allow more rapid improvement and give the operator greater control. Not only did new programmable devices emerge, but a whole open source movement around the P4 programming language.We invited Professor Nate Foster, who leads the P4 language ecosystem, to tell the story of how programmable forwarding planes came about.So far, we have focused on SDN  networks running over electrical and optical cables in datacenters, enterprises, and long-haul WANs. SDN was originally defined with wireline networks in mind.Yet, for cellular networks, the most widely used networks in the world, the need was even greater: Cellular networks have been held back for decades by closed, proprietary, and complex “standards” designed to allow equipment vendors to maintain a strong grip on the market. SDN provides an opportunity to open up networks, introducing well-defined control APIs and interfaces, moving control software to common operating systems running on commodity servers.This story has only just begun, but it started thanks to NSF-funded research in the mid 2000s, then boosted by DARPA-funded programs to support open source software for cellular infrastructure. We invited Guru Parulkar and Oğuz Sunay to tell the story, both of whom developed open source cellular systems at the Open Networking Foundation and for the DARPA-funded Pronto project.]]></content:encoded></item></channel></rss>
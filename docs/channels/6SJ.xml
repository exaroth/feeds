<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Go</title><link>https://konrad.website/feeds/</link><description></description><item><title>Built a cli tool for generating .gitignore files</title><link>https://www.reddit.com/r/golang/comments/1iq1ivv/built_a_cli_tool_for_generating_gitignore_files/</link><author>/u/SoaringSignificant</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:38:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built this mostly as an excuse to play around with Charmbracelet‚Äôs libraries like Bubble Tea and make a nice TUI, but it also solves the annoying problem of constantly looking up .gitignore templates. It‚Äôs a simple CLI tool that lets you grab templates straight from GitHub, TopTal, or even your own custom repository, all from the terminal. You can search through templates using a TUI interface, combine multiple ones like mixing Go and CLion, and even save your own locally so you don‚Äôt have to redo them every time. If you‚Äôre always setting up new projects and find yourself dealing with .gitignore files over and over, this just makes life a bit easier, hopefully. If that sounds useful, check it out here and give it a try. And if you‚Äôve got ideas to make the TUI better or want to add something cool, feel free to open a PR. Always happy to get feedback or contributions!]]></content:encoded></item><item><title>ED25519 Digital Signatures In Go</title><link>https://www.reddit.com/r/golang/comments/1iq1i84/ed25519_digital_signatures_in_go/</link><author>/u/mejaz-01</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 13:37:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/mejaz-01 ]]></content:encoded></item><item><title>Go Nullable with Generics v2.0.0 - now supports omitzero</title><link>https://github.com/LukaGiorgadze/gonull</link><author>/u/Money-Relative-1184</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 11:00:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>what do you use golang for?</title><link>https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/</link><author>/u/Notalabel_4566</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 10:24:28 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is there any other major use than web development?]]></content:encoded></item><item><title>Webassembly and go 2025</title><link>https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/</link><author>/u/KosekiBoto</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 15 Feb 2025 05:00:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[so I found this video and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you   submitted by    /u/KosekiBoto ]]></content:encoded></item><item><title>GOGC &amp; GOMEMLIMIT ?</title><link>https://www.reddit.com/r/golang/comments/1ipnxxk/gogc_gomemlimit/</link><author>/u/mistyrouge</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 23:19:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[If the GC cost is fixed with regards to the amount of memory being freed up. Why would I not want to put  and  to say 70% of the memory I have available? Specially in an application that is known to be cpu bound.   submitted by    /u/mistyrouge ]]></content:encoded></item><item><title>GitHub - Clivern/Peanut: üê∫ Deploy Databases and Services Easily for Development and Testing Pipelines.</title><link>https://github.com/Clivern/Peanut</link><author>/u/Clivern</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 21:07:48 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Shutdown Go server</title><link>https://www.reddit.com/r/golang/comments/1ipj5zn/shutdown_go_server/</link><author>/u/Kennedy-Vanilla</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 19:46:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, recently I saw that many people shutdown their servers like this or similarserverCtx, serverStopCtx serverCtx, serverStopCtx := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { <-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { <-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal("graceful shutdown timed out.. forcing exit.") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf("error shutting down server: %v", err) } serverStopCtx() }() log.Printf("Server starting on port %s...\n", port) err = server.ListenAndServe() if err != nil && err != http.ErrServerClosed { log.Printf("error starting server: %v", err) os.Exit(1) } <-serverCtx.Done() log.Println("Server stopped") } := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { <-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { <-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal("graceful shutdown timed out.. forcing exit.") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf("error shutting down server: %v", err) } serverStopCtx() }() log.Printf("Server starting on port %s...\n", port) err = server.ListenAndServe() if err != nil && err != http.ErrServerClosed { log.Printf("error starting server: %v", err) os.Exit(1) } <-serverCtx.Done() log.Println("Server stopped") Is it necessary? Like it's so many code for the simple operation   submitted by    /u/Kennedy-Vanilla ]]></content:encoded></item><item><title>Help me to understand how one feature works in this project</title><link>https://www.reddit.com/r/golang/comments/1iphseo/help_me_to_understand_how_one_feature_works_in/</link><author>/u/n0zz</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 18:48:39 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[There is a `metrics_denylist` option. But I'm not sure how it works. Does it filter list of metrics to fetch before it does fetch them from Cloudflare API? Or does it work after all metrics are already fetched?]]></content:encoded></item><item><title>Walking with filesystems: using the fs.FS interface</title><link>https://bitfieldconsulting.com/posts/filesystems</link><author>/u/AlexandraLinnea</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 11:18:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitHub - gopher-fleece/gleece: Gleece - bringing joy and ease to API development in Go!</title><link>https://github.com/gopher-fleece/gleece</link><author>/u/h_talker</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 08:21:26 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I come from the Node.js / TypeScript ecosystem and have recently started working with Go as the technology for high-performance and CPU-intensive microservices.I was missing the TSOA approach and code styling for REST API implementation, meaning writing ordinary functions and declaring HTTP info, where the framework handles the rest - routing, validation, documentation, authentication, etc.So... I have created, with my colleague Yuval, the Gleece project that does exactly that.Since Go doesn't provide an annotation mechanism (as TSOA uses in JS), we used comments for HTTP info. To make it easier to work with, we also created a VS Code extension to highlight and provide visibility to the HTTP info.Feel free to use it, and I would love any feedback üôÇ]]></content:encoded></item><item><title>How to build a distributed request throttler on client side?</title><link>https://www.reddit.com/r/golang/comments/1ip5v2r/how_to_build_a_distributed_request_throttler_on/</link><author>/u/Polonium_Braces</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 08:10:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi everyone, I'm integrating my APIs with a new system (server to server api calls) - the catch being the downstream server can't handle more than 50 RPS, and would ultimately die/restart after this. I'm looking for a way to limit my requests from the client - I don't want to outright deny them from a server side rate limiter, but just limit them on client end to not breach this 50 RPS threshold. I could do this using channels, but issue is my API would be running in , so need of a distributed system. I'm not able to think of good approaches, any help would be appreciated here! I use GO mainly.]]></content:encoded></item><item><title>Type safe Go money library</title><link>https://www.reddit.com/r/golang/comments/1ip4nxm/type_safe_go_money_library/</link><author>/u/HawkSecure4957</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 06:43:57 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello, community I have been working on money library that is type safe, it's in beta and haven't been test against production. But I would like to share it with community.]]></content:encoded></item><item><title>Cloudflare Cli called flarectl is no longer supported it seems</title><link>https://www.reddit.com/r/golang/comments/1ip19zv/cloudflare_cli_called_flarectl_is_no_longer/</link><author>/u/gedw99</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 03:19:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Flarectl is really valuable for working with Cloudflare, but its dead these days.For those with slow meat sticks : isn't a part of the > v1 libraries, it only exists on the v0 branch which is now out of active development. you'll need to use the  branch if you want to keep building .we don't offer a CLI tool today that is auto generated (like the libraries). it is on the roadmap but no dates sorry." v4 is the latest and greatest PKG for Cloudflare.v2 was released in april 2024, then v3 in September, then v4 a month ago.I am reaching out to the community to see if anyone is maintaining a CLI that uses v4...Heaps of forks, but hard work to go through and find one :)]]></content:encoded></item><item><title>Installing Golang for non-sudo users on Ubuntu</title><link>https://www.reddit.com/r/golang/comments/1ioz4pq/installing_golang_for_nonsudo_users_on_ubuntu/</link><author>/u/Mycroft2046</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 14 Feb 2025 01:25:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am trying to install Golang for non-sudo user. The official instructions unzips the Go tar file in  and then add  to PATH. Can I unzip it in  instead and add /home/username/.local/go/bin to PATH instead? Common sense tells me that it should work, but I don't know if there is any specific Go feature that relies on Go being installed in .]]></content:encoded></item><item><title>How protobuf works: the art of data encoding</title><link>https://victoriametrics.com/blog/go-protobuf/</link><author>/u/valyala</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 21:44:22 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[How Protobuf Works‚ÄîThe Art of Data EncodingProtobuf (Protocol Buffers) is a way to serialize data into a compact binary format. This makes it smaller in size and faster to transmit over the network, though at the cost of being less human-readable.It works by defining data structures in a  file ‚Äî a blueprint specifying what fields exist, their types, the services, etc. From there, this file is used to generate code in different programming languages, allowing programs to efficiently encode and decode the data.‚ÄúDoes Protobuf take longer to serialize and deserialize data compared to JSON?‚ÄùThere are many compilers and implementations, so there‚Äôs no single definitive answer to this question. However, let‚Äôs run a simple benchmark comparing google.golang.org/protobuf and  in Go. While this isn‚Äôt a perfectly fair comparison, it does reflect common choices.We also benchmarked another Protobuf-compatible solution, easyproto, which is designed for high performance and low memory usage with some trade-offs. is just a simpler way to work with protobuf encoding without needing the  compiler or code generation. It follows the exact same wire format as protobuf (proto3).The benchmark below was run on a local machine using Go 1.23.5, libprotoc 29.2:No surprise here‚ÄîProtocol Buffers produces a much smaller output, 99 bytes compared to JSON‚Äôs 214 bytes for the same data structure. This difference only becomes more noticeable as the data grows.Looking at serialization: serialization takes just 74.21 nanoseconds, with zero allocations.Protobuf () takes 133.0 nanoseconds and makes one memory allocation of 112 bytes.JSON () takes 248.8 nanoseconds with one allocation of 224 bytes.Deserialization shows an even bigger gap: deserialization takes 70.61 nanoseconds, with 3 allocations using 112 bytes.Protobuf () takes 293.6 nanoseconds with 13 allocations using 448 bytes.JSON () takes 1457 nanoseconds with 18 allocations using 592 bytes.In today‚Äôs discussion, we‚Äôll break down how binary data is structured in Proto3 and examine the struct generated by .Field Key (or Tag) Encoding
#To encode a field, we need two things: where the field is in the message and what its value is. We‚Äôll use the code snippet below along the way to explain the encoding process:The first thing to figure out is how each field is identified. Every field in the message has two parts: a unique number and a type (the actual name of the field doesn‚Äôt matter in this case):The  assigned to each field is known as the .The  determines something called the , which tells the system how the value should be encoded (and how to decode it later).Protobuf has 5 wire types:: Used for encoding integers (, , , , , ), booleans (bool), and enumerations (): Used for fixed-length 64-bit data types such as , , and .: Used for strings, byte arrays, embedded messages, and packed repeated fields.: Were used for groups, but now deprecated.: Used for fixed-length 32-bit data types such as , , and .These two pieces‚Äîfield number and wire type‚Äîare combined into a single byte called the . Protobuf uses this tag to figure out which field is being referenced in the message. The wire type takes up the least significant 3 bits (the rightmost ones), while the field number fills the rest:With that in mind, the tag values for those fields in our example would be:How Protobuf assigns tags to message fieldsProtobuf doesn‚Äôt use the same encoding for every type of value. Instead, it groups them into three categories: varint, length-delimited, and fixed-width encoding.Varint Encoding for Wire Type 0
#Storing small numbers like 1, 2, or 3 in a full 4-byte () or 8-byte () format would be wasteful ‚Äî most of those bytes would just be zeros. Instead, Protobuf uses varint encoding, a technique where the number of bytes adjusts based on the size of the value. The smaller the number, the fewer bytes it takes.There‚Äôs a trade-off, though. The leftmost bit of each byte is reserved as a continuation bit, which signals whether the number keeps going:If the most significant bit (MSB) is 1, more bytes follow.If the MSB is 0, that‚Äôs the last byte, and the number is complete.Protobuf varint encoding process for the number 300For larger numbers, more bytes are needed‚Äîup to 10 for a full 64-bit integer. But in most cases, numbers tend to be small, so the space savings are significant. That makes varint a good choice when working with values that usually stay within a lower range.Here‚Äôs the problem: encoding a negative number, say -1. In a 32-bit integer, -1 is represented as:With varint encoding, that means 5 bytes are needed. With 64-bit integer, it‚Äôs 10 bytes:This happens because varint treats numbers as unsigned, meaning negative values end up consuming extra space. To avoid this issue, Protobuf takes a different route for signed integers (, ) ‚Äî it uses zig-zag varint encoding.Like the name suggests, it zigzags between positive and negative numbers:A positive number  is encoded as .A negative number  is encoded as .Once converted, the number is encoded using varint. This avoids the inefficiency of storing negative numbers as large multi-byte values.In Go, you can try this yourself using the  package from google.golang.org/protobuf, or even the standard  package. Go indeed supports both regular varint and zig-zag varint encoding. The  package provides two sets of functions:Unsigned integers:  and .Signed integers:  and .Here‚Äôs a quick test with :At this point, the  field equals to 300 can be fully represented in 3 bytes:Protobuf varint encoding for the ID fieldLength-Delimited Encoding for Wire Type 2
#Length-delimited encoding is used for data types that . This includes strings, byte arrays, embedded messages, and packed repeated fields.The idea is straightforward, it breaks the value into two parts: first, a prefix that tells you the length (in bytes) of what‚Äôs coming next, and then the actual data. In binary format, it looks like this: .For example, in our case, the  field contains , which is 9 bytes long when encoded in UTF-8. That means the encoded value would be:Protobuf length-delimited encoding for a string fieldWe‚Äôre not getting into how UTF-8 encoding works here, but just like varint, it has its own way of handling multi-byte characters and continuation bits. That‚Äôs the general idea.Fixed-Width Encoding for Wire Type 1, 5
#Fixed-width encoding is used for fields with a set size, like , , , , and floating-point numbers (, ). Nothing new here ‚Äî these values are stored in a simple way without extra length prefixes or varint tricks.That means the height field is encoded using IEEE 754 32-bit floating-point representation as usual:Protobuf fixed-width encoding for a float fieldNow, putting everything together, the final encoded message looks like this:One last thing, if a field has a default value (, , , etc.), it doesn‚Äôt get encoded at all. This works just like  in JSON.Protobuf is built with both backward compatibility (newer systems can still read older messages) and forward compatibility (older systems can still handle newer messages).To see how this works in practice, let‚Äôs look at how deserialization happens in Go.When decoding a message, Protobuf reads the data byte by byte. For each field, it starts by reading a tag, which contains two things. You probably already know from the encoding part:The , which tells the decoder how to interpret the next few bytes.The , which helps the decoder find the right field in the message definition.If the field number matches one in the current message definition, the decoder processes it normally based on the field‚Äôs type. However, what happens when the decoder comes across a field number that isn‚Äôt in the message definition?Instead of failing, Protobuf has a built-in way to handle this. It treats the field as  and skips over it using the wire type information to figure out how many bytes to ignore. These unknown fields aren‚Äôt just thrown away‚Äîthey‚Äôre actually stored in a separate section of the message.That‚Äôs why you‚Äôll see an option in the unmarshal function called , which lets you choose whether to keep these unknown fields or drop them entirely.‚ÄúWhy don‚Äôt we just discard unknown fields?‚ÄùIf the message gets passed to newer code that does understand these fields, the data is still there and can be interpreted correctly:Even though we unmarshaled the message into the newer version , the  field wasn‚Äôt lost. When re-encoded, it‚Äôs still there:Now, if we reverse the situation: where a field exists in the current message definition but wasn‚Äôt present in the received message, the field simply gets its default value. This lines up with how Protobuf encodes data: fields with default values aren‚Äôt stored in the first place.So what should we take away from this discussion?The order of fields doesn‚Äôt matter. What does matter is the  (and the ).To keep compatibility, don‚Äôt reuse or change tag numbers, and don‚Äôt change a field‚Äôs type.If you remove a field, reserve its  to prevent future reuse: .‚ÄúWhat if I change a field‚Äôs type but keep the same wire type?‚ÄùEven if two types use the same wire type, they don‚Äôt necessarily interpret the bytes the same way. A few examples make this clear: and  both use varint encoding, but a nonzero  value could be mistakenly read as true if interpreted as a boolean. and  also use varint encoding, but  treats the bytes as signed, while  treats them as unsigned. That means a negative  would show up as a huge positive number if read as .The Message in .pb.go File
#Let‚Äôs take a look at the Person struct and see how everything we‚Äôve talked about starts to feel familiar:Each field follows exactly what we expect based on its encoding: uses bytes encoding (length-delimited) with field number 1. uses varint encoding with field number 2. uses fixed32 encoding with field number 3.Then, we have , which‚Äîno surprise, stores any  fields that show up during deserialization. This is just a  that holds data for fields the current message definition doesn‚Äôt recognize.When you call  on a message for the first time, Protobuf calculates the size and saves it in . Later, if you call  again without modifying the message with  option, it just reuses the cached size instead of recalculating everything.This option tells Protobuf: ‚Äúwe‚Äôve already calculated the size of this message before, so just use that cached size instead of recalculating it‚Äù.
For it to work correctly, two things must be true: The size was already calculated before, and the message (and any submessages) hasn‚Äôt changed at all. If there‚Äôs any doubt, don‚Äôt use it ‚Äî Protobuf will handle size calculations correctly on its own.Caching the size of the message is good in 2 cases:For message fields nested inside other messages, Protocol Buffers needs to write a length prefix before writing the actual message content.Knowing the size upfront allows Protocol Buffers to allocate exactly the right amount of memory for the output buffer. Without knowing the size, it would need to continuously grow the buffer.‚ÄúHow about the  field?‚ÄùEvery time Protocol Buffers needs to do something with your message (like converting it to bytes), it uses the  field to find out how to handle that message. Every Protobuf message in Go needs to support  (introspection of its structure at runtime).The  field provides a allocation-free way to implement this by leveraging the  package‚Äîa technique that, while risky, improves performance by manipulating memory directly.When you write , the  field isn‚Äôt there; it is lazily set and initialized when you encode or decode. Therefore, the first time you encode or decode, there might be a slight performance overhead.And‚Ä¶ that‚Äôs also the final part of this discussion on the way to understand how Protobuf works.Hi, I‚Äôm Phuong Le, a software engineer at VictoriaMetrics. The writing style above focuses on clarity and simplicity, explaining concepts in a way that‚Äôs easy to understand, even if it‚Äôs not always perfectly aligned with academic precision.If you spot anything that‚Äôs outdated or if you have questions, don‚Äôt hesitate to reach out. You can drop me a DM on X(@func25).If you want to monitor your services, track metrics, and see how everything performs, you might want to check out VictoriaMetrics. It‚Äôs a fast, , and cost-saving way to keep an eye on your infrastructure.And we‚Äôre Gophers, enthusiasts who love researching, experimenting, and sharing knowledge about Go and its ecosystem.]]></content:encoded></item><item><title>Workspace and session manager built in go</title><link>https://github.com/GianlucaP106/mynav</link><author>/u/One_Mess_1093</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 17:39:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The 3 ways of integrating simple AI support in a Go program</title><link>https://sqirvy.xyz/posts/golang-ai/</link><author>/u/quad99</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 17:25:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[It seems to be popular, if not mandatory, to add AI to apps, whether they need it or not. It turns out that with Go its reasonably easy to make queries to AI from inside a Go program. Assume you want to connect to an AI LLM provider, send it a query and get the results back.When integrating Large Language Models (LLMs) into your Go applications, there are several approaches you can take. This post explores three different methods using the Sqirvy client library as examples.The full code described here is in github.com/dmh2000/sqirvy-ai, along with example usage in command line programs and a sample web app. The code includes support for Anthropic, Meta-llama, OpenAI and Gemini models.This Go library implements methods for simple queries to various AI providers.The directory structure for sqirvy-ai. (abbreviated)‚îú‚îÄ‚îÄ pkg
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ sqirvy             (what this post is about)
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ anthropic.go   (use the anthropic native library)
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ client.go      (the interface)
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ meta-llama.go  (use LangChain)
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ openai.go      (use HTTP API)
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ util
‚îú‚îÄ‚îÄ cmd 
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ sqirvy : take input from file or stdin and execute a query
‚îî‚îÄ‚îÄ web
    ‚îî‚îÄ‚îÄ sqirvy-web (a simple web app to compare LLMs)
The Sqirvy library provides an interface for simple queries to various LLM providers. It illustrates  using the different approaches to coding a simple text query.Three Integration Approaches1. Direct HTTP API Integration (OpenAI Example)The most straightforward approach is to directly use the provider‚Äôs HTTP API. This gives you complete control but requires more boilerplate code. Here‚Äôs how OpenAI integration works in Sqirvy:Full control over request/response handlingDirect mapping to API documentationNeed to handle HTTP details manuallyMust implement retry logic yourself2. Official SDK Integration (Anthropic Example)Using an official SDK provides a more polished experience with built-in types and error handling.Automatic retries and best practicesDependency on external packageLess flexibility for customizationMay lag behind API updatesUsing LangChain provides a unified interface across multiple providers. LangChain has official SDK‚Äôs for
Python and Javascript but not Go. There is an ‚Äòunofficial‚Äô, but popular and well tested Go SDK.Unified interface across providersRich ecosystem of tools and chainsEasy to switch between providersAdditional abstraction layerMay not expose provider-specific featuresLarger dependency footprintConsider these factors when choosing an approach:Want minimal dependenciesAre only using one providerNeed to match API docs exactlyWant type safety and official supportNeed built-in best practicesAre primarily using one providerValue ease of use over flexibilityNeed to support multiple providersWant access to higher-level abstractionsPlan to build complex chainsValue provider interchangeabilityEach approach has its merits, and the choice depends on your specific needs. Sqirvy demonstrates all three approaches, allowing you to see how they work in practice and choose the one that best fits your use case.Remember to always handle your API keys securely and respect rate limits regardless of which approach you choose.]]></content:encoded></item><item><title>X11 Wallpaper Application</title><link>https://www.reddit.com/r/golang/comments/1ionmv7/x11_wallpaper_application/</link><author>/u/MrChip53</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 16:56:09 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Thought I'd show off a simple X11 wallpaper application I made called wall collage. It will scan provided folders for images and change your X11 background on an interval.Start the daemon providing a folder: wall-collage daemon -f [folder]Then in a separate terminal you can use the cli to change settings for the daemon.It will randomly pick images and set your background to a single landscape image or 3 side by side portrait images.Constructive criticism welcome!]]></content:encoded></item><item><title>Building RAG systems in Go with Ent, Atlas, and pgvector</title><link>https://entgo.io/blog/2025/02/12/rag-with-ent-atlas-pgvector</link><author>/u/rotemtam</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 13:02:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[In this blog post, we will explore how to build a RAG
(Retrieval Augmented Generation) system using Ent, Atlas, and
pgvector.RAG is a technique that augments the power of generative models by incorporating a retrieval step. Instead of relying
solely on the model‚Äôs internal knowledge, we can retrieve relevant documents or data from an external source and use
that information to produce more accurate, context-aware responses. This approach is particularly useful when building
applications such as question-answering systems, chatbots, or any scenario where up-to-date or domain-specific knowledge
is needed.Setting Up our Ent schema‚ÄãLet's begin our tutorial by initializing the Go module which we will be using for our project:In this project we will use Ent, an entity framework for Go, to define our database schema. The database will store
the documents we want to retrieve (chunked to a fixed size) and the vectors representing each chunk. Initialize the Ent
project by running the following command:This command creates placeholders for our data models. Our project should look like this:Next, let's define the schema for the  model. Open the  file and define the schema as follows:This schema defines a  entity with three fields: , , and . The  field stores the path
of the document,  stores the chunk number, and  stores the chunked text data. We also define an edge to
the  entity, which will store the vector representation of the chunk.Before we proceed, let's install the  package.  is a PostgreSQL extension that provides support for
vector operations and similarity search. We will need it to store and retrieve the vector representations of our chunks.Next, let's define the schema for the  model. Open the  file and define the schema
as follows:This schema defines an  entity with a single field  of type . The 
field stores the vector representation of the chunk. We also define an edge to the  entity and an index on the
 field using the  index type and  operator class. This index will enable us to perform
efficient similarity searches on the embeddings.Finally, let's generate the Ent code by running the following commands:Ent will generate the necessary code for our models based on the schema definitions.Next, let's set up the PostgreSQL database. We will use Docker to run a PostgreSQL instance locally. As we need the
 extension, we will use the  Docker image, which comes with the extension
pre-installed.We will be using Atlas, a database schema-as-code tool that integrates with Ent, to manage our
database schema. Install Atlas by running the following command:As we are going to managing extensions, we need an Atlas Pro account. You can sign up for a free trial by running:Working without a migration toolIf you would like to skip using Atlas, you can apply the required schema directly to the database
using the statements in this fileNow, let's create our Atlas configuration which composes the  file with the Ent schema:This configuration defines a composite schema that includes the  file and the Ent schema. We also define an
environment named  that uses the composite schema which we will use for local development. The  field specifies
the Dev Database URL, which is used by Atlas to normalize schemas and make
various calculations.Next, let's apply the schema to the database by running the following command:Atlas will load the desired state of the database from our configuration, compare it to the current state of the database,
and create a migration plan to bring the database to the desired state:In addition to planning the change, Atlas will also provide diagnostics and suggestions for optimizing the schema. In this
case it suggests reordering the columns in the  table to reduce disk space. As we are not concerned with disk space
in this tutorial, we can proceed with the migration by selecting .Finally, we can verify that our schema was applied successfully, we can re-run the  command. Atlas
will output:Now that our database schema is set up, let's scaffold our CLI application. For this tutorial, we will be using
the  library to build a small app that can load, index
and query the documents in our database.First, install the  library:Next, create a new file named  and define the CLI application as follows:Create an additional file named  with the following content:Verify our scaffolded CLI application works by running:If everything is set up correctly, you should see the help output for the CLI application:Load the documents into the database‚ÄãNext, we need some markdown files to load into the database. Create a directory named  and add some markdown files
to it. For this example, I downloaded the  repository and used the  directory
as the source of markdown files.Now, let's implement the  command to load the markdown files into the database. Open the  file
and add the following code:This code defines the  method for the  command. The method reads the markdown files from the specified
path, breaks them into chunks of 1000 tokens each, and saves them to the database. We use the  method to
create a new Ent client using the database URL specified in the CLI options.Finally, let's run the  command to load the markdown files into the database:After the command completes, you should see the chunks loaded into the database. To verify run:You should see something similar to:Now that we have loaded the documents into the database, we need to create embeddings for each chunk. We will use the
OpenAI API to generate embeddings for the chunks. To do this, we need to install the  package:We will be reading this key from the environment variable , so let's set it:Next, let's implement the  command to create embeddings for the chunks. Open the  file and
add the following code:We have defined the  method for the  command. The method queries the database for chunks that do not have
embeddings, generates embeddings for each chunk using the OpenAI API, and saves the embeddings to the database.Finally, let's run the  command to create embeddings for the chunks:You should see logs similar to: Now that we have loaded the documents and created embeddings for the chunks, we can implement
the  command to ask questions about the indexed documents. Open the  file and add the following code:This is where all of the parts come together. After preparing our database with the documents and their embeddings, we
can now ask questions about them. Let's break down the  command:We begin by transforming the user's question into a vector using the OpenAI API. Using this vector we would like
to find the most similar embeddings in our database. We query the database for the embeddings, order them by similarity
using 's  operator, and limit the results to the top 5.Next, we prepare the information from the top 5 chunks to be used as context for the question. We then format the
question and the context into a single string.Then, we use the OpenAI API to generate a response to the question. We pass the question and context to the API
and receive a response. We then render the response using the  package to display it in the terminal.Before running the  command, let's install the  package:Finally, let's run the  command to ask a question about the indexed documents:And our RAG system responds:Amazing! We have successfully built a RAG system using Ent, Atlas, and pgvector.
We can now ask questions about the documents we loaded into the database and receive context-aware responses.Here are some more Q&As with our nifty RAG system:In this blog post, we explored how to build a RAG system using Ent, Atlas, and pgvector. Special thanks to
Eli Bendersky for the informative
blog post and for his great Go writing over the years!]]></content:encoded></item><item><title>The Ultimate Guide to Parallel Testing in Go: -p, -parallel, and t.Parallel() Demystified</title><link>https://medium.com/@emil.musician/the-ultimate-guide-to-parallel-testing-in-go-p-parallel-and-t-parallel-demystified-50bb6ca07046</link><author>/u/urlaklbek</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 12:34:27 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[> Original post: https://dev.to/emil_valeev/the-ultimate-guide-to-parallel-testing-in-go-p-parallel-and-tparallel-demystified-1c1oGo has three ways to tweak parallelism in tests: function callHow many of you know what they do? Why do we need all three? :slight_smile:It turns out  controls "how many test packages can run in parallel,"  determines "how many test functions inside a single test package can run in parallel," and finally,  marks a test function, telling Go that it can be run in parallel with other functions in the same package.Example: Let‚Äôs say we have 999 packages with 999 test functions each, but not a single test function uses . What will happen?Answer:  test packages will run in parallel (where  equals the number of CPU cores), but each package will run its test functions sequentially. So,  test functions will run in parallel.Another example: same scenario, but we add . What will happen?Answer: completely sequential execution.One more example: same as before, but this time we add . Well,  combined with  results in "only one package runs at a time, but  test functions run in parallel," where  is again the number of CPU cores.The most fine-grained control is achieved using all three:  and .We need to be careful with stateful tests (as usual, state is the root of all evil, but it‚Äôs impossible to avoid).Go developer? Check out Nevalang ‚Äî it‚Äôs a work-in-progress programming language built on top of Go, that fixes some of Go‚Äôs issues such as data-races, nil pointer-dereference, etc. In 2025 it will have visual editor, Rust-like enums and much more. Eventually you will be able to call it from Go and vice-versa.Interested? Give us a star and join our community. We need developers (not just gophers!) to contribute and test the language. Let‚Äôs change programming together ‚Äî Go is awesome but we can do even better:]]></content:encoded></item><item><title>Executing one Ginkgo/Gomega Test via vscode</title><link>https://www.reddit.com/r/golang/comments/1iofnof/executing_one_ginkgogomega_test_via_vscode/</link><author>/u/guettli</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 09:51:11 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Plain Go unittest an be easily called via vscode. For example via "Run Test at Cursor".Is there a way to get "Run Test at Cursor" for Ginkgo/Gomega working?   submitted by    /u/guettli ]]></content:encoded></item><item><title>Make sure you type the names of your dependencies correctly!</title><link>https://www.reddit.com/r/golang/comments/1iobvuk/make_sure_you_type_the_names_of_your_dependencies/</link><author>/u/jezemine</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 13 Feb 2025 05:18:16 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/jezemine ]]></content:encoded></item><item><title>Extensible Wasm Applications with Go</title><link>https://go.dev/blog/wasmexport</link><author>Cherry Mui</author><category>dev</category><category>official</category><category>go</category><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Dev - Golang Blog</source><content:encoded><![CDATA[
      Cherry Mui
      13 February 2025
      Go 1.24 enhances its WebAssembly (Wasm) capabilities with the
addition of the  directive and the ability to build a reactor
for WebAssembly System Interface (WASI).
These features enable Go developers to export Go functions to Wasm,
facilitating better integration with Wasm hosts and expanding the possibilities
for Go-based Wasm applications.WebAssembly and the WebAssembly System InterfaceWebAssembly (Wasm) is a binary instruction format
that was initially created for web browsers, providing the execution of
high-performance, low-level code at speeds approaching native performance.
Since then, Wasm‚Äôs utility has expanded, and it is now used in various
environments beyond the browser.
Notably, cloud providers offer services that directly execute Wasm
executables, taking advantage of the
WebAssembly System Interface (WASI) system call API.
WASI allows these executables to interact with system resources.Go first added support for compiling to Wasm in the 1.11 release, through the
 port.
Go 1.21 added a new port targeting the WASI preview 1 syscall API through the
new  port.Exporting Go Functions to Wasm with Go 1.24 introduces a new compiler directive, , which allows
developers to export Go functions to be called from outside of the
Wasm module, typically from a host application that runs the Wasm runtime.
This directive instructs the compiler to make the annotated function available
as a Wasm export
in the resulting Wasm binary.To use the  directive, simply add it to a function definition://go:wasmexport add
func add(a, b int32) int32 { return a + b }
With this, the Wasm module will have an exported function named  that
can be called from the host.This is analogous to the cgo  directive,
which makes the function available to be called from C,
though  uses a different, simpler mechanism.A WASI reactor is a WebAssembly module that operates continuously, and
can be called upon multiple times to react on events or requests.
Unlike a ‚Äúcommand‚Äù module, which terminates after its main function finishes,
a reactor instance remains live after initialization, and its exports remain
accessible.With Go 1.24, one can build a WASI reactor with the  build
flag.$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm
The build flag signals to the linker not to generate the  function
(the entry point for a command module), and instead generate an
 function, which performs runtime and package initialization,
along with any exported functions and their dependencies.
The  function must be called before any other exported functions.
The  function will not be automatically invoked.To use a WASI reactor, the host application first initializes it by calling
, then simply invoke the exported functions.
Here is an example using Wazero, a Go-based Wasm runtime
implementation:// Create a Wasm runtime, set up WASI.
r := wazero.NewRuntime(ctx)
defer r.Close(ctx)
wasi_snapshot_preview1.MustInstantiate(ctx, r)

// Configure the module to initialize the reactor.
config := wazero.NewModuleConfig().WithStartFunctions("_initialize")

// Instantiate the module.
wasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)

// Call the exported function.
fn := wasmModule.ExportedFunction("add")
var a, b int32 = 1, 2
res, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))
c := api.DecodeI32(res[0])
fmt.Printf("add(%d, %d) = %d\n", a, b, c)

// The instance is still alive. We can call the function again.
res, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))
fmt.Printf("add(%d, %d) = %d\n", b, c, api.DecodeI32(res[0]))
The  directive and the reactor build mode allow applications to
be extended by calling into Go-based Wasm code.
This is particularly valuable for applications that have adopted Wasm as a
plugin or extension mechanism with well-defined interfaces.
By exporting Go functions, applications can leverage the Go Wasm modules to
provide functionality without needing to recompile the entire application.
Furthermore, building as a reactor ensures that the exported functions can be
called multiple times without requiring reinitialization, making it suitable
for long-running applications or services.Supporting rich types between the host and the clientGo 1.24 also relaxes the constraints on types that can be used as input and
result parameters with  functions.
For example, one can pass a bool, a string, a pointer to an , or a
pointer to a struct which embeds  and contains supported
field types
(see the documentation for detail).
This allows Go Wasm applications to be written in a more natural and ergonomic
way, and removes some unnecessary type conversions.While Go 1.24 has made significant enhancements to its Wasm capabilities,
there are still some notable limitations.Wasm is a single-threaded architecture with no parallelism.
A  function can spawn new goroutines.
But if a function creates a background goroutine, it will not continue
executing when the  function returns, until calling back into
the Go-based Wasm module.While some type restrictions have been relaxed in Go 1.24, there are still
limitations on the types that can be used with  and
 functions.
Due to the unfortunate mismatch between the 64-bit architecture of the client
and the 32-bit architecture of the host, it is not possible to pass pointers in
memory.
For example, a  function cannot take a pointer to a struct that
contains a pointer-typed field.The addition of the ability to build a WASI reactor and export Go functions to
Wasm in Go 1.24 represent a significant step forward for Go‚Äôs WebAssembly
capabilities.
These features empower developers to create more versatile and powerful Go-based
Wasm applications, opening up new possibilities for Go in the Wasm ecosystem.]]></content:encoded></item><item><title>What are some good validation packages for validating api requests in golang?</title><link>https://www.reddit.com/r/golang/comments/1io2lt6/what_are_some_good_validation_packages_for/</link><author>/u/gwwsc</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 21:41:36 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is there any package validator like Zod (in JS/TS ecosystem) in golang? It would be better if it has friendly error messages and also want to control the error messages that are thrown.   submitted by    /u/gwwsc ]]></content:encoded></item><item><title>Any good repo to look for a clean and well structure golang code? if possible, an API</title><link>https://www.reddit.com/r/golang/comments/1io25wu/any_good_repo_to_look_for_a_clean_and_well/</link><author>/u/Weekly_Potato8103</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 21:23:09 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm using go-gin and gorm to implement an API. This is my first experience with go (coming from C#), and so far it has been OK.I've been coding thanks to some recommendations using an AI code assistant, but I want to check if there is any open source repo where I can learn about how to structure the codes, rules for validation of errors, etc.]]></content:encoded></item><item><title>Tools to generate PDF files?</title><link>https://www.reddit.com/r/golang/comments/1io1l9m/tools_to_generate_pdf_files/</link><author>/u/Suspicious-Olive7903</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 20:59:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey! Can someone recommend any CLI tools or other cool ways to generate PDF-s for invoices and tickets (QR codes)? Generation will be done everytime customer purchases product from website and after that a job will start to generate invoice with tickets that will be sent to email. Some options I came up with:Write small Go web server that renders contents as HTML and use chromedp to instruct browser to generate PDF out of it. Quite slow and resource heavyLaTeX...? It was nice to use it for university docs, but I am not sure it is good use in this case   submitted by    /u/Suspicious-Olive7903 ]]></content:encoded></item><item><title>We Replaced Our React Frontend with Go and WebAssembly</title><link>https://dagger.io/blog/replaced-react-with-go</link><author>/u/m-unknown-2025</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 19:36:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[A few weeks ago, we launched Dagger Cloud v3, a completely new user interface for Dagger Cloud. One of the main differences between v3 and its v2 predecessor is that the new UI is written in WebAssembly (WASM) using Go. At first glance, this might seem an odd choice - Go typically isn't the first language you think of when deciding to program a Web UI - but we had good reasons. In this blog post, I'll explain why we chose WebAssembly, some of our implementation challenges (and how we worked around them), and the results.Two Codebases = More Work, Fewer FeaturesDagger works by building up a DAG of operations and evaluating them, often in parallel. By nature, this is a difficult thing to display. To help users make sense of it, we offer two real-time visualization interfaces: the Dagger terminal UI (TUI), included in the Dagger CLI, and Dagger Cloud, an online Web dashboard. The Dagger TUI is implemented in Go, and Dagger Cloud (pre-v3) was written in React.Obviously, we want both user interfaces to be as close to each other as possible. But the actual act of interpreting Dagger's event stream in real-time and producing a UI is pretty involved. Some of the more complex event streams we've seen have hundreds of thousands of OpenTelemetry spans, and managing the data structures around them gets very complicated, very quickly. The Web UI often couldn't keep up with the huge volume of data it had to process and it would become laggy and slow; to fix this performance bottleneck, we were forced into a different implementation model for the React application.So, we ended up with two interfaces trying to accomplish the same thing, one of them in one language and ecosystem (TypeScript/React), the other in a totally different language and ecosystem (Go), and we couldn't easily share business logic between them. As a small team, we need to ship fast. Having to re-implement every feature twice was just a massive tax on our velocity.We started thinking about a new approach to Dagger Cloud, with two main goals:Unify the codebases, to eliminate duplication and make it more efficient to ship new featuresDeliver on the promise of a crisp, snappy Web UI, matching the speed and performance of the terminal UIChoosing Go + WebAssemblyOur starting goal was to be able to reuse one codebase for both Dagger Cloud and the TUI. We decided fairly early to make it a Go codebase. Technically, we could have gone the other way and used TypeScript for the TUI. But we're primarily a team of Go engineers, so selecting Go made it easier for others in the team to contribute, to add a feature or drop in for a few hours to help debug an issue. In addition to standardizing on a single language, it gave us flexibility and broke down silos in our team.Once we decided to run Go code directly in the browser, WebAssembly was the logical next step. But there were still a couple of challenges:The Go + WebAssembly combination is still not as mature as React and other JavaScript frameworks. There are no ready-made component libraries to pull from, the developer tooling isn't as rich, and so on. We knew that we would need to build most of our UI components from scratch.There is a hard 2 GB memory limit for WebAssembly applications in most browsers. We expected this to be a problem when viewing large traces, and we knew we would have to do a lot of optimization to minimize memory usage and keep the UI stable. This wasn't entirely bad though; the silver lining here was that any memory usage improvements made to the WebAssembly UI would also benefit TUI users, since it was now a shared codebase.Once we'd made the decision, the next question was, "how do we build this?" We decided to build the new WebAssembly-based UI in the Go-app framework. Go-app is a high-level framework specifically for Progressive Web Apps (PWAs) in WebAssembly. It offers key Go benefits, like fast compilation and native static typing, and it also follows a component-based UI model, like React, which made the transition easier.Since the Go + WebAssembly combination isn't mainstream, there was some healthy skepticism within the Dagger team about its feasibility. For example, there was no real ecosystem for Go-app UI components and we knew we‚Äôd have to write our own, but we weren‚Äôt sure how easy or difficult this would be. We also had concerns over integrations with other services (Tailwind, Auth0, Intercom, PostHog), and about rendering many hundreds of live-updating components at the same time.¬†To answer these questions and de-risk the project, I spent almost a month prototyping, with the goal of re-implementing as much of the existing UI as possible in Go-app. As it turned out, there weren't many blockers: WebAssembly is already a well-documented open standard and most other questions were answered in Go-app‚Äôs own documentation. The biggest challenge, as expected, was the memory usage limit, which required careful design and optimization.From Prototype to ProductionOnce we had a working proof of concept, the team's comfort level increased significantly and we kicked off project "awesome wasm" to deliver a production implementation. Here are a few notes from the journey:Memory usage was easily the most existential threat to the project‚Äôs success. I spent a lot of time figuring out how to render 200k+ lines of log output without crashing. This led to optimizations deep in our virtual terminal rendering library, which dramatically reduced TUI memory usage at the same time (as mentioned already, sharing codebases means that important optimizations in one interface become "free" in the other!)Go WASM is slow at parsing large amounts of JSON, which led to dramatic architecture changes and the creation of a ‚Äúsmart backend‚Äù for incremental data loading over WebSockets, using Go's rarely-used encoding/gob format.Initially, the WASM file was around 32 MB. By applying Brotli compression, we were able to bring it down to around 4.6 MB. We tried to perform Brotli compression on-the-fly in our CDN but the file was too large, so eventually we just included the compression step into our build process.Apart from the memory challenges, most of our other initial worries turned out unfounded. The UI components weren‚Äôt very hard to write, integrations with other services were straightforward, and I found good techniques for handling component updates in real-time.There were a number of useful NPM packages I found, so I wondered if I could use them with Go. WebAssembly has a straightforward interface to both Go and JavaScript, so I built a Dagger module that uses Browserify to load an NPM package. This module allows us to generate a JavaScript file that can be included in a Go application. This means that we can work primarily in Go and then, if needed, we have a way to load helpers that are implemented in native JavaScript.Disclaimer: I'm not a React professional so with that in mind...it seemed to me that React had a very rigid way of implementing components, while Go-app was much more flexible. In Go-app, you can have any component update whenever you like, which gives you many more degrees of freedom for optimization. For example, I needed to optimize a component rendering 150,000+ lines of output. Just having the ability to try different approaches and then pick the one that worked best, made the entire exercise much easier!Even though Go-app doesn't have React-like developer tools built into the browser, I was able to use Go's own tools (pprof) plus the default profiler built into the browser for profiling and debugging. This was very useful to inspect functions calls, track CPU and memory usage, and evaluate the effectiveness of different approaches for optimizing memory usage.I discovered a side benefit of using Go-app: since Dagger Cloud is built as a PWA, it can be installed as a desktop or a mobile application. This makes it possible to launch Dagger Cloud like a native application and get a full-screen experience without needing to open a browser first, or just have a dedicated icon in your desktop taskbar/dock.We soft-launched Dagger Cloud v3 to our Dagger Commanders a few weeks ago to collect feedback and made it available to everyone shortly thereafter.Our switch from React to WASM has resulted in a more consistent user experience across all Dagger interfaces, and better overall performance and lower memory usage, especially when rendering large and complex traces.From an engineering perspective too, the benefits to our team are significant. Optimizations very often involve just as much, if not more, work than actually implementing features. So it's great to not have to spend time optimizing the Web UI, and then more time optimizing the TUI, and instead actually focus on delivering new features.Dagger Cloud v3 has the Dagger community buzzing and one of the more common questions we've been fielding recently is: who should consider doing this and who shouldn't?We want to be clear that we're not generally recommending making front-ends in Go. We had some very good reasons to do it: a team of strong Go engineers; a complex UI that TypeScript/React didn't scale well for; a requirement for standardization and reuse between two codebases; and a company-wide mandate to increase our velocity. That's a fairly specific set of circumstances. If you're in similar circumstances, this is certainly an option worth evaluating; if not, there are other tools and standards that you should consider first.Dagger Cloud v3 is still in beta and we're excited for you to try it out. If you'd like to know more about our implementation or simply have feedback to share on the new UI, join our Discord and let us know what you think!]]></content:encoded></item><item><title>Anyone else seeing old unit tests that involve concurrent code fail with v1.24?</title><link>https://www.reddit.com/r/golang/comments/1inw7nc/anyone_else_seeing_old_unit_tests_that_involve/</link><author>/u/skesisfunk</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 17:21:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I have a unit test I wrote over a year ago that tests some code like this (some naming changed to anonymize the code because this is for work):t := time.NewTicker(refreshInterval) defer t.Stop() for { select { case <-t.C: switch s := foo.State(); s.Status { // cases based on the value of s.Status (not important for this example) } case <-ctx.Done(): return fmt.Errorf("context cancelled: %w", ctx.Err()) } In one of the unit tests we are testing the case where the context is cancelled so the in the unit test I pass a context that was created with  and the  function is called before the function with the code above is called. Now for the unit test  is a stub that returns items from pre-defined slice when  is called. For this test since the context is already cancelled I didn't populate anything in the stub implementation of  for returns from the  method. Now I have to put two stub returns to make the test pass consistently.This unit test was passing every time both in CI and locally. When developing these tests I even made sure to run multiple times using  in between just to root out any race conditions in the test. Just now I bumped the golang version because I wanted to clean up some code by leveraging the new  feature. Nothing I did touched these tests or the system under test but this unit test now fails about half the time with a panic because of an out of range index on a slice in the stub implementation of  . And, again, this unit test is over a year old, the test suite runs in CI and I run it locally every time I develop on this project. I have not seen this issue before today. Given that v1.24 made some changes to how concurrency works in tests it seems possible that this new failure is due to v1.24. I know those changes are experimental but presumably they changed some implementation details and maybe they accidentally made a breaking change that only affects some unit test schemes. Anyway I just want to ask the community here if anyone else has experienced this. ]]></content:encoded></item><item><title>Parallel Streaming Pattern in Go: How to Scan Large S3 or GCS Buckets Significantly Faster</title><link>https://destel.dev/blog/fast-listing-of-files-from-s3-gcs-and-other-object-storages</link><author>/u/destel116</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 16:25:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Somewhere in the second half of 2024, I needed to check a large Google Cloud Storage bucket to find files that weren‚Äôt referenced in the database, and delete them. A simple task ‚Äî traverse all files in the bucket and remove the ones I didn‚Äôt need. The bucket was large, and it was immediately clear that the deletion operation would become the main bottleneck. No problem ‚Äî it could be parallelized and made, roughly speaking, arbitrarily fast by using enough goroutines. However, when I implemented it and ran the script, I was surprised to discover that the file listing operation had itself become a new bottleneck‚Ä¶Most SDKs for object storages provide nice iterator-like APIs for traversing files. Under the hood files are fetched page by page. The typical page size is 1000 meaning that SDK needs to perform 10k requests to traverse a 10 million file bucket. This may not sound like a big deal, but for large buckets, this can easily become a bottleneck.Here‚Äôs what my code looked like when I first encountered this problem. It‚Äôs for the Google Cloud Storage, but once all concepts are covered, I‚Äôll demonstrate the Amazon S3 version as well.The code, I believe, is self-explanatory. It uses the rill concurrency toolkit that I built. It works well for cases like this and removes most of the boilerplate. The only somewhat complicated part that may need explanation is the  function. This function is a wrapper around  function . It takes the same arguments but returns a stream (Go channel) instead of a bucket iterator.First, we get a stream of all files, then filter out the ‚Äúgood‚Äù ones, and finally delete everything that remains from the bucket. Here, both filtering and deletion operations are concurrent and use different numbers of goroutines. But the listing is sequential and just streams all files in alphabetical (lexicographical) order.Can We Make the Listing Operation Concurrent?Yes, we can, but we need to know the bucket structure. Let‚Äôs look at the example ‚Äî a bucket that stores users‚Äô files in the users/{username}/{filename} format. Let‚Äôs also assume for simplicity that usernames consist only of lowercase Latin characters.It may look like a directory tree with some folders, but it‚Äôs not the case. For the sake of our goal, it‚Äôs much better to think of it as an alphabetical list of filenames, where these names can also contain slashes.We can pick one filename, for example, , and make two queries: all files that go before and after it in the list. This filename becomes a split point that partitions bucket into two parts.We don‚Äôt even need to know an exact filename to create such a split point. For instance, we can take  as a split point. This way the first part will have all users whose names start with letters , and the second part will have the  users.Let‚Äôs look at the example where we, in a similar fashion, partition the bucket into 3 parts, depending on the first letter of the username. Here we use  function to stream users from each part independently and concurrently:This can achieve up to 3x gain in listing speed, assuming that files are distributed more or less evenly between the partitions.Dynamic Split Points and FlatMapThe previous example used  to combine several parallel streams, but there‚Äôs a more convenient tool for the cases where the number of split points is large or dynamic ‚Äî .First, let‚Äôs define a  type that represents a range of filenames between the two consecutive split points:Now the previous example can be rewritten. We first generate a stream of ranges and then convert it into a unified stream of files using :Seems like not much benefit, but now we can:Generate ranges programmaticallyControl the level of concurrency. For instance, we can have 1000 ranges, but stream files from at most 5 of them at the same time.Let‚Äôs use  to programmatically create a separate range for each lowercase letter of the alphabet, and then stream files from up to 10 ranges at the same time:Above we used 26 split points which resulted in 27 ranges. To achieve a significant speedup we would need a large number of goroutines (50, 100, or even more), which in turn requires an even larger number of ranges.Let‚Äôs put together a complete example. Here we‚Äôre partitioning based on the first two characters of the username. We‚Äôre also using a less restricted alphabet, allowing uppercase and lowercase characters plus digits.This produces 62 * 62 + 1 = 3845 ranges. Concurrency levels are: 50 for listing, 10 for database, and 100 for file deletions. I am listing a full code below to demonstrate how concise it is:  The new version of Go brings the support for generic type aliases. Rill v0.7 ships with  type alias making it possible to simplify the return type of the  function, like this:My Bucket Has Different StructureNo one knows your bucket structure better than you do, and there‚Äôs no single solution that fits every case. The goal is to find a set of split points that:Is large enough to meet your target concurrency levelPartition the bucket into more or less equal-sized parts. This should not be perfect though.Let‚Äôs look at how we can partition a bucket where files follow the  pattern and team IDs are integers. It‚Äôs not immediately clear how to do it, because in the integer world , but when converted to strings .Let‚Äôs assume we have one million teams and we want about 100 partitions. The trick here is to use all 2-digit numbers as split points (excluding the number 10). Below is the list of resulting ranges and team IDs that fall into each range:And range generation code is quite similar to what we‚Äôve seen beforeThis produces 89+1 ranges with an almost even distribution of teams among them. If you need more ranges you can use 3-digit numbers (101 - 999) instead of 2-digit ones. And if you need less you can increment  by 2 on each iteration. Simply put, you can be as creative as you wish, just make sure that the split points you generate are ordered lexicographically. It can even make sense to create some sort of constructor for the  struct that panics if . This could help to catch ordering errors during development.How to Do the Same Thing for the Amazon S3?Let‚Äôs rewrite the last large Google Cloud Storage example for the Amazon S3. Most parts are very similar:Range generation code is the sameDatabase interaction code is the sameFile deletion code is almost the sameFlatMap part is very similarThe main difference is in the streaming wrapper around the  API ‚Äî the  function. This Amazon API has a  argument, but does not have an  as we‚Äôve seen in Google API. It‚Äôs not a problem, but rather a small inconvenience: we‚Äôll just break the iteration manually as soon as we reach a filename that‚Äôs out of range. The full code is below:Let‚Äôs look at how parallel traversal affects the API costs. Most object storage services charge per LIST request, so it‚Äôs natural to wonder if parallelization significantly increases the costs.The key to understanding this lies in how pagination works. When we‚Äôre scanning a bucket sequentially, we get a series of full pages (1000 items each) and usually one partial page at the end. What happens when we split the bucket into ranges? Let‚Äôs do the math.Suppose we partition our bucket into 1000 ranges to enable very high levels of concurrency. The worst case scenario is that each range will have one partial page ‚Äî that‚Äôs 1000 additional LIST requests compared to a sequential scan. This sounds like a lot, but in reality, it adds only about $0.005 to the total cost.So effectively, this method has the same cost efficiency as a regular sequential scan, while being significantly faster.The same technique works for any object storage that supports range-based queries or at least just StartAfter queries as demonstrated for S3. With a well-chosen set of split points, both listing and deletion operations can scale almost linearly.When I applied this approach to production GCS buckets, operations that previously took hours completed in minutes. The main bottleneck typically shifts from the storage service to local hardware resources, as cloud object storages scale remarkably well with concurrent requests.The resulting code is concise, despite handling complex operations like parallel listing, filtering, and deletion. This is where rill‚Äôs composable concurrency model shines ‚Äî it lets you focus on the core logic while abstracting away all the complexity of channel, goroutine, and error management.Finally, the API costs of this method are practically identical to a regular sequential scan.]]></content:encoded></item><item><title>Practical OpenAPI in Go</title><link>https://packagemain.tech/p/practical-openapi-in-golang</link><author>/u/der_gopher</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 15:02:22 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[OpenAPIIn this practical guide I want to walk you through all important parts involved in architecting, implementing and consuming the API with help of OpenAPI standard.Before we dive in, it's helpful to have a basic understanding of the following:MultiplayerThe main idea of OpenAPI is to be able to describe in agnostic terms, decoupling them from any specific programming language. Consumers of your API specification do not need to understand the guts of your application or try to learn Lisp or Haskell if that‚Äôs what you chose to write it in. They can understand exactly what they need from your API specification, written in a simple and expressive language.v3.1.1It all starts with defining what the API should provide for its consumers and what it is for. While this stage isn't always purely technical, having a sketch of your API design in OAS when gathering requirements gives you a headstart when starting design.OpenAPI editorAnd it's important to understand that it's not only about writing JSON/YAML spec, but actually agreeing on the API design.oneGoogle Maps OASswagger-cli bundle -o _bundle/openapi.yaml openapi.yamlhereOpenAPI Object# schema version
openapi: 3.1.1

# docs
info:
  title: Smart Home API
  description: API Specification for Smart Home API
  version: 0.0.1

# optional servers for public APIs
servers:
  - url: "https://..."

# tags are used to group the endpoints
tags:
  - name: device
    description: Manage devices
  - name: room
    description: Manage rooms

# endpoints go here
paths:
  # ...

# reusable objects such as schemas, error types, request bodies
components:
  # ...

# security mechanisms, should correspond to components.securitySchemes
security:
  - apiKeyAuth: []DELETE /devices/{deviceId}paths:

  # the path has a parameter in it
  /devices/{deviceId}:
    get:
      tags:
        - device
      summary: Get Device
      operationId: getDevice

      parameters:
        - name: deviceId
          in: path
          required: true
          schema:
            $ref: "#/components/schemas/ULID"

      responses:

        "200":
          description: Success
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Device"

        "404":
          description: Not Found
          content:
            application/json:
              schema:
                # use common type for 404 errors
                $ref: "#/components/schemas/ErrorNotFound"responses:
  "200":
    content:
      application/json:
        examples:
          new_device:
            value: # any value
components:
  schemas:
    Device:
      type: object
      properties:
        id:
          $ref: '#/components/schemas/ULID'
        name:
          type: string
      required:
        - id
        - namecomponents:
  schemas:
    WithId:
      type: object
      required:
        - id
      properties:
        id:
          $ref: "#/components/schemas/ULID"

    WithName:
      type: object
      required:
        - name
      properties:
        name:
          type: string

    Device:
      allOf:
        - $ref: "#/components/schemas/WithId"
        - $ref: "#/components/schemas/WithName"ULIDULID:
  type: string
  minLength: 26
  maxLength: 26

  # example is useful for Swagger docs
  example: 01ARZ3NDEKTSV4RRFFQ69G5FAV

  x-go-type: ulid.ULID
  x-go-type-import:
    path: github.com/oklog/ulid/v2hereopeanapi.toolsoapi-codegenIssuego install github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen@latestecho# oapi-codegen.yaml

package: api
output: pkg/api/api.gen.go

generate:
  strict-server: true
  models: true
  echo-server: trueWe can now generate the server code using the following command:oapi-codegen --config=oapi-codegen.yaml openapi.yamltype StrictServerInterface interface {

  // List Devices
  // (GET /devices)
  ListDevices(ctx context.Context, request ListDevicesRequestObject) (ListDevicesResponseObject, error)

  // Get Device
  // (GET /devices/{deviceId})
  GetDevice(ctx context.Context, request GetDeviceRequestObject) (GetDeviceResponseObject, error)

}All our types are also generated:type ULID = ulid.ULID

type Device struct {
	Id   ULID   `json:"id"`
	Name string `json:"name"`
}

// ...Code to parse the requests automaticallypackage api

import "context"

type Server struct{}

func NewServer() Server {
	return Server{}
}

func (Server) ListDevices(ctx context.Context, request ListDevicesRequestObject) (ListDevicesResponseObject, error) {
	// actual implementation
	return ListDevices200JSONResponse{}, nil
}

func (Server) GetDevice(ctx context.Context, request GetDeviceRequestObject) (GetDeviceResponseObject, error) {
	// actual implementation
	return GetDevice200JSONResponse{}, nil
}That leaves us to start the echo server itself. Note that we don't need to write any endpoints manually now, and all request and response parsing is handled for us. Still, we need to validate the requests inside our implementation.package main

import (
	"oapiexample/pkg/api"

	"github.com/labstack/echo/v4"
)

func main() {
	server := api.NewServer()

	e := echo.New()

	api.RegisterHandlers(e, api.NewStrictHandler(
		server,
		// add middlewares here if needed
		[]api.StrictMiddlewareFunc{},
	))

	e.Start("127.0.0.1:8080")
}here//go:embed pkg/api/index.html
//go:embed openapi.yaml
var swaggerUI embed.FS

func main() {
	// ...

	// serve swagger docs
	e.GET("/swagger/*", echo.WrapHandler(http.StripPrefix("/swagger/", http.FileServer(http.FS(swaggerUI)))))
}importgo-swaggerswagswaggest/restopeanapi.toolsopenapi-typescriptHere's how you can generate the Typescript code for local or remote schemas:# Local schema
npx openapi-typescript openapi.yaml -o ./client/schema.d.ts

# Remote schema
npx openapi-typescript https://.../openapi.yaml -o ./client/schema.d.tsOpenAPI is a de-facto standard for designing, implementing and consuming the REST APIs, so it's crucial to understand how it works. I hope this article has provided a useful introduction to OpenAPI Specification, as well as practical tips and examples for how to use OAS to architect, implement and consume APIs.Multiplayer]]></content:encoded></item><item><title>Go 1.24 - Weak Pointers, Generic Aliases, Tool go.mod directive &amp; More</title><link>https://youtu.be/cmtFI9eZ_UE</link><author>/u/JollyShopland</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 14:57:54 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go 1.24&apos;s `omitzero` is another one of the best additions to the ecosystem in years</title><link>https://www.jvt.me/posts/2025/02/12/go-omitzero-124/</link><author>/u/profgumby</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Feb 2025 10:21:18 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[As someone who enjoys capitalising on my blog's successes, as well as Go 1.24 being released ~12 hours ago, I thought I'd write a post about , too.Ignoring optional fields in JSON, with  (prior to Go 1.24)When you have a struct that's being converted to JSON, you've likely got some fields that are optional.For instance, let's take the following struct:Let's say that the  should only be set after the underlying record in the database has been updated  its creation.With this in mind, we may only want to return the field's value in JSON responses, if it has a value.If we were to use the above struct, and marshal the data (with  at its zero value):This then results in an unwanted  JSON field:As of Go 1.23 (and prior) the best way to make  optional is to use an optional pointer and the  JSON struct tag:This allows you to not specify  resulting in its zero value () which will be treated as absent via the  struct tag, and therefore will not get marshalled:This works  but can also be a little awkward to work with, given you now have to work with a pointer.For those who may recognise me as the co-maintainer of , you will appreciate that how best to (un)marshal JSON in Go is something near to my heart, and over the years we've had various attempts to try and simplify this experience for our users, none of which have quite hit the mark.Ignoring optional fields in JSON, with  (from Go 1.24)However, from Go 1.24, we now have the  JSON struct tag which allows us to use the  zero value of a type as an indication for it not to be marshalled.This allows us to use the following struct definition:Notice that we can now use a non-pointer type, which improves how we're able to interact with the type, and it marshals as expected:As noted, one key thing is the reduction of dealing with a lot of extra pointers. Although in this contrived example, there is only one pointer, this can get quite awkward when i.e. nesting structs with optional fields:This can be a bit of a drag when working with code like this, and gets fairly repetitive, so if you can avoid the pointer-heavy types, that's awesome!As noted in the proposal for , this also has clearer semantics, given that  could be called "empty", but does not get treated as "empty" according to .If you've not had a play with 's interactive tour of Go 1.24 yet, it has a good section on  which shows this more interactively, and allows you to edit the code snippets in the browser and re-run them.]]></content:encoded></item><item><title>Go 1.24 arrives</title><link>https://golangweekly.com/issues/542</link><author></author><category>dev</category><category>go</category><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><source url="https://golangweekly.com/">Dev - Golang Weekly</source><content:encoded><![CDATA[DB Fiddle is a handy online database 'sandbox' for playing with various versions of MySQL, Postgres, and SQLite direct from the browser.]]></content:encoded></item><item><title>Just released - golangci-lint v1.64.2 - support for go 1.24.0</title><link>https://www.reddit.com/r/golang/comments/1inbrl3/just_released_golangcilint_v1642_support_for_go/</link><author>/u/Velkow</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 22:43:21 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/Velkow ]]></content:encoded></item><item><title>I have made my first golang CLI</title><link>https://www.reddit.com/r/golang/comments/1inb0md/i_have_made_my_first_golang_cli/</link><author>/u/Halabito8</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 22:12:05 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built a CLI tool in Go to help me stay on top of my ClickUp tasks. I‚Äôve got a bunch of lists, and it‚Äôs a pain to figure out what to tackle first since ClickUp doesn‚Äôt let me see everything in one place easily.This tool pulls all my to-dos and in-progress tasks, then generates two reports:A single merged list sorted by priority and due date.Tasks grouped by their respective lists.Now, I can quickly see what‚Äôs due today and what actually matters. It‚Äôs just a reporting tool for now, but it‚Äôs already making my workflow way smoother.]]></content:encoded></item><item><title>Need help in deciding Gorm vs sqlc</title><link>https://www.reddit.com/r/golang/comments/1in73ju/need_help_in_deciding_gorm_vs_sqlc/</link><author>/u/gwwsc</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 19:31:09 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Should I use gorm or sqlc for my project? I am new to go. I have tried both. In gorm it feels more like getting to know who to work with things the gorm way and it is expected because with orm you have to follow their conventions.But the thing with sqlc is how do I define my model files in code? In gorm atleast I have the model fiels to reference the table structure.With sqlc I have to rely on the sql migration files. Is this a good approach?]]></content:encoded></item><item><title>Go 1.24 is released (self.golang)</title><link>https://www.reddit.com/r/golang/comments/1in5nuw/go_124_is_released_selfgolang/</link><author>/u/rtuidrvsbrdiusbrvjdf</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 18:33:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/rtuidrvsbrdiusbrvjdf ]]></content:encoded></item><item><title>Go 1.24.0 tagged</title><link>https://www.reddit.com/r/golang/comments/1in559i/go_1240_tagged/</link><author>/u/khnorgaard</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 18:12:48 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/khnorgaard ]]></content:encoded></item><item><title>SIPgo new release</title><link>https://www.reddit.com/r/golang/comments/1in3n15/sipgo_new_release/</link><author>/u/emiago</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 17:11:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/emiago ]]></content:encoded></item><item><title>Simple strategy to understand error handling in Go</title><link>https://www.reddit.com/r/golang/comments/1in0tiw/simple_strategy_to_understand_error_handling_in_go/</link><author>/u/zakariachahboun</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 15:13:53 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Error handling in Go is a topic that often sparks debate. Some prefer wrapping errors for more context, while others stick to returning and checking errors as simply as possible.I‚Äôve been refining my own approach over time, balancing readability, maintainability, and practicality. I recently wrote an article discussing some key strategies and best practices I use. Would love to hear how others handle errors in their Go projects !This is a summary of the strategy for beginners:Low-level functions (utils, DB calls, API calls, etc.)Return errors as-is (don't log)Callers higher up should decide how to handle the errorBusiness logic (services, middle-layer functions)Adds context while preserving original error detailsHTTP Handlers, CLI commands, or main()Log errors, return user-friendly messagesAvoid exposing internal details to usersHow do you structure error handling in your Go projects? Any techniques or best practices that have worked well for you?]]></content:encoded></item><item><title>Rendering React on Golang with V8Go</title><link>https://itnext.io/rendering-react-on-golang-with-v8go-fd84cdad2844?source=friends_link&amp;amp;sk=040bff7b0a2ca9d02179c7897476ace4</link><author>/u/congolomera</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 13:03:36 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Server-side rendering (SSR) is essential for improving web performance, SEO, and initial page load experience. React provides built-in SSR capabilities through , but this typically requires a Node.js environment.What if we want to run and render React components without Node.js? This could be useful in cases where:We‚Äôre building a Go-based web server and want to keep everything self-contained.We don‚Äôt want to spin up an external Node.js process just to render React.We‚Äôre running in environments with limited dependencies, such as embedded systems or lightweight containers.For more details, you can see the full source code on GitHubhttps://github.com/highercomve/go-react-ssrInstead of running a separate Node.js process, we can embed , the JavaScript engine that powers Chrome, directly into our Go application using .This approach provides several benefits:: No need to start and communicate with an external Node.js process.: Everything runs inside the Go application, reducing complexity.: We control how the JavaScript‚Ä¶]]></content:encoded></item><item><title>Optimization problems in high-performance programs with Go</title><link>https://www.reddit.com/r/golang/comments/1imo7jt/optimization_problems_in_highperformance_programs/</link><author>/u/JotaEspig</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 02:36:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello guys, I'm currently working on a chess engine using golang, where one of the most important things about it is the performance. After running a profiler I got this:Showing nodes accounting for 21.63s, 48.64% of 44.47s totalDropped 1385 nodes (cum <= 0.22s)Showing top 15 nodes out of 18811.36s 25.55% 25.55% 11.43s 25.70% runtime.cgocall C:\Program Files\Go\src\runtime\cgocall.go:1672.35s 5.28% 30.83% 2.35s 5.28% runtime.stdcall2 C:\Program Files\Go\src\runtime\os_windows.go:10002.06s 4.63% 35.46% 2.06s 4.63% runtime.stdcall1 C:\Program Files\Go\src\runtime\os_windows.go:9911.06s 2.38% 37.85% 1.06s 2.38% runtime.stdcall0 C:\Program Files\Go\src\runtime\os_windows.go:9820.71s 1.60% 39.44% 0.71s 1.60% runtime.scanobject C:\Program Files\Go\src\runtime\mgcmark.go:14460.68s 1.53% 40.97% 0.68s 1.53% runtime.stdcall3 C:\Program Files\Go\src\runtime\os_windows.go:10090.59s 1.33% 42.30% 0.59s 1.33% runtime.procyield C:\Program Files\Go\src\runtime\asm_amd64.s:8070.50s 1.12% 43.42% 0.50s 1.12% runtime.stdcall4 C:\Program Files\Go\src\runtime\os_windows.go:10180.44s 0.99% 44.41% 0.44s 0.99% runtime.stdcall7 C:\Program Files\Go\src\runtime\os_windows.go:10450.38s 0.85% 45.27% 0.38s 0.85% runtime.memclrNoHeapPointers C:\Program Files\Go\src\runtime\memclr_amd64.s:930.38s 0.85% 46.12% 0.38s 0.85% runtime.scanblock C:\Program Files\Go\src\runtime\mgcmark.go:13620.31s 0.7% 46.82% 0.31s 0.7% runtime.scanblock C:\Program Files\Go\src\runtime\mgcmark.go:13590.29s 0.65% 47.47% 0.29s 0.65% runtime.(*mspan).base C:\Program Files\Go\src\runtime\mheap.go:4920.25s 0.56% 48.64% 0.40s 0.9% gce/pkg/chess.(*Board).IsKingInCheck D:\jotin\Documents\Informatica\projects\go-chess-engine\pkg\chess\board.go:150Apparently, the cpu usage is mostly at runtime. Why is that? How can I possibly avoid this?I already try to preallocate everythin I can, but not so much improvement.At the moment, the program can process and average of 25k nodes per seconds (node is a final position). One of the best engines in the world (Stockfish) runs at 2000 knps (2 million nodes per second). I would love to reach 100 knps. Any idea?]]></content:encoded></item><item><title>Go 1.24 is released!</title><link>https://go.dev/blog/go1.24</link><author>Junyang Shao, on behalf of the Go team</author><category>dev</category><category>official</category><category>go</category><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Dev - Golang Blog</source><content:encoded><![CDATA[
      Junyang Shao, on behalf of the Go team
      11 February 2025
      Today the Go team is excited to release Go 1.24,
which you can get by visiting the download page.Go 1.24 comes with many improvements over Go 1.23. Here are some of the notable
changes; for the full list, refer to the release notes.Several performance improvements in the runtime have decreased CPU overhead
by 2‚Äì3% on average across a suite of representative benchmarks. These
improvements include a new builtin  implementation based on
Swiss Tables, more efficient
memory allocation of small objects, and a new runtime-internal mutex
implementation.The  command now provides a mechanism for tracking tool dependencies for a
module. Use  to add a  directive to the current module. Use
 to run the tools declared with the  directive.
Read more on the go command in the release notes.The new  analyzer in  subcommand reports common mistakes in
declarations of tests, fuzzers, benchmarks, and examples in test packages.
Read more on vet in the release notes.Standard library additionsImproved WebAssembly supportGo 1.24 adds a new  directive for Go programs to export
functions to the WebAssembly host, and supports building a Go program as a WASI
reactor/library.
Read more on WebAssembly in the release notes.Please read the Go 1.24 release notes for the complete and
detailed information. Don‚Äôt forget to watch for follow-up blog posts that
will go in more depth on some of the topics mentioned here!Thank you to everyone who contributed to this release by writing code and
documentation, reporting bugs, sharing feedback, and testing the release
candidates. Your efforts helped to ensure that Go 1.24 is as stable as possible.
As always, if you notice any problems, please file an issue.]]></content:encoded></item><item><title>How popular is sqlc in production go projects??</title><link>https://www.reddit.com/r/golang/comments/1imhs5l/how_popular_is_sqlc_in_production_go_projects/</link><author>/u/trash-dev</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 10 Feb 2025 21:34:25 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I've started building my first project in golang to build a multi vendor e-commerce application backend on my own.I chose to go with sqlc over gorm to do my db queries. And it has been great. (Chose to go with it since I felt like gorm lacked a certain sense of beauty/simplicity)But I wonder how widely is it used in production applications. Or is gorm the standard way most companies prefer?About me: a hobbyist programming enthusiast to now actively learning programming to get a job in tech. Learning go backend since currently I'm too grub brained to go with any harder low level languages. ]]></content:encoded></item><item><title>Some real life examples of Golang usage in devops?</title><link>https://www.reddit.com/r/golang/comments/1imhfas/some_real_life_examples_of_golang_usage_in_devops/</link><author>/u/Vyalkuran</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 10 Feb 2025 21:19:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello everyone! I do backend engineering in Java but recently I've been taking an interest into devops/infra stuff, and I often hear or even see in job listings that Go experience is recommended. What kind of code do you actually write? From my limited experience, I have yet to see the usecase for it. I would assume it's to automate something but I can't really think what. Like.... with most tools you already interact with the CLI no? Like terraform... kubectl, aws CLI. Do you just automate those and run an EXE?I'd really like to do some project for my portfolio but can't really think where or what to begin with.]]></content:encoded></item><item><title>Automation with jq/yq and titpetric/etl</title><link>https://www.reddit.com/r/golang/comments/1imcs0j/automation_with_jqyq_and_titpetricetl/</link><author>/u/titpetric</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 10 Feb 2025 18:13:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I maintain a few sources of truth for automation jobs in json and yaml and often reach for jq/yq for extracting the relevant parts for automation. Sometimes that automation sources and processes data in multiple steps, a sort of DAG, and I end up piping and storing json on disk which makes it from my perspective quite clunky because now I have to design a filesystem layout to hold structured data.My approach was to write titpetric/etl, a complementary tool that pipes the data to a database, and includes sqlite support (as well as mysql, postgres, if some data persistence is required). I'm orchestrating some continous testing/ data analysis job which is a fancy loop over a git repo, and it's nice to have a database ingest with a create table statement and a few lines of bash.I find it super useful for devops automation and support tooling where some part of a live system may be probed and the result extracted as a relational sqlite .db file or directly updated in a database. The main benefit is that it's minimal and json first. Vuex for bash, if someone is familiar.rant: Maybe airflow or something else is the thing, but i had it through the roof with yaml encoded bash scrips (gha, taskfiles, etc.) and python and AI code, and would rather make the bash experience nicer for this. Less code needs to be written. /rant]]></content:encoded></item><item><title>Go Module Mirror served backdoor to devs for 3+ years</title><link>https://arstechnica.com/security/2025/02/backdoored-package-in-go-mirror-site-went-unnoticed-for-3-years/</link><author>/u/guitarpawat</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 10 Feb 2025 17:55:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[A mirror proxy Google runs on behalf of developers of the Go programming language pushed a backdoored package for more than three years until Monday, after researchers who spotted the malicious code petitioned for it to be taken down twice.The service, known as the Go Module Mirror, caches open source packages available on GitHub and elsewhere so that downloads are faster and to ensure they are compatible with the rest of the Go ecosystem. By default, when someone uses command-line tools built into Go to download or install packages, requests are routed through the service. A description on the site says the proxy is provided by the Go team and ‚Äúrun by Google.‚ÄùSince November 2021, the Go Module Mirror has been hosting a backdoored version of a widely used module, security firm Socket said Monday. The file uses ‚Äútyposquatting,‚Äù a technique that gives malicious files names similar to widely used legitimate ones and plants them in popular repositories. In the event someone makes a typo or even a minor variation from the correct name when fetching a file with the command line, they land on the malicious file instead of the one they wanted. (A similar typosquatting scheme is common with domain names, too.)The malicious module was named boltdb-go/bolt, a variation of widely adopted boltdb/bolt, which 8,367 other packages depend on to run. The malicious package first appeared on GitHub. The file there was eventually reverted back to the legitimate version, but by then, the Go Module Mirror had cached the backdoored one and stored it for the next three years.‚ÄúThe success of this attack relied on the design of the Go Module Proxy service, which prioritizes caching for performance and availability,‚Äù Socket researchers wrote. ‚ÄúOnce a module version is cached, it remains accessible through the Go Module Proxy, even if the original source is later modified. While this design benefits legitimate use cases, the threat actor exploited it to persistently distribute malicious code despite subsequent changes to the repository.‚Äù]]></content:encoded></item><item><title>Gowall v.0.2.0 - A swiss army knife CLI for image processing</title><link>https://www.reddit.com/r/golang/comments/1imalc5/gowall_v020_a_swiss_army_knife_cli_for_image/</link><author>/u/FormationHeaven</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 10 Feb 2025 16:47:08 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello all, after a gazillion months i have finally released gowall v0.2.0 yesterday the swiss army knife tool for image processing :) This is my first ever go project that i made 7 months ago and with constant updates its finally in a place where i'm happy with it.First packagement & Image previewAvailable in Arch, NixOS, Fedora (New),Void (otherwise build from source)Terminal Image preview extended to these terminal emulators : Kitty , Ghostty (New), Konsole(New)The tldr of all the features is as follows :Convert a Wallpaper's theme ( Color correction) <- Improved Massive improvements, up to 40-60% performance gains, fixes all rough color transitions, ensuring a smooth color conversion. AI image Upscaling <- New Convert an image to pixel artExtract the color paletteCreate gifs from images <- NewEffects (Mirror,flip,grayscale,brightness) <- NewReplace a specific color in an imageDraw on the Image (borders)Remove the background of the imageAlso quality improvements for gowall to be used in scripts, see here & more config.yml options.Lastly spoilers for the main feature of the next update : OCR ]]></content:encoded></item><item><title>Entity &amp; dialogue editor using modernc.org/tk9.0</title><link>https://opu.peklo.biz/p/25/02/10/1739201244-304f9.jpg</link><author>/u/0xjnml</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 10 Feb 2025 15:28:57 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>